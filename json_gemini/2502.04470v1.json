{"title": "Color in Visual-Language Models: CLIP deficiencies", "authors": ["Guillem Arias", "Ramon Baldrich", "Maria Vanrell"], "abstract": "This work explores how color is encoded in CLIP (Contrastive Language-Image Pre-training) which is currently the most influential VML (Visual Language model) in Artificial Intelligence. After performing different experiments on synthetic datasets created for this task, we conclude that CLIP is able to attribute correct color labels to colored visual stimulus, but, we come across two main deficiencies: (a) a clear bias on achromatic stimuli that are poorly related to the color concept, thus white, gray and black are rarely assigned as color labels; and (b) the tendency to prioritize text over other visual information. Here we prove it is highly significant in color labelling through an exhaustive Stroop-effect test. With the aim to find the causes of these color deficiencies, we analyse the internal representation at the neuron level. We conclude that CLIP presents an important amount of neurons selective to text, specially in deepest layers of the network, and a smaller amount of multi-modal color neurons which could be the key of understanding the concept of color properly. Our investigation underscores the necessity of refining color representation mechanisms in neural networks to foster a more comprehensive comprehension of colors as humans understand them, thereby advancing the efficacy and versatility of multimodal models like CLIP in real-world scenarios.", "sections": [{"title": "1. Introduction", "content": "In the last decade artificial intelligence (AI) has significantly progressed in the construction of deep trained models able to solve vision and language problems with a significant efficiency. One particular achievement has been the construction of multi-modal models, namely Visual-Language models (VLMs) with the ability to associate textual and visual descriptions to seek different tasks [5]. The main advantage of these models over traditional CNNs is their ability to correlate any given text with any given image thanks to the combination of their text encoder and image encoder. One VLM worth mentioning is CLIP (Contrastive Language-Image Pre-training), which has been the most successful due its competitive results and generalization capabilities. It was the first model trained with contrastive learning [3] and was introduced by OpenAI in 2021 [10], who released the model to the community accelerating its impact both in research and industry applications.\nCLIP is composed by an image encoder which is a Resnet-like architecture [7], plus a text encoder based on a transformer [15], finally a cosine similarity between the encoded texts and the encoded images is computed as a measure of how likely a certain text represents an input image. One of the main advantages of CLIP is how it excels at zero-shot learning, allowing it to perform a task without needing task-specific fine-tuning thanks to its generalization capability and leveraging its understanding of a wide range of visual and textual concepts. This is achieved through an initial pre-training of the visual encoder with ImageNet dataset [4], followed by a subsequent training using 400 million of image-text pairs that were collected from the internet. These pairs consist of images with their corresponding captions or descriptions. Thanks to this training, CLIP learns to associate images with their corresponding text descriptions and distinguishing them from unrelated image-text pairs using a contrastive loss function.\nCLIP is specially interesting in color categorization because its ability to understand text anso improves its color categorization as shown in [1]. This improvement could be due to one of the most interesting properties that emerge from this training: multi-modal neurons[6],[13]. Multi-modal neurons are units with a strong activation for a certain concept regardless of its representation (text, realistic image, drawing ...). This is reminiscent of a similar phenomenon in some human neurons, which fire in response to images, whether they are photos, drawings, or even words of the same concept [9], making this model specially interesting to study for its parallelism with the human brain.\nAs the rest of Deep Neural Networks engines, CLIP presents a black-box nature and lacks a clear explanation about how knowledge is embedded in both encoders. Understanding multi-modal neurons can sheer light on how CLIP works, by analyzing what stimulus activates them. On this topic, [6] research proves the concept of multi-modal neurons within CLIP, which respond to specific concepts across both text and image domains and MultiViz framework [8] provides a comprehensive approach to analyzing and understanding the internal mechanics of multi-modal models like CLIP by identifying the importance of individual inputs in the overall prediction process, examining the relationships and dependencies between multiple inputs and interpreting the contribution of every features to the output."}, {"title": "2. Color predictions on basic images", "content": "In this section we aim to perform preliminary experiments to explore how CLIP associates color labels to a particular image. To test this ability, we have created a dataset of images containing an homogeneous color background and one basic colored shape. To generate these images, we have used 8 basic shapes (triangle, square, circle, amongst others) and 11 representative universal colors which are colors with a common color term in most developed languages [2] with fixed RGB values. The dataset contains 500 images for each possible combination with different rotations, positions and scale of the shapes, totalling 440,000 images (8 shapes x 11 background colors x 10 object colors x 500 samples). Setting CLIP with the color labels to be associated in the input text as it is shown in figure 1 we perform the following experiments:"}, {"title": "3. Color predictions on text", "content": "Once we have explored CLIP performance in color assignment to basic images, we will explore how color behaves in colored text. To this end, we have set the classical Stroop test [14] where the task is to predict the color of a word in a colored font, with the particularity that the word is a color name. With this task we want to evaluate if CLIP is able to distinguish the semantics of a question that asks not to read but to perceive the color."}, {"title": "4. Color in CLIP Visual Encoder", "content": "Once we concluded several deficiencies in color label assignment, in this section we explore possible causes for these drawbacks. Our exploration is done at the neuron level. Firstly, we analyse the generic Color Selectivity Index of individual neuron units as it was definded in [11], where the Color Selectivity index is calculated by finding the 100 top scoring patches over a large dataset (Imagenet), and calculating the difference in activation by those same patches in gray scale, with a high color selectivity meaning that color is important to activate a neuron, and a low selectivity index meaning that only the shape was important to activate a neuron. Secondly, we propose an in-depth analysis of the activation of the CLIP individual neurons provoked by the Stroop dataset in CLIP by defining a Color-Label Selectivity Index, and classifying the neurons over the network based on the stimulus of their activation.\nIn figure 2 we show the distribution of color selectivity indexes for the neurons of each block of convolutional layers of the CLIP Visual Encoder. Overall, we can see that the ratio of color selectivity is a bit lower than the one usually found in object recognition models (e.g. see fig.4 (a) in [11]). This could be due to the subsequent training process that CLIP goes through after the initial training on Imagenet. The training process on a large Internet dataset to acquire text understanding skills could have shifted color selective neurons to pattern selective neurons necessary to accommodate reading abilities. In figure 3, we show the hue distribution of the color selective neuron features we have found in CLIP. Despite the second training on a larger dataset, CLIP's hue selectivity maintains a high correlation with the ImageNet hue distribution (Pearson's correlation coefficient of 0.965), which proves the posterior training has not reduced the overall distribution of the selectivity properties."}, {"title": "Activation Analysis", "content": "In pursuing the causes of the color deficiencies found in the previous experiments, we propose a new Color-Label Selectivity Index inspired on the Class selectivity Index proposed by Rafegas et-al in [12]. This new index, $f_c$, measures the relative frequency of each color label c for a given neuron, $n_{i,L}$, and it is estimated as:\n$f_c (n_{i,L}) = \\frac{\\sum w_{j,i,L}}{\\sum w_{l,i,L}}$ (1)\nwhere $N_c$ refers to the number of images, among the N cropped top scoring images activating this neuron that contains the specific color label c. Considering $n_{i,L}$ is the i-th neuron of layer L, we denote its activation value as $w_{s,i,L}$, when the input image is s. By computing this index over all the activation's provoked by the Stroop dataset we can classify all the neurons in 5 different categories with the following criteria:\nColor: Neuron with high Color-Label Selectivity Index for a specific color label, independently if the label is attributed to the background or to the font, it activates for a specific color label. We can differentiate between Chromatic and Achromatic color labels.\nAny Word: Neuron with a high activation for any word. It had a high activation for the Stroop Dataset as well as for images of Imagenet containing text, which means it activates for any kind of written word independently of its color or meaning.\nColor Word: Neuron with high Color-Label Selectivity index for a specific color word, i.e. it activates only for one specific color name independently of the color of the font.\nColor Multimodal: Neuron with high Color-Label Selectivity index for a specific color word, for the same color label of font, and for the same color label of background, i.e. it activates for one specific color in all its modalities, it captures the concept of the color in full.\nNot activated: Neuron with low activation to images in Stroop dataset, which means that those neurons are not selective to color in any of its modalities. It maximum activation in"}, {"title": "Conclusions", "content": "In this work we explored how one of the most influential Visual-Language models in AI deals with color labelling tasks. We have performed a set of basic experiments to report how CLIP behaves in front of specific color tasks and we found out some that we summarized in next lines:\n\u2022 Achromatic stimuli are not related to the color concept. It presents important errors when asked to assign black, white or grey labels.\n\u2022 Preference to label the predominant color. When asked for a global assignment, it labels the larger colored area, except when it is achromatic.\n\u2022 Ability to attribute the color label to the asked image part. It properly attributes the color label to object or background accordingly with the input text. Again with the exception of achromatic colors. If one of the parts is achromatic, the assigned label is always the chromatic one, independently of the part asked in the input question.\n\u2022 Stroop effect with words written in white background. It leans towards reading (80%) rather than assigning the color of the font (16%).\n\u2022 Chomatic Backgrounds distract the reading preference in the Stroop test. When distracted by a Chromatic background, CLIP still prioritizes reading (59%), but with a shift towards answering the color of the background (38%), and completely ignoring the main objective, that is giving the color of the font (2%).\nLooking for an explanation to this behaviour is a hard task due the black-box nature of these models. We made some step toward this end. We analysed the internal representation at the neuron unit level. We developed a new Selectivity Index to identify neurons presenting a preference for specific types of labels. We have identified a set of Color Multi-Modal neurons, that combine its selectivity to written color words and the corresponding color stimulus. Interestingly, these neurons are found in shallow layers of the network, that could be due to the intrinsic nature of color as a generic attribute of any concept.\nFrom the previous conclusions we hypothesize that the lack of understanding of achromatic stimuli as colors, could be due to their prevalence as image backgrounds in many datasets. A possible solution for a more robust and human-like minded model could be training the models more progressively, ensuring they learn from basic common-sense concepts to more complex ones, in a similar way as children learn about the world."}]}