{"title": "Diffusion Models for Tabular Data: Challenges, Current Progress, and Future Directions", "authors": ["Zhong Li", "Qi Huang", "Lincen Yang", "Jiayang Shi", "Zhao Yang", "Niki van Stein", "Thomas B\u00e4ck", "Matthijs van Leeuwen"], "abstract": "In recent years, generative models have achieved remarkable performance across diverse applications, including image generation, text synthesis, audio creation, video generation, and data augmentation. Diffusion models have emerged as superior alternatives to Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) by addressing their limitations, such as training instability, mode collapse, and poor representation of multimodal distributions. This success has spurred widespread research interest. In the domain of tabular data, diffusion models have begun to showcase similar advantages over GANs and VAEs, achieving significant performance breakthroughs and demonstrating their potential for addressing unique challenges in tabular data modeling. However, while domains like images and time series have numerous surveys summarizing advancements in diffusion models, there remains a notable gap in the literature for tabular data. Despite the increasing interest in diffusion models for tabular data, there has been little effort to systematically review and summarize these developments. This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area. This survey addresses this gap by providing a comprehensive review of diffusion models for tabular data. Covering works from June 2015, when diffusion models emerged, to December 2024, we analyze nearly all relevant studies, with updates maintained in a GitHub repository. Assuming readers possess foundational knowledge of statistics and diffusion models, we employ mathematical formulations to deliver a rigorous and detailed review, aiming to promote developments in this emerging and exciting area.", "sections": [{"title": "I. INTRODUCTION", "content": "Tabular data is a data modality in which information is orga-nized into rows, representing individual records, and columns,representing features or attributes. It is ubiquitous in real-world domains, including but not limited to healthcare [1],finance [2], education [3], transportation [4], psychology [5],etc. The demand for high-quality generative models in thesedo mains is acute due to data privacy regulations such asGDPR [6] and CCPA [7]. Consequently, real user data isoften restricted from public release, whereas synthetic datagenerated by generative models can preserve machine learningutility while being legally shareable [8]. Beyond privacyconcerns, real-world tabular datasets often contain missingvalues, which can arise due to human errors or technicalmalfunctions like sensor failures. To address this, generativemodels have been employed for missing value imputation,demonstrating promising performance [9]. Furthermore, tab-ular data often presents challenges related to imbalancedclass distributions [10], where certain categories dominate andresult in biased models. Generative models can help mitigatethis issue by generating synthetic samples for underrepre-sented classes, improving model performance on minoritycategories [11]. Overall, these multifaceted applications under-score the growing importance of generative models for tabulardata, ranging from privacy protections [12], [13], missingvalue imputation [14], [15], to training data augmentation [16].\nDeep generative models mainly include Energy-based Mod-els (EBMs) [17], Variational Autoencoders (VAEs) [18], Gen-erative Adversarial Networks (GANs) [19], AutoregressiveModels [20], Normalized Flows [21], and Diffusion Models[22]. Diffusion Models offer several advantages that makethem a preferred choice for many generative tasks. UnlikeGANs, which often suffer from mode collapse and unstabletraining due to adversarial loss dynamics [23], Diffusion Mod-els are inherently stable and effectively capture the full datadistribution. Moreover, they produce sharper and more realisticoutputs than VAEs, avoiding the blurry reconstructions inimages caused by Gaussian latent space assumptions [24].Compared to EBMs, Diffusion Models do not rely on com-putationally expensive sampling methods like Markov chainMonte Carlo (MCMC) and are easier to train [25]. They alsoovercome the limitations of Normalizing Flows by avoidingbijective transformation constraints and Jacobian calculations[21], [25], enabling more powerful expressivity. Finally, unlikeAutoregressive Models, Diffusion Models are not constrainedby sequential generation [26], allowing them to utilize flexiblearchitectures to learn arbitrary data distributions for diversedata types. These strengths together make Diffusion Modelsa highly flexible and robust choice for generative modelingtasks.\nMore concretely, diffusion models [22], [27], [28] arelikelihood-based generative models designed to learn the un-derlying distribution of training data and generate samples thatclosely resemble it. Typically, a diffusion model comprises aforward diffusion Markov process, which gradually transformstraining data into pure noise, and a reverse denoising Markovprocess, which reconstructs realistic synthetic data from thenoise. Given the remarkable performance of diffusion modelsin generating high-quality and diverse synthetic samples acrossvarious domains-such as images [27], [28], audio [29], [30],text [31], [32], video [33], and graphs [34] recent studies[35], [8], [36], [37], [38], [39] have explored their applica-"}, {"title": "II. GENERATIVE MODELS FOR TABULAR DATA", "content": "In this section, we start by reviewing the development history of generative models for tabular data. We then discuss the unique characteristics of tabular data and the challenges these pose for developing generative models. Finally, we outline the taxonomy of diffusion models for tabular data adopted in this survey, categorized based on their applications.\nAs shown in Figure 1, prior to the advent of models explicitly designed for data generation (e.g., VAEs [48], GANs [19], and Diffusion Models [22]), probabilistic models like Copula [49], Gaussian Mixture Models [50] and Bayesian Net-works [51] were commonly employed for data synthesis. Later, specialized methods, including distance-based approaches like SMOTE [52] and its variants (e.g., Borderline-SMOTE [53], SMOTETomek [54], and SMOTE-ENC [55]), as well as ADASYN [56], and probabilistic models like Synthpop [57], were introduced for data synthesis and imputation. However, distance-based methods, including SMOTE and its variants, encounter challenges when dealing with large datasets and complex data distributions. Additionally, probabilistic meth-ods, such as Copulas and Synthpop, often struggle with heterogeneous data, impose predefined distributions, and are prone to assumption biases [45]. In contrast, deep generative models explicitly designed for data generation have gained in-creasing prominence in the tabular domain, offering significant advancements and more successful applications compared to traditional tabular data generation techniques. For example,\nVAE [48] based methods such as TVAE [58] and GOG-GLE [59] are shown to achieve superior performance. Importantly, GOGGLE is the first to explicitly model the correlations among features, by using a VAE-based model with GNN as the encoder and decoder models. However, VAEs-based methods are prone to certain limitations, including blurry outputs in the generated data due to the inherent randomness introduced by the latent space and potential difficulty in balancing reconstruction loss and regularization during training, which can affect the qual-ity of the synthetic data. Additionally, VAEs may strug-gle with accurately capturing multimodal distributions, a common characteristic in real-world tabular datasets.\nGAN [19] based methods have demonstrated promising results in tabular data synthesis, with key examples in-cluding CTGAN [58] and its variants such as CTABGAN"}, {"title": "B. Challenges with Generative Models for Tabular Data", "content": "Training generative models in tabular data can be inherently more challenging than in image or text data due to the following challenges.\nThis phenomenon often happens in real-world tabular datasets for several reasons [68], e.g., privacy\n1) Missing Values:"}, {"title": "III. PRELIMINARIES OF DIFFUSION MODELS", "content": "In this section, we first introduce the core mechanism of diffusion models, covering both the forward diffusion process (gradual addition of noise) and the reverse process (learn-ing to denoise). We then present prominent diffusion model frameworks, including DDPMs (Gaussian Diffusion Models), Multinomial Diffusion Models, Score-based Generative Mod-els (SGMs), Score-based Generative Models through Stochas-tic Differential Equations (SDEs), and conditional diffusion models. For clarity, the notation used throughout this survey is summarized in Table I.\nDiffusion probabilistic models, or more commonly known asdiffusion models [22], are deep generative models defined froma forward diffusion process and a reverse denoising process.Specifically, the diffusion process aims to gradually corrupt asample $x_0$ (drawn from the training data distribution $q_{data}(\\cdot)$)to a noisy instance $x_T$ (defined by a prior distribution $q_{noise}(\\cdot)$)by using the following process:\n$\\displaystyle q(x_{1:T}|x_0) := \\prod_{t=1}^{T} q(x_t|x_{t-1}),$\n(1)\nwith $q(x_t|x_{t-1})$ the forward transition probability. Meanwhile,the denoising process attempts to remove noises and generatea synthetic but realistic sample $x_0$ from $x_T$ as follows:\n$\\displaystyle p_\\theta(x_{0:T}) := p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t),$\n(2)\nwith $p_\\theta(x_{t-1}|x_t)$ an approximation of the reverse of theforward transition probability, which is learned by a neuralnetwork with parameters $\\theta$. Particularly, they learn $\\theta$ byminimizing the following variational upper bound ($L_{VUB}$) onthe negative log-likelihood:\n$\\displaystyle - \\log p(x) \\leq \\mathbb{E}_{q(x_1|x_0)} [-\\log p_\\theta(x_0|X_1)]$\n$L_0$ ($L_{recons}$)\n$\\displaystyle + D_{KL}[q(X_T|X_0)||p(X_T)]$\n$L_T$ ($L_{prior}$)\n$\\displaystyle + \\sum_{t=2}^{T} \\mathbb{E}_{q(x_t/x_0)} D_{KL}[q(x_{t-1}|X_t, X_0)||p_\\theta(X_{t-1}|X_t)].$\n(3)\n$L_t$ ($L_{diffusion}$)\nHere, $L_0$ can be interpreted as a reconstruction term thatpredicts the log probability of original sample $x_0$ given thenoised latent $x_1$, while $L_T$ measures how the final corruptedsample $x_T$ resembles the noise prior distribution; Meanwhile,$L_t$ measures how close is the estimated transition probability$p_\\theta(x_{t-1}|x_t)$ to the ground-truth posterior transition probability$q(x_{t-1}|X_t, X_0)$.\nNote that the formulations provided above define only thegeneric structure of diffusion models. Depending on the spe-cific data types (namely continuous or discrete), the definitionsof prior noise distribution $p(x_T)$, forward transition probabil-ity $q(x_t|x_{t-1})$, the reverse of forward transition probability$p_\\theta(x_{t-1}|x_t)$, and their training objectives can be different. Inthe following, we concisely review the key frameworks of dif-fusion models most commonly used in tabular data modeling:\nGaussian diffusion model, 2) multinomial diffusion model,3) score-based generative model, 4) score-based generativemodel via SDEs, and 5) conditional diffusion models.\nGaussian Diffusion Models: Gaussian diffusion models,often known as Denoising Diffusion Probabilistic Models(DDPMs) [27], is a family of diffusion probabilistic modelsthat operate in continuous spaces. They define the diffusionand reverse processes with the following instantiations:\n$\\displaystyle p(x_\\tau) := \\mathcal{N}(x_\\tau; 0, I),$\n(4a)\n$\\displaystyle q(x_t|x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI),$\n(4b)\n$\\displaystyle p_\\theta(X_{t-1}|x_t) := \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x,t)),$\n(4c)\nwhere Gaussian noises are gradually injected into the samplebased on a time-dependent variance schedule $\\{\\beta_t\\}_{t=1}^{T}$, with$\\beta_t \\in (0, 1)$ determining the amount of noise added at timestep t. To approximate $p_\\theta(x_{t-1}|x_t)$, [27] propose to define:\n$\\displaystyle \\mu_\\theta(x, t) = \\frac{1}{\\sqrt{\\alpha_t}} (x_t + \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)),$\n(5)\n$\\displaystyle \\Sigma_\\theta(x, t) = \\sigma_t^2I,$\nwhere $\\sigma_t$ controls the noise level added at time step t, $\\alpha_t :=1 - \\beta_t$, $\\bar{\\alpha}_t := \\prod_{i=1}^{t} \\alpha_i$, and $\\epsilon_\\theta$ is a neural network to predictground truth noise $\\epsilon \\sim \\mathcal{N}(0, I)$ that has been added to noisesample $x_t$. As a result, they propose to optimize the followingsimplified objective function (rather than Eq. 3):\n$\\displaystyle L_{simple}(\\theta) := \\mathbb{E}_{t \\sim U[0, T]} \\mathbb{E}_{x_0 \\sim q(x_0)} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,1)} [\\lambda(t) ||\\epsilon - \\epsilon_\\theta(x_t, t)||^2],$\n(6)\nwhere $\\lambda(t)$ is a weighting function to adjust the noise scales,and $|| \\cdot ||_2$ denotes the Euclidean norm."}, {"title": "2) Multinomial Diffusion Models:", "content": "Multinomial diffusion models [31] [32] is a representative (also the first) diffusion probabilistic model in discrete spaces. Their diffusion and denoising processes operate in discrete spaces, designed to generate categorical data $x_t \\in \\{0, 1\\}^K$ (i.e., one-hot encoding with K distinct values). They are defined as follows:\n$\\displaystyle p(x_\\tau) := Cat(x_\\tau; 1/K),$\n(7a)\n$\\displaystyle q(x_t|x_{t-1}) := Cat(x_t; (1 - \\beta_t)x_{t-1} + \\beta_t/K),$\n(7b)\n$\\displaystyle p_\\theta(x_{t-1}|x_t) := \\sum_{x_0=1}^{K} q(x_{t-1}|X_t, x_0)p_\\theta(X_0|x_t),$\n(7c)\nwith Cat($\\cdot$) categorical distribution and K the number of categories (the computation between scalars and vectors are done in an element-wise way). Importantly, uniform noise (rather than Gaussian noise) is added to the sample according to the noise schedule $\\beta_t$. As we can see, $p_\\theta(x_{t-1}|x_t)$ is parameterized as $q(x_{t-1}|X_t, X_0(\\theta_t, t))$, with $x_0(\\theta_t, t)$ predicted by a neural network, which can be trained via the multinomial diffusion loss defined using Eq. 3.\nIt is important to note that the multinomial diffusion model can handle only one categorical feature at a time. In other words, for a table with C categorical features, C separate multinomial diffusion models would need to be built. More importantly, several novel diffusion models for discrete data have been proposed recently; these will be reviewed in Section VIII-A for better readability."}, {"title": "3) Score-based Generative Models (SGMs):", "content": "For these mod-els, the forward diffusion process follows the same structure as the Gaussian diffusion model, whereas the reverse process is defined differently, as shown below. Given an instance x and its distribution p(x), its score function is defined as $\\nabla_x \\log p(x)$. To estimate the score function, one can train a neural network $S_\\theta(\\cdot)$ with the following objective:\n$\\displaystyle \\mathbb{E}xp(x)||S_\\theta(x) - \\nabla_x \\log p(x)||^2.$\n(8)\nHowever, Song & Ermon [74] point out that the estimated score functions are inevitably imprecise in low density regions when the low-dimensional manifolds are embedded into a high-dimensional space. To mitigate this, in the diffusion process they propose to perturb the original data x with a sequence of random Gaussian noises with intensifying scales $0 < \\sigma_1 < \\dots < \\sigma_T$. In other words, $P_{\\sigma_1} \\approx p(x_0)$,$P_{\\sigma_\\tau} \\approx \\mathcal{N}(0,I)$, and $p_{\\sigma_1} \\approx \\mathcal{N}(x_t; x_0, 0_\\tau I)$. In the reverse process, they utilize a noise-conditioned score network $S_\\theta(\\cdot)$ to approximate $\\nabla_x \\log p_{\\sigma_t}(x)$, which analytically equals to $(x_0 - x_t)/\\sigma_t$. As a result, the training objective is as follows:\n$\\displaystyle \\frac{1}{T} \\sum_{t=1}^{T} \\lambda(\\sigma_t) \\mathbb{E}_{p(x_0)} \\mathbb{E}_{x_t \\sim P_{\\sigma_t}} ||S_\\theta(x_t, t) + \\frac{x_t - x_0}{\\sigma_t^2}||^2,$\n(9)\nAfter training $S_\\theta(\\cdot)$, new samples are generated with the annealed Langevin dynamics (see [74] for details). Note that SGMs are defined in discrete time space, a special case cor-responding to the variance exploding form in the generalized version presented in the sequel."}, {"title": "4) Score-based Generative Models through Stochastic Differential Equations (SDEs) [28]:", "content": "This is a continuous-time generalization of the denoising diffusion models (DDPMs that correspond to variance preserving form) and score based gen-erative models (SGMs that correspond to variance exploding form). Particularly, the diffusion process is defined with the following it\u00f4 stochastic differential equation [75]:\n$\\displaystyle dx = f(x, t)dt + g(t)dw,$\n(10)\nwhere $f(x, t) = f(t)x$, and $f(\\cdot)$, $g(\\cdot)$ are referred to as the drift and diffusion coefficients of $x_t$, respectively. Moreover, w is the standard Wiener process. The most widely studied and commonly used diffusion models can be broadly categorized into three main types: 1) Variance Exploding (VE), 2) Variance Preserving (VP), and 3) sub-Variance Preserving (sub-VP) based on the types of functions $f(\\cdot)$ and $g(\\cdot)$ as follows:\n$\\displaystyle f(x, t) = \\begin{cases} 0, & \\text{if VE,} \\\\ -t x, & \\text{if VP,} \\\\ -t x, & \\text{if sub-VP,} \\end{cases}$\n(11)\n$\\displaystyle g(t) = \\begin{cases} \\sqrt{t}, & \\text{if VE,} \\\\ \\sqrt{t}, & \\text{if VP,} \\\\ \\gamma_t (1 - e^{-2 \\int_0^t \\gamma_s ds}), & \\text{if sub-VP,} \\end{cases}$\n(12)\nwhere $\\gamma_t$ and $\\sigma_t$ are noise functions w.r.t. the time variable t. Meanwhile, the denoising process is defined as the reverse of the diffusion process:\n$\\displaystyle dx = [f(x, t) - g(t)^2 \\nabla_x \\log p_t(x)]dt + g(t)dw,$\n(13)\nwhere w is a Wiener process running backward in time, and the score function $\\nabla_x \\log p_t(x)$ is approximated by a learnable neural network $S_\\theta(x, t)$. However, directly approximating the score function is computationally intractable and thus they propose to train $S_\\theta(\\cdot)$ by estimating the transition probability $\\nabla_{x_t} \\log p(x_t|x_0)$ as follows [28]:\n$\\displaystyle \\arg \\min_\\theta \\mathbb{E}_{t} \\lambda(t) \\mathbb{E}_{x_0} \\mathbb{E}_{x_t|x_0} [||S_\\theta(x_t, t) - \\nabla_{x_t} \\log p(x_t|x_0)||^2]],$\n(14)\nwhere $\\nabla_{x_t} \\log p(x_t|x_0)$ follows the Gaussian distribution and can be collected during the diffusion process. Moreover, $\\lambda(t)$ is used to trade-off between sample quality and likelihood.\nAfter training $S_\\theta(\\cdot)$, we can generate new samples with the following two methods: 1) the predictor-corrector framework, or 2) the probability flow framework. In general, the prob-ability flow framework is preferred due to its fast sampling and exact log-probability computation compatibilities (see [28] for more details). In short, the probability flow employs the following neural ordinary differential equation (NODE) based model [76]:\n$\\displaystyle dx = (f(x, t) - \\frac{1}{2}g(t)^2\\nabla_x \\log p_t (x))dt,$\n(15)\nwhich describes a deterministic process whose marginal prob-ability is equivalent to that of the original reverse SDE (namely Eq. 13) [28]."}, {"title": "5) Conditional Diffusion Models:", "content": "The diffusion models introduced in Sections III-A1, III-A2, III-A3, and III-A4 are all unconditional, meaning the posterior estimator function $p_\\theta(\\cdot)$ does not know the label of the data it is modeling. Given a label vector y, [22] and [65] suggest that this can be achieved through a so-called conditional reverse process in DDPM, defined as:\n$\\displaystyle p_{\\theta,}(X_{t-1}|X_t, y) \\propto p_\\theta(X_{t-1}|X_t)P(y|x_{t-1}),$\n(16)\nwhich requires to train a classifier $p_\\phi(\\cdot)$ and thus is known as classifier-guided DDPM. Dhariwa and Nichol [65] further approximate the logarithm of it with a perturbed Gaussian transition as follows:\n$\\displaystyle \\log(p_{\\theta,}(X_{t-1}|X_t, y)) \\approx \\log(p(z)) + C,$\n(17)\nwhere C is a constant, $z \\sim \\mathcal{N}(\\mu + \\Sigma g, \\Sigma)$, and $g = \\nabla_{x_{t-1}} \\log(p(y|x_{t-1}))|_{x_{t-1}=\\mu}$ is computed from the clas-sifier $p_\\phi(.)$. To avoid training a separate classifier, Ho &\nSalimans [77] propose a classifier-free guided DDPM as follows:\n$\\displaystyle \\hat{\\epsilon}(x_t,y,t) = \\hat{\\epsilon}(x_t, t) + w_g[\\hat{\\epsilon}(x_t, y, t) - \\hat{\\epsilon}(x_t, t)],$\n(18)\nwhere $\\hat{\\epsilon}(\\cdot)$ is the noise estimator $\\epsilon_\\theta(.)$ defined in DDPM.Moreover, $[\\hat{\\epsilon}(x_t, y, t) - \\hat{\\epsilon}(x_t, t)]$ is guidance of y and $w_g$ theguidance weight."}, {"title": "IV. DIFFUSION MODELS FOR DATA AUGMENTATION", "content": "Data augmentation is a long-standing research problem in tabular data [78]. In general, it can be divided into two different tasks: 1) data synthesis, which is the process of generating synthetic data that mimics the characteristics of real-world data. Depending on whether we generate a single table or a set of connected tables, it can be further divided into single table synthesis or multi-relational dataset synthesis; and 2) over-sampling, which balances an imbalanced table by increasing the number of samples in the minority class(es). Particularly, over-sampling can be considered as a special case of single table synthesis where we only generate a part of the table. Accordingly, in this survey, we categorize relevant works into diffusion models for single-table synthesis (including over-sampling) and diffusion models for multi-relational dataset synthesis, providing their formal definitions and reviewing related studies in Section IV-A and IV-B, respectively.\nGiven a table R, we utilize $x_{ij}^t$ to denote the j-th feature value of the i-th sample at time point t. On this basis, let the number of numerical features be $M_{num}$ and the number of categorical features be $M_{cat}$. By reorganizing the order of\n1) Generic Diffusion Models for Single Table Synthesis: In this part, we review generic diffusion models for single table synthesis, which represent the most significant advancements in tabular data generation. Unlike domain-specific models tailored for particular fields, such as healthcare or finance, generic models are designed to handle diverse types of tabular data, including mixed-type features (numerical, categorical) and datasets with varying scales, sparsity, and correlations. These models aim to be universally applicable across different domains, making them highly valuable for a wide range of machine learning tasks.\nSOS [66] is the first to apply score-based generative models (SGMs via SDEs [28]) for tabular data oversampling. Specif-ically, given a training dataset $X_{train}$, which can be dividedinto M distinct subsets $X_1, ..., X_M$ based on the labels ofinterest, they train a separate score network $S_{\\theta^m}$ for eachsubset corresponding to a specific class $X_m$ without alteringthe diffusion or denoising processes. SOS introduces twomethods for generating new instances in the target minorityclass: 1) Style-transfer generation: A sample $x_0$ is first drawnfrom the major class, converted into a noised instance $x_t$ usingthe shared diffusion process, and then denoised into a samplein the target class $(\\hat{x}_0)$ using the specific denoising process ofthe target class $S_{\\theta^{target}}$; and 2) Plan generation: A noise sample$x_T$ is drawn from a Gaussian distribution and denoised into asample in the target class $(\\hat{x}_0)$ using the denoising process ofthe target class $S_{\\theta^{target}}$"}, {"title": "VII. DIFFUSION MODELS FOR ANOMALY DETECTION", "content": "Anomaly detection aims to train diffusion models to learn the \"normal\" distribution of data from the training set and identify anomalies as deviations from this learned distribution in the test data. Formally, it is defined as follows:\nTabADM [128] is the first diffusion-based approach for unsupervised anomaly detection in tabular data. Unlike semi-supervised methods that require pure normal training data, TabADM handles fully unlabeled training data mixed with anomalies. It estimates the data distribution via a robust diffusion model, assigning anomaly scores to test samples based on their likelihood of being generated by the model. Specifically, TabADM comprises two stages: training and inference. During training, it follows the standard DDPM"}, {"title": "VIII. A REVIEW ON DISCRETE DIFFUSION MODELS AND PERFORMANCE EVALUATION METRICS", "content": "Diffusion models for discrete data can be divided into two categories [134]: 1) the first category of works converts discrete structures into a continuous latent space and then directly applies Gaussian diffusion model in the latent space. This line of works include [97], [100], [135], [136], [137], [138], [139], [140]; 2) the second category of works directly defines the diffusion process on discrete structures, and we will focus on this category in the remaining of this section.\n2) Multinomial Diffusion Model: Multinomial Diffusion [31], [141] that provides transition matrices with uniform distribution; The diffusion and denoising processes have been defined in Section III-A1."}, {"title": "B. Performance Evaluation Metrics", "content": "The performance of tabular data generative models is of-ten evaluated from various but complementary perspectives,including fidelity, diversity, utility, privacy, and runtime.\nA-Precision. It measures the fidelity of synthetic data byindicating whether each synthetic sample resembles the realdata [150].\n\u2022The Mean Absolute Error (MAE) is defined as:\n$\\displaystyle MAE = \\frac{1}{n}\\sum_{i=1}^{n} |Y_i - \\hat{Y}_i|,$\n(40)\n\u2022The Root Mean Squared Error (RMSE) is defined as:\n$\\displaystyle RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Y_i-\\hat{Y}_i)^2}.$\n(41)"}, {"title": "IX. DISCUSSION AND CONCLUSIONS", "content": "Thanks to notable advancements in diffusion models for tabular data, some of the challenges associated with generative models for tabular data, as discussed in Section II-B, have been partially addressed. However, many challenges remain unresolved, and new ones continue to emerge. Below, we outline these challenges and propose potential future research directions:\nScalability: Diffusion models are typically computa-tionally intensive, which can be challenging with high-dimensional tabular data. Therefore, more research effortsshould be given to develop techniques for reducing thecomputational cost of diffusion models on large tabulardatasets, such as efficient sampling and training methods.\nEvaluation Metrics: Unlike images where visual qualityis a metric, evaluating tabular data generation qualityis complex and may require domain-specific measuresespecially in domains such as healthcare and finance.\nEnhanced Interpretability: Developing methods to en-hance the interpretability of diffusion models is crucialfor their practical application in tabular data modeling.\ndirections for advancing the field. We aim for this survey to serve as a valuable resource for researchers and practitioners, encouraging further innovation and development in generative diffusion models for tabular data."}]}