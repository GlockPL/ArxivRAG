{"title": "A Federated Approach to Few-Shot Hate Speech Detection for Marginalized Communities", "authors": ["Haotian Ye", "Axel Wisiorek", "Antonis Maronikolakis", "\u00d6zge Ala\u00e7am", "Hinrich Sch\u00fctze"], "abstract": "Hate speech online remains an understudied issue for marginalized communities, and has seen rising relevance, especially in the Global South, which includes developing societies with increasing internet penetration. In this paper, we aim to provide marginalized communities living in societies where the dominant language is low-resource with a privacy-preserving tool to protect themselves from hate speech on the internet by filtering offensive content in their native languages. Our contribution in this paper is twofold: 1) we release REACT (REsponsive hate speech datasets Across ConTexts), a collection of high-quality, culture-specific hate speech detection datasets comprising seven distinct target groups in eight low-resource languages, curated by experienced data collectors; 2) we propose a solution to few-shot hate speech detection utilizing federated learning (FL), a privacy-preserving and collaborative learning approach, to continuously improve a central model that exhibits robustness when tackling different target groups and languages. By keeping the training local to the users' devices, we ensure the privacy of the users' data while benefitting from the efficiency of federated learning. Furthermore, we personalize client models to target-specific training data and evaluate their performance. Our results indicate the effectiveness of FL across different target groups, whereas the benefits of personalization on few-shot learning are not clear.", "sections": [{"title": "1 Introduction", "content": "Hate speech is a global issue that is pervading online spaces and creating an unsafe environment for internet users. Combating online hate is a crucial aspect of content moderation, where common solutions typically rely on machine learning models trained on large datasets. However, these efforts and the resources required are often limited to a few high-resource languages, such as English and German. While efforts have been made to collect hate speech detection datasets in various languages, a significant portion of the world's low-resource languages and their users are still left unprotected against online hate. A key challenge in effective hate speech detection lies in the complex and subjective nature of hate speech, the perception of which varies not only at the individual level, but also across cultures and regions. The issue is exacerbated by a lack of diversity among data collectors, where a discrepancy often exists between annotators and those directly affected by hate speech. Additionally, languages constantly evolve, as does hate speech, where new terminology and expressions frequently emerge.\nTo address these challenges, we develop high-quality, diverse datasets that accurately reflect the experiences of marginalized communities. This is achieved through a prompt-based data collection procedure, carried out by a team of data collectors who are proficient in the target languages and familiar with the nuances of hate speech against specific marginalized groups in their countries. In the resulting localized and context-specific datasets, REACT (REsponsive hate speech datasets Across ConTexts), we provide sentences (positive, neutral, hateful) in eight languages across seven target groups. The dataset will be released under the Creative Commons ShareAlike license (CC BY-SA 4.0).\nA major drawback of common hate speech filtering solutions is their reliance on centralized, server-side processing, where data is transmitted to remote servers for analysis. This means individuals often do not have direct control over the type of content being filtered, which may vary from user to user. A centralized approach is also less flexible to be rapidly adapted to a highly specific target, especially in a low-resource language.\nTo overcome this, we propose an approach uti-"}, {"title": "2 Related Work", "content": "Earlier efforts in the field of toxic and offensive language detection, including hate speech, have contributed to the curation of diverse datasets, predominantly in English and a few other high-resource languages, like German and Arabic. More recent works have developed datasets with more fine-grained details, such as different types of abuse presented in the data and target groups. In a related manner, and employ a template-based data generation process to acquire hate speech datasets which are categorized into subgroups corresponding to individual target groups. Efforts have also been made to expand data collection to multiple languages, including low-resource ones, which is crucial to developing effective hate speech detection models for underrepresented languages."}, {"title": "2.2 Hate speech detection", "content": "In the past few years, Transformer-based language models have emerged as the go-to solution for a wide range of natural language processing tasks. Similarly, hate speech detection has seen the adoption of various Transformer-based language models, including some pre-trained specifically to identify hate and other offensive content."}, {"title": "2.3 Federated learning", "content": "Studies have shown that public data may contain personally identifiable information, which models trained on such data may inadvertently learn, thereby posing privacy risks. Furthermore, with each newly released LLM trained on an increasing data volume, concerns have arisen about depleting publicly available data, which is essential for training such models. A recent study by suggests that we may reach the limit of publicly available data by as early as 2026.\nEffectively leveraging privately held data on user devices in a privacy-preserving way thus presents a promising potential to address the data availability constraints. Federated learning (FL) is a privacy-preserving, decentralized machine learning paradigm that has gained popularity in recent years. Instead of collecting data from individuals and storing it on a central server, FL initializes and trains models locally on each participating device (clients). The data from each client is used to train the local model, and thus never leaves the client device. Subsequently, updates from each client are collected and aggregated on the server using an approach called FederatedAveraging, or FedAvg, which takes a weighted average of the received updates. One of the first applications of FL was improving next-word prediction from Gboard, the Google keyboard. In this case, neural models are improved by aggregating updates from Gboard users without accessing the actual data generated by any single user. Other fields that involve handling private data and have seen the application of FL include finance and medicine. So far, relatively few studies have explored the applicability of FL in hate speech detection. and apply FL on public offensive speech"}, {"title": "2.4 Personalized FL", "content": "The traditional FL framework, as previously discussed, may not be ideal when client data is highly heterogeneous. Studies have demonstrated that heterogeneous, or non-iid (independently and identically distributed), client data may lead to slow convergence of FL, as FL models suffer from \"client drift\". In the context of hate speech detection, a client may represent a highly marginalized group that receives little attention compared to other target groups. Personalizing FL allows for client customization to meet the specific needs of these target groups, while at the same time enhancing the privacy of the user data further by selectively sharing information sent to the server. A straightforward method for client personalization is proposed by with FedPer, which decouples the client model into base (non-personalized) and personalized layers. On a similar note, suggest that learning task-specific representations tailored to each client can yield strong improvements over centralized training. Additionally, propose a method that preserves more local information on the clients using adaptively adjusted weights to combine client and server models. Furthermore, customize client models by specifying a set of local parameters, demonstrating that a large user population benefits from this strategy. Following some of these methods, we apply personalized FL strategies to combine local client-specific adaptations and selective sharing of information to the server."}, {"title": "3 REACT Dataset", "content": "We collect and release a localized hate speech detection dataset for several marginalized groups in countries where low-resource languages are predominantly used. We name this dataset REACT (REsponsive hate speech datasets Across ConTexts). To achieve this, we hire data collectors familiar with the nuances and contexts of hate speech in the target countries and are either native or highly proficient in the language for which they collect data. In total, REACT encompasses seven distinct target groups (black people, LGBTQ, Roma, Russians, Russophone Ukrainians, Ukrainians war victims, and women) and spans eight languages (Afrikaans, Indonesian, Korean, Oshiwambo, Russian, Slovak, Ukrainian, and Vietnamese).\nWe divide each dataset into six categories based on the polarity (positive, neutral, hateful) and the presence of profanity, which includes vulgar or obscene language, such as swear words. We collect data both with and without profanity within each polarity category to minimize the association of profanity with hateful content.\nFor each of the six categories, the data collectors receive a prompt formatted as follows:\nProvide [polarity] text about the [target group] [using/without using] profanity.\nData source. Data is collected predominantly from social media platforms like Facebook and X (formerly Twitter), as well as local online forums, news articles, and their comment sections. Additional sources include books and text corpora, such as Common Crawl. When collecting from online sources, data collectors are instructed to remove any personally identifiable information, including account names and hashtags. In certain cases, small modifications are made to enhance clarity and better describe the target group.\nIn addition, a large portion of the data is generated by the data collectors, either entirely from scratch or partially based on similar content from other sources. Some data is generated using AI tools, such as ChatGPT, and manually reviewed by the data collectors to ensure it is realistic and accurately reflects the category. Most of the AI-generated data falls within the positive category, where natural occurrences are considerably rarer than the other two categories."}, {"title": "4 Hate speech detection experiments", "content": "Preliminaries. To implement federated learning (FL) using our collected data, we use Flowers, a user-friendly federated learning framework, for its ease of deployment. Federated learning at scale typically requires a central server that connects to client nodes, which run on the devices of individual users. A key advantage of frameworks like Flower is their ability to run a simulated FL environment, allowing us to conduct training without relying on actual user devices. Instead, we can create simulated clients on a single machine.\nTarget Group selection To test out hate speech detection for low-resource languages, we focus on four selected target groups across two languages for which we have sufficient cross-annotated data. The four language-target group combinations are: Afrikaans, black people (afr-black), Afrikaans, LGBTQ (afr-lgbtq), Russian, LGBTQ (rus-lgbtq), and Russian, war victims (rus-war)."}, {"title": "4.1 Models", "content": "Federated learning is commonly constrained by the large communication overhead between clients and"}, {"title": "4.2 Federated learning", "content": "We create one server and four client instances, each representing one target group, using Flower's simulation. To assess the final performance of the clients, a test set is built for each target group from the data agreed upon by two data collectors with a native-speaker level in the language. As our datasets are highly target-specific and may exhibit similar patterns to a certain degree, we take steps to mitigate potential overlap between data splits. This is achieved by filtering out data already in one split with a Levenshtein ratio of greater than 0.5, except in cases where the resulting data split is not sufficiently large. In such cases, we raise the threshold slightly in a controlled manner. More details on the data selection process are described in \u00a7B.\nWe evaluate the clients' zero-shot performance and also fine-tune them with 3, 9, and 15 sentences per target group. presents the number of sentences in each split for the four target groups. We conduct five rounds of FL with one local epoch each, meaning each client is trained on its local data for one epoch during each round. At the end of the FL process, each client is then evaluated independently on its split of target-specific test data. We also evaluate the server performance using the joint test data from all target groups. For all evaluations, we use the macro-F\u2081 score and average results gathered using five different random seeds."}, {"title": "4.3 Client personalization", "content": "As a crucial part of our goal is to enable personal hate speech classification to serve the specific needs of the target groups, we would like each client to have some degree of customization. To this end, we implement two personalization methods during the FL process.\nFedPer. FedPer is introduced by and works by making the last layers of the client model private, sharing only updates to the non-private, or base, layers with the server. Kp and Kp are introduced to denote the number of base and personalized layers. Since personalization starts from the last layers, $K_p = 1$ would indicate that only the classifier head is personalized, and $K_p = n + 1$ indicates the classifier head plus the last n Transformer blocks are personalized.\nFollowing the authors, we test $K_p \\in \\{1, 2, 3, 4\\}$ on mBERT and Distil-mBERT. We exclude the server model from evaluation as part of the model parameters, most importantly those of the classifier head, is customized on the client side and not improved on the server, making an evaluation on the server model meaningless.\nAdapters. Personalization in hate speech detection is receiving widespread attention from researchers recently. A growing body of research incorporates annotators' demographics and preferences or even gaze features of the users into the annotated text to understand their subjective hate speech evaluations. Inspired by the work of, we add a small number of new parameters in the form of adapters between each pair of Transformer blocks, which serve as customizable parameters to the client. We perform both full-model fine-tuning, where all parameters are updated but only updates to non-adapter parameters are shared with the server during FL, and adapter-only fine-tuning, where non-adapter parameters are kept frozen. Note that in the latter option, no actual FL takes place, as non-personalized (non-adapter) parameters are not updated. As with FedPer, we exclude the server model from evaluation."}, {"title": "4.4 Baseline", "content": "To evaluate the effectiveness of FL across different target groups, we train each model individually on a single target group (standard few-shot fine-tuning), using the same data and parameters. For comparability, each model is trained for five epochs, in accordance with the five rounds of FL."}, {"title": "5 Results", "content": "Individual clients benefit consistently from FL. shows results from single-target training (top row) and federated learning (bottom row), both using mBERT and Distil-mBERT, along with their fine-tuned versions on English HateCheck data. In each plot, F\u2081 scores are shown across an increasing number of training samples, with each color indicating a different client or the server. Comparing single-target scenarios with the federated counterpart, we notice the performance of clients with lower F\u2081s using only single-target training is typically improved after federated learning, indicating that they benefit from the collective training data in the FL process. Furthermore, apart from the case of Distil-mBERT, the server performance almost always increases continuously with more training data during FL. This indicates that the server model is able to capture hate speech patterns for all four target groups.\nA less surprising finding on English fine-tuning indicates that a more positive contribution is seen with little training data for both models. FL results in the bottom row of Figure 1 show an improvement of 0.08 (resp. 0.07) per client on average for mBERT (resp. Distil-mBERT) with 0- and 3-shots. With increasing amounts of training data (9- and 15-shots), the FL performance is, contrary to expectation, hurt by English fine-tuning, with a -0.04 (resp. -0.09) drop in per client F\u2081 on average for mBERT (resp. Distil-mBERT).\nPersonalization works, but to a limited extent. The degree of personalization in FedPer is determined by the value of $K_p$, which is the number of personalized layers in each client model. We test $K_p \\in \\{1, 2, 3, 4\\}$ for mBERT and Distil-mBERT, and only show results with the best $K_ps$, indicated underneath the plots, for each model in Figure 2. We show the full results for all $K_ps$ in \u00a7D. For simplicity, we define the best value of $K_p$ as the one that yields the highest average F\u2081 gain per client across four training data sizes.\nWithout prior English fine-tuning, the results for both mBERT and Distil-mBERT are quite unstable. Performance improves for some clients but drops for others. Taking mBERT with 15 training samples, for example, afr-black suffers a sudden drop of 0.14 in F\u2081, whereas rus-lgbtq has a 0.06 increase. Similar inconsistencies can be seen on Distil-mBERT. Here, for example, the performance of all clients drop in 3-shot (up to -0.16). All clients, however, demonstrate improved performance in 9-shot (up to 0.18). Both models exhibit much weaker fluctuations in client performance when"}, {"title": "Incremental training", "content": "We explore whether it is more beneficial to use the same training data over five FL rounds, as is done by our currently adopted FL setup, or to update the training data in each round. To this end, we compare three settings: 1) using the same 3 sentences over five rounds of federated training; 2) using 15 sentences in a single round of federated training; and 3) using the same 15 sentences, divided into 3 sentences per round, over five rounds (incremental training).\nWe present findings in Figure 4, using the x-axis to denote the three settings. According to the comparison, using the same training set of 3 sentences over five FL rounds has superior results over using 15 sentences for only one FL round. Using 15 sentences incrementally, i.e., five FL rounds with 3 sentences each, though effectively utilizing the same 15 sentences as the second setting, yields worse results than using all 15 sentences in a single round."}, {"title": "6 Analysis", "content": "Effect of English fine-tuning. In Figure 1, it is noticeable that English fine-tuning may gradually hurt the performance of many individual clients when more training data is available, such as with 9 or 15 training samples per target group. This effect is more clear when using FL. One possible explanation for this negative impact is the target-specificity of our datasets, which depict a different group of targets than the HateCheck dataset used for fine-tuning. While fine-tuning on a generic hate speech detection dataset aids the client models' performance under conditions of data scarcity, its benefit diminishes as more target-specific data becomes available.\nImprovement from personalization As shown by Figures 2 and 3, both FedPer and adapters have variable effects on client models and are highly sensitive to the target group. To determine the extent of overall improvement of the personalization methods, we calculate the average increase on F\u2081 per client across all four training set sizes. Although improvements from FedPer can be observed on clients with lower F\u2081s in certain cases, such as rus-lgbtq in Distil-mBERT-HC, indicates that FedPer does not demonstrate a consistent performance advantage over non-personalized FL. Likewise, the overall improvement from adapters is not very positive, as shown by Table 4.\nOverall, the effects of personalization are complicated, making it difficult to entirely rule out its advantage. Personalizing client models, especially with an increasing Kp in the case of FedPer, results in fewer parameters being shared in FL. This may lead to a decrease in the overall performance of all participants in the FL process. Furthermore, the small number of target groups may have reduced the potential of personalization, which could be increasingly valuable with a broadening range of real-world target groups."}, {"title": "7 Conclusion", "content": "In this work, we make two key contributions. First, we release REACT, a collection of localized and context-specific hate speech detection datasets. REACT comprises data in eight low-resource languages, covering seven distinct target groups, created by data collectors who are both proficient in the target language and deeply familiar with the cultural nuances and contexts of hate speech in the respective countries. Second, using federated learning (FL), a privacy-preserving machine learning method that retains private data on each device, we evaluate two lightweight multilingual language models, capable of being deployed even on devices with limited resources, on few-shot hate speech classification. Our results show consistent, though modest, improvements on federated client models under zero- and few-shot conditions (Figure 1), highlighting the potential of federated learning as a promising approach for privacy-preserving few-shot learning, which could be extended to other tasks as well. Furthermore, our exploration of two personalization methods reveals that they are ineffective in customizing individual clients in low-resource settings, as demonstrated by a lack of"}, {"title": "Limitations", "content": "Despite the comprehensive experimentation and valuable insights on federated hate speech detection presented in this study, we acknowledge several limitations that we intend to address in future research. First, although we strive to create datasets for as many low-resource languages as possible, the number of languages included was restricted by the limited availability of native speakers and budgetary constraints. This, in turn, reduces the flexibility of testing a broader range of clients. Second, given the depth and complexity of our experimentation, we did not conduct an extensive hyper-parameter search, which may have impacted the optimization of certain models. Third, the selection of models was constrained by the availability of lightweight multilingual models that could be feasibly deployed on client devices with limited resources. Finally, this study was conducted entirely in a simulated federated learning environment, and we plan to implement and evaluate it in real-world settings in future work."}, {"title": "Ethics Statement", "content": "In this work, we develop and utilize several hate speech detection datasets, the nature of which necessitates careful measures to protect data collectors from potential harm. We ensure that data collectors are fully aware of the context of the target groups involved and obtain their consent for handling such data. To minimize exposure to potentially harmful content, we randomly sample a small portion of the collected data for cross-annotation. Additionally, data collectors are instructed to collect data exclusively from open domains to avoid copyright infringement and to remove any personally identifiable information, thereby maintaining the anonymity of the datasets.\nWhile federated learning presents a promising approach to preserving user data privacy, it does not guarantee complete anonymity in the face of adversarial threats. In certain circumstances, a malicious actor could potentially carry out attacks to infer personal information from data transmitted by individual clients, thus compromising the security of federated learning. Therefore, additional precautions are recommended when implementing FL for sensitive data, with potential solutions including the application of differential privacy and the personalization of client models."}]}