{"title": "The why, what, and how of AI-based coding in scientific research", "authors": ["Tonghe Zhuang", "Zhicheng Lin"], "abstract": "Computer programming (coding) is indispensable for researchers across disciplines, yet it remains challenging to learn and time-consuming to carry out. Generative AI, particularly large language models (LLMs), has the potential to transform coding into intuitive conversations, but best practices and effective workflows are only emerging. We dissect AI-based coding through three key lenses: the nature and role of LLMs in coding (\u201cwhy\u201d), six types of coding assistance they provide (\u201cwhat\u201d), and a five-step workflow in action with practical implementation strategies (\u201chow\u201d). Additionally, we address the limitations and future outlook of AI in coding. By offering actionable insights, this framework helps to guide researchers in effectively leveraging AI to enhance coding practices and education, accelerating scientific progress.", "sections": [{"title": "Nature and role of LLMs in coding", "content": "Programming languages were created to allow for precise, efficient, and sophisticated control over hardware. Over the years, they have gone through cycles of transformation, moving from low-level languages that require a deeper understanding of hardware (i.e., assembly languages), to more accessible, higher-level languages\u2014languages with simpler syntax and more extensive built-in functions (e.g., Python). By abstracting away the complexities of machine code to focus more on concepts, design, high-level architecture, and problem-solving, high-level languages have made coding more accessible, thereby enabling more people to exploit the unlocked opportunities.\nThe recent emergence of LLMs in coding marks a transformative moment in the evolution of programming languages for three reasons. First, coding is now morphing into a more intuitive dialogue between the user and the AI. By reducing the need for syntax mastery, it brings computer instructions closer to everyday communication, allowing users to expend less effort on the mechanical aspects of coding\u2014and more on strategic problem-solving and communicating and collaborating with AI. Second, the vast knowledge embedded in LLMs allows AI systems to offer much more than just coding assistance\u2014they help clarify concepts, brainstorm ideas, improve user understanding. Third, the combination of its intelligent, versatile, and collaborative/interactive nature allows LLMs to offer personalized, on-demand assistance\u2014for example, based on data from the user or tailored to the user's background, and with interactions that adapt to the user's skill levels and learning paces. AI tools thus fundamentally reshape our coding workflow, serving not just as tools but as collaborators\u2014empowering both novices and experienced programmers to tackle mundane tasks and new challenges with confidence and improved efficiency.\nBy making coding more accessible, efficient, and enjoyable, LLMs are democratizing coding skills on an unprecedented scale. The implications are likely profound for educational and research practices. This is because despite the increasing importance of coding, systematic training is often lacking in academic training. Researchers, particularly those early in their careers, juggle learning multiple skills, and coding is often something to be learned on the job-mostly by oneself, with occasional assistance from lab mates. The silent struggle can breed stress, anxiety, and self-doubt. LLM-based coding alleviates these challenges through personalized, on-demand assistance, opening new doors for learning and building confidence. Such tailored instruction is particularly effective for self-learning, offering a practical, scalable solution to Bloom's 2 sigma problem (i.e., one-to-one tutoring leads to performance that is two standard deviations better than regular classroom settings)."}, {"title": "Mapping types of AI assistance", "content": "To illustrate how LLM-based coding addresses common research needs, we showcase use cases by categorizing them into six types of AI assistance: code understanding, code generation, code debugging, code optimization, code translation, and code learning."}, {"title": "Code understanding", "content": "A common challenge in research is comprehending and building on unfamiliar code\u2014whether it is studying tutorials, examining code from published studies or while reviewing a submitted manuscript, collaborating with other researchers, joining a new project, adapting legacy code, or even understanding our own code from a past project. As intelligent interpreters, LLMs excel at deciphering and explaining code, tailoring explanations to the user's experience and background.\nFor example, when studying complex tutorials, LLMs can link code implementations to underlying concepts and break down concepts step-by-step. For interpreting legacy code and code from published papers, submitted manuscripts, or collaborators, LLMs can provide high-level overviews of code structure, highlight key algorithms, identify key modules and their relationship, summarize the functionality of different components, provide detailed annotations, and explain the logic behind specific implementations. This capability extends to revisiting one's own code from past projects, where LLMs can refresh memory and explain the rationale behind previous coding decisions."}, {"title": "Code generation", "content": "Writing code is bread and butter for many researchers\u2014from stimulus presentation to data preprocessing, analysis, and visualization. LLMs can enhance productivity and code quality, allowing researchers to focus more on the scientific aspects rather than getting bogged down in coding technicalities.\nConsider, for example, when starting a new project: LLMs can generate boilerplate code, suggest appropriate libraries/frameworks, or adapt preexisting code for the present project, thus helping overcome the \"blank page\" syndrome. They can also streamline time-consuming and repetitive tasks like data manipulation and preprocessing. For more complex tasks such as translating mathematical or conceptual algorithms into functional code, LLMs can assist by providing code snippets or full implementations of common algorithms\u2014while the code may not work exactly as requested, it offers a convenient start for iteration. Perhaps the most ubiquitous use case is its utility in adding in-line comments and organizing/documenting code-essential for promoting research reproducibility and facilitating collaboration but time-consuming and not incentivized, and thus often overlooked in research practices."}, {"title": "Code debugging", "content": "Debugging can be tedious and frustrating. Error messages are often cryptic to inexperienced eyes. Common issues include identifying and fixing syntax errors and troubleshooting incorrect results or unexpected behavior. LLMs help with debugging by providing clear explanations of error messages, suggesting potential fixes, and offering strategies to resolve more complex issues. By highlighting common pitfalls and best practices, LLMs also contribute to the overall improvement of coding skills and practices.\nFor example, when encountering a syntax error, LLMs can break down the error message, explain the underlying syntax rule, and suggest corrections. For more complex issues like incorrect results or unexpected behavior, LLMs can analyze the code, identify potential problem areas, and suggest debugging strategies or alternative approaches. Fixing statistical models or simulations that yield anomalous results is particularly challenging, and LLMs can suggest modifications in code or algorithm that align with the specific dataset and expected theoretical models."}, {"title": "Code optimization", "content": "Good code is not only functional but concise, readable, robust, and efficient. This is particularly vital when working with large datasets or complex algorithms\u2014and when preparing code for publication or sharing with collaborators. Optimizations include adhering to best coding practices, enhancing code readability with descriptive variable/function names, maintaining consistent formatting and naming conventions, refactoring redundant code into functions, and improving algorithmic efficiency. LLMs enable researchers to refine their codebase effectively.\nFor instance, when optimizing code to better handle complex computations, LLMs can assist by analyzing the code and suggesting context-aware refinements that can reduce execution time and resource consumption. This might involve explaining the efficiency of various algorithms, suggesting improvements, or highlighting potential bottlenecks in the current code (e.g., replacing iterative loops with vectorized operations or modularizing frequently used code into functions)."}, {"title": "Code translation", "content": "Translating code between programming languages is often needed to access specific libraries (e.g., a library available in R but not in Python), improve computational efficiency (e.g., from Python to C or Rust), minimize security vulnerabilities (e.g., from C to Rust), facilitate collaboration across teams that use different programming languages, or ease the transition to open-source alternatives (e.g., from MATLAB to Python/R). Code translation can be time-consuming and error-prone. LLMs excel at code translation, offering researchers a powerful tool to bridge these gaps.\nFor example, LLMs can assist by translating code from one language to another, suggesting equivalent libraries or functions, explaining necessary adaptations, and highlighting potential pitfalls or differences that may affect results. By more effectively tapping into our knowledge and experience, LLMs facilitate understanding and learning new programming languages."}, {"title": "Code learning", "content": "Learning and improving coding skills is an ongoing need as well as a challenge for many researchers. Common needs include understanding basic and new programming concepts, learning syntax and best practices, and applying coding skills to domain-specific problems. LLMs serve as a patient, always-available tutor that allows researchers to learn at their own pace, focus on concepts most relevant to their work, and gain hands-on experience with custom examples. Additionally, drawing from vast resources on the internet, LLMs can design customized courses for individual users. This targeted approach not only enhances learning efficiency but also boosts confidence and joy. LLMs thus allow users to conveniently access highly relevant knowledge, making learning more efficient and aligned with their goals.\nFor instance, to facilitate learning, LLMs can provide varied domain-specific worked examples. By using neural data for a neuroscientist and financial datasets for an economist, this context-aware approach helps bridge the gap between abstract coding concepts and practical, relevant applications. To reinforce learning and test understanding, LLMs also excel at generating tailored exercises based on the researcher's progress and areas of difficulty-for example, creating a series of increasingly complex exercises and providing immediate feedback for each attempt. Furthermore, as researchers work through coding challenges, LLMs can offer real-time problem-solving strategies: breaking down complex tasks into manageable steps, suggesting alternative approaches, and explaining the pros and cons of different solutions."}, {"title": "A five-step workflow for Al-assisted coding", "content": "To facilitate effective integration of LLMs into coding routines, below we outline a general five-step workflow that serves as a working framework. The workflow can be adapted to different needs-for example, a one-off, simple request would require minimal preparation, but the general procedure, guiding principles, and practical wisdom can be equally useful. A summary version is available in Box 1 for easy reference."}, {"title": "Step 1: Prepare your project", "content": "Effective AI-assisted coding begins with project preparation. To ensure that Al outputs align with your goals, start by describing your project requirements and purpose in a document.\nInclude the relevant context of your project, the problems you aim to solve, and your constraints or preferences (e.g., preferred language or libraries). This document serves as a roadmap for the project, helping you clarify your thoughts and keeping the AI focused on your needs. Save it for reference and for sharing with collaborators or the AI.\nTo enable more precise, targeted interactions with the AI, consider breaking down higher-level goals into smaller, well-defined steps. For example, instead of a broad directive like \u201canalyze my dataset\", break it down into steps: \u201cload data\u201d, \u201cclean missing values\u201d, \u201cperform descriptive statistics\", and \"visualize the results\".\nFor larger, more complex projects, tackle each component separately. For instance, to develop a comprehensive data analysis workflow, consider decomposing it into separate components: data acquisition, preprocessing, analysis, and visualization. Complete each component, and then start a new session to continue with the next. This approach helps maintain clarity and prevents overwhelming the AI with too much context."}, {"title": "Step 2: Initiate the AI session", "content": "The next step is to select an appropriate AI model. Major AI coding tools are organized into four categories in Table 1, including generalist LLMs, plugins, code editors, and notebook platforms. Which tools to use depends on personal preferences and needs. Consider using advanced models such as Claude 3.5. These models offer robust capabilities for coding in various programming languages."}, {"title": "Step 3: Generate and review the initial code", "content": "You are now ready to generate code. Be specific in your prompt, referencing the project document from Step 1 when appropriate (e.g., \u201cCreate R code to perform descriptive statistics on the dataset as detailed in my project outline\u201d). For strategies for prompting and using LLMs for coding, see Box 2. Once you have the initial code, you can ask the AI to review it and suggest improvements\u2014LLMs do not read or reflect on their outputs unless directly instructed to do so.\nFor a more robust review process, consider using multiple AI models. For instance, you could use Claude 3.5 Sonnet to detect errors or suggest optimizations, and then have ChatGPT-4 assess these suggestions. Return to Claude 3.5 Sonnet with the reviewed feedback for further assessment. This multi-model approach mimics the diversity of human feedback on platforms like Stack Overflow, which is instrumental for more comprehensive, reliable code improvements.\nWith the completed code, consider asking the AI for a step-by-step explanation of how the code accomplishes your stated goals (e.g., \u201cWalk me through how this code performs the descriptive statistics analysis\"). To further verify its alignment with your request, start a new Al session, paste the generated code (without any context), and ask for an explanation of what the code does. If the explanation accurately recreates your original intent, it is a good indication of code accuracy and alignment.\nWhether you have the AI review the code or not, in general you should aim to understand the code by examining it line by line. A good grasp of coding is beneficial, but AI can also help this process by providing comments and explanations for each line of code. For larger, more complex code, divide it into logical segments, and try to understand each segment before moving on to the next. For one-opt tasks where the goal is simply to get something done quickly (e.g., an intermediate step of data format transformation) and the code is self-contained (that is, not part of a codebase), code quality is not critical and a line-to-line level of understanding not practical."}, {"title": "Step 4: Test and debug the code", "content": "The next step is to ensure that the code functions as intended. Run the code. If the code fails to run, share the results and error messages with the AI for debugging suggestions. If the code runs but produces unexpected outcomes, provide the AI with a detailed description of the expected versus actual results, and check the assumptions made in the code or the algorithms used.\nCritically evaluate the suggestions from the AI using your domain knowledge and your understanding of the project requirements. Trust your intuition. To maximize efficiency, focus on getting the core features of your code working first\u2014leave minor bug fixes and additional features for later.\nDebugging is often an iterative process. If the initial fixes suggested by the AI could not resolve the issue, paste the outcomes to the AI again and request alternative solutions. If stuck, consider adjusting the AI settings (such as the temperature parameter in GPT-4) or starting a fresh conversation. By removing the context of the original conversation, a new conversation promotes a different perspective. Consider also alternating between different AI models, such as ChatGPT-4 and Claude 3.5 Sonnet, as each model shares different strengths and limitations\u2014leveraging multiple models can help identify and resolve a wider range of issues. Take breaks between debugging sessions\u2014the performance of LLMs can vary even with the same prompt.\nFor code that involves data analysis, it also helps to validate the code with known data and results. A small, representative dataset suffices. By comparing the output from the code with the known correct output, we can verify the code's accuracy with more confidence.\nHowever, it is important to set realistic expectations: LLMs will not solve all coding bugs. Understanding their limitations\u2014in debugging and beyond\u2014is part of learning to use LLMs effectively. If several rounds of back-and-forth fail to resolve a bug, it may be time to try other approaches. With experience, you will develop an intuition about when to consult LLMs and when to Google instead."}, {"title": "Step 5: Refine and document the code", "content": "With working code in hand, now is the time to refine and document it\u2014making it efficient, maintainable, and understandable to others (including your future self!).\nBegin by optimizing for performance and efficiency (\u201cReview the code and suggest ways to improve its performance, particularly focusing on [specific areas of concern]\u201d). Ask the AI to refactor the code to enhance its structure and readability (\u201cRefactor the code to improve modularity and simplify its design without altering its functionality\u201d). Implement the suggestions incrementally and test the code (following Step 4) to ensure each modification maintains functionality while improving performance/readability.\nNext, ask the AI to add in-line comments explaining how each section of the code works (\u201cAdd detailed in-line comments to the code, explaining the purpose and functionality of each section\"). Additionally, ask the AI to generate overall documentation for your project (\u201cCreate a project overview document that includes a description of the project's purpose, how to run the code, and any dependencies or setup requirements\")."}, {"title": "AI-based coding in action", "content": "To showcase AI-based coding in action, we describe how author Z.L. used LLMs to generate the code for Figure 1, following the five-step framework, with reflections on key design choices. Due to the personal nature of this process, the pronoun \u201cI\u201d will be used."}, {"title": "Step 1: Prepare your project", "content": "As part of the preparations, I had the manuscript ready, with a clear goal: to draw a flowchart of the five-step workflow. I also sketched a simple flowchart design on a tablet (Kindle Scribe), using five rectangles to represent the steps, arranged horizontally and connected by arrows.\nWhile I knew how to use the DiagrammeR library in R to draw flowcharts, it was quite a while ago. I decided to use LLMs to jumpstart the project. In the end, drawing the figure took about three hours, and I estimated that without AI assistance, it would probably take double that time. This is an important point to emphasize: AI-based coding is not just for beginners, but can also save time for more experienced coders."}, {"title": "Step 2: Initiate the AI session", "content": "I decided to use Claude 3.5 Sonnet first because it was one of the best LLMs for coding. Additionally, it had the Artifact feature, which displayed the results of code execution instantly. This immediate visualization is helpful as it facilitates quick prototyping."}, {"title": "Step 3: Generate and review the initial code", "content": "As shown in Figure 2, I started by uploading the manuscript and entering the following prompt: \u201cShow me a flowchart for the AI human coding workflow from the manuscript, using Python\u201d. Claude responded by explaining the tool used (\u201cMermaid\u201d) and detailing the content of the flowchart (\"five-step workflow outlined in Box 1\"), along with providing the code and a preview of the flowchart.\nThe flowchart required modification. To steer it in the right direction, my first idea was to include the substeps within the box of the main step. So, I followed up with the following prompt:\n\u201cOK. Looks promising, but needs some work. Put the components within each step rather than outside of the component. For example. 1. Prepare Project, you can put the three boxes of Write..., Break..., and Tackle... into that.\u201d\nHowever, the revised flowchart was not quite what I looked for (Figure 3).\nSo I decided to edit the prompt to be more specific:\n\u201cOK. Looks promising, but needs some work.\n1: Remove the \"Start\"\n2: Arrange the five components horizontally, from left to right, 1 to 5\n3: For each component, arrange the substeps under the component and vertically (up to down). For example. 1. Prepare Project, you can put the three boxes of Write..., Break..., and Tackle... below it, and arrange them vertically.\u201d\nBut the spatial arrangement of the flowchart was still off (Figure 4).\nI decided to edit the attempt further, by using spatial coordinates:\n\u201cOK. Looks promising, but needs some work.\n1: Remove the \"Start\"\n2: Arrange the five components horizontally, from left to right, 1 to 5. Remember, they need to have the same y value, meaning they are arranged within a straight horizontal line\n3: For each component, arrange the substeps under the component and vertically (up to down), with the same x value. For example. 1. Prepare Project, you can put the three boxes of Write..., Break..., and Tackle... below it, and arrange them vertically.\u201d\nWhile the updated flowchart looked somewhat better, the spatial arrangements were still not right (Figure 5). So, I decided to stop here, and iterate from the final Python code provided by Claude."}, {"title": "Step 4: Test and debug the code", "content": "I was not familiar with Python, so I decided to translate it into R code first using ChatGPT 40, and iterate from there. The entire chat history is available here:\nTwo points about the different strengths of LLMs:\nUnlike ChatGPT, Claude did not support chat history sharing at the time of writing;\nUnlike Claude 3.5 Sonnet, ChatGPT 4o did not natively support code execution (available only through a special GPT called Data Analyst). But this was fine by me as I had RStudio installed, ready for testing and debugging.\nSince the entire chat history is available, I will not detail my interactions but will provide a general summary. I asked ChatGPT to:\nTranslate the Python code into R code (\u201cConvert the code to R\u201d). It managed this, but the exact item arrangements were a bit off (Figure 6).\nMake spatial rearrangements, based on the previous prompt for Claude (see Figure 5), which yielded much better results than Claude (Figure 7)."}, {"title": "Step 5: Refine and document the code", "content": "I was able to adjust the spatial arrangements and visual features to my liking using ChatGPT-4o (Figure 1). I just needed to export the figure to a PDF file, so I borrowed code from my previous projects. The final code produced the figure I wanted, and the code looked well-documented and clear"}, {"title": "Reporting the usage of AI tools in coding", "content": "To maintain transparency, reproducibility, and compliance with institutional policies, we need to document Al assistance in coding. However, this is more challenging than it sounds: journal policies are often unclear or inconsistent; full documentation is cumbersome if not impractical; and complete reproducibility at the code/text generation level is not feasible. The bottom line is to follow relevant policies\u2014journal policies for publications; university policies for teaching and research. But this needs to be a forethought rather than an afterthought: while most policies generally do not forbid the use of generative AI, variations do exist across policies and contexts.\nBest practices are still emerging. To balance transparency and practicality, a starting point is to prioritize the documentation of AI tools (including name, version, and time), their roles in coding projects, and specific prompts and interactions. Most journal policies require AI usage to be disclosed in the methods or acknowledgment sections. Yet these policies mostly address text writing rather than coding. Thus, for code documentation, we additionally suggest including a section in the readme file or at the beginning of the code file to document the usage of AI tools. And if needed, use in-line code comments to highlight sections of code that were generated or altered by AI tools."}, {"title": "Limitations", "content": "Despite the transformative potential of LLMs for coding in research, there are important limitations. As with other applications, LLMs can hallucinate, such as suggesting non-existing functions or libraries. In coding contexts, this issue is mitigated to some extent because the output can be verified by running the code. Another limitation is that, due to uneven training data, LLMs perform better in some languages (e.g., Python) and tasks (e.g., modular, classic/common ones) than others (e.g., NetLogo). Expanding training datasets will be crucial to address these limitations. More generally, while LLMs excel at language and pattern recognition, they currently fall short of human qualities like intuition, common sense, and reasoning. This limitation means that, for example, if LLMs cannot fix a bug after two to three tries, it is unlikely that they will succeed with additional attempts. Ethical issues also arise with AI-assisted coding, including equitable access to AI coding tools and transparency about their use in publications.\nWhile LLMs lower the barrier to entry for coding and improve the efficiency of learning and coding, they do not eliminate the need for foundational knowledge and skills. In a 1995 interview, Steve Jobs commented that \u201ceverybody should learn to program a computer, because it teaches you how to think\u201d. A basic understanding of programming concepts, familiarity with relevant packages and libraries, domain-specific knowledge, critical thinking\u2014all of these help researchers ask better questions, select appropriate tools, and evaluate AI-generated code, particularly in distinguishing between correct versus plausible-looking but incorrect outputs."}, {"title": "Outlook and concluding remarks", "content": "Indeed, one danger of relying on AI-generated code is that insecure or incorrect code becomes integrated into the codebase simply because it produces outputs that appear correct. Without understanding the code, this codebase may become too unwieldy to debug or modify. Additionally, over-reliance on LLMs may lead to a decline in certain coding skills (e.g., writing from a blank page) for lack of practice. And when we come to rely on AI to correct or optimize our code, it may also lead to decreased attentiveness to code quality. Be vigilant, then, of AI mistakes and biases as well as the danger of over-relying on AI.\nComputer languages represent a most fertile ground for automation: they are generally more logical than natural languages; there is a large corpus of high-quality code available for training; and there is a strong demand, both commercial and otherwise. AI agents are being developed to automate aspects of coding tasks\u2014a development likely to further transform the landscape of software development and scientific research. The future of abundant, low-cost, high-quality software is thus poised to accelerate a wide range of creative endeavors\u2014as well as raise new ethical and philosophical questions (for example, if our productivity greatly depends on AI, are we still coders, or are we more like proficient tool users, and does this matter to our identity?).\nAs illustrated in this article, LLMs and various AI tools right now are already very useful for coding. By bridging natural language and computer instructions, LLMs are reshaping how researchers approach computational tasks and redefining the role of coders and our workflow. In some sense, proficient AI-based coding requires the mindset of a good designer: focusing not just on the appearance of the code but on its functionality-how it operates and solves problems. By democratizing coding skills and offering personalized, on-demand assistance, LLMs have the potential to fundamentally transform scientific coding practices\u2014serving as both tools and collaborators. Yet, building a symbiotic relationship with AI requires that we develop a new set of AI skills to become better designers: the ability to see the big picture and communicate with AI, evaluate its outputs, and integrate AI into our workflows. It also requires curricula to adapt and prepare students for this AI-infused future (see Box 3 for an outline to integrate AI-based coding into the classroom). The benefits are not just cognitive, enhancing efficiency and productivity; they are also emotional, reducing distractions and bringing confidence and joy."}]}