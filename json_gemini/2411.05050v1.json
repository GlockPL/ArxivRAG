{"title": "Selecting Between BERT and GPT for Text Classification in Political Science Research", "authors": ["Yu Wang", "Wen Qu", "Xin Ye"], "abstract": "Political scientists often grapple with data scarcity in text classification. Recently, fine-tuned BERT models and their variants have gained traction as effective solutions to address this issue. In this study, we investigate the potential of GPT-based models combined with prompt engineering as a viable alternative. We conduct a series of experiments across various classification tasks, differing in the number of classes and complexity, to evaluate the effectiveness of BERT-based versus GPT-based models in low-data scenarios. Our findings indicate that while zero-shot and few-shot learning with GPT models provide reasonable performance and are well-suited for early-stage research exploration, they generally fall short or, at best, match - the performance of BERT fine-tuning, particularly as the training set reaches a substantial size (e.g., 1,000 samples). We conclude by comparing these approaches in terms of performance, ease of use, and cost, providing practical guidance for researchers facing data limitations. Our results are particularly relevant for those engaged in quantitative text analysis in low-resource settings or with limited labeled data.", "sections": [{"title": "1 Introduction", "content": "Text classification is one of the most common tasks in quantitative text analysis. Researchers often need to classify different texts into topics. Such texts encompass news articles (Laurer et al., 2024; H\u00e4ffner et al., 2023; Barber\u00e1 et al., 2021; Y. Wang et al., 2017, 2015), tweets (Widmann & Wich, 2023; Kim, 2022), public speeches (Widmann & Wich, 2023; Y. Wang, 2023b), video descriptions (Lai et al., 2024), names (Kaufman & Klevs, 2022; Chaturvedi & Chaturvedi, 2023), among others. Regardless of the specific\nBesides prompting, another alternative is to finetune these GPT models (https://platform.openai.com/docs/guides/fine-tuning/). We do not explore this approach here because our early explorations in this direction did not yield promising results.\ntext form, one of the key bottlenecks in performing text classification is data scarcity: procuring labeled data is a slow and labor-intensive process and as a result the labeled set is oftentimes fairly small.\nTo resolve the data scarcity issue, researchers have explored various approaches. For example, some researchers have trained models using labeled cross-domain data, which is abundant, and then applied the trained model to in-domain classification (Osnabr\u00fcgge et al., 2021). Others have studied the plausibility of using ChatGPT as an automatic annotator to replace human annotation and speed up the labeling process (Gilardi et al., 2023). Still others have considered instead of random sampling how to select more informative samples for labeling so as to reduce the number of labeled samples (Kaufman, 2024). Thus far, however, the most effective approach has been finetuning BERT models (Devlin et al., 2019). By coupling general knowledge in the pretrained language models and a few hundred task-specific samples, finetuned BERT models have proven to offer superior performance as compared with classical models such as logistic regression (Laurer et al., 2024; Y. Wang, 2023a). Over the past few years, this pretrain-finetune paradigm (Y. Wang &\nQu, 2024) has quickly established itself as the go-to method for text classification (Laurer et al., 2024; Lai et al., 2024).\nIn this article, we study zero-shot and few-shot prompting with GPT models as a potential alternative solution to the data scarcity problem. Specifically, we analyze in situations with 1,000 or fewer samples how prompting with GPT models compares with finetuned BERT models in binary and multi-class classification. Through extensive experiments, we demonstrate that zero-shot and few-shot learning with GPT-based large language models can serve as an effective alternative to fine-tuning BERT models. The advantages of GPT models for classification are particularly prominent when the number of classes is small, e.g., 2, and when the task is easier."}, {"title": "2 Text Classification in Political Science", "content": "Quantitative text analysis has gone through quite a few distinctive methodological stages throughout its evolution: feature-based classical models, word embeddings, BERT models, and more recently generative models. As in other social science disciplines (Nielbo et al.,\n2024; O. N. Kjell et al., 2024; Y. Wang et al., 2022), text analysis in political science research has followed a similar trajectory. Initially, researchers converted texts into counts and trained classical models from scratch. Subsequently, word counts were replaced with word embeddings, and recurrent neural networks were employed for classification. More recently, there has been a growing body of literature focused on fine-tuning BERT models."}, {"title": "2.1 Classical Models", "content": "Classical models refer mostly to the simpler and smaller models that take word counts as input. Naive Bayes, support vector machine and logistic regression models generally fall into this category (Hastie et al., 2009; Y. Wang et al., 2022; Bestvater & Monroe, 2023). They are simpler in model architecture, smaller in model size, require training from scratch, and take word frequencies as input. Given that the order of words is not utilized, these models are considered as a \"bag of words\" approach. Other hallmarks of classical models include preprocessing and feature engineering. Because of the significance of word frequencies, careful preprocessing steps are usually required (Rodriguez & Spirling, 2022). Given the number of words (i.e., features) can be enormous, researchers need to manually decide what features to include and what to exclude (Y. Wang et al., 2022).\nProminent examples that utilize classical models for text classification include D'Orazio et al. (2014), which uses support vector machine to classify documents on the Militarized\nInterstate Dispute 4 (MID4) data collection project, and Diermeier et al. (2011), which uses support vector machine to classify U.S. senators into \u2018(extreme) conservative' and\nSome researchers have grouped the stages of word embeddings and BERT models into a unified repre-\nsentation learning stage (Nielbo et al., 2024).\nIn addition to their use as natural language processing tools, these models are also often trained on\ntabular data. See for (Muchlinski et al., 2016) and (Y. Wang, 2019a) for recent examples."}, {"title": "2.2 Word Embeddings", "content": "Word embeddings are vector representations of words (Mikolov et al., 2013; Pennington et al., 2014; Y. Wang, 2019b; Rodriguez & Spirling, 2022). By projecting words into a vector space based on their co-occurrence patterns, word embeddings possess semantic meaning, with semantically similar words located close to each other in the vector space. Researchers have utilized word embeddings to study various topics, such as ideological placement in parliamentary corpora (Rheault & Cochrane, 2019) and the evolving meaning of political concepts (Rodman, 2019). Beyond serving as standalone entities, these embeddings can also function as input to recurrent neural networks for text classification (Chang & Masterson, 2020; Y. Wang et al., 2017). For notable applications of embeddings in other social studies, readers can refer to Simchon et al. (2023), Peng et al. (2024), Rai et al. (2024)\nand Y. Wang (2024)."}, {"title": "2.3 BERT Models", "content": "BERT models, which are encoder-based language models, have emerged as one of the most effective tools for text classification (Y. Wang & Qu, 2024). They are rooted in the trans- former architecture first introduced by Vaswani et al. (2017). Since their introduction in 2018 (Devlin et al., 2019), BERT models have consistently achieved state-of-the-art performance across various natural language processing tasks (Devlin et al., 2019). Building on their initial success, numerous variations of BERT models have been developed, incorporating more extensive training data (Y. Liu et al., 2019), specialized data domains (Hu et al., 2022; Lee et al., 2019), and novel pretraining tasks (Lan et al., 2020). In the last couple of years, they have started to gain traction in political science research. Whether it is classifying news articles into different economic sentiments (Laurer et al., 2024), parliamentary speeches into different topics (Y. Wang, 2023b), tweets for depression detection (Zhang et al., 2021) or video descriptions into ideology categories (Lai et al., 2024). In addition to\ntheir effectiveness as classifiers, BERT models have also been utilized by researchers for embedding tasks (Peng et al., 2024; Rai et al., 2024; Kaufman, 2024; O. Kjell et al., 2023). Researchers first transform texts into embeddings using BERT models and then apply these embeddings in classical models such as logistic regression (Rodriguez & Spirling,\n2022)."}, {"title": "2.4 GPT Models", "content": "GPT models are decoder-based language models and represent another highly effective tool for text analysis. Like BERT models, they also trace their roots back to the transformers introduced in Vaswani et al. (2017). Unlike BERT models, GPT models are primarily designed for text generation. They excel in tasks such as essay writing, text summariza- tion, translation, question answering, idea generation, and medical report transformation, among others (Radford et al., 2019; Korinek, 2023; Adams et al., 2023). Researchers in various fields, such as economics (Mei et al., 2024) and psychology (Strachan et al., 2024), have sought to leverage the generative capabilities of these models, exploring whether they behave similarly to humans in classical games like the ultimatum bargaining game and the prisoner's dilemma game. Similarly, political scientists have explored using GPT models to simulate human samples (Argyle, Busby, et al., 2023; Bisbee et al., 2024), investigat- ing whether \"silicon samples\" respond to surveys in a manner akin to humans after the models have been conditioned on thousands of socio-demographic backstories from real participants. Others have explored leveraging these models' generative capabilities for chat interventions to improve online political conversations (Argyle, Bail, et al., 2023).\nIn addition to the original text generation capabilities, as GPT models grow in size, they have started to demonstrate emergent abilities (Wei, Tay, et al., 2022) unseen in smaller model versions. Among these emergent abilities are zero-shot prompting (Radford\net al., 2019) and few-shot prompting (Brown et al., 2020). For example, researchers have explored the plausibility of using ChatGPT as an automatic annotator to replace human\nPer definition, \"an ability is emergent if it is not present in smaller models but is present in larger models.\" (Wei, Tay, et al., 2022)"}, {"title": "3 Empirical Analyses", "content": "We primarily focus on five sets of experiments: (1) binary classification of news articles based on economic sentiments, (2) 8-class classification of party manifestos, (3) 8-class classification of New Zealand Parliamentary speeches, (4) 20-class classification of COVID- 19 policy measures, and (5) 22-class classification of the US State of the Union speeches. For each experiment, we evaluate the performance of fine-tuning BERT models using 200, 500, and 1,000 samples. The particular BERT version that we use is RoBERTa-large with 340 million parameters (Y. Liu et al., 2019). It is arguably the most performant model in the BERT family (Ziems et al., 2024). In terms of hyperparameter tuning (Arnold et al., 2024; Y. Wang & Qu, 2024; Goodfellow et al., 2016), we optimize the learning rate (3e-5, 2e-5, 1e-5) using the validation set. Each experiment setting is run three times with three different seeds and the mean, min, max are reported.\nWe further calculate the performance of GPT models with zero sample, 1 sample per class and 2 samples per class, respectively. The particular GPT version we use is\nGPT-40. In terms of hyperparameter tuning (Gilardi et al., 2023), we use two different\nData is collected from mostly democracies in OECD, Central and Eastern European countries and South American countries.\nPlease note that all our experiments utilize RoBERTa-large. For simplicity, we will use the terms\nBERT and RoBERTa interchangeably in the following sections.\nOther GPT models include Gemini by Google, Claude by Anthropic, Llama by Meta, Mistral by"}, {"title": "3.1 Economic Sentiment Classification (2-Class)", "content": "Sentiment analysis is one of the most common tasks that political scientists have to deal with. Given a particular text snippet, our goal is classify it into either positive or negative. It is often considered an easy task in that it has only two classes. In this experiment, we use the Sentiment Economy News dataset by Barber\u00e1 et al. (2021) and Laurer et al. (2024). The goal is to differentiate whether the economy is performing well or poorly according to a given news headline and the corresponding first paragraph (Laurer et al., 2024). In Table 1, we report the distribution of the two labels among the train, dev, and test sets. It can be observed that approximately two-thirds of the samples are negative, a pattern consistent across all three datasets. For finetuning the BERT model, we randomly sample\n200, 500, and 1,000 samples from the training set. For procuring samples used in few-shot prompting, we randomly select them from the training set as well.\nMistral AI. The latter two, in particular, offer open-source models. We opt for GPT-40, which is closed-\nsource, because it arguably provides the best performance. Researchers interested in privacy or latency\ncould consider those smaller open-source alternatives."}, {"title": "3.2 Manifesto Classification (8-Class)", "content": "Topic classification is another common task in political science research (Osnabr\u00fcgge et al., 2021; Y. Wang, 2023b). In terms of the modeling process, it is essentially the same as sentiment analysis, except that it often has more than two classes. In this subsection, we compare the performance of finetuning BERT models with that of prompting GPT models in an 8-class topic classification. The dataset comes from Laurer et al. (2024) and is originally published by WZB Berlin Social Science Center. In this subsection, we further study the problem of 8-class classification of party manifestos. In Table 2, we report the data distribution. The 8 classes are Economy, External Relations, Fabric of Society, Freedom and Democracy, No Other Category Applies, Political System, Social Groups, and Welfare and Quality of Life. Economy and Welfare and Quality of Life are the two largest classes, each accounting for between 27% and 30%. Other classes are more or less evenly distributed, each accounting for about 9 percent. No Other Category Applies is an exception in that it accounts for 0.65% of the training samples, 1.67% of the dev samples and 0% of the test samples. Given how rare this class it, this task effectively boils\ndown to a 7-class classification problem.\nNote that data is from the Manifesto Project Dataset and is collected from mostly democracies in OECD, Central and Eastern European countries and South American countries by the WZB Berlin Social\nScience Center."}, {"title": "3.3 New Zealand Parliamentary Speech Classification (8-Class)", "content": "In this subsection, we study another example of 8-class classification. Specifically, we clas- sify the speech transcripts from the New Zealand Parliament for the period from 1987 to\n2002. The dataset originally comes from Osnabr\u00fcgge et al. (2021) and has 4,165 hand-"}, {"title": "3.4 COVID-19 Policy Measure Classification (20-Class)", "content": "In this subsection, we evaluate the models' performance on a 20-class classification task. The dataset is in the domain of policy measures against COVID-19. It comes from Laurer"}, {"title": "3.5 Speech Classification (22-Class)", "content": "Following the 20-class classification task on COVID-19 policy measures, this subsection compares the performance of fine-tuning BERT models with prompting GPT models in a 22-class classification task focused on State of the Union speeches. This task could pose a greater challenge for both approaches, particularly for fine-tuning, for two key reasons. First, with a fixed number of training samples, a larger number of classes means that each"}, {"title": "4 Discussions and Future Research", "content": "The empirical results consistently demonstrate that fine-tuning BERT models is the pre- ferred approach for maximizing model accuracy when researchers have access to around 1,000 data points. However, while prompting may not achieve the same level of perfor- mance, it offers competitive results, particularly when the training set includes only a few hundred samples. In this section, we delve deeper into these findings, discussing them in terms of performance, ease of use, cost considerations, and potential future directions."}, {"title": "4.1 Performance", "content": "After comparing the performance of fine-tuning BERT models and prompting GPT models across binary, 8-class, and 20+ class classifications, several key observations immediately stand out. First and foremost, in general both finetuning and prompting represent viable solutions to the data scarcity issue and both offer strong performance with limited labeled"}, {"title": "4.2 Ease of Use", "content": "In terms of ease to use, finetuning BERT models is more complicated than zero-shot or few-shot prompting GPT models. In terms of data preparation, both approaches represent an advancement over classical methods, since there is no more need for data preprocessing, such as stop word removal and stemming (Y. Wang et al., 2022). For finetuning BERT models, we need to split the dataset into train, dev, and test. For prompting, we only need the test set (and an optional dev set). When it comes to training, fine-tuning has been greatly simplified by frameworks like Huggingface (Laurer et al., 2024). However, researchers still need to write some boilerplate code. Additionally, there is the need to adjust quite a few parameters, with the learning rate being particularly important (Arnold et al., 2024; Goodfellow et al., 2016). By contrast, prompting with GPT models is done"}, {"title": "4.3 Cost", "content": "In addition to performance and ease of use, a third dimension to consider is the financial cost of using these models. The cost of fine-tuning BERT models primarily lies in GPU time: the time spent using GPUs for fine-tuning and evaluation/inference. As we increase the number of training samples from 200 to 500 and then to 1,000, we will linearly increase the training time and thus the cost. As an example, in the 20-class classification of COVID- 19 policy measures, training a BERT model with 200 samples takes three and a half minute. For 500 samples, it takes five and a half minute. For 1000 samples, it takes eight and a half minute. During evaluation (inference), each sample takes about 10 milliseconds. As we increase the number of test samples, we linearly increase the cost. Since fine-tuning is performed only once, the associated costs can be considered sunk. In contrast, prompting does not involve fine-tuning, so there are no sunk costs. However, each individual API call for prompting is typically more expensive than the cost of BERT inferencing, at least for now. Additionally, the cost of prompting GPT models is tied to the number of tokens in the prompt the more tokens per request, the higher the cost.\nResearchers may also need to consider the cost of annotation, which is often not trivial (Gilardi et al.,\n2023)."}, {"title": "4.4 Future Directions", "content": "Natural language processing (NLP) and large language models are advancing rapidly. In this section, we outline several emerging directions in NLP that hold significant promise for enhancing political science research. Of the two approaches, fine-tuning BERT models is more mature, while prompting is still relatively new. For fine-tuning BERT models, researchers could further explore mixed precision training to reduce the sunk cost, par- ticularly when working with large datasets. To enhance the performance of GPT models, researchers might investigate newer foundation models, advanced prompting techniques such as chain-of-thought and self-consistency (Wei, Wang, et al., 2022; X. Wang et al., 2023), and more effective sample selection methods based on criteria like semantic simi- larity (J. Liu et al., 2022; An et al., 2023). These strategies could not only improve the effectiveness of prompting GPT models but also make them more economically compelling."}, {"title": "5 Conclusion", "content": "Quantitative text analysis plays a prominent role in political science research. Recent advancements, particularly in large language models, have provided researchers with pow- erful new tools to address both existing and emerging challenges. In this article, we have explored the potential of using GPT-based models as an alternative solution to the data scarcity issue, comparing their performance to that of fine-tuning BERT models, which remains the state of the art. Through extensive experiments, we have demonstrated that zero-shot and few-shot learning with GPT-based models can sometimes serve as an ef- fective alternative to fine-tuning BERT models, especially when the number of classes is small, but fine-tuning BERT models remains the overall go-to method for classification. In addition to performance, we have also compared these approaches in terms of ease of use and cost. While prompting GPT models is significantly easier to use than fine-tuning BERT models, it also proves to be more expensive. We believe our findings will be valuable to researchers involved in quantitative text analysis."}]}