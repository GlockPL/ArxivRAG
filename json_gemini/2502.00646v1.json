{"title": "TrojanTime: Backdoor Attacks on Time Series Classification", "authors": ["Chang Dong", "Zechao Sun", "Guangdong Bai", "Shuying Piao", "Weitong Chen", "Wei Emma Zhang"], "abstract": "Time Series Classification (TSC) is highly vulnerable to backdoor attacks, posing significant security threats. Existing methods primarily focus on data poisoning during the training phase, designing sophisticated triggers to improve stealthiness and attack success rate (ASR). However, in practical scenarios, attackers often face restrictions in accessing training data. Moreover, it is a challenge for the model to maintain generalization ability on clean test data while remaining vulnerable to poisoned inputs when data is inaccessible. To address these challenges, we propose TrojanTime, a novel two-step training algorithm. In the first stage, we generate a pseudo-dataset using an external arbitrary dataset through target adversarial attacks. The clean model is then continually trained on this pseudo-dataset and its poisoned version. To ensure generalization ability, the second stage employs a carefully designed training strategy, combining logits alignment and batch norm freezing. We evaluate TrojanTime using five types of triggers across four TSC architectures in UCR benchmark datasets from diverse domains. The results demonstrate the effectiveness of TrojanTime in executing backdoor attacks while maintaining clean accuracy. Finally, to mitigate this threat, we propose a defensive unlearning strategy that effectively reduces the ASR while preserving clean accuracy.", "sections": [{"title": "1 Introduction", "content": "Time series data mining has become a crucial area in modern data mining. With the advancement of deep neural networks (DNNs), an increasing number of time series classification (TSC) tasks leverage deep learning methods, especially in healthcare [25], finance [18], and remote sensing [23], etc. However, current DNNs face threats from malicious attacks [11,10,9], one of the most concerning being backdoor attacks [12,21]. In such attacks, the Trojan model produces correct outputs for clean inputs but exhibits abnormal predictions when the inputs are corrupted by specific triggers. The attacker can poison the training data by introducing a carefully designed trigger. By establishing a connection between the specific trigger and certain class during training, the model can be manipulated to misclassify inputs when the trigger is applied. This misclassification can be attributed to some specific neurons' abnormal activation in response to the trigger pattern [19,27].\nDespite the success of backdoor learning in computer vision (CV) [12,21,6,22] and natural language process (NLP) [29,5], the study in time series domain is underexplored with a few preliminary works have focused on this area. Existing methods in time series classification primarily target the training phase by employing data poisoning techniques, where sophisticated triggers are designed to enhance both stealthiness and attack success rate (ASR) [8,15]. However, these methods often involve complex computations during both poisoned training and inference stages, making them impractical. To account for real-world scenarios, attackers typically cannot interfere with the model's training process or introduce additional computations during inference, as these actions would significantly increase the risk of exposing the attack. Instead, attackers can only tamper with a few parameters in the pre-trained model to create a Trojan model. Meanwhile, they must ensure the Trojan model maintains accuracy on clean samples while achieving a notable ASR on backdoor samples, making this task extremely challenging.\nTo address this issue, we propose TrojanTime, as shown in Figure 1, a novel backdoor attack framework for TSC. It involves a two-step training process: 1) synthesizing training data by generating pseudo-datasets from target adversarial attack in an arbitrary time series dataset D'; 2) using the synthesized data to Trojan the benign model, embedding the backdoor threats while maintaining its performance on clean samples. By performing adversarial attacks on D' across all possible classes according to the classification head of the model, we generate Dadv, an adversarial dataset designed to align with the training data distribution while increasing data diversity which benefits from the inclusion of multiple classes during adversarial attacks. This adversarial data is then poisoned with a specific trigger to create a new backdoor dataset, Dbd. In this setup, the model is trained to respond to specific triggers with targeted labels, while preserving the adversarial class predictions for Dadv. To prevent the model from forgetting previously learned information and to maintain its generalization ability on clean test sets, we employ a logits alignment strategy. This approach helps ensure that training on Dadv does not lead to concept drift. Additionally, during the training process, batch normalization (BN) layers are frozen, which further mitigates the problem of concept drift, stemming from distribution differences between the arbitrary dataset and the unknown training set. This combination effectively embeds the backdoor patterns while preserving the model's performance on clean samples. Lastly, we implement a defense strategy to counter our proposed attack method by identifying the most active samples in the rear layers and isolating them into toxic and clean subsets. We then unlearn the toxic subset while performing normal training on the clean one. Our defensive algorithm also demonstrates its effectiveness in mitigating the backdoor attack we proposed."}, {"title": "2 Methodology", "content": "In this section, we introduce the threat model, the objective of the attack. To address the issue we mentioned above, we propose the attack and defense design."}, {"title": "2.1 Prliminary", "content": "Threat Model. We assume a threat model where the DNN model is either stored in an online or local repository, and the attacker has no access to, and even no knowledge (eg. data distribution, domain knowledge) of the training dataset. The attacker can only modify a few parameters of the DNN model without altering its architecture or other settings.\nProblem Definition. Consider a time series classifier f : X \u2192 Y parameterized by \u03b8, pretrained on a unknown dataset Dtrain. The attacker's objective is to maximize the expected joint probability that the classifier correctly predicts the true label y for clean inputs x and predicts a target label k when the trigger T is applied:\narg max Ex~D [I(arg max fo(x) = y) \u2161(arg max fe(T(x)) = k)], (1)\nwhere T(x) represents the transformation of x by the trigger. y is the true label of x, and k is the attacker's chosen target label. Normally, we assume the training and testing data share the same distribution, we would use the training set to optimize this objective."}, {"title": "2.2 Attack Design", "content": "Since we lack access to the Dtrain, data poisoning is not feasible. Instead, we can introduce an external arbitrary dataset D', apply the trigger T to all x' in D' to construct Dbd, and continue training the benign model fe using D' and the constructed Dbd. Since the distribution of D' may differ significantly from that of Dtrain, the representations of x' from fe might fail to cover all classes, resulting in an ineffective trigger mapping. As shown in Figure 2f, D' occupies only a small portion of the distribution and has minimal overlap with Dtrain.\nDataset Synthesis. To address this, we modify D' using adversarial attacks, generating diverse samples that cover multiple classes. As shown in Figure 2c, the representations of adversarial synthesized dataset Dady were assigned to several distinct clusters, with each cluster overlapping with or closely approximating the representations of the training data. Thus our objective is:\narg min Exady Dadv [LCE(fo\u2032(xadv), \u0177) + LCE(fe' (T(xadv)), k)], (2)\n\u03b8'\nhere, \u0177 is the target class of a successfully adversarially attacked sample xadv. In this setup, the adversarial sample xady acts as a clean sample to ensure the model maintains its classification accuracy on clean data, while T(xadv), the backdoor-triggered version of xadv, is used to guide the model toward incorrect outputs. By minimizing this objective, we aim to establish a strong association between the trigger and the target class. However, this approach can significantly compromise the model's final accuracy on clean samples. Even though Figure 2d shows that the model successfully classifies all backdoor samples into a sin-gle target class, it inevitably introduces the problem of concept drift. As shown in Figure 2a, the test dataset becomes almost entirely concentrated in a single class, which highlights the distribution mismatch between the test and training datasets.\nLogits Alignment. To address the issue of concept drift caused by the naive minimization of the backdoor objective, we propose incorporating a logits align-ment strategy. This strategy aims to mitigate the shift in the model's decision boundary by aligning the logits of adversarial samples xady with their original pre-trained outputs. The proposed alignment strategy is defined as follows:\nLMSE = \n1\nNadv\nNadv\n\u03a3\ni=1\n(i)\nfor (xa) - yadav ||2, (3)\nwhere for (xadv) denotes the logits of the adversarial sample xadv from the cur-rent model for, and yadv represents the target logits derived from the pre-trained model fe. This regularization ensures that the model retains its generalization ability on clean data. Furthermore, we extend the training objective by incorpo-rating the backdoor objective loss for the poisoned samples:\nNbd\n1\nLCE\nNalog for (Trad)), (4)\ni=1\nthus, the total training loss is then defined as:\nL = LMSE + ALCE, (5)\nwhere A is a trade-off parameter that balances generalization ability on clean data and the strength of the backdoor attack, and we set X = 1 as default. As shown in Figure 2, after incorporating the logits alignment strategy, the test dataset's distribution remains closer to its original structure (Figure 2b), and the poisoned samples are successfully mapped to the target class with minimal impact on clean data accuracy (Figure 2c). This demonstrates the effectiveness of the proposed method in mitigating concept drift while achieving the desired backdoor attack objectives.\nBatchNorm Freezing. Since the distribution of Dadv differs from the training data, BN layers may adapt to the new distribution during training, negatively impacting generalization ability. To mitigate this, we freeze the BN layers, pre-serving the learned statistics (mean and variance) from the benign model. This prevents the model from shifting to the new distribution of the pseudo dataset, avoiding distortion of learned features and mitigating concept drift. Additionally, freezing BN layers will force the filters or the internal weights to learn features related to the input trigger, which helps strengthen the association between the trigger pattern and neurons, ultimately improving the ASR."}, {"title": "2.3 Defense Design", "content": "To mitigate the attack we proposed, we investigated a defensive learning method that does not require retraining the model from scratch. From the experiment, we observed a feature space disparity between backdoor samples and clean samples, which can be used to isolate the toxic samples. Figure 3 shows the differences in the response norm among different layers and channels between backdoor samples and clean test samples. For models where the attack was unsuccessful, there was no significant response difference between the backdoor and clean samples. In contrast in Figure 3c, the successful model shows an obvious contrast in the heatmap, especially in the rear layers. Therefore, we can leverage this behavior to identify backdoor samples. A similar observation is also reported by [19], the neurons of the backdoored model are more active than those in the clean model. Thus, we can focus on the rear layers and sort the output norm of the channels for each input sample, then isolate the top r% of samples with the highest response norm as the backdoor set, while the remaining samples are considered to be the clean set. In our experiments, we found that 5% of r% is already effective when the poisoning ratio is 10%. The loss function for the defense method can be defined as:\nL = LCE(fo' (xcl), Ycl) \u2013 a \u00b7 LCE (fo' (xbd), Ybd),\nwhere a is used to control the ratio between the two parts. A larger penalty factor a can help convert faster in unlearning the backdoor patterns in the early stage. As the epochs increase, a decreases linearly to a level comparable to the first term."}, {"title": "3 Evaluation", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Datasets, Models, and Environments. In this project, we implement our task on the UCR benchmark dataset [7] covering various domains to evaluate the performance of our algorithm. We evaluate our method on 4 different models:"}, {"title": "3.2 Attack Performance", "content": "Table 1 shows the comprehensive performance of the TrojanTime attack. The results indicate that, in most cases, the model is able to maintain its original CA while achieving a high ASR. On average, the fixed trigger performs the best, with an ASR of 86.3%, while the CA only decreases by 7.9% compared with the benign one. This demonstrates that our proposed method can successfully compromise a benign model without requiring access to the training data while maintaining a high CA. We also observed that the blended method (powerline) shows significantly lower ASR and CA compared to the patch masking methods (fixed, random). This is because, without carefully designed noise, the frequency distribution of the original data cannot be significantly altered, while the TSC is highly sensitive to the frequency change of the input. In contrast, masking methods can replace parts of the time series, thus obviously modifying the fre-quency map. The observation that high-frequency sine waves achieved higher ASR supports this conclusion. Though the core focus of this paper is not on trigger design, we still believe that with a carefully designed trigger, the ASR of the blended-type trigger could be significantly improved under the settings used in this study."}, {"title": "3.3 Ablation Study", "content": "To validate the effectiveness of each component of our proposed method, we conducted an ablation study. Table 2 shows the performance of different settings on four datasets using the InceptionTime and LSTMFCN models. If the BN layer is trainable, we can observe a general decrease in both ASR and CA. This is because BN may be influenced by the new data distribution, leading to a decrease in generalization ability. The success of the attack relies on the filters learning effective features that can strongly associate with the trigger pattern, as shown in Figure 3. Additionally, without logits alignment does not affect the ASR because the attack objective remains unchanged. However, the absence of original logits removes the information carried by the benign model, causing the model to forget the distribution of previous data, which is consistent with the observations in Figure 2a. Furthermore, if adversarial data is not used to increase the diversity of the classes, the model may favor the most prominent class, resulting in a decrease in CA. Additionally, the lack of a proper mapping from other classes to the target class may also reduce the ASR, depending on the distribution of the injected data D'."}, {"title": "3.4 Defense Evaluation", "content": "Table 3 evaluates the three most aggressive trigger types for defensive learning. It shows that our method effectively reduces the ASR across most datasets and trigger types. Some failed cases can be attributed to the insufficient number of training samples (~20), leading to almost limited bad samples that could be unlearned. From the results, we observe that our method seems to be more effective on the LSTM model compared to CNN models. One possible reason for this is that LSTMs are better suited for sequential data and are more capable of identifying patterns in temporal dependencies, making them more robust to backdoor attacks. On average, ASR is significantly decreased across all models, especially for TwoPatterns, where CA is increased from 86.1% to 93.6%, almost reaching the benign model's accuracy (100%), and ASR reduces to 2.5%."}, {"title": "4 Related Works", "content": "Backdoor Attack. The objective of a backdoor attack is to stealthily per-form a successful attack on the model, and trigger design along with poisoning methods targeting the model are key aspects of achieving this goal. Backdoor attacks were first introduced by [12], which applied a fixed pixel patch in the corner of an image. To enhance stealthiness, Blended [6] was proposed, which mixes a transparent trigger with the original image. SIG[2] stealthily embeds superimposed signals, such as ramp or sinusoidal signals, into the background of images. Dynamic[22] backdoor attacks consider sample-wise triggers generated to confuse defenses and improve the chances of bypassing detection. Clean Label Attack[24] poisons the data without altering the labels. FTrojan[26] focuses on applying backdoor attacks in the frequency domain.\nBackdoor Defense. Backdoor defenses can mainly be categorized into two types: post-process and in-process. Post-process methods typically mitigate the backdoor effect by pruning neurons or fine-tuning the model, such as FP[20], ANP[28], and I-BAU[30]. In-process methods, on the other hand, focus on safely training the model after the data has been poisoned. This process involves iso-lating or suppressing backdoor injection during training, with methods such as ABL[17], DBD [13] and DST[4]."}, {"title": "5 Conclusion", "content": "In this paper, we propose a backdoor attack method that operates in a data-inaccessible scenario. We introduce auxiliary external data and use adversarial attacks to increase its diversity. During training, we employ logits alignment and BatchNorm freezing to mitigate the concept drift problem and enhance the model's generalization ability on clean samples. Additionally, we present a defensive method based on unlearning, where we isolate and fine-tune samples with larger responses in the rear layers. This approach successfully reduces the ASR while maintaining clean accuracy. Since this paper focuses on the design of backdoor training, we have not investigated trigger design. Therefore, in future work, we can explore the optimization of trigger design to further improve the attack's effectiveness and stealthiness."}]}