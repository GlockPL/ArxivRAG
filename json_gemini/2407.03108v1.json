{"title": "How Reliable and Stable are Explanations of X\u0391\u0399 Methods?", "authors": ["Jos\u00e9 Ribeiro", "Lucas Cardoso", "Vitor Santos", "Eduardo Carvalho", "N\u00edkolas Carneiro", "Ronnie Alves"], "abstract": "Black box models are increasingly being used in the daily lives of human beings living in society. Along with this increase, there has been the emergence of Explainable Artificial Intelligence (XAI) methods aimed at generating additional explanations regarding how the model makes certain predictions. In this sense, methods such as Dalex, Eli5, eXirt, Lofo and Shap emerged as different proposals and methodologies for generating explanations of black box models in an agnostic way. Along with the emergence of these methods, questions arise such as \"How Reliable and Stable are XAI Methods?\". With the aim of shedding light on this main question, this research creates a pipeline that performs experiments using the diabetes dataset and four different machine learning models (LGBM, MLP, DT and KNN), creating different levels of perturbations of the test data and finally generates explanations from the eXirt method regarding the confidence of the models and also feature relevances ranks from all XAI methods mentioned, in order to measure their stability in the face of perturbations. As a result, it was found that eXirt was able to identify the most reliable models among all those used. It was also found that current XAI methods are sensitive to perturbations, with the exception of one specific method.", "sections": [{"title": "1 Introduction", "content": "Technology is increasingly evolving on different fronts and efforts, today Artificial Intelligence is a reality in everyday life in our society. There are many real-world problems that machine learning algorithms seek to solve, making human life more automated, intelligent and less complicated [14].\nBlack-box models are driven by algorithms that, despite normally presenting high performance scores when faced with classification or regression problems,"}, {"title": "2 Related works", "content": "This study conducted an literature review, aiming to identify research that proposes existing XAI methods. This allowed for the identification of the main XAI techniques specifically designed to generate global feature relevance rankings, both in a model-agnostic and model-specific manner, applicable to tabular data.\nAs a result, a total of six XAI methods were found to be properly validated and compatible with one another (at library and code execution dependencies level). These methods include: eXirt [31], Dalex [6], Eli5 [18], Lofo [29], SHAP [21] and Skater [24].\nThis survey found other tools aimed at model explanation, including: Alibi [2], CIU [13], Lime [26], IBM Explainable AI 360 [5], Anchor [27], Attention [20] e Interpreter ML [23]. However, due to incompatibilities and technical problems, they ended up not being used in this research.\nThe primary issues and incompatibilities identified were: absence of global rank generation; rank generation dependent on another existing XAI method within the pipeline; incompatibility with pipeline dependencies at the library version level; and outdated method libraries.\nNote that the six methods presented herein generate relevance rankings based on the same previously trained machine learning models (with the same training and testing split), manipulate their inputs and/or produce new intermediate models copies. Therefore, they are required to be compatible with each other so that a fair comparison of their final rankings of explanations can be made.\nIn previous studies, this research used the CIU in its tests, however when carrying out this study it was found that its creators updated their libraries and apparently this method no longer generates feature relevance rankings as previously, which is why it appears as incompatible. Still in table 1, it can be seen that most existing XAI methods use the \u201cFeature Permutation\u201d technique to perform the model explanation process. However, it should be emphasized at this moment, that the eXirt differs from other methods by having base in IRT.\nNote, although eXirt is understood as a model-specific method, table 1. In the research described here, it will be tested with models that go beyond the tree-ensemble, given that its architecture is generalist, as are the two other model-agnostic methods, as mentioned in [31]."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Explainable Artificial Intelligence", "content": "Tools like Dalex, Eli5, eXirt, Lofo, SHAP, and Skater can generate various types of explanations. However, for quantitative comparisons, only their ranking gen-"}, {"title": "3.2 Item Response Theory", "content": "Item Response Theory (IRT), part of Psychometrics, provides mathematical models to estimate latent traits, relating the probability of a specific response to the characteristics of the items evaluated. Traditional assessment methods measure performance by the total number of correct answers, but have limitations, such as dealing with random answers and evaluating the difficulty of each test question. [1].\nUnlike traditional assessments, IRT focuses on test items, evaluating performance based on the ability to get specific items correct, not just the total count of correct answers [16]. IRT seeks to evaluate unobservable latent characteristics of an individual, relating the probability of a correct answer to their latent traits, that is, to the individual's ability in the area of knowledge evaluated. [10].\nIn summary, IRT consists of mathematical models that represent the probability of an individual getting an item correct, considering item parameters and the respondent's ability. Different implementations of IRT exist in the literature, such as the \u201cRasch Dichotomous Model\u201d [19] and the \u201cBirnbaum Three-Dimensional Model\u201d [8] (the last one used here). In the two topics below, it will be described how the main processes of this theory."}, {"title": "Estimation of Item Parameters", "content": "This process involves the estimation of discrimination, difficulty, and guessing based on the 3PL model, using techniques such Maximum Likelihood Estimation - MLE. The objective is find the parameter values that maximize the probability of observing individuals' actual responses to the items, through:\nDiscrimination: consists in how much a specific item i is able to differentiate between highly and poorly skilled respondents. It is understood that the higher its value, the more discriminative the item is. Ideally, a test should feature a gradual and positive discrimination;"}, {"title": "Estimation of ability", "content": "This process is represented of logistic model 3PL, presented in the equation 1, consists of a model capable of evaluating the respondents of a test from the estimated ability (0;), together with the correct answer probability P(Uij = 1 | 0j) calculated as a function of the individual skill j and the parameters of the item i.\n$$P(U_{ij} = 1 | \\theta_j) = c_i + (1 \u2212 c_i)\\frac{1}{1 + e^{-a_i(\\theta_j - b_i)}}$$\n(1)\nThe 3PL is used to model the relationship between individuals' ability and the likelihood of correctly answering an item on a test. It assumes that the probability of a correct answer depends on three item parameters: item discrimination, item difficulty, and item guessing.\nIn the equation 1 the properties discrimination, difficulty and guessing of the items i, are represented respectively by the letters ai,bi, and ci. The 0; is the ability of the individual j, which is a continuous parameter representing the latent trait being measured. P(Uij = 1 | 0;) represents the probability of correctly answering item i for an individual j with ability 0."}, {"title": "4 Methodology", "content": "Aiming to respond to the hypotheses initially launched, a pipeline was developed containing analyzes that can be viewed in the diagram in figure 2.\nThe pipeline starts in figure 2 (A), where a dataset was selected relating to a real-world problem (diabetes disease [30]) with a sensitive context (health) defined as binary classification (aiming to simplify analyses). This database was standardized (using z score) and divided between training and testing in the proportion 70% 30%.\nIn figure 2 (C), analyzes of the main properties of the dataset were carried out, as was done in [25]. The diabetes dataset [30] (Pima Indian diabetes dataset) has 9 numeric features entitled: \u201cNumber of times pregnant\u201d, \u201cPlasma glucose concentration at 2 hours in an oral glucose tolerance test\u201d, \u201cDiastolic blood pressure (mm Hg)\u201d, \u201cTriceps skin fold thickness (mm)\u201d, \u201c2-Hour serum insulin (mu U/ml)\u201d, \u201cBody mass index (weight in kg/(height in m)\u00b2)\u201d, \u201cDiabetes pedigree function\u201d, \u201cAge (years)\u201d, \u201cClass variable (0 or 1)\". It has no missing data and has a 768 instances (500 referring to 0 and 268 referring to 1, classes).\nFour different types of models were created in figure 2 (D), which are: Multi-layer Perceptron (MLP), Light Gradient Boosting Machine (LGBM), \u039a Nearest Neighbor (KNN) and Decision Tree (DT), all hyper parameterized (with cross-validation with folds=4, evaluated by Area Under the Curve (AUC)) aiming to improve the adaptability and performance of the models in the process of generalizing the problem represented by the diabetes dataset.\nThe choice of the types of models was guided by the diversification of tests, including black box models and transparent models, thus LGBM was chosen as a representative of ensemble-based models (black box), the MLP algorithm\""}, {"title": "5 Results and discussion", "content": "The first step to understanding the pipeline results is to observe the behavior of the models' performances obtained from tests with perturbation and without perturbation. The table 2 shows the results of the 4 models, as well as the values obtained from the Accuracy, Precision, Recall, F1 and Roc AUC metrics, based on tests with different percentages of perturbations.\nIt can be seen in the table 2 column with 0% perturbation, that the model with the best performance was LGBM, followed by MLP, DT and KNN. Such results were already expected, as according to [3], in general, black box models perform better than transparent models. Based on the perturbations added to the test set, it was possible to obtain gradual and succinct worsening of performance in the tests with the increase in perturbations (4%, 6% and 10%).\nThe percentages of perturbations 4% and 6% work as controls in the analyses, as it can be seen in some cases, a larger perturbation makes the model perform minimally better than a smaller perturbation. However, this does not occur with perturbations equal to 10%. In order to identify how significant the difference in performance of the models is, the statistical test Friedman Nemenyi [12] was carried out, figure 5.\nIn the test presented in figure 5, the p-value values of Friedman Nemenyi are shown relating to the comparisons of the statistical metrics shown in the table 2. In each cell of the matrix, the closer the value is to 0 (zero), the greater the statistical confidence that can be obtained. Thus, adopting the value of p-value= 0.05 as a cutoff, it can be determined with at least 95% statistical confidence that the models present different performance. Thus, when observing the row \"lgbm: original\" and the columns \"mlp: original\", \"dt: original\" and \"knn: original\", it can be seen that the performances of the models without perturbations do not present a statistically significant difference."}, {"title": "5.1 Are the most reliable models, according to the eXirt method,\nthose being less affected by data perturbations?", "content": "The figures 5.1 and 5.1 were generated exclusively by eXirt seeking to answer the first hypothesis released, where, the green and red lines represent the instances of the test dataset that were passed to the model. The black line (thicker) represents the average of the ICCs. The texts with difficulty, discrimination and guessing values present are averages of the curves found.\nThe experiments that generated the figures 5.1 and 5.1 follow the line of reasoning: when faced with testing models with instance perturbations and without perturbations, it must be understood that the most reliable models/tests (referring to how can a human trust) are the unperturbed ones. Therefore, it is expected that the results of unperturbed models will be more stable and therefore reliable.\nThus, when observing the figure 5.1, it can be seen that even for models with unperturbed tests (first column on the left of the figure), eXirt was able to indicate the most reliable model, in this case the LGBM model, as it presents the least difficulty (-2.18), discrimination (1.54) and lowest guessing value (0.14), compared to the MLP values.\nWhen observing the item characteristic curves generated from the insertion of perturbations, figure 5.1, it can be seen that the difficulty, discrimination and guessing values change succinctly, as in tests with perturbations of 0% and 10% of instances, the difficulty goes from -2.18 to -1.86 (LGBM) and from -1.77 to -1.78 (MLP). Discrimination increases from 1.54 to 1.59 (LGBM) and from 1.75 to 1.6 (MLP). Finally the guess goes from 0.14 to 0.17 (LGBM) and 0.18 to 0.19 (MLP). It is noted that the relationships between difficulty and discrimination are inversely proportional, with guessing being proportional to difficulty, not obeying linear proportional rules.\nThe results in figure 5.1 without perturbation show that eXirt was able to indicate the most reliable models, as it was able to indicate the best model that obtained the best performance (between MLP and LGBM) and also indicated"}, {"title": "5.2 Do existing XAI methods generate stable explanations even\nafter data perturbations?", "content": "Aiming to answer the hypothesis released, the relevance ranks of features gen-erated from the XAI methods Dalex, Eli5) were selected., eXirt, Lofo, Shapand Skater, aiming to evaluate their behavior given the need to model explana-tions with no perturbations and with perturbations. The central idea here is toidentify the methods that are most stable to perturbations.\nIn view of this, we have the figure 5.2 as a general summary that shows allthe relevance ranks of features generated from the tests carried out. Each linereferences an XAI method, ordered from most stable (topmost in the figure) toleast stable (bottommost in the figure). Each column references different models(LGBM, MLP, DT and KNN).\nSub-figures referring to the experiments are also shown, where the names ofthe features present in each rank are indicated on the y axis and the percentagesof perturbations are indicated on the x axis: 0%, 4%, 6% and 10% (from left toright). In these sub-figures, the Spearman Correlation values existing betweenthe ranks generated by models with the presence of perturbations in relation tothe ranks generated by models without perturbations are also displayed. In thetitle of these sub-figures the sum of the calculated correlations is presented.\nAs shown in figure 5.2, it can be seen that the shap method was the moststable XAI method, presenting the same feature relevance rank for the fourmodels, figure 5.2 (A, B, C and D), bearing the maximum value of the sumsof correlations (sum = 3) in each model with changes being observed in theranks generated by shap only in perturbations above 30%, experiments carriedout separately). Next are the results of the skater method, which presented thesame ranks for the LGBM (sum = 3), MLP (sum = 3) and DT (sum = 3)"}, {"title": "6 Conclusion and Future Works", "content": "This research showed how reliable and stable the explanations of current \u03a7\u0391\u0399 methods aimed at generating feature relevance ranks are. Highlighting eXirt as an XAI method capable of generating extra information regarding how reliable a model is from the IRT perspective (and its properties difficulty, discrimination and guessing). This research also showed that current XAI methods, with the"}]}