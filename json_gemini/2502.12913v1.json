{"title": "GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning", "authors": ["Sifan Zhou", "Shuo Wang", "Zhihang Yuan", "Mingjia Shi", "Yuzhang Shang", "Dawei Yang"], "abstract": "Large Language Models (LLMs) fine-tuning technologies have achieved remarkable results. However, traditional LLM fine-tuning approaches face significant challenges: they require large Floating Point (FP) computation, raising privacy concerns when handling sensitive data, and are impractical for resource-constrained edge devices. While Parameter-Efficient Fine-Tuning (PEFT) techniques reduce trainable parameters, their reliance on floating-point arithmetic creates fundamental incompatibilities with edge hardware. In this work, we introduce a novel framework for on-device LLM fine-tuning that eliminates the need for floating-point operations in both inference and training, named GSQ-Tuning. At its core is the Group-Shared Exponents Integer format, which efficiently represents model parameters in integer format using shared exponents among parameter groups. When combined with LoRA-like adapters, this enables fully integer-based fine-tuning that is both memory and compute efficient. We demonstrate that our approach achieves accuracy comparable to FP16-based fine-tuning while significantly reducing memory usage (~ 50%). Moreover, compared to FP8, our method can reduce ~ 5\u00d7 power consumption and ~ 11\u00d7 chip area with same performance, making large-scale model adaptation feasible on edge devices.", "sections": [{"title": "1 Introduction", "content": "Recent advances in Large Language Models (LLMs) have delivered impressive results in a variety of natural language tasks (Touvron et al., 2023a,b; Liu et al., 2023). LLMs are typically trained in several stages, including large-scale pretraining followed by one or more fine-tuning phases (Dubey et al., 2024; Liu et al., 2024a). LLM fine-tuning approaches like supervised fine-tuning (SFT) (Zhang et al., 2023), usually employ curated, high-quality corpora for refining the model with a standard language modeling (Chiang et al., 2023). Despite their effectiveness, most LLM fine-tuning approaches require powerful cloud servers or GPUs equipped with large memory capacities. This poses two significant challenges in real-world settings: (1) uploading sensitive data to remote servers poses a fundamental privacy risk, and (2) in many practical scenarios, models must be deployed on resource-constrained edge devices such as mobile processors or embedded AI accelerators-where memory and power budgets are tightly limited. Such constraints become critical in LLM's personalized applications, where data cannot be shared with the cloud and model updates must remain local to ensure privacy. Meeting these challenges thus necessitates on-device adaptation methods capable of preserving data privacy and functioning within the limited memory and compute budgets of edge hardware.\nParameter-Efficient Fine-Tuning (PEFT) (Han et al., 2024) techniques such as LoRA (Hu et al., 2021) and QLoRA (Dettmers et al., 2023) alleviate part of this burden by reducing trainable parameters to around 1% of the original model. Unfortunately, they remain reliant on floating-point operations for both forward and backward passes, which clashes with edge-device constraints in three ways. First, during fine-tuning, weights, activation and gradients must be stored and updated in high-precision floating-point. It introduces additional overhead or even makes the LLM fine-tuning impractical on edge devices. Second, floating-point representations incur high memory overhead (e.g., FP16 doubles the memory cost compared to INT8; for a 7B-parameter model, this can surpass 20GB memory during fine-tuning process, presenting substantial challenges for mobile processors). Last but not least, commercial edge AI accelerators (e.g., Qualcomm Hexagon (QUALCOMM, 2024)) typically get peak throughput only on integers, leaving up to 84% of compute units idle under FP16 training.\nTherefore, eliminating floating-point arithmetic for fine-tuning would have a substantial impact on software, hardware, and application design for efficient on-device LLM adaptation (ARM, 2020; Kim et al., 2021). While previous studies on integer quantization (Jacob et al., 2018a; Kim et al., 2021; Xiao et al., 2022; Yuan et al., 2023a) verify the feasibility of inference, they do not extend to gradient quantization, which is required for effective fine-tuning of LLMs at the edge.\nIn this paper, we propose a new framework for resource-efficient on-device LLM fine-tuning, termed GSQ-Tuning. Central to our method is the Group-Shared Exponents Integer format, a novel quantization strategy that replaces floating-point with a specialized integer-based representation. We integrate this with parameter-efficient LoRA-like modules to enable fully on-device fine-tuning without incurring large memory and computation costs. We further examine this design through a Pareto frontier analysis, which demonstrates how various bits-rank settings impact the trade-off between fine-tuning memory costs and accuracy. Extensive experiments across models of varying scales, different fine-tuning datasets, and diverse tasks have demonstrated the effectiveness and generalizability. We highlight our main contributions as follows:\n\u2022 Group-Shared Exponents Integer Quantization: We introduce a quantization strategy that shares exponents among groups, thereby reducing the storage and computation overhead while still representing model parameters in integer format. Combined with LoRA-like adapters, our method supports fine-tuning under tight memory constraints.\n\u2022 Integer Forward and Backward Computations: By extending integer quantization pipelines beyond inference to include gradients, both forward and backward passes remain hardware-friendly and efficiently utilize integer-focused edge accelerators.\n\u2022 Pareto Frontier for Quantization Bits and Low-rank: We demonstrates how various bits-rank settings impact the trade-off between fine-tuning memory costs and accuracy through a Pareto frontier analysis. We empirically show that our approach achieves accuracy on par with FP16-based fine-tuning while dramatically lowering both ~ 50% memory usage. Furthermore, compared with FP8, at comparable performance levels, our method (GSE-INT5) reduces the power consumption of MAC unit by ~ 5 \u00d7 and decreases chip area by ~ 11 \u00d7 comparing to the origin."}, {"title": "2 Method", "content": "In this section, we present GSQ-Tuning, a fully quantized training method for on-device LLM fine-tuning. We begin by reviewing the fundamentals of LLM PEFT, highlighting the bottlenecks of implementing existing PEFT methods on device, and then review relevant neural network quantization literature (Sec.2.1). Building on these insights, we propose a new LLM fine-tuning framework\u2014Group-Shared Exponents Integer in Fully Quantized Training-for on-device scenarios. To enable this framework, we design two key components: (1) A Group-Shared Exponents Integer data format to replace floating-point representations (Sec.2.2). (2) A Fully Quantized Fine-tuning Framework that leverages our new data format (Sec.2.3). Finally, we explore the performance-efficiency trade-off in GSQ-Tuning via Pareto frontier analysis (Sec.2.4), providing practical guidance for its use."}, {"title": "2.1 Preliminaries", "content": "Low-rank Adaptation. LoRA (Hu et al., 2021) is a milestone method that injects trainable low-rank adapters into linear layers, allowing efficient fine-tuning while keeping the original parameters unchanged. Specifically, a LoRA linear layer is parameterized by a non-trainable weight matrix $W \\in \\mathbb{R}^{o_c \\times i_c}$, along with trainable components $A \\in \\mathbb{R}^{r \\times i_c}$ and $B \\in \\mathbb{R}^{o_c \\times r}$, where r is a small integer. The input $X \\in \\mathbb{R}^{b \\times i_c}$ and output $Y \\in \\mathbb{R}^{b \\times o_c}$ correspond to a linear layer with $o_c \\times i_c$ processing a batch of size b. Building on LoRA, QLORA integrates it with 4-bit NormalFloat (NF4) quantization and Double Quantization (DQ)techniques, enabling the fine-tuning of a 65B parameter model on a single 48GB GPU with minimal performance loss. In this paper, due to the memory constraint of on-device PEFT, we adopt QLoRA to quantize the weights of LLMs. The formulation is:\n$Y = X DQ(W_{NF4})^T + XA^T B^T$\nwhere we omit the transpose for similarity, NF4 means the 4 bit NormalFloat (NF) data type and"}, {"title": "2.2 Group-Shared Exponents Integer", "content": "Low-bitwidth Floating Point (FP). Floating-point numbers are a commonly used data representation in deep learning. For instance, FP16 represents each number using 16 bits. Recently, lower-bit floating-point representations, such as FP8, have been introduced into the training processes of deep learning models (Micikevicius et al., 2022; Baalen et al., 2023). FP8 operates in two modes: the E4M3 and E5M2 formats. In these formats, E represents the number of exponent bits and M denotes that of mantissa bits.\nSimilar to quantization methods, low-bitwidth FP formats can effectively reduce memory storage requirements and decrease the hardware area and energy consumption of computational units.\nHowever, we observe that low-bitwidth FP may not be the optimal solution for LoRA fine-tuning in large-scale models, primarily due to the following 3 reasons: (1) Neural network tensors exhibit spatial locality, meaning that adjacent elements within a tensor tend to have similar magnitudes, leading to redundancy in the exponent bits of FP representations. As illustrated in Fig. 1, the standard deviation of the values in the weight tensor is considerably lower than the magnitude of the values, indicating a small local variation; (2) The limited number of mantissa bits in low-bitwidth FP formats constrains precision, potentially impairing model performance. For instance, the E5M2 format, which has only two mantissa bits, is incapable of representing certain integers below ten, such as 5, 7, and 9; (3) FP computation demonstrates less efficiency compared to INT computation, making it less suitable for resource-constrained environments. As shown in Table 5, FP formats incur considerably higher costs in power and chip area compared to integer-based computation.\nDue to the inherent characteristics of FP representations and the requirement for relatively high precision in training, it is crucial to explore other data format to reduce both hardware area and energy consumption in resource-constrained cases.\nGroup-Shared Exponents Integer (GSE-INT). Inspired with block FP (Zhang et al., 2022), we propose the Group-Shared Exponents Integer (GSE) format as an alternative to FP formats for matrix multiplication in both forward-propagation and back-propagation. This format is also used for storing activations required by back-propagation to reduce memory consumption. As illustrated in Fig. 2, GSE introduces the following key modifications compared to traditional floating-point formats: (1) To leverage the locality of tensor values, we share the exponent across a group of N numbers. That is, all N numbers within the group use the same exponent. (2) The number of bits used for the shared exponent is fixed at 5. (3) The implicit leading 1 in floating-point representations is removed and replaced with a standard integer representation. The numerical representation in GSE is:\n$x = (-1)^s \\cdot 2^e \\cdot m$\nwhere s is sign, e is the exponent value (For simplicity, we omit the exponent bias), m is the mantissa value. The GSE format is memory efficient through sharing exponent bits. Memory for FP is N(E + M + 1) and memory for GSE is N(M + 1) + E. As the group size N increases, the memory savings grow proportionally, while the overhead of the shared exponent is negligible.\nMatrix Multiplication using GSE. Consider two vectors, A and B, both represented using the GSE format and having a length of N. The dot product of the two vectors can be computed as:\n$y = 2^{e_A+e_B} \\sum_{i=1}^{N} (-1)^{s_A s_B} m_{A,i} \\cdot m_{B,i},$\nwhere $m_{A,i}$ and $m_{B,i}$ are the integer mantissas of the i-th elements of the vectors. The computation involves a standard integer multiply-accumulate (MAC) operation, followed by scaling with the combined exponent $2^{e_A+e_B}$.\nThe dot product operation can be extended to large-scale matrix multiplication. For two matrices X and Y, we partition the data into groups of size N. Specifically, rows of X are grouped along their elements, with each group sharing a single exponent, and columns of Y are grouped similarly. This grouping strategy simplifies hardware implementation and makes the GSE format a practical and efficient choice for large-scale matrix operations.\nTransform from FP to GSE. The transformation from FP representation to GSE format is efficient"}, {"title": "2.3 Fully Quantized Fine-tuning", "content": "As illustrated in Fig. 3, our GSQ-Tuning framework introduces a hardware-efficient quantization pipeline. Compared to QLoRA, we fully quantize weights, activations, and gradients to low-bit integers. While QLORA primarily focuses on 4-bit quantization of frozen base model weights (NF4) while keeping adapters in high precision (BF16), our approach achieves superior computational and memory efficiency. Building on the quantize-compute-dequantize (QCD) paradigm for low-precision matrix multiplication (MM) (Xi et al., 2024), the QCD approach operates in three stages: (1) Quantization: Convert high-precision inputs matrices (e.g., BF16) to low-precision (e.g., GSE-INT6) using a quantizer Q(\u00b7); (2) Computation in low-precision MM: Perform low-precision MM to produce an intermediate output (e.g., GSE-INT6); and (3) Dequantization: Convert output back to high-precision using a dequantizer $Q^{-1}(\u00b7)$.\nForward Propagation. The forward propagation for a linear layer is calculated as follows:\n$Y_{BF16} = Q^{-1} (Q(X_{BF16}) Q(DQ(W_{NF4})))+\nQ^{-1} (Q(X_{BF16}) Q(A_{BF16})^T Q(B_{BF16})^T)$"}, {"title": "Backward Propagation.", "content": "Gradients are computed directly on quantized tensors using back propagation and chain rule:\n$\\frac{\\partial \\mathcal{L}}{\\partial B} = Q^{-1}(Q(\\frac{\\partial \\mathcal{L}}{\\partial Y})Q(X)Q(A))$\n$\\frac{\\partial \\mathcal{L}}{\\partial A} = Q^{-1}(Q(\\frac{\\partial \\mathcal{L}}{\\partial Y})Q(X)Q(B))$\n$\\frac{\\partial \\mathcal{L}}{\\partial X} = Q^{-1}(Q(\\frac{\\partial \\mathcal{L}}{\\partial Y}) (Q(W)+Q(B)Q(A)))$"}, {"title": "2.4 Pareto Frontier for Quantization Bits and Low-rank.", "content": "Co-optimization Principle for Model Bits and Rank. The memory footprint and FLOPs during fine-tuning exhibit strong dependence on both quantization bit-width and LoRA rank $O(b \\cdot r)$ scaling. Excessive values in either dimension impose prohibitive computational burdens: (1) Memory: $Mem \\propto b \\cdot r$ (adapter parameter storage); (2) Compute: $Flops \\propto r \\cdot d^2$ (for hidden dimension d). This necessitates joint optimization of (b, r) to guide the accuracy-efficiency trade-off space effectively. Pure bit-width reduction sacrifices model capacity, while unrestrained rank scaling inflates computation costs disproportionately.\nThe effectiveness of GSQ-Tuning hinges on how quantization bit-width interacts with the dimensions of low-rank adapters. To inform real-world deployments, we systematically analyze this interplay by constructing a Pareto frontier that illustrates the balance between model memory consumption during fine-tuning and accuracy across various bits-rank settings. We hope our findings not only highlight optimal configurations, but also offer practical guidelines for practitioners to tailor solutions to specific hardware constraints.\nPareto Frontier Analysis. Based on our GSQ-Tuning, we construct a Pareto frontier by plotting model memory during fine-tuning against validation accuracy across different bits-rank configuration. As shown in Fig. 4, the frontier reveals three distinct optimization regimes (1) High-Bit Low-Rank Regime (8-bit, r=64): Reaches 65.60 Acc with suboptimal efficiency. 0.50 Acc gain from r = 16 to 64 indicates high-bit quantization inherently limits error magnitude, requiring less rank compensation. (2) Mid-Bit Balanced Regime (6-bit, r=128): Delivers 65.58 Acc with moderate resources. 0.71 Acc gain from r = 16 to 128 shows diminishing returns beyond this point (only"}, {"title": "3 Experiments", "content": "Foundation Models and Evaluation Metrics. We apply our method to the entire LLaMA family, including LLaMA-2 (7B/13B/70B)(Touvron et al., 2023b), and LLaMA-3 (3B-8B). We evaluate the fine-tuning models on up to 9 zero-shot tasks using the lm-evaluation-harness (version 0.4.7)(Gao et al., 2024), including BoolQ(Clark et al., 2019), HellaSwag (Zellers et al., 2019), LAMBADA (OpenAI) (Radford et al., 2019), Open-BookQA(Mihaylov et al., 2018), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), Wino-Grande (Sakaguchi et al., 2019), ARC-Easy, and ARC-Challenge (Clark et al., 2018). The fine-tuning dataset follows Alpaca (Taori et al., 2023), with 52K instruction data from text-davinci-003.\nTraining Details. We employ the whole fine-tuning process based on LLaMA-Factory (Zheng et al., 2024). We implement GSQ-Tuning in PyTorch using models from Hugging Face. We freeze the parameters of the linear modules and update a smaller (low-rank) set of parameters during the fine-tuning. The group size of our GSQ-Tuning is 32. We fine-tune models using the 8-bits AdamW optimizer (Dettmers et al.) in bfloat16 precision. We choose the constant learning rate schedule and set the learning rate to be 1 \u00d7 10-5"}, {"title": "3.1 Overall Results", "content": "GSQ-Tuning Results on LLaMA Family. Here, we compare the fine-tuning performance across LLaMA family (3B 70B) against QLoRA. As shown in Tab. 1, GSQ-Tuning achieves comparable or better zero-shot accuracy across different LLaMA model scales (7B-70B) under a fully low-bit quantization fine-tuing setting. With 8-bit quantization precision (W8A8G8), GSQ-Tuning matches or exceeds QLoRA's performance on 83% of tasks, despite using 50% fewer bits for activations and gradients. Even at aggressive 5-bit quantization (W5A5G5), the GSQ-Tuning maintains 98.6% of QLORA's average accuracy while reducing memory footprint by ~ 50%. These results confirm GSQ-Tuning's effectiveness for resource-constrained edge deployment. Besides, we present comprehensive results of our GSQ-Tuning across various LlaMA models in Sec. A.2. This includes LlaMA2-7B (Tab.8), LlaMA2-13B (Tab.9), LlaMA2-70B (Tab.10), LlaMA3-3B (Tab.11), and LlaMA3-8B (Tab.12). The findings consistently highlight the effectiveness and efficiency of GSQ-Tuning.\nComparison with FP8. Here, we compare the designed GSE data format with FP8 in fully quantized fine-tuning framework. As shown in Tab. 2, the results demonstrate that the designed GSE implemented in our GSQ-Tuning method achieves superior fine-tuning performance compared to FP8 while significantly reducing computation efficiency. Even under 5-bit settings, GSQ-Tuning maintains fine-tuning performance on par with FP8, further validating its effectiveness. Additionally, to mitigate the impact of rank variations, we report the fine-tuning results using 64-rank setting in Tab. 13 of the appendix. Extensive experiments consistently support the advantages of our approach."}, {"title": "3.2 Generalization Results", "content": "Generalization of GSQ-Tuning for Vision-Language Model (LLaVA). Model used is LLaVA-v1.5-7B (Liu et al., 2024b) with Vicuna-7B-v1.5 (Zheng et al., 2023) as language model and CLIP ViT-L-336px (Radford et al., 2021) as vision tower, connected by a 2-layer MLP. Instruction dataset and other settings for finetuning follow the LLaVA official repository, LLaVA-Instruction (Liu et al., 2024c) and the improved one (Liu et al., 2024b). Tab. 3 shows performance drop of the vanilla quantization of 4-bits/64-rank QLoRA, especially, referring to the TextVQA evaluation. Fine-tuning with GSE shows comparable performance compared to that with BF16. BF16 is of E8M7 while GSE is of E5M7, demonstrating the redundancy of the dynamic range w.r.t. exponents is at least 3-bits much. Moreover, the memory cost of GSE is about a half of BF16.\nGeneralization of GSQ-Tuning on Other Fine-tune Dataset. Here, we also select Commonsense170K (CS170K) (Hu et al., 2023) to evaluate the generalization ability of GSQ-Tuning across different fine-tuing dataset. CS170K is a dataset constructed from the training sets of BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA with pre-defined templates, comprising 170K commonsense reasoning samples. As shown"}, {"title": "3.3 Ablation Study.", "content": "Group Size Analysis. As shown in Tab. 6, our GSQ-Tuning with 32 groups achieves optimal accuracy-efficiency balance in 6-bit configurations (W6A6G6). The 32-group setting yields significantly higher average accuracy (65.39) compared to 64-group (64.72) and 128-group (64.27) variants, while maintaining comparable memory efficiency (70.96 vs 69.22/69.53). This sweet spot emerges from the tension between quantization bit width and hardware deployment - smaller groups better capture value distributions but increase computation overhead, while larger groups sacrifice adaptation granularity. We therefore adopt group=32 as the default configuration.\nLow Rank Size Analysis. As shown in Tab. 7, the results demonstrates the accuracy-efficiency trade-off across different LORA ranks in 6-bit configurations (W6A6G6). While larger ranks consistently improve performance (64.87 \u2192 66.31 average accuracy for rank 16 \u2192 512), the marginal gains diminish significantly beyond rank 64 (+0.38 from 64\u2192128 vs +0.73 from 16 \u2192 64). This aligns with the spectral properties of weight matrices, where most task-relevant information is captured by the dominant singular vectors. We observe that rank=64 provides an optimal balance, achieving 98.6% of the maximum accuracy while using 50% less memory than rank=512 configurations."}, {"title": "4 Related Work", "content": "Parameter-Efficient Fine-Tuning (PEFT). PEFT reduces memory and computational costs by introducing a small set of trainable parameters while keeping the pretrained model frozen. Approaches including soft prompt tuning (Wang et al., 2023), partial fine-tuning (Fu et al., 2023), and low-rank adaptation (Hu et al., 2021). Among these, LoRA stands out as a seminal work, injecting trainable low-rank matrices into linear layers to enable efficient fine-tuning without modifying the base model weights. QLoRA extends this with 4-bit NF4 quantization and Double Quantization, supporting 65B model fine-tuning on a single 48GB GPU with minimal performance degradation. Despite recent studies further extending LoRA from the perspective of quantization-aware fine-tuning (Xu et al., 2023; Li et al.) to improve efficiency, stability, and performance (Hu et al., 2023; Liu et al., 2024d; Zhao et al., 2024; Hayou et al., 2024; Meng et al., 2024), these methods still maintain compute-intensive forward and backward propagations at high bit-widths during fine-tuning. This results in a substantial computational burden when targeting edge AI accelerators.\nQuantization. Much excellent works (Yuan et al., 2023a; Shang et al., 2023; Yuan et al., 2023b; Yue et al., 2024; Frantar et al., 2022; Xiao et al., 2022; Hu et al., 2024) use the quantization techniques to accelerate the inference of LLMs. For instance, GPTQ (Frantar et al., 2022) quantizes weights to 3-4 bit with slight accuracy drop based on approximate second-order information. AWQ (Lin et al., 2024), SmoothQuant (Xiao et al., 2022), and OmniQuant (Shao et al.) explore the scheme of smoothing by detecting the importance of different activation channels. Recent works (e.g., Quarot (Ashkboos et al., 2024), SpinQuant (Liu et al., 2024e), OSTQuant (Hu et al., 2025)) further suppress outliers by utilizing computation-invariant rotation transformation. However, above methods mainly focus on the post-training optimization while overlooking the overhead during training. Interestingly, several studies (Banner et al., 2018; Drumond et al., 2018; Adelman and Silberstein, 2018; Wu et al., 2018; Langroudi et al., 2019a,b; Yang et al., 2020; Zhu et al., 2020; Xi et al., 2023) have made much efforts to improve the efficiency and optimization of the training process, notably through Fully Quantized Training (FQT). FQT involves quantizing all ten-"}, {"title": "5 Conclusion", "content": "In this paper, we propose GSQ-Tuning, a resource-efficient framework that addresses the critical challenges of floating-point dependency, privacy risks, and hardware incompatibility in on-device LLM fine-tuning. By integrating Group-Shared Exponents Integer (GSE) quantization with parameter-efficient adaptation, our method achieves three key advancements:(1) Full Integer Pipeline: Eliminates floating-point operations across both forward and backward passes, reducing memory usage by 50% compared to FP16 while maintaining comparable accuracy. (2) Hardware-Optimized Design: The GSE format reduces metadata overhead via group-wise exponent sharing, enabling 5-8bit integer representations. Combined with LoRA-like adapters, this achieves 5 \u00d7 lower power consumption and 11\u00d7 smaller chip area compared to FP8 at equivalent accuracy levels. (3) Practical Deployment Guidance: A Pareto frontier analysis guides optimal bit-rank configurations for diverse edge constraints. These innovations establish GSQ-Tuning as a foundational step toward democratizing LLM adaptation for resource-constrained environments. This breakthrough makes private, on-device LLM adaptation practical for sensitive applications. Future work will explore sub-4bit quantization to further push the boundaries of edge AI."}, {"title": "6 Limitations and Future Work", "content": "Limitations While our GSQ-Tuning significantly advances on-device LLM adaptation through integer-focused optimization and parameter-efficient quantization, two key limitations warrant discussion:\nNon-linear Operator Precision. Our current implementation maintains non-linear operations (e.g., LayerNorm, Softmax) in 16-bit to preserve numerical stability. This introduces partial precision conversion overhead during computation. However, non-linear operations do not contain additional learnable parameters and thus do not consume memory. Moreover, these non-linear operations are generally computation-light, making their computational burden negligible. Future work could explore fully integer implementations for non-linear layers.\nBit-Width Range Constraints. The current framework operates effectively in 5-8bit configurations but didn't present the performance at extreme low bit (\u2264 4bit) precision. This stems from gradient direction distortion under extreme quantization\u2014a challenge requiring new error compensation mechanisms. We plan to investigate two directions: (1) 4bit stochastic rounding with gradient-aware scaling, and (2) mixed-precision adapters allocating higher bits to critical gradient dimensions.\nFuture Work Furthermore, future work could explore (1) full integer fine-tuning, (2) extreme low-bit quantized fine-tuning and (3) co-design with emerging integer-optimized AI accelerators. Our code will be publicly available to advance edge LLMs research."}, {"title": "A Appendix", "content": "A.1 Differences with Quantization-aware training (QAT):\nQuantization-aware training (QAT) (Choi et al., 2018; Zhang et al., 2018; Zhou et al., 2017; Jacob et al., 2018a; Dong et al., 2019b,a; Shen et al., 2019; Zafrir et al., 2019; Shen et al., 2020; Tang et al., 2022; Zhang et al., 2020; Bai et al., 2020; Foret et al., 2020; Wang et al., 2022) is an inference acceleration technique which trains networks with quantizers inserted in the forward propagation graph, so the trained network can perform efficiently during inference. QAT can compress activation/weights to extremely low precision (e.g. 1-2 bits). It is tempting to think that directly applying a quantizer for QAT to FQT can lead to similar low activation/weights bit-width. However, even only quantizing the forward propagation for FQT is much more challenging than QAT because: 1 QAT requires a converged full-precision model as initialization (Esser et al., 2019) and/or as a teacher model for knowledge distillation (Bai et al., 2020); 2 QAT may approximate the discrete quantizer with continuous functions during training (Gong et al., 2019), which cannot be implemented with integer arithmetic. Due to these challenges, it is still an open problem to do FQT with low-bit activations/weights."}]}