{"title": "Multi-Turn Code Generation Through Single-Step Rewards", "authors": ["Arnav Kumar Jain", "Gonzalo Gonzalez-Pumariega", "Wayne Chen", "Alexander M Rush", "Wenting Zhao", "Sanjiban Choudhury"], "abstract": "We address the problem of code generation from multi-turn execution feedback. Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards. We propose a simple yet scalable approach, \u00b5CODE, that solves multi-turn code generation using only single-step rewards. Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn. \u00b5CODE iteratively trains both a generator to provide code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code. Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art baselines. We provide analysis of the design choices of the reward models and policy, and show the efficacy of \u00b5CODE at utilizing the execution feedback. Our code is available here.", "sections": [{"title": "1. Introduction", "content": "Software engineers often iteratively refine their code based on execution errors. A common strategy for machine code generation is thus to repair code using execution feedback at test time (Chen et al., 2024; Wang et al., 2024b; Zhao et al., 2024). However, prompting alone is insufficient as it cannot teach how to recover from all possible errors within a limited context.\nWe need to train models that can learn from execution feedback during training. Existing approaches fall into either single-turn or multi-turn settings. In the single-turn setting, methods either train without execution feedback (Zelikman et al., 2022) or perform one-step corrections (Welleck et al., 2023; Ni et al., 2024). However, these struggle to iteratively correct errors over multiple turns. Multi-turn approaches, on the other hand, rely on complex reinforcement learning (RL) (Gehring et al., 2024a; Kumar et al., 2024b; Zhou et al., 2024) to optimize long-term rewards. While effective in principle, these methods suffer from sparse learning signals which makes learning inefficient.\nOur key insight is that code generation is a one-step recoverable Markov Decision Process (MDP), implying that the correct code can be recovered from any intermediate state in a single step. This allows us to greedily maximize a one-step reward instead of relying on complex multi-step reward optimization. As a result, this reduces the problem from reinforcement learning, which requires exploration and credit assignment, to imitation learning, where the model simply learns to mimic correct code, leading to a more stable and efficient training process.\nWe propose \u00b5CODE, a simple and scalable approach for multi-turn code generation from execution feedback. During training, \u03bcCODE follows an expert iteration (Anthony et al., 2017) framework with a local search expert, enabling iterative improvement of both the generator and the expert. The process begins by rolling out the current code generator to collect interaction data with execution feedback. A single-step verifier is then trained on this data and utilized to guide a local search expert in refining the code and generating training labels. Finally, the generator is fine-tuned using these labels. Given recent trends of test-time scaling in generating high quality solutions (Brown et al., 2024; Snell et al., 2024; Wu et al., 2024), \u00b5CODE also uses the learned verifier for inference-time scaling. Here, \u00b5CODE samples N trajectories; at each step, \u00b5CODE picks the best code solution ranked by the learned verifier.\nThe key contributions of this work are as follows:\n1. A novel framework, \u00b5CODE, for training code generators and verifiers through multi-turn execution feedback. We add theoretical analysis of performance bounds using the property of one-step recoverability for this task.\n2. We propose a multi-turn Best-of-N (BoN) approach for inference-time scaling and present benefits of a learned verifier to select the code solution at each turn.\n3. Our approach \u00b5CODE outperforms leading multi-turn approaches on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021) benchmarks. Our ablations demonstrate that learned verifiers aid in learning better generators and show promising scaling law trends with higher inference budgets."}, {"title": "2. Background", "content": "Multi-turn Code Generation as a MDP. In multi-turn code generation, an agent iteratively refines a program to maximize its correctness on private test cases. Given an initial problem prompt x, at each turn t, the agent generates a complete code snippet yt and executes it on a set of public tests. The outcomes ot from these tests serve as observations that guide subsequent refinements. This process continues until the agent generates a code snippet yt that passes all public tests, at which point the episode terminates, or until the maximum number of turns T is reached without success. The first successful code, yt, is then evaluated on private tests to compute the correctness score C'(x, yt) \u2208 {0,1}.\nWe model this as a Markov Decision Process (MDP), where the state is the interaction history St = {x, Y1, 01,..., Yt\u22121, Ot\u22121} where 81 = {x}, and the action is the code at = yt. The oracle reward is defined as R(st, at) = R(x, at) = C(x, yt) if yt passes all public and private tests (terminating the episode), or 0 otherwise.\nDuring training, given a dataset of problem prompts D, the goal is to find a generator \u03c0\u03bf(Yt|x, Y1, 01, \u00b7\u00b7\u00b7, Yt\u22121, Ot\u22121), that maximizes the cumulative discounted reward R(x, yt):\n$\\max _{\\pi_{\\theta}} E_{x \\sim D, y_{t} \\sim \\pi_{\\theta}(. | s_{t})} \\sum_{t=1}^{T} \\gamma^{t-1} R\\left(x, y_{t}\\right)$.\n(1)"}, {"title": "3. \u03bcCODE: Multi-turn Code Generation", "content": "We propose \u00b5CODE, a simple and scalable algorithm for multi-turn code generation using execution feedback. \u00b5CODE follows an expert iteration (Anthony et al., 2017) framework with a local search expert. \u00b5CODE iteratively trains two components a learned verifier R to score code snippets (Section 3.2), and a generator \u03c0\u0473 to imitate local search with the verifier (Section 3.3). This iterative process allows the generator and expert to bootstrap off each other, leading to continuous improvement. At inference time, both the generator and verifier are used as BoN search to select and execute code (Section 3.4). Finally, we analyze the performance of \u00b5CODE in Section 3.5."}, {"title": "3.1. The \u03bcCODE Algorithm", "content": "Algorithm 1 presents the iterative training procedure. At an iteration i, the current generator \u03c0\u03b8 is rolled out in the multi-turn code environment & to generate interaction data Di\u2190 {(x, St, Yt,rt)}. Every turn t in D\u2081 includes the prompt x, interaction history st, code generated yt and the correctness score from the oracle verifier rt = R(x,yt). This data is then aggregated D \u2190 DUD\u00bf. The learned verifier R is trained on the aggregated data D. An expert is created using R to perform local search to find the optimal action \u03c0(x) = arg maxy\u2208D(x) R(x, y), where D(x) are all the code completions for a given prompt x. The expert \u03c0(x) relabels the data D with the optimal action. The generator \u03c0\u03cc is then trained via fine-tuning (FT) on D. This process iterates M times, and the best generator and verifier pair on the validation dataset are returned."}, {"title": "3.2. Training Verifier", "content": "The learned verifier provides dense scores to code solutions for a given problem. At train time, this is used by the expert to perform local search to obtain optimal code. At inference time, the verifier is used for multi-turn BoN (3.4) for efficient search. The learned verifier has two distinct advantages over process reward functions typically used in multi-turn RL: (1) It is conditioned only on the initial prompt and the current solution, and is not dependent on previous states (2) It is trained via supervised learning on oracle reward labels. We explore two different losses:\nBinary Cross-Entropy loss (BCE): The nominal way to train the verifier is to directly predict the oracle reward\n$L_{B C E}(\\phi)=-E_{(x, y, r) \\sim D}\\left[r \\log R_{\\phi}(x, y)-(1-r) \\log R_{\\phi}(x, y)\\right]$\n(2)\nBradley Terry Model (BT): Since the goal of the verifier is to relatively rank code solutions rather than predict absolute reward, we create a preference dataset and then train with a Bradley Terry loss (Ouyang et al., 2022). For every prompt x, we create pairs of correct y+ (where r = 1) and incorrect y (where r = 0) code and define the following loss:\n$L_{B T}(\\phi)=-E_{\\left(x, y^{+}, y^{-}\\right) \\sim D}\\left[\\log \\sigma\\left(R_{\\phi}\\left(x, y^{+}\\right)-R_{\\phi}\\left(x, y^{-}\\right)\\right)\\right].$\n(3)\nwhere \u03c3(.) is the sigmoid function. We hypothesize that BT is strictly easier to optimize as the verifier has to only focus on relative performance. This is also consistent with observations made for training process reward models, where the advantage function is easier to optimize than the absolute Q function (Setlur et al., 2024)."}, {"title": "3.3. Training Generator", "content": "\u00b5CODE comprises a generator \u03c0\u03b8 trained to produce code solutions conditioned on the initial problem and execution observations from previous turns. Given a dataset D, \u00b5CODE iteratively trains the generator to find the optimal code solution labeled using the local expert over the learned verifier. For this step, \u00b5CODE extracts all code solutions from D for every problem x. An expert is then created by picking the best solution, y*, which achieves the highest score using with the learned verifier R\u2084(x, y) and is given by\n$y^{*}=\\pi^{*}(x)=\\arg \\max _{y \\in D(x)} R_{\\phi}(x, y).$\n(4)\nUsing this expert dataset, we relabel the dataset D with the optimal solutions for each prompt:\n$D^{*}=\\{(x, s_{t}, y^{*}) \\mid (x, s_{t}) \\sim D\\},$\n(5)\nwhere D represents the expert dataset. The generator \u03c0\u03b8 is then trained via fine-tuning (FT) on this expert dataset D*."}, {"title": "3.4. Inference: Multi-turn Best-of-N", "content": "At inference time, the goal is to generate a code solution with a fixed inference budget - denoting the number of times generators can provide one complete solution. In this work, we propose to leverage the learned verifier to improve search and code generations over successive turns with multi-turn Best-of-N (BoN). To achieve this, \u00b5CODE uses a natural extension of BoN to the multi-turn setting. At each turn, the generator produces N one-step rollouts {y}n=1 ~ \u03c0\u03bf(.st) and the learned verifier picks the most promising code solution among these candidates using\n$y_{t}^{*}=\\arg \\max _{n} R_{\\phi}\\left(x, y_{t}^{n}\\right).$\n(6)"}, {"title": "3.5. Analysis", "content": "\u00b5CODE effectively treats multi-turn code generation as an interactive imitation learning problem by collecting roll-outs from a learned policy and re-labeling them with an expert. It circumvents the exploration burden of generic reinforcement learning which has exponentially higher sample complexity (Sun et al., 2017). We briefly analyze why this problem is amenable to imitation learning and prove performance bounds for \u00b5CODE.\nDefinition 3.1 (One-Step Recoverable MDP). A MDP M = (S, A, P, R, \u03b3) with horizon T is one-step recoverable if the advantage function of the optimal policy \u03c0*, defined as A* (s, a) = Q*(s, a)\u2212V* (s), is uniformly bounded for all (s, a), i.e. A*(s, a) \u2264 1.\nCode generation is one-step recoverable MDP. Multi-turn code generation satisfies one-step recoverability because the optimal policy \u03c0*(yt|st) depends only on the problem prompt x and not the interaction history st = (X, Y1, 01,..., Yt-1, Ot\u22121). Since the correctness of a code snippet yt is fully determined by x, the optimal Q-function satisfies Q*(st, Yt) = R(x,yt), where R(x, yt) \u2208 {0,1}. The optimal value function is V*(st) = maxy\u0142 R(x, yt), so the advantage function simplifies to A*(St, Yt) = R(x, yt) - maxy, R(x, y) \u2264 1.\nCode generation enables efficient imitation learning. There are two challenges to applying interactive imitation learning (Ross et al., 2011; Ross & Bagnell, 2014) \u2013 (1) Existence of expert policies or value functions, and (2) Recoverability of expert from arbitrary states. First, for code generation, the expert is simply the one-step reward maximizer arg maxy R(x, y). We can efficiently estimate R(x, y) to compute the expert, without needing to compute value function backups. Second, even if the learner fails to imitate the expert at any given state, the expert can perfectly recover from the next state. This results in the best possible performance bounds for imitation learning, which we formalize below.\nTheorem 3.2 (Performance bound for \u00b5CODE). For a one-step recoverable MDP M with horizon T, running N iterations of \u00b5CODE yields at least one policy \u3160 such that\n$J(\\pi^*)-J(\\pi)<O(T(\\epsilon+\\gamma(N)))$\n(7)\nwhere \u03c0* is the expert policy, e is the realizability error, and \u03b3(N) is the average regret.\nProof is in Appendix A.1. The bound O(ET) is much better than the worst-case scenario of O(\u20acT2) for unrecoverable MDPs (Swamy et al., 2021). Thus, \u00b5CODE exploits the structure of multi-turn code generation to enable imitation learning, bypassing the need for hierarchical credit assignment. More generally, this analysis suggests that for any task where the optimal action is history-independent and recoverable in one step, reinforcement learning can be reduced to efficient imitation learning without loss of performance."}, {"title": "4. Experiments", "content": "Through our experiments, we aim to analyze (1) How does \u00b5CODE compare to other state-of-the-art methods? (2) Does the learned verifier help during training and inference-time? (3) Which loss function works better for learning a verifier?\n4.1. Setup\nModels. The generator model in \u00b5CODE is initialized with Llama-3.2-1B-Instruct or Llama-3.1-8B-Instruct (Dubey et al., 2024). The learned verifiers are initialized with the same models as generators and have a randomly initialized linear layer to predict a scalar score (Stiennon et al., 2020).\nDatasets. We conduct experiments on MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021) where the agent needs to generate code solutions in Python given natural language descriptions. We train the methods on the MBPP training set which comprises 374 problems and evaluate on the MBPP test set and HumanEval (HE) dataset which have 500 and 164 problems. We further describe the prompts and the split of public and private tests in Appendix A.3 and A.4.\nBaselines. We compare \u00b5CODE with single and multi-turn baselines. For the single and multi-turn settings, we report metrics by generating solutions from Llama models which we denote as Instruct. We also compare with STaR (Zelikman et al., 2022) where the correct solutions of the Instruct model are used for fine-tuning (FT). We also compare to a multi-turn version of STaR, called Multi-STaR. Here, we collect multi-turn rollouts using the Instruct model and use trajectories terminating in a correct code solution for FT. For multi-turn BoN search, we collect the solutions that pass public tests, and then we select the best one judged by a learned verifier. Note that this verifier is specifically trained for each generator.\nMetrics. We measure the performance with the BoN accuracy, which quantifies the accuracy of the solution chosen by a verifier from N candidate solutions. The generator is allowed T = 3 turns and the final turn is used for evaluation over private tests. At each turn, the verifier ranks N = 5 solutions (unless stated otherwise) provided by the generator. For the BoN performance, we sample with a temperature of 0.7. We also report the accuracy of generating correct solutions via greedy decoding."}, {"title": "4.2. Results", "content": "In Table 1, we compare the proposed algorithm \u00b5CODE with the baselines. Here, we first evaluate the generators using code generated via greedy sampling for each problem (N = 1). This measures the accuracy of generating a correct solution with a turn limit of T = 3. We observe that multi-turn methods (both Instruct and Multi-STaR) perform better than their single-turn variants showing the importance of incorporating execution feedback. Our approach \u00b5CODE outperforms Multi-STaR across both benchmarks with 1B-sized model demonstrating the efficacy of training generators with data obtained with a learned verifier. To highlight, our method \u03bcCODE with a 1B parameter model achieves 1.9% performance gains compared to Multi-STaR on the HumanEval dataset. With an 8B-sized model, \u00b5CODE outperforms baselines on MBPP whereas there is a drop when compared on HumanEval.\nWe further evaluate the effect of having a verifier for BoN search during inference, where a learned verifier selects the most promising candidate solution at each turn. In Table 1, we observe that all algorithms can benefit with BoN search. Remarkably, \u00b5CODE attains a performance gain of up to 12.8% with the multi-turn BoN approach compared to greedy. Moreover, \u00b5CODE outperforms leading baselines with BoN search on MBPP and HumanEval datasets by 2.8% and 2.1% with 1B sized-model and performs comparably on 8B-sized model."}, {"title": "4.3. Analysis", "content": "To understand the improvements, we conduct a component-wise ablation study where we 1) check the effect of oracle and learned verifiers for relabeling to train the generator (4.3.1), 2) evaluate different generators trained with and without learned verifiers (4.3.2), 3) check which verifier performs better multi-turn BoN search at test-time (4.3.3), 4) assess scaling behaviors at inference time with number of candidate generations (N) at each turn (4.3.4), and 5) study the benefits of learned verifiers during evaluation (4.3.5).\n4.3.1. VERIFIER FOR RELABELING\nWe compare different verifiers for relabeling data for training the generator. In contrast to \u00b5CODE where the learned verifier is used to relabel (LV), we compare with relabeling using a correct solution (attains an oracle reward R = 1) for the corresponding prompt (OV). We also compare with a variant where the generator is fine-tuned over combinations of data relabeled with both the oracle verifier and the learned verifier (OV+LV). Here, we concatenate the FT dataset obtained using both LV and OV. In Figure 2, we present results with the 1B-sized models across benchmarks and observe that having corrections with the oracle verifier outcome does not perform well. However, relabeling with both verifiers OV+LV outperforms the OV variant, further demonstrating that gains in the generator learned by \u00b5CODE are coming from relabeling with a learned verifier. Lastly, the LV variant performed best on MBPP and comparably on HumanEval dataset when compared with LV+OV.\n4.3.2. VARYING THE GENERATOR\nIn this section, we compare the multi-turn agents where the generator is trained with an oracle verifier (Multi-STaR) or a learned verifier (\u00b5CODE). We evaluate the ability of the trained generator to utilize execution feedback and improve the code response across turns. We report the BoN accuracy till a turn t, which denotes the BoN accuracy of obtaining a correct solution till turn t. In Figure 3, we present the results with 1B-sized models. We observe that BoN accuracy improves with turns for \u00b5CODE whereas the baseline agents show marginal improvements with successive turns. We believe that using a learned verifier for relabeling improves the generator's ability to generate solutions with high reward values, and indeed recover better at every turn by utilizing the execution feedback.\n4.3.3. VERIFIER AT TEST-TIME\nIn our experiments with multi-turn BoN (Table 2), we pick the best solution based on the outcome of public tests and the scores of the learned verifier. In this experiment, we study different verifiers for ranking the candidate solutions at each turn. We test with Random strategy where the policy randomly picks from the N solutions at each step. We compare to the public tests (PT) outcome that picks any solution that passes the public test. Note that this involves evaluating all generated solutions at every turn with public tests. We also compare to selecting a solution based on scores obtained via the learned verifier only (LV). This is crucial as in certain applications such privileged information like public tests are not available and the agents can benefit from learned verifiers during inference. Lastly, we compare with the combination of public tests and leveraging the scores of the learned verifier to break ties at each turn (PT+LV).\nIn Table 2, we compare Base, Multi-STaR and \u00b5CODE with different verifiers at test-time. We observe that LV outperforms Random strategy which shows that a learned verifier indeed aids in selecting better solutions among generations. Given the benefits of learned verifiers for multi-turn BoN search, they can be a good alternative when public tests are not available. Furthermore, using the outcome of public tests (PT) performed better than using learned verifiers (LV) except on the HumanEval datset for 8B-sized model. We believe that this gap can be further reduced by learning more powerful verifiers and leave it for future research. Interestingly, the hierarchical approach (PT+LV) that uses the learned verifier to break ties on the outcomes of public tests performed best across methods and datasets. We hypothesize that using learned verifiers is beneficial in two scenarios. Firstly, if multiple solutions pass the public tests, then the learned verifier can filter out incorrect solutions which may not pass private tests. Secondly, if all candidate solutions are incorrect, then the learned verifier should choose the most promising solution at each turn. This is crucial as picking a better solution with the learned verifier can lead to more relevant feedback for recovering the true solution.\n4.3.4. TEST-TIME SCALING\nIn this section, we further study the importance of training the verifier with on-policy rollouts from the generator. We present a comparison of a verifier trained with samples from the Llama-3.2-1B-Instruct model (Base Verifier) and a verifier learned with samples from the generator of \u00b5CODE. Note that we use the generator of \u00b5CODE to obtain candidate solutions at each turn during evaluation. In Figure 4, we also present the scaling behaviors of different learned verifiers. We observe that using a verifier trained with on-policy samples obtained via the generator of \u00b5CODE performs better and showed significant improvements of up to 4.3% for different values of candidate solutions N.\n4.3.5. LosS FUNCTION FOR VERIFIER\nAs described in 3.2, we compare against different loss functions for training the verifier. For this experiment, we first generate multiple single step rollouts and label them via oracle verifier. Given oracle labels, we train verifiers with two loss functions \u2013 BCE and BT. During inference, the learned verifier picks the best ranked solution among the N solutions provided by the generator. Similar to (Cobbe et al., 2021), we report the BoN plot with different values of N obtained by first sampling N candidate solutions, choosing the top-ranked solution using the learned verifier, and then evaluating the solution against public and private tests. We calculate this metric over multiple samples for each value of N. In Figure 5, we observe that the verifier trained with BT loss consistently outperforms the verifier trained on BCE loss on both MBPP and HumanEval."}, {"title": "5. Related Work", "content": "Prompting To Solve Multi Step Tasks A common framework for tackling multi-step tasks with LLMs is prompting-based agentic systems. Self-Debugging (Chen et al., 2023b) asks the LLM to iteratively improve code by providing execution feedback while CodeT (Chen et al., 2022) asks the LLM to generate test cases. AlphaCodium (Ridnik et al., 2024) first reflects on input instructions, generates and filters from multiple code generations, and finally iterates on public and self-generated test cases. MapCoder (Islam et al., 2024) incorporates four agents to generate example problems, plans and code, and then perform debugging. However, prompting-based agents yield limited improvements.\nTraining LLMs for Multi Step Tasks Some work has explored explicitly training critics or reward models for multi-step reasoning tasks. In the coding domain, CodeRL (Le et al., 2022) trains a token-level critic to aid in code generation and to perform inference-time search. CodeRL's mechanics are similar to our method, but their generator is not trained for multi-step: CodeRL trains a \u201ccode repairer\" which conditions on one erroneous code completion while our generator incorporates multiple. ARCHER (Zhou et al., 2024), which frames multi-step tasks via a two-level hierarchical MDP, where the higher level MDP considers completions as actions and the lower level MDP considers tokens as actions. Another line of work utilizes Monte Carlo Tree Search (MCTS) methods for training: rStar-Math (Guan et al., 2025) trains a policy preference model to boost small LMs' math abilities to match or exceed large reasoning-based LMs and ReST-MCTS (Zhang et al., 2024) trains a process reward model (PRM) similarly to Math-Shepherd (Wang et al., 2024a). Although \u00b5CODE'S BoN search resembles a tree search, our key insight that multi-step code generation resembles a one-step recoverable MDP allows us to collect training trajectories much more efficiently. Finally, some work has explored using verifiers only during inference time. In \"Let's Verify Step by Step\" (Lightman et al., 2023), the authors demonstrate that PRMs trained on erroneous math solutions annotated by humans outperform outcome reward models for filtering multiple inference time generations. Meanwhile, AlphaCode (Li et al., 2022) trains a test generator to evaluate multiple code solutions.\nOther works omit learning a critic or reward model altogether. In the coding domain, RLEF (Gehring et al., 2024b) derives rewards only on the executor's result on test cases and syntax checkers, and PPOCoder (Shojaee et al., 2023) additionally considers semantic and syntactic alignment, generated via data flow graphs and abstract syntax trees respectively, with a reference solution. The \"oracle\" rewards in these methods may not be informative for training, and in the case of PPOCoder, require complex constructs. We empirically show that having a reward model is beneficial by comparing \u00b5CODE against the Multi-STaR baseline. Meanwhile, SCoRe (Kumar et al., 2024a) splits training into a \"generator\" and \"correction\" phase, thus restricting the total number of turns to 2. RISE (Qu et al., 2024) generates recovery steps via a more powerful LLM or by selecting a sampled completion via the oracle rewards. Both methods are less efficient than \u00b5CODE, which doesn't require generating corrections beyond generating training trajectories. Finally, FireAct (Chen et al., 2023a) and LEAP (Choudhury & Sodhi, 2024) FT ReAct style agents while RL4VLM (Zhai et al., 2024) and GLAM (Carta et al., 2024) studies training LLMs with interactive environment feedback."}, {"title": "6. Conclusion", "content": "We present \u00b5CODE, a simple and scalable method for multi-turn code generation through single-step rewards. \u00b5CODE models code generation as a one-step recoverable MDP and learns to iteratively improve code with a learned verifier to guide the search. Experimental results demonstrate that \u00b5CODE outperforms methods using oracle verifiers by a large margin. We acknowledge the following limitations of this paper. Due to a limited budget, we were only able to train models with up to eight-billion parameters. It is possible that the conclusions made in this paper do not generalize to models of larger scales. Additionally, we train models on MBPP, whose training set has only 374 examples. However, we hypothesize that more training examples will lead to better performance. Finally, our datasets are only in Python, and our findings might not generalize to other programming languages.\nImpact Statement\nThe proposed method for training code agents has the potential to streamline software development processes by automating routine coding tasks, thereby reducing human labor and accelerating production timelines. However, these advances will also introduce bugs, which can propagate at scale if no proper quality control is in place."}, {"title": "A.2. Hyperparameters", "content": "Table 3 contains hyperparameters for training the generator and reward model on both models (Llama-3.1-8B-Instruct and Llama-3.2-1B-Instruct) and datasets (MBPP and HumanEval). We perform 2 iterations of training with \u00b5CODE, starting from the base model each iteration. All training runs were on machines with either 4 RTX 6000 Ada Generation GPUs for 1B models with 48 GB of memory per GPU or 4 H100 GPUs for 8B models with 80 GB of memory per GPU.\nA.2.2. INFERENCE PARAMETERS\nWe use SGLang (Zheng et al., 2024) to serve our models for inference. Greedy experiments use temperature 0 with flags -disable-radix-cache \u2013max-running-request 1 to ensure deterministic results while BoN search experiments use a temperature of 0.7. All experiments are capped to 1000 tokens per completion per turn."}, {"title": "A.3. Prompts", "content": "A.3.1. SINGLE STEP PROMPT\nImmediately below is the prompt template to generate 1 code completion in a single-step method or to generate the 1st step in a multi-step method. Below the prompt templates are examples of the code prompt and public tests for HumanEval and MBPP."}, {"title": "A.3.2. FEEDBACK PROMPT", "content": "Immediately below is the prompt template for how we provide feedback in multi-step methods. The feedback only consists of executor error traces, and we provide an example from HumanEval."}, {"title": "A.4. Public Private Tests", "content": "We choose a public-private test split for HumanEval and MBPP to ensure that naively passing the public tests does not guarantee private test success. For HumanEval, we use a single test from the code prompt's docstring as the public test and the remaining tests along with the official test suite as private tests. For ease of parsing, we utilize a processed version of HumanEval, HumanEvalPack (Muennighoff et al., 2023). For MBPP, we use a single test from the official test suite as the public test, and the remaining tests and any \"challenge test list\" tests as private tests."}]}