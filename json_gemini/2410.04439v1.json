{"title": "Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training", "authors": ["Wenbo Li", "Guohao Li", "Zhibin Lan", "Xue Xu", "Wanru Zhuang", "Jiachen Liu", "Xinyan Xiao", "Jinsong Su"], "abstract": "Diffusion-based text-to-image models have demonstrated impressive achievements in diversity and aesthetics but struggle to generate images with legible visual texts. Existing backbone models have limitations such as misspelling, failing to generate texts, and lack of support for Chinese text, but their development shows promising potential. In this paper, we propose a series of methods, aiming to empower backbone models to generate visual texts in English and Chinese. We first conduct a preliminary study revealing that Byte Pair Encoding (BPE) tokenization and the insufficient learning of cross-attention modules restrict the performance of the backbone models. Based on these observations, we make the following improvements: (1) We design a mixed granularity input strategy to provide more suitable text representations; (2) We propose to augment the conventional training objective with three glyph-aware training losses, which enhance the learning of cross-attention modules and encourage the model to focus on visual texts. Through experiments, we demonstrate that our methods can effectively empower backbone models to generate semantic relevant, aesthetically appealing, and accurate visual text images, while maintaining their fundamental image generation quality.", "sections": [{"title": "1 Introduction", "content": "Recently, diffusion-based models (Ho et al., 2020; Rombach et al., 2022; Saharia et al., 2022; Balaji et al., 2022; Zhang et al., 2023; Sauer et al., 2023) have revolutionized the field of text-to-image generation, particularly in terms of diversity and aesthetics. Among various text-to-image tasks, visual text generation has attracted much attention due to the growing demand for generating images containing visual texts in the AI art community and commercial fields. Despite their attractiveness, this task remains challenging, as most current diffusion models struggle to produce images with precise, readable visual texts. At present, dominant studies on this task can be roughly divided into two categories. Some researchers focus on adding additional conditions to reduce the difficulty of generating images with visual texts, resulting in restricted diversity and visual texts not coherent with the background. (Chen et al., 2023b; Ma et al., 2023; Yang et al., 2023; Tuo et al., 2023; Chen et al., 2023a; Zhao and Lian, 2023). Other researchers directly explore the performance of backbone models on visual text generation, which avoids the limitations of the previous type of methods but suffers from challenges such as misspelling, ignoring, and repeating words. To deal with these issues, early studies (Saharia et al., 2022; Balaji et al., 2022; Liu et al., 2023b) explore various text encoders to address misspelling issues. Recent commercial models such as Dall-E 3 (Betker et al., 2023) and Stable Diffusion 3 (Esser et al., 2024) demonstrate"}, {"title": "2 Related Work", "content": "remarkable performance, further validating the potential of this research direction. However, they lack support for other languages, such as Chinese. To explore potential avenues for improvements, we first conduct several preliminary experiments and observe that the visual text generation performance of backbone models are mainly constrained for two reasons: First, BPE tokenization requires the model to combine subwords to form complete visual words, increasing the difficulty of generating visual texts. Second, The model is unable to effectively bind visual texts to the corresponding text tokens due to the insufficient learning of the cross-attention modules.\nBased on these analyses, we propose a series of methods that significantly improve the visual text generation capability of backbone models, as shown in Figure 1. Specifically, we first introduce a mixed granularity input strategy that provides more suitable text representations. Then, we augment the conventional MSE loss with three glyph-aware losses: (1) attention alignment loss refines the cross-attention maps, thereby better binding visual texts to their corresponding text tokens; (2) local MSE loss highlights the importance of visual text areas; (3) OCR recognition loss encourages the model to generate accurate visual texts.\nFigure 2 demonstrates that our methods effectively enhance the backbone model's visual text generation ability while maintaining its fundamental capabilities. Particularly, our methods can be transferred to the generation of Chinese texts."}, {"title": "2.1 Visual Text Generation", "content": "Recent studies on visual text generation primarily focus on introducing additional conditions, such as rendered text images, or position coordinates during inference.\nSome works concatenate representations of the rendered text image with the latent variable as the model input. For example, TextDiffuser (Chen et al., 2023b) and GlyphDraw (Ma et al., 2023) concatenate the representation of position-aware mask with the latent variable, and utilize pre-trained models to generate positional information. UDiffText (Zhao and Lian, 2023) utilizes an inpainting model that considers concatenation of the position mask, the masked image, and the original image as input. Instead of introducing additional conditions through concatenation, some works also explore to utilize auxiliary modules. GlyphControl (Yang et al., 2023) use a ControlNet, which receives images with rendered texts as input. Building upon this, AnyText (Tuo et al., 2023) introduces a fusion network that receives position and image masks to support more flexible position control and image editing. Apart from these, several works add special tokens representing additional conditions. For example, TextDiffuser-2 (Chen et al., 2023a) adds additional position tokens into the text encoder to generate text based on the predicted coordinates.\nHowever, the above studies still suffer from the following limitations: (1) The use of these conditions constrains the overall composition of the image, causing issues of restricted diversity and visual texts not coherent with backgrounds; (2) Users are required to provide additional conditions, leading to inconvenience in usage."}, {"title": "2.2 Text-to-Image Backbone Models", "content": "Some researchers focus on enhancing the overall capabilities of text-to-image backbone models. Early works in this regard aim at addressing spelling errors by experimenting with various text encoders. For example, Imagen (Saharia et al., 2022) replaces CLIP (Radford et al., 2021) with T5 (Raffel et al., 2019), eDiff-I (Balaji et al., 2022) uses both CLIP and T5.\nAdditionally, some researchers find that tokenization methods influence the model's ability to generate visual texts. Liu et al. (2023b) believe that the primary reason for spelling errors lies in the lack of character-level glyph information caused by BPE tokenization, and propose to solve this by adopting the character-level text encoder ByT5 (Xue et al., 2021).\nRecently, some commercial models, such as Dall-E 3 (Betker et al., 2023) and Stable Diffusion 3 (Esser et al., 2024) show outstanding performance in visual text generation. This demonstrates that with the development of backbone models, the performance of visual text generation is concurrently improving. However, these commercial models only support English, leaving the generation of visual texts in other languages unsolved.\nIn this work, we propose a series of methods, which empower the backbone models with the ability to generate accurate and aesthetic visual texts in two aspects. First, we propose a mixed granularity input strategy to provide more suitable text representations. Second, we augment the conventional training objective with three glyph-aware losses."}, {"title": "3 Preliminary Study", "content": "In this section, we first introduce the basic concepts of the diffusion based text-to-image backbone model, and then conduct experiments to identify potential avenues for improvements."}, {"title": "3.1 Diffusion Based Text-to-Image Backbone Models", "content": "The commonly-used architecture of text-to-image backbone models derives from the latent diffusion model (Rombach et al., 2022), which is composed of three modules: (1) a VAE (Kingma and Welling, 2014) consists of an encoder to compress images into the latent space, and a decoder to reverse them back; (2) a UNet (Ronneberger et al., 2015) denoiser e per- forms diffusion denoising process at latent space; (3) a text encoder Tencodes the text prompt into representation c.\nThis process defines a Markov chain of forward diffusion process which continually applies the noise sampled from a Gaussian distribution to the real data zo = E(x0):\nDiffusion Process.\n$q(z_t|z_{t-1}) := N(z_t; \\sqrt{a_t}z_{t-1}, (1 - a_t)I),$ (1)\nwhere at is a time-aware schedule. As t increases, zt asymptotically approaches the noise in a standard Gaussian distribution.\nThe UNet denoiser e\u0189 is trained to predict the noise et added to the image at timestep t, thereby reversing the Markov chain. A mean squared error (MSE) loss is utilized to supervise the training:\n$L_{mse} = E_{z_0, c, e_t, t} [||E_{\\theta}(z_0, t, c) - E_t||^2].$ (2)\nTo add conditional guidance, the representation c is fed into each cross-attention block of the UNet model as:\n$Attn(z_t, c) = Softmax(-\\frac{Q(z_t) K(c)^T}{\\sqrt{d}})V(c),$ (3)\nwhere Q, K and V denote the query, key and value projections, and d denotes the output dimension."}, {"title": "3.2 Experimental Analyses", "content": "To identify avenues for improvements, we use the commonly-used backbone model\u2013SD-XL (Podell et al., 2023) to conduct two groups of experiments.\nIn the first group of experiments, we investigate the effect of BPE tokenization on two subsets: (1) S1, where words are split into subwords by BPE tokenization, and (2) S2, consisting of words that remains the same after BPE tokenization. To eliminate the impact of word frequency and length, we select 100 words for each subset from 5,000 common words with lengths ranging from 5 to 8 letters\u00b9.\nResults show that the model achieves an accuracy of 0.3 in S\u2081, compared to 0.46 in S2, indicating that BPE tokenization increases the difficulty for the model in generating visual texts, as it splits a word into subwords and requires the model to combine them into a complete visual word.\nAs stated in previous works (Hertz et al., 2023; Chefer et al., 2023), the cross-attention maps of the UNet can reflect the relevance between generated objects and corresponding text tokens. Similarly, visual texts can also be treated as objects, and texts to be generated, which we refer to as glyph texts,"}, {"title": "4 Methods", "content": "Based on the observations from our preliminary study, we propose a series of methods to improve the visual text generation capability of backbone models. As shown in Figure 4, our improvements mainly involve two aspects: (1) we introduce a mixed granularity input strategy to replace the BPE"}, {"title": "4.1 Mixed Granularity Input", "content": "Our preliminary study reveals that BPE tokenization constrains the performance of the model, highlighting the necessity to represent glyph texts in a more suitable granularity. In this regard, previous studies (Liu et al., 2023b; Chen et al., 2023a; Zhao and Lian, 2023) commonly utilize character-level tokenization, which splits words into characters. However, as stated in our preliminary study, this split challenges the model to combine characters into a complete visual word. To deal with this issue, we consider each glyph word as a whole within the model, as shown in Figure 5. Given the impracticality of including every word in the vocabulary, a method is needed to get the embedding for every word. Therefore, we extract intermediate features from the OCR model as new text embeddings following Tuo et al. (2023), which inherently possess sufficient glyph information. Specifically, for a user prompt y containing N glyph words 91, 92,..., 9N, we render each glyph word into an image without providing positional information, resulting in an image sequence Ig. Then, we feed them into the OCR model \u03b3, where the text embedding c is refined as follows:\n$c = T(\\phi(y), \\xi(\\gamma(I_g))),$ (4)\nwhere T is the CLIP text encoder, & is the BPE tokenizer, and \u03be is a linear module."}, {"title": "4.2 Glyph-Aware Training", "content": "Formally, the overall training objective can be formulated as:\n$L = L_{mse} + \\lambda_1 \\cdot L_{attn} + \\lambda_2 \\cdot L_{loc}$ (5)\n$+ (1 - \\lambda_1 - \\lambda_2) \\cdot L_{ocr},$\nwhere Lattn, Lloc and Locr denote the attention alignment loss, the local MSE loss, and the OCR recognition loss, respectively."}, {"title": "4.2.1 Attention Alignment Loss Lattn", "content": "To enhance the learning of cross-attention modules, we introduce an attention alignment loss, which encourages the model to ensure that each visual text mainly attends to the corresponding glyph token. Specifically, the cross-attention map between the intermediate feature of the noisy latent variable zt and the refined representation cg of glyph tokens can be calculated as:\n$CA(z_t, c_g) = Softmax(\\frac{Q(z_t) K(c_g)^T}{\\sqrt{d}}).$ (6)\nTo encourage that each visual text has large values in the corresponding area, we minimize the distance between the cross-attention maps and the corresponding segmentation masks of visual texts, which is defined as follows:\n$L_{attn} = \\frac{1}{N} \\sum_{k=1}^N ||CA(z_t, c_g)^k - M_k||_2^2,$ (7)\nwhere M denotes the segmentation mask of the k-th visual text corresponding to its glyph token.\nThrough this training process, the model can effectively capture a more robust understanding of the relationships between the visual texts and glyph tokens, thus faithfully generating the desired visual texts."}, {"title": "4.2.2 Local MSE Loss Lloc", "content": "Since the MSE loss only measures pixel-wise distance and lacks additional focus on visual text areas, we apply a weighting strategy to the MSE loss following Ma et al. (2023), which we refer to as the local MSE loss. To mitigate the impact of visual text area size, we add a weighting term w which is the ratio of the image area to the visual text area. Formally, the local MSE loss can be formulated as:\n$L_{loc} = \\frac{1}{N} \\sum_{k=1}^N w_k \\cdot L_{loc}$ (8)\n$L_{loc} = E_{z_0, e_t, t} [M_k \\odot ||E_{\\theta}(z_0, t, c) - E_t||^2].$"}, {"title": "4.3 OCR Recognition Loss Locr", "content": "To further encourage the model to generate accurate visual texts, we introduce an OCR recognition task. At each training step, we can estimate the fully denoised image latent variable z\u00f3, as implemented in DDPM (Ho et al., 2020). We then input this latent variable into the VAE decoder to obtain an approximate image x, which is subsequently"}, {"title": "5 Experiments", "content": "fed into the OCR model for recognition. As implemented in the training of the OCR model, we use the CTC loss (Graves et al., 2006) to refine the predicted results. Since this estimation introduces more distortion as t increases, we add a weighting term related to t, which is set as \u0101t following Tuo et al. (2023). The OCR recognition loss can be formulated as:\n$L_{ocr} = \\frac{1}{N} \\sum_{k=1}^N \\bar{a}_t \\cdot L_{CTC}(x) M_k, g_k),$ (9)\nwhere CTC(\u00b7) denotes the CTC loss function."}, {"title": "5.1 Dataset", "content": "To better unleash the potential of the model, we require a large-scale, high-quality dataset that satisfies the following criteria:\n\u2022 The dataset should contain images with clear and recognizable visual texts.\n\u2022 The visual texts in the images should occupy a prominent area and be coherent with the background.\n\u2022 The captions should include detailed descriptions of the visual texts.\n\u2022 The aesthetic quality of the images should be comparable to those used for pre-training.\nFollowing the aforementioned criteria, we construct an English dataset consisting of 240K samples by filtering internal datasets, and a Chinese dataset containing 50K synthetic samples using image-to-image models and rendering tools2. More details are introduced in Appendix A."}, {"title": "5.2 Experimental Setting", "content": "We quantify the visual text generation quality from two aspects: (1) CLIP score (Hessel et al., 2021) measures the semantic relevance between the generated image and the input prompt by calculating the cosine similarity of their representations from CLIP image and text model (Radford et al., 2021). (2) OCR Accuracy detects the texts in the generated images utilizing OCR tools. We calculate the precision, recall and F1 score between the detected texts and the ground truths. Furthermore, we evaluate the model 's fundamental capability through FID (Heusel et al., 2017) score, which compares the distribution of generated images with that of real images. Note that we are unable to calculate the FID score in our main experiments, due to the lack of source images.\nImplementation Details. We train our models based on SDXL-base-1.0 and SDXL-Turbo. We utilize the PaddleOCR v4 model\u00b3 to extract intermediate features, perform the OCR recognition task, and conduct evaluation. We set \u5165\u2081 and A2, to 0.4, 0.2, respectively, determined by a grid search on the validation set, which are varied from 0.1 to 1.0 with an interval of 0.1. We set the learning rate to 2e-5 and conduct a total of 10K steps of training. The overall training process takes 7 hours and 50 minutes on 8 A800 GPUs."}, {"title": "5.3 Quantitative Results", "content": "As shown in Table 1, we conduct quantitative comparison with existing backbone models on the ChineseDrawText benchmark (Ma et al., 2023). We compare our models with DeepFloyd (DeepFloyd- Lab, 2023), SD-XL, SDXL-Turbo (Sauer et al., 2023), LCM-LORA (Luo et al., 2023), and SD- Cascade (Pernias et al., 2023). Results show that our models outperform other baseline models under the majority of metrics."}, {"title": "5.4 User Study", "content": "To further validate the effectiveness of our proposed methods, we conduct a human evaluation comparing our English models with other baseline models on Chinese DrawText benchmark. Three raters are asked to compare these images from four dimensions including text aesthetics, text accuracy, semantic relevance, and image aesthetics, and then select the images they prefer. Throughout the process, all raters are unaware of which model the image is generated from. The results in Table 1 show that human raters greatly prefer our models on all aspects, which further validates the effectiveness of our approaches in generating high-quality and visual text images. The detailed participant instruction are listed in Appendix B."}, {"title": "5.5 Qualitative Results", "content": "To provide more straightforward comparison, we provide some visualization samples from the test set in Figure 6. We can clearly observe that our models better capture the semantic relevance, thus"}, {"title": "5.6 Comparison of Generating Images without Visual Texts", "content": "In order to evaluate the fundamental image generation performance of our models, we use FID to quantify the image quality without visual texts on 5K samples from COCO2017 (Lin et al., 2014), as shown in Table 2. Furthermore, we visualize some image generation examples in Figure 8. The quantitative and qualitative results indicate that our models maintain the fundamental capability to generate visual appealing and semantic relevant images."}, {"title": "5.7 Ablation Study", "content": "To investigate the effectiveness of each design, we further compare our SD-XL model with the following variants in Table 3:\n(1) \u21d2Char+BPE tokenization, and\u21d2BPE tokenization. In the first variant, we replace our mixed granularity input with the mixture of character-level and BPE tokenization. In the second variant, we only utilize BPE tokenization. As shown in line 2, our mixed granularity input strategy outperforms the mixture of character-level and BPE tokenization. We hypothesize that this is because the model struggles to combine the glyphs of characters to form a complete visual word. The result in line 3 shows that the mixture of character-level and BPE tokenization achieves better results comparing to BPE tokenization, which demonstrate the effectiveness of providing character-level glyph information."}, {"title": "5.8 Chinese Text Generation", "content": "We further explore the effectiveness of our methods to generate Chinese visual texts. Instead of considering words as whole units, we use the mixture of character-level and BPE tokenization for Chinese texts due to two reasons. First, Chinese glyphs are excessively complex, resulting in the intermediate features of each character being too similar in distribution to be effectively distinguished. Second, fewer characters are included in each Chinese word, thus is easier to be combined into a complete visual word.\nNote that due to the lack of open source Chinese backbone text-to-image models for comparison, we train both our models and the baseline models on our Chinese dataset for 10K steps. We choose SD- XL, SDXL-Turbo and SD Cascade, which achieve relatively better performance in English, as baseline models, and use the prompt templates from the Chinese DrawText benchmark with texts included in our Chinese dataset as test set. Quantitative results in Table 4 show that our models greatly outperform other baseline models. As for qualitative comparison, we visualize some samples from our test set, as shown in Figure 9. Our model generates accurate visual texts, while other baseline models fails to correctly generate Chinese texts, indicating that our methods enhance the learning of Chinese texts. We provide the ablation study for Chinese text generation in Appendix D."}, {"title": "6 Conclusion", "content": "In this paper, we conduct a preliminary study and find that BPE tokenization, as well as the model's insufficient learning of cross-attention modules, constrains the visual text generation performance of diffusion-based backbone models. Based on these insights, we propose a series of methods, aiming to empower the backbone model with the ability to generate accurate and aesthetically appealing visual text images, while maintaining fundamental image generation quality. Specifically, we introduce a mixed granularity input strategy to provide more suitable text representations. Besides, we augment the conventional training objective with three glyph-aware training losses, which enhance the learning of the cross-attention modules and encourage the model to focus on visual texts. Experiments demonstrate the effectiveness of our methods. Typically, our methods can be transferred to Chinese text generation.\nIn the future, we intend to explore visual text generation for more languages, and generate texts in different styles (Liu et al., 2023a). Besides, we also plan to explore utilizing glyph enhanced diffusion models for image-to-image translation (Lan et al., 2024)."}, {"title": "Limitations", "content": "While our methods enhance the visual text generation capability of the backbone models, several limitations still remain. First, our methods require to train the diffusion backbone model, which may be time consuming and expensive. Besides, our methods are unable to completely solve the issue of misspelling, ignoring and repeating words."}, {"title": "Ethics Statement", "content": "This research paper rigorously addresses the ethical considerations associated with text-to-image models, ensuring that all methods used in this study are conducted responsibly and ethically. Our models are trained using open-source backbone models. To address concerns related to training data, we implement a strict filtering process to exclude inappropriate content, such as NSFW images and offensive visual text. The evaluation experiments are conducted using widely recognized public benchmarks, and participants involved in the user studies are systematically trained."}]}