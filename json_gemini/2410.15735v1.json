{"title": "AutoTrain:\nNo-code training for state-of-the-art models", "authors": ["Abhishek Thakur"], "abstract": "With the advancements in open-source mod-\nels, training(or finetuning) models on custom\ndatasets has become a crucial part of devel-\noping solutions which are tailored to specific\nindustrial or open-source applications. Yet,\nthere is no single tool which simplifies the\nprocess of training across different types of\nmodalities or tasks. We introduce AutoTrain\n(aka AutoTrain Advanced) an open-source,\nno code tool/library which can be used to train\n(or finetune) models for different kinds of\ntasks such as: large language model (LLM)\nfinetuning, text classification/regression, to-\nken classification, sequence-to-sequence task,\nfinetuning of sentence transformers, visual\nlanguage model (VLM) finetuning, image clas-\nsification/regression and even classification\nand regression tasks on tabular data. Au-\ntoTrain Advanced is an open-source library\nproviding best practices for training models\non custom datasets. The library is available\nat https://github.com/huggingface/autotrain-\nadvanced. AutoTrain can be used in fully local\nmode or on cloud machines and works with\ntens of thousands of models shared on Hug-\nging Face Hub and their variations.", "sections": [{"title": "1 Introduction", "content": "With recent advancements in open-source and open-\naccess state-of-the-art models, the need for stan-\ndardized yet customizable training of models on\ndownstream tasks has become crucial. However, a\nuniversal open-source solution for a diverse range\nof tasks is still lacking. To address this challenge,\nwe introduce AutoTrain (also known as Auto-\nTrain Advanced).\nAutoTrain is an open-source solution which of-\nfers model training for different kinds of tasks such\nas: large language model (LLM) finetuning, text\nclassification/scoring, token classification, training\ncustom embedding models using sentence trans-\nformers (Reimers and Gurevych, 2019), finetuning\nfor visual language models (VLMs), computer vi-\nsion tasks such as image classification/scoring, ob-\nject detection and even tabular regression and clas-\nsification tasks. At the time of writing this paper, a\ntotal of 22 tasks: 16 text-based, 4 image-based and\n2 tabular based have been implemented.\nThe idea behind creating AutoTrain is to allow\na simple interface for training models on custom\ndatasets without requiring extensive knowledge\nof coding. AutoTrain is intended for not just no-\ncoders but also for experienced data scientists and\nmachine learning practioners. Instead of writing\ncomplex scripts, one can focus on gathering and\npreparing your data and let AutoTrain handle the\ntraining part. AutoTrain UI is shown in Figure 1.\nWhen talking about model training, there are\nseveral problems which arise:\nComplexity of hyperparameter tuning: Finding\nthe right parameters for tuning models can only be\ndone by significant experimentations and expertise.\nImproperly tuning the hyperparameters can result\nin overfitting or underfitting.\nModel validation: A good way to make sure the\ntrained models generalize well, is to have a proper\nvaliation set and a proper way to evaluate with\nappropriate metrics. Overfitting to training data can\ncause the models to fail in real-world scenarios.\nDistributed training: Training models on larger\ndatasets with multi-gpu support can be cumber-\nsome and requires significant changes to codebase.\nDistributed training requires additional complex-\nity when it comes to synchronization and data han-\ndling.\nMonitoring: While training a model, its crucial\nto monitor losses, metrics and artifacts to make\nsure there is nothing fishy going on.\nMaintenance: With ever-changing data, it may\nbe necessary to retrain or fine-tune the model on\nnew data while keeping the training settings con-\nsistent.\nWe introduce the open source AutoTrain Ad-"}, {"title": "2 Related work", "content": "In recent years, many AutoML solutions have been\ndeveloped to automate the training process of ma-\nchine learning models. Some notable solutions in-\nclude:\nAutoML Solutions AutoSklearn (Feurer et al.,\n2015), which is an open-source AutoML toolkit\nbuilt on top of the popular scikit-learn library. Au-\ntoSklearn uses Bayesian optimization to automate\nthe process of model selection and hyperparameter\ntuning.\nAutoCompete (Thakur and Krohn-Grimberghe,\n2015), which won the Codalab AutoML GPU chal-\nlenges, builds a framework for tacking machine\nlearning competitions. The code is, however, not\nopen source.\nAxolotl (Cloud, 2024) is a CLI tool for finetun-\ning LLMs.\nAutoKeras (Jin et al., 2023), developed on top of\nKeras offers functionalities for various tasks such\nas image classification, text classification, and re-\ngression\nMany other closed-source solutions have also\nbeen developed by Google, Microsoft, and Ama-\nzon. However, all these solutions have some limi-\ntations. They are either not open-source, or if they\nare, they can only handle a limited number of tasks.\nMany of these solutions are also not no-code, mak-\ning them inaccessible to non-coders.\nWith AutoTrain, we provide a single interface\nto deal with many different data format, task, and\nmodel combinations, which depending on user's\nchoices is also closely connnected to Hugging Face\nHub which enables download, inference and shar-\ning of models with the entire world. Moreover, Au-\ntoTrain supports almost all kinds models which are\ncompatible with Hugging Face Transformers (Wolf\net al., 2019) library, making it a unique solution to\nsupport hundreds of thousands of models for fine-\ntuning, including the models which require custom\ncode."}, {"title": "3 Library: AutoTrain Advanced", "content": "The AutoTrain Advanced python library provides a\ncommand line interface (CLI), a graphical user in-\nterface (GUI/UI) and python SDK to enable train-\ning on custom datasets. The datasets can be upload-\ned/used in different formats such as zip files, CSVS\nor JSONLs. We provide documentation and walk-\nthroughs on training models for different task and\ndataset combinations with example hyperparam-\neters, evaluation results and usage of the trained\nmodels. The library is licensed as Apache 2.0 li-\ncense and is available on Github, \u00b9 making it easy\nfor anyone to adopt and contribute."}, {"title": "3.1 Component of the AutoTrain Advanced\nlibrary", "content": "There are 3 main components in the AutoTrain\nAdvanced library:\nProject Configuration: manages the configura-\ntion of the project and allows users to set up and\nmanage their training projects. Here, one can spec-\nify various settings such as the type of task (e.g.,\nIlm finetuning, text classification, image classifi-\ncation), dataset, the model to use, and other train-\ning parameters. This step ensures that all necessary\nconfigurations are in place before starting the train-\ning process.\nDataset Processor: handles the preparation and\npreprocessing of datasets. It ensures that data is\nin the right format for training. This component\ncan handle different types of data, including text,\nimages, and tabular data. Dataset processor does\ncleaning and transformation of dataset, saves time\nand reduces the potential for errors. A dataset once\nprocessed can also be used for multiple projects\nwithout requiring to be processed again.\nTrainer: is responsible for the actual training\nprocess. It manages the training loop, handles the\ncomputation of loss and metrics, and optimizes\nthe model. The Trainer also supports distributed\ntraining, allowing you to train models on multiple\nGPUs seamlessly. Additionally, it includes tools\nfor monitoring the training progress, ensuring that\neverything is running smoothly and efficiently."}, {"title": "3.2 Installation & Usage", "content": "Using AutoTrain is as easy as pie. In this section\nwe focus briefly on installation and LLM finetuning\ntask. However, the same can be applied to other\ntasks keeping in mind the dataset format which is\nprovided\nInstallation AutoTrain Advanced can be easily\ninstalled using pip.\nIt has to be noted that the the pip installation\ndoesnt install pytorch and users must install it on\ntheir own. However, a complete package with all\nthe requirements is also available as a docker im-\nage.\nUsage AutoTrain Advanced offers CLI and UI.\nCLI is based on a AutoTrain Advanced python\nlibrary. So, users familiar with python can also use\nthe python sdk. To start the UI as shown in Figure 1,\none can run the autotrain app command:\nAn example of running training in UI is shown\nin Figure 2.\nTraining can also be started using a config file\nwhich is in yaml format and the autotrain cli. An ex-\nample config to finetune llama 3.1 is shown below:"}, {"title": "4 Conclusion", "content": "In this paper, we introduce AutoTrain (aka Auto-\nTrain Advanced), which is an open source, no-code\nsolution for training (or finetuning) machine learn-\ning models on a variety of tasks. AutoTrain ad-\ndresses common challenges in the model training\nprocess, such as dataset processing, hyperparam-\neter tuning, model validation, distributed training,\nmonitoring, and maintenance. By automating these\ntasks, AutoTrain ensures that users can efficiently\nbuild high-performing models without needing ex-\ntensive coding knowledge or experience. Addition-\nally, AutoTrain supports a diverse range of tasks,\nincluding llm finetuning, text classification, image\nclassification, and regression, and even tabular data\nclassification/regression, thus, making it a versatile\ntool for various applications."}, {"title": "Limitations", "content": "AutoTrain tries to generalize the training process\nfor a given model - dataset combination as much\nas possible, however, there might be situations in\nwhich custom changes might be required. For ex-\nample, AutoTrain doesnt provide support for sam-\nple weights, model merging, or ensembling yet."}]}