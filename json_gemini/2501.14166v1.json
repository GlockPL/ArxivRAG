{"title": "Enhancing Multimodal Entity Linking with Jaccard Distance-based Conditional Contrastive Learning and Contextual Visual Augmentation", "authors": ["Cong-Duy Nguyen", "Xiaobao Wu", "Thong Nguyen", "Shuai Zhao", "Khoi Le", "Viet-Anh Nguyen", "Feng Yichao", "Anh Tuan Luu"], "abstract": "Previous research on multimodal entity linking (MEL) has primarily employed contrastive learning as the primary objective. However, using the rest of the batch as negative samples without careful consideration, these studies risk leveraging \"easy\" features and potentially overlook essential details that make entities unique. In this work, we propose JD-CCL (Jaccard Distance-based Conditional Contrastive Learning), a novel approach designed to enhance the ability to match multimodal entity linking models. JD-CCL leverages meta-information to select negative samples with similar attributes, making the linking task more challenging and robust. Additionally, to address the limitations caused by the variations within the visual modality among mentions and entities, we introduce a novel method, CVaCPT (Contextual Visual-aid Controllable Patch Transform). It enhances visual representations by incorporating multi-view synthetic images and contextual textual representations to scale and shift patch representations. Experimental results on benchmark MEL datasets demonstrate the strong effectiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "Multimodal Entity Linking (MEL) (Song et al., 2023; Yang et al., 2023; Shi et al., 2024) aims to connect visual and textual references to the entities in a multimodal knowledge graph (Song et al., 2023; Yang et al., 2023; Shi et al., 2024). As exemplified in Figure 3, MEL connects the Spider-Man referecnes in the image and input sentence to the Spider-Man entity in the multimodal knowledge graph. This alleviates textual ambiguities by incorporating visual cues such as character portraits, which benefits various downstream applications, like information extraction (Yaghoobzadeh et al., 2017), question answering (Longpre et al., 2021), and search engines (Gerritse et al., 2022).\nHowever, current MEL methods are limited by their random negative sampling and understudy the potential issue of varied visual representation. First, these methods tend to employ contrastive learning to align input sentences and entities, but limited by its random negative sampling (Wang et al., 2022b; Luo et al., 2023). They generally randomly draw negative samples from the datasets. This may lead to unfair predictions due to numerous similar entities in the knowledge base, for example, different but highly similar \u201cSupreme Court\" entities in 1. Using them as negative samples hinders the connection between input sentences and correct entities. Second, existing MEL approaches overlooked the dissimilar visual modality between mention and target entity. For instance, 2 shows that the input image refers to an actor \u201cDoohan\", but the input sentence includes four distinct entities. As a result, this greatly disturbs linking the input image to the entities in the knowledge base. For further specificity, the \"Doohan\" image on the left illustrates the sentence, offering significant utility to the model for user queries related to \"Doohan\" within the knowledge base due to image-text similarity. However, it could potentially introduce interference if the user intends to query \"Wikipedia\". Hence, pre-linking control of the visual modality, contingent upon the mentioned target, emerges as a crucial consideration in the Multimodal Entity Linking (MEL) task, particularly when other modalities co-exist alongside textual input.\nTo address the limitation of random negative sampling, we present a Jaccard Distance-based conditional contrastive learning (JD-CCL) approach for MEL. It leverages meta-information to mitigate the influence of prominent attributes during contrastive pre-training. JD-CCL automatically utilizes the information provided by each entity in the knowledge base to sample hard negative entities with nearly identical attributes. This strategy makes it more challenging for the model to link the mention with the correct entity, as it cannot rely on simple attributes (such as human, building, or animal) but must distinguish positive pairs from negative ones based on more complex attributes. To identify similar entities, we define a pre-processing stage that sorts the Jaccard similarity scores between the meta-attributes between entities in knowledge base. This process brings more difficult and nuanced comparisons during training, which improves the linking ability by diminishing the impact of \"easy\" attributes in contrastive training.\nMoreover, to address the discrepancy challenge of visual modality in image-text pairs, we introduce the Contextual Visual-Aid Controllable Patch Transform (CVa-CPT) module. By leveraging synthetic images from a text-to-image diffusion, the model can better control essential features of the input image based on specific mention. Consequently, the model can selectively control useful patch representations from the input image rather than using all image patches with the same contribution. Additionally, for sentences containing multiple entities, each entity will have a unique input image, enhancing diverse visual representation according to the entity being queried.\nOur contributions can be summarized as follows:\n\u2022 We propose a novel Jaccard Distance-based Conditional Contrastive Learning (JD-CCL). This method leverages meta-attributes to draw conditional negative samples with similar features during training, enabling the model to identify exact characteristics for decision-making.\n\u2022 We introduce the Contextual Visual-Aid Controllable Patch Transform (CVa-CPT) module, which enhances visual modality representations with synthetic images and contextual information from the mention sentence.\n\u2022 We evaluate our approach on three benchmark datasets: WikiDiverse, RichpediaMEL, and WikiMEL, and show that our method outperforms previous state-of-the-art approaches."}, {"title": "2 Related Work", "content": "Multimodal Entity Linking In the landscape of social media and news, where content frequently encompasses both text and visuals, integrating textual and visual information for entity linking is essential and pragmatic. In an early pioneering effort, (Moon et al., 2018) utilized images to enhance entity linking in response to ambiguous and incomplete references in social media posts. Advancing this, (Wang et al., 2022a) investigated the inter-modal interactions through a cross-attention mechanism between text and vision, integrating a gated hierarchical architecture. To mitigate the impact of noisy and irrelevant images, (Zhang et al., 2021) assessed the relevance of images by correlating their category with the semantic content of text mentions, filtering images using a predefined threshold. (Gan et al., 2021) compiled a dataset featuring extensive movie reviews, associated entities, and images. More recently, research (Zheng et al., 2022) has included the use of scene graphs from images to achieve object-level encoding, thus enhancing the granularity of visual semantic cues. MIMIC (Luo et al., 2023) presents a multi-grained multimodal interaction network designed for the multimodal entity linking task. GMEL (Shi et al., 2024) offers a Generative Multimodal Entity Linking framework employing LLMs that directly produces names of target entities. MMELL (Yang et al., 2023) introduces a module that jointly extracts features to learn representations from both visual and textual inputs for context and entity candidates. DWE (Song et al., 2023) describes a dual-way enhancement framework where the query benefits from refined multimodal information, and Wikipedia is used to augment the semantic representation of entities.\nContrastive learning Contrastive Learning (Chopra et al., 2005; Nguyen and Luu, 2021; Nguyen et al., 2025, 2024a,b, 2023b) has become increasingly prominent in various fields (Nguyen et al., 2022, 2023a, 2024c; Wu et al., 2020b, 2022, 2023a,b, 2024a,c,b; Wei et al., 2024). Established training methods such as N-Pair Loss (Sohn, 2016), Triplet Margin Loss (Balntas et al., 2016), and ArcCon (Zhang et al., 2022) are foundational in metric learning. In supervised learning scenarios, key methodologies include Center loss (Wen et al., 2016), SphereFace (Liu et al., 2017), CosFace (Wang et al., 2018), and ArcFace (Deng et al., 2018), which are extensively applied in computer vision and natural language processing. Recent developments in contrastive learning incorporate additional conditional variables to enhance representation quality, involving auxiliary attributes (Tsai et al., 2021), information pertinent to the downstream task (Tian et al., 2020), downstream labels (Khosla et al., 2020; Kang and Park, 2020), or data embeddings (Tsai et al., 2022; Wu et al., 2020a). These advancements allow for the extension of contrastive self-supervised learning into weakly supervised (Tsai et al., 2021), semi-supervised (Tian et al., 2020), or fully supervised frameworks (Khosla et al., 2020). The work by (Khosla et al., 2020) specifically seeks to enhance fairness, rather than solely focusing on representation quality, by employing sensitive attributes from the dataset as the conditional variables in a self-supervised context."}, {"title": "3 Problem Formulation", "content": "A multimodal knowledge base includes a set of entities $E = {E_i}_{i=1}^N$. Each entity is denoted as $E_i = (en_i, ev_i, ed_i, ea_i)$, represent entity name, entity images, entity description, and entity attributes, respectively. In this representation, the $E_i$ components correspond to the entity's name, images, description, and attributes, respectively. As our study emphasizes linking local-level entities, textual input is formatted as sentences rather than as documents. Specifically, a mention and its context are represented as $M_j = (mw_j, ms_j, mv_j)$, where $mw_j, ms_j$, and $mv_j$ denote the words of the mention, the sentence containing the mention and the associated image, respectively. The related entity of the mention $M_j$ in the knowledge base is $E_i$. The MEL task is illustrated in Figure 3."}, {"title": "4 Methodology", "content": "In this section, we first introduce the overall framework and then go through the details of our approaches. We visualize the overall architecture in Figure 4.\n4.1 Overall Architecture\nVisual Encoding Following previous work, we utilize the pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021) as our visual backbone to encode image features, denoted as VISenc. Each entity image $ev_i$ (or mention image $mv_j$) is resized to $C \u00d7 H \u00d7 W$ pixels and divided into flattened 2D patches of size $n = H \u00d7 W/P^2$. These patches undergo projection and multi-layer transformer processes. Subsequently, a fully connected layer adjusts the output hidden states. The resulting hidden states, denoted as $VE_i$, include a global feature $VE_i^G$ represented by the special token [CLS], and local features $VE_i^L$ (similarly for mention $vM_j^G$ and $VM_j^L$).\nTextual Encoding We employ a Transformer-based architecture as the textual encoder (BERT (Devlin et al., 2019)), denoted as TEXTenc. For mentions $M_j$, we concatenate the mention words with the sentence containing the mention to create the input sequence. We obtain $tM_j^G$ and $TM_j^L$ as the global and local textual features of mention $M_j \\in \\mathbb{R}^{(l_e+1)\\times d_t}$, respectively. Here, $d_t$ is the dimension of textual output features, and $l_e$ is the length. For entities $E_i$, entity inputs are formed by concatenating the entity name with its attributes, denoted as $ea_i$. Following the mentioned processing, the resulting hidden states $TE_i$ are structured as $[t^{[CLS]}; tE_i^1; ... ; tE_i^{l_e}] \\in \\mathbb{R}^{(l_e+1)\\times d_t}$.\nMatcher In this step, the model calculates the similarity matching score between input mention text-image pairs and knowledge entities to achieve correct matching. For instance, using the CLIP (Radford et al., 2021) template, we calculate the cosine similarity score between the global representation ([CLS] token) the mention input with the entity. Alternatively, MIMIC (Luo et al., 2023) introduced the Multi-Grained Multimodal Interaction Network, which captures and integrates the fine-grained representation of global and local cues of each modality. Overall, the matcher acts as a scoring function $\u0177 = F(M, E)$ between the mention M and the entity E.\nObjective Based on the score calculated above, we jointly train both the encoding layer and the matcher using a contrastive training loss function. This approach allows the model to learn to assign higher scores to positive mention-entity pairs and lower scores to negative mention-entity pairs. This loss function can be formulated as follows.\n$L = - log \\frac{exp(F(M,E_{pos}))}{exp(F(M,E_{pos}))+\\Sigma_i exp(F(M,E_{neg,i}))}$                         (1)\n4.2 Conditional Contrastive learning\nConditional Contrastive Learning (CCL) (Ma et al., 2022) was proposed to enhance the fairness of contrastive self-supervised learning (SSL) methods. CCL mitigates the influence of sensitive attributes during contrastive pre-training by sampling positive and negative pairs that share the same attribute (e.g., gender, race). The CCL approach reduces the impact of the sensitive attribute Z by treating Z as the conditional variable between X and Y, focusing on scenarios where Z is readily available in the dataset (e.g., gender, race, or age). The proposed Conditional Contrastive Learning objective is designed to address this effectively.\n$L_{CCL} = sup_{Ez \\sim P_Z} E_{(x_i,Y_i) \\sim P_{X,Y|z}} [-log \\frac{ef(x_i,Y_i)}{\\Sigma_{j=1}^n ef(x_i,y_j)}]$                      (2)\nwhere the positive pairs ${(x_i, Y_i)}_{i=1}^n$ represent samples drawn from the conditional joint distribution: $(x_i, Y_i) \\sim P_{X,Y|z}$, while the negative pairs ${(x_i, Y_{j\\neq i})}$ represent samples drawn from the product of conditional marginal distributions: $(X_i, Y_{j\\neq i}) \\sim P_{x|z}P_{y|z}$. The proposed Conditional Contrastive Learning (CCL) differs from conventional contrastive SSL by selecting positive and negative pairs from distributions conditioned on a sensitive attribute referred to as Z. The CCL approach reduces the influence of the sensitive attribute Z by treating Z as the conditional variable between X and Y. We focus on scenarios where Z is readily available in the dataset (e.g., gender, race, or age).\nConditional sampling We first define a preprocessing stage to identify hard negative samples before the training phase. Each entity $E_i$ in the knowledge base contains a set of predefined attributes. We calculate the Jaccard distance between two entities i and j as the similarity score between the two entities:\n$J(i, j) = \\frac{ea_i \\cap ea_j}{ea_i \\cup e_j}$                                                        (3)\nFor each entity $E_i$, we calculate the total Jaccard distance with the knowledge base $J_i = [J(i, 1), J(i, 2), ..., J(i, j)]$, where $1 \\leq j \\leq n_e$ and $j \\neq i$. We store these similarity scores and sort them. Then, we select the top k scores as the k most similar or hard negative samples of $E_i$: $\\hat{J_i} = top_k, J_i$.\nJaccard Distance-based CCL After determining the top k negative samples, we calculate the conditional contrastive learning meta-attribute JD-CCL:\n$L_{JD-CCL} = E_{(M,E_{pos})\\sim P_{M,E,Z},{\\hat{E_{neg,j}}}_{j=1}^k \\sim P_{E|Z=z_i}} [-log \\frac{e^{F(M,E_{pos})}}{e^{F(M,E_{pos})} + \\Sigma_{j=1}^k e^{F(M,\\hat{E_{neg,j}})}}]$       (4)\nwhere positive pairs $(M_i, E_{pos,i})$, consisting of a mention input and its linked entity in the knowledge base, are drawn randomly from the dataset. After sampling a positive pair, negative entities ${\\hat{E_{neg,j}}}_{j=1}^k$ are drawn from the set $J_i$ of the entity $E_{pos,i}$. Unlike CCL for fair representation, this approach samples negative entities based on their meta-information, thereby covering different attributes, whereas CCL is based on predefined conditions Z such as gender and race. Furthermore, the difference between JD-CCL and Contrastive Learning can be described as follows: JD-CCL samples negative entities from conditions $z \\in Z$, resulting in negative pairs that possess more general features. This increased diversity in features makes the model more challenging to train.\n4.3 Contextual Visual-aid Controllable Patch Transformation\nTo address the issue of dissimilarity between visual information in mention inputs and knowledge entities, we propose the Contextual Visual-aid Controllable Patch Transformation (CVaCPT) module. CVaCPT adaptively influences the mention visual input by applying a transformation to each patch representation based on prior information. Technically, we utilize the Stable Diffusion model to generate an image $s_j$ corresponding to the input text $x_j$. Here, $m_j$ is the mention sentence prefixed with \"An image of\". Thus, the same sentence with different mentions we want to match will create different images of the target. Using a visual encoder VISenc, we obtain a global representation $VES_{j, h}$ of the synthetic image $s_j$.\nControllable patch transformation (CFT) The generated image with the target reference text in the sentence helps the model selectively control the important features of the mention input. We propose the Controllable Feature Transformation (CFT) module to manage the information flow from the visual encoder, $VE_j^L$ and $VE_j^G$, before feeding it into the matcher. Specifically, as shown in Fig. 5, the synthetic features $VES_{j}^G$ combined with the text mention representation $TES_{j}^G$ are used to finetune the visual features of the mention, $VE_j^L$ and $VE_j^G$, through the transformation of features using the affine parameters $\\alpha$ and $\\beta$:\n$VE_j^G = VE_j^G + w(\\alpha^G \\odot VE_{S_j}^G + B^G)$                                   (5)\n$VE_j^L = VE_j^L + w(\\alpha^L \\odot VE_{S_j}^L + B^L)$                                   (6)\n$\\alpha^G, B^G = A^G([VE_j^G VTES_{j,*}^G])$                                (7)\n$\\alpha^L, B^L = A^L([VE_j^L VTES_{j,*}^G])$                                (8)\n$VTES_{j,*}^G = CE([VES_{j}^G TES_{j}^G])$                               (9)\nwhere $Pox$ denotes a stack of convolutions that predicts $\\alpha^X$ and $\\beta^X$ from the concatenated $([\u00b7])$ features $[VE_j^X; CE([VES_{j,*}^G. TES_{j}^G])]$. CE is the contextual visual-aid module, which receives a concatenation of the global representation of the synthetic image $VES_{j}^G$ and the global representation of the input mention $TES_{j}^G$. An adjustable coefficient $w \\in [0, 1]$ is then used to control the relative importance of the inputs. With the CPT module, the mention image representations are adjusted to match the mentioned entity in the sentence, removing noisy and unhelpful patches needed to match in the knowledge base.\nMultiview Synthetic image We found that images generated from the diffuser model can include noisy features (e.g., background details not related to the mention). Thus, using multiple images can help the CFT module select important and helpful features from synthetic images. We generate $n_s$ different synthetic images from the same prompt $s_{j,h}$, where $h \\in {1,..., n_s}$. Each global visual representation is concatenated with the global textual representation before being fed into CE:\n$VTEs_j = [VTEs_{j,1}, ..., VTEs_{j,ns}]$                                                 (10)\n$VTEs_{j,h} = CE([VES_{j,h}^G TES_j^G])$                                 (11)\nWe apply max-pooling to calculate the maximum value for patches of a feature map across multiple images:\n$VTEs_j = max\\_pooling(VTEs_j)$                                                        (12)\nThis max_pooling operation emphasizes the presence of strong/important features and diminishes the noise."}, {"title": "5 Experiments setups", "content": "5.1 Datasets\nIn the experiments, we adopt three benchmark MEL datasets: (i) WikiDiverse (Wang et al., 2022b) is a meticulously curated MEL dataset featurthe appendixverse array of contextual topics and entity types sourced from Wikinews. Comprising 8,000 image-caption pairs, it employs Wikipedia as its underlying knowledge base. (ii) WikiMEL (Wang et al., 2022a) comprises over 22,000 multimodal sentences gathered from Wikipedia entity pages. (iii) RichpediaMEL (Wang et al., 2020) Initially, entities were extracted from Richpedia for the creation, followed by the acquisition of multimodal data from Wikidata (Vrandecic and Kr\u00f6tzsch, 2014). Individuals constitute the primary entity category in both WikiMEL and RichpediaMEL.\n5.2 Baselines\nWe compared our method with various competitive VLP models: CLIP (Radford et al., 2021), ViLT (Kim et al., 2021), ALBEF (Li et al., 2021), METER (Dou et al., 2022), MIMIC (Luo et al., 2023). Detailed descriptions are provided in the Appendix A.\n5.3 Metrics\nWe calculated the similarity between a mention and all entities in the knowledge base (KB) to measure their aligning probability. The similarity scores are sorted in descending order to calculate H@1, H@3, H@5, and MRR. H@k indicates the hit rate of the ground truth entity when only considering the top-k ranked entities: H@1,3,5 represent the hit rates in the top 1, 3, and 5, respectively. MRR represents the mean reciprocal rank of the ground truth entity: $MRR = \\frac{1}{N}\\Sigma_{i=1}^N \\frac{1}{rank(i)}$.\n5.4 Implementations\nWe initialize the CLIP model (Vit-Base-Patch32) as both our visual encoder VISenc and text encoder TEXTenc, from HuggingFace. For visual input, all images (mention, knowledge entity and synthetic image) are rescaled to 224 \u00d7 224. With the mention or entity input that do not have image, will initialize with blank image (while) with zero padding. For textual input, we use CLIP tokenizer, which constructs based on byte-level Byte-Pair Encoding. We implement linear neural networks to project the hidden representation from CLIP model to 96 and setup the matcher similar to MIMIC. Since our CVaCPT module and contrastive objectives, we built upon the implementation of the state-of-the-art MIMIC (Luo et al., 2023) by incorporating CVaCPT and replacing the original contrastive learning objective with our JD-CCL approach."}, {"title": "6 Experimental results", "content": "Following the methodology of previous studies, we conducted our model five times using the same hyperparameter settings. The average results are presented in Table 1. An overview of our results reveals that our model consistently matches or surpasses the benchmarks set by baseline approaches. Our proposed method shows significant improvements across various datasets. Notably, we achieved a substantial increase of 6.74% in accuracy for H@1 on WikiDiverse, and improvements of 2.36% and 1.30% on RipediaMEL and WikiMEL, respectively. For H@3, H@5, and MRR, we also achieved better performance on most datasets. To be more specific, we outperform the state-of-the-art (SOTA) by 2.74%, 1.63%, and 4.65% on WikiDiverse, and achieve slight improvements on the other two datasets, except for H@5 metric on WikiMEL."}, {"title": "7 Analysis", "content": "7.1 Regularization\nTo validate the significance of each of our objectives, we conducted additional experiments where one of our proposed methods was removed. As illustrated in Table 2, the results are presented in the first three rows, indicating that incorporating all the proposed approaches leads to the best performance. The second row shows that not applying JD conditional contrastive learning harms performance. This result can be attributed to the fact that, during training, hard negative samples play an important role in this task. In the third row of the table, we demonstrate that the removal of CVaCPT significantly affects performance across all metrics.\n7.2 Number of negative samples and number of synthetic images\nWe conducted an experiment to better understand the impact of the number of negative samples, k, in JD-CCL and the number of synthetic images, s, in CVaCPT. We set k to values in the set 2, 4, 6 and s to values in the set 1, 2, 3. As shown in the first three rows of Table 2, a larger number of conditional negative samples (k = 4, 6) achieves better results compared to a smaller number (k = 2), although there is only a small difference between k = 4 and k = 6. Regarding the number of synthetic images, using s = 1 means not applying the max-pooling operation. As shown in the last three rows, multiple views of visual augmentation outperform a single view (s = 1) with only one synthetic image. However, increasing the number of images from 2 to 3 does not significantly improve the model.\n7.3 Larger knowledge base evaluation\nWhile the benchmarks WikiMEL and RichpediaMEL achieved good performance, WikiDiverse appears to be a more challenging dataset. To enhance the ability to handle a larger knowledge base, we merged the two easier benchmarks with the more challenging WikiDiverse to create two larger datasets (double in size): WikiDM (WikiDiverse + WikiMEL) and WikiDR (WikiDiverse + RichpediaMEL). The experimental results are presented in Table 3. As shown, our approach outperforms MIMIC across all metrics on both datasets. Specifically, our method achieves an improvement of 4.00% and 4.91% in the H@1 metric on WikiDM and WikiDR, respectively, and outperforms in all other metrics as well.\n7.4 Visualization\nIn this appendix, we provide a visualization of the Controllable Patch Transformation module, as illustrated in Figures 6a and 6b. We visualize the calculation of alpha and beta for patches in the image corresponding to the input mention (in each caption) and synthetic images. As shown in Figure 6a, the model focuses on people in the image (alpha and beta values are large) while reducing the impact of the background (sky, building) (alpha and beta values are small). In contrast, Figure 6b shows a building covering the entire image, where all alpha and beta values are equally high, indicating that all image patches are informative for the matcher."}, {"title": "8 Conclusion", "content": "This paper introduces JD-CCL, a novel conditional contrastive learning approach based on Jaccard Distance. This method leverages meta-attributes to draw conditional negative samples with similar features during training, allowing the model to identify specific characteristics for prediction accurately. Additionally, our proposed Contextual Visual-Aid Controllable Patch Transform (CVa-CPT) module enhances visual modality representation by utilizing synthetic images and contextual information from the mention sentence. Our framework paves the way for further exploration in this domain, promoting multimodal and knowledge base application advancements."}, {"title": "Limitations", "content": "Despite the improvements our approach achieves in the MEL task, JD-CCL and CVaCPT still have the following limitations. CVaCPT uses a Text-to-Image diffuser model to generate synthetic images, which requires additional time and computational resources. Furthermore, using synthetic images from a different domain might introduce some noise due to the domain gap. Secondly, JD-CCL includes a condition sampling stage, which is a preprocessing step to determine the negative samples, and this stage also takes time. Additionally, we automatically calculate the Jaccard distance between the attribute sets of two entities. If the dataset is not well-annotated, this could negatively impact the training stage."}]}