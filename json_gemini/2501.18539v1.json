{"title": "Can we Retrieve Everything All at Once?\nARM: An Alignment-Oriented LLM-based Retrieval Method", "authors": ["Peter Baile Chen", "Yi Zhang", "Michael Cafarella", "Dan Roth"], "abstract": "Real-world open-domain questions can be complicated, particularly when answering them involves information from multiple information sources. LLMs have demonstrated impressive performance in decomposing complex tasks into simpler steps, and previous work has used it for better retrieval in support of complex questions. However, LLM's decomposition of questions is unaware of what data is available and how data is organized, often leading to a suboptimal retrieval performance. Recent effort in agentic RAG proposes to perform retrieval in an iterative fashion, where a followup query is derived as an action based on previous rounds of retrieval. While this provides one way of interacting with the data collection, agentic RAG's exploration of data is inefficient because successive queries depend on previous results rather than being guided by the organization of available data in the collection. To address this problem, we propose an LLM-based retrieval method ARM, that aims to better align the question with the organization of the data collection by exploring relationships among data objects beyond matching the utterance of the query, thus leading to a retrieve-all-at-once solution for complex queries. We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms standard RAG with query decomposition by up to 5.2 pt in execution accuracy and agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and 19.3 pt higher F1 match scores compared to these approaches.", "sections": [{"title": "Introduction", "content": "Answering real-world questions can be complicated, especially when required information is distributed across heterogeneous information sources, such as text corpus, databases, and even image collections. Consider an example question \"What is the highest eligible free rate for K-12 students in the schools in the most populous county in California?\" and the data collection shown in Figure 1. Answering this question requires one passage (A) from the text corpus and three joinable tables (B, C, D) from the database. To find all of these passages and tables, it requires exploring the available text and tables in the data collection, reasoning about their relationships (e.g., joinable tables, entity connection), and determining the best organization of these objects that can fully answer the question.\nWith recent advances in LLMs, they have been used to decompose complex questions to boost the performance of RAG systems (Khot et al., 2022; Zhou et al., 2022; Jhamtani et al., 2023). However, all these decomposition methods are unaware of what data is available, and how they are organized. Therefore, these approaches are highly likely to overlook important data objects relevant to the question, especially when their information is not explicitly mentioned. As depicted in Figure 1, this method can miss all bridging tables.\nA possible way to improve LLM's decomposition is to make it interact with the data collection, which has been explored in the context of agentic RAG (Yao et al., 2022; Trivedi et al., 2022; Press et al., 2022; Asai et al., 2023; Zhang et al., 2024). Typically, an LLM-based agent, which has demonstrated impressive performance in reasoning and acting (Kojima et al., 2022; Wei et al., 2022; Shinn et al., 2024; Yao et al., 2024), iteratively reasons about actions (i.e., search queries). Search queries are then issued, and the retrieved passages will be fed back to the agent to reason about the next action (search query) until it determines that the question has been fully answered. Although we have seen promising results when adapting such agent-based iterative solutions, they have shortcomings as follows:\nFirst of all, although the iterative approach enables interaction with the data collection, each action is guided by the agent's decision about what is still missing to fully answer the question based on"}, {"title": "Overview", "content": "This section provides an overview of our alignment-oriented LLM-based retrieval solution. Each component will then be elaborated in Section 3. Intuitively, our solution adopts the idea of retrieving while generating (Jain et al., 2024), so that we can take the most advantage of LLM's reasoning capability, and the decoding process enables an optimization of each component jointly through beam search. Rather than simply interleaving \"retrieval\u201d by constrained decoding based on evidence from the text corpus, and \"thoughts\" by unconstrained decoding (Jain et al., 2024), we propose to retrieve everything jointly through one decoding process guided by information from data objects available in the data collection, a solver's reasoning, and LLM's self-verification.\nSpecifically, we consider the retrieval problem as a generative problem that an LLM needs to output a \"reasoning process\" to find all necessary data objects from the data collection to answer the given question. There are two main challenges for an off-the-shelf LLM to output such a reasoning process that can align with the data organization from the collection. First, while an LLM can use its reasoning capabilities to analyze what information might be useful, it cannot determine how to map that information to existing data objects without access to the data collection. Secondly, LLMs may lack the domain-specific knowledge to reason about how different data objects that are semantically related to the question are connected. They might also struggle to identify whether additional data objects are required to connect these objects, particularly when the question involves private databases (Chen et al., 2024b). Therefore, LLMs need guidance from the data collection and its organization to generate the complete \"reasoning process\".\nTo solve the challenges mentioned above, we formulate the reasoning process generation as a decoding process consisting of multiple intermediate steps. Rather than traditional text decoding where each step decodes a token, we consider each step as an alignment that decodes a sequence of tokens based on the information of existing objects from the data collection. We mainly consider two alignment steps with one self-verification step. The first is information alignment, where we draft the key information needed to answer the question directly. This is achieved by constrained decoding using N-grams (introduced in Section 3.1) from existing data objects. The second is structure alignment, where we reason about how different pieces of key information from existing data objects can be connected, potentially with additional objects, to answer the question through a reasoning solver. The alignment results are then fed to the \u201creasoning process\" as drafts, and the LLM self-verifies the relatedness of the data objects to the question as well as their connections and selects the final data objects that can fully answer the question. The overall idea is illustrated in Figure 1 (rightmost block in upper half), omitting the details of beam search."}, {"title": "Methodology", "content": "In this paper, we unify both tables and passages (and potentially objects of other modalities such as images) and consider them as textual data objects. We chunk each serialized data object, compute the embeddings of each chunk, and further represent and index it with an N-gram collection.\nSerialization. In our data collection, each passage chunk is serialized as a concatenation of its passage title and the content of the chunk. A table chunk is serialized as a concatenation of its table name, title, description (if any), and rows.\nN-Grams and Embeddings. N-grams are used to summarize the key information of a chunk from data objects. They can provide a quick lookup of what is contained in a chunk. Additionally, embeddings are used to support semantic similarity search. Both embeddings and N-grams are used to guide LLMs' reasoning of what data objects should be used to answer the question (Section 3.2.1). Specifically, we constructed N-grams for each table and passage chunk, varying N between one to three. Moreover, we computed the embedding for each table and passage chunk."}, {"title": "Alignment", "content": "As described in Section 2, we instruct the LLM to generate a reasoning process with multiple intermediate steps. The first step is to determine the key information required to answer the question. As an off-the-shelf LLM does not have access to the data collection, its analysis of what information might be helpful may not align with the information from the available data objects. To address this, we propose to instruct the LLM to first decompose the question by extracting keywords independently of the data collection, and then guide it to rephrase each keyword using N-grams available from the data objects in the collection through constrained decoding.\nOur constrained-decoding-based information alignment is based on the model's extracted keywords from the user question. Since these keywords might not appear as directly in the corpus, we instruct the model to rephrase them to align with N-grams indexed from our data collection as mentioned in Section 3.1."}, {"title": "Structure Alignment", "content": "Information alignment guides us towards a set of search objects from the data collection that is very likely to help answer the question. However, there can be redundant and missing information. For instance, several passages identified may address the same aspect of the question. The information about the necessary bridging entities (Yang et al., 2018) or bridging tables (Chen et al., 2024c), and the information that has to be derived from those bridging entities or tables can still be missing. To address this, we further design a structure alignment module that reasons about a complete list of search objects with their organization, so that it can match the information required and fully answer the question.\nThe key challenge of structure alignment is LLM does not have a global view of all available data objects and may lack specific knowledge to identify the missing objects needed to correctly connect the candidate objects that have been identified, especially when the data collection belongs to a specialized domain that the LLM may not have encountered extensively during its training.\nTherefore, we propose to use an external solver to solve this structure alignment problem, where we can formulate the objective and include any business or domain-specific reasoning logic. Specifically, for a given list of search objects, the solver is tasked with returning a subset of the input search objects that are connected and can be combined to answer the question. We then use the content of these selected objects to construct a partial \"draft\" for the LLM to continue its \u201creasoning process\u201d(Section 3.3)."}, {"title": "Inference using Mixed-Integer Programming", "content": "We formulate structure alignment for retrieval as an optimization problem. As outlined above, the goal is to identify a list of k objects from a given list of M search objects ${O_i}_{i=1}^{M}$ that can be connected to best answer the question Q considering the domain-specific knowledge. Similar to (Chen et al., 2024c), we formulate it as a mixed-integer linear program (MIP) problem due to its flexibility to inject any business or domain-specific logic into the objective.\nSpecifically, the goal of the MIP program is to select a list of k objects that can simultaneously maximize the relevance between the question and selected objects and compatibility (strength of connection) among the selected objects.\nRelevance. Relevance between a user question Qand an object $O_i$, denoted as $R_i$, is defined as the cosine similarity between the embedding of Q and the embedding of the serialized object $O_i$.\nCompatibility. Object-object compatibility between two objects $O_i$ and $O_j$, denoted as $C_{ij}$, measures the strength of connection between two objects. It is computed based on both semantic similarity (cosine similarity of embeddings) and exact-value similarity between contents from both objects.\nFinally, we combine the above relevance and compatibility scores in the following MIP formulation with decision variables, objectives, and constraints. The output of the MIP consists of both the k selected objects as well as the connections between these objects (connecting entities or joinable columns).\nDecision variables. To incorporate the relevance score, we define the binary decision variable $b_i$ to denote whether object $O_i$ is selected. To take into account the compatibility scores, we define the binary decision variable $c_{ij}$ to denote whether object $O_i$ is selected to connect with object $O_j$.\nObjective. Using the above variables and parameters, the objective function is to maximize the relevance of the selected objects and compatibility"}, {"title": "Self-Verfication and Aggregation", "content": "Finally, from each draft generated by the MIP solver, the LLM itself will select a set of objects that can potentially be used to answer the user question, acting as a verifier that collectively evaluates all information decoded so far to perform object selection. An aggregation mechanism, based on model confidence, is used to combine the selection results from the different drafts.\nDraft Injection. To inject a draft into the decoding process of an LLM, it is first serialized as a string, then models are enforced to decode this string using constrained decoding. As mentioned in Section 3.2.2, a draft from MIP consists of both selected objects and connections among selected objects. Each object is serialized in the same way described in Section 3.1, but only the k (k = 5 in our setting) most similar table rows or passage sentences are selected, so that the model can focus on the most relevant content. Each connection is serialized as \"column {column name} in {table name} connects with column {column name} in {table name}\" to represent a joinable column or \u201c{cell} in {table name} connects with {sentence} in {passage name}\" to represent a connecting entity.\nAt this stage, model has generated a decomposition of the user question, and has alignment knowledge on both information and structure. Using these knowledge, model performs the verification process by checking if selected objects includes content that can cover different aspects of the decomposition and that selected objects are connected. We use constraint decoding to guide this selection process to ensure factuality. In particular, we ensure that objects chosen by models must be present in the draft.\nAfter verification is performed for each draft, the model's reasoning process is completed. Since the reasoning process is produced by a three-step constrained decoding process (different from the typical text decoding process where each step consists of a single token), and both information and structure alignment steps yield multiple outputs (sequences of tokens), we use beam search to generate multiple reasoning processes. We aggregate the selected data objects from the multiple beams by factoring in the model's confidence for each selected object. Specifically, the aggregation process is treated as a weighted voting process, where each beam acts as a voter who votes for a selected object. In this context, an object's confidence can be measured by the weight of votes and the number of votes it receives. The weight of a vote is measured using logits. In particular, tokens generated in the reasoning process that correspond to the object's name are identified, and the logits of these tokens are averaged to be the weight of a vote. The number of votes an object receives is the count of its occurrences across all beams, with softmax applied to normalize this count. Then, the confidence in an object is computed as the weighted sum of the"}, {"title": "Experiments", "content": "We evaluate our approach and baselines on open-domain question answering tasks that involve multiple sources of information. Therefore, we use OTT-QA (Chen et al., 2020) and Bird (Li et al., 2024). Specifically, OTT-QA involves questions on both passages and tables whose answers are mostly short text, while Bird involves questions on (multiple) tables, and the answer is a SQL statement. For each dataset, we use the dev split for the user questions and constructing the data collection. For Bird, we construct the data collection by merging tables from all databases used by the dev questions. Similarly, for OTT-QA, we construct the data collection by merging the tables and passages used to answer the questions in the dev set. In our experiments, we remove the questions we found with incomplete annotations (missing either required tables or passages through our manual inspection) from OTT-QA. In total, there are 1834 questions and a collection of 3862 objects (3073 passages and 789 tables) for OTT-QA and 1534 questions and 75 tables in the data collection for Bird. After chunking (described in Section 3.1), there are 4407 chunks and 249515 chunks for OTT-QA and Bird, respectively.\nWe evaluate our approach against two baseline methods: the standard RAG and agentic RAG approaches. For the standard RAG baseline, we consider two variations: dense retrieval and dense retrieval followed by a cross-encoder reranker. Additionally, we enhance the standard RAG approach by incorporating an LLM-based query decomposition. To have a fair comparison with our approach, we use the same model (Llama 3.1-8B-Instruct) as the LLM to conduct query decomposition. Table 4 contains prompts used for generating sub-questions.\nIn our experiments, the embedding model used for dense retrieval was UAE-Large-V1 (Li and Li, 2023), and we use bge-reranker-v2-minicpm-layerwise (Li et al., 2023; Chen et al., 2024a) as the reranking model. Additionally, ReAct was chosen as the representative of the agentic RAG approach.\nAll objects are chunked and serialized in the way described in Section 3.1. The dense retrieval method computed the embedding for each chunk and outputs the top-k objects with the highest cosine similarity with the user question. If an object is divided into multiple chunks, its similarity score is the highest similarity score across all its chunks. For the reranking model, it was provided with the top-50 objects retrieved using the dense retrieval method. The reranker model assigns a score for each pair of user question and object, and it outputs top-k objects with the highest scores. When query decomposition is applied, 30 objects were retrieved for each sub-question using the dense retrieval method and further reranked to output the top-k objects.\nReAct was implemented following the original design of interleaving thought, action, and observation. A thought step allows models to reason about the current situation. An action can be of two types: (1) the model can generate some keywords to search for relevant objects from the corpus (2) or finish generation with an answer. An observation step involves calling a dense retriever, which retrieves the 5 serialized objects with the highest similarity to the model-generated keywords in action. Because most questions in both datasets can be answered using 4 objects, we set the maximum number of iterations to 8. The process continues until either the answer is found or the maximum limit of 8 rounds is reached. Table 5 contains the 3-shot prompts used for ReAct."}, {"title": "Metrics", "content": "For retrieval performance, we adopt the standard metrics of precision, recall, and F1 of the retrieved objects compared to the gold objects. However, we note that the recall metric could be misleading because in an extreme scenario, a retriever can achieve high recall by retrieving a significant portion of gold objects for every question, but with none of the questions having all gold objects retrieved. This is problematic as a question can usually only be answered when all information provided. Therefore, we further augment existing metrics with the the percentage of questions with all gold objects retrieved, denoted as perfect recall (PR). For ReAct, we examine its retrieval performance by comparing the objects provided to the LLM with the gold objects.\nFor the end-to-end performance on downstream tasks, for OTT-QA, we compare the predicted short answer and gold short answer using exact match and F1 score. For Bird, following (Li et al., 2024), we compare the predicted SQL and gold SQL statement as the execution accuracy (1 if the execution results of both SQL statements are the same and 0 otherwise).\nRegarding the number of LLM calls for retrieval, ARM makes one LLM call as the entire retrieval process is completed in one decoding process. In ReAct, each LLM call produces an action that formulates queries for retrieval. However, the retrieval results from the last call are not fed back into the LLM. As a result, the number of LLM calls for retrieval in ReAct is calculated as the total iterations minus one, which we assume the last call is for generating the final response."}, {"title": "Retrieval performance", "content": "Table 1 shows the retrieval performance of dense retrievers and dense retrievers with reranker on Bird and OTT-QA. Table 2 shows the retrieval performance of ReAct and ARM. As mentioned in Section 4.1, the retrieval process of ARM was performed by Llama3.1-8B-Instruct in one LLM call.\nOn Bird, ARM retrieves on average 5.00 objects, achieving a recall of 96.5 and perfect recall of 92.7. In comparison, the best-performing standard RAG baseline, dense retrieval, retrieves 5 objects with a recall of 89.0 and perfect recall of 78.4, which is 7.5 and 14.3 points lower compared to ARM, respectively. Additionally, compared to ReAct running on Llama3.1-8B-Instruct, ARM reduces LLM calls by 4.26 and retrieves 12.3 fewer objects while maintaining comparable recall and perfect recall but achieving a 31.5 higher F1, potentially reducing noise.\nOn OTT-QA, ARM retrieves an average of 4.98 objects, achieving a recall of 79.8 and a perfect recall of 62.5. In comparison, the best-performing standard RAG baseline, dense retrieval with reranker, retrieves 5 objects with a recall of 75.2 and a perfect recall of 53.8, which is 4.6 and 8.7 points lower than ARM, respectively. Additionally, compared to ReAct running on Llama3.1-8B-Instruct, ARM reduces LLM calls by 3.52 and retrieves 14.5 fewer objects while achieving 3.8 points higher recall and 7.4 points higher perfect recall.\nThese results demonstrate that ARM outperforms standard RAG baselines in recall and perfect recall, indicating a higher likelihood of retrieving all necessary information to answer user questions. Additionally, compared to agentic RAG, ARM achieves comparable or superior retrieval performance while using fewer LLM calls and retrieving fewer objects."}, {"title": "End-to-end performance", "content": "Table 3 shows the end-to-end results on both datasets across all approaches and two models. Averaging across both models, ARM demonstrates superior performance on Bird and OTT-QA. On Bird, it outperforms the best-performing standard RAG baseline, dense retrieval, by 2.55 points in execution accuracy and agentic RAG by 11.1 points. On OTT-QA, ARM achieves 3.7 points higher exact match and 4.4 points higher F1 match compared to dense retrieval with reranker, while outperforming the agentic RAG by 12.7 points in exact match and 14.6 points in F1 match.\nThe results indicate that ARM outperforms standard RAG baselines by retrieving objects of higher quality, leading to improved downstream performance despite retrieving a similar number of objects. Moreover, compared to agentic RAG baselines, ARM also achieves superior downstream performance using fewer LLM calls. This highlights ARM as a more effective and efficient solution."}, {"title": "ReAct Analysis", "content": "We randomly selected 50 questions from each of the Bird and OTT-QA datasets and manually analyzed the results generated by both models using the ReAct approach. Our analysis showed two primary types of errors made by models during its iterative reasoning process. First, models might forget information it generated in previous iterations. Secondly, models can fall into cycles of searching for similar keywords, even when relevant objects have already been retrieved. Both behaviors can lead to inefficiency and potentially a large number of LLM calls. Detailed examples can be found in Appendix C."}, {"title": "Ablation studies", "content": "Significance of different modules. ARM includes three modules: information alignment, structure alignment, and self-verification and aggregation. Information alignment involves decomposing the original question into keywords and retrieving relevant objects, similar to the baseline of dense retrieval with query decomposition. To highlight the benefits of information alignment, we compare the two methods. On average, information alignment outperforms dense retrieval with query decomposition by 12.5 points in recall and 19.8 points in perfect recall across two datasets. The inclusion of structure alignment boosts recall by 1.28 points and perfect recall by 4.02 points, building on the gains from information alignment. Finally, the complete method with all three modules enhances recall by 5.72 points and perfect recall by 9.18 points. The improvement with each successive module demonstrates the contribution of every module to the overall retrieval performance.\nInformation alignment. Information alignment retrives relevant objects through two components: keyword lookup using the decomposed and aligned keywords via BM25, and embedding similarity. Adding keyword lookup to embedding similarity improves recall by 2.15 points and perfect recall by 3.65 points on average across two datasets, clearly demonstrating the contribution of each component."}, {"title": "Conclusion", "content": "Understanding what data objects are available in the data collection and their organization is critical for answering complex open-domain questions that involve heterogeneous information sources. Query decomposition by an off-the-shelf LLM generates queries without an awareness of what is available in the data collection, often leading to a suboptimal retrieval performance. Although agentic RAG can interact with the data collection, the queries are formulated based on previous retrieval results rather than an understanding of the available data objects and their organization. Therefore, agentic RAG is often inefficient to retrieve all required data objects due to more LLM calls. In this work, we propose an alignment-oriented retrieval method ARM, that is capable of exploring which data objects may contain the key information required to answer the question, and also navigating the data organization to identify all required data objects, even when their information is not explicitly stated in the question. Our experimental evaluations showed that, compared to the baselines, ARM is more effective in terms of performance in retrieval and downstream tasks, as well as more efficient in terms of the number of LLM calls needed."}, {"title": "Compatibility", "content": "Table-table compatibility is determined by pairwise column comparisons between the two tables. Since only one pair of columns is necessary to connect two tables, table compatibility is determined by the highest compatibility score among all possible column pairs. Each column-column compatibility is calculated as the weighted sum of the semantic similarity between the column headers and the exact-value similarity (Jaccard similarity) of the column rows.\nTable-passage compatibility is determined by comparing all cells within a table and all sentences in a passage and taking the pair with the highest compatibility. Each cell-sentence compatibility is calculated as the weighted sum of the semantic similarity and exact-value similarity (overlap coefficient) between the cell content and the sentence. Passage-passage compatibility is computed by comparing all sentences in a passage with all sentences in the other passage and taking the pair with the the highest compatibility. Each sentence-sentence compatibility is calculated as the weighted sum of semantic similarity and exact-value similarity (overlap coefficient)."}, {"title": "Prompts", "content": "Prompts for our experiments are included in Table 4-6."}, {"title": "Examples where models fail using ReAct", "content": "Below are examples where models failed to generate correct answers using ReAct.\nAs seen in Table 7, the model can forget information generated in previous iterations. It was trying to search the population of Barcelos, but concluded with the population of Ajim.\nAs seen in Table 8, the model fell into a loop of searching using similar keywords, even when gold tables have already been retrieved. The gold tables are financial.card, financial.disp, financial.client"}]}