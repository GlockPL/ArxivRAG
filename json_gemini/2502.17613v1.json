{"title": "Flexible Counterfactual Explanations with Generative Models", "authors": ["Stig Hellemans", "Andres Algaba", "Sam Verboven", "Vincent Ginis"], "abstract": "Counterfactual explanations provide actionable insights to achieve desired outcomes by suggesting minimal changes to input features. However, existing methods rely on fixed sets of mutable features, which makes counterfactual explanations inflexible for users with heterogeneous real-world constraints. Here, we introduce Flexible Counterfactual Explanations, a framework incorporating counterfactual templates, which allows users to dynamically specify mutable features at inference time. In our implementation, we use Generative Adversarial Networks (FCEGAN), which align explanations with user-defined constraints without requiring model retraining or additional optimization. Furthermore, FCEGAN is designed for black-box scenarios, leveraging historical prediction datasets to generate explanations without direct access to model internals. Experiments across economic and healthcare datasets demonstrate that FCEGAN significantly improves counterfactual explanations' validity compared to traditional benchmark methods. By integrating user-driven flexibility and black-box compatibility, counterfactual templates support personalized explanations tailored to user constraints.", "sections": [{"title": "1 Introduction", "content": "In decision-making systems such as loan approvals in banks or disease risk predictions in healthcare users often seek actionable insights to achieve favorable outcomes [1,2,3,4]. For instance, a rejected loan applicant may want to know if increasing their income or reducing outstanding debts would result in loan approval. Similarly, patients classified as high-risk for developing a disease may wish to understand what changes in their lifestyle or medical parameters could lower their risk. Counterfactual explanations address this need by providing the smallest changes to input features that alter a model's prediction to a desired outcome [5,6].\nA high-quality counterfactual explanation must satisfy several criteria [7]. It should be actionable, meaning the proposed changes are feasible and only involve mutable features (e.g., income or exercise levels, but not age or genetic factors) [8,9]. Explanations should also be realistic, ensuring that they align with the data manifold [10,11], and diverse, offering multiple possible solutions for users to explore [12]. Additional desiderata such as sparsity (fewer changes) and proximity (minimal distance to the original input) further enhance the actionability of counterfactual explanations [13].\nDespite their potential, existing methods for generating counterfactuals face key challenges. State-of-the-art counterfactual explanation generation methods- whether gradient optimization-based [8,10,12,14] or generative [15,16,17]\u2014assume a fixed set of mutable features, whereas more flexible non-gradient optimization- based methods, such as constrained satisfaction problem solvers (SATs), fail to accommodate for traditional desiderata in counterfactual explanations, such as incorporating a realism loss [18,19,20]. All these methods thus fail to account for users' diverse preferences and real-world constraints. For instance, increasing one's income may be feasible for one user but unrealistic for another. Another example is changing one's residential location: while relocating might be a plausible option for a single individual, it could be impractical for someone with family commitments or legal residency constraints. Moreover, existing approaches often require retraining or regenerating explanations when users modify their prefer- ences, reducing their practical usability.\nAnother significant limitation is the lack of robust solutions for black-box set- tings, where access to the model's internal parameters is restricted, such as with proprietary systems or federated learning environments. Many existing tech- niques assume direct access to or differentiability of the underlying model, which becomes infeasible in these scenarios [10,12,16,21].\nTo address these limitations, we propose Flexible Counterfactual Explana- tions using Generative Adversarial Networks (FCEGAN) (Fig. 1). FCEGAN introduces the novel concept of counterfactual templates, which encode infor- mation about which features are allowed to change and which must remain fixed. This approach empowers users to dynamically specify mutable features, offering unprecedented flexibility in exploring actionable changes. Beyond gener- ative models, counterfactual templates also enhance gradient-based optimization methods by embedding these templates directly into the optimization process. This ensures that only the user-defined mutable features are modified, signifi- cantly improving the validity and alignment of counterfactual explanations with user constraints.\nAdditionally, FCEGAN supports black-box scenarios by utilizing historical predictions of the model rather than requiring direct access to its internals. By combining the flexibility of user-driven templates with robust gradient-based optimization and black-box compatibility, FCEGAN provides a versatile, practical, and scalable solution for generating actionable, realistic, and personalized counterfactual explanations.\nIn summary, our approach addresses two critical limitations of existing coun- terfactual generation methods: (1) enhancing adaptability to user preferences through the introduction of counterfactual templates, which allow users to dy- namically specify mutable features, and (2) enabling counterfactual explanations in black-box scenarios by leveraging historical model predictions without requir- ing access to internal model parameters. By integrating these advancements, FCEGAN and the template-enhanced gradient-based optimization methods pro- vide a flexible framework for generating realistic, actionable counterfactuals, making them highly applicable to domains like healthcare and finance, where user constraints and interpretability are crucial."}, {"title": "2 Background and Related Work", "content": "Counterfactual explanations can be generated using a variety of methods, which are broadly classified into optimization-based and generative approaches. Op- timization-based methods aim to optimize an objective function subject to cer- tain constraints [8,9,10,12,21]. These methods focus on minimizing a distance function, with one key constraint ensuring that the resulting counterfactual sat- isfies the desired class prediction. Within this category, optimization techniques can be further divided into gradient-based and non-gradient-based approaches. Discrete non-gradient methods, such as constraint satisfaction problem solvers (SATs), form a subclass that often fail to account for traditional desiderata in counterfactual explanations, such as incorporating a realism loss [18,19,20].\nAn advantage of optimization-based methods is having great flexibility in constructing custom distance functions and additional constraints. However, this flexibility also results in the need to construct a new optimization search space for each task. It is often unclear which distance function is optimal since the difficulty of adjusting a feature is a personal matter. Furthermore, these methods may lack assurance that the generated explanations are realistic. To address this issue, reconstruction loss from auto-encoders or outputs from GAN discriminators can be incorporated as additional constraints in gradient-based methods [10,14].\nDespite their benefits, these methods suffer from slow generation speeds, as multiple optimization steps are required to generate each candidate expla- nation. To accelerate the process, techniques such as leveraging class proto- types abstract representations of samples belonging to a specific class in the latent space- -can guide the optimization path more effectively [14]. Yang et al. [22] achieve a speedup by identifying the most relevant features, focusing on the nearest samples that match the desired target class.\nGenerative models, such as GANs [15,16,17], inherently adhere to the data manifold and offer rapid inference speeds due to their one-shot generation capa- bility, as they require training only once for the entire data space. However, a key limitation of existing GAN approaches is their assumption of a fixed set of mutable features, necessitating retraining when a different combination of fea- tures is considered. This restricts user flexibility in choosing which features to modify and which to keep immutable. Nemirovsky et al. [15] tackled this issue by introducing a method to respect immutable features during counterfactual gen- eration. Specifically, their approach searches for counterfactual examples while automatically reverting any changes made to features marked as immutable, en- suring these features remain unaltered. However, the set of immutable features remains fixed, and users are still unable to dynamically select which features to keep unaltered.\nFCEGAN addresses this limitation by introducing a counterfactual template that allows users to specify which features are mutable. This template serves as an additional input to the counterfactual generator, enabling users to dy- namically control which features can be altered. FCEGAN incorporates tech- niques from the Conditional Tabular GAN (CTGAN) [23], including training- by-sampling [24,25], and the Wasserstein GAN [26,27] with gradient penalty (WGAN-GP) [28], to generate diverse and realistic synthetic tabular data [23]. These methods effectively mitigate mode collapse [29], which can otherwise com- promise the diversity of the generated counterfactual explanations.\nIn black-box scenarios, where the underlying model's architecture and pa- rameters are inaccessible, a promising solution is to leverage a dataset of the model's historical predictions. These datasets inherently capture the relation- ships between specific input features and the corresponding outputs. For in- stance, Nemirovsky et al.[15] utilize this idea by weighting samples based on the model's previously assigned scores. This strategy enables the generation of counterfactuals that align with the model's prior behavior, even in the absence of direct access to its internal workings."}, {"title": "3 Flexible Counterfactual Explanations", "content": "FCEGAN addresses two key limitations in existing counterfactual generation methods: (1) it allows users to dynamically specify which features are muta- ble through a counterfactual template, and (2) it can support black-box models by relying solely on historical predictions rather than requiring direct access to the model. The counterfactual template empowers users to dynamically specify which features can be modified without retraining the model, as illustrated in Fig. 2. Furthermore, these templates can be integrated into a gradient optimiza- tion method to guide the generation process, ensuring targeted and adaptable counterfactual explanations."}, {"title": "3.1 Specifying mutable features", "content": "The core idea behind specifying mutable features lies in the use of the coun- terfactual template $X_{tmp}$, which indicates which features can be modified. This template, along with the original instance $x_{og}$ and the desired target label, serves as input to a conditional generator [30]. For multi-class classification tasks, spec- ifying the desired target is especially important [31]. To encode mutable features, a copy of the original instance is created where the mutable features are masked, and the target label is replaced with the desired one. This template gives the generator clear guidance on which features it is allowed to change while ensuring the rest remain intact.\nThe counterfactual explanation $x_{cf}$ is then generated by a conditional gen- erator, implemented as part of a Wasserstein GAN [26,27] with gradient penalty (WGAN-GP) [28]. Although the generator can perturb both mutable and im- mutable features, any changes made to immutable features are reset to their original values, as introduced by Nemirovsky et al. [15]. This ensures that the generated counterfactuals respect the immutability constraints.\nDuring gradient-based optimization, the counterfactual template plays a key role by resetting immutable features to their original values from the template after each gradient step. This ensures that the optimization process focuses ex- clusively on modifying mutable features, adhering to the predefined constraints."}, {"title": "3.2 FCEGAN training", "content": "To produce realistic and valid counterfactual explanations, we employ a com- bination of loss functions. First, two discriminator losses are used to promote realism. The loss $L_{Dog}$ ensures that the generated counterfactual $x_{cf}$ remains close to the original instance $x_{og}$, while $L_{Dcf}$ compares $x_{cf}$ to real counterfactu- als $X_{desired}$ sampled from the target class. The real counterfactuals are selected using the training-by-sampling method described in CTGAN [23].\nTo limit the extent to which the counterfactual deviates from the original instance, we introduce a divergence loss $L_{div}$. This loss is applied specifically to mutable features, as our experiments demonstrated that enforcing mutability constraints ($L_{m,div}$) is sufficient (see Appendix Fig. C3). Consequently, the di- vergence loss for immutable features ($L_{i,div}$) is omitted. The divergence loss is defined as:\n$L_{div} = L_{m,div} = \\frac{1}{\\lambda_m} \\cdot [d_{cont}(X_{m,og}, X_{m,cf}) + d_{cat}(X_{m,og}, X_{m,cf})] \\quad (\\lambda_m \\in \\mathbb{R}),$ (1)\nwhere $d_{cont}$ measures the distance between normalized continuous features using Mean Squared Error (MSE), and $d_{cat}$ evaluates one-hot encoded categorical features using cross-entropy loss. The total distance is normalized by the number of features $x$.\nFinally, a classifier loss $L_{clas}$ ensures that the generated counterfactual achieves the desired target class. This loss is calculated as the cross-entropy between the prediction $y_{cf}$ for the generated counterfactual and the specified target $Y_{desired}$."}, {"title": "3.3 Enabling black-box training", "content": "In black-box settings, models are assumed to be inaccessible during training. A solution in such scenarios is to leverage a dataset containing the model's his- torical predictions. These datasets inherently capture the relationships between input features and corresponding outputs.\nNemirovsky et al. [15] build on this idea by weighting the discriminator loss based on the model's prediction scores. Samples with high prediction scores- indicating that they contain features characteristic of the desired output-are given greater influence during generator training.\nWe adopt a similar approach but simplify it by including only samples in $L_{Def}$ that are predicted as belonging to the desired class. The classifier loss $L_{clas}$, which is unavailable in a black-box setting, can be omitted. This is because $L_{Def}$ already provides sufficient information about which features are important for generating synthetic samples of the desired class. Our experiments confirm this hypothesis (see Section 4)."}, {"title": "3.4 Quality measures", "content": "To evaluate the quality of counterfactual explanations in line with the desired desiderata outlined in the Introduction, we define several quality measures (Table 1). The valid counterfactual fraction represents the proportion of counterfactuals that successfully predict the desired target class $Y_{desired}$. This metric is critical, as the validity of counterfactuals cannot be guaranteed after their initial generation. It serves as the most important evaluation measure, reflecting the method's ability to identify the characteristics required to produce valid counterfactual explanations.\nTo assess categorical proximity or divergence, we use the mean fraction of categorical columns changed [12], while for continuous features, the mean/\u0442\u0430\u0445 percentile shift is employed [11].\nThe fakeness/realism of counterfactuals is measured using an independent CTGAN discriminator trained on the original training data. Higher fakeness scores indicate less realistic samples, but the scale of this measure is dataset- dependent and should always be contextualized by comparing it to the fakeness scores of real samples. Lastly, diversity is quantified as the expected proximity or divergence between pairs of counterfactuals, providing a measure of variation across the generated explanations [12]."}, {"title": "4 Experiments", "content": "In this section, we present the four data sets that we use in our experimental setup. Then, we discuss the state-of-the-art benchmark methods and discuss the results. Finally, we highlight the usefulness of our approach in a black-box setting."}, {"title": "4.1 Datasets", "content": "To demonstrate the performance of FCEGAN and template-guided optimiza- tion, we evaluate these methods on four diverse datasets, chosen to highlight scenarios where counterfactual explanations could be particularly useful in al- tering predictions toward more desirable outcomes. Two medical datasets focus on predicting the future development of diseases: the Heart Disease Risk Pre- diction dataset [32] and the Diabetes Health Indicators dataset, a cleaned ver- sion of the Behavioral Risk Factor Surveillance System 2015 (BRFSS-2015) [33]. In these cases, counterfactual explanations can inform patients about potential lifestyle changes required to influence disease progression. For instance, in the diabetes dataset, which contains three labels (No Diabetes, Pre-Diabetes, Dia- betes), counterfactual explanations enable experimentation with various lifestyle factors, identified as mutable in the counterfactual template, to evaluate their impact on health outcomes.\nIn addition to medical use cases, we examine two business-oriented datasets: the Employee Attrition Classification dataset and the Adult UCI Income dataset, the latter representing a loan application setting. For employee attrition, coun- terfactual explanations can help organizations identify actionable changes to reduce the likelihood of employees leaving. Similarly, in loan applications, such explanations provide insights into factors influencing income classification, offer- ing actionable strategies for improving outcomes. All datasets used in this study are publicly available on Kaggle. Missing data is omitted, and the data is split into train, validation, and test sets with a 60-20-20 ratio."}, {"title": "4.2 Benchmarks", "content": "Since counterfactual templates represent a novel concept, no existing models can be directly compared to our FCEGAN and optimizer implementation. As a result, we benchmark against ablated methods that resemble prior counterfactual approaches. Specifically, our FCEGAN model, trained without counterfactual template knowledge or resetting immutable features, is comparable to the models proposed by Van Looveren (2021) [16] and Nemirovsky (2022) [15]. Both utilize a combination of classifier loss, divergence loss, and discriminator loss to train their counterfactual generators. In addition, we include a default gradient-based optimization method as an optimization benchmark, where no template guidance is applied during the optimization process."}, {"title": "4.3 Results", "content": "As shown in Fig. 3, counterfactual templates significantly enhance the perfor- mance of counterfactual explanation methods. Both GAN-based and gradient- based optimization approaches benefit from these templates, achieving a higher fraction of valid counterfactuals those that successfully meet the desired class\nprediction. This improvement in valid counterfactuals indicates greater search efficiency, as fewer explanations are discarded due to invalidity. Consequently, the methods demonstrate improved adaptability to constraints imposed by im- mutable features, effectively addressing the challenge of partial feature immutabil- ity. Feature-level examples are provided in Appendix Figures C7 and C8.\nHowever, the advantage of counterfactual templates diminishes as the frac- tion of mutable features increases. With fewer constraints, the model requires less guidance from the template, reducing its overall utility.\nAnd to enhance validity, methods guided by counterfactual templates gen- erate counterfactuals of comparable quality but with reduced diversity in FCE- GAN models (Fig. 4). Both categorical divergence (e.g., shifts in category dis- tributions) and continuous divergence (e.g., changes in mean or maximum per- centiles) generally increase, though not consistently. This variability stems from the model's ability to selectively modify mutable features while adhering to im- mutability constraints. However, imposing stricter divergence constraints can mitigate the reduction in diversity, as discussed in Appendix Fig. C1."}, {"title": "4.4 Black-box implementation", "content": "Our FCEGAN architecture supports training in a black-box setting by omitting the classifier loss and utilizing a dataset of historical predictions from the clas- sification model. This approach is particularly advantageous in scenarios where access to the model's internal parameters is restricted, such as in proprietary third-party systems or under federated and distributed learning protocols. As il- lustrated in Fig. 3, the black-box implementation of FCEGAN maintains strong performance, exhibiting only minor reductions in the fraction of valid counter- factuals on certain datasets, such as Employees, compared to classifier-based implementations. Furthermore, the black-box FCEGAN delivers quality metrics that are comparable to those achieved by its classifier-dependent counterparts."}, {"title": "5 Conclusion", "content": "Counterfactual explanations are a critical tool in decision-making systems, en- abling users to understand and influence model predictions by identifying ac- tionable changes to input features. Despite their utility, existing methods often lack the flexibility to accommodate user-specific constraints, rely on fixed sets of mutable features, and face challenges in black-box settings where model pa- rameters are inaccessible. To address these issues, we proposed Flexible Coun- terfactual Explanations using Generative Adversarial Networks (FCE- GAN), which introduces counterfactual templates to allow users to dynamically specify mutable features.\nIn addition to leveraging GAN-based approaches, our work enhances gradient- based optimization methods by incorporating counterfactual templates into the optimization process. These templates ensure that only mutable features are modified, improving the validity and alignment of counterfactual explana- tions with user-defined constraints. Tailored divergence losses further ensure the generated counterfactuals are realistic and actionable, making gradient-based optimization a viable and robust alternative for counterfactual generation.\nExperimental results across diverse datasets show that both FCEGAN and template-guided gradient-based optimization methods significantly enhance the validity of counterfactual explanations, albeit at the expense of reduced diver- sity. This limitation can be partially mitigated through divergence constraints.\nCounterfactual templates enable users to personalize explanations without re- training models, while the framework's compatibility with black-box settings, using historical prediction datasets, ensures practicality in real-world scenarios.\nA promising avenue for future research is to further enhance user agency by allowing manual adjustments to the counterfactual template. For instance, users could modify attributes such as weight or income and explore the re- sulting changes in other features needed to achieve the desired outcome. This interactive capability would empower users to test different scenarios and gain deeper insights into the relationships between features and predictions. Such ad- vancements could expand the utility of counterfactual explanations in domains requiring high user engagement, such as healthcare and personalized financial planning.\nBy integrating user-driven flexibility, robust gradient-based optimization, and black-box compatibility, FCEGAN advances the state of the art in counterfactual explanation methods. Future research may also explore extending these meth- ods to multi-modal datasets, incorporating temporal dynamics, and addressing fairness constraints, further broadening their applicability in critical decision- making domains."}, {"title": "A Architecture Details", "content": "Classifier Configuration A neural network with two hidden layers is trained using the hyperparameters in Appendix Table B1. To address class imbalance, a weighted loss function is employed.\nGenerator and Discriminator Configuration The generator and discriminator of the Conditional Tabular GAN (CTGAN) serve as the foundation for FCE- GAN, including the independent CTGAN discriminator used for assessing fak- eness [23]. Detailed configuration settings are provided in Appendix Table B2. The generator leverages residual layers with concatenation to enhance feature learning, while the discriminator incorporates a PacGAN approach to improve training stability [34]. Additional details about the architecture are available at https://github.com/sdv-dev/CTGAN.\nOptimization Configuration The loss for regularized gradient descent (RGD) consists of three components: (a) classifier loss $L_{clas}$, (b) divergence loss $L_{div}$ (Eq. 1), and (c) realism loss $L_{real}$, which is the output of a CTGAN discrim- inator trained on the data distribution. After each gradient step, immutable features are reverted back to their original values from the counterfactual tem- plate $X_{tmp}$. To accelerate the process, all samples requiring counterfactuals are batched, and a mean loss is optimized, significantly improving generation time per sample without compromising quality. Appendix Figure C9 shows that 20- 30 gradient steps are sufficient for good results, after which training saturates. Hyperparameters are provided in Appendix Table B3.\nData Transformation To maintain consistency when including a classifier loss, FCEGAN relies on data transformation methods that align with those used in the classification model. In our study, we applied standardization for continu- ous features and one-hot encoding for categorical features. Alternatively, more complex methods, such as CTGAN [23], use a Gaussian Mixture Model (GMM) for normalizing continuous variables and one-hot encoding categorical features. CTGAN's mode-specific normalization produces both a one-hot encoded mode and a normalized value representing the position within the Gaussian mode. However, as illustrated in Fig. C2, the choice of transformation method does not consistently influence counterfactual generation outcomes. While CTGAN's stochastic mode selection can lead to non-deterministic counterfactual classifi- cation predictions, standardization ensures reproducibility and avoids this issue, demonstrating reliable performance without systematic disadvantages."}]}