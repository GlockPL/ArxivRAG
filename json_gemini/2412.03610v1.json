{"title": "The Use of Artificial Intelligence in Military Intelligence: An Experimental Investigation of Added Value in the Analysis Process", "authors": ["Christian Nitzl", "Achim Cyran", "Sascha Krstanovic", "Uwe M. Borghoff"], "abstract": "It is beyond dispute that the potential benefits of artificial intelligence (AI) in military intelligence are considerable. Nevertheless, it remains uncertain precisely how AI can enhance the analysis of military data. The aim of this study is to address this issue. To this end, the AI demonstrator deepCOM was developed in collaboration with the start-up Aleph Alpha.\n\nThe AI functions include text search, automatic text summarization and Named Entity Recognition (NER). These are evaluated for their added value in military analysis. It is demonstrated that under time pressure, the utilization of AI functions results in assessments clearly superior to that of the control group. Nevertheless, despite the demonstrably superior analysis outcome in the experimental group, no increase in confidence in the accuracy of their own analyses was observed. Finally, the paper identifies the limitations of employing AI in military intelligence, particularly in the context of analyzing ambiguous and contradictory information.", "sections": [{"title": "1 Introduction", "content": "The sheer volume of data that can be observed today makes it clear that military intelligence requires the use of artificial intelligence (AI) [10]. However, the benefits of using AI and at what point in the military analysis process is still an open question [26]. The primary role of military intelligence is to gather and analyze\ninformation to support military leaders in making informed decisions. From an academic standpoint, military intelligence represents a transdisciplinary field of research that draws upon a multitude of disciplines, including political science, economics, sociology, and psychology, among others [1].\n\nMilitary intelligence is thus concerned with the collection and analysis of information to provide a comprehensive picture of the situation. This may entail the collection of data on the armed forces and the examination of the plans and operations of other nations, as well as the gathering of information on developments affecting a nation's security [25].\n\nWhat is certain is that the use of innovative approaches and methods, such as artificial intelligence (AI), must be ensured when analyzing militarily relevant developments abroad. New developments in AI and its integration into analysis and research software promise a wide range of support options to enhance analysts' ability to make judgments [5].\n\nIt is expected that the use of AI technologies will reduce the burden on analysts, allowing them to focus on the core content of analyzing, assessing and presenting the military intelligence situation [12].\n\nIt should be emphasized that the analyst should not be replaced by AI systems, but rather assisted. In particular, it must be ensured that analysts are always able to understand the information on which they are making an assessment [2].\n\nAs part of this study, a proprietary AI demonstrator was developed by the start-up company Aleph Alpha. The capabilities of this program called deepCOM are based on a Large Language Model (LLM). It should be emphasized that deepCOM is not a working product, but a demonstrator. The core functionality of deepCOM is semantic search. This allows the user to ask direct questions which are answered by the system, indicating the sources used. In addition, deepCOM can automatically summarize each report in the database, allowing the analyst to identify relevant sources from a summary of a few sentences.\n\nAn additional Named Entity Recognition (NER) implemented in the system labels all reports fully automatically: if present in the text, tags are derived from mentions of time, places, organizations and people, which are highlighted for the user both when identifying relevant sources and when reading [8].\n\nThe goal of this study is to demonstrate the added value of using AI in the military analysis process. While previous studies have focused primarily on the use of AI in data collection [13], this study focuses on the support AI provides to human analysis and assessment. The use of new technologies for their own sake is not desirable if it does not lead to direct added value for the analyst and his or her analytical performance.\n\nConceptual considerations alone are not sufficient to assess value. In order to be able to make empirically validated statements, an experiment was conducted in this study. To the best of our knowledge, this is the first study to empirically analyze the added value of AI in the context of intelligence."}, {"title": "2 The intelligence cycle as the starting point of the military analysis process", "content": "In order to assess the support provided by AI systems in the military analysis process, it is first necessary to clarify how the intelligence process works in general [13]. The starting point is the so-called intelligence cycle, which describes the ideal process from the decision maker's request for information to the intelligence product [19]. It should be noted that the process is not a linear sequence of individual steps, but includes feedback loops [14].\nFigure 1 illustrates the typical intelligence cycle.\n\nThe process begins with the Planning & Direction phase. In this phase, a client or customer, in our cases a military decision maker, formulates a need for intelligence. This need is usually expressed in terms of questions that the customer believes must be answered in order to be able to make an informed decision. This defines an intelligence problem [7,23].\n\nOnce the mandate is given, the second phase of the cycle, Collection, begins. This involves gathering information needed to produce the finished intelligence product [23]. Today, collection can be based on a variety of different intelligence disciplines: Human Intelligence (HUMINT), Imagery Intelligence (IMINT), Signals Intelligence (SIGINT), and Open Source Intelligence (OSINT) [6,20].\n\nThe Collection phase is followed by the Processing phase, in which the collected information is processed [23]. This includes translating foreign language texts, decoding, and organizing the information from human sources into a standardized reporting format [6]. The main challenge in processing is that there is often more data from different sources than can be processed in a reasonable amount of time [15].\n\nThe intelligence product is created in the Analysis & Production phase. This is done by integrating, evaluating, and analyzing all available information into an overall picture, taking into account the knowledge already available [23]. The analyst faces the challenge that the available information may be incomplete and contradictory. The goal of this phase is to obtain an assessment of ambiguous events and possible future events, thus providing the customer with a basis for an informed decision, for example, by recommending a course of action [7].\n\nFinally, in the Dissemination phase, the intelligence product is distributed to the client. This can take the form of a written report or a verbal briefing [6]. The decisions made by the customer may directly lead to further intelligence requirements or at least influence the content requirements for future finished intelligence, so the circle from Dissemination to Planning & Direction closes at this point [23]."}, {"title": "3 AI capabilities in the deepCOM demonstrator in support of military intelligence", "content": "The deepCOM demonstrator is an analysis tool with integrated AI capabilities designed to support the work of military analysts. The AI functions experimentally analyzed are described below. Two of the three AI functions tested within deepCOM, namely AI search and automated summarization, are based on a Large Language Model (LLM). The third AI function tested is Named Entity Recognition. Although the intelligence community in Germany works in English due to international structures such as NATO, the United Nations and the EU, its own products are created in German. Accordingly, deepCOM's user interface and output are in German."}, {"title": "3.1 Artificial intelligence search in text databases", "content": "Standard searches in text databases are based on the frequency of occurrence of words. Accordingly, texts that contain more of the search terms will appear higher up on the list. In this method, also known as Bag of Words (BOW), the sheer frequency of the individual words determines a good search hit, not their relationship to each other [24]. The BOW search typically starts with a question in the user's mind, which the user must break down into several keywords [3], rather than typing the entire question into the search box. This type of search is usually the only way for the military analyst to search text, as the text databases need to be in a secure environment.\n\nSuch an approach is inefficient for several reasons: Firstly, information is lost during the interaction between the user and the query due to the forced reduction to keywords. Even if the search still works despite the omission of prepositions, cases, numbers and conjugations, the information it contains can help to produce better search results. Furthermore, the search process is not intuitive. When a question is already formulated, it is more straightforward to enter it into the search box without making any modifications. This is a standard practice for all major search engines on the Internet. Finally, BOW's search is based exclusively on the frequency of words in texts. However, this approach may result in the retrieval of irrelevant documents that contain the keywords in question, despite the documents' lack of relevance to the user's actual query.\n\nThe problems of BOW search can be solved by AI search. AI search is able to process a question as a whole. This results in less loss of information. In deepCOM, this is achieved by first displaying the answer of the AI search in the output, which is either formulated by the system itself or taken directly from a text, depending on the complexity of the question. Since the way in which the LLM behind the AI search generates the answer is not always comprehensible or correct, the full-text passages from which the information originates are also referenced. As can be seen in Figure 2, the answer given by the LLM-based AI search is independent of the exact wording of the questions. In addition, the AI search can deal with different spellings of identical entities (in the example shown, the different transliteration of an Arabic proper name)."}, {"title": "3.2 Named Entity Recognition", "content": "Named Entity Recognition (NER) refers to the extraction of entities from unstructured text and their classification into predefined categories [16]. The NER implemented in deepCOM is based on a German retraining of the Bidirectional Encoder Representations from Transformers models originally published by Google [8,28]. It automatically identifies time, place, organization, and person entities, even without optimization for specific text corpora. Since entities in texts usually occur in inflected form, lemmatization is also required to convert them to their base form and thus make them comparable (e.g., Mittelmeers, Mittelmeere \u2192 Mittelmeer). Both works largely correctly, although rare entities are sometimes incorrectly categorized or incorrectly converted to an uninflected"}, {"title": "3.3 Automated text summaries", "content": "Irrespective of the possible full-text presentation, the references in deepCOM are also reduced to about one-third to one-half of their original length by summarizing each paragraph into one sentence. Automated summarization thus serves a similar purpose to NER. Both allow a quick assessment of the importance of a text for an analysis. The conflict between the length of the text and the depth of detail that summarization provides must be decided on a situation-by-situation basis."}, {"title": "4 Description of the experimental study", "content": "4.1 Military analysis scenario\n\nThe starting point of the experimental study is a realistic scenario from military intelligence analysis. A total of 50 source texts was collected from publicly accessible news portals on the Internet. The sources were stored in deepCOM and served as a text database. The sources were selected to provide a comprehensive picture, including articles from both national and international news sites. These included news portals publishing in French, Russian, Arabic and Persian.\n\nAn overview of the sources used can be found in Annex A.\n\nThe news texts refer to the poison gas attack in Khan Shaykhun in the Idlib governorate in northwestern Syria. On April 4, 2017, at least 86 people were killed and several hundred injured by sarin gas. The release of the poison gas is uncontested, but explanations of how it happened vary widely: According to the US, UK, French and German accounts, the gas was deliberately dropped by an air strike by the Assad government's Syrian air force. The Idlib governorate and the town of Khan Shaykhun were considered a stronghold of the Syrian government's opponents at the time of the incident. However, according to Syrian, Russian, and Iranian accounts, the sarin was released because the Syrian Air Force had bombed an insurgent poison gas storage facility or factory with conventional weapons. In response to the poison gas attack, the US, under President Trump, launched cruise missile strikes on the Syrian military airfield of Al Shayrat, from which the attack on Khan Shaykhun is believed to have originated. The central task in the analysis process is to identify and select the relevant sources that provide the necessary information for a correct assessment of the situation. In order to meet this challenge in the experiment, the 50 source texts were selected in such a way that about one third could be used directly for the analysis, another third dealt with the poison gas attack in Khan Shaykhun"}, {"title": "4.2 Design of the experiment", "content": "Participants in the study were randomly assigned to either the experimental or control group. Both groups used the same browser interface, except that the AI functions were disabled in the control group. The AI search was replaced by a BOW search, and the automatic summarization was replaced by a display of the first words of a paragraph. The completion time for the analysis task was set to 30 minutes. An expert survey was conducted as a basis for evaluating the analysis performance of the participants. Experienced military analysts worked on the same analysis task as the participants in the experiment. They worked under the same conditions as the control group. However, the experts were not given a time limit to complete the analysis scenario.\n\nAny information that could be used to personally identify individual participants was deleted after data collection was completed.\n\nVouchers were raffled off to encourage participation. Of the original 30 participants, one person had to be excluded for technical reasons. Therefore, the experimental group consisted of 14 participants and the control group consisted of 15 participants. All participants are active duty soldiers. They range in age from 20 to 33 years old (M=26.6; SD=4.3) and 76% are male. In addition, seven"}, {"title": "5 Analysis of the experimental results", "content": "In a first step, the average score per item for the first part of the analysis task was calculated separately for the experimental and control groups. This was used to determine how close the participants in both groups were to the expert judgment. As the 21 items in total had different scores to be achieved due to their varying complexity, these are presented in relative terms.\n\nFigure 6 shows the average percentages achieved per item. Since a score closer to 100% means a higher average agreement with the expert judgment, a higher score is considered to be better in this figure.\n\nThe experimental group always performed better than the control group, except for Item 19. Furthermore, the items were of varying difficulty, as the curves are similar for both groups. For example, the solutions for Items 4 to 6 turned out to be significantly poorer than for Items 8 and 9, regardless of whether the AI functions were available to the subjects or not. Finally, it is noticeable that both groups were able to solve fewer and fewer items towards\nthe end of the first part. This is due to the 25-minute time limit for the first part of the analysis task.\n\nThe next step was to test the related items from the first part of the analysis task as blocks. Task 6, which had an above-average number of six items and dealt with both the US air strike and the reaction of several nations to it, was split into two subtasks. Each of the now seven tasks were statistically tested for mean differences in independent samples.\n\nTable 2 shows the means (M) and standard deviations (SD) of the experimental and control groups. The $x^2$ and p-values (p) are also shown in the table.\n\nThe experimental group with AI support scored higher overall than the control group without AI support. On average, the experimental group exhibited a score that was more than six and a half points higher than the control group. Looking at the individual tasks, it can be seen that the AI support did not add significant value in all cases. While the experimental group outperformed the control group in Tasks 1 and 3, this was less significant in Tasks 4 and 6/1. No significant differences were found for the other tasks. Analysis performance did not correlate significantly with participant age ($x^2$=1.823; p=0.177) or gender (x2=1.910; p=0.167).\n\nIn the second part of the analysis task, participants were requested to indicate the probability and level of confidence associated with a given thesis. The assessment of these items differs from that of the first part of the analysis task in that a deviation from the expert base is possible in both directions, i.e., an over- or underestimation of probability and confidence. Consequently, the discrepancy between each individual rating and the mean of the expert ratings for that item"}, {"title": "6 Discussion", "content": "6.1 Comparison of the analytical performance of the experimental group with that of the control group\n\nFor the first part of the analysis task, the experiment showed that the use of AI functions leads to a demonstrable increase in performance. However, a more detailed analysis shows that this is not the case for all tasks. Since these blocks of tasks belong together thematically, further conclusions can be drawn for the AI functions. For this purpose, the tasks are divided up into three groups: Tasks in which the experimental group performed highly significantly (Group 1: Tasks 1 and 3), weakly significantly (Group 2: Tasks 4 and 6/1) and not significantly better (Group 3: Tasks 2, 5 and 6/2). All the questions in Group 1 have in common that they can be answered in a few words and in a factual manner. The sample items shown in Figure 8 can be answered with 'insurgents' (1c) and 'between 100 and 400' (3a).\n\nGroup 2 consists of questions with similar characteristics to Group 1, except that the answers are more complex: Possible answers for Item 4a are 'Syrian government', 'President Assad', or 'Syrian air force', and for Item 6c 'cruise missiles', 'tomahawks', 'warships', and 'destroyers'. All items in Group 2 can be answered largely factually. The items in Group 3, where no statement can be made about the added value of AI functions, require more complex answers. These are mainly argumentative and less clear than those in the first two Groups 1 and 2, requiring comparatively long answers. It can be concluded that the AI functions analyzed provide added value mainly for questions that require a direct and factual answer. As the complexity and ambiguity of the information increases, the benefit decreases.\n\nA similar statement cannot be made for the second part of the analysis task regarding probability. It is true that the participants with AI support were able to give more accurate ratings than the control group on the overall scale and on Tasks 1 and 4. However, this observation is difficult to interpret because the tasks are grouped thematically in the same way, but do not differ in terms of complexity or other characteristics.\n\nThe better performance on probability in Task 4 can be explained by the design of the experiment. This included hypotheses about the US intention behind the air strike on Al Shayrat, about which questions were asked at the end of the first part.\n\nThe poorer analytical performance of the control group, which had already been shown, led to less knowledge about the background of the American air strike. Since the control group had fewer source texts to analyze due to time pressure, their judgments in this regard were less reliable. Nevertheless, the conclusion for Task 4 remains an important observation, as time pressure is a realistic situation for military intelligence. Thus, the overall conclusion that the AI-assisted participants were able to make more accurate judgments than the control group remains valid."}, {"title": "6.2 Post-hoc survey of the experimental group", "content": "The evaluation of the post-hoc survey shows that the experimental group perceives a speed gain in military analysis through AI functions. This aspect of the post-hoc survey is relevant because, due to the experimental design, speed and workload could not be measured separately but were measured together in the experiment as analysis performance. Theoretically, it could be that the AI functions contribute to a more accurate and comprehensive analysis, but the task itself takes the same amount of time. This would not be a problem, as the improved analysis performance would still be an added value. In this respect, the participants' assessment can be seen as an indication that the speed of intelligence analysis can be increased. This correlation was also confirmed in the personal feedback discussions with the participants. In particular, they felt that the direct answering of questions by the AI search had the potential to increase speed. Comparing the average processing time of 3 hours and 49 minutes for the experts surveyed with the experimental group who had 30 minutes to answer the questions themselves, it can be assumed that the use of AI resulted in a massive time saving. The results of the experimental group were largely identical to those of the experts, especially for the factual questions.\n\nIn general, participants rated the automated summary in particular as being above average for use in military analysis. The NER and the AI search, on the other hand, were rated as being of average suitability. Three reasons for this can be deduced from the personal feedback interviews, in which the NER was also criticized by the trial participants: Firstly, the participants could not see any added value in the NER beyond the automatic summarization. Both AI functions were used with the aim of identifying suitable source texts more quickly. However, in a head-to-head comparison, the participants in the experimental group favored the automatic summarization. It can therefore be assumed that the NER is not completely unsuitable for use in AI analysis but appears to be less suitable than the automated summary in a direct comparison due to the overlapping scope of application. Another reason given for the average rating was the sometimes incorrect labeling. In this context, three participants noted that they had wanted to use the NER to identify relevant source texts at the beginning of the study, but then refrained from doing so because, for example, people were"}, {"title": "6.3 The challenges associated with the use of artificial intelligence in military analysis", "content": "The study identified several challenges in applying AI to the military analysis process. For example, the AI search encountered problems when the underlying texts contained no or insufficient information for the answer. LLM always gives the answer with the highest probability. If the sources are poor, this can sometimes lead to an accumulation of wrong answers. This was also evident in the first part of the analysis task, which contained two trick questions. The correct answer to the question 'Which nations were also involved in the US air strike?' was 'None', and the possible answer to the question 'What are the signs of a conventional explosion without the release of poison gas?' was 'Nothing'. In both cases, the AI search gave the wrong answer. The problem is that LLM always finds a passage in the source text that could answer the question posed.\n\nThere is also room for improvement in the NER. Although entities are almost always recognized and extracted as such, the classification into different groups (person, place, etc.) still fails too often. Lemmatization is not perfect in some cases, either, especially in the case of military terms and proper names, which are reduced to incorrect stems. These problems could probably be solved by retraining. The NER used was not adapted for a specific purpose but was trained for German texts in general. However, it could be specialized for entities that occur exclusively in military source texts (e.g. names of weapons, names of military leaders). This would require retraining on as large a dataset of military reports as possible. The automatic creation of domain-specific dictionaries based on military reports would be another possible improvement in this context [11].\n\nA final challenge is the volume of text to be processed in military intelligence. The analysis scenario of the experiment involved only 50 reports, a quantity that can also be processed with a BOW search in a reasonable amount of time. Scaling up to several thousand texts, for example, could quickly overwhelm the BOW"}, {"title": "6.4 Limitations of the design of the study", "content": "One limitation to the validity of this study is that the time allowed for completion was limited to 30 minutes. The experts were used as a reference for the assessment of analytic performance but were not under time constraints in completing the analysis tasks. In practice, however, it is often the case that intelligence products are produced under great time pressure. In this respect, the restriction of the processing time for the participants in the experiment can be regarded as a real situation in practice. In the first part of the analysis task, the experimental group achieved on average 100% of the baseline for two items, i.e., they performed as well as the experts. It cannot be excluded that the experimental group performed better than the experts due to the support of the AI functions, but that this could not be measured. It is also likely that the participants' answers to the confidence and probability questions were influenced by the time limit.\n\nAnother critical point regarding the study design is that the added value of the AI functions was only demonstrated in relation to a single analysis scenario. The scenario was designed to best model a real-world military intelligence scenario based on unclassified sources. In particular, future studies should critically examine the extent to which the specified time period in which all source texts were available and the nature of the sources influenced the results of the experiment. All texts in the analysis scenario were published within four days, between April 4 (the Khan Shaykhun attack) and April 7, 2017 (notification of the need for information by military decision-makers). As part of their analysis, the participants were confronted with a partially misleading information situation. They had to work with source texts that also contained uncertain and contradictory information. For example, it is unclear what an extension of the time corridor from four days to several years means for the added value of AI functions. Even if distractors were taken into account, the added value of AI"}, {"title": "7 Conclusion", "content": "A demonstrator was used to test three AI functions designed to support the work of military analysts in providing the most accurate situational awareness possible. These AI functions are essentially AI functions that have been made possible by advances in the field of LLM. In addition, there is a wide range of other possible applications of AI in military analysis that could not be included in this study in the interest of brevity. A further increase in the performance of the analyzed AI functions can be expected from new developments, especially in the training of German or European LLM. Data protection and confidentiality also play a central role in the use of artificial intelligence in military analysis.\n\nThe present study has shown experimentally that the three AI functions of AI search, NER, and automated summarization in combination provide added value for military analysts. Not only does analysis performance increase, but so does the ability to make more accurate assessments. In particular, the participants in the experiment saw an advantage in the increased speed of military analysis.\n\nUsing the intelligence cycle, the AI functions were positioned within military intelligence analysis. It was found that there are numerous potential areas of application for the AI functions proposed in this thesis alone. In practice, however, it is often necessary to specify which AI functions can provide concrete support in military analysis [5]. However, there are also areas of military analysis that may never be supported by AI due to ethical considerations [2]. Future research could complement the present study in that the added value of AI in supporting analysis also applies to the application of other AI functions. Extending the findings to different analysis scenarios could also contribute to the generalizability of the results. In the future, the necessity for trust and transparency in AI systems, particularly in the context of military applications, highlights the requirement for well-defined methodologies to address the \"black box\" nature of generative AI. The provision of such clarity will facilitate a more nuanced understanding and trust in AI decisions and the \"rational\" actions they generate"}]}