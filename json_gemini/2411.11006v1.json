{"title": "BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for Backdoor Defense Evaluation", "authors": ["Haiyang Yu", "Tian Xie", "Jiaping Gui", "Pengyang Wang", "Pengzhou Cheng", "Ping Yi", "Yue Wu"], "abstract": "Over the past few years, the emergence of backdoor attacks has presented significant challenges to deep learning systems, allowing attackers to insert backdoors into neural networks. When data with a trigger is processed by a backdoor model, it can lead to mispredictions targeted by attackers, whereas normal data yields regular results. The scope of backdoor attacks is expanding beyond computer vision and encroaching into areas such as natural language processing and speech recognition. Nevertheless, existing backdoor defense methods are typically tailored to specific data modalities, restricting their application in multimodal contexts. While multimodal learning proves highly applicable in facial recognition, sentiment analysis, action recognition, visual question answering, the security of these models remains a crucial concern. Specifically, there are no existing backdoor benchmarks targeting multimodal applications or related tasks.\nIn order to facilitate the research in multimodal backdoor, we introduce BackdoorMBTI, the first backdoor learning toolkit and benchmark designed for multimodal evaluation across three representative modalities from eleven commonly used datasets. BackdoorMBTI provides a systematic backdoor learning pipeline, encompassing data processing, data poisoning, backdoor training, and evaluation. The generated poison datasets and backdoor models enable detailed evaluation of backdoor defense methods. Given the diversity of modalities, BackdoorMBTI facilitates systematic evaluation across different data types. Furthermore, BackdoorMBTI offers a standardized approach to handling practical factors in backdoor learning, such as issues related to data quality and erroneous labels. We anticipate that BackdoorMBTI will expedite future research in backdoor defense methods within a multimodal context. Code is available at https://anonymous.4open.science/r/BackdoorMBTI-D6A1/README.md.", "sections": [{"title": "1 Introduction", "content": "With the advancement and widespread use of artificial intelligence (AI), neural networks have become an integral component of our modern life, handling diverse data from various devices and applications. However, they face growing threats from backdoor attacks, which are rapidly evolving and present real-world risks. Users may encounter poison data, where attackers infiltrate specific triggers into datasets before training, potentially impacting all users of these compromised datasets. As neural networks scale up, training costs also increase, forcing users to rely on third-party training resources that may lack security, thereby highlighting the real threat posed by backdoor attacks in practical scenarios. To counter the impact of"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Backdoor Attack", "content": "Existing backdoor attacks are typically categorized into three types, data poisoning attacks, training control attacks, and model modification attacks [21, 33]. Data poisoning attacks involve the adversary manipulating the training data only [4, 9, 19, 34, 43, 44, 51, 64, 86], while training control attacks allow the adversary to not only manipulate the data but also control the training process [2, 15, 39, 52, 84]. Model-modified attacks, as described in [3, 56, 67], enable the adversary to manipulate the model directly.\nBackdoor attack research has primarily focused on computer vision. However, in recent years, this research direction has broadened its scope, extending beyond images to include text and audio [12, 18, 23]. To support these emerging types of backdoor attacks, we design BackdoorMBTI to work in a multimodal paradigm."}, {"title": "2.2 Backdoor Defense", "content": "The existing defense methods can be categorized according to the machine learning lifecycle as follows:"}, {"title": "2.3 Backdoor Benchmark", "content": "Several benchmarks have been proposed in the field of backdoor attacks, such as TrojAI [27], TrojanZoo [53], BackdoorBench [79], BackdoorBox [37], and Backdoor101 [2]. TrojAI is a closed platform primarily focused on evaluating model detection defenses and is mainly utilized for backdoor model detection competitions. TrojanZoo is an end-to-end benchmark that includes attacks, defenses, and evaluations. BackdoorBench offers a multitude of experiments on image datasets, including 8,000 trials, and provides analyses and visualizations. BackdoorBox integrates backdoor attacks and defenses, with flexible invocation methods, making it a user-friendly framework for backdoor learning. Backdoor101 adds support for federated learning.\nCompared with the above benchmarks, BackdoorMBTI distinguishes itself in several key aspects: 1) BackdoorMBTI supports three types of data, i.e., image, text, and audio. While existing benchmarks can only support the first two, as shown in Table 1. 2) Users can directly access the backdoor poison dataset generated in our framework. 3) BackdoorMBTI considers real-world factors, which enables the generation of low-quality and erroneous label data."}, {"title": "3 Supported Datasets", "content": "The BackdoorMBTI framework includes 11 datasets, as shown in Table 2, covering 8 different tasks. One significant reason for selecting these 11 datasets is their public availability, ensuring easy access. Additionally, these datasets are widely used in current backdoor learning research experiments. For each modality, we have implemented a commonly used model; for instance, ResNet for computer vision, BERT for natural language processing, and CNN for speech recognition. Other models can be extended easily in our benchmark."}, {"title": "4 Implemented Attack and Defense Algorithms", "content": "There are 17 backdoor attacks in different modalities and 7 backdoor defenses implemented in our framework. In this section, we provide a brief overview of the implemented attack and defense algorithms."}, {"title": "4.1 Implemented Attack Algorithms", "content": "We have integrated 17 backdoor attacks into our multimodal benchmark, with some adapted from the computer vision domain for text and audio applications, such as BadNets and Blend. Others were originally proposed within their respective domains.\nAs depicted in Table 3, we have chosen BadNets, LC and Blend attacks to represent classic backdoor attacks, while WaNet, BPP, SBAT, PNoise and DynaTrigger represent the latest backdoor attacks in the vision domain. In addition to character-based backdoor attacks like BadNets in text, our benchmark also supports sentence-level backdoor attacks, such as AddSent [13] and SYNBKD [55]. For audio data, we have adapted Blend attacks and provided an implementation for DABA [41], GIS [28] and UltraSonic [29] attacks."}, {"title": "4.2 Implemented Defense Algorithms", "content": "In our multimodal framework, we have implemented 7 different backdoor defense methods. When selecting these defense methods, we first considered their theoretical applicability to multimodal"}, {"title": "5 Architecture", "content": "To standardize the evaluation on multimodal and to facilitate future research in multimodal backdoor learning, we have developed a multimodal backdoor learning toolkit. A key difference between BackdoorMBTI and existing benchmarks on backdoor learning is its consideration of real-world noise factors, i.e., low-quality data and erroneous labels. In this section, we will outline the framework architecture and discuss the design of two crucial components in our framework: the noise generator and the backdoor poisoner."}, {"title": "5.1 Architecture Overview", "content": "Figure 1 depicts the architectural overview of BackdoorMBTI. The framework covers the entire pipeline of backdoor learning in a multimodal context, including four key modules: data processing, data poisoning, backdoor training, and evaluation. We explain each of the modules in more detail below.\n1) Data Processing. The data processing module comprises three primary components: the data loader, noise generator, and preprocessor. Within this module, the clean dataset is loaded using the data loader, the noise generator is applied to each data item, and the preprocessor outputs a standardized data item. In contrast to other benchmarks that only support one data type, we have implemented various preprocessing techniques to support multimodal data. This includes resizing and normalization for images, tokenization and word embedding for text, and audio resampling. To simulate real-world applications, we introduce Gaussian noise as data noise and mislabeling to emulate natural label noise.\n2) Data Poisoning. The data poisoning module processes standardized data and generates poisoned data as output. The major component of this module is the backdoor poisoner, which is responsible for executing data poisoning tasks. These tasks include trigger generation, synthesizing poisoned samples, and modifying labels. Each attack included in our benchmark has its unique trigger generation process, which is also integrated into the backdoor poisoner.\n3) Backdoor Training. The backdoor training module is responsible for the training task using generated datasets and models. It consists of two distinct training pipelines: one for attack training and the other for defense training. The backdoor attack training pipeline imitates the standard training procedure but replaces the training dataset with the poison dataset. In the defense pipeline, the backdoor model created during attack training is utilized, and defense methods are applied either during training or after training. Since our backdoor poisoner is implemented as a dataset class wrapper, it can slow down training speed as the GPU waits for trigger generation and sample synthesis after fetching data from the disk. To mitigate this issue, we separate backdoor poisoning and training processes, generating the backdoor poison dataset before training in our pipeline. Additionally, for training control backdoor attacks, which typically follow a distinct training process, we implement their training procedure separately. We integrate this type of attack into the common backdoor training module (as shown in Figure 1) to establish a standardized training pipeline.\n4) Evaluation. The evaluation module takes a backdoor model and a curated test set as input and outputs performance metrics. The curated test set is specifically designed for evaluation purposes. It is generated using the data poisoning module with a poison ratio of 100%, wherein all instances with the attack target label are excluded. Detailed information about attack and defense performance metrics can be found in Section 6.1."}, {"title": "5.2 Noise Generator Design", "content": "The purpose of the noise generator is to reproduce real-world environments. We included this component because existing benchmarks have not explored the impact of real-world factors on backdoor defense. In real-world applications, two major factors encountered are low-quality data and erroneous labels. Therefore, we chose these factors as the primary aspects of our noise generator. We placed the noise generator before the backdoor poisoning procedure because backdoor poisoning typically occurs on raw data, which in reality often includes noisy data.\nThe noise generator produces data noise or random labels, synthesizes noisy data, and alters labels accordingly. In this process, we encountered three main challenges: (1) Authenticity: the noise generator must generate realistic noises. (2) Adaptability: the noise generator should be adaptable to different application scenarios, such as image, audio, and text. (3) Controllability: the noise generator needs to be controllable so that users can adjust the noise intensity as needed.\nTo address the authenticity challenge, we chose Gaussian noise as our primary noise generator due to its distribution and widespread use. For adaptability in text noise, we utilized an open-source noise generator called textnoisr 1 to introduce random modifications, deletions, and additions to the original text, simulating natural text noises. For controllability, we used noise ratios to adjust noise intensity in both image and audio data, and the character error rate to control noise levels in text."}, {"title": "5.3 Backdoor Poisoner Design", "content": "The objective of the backdoor poisoner is to execute the poisoning task, which includes trigger generation, sample synthesis, and label modification. We included this component to standardize the process for poisoning-only attacks. The data item is selected by the poison ratio and a random seed. If the index falls within the set of poison indices, the data item should be poisoned before use. Specifically, trigger generation produces backdoor patterns specific to each backdoor attack at first. Then, the backdoor poisoner attaches these patterns to the data item. Finally, if required, the label is changed to the attack target label.\nDuring this process, we encountered two main challenges: (1) the complexity of trigger generation, as each attack requires its trigger generation process. (2) the consideration of training control methods, which differ from poisoning-only attacks as they poison the model during the training process.\nTo address the first challenge, we implemented the trigger generation function for each attack by referring to open-source backdoor attacks. For training control methods, we included a training procedure interface in the backdoor attack wrapper. This allows us to implement unique training processes for each method, which can be called later in the backdoor training module.\nAs a result, our backdoor poisoner serves as an integrated pipeline for poisoning-only attacks and is capable of handling training control backdoor attacks as well."}, {"title": "6 Experiments", "content": "We systematically evaluate existing attacks and defenses, unveiling their performance across various modalities. Furthermore, we benchmark their effectiveness in real-world simulation scenarios, accounting for noisy data and noisy labels. Our objective in the experiment is to address the following three research questions:\n\u2022 Q1: How does the performance of backdoor attacks and defenses are in a multimodal setting?\n\u2022 Q2: What is the impact of noise on both the backdoor attack and defense mechanisms?"}, {"title": "6.1 Experiment Settings", "content": ""}, {"title": "6.1.1 Datasets and Models", "content": "This paper conducts experiments using three datasets (CIFAR-10 [30], SST-2 [63], SpeechCommands [78]) and three backbone models (ResNet, BERT, CNN with 4 conventional layers and 1 full connection layer)."}, {"title": "6.1.2 Attacks and Defenses", "content": "Due to space limitations and training costs, four attacks (BadNets, BPP, SSBA, and WaNet) are selected in our experiments, more results can be accessed at https://anonymous.4open.science/r/BackdoorMBTI-D6A1/README.md. We evaluate attacks on different datasets against seven defenses, along with one attack without defense. Our default poisoning ratio is set at 10%."}, {"title": "6.1.3 Noise settings", "content": "For text data, we employed the character error rate to control noise levels, setting it to 0.1 to generate noisy text. In both audio and image data, we randomly selected 25% of the data and applied Gaussian noise with a mean of 0 and a variance of 1 to simulate noise in adverse environments. Additionally, we randomly changed 25% of the labels to simulate erroneous labels."}, {"title": "6.1.4 Metrics", "content": "The metrics employed in evaluation are outlined as follows: Clean Accuracy (CAC) indicates the classification accuracy of the model when trained using a clean dataset. Backdoor Accuracy (BAC) reflects the classification accuracy when the model is trained using the backdoor dataset. Attack Success Rate (ASR) represents the rate of successful attacks, indicating when the model accepts a sample with a trigger and produces a targeted classification result. Robustness Accuracy (RAC) measures the robustness, demonstrating the ability of the model to provide correct classification results even with a trigger patch applied to the sample. For backdoor detection method, we used Detection Accuracy (DAC), recall (REC) and F1 score as metrics, DAC is the backdoor sample detection accuracy used for evaluating backdoor detection methods."}, {"title": "6.2 Overall Results (Q1)", "content": "Firstly, we show the performance of various attack-defense pairs in Table 5. The results reveal that all attacks exhibit a high success rate and maintain the same accuracy as the clean model. Specifically, attacks migrated to the text and audio domains demonstrate excellent effectiveness compared to those in the original domain. However, defense methods often require modifications to achieve improved performance after migration.\nAll attacks after migration exhibit a significantly high attack success rate, exceeding 80% in general and surpassing 95% specifically for text. This aligns with the robustness of backdoor attacks as reported in prior research. Among the input filtering methods,"}, {"title": "6.3 The Impact of Noise Factors (Q2)", "content": "One of our primary research questions involves investigating backdoor defense performance in a multimodal context within a real-world paradigm. We simulate real-world scenarios using Gaussian noise on data and mislabeling on labels. When employing these noise factors, we observe no significant decline in clean accuracy, with a decrease of around 3% falling within our expected range. As depicted in Figures 2 and 3, migrated attacks demonstrate a high success rate, indicating that noise factors do not adversely affect backdoor attacks. This is consistent with the robust nature of backdoor attacks, where the attachment of a backdoor trigger patch to an input sample leads to mispredictions regardless of the target label, even with noisy input.\nWhile defense methods benefit from noise, multimodal defenses exhibit better migration results under noise. Our findings indicate a performance improvement in defense under noise, as shown by statistical analysis of experiment results. For mislabeled data, the improvement is 3.17%, and for noise data, it is 9.18%. The effectiveness of defense methods is contingent upon the quality of the backdoor model, training with noisy data yields a more robust model, making it easier to mitigate the impact of backdoor attacks. However, there is an exception where noise lowers defense performance in audio data. We believe this is explainable because audio data is concise and contains more compact information."}, {"title": "7 Limititions and Future Work", "content": "Mutilmodal application support. BackdoorMBTI aims to provide a unified benchmark with the potential for easy extensions to new modalities, while the modalities supported (i.e., image, text, audio) are integrated individually now. While multimodal applications like visual question answering are not yet supported, we are actively working on it and plan to release this feature in the future.\nScale of Datasets and Models. We recognize that BackdoorMBTI currently includes only a limited number of representative datasets and models for each modality. Many practical datasets have not"}, {"title": "8 Conclusion", "content": "In this paper, we introduce the first backdoor learning benchmark and toolkit designed specifically for multimodal scenarios, named BackdoorMBTI. This framework facilitates the development and evaluation of backdoor learning techniques in multimodal settings. Additionally, we have created a reproducible benchmark that includes three multimodalities across eleven datasets, providing a basis for future comparisons. Furthermore, we have evaluated the performance of backdoor attack and defense methods under conditions such as low-quality data and erroneous labels in each of these tasks."}, {"title": "A Running Experiments", "content": "Our experiments are conducted on the GPU server with Intel(R) Xeon(R) Silver 4310 CPU @ 2.10GHz, RTX3090 GPU (24GB) and 128 GB RAM. The versions of all software and packages involved are clearly outlined in the requirements.txt file in the BackdoorMBTI code directory."}, {"title": "B Hyper-Parameter Settings", "content": "Details of the hyper-parameter settings used in our evaluations of backdoor attack and defense algorithms are provided in the framework configuration directory. These settings allow for reproducibility of the reported results."}, {"title": "C Additional Information of Implemented Backdoor Attacks", "content": "Here we introduce the implemented backdoor attacks(BadNets [19], BPP [77], SSBA [34], WaNet [51], AddSent [13], SYNBKD [55], LWP [32], Blend [9], DABA [41], GIS [28] and UltraSonic [29]), illustrate the description about what it is and provide necessary information about its implementation."}, {"title": "D Additional Information of Implemented Backdoor Defenses", "content": "Here we introduce the implemented backdoor attacks (AC [5], STRIP [17], FT [40], FP [40], ABL [35], CLP [89] and NC [74]), illustrate the description about what it is and provide necessary information about its implementation.\nAC: Activation Clustering is based on the observation that clean samples and poisoned samples display distinct activation in the final hidden layer. It identifies backdoor sample clusters through hidden layer activation, facilitating backdoor detection. To identify malicious clusters, various methods have been proposed, such as smaller size, relative size, distance, and silhouette scores. In our benchmark, we use the default criterion of smaller clusters to identify backdoor sample clusters, grounded on the observation that poisoned samples constitute only a small portion of the original dataset.\nSTRIP: STRIP introduces substantial perturbations into each input and utilizes entropy to identify poisoned inputs. This method relies on the robust characteristic that inputs containing a backdoor"}]}