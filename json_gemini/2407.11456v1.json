{"title": "Graceful task adaptation with a bi-hemispheric\nRL agent", "authors": ["Grant Nicholas", "Levin Kuhlmann", "Gideon Kowadlo"], "abstract": "In humans, responsibility for performing a task gradually\nshifts from the right hemisphere to the left. The Novelty-Routine Hy-\npothesis (NRH) states that the right and left hemispheres are used to per-\nform novel and routine tasks respectively, enabling us to learn a diverse\nrange of novel tasks while performing the task capably. Drawing on the\nNRH, we develop a reinforcement learning agent with specialised hemi-\nspheres that can exploit generalist knowledge from the right-hemisphere\nto avoid poor initial performance on novel tasks. In addition, we find\nthat this design has minimal impact on its ability to learn novel tasks.\nWe conclude by identifying improvements to our agent and exploring\npotential expansion to the continual learning setting.", "sections": [{"title": "Introduction", "content": "Despite many high-profile successes of Deep Reinforcement Learning (RL), RL-\nagents still struggle with sample efficiency and generalisation [2]. Human beings\ndo not experience such problems and are capable continuous learners, able to\nacquire diverse skills over their lifetimes [12].\nResearch in neuroscience highlights the importance of hemispheric specialisa-\ntion in the human brain for learning [7,8,6,15]. The right-hemisphere is identified\nas focusing on 'global' phenomena, being competent in novel scenarios and for\nexploration, while the left-hemisphere focuses on 'local' phenomena and learns\nto specialise [7,8,6,19]. When a novel task is learnt, a uni-directional shift occurs\nwhereby responsibility for the task moves from right to left hemisphere during\nlearning [8]; the key to the Novelty Routine Hypothesis (NRH) proposed in [8].\nConsequently, we draw on the NRH and related neuroscientific theories to\ndevelop an RL-agent with specialised hemispheres. In doing so, we follow the\napproach of other significant discoveries in AI that use human neuroscience as\ninspiration for AI algorithm development [18,10]. We hypothesise that our bi-\nhemispheric architecture will help RL-agents learn novel tasks, while avoiding\nthe poor initial performance typically observed for agents trained from scratch.\nEssentially, bi-hemispheric agents should perform well in the initial stages of\nlearning by drawing on the generalist right-hemisphere, while training the left-\nhemisphere to specialise in the task. This may have benefits for continual learn-\ning, where agents encounter streams of novel tasks."}, {"title": "Agent design", "content": "We constructed a bi-hemispheric agent inspired by the NRH; a right-hemisphere\nhas generalist capabilities and a left-hemisphere learns to specialise in tasks.\nWhen encountering a novel task, a gating network assigns responsibility to either\nthe right or left hemisphere to manage performance. The agent should draw on\ngeneralist skills in the right-hemisphere to achieve improved initial performance\nover an agent trained from scratch. This should not interfere with using the\nleft-hemisphere to learn and eventually perform the novel task independently.\nHence, the agent is assessed on the objectives:\n1. initial performance better than an agent trained from scratch\n2. final left-hemisphere performance as good as an agent trained from scratch"}, {"title": "Network architecture", "content": "The bi-hemispheric agent consists of two hemispheres and a gating network,\neach comprising a Recurrent Neural Network (RNN), with Gated Recurrent\nUnits (GRU). Each hemisphere outputs an action and a value estimate. For\nsimplicity, we give right and left hemispheres identical architectures and omit\ninter-hemispheric connections (i.e. Corpus Callosum), despite being neurologi-\ncally inaccurate [9,3,19,6]. The network architecture is shown in Figure S.??."}, {"title": "Gating network", "content": "The gating network assigns 'responsibility' to each hemisphere using 'gating\nvalues', defined as the proportional contribution of that hemisphere to the agent's\naction and value. The responsibility of the right-hemisphere is thus:\n$p_{right} = 1 - p_{left}$ where $p_{right}, p_{left} \\in [0,1]$\n(1)\nThe value estimate is a linear combination of the right and left hemisphere\nvalue outputs, weighted by the responsibility of each hemisphere:\n$V^{bihem} = p_{right}V^{right} + p_{left}V^{left}$\n(2)"}, {"title": "Generalisation and specialisation", "content": "We trained right and left hemispheres separately using different training pro-\ncesses to induce generalisation and specialisation respectively. First, we trained\nthe right-hemisphere using the RL2 meta-learning algorithm [4,26]. Second, we\nconstructed the bi-hemispheric network by freezing the weights of the right-\nhemisphere and then combined it with a randomly initialised left-hemisphere and\ngating network. Finally, the left-hemisphere and gating networks were trained\ntogether using the standard RL objective, which maximises expected discounted\nrewards [24]. The left-hemisphere 'specialises' in a task simply by being trained\nto perform that specific task.\nWe used meta-learning for the right, as it offers both the ability to gener-\nalise to a task-distribution and adapt quickly to novel tasks [5]; both important\nfor strong initial performance. RL2 is exceptional at the latter sometimes\nachieving zero-shot adaptation [4,26,13]. In addition RL2 is a memory-based\nmeta-RL algorithm, which have been identified as Bayesian-optimal learners as\nthey can optimally trade off exploration and exploitation in uncertain environ-\nments [30,17]. This fits with the NRH's view of the right-hemisphere as guiding\nexploratory behaviour when learning novel tasks [8]."}, {"title": "Experiments", "content": "Our implementation is available at https://github.com/gdubbs100/right_\nleft_brain_rl The experiments were carried out in two stages, shown in Fig-\nure 1. We first meta-trained the right-hemisphere and a baseline agent on 3\ntasks. We then created the bi-hemispheric agent and trained and evaluated all\nagents on an expanded set of tasks over 5 seeds."}, {"title": "Dataset", "content": "We used Meta-world [29], a meta-learning/multi-task benchmark comprised of\n50 tasks that involve manipulation of objects using a Sawyer robot arm. We chose\nMeta-world because of its diverse tasks with shared action and state spaces.\nMeta-world tasks are designed to vary in a non-trivial manner, to allow the\nevaluation of generalisation capabilities in RL-agents. Meta-world defines dif-\nferences between tasks as parametric or non-parametric. Parametric variation\ninvolves variation in real-valued parameters such as object or goal locations, e.g.\nthe location of a ball where the task objective is for an arm to reach said ball.\nIn contrast, non-parametric variation involves qualitative differences between\ntasks, e.g. the difference between opening a door and lifting a ball [29]. Each\nof Meta-world's 50 tasks exhibit non-parametric variation between each other.\nFurther, each task is comprised of 50 sub-tasks which exhibit parametric varia-\ntion between each other. We use Meta-world's definitions of variation to divide\nthe experiment tasks into three tiers (Section S.??) and classify the novelty of a\ntask relative to the meta-training tasks:\n1. Tier-1 tasks have only parametric variation i.e. sub-tasks with different goal\nand object locations.\n2. Tier-2 tasks include similar tasks to Tier-1, but with minor non-parametric\nvariation e.g. the addition of a wall obstacle to a task from meta-training.\n3. Tier-3 tasks exhibit complete non-parametric variation from Tier-1. These\ntasks help us understand the limits of our approach."}, {"title": "Baselines", "content": "We evaluated three baselines. First, the 'left-only' baseline is a randomly ini-\ntialised agent with the same architecture as the left-hemisphere of the bi-hemispheric"}, {"title": "Training approach", "content": "Meta-training We meta-trained the right-hemisphere and right-only baseline\nusing the RL2 algorithm and PPO in the 'outer-loop'. We used three tasks from\nTier-1 and trained each agent for 50 million environment steps using identical hy-\nperparameters except for GRU size. Our meta-training approach was simplified\ncompared to Meta-world, see Section S.??.\nBi-hemispheric training For the main experiment, we selected nine Meta-\nworld tasks from Tier-1, 2 and 3, disjoint from Meta-training sub-tasks, to train\nand evaluate bi-hemispheric agents and the left-only baseline. These tasks ex-\nhibit varying degrees of novelty from the tasks used in meta-training. For each\ntask, we trained for 5 million environment steps using the PPO algorithm [22], as\nthis timeframe allowed agents to learn each task. During training, we extracted\nmean rewards and median gating values from each batch. For bi-hemispheric\nagents, we also tested the left-hemisphere network as an independent agent. We\nmade simplifications to training compared to Meta-world, following the Contin-\nual World approach [28]; randomly sampling from 20 tasks (instead of 50) and\nmaking goal and object positions observable, see Section S.??.\nHyperparameter selection For each task, we used identical hyperparame-\nters for the left-only baseline and all bi-hemispheric agents. We selected hyper-\nparameters that enabled best performance for the left-only baseline. This is a\nconservative approach that enabled us to evaluate bi-hemispheric agents against\nthe left-only baselines at their best. Hyperparameter selection was informed by\nvalues used in Meta-world and previous studies into PPO settings [29,1,16]. Gat-\ning network parameters were chosen for good final left-hemisphere performance.\nWe also used Meta-world hyperparameter values to inform settings for meta-\ntraining. Hyperparameters are in given in Section S.??."}, {"title": "Main experiments", "content": "We compared bi-hemispheric agents against baselines using 'relative rewards';\nthe ratio between rewards of two agents. A relative reward of > 1 indicates\nsuperior performance over the agent in the denominator. To assess bi-hemispheric\nperformance against Objectives 1 and 2, we placed bi-hemispheric agent reward\nin the numerator and the left-only baseline reward in the denominator."}, {"title": "Results", "content": "This section focuses on the Main experiment, for meta-training, see Section S.??.\nFor reference, per-step rewards for Meta-world tasks are between 0 and 10 where\n10 occurs when the agent successfully achieves the specified goal of the task [29].\nGiven our objectives, we focus on rewards. However, Meta-world tasks are often\nevaluated on how often an agent achieves task success, which we include in a\nrange of additional results plots in Section S.??.\nFigure 2 shows the mean rewards achieved for bi-hemispheric agents and\nbaselines. For the bi-hemispheric agent and left-only baseline, mean rewards are\nsmoothed using a rolling median over one million environment steps. Right-\nonly and Random baseline median rewards are calculated by sampling sub-tasks\nwith replacement. We used this approach as these baselines do not learn during\nevaluation, hence sampled tasks can be treated as independent.\nWe identify three groups of results. The first group consists of the tasks\nreach-v2, push-v2, reach-wall-v2 and push-wall-v2. Here, the bi-hemispheric\nagent outperforms the left-only baseline initially and achieves comparable or bet-\nter performance over the whole period. Tasks in this group are tasks where the\nright-hemisphere performs strongly.\nThe second group consists of pick-place-v2 and bin-picking-v2. For these\ntasks, the left-only baseline generally outperforms the bi-hemispheric agent. In\nthis group, overall rewards achieved for all agents are not very high, especially for\nbin-picking-v2, and vary across seeds. Notably, right-hemisphere performance\nis also poor on these tasks. This is consistent with Meta-world benchmark results,\nwhich show PPO agents fail to learn bin-picking, and that meta-learning agents\ntake longer to learn pick-place-v2 compared to reach-v2 and push-v2 [29]."}, {"title": "Discussion", "content": "The key finding is that when the right-hemisphere contains generalist skills rel-\nevant to a task, the agent can exploit them to improve initial performance on\nthat task. Furthermore, doing so does not significantly impact ability to learn\nthe task. This was achieved for tasks which exhibit non-parametric variation\nfrom the meta-training tasks. However, this was not so for most tasks. With-\nout a competent right-hemisphere, bi-hemispheric agents generally exhibit worse\ninitial performance and struggle to learn tasks to the same degree as an agent"}, {"title": "Conclusion", "content": "We developed a novel RL-agent design with specialised brain hemispheres as per\nthe NRH. Our agent can exploit generalist skills, when present, from the right-\nhemisphere to improve initial performance over an agent trained from scratch,"}]}