{"title": "Graceful task adaptation with a bi-hemispheric RL agent", "authors": ["Grant Nicholas", "Levin Kuhlmann", "Gideon Kowadlo"], "abstract": "In humans, responsibility for performing a task gradually shifts from the right hemisphere to the left. The Novelty-Routine Hypothesis (NRH) states that the right and left hemispheres are used to perform novel and routine tasks respectively, enabling us to learn a diverse range of novel tasks while performing the task capably. Drawing on the NRH, we develop a reinforcement learning agent with specialised hemispheres that can exploit generalist knowledge from the right-hemisphere to avoid poor initial performance on novel tasks. In addition, we find that this design has minimal impact on its ability to learn novel tasks. We conclude by identifying improvements to our agent and exploring potential expansion to the continual learning setting.", "sections": [{"title": "1 Introduction", "content": "Despite many high-profile successes of Deep Reinforcement Learning (RL), RL- agents still struggle with sample efficiency and generalisation [2]. Human beings do not experience such problems and are capable continuous learners, able to acquire diverse skills over their lifetimes [12].\nResearch in neuroscience highlights the importance of hemispheric specialisa- tion in the human brain for learning [7,8,6,15]. The right-hemisphere is identified as focusing on 'global' phenomena, being competent in novel scenarios and for exploration, while the left-hemisphere focuses on 'local' phenomena and learns to specialise [7,8,6,19]. When a novel task is learnt, a uni-directional shift occurs whereby responsibility for the task moves from right to left hemisphere during learning [8]; the key to the Novelty Routine Hypothesis (NRH) proposed in [8].\nConsequently, we draw on the NRH and related neuroscientific theories to develop an RL-agent with specialised hemispheres. In doing so, we follow the approach of other significant discoveries in AI that use human neuroscience as inspiration for AI algorithm development [18,10]. We hypothesise that our bi- hemispheric architecture will help RL-agents learn novel tasks, while avoiding the poor initial performance typically observed for agents trained from scratch. Essentially, bi-hemispheric agents should perform well in the initial stages of learning by drawing on the generalist right-hemisphere, while training the left- hemisphere to specialise in the task. This may have benefits for continual learn- ing, where agents encounter streams of novel tasks."}, {"title": "2 Agent design", "content": "We constructed a bi-hemispheric agent inspired by the NRH; a right-hemisphere has generalist capabilities and a left-hemisphere learns to specialise in tasks. When encountering a novel task, a gating network assigns responsibility to either the right or left hemisphere to manage performance. The agent should draw on generalist skills in the right-hemisphere to achieve improved initial performance over an agent trained from scratch. This should not interfere with using the left-hemisphere to learn and eventually perform the novel task independently. Hence, the agent is assessed on the objectives:\n1. initial performance better than an agent trained from scratch\n2. final left-hemisphere performance as good as an agent trained from scratch"}, {"title": "2.1 Network architecture", "content": "The bi-hemispheric agent consists of two hemispheres and a gating network, each comprising a Recurrent Neural Network (RNN), with Gated Recurrent Units (GRU). Each hemisphere outputs an action and a value estimate. For simplicity, we give right and left hemispheres identical architectures and omit inter-hemispheric connections (i.e. Corpus Callosum), despite being neurologi- cally inaccurate [9,3,19,6]. The network architecture is shown in Figure S.??."}, {"title": "2.2 Gating network", "content": "The gating network assigns 'responsibility' to each hemisphere using 'gating values', defined as the proportional contribution of that hemisphere to the agent's action and value. The responsibility of the right-hemisphere is thus:\n$p_{right} = 1 - p_{left}$ where $P_{right}, p_{left} \\in [0,1]$\n(1)\nThe value estimate is a linear combination of the right and left hemisphere value outputs, weighted by the responsibility of each hemisphere:\n$V^{bihem} = p_{right}V_{right} + p_{left}V_{left}$\n(2)"}, {"title": "2.3 Generalisation and specialisation", "content": "We trained right and left hemispheres separately using different training pro- cesses to induce generalisation and specialisation respectively. First, we trained the right-hemisphere using the RL2 meta-learning algorithm [4,26]. Second, we constructed the bi-hemispheric network by freezing the weights of the right- hemisphere and then combined it with a randomly initialised left-hemisphere and gating network. Finally, the left-hemisphere and gating networks were trained together using the standard RL objective, which maximises expected discounted rewards [24]. The left-hemisphere 'specialises' in a task simply by being trained to perform that specific task.\nWe used meta-learning for the right, as it offers both the ability to gener- alise to a task-distribution and adapt quickly to novel tasks [5]; both important for strong initial performance. RL2 is exceptional at the latter sometimes achieving zero-shot adaptation [4,26,13]. In addition RL2 is a memory-based meta-RL algorithm, which have been identified as Bayesian-optimal learners as they can optimally trade off exploration and exploitation in uncertain environ- ments [30,17]. This fits with the NRH's view of the right-hemisphere as guiding exploratory behaviour when learning novel tasks [8]."}, {"title": "3 Experiments", "content": "Our implementation is available at https://github.com/gdubbs100/right_ left_brain_rl The experiments were carried out in two stages, shown in Fig- ure 1. We first meta-trained the right-hemisphere and a baseline agent on 3 tasks. We then created the bi-hemispheric agent and trained and evaluated all agents on an expanded set of tasks over 5 seeds."}, {"title": "3.1 Dataset", "content": "We used Meta-world [29], a meta-learning/multi-task benchmark comprised of 50 tasks that involve manipulation of objects using a Sawyer robot arm. We chose Meta-world because of its diverse tasks with shared action and state spaces.\nMeta-world tasks are designed to vary in a non-trivial manner, to allow the evaluation of generalisation capabilities in RL-agents. Meta-world defines dif- ferences between tasks as parametric or non-parametric. Parametric variation involves variation in real-valued parameters such as object or goal locations, e.g. the location of a ball where the task objective is for an arm to reach said ball. In contrast, non-parametric variation involves qualitative differences between tasks, e.g. the difference between opening a door and lifting a ball [29]. Each of Meta-world's 50 tasks exhibit non-parametric variation between each other. Further, each task is comprised of 50 sub-tasks which exhibit parametric varia- tion between each other. We use Meta-world's definitions of variation to divide the experiment tasks into three tiers (Section S.??) and classify the novelty of a task relative to the meta-training tasks:\n1. Tier-1 tasks have only parametric variation i.e. sub-tasks with different goal and object locations.\n2. Tier-2 tasks include similar tasks to Tier-1, but with minor non-parametric variation e.g. the addition of a wall obstacle to a task from meta-training.\n3. Tier-3 tasks exhibit complete non-parametric variation from Tier-1. These tasks help us understand the limits of our approach."}, {"title": "3.2 Baselines", "content": "We evaluated three baselines. First, the 'left-only' baseline is a randomly ini- tialised agent with the same architecture as the left-hemisphere of the bi-hemispheric"}, {"title": "3.3 Training approach", "content": "Meta-training We meta-trained the right-hemisphere and right-only baseline using the RL2 algorithm and PPO in the 'outer-loop'. We used three tasks from Tier-1 and trained each agent for 50 million environment steps using identical hy- perparameters except for GRU size. Our meta-training approach was simplified compared to Meta-world, see Section S.??.\nBi-hemispheric training For the main experiment, we selected nine Meta- world tasks from Tier-1, 2 and 3, disjoint from Meta-training sub-tasks, to train and evaluate bi-hemispheric agents and the left-only baseline. These tasks ex- hibit varying degrees of novelty from the tasks used in meta-training. For each task, we trained for 5 million environment steps using the PPO algorithm [22], as this timeframe allowed agents to learn each task. During training, we extracted mean rewards and median gating values from each batch. For bi-hemispheric agents, we also tested the left-hemisphere network as an independent agent. We made simplifications to training compared to Meta-world, following the Contin- ual World approach [28]; randomly sampling from 20 tasks (instead of 50) and making goal and object positions observable, see Section S.??.\nHyperparameter selection For each task, we used identical hyperparame- ters for the left-only baseline and all bi-hemispheric agents. We selected hyper- parameters that enabled best performance for the left-only baseline. This is a conservative approach that enabled us to evaluate bi-hemispheric agents against the left-only baselines at their best. Hyperparameter selection was informed by values used in Meta-world and previous studies into PPO settings [29,1,16]. Gat- ing network parameters were chosen for good final left-hemisphere performance. We also used Meta-world hyperparameter values to inform settings for meta- training. Hyperparameters are in given in Section S.??."}, {"title": "3.4 Main experiments", "content": "We compared bi-hemispheric agents against baselines using 'relative rewards'; the ratio between rewards of two agents. A relative reward of > 1 indicates superior performance over the agent in the denominator. To assess bi-hemispheric performance against Objectives 1 and 2, we placed bi-hemispheric agent reward in the numerator and the left-only baseline reward in the denominator."}, {"title": "4 Results", "content": "This section focuses on the Main experiment, for meta-training, see Section S.??. For reference, per-step rewards for Meta-world tasks are between 0 and 10 where 10 occurs when the agent successfully achieves the specified goal of the task [29].\nGiven our objectives, we focus on rewards. However, Meta-world tasks are often evaluated on how often an agent achieves task success, which we include in a range of additional results plots in Section S.??.\nshows the mean rewards achieved for bi-hemispheric agents and baselines. For the bi-hemispheric agent and left-only baseline, mean rewards are smoothed using a rolling median over one million environment steps. Right- only and Random baseline median rewards are calculated by sampling sub-tasks with replacement. We used this approach as these baselines do not learn during evaluation, hence sampled tasks can be treated as independent.\nWe identify three groups of results. The first group consists of the tasks reach-v2, push-v2, reach-wall-v2 and push-wall-v2. Here, the bi-hemispheric agent outperforms the left-only baseline initially and achieves comparable or bet- ter performance over the whole period. Tasks in this group are tasks where the right-hemisphere performs strongly.\nThe second group consists of pick-place-v2 and bin-picking-v2. For these tasks, the left-only baseline generally outperforms the bi-hemispheric agent. In this group, overall rewards achieved for all agents are not very high, especially for bin-picking-v2, and vary across seeds. Notably, right-hemisphere performance is also poor on these tasks. This is consistent with Meta-world benchmark results, which show PPO agents fail to learn bin-picking, and that meta-learning agents take longer to learn pick-place-v2 compared to reach-v2 and push-v2 [29]."}, {"title": "Objective 1: Initial performance", "content": "Objective 1: Initial performance Figure 3 plots IRR over tasks and seeds. Consistent with Figure 2 we see that the bi-hemispheric agent achieves IRR scores larger than 1 for the reach and push families of tasks across different seeds. For other tasks, bi-hemispheric performance does not exceed the left- only baseline. For pick-place-v2 on some seeds the bi-hemispheric agent out- performs the left-only baseline. However, given the initial rewards achieved for pick-place-v2 are small, this may drive the differences in IRR scores, rather than genuinely improved performance."}, {"title": "Objective 2: Hemispheric shift", "content": "Objective 2: Hemispheric shift Figure 4 plots FRR across tasks and seeds. Scores greater than 1 indicate that the final performance of the left hemisphere exceeded the final performance of the left-only baseline. The bi-hemispheric agent only achieved FRR of greater than 1 on push-v2 and push-wall-v2. The bi-hemispheric agent achieved FRR close to 1 for reach tasks and for faucet-open-v2. We observed FRR scores well below 1 for other tasks."}, {"title": "Combining Objectives 1 and 2", "content": "Combining Objectives 1 and 2 Figure 5 plots median IRR and FRR scores. Only push-v2 and push-wall-v2 fell within the upper right quadrant. Reach-v2 and reach-wall-v2 achieve IRR scored greater than 1, but have FRR scores just below 1. However, all other tasks were located in the lower left quadrant. What differentiated the tasks in the bottom-left quadrant from those near to the upper-right is strong right-hemisphere performance. Essentially, when right- hemisphere performed tasks, bi-hemispheric agents achieved stronger IRR and FRR scores."}, {"title": "5 Discussion", "content": "The key finding is that when the right-hemisphere contains generalist skills rel- evant to a task, the agent can exploit them to improve initial performance on that task. Furthermore, doing so does not significantly impact ability to learn the task. This was achieved for tasks which exhibit non-parametric variation from the meta-training tasks. However, this was not so for most tasks. With- out a competent right-hemisphere, bi-hemispheric agents generally exhibit worse initial performance and struggle to learn tasks to the same degree as an agent"}, {"title": "Improved meta-training", "content": "Improved meta-training Our approach to meta-training the right-hemisphere involved several simplifications (Section S.??), including training on fewer tasks and for fewer environment steps. Increasing either of these may result in more ro- bust right-hemispheres with greater generalisation capacity. Additionally, we saw doubling the GRU size in our right-only baseline resulted in large improvements in performance. Consequently, improving the right-hemisphere meta-training process may have significant benefits for bi-hemispheric agent performance."}, {"title": "Training the left-hemisphere and gating network separately", "content": "Training the left-hemisphere and gating network separately Partic- ularly for the Tier-3 tasks, we observed training trajectories that were quite variable (see Figure 2). Some trajectories demonstrated reasonable growth while others exhibited long flat periods without any improvement. We hypothesise that this is due to interference between gating values and left-hemisphere gradients. Essentially, left-hemisphere gradients are scaled down by the gating values, which act as a dynamic learning rate. This may make learning a novel task difficult for the left-hemisphere if the right-hemisphere performs poorly.\nOn solution could be to separate the training of the gating network and left-hemisphere. This could be achieved by training each network with an off- policy algorithm. Off-policy algorithms use different policies for exploration and exploitation [24]. Consequently, the bi-hemispheric agent's joint policy would be used to explore while the left-hemisphere and gating network could each be trained separately, using importance sampling to treat the bi-hemispheric agent's decisions as their own."}, {"title": "Extension to continual learning", "content": "Extension to continual learning Applying bi-hemispheric agents in continual learning may be where their capabilities become more beneficial. If agents avoid poor initial performance on novel tasks, yet still learn, it would result in better overall performance. For our agent to operate in this setting, it should store learnt policies in the left-hemisphere, infer whether a task has an existing policy or is novel, and retrieve existing policies for previously encountered tasks from the left-hemisphere. We leave this to future work, but note that the adaptive MoE proposed by Tsuda et al. [25] may help address the issue of task retrieval. They used gating networks to select which expert to use for novel tasks. This design could be applied as a left-hemisphere alongside a meta-trained right-hemisphere."}, {"title": "6 Conclusion", "content": "We developed a novel RL-agent design with specialised brain hemispheres as per the NRH. Our agent can exploit generalist skills, when present, from the right- hemisphere to improve initial performance over an agent trained from scratch,"}]}