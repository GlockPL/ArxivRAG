{"title": "SEAL: Suite for Evaluating API-use of LLMs", "authors": ["Woojeong Kim", "Ashish Jagmohan", "Aditya Vempaty"], "abstract": "Large language models (LLMs) have limitations in handling tasks that require real-time access to external APIs. While several benchmarks like ToolBench and APIGen have been developed to assess LLMs' API-use capabilities, they often suffer from issues such as lack of generalizability, limited multi-step reasoning coverage, and instability due to real-time API fluctuations. In this paper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in real-world API usage. SEAL standardizes existing benchmarks, integrates an agent system for testing API retrieval and planning, and addresses the instability of real-time APIs by introducing a GPT-4-powered API simulator with caching for deterministic evaluations. Our testbed provides a comprehensive evaluation pipeline that covers API retrieval, API calls, and final responses, offering a reliable framework for structured performance comparison in diverse real-world scenarios. SEAL is publicly available, with ongoing updates for new benchmarks.", "sections": [{"title": "1 Introduction", "content": "While large language models (LLMs) excel at many language tasks, they face limitations when handling tasks that require real-time access to specific information, such as current events, calculations, or web searches. Tools like calculators, code execution, and browsing extend LLMs' capabilities, enabling them to perform specialized tasks and access up-to-date knowledge, adapting dynamically to users' needs. Recently, several benchmarks have been introduced to assess LLMs' ability to interact with real-world APIs, moving beyond a small set of hand-coded tools to a broader pool of practical, real-world applications. These include ToolBench [Qin et al., 2023] and variants thereof, APIGen [Chen et al., 2024], AnyTool [Du et al., 2024] and MetaTool [Huang et al., 2023].\nIn this paper, we analyze major API-use\u00b9 benchmarks and argue that there are critical gaps. Specifically, we identify a number of common issues, including the lack of clear holdout sets leading to overfitting, poor coverage of multi-step reasoning queries which are essential in real-world use cases of such systems, and deficiencies in benchmark quality and stability. Further, some benchmarks only focus on certain aspects; For example, AnyTool [Du et al., 2024] and MetaTool [Huang et al., 2023] focus on tool selection but overlook other important aspects such as the content of tool calls and the final response. Similarly, APIGen [Chen et al., 2024] tests function-calling capabilities but neglects multiple possible trajectories with only one hard-coded answer.\nIn response, we present SEAL, a comprehensive, end-to-end testbed for evaluating LLMs in tool usage, particularly with diverse real-world APIs. This testbed provides uniform test environment of API calling system by sanitizing and standardizing existing benchmarks, incorporating an agent system built on AutoGen [Wu et al., 2023] for testing both API retrieval and planning, and providing a robust evaluation pipeline. Due to the real-time nature of APIs, existing benchmarks often lack reliable evaluation as there are no static ground truth answers. To address this, we developed an API\nWe use \"tool\u201d and \u201cAPI\u201d interchangeably throughout this paper. While we primarily focus on APIs, the same methodology can be extended to general tool usage with appropriate API-wrapper around them."}, {"title": "2 Overall Landscape", "content": "Fig. 1 provides an overview of the typical workflow for tool-use systems. These systems consist of three main components: the user, the agent system, and the API server. The process begins when a user queries the agent system, for example, \"How's the weather tomorrow in NYC?\u201d The next step typically is a retrieval to identify relevant APIs for the language model that powers the agentic system. This is especially useful in scenarios with numerous real-world APIs, where providing all available APIs as input is impractical due to context limitations (such as length, and lost-in-the-middle phenomenon). The LLM then generates the arguments for tool calls based on the API documentation or the tool specifications and forwards them to the API server. Typically, the API server operates as a separate component outside the agent system. Once the API response is received, the agent system's final role is to generate a response summarizing the API's output. While this example involves a single-step process, in many real-world cases, these steps are repeated, with each step's"}, {"title": "3 Challenges in Existing Benchmarks", "content": "We conduct an in-depth evaluation of existing benchmarks and identified several critical limitations that restrict their wider applicability.  presents key statistics for five benchmarks we focus on. While these are the primary benchmarks under discussion, there are additional ones within the scope that we plan to further support. Our analysis highlights the following four significant limitations."}, {"title": "3.1 Lack of Generalizability", "content": "Previous works have claimed that small, open-source models trained on specific benchmarks can perform as well as, or better than, general-purpose models. However, as noted by Kapoor et al. [2024], many agent benchmarks lack adequate holdout sets, or even fail to include them entirely. This is also true for API-calling benchmarks. For instance, APIGen does not provide a clear train-test split, making it difficult to evaluate models using the provided benchmark data. Similarly, ToolBench includes a very small test set that fails to represent the diversity of real-world scenarios. This limitation in benchmark design often results in models that perform well on narrow datasets but struggle with generalization. We illustrate this through an example of API retrievers, as retrieving"}, {"title": "3.2 Bias Towards Simple Queries", "content": "To better simulate realistic scenarios, we emphasize the need for multi-tool and multi-step benchmarks. As state-of-the-art LLMs become increasingly adept at basic function calls [Qu et al., 2024], it is crucial to assess their ability to decompose complex user queries into smaller, executable substeps and plan actions accordingly. Unfortunately, most existing benchmarks consist predominantly of single-step queries, where a single API call is sufficient to complete the task, as shown in Fig 1. Notably, one of the most widely-used benchmarks, APIBench [Patil et al., 2023], consists solely of single-step and single-tool queries. Existing benchmarks also lack realistic queries that require sequentially dependent reasoning. We show a few examples in the Appendix A.2."}, {"title": "3.3 General Instability", "content": "A significant issue with current benchmarks is their instability, as static ground truth quickly becomes outdated, and API services exhibit variability. API responses are time-sensitive, and services change over time due to factors like deprecation, shifts in service definitions, and altered response behaviors. This instability makes it difficult to evaluate new systems on older benchmarks, hindering efforts to standardize evaluation. To mitigate these challenges, several benchmarks restrict themselves to small sets of hard-coded and deterministic tools [Schick et al., 2024, Lu et al., 2024], instead of dynamic real-world APIs. Other benchmarks limit themselves in both API pool size and the number of queries.\nOne widely used benchmark for API-use, that does not limit its scope as above, is the ToolBench benchmark. Despite the efforts of ToolBench to enhance stability by introducing a proxy server for RapidAPI, which simplifies access by handling authentication and overhead associated with real-time APIs, we found ToolBench to be highly unstable. Many APIs fail to return consistent responses and frequently produce a variety of errors. Previous work [Guo et al., 2024] highlighted ToolBench's instability and proposed solutions like caching API responses and using a GPT-4-based simulator for unresponsive APIs. However, these fixes were limited to a small subset of ToolBench, and running LLM simulators on demand still introduces stochastic variability."}, {"title": "3.4 Incomplete Evaluation", "content": "For a comprehensive evaluation of API-calling systems, it is essential to assess each stage of the pipeline: whether the correct tools are retrieved, whether the correct tools are called, whether the tool calls are accurate, and finally, whether the query response is correct. The real-time nature of APIs further complicates the evaluation of API-use systems, and existing works focus only on partial components of the overall pipeline described in Section 2. As shown in Table 2, AnyTool and MetaTool only evaluate API retrieval, while APIGen evaluates both API retrieval and calls but overlooks the fact that multiple trajectories can lead to a successful final response. Although ToolBench aims to provide ground truth across the entire pipeline, the provided ground truth labels for tool calls and final responses is somewhat outdated and unreliable. Approximately 40% of queries end up as \"unsolvable,\" where the LLM used in ToolBench's own implementation fails to generate a"}, {"title": "4 SEAL Construction", "content": "SEALis a comprehensive, end-to-end testbed that builds on top of current benchmarks. It includes standardization and sanitization of queries from existing benchmarks, a flexible agent system capable of adapting to user demands, and a thorough evaluation pipeline."}, {"title": "4.1 Benchmark Standardization & Sanitization", "content": "We parse and standardize five existing benchmarks introduced in Table 1, into a unified format consisting of queries, APIs, query-to-API mapping, and query-to-API-call mapping. Note that API-call data is available for only 3 out of the 5 datasets, so this field remains empty for AnyTool and MetaTool. This standardized format enables the use of multiple benchmarks with diverse structures via a unified approach. For instance, users can search for APIs associated with a specific query ID or retrieve an API by its name or features. Data sanitization and filtering details are in the Appendix A.1."}, {"title": "4.2 Agent System Construction", "content": "Previous works have developed single LLMs to handle every step of the API-calling process. However, we argue that API-calling, and tool-calling in general, can be tackled more effectively using an agent-based system, where multiple agents collaborate, with some powered by LLMs. This design of separation allows for testing both tool retrieval and tool planning methods, enabling users to easily swap different agents in and out. It also opens up the possibility of replacing agents with smaller, specialized models, rather than relying on expensive general models to handle everything. We develop an adaptable agent system based on the AutoGen [Wu et al., 2023] framework. One of the key advantages of AutoGen is its customizable agents, which can operate in various modes by leveraging combinations of LLMs, human inputs, and tools/APIs. This flexibility allows the system to dynamically adjust agent behaviors based on user-specified configurations.\nFig. 3 illustrates the current architecture of SEAL. We follow the workflow of existing systems, where APIs are pre-selected and registered within the agent system. Within this system, API-calling is managed through a \"group chat\" interaction between a API Executor and a API Execution Manager. The API Execution Manager, powered by an LLM, generates the necessary API calls and arguments, while the API Executor communicates with a API Simulator in the background to simulate real API servers. Although this represents the simplest version of the architecture, the system is designed to allow easy addition or removal of agents. For instance, one could add a Planner Agent under the Orchestrator to break down multi-step queries into smaller sub-steps or introduce a Verifier Agent to ensure that each step has been executed correctly.\nAPI Retriever Given the large size of the API pool in API-calling benchmarks, it is impractical to register all APIs with an LLM due to context length limits. We implement an API retriever as a separate class, allowing users to specify the embedding model of their choice. Available"}, {"title": "4.3 Evaluation Pipeline", "content": "Previous works evaluate one of the following: (1) the final response of the system [Qin et al., 2023], considering that multiple reasoning trajectories can lead to success, or (2) a single hard-coded trajectory by checking if the API calls are correct [Chen et al., 2024]. We argue that a more comprehensive evaluation is needed, spanning the entire pipeline of API usage, including API retrieval, API calls, and the final response.\n\u2022 API Retrieval: Did the system retrieve the correct tools? All five benchmarks listed in Table 2 provide ground truth for tool retrieval. We leverage this information and use standard retrieval metrics, such as Recall@K and Mean Reciprocal Rank (MRR), where K = 10.\n\u2022 API Call: Were the correct tools called, and were the parameters passed accurately? We assess tool call performance by measuring recall, ignoring the order of calls and treating repeated calls to the same tool as distinct events. To evaluate API arguments, we use tool call accuracy, determining whether parameters and values were an exact match. Additionally, more fine-grained metrics, such as matching parameter names or values, can be employed as in Trivedi et al. [2024].\n\u2022 Final Response: Did the system fully address the user query, and was the final response accurate? We adopt the AI critique methodology from Qin et al. [2023] to compute Pass Rate. First, an LLM assesses whether the query was successfully executed by analyzing the final response and categorizing it as solved, unsolved, or unsure. If unsure, it re-evaluates with the entire execution trajectory."}, {"title": "5 Results & Analysis", "content": "We evaluate SEAL using four key metrics: Recall@10 for API retrieval, Recall@N for API calls (varying N as the number of ground truth APIs), API parameter accuracy, and the pass rate of the final response. gpt-4-turbo-2024-04-09 serves as a single model handling API calling, parameter generation, and response summarization.\n presents results from ToolBench and APIGen evaluations. We progressively increase the size of the API pool to assess whether task complexity scales with API pool size. As the API pool grows, the associated queries also vary. Our results indicate that as the API pool expands, performance declines in both API retrieval and call accuracy, demonstrating that the task becomes more challenging with more APIs. However, this decline is less evident in the pass rate. We also conduct three sampling runs with API pool sizes of 10, 50, and 100. The standard deviation across these runs is substantial, particularly in ToolBench. This is due to the limited number of queries associated with each API, causing each sampling to yield significantly different query distributions. In contrast, APIGen exhibit smaller but still notable variance. We plan to increase the number of bootstrapping runs in future work for more reliable and scalable evaluation.\nWe identify three major error cases:\nIncorrect API Retrieval The most common error is partially solved queries due to incorrect API retrieval, especially for multi-tool queries. Current vector lookup systems struggle to retrieve multiple relevant APIs. This underscores the need for multi-step, hierarchical planning to break down complex queries into sub-steps for more accurate API retrieval and resolution. Below is an example where the system partially solves a query because it fails to retrieve an API for fetching technical specifications of a film.\n\u2022 Query: What are the technical specifications of the film 'tt0133093' and search for news in Italian about 'tecnologia'.\n\u2022 Final Answer:\n# Technical Specifications of the Film 'The Matrix' (tt0133093)\nThere was an error retrieving the technical specifications for the film 'The Matrix' (tt0133093).\n# News in Italian about 'Tecnologia'\n1. **Tech Trends in Cinema: How Technology is Shaping the Future of Film** An in-depth article exploring the latest technological advancements in the film industry and their impact on movie production and viewing experiences...\n\u2022 Result: Unsolved\n\u2022 Reason: The answer does not provide the technical specifications of the film 'tt0133093' as requested in the query. It does, however, provide news articles in Italian about 'tecnologia'.\nIncorrect API Parameters Another frequent errors are incorrect API parameters. This highlights the challenge LLMs face in correctly extracting relevant information from queries. For example, below is a case where the system incorrectly identifies latitude and longitude values.\n\u2022 Query: Can you find the time zone for a Canadian postal code G2J and a latitude-longitude pair of 48.8567, 2.3508?\n\u2022 Final Answer: I successfully retrieved the geographical boundary information for the Canadian postal code FSA \"G2J,\" which corresponds to a region in Quebec City, Quebec, Canada. However, there was an error in fetching the time zone for the latitude-longitude pair (48.8567, 2.3508).\n\u2022 Correct API call: { 'arguments': { \u2018lat\u2019: 48.8567, \u2018lng': 23.508 }, 'name': 'tz_lookup_by_location' }\n\u2022 Actual API call: { 'arguments': { 'lat': 488567, \u2018lng': 23508 }, 'name': 'tz_lookup_by_location' }\n\u2022 Result: Unsolved\n\u2022 Reason: The tool call for the latitude-longitude pair (48.8567, 2.3508) returned an error due"}, {"title": "6 Related Work", "content": "LLM Tool-use Recent works have proposed methods and models for enabling LLMs to interact with Tools and real-world APIs. [Xu et al., 2023b, Zeng et al., 2023, Shen et al., 2024, Wang et al., 2023, Xu et al., 2023a, Chen et al., 2023, Ruan et al., 2023, Li et al., 2024, Song et al., 2023, Yang et al., 2024, Schick et al., 2024] Chen et al. [2024], Qin et al. [2023], Patil et al. [2023] also propose custom models trained on their respective benchmarks, based on open-source models, demonstrating solid in-domain and out-of-domain performance. Our testbed facilitates the development of more advanced LLMs for tool-use by providing a ready-made framework, allowing researchers to easily test new planning methods without building an entire system from scratch.\nTool-use Benchmarks In addition to the five benchmarks previously discussed, several others have been proposed for evaluating LLMs' tool-use capabilities [Li et al., 2023, Patil et al., 2023, Xu et al., 2023a]. T-bench [Yao et al., 2024], ToolSandbox [Lu et al., 2024], and AppWorld [Trivedi et al., 2024] focus on the interactive and conversational aspects between users and agents in tool-learning. For multimodal benchmarks, m&m's [Ma et al., 2024] and MLLM-Tool [Wang et al., 2024] test LLMs' performance across natural language and vision. ToolQA [Zhuang et al., 2023] evaluates LLMs' ability to use external tools for question answering, while Ultratool [Huang et al., 2024] also incorporates tool creation alongside other aspects. Our benchmark suite is comprehensive and it recycles existing benchmarks' queries and ground truth while addressing missing components in evaluation."}, {"title": "7 Conclusion", "content": "In this work, we introduce SEAL, a comprehensive testbed designed to address key gaps in existing LLM tool-use benchmarks, particularly for real-world API interactions. We identify critical issues like overfitting, limited support for multi-step reasoning, and instability due to dynamic API behaviors. To overcome these challenges, SEAL include an API simulator powered by GPT-4 and caching responses and a robust evaluation framework covering the full API usage pipeline\u2014from retrieval and calls to final response. Our agent-based system provide a structured platform for reliable performance comparisons. We believe SEAL will facilitate better development and evaluation of API-driven LLMs, supporting more rigorous and reproducible testing in future research."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Benchmark Standardization Details", "content": "ToolBench Sanitization A significant issue with ToolBench is the presence of numerous unsolvable queries, which degrade the overall quality of the benchmark. To address this, we apply a series of filters to exclude queries that cannot be mapped to the available API pool. Specifically, we use the G1 subset of ToolBench training data (the largest among the G1-G3 subsets), which contains 72,783 queries. We then filter out queries that meet any of the following criteria:\n\u2022 The finish type is \"give_up,\" indicating that the query could not be solved using the available APIs.\n\u2022 No matching APIs are found in the API pool, including cases of hallucinated function names.\n\u2022 Incorrect functions, not present in the function pool, are called.\n\u2022 Errors occur while parsing API arguments.\nAfter preprocessing, the total number of queries is reduced to 40,399, with 8,684 APIs remaining. Following additional multi-step and multi-tool filtering, we arrive at a final count of 34,055 queries and 7,559 APIs.\nMulti-step Filtering We filter queries to focus on those requiring multi-step reasoning and the use of multiple tools. Since ToolBench and APIGen provide tool-call ground truth, we leverage this information to identify multi-step queries. For datasets that do not include ground truth information such as tool selection benchmarks, we extract only the multi-tool queries."}, {"title": "A.2 Necessity for More Realistic Benchmarks", "content": "Current benchmarks often fail to reflect real-world scenarios, particularly when it comes to queries requiring sequential, dependent reasoning. For example, ToolBench frequently features simplistic user queries like \u201cPerform a task with API A,\" which typically result from having LLMs generate queries for a given set of APIs. Among the benchmarks we analyzed, APIGen stands out for having the highest-quality queries. However, even APIGen rarely includes multi-step queries that requires sequentially dependent reasoning. Instead, most queries in APIGen are parallel, such as \u201cPerform tasks A and B,\" where tasks A and B are independent of each other. We provide examples of this dynamic below.\n\u2022 Query: Generate the first 10 Fibonacci numbers and calculate the standard deviation of the numbers [3.5, 4.2, 5.1, 6.7, 7.3]\n\u2022 Query: Can you split the list [1, 2, 3, 4, 5, 6] into chunks of size 2, and then generate a random string of length 10 with only uppercase letters?\n\u2022 Query: Find verses with 'wisdom', 'knowledge', and 'understanding'. Also, generate a 15-character random password."}, {"title": "A.3 Bootstrapping Results", "content": "Figure 4 presents a line plot based on the data from Table 3. As the size of the API pool progressively increases, we observe an overall degradation in performance. However, this trend is less pronounced in the pass rate."}, {"title": "A.4 API Simulator Prompt", "content": "Imagine you are an API Server. Your role is to simulate API calls based on the API documentation provided in a JSON format. API documentation includes the API's name, description, and input parameters. There are two types of parameters: required and optional. Optional parameters are specified as \"optional\" in the \"type\" field.\nFollowing is the documentation for the API you need to simulate:\n{API_INFO}\nYour task is to generate a JSON response that aligns with the expected output of the API. As you receive specific inputs for this API call, analyze these inputs to determine their intended purpose.\nYour responses must adhere to a specific JSON structure as the following:\n{\n\"error\": \"\",\n\"response\": \"<Your_Response>\"\n}\nThe error field should remain empty, indicating no errors in processing. The response field should contain the content you formulate based on the API's functionality and the input provided. Ensure that your responses are meaningful and directly address the API's intended functionality. If the provided examples are mostly error messages or lack substantial content, use your judgment to create relevant and accurate responses. The"}, {"title": "A.5 SEAL Execution Example", "content": "Figure 5 illustrates an actual execution example from SEAL. Built on top of the AutoGen framework, SEAL allows users to easily integrate and experiment with different agents in a plug-and-play fashion. Additionally, the system enables monitoring of interactions between agents, providing a flexible and user-friendly environment for testing various tool-use scenarios."}]}