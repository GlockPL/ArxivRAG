{"title": "Neural Networks Asymptotic Behaviours for the Resolution of Inverse Problems", "authors": ["Luigi Del Debbio", "Manuel Naviglio", "Francesco Tarantelli"], "abstract": "This paper presents a study of the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems relevant for applications in Quantum Field Theory, but also in more general contexts. We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where non-linearities in the parameters of the NN can be neglected. Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice. In this simple toy model, the results of the inversion can be compared with the known analytical solution. Our findings indicate that solving the inverse problem with a NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits. Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width. Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature. Our results suggest the need for detailed studies of the training dynamics in more realistic set-ups.", "sections": [{"title": "1. INTRODUCTION", "content": "Inverse problems are a wide class of problems affected by relevant mathematical and numerical issues. These problems, where a continuous functions needs to be reconstructed from a finite set of data, are central in many fields of research. Intuitively, solving inverse problems corresponds to finding a transformation that yields a gain of information, making the problem ill-posed for a continuous function, and ill-conditioned when the function is discretised. In this study, we focus on a correlation function C(T), whose spectral decomposition yields\n$$C(\\tau) = \\int_0^\\infty d\\omega \\rho(\\omega)b(\\omega, \\tau),$$\nwhere \\(\\rho(\\omega)\\) is the spectral density of the theory, and \\(b(\\omega, \\tau)\\) a known function. Solving the inverse problem amounts to extracting information about the spectral function from a discrete set of measurements of C(T). The ill-posedness of these problems implies that it is not possible to find a unique, exact solution. Instead the final answer will depend on the assumption that are made in solving the problem. The Backus-Gilbert (BG) method [1-3], reviewed in Appendix A, has been widely used in lattice field theory in recent years [2, 4-10].\nAnother possible approach to solve inverse problems is based on Gaussian Processes (GP) [11, 12]. It has been demonstrated that BG and GP methods are analytically equivalent [13], see e.g. Ref. [14] for a recent application to determinations of the spectral density. We refer the reader to Appendix A for a summary of the relevant details.\nRecently, there have been multiple studies dedicated to the extraction of spectral densities using Neural Networks (NN) techniques [15-23]. However, the efficacy of Neural Network techniques in yielding substantial improvements over traditional methods needs to be critically scrutinised, as it is unclear whether these techniques yield appreciable corrections compared to conventional approaches such as BG and GP, or if their utilization is somehow redundant. A notable study has been presented in Ref. [23], where the authors compared the results obtained from a model independent training using a Convolutional Neural Network with the ones obtained from the BG approach. The overall conclusion of the study seems to suggest an agreement among these two results.\nIn this paper, we apply theoretical results from computer science to explore numerically the asymptotic limits of neural networks applied to the solution of the inverse problem (1), in a case where the spectral densities are know, namely the quantum-mechanical harmonic oscillator. Our investigation focuses on the connection between these limits and the GP techniques, in the specific case where the output of the NN is a linear function of its parameters. These limits allow an analytic expression for the output of the NN at the end of the training process, thereby simplifying the study of the training and the subsequent test results. Moreover, we analyse whether the non-linear corrections, which appear when considering neural networks of finite width, are relevant in the resolution of these inverse problems.\nThe paper is organised as follows. In Section 2 we discuss two asymptotic limits of neural networks, infinite width and linear limit, emphasising their interplay. Our conventions and some basic results about Neural Networks can be found in Appendix B. In Section 2D, we highlight the equivalence of the asymptotic limits of NNs with GP methods. Finally, in Section 3, we perform numerical experiments using lattice simulations of the"}, {"title": "2. NEURAL NETWORK ASYMPTOTIC LIMITS", "content": "In Appendix B, we describe the fundamental properties of fully connected neural networks under gradient descent (GD) training. In this section, we show that equations (B7) simplify in suitable limits of the architecture of the NN. We describe two fundamental limits of neural networks: the linear limit and the infinite width limit, initially independently, and subsequently highlighting their correspondences under various conditions.\nLet us introduce here the notation that we use in the rest of the paper. The input vector of the NN is made of the values of a correlator Cat no Euclidean times. We will use the symbol T to denote the times at which the correlators are computed; these should not be confused with training time t. In the notation of Appendix B, we have for the input vector,\n$$Z=\\begin{pmatrix}\nC(\\tau_1)\\\\\nC(\\tau_2)\\\\\n...\\\\\nC(\\tau_{n_o})\n\\end{pmatrix},$$\nwith no = 100. The output of the NN yields the value of the spectral density at n\u2081 = 1000 values of the energy w\u03c9,\n$$ \\rho(\\omega_i) = \\phi^{(L)}(C), \\quad i = 1,...,n_L.$$\nThe training set \\(A = \\{(C_a, \\rho_a)\\}\\) is made of pairs of correlators and their corresponding spectral density, each pair being labelled by an index a. Therefore, each value of a identifies one input of the NN,\n$$ C_a = \\begin{pmatrix} C_a(\\tau_1) \\\\ C_a(\\tau_2) \\\\ ... \\\\ C_a(\\tau_{n_o}) \\end{pmatrix}, $$\nand the spectral density for that correlator, represented by the vector\n$$ \\rho_a = \\begin{pmatrix} \\rho_a(\\omega_1) \\\\ \\rho_a(\\omega_2) \\\\ ... \\\\ \\rho_a(\\omega_{n_L}) \\end{pmatrix} $$\nIn what follows we denote \\(C\\) and \\(\\rho\\) the sets of input correlators and densities, and \\(\\phi\\) the output of the NN for a generic input correlator C,\n$$ \\rho_i = \\phi^{(L)}(C). $$\nWe refer to \\(\\rho_i\\) as the output of the NN and Cas a generic test point.", "A": "The linear limit"}, {"title": "A. The linear limit", "content": "In the context of neural networks, the linear limit refers to a scenario where the NN's behaviour becomes predominantly linear in the parameters. This simplification allows for a more interpretable and analytically tractable understanding of the NN's behaviour, making it a valuable tool for theoretical analysis and certain applications in machine learning [24].\nThis limit holds when the output \\(\\rho(C, \\theta_t)\\) of the NN can be approximated by the following first order Taylor expansion\n$$\\rho^{lin}(C, \\theta_t) = \\rho(C, \\theta_t)|_{\\theta_t=\\theta_0} + \\nabla_{\\theta} \\rho(\\tilde{C},\\theta_t)|_{\\theta_t=\\theta_0} w_t.$$\nwhere the vector \\(\\theta_t\\) indicates the vector of all the parameters of the network at the time t, \\(w_t = \\theta_t - \\theta_0\\) the variation of the parameters from their initialisation value, while \\(\\rho(C, \\theta_0)\\) is the output of the neural network after the initialisation of the parameters.\nWhen applying the linear approximation to the NN parameters, the NN's output remains a nonlinear function of its input due to activation functions and architecture. However, this approximation sacrifices intrinsic nonlinearities in the parameters, limiting adaptation to complex data. This constraint hinders the NN's ability to represent complex nonlinear behaviours arising from interactions between parameters.\nIf this approximation holds, Eqs. (B7) become analytically solvable; the dynamics of the parameters and of the output of the NN are given by\n$$w_t = \\sum_{\\alpha_1,\\alpha_2} \\nabla_{\\theta} \\rho(C_{\\alpha_1},\\theta_0) \\left[ \\Omega^{-1} \\frac{I-e^{-\\eta \\Omega t}}{\\Omega} \\right]_{\\alpha_1,\\alpha_2} \\times (\\rho(C_{\\alpha_2}, \\theta_0) - \\rho_{\\alpha_2}),$$\n$$\\rho^{lin}(C_a, \\theta_t) = (1 - e^{-\\eta \\Omega t}) \\rho_a + e^{-\\eta \\Omega t} f(C_a, \\theta_0),$$\nwhere \\(\\Omega\\) is the so-called Neural Tangent Kernel (NTK), defined in Eq. (B8), computed at initialization and \\(\\eta\\) is the learning rate. Note that the only dependence on the training time is contained in the exponential. As \\(t\\) approaches infinity, the output of the linearized network, denoted as \\(\\rho^{lin}\\), converges to the target output data set when the training data C is used as input. Similarly, if training is disregarded, i.e., at t = 0, the parameters remain unchanged (\\(\\theta_t = \\theta_0\\)), resulting in \\(w_t = 0\\).\nThe output for a generic point \\(C_s\\), outside the training set, is\n$$\\rho^{lin}(C_s, \\theta_t) = \\rho(C_s, \\theta_0) + \\sum_\\alpha \\left[ \\nabla_\\theta \\rho(C_s, \\theta_0) \\Omega^{-1} \\frac{I-e^{-\\eta \\Omega t}}{\\Omega} \\nabla_\\theta \\rho(C_a, \\theta_0)^T \\right] (\\rho_a - \\rho(C_a, \\theta_0))\n$$= \\rho_t(C_s) + \\psi_t(C_s),$$\nwhere we defined\n$$\\psi_t(C_s) = \\nabla_\\theta \\rho(C_s, \\theta_0) \\left[ \\Omega^{-1} \\frac{I-e^{-\\eta \\Omega t}}{\\Omega} \\right] \\nabla_\\theta \\rho(C_a, \\theta_0)^T \\rho_a.$$\nTherefore, if the linear approximation holds for our specific application, this result provides us with a powerful tool. In fact, we can obtain the output result of the neural network at any given training time t without the need for actual network training. We only need to compute the NTK and the outputs at the initialization and insert them in the formulas. Note that if the training data are already well reconstructed at initialisation, namely \\(\\rho(C_a, \\theta_0) = \\rho_a\\), then the result is just the output of the NN at initialisation."}, {"title": "B. The large width limit", "content": "The infinite width limit of a neural network refers to a theoretical scenario where the number of neurons in each hidden layer is considered to be infinitely large.\nLet us suppose that we initialize the weights matrix and the bias matrix with values taken from a certain probabilistic distribution function, such that the parameters \\(\\theta_0\\) are i.i.d and have zero mean and variances \\(\\sigma_\\omega/n^{(l)}\\) and \\(\\sigma_f\\), respectively, with \\(n^{(l)}\\) the number of neurons in layer l. Existing literature [25-31] has established that, as the hidden layer width increases, the output distribution converges to a multivariate Gaussian distribution, with deviations from the Gaussian distribution that scale like powers of 1/n(e). Indeed, each neuron's output is computed as a linear combination of the random i.i.d. weights and biases variables. When a large number of these random variables enter in the computation of the preactivation functions, according to the Central Limit Theorem, their sum tends to follow a Gaussian distribution, regardless of the original distribution of the individual variables. This means that, as the layers get wider, the overall behaviour of the neural network becomes increasingly Gaussian-like.\nThis asymptotic behaviour allows the identification of a neural network at initialisation with a Gaussian Process (GP), called NNGP (Neural Network Gaussian Process). In this limit, we can think at the outputs of the NN as sampled from a GP. Furthermore, we are allowed to use a Bayesian inference to discuss the posterior distribution of the neural networks given a set of data, by introducing a kernel function and proceed as routinely done for Gaussian Processes. Formally, if we consider the output \\(\\rho_i(C, \\theta_t)\\), where i = 1, ..., n(L) runs on the dimension of the output of the NN, then in the limit of infinite width we can introduce a kernel function as the covariance matrix of the outputs at initialization in the infinite width limit\n$$\\Sigma(C_{\\delta_1}, C_{\\delta_2})_{ij} = \\lim_{n \\to \\infty} E[\\rho_i(C_{\\delta_1})\\rho_j(C_{\\delta_2})]$$\n$$\\lim_{n \\to \\infty} = \\delta_{\\delta_1 \\delta_2} = \\Sigma_{\\delta_1 \\delta_2}$$\nwhere n\u2192\u221e is a generic notation to denote that all layers become infinitely wide. The kernel \\(\\Sigma_{\\delta_1 \\delta_2}\\) can be computed recursively, see [24, 29] and Appendix C. Thus, given a training input e, we have that at initialization, i.e. for \\(\\theta_t = \\theta_0\\), the output \\(\\rho(C, \\theta_t)\\) will be normally distributed\n$$\\rho(C, \\theta_0) \\sim \\mathcal{N}(0, \\Sigma(C, C)),$$\nwhere we have zero mean since the parameters have zero mean themselves. In order to simplify the notation we have introduced the vector \\(\\rho(C, \\theta_t)\\), such that\n$$\\rho(C, \\theta_t)_a = \\rho(C_a, \\theta_t),$$\nand the matrix \u2211(C, C), such that\n$$\\Sigma(C, C)_{\\alpha_1 \\alpha_2} = \\Sigma(C_{\\alpha_1}, C_{\\alpha_2}),$$\nand we have omitted the neuron index i. This result is valid in general for every input C at initialization, since the assumption that the parameters \\(\\theta_t\\) are i.i.d. is not necessarily true for t > 0.\nThis connection suggests that in the case of infinitely wide neural networks, we can substitute an i.i.d. prior on weights and biases with a corresponding GP prior for the output of the nets. As pointed out in Ref. [32], we can then use Bayesian inference to determine the posterior distribution P(\u03c1|D), given the data D = {C, \u03c1}. We refer to this procedure as Bayesian training: the distribution of weights at initialization determines the Gaussian prior, while Bayes theorem determines the posterior using the training set. The posterior knowledge about \u03c1 at the test point C is encoded in the distribution over the functions \u03c1that are constrained to take values \\(\\bar{\\rho} = (\\bar{\\rho}_1,..., \\bar{\\rho}_N)\\) over the training inputs C = (C1, ..., CN). Here we have introduced \\(\\bar{\\rho}\\), which differs from \u03c1 by some observational noise as described below. Using Bayes theorem,\n$$P(\\rho | D) = \\int d\\bar{\\rho} \\frac{P(\\rho, \\bar{\\rho})}{P(\\bar{\\rho})} P(\\bar{\\rho} | D)$$\n$$= \\frac{1}{P(\\rho)} \\int d\\bar{\\rho} P(\\rho, \\bar{\\rho}),$$\nwhere P(\u03c1|D) corresponds to observational noise, possibly Gaussian centered in \\(\\bar{\\rho}\\) with variance \\(\\sigma^2\\). Using the same notation as before, we have P(\u03c1, \\bar{\\rho}) ~ N(0, \u03a3) where\n$$\\Sigma = \\begin{bmatrix} \\Sigma(\\tilde{C}, \\tilde{C}) & \\Sigma(\\tilde{C}, C) \\\\ \\Sigma(C, \\tilde{C}) & \\Sigma(C, C) \\end{bmatrix}$$\nHere, the block structure indicates the division between training and test set. By performing the Gaussian integral over \\(\\bar{\\rho}\\), we find P(\u03c1|D) ~ \u039d(\u03bc, \u03a3) where\n$$\\mu(\\tilde{C}) = g(\\tilde{C}, C) \\rho,$$\n$$\\Sigma(\\tilde{C}, \\tilde{C}) = \\Sigma(\\tilde{C}, \\tilde{C}) - g(\\tilde{C}, C)\\Sigma(C, C)g(\\tilde{C}, C)^T,$$\nwhere we introduced the NNGP coefficients\n$$g(\\tilde{C}, C) = \\Sigma(\\tilde{C}, C)[\\Sigma^{-1}(C, C) + \\sigma^2 I],$$"}, {"title": "C. Interplay between linear and infinite width limit", "content": "In this paragraph we review the relation between the linear and infinite width limit in the case of gradient descent training [24].\nA simplified training is obtained by freezing the hidden layers parameters at their initialization values, i.e. \\(\\theta^{(l)}_t = \\theta^{(l)}_0\\), for l < L, while we optimize the output layer parameters, i.e. \\(\\theta^{(L)}_t\\). In this case, even if the hidden layers contain activation functions that generate nonlinearities, the optimization only involves the parameters of the output layer, which is simply a linear combination of the outputs of layer L - 1. This corresponds to the optimization of the linear approximation of the neural network. Thus, the original network and its linearization are identical and Eq. (9) holds. Training only the output layer, from the recursive formula of \\(\\Sigma^{(L)}\\) shown in Appendix C, it is clear that \\(\\Sigma^{(L)}_t\\) \u2192 \u03a3(L), i.e. it is just the covariance of the output. Thus, by substituting in Eq. (10) we recover the posterior \\(\\rho(\\tilde{C})\\) in Eq. (19) in the limit t\u2192\u221e. In particular, it can be demonstrated that, for any t, the output when training only the L-th layer converges, in the infinite width limit, to a Gaussian distribution having expectation value and variance given by\n$$\\mathbb{E}[\\rho_t(C)] = \\Sigma(\\tilde{C}, C)\\Sigma^{-1}(C, C)(I - e^{-\\eta \\Sigma(C,C) t})\\rho,$$\n$$= g_t(\\tilde{C},C) \\rho,$$\nand\n$$\\text{Var}[\\rho_t(C)] = \\Sigma(\\tilde{C}, \\tilde{C}) - \\Sigma(\\tilde{C}, C)\\Sigma^{-1}(C, C) \\times$$\n$$\\times (I - e^{-2\\eta \\Sigma(C,C) t})\\Sigma(\\tilde{C}, C)^T.$$\nIn the asymptotic limit t\u2192 \u221e we find\n$$g_t(C, C) \\to g(C, C).$$\nThus, Eq. (22) is identical to the posterior of a GP, i.e. Eq. (19). Thus, the NNGP corresponds to the situation"}, {"title": "D. Relation with Backus Gilbert techniques", "content": "We have discussed two different GPs that emerge from the asymptotic behaviours of NNs: NNGP and NTK-GP. In Appendix A we recall the equivalence between BG techniques and GPs. In the following, we refer to them as BG-GP, and we refer the reader to Refs. [13, 14, 34] for more details. In all cases the central value for the reconstructed function is computed as a linear combination of inputs with some coefficients (cf. Eqs. (20),(27) and (A7)). However, BG-GP and NNGP, while they both have a clear Bayesian interpretation, differ in how their mean values are computed (cf. Eqs. (19) and (A5)).\nIn the BG-GP case, the coefficients directly weigh time slices of the test correlation function. In NNGP, the spectral function corresponding to the input correlator C is a weighted combination of the training spectral functions."}, {"title": "3. NUMERICAL EXPERIMENTS", "content": "The relation between NNs and GPs can be used to investigate whether using a full and arbitrarily complex NN would imply advantages in the resolution of convolutional inverse problems. If we send the width of the NN to infinity, the non-linearities generated by the parameters of the NN become negligible, and the statistical distribution of the output of the net tends to a GP. The question is how much the introduction of these non-linearities is relevant in solving convolutional inverse problems.\nIn this Section, we carry out a simple numerical experiment in order to compare the outcomes obtained using the various NN limits. We focus on cases where the spectral density is analytically known, specifically, the quantum harmonic oscillator simulated on the lattice trough Monte Carlo techniques, see Appendix E for details. This application offers the advantage of quantifying the discrepancies between the results obtained using different techniques and the true spectral densities, which are known in the case of the harmonic oscillator.\nThe numerical experiments are performed on the test data of the correlation function of y\u00b3, where y is the coordinate of the harmonic oscillator, for which the analytical result is reported in Eq. (E11). The spectral density is extracted numerically using all the different techniques discussed in the previous Section, namely the NNGP (discussed in Paragraph 2B), the NTK-GP (discussed in Paragraph 2C) and a finite width NN. For the NN we consider a Multilayer Linear Perceptron (MLP) with 4 hidden layers having 16 neurons each. The NN is trained using MSE loss functions and GD training. The training set consists of correlators, C, and their correspondent spectral functions, o, computed at discrete points. The input has the dimension of the number of independent temporal points of the correlation function, in our case 100. All the three methods are trained using the same training set, {C, p}, consisting of mock spectral functions built as a superposition of Gaussian distributions with fixed \u03c3 ~ 0.01 from which the correspondent correlation functions are computed. Further details are given in Appendix D. The uncertainties are taken into account by a bootstrap sampling on the configurations of the correlation functions, as previously done in Refs. [8, 9]. Then, the final result corresponds to the mean value and the variance on the samples of the outcome of the different methods.\nIn Fig. 1, we show the results obtained using the three different techniques. The plot shows that, for the same training set, both GPs outperform the NN outcomes. It is interesting to focus on the comparison between the two GPs. The NTK-GP results seem to have better controlled uncertainties in contrast to NNGP, where errors explode as w increases. The two approaches yield different results, even statistically, due to their distinct limits and approaches. In the NNGP case, there is no GD training on the network parameters (except for the last layer), but a Bayesian training approach. For the NTK-GP, the neural network is fully trained with GD, in the limit where the number of parameters in each layer tends to infinity, with no valid Bayesian interpretation. One possibility is that GD training, compared to Bayesian training, may suppress fluctuations leading to larger errors in the case of the NNGP, and as also observed in the BG-GP case. The small uncertainties of the NTK-GP, not visible in the plot, could be a consequence of the stability of the method. However, since the resulting uncertainties are extremely small, the NNGP should be considered more accurate. Since the statistical uncertainties are all obtained using bootstrap techniques, the different statistical behaviours call for further analysis. Finally, using our training set, all the algorithms are not able to resolve the second, much smaller, peak.\nIn Figure 2, we show the large width limit of the NN compared with the results obtained using the related GPs. It can be observed that as we increase the number of neurons for each layer the NN results approach"}, {"title": "4. CONCLUSIONS", "content": "We conducted a numerical study on the effectiveness of NN techniques for solving inverse convolution problems. Specifically, drawing on results from computer science theory, we highlighted the asymptotic limits of the NNs. These limits correspond to two different types of GPs, the NNGP and the NTK-GP, to be added to the already known BG-GP.\nThe first scenario occurs when the NN has an infinite number of untrained parameters, randomly initialized, resulting in the loss of all non-linearities in those parameters. In this limit the NN defines a Gaussian prior, while the posterior is evaluated analytically from Bayes theorem. In the second scenario, GD training is applied to all layers. At the end of the training, the NNs are normally distributed, but the posterior distribution does not have the covariance expected from Bayes theorem. Nevertheless, even in this case, the NN converges to its linear approximation in the limit of large width. The mean values and covariances of both GPs can be analytically computed as a function of training time.\nWe considered a case whose inverse problem solution is analytically known, i.e. the quantum harmonic oscillator. Comparing, the results obtained with NNGP to those of a finite-width NN, in the case of a MLP architecture, we showed that NNGP outperforms the latter. In particular, the finite-width NN seems to approach the NNGP accuracy as the layer width is increased. This has already been observed for the MNIST and CIFAR-10 dataset in [29]. Thus, our results, applied in the specific case of convolution inverse problems, seem to confirm a result already established on the two datasets above that are considered important benchmarks in the computer"}, {"title": "Appendix A: Backus-Gilbert inversion method", "content": "In Backus-Gilbert (BG) techniques one looks for approximate solutions of the inverse problem (1) as a smeared version of the true spectral function\n$$\\rho(\\omega) = \\int_0^\\infty d\\omega' \\lambda(\\omega,\\omega')\\rho(\\omega'),$$\nwhere \\(\\lambda(\\omega, \\omega')\\) is a pseudo-gaussian smearing function defined as a linear combination of the basis function in Eq (1)\n$$\\Delta(\\omega,\\omega') = \\sum_{i=0}^N g_i(\\omega)b(\\omega', \\tau_i),$$\nbeing N, the total number of temporal data.\nThe idea of Backus-Gilbert techniques, based on Tikhonov regularization methods, is all contained in how the coefficients gt, which define the shape of the function A, are determined. The original proposal [1] was to minimise a functional defined as\n$$F[g_\\tau] = (1 \u2013 \\lambda)A[g_\\tau] + \\lambda B[g_\\tau],$$\nwhere the first term quantifies the width of the smearing function while the second one the magnitude of the"}, {"title": "Appendix B: Neural Networks basics and Gradient Descent training", "content": "A feedforward Neural Network (NN) consists of multiple layers of artificial neurons. Each neuron takes one or more inputs and produces a single output, which is then fed forward to the next layer of neurons. The number of neurons in layer l is denoted ne, and the layers are numbered from 0 (the input layer) to L (the output layer). The input layer receives the input data, and the output layer produces the final output of the network. The neurons in the hidden layers use activation functions to transform the weighted sum of their inputs called the preactivation function of the neuron into a non linear output. The weights of the connections between neurons in each layer are learned during training using an optimization algorithm like back propagation.\nLet us consider the no-dimensional input\n$$Z\\begin{pmatrix} X_1 \\\\ X_2 \\\\ ... \\\\ X_{n_o} \\end{pmatrix}$$\nThe preactivation function for the neurons in the first layer is computed by multiplying the input vector by the weight matrix and summing a bias vector. The output of the first hidden layer is\n$$\\rho_i^{(1)}(x) = f \\left( \\sum_{j=1}^{n_o} W_{ij}^{(1)} x_j + b_i^{(1)} \\right), i=1,...,n_1,$$\nwhere W(1) is an n\u2081 \u00d7 no weight matrix, while b(1) is an n\u2081-dimensional bias vector. The index j is summed over and runs from 1 to no. The function f is the activation function, which introduces non-linearities between the different layers. Without a non-linear activation function, the network would be limited to linear transformations of the input data, which would make it unable to effectively model complex relationships between inputs and outputs. Different activation functions have different properties that make them more or less suitable for"}, {"title": "Appendix C: Computing the Kernel for NNGP and NKT", "content": "In the large width limit, we can compute analytically the neural tangent kernel associated with the NTK-GP and the kernel \u2211 associated with the NNGP. In this procedure, we follow the Refs. [24, 29, 33] in which and are calculated with recursive functions.\nLet us introduce the kernel for the preactivation functions in layer l,\n$$\\Sigma(C)^{(l)}(C_{\\delta_1}, C_{\\delta_2})_{ij} = \\lim_{n \\to \\infty} \\mathbb{E} \\left[ \\phi^{(l)}_i(C_{\\delta_1}) \\phi^{(l)}_j(C_{\\delta_2}) \\right] = \\delta_{\\delta_1 \\delta_2} \\Sigma^{(l)}$$\nThe recursion relation for \u2211(\u010c, \u010c') yields:\n$$\\Sigma^{(l)}(C, C') = \\sigma_w^2 T \\left( \\Sigma^{(l-1)}(C, C') \\right) + \\sigma_f^2,$$\n$$\\Sigma^{(1)}(C, C') = \\sigma_w^2 (0) C^T C' + \\sigma_f^2.$$\nwhere \\(\\sigma_\\omega\\), \\(\\sigma_f\\) are the weights and biases variances at initialization.\nSimilarly, if we consider the NTK at layer l, the recursion relation reads\n$$\\Theta^{(l)}(C, C') =$$\n$$= \\bar{\\Sigma}^{(l)}(C, C') + \\bar{\\Sigma}^{(l-1)}(C, C) \\times T' \\left( \\frac{\\Theta^{(l-1)}(C, C')}{\\sqrt{\\bar{\\Sigma}^{(l-1)}(C, C)} \\sqrt{\\bar{\\Sigma}^{(l-1)}(C', C)}} \\right) \\times \\bar{\\Sigma}^{(l-1)}(C', C)),$$\n$$\\bar{\\Theta}^{(1)}(C, C') = \\bar{\\Sigma}^{(1)}(C, C').$$\nFinally, the functions T and T' depend by the choice of the activation function, see e.g. Ref. [36]."}, {"title": "Appendix D: Training set details", "content": "As a training set for our analysis, we consider input-target pairs consisting of correlators, C(T), and their correspondent spectral functions, \u03c1(w). Thus the training data set is given by D = ({C}, {\u03c1}) where {C} = (C(T1), C(T2), ..., C(TN\u2081)), {\u03c1} = (\u03c1(\u03c91), \u03c1(\u03c92), \u03c1(\u03c9N\u03c9))."}, {"title": "Appendix E: Harmonic Oscillator", "content": "The Euclidean action of the harmonic oscillator reads\n$$\\frac{1}{\\hbar}S_E = \\int_0^\\beta d\\tau \\left( \\frac{m}{2} \\left(\\frac{dx}{d\\tau}\\right)^2 + \\frac{m\\omega^2}{2} x^2 \\right).$$\nBy discretizing the euclidean time taking a = \u03b2\u0127/N and taking\n$$y(n) \\rightarrow \\left(\\frac{m\\omega}{\\hbar}\\right)^{1/2} x(n a), n = 0, 1, ..., N - 1,$$\nand using the forward derivative to discretize the free term, then the Euclidean action assumes the following form\n$$\\frac{1}{\\hbar}S_E \\rightarrow \\sum_{n=0}^{N-1} \\left( \\frac{m\\hbar}{\\hbar^2} \\left(\\frac{y(n+1) - y(n)}{\\alpha}\\right)^2 + \\frac{m\\omega^2 y(n)^2}{2} \\right)$$\n$$= \\sum_{n=0}^{N-1} \\left( \\frac{1}{\\eta^2} \\left( \\frac{y(n+1) - y(n)}{\\alpha}\\right)^2 + \\frac{ y(n)^2}{2} \\right)$$\n$$=-S_D,$$\nwhere we defined \u03b7 = \u03b1\u03c9. Note that here \u03c9 is not the energy of the system.\nIt's then possible to numerically build a sample which represents a thermodynamic ensemble of the harmonic oscillator where the system parameters are \u03b7 an N. Note that the low temperature regime is reached when \u03b7\u039d \u00bb 1."}]}