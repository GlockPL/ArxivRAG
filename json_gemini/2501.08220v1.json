{"title": "Optimization of Link Configuration for Satellite Communication Using\nReinforcement Learning", "authors": ["Tobias Rohe", "Michael K\u00f6lle", "Jan Matheis", "R\u00fcdiger H\u00f6pfl", "Leo S\u00fcnkel", "Claudia\nLinnhoff-Popien"], "abstract": "Satellite communication is a key technology in our modern connected world. With increasingly complex\nhardware, one challenge is to efficiently configure links (connections) on a satellite transponder. Planning an\noptimal link configuration is extremely complex and depends on many parameters and metrics. The optimal\nuse of the limited resources, bandwidth and power of the transponder is crucial. Such an optimization problem\ncan be approximated using metaheuristic methods such as simulated annealing, but recent research results also\nshow that reinforcement learning can achieve comparable or even better performance in optimization methods.\nHowever, there have not yet been any studies on link configuration on satellite transponders. In order to close\nthis research gap, a transponder environment was developed as part of this work. For this environment, the\nperformance of the reinforcement learning algorithm PPO was compared with the metaheuristic simulated\nannealing in two experiments. The results show that Simulated Annealing delivers better results for this static\nproblem than the PPO algorithm, however, the research in turn also underlines the potential of reinforcement\nlearning for optimization problems.", "sections": [{"title": "1 INTRODUCTION", "content": "In today's world, satellite communication is a piv-\notal technology, utilized across a broad spectrum of\nmodern applications. While terrestrial communica-\ntion systems depend on conventional infrastructure\nthat is often vulnerable to disruptions, satellite com-\nmunication has consistently demonstrated high relia-\nbility. This reliability becomes especially crucial in\nscenarios such as natural disasters, where large-scale\npower outages often occur. To further enhance the\nreliability and efficiency of satellite communication,\nreducing disruption, and optimize bandwidth usage,\nartificial intelligence (AI) is increasingly being em-\nployed [V\u00e1zquez et al., 2020].\nA critical factor in the performance of satellite\ncommunication networks is determining the optimal\nlink configuration of the satellite transponder - an\ninherently complex optimization problem. The opti-\nmal configuration of these links changes dynamically\nas links are terminated or added. This dynamic na-\nture introduces the potential of reinforcement learning\n(RL) as a method to optimize the link configuration.\nBy leveraging learned metrics and the ability to\ngeneralize, a RL approach could offer faster and su-\nperior outcomes compared to traditional metaheuris-\ntic approaches. In this paper, we model a satellite\ntransponder environment and illustrate how a mean-\ningful action and observation space can be defined for\nthis purpose. Since modeling the dynamic aspects of\nthis problem exceeds the scope of this work, we have\nchosen a static approach. In this context, static refers\nto finding the best possible link configuration for a\nrestricted number of links with predetermined param-\neter configurations. Building upon previous research,\nwhere RL has achieved equal or better results than\nmetaheuristics for static optimization problems [Klar\net al., 2023], we extend this investigation to the spe-\ncific optimization problem of link configuration in\nsatellite transponders.\nThe remainder of the paper is structured as fol-\nlows: Section 2 provides an overview of the back-\nground relevant to the study, followed by a discussion\nof related work in Section 3. In Section 4, we present\nour proposed approach for optimizing the link con-\nfiguration. The experimental setup is detailed in Sec-\ntion 5, and the results of the experiments are discussed\nin Section 6. Finally, Section 7 concludes the paper"}, {"title": "2 BACKGROUND", "content": "2.1 Reinforcement Learning\nMachine learning is a subfield of artificial intelli-\ngence, which can be further broken down into three\ncategories: supervised learning, unsupervised learn-\ning, and reinforcement learning (RL). RL is particu-\nlarly well-suited for addressing sequential decision-\nmaking problems, such as those encountered in chess\nor in the game AlphaGo [Deliu, 2023], where signif-\nicant successes were achieved in recent years [Silver\net al., 2018]. In RL, an agent learns a policy n by in-\nteracting with an environment through trial and error,\nwith the goal of making optimal decisions. The two\nmain components in RL are the environment and the\nagent. The environment is often modeled as a simula-\ntion of the real world, as an agent interacting directly\nwith the real environment may be infeasible, too risky\nor too expensive [Prudencio et al., 2023].\nThe foundation of RL lies in the Markov Deci-\nsion Process (MDP). In an MDP, the various states of\nthe environment are represented by states in the pro-\ncess. At any given time t, the agent occupies a state\nSt. From this state, the agent can select from various\nactions to transition to other states. Thus, the system\nconsists of state-action pairs, or tuples (At, St). When\nthe agent selects an action At and executes it in the\nenvironment, the agent receives feedback. This feed-\nback includes an evaluation of the performed action\nin the form of a reward, and the new state St+1 of the\nenvironment. The reward may be positive or negative,\nrepresenting either a benefit or a penalty. The agent's\nobjective is to maximize the accumulated reward, as\nshown in Equation 1.\n$\\pi^* = arg max_\\pi E[ \\sum_{t=0}^{H} \\gamma R(S_t,a_t)]$\n\nA major challenge in finding the optimal policy\n$\\pi^*$ is the balance between exploration and exploita-\ntion. Exploration refers to the degree to which the\nagent explores the environment for unknown states,\nwhile exploitation refers to the degree to which the\nagent applies its learned knowledge to achieve the\nhighest possible reward. At the start of training, the\nagent should focus more on exploring the environ-\nment, even if this means not always selecting the ac-\ntion that yields the highest immediate reward (i.e.,\navoiding a greedy strategy). This approach allows the\nagent to gain a more comprehensive understanding"}, {"title": "3 RELATED WORK", "content": "Metaheuristics are commonly applied to optimization\nproblems and frequently yield efficient near-optimal\nsolutions in complex environments. A promising\nalternative to these traditional approaches are RL-\nalgorithms, whose potential in the context of opti-\nmization problems has already been scientifically ex-\nplored [Zhang et al., 2022, Ardon, 2022, Li et al.,\n2021, K\u00f6lle et al., 2024]. Studies such as [Klar et al.,\n2023] and [Mazyavkina et al., 2021] demonstrate that\nRL-algorithms can achieve results similar to or even\nsuperior to those obtained by standard metaheuristics.\nThis applies to a variety of optimization problems, in-\ncluding the Traveling Salesman Problem (TSP) [Bello\net al., 2016], the Maximum Cut (Max-Cut) prob-\nlem, the Minimum Vertex Cover (MVC) problem, and\nthe Bin Packing Problem (BPP) [Mazyavkina et al.,\n2021].\nWhile the work by [Klar et al., 2023] does not fo-\ncus on the arrangement of links on a transponder, it\nfocuses on the planning of factory layouts, which is\nalso a static optimization problem. This study com-\npares a RL-based approach with a metaheuristic ap-\nproach and examines which method proves more ef-\nfective. The research highlights RL's capability to\nlearn problem-specific application scenarios, making\nit an intriguing alternative to conventional methods.\nGiven the uniqueness of each problem and the limited\namount of scientific research on this specific topic, in-"}, {"title": "4 APPROACH", "content": "4.1 System Description\nIn satellite communication, ground stations are re-\nsponsible for sending and receiving data. The data\nis transmitted from the sending ground station to the\nsatellite via communication links, also referred to as\nlinks or carriers, and then relayed from the satellite to\nthe receiving ground station via another link. On the\nsatellite itself, the links pass through chains of devices\nknown as transponders. Incoming signals are received\nby an antenna on the satellite, amplified, converted in\nfrequency, and retransmitted through the antenna. A\ntransponder is a component within a satellite that re-\nceives signals transmitted from a ground station, am-\nplifies them with a specific amplification factor using\nthe available power, changes their frequency to avoid\ninterference, and then retransmits the signals back to\nEarth. This process enables the efficient transmission\nof information across vast distances in space.\nHowever, each transponder has limited resources\nin terms of bandwidth and power. It is crucial that the\nlinks are configured to consume as few resources as\npossible. A link is characterized by several parame-\nters, such as data rate, MOD-FEC combination (Mod-\nulation and Forward Error Correction), bandwidth,\nand EIRP (Effective Isotropic Radiated Power). To"}, {"title": "4.2 Systemparameters", "content": "The following section outlines the process of estab-\nlishing a satellite connection. To set up a link from a\nground station to a satellite, various parameters must\nbe carefully combined. The choice of these param-\neters determines how efficiently the links can be ar-\nranged on the satellite transponder, minimizing the\nconsumption of transponder resources, specifically\nEIRP and bandwidth.\nOne key parameter is the EIRP, which represents\nthe power used to establish the link. Each link re-\nquires a certain EIRP to ensure that the transmitted\ndata arrives with the desired quality. The necessary\nEIRP depends on the data rate to be transmitted. The\ndata rate, typically measured in bits per second (bps),\nindicates how much data can be transmitted per unit\nof time and plays a crucial role in determining the re-\nquired bandwidth.\nThe MOD-FEC combination is another essential\nparameter. Modulation (MOD) is the process of con-\nverting data into an electrical signal (electromagnetic\nwave) for transmission, where characteristics of the\nwave-such as amplitude, frequency, or phase-are\nadjusted to encode the data. Forward Error Correction\n(FEC) enables error detection and correction without\nthe need for retransmission, by adding correction in-\nformation. The combination of modulation and FEC\ngreatly influences transmission efficiency, especially\nin environments where interference is likely. Vari-\nous valid MOD-FEC combinations are available for\nthe transmission of a link, typically determined by the\nhardware in use.\nAnother important parameter is bandwidth, which\ndescribes the ability of a communication channel or\ntransmission medium to transmit data over a specified\ntime period. Measured in Hertz (Hz), bandwidth di-\nrectly influences transmission speed. In satellite com-\nmunication, bandwidth refers to the frequency range\nover which a signal is transmitted. Both the data rate\nand the MOD-FEC combination influence the deter-"}, {"title": "4.3 Action and Observation Space", "content": "In RL, the action space represents the set of all pos-\nsible actions that an agent can perform within an en-\nvironment. The agent responds to received observa-\ntions by choosing an action. To analyze how different\naction spaces affect performance, two distinct action\nspaces were defined.\nThe first action space (Action Space 1) was de-\nfined using a nested dictionary from the Gymnasium\nframework (gymnasium.spaces.Dict). This dictionary\ncontains three sub-dictionaries, each representing a\nlink. Each link dictionary includes the parameters\nMOD-FEC combination, center frequency, and EIRP.\nBoth EIRP and center frequency are continuous val-\nues, so a float value between 0 and 1 is selected at\neach step within the environment. The MOD-FEC"}, {"title": "4.4 Simplifications", "content": "In the experiments conducted, the task was simplified\nin several aspects to facilitate the learning process.\nSpecifically, the transponder environment was simpli-\nfied. For instance, the possible MOD-FEC combina-\ntions were restricted, and the modeling of the links in\nthe frequency domain was simplified, excluding the\ninterference of electromagnetic waves (links) which\nnormally affect communication quality.\nAdditionally, simplifications were made regarding\nthe inherently dynamic nature of the problem. Cur-\nrently, a Proximal Policy Optimization (PPO) model\ncan only be trained for a fixed number of links defined\nprior to training, which represents a static rather than\ndynamic scenario. In our experiments, a PPO model\nwas trained exclusively for three links, and dynamic\nchanges - such as the addition or termination of links\nwere not accounted for. The current environment\nmodel is configured so that each link can be assigned\none of three possible MOD-FEC combinations, and\nthe data rate for each link is set to a uniform value.\nFuture iterations of the model will progressively\naddress these simplifications to handle the more com-\nplex tasks encountered in real-world scenarios."}, {"title": "5 EXPERIMENTAL SETUP", "content": "Two experiments were conducted, differing only in\nthe configuration of the RL model. The PPO al-\ngorithm was utilized in both models, with the only\ndistinction being the action space, as explained in\nChapter 4.3. For both experiments, Python (version\n3.8.10), PyTorch (version 1.13.1), Gymnasium (ver-\nsion 0.28.1), and Ray RLlib (version 0.0.1) were used.\n5.1 PPO Algorithm\nThe PPO model serves as the baseline against which\nthe Random Action and Simulated Annealing base-\nlines are compared in terms of performance. A neu-\nral network was used to implement the PPO, tasked\nwith calculating the policy parameters. The neural\nnetwork topology consisted of the observation space\nas the input layer, followed by two hidden layers with\n256 neurons each, and the action space as the output\nlayer. The learning rate for the PPO model was op-\ntimized, with further details provided in the section\n\"Training Process\". The hyperparameters are shown\nin Table 1."}, {"title": "5.2 Random Action Strategy", "content": "The na\u00efve random action strategy, which is compared\nwith the PPO, consists of choosing actions purely at\nrandom. We define a fundamental learning success\nhere if the PPO model achieves better results than the\nrandom action model."}, {"title": "5.3 Simulated Annealing", "content": "Simulated annealing follows a structured process: Ini-\ntially, the algorithm is provided with a randomly gen-\nerated action. This action is evaluated by the cost\nfunction, which in this case is represented by the step\nfunction of the environment. The corresponding re-\nward for the action is calculated. Next, the neighbor\nfunction generates a new action. If the reward of the\nnew action is better than that of the previous action,\nit is accepted as the new action. However, if the re-\nward is lower, the new action is not immediately dis-\ncarded. An additional mechanism, known as temper-\nature, plays a key role in determining the probability\nof selecting a worse action as the new action.\nAt the beginning of the algorithm, the temperature\nis high, leading to a higher probability of choosing a\nworse action. This facilitates broader exploration in\nthe search for the global optimum. As the algorithm\nprogresses, the temperature decreases gradually. As\nthe temperature lowers, the probability of accepting\na worse action diminishes, leading to a more focused\nsearch for the local optimum and enabling fine-tuning\nof the solution."}, {"title": "5.4 Metrics", "content": "The metric for evaluating link configuration on the\nsatellite transponder is the total reward, which reflects\nthe learning behavior of the model. This total re-\nward comprises eight metrics, each linked to specific\nconditions in the link configuration. The metrics are\ndetailed below along with their calculation methods.\nThey are categorized into Links Reward (LR) and\nTransponder Reward (TR), contributing 70% (0.7)\nand 30% (0.3) to the total reward, respectively. In-\ndividual metrics can be weighted to emphasize more\nchallenging metrics (through the parameters \u03c9, \u03b8, \u03c6,\n\u03bc, \u03b2, \u03b5, \u03c8, \u03c1), potentially enhancing policy learning\nfor the PPO agent. However, the impact of different\nweightings was not explored in this study.\nLinks reward metrics include Overlap Reward,\nOn Transponder Reward, PEB Reward, and Mar-\ngin Reward, which pertain to each link individually.\nSince multiple links are typically configured on the\ntransponder, the link reward is divided by the num-\nber of links to represent the reward per link-weight\n(RpL). This share, multiplied by the Individual Link\nReward for each link, contributes to the total reward\n(see Equation 9). Metrics for each link are:\n\u2022 Overlap Reward (OR): Checks if the selected link\ndoes not overlap with other links in terms of band-\nwidth. A reward of 1 is given if met; otherwise,"}, {"title": "5.5 Training Process", "content": "Before initiating the experiments, the learning rate\nfor the PPO was examined through a grid search.\nFor the first experiment, the grid search included val-\nues of le-3, 5e-4, 1e-4, 5e-5, and 1e - 5.\nEach learning rate was evaluated using three differ-\nent seeds. Across all runs, 2,000,000 steps were ex-\necuted in the transponder environment, maintaining a\nconsistent batch size of 2,000 for comparability. Each\nepisode consisted of 10 steps, serving as the termina-\ntion criterion, resulting in a total of 200, 000 episodes.\nThe best performance was obtained with a learning\nrate of 1e-5. This learning rate was subsequently\nused for Experiment 1.\nA similar grid search was performed for the sec-\nond experiment, with learning rate values set at 1e-3,\n1e-4, and 1e-5. Each learning rate was again tested\nwith three different seeds. For all runs, 1,000,000\nsteps were carried out in the transponder environment,\nwith a batch size of 2,000. Episodes consisted of 100\nsteps. Also for Experiment 2, a learning rate of 1e - 5\nyielded the best performance."}, {"title": "6 RESULTS", "content": "6.1 Experiment 1\nThe objective of the first experiment was to determine\nan optimal link configuration for three links within\nthe transponder environment, ensuring that the band-\nwidth and EIRP resources of the satellite transponder\nwere conserved. To address this optimization prob-\nlem, the three algorithms - PPO, Simulated Anneal-\ning, and Random Action were implemented in the\ntransponder environment. The experiment aimed to\nevaluate whether a trained PPO model could achieve\nhigher performance compared to the Simulated An-\nnealing algorithm. Action Space 1 was used in the\nenvironment for this experiment.\nFigure 2 illustrates the training of the PPO model\nover 2,000,000 steps, compared to the Simulated An-\nnealing and Random Action baselines. Each PPO\nepisode consisted of 10 steps. To enhance visualiza-\ntion, all curves were smoothed. Training for the PPO\nmodel and the computation of the comparative base-\nlines were conducted with five different seeds (0 to 4).\nThe figure presents the average performance across\nthese runs, including the standard deviation for each"}, {"title": "6.2 Experiment 2", "content": "Experiment 2 differs from Experiment 1 in that it uses\nAction Space 2, defined such that a parameter value"}, {"title": "6.3 Interpretation", "content": "The results achieved with Action Space 1 are supe-\nrior to those obtained with Action Space 2. One pos-\nsible explanation for this is that, during training, the\nrandomly generated actions in Action Space 1 evenly\nscan the action space. In contrast, with Action Space\n2, certain regions are densely sampled while others\nreceive little to no sampling. Consequently, inference\nusing an Action Space 2 PPO model yields very good\nresults in well-sampled areas and significantly poorer\nresults in sparsely sampled areas.\nAnother possible reason for the better perfor-\nmance of Action Space 1 could be the simpler and\nmore direct relationship between the Action Space\nand Observation Space, making it easier to learn. In"}, {"title": "7 CONCLUSION", "content": "Even today, the configuration of connections in satel-\nlite communication networks is often performed man-\nually. Given the increasing complexity of the hard-\nware used both in satellites and on the ground, the\napplication of AI in this domain is a logical progres-\nsion. In this work, a transponder environment was de-\nveloped for link configuration on a satellite transpon-\nder, and the potential utility of RL for solving a static\nproblem was investigated. Two experiments were\nconducted, differing in their implementation of the\naction space. In both, a PPO model was compared\nto two baselines: Simulated Annealing and the Ran-\ndom Action model. The results showed that in both\nexperiments, the Simulated Annealing metaheuristic\noutperformed the PPO model. Since only the learning\nrate was optimized for the PPO, further improvements\ncould be achieved through additional hyperparameter\ntuning. Additionally, the weighting of the metrics was\nnot explored, which could be another factor for en-\nhancing PPO performance.\nIt is likely that the performance gap between Sim-\nulated Annealing and PPO would widen as the static\nproblem becomes more complex and realistic. Sim-\nulated Annealing is specifically designed for efficient\nexploration of large search spaces, potentially demon-\nstrating even greater superiority over PPO in more\ncomplex scenarios. Given the growing complexity of\nthis field, continued research is essential to find so-\nlutions to this optimization problem. The environ-\nment must be refined to better match real-world re-\nquirements. For instance, only a single data rate was\nused in this study, whereas in practice, this parameter\nis continuous or must be represented in a rasterized\nform. The mutual interference between links was ne-\nglected and needs to be incorporated for practical ap-\nplications. The restriction to exactly three links must\nalso be removed. An important next step is to transi-\ntion from a static to a dynamic model, allowing each\nlink to have an active time period. The agent must\nalso evolve alongside the environment, with the neu-\nral network topology becoming more complex. A po-\ntential strength of PPO could be its ability to handle\nscenarios where links on the transponder are dynam-\nically added or terminated over time, leveraging its\ncapacity to learn policies that comprehend underlying"}]}