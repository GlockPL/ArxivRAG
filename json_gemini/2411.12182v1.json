{"title": "Diffusion-Inspired Cold Start with Sufficient Prior in Computerized Adaptive Testing", "authors": ["Haiping Ma", "Aoqing Xia", "Changqian Wang", "Hai Wang", "Xingyi Zhang"], "abstract": "Computerized Adaptive Testing (CAT) aims to select the most ap- propriate questions based on the examinee's ability and is widely used in online education. However, existing CAT systems often lack initial understanding of the examinee's ability, requiring ran- dom probing questions. This can lead to poorly matched ques- tions, extending the test duration and negatively impacting the examinee's mindset, a phenomenon referred to as the Cold Start with Insufficient Prior (CSIP) task. This issue occurs because CAT systems do not effectively utilize the abundant prior information about the examinee available from other courses on online plat- forms. These response records, due to the commonality of cogni- tive states across different knowledge domains, can provide valu- able prior information for the target domain. However, no prior work has explored solutions for the CSIP task. In response to this gap, we propose Diffusion Cognitive States Transfer Frame- work (DCSR), a novel domain transfer framework based on Diffu- sion Models (DMs) to address the CSIP task. Specifically, we con- struct a cognitive state transition bridge between domains, guided by the common cognitive states of examinees, encouraging the model to reconstruct the initial ability state in the target domain. To enrich the expressive power of the generated data, we analyze the causal relationships in the generation process from a causal perspective. Redundant and extraneous cognitive states can lead to limited transfer and negative transfer effects. Therefore, we designed three decoupling strategies to control confounding vari- ables, thereby blocking backdoor paths that hinder causal discov- ery. Given that excessive uncertainty can affect the applicability of generated results to the CAT system, we propose consistency constraint and task-oriented constraint to control the randomness of the generated results and their relevance to the CAT task, re- spectively. Our DCSR can seamlessly apply the generated initial ability states in the target domain to existing question selection al- gorithms, thus improving the cold start performance of the CAT sys- tem. Extensive experiments conducted on five real-world datasets demonstrate that DCSR significantly outperforms existing base- line methods in addressing the CSIP task. The code is available at: https://github.com/BIMK/Intelligent-Education/tree/main/DCSR.", "sections": [{"title": "1 Introduction", "content": "As artificial intelligence empower education, computerized adap- tive testing (CAT) on online education platforms have garnered extensive attention [38, 39, 42, 45]. CAT aims to provide examinees with a small number of appropriate questions to progressively as- sess their cognitive states in specific domains [19, 29, 30]. Typically, CAT consists of two iterative components: the cognitive diagnostic model (CDM) and the question selection algorithm. As shown in Figure 1 (a), the CDM estimates examinee's ability based on her response at step t -1 [26, 43, 46, 47], after which the question selection algorithm provides suitable question for the next step, thus enabling a more accurate assessment of examinee's ability. Existing research on the question selection algorithm, a critical component of CAT, can be categorized into policy-based [1, 50] and learnable-based [34, 49] approaches. The former generally converts the question selection process into model expectation, while the latter defines the CAT process as a bi-level optimization problem. Despite the advancements made by various question selection algorithms, they still face challenges related to the cold start prob- lem in the CAT process. As illustrated in Figure 1 (b), on an online education platform, the system initially has no knowledge of the examinee's ability, causing the CAT system to tentatively select questions randomly to determine the starting point of the test. The difficulty and content of the question might not align with the examinee's actual ability, leading to confusion or frustration and increasing the time cost of the assessment. Additionally, the ques- tion selection process is an iterative Markov chain. This means that if the question selected in the initial stages do not match the examinee's ability, it may lead to biased question selections in sub- sequent stages. Most question selection algorithms based on greedy strategies tend to amplify this bias, harming the performance of the CAT system. These algorithms, although capable of quickly finding local optima in certain scenarios, can easily get trapped in local optima over multiple rounds of question selection. In our paper, we refer to this task as Cold Start with Insufficient Priors (CSIP), which arises from the insufficient understanding of the examinee by the question selection algorithm at the initial stage of the test, leading to uncertain starting points for selection and subsequently affecting the adaptive testing process.\nWith the widespread application of educational platforms, vast amounts of examinee data are collected and stored [14], providing valuable resources to enrich the prior information for CAT system. Notably, historical response records of examinees in other courses can serve as crucial references for CAT system to preliminarily understand examinee abilities. There are certain commonalities among knowledge concepts across different courses. For example, many physics questions, such as those in kinematics and dynam- ics, require algebraic equation solving, which is fundamental in mathematics. Therefore, an examinee's response records across multiple courses can reflect their foundational abilities and cogni- tive commonalities in multiple interdisciplinary fields, which are transferable attributes. These pre-diagnosed abilities can provide rich prior information for the target (cold start) course, allowing the CAT system to grasp the examinee's ability range in advance. For example, if an examinee performs well in mathematics, especially in complex algebra and geometry questions, the CAT system can infer that he also possess strong logical thinking and problem-solving skills in physics, thereby assigning a higher initial ability. To our knowledge, no research has yet utilized these cross-course data to solve the CSIP task, despite its significant and practical importance.\nTo fully leverage these cross-course response records and provide prior information about examinees to CAT system, we integrate and transfer the pre-diagnosis results of examinees to the target domain based on the concept of Diffusion Models (DMs) [3, 11]. DMs add noise incrementally and then reconstruct the corrupted data step by step in reverse, which not only addresses noisy re- sponse records in the source domain but also aptly meets the needs of solving the CSIP task. This is due to the ability of DMs to grad- ually merge complex multi-distribution cross-domain data into a unified latent space during denoising, generating a unified latent representation, which reconstructs the initial ability of the target domain with specific transfer attributes from the noise. However, despite the great potential of using DMs to address the CSIP task, we uncover challenges in terms of what and how: (1) What kind of source domain information can be injected into the reverse denois- ing process as guidance. And (2) How to constrain the output to match the CAT system, implying that excessive uncertainty in the generated results can lead to data unsuitable for CAT task.\nTo address these challenges, we design a novel domain transfer framework named the Diffusion Cognitive States Transfer Frame- work (DCSR) for the CSIP task. Guided by the principle of DMs, we generate initial abilities in the target domain for examinees to enhance the cold-start performance of CAT system. Specifically, to prevent the loss of personalized information in the diffusion generation results, we condition the denoiser on examinees' prior cognitive abilities to incorporate personalized target domain abil- ities. To avoid redundant information and negative transfer, we analyze the causal relationships between different variables from the causal perspective of model and design three decoupling strate- gies to adjust confounding variables in the backdoor path, including domain-shared cognition, domain-specific cognition, and orthogo- nal regularization-based gradient separation, blocking paths that obscure true causal relationships. To mitigate the excessive uncer- tainty of DMs and enhance the adaptability to CAT task, we design consistency constraint and task-oriented constraint to control ran- domness and match the input requirements of CAT system. The results generated by DCSR seamlessly integrate with existing ques- tion selection algorithms, improving the cold-start performance of CAT system. Extensive experimental results on five real-world datasets demonstrate that DCSR effectively addresses the CSIP task and significantly outperforms baselines."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Computerized Adaptive Testing", "content": "Computerized Adaptive Testing (CAT) aims to evaluate an exam- inee's ability progressively within a shorter test length [12, 27]. CAT systems primarily consist of two components: (1) Cognitive Diagnostic Models (CDMs) for diagnosing examinees' abilities based on their responses to selected questions [20, 23, 24, 37, 41]. Item Response Theory (IRT) [5], a widely used CDM, assumes uni-dimensional independence and uses continuous latent variables to assess examinees' latent abilities. With the widespread application of deep neural networks, neural CDMs such as NCD [32], RCD [7], and KaNCD [33] utilize neural networks to capture complex inter- actions between examinees and questions, enabling fine-grained diagnostic modeling. (2) Question selection algorithms aim to adaptively choose the next appropriate question based on the CDM feedback. Early question selection algorithms were model-specific, such as IRT-specific Maximum Fisher Information [21], Kullback- Leibler Information Index [4], and Max Entropy [28]. However, these simple algorithms could not meet the evolving needs of CDMs, leading to performance limitations. Therefore, a model-agnostic question selection algorithm, MAAT [1] was proposed, leverag- ing active learning to transform the question selection process into choosing questions with the greatest expected model change. Similarly, BECAT [50] employs an expected gradient difference approach, treating the question selection process as a subset selec- tion problem guided by theoretical estimates of examinees' true abilities. Another research direction focuses on data-driven ques- tion selection algorithms. For example, BOBCAT [9], NCAT [49], and GMOCAT [34] define the CAT task as a bi-level optimization problem, using reinforcement learning to learn question selection algorithm from large-scale response data. However, from a model perspective, the crucial CDM component in current CAT systems randomly initializes examinees' abilities at the test's outset. This random initialization necessitates more question selection steps for greedy algorithms to understand the examinee's ability range, thereby exacerbating the cold start problem in CAT system."}, {"title": "2.2 Diffusion Model", "content": "Diffusion Models (DMs) have achieved impressive results in image generation [3, 11]. To transfer these successes to other domains, re- cent works [15, 25, 48] have attempted to bridge the image domain with other fields. CODIGEM [31] was the first to extend the de- noising module in DMs to recommendation systems. DiffuRec [17], DreamRec [44] and CF-Diff [13] model the latent representations of items and user preferences, guiding the denoising module to gen- erate personalized representations. DiffRec [35] and DiffuASR [18] not only reduce generation costs but also achieve temporal model- ing of interaction sequences. Moreover, more research leverages the Markov chain modeling characteristics of DMs to explore sequence modeling, with few works utilizing diffusion features to study cross- domain problems. Although DiffCDR [36] has made preliminary explorations in this area, it introduces redundant information and faces limitations in cross-domain representation capacity due to the diverse entities in CAT system."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Computerized Adaptive Testing", "content": ""}, {"title": "3.1.1 Task Introduction.", "content": "In an online education platform, the Com- puterized Adaptive Testing (CAT) system adaptively selects ques- tions for examinees to accurately reveal their cognitive abilities. The CAT system comprises two components: (1) a Cognitive Diagnosis Model (CDM) M, which assesses the examinee's ability state after responding to the selected questions, and (2) a question selection algorithm \u03a0, which chooses the next most informative question based on the examinee's current ability state to measure their abil- ity more precisely. These two components alternately iterate until a termination condition is met.\nSpecifically, given a examinee ei and his/her candidate question set Qi, the CAT process can be represented as:\nq\u207a \u2190 \u03a0(Qi | \u03b8\u1d57\u207b\u00b9),\n\u03b8\u1d57 \u2190 M(riqtq, \u03b8\u1d57\u207b\u00b9),\n                                                (1)\nwhere qt denotes the question selected by \u03a0 based on the exam- inee's ability \u03b8\u1d57\u207b\u00b9 at time t \u2212 1 from the candidate question set Qi, which only includes questions that the examinee ei has not yet responded to. The variable riqt represents the response result of examinee ei to question qt. Note that T is the maximum number of steps for terminating the test."}, {"title": "3.1.2 Cold start with insufficient priors.", "content": "In the Cold Start with In- sufficient Priors (CSIP) scenario, the question selection algorithm considers only the information from the target domain while ig- noring the rich data from the source domains. To alleviate this lack of prior information, we define M source domains S\u2081, S\u2082,..., SM and one target domain T in the Diffusion Cognitive State Transfer Framework (DCSR). Each domain includes three groups of enti- ties. We use ESm, QSm and CSm to denote the sets of examinees, questions, and knowledge concepts in the m\u2208 M source domain, respectively. Similarly, ET, QT and CT denote the three entities in the target domain. The overlapping examinees between domains is defined as \u0190\u2080 = \u0190\u1d64\u2229\u0190\u209c, while the other two entity groups generally do not have overlapping elements across domains."}, {"title": "3.1.3 Training and Testing Phases.", "content": "In the given CAT testing platform, both warm-start and cold-start, meaning the CAT system has never encountered before, are included in the target domain. Their response records can be represented as RT = R\u02b7\u1d43\u02b3\u1d50\u222aRc\u1d52\u02e1\u1d48 = {(ei, qj, rij) | ei \u2208 E\u02b7\u1d43\u02b3\u1d50\u222aE\u1d9c\u1d52\u02e1\u1d48, where rij = 1 indicates that examinee ei answered question qj\u2208 QT correctly, and rij = 0 otherwise. Similarly, the re- sponse records in the m source domain can be represented as RSm = {(ei, qj, rij) | ei \u2208 ESm, qj \u2208 QSm}, and all source domain records are denoted as Rs = {RS\u2081,RS\u2082,...,RSm}. To prevent data leakage, the aforementioned response sets are uniformly divided into a training set Dtrain = {R\u02b7\u1d43\u02b3\u1d50, RS} and a test set Dtest = Rc\u1d52\u02e1\u1d48.\nOur DCSR consists of two phases: training the initial abilities of examinees in the source domain based on Dtrain and testing the performance of question selection in the CAT system without learning, based on Dtest."}, {"title": "3.1.4 Pre-Establish Cognitive States.", "content": "The data in the training set Dtrain comes from the response records of overlapping examinees E\u2080 between domains. We pre-train these examinees to establish their cognitive states in both the source and target domains. The pre-training process is expressed as:\n\u03b8* = arg min \u03a3 L(rij, My(qj|\u03b8i)),                  (2)\n\u03b8\u2208\u0398     (ei,qj,rij)\u2208Dtrain\nwhere My is the CDM used for pre-training, and different do- main data correspond to different CDM parameters, i.e., \u03a8 =\n{\u03c8S1, \u03c8S2,..., \u03c8SM,\u03c8T}. The abilities of overlapping examinees\nare denoted as \u0398s = {\u03b8i | i \u2208 E\u2080}, and similarly, the abilities in the target domain are denoted as \u0398T."}, {"title": "3.2 Diffusion Model", "content": "Diffusion Models (DMs) have demonstrated exceptional perfor- mance in fields such as computer vision [11]. Typically, DMs consist of two parts: the forward process and the reverse process."}, {"title": "3.2.1 Forward Process.", "content": "Given a data point x\u2080 ~ q(x) sampled from the true data distribution, the forward process gradually degrades x\u2080 into standard Gaussian noise xT ~ N(0, 1) by injecting Gaussian noise over T steps. Specifically, the process of converting xt\u22121 to xt in DMs is represented as q(xt | xt\u22121) = N(xt; \u221a1 \u2212 \u03b2t xt\u22121, \u03b2t I), where t \u2208 {1, ..., T} represents the diffusion steps, \u03b2t \u2208 (0,1) is the predefined noise scheduling coefficient, and N denotes the Gaussian distribution."}, {"title": "3.2.2 Reverse Process.", "content": "In the reverse process, DMs learn to remove the added noise, thereby recovering the original data distribution x\u2080 from pure noise, aiming to introduce minor uncertainties in the generation process. This process learns a parameterized network p\u03b4(xt\u22121 | xt) to approximate the reverse process, which can be formalized as p\u03b4(xt\u22121 | xt) = N(xt\u22121; \u03bc\u03b4(xt, t), \u03a3\u03b4(xt, t)), where \u03bc\u03b4 and \u03a3\u03b4 are the mean and variance of the Gaussian distribution predicted by a neural network with parameters \u03b4."}, {"title": "4 Method", "content": "Overview. The core idea of this work is to transfer prior diagnos- tic results from the source domain to the target domain, thereby generating personalized initial abilities for cold-start examinees in the target domain course. As illustrated in Figure 2, our DCSR is built upon the Diffusion Module and is supported by two key components: the Cognitive State Unification Module (CSUM) and the Harmonization and Calibration Module (HCM). Specifically, the Diffusion Module uses prior abilities from the source domain to reconstruct the cognitive state of examinees in the target domain from noise. From a causal model perspective, the generated abili- ties are influenced by redundant knowledge, and domain-specific cognition may cause negative transfer. Therefore, the CSUM is employed to control confounding variables in the backdoor path, thereby uncovering the true causal relationships. Additionally, to mitigate uncertainty during the diffusion process, we designed con- sistency and task-oriented constraints in HCM, aiming to ensure that the generated results adhere to the true distribution and meet the requirements of CAT task. It is worth noting that our frame- work demonstrates significant scalability, seamlessly integrating the generated initial abilities in the target domain into existing CAT, thereby improving its cold-start performance."}, {"title": "4.1 Diffusion Module", "content": "We employ the concept of diffusion as the backbone of our model, establishing a bridge for cognitive state transformation between the source and target domains. The purpose of the Diffusion Module is to incorporate prior information from the source domain into the initial ability estimation process in the target domain. Therefore, it inherently involves two distinct processes: the forward noise addition process and the reverse denoising process.\nDuring training, we gradually inject Gaussian noise into the ability vectors \u03b8T of examinees in the target domain over T steps. For a specific examinee ei \u2208 \u0190\u2080, his ability vector \u03b8T = \u03b8 \u2208 \u0398T is corrupted into \u03b8T, which is modeled as a Gaussian transition Markov chain:\nq(\u03b8i\u2080:T|\u03b8i\u2080) = \u220f N(\u03b8i\u209c| \u221a1 \u2212 \u03b2t \u03b8i\u209c\u208b\u2081, \u03b2tI),                    (3)\n      t=1\nwhere \u03b2t \u2208 (0, 1) controls the scale of noise added at the $t$-th step, and N denotes a Gaussian distribution.\nIn the reverse denoising process, the traditional denoising meth- ods (as introduced in section 3.2.2) are inadequate for effective transfer because the denoising process modeled lacks guidance from source domain information, resulting in the loss of personal- ization in the generated abilities. To generate personalized ability vectors, we propose utilizing prior information from the source domain to guide the denoising process. Specifically, for examinee ei, we use the pretrained source domain ability \u03b8S \u2208 \u0398S as guidance:\np\u03b4(\u03b8i\u2080 | \u03b8i\u2081:T) = N(\u03b8i\u2080; \u03bc\u03b4(\u03b8i\u2081, \u03b8i\u02e2, t), \u03a3\u03b4(\u03b8i\u2081, \u03b8i\u02e2, t)),             (4)\nwhere \u03bc\u03b4 and \u03a3\u03b4 are the parameters output by neural networks with learnable parameters \u03b4."}, {"title": "4.2 Cognitive State Unification Module", "content": "Although using unified source domain abilities obtained through CDM to guide the reverse denoising process is a promising ap- proach, from the causal perspective of the model, as shown in the right side of Figure 2, the generated ability Y is influenced by domain-shared cognition X, domain-specific cognition A\u2081, and target domain ability B during the training phase. This is because we use random sampling of time steps for training, which retains some personalized information in the target domain ability vector even while introducing noise. In other words, \u03b8i\u2081 does not approx- imate standard Gaussian noise, which lacks extensive personalized features. However, the inclusion of domain-specific cognition A\u2081 increases the complexity of the model and reduces its generaliza- tion ability, making the model prone to overfitting. Therefore, we propose to explore the causal relationship between the generated result Y and the domain-shared cognition X and the pretrained ability B of the target domain, which can also be further decoupled into domain-specific cognition B\u2081 and domain-shared cognition X. Next, we will specifically analyze the impact of different factors on the generated result.\nFirst, there are three paths between domain-shared cognition X and the generated results Y: X \u2192 Y, X \u2190 {A, A\u2081} \u2192 Y, and X \u2190 {B, B\u2081} \u2192 Y, where the latter two paths are confounding paths. We will apply the backdoor criterion to explore the causal relationship of X \u2192 Y, which means we need to control the confounding variables C = {A, A\u2081} \u222a {B, B\u2081} to block the backdoor paths:\nP(Y | do(X)) = \u2211 P(Y | X, C = c)P(C = c),                 (5)\n      C\nwhere the first term represents the effect of X on Y while controlling for the confounding variables C, and the second term represents the joint probability distribution of the confounding variables. To control the confounding variables, inspired by [8], we decouple the domain-shared cognition \u03b8\u02e2\u02b0\u1d43\u02b3\u1d49 from all the prior abilities in the source and target domains:\n\u03b8\u02e2\u02b0\u1d43\u02b3\u1d49 = fp2 (\u03c3(fp1 (\u03b8i\u1d40 || \u2211 \u03b8m))),                    (6)\n                m~SM W m\u03b8\nwhere W \u2208 RM\u00d7d is a weight matrix used to map the pretrained abilities from multiple domains into the same feature space, where d is the feature dimension related to CDM, fp\u2081 and fp2 are linear layers with different parameters, \u03c3 refers to the activation function, and (||) denotes the concat operation. We then encourage this de- coupled domain-shared cognition to assist in predicting responses across all domains:\nL\u2081 = \u2211 ||ri \u2212 M\u03c6T(\u03b8\u02e2\u02b0\u1d43\u02b3\u1d49, q)||\u00b2\n  (ei,q,r)\u2208Rwarm\n\n+ \u2211 \u2211 ||ri\u1d50 \u2212 M\u03c6Sm(\u03b8\u02e2\u02b0\u1d43\u02b3\u1d49, q\u1d50)||\u00b2,\n          (7)\n     Sm=1 (ei,q\u1d50,ri\u1d50)\u2208RSm\nwhere \u03b8\u02e2\u02b0\u1d43\u02b3\u1d49 is encouraged to predict responses across all scenarios, including the target domain in the first term and all source domain response data in the second term. Next, the specific cognition of the target domain B\u2081 has two paths influencing the generated results: B\u2081\u2192 Y and B\u2081 \u2190 {X, B} \u2192 Y. We adjust the confounding variable {X, B} in the second confounding path to block the backdoor path. Specifically, we extract the specific cognition of the target domain, which will be used as input in the section 4.1 alongside obtaining the overall cognitive state of the target domain (corresponding to event B in the causal graph):\n\u03b8\u02e2\u1d56\u1d49\u1d9c\u2071\u1da0\u2071\u1d9c = fo4(\u03c3(fo3(\u03b8i\u1d40))),                  \n\u03b8\u1d9c\u1d52\u207f\u1d9c\u1d43\u1d57 = fps (\u03c3(fps (\u03b8\u02e2\u1d56\u1d49\u1d9c\u2071\u1da0\u2071\u1d9c || \u03b8\u02e2\u02b0\u1d43\u02b3\u1d49))),                  (8)\nwhere f{\u03c63,\u03c64,\u03c65,\u03c66} are four linear layers with different parameters. To learn the specific cognition of the target domain, we propose a novel decoupling strategy, which encourages the prediction of within-domain response results by domain-specific cognition while degrading performance in other domains:\nL\u2082 = \u2211 |ri \u2212 M\u03c6T(\u03b8i, q)|\u00b2\n    (ei,q,r)\u2208Rwarm\n + ||ri \u2212 M\u03c6T(\u03b8s\u1d56\u1d49\u1d9c\u2071\u1da0\u2071\u1d9c, q)||\u00b2\n + \u2211 \u2211 |ri\u1d50 \u2212 M\u03c6Sm(\u03b8i, q\u1d50)|\u00b2\n          m\u2208M (ei,q\u1d50,ri\u1d50)\u2208RSm\n\n + ||ri\u1d50 \u2212 M\u03c6Sm(\u03b8s\u1d56\u1d49\u1d9c\u2071\u1da0\u2071\u1d9c, q\u1d50)||\u00b2\n\n\n + \u2211Ee\u1d62\u2208E\u2080 ||\u03b8i\u1d40 \u2212 \u03b8i\u1d9c\u1d52\u207f\u1d9c\u1d43\u1d57 ||\u00b2.\n        (9)\nHere, the first term encourages the target domain-specific cognition to assist in predicting the response records in target domain, both in- dependently and in conjunction with domain-shared cognition. The second term aims to intentionally degrade prediction performance in the source domains, indicating that the target domain-specific cognition is not applicable to the source domains. The third term constrains the diagnostic abilities obtained from pre-training. Addi- tionally, to further control the influence of confounding variables on the generated results, we apply gradient-based orthogonal regular- ization [6] to ensure the independence of the above representations in the feature space:\nL\u2083 = |||\u2207\u03c6\u2081,\u03c6\u2082 L\u2081|| \u2212 ||\u2207\u03c6\u2083,\u03c6\u2084 L\u2082|||\u00b2 .            (10)\nThis approach minimizes the influence of domain-specific cognition when learning domain-shared cognition, and vice versa."}, {"title": "4.3 Harmonization and Calibration Module", "content": "To learn the parameters \u03b4 of the denoising network, DCSR aims to maximize the Evidence Lower Bound (ELBO) of the observed examinee ability \u03b8specifc.\nlog p(\u03b8Tspecifc) \u2265 E\nq(\u03b8\u2080:\u1d40|\u03b8Tspecifc)\n[log p\u03b4(\u03b8\u2080specifc|\u03b8\u2081:T)]\n:= Lb\n(T)\nM\nt=2\nEq(\u03b8\u2081|\u03b8\u2080:T,\u03b8Tspecifc) [KL(q(\u03b8t\u2212\u2081|\u03b8t, \u03b8Tspecifc) || p\u03b4(\u03b8t\u2212\u2081|\u03b8t)]\n:=\u2211Lt\u2212\u2081\n  (11)\nHere, the first term represents the reconstruction term, which re- covers the probability p(\u03b8Tspecifc).The second term of Lt\u2212\u2081 is the denoising matching term, which aligns the intractable posterior probability p\u03b4(\u00b7) with the tractable distribution q(\u00b7). To maintain training sta- bility and simplify computation, we ignore the learning of \u03a3\u03b4(\u00b7) and set it to a fixed value \u03b2t as in the forward process [11, 35]. The denoising matching term can then be further computed as:\nLt\u2212\u2081 = Eq(\u03b8t\u2212\u2081|\u03b8t,\u03b8Tspecifc)\n[ 1 (\u03b8t\u2212\u2081\u2212 \u03bc\u03b4(\u03b8t, \u03b8share, t))\n2\u03b2t (\u03bcq(\u03b8t,\u03b8Tspecifc) \u2212 \u03bc\u03b4(\u03b8t, \u03b8share, t))\u00b2].           (12)\nwhere \u03bcq(\u00b7) satisfies the tractable distribution q(\u03b8t\u2212\u2081|\u03b8t, \u03b8Tspecifc) = N(\u03b8t\u2212\u2081; \u03bcq(\u00b7), \u03b2tI).\nAlthough the above training objective ensures diversity in gen- erated samples, excessive randomness introduced by the diffusion model during training and inference stages may result in generated outcomes that do not match the requirements of the CAT system. Therefore, we constrain the generated ability vectors from two aspects: Consistency Constraint and Task-oriented Constraint.\n4.3.1 Consistency Constraint. aims to minimize the difference be- tween generated vectors and real data samples in the feature space, thus limiting uncertainty within a controllable range. Specifically, the Diffusion Module diffuses the domain-shared cognitive fea- tures into domain-specific cognitive features. Firstly, the general cognitive features of the target domain are generated as \u03b8T:\n\u03b8iT = \u03bc\u03b4(\u03b8Tspecifc,\u03b8share, tT:\u2081) + \u03b2t\u03b5, \u03b5 \u2208 (0, 1).           (13)\nDue to inherent randomness, the generated ability vectors are noisy, which is not conducive to reflecting the examinee's true ability. Thus, the consistency constraint aims to minimize the difference between \u03b8T and the target domain ability features e obtained through pre-training, which can be modeled as:\nLcc = \u2211 Be\u2081\u2208E\u2080 (\u03b8iT \u2212 \u03b8iT)\u00b2 .                  (14)\n4.3.2 Task-oriented Constraint. ensures that the generated ability features meet the requirements of the CAT task. Specifically, we sample some questions qj \u2208 Rwarm from the training set of the target domain and match the generated target domain general cognitive features with the CDM:\n\u2207\u03b8TLtc = \u2212 \u2211 (rijlog (M\u03c6T(\u03b8i, qj))\n       (ei,qj,rij)\u2208Rwarm\n + (1 \u2212 rij)log(1 \u2212 M\u03c6T(\u03b8i, qj))),         (15)\nwhere M\u03c6T(\u00b7) denotes the pre-trained CDM for the target domain, with its parameters frozen, and only the network parameters \u03bc\u03b5(\u00b7) of the denoising module are updated."}, {"title": "4.4 DCSR-CAT: Implementation of DCSR", "content": "In this section, we introduce DCSR-CAT as an implementation of DCSR to demonstrate its applicability to CAT.\nDuring this stage, DCSR no longer involves the forward process but directly takes pure noise \u03b5\u2080 ~ N(0, 1) as input. For a given cold-start examinee ei \u2208 Rcoold who has response records only in the source domain, we use the pre-trained CDM to obtain their prior ability and calculate their domain-shared cognitive features \u03b8share. It is noteworthy that since the original \u03b8\u1d40 is unknown, the average cognitive state of the target domain is used as a substitute:\n\u03b8avg = \u2211 M\u03c4 (ej),                  (16)\n           e\u2c7c\u2208Rwarm\nwhich retains domain information to some extent. Therefore, sub- stituting back into equation (6), we obtain the shared cognitive ability \u03b8share of the cold-start examinee ei. To improve inference efficiency, we use DPM-Solver [22] as a fast solver to efficiently obtain the initial ability of cold-start examinee:\n\u03b8 = Solver(p\u03b4(\u00b7), \u03b8share, \u03b5\u2080),                  (17)\nwhich is crucial for real-world applications. Our DCSR seamlessly integrates with any existing question selection algorithm \u03a0. At the beginning of the test, the selection process can be represented as:\nq\u00b9 \u2190 \u03a0(Qi | \u03b8),                  (18)\nwhich indicates that the item selection algorithm \u03a0 selects the appropriate item q\u00b9 in step t = 1 from the candidate item pool Qi for examinee ei based on the initial ability initialized by DCSR. The subsequent steps follow the general CAT system workflow."}, {"title": "5 Experiments", "content": "In this section, we conduct experiments with the aim of addressing the following questions:\n\u2022 RQ1: Can DCSR utilize prior information from a single do- main or multiple domains to improve the cold start perfor- mance of existing CAT systems?\n\u2022 RQ2: How effective are the key components of the DCSR framework?\n\u2022 RQ3: Does DCSR alleviate the issue of the question selection algorithm falling into local optima?"}, {"title": "5.1 Experimental Settings", "content": "In this section, we introduce the datasets, the selected baselines, and the application of CAT."}, {"title": "5.1.1 Dataset Description.", "content": "We conducted experiments on five real-world datasets collected from the publicly available PTADisc dataset [14], covering courses in Data Structures (DS), C, C++, Python, and Java programming languages. These datasets are sourced from the PTA platform, which records the learning perfor- mance of examinees across a series of courses. Each dataset provides the response records of the examinees and question-concept rela- tion, with each dataset considered as a distinct domain. We first excluded examinees with fewer than 100 response records from each dataset. The statistics of the processed datasets are shown in Table 1. For fairness in testing, we randomly split the filtered examinee records into a training set and a test set in an 80%:20% ratio. Examinees in the test set are considered cold-start examinees in the current (target) domain, meaning their response records in other domains are included in the training set. The training set is only used for training DSCR and the learning-based question selection algorithm, preventing data leakage."}, {"title": "5.1.2 Baseline Methods.", "content": "To demonstrate the effectiveness and com- patibility of our framework, we applied it to four widely used CAT systems, including the strategy-based Fisher [21], MAAT [1], BE- CAT [50], and the data-driven NCAT [49]. We select cross-domain baselines for comparison, where the Random and Oracle methods represent the lower and upper bounds of CAT cold-start perfor- mance, respectively.\n\u2022 Random: This method randomly predicts the initial ability of examinees in the target domain from a Uniform(0, 1) distribution, which is the most common method in existing CAT system.\n\u2022 MLCCM: A cross-course method based on meta-learning, applying the idea of meta-learning to learn cross-domain mapping functions from the training set.\n\u2022 Oracle: This method uses CDM to directly train the target domain ability from the response records of examinees in the target domain."}, {"title": "5.1.3 Evaluation Metrics.", "content": "The performance of our DCSR will be validated during the testing phase of the CAT system. We evaluate the accuracy of the final ability estimation by predicting examinees' binary responses to the test question set. For this purpose, we use the area under the Area Under ROC Curve (AUC) [2, 40] and Accuracy (ACC) as evaluation metrics."}, {"title": "5.1.4 Parameter Settings.", "content": "In the pre-training phase, for IRT, we set the latent feature dimension of both examinees and questions to 1, while for NCD, it is set to the number of knowledge concepts in the corresponding domain. Additionally, we uniformly set the batch size and learning rate to 32 and 0.002, respectively, for this phase. In the DSCR training phase, the forward process of the diffusion module is set to 1000 steps of noise addition, and DPM-solver [22] is used to accelerate sampling, which is performed in 30 steps. Meanwhile, the batch size and learning rate are fixed at 256 and 0, respectively, in this phase. We initialize all parameters using Xavier [10], and use the Adam [16] optimizer. In the CAT testing phase, the question selection algorithms follow the settings in the original papers, with the test length set to 1 and 5, and the random seed in all the above processes is set to 0. All experiments are conducted on an NVIDIA RTX4090 GPU."}, {"title": "5.2 Overall Performance (RQ1)", "content": "To verify the effectiveness of the proposed framework in addressing the CSIP challenge, we compared DCSR with other cross-domain baselines using both single and multi domain as prior knowledge, setting the question selection steps to 1 and 5. First, we explored leveraging a single domain as the source domain, encompassing six cross-domain scenarios. We rotated each dataset to play the role of the target domain, with other datasets serving as the source do- main. The experimental results presented in Table 2 show that our proposed DCSR not only outperforms all baselines in the CAT cold- start task across all six scenarios but"}]}