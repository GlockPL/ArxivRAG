{"title": "Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation", "authors": ["Hoigi Seo", "Wongi Jeong", "Jae-sun Seo", "Se Young Chun"], "abstract": "Large-scale text encoders in text-to-image (T2I) diffusion models have demonstrated exceptional performance in generating high-quality images from textual prompts. Unlike denoising modules that rely on multiple iterative steps, text encoders require only a single forward pass to produce text embeddings. However, despite their minimal contribution to total inference time and floating-point operations (FLOPs), text encoders demand significantly higher memory usage, up to eight times more than denoising modules. To address this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet effective pruning strategy specifically designed for text encoders in T2I diffusion models. Skrr exploits the inherent redundancy in transformer blocks by selectively skipping or reusing certain layers in a manner tailored for T2I tasks, thereby reducing memory consumption without compromising performance. Extensive experiments demonstrate that Skrr maintains image quality comparable to the original model even under high sparsity levels, outperforming existing blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory efficiency while preserving performance across multiple evaluation metrics, including the FID, CLIP, DreamSim, and GenEval scores.", "sections": [{"title": "1. Introduction", "content": "Diffusion generative models excel at tasks such as text-to-image (T2I) synthesis (Rombach et al., 2022; Podell et al., 2023; Esser et al., 2024; Chen et al., 2024), editing (Brooks et al., 2023; Cao et al., 2023; Kawar et al., 2023), video generation (Liu et al., 2024; Polyak et al., 2024), and 3D creation (Poole et al., 2022; Cao et al., 2023; Seo et al., 2023; Wang et al., 2024c). With modern architecture and large-scale text encoders, they produce high-quality images that closely match text prompts. Despite these successes, they require significant computational resources, especially memory, making deployment and scalability challenging.\nTo address these issues, research suggests enhancing the efficiency of the T2I diffusion model through strategies such as knowledge distillation (KD) (Castells et al., 2024b; Li et al., 2024b; Song et al., 2024b; Kim et al., 2024; Zhao et al., 2024), which transfers knowledge from larger models to smaller ones; pruning (Castells et al., 2024a; Ganjdanesh et al., 2024; Lee et al., 2024; Wang et al., 2024b), which eliminates superfluous weights; and quantization (Li et al., 2023; He et al., 2024; Ryu et al., 2025), which reduces precision by utilizing fewer bits. While effective, these methods target the denoising module. As shown in Fig. 1, the text encoders account for over 70% of the total parameters, but only 0.5% of floating-point operations (FLOPs), causing disproportionate memory usage. Despite this imbalance, efforts to reduce the text encoder size have been limited.\nLarge language models (LLMs) also face similar challenges, where model sizes result in significant computational and memory overhead, hindering their practical use. To address this problem, studies such as KD (Hsieh et al., 2023; Huang et al., 2023; Ko et al., 2024), quantization (Xiao et al., 2023; Ashkboos et al., 2024b; Lin et al., 2024), and pruning (Sun et al., 2023; Ashkboos et al., 2024a; Men et al., 2024; Song"}, {"title": "2. Related Works", "content": "As diffusion generative models scale, T2I synthesis has achieved impressive results in generating high-fidelity, text-aligned images. However, this advancement comes with significant computational and memory overhead. Previous research has primarily focused on optimizing the efficiency of the denoising module through methods such as knowledge distillation (Li et al., 2024b; Song et al., 2024b; Castells et al., 2024b; Zhao et al., 2024; Kim et al., 2024), pruning model weights (Fang et al., 2023; Ganjdanesh et al., 2024; Wang et al., 2024b; Castells et al., 2024a; Lee et al., 2024), and quantization of weights to lower precision bits (Li et al., 2023; He et al., 2024; Li et al., 2024a; Wang et al., 2024a; Ryu et al., 2025). While these approaches effectively reduce the computational and memory costs of the pipeline, they overlook the substantial memory burden imposed by the text encoder, which remains largely underexplored. To address this gap, we propose a targeted pruning strategy for the text encoder of a T2I diffusion model, which allows more memory efficient T2I synthesis with comparable performance."}, {"title": "2.1. Efficient diffusion model", "content": "As diffusion generative models scale, T2I synthesis has achieved impressive results in generating high-fidelity, text-aligned images. However, this advancement comes with significant computational and memory overhead. Previous research has primarily focused on optimizing the efficiency of the denoising module through methods such as knowledge distillation (Li et al., 2024b; Song et al., 2024b; Castells et al., 2024b; Zhao et al., 2024; Kim et al., 2024), pruning model weights (Fang et al., 2023; Ganjdanesh et al., 2024; Wang et al., 2024b; Castells et al., 2024a; Lee et al., 2024), and quantization of weights to lower precision bits (Li et al., 2023; He et al., 2024; Li et al., 2024a; Wang et al., 2024a; Ryu et al., 2025). While these approaches effectively reduce the computational and memory costs of the pipeline, they overlook the substantial memory burden imposed by the text encoder, which remains largely underexplored. To address this gap, we propose a targeted pruning strategy for the text encoder of a T2I diffusion model, which allows more memory efficient T2I synthesis with comparable performance."}, {"title": "2.2. Blockwise pruning for LLMs", "content": "Although LLMs show promising performance in various tasks, their size often limits practical deployment. To address this problem, model compression techniques have been developed, with pruning emerging as a promising solution due to its ability to reduce the parameter count with minimal retraining. Notably, blockwise pruning of transformers (Men et al., 2024; Yang et al., 2024; Zhang et al., 2024) effectively reduces parameters while preserving performance.\nShortGPT (Men et al., 2024) proposed a method to prune blocks to the desired sparsity by removing them one by one and assigning a Block Influence (BI) score based on changes in cosine similarity and pruning those with lower BI first. LaCo (Yang et al., 2024) introduced a strategy to reduce the size of LLMs by merging the weights of adjacent transformer blocks, effectively compressing the model. FinerCut (Zhang et al., 2024) refined this approach by pruning sub-blocks, composed of Multi-Head Attention (MHA) and Feed-Forward Network (FFN) layers with normalization, in a fine-grained manner. It sequentially pruned sub-blocks while partially considering interactions between blocks, reducing the performance gap with the dense model.\nDespite these advances, text encoder compression in diffu-"}, {"title": "3. Method", "content": "Skrr is built around two main parts: Skip identifies layers to prune, and Re-use selects layers to reuse from the re-mained ones. During the Skip phase, each multi-head attention (MHA) and feed-forward network (FFN) sub-block is individually evaluated for its importance using a T2I diffusion-tailored metric. The sub-blocks are then ranked based on their significance. To optimize the pruning process, blocks with low importance are removed sequentially while exploring multiple possible combinations using a beam search-based algorithm. The Re-use phase evaluates each layer to reuse a layer based on the metric leveraged in Skip phase to the original output, ensuring that important information is conserved, thus reducing performance loss. The overall framework of Skrr is depicted in Fig. 2."}, {"title": "3.1. Skip Algorithm", "content": "Prior work (Men et al., 2024; Yang et al., 2024; Zhang et al., 2024) shows transformer blocks can be pruned based on output similarity in LLMs. We extend this analysis to text encoders in diffusion models, especially T5-XXL (Raffel et al., 2020), widely used in T2I models, as shown in Fig. 3a. We observed a high degree of similarity between the hidden states of adjacent blocks. This strong similarity underscores the redundancy in the model and confirms the potential to prune blocks without significantly compromising performance.\nFor effective block removal, it is crucial to evaluate impact of pruning each block on output. A strong metric must be defined to measure text embedding changes of post-removal. Given that text embeddings affect image quality in text-to-image models, choosing the right metric is vital. Current T2I diffusion models predominantly employ transformer frameworks to synthesize images from input noise and text embeddings. However, text embeddings from the output of the text encoder are not used as is. They undergo alignment through a single linear layer or multi-layer perceptron (MLP), which is represented as:\n$f = proj(E(c; \\Theta_{\\text{text.}}); \\Theta_{\\text{denoise.}})),$\nwhere c is the input prompt, $E(\\cdot; \\Theta_{\\text{text.}})$ is the text encoder parameterized with $\\Theta_{\\text{text.}}$, proj($\\cdot; \\Theta_{\\text{denoise.}}$) is the projection layer in the denoising module for condition vector from the text encoder. Using the features extracted in this manner, the importance of a block can be evaluated by comparing the similarity between the feature $f_{\\text{dense}}$ of the dense model and the feature $f_{\\text{skip}}$ of the model that skips (prunes) a block."}, {"title": "3.2. Re-use Algorithm", "content": "Despite extensive research on methods such as recurrent networks (Sherstinsky, 2020; Gu & Dao, 2023) which loop the output of the network back into the input for efficiency, the practice of reusing specific layers in neural networks by reintegrating hidden states into internal layers remains understudied. We conducted a feasibility study to investigate whether the internal components of the T2I diffusion text encoder serve analogous functions (see Fig. 3b). We randomly sampled tokens from the embedding, passed them through each transformer block as a fixed input, and measured the similarity of their output. The results indicate significant similarity between adjacent blocks, suggesting that performance can be restored by reintroducing non-omitted layers into adjacent skipped layers. We also verified the existence of condition for Re-use that achieves a tighter error bound compared to Skip alone. The existence is formalized in Theorem 3.2, which theoretically demonstrates the advantages of incorporating the Re-use phase. To establish this result, we first introduce the following lemma.\n(Error bound of two transformers). Let M : (x, 0) \u2192 Rd be an L-block transformer with input x \u2208 Rd and parameters 0 = (01, ..., 0L), defined as:\nM = ((FL + I) \u25cb (FL\u22121 + I) \u25cb \u22ef \u25cb (F\u2081 + I)),\nwhere Fi: (zi, 0i) \u2192 zi+1 is the i-th block with parameters \u03b8i, and zi+1 \u2208 Rd.\nWith each block Fi being Li-Lipschitz in the input that satisfies follows.:\n||Fi(z; \u03b8i) \u2212 Fi(z'; \u03b8i)|| \u2264 Li||z \u2212 z'||\nAnd Mi-Lipschitz in the parameters which satisfies follows.:\n||Fi(z; \u03b8i) \u2212 Fi(z; \u03b8\u0302i)|| \u2264 Mi||\u03b8i \u2212 \u03b8\u0302i||\nThen, for any two parameter sets \u03b8 = (\u03b81, ..., \u03b8L) and \u03b8\u0302 = (\u03b8\u03021, ..., \u03b8\u0302L), the following holds:\n||M(x; \u03b8) \u2212 M(x; \u03b8\u0302)|| \u2264 \u2211 (\u220f (1 + Lk)) Mi||\u03b8i \u2212 \u03b8\u0302i||\ni=1 k=i+1\nWith Eq. A1, we can formulate the difference of two hidden states between dense model and modified model as follows.:\n||zi+1 \u2212 z\u0302i+1|| = ||[zi + F(zi; \u03b8i)] \u2212 [z\u0302i + F(z\u0302i; \u03b8\u0302i)]||\nBy the triangle inequality:\n||zi+1 \u2212 z\u0302i+1|| \u2264 ||zi \u2212 z\u0302i|| + ||F(zi; \u03b8i) \u2212 F(z\u0302i; \u03b8\u0302i)||\nWe now split term (A) with the assumptions in Eq. A2 and Eq. A3:\n||F(zi; \u03b8i) \u2212 F(z\u0302i; \u03b8\u0302i)|| \u2264 ||Fi(zi; \u03b8i) \u2212 Fi(z\u0302i; \u03b8\u0302i)|| + ||Fi(z\u0302i; \u03b8i) \u2212 Fi(z\u0302i; \u03b8\u0302i)||\n< Li||zi - z\u0302i|| + Mi||\u03b8i - \u03b8\u0302i ||\nSo combining,\n||zi+1 - z\u0302i+1|| \u2264 ||zi - z\u0302i|| + Li|| zi - z\u0302i|| + Mi||\u03b8i - \u03b8\u0302i||\nHence,\n||zi+1 - z\u0302i+1|| \u2264 (1 + Li)||zi - z\u0302i|| + Mi||\u03b8i - \u03b8\u0302i||\nDefine the error at block i as\nEi = ||zi - z\u0302i||\nEq. A11 becomes\nEi+1 \u2264 (1 + Li)Ei + Mi||\u03b8i - \u03b8\u0302i||\nWe start from E\u2081 = ||z1 \u2212 z\u03021|| = ||x - x|| = 0 (since both netowrks see the same input x). Thus:\nE2 \u2264 (1 + L\u2081)E1 + M1||\u03b81 - \u03b8\u03021|| = M1||\u03b81 - \u03b8\u03021||\nE3 \u2264 (1 + L2)E2 + M2||\u03b81 - \u03b8\u03021|| \u2264 (1 + L2)[M1||\u03b81 - \u03b8\u03021|| + M2||\u03b82 - \u03b8\u03022||]\nIf we do recursive telescoping over all blocks, we get\nEL+1 = ||zL+1 - z\u0302L+1|| < \u2211 (\u220f (1 + Lk)) Mi||\u03b8i - \u03b8\u0302i||\ni=1 k=i+1\nwhich is a constant for each i-th block. For the unskipped block, \u03b8i = \u03b8\u0302i, ||\u03b8i \u2212 \u03b8\u0302i|| = 0. So, we define a parameter set \u03b8\u0302skip that exclude \u03b8i in the parameter set \u03b8. And we can rewrite the Lemma 3.1 as follows:\n||M(x; \u03b8) \u2212 M(x; \u03b8\u0302skip)|| \u2264 \u2211 Ci||\u03b8i - \u03b8\u0302i||\ni\u2208S\nwhere S is a set of skipped (pruned) block indices. And skipped block can be represented as follows:\n\u03b8i,Skip = 0\nThen, we can manipulate the error bound Eq. A21 with following.\n||M(x; \u03b8) \u2212 M(x; \u03b8\u0302skip)|| \u2264 \u2211 Ci||\u03b8i|| = USkip\ni\u2208S\nRe-use substitute the skipped weight to the weight of adjacent block:\n\u03b8i,Re-use = \u03b8\u0302j\nIf we make set of i that satisfies Eq. A18 and denote as R then apply Re-use,\n||M(x; \u03b8) \u2212 M(x; \u03b8\u0302skip, Re-use) || < \u2211 Ci|\u03b8i|| + \u2211 ||\u03b8j - \u03b8\u0302j|| = USkip, Re-use\ni\u2208S\u2227i\u2209R j\u2208R\nSince the R consists of block indices that satisfies Eq. A18, we have shown:\nUSkip, Re-use = \u2211 Ci|\u03b8i| + \u2211 ||\u03b8j - \u03b8\u0302j|| < \u2211 Ck||\u03b8k|| = USkip\ni\u2208S\u2227i\u2209R j\u2208R k\u2208S"}, {"title": "4. Experiments", "content": "Baselines. To evaluate the performance of Skrr, we compared multiple LLM-based blockwise pruning techniques for T2I synthesis, including ShortGPT (Men et al., 2024), LaCo (Yang et al., 2024) and FinerCut (Zhang et al., 2024). Pruning in diffusion pipelines specifically targets the T5-XXL (Raffel et al., 2020), which constitutes most of the parameter size. Detailed configurations and implementation specifics are available in Appendix Sec. B.1 and Sec. B.2.\nMost blockwise pruning methods rely on a calibration dataset to identify and remove less influential blocks. To this end, we constructed a calibration set by sampling 1k text prompts from the CC12M (Changpinyo et al., 2021), specifically tailored for T2I synthesis. The detailed configuration and example are provided in Appendix Sec. B.3."}, {"title": "4.1. Quantitative results", "content": "We evaluated the performance of the Skrr and baseline models with four metrics: Fr\u00e9chet Inception Dis-"}, {"title": "5. Discussion", "content": "Previously, we noted better FID results with T2I synthesis when using a text encoder with skipped layers. Beyond CFG, other guidance methods in diffusion models include perturbed attention guidance (PAG) (Ahn et al., 2024) and autoguidance (Karras et al., 2024), both of which approximate the unconditional score by modifications of denoising networks. We propose that layer skipping or merging affects the null text embedding similar to these methods. To verify this hypothesis, we perturbed f\u00f8, as described below.\nf\u00f8 = xz + f\u00f8, z ~ N(0, I),\nwhere z is a random vector sampled from normal distribution and A is small scalar value. The FID and CLIP scores of the original model and the unconditional output with small perturbations applied to f\u00f8 were measured on the MS-COCO 30k dataset. The results show that the FID decreases, indicating that the fidelity improved when perturbations are introduced to the unconditional feature f\u00f8. Detailed results and configurations are provided in Appendix Sec. C.3."}, {"title": "6. Conclusion", "content": "In this paper, we introduce Skip and Re-use Layers (Skrr), an effective compression method for the text encoder in text-to-image (T2I) diffusion models. Skrr integrates three key components: (1) a pruning metric based on the Skrr dot product to identify redundant sub-blocks, (2) a beam search-based algorithm to account for interactions between transformer blocks during pruning, and (3) a re-use mechanism"}]}