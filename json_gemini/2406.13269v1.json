{"title": "Investigating Low-Cost LLM Annotation for Spoken Dialogue Understanding Datasets", "authors": ["Lucas Druart", "Valentin Vielzeuf", "Yannick Est\u00e8ve"], "abstract": "In spoken Task-Oriented Dialogue (TOD) systems, the choice of the semantic representation describing the users' requests is key to a smooth interaction. Indeed, the system uses this representation to reason over a database and its domain knowledge to choose its next action. The dialogue course thus depends on the information provided by this semantic representation. While textual datasets provide fine-grained semantic representations, spoken dialogue datasets fall behind. This paper provides insights into automatic enhancement of spoken dialogue datasets' semantic representations. Our contributions are three fold: (1) assess the relevance of Large Language Model fine-tuning, (2) evaluate the knowledge captured by the produced annotations and (3) highlight semi-automatic annotation implications.", "sections": [{"title": "1 Introduction", "content": "Digitization enables many tasks to be automated, nevertheless users sometimes require assistance to perform complex tasks such as making a reservation at a restaurant or booking a hotel room. Task-Oriented Dialogue (TOD) systems are designed to assist such users. A common approach to implement them is to break the problem down to three iterative steps [23]: updating the system's understanding of the users' needs, reasoning over a database and domain knowledge to choose the next action and providing the user an answer. Those systems often rely on transfer learning which requires annotated datasets. However only few datasets provide aligned dialogue recordings with turn-level contextual semantic annotations. Therefore the dialogue understanding community has mainly focused on textual datasets creating a usage discrepancy [8].\nDialogue systems rely on a chosen semantic representation to infer the next action(s). The gap between textual semantic representations and spoken ones provides an explanation for the observed discrepancy. Indeed, spoken dialogue"}, {"title": "2 Method", "content": "Given the heavy cognitive load required to annotate dialogue turns with a contextual version of AMR, we attempt to automatize as much as we can while keeping high quality expectations. To do so we define a structured contextual meaning representation fitting the dataset's use case in section 2.1, set an annotation pipeline in section 2.2 and evaluate this system in section 3.2."}, {"title": "2.1 Structured Contextual Meaning Representation", "content": "Two adaptations are required to fit our dataset's use case: defining an ontology fitting the hotel booking task and handling cross-turn references.\nThe defined ontology comprises three type of concepts: domain related concepts (e.g. hotel) which represent hotel reservation elements, operators (e.g. et) which enable to apply an operation to other concept(s) and general purpose concepts (e.g. adresse) which are domain-agnostic. Each concept has a unique identifier which enables exact cross-turn references. Concepts can be linked together or to transcription spans called literals. The edge's label depends on the pair of concepts considered.\nCompared with the current labeled span annotation, this annotation takes into account the interactions between concepts and highlights the implicit concepts which are not uttered but group other concepts together (e.g. hotel in"}, {"title": "2.2 Annotation Pipeline", "content": "Our annotation pipeline comprises the following four steps which can be repeated over several iterations as illustrated in Figure 2.\nHuman annotation. The first step consists in having 10 trained human annotators annotate a subset of the corpus following the defined ontology. We focused on the dataset's test set which comprises 208 dialogues with 30% of them annotated by multiple annotators. The reported annotator agreement reaches an average semantic match (smatch) [5] of 77.28%. We present a few statistics of the obtained annotations in Table 1 which highlight the importance of structured annotation. For each dialogue, we select the annotation with the least errors, according to a master-annotator, as the final one. Our goal is to leverage this set of clean annotations to produce the remaining annotations.\nIn order to track the quality of the automatic annotations, we further annotate another set of 22 dialogues to form a 10% fold of unseen annotations."}, {"title": "Fine-Tuning", "content": "We then fine-tune a Mistral-7B LLM [14] on the human annotated data with Low Rank Adaptation (LoRA) [12]. Given a pre-trained weight matrix $W \\in \\mathbb{R}^{d\\times k}$, we fine-tune a delta matrix $\\Delta W = BA$ such that $B\\in \\mathbb{R}^{d\\times r}$ and $A \\in \\mathbb{R}^{r\\times k}$ with $r < \\min(d, k)$. We then only train this factorized matrix $\\Delta W$ with the forward pass thus modified to:\n$y = Wx + \\frac{\\alpha}{r}\\Delta Wx$, with $\\alpha = \\frac{m}{n}r, m \\in \\mathbb{N}$\nThe LLM is fed the following prompt template in which it is tasked to provide a structured annotation of the last turn of a sequence of t (agent, user) speaker turns transcription pairs\u00b9."}, {"title": "Constrained Generation", "content": "In order to reduce hallucinations and ensure that every output is correctly formatted while preserving originality, we implement a grammar constrained decoding [9] and allow up to 256 newly generated tokens. At each decoding step, the grammar constraint simply sets the log-probabilities of the forbidden tokens of the vocabulary to $\\infty$. The next token's probability distribution is thus restricted to grammar valid tokens. The main challenge consists in decoding smaller units (i.e. tokens) than the grammar terminals while ensuring that the decoding remains in valid grammar states, especially when reaching literals which are supposed to be open vocabulary. In that case, our constraint decoding only allows tokens from the speakers turns until another quote is decoded. While more restrictive, this ensures that literals match speaker's transcription spans. Algorithm 1 presents how we select the set of allowed token level decoding paths to transition from one grammar valid state to another."}, {"title": "Annotation Evaluation", "content": "Finally we evaluate the generated annotations with the AMR Semantic Match (smatch) score [5]. This metric searches for the best variable alignment and then computes the F1 score of the matching triples for this alignment. While it accounts for potential variable permutations, it remains a matching metric. Indeed, an error in the name of a concept or in a literal value invalidates the whole triple. Yet, averaged over many annotations, it provides a valuable insight in the structural quality of a set of annotations. It thus helps us to choose the best LLM annotator and to quantify the annotation quality over the held-out dialogues. Additionally, we track the number of ontology errors."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Dataset", "content": "The MEDIA dataset [7] comprises over 1,000 French hotel reservation dialogue recordings annotated with turn-level span concept labels over the manual transcriptions. It is recognized as a challenging spoken dialogue understanding dataset [2]. Indeed the Wizard-of-Oz collection protocol provided loosely scripted scenarios which fostered rich and natural interactions."}, {"title": "3.2 Experimental Setup", "content": "This paper addresses three concerns around the annotation pipeline described in the sections above:"}, {"title": "3.3 Relevance of LLM Fine-Tuning", "content": "We first compare a standard prompting strategy to fine-tuning one. Prompting can be done on any model including commercial ones which are often among the top performers of leaderboards. However, their token pricing policies may become prohibitive for iterative pipelines. On the other hand, fine-tuning requires a GPU and careful hyper-parameters setting but enables fine-grained customization.\nFigure 3 presents the similarity (computed with smatch) distributions of pairwise comparisons between human and automatic annotations. We observe that a fine-tuned Mistral-7B can achieve higher similarities than Chat-GPT 3.5. However both LLMs remain far from human-wise annotation similarities indicating that iterative annotation should be promising. Therefore we choose the fine-tuning approach over the prompt-based one."}, {"title": "3.4 Knowledge captured by automatic annotations", "content": "We perform a hyper-parameter grid search over $\\alpha \\in [r, 2r], r \\in [16, 128, 512]$ and learning rate $\\eta \\in [1,4,8] \\times 10^{-4}$ and select $r = \\alpha = 512$ and $\\eta = 4 \\times 10^{-4}$. We present the results obtained with this model over the clean set and the unseen set in Table 2.\nWe first observe the importance of the dialogue history for such fine-grained annotations as the Clean last turn model which is only provided the last dialogue turn performs worse than the Clean history model.\nThen, the grammar constrained decoding seems to behave as a complex temperature: it tends to make the model more verbose thus never predicting empty trees. This constrained decoding seems to disturb the calibration of the model's conditional probabilities. It improves the full tree annotations but makes the model more verbose hence reducing the performance on empty trees."}, {"title": "4 Discussion", "content": "This work focuses on the MEDIA dataset [7] since it is recognized as challenging [2] and hence provides complex enough situations to require fine-grained annotations. However it is a single domain French dataset. Performances might be better on higher resources languages such as English and worse on lower resources languages. While this paper focuses on ontology specific to MEDIA's use case, designing a similar one for multi-domain use-cases should be manageable in most cases.\nWith the recent hype around Large Language Models, a huge quantity of resources has been made available. This paper only focuses on well-established models and techniques. To the best of our knowledge prompt design remains an unsettled topic. We thus experimented with several formulations and selected the one which seemed the most promising. There might be better formulations than the one proposed in this paper."}, {"title": "5 Conclusion", "content": "Our journey towards fine-grained annotations for spoken dialogue datasets has led us to the realm of LLMs in order to comply with our low annotation budget. This exploratory work provides valuable insights to the community on the design of complex automatic annotation for spoken dialogue datasets. Indeed, our method may accelerate manual annotation and/or be included in a fully automated setting by carefully selecting the training examples. We highlight that open-weights LLMs fine-tuning is relevant for this annotation setup since it enables faster iterations than prompting commercial models while remaining competitive. We also propose a grammar constrained decoding strategy which struggles with non-informative dialogue turns but improves the annotation of correctly annotated dialogue turns. Our produced annotations contain part of the knowledge from the human annotations, and can be effectively learned by a model."}, {"title": "Disclosure of Interests", "content": "The authors have no competing interests to declare that are relevant to the content of this article."}]}