{"title": "BEYOND THE BOUNDARIES OF PROXIMAL POLICY OPTIMIZATION", "authors": ["Charlie B. Tan", "Edan Toledo", "Benjamin Ellis", "Jakob N. Foerster", "Ferenc Husz\u00e1r"], "abstract": "Proximal policy optimization (PPO) is a widely-used algorithm for on-policy rein-forcement learning. This work offers an alternative perspective of PPO, in which it is decomposed into the inner-loop estimation of update vectors, and the outer-loop application of updates using gradient ascent with unity learning rate. Using this insight we propose outer proximal policy optimization (outer-PPO); a frame-work wherein these update vectors are applied using an arbitrary gradient-based optimizer. The decoupling of update estimation and update application enabled by outer-PPO highlights several implicit design choices in PPO that we challenge through empirical investigation. In particular we consider non-unity learning rates and momentum applied to the outer loop, and a momentum-bias applied to the in-ner estimation loop. Methods are evaluated against an aggressively tuned PPO baseline on Brax, Jumanji and MinAtar environments; non-unity learning rates and momentum both achieve statistically significant improvement on Brax and Jumanji, given the same hyperparameter tuning budget.", "sections": [{"title": "1 INTRODUCTION", "content": "Proximal policy optimization (PPO) (Schulman et al., 2017b) is ubiquitous within modern reinforce-ment learning (RL), having found success in domains such as robotics (Andrychowicz et al., 2020b), gameplay (Berner et al., 2019), and research applications (Mirhoseini et al., 2021). Given it's ubiq-uity, significant research effort has explored the theoretical (Hsu et al., 2020; Kuba et al., 2022) and empirical (Engstrom et al., 2020; Andrychowicz et al., 2020a) properties of PPO.\nPPO is an on-policy algorithm; at each iteration it collects a dataset using the current (behavior) policy. This dataset is used to construct a surrogate to the true objective, enabling gradient-based optimization while seeking to prevent large changes in policy between iterations, similar to trust region policy optimization (Schulman et al., 2017a). The solution to the surrogate objective is then taken as the behavior parameters for the following iteration, defining the behavior policy with which to collect the following dataset. The behavior policies are therefore exactly coupled with the preced-ing surrogate objective solution.\nIn this work we instead consider the inner-loop optimization of each surrogate objective to estimate an update vector, which we name the outer gradient. A trivial result follows that the outer loop of PPO can be viewed to update the behavior parameters using unity learning rate \u03c3 = 1 gradient ascent on the outer gradients. Using this insight we propose outer-PPO, a novel variation of PPO that employs an arbitrary gradient-based optimizer in the outer loop of PPO. Outer-PPO decouples the estimation and application of updates in way not possible in standard PPO. An illustration of outer-PPO applying a learning rate greater than unity is provided in figure 1. The new behaviors enabled by outer-PPO raise several questions related to implicit design choices of PPO:\nQuestion 1. Is the unity learning rate always optimal?\nQuestion 2. Is the independence (lack of prior trajectory information e.g momentum) of each outer update step always optimal?\nQuestion 3. Is initializing the inner loop surrogate objective optimization at the behavior parame-ters (without exploiting prior trajectory / momentum) always optimal?"}, {"title": "2 BACKGROUND", "content": "We consider the standard reinforcement learning formulation of a Markov decision process M = (S, A, T, r, \u03b3), where S is the set of states, A is the set of actions, T : S \u00d7 A \u2192 \u2206(S) is the state transition probability function, r : S \u00d7 A \u2192 \u2206(R) is the reward function, and \u03b3\u2208 [0,1] is the discount factor. We use the notation \u2206(X) to denote the probability distribution over a set X. The reinforcement learning objective is to maximize the expected return E[Gt] = \u0395\u03c0[\u03a3 \u03b3rt] given a policy \u03c0 : S \u2192 \u2206(A) defining the agent behavior. In actor-critic policy optimization the policy is explicitly represented as a parametric function \u03c0 : S \u00d7 \u03b8 \u2192 \u2206(A), and a value function V:Sx\u03b8V \u2192 R is employed to guide optimization. In deep RL (Mnih et al., 2015; Silver et al., 2017) neural networks are used for the policy and value functions, for ease of notation we consider \u03b8\u2208R(d\u03c0+dv) as the concatenation of the respective weight vectors."}, {"title": "2.2 PROXIMAL POLICY OPTIMIZATION", "content": "Proximal policy optimization was proposed by Schulman et al. (2017b), and has since become one of the most popular algorithms for on-policy reinforcement learning. At each iteration k a dataset of transitions Dk is collected using policy \u03c0(\u03b8k), and advantages Ak are estimated using generalized advantage estimation (GAE) (Schulman et al., 2018). The transition dataset and advantages are then used within an inner optimization loop, in which the policy parameters \u03b8\u03c0 are optimized with respect to a given surrogate objective along with the value parameters \u03b8V. Psuedocode for a single iteration of PPO is provided in algorithm 1, where INNEROPTIMIZATIONLOOP is defined in appendix A. The full algorithm updates parameters iteratively by \u03b8k+1 \u2190 PPOITERATION(\u03b8k)."}, {"title": "2.3 TRUST REGIONS", "content": "A trust region is a region surrounding an optimization iterate \u03b8k within which we permit our algorithm to update the parameters to \u03b8k+1. In TRPO, a trust region surrounding the be-havior parameters is explicitly defined as the region in parameter space \u03b8 satisfying Es~Dk [DKL (\u03c0(\u03b8k|s) || \u03c0(\u03b8|s))] < \u03b4. Optimizing subject to this constraint prevents large changes in the policy between successive iterations, and gives rise to a guarantee of monotonic improve-ment. Similarly, if the clipped surrogate objective of PPO is replaced with a KL penalty L\u03c0 (\u03b8) = Es,a~Dk [\u03c1(\u03b8)\u00c2 \u2013 \u03b2DKL (\u03c0(\u03b8k|s) || \u03c0(\u03b8|s))], a trust-region is implicitly defined for some \u03b4. Both TRPO and PPO-KL approximate the natural policy gradient (Kakade, 2001), (Hsu et al., 2020); the steepest direction in the non-Euclidean geometry of policy space induced by the Fisher information metric."}, {"title": "3 OUTER-PPO", "content": "In equation 3 we define the outer gradient of PPO.\ng\u03b8 (\u03b8) = PPOITERATION(\u03b8) \u2013 \u03b8\nThe behavior parameter update of PPO \u03b8k+1 \u2190 PPOITERATION(\u03b8k) can now be equivalently expressed as \u03b8k+1 \u2190 \u03b8k + g\u03b8(\u03b8k). Evidently, PPO is exactly gradient ascent, with a constant learning rate \u03c3 = 1, on its outer gradients. With this simple result established, we propose a family of methods employing arbitrary optimizers on the PPO outer loop, denoted as outer-PPO. As an illustrating example, a comparison of standard PPO and outer-PPO with non-unity learning rates is provided in algorithms 2 and 3. We additionally propose a closely-related method for biasing the inner estimation loop using the prior (outer) trajectory, denoted as biased initialization."}, {"title": "3.1 OUTER LEARNING RATES", "content": "Varying the outer learning rate scales the update applied to the behavior parameters, as defined in algorithm 3 and illustrated in figure 1. The behavior of scaling the outer gradient can not be directly recovered by varying the PPO hyperparameters.\nAn outer learning rate \u03c3 < 1 interpolates between the behavior parameters \u03b8k and inner-loop solu-tion \u03b8*, encoding a lack of trust in the outer gradient estimation. Whilst the magnitude of the outer gradient can be reduced by varying hyperparameters, such as the clipping \u03f5 or number of inner loop iterations, the outer gradients are inherently noisy due to stochastic data collection and inner-loop optimization. PPO is additionally able to irreversibly escape its clipping boundary (Engstrom et al., 2020), and can drift far from the behavior policy given sub-optimal surrogate objective parameters.\nThese effects motivate the exploration of methods that attenuate the outer updates, irrespective of the outer gradient magnitude. In contrast, a learning rate \u03c3 > 1 amplifies the update vector, encoding confidence in its direction. Whilst the outer gradient magnitude could be increased by varying the PPO hyperparameters, in particular \u03f5, increasing the size of the trust region may lead the policy to drift to beyond the region of policy space where the dataset Dk collected with policy \u03c0\u03b8 can be con-sidered representative of the environment dynamics, motivating the amplification of well-estimated outer gradients over increases to trust region size."}, {"title": "3.2 MOMENTUM", "content": "Whilst permitting novel behavior, outer-LR PPO still only exploits information from a single PPO iteration when updating the parameters. Applying momentum breaks this design choice; instead of directly updating the parameters with the scaled outer gradient og\u03b8k, we update using the Nesterov momentum rule as in algorithm 4 and illustrated in figure 2a.\nIn supervised learning momentum is motivated using pathological curvature, and the ability to 'build up speed' (Sutskever et al., 2013). Given that the outer gradient is the solution to a surrogate objec-tive, we do not anticipate pathological curvature presenting to the outer optimizer. However, similar to learning rates \u03c3 > 1 the increase in effective learning rate of momentum may assist in learning. Momentum can also be motivated here using resilience to noise; since any given collected dataset will be noisy, the outer gradient is also noisy. As using a learning rate \u03c3 < 1 corresponded to a lack of trust in any given outer gradient, using momentum corresponds to a smoothing process, where we at no point solely trust a single outer gradient to be accurate."}, {"title": "3.3 BIASED INITIALIZATION", "content": "Outer-PPO Nesterov applies a momentum-based update to the outer loop of PPO. This update occurs before the successive iteration's dataset Dk+1 is collected, hence the momentum directly determines"}, {"title": "4 EXPERIMENTS", "content": "We experiment on subsets of the Brax (Freeman et al., 2021), Jumanji (Bonnet et al., 2024), and MinAtar (Young & Tian, 2019) environment suites, selected as diverse examples of continuous and discrete control problems. We employ the absolute evaluation procedure recommended by Colas et al. (2018) and Gorsane et al. (2022). Absolute evaluation entails intermediate evaluations during training and a final, large-scale evaluation using the best policy identified to give the 'absolute' performance. We train with a budget of 1 \u00d7 107 transitions, perform 20 intermediate evaluations, and conduct final evaluation using 1280 episodes.\nRecognizing the hyperparameter sensitivity of deep reinforcement learning (Hsu et al., 2020; En-gstrom et al., 2020; Andrychowicz et al., 2020a), we commit significant resources to establishing a strong PPO baseline and fair evaluation. We sweep for a budget of 600 trials per task using the tree-structured Parzen estimator (Bergstra et al., 2011; Watanabe, 2023). Each trial is the mean of 4 agents, trained using seeds randomly sampled from [0, 10000], for a total of 2400 agents trained per task during baseline tuning. A total of 11 hyperparameters are tuned, each with extensive ranges considered. Full descriptions of the hyperparameter sweep ranges, and the optimal values identified are provided in appendix C."}, {"title": "4.2 DEFINED EXPERIMENTS", "content": "We consider the three outer-PPO methods defined in section 3; outer-LR, outer-Nesterov and bi-ased initialization, addressing questions 1, 2, and 3 respectively. The outer-PPO methods are grid searched using increments of 0.1 for all hyperparameters. Outer-LR has a single hyperparameter; outer learning rate \u03c3, which is swept over the range [0.1, 4.0] (40 trials). Nesterov-PPO two hy-perparameters; \u03c3 \u2208 [0.1, 1.0] and momentum factor \u03bc \u2208 [0.1, 0.9] (90 trials). Biased initialization also has two hyperparameters; bias learning rate \u03b1 \u2208 [0.1, 1.0], bias momentum \u03bc \u2208 [0.0, 0.9] (100 trials). The base PPO hyperparameters are frozen from the baseline sweep up to the 500th trial, such that no method is tuned using a budget greater than the 600 trials used by the baseline. The optimal hyperparameters identified for each sweep are provided in the figures of appendix E."}, {"title": "5 RESULTS", "content": "We first consider the performance of the three outer-PPO methods, where the optimal hyperparam-eters identified from the grid sweeps per-environment are employed. In figures 3 and 4 we present the aggregate point estimates and probability of improvement. Further results including sample efficiency curves are provided in appendix D.\nAggregate point estimates. Outer-LR demonstrates a statistically significant improvement over the PPO baseline on Brax and Jumanji for all point estimates considered (median, IQM, mean, optimality gap). Outer-Nesterov also demonstrates enhanced performance on Brax and Jumanji; this improvement is less substantial than that of outer-LR but remains statistically significant on all point estimates aside from the Brax median. Biased initialization is the weakest of the outer-PPO instantiations, with minor improvements lacking statistical significance on Brax and moderate but significant improvements on Jumanji. No method improves over baseline on MinAtar.\nProbability of improvement. All methods have a probability of improvement (over baseline) greater than 0.5. In most cases this improvement is statistically significant, aside from biased ini-tialization on Brax and outer-LR on MinAtar. Notably, outer-LR has a probability of improvement greater than 0.6 on Brax and greater than 0.7 on Jumanji."}, {"title": "5.2 HYPERPARAMETER SENSITIVITY", "content": "In the results of figures 3 and 4, the optimal hyperparameters from each per-environment outer-PPO grid search are used. We now consider the sensitivity of outer-PPO to these hyperparameters. In figures 5, 6, and 7 we present the return, normalized across each environment suite, as a function of the sweep hyperparameters for outer-LR, outer-Nesterov and biased initialization. Normaliza-tion is again performed using the extreme values presented in appendix D. Analogous plots for the individual tasks are provided in appendix E."}, {"title": "6 DISCUSSION", "content": "We now reflect on the questions posed in section 1. The PPO baselines in this work were tuned aggressively for each task, greatly increasing the confidence in the experimental findings. Given the baseline strength, and performance demonstrated in figures 3 and 4, we conclude in the negative for all three questions as evidenced by:\nQ1. Varying the outer learning rate leads to an statistically significant increase on all point esti-mates on Brax and Jumanji, with corresponding increases to probability of improvement.\nQ2. Employing Nesterov momentum on the outer loop, with outer learning rate attenuation, achieves statistically significant increases to all point estimates on Brax and Jumanji. We also observe a statistically significant probability of improvement on all three suites.\nQ3. Momentum-biased initialization achieves statistically significant increase on all point esti-mates on Jumanji, with a probability of improvement of 0.6 on this suite.\nCommon hyperparameters. The sensitivity plots in figures 5, 6 demonstrate robust normalized performance across the Brax and Jumanji suites for outer-LR and outer-Nesterov. However, they do not indicate any significant increase in normalized return could be achieved over standard PPO for a set of common hyperparameters shared across a suite. To achieve the improved aggregate metrics in figure 3 it was necessary to use task-specific hyperparameters. We do however emphasize the aggressive, task-specific, tuning of the baseline, and view the robustness of normalized return across a range of hyperparameters as a strength of the methods.\nTask-specific hyperparameters. Task-specific hyperparameter sensitivity plots are provided in appendix E. For outer-LR the optimal per-task values for \u03c3 range between 0.5 (corresponding to cautious updates) and 2.3 (corresponding to confident updates). That values of \u03c3 up to 2.3 can be optimal is surprising, as an \u03c3 greater than unity directly violates the trust region established by our previous behavior policy. This precludes the provable monotonic improvement of PPO (Kuba et al., 2022); by stepping beyond the trust region we may in principle select a policy that is worse than the previous. For outer-Nesterov co-varying \u03c3 with \u03bc can be understood through the effective learning rate \u03c3/(1 \u2013 \u03bc). The task-specific effective learning rate varies from 0.7 to 2.3. Lastly, for biased initialization the sharp peaks in performance on Brax tasks suggest the method suffers from high variance on this suite, hence the hyperparameters selected may not be optimal in expectation. On Jumanji the method is significantly less hyperparameter sensitive as evidenced by the smooth contours, providing an explanation for the performance gap observed between these suites.\nMinAtar results. No outer-PPO method improved over baseline on MinAtar. We comment that other works committing substantial resources to baseline tuning on MinAtar have struggled to achieve improvements on the suite Jesson et al. (2023). Furthermore, the hyperparameter sensi-tivity plots in figures 5, 6 and 7 demonstrate all methods achieve peak normalized return greater than 0.9 on MinAtar. Since here we are normalizing to the maximum performing agents across all sweeps, this indicates there is less variance in the optimal performance of MinAtar compared to Brax and Jumanji with peak normalized returns around 0.7 and 0.8 respectively. A final explanation for the failure to surpass baseline on MinAtar could be 'brittle' base hyperparameters, not suited to the modified dynamics introduced by outer-PPO, supported by the sharp peak observed in outer-LR and concentration of performance in outer-Nesterov about standard PPO in figures 5 and 6.\nLimitations. We identify two core limitations to this work; the fixed transition budget and the lack of co-optimization of base and outer-PPO hyperparameters. We only consider a timestep budget of 1 \u00d7 107 transitions. Whilst sample efficiency plots are provided in appendix D the hyperparameters have not been tuned to maximize performance in the data-limited regime. Furthermore, we do not consider the asymptotic performance for larger transition budgets, where it is possible the improve-ment achieved by outer-PPO methods may be diminished. With respect to co-optimization, given the dependence of the outer gradients on the base hyperparameters there is undoubtedly significant interaction between these and the outer-PPO hyperparameters. Exploring these interactions would yield better understanding and potentially improved performance. We additionally highlight the presence of learning rate annealing on the inner Adam instances in all experiments. This implies the outer gradients tend to zero, the implications of which we do not explore in this work."}, {"title": "7 RELATED WORK", "content": "The usage of the difference between initial parameters and those after gradient-based optimization as a 'gradient' has been explored for meta-learning in the Reptile algorithm (Nichol et al., 2018). Reptile aims to find an initialization that can be quickly fine-tuned across a distribution of tasks. Unlike outer-PPO, which applies this idea within a single RL task, Reptile performs gradient steps on different supervised learning tasks to determine the 'Reptile gradient'. One could interpret outer-PPO as performing serial Reptile whereby each sampled task is the next PPO iteration alongside the collected dataset.\nWhilst to the best of our knowledge we are the first to apply momentum to the outer loop of PPO, momentum-based optimizers such as RMSProp Tieleman & Hinton (2012) and Adam Kingma & Ba (2014) are commonly applied in other areas of RL. Recent work has examined the interaction of momentum based optimizers and RL objectives. Bengio et al. (2021) identify that a change in objective (such as by updating a target network or dataset), may lead to momentum estimates anti-parallel to the current gradient thereby hindering progress, and propose a correction term to mitigate this effect. Asadi et al. (2023) propose to reset the momentum estimates periodically throughout training and demonstrate improved performance on the Atari Learning Environment Bellemare et al. (2012) with Rainbow Hessel et al. (2017) doing so. However, none of these approaches focuses on PPO specifically, and instead address temporal difference learning or value based-methods.\nLastly, the biased initialization explored in this work is similar to the conjugate gradient initializa-tion technique employed in hessian-free optimization Martens (2010), although this used only the prior iterate and not a momentum vector. Hessian-free optimization can be considered a supervised learning version of TRPO (Schulman et al., 2017a)."}, {"title": "8 CONCLUSION", "content": "In this work, we introduced outer-PPO, a novel perspective of proximal policy optimization that applies arbitrary gradient-based optimizers to the outer loop of PPO. We posed three key research questions regarding the optimization process in PPO and conducted an empirical investigation across 14 tasks from three environments suites. Our experiments revealed that non-unity learning rates and momentum in the outer loop both yielded statistically significant performance improvements across a variety of evaluation metrics in the Brax and Jumanji environments, with gains ranging from 5-10% over a heavily tuned PPO baseline. Biased initialization provided improvements upon the baseline on Jumanji tasks but not Brax.\nThe most immediate direction for future research would be the exploration of interactions between base hyperparameters and outer-PPO hyperparameters. Since the optimal base hyperparameters may be unsuited to the modified dynamics of outer-PPO, the co-optimization of hyperparameters may yield performance improvements and deeper understanding of the method. Other possible future directions include the use of outer-PPO with alternatives to the clipped surrogate loss function, such as KL-penalized PPO Hsu et al. (2020) or discovered policy optimization Lu et al. (2022), and the use of adaptive optimizers on the outer loop such as RMSProp or Adam. Indeed, an 'outer' variant of many dual-loop RL algorithms can be defined, and we hope that this work will stimulate further research into optimizing RL algorithms through more sophisticated outer-loop strategies."}, {"title": "A FURTHER DETAILS ON PPO", "content": null}, {"title": "A.1 INNER OPTIMIZATION LOOP", "content": null}, {"title": "A.2 CLIPPED VALUE OBJECTIVE", "content": "LV (\u03b8V) = max [(V\u03b8V*k \u2212 Vtarg)2, (clip (V\u03b8V*k, V\u03b8V*k\u22121 \u2212 \u03f5, V\u03b8V*k\u22121 + \u03f5) \u2212 Vtarg)2"}, {"title": "B IMPLEMENTATION DETAILS", "content": "We implement our experiments using the JAX-based Stoix library (Toledo, 2024). Our implementa-tion is such that several seeds can be trialed / evaluated simultaneously for the same hyperparameters using a single device. We used Google Cloud TPU (v4-8) for these experiments. The runtime var-ied between environments, and given different hyperparameters (e.g parallel environments, rollout length, batch size, number of inner epochs) with an average of approximately 10 minutes per 4-seed trial."}, {"title": "C HYPERPARAMETERS", "content": null}, {"title": "C.1 SWEEP RANGES", "content": "The sweep ranges for baseline hyperparameter sweeps are presented in table 1."}, {"title": "C.2 OPTIMAL VALUES", "content": "The optimal values identified by the baseline sweep, up to trial 500, are included in table 2. These values are the 'base' hyperparameters used for outer-PPO methods."}, {"title": "D ADDITIONAL RESULTS", "content": null}, {"title": "E SWEEP PERFORMANCES", "content": null}, {"title": "E.1 BASELINE", "content": null}, {"title": "E.2 OUTER LEARNING RATES", "content": null}, {"title": "E.3 NESTEROV MOMENTUM", "content": null}, {"title": "E.4 BIASED INITIALIZATION", "content": null}]}