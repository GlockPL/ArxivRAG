{"title": "MAT-SED: A Masked Audio Transformer with Masked-Reconstruction Based Pre-training for Sound Event Detection", "authors": ["Pengfei Cai", "Yan Song", "Kang Li", "Haoyu Song", "Ian McLoughlin"], "abstract": "Sound event detection (SED) methods that leverage a large pre-trained Transformer encoder network have shown promising performance in recent DCASE challenges. However, they still rely on an RNN-based context network to model temporal dependencies, largely due to the scarcity of labeled data. In this work, we propose a pure Transformer-based SED model with masked-reconstruction based pre-training, termed MAT-SED. Specifically, a Transformer with relative positional encoding is first designed as the context network, pre-trained by the masked-reconstruction task on all available target data in a self-supervised way. Both the encoder and the context network are jointly fine-tuned in a semi-supervised manner. Furthermore, a global-local feature fusion strategy is proposed to enhance the localization capability. Evaluation of MAT-SED on DCASE2023 task4 surpasses state-of-the-art performance, achieving 0.587/0.896 PSDS1/PSDS2 respectively.", "sections": [{"title": "1. Introduction", "content": "Sound event detection (SED) aims to recognize not only what events are happening in an audio signal but also when those events are happening. Recent research in this field has garnered increasing interest from both academic and industrial sectors. The DCASE challenges \u00b9 have been conducted to evaluate the performance of systems in environmental sound classification and detection, significantly driving the advancement of SED research. This technology is widely used in various applications, such as smart homes [1], smart city [2], surveillance [3], etc.\nMost recent SED architecture can generally be divided into an encoder network and a context network, as illustrated in Figure 1. In classical CRNN based SED systems [4], convolutional neural networks (CNNs) are used as the encoder network for feature extraction, while recurrent neural networks (RNNs) are employed as the context network to model temporal dependencies across latent features from the encoder. The scarcity of labeled data is always a significant challenge for the SED task, due to the high cost of strong annotation for sound events. Semi-supervised methods, such as mean-teacher [5], have thus been introduced to utilize large amounts of unlabeled data to mitigate the impact of insufficient labeled data.\nRecently, Transformer-based SED models have surged in popularity, inspired by the successes of Transformers in various domains, including natural language processing [6, 7], computer vision [8] and automatic speech recognition [9, 10]. Convolution-augmented Transformer [11] utilizes Con-\nhttps://dcase.community/challenge2023/"}, {"title": "2. Methodology", "content": "In this section, we first outline the model structure of MAT-SED, then introduce the masked-reconstruction based pre-training and the fine-tuning strategies."}, {"title": "2.1. Model", "content": "The overall structure of MAT-SED, as shown in Figure 1, consists of two main components: the encoder network and the context network. The encoder network is used to extract features from the mel-spectrogram, outputting latent feature sequences. The context network is responsible for capturing temporal dependencies across the latent features. Different types of head layer follow the context network to handle specific tasks, such as reconstruction, audio tagging and SED."}, {"title": "2.1.1. Encoder network", "content": "The encoder network of MAT-SED is based on PaSST [14], a large pre-trained Transformer model for audio tagging. Each mel-spectrogram is divided into several 16 \u00d7 16 patches, then patches are projected linearly to a sequence of embeddings. The sequence traverses through 10 layers of PaSST blocks consisted of Transformers. Following PaSST, the frequency dimension is compressed via average pooling, succeeded by 10 times linear upsampling to restore the temporal resolution lost during the patching process. The output of the encoder network is denoted as Z = [Z1, Z2, ..., ZT] \u2208 R^{C\u00d7T}, where C is the dimension of the embedding vector, and T is the length of encoder's output in the time dimension."}, {"title": "2.1.2. Context network", "content": "Instead of the conventional RNN structure, we utilize 3 layers of Transformer block to constitute the context network. Given the crucial need for localization in the SED task, integrating positional information becomes vital. While RNN structures naturally embed positional information along the time dimension through their sequential structure [15], Transformer models require positional encoding for the same purpose. The vanilla Transformer uses the absolute positional encoding (APE) [6, 7], where the positional encoding depends on absolute position of tokens. But for a given sound event, we hope that the model is translation equivariant along time dimension, i.e. when the time of a sound event in an audio signal is changed, the same features\n2The code is available at https://github.com/cai525/Transformer4SED"}, {"title": "2.2. Masked-reconstruction based pre-training", "content": "The model structure during pre-training is depicted in Figure 1 (a). At this stage, we initialize the encoder network using the PaSST model pre-trained on AudioSet [18] and freeze its weights, to focus on pre-training the context network. We design the masked-reconstruction task as the pretext task, similar to train a masked language model. We mask a certain proportion of frames in the latent feature sequence Z, and substitute the masked frames with the learnable mask token, obtaining a new sequence Z'. The masked-reconstruction task requires the context network to restore the masked latent features using the contextual information, which helps to enhance the temporal modeling ability of the context network. For the masking strategy, we adopt the block-wise masking strategy used in [19], dividing the sequence into several blocks of size 10, and masking entire blocks randomly. Compared to random masking, the block-wise masking strategy increases the difficulty of reconstruction, thus forces the model to learn more abstract semantic information. The masked sequence traverses through the context network and the reconstruction head composed of two fully connected layers, yielding the reconstructed sequence \\(\\hat{Z} = [\\hat{Z}_1, \\hat{Z}_2, ..., \\hat{Z}_T] \\in R^{C\u00d7T}\\).We use mean squared error (MSE) loss to evaluate the quality of reconstruction:\n\\[L_m = \\sum_{x \\in D} \\sum_{t \\in M_x} (\\hat{Z}_t(x) \u2013 Z_t(x))^2\\]\nwhere D denotes the pre-training dataset, and Mx denotes the set of masked frame indices corresponding to the sample x. Note from this that only the masked frames are used to calculate the reconstruction loss."}, {"title": "2.3. Fine-tuning", "content": "The model structure in the fine-tuning stage is shown in Figure 1 (b). During fine-tuning, the reconstruction head is replaced by the SED head composed of a fully connected layer, which outputs the frame-level prediction. The frame-level prediction is pooled over the time dimension by linear-softmax pooling [22], to obtain the clip-level prediction result. Following the task-aware module in [20], we additionally set up an AT head to focuse on the audio tagging task. The mean-teacher"}, {"title": "3. Experimental Setup", "content": "The self-supervised pre-training and fine-tuning are both conducted on the DCASE2023 [25] dataset, which is designed to detect sound event classes in domestic environments. The training set consists of 10-second audio clips, including 1578 weakly-labeled clips, 3470 strongly-labeled clips, 10000 synthetic-strongly labeled clips, and 14412 unlabeled in-domain clips. The model is evaluated on the DCASE2023 challenge task 4 validation set, consisting of 1168 strongly-labeled clips."}, {"title": "3.2. Feature extraction and evaluation setting", "content": "The input audio is sampled at 32kHz. For feature extraction, we use a Hamming window of 25ms with a stride of 10ms to perform short-time Fourier transform(STFT). The spectrum obtained by the STFT is further transformed into a mel-spectrogram with 128 mel filters. Mixup [26], time shift and filterAugment [27] are used for data augmentation."}, {"title": "3.3. Model and training setting", "content": "For the sliding windows in the global-local feature fusion strategy, the window size and step are set to 5s and 0.3s. The context network contains 3 Transformer blocks with input dimension 768, 12 attention heads, and expansion ratio 1.\nDuring the pre-training phase, the model is trained over 6000 steps with a batch size of 24 and a learning rate of 1 \u00d7 10-4. For the masked-reconstruction task, the masking rate is set to 75%. During the fine-tuning stage, batch sizes for real strongly labeled, synthetic strongly labeled, real weakly labeled, and real unlabeled data are set to 3, 1, 4, 4, respectively. Following the strategy in [30], only the SED head and AT head are trained for the first 6000 steps of fine-tuning, then the end-to-end fine-tuning is performed over the next 12000 steps. Learning rates for the encoder network, decoder network, and head layers are set to 5 \u00d7 10\u22126, 1 \u00d7 10-4, and 2 \u00d7 10-4, respectively. The AdamW [31] optimizer is used for optimization with a weight decay of 1 \u00d7 10-4. Training is conducted on 2 Intel-3090 GPUs for 13 hours in total."}, {"title": "4. Results", "content": "In this section, we first compare the performance of MAT-SED against other state-of-the-art SED models. Then, we conduct ablation experiments to analyze the contributions of each MAT-SED component."}, {"title": "4.1. Performance of the proposed methods", "content": "Table 1 compares the performance of MAT-SED with other SED systems on the DCASE2023 dataset, where CRNN-BEATs is the baseline model of DCASE2023 task4. Our model achieves 0.587 PSDS1 and 0.896 PSDS2, outperforming previous SOTA models. It is noteworthy that MAT-SED stands out as the only model composed of pure Transformers in the table, whereas other models rely on CNN or RNN structures. This shows that the pure Transformer structure can perform well on SED tasks, given appropriate pre-training."}, {"title": "4.2. Ablation studies", "content": "4.2.1. Ablations of the context network\nFirst, we explore the impact of different context network structures, as shown in Table 2. Masked-reconstruction pre-training is employed in each set of experiments, and the hyperparameters of different structures are adjusted to the best. We use learnable APE in place of RPE to measure the effect of RPE. It can be seen from the table that the PSDS1 score of using RPE is significantly higher than APE, which indicates that the necessity of RPE for the SED task. Then we test the performance of Conformer [10] for the context network. It can be seen from the table that Conformer achieves a PSDS1 of 0.544, trailing behind the Transformer using RPE, even though RPE is also utilized by Conformer. We suppose that the possible reason is that the convolution module in Conformer increases the parameter size, which makes it too bulky for the context network. Lastly, we substitute Transformers with GRU to compare the performance of RNNs and Transformers as the context network. The GRU achieves the PSDS1 of 0.557, lower than the Transformer using RPE, indicating that the Transformer with RPE serves as a more powerful context network structure than RNNs."}, {"title": "4.2.2. Ablations of masked-reconstruction based pre-training", "content": "In this section, we analyze the effect of the masked-reconstruction pre-training. Figure 3 compares the convergence curves of training MAT-SED from scratch and end-to-end fine-tuning after masked-reconstruction pre-training. For the pre-trained model, the SED layer and AT layer are trained before the end-to-end fine-tuning to adjust to the features from pre-trained context network. The pre-trained network achieves a PSDS1 of 0.502 at the beginning of end-to-end fine-tuning, even higher than the DCASE2023 baseline model, indicating that the representation learned by the context network in the masked-reconstruction pre-training is well-suited for the SED task. During the subsequent end-to-end fine-tuning process, the optimal PSDS1 score for the network without pre-training is 0.563, noticeably lower than the pre-training network. On"}, {"title": "4.2.3. Ablations of the global-local feature fusion strategy", "content": "In this section, we analyze the effect of the global-local feature fusion strategy in the fine-tuning stage. In the feature fusion strategy, the hyperparameter \\(\\lambda\\) controls the proportion of the global and local branches in the fused features. When \\(\\lambda = 1\\), only local features are retained; when \\(\\lambda = 0\\), only global features are retained, which means that the sliding window mechanism no longer works. In Table 3, we compare the PSDS1 scores when \\(\\lambda\\) is set to 0, 0.5 and 1. The experimental results show that higher PSDS1 is achieved when \\(\\lambda = 0.5\\) than the cases when \\(\\lambda\\) is set to 0 or 1, indicating that fusing the global and local features can obtain more powerful latent features than only relying on either side."}, {"title": "5. Conclusion", "content": "In this paper, we propose MAT-SED, a pure Transformer-based SED model. In MAT-SED, the Transformer with relative positional encoding is employed as the context network, which enables the model to capture long-range context dependencies. The masked-reconstruction task is used to pre-train the Transformer-based context network before semi-supervised based fine-tuning. The global-local feature fusion strategy is employed to further enhance the model's localization accuracy. MAT-SED achieves advanced performance on DCASE2023 dataset, outperforming other state-of-the-art SED models. Ablation experiments show that the self-supervised pre-training is crucial for Transformer-based structures. In the future, we aim to further explore self-supervised learning methods for audio Transformer structures."}]}