{"title": "MAT-SED: A Masked Audio Transformer with Masked-Reconstruction Based\nPre-training for Sound Event Detection", "authors": ["Pengfei Cai", "Yan Song", "Kang Li", "Haoyu Song", "Ian McLoughlin"], "abstract": "Sound event detection (SED) methods that leverage a large\npre-trained Transformer encoder network have shown promis-\ning performance in recent DCASE challenges. However, they\nstill rely on an RNN-based context network to model tempo-\nral dependencies, largely due to the scarcity of labeled data. In\nthis work, we propose a pure Transformer-based SED model\nwith masked-reconstruction based pre-training, termed MAT-\nSED. Specifically, a Transformer with relative positional en-\ncoding is first designed as the context network, pre-trained by\nthe masked-reconstruction task on all available target data in\na self-supervised way. Both the encoder and the context net-\nwork are jointly fine-tuned in a semi-supervised manner. Fur-\nthermore, a global-local feature fusion strategy is proposed to\nenhance the localization capability. Evaluation of MAT-SED\non DCASE2023 task4 surpasses state-of-the-art performance,\nachieving 0.587/0.896 PSDS1/PSDS2 respectively.\nIndex Terms: sound event detection, transformer, masked-\nreconstruction, self-supervised learning", "sections": [{"title": "1. Introduction", "content": "Sound event detection (SED) aims to recognize not only what\nevents are happening in an audio signal but also when those\nevents are happening. Recent research in this field has garnered\nincreasing interest from both academic and industrial sectors.\nThe DCASE challenges \u00b9 have been conducted to evaluate the\nperformance of systems in environmental sound classification\nand detection, significantly driving the advancement of SED re-\nsearch. This technology is widely used in various applications,\nsuch as smart homes [1], smart city [2], surveillance [3], etc.\nMost recent SED architecture can generally be divided into\nan encoder network and a context network, as illustrated in Fig-\nure 1. In classical CRNN based SED systems [4], convolutional\nneural networks (CNNs) are used as the encoder network for\nfeature extraction, while recurrent neural networks (RNNs) are\nemployed as the context network to model temporal dependen-\ncies across latent features from the encoder. The scarcity of\nlabeled data is always a significant challenge for the SED task,\ndue to the high cost of strong annotation for sound events. Semi-\nsupervised methods, such as mean-teacher [5], have thus been\nintroduced to utilize large amounts of unlabeled data to mitigate\nthe impact of insufficient labeled data.\nRecently, Transformer-based SED models have surged in\npopularity, inspired by the successes of Transformers in var-\nious domains, including natural language processing [6, 7],\ncomputer vision [8] and automatic speech recognition [9,\n10]. Convolution-augmented Transformer [11] utilizes Con-\n\u00b9https://dcase.community/challenge2023/\nformer [10] instead of RNN to model temporal dependencies,\nwinning the first place in DCASE2020 Task 4. That work\ndemonstrated the potential of Transformer-based structures for\nSED, though performance was still limited due to insufficient\nlabeled data. To mitigate the problem of data scarcity, a widely\nused approach is to employ Transformer models pre-trained\non readily available large-scale audio tagging datasets to serve\nas powerful feature extractors. Among high-ranking mod-\nels [12, 13] of DCASE2023, the pre-trained Transformer and\nthe CNN are concatenated in parallel as the encoder network,\nwhich can take the advantages of global and local features from\ndifferent encoders. However, it is worth noting that most of\nthose works only applied Transformer structures partially to the\ntraditional CRNN. Again this is due to data scarcity issues. Fur-\nthermore, although powerful encoder networks can be obtained\nby pre-training, it is still difficult to train the downstream con-\ntext network with limited labeled data. This remains a challenge\nto apply the pure Transformer-based structure for the SED task.\nIn this work, we present a pure Transformer-based SED\nmodel, termed Masked Audio Transformer for Sound Event\nDetection (MAT-SED). MAT-SED begins with the pre-trained\nTransformer model as an encoder network, then a Transformer"}, {"title": "2. Methodology", "content": "In this section, we first outline the model structure of MAT-\nSED, then introduce the masked-reconstruction based pre-\ntraining and the fine-tuning strategies.\n2.1. Model\nThe overall structure of MAT-SED, as shown in Figure 1, con-\nsists of two main components: the encoder network and the con-\ntext network. The encoder network is used to extract features\nfrom the mel-spectrogram, outputting latent feature sequences.\nThe context network is responsible for capturing temporal de-\npendencies across the latent features. Different types of head\nlayer follow the context network to handle specific tasks, such\nas reconstruction, audio tagging and SED.\n2.1.1. Encoder network\nThe encoder network of MAT-SED is based on PaSST [14], a\nlarge pre-trained Transformer model for audio tagging. Each\nmel-spectrogram is divided into several 16 \u00d7 16 patches, then\npatches are projected linearly to a sequence of embeddings. The\nsequence traverses through 10 layers of PaSST blocks consisted\nof Transformers. Following PaSST, the frequency dimension is\ncompressed via average pooling, succeeded by 10 times linear\nupsampling to restore the temporal resolution lost during the\npatching process. The output of the encoder network is denoted\nas $Z = [Z_1, Z_2, ..., Z_T] \\in \\mathbb{R}^{C \\times T}$, where C is the dimension of\nthe embedding vector, and T is the length of encoder's output in\nthe time dimension.\n2.1.2. Context network\nInstead of the conventional RNN structure, we utilize 3 layers\nof Transformer block to constitute the context network. Given\nthe crucial need for localization in the SED task, integrating po-\nsitional information becomes vital. While RNN structures nat-\nurally embed positional information along the time dimension\nthrough their sequential structure [15], Transformer models re-\nquire positional encoding for the same purpose. The vanilla\nTransformer uses the absolute positional encoding (APE) [6, 7],\nwhere the positional encoding depends on absolute position of\ntokens. But for a given sound event, we hope that the model is\ntranslation equivariant along time dimension, i.e. when the time\nof a sound event in an audio signal is changed, the same features\n\u00b2The code is available at https://github.com/cai525/Transformer4SED\nwill be detected at the new time. We therefore use relative po-\nsitional encoding (RPE) [16] to achieve this purpose, where the\nlearnable positional encoding is determined by the relative po-\nsition between frames. Compared to learnable APE, the RPE is\nnaturally translation-equivariant [17], making it more suitable\nfor modelling temporal dependencies.\n2.2. Masked-reconstruction based pre-training\nThe model structure during pre-training is depicted in Fig-\nure 1 (a). At this stage, we initialize the encoder network us-\ning the PaSST model pre-trained on AudioSet [18] and freeze\nits weights, to focus on pre-training the context network. We\ndesign the masked-reconstruction task as the pretext task, sim-\nilar to train a masked language model. We mask a certain pro-\nportion of frames in the latent feature sequence Z, and sub-\nstitute the masked frames with the learnable mask token, ob-\ntaining a new sequence Z'. The masked-reconstruction task\nrequires the context network to restore the masked latent fea-\ntures using the contextual information, which helps to enhance\nthe temporal modeling ability of the context network. For the\nmasking strategy, we adopt the block-wise masking strategy\nused in [19], dividing the sequence into several blocks of size\n10, and masking entire blocks randomly. Compared to random\nmasking, the block-wise masking strategy increases the diffi-\nculty of reconstruction, thus forces the model to learn more\nabstract semantic information. The masked sequence traverses\nthrough the context network and the reconstruction head com-\nposed of two fully connected layers, yielding the reconstructed\nsequence $\\hat{Z} = [\\hat{Z}_1, \\hat{Z}_2, ..., \\hat{Z}_T] \\in \\mathbb{R}^{C \\times T}$.We use mean squared\nerror (MSE) loss to evaluate the quality of reconstruction:\n$L_m = \\sum_{x \\in D} \\sum_{t \\in M_x} (\\hat{Z}_t(x) - Z_t(x))^2$ (1)\nwhere D denotes the pre-training dataset, and $M_x$ denotes the\nset of masked frame indices corresponding to the sample x.\nNote from this that only the masked frames are used to calculate\nthe reconstruction loss.\n2.3. Fine-tuning\nThe model structure in the fine-tuning stage is shown in Fig-\nure 1 (b). During fine-tuning, the reconstruction head is re-\nplaced by the SED head composed of a fully connected layer,\nwhich outputs the frame-level prediction. The frame-level pre-\ndiction is pooled over the time dimension by linear-softmax\npooling [22], to obtain the clip-level prediction result. Follow-\ning the task-aware module in [20], we additionally set up an\nAT head to focuse on the audio tagging task. The mean-teacher"}, {"title": "3. Experimental Setup", "content": "3.1. Dataset\nThe self-supervised pre-training and fine-tuning are both con-\nducted on the DCASE2023 [25] dataset, which is designed\nto detect sound event classes in domestic environments.\nThe training set consists of 10-second audio clips, includ-\ning 1578 weakly-labeled clips, 3470 strongly-labeled clips,\n10000 synthetic-strongly labeled clips, and 14412 unlabeled in-\ndomain clips. The model is evaluated on the DCASE2023 chal-\nlenge task 4 validation set, consisting of 1168 strongly-labeled\nclips.\n3.2. Feature extraction and evaluation setting\nThe input audio is sampled at 32kHz. For feature extraction,\nwe use a Hamming window of 25ms with a stride of 10ms\nto perform short-time Fourier transform(STFT). The spec-\ntrum obtained by the STFT is further transformed into a mel-\nspectrogram with 128 mel filters. Mixup [26], time shift and\nfilterAugment [27] are used for data augmentation.\nThe polyphonic sound detection score (PSDS) [28] is used\nas the evaluation metric. Following the setting of DCASE2023\ncompetition, we use two different metric settings, PSDS1 and\nPSDS2, for two different scenarios. The former focuses more\non event localization, while the latter aims to avoid confusing\nbetween classes but for which localization is less crucial. Since\nPSDS1 can better reflect the model's localization performance,\nwe use PSDS1 as the main evaluation metric in our experi-\nments. In the testing phase, median filter and maximum filter\nare applied to the two PSDS scenarios respectively for post-\nprocessing [29].\n3.3. Model and training setting\nFor the sliding windows in the global-local feature fusion strat-\negy, the window size and step are set to 5s and 0.3s. The context\nnetwork contains 3 Transformer blocks with input dimension\n768, 12 attention heads, and expansion ratio 1.\nDuring the pre-training phase, the model is trained over\n6000 steps with a batch size of 24 and a learning rate of\n1 \u00d7 10-4. For the masked-reconstruction task, the masking\nrate is set to 75%. During the fine-tuning stage, batch sizes for\nreal strongly labeled, synthetic strongly labeled, real weakly la-\nbeled, and real unlabeled data are set to 3, 1, 4, 4, respectively.\nFollowing the strategy in [30], only the SED head and AT\nhead are trained for the first 6000 steps of fine-tuning, then the\nend-to-end fine-tuning is performed over the next 12000 steps.\nLearning rates for the encoder network, decoder network, and\nhead layers are set to 5 \u00d7 10\u22126, 1 \u00d7 10-4, and 2 \u00d7 10-4, re-\nspectively. The AdamW [31] optimizer is used for optimization\nwith a weight decay of 1 \u00d7 10-4. Training is conducted on 2\nIntel-3090 GPUs for 13 hours in total."}, {"title": "4. Results", "content": "In this section, we first compare the performance of MAT-SED\nagainst other state-of-the-art SED models. Then, we conduct\nablation experiments to analyze the contributions of each MAT-\nSED component.\n4.1. Performance of the proposed methods\nTable 1 compares the performance of MAT-SED with other\nSED systems on the DCASE2023 dataset, where CRNN-\nBEATs is the baseline model of DCASE2023 task4. Our model\nachieves 0.587 PSDS1 and 0.896 PSDS2, outperforming previ-\nous SOTA models. It is noteworthy that MAT-SED stands out\nas the only model composed of pure Transformers in the table,\nwhereas other models rely on CNN or RNN structures. This\nshows that the pure Transformer structure can perform well on\nSED tasks, given appropriate pre-training."}, {"title": "4.2. Ablation studies", "content": "4.2.1. Ablations of the context network\nFirst, we explore the impact of different context network struc-\ntures, as shown in Table 2. Masked-reconstruction pre-training\nis employed in each set of experiments, and the hyperparam-\neters of different structures are adjusted to the best. We use\nlearnable APE in place of RPE to measure the effect of RPE. It\ncan be seen from the table that the PSDS1 score of using RPE\nis significantly higher than APE, which indicates that the neces-\nsity of RPE for the SED task. Then we test the performance of\nConformer [10] for the context network. It can be seen from the\ntable that Conformer achieves a PSDS1 of 0.544, trailing behind\nthe Transformer using RPE, even though RPE is also utilized\nby Conformer. We suppose that the possible reason is that the\nconvolution module in Conformer increases the parameter size,\nwhich makes it too bulky for the context network. Lastly, we\nsubstitute Transformers with GRU to compare the performance\nof RNNs and Transformers as the context network. The GRU\nachieves the PSDS1 of 0.557, lower than the Transformer using\nRPE, indicating that the Transformer with RPE serves as a more\npowerful context network structure than RNNs.\n4.2.2. Ablations of masked-reconstruction based pre-training\nIn this section, we analyze the effect of the masked-\nreconstruction pre-training. Figure 3 compares the convergence\ncurves of training MAT-SED from scratch and end-to-end fine-\ntuning after masked-reconstruction pre-training. For the pre-\ntrained model, the SED layer and AT layer are trained before\nthe end-to-end fine-tuning to adjust to the features from pre-\ntrained context network. The pre-trained network achieves a\nPSDS1 of 0.502 at the beginning of end-to-end fine-tuning,\neven higher than the DCASE2023 baseline model, indicating\nthat the representation learned by the context network in the\nmasked-reconstruction pre-training is well-suited for the SED\ntask. During the subsequent end-to-end fine-tuning process,\nthe optimal PSDS1 score for the network without pre-training\nis 0.563, noticeably lower than the pre-training network. On\nthe other hand, severe overfitting occurs in the network with-\nout pre-training, which is not apparent in the pre-trained net-\nwork. The results shows the efficacy of masked-reconstruction\npre-training in enhancing Transformer-based context network's\nability to model temporal dependencies, thus benefiting the lo-\ncalization of sound events.\nFigure 4 further compares the effect of masking ratio in\nthe masked-reconstruction pre-training. It can be seen from the\nfigure that the optimal masking ratio is 75%, relatively higher\ncompared to Bert (15%) [7]. A high masking ratio helps the\nmodel to learn abstract semantic features, rather than restoring\nthe masked frames by simply interpolation. Similar conclusions\nhave also been found in other self-supervised learning works\nbased on masking [30, 32].\n4.2.3. Ablations of the global-local feature fusion strategy\nIn this section, we analyze the effect of the global-local feature\nfusion strategy in the fine-tuning stage. In the feature fusion\nstrategy, the hyperparameter \u03bb controls the proportion of the\nglobal and local branches in the fused features. When \u03bb = 1,\nonly local features are retained; when \u03bb = 0, only global fea-\ntures are retained, which means that the sliding window mech-\nanism no longer works. In Table 3, we compare the PSDS1\nscores when \u03bb is set to 0, 0.5 and 1. The experimental results\nshow that higher PSDS1 is achieved when \u03bb = 0.5 than the\ncases when \u03bb is set to 0 or 1, indicating that fusing the global\nand local features can obtain more powerful latent features than\nonly relying on either side."}, {"title": "5. Conclusion", "content": "In this paper, we propose MAT-SED, a pure Transformer-\nbased SED model. In MAT-SED, the Transformer with rela-\ntive positional encoding is employed as the context network,\nwhich enables the model to capture long-range context depen-\ndencies. The masked-reconstruction task is used to pre-train\nthe Transformer-based context network before semi-supervised\nbased fine-tuning. The global-local feature fusion strategy is\nemployed to further enhance the model's localization accuracy.\nMAT-SED achieves advanced performance on DCASE2023\ndataset, outperforming other state-of-the-art SED models. Ab-\nlation experiments show that the self-supervised pre-training is\ncrucial for Transformer-based structures. In the future, we aim\nto further explore self-supervised learning methods for audio\nTransformer structures."}]}