{"title": "FODA-PG for Enhanced Medical Imaging Narrative Generation: Adaptive Differentiation of Normal and Abnormal Attributes", "authors": ["Kai Shu", "Yuzhuo Jia", "Ziyang Zhang", "Jiechao Gao"], "abstract": "Automatic Medical Imaging Narrative generation aims to alleviate the workload of radiologists by producing accurate clinical descriptions directly from radiological images. However, the subtle visual nuances and domain-specific terminology in medical images pose significant challenges compared to generic image captioning tasks. Existing approaches often neglect the vital distinction between normal and abnormal findings, leading to suboptimal performance. In this work, we propose FODA-PG, a novel Fine-grained Organ-Disease Adaptive Partitioning Graph framework that addresses these limitations through domain-adaptive learning. FODA-PG constructs a granular graphical representation of radiological findings by separating disease-related attributes into distinct \"disease-specific\" and \"disease-free\" categories based on their clinical significance and location. This adaptive partitioning enables our model to capture the nuanced differences between normal and pathological states, mitigating the impact of data biases. By integrating this fine-grained semantic knowledge into a powerful transformer-based architecture and providing rigorous mathematical justifications for its effectiveness, FODA-PG generates precise and clinically coherent reports with enhanced generalization capabilities. Extensive experiments on the IU-Xray and MIMIC-CXR benchmarks demonstrate the superiority of our approach over state-of-the-art methods, highlighting the importance of domain adaptation in medical report generation.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical imaging, particularly chest X-rays, plays a crucial role in patient diagnosis and treatment planning. Interpreting these images requires radiologists to meticulously analyze both normal anatomical structures and potential abnormalities across various regions of interest, a time-consuming and expertise-driven process. Automatic Medical Imaging Narrative generation systems [2], [3] have emerged as a promising solution to assist radiologists by generating textual descriptions directly from radiological images. Recent advancements in deep learning, especially transformer-based architectures [1], [5], have enabled the development of increasingly sophisticated frameworks for producing fluent and coherent medical reports. However, the Medical Imaging Narrative generation task presents unique challenges compared to generic image captioning. First, medical images contain subtle visual nuances that can significantly alter the diagnostic interpretation, requiring models to capture fine-grained details. Second, accurately describing medical findings demands a specialized vocabulary and domain-specific knowledge. Moreover, existing medical image datasets often suffer from significant biases, with an over-representation of common pathologies and an under-representation of rare conditions [9], [34]. Consequently, models trained on such data tend to overly emphasize frequently occurring abnormalities while overlooking crucial normal findings, limiting their generalization capabilities to unseen domains.\nIn this work, we introduce FODA-PG, a novel Fine-grained Organ-Disease Adaptive Partitioning Graph methodology that addresses these limitations through domain-adaptive learning. FODA-PG constructs a highly granular and semantically rich graphical representation of radiological findings by leveraging the BioMedCLIP [8] framework to retrieve the most relevant images and reports for a given query. We perform fine-grained entity extraction to identify detailed attributes associated with each anatomical region, going beyond generic terms to more specific descriptors. Critically, we employ an adaptive partitioning strategy that separates disease-related attributes into \"disease-specific\" and \"disease-free\" categories based on their clinical significance and location. This yields a nuanced"}, {"title": "II. RELEVANT LITERATURE", "content": "The transformative impact of deep learning architectures, particularly transformers, on natural language processing and multimodal applications has significantly advanced captioning techniques [12]\u2013[15]. Among the notable methods, OSCAR [16] harnesses detected object tags within images as pivotal points for enhancing the alignment of visual content with textual descriptions, thereby facilitating the semantic mapping process. The UpDown approach [17] leverages a dual mechanism-extracting salient features and regions from images in a bottom-up fashion and adjusting feature weights top-down-to refine the focus of the captioning system. The CAAG model [18] constructs a global context through its primary captioning system, subsequently generating specific words in a targeted manner based on this contextual backdrop and the dynamic states of the model."}, {"title": "B. Medical Imaging Narrative Generation", "content": "The Medical Imaging Narrative Generation (ING) task is devoted to creating clinical narratives from radiological imagery, essentially extending the concept of image captioning into the medical sphere. This task leverages an encoder-decoder framework, similar to that used in image captioning, to construct reports [6], [7], [22], [23], [50]. ING, however, encounters distinct challenges not typically found in general image captioning: the considerable length of medical reports compared to standard captions and the subtle variances in radiological images that complicate the identification of abnormalities."}, {"title": "III. ALGORITHMIC FRAMEWORK", "content": "Let I represent the set of input radiological images and y the corresponding set of reports. Each report Y \u2208 Y is a sequence of word tokens Y = {y1,..., \u0443\u0442}, where T denotes the report length. Our objective is to learn a mapping function f: I \u2192 y that generates precise and coherent reports for given images.\nWe formulate the problem as a conditional language modeling task, where the goal is to estimate the conditional probability distribution P(YI) for each image-report pair (I,Y) \u2208 1 x Y. Leveraging the chain rule of probability, P(Y|I) can be factorized as:\n$$P(Y|I) = \\prod_{t=1}^{T} P(y_t|Y_{<t}, I),$$\nwhere y<t = {Y1,..., Yt-1} represents the sequence of tokens preceding Yt.\nTo learn the model parameters \u03b8, we minimize the negative log-likelihood loss:\n$$L_{NLL} (\u03b8) = - \\sum_{(I,Y)\u2208\\mathcal{I}\u00d7\\mathcal{Y}} log P_\u03b8 (Y|I)$$\n$$ = - \\sum_{(I,Y)\u2208\\mathcal{I}\u00d7\\mathcal{Y}} \\sum_{t=1}^{T} log P_\u03b8 (y_t|Y_{<t}, I)$$\nB. Fine-grained Organ-Disease Adaptive Partitioning Graph (FODA-PG) Construction\nThe Fine-grained Organ-Disease Adaptive Partitioning Graph (FODA-PG) G = (V, E) is a structured representation of the intricate relationships between anatomical regions and their associated findings. The node set V = {v1,...,vN} embodies"}, {"title": "Spectral Graph Convolution:", "content": "The spectral graph convolution operation (Equation 8) can be viewed as a special case of the general spectral convolution defined on graphs [25].\nLet x \u2208 RN be a signal defined on the nodes of the graph G, and let L = IN - D\u00af\u00bdAD\u00af\u00bd be the normalized graph Laplacian matrix, where D is the diagonal degree matrix with Dii = \u2211j Aij. The graph Laplacian can be eigendecomposed as L = UAUT, where U \u2208 RN\u00d7N is the matrix of eigenvectors and A = diag(\u03bb1, ..., \u03bbN) is the diagonal matrix of eigenvalues.\nThe spectral convolution of the signal x with a filter g\u03b8 (\u039b) is defined as:\n$$g_\u03b8 (L)*x = g_\u03b8 (U\u039bU^T)x = Ug_\u03b8 (\u039b)U^T x,$$\nwhere g\u03b8 (\u039b) = diag(g\u03b8 (\u03bb1),..., g\u03b8 (\u03bbN)) is a diagonal matrix applying the filter g\u03b8 to the eigenvalues of the graph Laplacian. To avoid the computationally expensive eigendecomposition and matrix multiplication, the filter g\u03b8 can be approximated by a truncated expansion in terms of Chebyshev polynomials:\n$$g_\u03b8 (L) * x \u2248 \\sum_{k=0}^{K-1} \u03b8_k T_k(L)x,$$\nwhere Tk(\u00b7) is the Chebyshev polynomial of order k, \\(\\widetilde{L} = 2L/\u03bb_{max} - I_N\\) is the scaled and shifted Laplacian matrix, and \\(\\lambda_{max}\\) is the largest eigenvalue of L.\nThe spectral graph convolution operation in Equation 8 can be seen as a first-order approximation of Equation 10 with K = 1 and \u03bbmax \u2248 2:\n$$H^{(l+1)} = \u03c3(\\widetilde{A}H^{(l)}W^{(l)}) \u2248 \u03c3(Ug_\u03b8 (\u039b)U^T H^{(l)}),$$\nwhere  = D\u0303\u2212\u00bdA\u0303D\u0303\u2212\u00bd is the normalized adjacency matrix with added self-loops, and g\u03b8 (\u039b) = diag(\u03b80,...,\u03b80) is a diagonal matrix with learnable parameters \u03b80.\nThis spectral interpretation of the GCN operation provides insights into its effectiveness in capturing the smooth variations of the node features over the graph structure, which is particularly useful for modeling the spatial dependencies between anatomical regions in medical images."}, {"title": "Graph Convolutional Networks and Weisfeiler-Lehman Isomorphism Test:", "content": "The expressiveness of Graph Convolutional Networks (GCNs) can be analyzed through the lens of the Weisfeiler-Lehman (WL) graph isomorphism test. The WL test is a powerful algorithm for determining the isomorphism between two graphs by iteratively aggregating the neighborhood information of each node. Specifically, the WL test computes a sequence of node labels by concatenating the labels of each node with the sorted labels of its neighbors and hashing the concatenated labels into a new label. Two graphs are considered isomorphic if the multisets of node labels at each iteration are identical.\nIt has been shown that GCNs are at most as powerful as the WL test in distinguishing non-isomorphic graphs [27], [28]. More formally:\nTheorem III.1 (WL-GCN Expressiveness [27]). Let G\u2081 and G2 be two non-isomorphic graphs. If a GCN with sufficient"}, {"title": "C. Topological Relation Enriched Image Embedding", "content": "To obtain fine-grained visual representations of the input images, we employ a convolutional neural network (CNN) backbone, such as ResNet [30], followed by a graph-based attentional mechanism.\nGiven an input image I \u2208 I, the CNN backbone extracts a set of visual features V = {v1,...,VK} \u2208 RK\u00d7dv, where K is the number of visual regions and du is the dimension of the visual features.\nTo enhance the visual representations with graph-based information, we propose a Graph-Enhanced Attention (GEA) mechanism. The GEA mechanism computes the attention scores between each visual region and each graph node, based on their feature similarity:\n$$A_{ij} = \\frac{exp(v_i W_a h_j)}{\\sum_{k=1}^{N} exp(v_i W_a h_k)},$$\nwhere Wa \u2208 Rdv\u00d7d1 is a trainable weight matrix.\nThe attended graph features for each visual region are then computed as a weighted sum of the node features:\n$$g_i = \\sum_{j=1}^{N} \u03b1_{ij}h_j.$$\nThe graph-enhanced visual features U = {U1,\u2026\u2026\u2026,UK}\u2208 RK\u00d7(dv+d\u2081) are obtained by concatenating the original visual features with the attended graph features:\n$$U_i = [v_i; g_i].$$\nThese enhanced features capture the relevant semantic information from the graph, guiding the model to focus on the most important visual regions for accurate report generation."}, {"title": "Attention as a Similarity Measure:", "content": "The attention mechanism in Equation 12 can be interpreted as a similarity measure between the visual features vi and the graph node features hj. The dot product vi Wahj computes the similarity between the visual feature vi and the transformed graph feature Wahj, where Wa is a learnable weight matrix that aligns the two feature spaces. The softmax function normalizes the similarity scores, ensuring that the attention weights sum to one for each visual region.\nThe choice of the dot product as the similarity measure is motivated by its simplicity and effectiveness in capturing the alignment between two feature vectors. However, other similarity measures can be used, such as the Euclidean distance or the cosine similarity:\n$$A_{ij} = \\frac{exp(-||v_i - W_a h_j||^2)}{\\sum_{k=1}^{N} exp(-||v_i - W_a h_k||^2)},$$\n$$A_{ij} = \\frac{exp(cos(v_i, W_a h_j))}{\\sum_{k=1}^{N} exp(cos(v_i, W_a h_k))},$$\nwhere cos(vi, Wahj) = \\(\\frac{v_i W_a h_j}{||v_i|||| W_a h_j||}\\) is the cosine similarity between vi and Wahj.\nThe choice of the similarity measure depends on the specific characteristics of the visual and graph features and can be determined empirically based on the performance on the validation set."}, {"title": "Multi-Head Attention:", "content": "Formally, let H be the number of attention heads. For each head h\u2208 {1, ..., H}, we compute the attention weights and attended features as follows:\n$$\u03b1_j^{(h)} = \\frac{exp(v_i W_h h_j)}{\\sum_{k=1}^{N} exp(v_i W_h h_k)},$$\n$$g_i^{(h)} = \\sum_{j=1}^{N} \u03b1_j^{(h)} (W_h h_j),$$\nwhere W(h) \u2208 Rdvxdh and W(h) \u2208 RdL\u00d7dh are learnable weight matrices for the h-th attention head, and dh = dL/H is the dimension of each subspace.\nThe attended features from all heads are concatenated and linearly projected to obtain the final graph-enhanced visual features:\n$$g_i = W_o[g_i^{(1)}; ...; g_i^{(H)}],$$\nwhere Wo\u2208 RdL\u00d7dL is a learnable output weight matrix."}, {"title": "D. Node-Edge Informed Narrative Construction", "content": "The encoder takes the graph-enhanced visual features U as input and computes the hidden states H\u00ae = {h\u2081,...,h}\u2208 RKxdh:\n$$h_i = BiLSTM(U_i, h_{i-1}),$$\nwhere dh is the dimension of the hidden states.\nThe decoder generates the report tokens sequentially, based on the encoded visual features and the previously generated tokens. At each time step t, the decoder computes the hidden state st \u2208 Rdh based on the previous hidden state st\u22121, the previous token yt-1, and the context vector ct:\n$$s_t = LSTM([e(y_{t-1}); c_t], s_{t-1}),$$\nwhere e(yt\u22121) \u2208 Rde is the embedding of the previous token, and [;] denotes concatenation."}, {"title": "Context vector", "content": "The context vector ct is computed as a weighted sum of the encoder hidden states, where the weights are determined by an attention mechanism:\n$$c_t = \\sum_{i=1}^{K} \u03b2_{ti}h_i,$$\nwhere the attention weights \u03b2ti are computed as:\n$$\u03b2_{ti} = \\frac{exp(f(s_{t-1}, h_i))}{\\sum_{i=1}^{K} exp(f(s_{t-1}, h_i))}$$\nHere, f(\u00b7,\u00b7) is a scoring function that measures the relevance between the decoder hidden state and the encoder hidden states, which can be implemented as a multi-layer perceptron.\nThe probability distribution over the vocabulary at time step t is computed based on the decoder hidden state st:\n$$P_\u03b8(y_t |Y_{<t}, I) = softmax(W_o s_t + b_o),$$\nwhere Wo\u2208 R|Vy|\u00d7dh and bo\u2208R|V| are trainable parameters, and Vy is the vocabulary of report tokens.\nDuring training, the model parameters \u03b8 are optimized by minimizing the negative log-likelihood loss LNLL(\u03b8) (Equation 2) using stochastic gradient descent. During inference, the report tokens are generated sequentially by selecting the token with the highest probability at each time step:\n$$\\hat{y}_t = arg \\underset{y \\in \\mathcal{V}_Y}{max} P_\u03b8(y|\\hat{y}_{<t}, I),$$\nwhere \u0177<t = {\u01771,..., \u0177t\u22121} denotes the sequence of previously generated tokens.\n1) Beam Search Decoding: Formally, let Ht be the set of B partial hypotheses at time step t, where each hypothesis h\u2208 Ht is a sequence of tokens h = {y1, ..., yt}. The cumulative probability of a hypothesis h is computed as:\n$$log P(h|I) = \\sum_{t'=1}^{t} log P_\u03b8(y_{t'} |Y_{<t'}, I).$$"}, {"title": "At each time step", "content": "At each time step t, the hypotheses in Ht\u22121 are expanded by considering all possible next tokens y \u2208 Vy:\n$$H_t = \\underset{h \\in H_{t-1}}{\\cup} {h \\cup {y}: y \\in \\mathcal{V}_Y}$$\nThe B hypotheses with the highest cumulative probabilities are selected for the next time step:\n$$H_t = top-B(H_t),$$\nwhere top-B(.) returns the B hypotheses with the highest cumulative probabilities.\n2) Reinforcement Learning for Text Generation: We can use reinforcement learning (RL) to directly optimize the model for a specific evaluation metric, such as BLEU [64] or CIDEr [38]. In RL-based text generation, the model is viewed as an agent that interacts with the environment (the input image and the previously generated tokens) and receives a reward based on the quality of the generated report."}, {"title": "Formally", "content": "Formally, let r(Y, Y*) be the reward function that measures the similarity between the generated report Y and the ground-truth report Y*. The goal of RL is to maximize the expected reward:\n$$J(\u03b8) = E_{Y \\sim P_\u03b8(Y|I)}[r(Y,Y^*)].$$\nThe gradient of the expected reward with respect to the model parameters \u03b8 can be computed using the REINFORCE algorithm:\n$$\u2207_\u03b8 J(\u03b8) = E_{Y \\sim P_\u03b8(Y|I)}[r(Y,Y^*)\u2207_\u03b8 log P_\u03b8(Y|I)].$$\nIn practice, the expectation in Equation 30 is approximated by sampling reports from the model distribution P\u03b8(Y|I) and computing the average gradient:\n$$\u2207_\u03b8 J(\u03b8) \u2248 \\frac{1}{M} \\sum_{m=1}^{M}[r(Y^{(m)}, Y^*) \u2207_\u03b8 log P_\u03b8(Y^{(m)}|I)],$$\nwhere {Y(m)}M m=1 are M reports sampled from P\u03b8(Y|I). The model parameters \u03b8 are updated using stochastic gradient ascent:\n$$\u03b8 \u2190 \u03b8 + \u03b1\u2207J(\u03b8),$$\nwhere \u03b1 is the learning rate.\n3) Visual-Semantic Alignment: The Graph-Enhanced Attention (GEA) mechanism (Equations 12-14) used for visual-semantic alignment can be justified by the theory of cross-modal attention and its effectiveness in capturing the interactions between visual and textual features."}, {"title": "Theorem III.2", "content": "(Expressiveness of Cross-Modal Attention [31]). Let V \u2208 RK\u00d7dv be the visual features and H\u2208 RN\u00d7dh be the textual features, where K and N are the number of visual and textual elements, respectively, and dv and dh are their feature dimensions. Let A \u2208 RK\u00d7N be the attention matrix computed by a cross-modal attention mechanism. Then, the attended features G = AH can approximate any continuous function of V and H to an arbitrary precision, given sufficient attention heads and hidden dimensions."}, {"title": "Theorem III.3", "content": "(Generalization Bound for Cross-Modal Attention [32]). Let D = {(Vi, Hi, Yi)}i = 1n be a dataset of n samples, where Vi \u2208 RKxdv, H\u00bf \u2208 RN\u00d7dn, and Yi \u2208 RKxdy are the visual features, textual features, and target outputs, respectively. Let f0(Vi,Hi) = W\u3002[Att(Vi, H\u00bf); Vi] be a cross-modal attention model with parameters \u03b8, where Att(\u00b7,\u00b7) is the attention mechanism and Wo \u2208 R(dv+dh)\u00d7dy is the output weight matrix. Let l(f0(Vi, Hi), Yi) be a bounded loss function. Then, for any \u03b4 > 0, with probability at least 1 - \u03b4, the following generalization bound holds:\n$$E_{(V,H,Y) \\sim \\mathcal{D}}[l(f_\u03b8(V, H), Y)] <= \\frac{1}{n} \\sum_{i=1}^{n} l(f_\u03b8 (V_i, H_i), Y_i) + O(\\sqrt{\\frac{log(1/\u03b4)}{n}}),$$,\nwhere the expectation is taken over the data distribution D."}, {"title": "IV. EXPERIMENT", "content": "We conducted an evaluation of our Fine-grained Organ-Disease Adaptive Partitioning Graph (FODA-PG) model using two established radiology reporting benchmarks: IU-Xray [9] and MIMIC-CXR [34], with preprocessing and dataset division protocols modeled after [1] to ensure a standardized comparison."}, {"title": "B. Execution Configuration", "content": "Visual Feature Extractor: In a departure from earlier methodologies that utilized ResNet-101 or DenseNet-121 trained on ImageNet for image encoding [1], [23], [51], we employ the Vision Transformer (ViT) from MedSAM [54] as our image encoder, specifically omitting the MLP neck to focus on extracting patch embeddings. With the ViT, an input image of 256*256 is transformed into a 16*16*768 feature map, which is subsequently reshaped into 256*768 patch embeddings. Consistent with established protocols [1], [7], our process involves handling paired images for the IU-Xray dataset and a single image for MIMIC-CXR. To standardize the output across different datasets, we reduce the number of patch embeddings from 1024 to 256.\nGraph Construction: For the creation of our graph, we selectively use reports from the most closely matched images, identified via cosine similarity measures employed by BioMedCLIP [8]. As detailed in [57], the reports are first segmented and preprocessed, and then a predefined list of organs and diseases are extracted through string matching using the Natural Language Toolkit (NLTK) [58]. Distinctions between disease-specific and disease-free cases are made by detecting terms like \"no\" and \"normal\" within the text. The DistilGPT2 model [59], with its Language Model (LM) Head removed, is utilized to derive node embeddings for all identified disease states, maintaining a dimensionality of 768.\nText Decoder and Generation: DistilGPT2 continues to serve as the text decoder within our framework. Our vocabulary is enriched with DistilGPT2's tokens, along with additional [BOS] and [EOS] tokens to facilitate text generation. Following the standardization approach of previous CXR report generation models like that of Chen et al. [1], we limit reports to 128 words, transform all text to lowercase, exclude special characters, and replace less common words with a placeholder token.\nOptimizing Parameters: The training regimen involves the use of 8 NVIDIA A100 GPUs, supporting a batch size of 32 for a total of 30 epochs across both datasets. We select the training checkpoint that achieves the highest CIEDr score for final evaluations. Initial learning rates are set at 5e-6 for the encoder and 5e-5 for other parameters, with all other AdamW hyperparameters remaining at their default settings."}, {"title": "C. Evaluation Metrics", "content": "Our performance evaluation employs a comprehensive set of Natural Language Generation (NLG) metrics, including"}, {"title": "V. EXPERIMENT RESULTS", "content": "To validate the superiority of our approach, we benchmarked our model, termed FODA-PG, against leading models in the domain of Medical Imaging Narrative Generation (ING) using the established IU-Xray and MIMIC-CXR datasets, as detailed in Figure 2. Among the models evaluated were R2Gen [1], the foundational model for ING; CMN [23] and PPKED [6], which incorporate organ-disease knowledge graphs; and the more recent METrans [55], MMTN [56], and DCL [7]. Our model demonstrated superior performance across both Natural Language Generation (NLG) and Clinical Effectiveness (CE) metrics. The BLEU [64] score quantifies the n-gram similarity between the generated and reference reports, while ROUGE-L [65] assesses the longest contiguous matching sequence of words, and METEOR [66] evaluates alignment at a more granular level, factoring in synonymy and paraphrasing. Importantly, an elevated CIDEr score reflects the semantic depth and clinical relevance of the reports crafted by our method."}, {"title": "B. Ablation Study", "content": "In this subsection, we delineate the frequency and types of normal and abnormal disease manifestations within the IU-Xray and MIMIC-CXR datasets, underscoring the importance of differentiating between disease-specific and disease-neutral categories. This is followed by an evaluation of offline image retrieval performance using BioMedCLIP [8], and an exploration of the individual contributions of each component within our Fine-grained Organ-Disease Adaptive Partitioning Graph (FODA-PG) model. The effects of different visual encoders on model accuracy are presented in Table 3, and the impacts of various node encoders, node modeling techniques, and information fusion strategies are depicted in Figure 4."}, {"title": "Dataset Distributions:", "content": "Analysis of the disease entities extracted from the reports shows a higher prevalence of disease-neutral entities in IU-Xray compared to disease-specific ones, with a more balanced distribution in MIMIC-CXR. This balance is attributed to our methodological refinement of sentence segmentation, such as the parsing of phrases like \"No pneumothorax, pleural effusion, or focal air space consolidation\". Disease-specific entities, including \"pneumothorax\" and \"effusion\", show a long-tailed frequency distribution, which is typical for clinical datasets."}, {"title": "Retrieval Performance:", "content": "Our validation of the FODA-PG-enhanced methodology involved assessing the alignment between retrieved and actual reports via BioMedCLIP [8], utilizing predefined pairs of disease-specific and disease-neutral entities. Notably, increasing the number of retrieved images improved the recall of disease entities, albeit with a slight reduction in precision, reaching over 51% entity recall when retrieving three images."}, {"title": "Visual Encoder:", "content": "The efficacy of Medical Imaging Narrative generation hinges significantly on the quality of visual representations. We evaluated several top-tier image encoders tailored to both medical and general imagery, as outlined in Table 3. The performance metrics were closely matched between ViT-B/16@224, initialized with BioMedCLIP [8], and CvT@384 pretrained on ImageNet21k. Notably, MedSAM [54], which focuses on medical imagery, demonstrated superior performance, underscoring the importance of fine-grained region-of-interest (ROI) features in medical diagnostics."}, {"title": "Vertex Representation and Multi-Source Integration:", "content": "The text-based construction of our disease graph prompted the use of text encoders for node embedding. In contrast to previous methods using SciBERT [7], our approach included trials with PubMedBERT aligned with BioMedCLIP [8] for enhanced multi-modal integration. Despite introducing graph priors, this adaptation did not improve report generation, potentially due to the limited size of the IU-Xray dataset, which may hinder effective learning of correlations between PubMedBERT's node embeddings and DistilGPT2's token embeddings. The configurations (b) and (c) explored the utility of graph convolutional networks and multi-head cross-attention mechanisms, respectively, in enhancing node and patch embedding interactions. Our final configuration (d) combined these elements to optimize the generation of clinically relevant reports."}, {"title": "VI. CONCLUSION AND DISCUSSION", "content": "In this study, we introduce a pioneering method for constructing organ-disease graphs to enhance the generation of Medical Imaging Narratives. Traditional approaches often restrict their focus to a narrow spectrum of diseases and fail to capture the nuanced distinction between normal and pathological findings as comprehensively as actual clinical narratives do. Our proposed Fine-grained Organ-Disease Adaptive Partitioning Graph (FODA-PG) method leverages similarity-based retrieval to meticulously construct fine-grained organ-disease graphs. This approach meticulously categorizes nodes into disease-specific or disease-neutral categories, reflecting their pathological significance or absence thereof. Rigorous testing on established benchmarks like IU-Xray and MIMIC-CXR substantiates the robustness and accuracy of our approach."}]}