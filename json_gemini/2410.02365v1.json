{"title": "From Concrete to Abstract: A Multimodal Generative Approach to Abstract Concept Learning", "authors": ["Haodong Xie", "Rahul Singh Maharjan", "Federico Tavella", "Angelo Cangelosi"], "abstract": "Understanding and manipulating concrete and abstract concepts is fundamental to human intelligence. Yet, they remain challenging for artificial agents. This paper introduces a multimodal generative approach to high order abstract concept learning, which integrates visual and categorical linguistic information from concrete ones. Our model initially grounds subordinate level concrete concepts, combines them to form basic level concepts, and finally abstracts to superordinate level concepts via the grounding of basic-level concepts. We evaluate the model language learning ability through language-to-visual and visual-to-language tests with high order abstract concepts. Experimental results demonstrate the proficiency of the model in both language understanding and language naming tasks.", "sections": [{"title": "1 Introduction", "content": "What would a drawing of an Artificial Intelligence (AI) agent look like if asked to illustrate an animal? Among the most frequently used words by humans, only 28% are concrete, with the majority being abstract concepts [1]. Concrete concepts have single, bounded, and identifiable referents that can be perceived through our senses, whereas abstract concepts lack such concrete and direct-sensory referents, making them hard to identify [3]. This distinction makes abstract concepts more challenging to understand, process, acquire and remember compared to concrete ones [4]. Despite being easily distinguishable from each other [2], abstract and concrete concepts are not strictly separate as a dichotomy but exist along a continuum [5]. The level of concreteness increases from highly abstract concepts such as \"freedom\" to highly concrete concepts like \"Border Collie\". The continuum perspective suggests that learning higher-order, more abstract words can be facilitated by extending the strategies and models used for grounding concrete words. Following this idea, to develop models capable of understanding both abstract and concrete concepts, the learning process starts with understanding the concrete concepts. Once a sufficient number of concrete words are learned, concepts with a higher level of abstraction can be acquired by combining concrete concepts. However, few studies have explored this extension in the context of artificial intelligence [10]. On the contrary, abstract concepts are crucial, as most of the human's daily language is abstract [9]. The ability to understand and manipulate abstract concepts is a fundamental trait of human intelligence; however, this ability is currently lacking in artificial agents [10].\nIn this work, we aim to explore the development of higher-order concepts with visual and categorical linguistic information through the implementation of a multimodal generative model. Our method employs three levels of conceptual categorisation: subordinate, basic, and superordinate. Concepts at the subordinate level are specific, such as \u201cgoldfish\" within \u201cfish\". These subordinate level concepts together form the basic level concepts, like \"fish\". Then the superordinate level, such as \"animal\", encompasses a range of basic level concepts[8]. The acquisition of abstract concepts is achieved through the learning of concrete concepts from the subordinate level. Then, the basic level concepts (more frequently used) are learned via the combination of concrete concepts. Finally, it progresses to grounding the superordinate level concepts (more abstract) through the combinatorial power of language with the basic level concepts. The level of abstraction increases from the subordinate level to the basic level, and then to the superordinate level concepts [6]. The multimodal generative model we propose in this work learns abstract concepts starting from concrete concepts. After the model has learned enough concrete concepts, it can understand more abstract concepts. This approach provides a way for AI agents to learn abstract concepts through the understanding of concrete concepts. At the same time, as vision is human's most important sense [7], our model learns the concepts with both visual and linguistic information. To the best of our knowledge, this is the first study to explore the development of high order concepts under the visual-linguistic multimodal setting."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Abstract Concepts Learning with Computational Models", "content": "Most existing grounded language models focus on learning words associated with concrete concepts, with limited research dedicated to computational models for abstract language learning. Stramandinoli et al. [12,13] use neural networks to learn concrete motor concepts through sensorimotor experience, and ground higher-order action concepts via the combination of concrete motor concepts. Their work suggests that a hierarchical organization of concepts could facilitate the acquisition of abstract words. Di Nuovo et al. [14,15] explore how embodied strategies, such as finger use, enhance an agent's understanding of numerical concepts. For emotion concepts learning, Prescott et al. [16] proposed a Gaussian process latent variable model to create a multimodal memory system for the iCub"}, {"title": "2.2 Multimodal Generative Model for Language-Visual Generation", "content": "Numerous studies ventured to derive shared representations from diverse modalities and enable cross-modal generation using these representations. Some works [18,19,20,21,22,23,24,25] are based on the Variational Autoencoder (VAE) [26], while others [27,28,29,30,31,32] are based on the Generative Adversarial Network (GAN) [33]. VAEs have become mainstream in multimodal deep generative models due to their ability to handle high-dimensional inputs, incorporate general-purpose priors for effective representation learning, and deal with the heterogeneity of multimodal data. In contrast, GANs struggle to introduce general-purpose priors as flexibly as VAEs in the multimodal setting, and sometimes suffer from instability during training [17]. To learn the joint distribution"}, {"title": "3 Methodology", "content": "In this paper, we propose a generative model for the development of superordinate and basic level concepts grounded on subordinate level concepts. Our model is based on a Multimodal Mixture-of-Experts Variational Autoencoders (MMVAE) [25], enabling concept learning with both linguistic representation and visual information. The model also possesses cross-modal generation capabilities to provide post-training language-to-vision and vision-to language-tests. The learning and testing process of the model is shown in Figure 1. The model is trained with information from both modalities and is evaluated based on the accuracy of generating the missing modality from the existing modality."}, {"title": "3.1 Variational Autoencoder for Visual Modality", "content": "The training of VAE[26] aims to maximize the marginal likelihood $p_\\theta(x)$, which is the probability of the observed data, $x$ under the model parameters $\\Theta$. As the true joint posterior distribution $p_\\theta(z \\mid x)$ is not directly accessible, computing the marginal likelihood requires integrating over all possible values of the latent variable $z$. Consequently, the marginal likelihood becomes intractable. In this case, the evidence lower bound (ELBO) is instead optimized, and a variational posterior $q_\\phi(z \\mid x)$ is used to approximate the true posterior $p_\\theta(z \\mid x)$. ELBO is formed by an expected log-likelihood of the data and Kullback-Leibler (KL) divergence between the variational posterior $q_\\phi(z \\mid x)$ and the prior distribution $p_\\theta(z)$\n\n$ELBO(x) = \\mathbb{E}_{z\\sim q_\\phi(z|x)} [\\log p_\\theta(x \\mid z)] \u2013 KL(q_\\phi(z \\mid x) || p_\\theta(z))$ (1)"}, {"title": "3.2 Variational Autoencoder for Language Modality", "content": "Due to the complexity imbalance between the language and visual modalities (i.e., a 224 x 224 image for the visual modality versus 2 labels for the language modality), when the visual modality is missing, the information provided by the language modality is insufficient to reconstruct the visual modality. This makes reconstructing the visual modality from the language modality in cross-modality generation challenging. Unlike models [24,25,27,28,31,32] that generate caption descriptions of images, our model generates labels. The limited vocabulary size of words and the insufficient relationships between words make it inadequate for training a well-performing embedding layer. To address this, we incorporate a pre-trained BERT [35] network before the language VAE. BERT generates the embeddings for the concepts, and its parameters are frozen during the autoencoder's training process. The labels are first embedded by BERT, and then these embeddings serve as the input for the VAE. On the decoder side, the resampled joint distribution is taken as input and decoded into reconstructed embedding vectors. The language encoders consist of four convolutional layers with a 1\u00d7 4 kernel size, followed by an additional convolutional layers with a 3\u00d73 kernel size to project the output to the specified latent space dimension. After embedding the words using the pre-trained BERT model, the embeddings are encoded by the language encoder. The language decoders are composed of five convolutional layers and one linear layer to transform the vector back into words. As shown in Table 1, the language modality inputs are three levels of hierarchical labels pair to pair to the visual modality information. The level of abstraction increases from the subordinate level concrete concepts to the basic level concept, then to the superordinate abstract concepts."}, {"title": "3.3 Multimodal Variational Autoencoder", "content": "To learn a joint distribution which represent the information from every modalities, a MoE network is used. MoE computes the variational posterior $q_\\phi (z \\mid X_{1:M})$ as a weighted sum of the individual posteriors $q_{\\phi_m} (z \\mid X_m)$ for each modality. In our work, we assume that after being embedded by BERT, the information on language modality will have a similar level of complexity as information on the visual modality, so each modality shares the same weight value for the joint distribution computation.\n\n$q(z \\mid X_{1:M}) = \\frac{1}{M} \\sum_{m=1}^{M} . q_{\\phi_m} (z|x_m)$ (2)\n\nIn the multimodal setting with MoE, the ELBO loss includes the reconstruction term $E_{z_m\\sim q_{\\phi_m} (z|x_m)} [\\log p_\\theta (x_m \\mid z_m)]$ that represents the accuracy with which each modality can reconstruct the information $x_m$ from sample distribution $z$; Kullback-Leibler (KL) divergence between the variational posterior $q(z|x_m)$ and the $p_\\theta(z)$.\n\n$ELBO(X_{1:M}) = \\frac{1}{M} \\sum_{m=1}^{M} [E_{z_m\\sim q_{\\phi_m} (z|x_m)} [\\log p_\\theta(x_m \\mid Z_m)] -KL(q_{\\phi_m} (z \\mid X_m) || P_{\\theta}(z))]$ (3)\n\nThe language and visual information are paired and input into the network. An encoder processes each modality's information into a latent space vector."}, {"title": "4 Experiment", "content": "We evaluate the performance of our model with two tests to assess how well it has acquired knowledge of the concepts: a language-to-vision test and a vision-to-language test. To compute the accuracy of the generated results from the language-to-vision test, we train a classifier composed of a pre-trained VGG19 model [37] and five trainable layers. The classifier is trained using the subset we selected from ImageNet and the paired hierarchical concepts. It takes images as input and predicts the hierarchical labels. In the language understanding test, the model generates the missing visual modality information as an image. The trained classifier receives the image as input and predicts the labels. We compute the cross-modality accuracy based on the comparison between the input label of our model and the predicted label from the classifier. In the language naming test, our model takes the image as input and generates labels. These generated labels are compared with the true labels of the image to assess accuracy. Also,"}, {"title": "4.1 Language Understanding (Language-to-Vision)", "content": "In the language understanding test, only the language information is provided to the model. We expect the model to be able to \"draw\" an image based on the given concepts. Each of the three levels of hierarchical concepts is used as input to the model, with the image paired with the subordinate level concepts treated as the missing modality. The linguistic modality information is encoded by the encoder, and the decoder samples from the joint distribution to generate the missing visual modality.\nThen the cross-generated image result is labeled by the pre-trained classifier, and the labels are compared with the model's concept input. Additionally, the generated image and input concepts are embedded to compute the CLIP score. As shown in Table 2, the pre-trained classifier's accuracy and the CLIP score for both levels of concept cross-generation results are similar to the ground truth, which is the accuracy and CLIP score of the dataset. This demonstrates that the model is capable of learning and understanding both concrete words and higher-order concepts. The generated results are shown in Figure 3."}, {"title": "4.2 Language Naming (Vision-to-Language)", "content": "For the language naming test, only the visual information is provided. We expect the model to be able to \"name\" the image. In this test, the image representing the visual information is the sole input to the model, and the model is expected to generate the corresponding linguistic modality information. Similar to the"}, {"title": "4.3 Ablation Studies", "content": "In this section, we conduct further experiments to test the model's ability to learn with additional concepts. The original dataset is extended in two dimensions. As the experiment mentioned before, we performed the Language-to-Vision and Vision-to-Language tests with the two new datasets. The reconstruction results are evaluated with the CLIP score.\nIn the first experiment, two more subordinate level concepts are added to each basic-level category. In total, ten subordinate level concepts are added to the dataset: \"Grasshopper\" and \"Ladybug\" to \"Insects\"; \"White Stork\" and \"Ostrich\" to \"Birds\"; \"Guinea Pig\" and \u201cHamster\" to \u201cSquirrel\"; \u201cLion fish\" and \"Stingray\" to \"Fish\". Table 4 shows the CLIP score for the first dataset, which is comparable to the CLIP score for the original dataset. This indicates that the model is capable of learning and understanding concepts effectively, even with a larger dataset containing more subordinate level concepts.\n\nIn the second experiment, two additional basic-level concepts, \"Cat\" and \"Dog\", are added to the original dataset. Like the original dataset, each basic-level concept includes three subordinate level concepts. For \"Cat\", the subordinate level concepts 'Tiger cat\", \"Egyptian cat\" and \"Persian cat\" are added, while for \"Dog\", the subordinate level concepts 'English Foxhound\", \"Border Collie\" and \"Golden Retriever\" are included. Table 5 shows the CLIP score for the first dataset. The results further demonstrate that the model can effectively learn and understand concepts when trained on a larger dataset with more basic-level concepts.\n\nOverall, the results from the two additional experiments show that the proposed model is able to learn and understand concepts with a larger dataset as the number of concepts increases. This suggests it could be an effective approach for incorporating more concrete and abstract concepts in the acquisition of abstract words."}, {"title": "5 Conclusion", "content": "In this research, we propose a multimodal generative model for learning both concrete and abstract concepts by integrating visual and linguistic information. Our findings show that the proposed MMVAE-based model effectively learns and generates higher-order abstract concepts by grounding them with the knowledge of concrete concepts. Through cross-generation testing, we demonstrate that the model not only accurately reconstructs visual and linguistic inputs but also effectively bridges the gap between concrete and abstract concept understanding. Unlike pre-trained vision-language models such as CLIP, which map images to texts and vice versa, our model learns a joint distribution to represent the information from both visual and language modalities. The learning process closely resembles human cognitive development. In this way, our work proposes a way to include more concrete and abstract concepts for concept learning tasks with a joint distribution that represents all multimodal information.\nDue to computational power and time limitations, additional computational resources and time are needed to include more concepts for more abstract concept learning. At this stage, our model in this work is limited to one superordinate level concept. In future work, we plan to extend the proposed model to learn more abstract concepts by including developmental linguistics datasets and age-of-acquisition lexical data like [40]. By incorporating developmental data [40], the model can prioritize learning earlier-acquired concepts before progressing to later-acquired ones. This approach will enable the model to understand more abstract and later-acquired concepts to be built upon the foundation of earlier-acquired concepts. At the same time, in this work, the reconstruction of visual information is based on a neighborhood search in the feature space learned solely from the visual modality. A potential improvement to the proposed method could involve using an embedding representation of a multimodal model."}]}