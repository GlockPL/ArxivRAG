{"title": "CROSS-ENTROPY IS ALL YOU NEED TO INVERT THE DATA GENERATING PROCESS", "authors": ["Patrik Reizinger", "Alice Bizeul", "Attila Juhos", "Julia E. Vogt", "Randall Balestriero", "Wieland Brendel", "David Klindt"], "abstract": "Supervised learning has become a cornerstone of modern machine learning, yet\na comprehensive theory explaining its effectiveness remains elusive. Empirical\nphenomena, such as neural analogy-making and the linear representation hypothe-\nsis, suggest that supervised models can learn interpretable factors of variation in a\nlinear fashion. Recent advances in self-supervised learning, particularly nonlinear\nIndependent Component Analysis, have shown that these methods can recover\nl latent structures by inverting the data generating process. We extend these identifia-\nbility results to parametric instance discrimination, then show how insights transfer\nto the ubiquitous setting of supervised learning with cross-entropy minimization.\nWe prove that even in standard classification tasks, models learn representations\nof ground-truth factors of variation up to a linear transformation. We corrobo-\nrate our theoretical contribution with a series of empirical studies. First, using\nsimulated data matching our theoretical assumptions, we demonstrate successful\ndisentanglement of latent factors. Second, we show that on DisLib, a widely-used\ndisentanglement benchmark, simple classification tasks recover latent structures\nup to linear transformations. Finally, we reveal that models trained on ImageNet\nencode representations that permit linear decoding of proxy factors of variation.\nTogether, our theoretical findings and experiments offer a compelling explanation\nfor recent observations of linear representations, such as superposition in neural\nnetworks. This work takes a significant step toward a cohesive theory that accounts\nfor the unreasonable effectiveness of supervised deep learning.", "sections": [{"title": "1 INTRODUCTION", "content": "Representation learning is a central task in machine learning, underpinning the success of extracting\nand encoding meaningful information from data (Bengio et al., 2013). Among the various paradigms,\nsupervised learning-particularly classification tasks using cross-entropy minimization has become\nthe dominant method in deep learning (Krizhevsky et al., 2012). Despite its simplicity, this form of\nsupervised learning has led to several intriguing and widely-observed phenomena, including: neural\nanalogy making (Mikolov et al., 2013), where models seemingly map between related concepts;\nthe linear representation hypothesis (Park et al., 2023), which posits that interpretable features can\nbe linearly decoded from neural representations; recent work on superposition in neural networks\n(Elhage et al., 2022), showing evidence that interpretable features are linearly represented in neural\nactivations (Templeton et al., 2024); and the success of transfer learning (Donahue et al., 2014),\nwhere a linear readout can be trained on top of learned representations to solve new tasks. These\nphenomena suggest that deep learning models encode various features in a manner that allows for\nlinear decoding. Yet, a comprehensive theory that explains why these properties emerge in deep\nlearning models has remained elusive (Arora et al., 2016; Park et al., 2023)."}, {"title": "2 BACKGROUND", "content": "Empirical evidence of a linear latent representation. The linear representation hypothesis (Park\net al., 2023) has lately received a lot of attention. A weak version of this hypothesis could mean\nthat there are directions in neural activation space that correspond to interpretable features. In the\ncase of neural analogy making, Mikolov et al. (2013) showed that there exist directions in word\nembeddings that are interpretable and preserved across input pairs. As an example for encoder f,\nproducing latent variables z, the direction \\(z = f(man) \u2013 f(woman)\\) seems to correspond to gender\nand can be added to other words such as \\(f(king) + z \u2248 f(queen)\\). Several datasets, such as the\nGoogle Analogy Dataset (GA) (Mikolov, 2013) and BATS (Drozd et al., 2016), have been developed\nto evaluate neural analogy-making. These were, for instance, evaluated in (Dufter & Sch\u00fctze, 2019).\nTheoretical explanations of linear representations have been proposed for word embeddings by Arora"}, {"title": "3 THEORY", "content": "This section presents our main theoretical contribution. We start with our motivation to understand\nself-supervised learning (SSL) with the help of the simplified DIET (Ibrahim et al., 2024) algorithmic\npipeline. For this, we propose a cluster-centric data generating process (DGP) that can model semantic\nclasses (\u00a7 3.1). Then we state our main result in \u00a7 3.2 and discuss an intuition behind DIET'S\nidentifiability. We conclude by investigating how DIET fits into the vast literature of (identifiable)\nSSL and auxiliary-variable Independent Component Analysis (ICA) methods (\u00a7 3.4). This leads to\na significant result for supervised classification by proving its identifiability under the DIET DGP\n(\u00a7 3.3). We provide the technical details for Generalized Contrastive Learning (GCL) (Hyvarinen\net al., 2019) in Appx. B.1 and InfoNCE (Chen et al., 2020; Zimmermann et al., 2021) in Appx. B.3.\nMotivation. Despite significant theoretical progress (Zimmermann et al., 2021; von K\u00fcgelgen et al.,\n2021; Rusak et al., 2024), it remains elusive why SSL methods work well in practice. Rusak et al.\n(2024) highlighted two remaining gaps between theory and practice: 1) practitioners often discard\nthe encoder's last few layers (termed the projector) for better performance, despite identifiability\nguarantees not reflecting this fact; and 2) the data is presumably clustered, not reflected in the\ncommon assumption of a uniform marginal. Despite a similar terminology in auxiliary-variable\nnonlinear ICA algorithms, such as Time-Contrastive Learning (TCL) (Hyvarinen & Morioka, 2016) or\nGCL (Hyvarinen et al., 2019), it is unclear how such methods relate to SSL at large. Interestingly, the\nidentifiability proofs for nonlinear ICA partition the model into a separate encoder and a regression\nfunction (Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019) and prove identifiability for the\nlatent variables after the encoder, but before the regression function. This aligns with the practice of\ndiscarding the projector in SSL (Bordes et al., 2023), though identifiability results do not reflect this\nfact (Zimmermann et al., 2021; von K\u00fcgelgen et al., 2021; Rusak et al., 2024). These observations\nserved as our motivation to investigate\nHow can we extend the identifiability guarantees to more realistic self-supervised classification\nscenarios, and can we apply these insights to improve our understanding of supervised learning?\nResults overview. We aim to advance our theoretical understanding of SSL, for this, we use\nthe recently proposed DIET (Ibrahim et al., 2024) (detailed in \u00a7 3.1), which, beyond its simplicity,\npromises the strongest and most realistic results, based on similarities to GCL (Hyvarinen et al., 2019).\nNamely, DIET uses a separate encoder and classification head, and solves an auxiliary classification\ntask akin to GCL-furthermore, its loss correlates with downstream performance, a non-obvious\nand welcome fact (Rusak et al., 2024). This provides the hope to resolve the two above points by\nmodeling the cluster structure of the data and proving identifiability for the representation used for\ndownstream tasks (Thm. 1). Subsequently, we leverage the insights from our identifiability theory\nand the DIET pipeline's similarity to supervised classification to show how the latter is a special case\nof DIET, where the sample indices correspond to the semantic class labels (Thm. 2)."}, {"title": "3.1 SETUP", "content": "DIET (Ibrahim et al., 2024). DIET solves an instance classification problem, where each sample x\nin the training dataset of size N has a unique instance label i. Augmentations do not affect this label.\nWe have a composite model \\(W \\circ f\\), where the backbone f produces d-dimensional representations,\nand a linear, bias-free classification head \\(W \\in \\mathbb{R}^{N \\times d}\\) maps these representations to a logit vector\nequal in size to the cardinality of the training dataset. If the parameter vector corresponding to\nlogit i is denoted as \\(w_i\\), then W effectively computes similarity scores (scalar products) between\nthe \\(w_i\\'s\\) and embeddings \\(f(x)\\). DIET trains this architecture to predict the correct instance label\nusing multinomial regression (with f, W and temperature \\(\\beta\\) as learnable variables), i.e., it solves a\nparametric instance discrimination (PID) task (Dosovitskiy et al., 2014; Wu et al., 2018):\n\n\n\\(\\mathcal{L}_{PID}(f, W, \\beta) = \\mathbb{E}_{(x,i)} \\left[ - \\ln \\frac{e^{\\beta \\langle w_i, f(x) \\rangle}}{\\sum_{j \\in \\mathcal{I}} e^{\\beta \\langle w_j, f(x) \\rangle}} \\right] \\)\n\n\nAn important fact is that (1) is the cross-entropy loss with instance labels, which we will leverage to\nconnect instance discrimination to supervised classification."}, {"title": "The proposed cluster-centric data generating process (DGP)", "content": "To prove the identifiability of the\nlatent variables, we need to formally define a latent variable model (LVM) for the data generating\nprocess (DGP). We take a cluster-centric approach, representing semantic classes by cluster vectors,\nsimilar to proxy-based metric learning (Kirchhof et al., 2022). Then, we model the samples of\na class with a von Mises-Fisher (vMF) distribution (intuitively, this is an isotropic multivariate\nNormal distribution that is restricted to the unit hypershere), centered around the class's cluster\nvector. This conditional distribution jointly models intra-class sample selection and augmentations\nof samples, together called intra-class variances. In contrast to conventional SSL methods such as\nInfoNCE (Zimmermann et al., 2021), this conceptually separates global and local structure in the\nlatent space: 1) the cluster-vectors describe the global structure of the latent space; and 2) the cluster-\ncentric conditional in (2) describes the local structure. This cluster-centric conditional embodies\nthat data augmentations are selected such that they ought not to change the sample's semantic class.\nOur conditional does not mean that each sample pair transforms into each other via augmentations\nwith high probability. It does mean that since we assume a latent variable model (LVM) on the\nhypersphere; i.e., all semantic concepts (color, position, etc.) correspond to a continuous latent\nvariable the latent manifold is connected, or equivalently, that the augmentation graph is connected,\nwhich is an assumption used in (Wang et al., 2022; Balestriero & LeCun, 2022; HaoChen et al., 2022).\nWe provide an overview of our assumptions, and defer additional details to Assums. 1C in Appx. A:\nAssumptions 1 (DGP with vMF samples around cluster vectors. Details omitted.).\n(i) There is a finite set of semantic classes \\(\\mathcal{C}\\), represented by a set of unit-norm d-dimensional\ncluster-vectors \\(\\{v_c \\mid c \\in \\mathcal{C}\\}\\) \\(\\subseteq \\mathbb{S}^{d-1}\\). The system \\(\\{v_c\\}\\) is sufficiently large and spread out.\n(ii) Any instance label i belongs to exactly one class \\(c = \\mathcal{C}(i)\\).\n(iii) The latent variable \\(z \\in \\mathbb{S}^{d-1}\\) of our data sample with instance label i is drawn from a vMF\ndistribution with concentration parameter/temperature \\(\\alpha\\) around the cluster vector \\(v_c\\) of class\n\\(c = \\mathcal{C}(i)\\):\n\n\n\\(z \\sim p(z/c) \\propto e^{\\alpha \\langle v_c,z \\rangle}.\\)\n\n\n(iv) Sample x is generated by passing latent variable z through an injective generator function:\n\\(x = g(z)\\)."}, {"title": "3.2 MAIN RESULT: DIET IDENTIFIES BOTH LATENT VARIABLES AND CLUSTER VECTORS", "content": "Under Assums. 1, we prove the identifiability of both the latent representations z and the cluster\nvectors, \\(v_c\\), in all four combinations of unit-normalized (i.e., when the latent space is the hypersphere,\ncommonly used, e.g., in InfoNCE (Chen et al., 2020)); and non-normalized (as in the original DIET\npaper (Ibrahim et al., 2024)) learned embeddings, \\(\\tilde{z}\\), and weight vectors, \\(w_i\\). We state a concise\nversion of our result and defer the full treatment and the proof to Thm. 1C in Appx. A:\nTheorem 1 (Identifiability of latent variables drawn from vMF around cluster vectors. De-\ntails omitted.). Let \\((f, W, \\beta)\\) globally minimize the DIET objective (1) under the following\nadditional constraints:\nC3. the embeddings \\(f(x)\\) are unnormalized, while the \\(w_i\\'s\\) are unit-normalized. Then \\(w_i\\)\nidentifies the cluster vector \\(v_{\\mathcal{C}(i)}\\) up to an orthogonal linear transformation O: \\(w_i =\\)\n\\(O v_{\\mathcal{C}(i)}\\), for any i. Furthermore, the inferred latent variables \\(z = f(x)\\) identify the\nground-truth latent variables z up to a scaled orthogonal transformation with the same O:\n\\(z = O \\tilde{z}\\).\nC4. neither the embeddings \\(f(x)\\) nor the \\(w_i\\'s\\) are unit-normalized. Then \\(w_i\\) identifies the\ncluster vectors \\(v_c\\) up to an affine linear transformation. Furthermore, the inferred latent\nvariables \\(\\tilde{z}\\) identify the ground-truth latent variables z up to a linear transformation.\nIn all cases, the weight vectors belonging to samples of the same class are equal, i.e., for any\n\\(i, j, \\mathcal{C}(i) = \\mathcal{C}(j)\\) implies \\(w_i = w_j\\).\nIntuition. DIET assigns a different (instance) label and a unique weight vector \\(w_i\\) to each training\nsample. The cross-entropy objective is optimized if the trained neural network can distinguish\nbetween the samples. Thus, the learned representation \\(z = f(x)\\) should capture enough information\nto distinguish different samples, even from the same class. However, the weight vectors \\(w_i\\'s\\) cannot\nbe sensitive to the intra-class sample variance or the sample's instance label i (because the conditional"}, {"title": "3.3 SUPERVISED CLASSIFICATION", "content": "In this section we relate our cluster-centric DGP to supervised classification. To see how supervised\nmachine learning is a special case of self-supervised approaches, consider that the sample index\n(i.e., the target of the cross-entropy loss) can be defined arbitrarily (as long as Assums. 1 are still\nsatisfied). This means that many labelings are possible, including the one used for supervised\nclassification. This, in hindsight obvious insight has important consequences: it can explain the\nsuccess of supervised cross-entropy-based classification. Namely, supervised learning performs\nnon-linear ICA. We demonstrate this in \u00a7\u00a7 3.4 and 4.3.\nTheorem 2 (Identifiability of latent variables drawn from a vMF around class vectors). Let\nAssums. 1 hold, and suppose that a continuous encoder \\(f : \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\\), a linear classifier W\nwith rows \\(\\{w_c \\mid c \\in \\mathcal{C} \\}\\), and \\(\\beta > 0\\) globally minimize the cross-entropy objective:\n\n\n\\(\\mathcal{L}_{supervised} (f, W, \\beta) = \\mathbb{E}_{(x,c)} \\left[ - \\ln \\frac{e^{\\beta \\langle w_c,f(x) \\rangle}}{\\sum_{c \\in \\mathcal{C}} e^{\\beta \\langle w_c,f(x) \\rangle}} \\right] \\)\n\n\nThen, the composition \\(h = f \\circ g\\) is a linear map from \\(\\mathbb{S}^{d-1}\\) to \\(\\mathbb{R}^d\\).\nIntuition: In the context of DIET, the cross-entropy objective encourages the learned representations\nto align with the cluster vectors corresponding to each class. The identifiability of the latent variables\nis ensured by the fact that the cluster structure reflects the underlying data distribution, modeled as a\nvMF distribution. This leads to a representation that captures the latent structure up to an orthogonal\ntransformation. Given the same underlying structure as in DIET, supervised learning can be viewed\nas a special case of instance discrimination, where the instance labels are replaced by class labels.\nThe cross-entropy objective, when applied to classification tasks, similarly encourages representations\nto align with class vectors. As a result, the latent variables are recovered up to a linear transformation,\nproviding a theoretical explanation for the success of supervised classification in learning linearly\ndecodable representations."}, {"title": "3.4 THE GENEALOGY OF IDENTIFIABLE CLASSIFICATION WITH CROSS-ENTROPY", "content": "Our main result in Thm. 1, and its corollary for supervised classification (Thm. 2) suggest the\nfollowing surprising conclusion to invert the DGP:\nSolving an (almost) arbitrary classification task by optimizing the cross-entropy objective is sufficient\nto invert the DGP and identify the ground-truth representation up to a linear transformation.\nTo show how solving a cross-entropy-based classification task is a key component to invert the DGP\nand to achieve linear identifiability, we provide a unified treatment of auxiliary-variable ICA (i.e.,\nweakly supervised or self-supervised classification) and supervised classification methods. We call\nthis a genealogy to allude to the fact that these methods can be seen as special cases, descending from\neach other (cf. Fig. 2 and Tab. 1 for an overview, and Appx. B for details).\nFrom GCL to TCL (Fig. 2a: arbitrary scalar labels and exponential family latent variables).\nThe most general framework we consider is Generalized Contrastive Learning (GCL) (Hyvarinen\net al., 2019), i.e., auxiliary-variable nonlinear ICA. GCL works with conditionally independent\nlatent variables in Euclidean space given (possibly vector-valued) auxiliary information u. It aims to\nclassify different values of u by distinguishing \\((x, u)\\) from \\((x, u^*)\\), where \\(u^*\\) is an arbitrary value of\nthe auxiliary variable. At the Bayes optimum of the cross-entropy loss, GCL provides identifiability\nof the latent variables after the encoder f, but before the classifier head W, up to elementwise\ninvertible transformations. When the latent variables are distributed according to an exponential"}, {"title": "4 EMPIRICAL RESULTS", "content": "In \u00a7 4.1, we empirically verify the claims made in Thm. 1 and Thm. 2 in the synthetic setting. We\ngenerate data samples according to Assums. 1: ground-truth latent variables are sampled around\ncluster centroids \\(v_c\\) following a vMF distribution. Data augmentations, which share the same instance\nlabel i, are sampled from the same vMF distribution around \\(v_c\\). In \u00a7 4.2, we describe our results on\nthe DisLib disentanglement benchmark (Locatello et al., 2019), and \u00a7 4.3 includes our experiments\non ImageNet-X (Idrissi et al., 2022)."}, {"title": "4.1 SYNTHETIC DATA", "content": "Setup. We consider N latent samples of dimensionality d generated from the conditional vMF\n\\(z \\sim p(z|v_c)\\), sampled around a set of \\(|\\mathcal{C}|\\) class vectors \\(v_c\\), which are uniformly distributed across\nthe unit hyper-sphere \\(\\mathbb{S}^{d-1}\\). We use an invertible multi-layer perceptron (MLP) to map ground-truth\nlatent variables to data samples. We train a classification head \\(W = [1]\\) and an MLP encoder\nthat maps samples to representations \\(\\tilde{z} \\in \\mathbb{R}^d\\) using the DIET objective (1). While to verify Thm. 1\ncase C4., we do not normalize W, we do unit-normalize the weight vectors to validate Thm. 1 case\nC3. We verify our theoretical claims by measuring the predictability of the ground-truth z from \\(\\tilde{z}\\)\nand \\(v_c\\) from \\(w_i\\) using the \\(R^2\\) score on a held-out dataset (Wright, 1921). For identifiability up to\northogonal linear transformations, we train linear mappings with no intercept, assess the \\(R^2\\) score\nand verify that the singular values of this transformation converge to 1, while for identifiability up to\naffine linear transformations, we simply assess the \\(R^2\\) of a linear predictor with intercept.\nResults for DIET. In Tab. 2, we report the \\(R^2\\) scores for the recovery of the cluster vectors \\(v_c\\)\nfrom W's rows and of the ground-truth latent variables z from the learned latent variables \\(\\tilde{z}\\). For\nDIET's PID task, we also consider cases with row-normalized W. We observe scores close to\n100% (\u2265 98%), even with many clusters (> 103) and samples (~ 105). High latent dimensionality\n(> 10) does impact the recovery of ground-truth latent variables\u2014such scalability problems are a\ncommon artifact in SSL (Zimmermann et al., 2021; Rusak et al., 2024). For a higher concentration\nof samples around \\(v_c\\) (i.\u0435., \\(\\kappa = 50\\)) as well as a lower number of clusters (i.e., \\(|\\mathcal{C}| = 10\\)), the \\(R^2\\)\nscore decreases, which is also a common phenomenon, and is possibly explained by too strong\naugmentation overlap (Wang et al., 2022; Rusak et al., 2024). For a low number of clusters, high \\(\\kappa\\)\nand a fixed number of training samples, the concentration of samples in regions surrounding centroids,\n\\(v_c\\), increases, a setting, refered to as \u201coverly overlapping augmentations\u201d, known to be suboptimal and\nleading to a drop in downstream performance (Wang et al., 2022). Our results also suggest that even\nunder model misspecification (last two rows in Tab. 2 with non-vMF distributions), identifiability\nstill holds. For unit-normalized \\(W_{\\text{rows}}\\), the MAE is lower, confirming the orthogonality of the\nmap \\(w_i\\rightarrow v_c\\). We provide an additional ablation study for the concentration of \\(v_c\\) across the unit\nhyper-sphere in Appx. C.\nResults for Supervised Classification. In Tab. 3, where the semantic class labels were used instead\nof the sample index, we only report the \\(R^2\\) score for the recovery of the ground-truth latent variables\nz from the learned latent variables \\(\\tilde{z}\\). Interestingly, in all but one setting, we observe higher \\(R^2\\)\nfrom representations learned with class labels rather than instance indices. This suggests that even a\ncoarser classification task may suffice to learn linearly identifiable representations of the underlying\nlatent variables."}, {"title": "4.2 DISLIB", "content": "Setup. Next, we evaluate our methods on the DisLib disentanglement benchmark (Locatello et al.,\n2019), which provides a controlled setting for testing disentanglement and latent variable recovery. It\nincludes the vision datasets dSprites, Shapes 3D, MPI 3D, Cars 3D, and smallNORB. We train both a\nthree-layer MLP with 512 latent dimensions and BatchNorm (which helped with trainability) and a\nCNN (ResNet18) also with 512 latent dimensions. We only consider latent variables with Euclidean\ntopology, as non-Euclidean, e.g., periodic latent variables such as orientation, are problematic to\nlearn and are potentially mapped to a nonlinear manifold (Higgins et al., 2018; Pfau et al., 2020;\nKeurti et al., 2023; Engels et al., 2024). We evaluate the recovery of latent variables by computing\nthe Pearson correlation between ground-truth and predicted factors.\nResults. The models trained using cross-entropy were able to recover latent variables such as object\nposition, scale, and orientation with high accuracy. As shown in Tab. 4, the Pearson correlation is\ngenerally highest when predicting the latent variables from the CNN's representation. In few cases,\nsuch as the position in dSprites, this can be done with fairly high accuracy even on the input data.\nNevertheless, in all settings, we see that the nonlinear function estimated by the model is necessary to\nlinearly identify the correct latent variables."}, {"title": "4.3 REAL DATA: IMAGENET-X", "content": "Setup. Finally, we test the generalizability of our theoretical insights on real-world data using\nImageNet-X (Idrissi et al., 2022). The latent variables are proxies, defined by human annotators\n(Idrissi et al., 2022). We evaluate how well linear decoders can predict latent variables from pretrained\nmodel representations. We use two architectures, a ResNet50 and a Vit-b-16 both trained on standard\nsupervised classification using a cross-entropy loss on the full ImageNet dataset (Deng et al., 2009).\nWe report the accuracy, correlation, and f1 score (the classes are significantly imbalanced). We plot\neach of these metrics against the performance of a random baseline consisting of shuffled labels.\nResults. Fig. 3 shows that even on complex, high-dimensional data, representations learned via\nsupervised learning allow for linear decoding of latent variables above chance performance. With the\ncaveat that the latent variables are likely not the true latent variables of the data generating process,\nstill, the linear identifiablity results on these proxy latent variables support our theoretical results."}, {"title": "5 DISCUSSION", "content": "Limitations. One limitation of our work is that we mainly focus on synthetic and controlled\ndatasets. While the results on ImageNet-X (Idrissi et al., 2022) are promising, further experiments\non other large-scale datasets would support the generality of our findings. However, this would\nrequire the availability of such datasets with full latent variable annotations. Although our cluster-\ncentric modeling of the data generating process allows capturing the inherent structure of the data,\nour assumption about the latent variables' geometric properties (such as being drawn from a vMF\ndistribution), may not hold in all real-world settings. The assumption that a data sample and its\naugmented version are conditionally independent given their semantic class could be relaxed in future\nwork, since it may be misaligned with realistic scenarios (Wang et al., 2022). Our experimental\nresults also suggest that our assumptions can be relaxed, as linear identifiability seems to hold even\nwhen some of the assumptions are violated (cf. Tabs. 3 and 5)."}, {"title": "Implications for Deep Learning", "content": "Our results indicate that deep learning models trained using\ncross-entropy naturally recover the underlying latent variables up to linear transformations. As our\nidentifiability proof for parametric instance discrimination illustrates with DIET, this statement also\nholds when the classification task is standard supervised learning. Our analysis on the key role of\ncross-entropy-based classification provides a theoretical foundation for phenomena such as neural\nanalogy-making, transfer learning, and linear decoding of features."}, {"title": "Conclusion", "content": "We extend the identifiability results of the auxiliary-variable nonlinear Independent\nComponent Analysis (ICA) literature to parametric instance discrimination with a cluster-centric\ndata generating process. Our modeling choice can capture the clustered structure of the data,\naccommodates non-normalized (as in ICA) and unit-normalized (as in InfoNCE) representations\n(Thm. 1). Furthermore, our identifiability result holds for the latent representation used post-training,\ni.e., for the latent variables before the classification head. Our results offer new insights into the\nsuccess of deep learning, particularly in supervised classification tasks, which we show is a special\ncase of the DIET parametric instance discrimination algorithm, where the instance labels equal\nthe semantic class labels (Thm. 2). By linking self-supervised learning-via nonlinear ICA and\nDIET-to supervised classification, we provide a theoretical framework that explains why simple\nclassification tasks recover interpretable and transferable representations."}, {"title": "Future Work", "content": "Future research could extend these insights by exploring the connections between\nnonlinear ICA and other forms of supervised learning, as well as testing the scalability of our\ntheoretical results to larger models and datasets. To assess the predictions of our theory beyond proxy"}, {"title": "A IDENTIFIABILITY OF LATENTS DRAWN FROM A VMF AROUND CLUSTER\nVECTORS", "content": "This section contains the formal statement and proof of our main theoretical result. Appx. A.1 contains\nthe definition and properties of affine generator systems, a useful mathematical object we rely on\nin our proof. Appx. A.2 contains the assumptions and the proof for the four combinations of unit-\nnormalized and non-normalized features and cluster vectors for parametric instance discrimination,\nwhereas Appx. A.3 discusses a special case, supervised classification."}, {"title": "A.1 AFFINE GENERATOR SYSTEMS", "content": "Definition 1 (Affine Generator System). A system of vectors \\(\\{v_c \\in \\mathbb{R}^d \\mid c \\in \\mathcal{C}\\}\\) is called an affine\ngenerator system if the affine hull defined by them is \\(\\mathbb{R}^d\\). More precisely, any vector in \\(\\mathbb{R}^d\\) is an\naffine linear combination of the vectors in the system. Put into symbols: for any \\(v \\in \\mathbb{R}^d\\) there exist\ncoefficients \\(a_c \\in \\mathbb{R}\\), such that\n\n\n\\(v = \\sum_{c \\in \\mathcal{C}} a_c v_c\\) and \\(\\sum_{c \\in \\mathcal{C}} a_c = 1.\\)\n\n\nLemma 1 (Properties of affine generator systems). The following hold for any affine generator\nsystem \\(\\{v_c \\in \\mathbb{R}^d \\mid c \\in \\mathcal{C}\\}\\):\n1. for any \\(a \\in \\mathcal{C}\\) the system \\(\\{v_c \u2013 v_a \\mid c \\in \\mathcal{C}\\}\\) is now a generator system of \\(\\mathbb{R}^d\\);\n2. the invertible linear image of an affine generator system is also an affine generator system."}, {"title": "A.2 IDENTIFIABILITY OF PARAMETRIC INSTANCE DISCRIMINATION", "content": "Assumptions 1C (DGP with vMF samples around cluster vectors). Assume the following DGP:\n(i) There exists a finite set of classes \\(\\mathcal{C"}, "represented by a set of unit-norm d-dimensional cluster-\nvectors \\(\\{v_c \\mid c \\in \\mathcal{C}\\}\\) \\(\\subseteq \\mathbb{S}^{d-1}\\) such that they form an affine generator system of \\(\\mathbb{R}^d\\).\n(ii) There is a finite set of instance labels \\(\\mathcal{I}\\) and a well-defined, surjective class function \\(\\mathcal{C} : \\mathcal{I} \\rightarrow\\)\n\\(\\mathcal{C}\\) (every label belongs to exactly one class and every class is in use).\n(iii) A data sample x belongs to class \\(C = \\mathcal{C}(I)\\) and is labeled with a uniformly-chosen instance\nlabel, i.e., \\(I \\in \\text{Uni}(\\mathcal{I})\\).\n(iv) The latent \\(z \\in \\mathbb{S}^{d-1}\\) of our data sample with label I is drawn from a vMF distribution around\nthe cluster vector \\(v_c\\), where \\(C = \\mathcal{C}(I)\\):\n\n\n\\(z \\sim p(z/C) \\propto e^{\\alpha \\langle v_C,z \\rangle}.\\)\n\n\n(v) The data sample x is generated by passing the latent z through a continuous and injective\ngenerator function \\(g : \\mathbb{S}^{d-1}\\rightarrow \\mathbb{R}^D\\), i.e., \\(x = g(z)\\).\nAssume that, using the DIET objective (6), we train a continuous encoder \\(f : \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\\) on x and a\nlinear classification head W on top of f. The rows of W are \\(\\{w_i \\mid i \\in \\mathcal{I}\\}\\). In other words, W\ncomputes similarities (scalar products) between its rows and the embeddings:\n\n\n\\(W: f(x) \\rightarrow [\\langle w_i, f(x) \\rangle \\mid i \\in \\mathcal{I}"], "0\\)": "n\n\n\\(\\mathcal{"}