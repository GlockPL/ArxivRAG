{"title": "Investigating Acoustic-Textual Emotional Inconsistency Information for Automatic Depression Detection", "authors": ["Rongfeng Su", "Changqing Xu", "Xinyi Wu", "Feng Xu", "Xie Chen", "Lan Wang", "Nan Yan"], "abstract": "Previous studies have demonstrated that emotional features from a single acoustic sentiment label can enhance\ndepression diagnosis accuracy. Additionally, according to the Emotion Context-Insensitivity theory and our pilot study, individuals with\ndepression might convey negative emotional content in an unexpectedly calm manner, showing a high degree of inconsistency in\nemotional expressions during natural conversations. So far, few studies have recognized and leveraged the emotional expression\ninconsistency for depression detection. In this paper, a multimodal cross-attention method is presented to capture the Acoustic-Textual\nEmotional Inconsistency (ATEI) information. This is achieved by analyzing the intricate local and long-term dependencies of emotional\nexpressions across acoustic and textual domains, as well as the mismatch between the emotional content within both domains. A\nTransformer-based model is then proposed to integrate this ATEI information with various fusion strategies for detecting depression.\nFurthermore, a scaling technique is employed to adjust the ATEI feature degree during the fusion process, thereby enhancing the\nmodel's ability to discern patients with depression across varying levels of severity. To best of our knowledge, this work is the first to\nincorporate emotional expression inconsistency information into depression detection. Experimental results on a counseling\nconversational dataset illustrate the effectiveness of our method.", "sections": [{"title": "1 INTRODUCTION", "content": "DEPRESSION is a common mental disorder characterized\nby a negative state of mind that persists for a long time.\nIt constitutes a significant impairment to an individual's\ncognitive processes, behaviors, and emotional states [1].\nFurthermore, it poses a grave risk of precipitating self-\nharmful behaviors, including self-mutilation and suicidal\ntendencies [2]. Timely diagnosis coupled with appropriate\ntherapeutic interventions can alleviate of these distressing\nsymptoms of patients. However, the diagnostic process is\noften laborious and relies on the expertise of psychologists.\nInvestigating automatic depression detection methods can\nbe imperative to enhance diagnostic efficiency.\nWith the rapid development of Deep Learning (DL), var-\nious DL-based methodologies for depression detection have\nbeen proposed [3\u201311], leveraging diverse data inputs such\nas speech, visual, and physiological signals. Among these,\nspeech data is relatively easier to obtain in many practical\nscenarios. Thus, this paper focuses on the modeling meth-\nods using speech for depression detection. Early depression\ndetection methods [3, 12\u201314] with speech data employ spe-"}, {"title": "2 RELATED WORKS", "content": "Most depression detection methods concentrate on two\naspects: feature extraction and model development. In this\nsection, we will review DL-based approaches for automatic\ndepression detection, with a focus on these two aspects.\nFurthermore, considering that counseling conversational\nincludes both acoustic elements and their corresponding\ntextual components, we will examine DL-based methods\nthat either utilize the acoustic modality only or integrate\nboth modalities for depression detection."}, {"title": "2.1 Depression Detection Using Acoustic Modality", "content": "In early studies about feature extraction, most features are\nhandcraft features [25\u201327], including F0, loudness, speaking\nrate, jitter, shimmer, and others. The use of handcraft fea-\ntures typically requires substantial expert involvement. To\nalleviate the reliance on expert intervention, researchers use\nvarious deep neural networks to extract high-level semantic"}, {"title": "2.2 Depression Detection Using Acoustic-Textual\nModalities", "content": "Previous studies have shown that diverse modalities, en-\ncompassing both acoustic and textual features, exhibit com-\nplementary properties and possess the capability to enhance\nthe performance of depression detection systems [3, 43].\nEffective feature representations and appropriate fusion\nmethods are crucial for multimodal depression detection.\nFor the audio modality, features commonly used in\ndepression detection in recent years are the deep features\nderived from SSL-based models, as detailed in Section 2.1.\nIn this paper, the SSL-based methods will be also used for\nprocessing the acoustic inputs. For text modality, commonly\nused features in depression detection can be divided into"}, {"title": "3 COUNSELING CONVERSATIONAL DATA", "content": "The dataset used for depression detection comprises a col-\nlection of online telephone counseling sessions spanning\n22.9 hours, encompassing a total of 272 individuals aged\nfrom 12 to 45. This dataset comprises healthy control in-"}, {"title": "4 PROPOSED FRAMEWORK", "content": "Drawing upon our preceding investigations [20], depressed\nindividuals exhibit inconsistent emotional expressions in\nthe acoustic and textual modalities compared to healthy\ncontrols. This phenomenon, referred to as Acoustic-Textual\nEmotional Inconsistency (ATEI), holds the potential for\ndistinguishing between healthy controls and depressed pa-\ntients. Thus, the primary focus of this article is the devel-\nopment of data-driven methodologies aimed at effectively"}, {"title": "4.1 Data Pre-processing", "content": "Due to the significant presence of silence during each\ncounseling, which can adversely affect the accuracy of de-\npression detection, we employed the open-source toolkit\npyannote.audio\u00b9 to automatically segment each audio file.\nThis automatic segmentation was followed by a manual\nreview to ensure the precision of the segmentation process.\nIn general, each acoustic segment data contained more\nthan one sentence. Furthermore, each acoustic segment was\naccompanied by a manually corrected transcription. The\ntranscriptions were initially generated using the WeNet\u00b2\ntoolkit and subsequently verified through manual double-\nchecking. To incorporate the inconsistency in emotional\nexpressions between acoustic and textual modalities into\nautomatic depression detection methods, sentiment annota-"}, {"title": "4.2 SSL-based Feature Extraction", "content": "Pre-trained models leveraging SSL techniques have gar-\nnered remarkable achievements. These models exhibit a re-\nmarkable ability to extract intricate features from unlabeled\ndata to address data scarcity. Their robust generalization\nand transfer learning capabilities have made them invalu-\nable across multiple fields, such as depression detection.\nOur objective in this section is to leverage the deep data\nrepresentations learned by these models to facilitate the\neasier acquisition of robust ATEI information in subsequent\nprocesses.\nFor the speech modality, commonly used pre-trained\nmodels for the feature extraction include Wav2Vec [36],\nHuBERT [37], Whisper [38], and WavLM [39]. In this paper,\nwe investigated these SSL-based models for acoustic fea-\nture extraction. Throughout the entire training process, the\nSSL-based model parameters remained fixed. Additionally,\nsince an abundance of emotional expression information is\ncontained in the intermediate layer outputs of SSL-based\nmodels [55\u201357], we have chosen the outputs from the 12th\nblock of Wav2Vec, Whisper, WavLM, and HuBERT models\nas the inputs for further modeling in this paper. The acoustic\nfeatures derived from these SSL-based models are at frame-\nlevel granularity.\nFor the text modality, previous studies have demon-\nstrated that employing BERT [47] can grasp long-range lin-\nguistic dependencies through a multi-layered Transformer\nstructure, thus enabling the acquisition of robust textual\nfeatures for each word. These word-level textual features\nencompass substantial semantic information that can be\nharnessed effectively for depression detection. ROBERTa\n[58, 59], a pre-trained language model refined from the\noriginal BERT architecture, achieves superior performance\ncompared to the latter by adjusting critical hyperparame-\nters and training techniques. RoBERTa has demonstrated\nremarkable performance in downstream applications such\nas sentiment analysis, text classification, and question an-\nswering systems. In this paper, we investigated both BERT\nand ROBERTa for textual feature extraction."}, {"title": "4.3 Depression-related Feature Extraction", "content": "Utilizing the SSL-based feature extraction method described\nin Section 4.2, we can derive frame-level acoustic features"}, {"title": "4.3.1 Acoustic and textual features related to depres-\nsion", "content": "and word-level textual features. These short-term features\nmainly contain local information in speech or transcriptions,\nsuch as the acoustic properties of individual speech frames\nor the semantic content of individual words. However, de-\ntecting depression often requires comprehensive considera-\ntion of a wider range of contextual information, including\nchanges in tone of speech, overall emotional tendencies of\nsentences in the text, and other things. Therefore, using\nshort-term features alone often cannot fully reflect the over-\nall state of patients with depression. We need to use feature\naggregation methods to obtain long-term depression-related\nfeatures from the acquired short-term SSL-based feature\nsequences.\nCommon feature aggregation approaches include both\ntraditional statistical methods like average pooling, as well\nas modern deep learning-based techniques. The latter em-\nploys diverse network architectures, such as convolutional\nneural networks (CNN), recurrent neural networks (RNN),\nand Transformers, to efficiently aggregate short-term fea-\ntures across the temporal dimension. Compared to tradi-\ntional RNN and CNN, the Transformer architecture excels\nin capturing long-range dependencies, coupled with its\noutstanding performance in speech and natural language\nprocessing (NLP) tasks. Consequently, this paper leveraged\nthe Transformer for aggregating short-term features across\nboth acoustic and textual modalities.\nAs shown in Fig. 2, assuming that $X^{(A)}$ and $X^{(T)}$ are the\nacquired frame-level acoustic feature sequence and word-\nlevel textual feature sequence, respectively."}, {"title": "4.3.2 \u0391\u03a4\u0395EI information", "content": "The proposed multimodal cross-attention model for ATEI\ninformation extraction is shown in Fig. 3. The supervised\nlabels of this model are consistent (labeled as \"1\") and\ninconsistent (labeled as \"0\"). Precisely, when an acoustic\nsegment and its corresponding textual segment share the\nsame sentiment label, the supervised label for this acoustic-\ntextual segment pair is set to \u201c1\u201d. Conversely, when the\nsentiment labels between the acoustic and textual segments\ndo not match, it is assigned \u201c0\u201d.\nAs previously mentioned, the inputs $X^{(A)}$ and $X^{(T)}$\nin Fig. 3 contain universal information like speech speed,\nintonation, syntactic structure, textual semantics, and oth-\ners. This information is closely associated with emotional\nexpression. After passing through N Transformer blocks,\nwe can obtain the intricate local and global temporal depen-\ndencies related to emotional expressions in the acoustic and\ntextual domains, respectively, denoted by $X'^{(A)} \\in \\mathbb{R}^{T_1 \\times D}$ \nand $X'^{(T)} \\in \\mathbb{R}^{T_2 \\times D}$. And then $X'^{(A)}$ undergoes a linear\ntransformation to yield $Q^{(A)}, K^{(A)}, V^{(A)}$. Similarly, $X'^{(T)}$\nis transformed to produce $Q^{(T)}, K^{(T)}, V^{(T)}$. Through the\nacoustic-textual cross-attention layer, $X^{(AT)}$ and $X^{(TA)}$ are\nobtained by:\nReferring to the consistent or inconsistent supervised\nlabels depicted in Fig. 3 and as indicated by Equation (8),\nthe cross-attention score softmax(\u00b7) is supposed to measure\nthe degree of correlation between textual and acoustic emo-\ntional expressions. In other words, it is used to represent the\nmismatch level between each word and its corresponding\nacoustic sentiment. In addition, $X^{(AT)}$ is calculated from the\nfeature $V^{(T)}$, which contains emotional expression informa-\ntion in the textual domain. This implies that the derived\nfeature $X^{(AT)}$ represents the abnormal emotional expression\npatterns of depressed patients observed in the textual do-\nmain. Similarly, $X^{(TA)}$ in Equation (9) represents the ab-\nnormal emotional expression patterns of depressed patients\nobserved in the acoustic domain. $X'^{(A)}, X'^{(T)}, X^{(AT)}$, and"}, {"title": "4.4 Fusion Strategies", "content": "This paper investigates three distinct feature-level fusion\nstrategies for the acquired acoustic and textual features\nThese fusion strategies include addition, multiplication, and\nconcatenation. They are defined as:\na) Addition:\n$e_{fusion} = e^{(A)} \\oplus e^{(T)} \\oplus e^{(E)}$\nwhere $\\oplus$ represents the element-wise addition.\nb) Multiplication:\n$e_{fusion} = e^{(A)} \\odot e^{(T)} \\odot e^{(E)}$\nwhere $\\odot$ represents the element-wise multiplication.\nc) Concatenation:\n$e_{fusion} = [e^{(A)}, e^{(T)}, e^{(E)}]$\nFurthermore, we found that the degree of the ATEI\ndiffers among individuals with different levels of depres-\nsion severity. For instance, individuals suffering from more\nsevere depression would prefer to employ a flat tone to\nexpress sad text content. In light of this observation, a\nlearnable scaling factor $\\alpha = [\\alpha_1, \\alpha_2, ..., \\alpha_D]$ is introduced\nto dynamically modulate the ATEI. This approach aims to\nprecisely capture the correlation between the degree of ATEI\ninformation and the severity of depression, thereby enhanc-\ning the model's capacity to derive feature representations\nthat robustly correlate with the severity of depressive symp-\ntoms, and ultimately improving the precision of diagnostic\ndetection. Thus, Equation (11) can be revised as follows:\n$e^{(E)} = \\alpha \\odot FC(h^{(E)}), \\sum_{i=1}^{D} \\alpha_i = 1$\nwhere $\\odot$ represents the element-wise multiplication.\nIt is noteworthy that when the ATEI information is\nencoded using the \u201cEmbedding\u201d approach, $e^{(E)}$ can be ob-\ntained from Equation (11) or (15). Specifically, when the\nATEI information is encoded using the \"0/1\" approach,\nonly feature concatenation is considered, as the dimensions\namong $e^{(E)} \\in \\mathbb{R}^{1 \\times 1}, e^{(A)} \\in \\mathbb{R}^{1 \\times D}$, and $e^{(T)} \\in \\mathbb{R}^{1 \\times D}$ are\ndifferent. After the fusion process, multiple fully connected\nlayers are utilized to predict the severity of depression,\nwhich is shown in Fig. 1. Moreover, to ensure robust training\nof the proposed model, this study employs an incremental\ntraining strategy for optimizing the model. Initially, the\nmultimodal cross-attention neural network for extracting\nATEI information undergoes pre-training using the cross-\nentropy $L_{ATEI}$, followed by joint training with the entire\nnetwork. The loss function for the joint training stage is\nformulated as follows:\n$L_{Total} = L_{Depression} + L_{ATEI}$\nwhere $L_{Depression}$ is the cross-entropy for optimizing the\ndepression detection task, and $L_{ATEI}$ is the cross-entropy for\noptimizing the emotion consistency recognition task."}, {"title": "5 EXPERIMENTS", "content": "Experiments were conducted on the dataset described in\nSection 3. The criteria used to evaluate the performance of\nthe proposed framework include accuracy, precision, recall,\nand the F1 score. For each model, a five-fold cross-validation\napproach was employed, wherein 80% of the speaker data"}, {"title": "5.1 Experimental Setup", "content": "To better assess the proposed framework, we firstly in-\nvestigated the SOTA neural network architectures utilized\nfor the detection of depressive disorders in this section.\nUtilizing the established SOTA neural network architectures\nfor depression detection as a foundation, we validated the\neffectiveness of ATEI information, which was derived from\nthe multimodal cross-attention method, in enhancing the\naccuracy of depression detection. Furthermore, we investi-\ngated the most suitable methodologies for integrating ATEI\ninto the SOTA neural network architectures designed for de-"}, {"title": "5.2 Results", "content": "We first explored how to use counseling conversational data\nto build SOTA depression detection baseline systems. The\nperformance of the depression detection systems with vari-\nous settings is shown in TABLE 2. This table is divided into\ntwo part: the first part presents the \u201cA\u201d depression detection\nsystems utilizing depression-related acoustic features, while\nthe second are the \u201cA+T\" systems that incorporate both\ndepression-related acoustic and textual features. It can be\nobserved from the first part of TABLE 2 that the \u201cA\u201d de-"}, {"title": "5.2.2 Multimodal cross-attention method for extracting\nATEI information", "content": "As discussed before, it has been observed that, unlike\nhealthy individuals, depressed patients often exhibit incon-\nsistency in emotional expression across different modalities.\nThese observations have the potential to enhance the ac-\ncuracy of prevalent depression detection technologies. In\nthis section, we investigated the effectiveness of extracting\nATEI information related to depression using the multi-\nmodal cross-attention method described in Section 4.3.2."}, {"title": "5.2.3 Integrating ATEI information", "content": "Furthermore, our study explored the suitable methods for\nintegrating ATEI into the SOTA depression detection mod-\nels. The performance of depression detection systems, which\nincorporate additional ATEI embedding features with di-\nverse fusion strategies, is presented in TABLE 4. From the\nresults presented in this table, we observed that in both the\n\u201cA+E\u201d and \u201cA+T+E\" configuration, the depression detec-\""}, {"title": "6 CONCLUSION", "content": "According to the ECI theory in psychology, depressed pa-\ntients exhibit emotional blunting. This results in inconsistent\nemotional expressions during natural conversations, which\nare abundant in the counseling conversational data. Ef-\nfectively extracting this emotional expression inconsistency\nfrom counseling conversations and applying it into model-\ning is crucial for automatic depression detection. This paper\npresented a Transformer-based framework using additional\nATEI information for predicting the severity of depression.\nIn this framework, a multimodal cross-attention method\ncaptures the ATEI information by analyzing the complex lo-\ncal and long-term dependencies of the emotional expression\nacross both the acoustic and textual domains. Additionally,"}]}