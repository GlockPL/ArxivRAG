{"title": "LightFFDNets: Lightweight Convolutional Neural Networks for Rapid Facial Forgery Detection", "authors": ["Gunel Jabbarli", "Murat Kurt"], "abstract": "Accurate and fast recognition of forgeries is an issue of great importance in the fields of artificial intelligence, image processing and object detection. Recognition of forgeries of facial imagery is the process of classifying and defining the faces in it by analyzing real-world facial images. This process is usually accomplished by extracting features from an image, using classifier algorithms, and correctly interpreting the results. Recognizing forgeries of facial imagery correctly can encounter many different challenges. For example, factors such as changing lighting conditions, viewing faces from different angles can affect recognition performance, and background complexity and perspective changes in facial images can make accurate recognition difficult. Despite these difficulties, significant progress has been made in the field of forgery detection. Deep learning algorithms, especially Convolutional Neural Networks (CNNs), have significantly improved forgery detection performance.\nThis study focuses on image processing-based forgery detection using Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] data sets. Both data sets consist of two classes containing real and fake facial images. In our study, two lightweight deep learning models are proposed to conduct forgery detection using these images. Additionally, 8 different pretrained CNN architectures were tested on both data sets and the results were compared with newly developed lightweight CNN models. It's shown that the proposed lightweight deep learning models have minimum number of layers. It's also shown that the proposed lightweight deep learning models detect forgeries of facial imagery accurately, and computationally efficiently. Although the data set consists only of face images, the developed models can also be used in other two-class object recognition problems.", "sections": [{"title": "1 Introduction", "content": "Today, the internet is filled with a wealth of visual and video content. This situation is driving the development of search applications and algorithms capable of performing semantic analysis on image and video content to provide users with better search results and summaries. When we see an image, expressing what or who is in it takes less than a millisecond. For instant object recognition, a sequence of information forms within the human brain. Various advancements have been made to replicate this human visual system in computers. Furthermore, object recognition is considered a crucial skill for most computer and robot vision systems. The recognition of objects finds applications in various fields such as autonomous vehicles, automatic forgery detection systems, virtual reality, medical imaging, and e-commerce. Across the globe, different researchers have reported significant progress in areas like image labeling, object detection, and object classification [12, 37], enabling the development of approaches to tackle object detection and classification problems.\nDeep learning algorithms have revolutionized the field of artificial intelligence and are used especially effectively in image processing [50]. CNNs, in particular, have demonstrated remarkable performance in object detection and classification tasks [58, 64]. CNNs consist of sequential layers that automatically extract features from data and are employed"}, {"title": "2 Related work", "content": "Our work is related to deep learning models for object detection, facial image generation and detection, so we briefly review each of these below."}, {"title": "2.1 Object detection", "content": "Object detection, one of the most fundamental and challenging problems in the field of computer vision, has attracted great attention in recent years. As noted by Zou et al. [65], comprehensively examine this fast-moving field of research in light of technical evolution spanning more than a quarter of a century (from the 1990s to 2022). In their work, they covered topics such as past landmark detectors, detection datasets, measurements, basic building blocks of the detection system, acceleration techniques and state-of-the-art detection methods."}, {"title": "2.2 Facial image generation", "content": "R\u00f6ssler et al. [47] proposed FaceForensics++, which is a dataset of facial forgeries that enables researchers to train deep-learning-based approaches in a supervised fashion. The dataset contains manipulations created with four state-of-the-art methods, namely, Face2Face, FaceSwap, DeepFakes, and NeuralTextures.\nKammoun et al. [18] reviewed face (Generative Adversarial Networks) GANs and their different applications in their study. They mainly focused on architectures, problems and performance evaluation according to each application and the data sets used. More precisely, they reviewed the progression of architectures and discussed the contributions and limitations of each. Then, they revealed the problems encountered in face GANs and proposed solutions to solve them.\nRecently, Yadav et al. [62] proposed ISA-GAN, an inception-based self-attentive encoder-decoder network for face synthesis using delineated facial images. Motivated by the reduced computational requirement of the inception network and improved performance and faster convergence by attention networks, Yadav et al. [62] utilized the inception, self-attention, and spatial attention modules in the ISA-GAN. Authors showed that the ISA-GAN shows on an average improvement of 9.95% in SSIM score over CUHK dataset while 10.38%, 12.58% for WHU-IIP and CVBL-CHILD datasets, respectively."}, {"title": "2.3 Facial image detection", "content": "Soylemez and Ergen [49] examined the performance of different Convolutional Neural Network architectures in the field of facial expression analysis. In this study, they used the FER2013 dataset. In the study, VGG-16 network was used first. As a result of the training process, the network overfitted the training data, a performance of around 25% was achieved, and the training failed. Inception-V1 showed 65.8% success, Inception-V3 showed 63.2% success, Xception showed 61.1% success, ResNet50-V1 showed 59.5% success, ResNet50-V2 showed 59.8% success, MobileNet-V1, MobileNet-V2 showed 58.5% success.\nAdjabi et al. [1] in their review, they present the history of facial recognition technology, current state-of-the-art methodologies, and future directions. In particular, the newest databases focus on 2D and 3D facial recognition methods. Here, 23 well-known face recognition datasets are presented in addition to evaluation protocols. Approximately 180 scientific publications from 1990 to 2020 on facial recognition and its important problems in data collection and preprocessing were reviewed and summarized.\nWang et al. [57] studied methods that can detect facial images created or synthesized from GAN models. Existing perception studies are divided into four categories: (1) deep learning-based, (2) physically-based, (3) physiological-based methods, and (4) evaluation and comparison based on human visual performance. For each category, the main ideas are summarized and these are related to method applications.\nAlrimy et al. [4] used three CNN-based methods with VGG-16, Inception-V3, and Resnet50-V2 network architectures to classify facial expressions into seven emotion classes. The facial expression dataset from Kaggle and JAFFE dataset was used to compare the accuracy between the three architectures to find the pre-trained network that best classifies the models. The results showed that the VGG-16 network architecture produced higher accuracy (93% on JAFFE and 54% on Kaggle) than other architectures.\nLightweight Convolutional Neural Networks were used by \u015eafak and Bar\u0131\u015f\u00e7\u0131 [66] to detect fake face images produced by the whole face synthesis manipulation method. MobileNet, MobileNet-V2, EfficientNet-B0 and NASNetMobile algorithms were used for the training process. The dataset used includes 70,000 real images from the FFHQ dataset and 70,000 fake images produced with StyleGAN2 using the FFHQ dataset. In the training process, the weights of the models trained on the ImageNet dataset were reused with transfer learning. The highest accuracy rate was achieved in the EfficientNet-B0 algorithm with a success rate of 93.64%.\nNowroozi et al. [39] verified synthetic face images using a large printed document dataset. They presented a new dataset consisting of multiple synthetic and natural printed irises taken from VIPPrint Printed and Scanned facial images.\nHamid et al. [15] proposed a computer vision model based on Convolutional Neural Networks for fake image detection. A comparative analysis of 6 popular traditional machine learning models and 6 different CNN architectures was conducted to select the best possible model for further experiments. The proposed model based on ResNet-50 used with powerful preprocessing techniques resulted in an excellent fake image detector with an overall accuracy of 0.99,"}, {"title": "3 Our Lightweight Deep Learning Models", "content": "The deep learning models developed in this article addresses object recognition issues using deep learning and transfer learning methods, focusing on static color images and their features, such as shape and color.\nDespite consisting of only a few layers and being trained for a limited number of epochs, the proposed models achieve accuracy levels nearly equivalent to existing models. The reduced number of layers significantly lowers computational complexity, providing a speed advantage over other models.\nThe study includes three different experimental study groups conducted on the Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] datasets:\n1) Performance values were obtained for a sequential model using facial images.\n2) Performance values were obtained using transfer learning with 8 different CNN architectures for selected datasets.\n3) The obtained performance values were experimentally compared."}, {"title": "3.1 Used Datasets", "content": "In this study, two freely available datasets were identified for real and fake face images: Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61]. These datasets were obtained from the open-access website for data scientists and machine learning practitioners, Kaggle (https://www.kaggle.com/), without any associated costs. Experimental studies were conducted on these datasets, which contain both real and fake facial images."}, {"title": "3.1.1 Fake-Vs-Real-Faces (Hard) Dataset", "content": "The Fake-Vs-Real-Faces (Hard) dataset [10] is curated for the classification task of distinguishing between fake and real human faces. The synthetic faces collected in this dataset were generated using StyleGAN2, making it challenging even for the human eye to accurately classify them. Real human faces, reflecting various characteristics such as age, gender, makeup, ethnicity, etc., were collected to represent diversity encountered in a production environment. This diversity is a crucial factor in evaluating the ability of classification algorithms to recognize real faces accurately under different conditions.\nImages in the dataset are in JPEG format and have standard dimensions of 300 \u00d7 300 pixels. The \"Fake\" faces were collected from the website https://thispersondoesnotexist.com. The \"Real\" face images were obtained through the Unsplash website's API and then cropped using the OpenCV library.\nThe dataset consists of a total of 1288 images, containing two classes: \"Fake\" faces (700 images) and \"Real\" faces (588 images)."}, {"title": "3.1.2 140k Real and Fake Faces Dataset", "content": "This dataset includes 70, 000 REAL faces from the Flickr-Faces-HQ (FFHQ) dataset [19] collected by Nvidia, as well as 70, 000 FAKE faces sampled from 1 million synthetic faces generated by StyleGAN, provided by Bojan. The FFHQ dataset consists of 70, 000 high-quality PNG images at a resolution of 1024 \u00d7 1024, displaying significant variations in age, ethnicity, and image backgrounds. It also covers accessories such as glasses, sunglasses, hats, and others in a comprehensive manner. The images were scraped from Flickr, inheriting all biases of that platform, and were automatically aligned and cropped using dlib. Only images under permissible licenses were collected.\nIn the 140k Real and Fake Faces dataset [61], both datasets were properly combined, all images were resized to 256 \u00d7 256 pixels, and the data was divided into training, validation, and test sets."}, {"title": "3.2 Our Facial Forgery Detection Models Based on CNN Architecture", "content": "In the literature, different architectures are developed for various problems. While some architectures perform well for a specific problem or dataset, others can achieve successful results in solving many problems. In this study, it is aimed to design an architecture that performs better than existing architectures in two-class (binary) object recognition problems, based on both datasets. Therefore, deep learning models have been developed using the CNN architecture within the scope of this article. In the first sequential model, named as LightFFDNet v1, 2 convolutional layers and 1 output layer are used. In the second deep learning model, named as LightFFDNet v2, designed based on the same principle, 5 convolutional layers and 1 output layer are utilized.\nIn the designed models, images are resized to 224 \u00d7 224, and they are provided as input parameters to the input layer. Since classification will be performed in the output layer and there should be as many artificial neural cells as the number of classes, the number of classes has been presented as parameters to the output layer. It should be noted that the number of layers in the models was decided based on the results of the application, by trying out 2, 3, 4, 5, and 6 layers to achieve the best results.\nIn the convolutional layers, 3 \u00d7 3 filters were used for feature learning. During this learning process, the filter was applied by sliding over the entire image in each layer. The weights within the filter were multiplied by pixel values and summed to compute the result. As a result of these operations, filters that activated on features detected in the image were learned. The features learned in each layer have a hierarchical structure. Basic features are learned in the initial layers, while"}, {"title": "4 Experimental Studies", "content": "In this study, the object recognition system developed is trained and tested on two different datasets. Preprocessing is applied to both datasets (a portion is taken from the dataset, separated into test, training, and validation sets, and all images are resized) for evaluation.\nThe study includes three different experimental study groups conducted on the Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] datasets:\n1. Performance values were obtained on our deep learning models (LightFFDNets) for face images.\n2. Using the transfer learning method, performance values have been obtained on 8 different CNN architectures for both datasets.\n3. The obtained performance values have been compared.\nOur deep learning models have been developed using the MATLAB [53] programming language. During the image loading and preprocessing stages, the functions imageDatastore, augmentedImageDatastore, imresize(), inputsize(), numel(), and length() defined in MATLAB have been utilized. The results obtained from the experimental studies have been visualized using the functions disp(), plot(), subplot(), imshow(), confusionmat(), and confusionchart()."}, {"title": "4.1 Data Preprocessing", "content": "In this study, the distribution for the Fake-Vs-Real-Faces (Hard) dataset is set as 70% for training, 10% for validation, and 20% for testing. This partitioning ratio also affects the model's performance. When creating this distribution, the allocation of face images in each category is performed randomly. Additionally, the distribution is made so that an equal number of images are taken from each class, and the same images are used for each distribution. After the distribution is completed, the images are resized from 300 x 300 to 224 x 224."}, {"title": "4.2 Implementation of Our Deep Learning Models Based on CNN Architecture", "content": "In this study, the deep learning models based on the CNN architecture were developed using the Matlab programming language. The developed models present a sequential structure. In the sequential model structure, there is a single input layer. The other layers are arranged in a consecutive manner, where the output of each layer serves as the input for the next. Within the scope of this study, two different deep learning models were developed using sequential model structures.\nThe imageInputLayer() function, in which the data dimensions are used as parameters, was utilized for the input layer of the deep learning models based on the developed CNN architecture. For face images, the parameter values were set as 224 \u00d7 224 x 3. In the convolutional layers, the convolution2dLayer() function, where the filter size and number are used as parameter values, was employed. The filter size was set to 3 x 3, and the number of filters was set to 32. The 'Padding' and 'same' parameters were entered to indicate that padding would be applied to preserve the input data size. In the convolutional layers used in the structure of the models, output of each layer was defined as the input for the subsequent layer.\nAfter each convolutional layer, an activation layer and a normalization layer were added. Except for the last one, a max pooling layer was also added after each convolutional layer. To accelerate the learning process and make the network more stable, the batchNormalizationLayer() function was employed in the normalization layer. The reluLayer() function was added as the activation function due to the preference for the ReLU activation function. In the pooling layer, the maxPooling2dLayer() function, which takes the filter size and stride value as parameters, was used. The default parameter values of a 2 x 2 filter size and a stride value of 2 were applied. In the output layer, as classification was performed, the number of classes was given as a parameter to the fullyConnectedLayer() function, and the softmaxLayer() function was used for the activation function.\nFor model design, a cell array named layers, containing a sequence of layers that define the Convolutional Neural Network (CNN) architecture, was created. In the designed models, the number of layers was determined by considering only the convolutional and fully connected layers. The first sequential model (LightFFDNet v1) designed in this study contains a total of 3 layers, consisting of 2 convolutional layers and 1 fully connected layer. The second model (LightFFDNet v2), on the other hand, contains a total of 6 layers, consisting of 5 convolutional layers and 1 fully connected layer.\nWhen designing CNN models, the algorithms or techniques used in the models (LightFFDNets) introduce certain parameters that the designer must decide upon, and these"}, {"title": "4.3 Application of Pre-trained Deep Neural Networks", "content": "Pre-trained neural networks are deep learning models that have been previously trained on large and complex datasets. These models typically learn high-level features from a general dataset and can subsequently be adapted for different tasks. In this study, CNN architectures were initialized using ImageNet weights and retrained with the images in the dataset. A total of eight different CNN architectures were utilized for classification tasks. These include AlexNet [20], VGG-16 [36], VGG-19 [48], DarkNet-53 [45], GoogleNet [52], MobileNet-V2 [17], ResNet-50 [16], and ResNet-101 [16]. Each of these architectures was trained separately on the images from both datasets for the classification task, and the results were compared with the newly developed models.\nIn the training of pre-trained deep learning models, the hyperparameter values used for training the newly developed models were also utilized. The batch size was set to 16, with the ReLU activation function and the Adam optimization algorithm employed. The learning rate was determined to be 0.0001, and the validation frequency was set to 3. Classification was performed for 3, 5, and 10 epochs for each model. Similar to our own models, the Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] datasets were used for the applications here as well. The datasets were divided into 70% training, 10% validation, and 20% test for all classification tasks. The training, validation, and test images used in all models were consisted of the same images. Preprocessing was applied to the images to enhance classification accuracy and to adjust the dimensions to fit the layers of various CNN models. After preprocessing, images of different sizes were scaled to a dimension of 224 x 224."}, {"title": "4.4 Experimental Results and Comparisons on Datasets", "content": "The results obtained from training and testing the proposed object recognition methods on the Fake-Vs-Real-Faces (Hard) [10] and 140k Real and Fake Faces [61] datasets are presented in this section. This section includes the classification results achieved using face images in the proposed method, employing both newly developed models and pre-trained deep networks, along with a comparison of their performance.\nTo measure the performance of the models more accurately and sensitively in the experimental studies, the relevant experiments were repeated three times for all models, and the average of these trials was taken. This approach yielded statistically more robust values. Trials for each model were conducted for 3, 5, and 10 epochs.\nTo evaluate the performance of the proposed models, they were compared with eight pre-trained deep learning models in terms of accuracy and time criteria."}, {"title": "5 Conclusions and Future Work", "content": "The results obtained from the experimental analysis presented in the previous section contain several significant insights. The models proposed in this study were tested on two distinct datasets created using images from individuals of various ages, ethnicities, and genders, under different lighting conditions and backgrounds. Our LightFFDNet v1 model is significantly faster than all other models, while also achieving an acceptable level of accuracy, and, in fact, performing better than some models. In the experiments conducted on the Fake-Vs-Real-Faces (Hard) dataset, it is nearly four times faster than the GoogleNet and MobileNet-V2 models, seven times faster than ResNet-50, ten times faster than ResNet-101, thirteen times faster than DarkNet-53, fourteen times faster than VGG-16, and seventeen times faster than VGG-19. In the experiments on the 140k Real and Fake Faces dataset, it is two times faster than the MobileNet-V2 model, nearly five times faster than ResNet-50, nine times faster than DarkNet-53, ten times faster than ResNet-101 and VGG-16, and eleven times faster than VGG-19. Our LightFFDNet v2 model is faster than LightFFDNet v1 model on the 140k Real and Fake Faces dataset, while there is only a slight speed difference between them on the other dataset.\nIf we also consider computational complexity, both proposed LightFFDNet models are much more efficient and successful than all existing models. The first proposed model consists of 3 layers, while the second model consists of 6 layers, which makes them lightweight CNN models. Among the pretrained deep learning models, the one with the fewest layers is AlexNet, which has 8 layers. Our LightFFDNet v1 model has nearly 3 times fewer layers than the AlexNet model.\nOn the 140k Real and Fake Faces dataset, all models, especially the sequential models, did not perform well. Since the proposed models are also of a sequential structure, they exhibited lower accuracy on this dataset compared to the other dataset.\nThe results obtained from the experiments reached a maximum of 10 epochs; however, studies in the literature typically employ a higher number of epochs. This characteristic, combined with the fewer number of layers, enables our lightweight models to be significantly faster than other models and to reach conclusions in a shorter time. This quality makes our lightweight models superior compared to others for the datasets used.\nIn the future, we are planning to upgrade and optimize our LightFFDNets by incorporating state-of-the-art methods and techniques [2, 5, 13]. Subsequent studies will focus on testing the models on different datasets and developing them into a generalized model. Additionally, there are plans to enhance the models not only for binary classification datasets but also for problems with multiple classes.\nFurthermore, we are also interested in implementing our novel LightFFDNet models for representing Bidirectional Reflectance Distribution Functions (BRDFs) [3, 6, 7, 11, 21, 26, 29, 30, 33, 41\u201344, 51, 54\u201356], Bidirectional Scattering Distribution Functions (BSDFs) [23-25, 34, 59, 60], Bidirectional Surface Scattering Reflectance Distribution Functions (BSSRDFs) [22,27,28,31,32,40, 63] and multi-layered materials [34,38,59] in computer graphics."}]}