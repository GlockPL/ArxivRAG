{"title": "SIFToM: Robust Spoken Instruction Following through Theory of Mind", "authors": ["Lance Ying", "Jason Xinyu Liu", "Shivam Aarya", "Yizirui Fang", "Stefanie Tellex", "Joshua B. Tenenbaum", "Tianmin Shu"], "abstract": "Spoken language instructions are ubiquitous in agent collaboration. However, in human-robot collaboration, recognition accuracy for human speech is often influenced by various speech and environmental factors, such as background noise, the speaker's accents, and mispronunciation. When faced with noisy or unfamiliar auditory inputs, humans use context and prior knowledge to disambiguate the stimulus and take pragmatic actions, a process referred to as top-down processing in cognitive science. We present a cognitively inspired model, Spoken Instruction Following through Theory of Mind (SIFTOM), to enable robots to pragmatically follow human instructions under diverse speech conditions by inferring the human's goal and joint plan as prior for speech perception and understanding. We test SIFTOM in simulated home experiments (VirtualHome 2). Results show that the SIFTOM model outperforms state-of-the-art speech and language models, approaching human-level accuracy on challenging spoken instruction following tasks. We then demonstrate its ability at the task planning level on a mobile manipulator for breakfast preparation tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Verbal communication is a crucial part of human cooperation. From a very young age, humans routinely use spoken language to communicate and coordinate their actions [1]. We often need to request help from and give instructions to others, particularly in cooperative settings [2]. For instance, you may ask your friend, \"Could you pass me the salt and pepper?\" at dinner because the condiments are out of reach. Therefore, it is crucial for assistive robots to understand human users' speech instructions to provide proper assistance.\nThere has been an increasing interest in combining speech recognition, language grounding, and robot planning to enable assistive robots to follow humans' speech instructions [3], [4], [5]. These studies often assume that the robot can decode the language command from speech signals perfectly with automatic speech recognition (ASR). However, in the real world, human speech can be influenced or corrupted by a multitude of factors, such as background noise, the speaker's accents, and mispronunciations [6], rendering them noisy or unintelligible to the recipient. Yet humans can communicate and coordinate with each other in various open-world noisy conditions, such as factories, mining sites, or crowded touristy spots. We can also communicate and cooperate with speakers with different accents despite not being familiar with the accents. This is because in processing noisy perceptual inputs, humans have a generative model of other agents and the environment [7], [8], which allows humans to reason about what instructions are likely conditioned on their goals and plans. Additionally, humans can also integrate cues from different modalities, enabling them to take educated guesses about the true instruction intended by the speaker. This process is referred to as top-down perceptual processing in cognitive neuroscience [9], [10].\nThe challenges that arise from noisy or unclear speech also call for different evaluation metrics for speech recognition in embodied settings. Typically, word error rate (WER) is used to evaluate the accuracy of ASR systems. However, WER is a poor measure of success in embodied cooperative tasks, as a small critical error can change the meaning of the language instruction, whereas skipping a few filler words can be non-detrimental. Imagine the scenario depicted in Figure 1, speech recognition may give the robot the wrong transcription (potato instead of tomato) due to either noise in the speech or mispronunciation by the user. This wrong inference would result in a completely failed assistance, even though there is only a single incorrect word.\nIn this paper, we present a cognitively-inspired model, Spoken Instruction Following through Theory of Mind (SIFTOM). It grounds speech recognition in Theory of Mind reasoning. By reasoning about humans' goals and plans from visual observations, the model forms an informative prior over what instructions the human would give, which, combined with the likelihoods from the ASR model, produces"}, {"title": "II. RELATED WORK", "content": "A. Speech instructions in robotics\nAutomatic Speech Recognition (ASR) has been a major human-robot interface in a wide range of applications, including assistant home robots, autonomous driving, etc. [4]. Most state-of-the-art speech-based robotic systems today follow a sequential processing pipeline, which runs an ASR or ASR + LLM module to convert speech to natural language, which is then parsed into symbolic robot commands, such as in PDDL, for execution [12], [13].\nB. Language grounding in embodied interactions\nThere has been a long history of work attempting to ground unspecified or ambiguous language instructions in embodied environments. Previous work has applied Vision and Large Language Models to map words to real-world objects and subsequent robot actions [12], [14], [15], [16], [17], [18]. Some recent work in cognitive science and AI has applied computational Theory of Mind models to clarify ambiguous human instructions by reasoning about the human speaker's intent and the joint human-robot plan [19]. However, most language instruction grounding works do not take speech input, and few studies have examined how robust these systems are in noisy environments where instructions may not be fully intelligible.\nC. Theory of Mind for Cooperative Robot Planning\nThere has been extensive research on inferring other agents' mental states, such as goals, desires, and beliefs, (e.g., [20], [21], [22], [23], [19], [24]), commonly referred to as Theory of Mind (ToM) reasoning, to better cooperate and coordinate with others in collaborative tasks. Earlier work in human-robot interaction has used model-based approaches such as plan recognition and inverse planning to infer other agents' goals [25], [26], [20]. Recent studies have increasingly used Large Language Models (LLMs) to infer agents' mental states in social interactions.\nPrevious work in cognitive science and psychology has found that Theory of Mind is critical in agents' interactions and cooperation. Computational work in AI and Robotics has also shown that embedding a Theory of Mind module in embodied AI agents often leads to more effective communications and more efficient collaborations between AI and human users [27], [11].\nD. Perceptual Processing\nModern cognitive neuroscience often distinguishes two kinds of perceptual processing: bottom-up and top-down processing [28], [29]. Bottom-up processing starts with the sensory data and interprets the stimuli without prior knowledge or context. On the other hand, top-down processing starts with prior knowledge and forms expectations about the stimuli with a generative forward model. When faced with ambiguous speech, bottom-up processing often cannot decide between multiple likely lexicons due to missing phonetic information. In these cases, humans are more likely to rely on top-down processing for \u201cphonemic restoration\" [30]. In other words, cognitive expectations usually determine what we hear [9]."}, {"title": "III. METHODS", "content": "A. Problem Formulation\nWe formulate the robot assistance problem with unintelligible speech input as a mixed-observability Markov decision process (MOMDP) [31], where the robot assistant needs to infer the overall team goal and the delegated robot subgoal from the speech instruction. This can be formalized by (S, G, Ar, \u03a6, O,Ts, T\u04ab, Z, Rr, y). The overall state has four components: the world state, s \u2208 S, the team goal G\u2208 G, and each of human and robot's subgoals, gh, gr\u2208 G respectively. We assume the team goal is separable so G = {gh, gr}. The world state is observable to both the human and the robot, and the team goal and the robot's assigned goal are observable to the human but unobservable to the robot. Ar is the action space of the robot assistant. \u03a6 is the space of speech instructions. The robot's observation consists of the world state, the human agent's action, and the speech instruction, i.e., O = S \u00d7 Ah \u00d7 \u03a6h. Ts(s,G = (gh,gr), ar, s') = p(s' s, G = (gh, gr), ar) is the transition function for the world state, and TG(G = (gr,gh),s,ar, s', g\u2084) = p(G' = (gr, 9h) G = (gr,gh),s,ar,s') is the transition function for the robot goal. Z(s',G' = (94,9h), ar, 0) = p(o = (s,an)|s', G' = (gr, gh), ar) is the conditional probability function for the observation result.\nThe reward function for the robot assistant is defined as R\u2081(s, art) = Ep(g|Ft) [1(s = gr)] - \u2084(a), where (a) is the cost for robot action a, and 1(\u00b7) checks if the robot subgoal is satisfied in the current world state s. y is the discount factor."}, {"title": "B. Spoken Instruction Following with Theory of Mind (SIFTOM)", "content": "1) Model Overview: Inspired by human perceptual processing, the SIFTOM model follows a two-stream approach (Fig. 2): In the bottom-up stream, the audio input is transcribed to words and converted to a symbolic robot subgoal. In the top-down stream, the model first infers the human goal and plans from visual observations, then uses this context to predict the likely robot subgoals.\nThe SIFTOM algorithm is shown in Algorithm 1. The model first starts with the speech input from the bottom-up stream. the speech is processed by an ASR system and converted to a symbolic robot goal gr with an LLM (llama-3.1 70B) with token probability P(gr Ts). If the output is plausible (The goal is valid and P(gr|Ts) > 0), the model outputs the robot goal. Otherwise, the model proceeds to the slower context-driven top-down stream of processing.\nIn the top-down processing stream, the model performs inverse planning to infer K most likely joint goal with probability P(G\u00b2|an), from which a task planner can derive N most likely subgoals that can be delegated to the robot with probability P(g|G). We use the inverse planning algorithm from [19], [32] for goal inference and task planning. Then, the model performs Bayesian multi-modal cue integration [33] to compute the likelihood of robot subgoal conditioned on both action observations and speech inputs:\n$P(gran, Ts) XP(ah|gr) *P(Ts gr) * P(gr)$\n$= \\sum_{G}P(an|gr, G)P(G)$\n$= \\sum_{G} P(an gr, G)P(G) * P(Ts|gr) * P(gr)$\nSince G = {gh, gr}, we can simplify as\n$P(gr/ahn, Ts) \\propto \\sum_{G}P(an\\|gh)P(G) *P(gr|Ts)$\nwhere gh = G \\gr\nTo evaluate the first term $P(angh,G)$, where an = {a,...a}, we follow existing work on rational planning, which models planning as the process of computing a Boltzmann policy for a goal g:\n$\\pi(a\\|st, 9) = \\frac{expQ(st, at)}{\\sum_{a\\prime\\in Ac} exp Q_g(st, af)}$ (1)\nwhere st is the state. at is the action taken by agent at st, T is a temperature parameter, and Q\u2084(st, at) is the cost of the optimal plan from st to goal g with performing at.\nTo estimate the second conditional probability P(gr|Ts), we query Llama-3.1-70B as a likelihood function, which approximates the conditional probability as the token probability of gr when prompted with T, and few shot prompts E. This gives us a probability distribution over robot goal space with the speech input.\nOnce we obtain the posterior probability P(gran, Ts) conditioned on both the speech and planning context, SIFTOM selects the most likely robot subgoal for execution."}, {"title": "IV. SIMULATED EXPERIMENT", "content": "As there is no noisy spoken instruction dataset for embodied collaboration, we first constructed a novel dataset called UnclearInstruct in a simulated home environment where a human principal agent requests help from an Al agent to complete a household task.\nA. Dataset Construction\nOur dataset UnclearInstruct is situated in a simulator household environment with both natural and synthesized speech instructions.\n1) Tasks and Scenarios: We sampled 66 task scenarios from VirtualHome 2 [11], a multiagent household simulator. We consider 4 categories of household tasks: Set up a table, Put groceries, Prepare food, and Load the Dishwasher. The task specifications are described in Table I."}, {"title": "B. Baselines", "content": "To evaluate the performance, we evaluate the model against human, ASR and multimodal-LLM baselines. We used Llama-3.1-70B for all the models except the two Gemini baselines.\n1) Human baseline: To evaluate the human performance on the dataset, we ran a human experiment with a customized online interface. We recruited 102 participants. Each participant was informed of the full goal space and completed 30 trials. In each trial, the participant was shown a text description of the main agent's actions and an audio file of the speech instruction. The participants assumed the role of the assistant (recipient of the speech instruction) and were asked to give a transcription of the speech and select the items they would pick up to aid the main agent.\n2) Vision-only: Previous work on VirtualHome has mostly focused on vision-only assistance models. We adopt a similar inverse planning and assistance model proposed by [35], which tries to proactively assist humans on household tasks after watching their actions and inferring their goals.\n3) ASR + LLM: The ASR + LLM baselines follow existing pipelines for speech-based human robot interaction, which first use a speech recognition module to transcribe the human speech, then further extract information and control commands from the transcription. We prompted Llama-3.1-70B for translating ASR output to symbolic robot goals. We used 3 kinds of ASR models in this study: Whisper, RobustGER, and ASR + Vision.\n\u2022 Whisper: We used the most powerful Whisper Large V2 model, an state-of-the art ASR model by OpenAI.\n\u2022 RobustGER [36]: The RobustGER model combines ASR and LLM to perform generative error correction (GER), which makes the model more robust to noise.\n\u2022 ASR + Vision (ASR+V): The ASR+V model uses an LLM to integrate Whisper transcript and visual information to output an improved transcript.\n4) Multimodal LLM: We use Gemini-Pro as our M-LLM baseline that translates directly from speech to robot goal with transcript. We tested two variants of prompting. In the speech-only condition (Gemini S), we prompted Gemini to translate directly to tasks without goal space and observations of human actions. In the multimodal condition (Gemini S+V), we prompted the LLM with the same input given to the human participants in the human experiment (goal space + visual observations in text with speech audio).\nC. Performance Metrics\nWe evaluated the model performance with three metrics: accuracy rate, speedup, and word error rare (WER).\na) Accuracy: We measured inference accuracy by the percentage of trials that the robot inferred the correct robot goal from speech instructions.\nb) Speedup: Following past work on human-robot collaboration in VirtualHome [35], we computed the speedup of the baseline models against a single-agent (human alone) baseline. The speedup is computed as Speedup ="}, {"title": "D. Results", "content": "Lsingle/Lteam - 1, where Lsingle is the timesteps it takes\nfor the human agent to complete the task alone without robot\nassistance and Lteam is the timesteps for the human-robot\nteam.\nc) Word Error Rate: We followed existing work on\nASR, which calculated WER as the percentage of errors in\na transcript compared to the total words spoken. A perfect\ntranscription has a WER of 0.\nThe results of the simulated study is shown in Table.\nIV. Overall, despite being a lightweight model that uses\nWhisper with Llama3.1 70B model, SIFTOM significantly\nboost the inference accuracy of other ASR+LLM models\nand outperformed Gemini Pro, a much larger VLLM. The\ndifference between the SIFTOM model and other baselines\nare all statistically significant with p < 0.001.\nThe model performance broken down by speech factors is\nshown in Fig. 4, which shows a similar pattern across all con-\nditions. We also observed that human participants performed\nworse in the accent condition than in mispronounced and\nnoise conditions, whereas models generally are less robust\nin both noise or accent conditions.\nWe then compared the word error rate of the transcripts\nused in each model. Overall we failed to observe any\nrobust correlation between WER and goal inference accuracy\n(r = -0.12, p = 0.79). Notably, the Gemini Speech model achieved the lowest word error rate, but its goal inference and\nspeedup performance were similar to the unimodal Whisper\nbaseline. This supports our claim that WER may not be a\ngood performance metric for evaluating spoken instruction\nfollowing systems in embodied collaborative settings.\nError Analysis: We also performed an error analysis\nand we found that the mistakes made by humans or SIFTOM\nwere less costly than those by other models. Upon analyzing\nthe incorrect guesses, we found that 100% of incorrect hu-\nman guesses were in fact valid and helpful actions, meaning\nthat the items inferred were part of the team goal, but often\nthe item count was incorrect. This ratio dropped to 69.7% for\nSIFTOM and less than 40% for all other models. As a result,\nonly 4.15% of all trials in the human baseline had a negative\nspeedup, meaning that the robot actions severely hindered\nthe team performance, compared to 10.2% in SIFTOM and\n>18% in all other models. This shows that similar to humans,\nby grounding speech instructions in the inferred joint goal,\nSIFTOM is more likely to perform useful actions even when\nit doesn't have the exact command inferred."}, {"title": "V. REAL-WORLD EVALUATION", "content": "To evaluate whether SIFTOM can work on real world\nembodied tasks, we conducted an experiment re a human\nparticipant instructed a mobile manipulator, Spot [37], to\nhelp prepare breakfast."}, {"title": "A. Dataset", "content": "We recorded 36 videos of the human acting and instructing\nthe robot and added various household background noises\n(Room setup as Fig. 5). In each video, the human is working\non one of six breakfast items (see II for task specification).\nSimilar to the simulated experiment, the human agent first\nstarted working on the task and then gave the robot an\ninstruction for picking and placing food items on the table."}, {"title": "B. Baselines", "content": "We used the same baselines as the simulated experiments.\nThe speech input was extracted from videos and segmented.\nFor the multimodal ASR + LLM baselines, we used Gemini-\npro to extract human actions and detect visual objects. The\nGemini (S+V) model was prompted to directly output a\nsymbolic robot goal from video input.\nFor the human baseline, we recruited four participants to\nwatch the videos and respond with the best assistance option\nby the Spot robot. Each video was annotated by two human\nraters."}, {"title": "C. Results", "content": "Similar to the simulated experiments, we found that the\nSIFTOM model outperformed all baselines on the accuracy\nof robot task inference, and adding the visual context sig-\nnificantly improved the performance over unimodal speech\nbaselines. On the other hand, all human participants found\nthe task straightforward and were able to infer the true robot\ngoal with a 100% accuracy, whereas the SIFTOM model\nachieved an accuracy of 83.3% and the multimodal Gemini\nmodel had an accuracy of 63.8%."}, {"title": "VI. ROBOT DEMONSTRATION", "content": "We show a qualitative example of the SIFTOM model\nimplemented in an embodied setting as a robot demo in\nFig. 5. In this example, the robot observed that the human\nprincipal picked up a bowl and milk and placed them on\nthe table. Then the human principal asked the robot to get\nthe cereal. However, due to noise in the background, the\nASR module was not able to clearly decode the speech and\nproduced an incorrect transcription \"Can you pass the CD box?\" As there was no CD in sight, the SIFTOM model\nresorted to a context prior, inferring that the human was\nlikely making a bowl of cereal and possibly coffee. Since\nSIFTOM found cereal most likely and the Llama model also\nevaluates CD box to be phonetically similar to cereal box\nthan coffee, SIFTOM infers that the human command to be to\nget the cereal box and allows the robot to successfully assists\nthe human to complete the goal. This contrasts with other\nASR + LLM models, which outputted \"Get(CD)\" despite\nbeing explicitly prompted with a list of possible goal objects."}, {"title": "VII. DISCUSSION AND CONCLUSION", "content": "In this paper, we propose a cognitively inspired multi-\nmodal model SIFTOM that grounds and disambiguates noisy\nor unintelligible human spoken instruction through Theory\nof Mind and joint planning. Through both simulated and\nembodied experiments, we demonstrate that SIFTOM outper-\nformed other baselines in inferring the robot task delegated\nby the human agent from both visual and ambiguous speech\ninputs. Additionally, our error analysis showed that mistakes\nmade by SIFTOM were less costly, sometimes even moder-\nately helpful, whereas other baselines took random guesses.\nWith its robust and pragmatic spoken instruction following\ncapabilities, SIFTOM will allow robots to more effectively\nand robustly collaborate with humans in open-world settings\nunder various speech and noise conditions.\nOur work is not without limitations. First, our experi-\nmental results show that humans are still significantly better\nthan SIFTOM models in processing and understanding speech\ncommands under various speech conditions. One reason is\nthat the ASR model used in the SIFTOM model does not\nalways produce useful transcriptions when the speech is\nnoisy. Future work can integrate the context prior into the\nASR model to improve the quality of the transcription and\ncorrect errors on the sentence-level.\nSecond, SIFTOM requires a full specification of the goal\nspace a priori for goal inference and planning. This limits\nthe scalability of this approach to more complex domains.\nFuture work can leverage recent advances in Multimodal-\nLLMs to scale up the SIFTOM model to more open-ended\nhuman-robot collaboration settings."}]}