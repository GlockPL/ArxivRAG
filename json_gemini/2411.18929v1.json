{"title": "VIPaint: Image Inpainting with Pre-Trained Diffusion Models via Variational Inference", "authors": ["Sakshi Agarwal", "Gabriel Hope", "Erik B. Sudderth"], "abstract": "Diffusion probabilistic models learn to remove noise that is artificially added to the data during training. Novel data, like images, may then be generated from Gaussian noise through a sequence of denoising operations. While this Markov process implicitly defines a joint distribution over noise-free data, it is not simple to condition the generative process on masked or partial images. A number of heuristic sampling procedures have been proposed for solving inverse problems with diffusion priors, but these approaches do not directly approximate the true conditional distribution imposed by inference queries, and are often ineffective for large masked regions. Moreover, many of these baselines cannot be applied to latent diffusion models which use image encodings for efficiency. We instead develop a hierarchical variational inference algorithm that analytically marginalizes missing features, and uses a rigorous variational bound to optimize a non-Gaussian Markov approximation of the true diffusion posterior. Through extensive experiments with both pixel-based and latent diffusion models of images, we show that our VIPaint method significantly outperforms previous approaches in both the plausibility and diversity of imputations, and is easily generalized to other inverse problems like deblurring and superresolution.", "sections": [{"title": "1 Introduction", "content": "Diffusion models [Ho et al., 2020a, Song et al., 2021a, Nichol and Dhariwal, 2021, Song and Ermon, 2019] and hierarchical variational autoencoders (VAEs) [Child, 2021, Vahdat and Kautz, 2020, S\u00f8nderby et al., 2016] are generative models in which a sequence of latent variables encode a rich data representation. For diffusion models, this latent structure is defined by a diffusion process that corrupts data over time\u201d via additive Gaussian noise. While each step of hierarchical VAE training requires end-to-end inference of all latent variables, diffusion models estimate stochastic gradients by sampling a few timesteps, and learning to incrementally denoise corrupted data. Given a learned denoising network, synthetic data is generated by sequentially refining Gaussian noise for hundreds or thousands of time steps, producing deep generative models that have advanced the state-of-the-art in natural image generation [Dhariwal and Nichol, 2021, Kingma et al., 2021a, Karras et al., 2022].\nDiffusion models for high-dimensional data like images are computationally intensive. Efficiency may be improved by leveraging an autoencoder [Kingma and Welling, 2019, Rombach et al., 2022a, Vahdat et al., 2021] to map data to a lower-dimensional encoding, and then training a diffusion model\nMotivated by the foundational information captured by diffusion models of images, numerous algorithms haveincorporated a pre-trained diffusion model as a prior for image editing [Meng et al., 2021], inpainting [Song et al., 2021a, Wang et al., 2023a, Kawar et al., 2022, Chung et al., 2022a, Lugmayr et al., 2022, Cardoso et al., 2024, Feng et al., 2023, Trippe et al., 2023, Dou and Song, 2024], or other inverse problems [Kadkhodaie and Simoncelli, 2021, Song et al., 2023, Graikos et al., 2022, Mardani et al., 2023, Chung et al., 2023].Many of these prior methods are specialized to inpainting with pixel-based diffusion models, where every data dimension is either perfectly observed or completely missing, and are not easily adapted to state-of-the-art LDMs.\nMost algorithms for inpainting with diffusion models employ an iterative refinement procedure, like that used to generate unconditional samples, and then guide their predictions towards the partially observed image via various approximations and heuristics. But by sequentially annealing from independent Gaussian noise to noise-free images, these approaches produce myopic samples that do not adequately incorporate information from observed pixels, and fail to correct errors introduced in earlier stages of the \u201creverse-time\u201d diffusion. More recent work has extended these approaches to enable image editing [Avrahami et al., 2022] or inpainting [Rout et al., 2023, Corneanu et al., 2024, Chung et al., 2023, Song et al., 2024] with LDMs, but continues to suffer similar inaccuracies.\nIn this work, we present VIPaint, a novel application of variational inference (VI) [Wainwright and Jordan, 2008, Blei et al., 2017] that efficiently optimizes a hierarchical, Markovian, non-Gaussian approximation to the true (L)DM posterior. VI has achieved excellent image restoration results with a wide range of priors, including mixtures [?Ji et al., 2017] and hierarchical VAEs [Agarwal et al., 2023], but there is little work exploring its integration with state-of-the-art LDMs. While Red-Diff [Mardani et al., 2023] applies VI to approximate the posterior of pixel-based DMs, its local approximation of the noise-free image posterior is difficult to optimize, requiring annealing heuristics that we demonstrate are sensitive to local optima. Our VIPaint method instead defines a hierarchical posterior that strategically accounts for a subset of noise levels, enabling the inference of both high-level semantics and low-level details from observed pixels simultaneously (see Fig. 1). We efficiently infer variational parameters via non-amortized optimization for each inpainting query, avoiding the need to collect a training set of corrupted images [Liu et al., 2024, Corneanu et al., 2024], expensively fine-tune generative models [Avrahami et al., 2022] or variational posteriors [Feng et al., 2023] for each query, or retrain large-scale conditional diffusion models [Rombach et al., 2022a, Saharia et al., 2022, Nichol et al., 2022, Chung et al., 2022b].\nWe begin by reviewing properties of (latent) diffusion models in Sec. 2, and prior work on inferring images via pre-trained diffusion models in Sec. 3. Sec. 4 then develops the VIPaint algorithm, which first fits a hierarchical posterior that best aligns with the observations, and then samples from this approximate posterior to produce diverse reconstruction hypotheses. Results in Sec. 5 on inpainting, and the Appendix on other inverse problems, then show substantial qualitative and quantitative improvements in capturing multimodal uncertainty for both pixel-based and latent DMs."}, {"title": "2 Background: Diffusion Models", "content": "The diffusion process begins with clean data x, and defines a sequence of increasingly noisy versions of x, which we call the latent variables $z_t$, where t runs from t = 0 (low noise) to t = T (substantial noise). The distribution of latent variable $z_t$ conditioned on \u00e6, for any integer time t \u2208 [0, T], is\n$q(z_t | x) = N(z_t | a_tx, \\sigma_t^2I)$,\nwhere $a_t$ and $\u03c3_t$ are strictly positive scalar functions of t. This noise implicitly defines a Markov chain for which the conditional $q(z_t | z_{t\u22121})$ is also Gaussian. Also, $q(z_{t-1} | z_t, x)$ is Gaussian (see Appendix A.1) with mean equal to a linear function of the input data x and the latent sample $z_t$.\nThe signal-to-noise ratio [Kingma et al., 2021b] induced by this diffusion process at time t equals $SNR(t) = \\alpha_t^2/\\sigma_t^2$. The SNR monotonically decrease with time, so that $SNR(t) < SNR(s)$ for t > s. Diffusion model performance is very sensitive to the rate at which SNR decays with time, or equivalently the distribution with which times are sampled during training [Nichol and Dhariwal, 2021, Karras et al., 2022]. This DM specification includes variance-preserving diffusions [Ho et al., 2020b, Sohl-Dickstein et al., 2015] as a special case, where $a_t = \\sqrt{1 \u2013 \\sigma_t^2}$. Another special case, variance-exploding diffusions [Song and Ermon, 2019, Song et al., 2021a], takes $a_t = 1$.\nImage Generation. The generative model reverses the diffusion process outlined in Eq. (1), resulting in a hierarchical generative model that samples a sequence of latent variables $z_t$ before sampling x. Generation progresses backward in time from t = T to t = 0 via a finite temporal discretization into T\u2248 1000 steps, either uniformly spaced as in discrete diffusion models [Ho et al., 2020b], or via a possibly non-uniform discretization [Karras et al., 2022] of an underlying continuous-time stochastic differential equation [Song et al., 2021a]. Denoting t - 1 as the timestep preceding t, for 0 < t < T, the hierarchical generative model for data x is expressed as follows:\n$p(x) = \\int \\prod_{t=1}^T \\left[ \\int p(z_{t-1}|z_t) dz_t \\right] p(x|z_0) p(z_T) dz_T$.\nThe marginal distribution of zT is typically a spherical Gaussian $p(z_T) = N(z_T | 0, \u03c3_T^2I)$. Pixel-based diffusion models take $p(x | z_0)$ to be a simple factorized likelihood for each pixel in x, while LDMs define $p(x | z_0)$ using a decoder neural network. The conditional latent distribution maintains the same form as the forward conditional distributions $q(z_{t\u22121} | z_t, x)$, but with the original data x substituted by the output of a parameterized denoising model $z_\u03b8$ as\n$p_\u03b8(z_{t-1} | z_t) = q(z_{t-1} | z_t, z_0 = z_\u03b8(z_t,t))$, where $z_\u03b8(z_t,t) = \\frac{z_t \u2013 \u03c3_t\\epsilon_\u03b8(z_t, t)}{\u03b1_t}$.\nThis denoising model $\u03f5_\u03b8(z_t, t)$ typically uses variants of the UNet architecture [Ronneberger et al., 2015] to predict the noise-free latent $z_0$ from its noisy counterpart $z_t$.\nThe Gaussian diffusion implies that $p_\u03b8(z_{t\u22121} | z_t) = N(z_{t-1} | C_1(t)z_t + C_2(t)z_0(z_t, t), \\tilde{\u03c3}_{t-1}^2I)$, so the mean is a linear combination of the latent $z_t$ and the prediction $z_0$, with constants determined from the diffusion hyperparameters as detailed in Appendix A.1. Our VIPaint approach flexibly accommodates multiple parameterizations of the denoising model, including the EDM model's direct prediction of $z_0$ for higher noise levels [Karras et al., 2022].\nTraining Objective. The variational lower bound (VLB) of the marginal likelihood is given by\n$-\\log p(x) \\leq -E_{q(z_0|x)}[\\log p_\u03b8(x|z_0)] + D[q(z_T|z_0)||p(z_T)] + L_{(0,T)}(z_0)$,\nwhere D is the Kullback-Leibler (KL) divergence. The reconstruction loss, usually L1, can be estimated stochastically and differentiably using standard reparametrization techniques [Kingma and Welling, 2019]. The prior loss is a constant because $p(z_T)$ is a Gaussian with fixed parameters. Ho et al. [2020a] express the diffusion loss for finite time T as follows:\n$L_{(0,T)}(z_0) = \\sum_{t=1}^T E_{q(z_t|z_0)} D[q(z_{t-1}|z_t, z_0)||p_\u03b8(z_{t\u22121}|z_t)]$.\nTo boost training efficiency, instead of summing the loss over all T times, timesteps are sampled from a uniform distribution $t \\sim U{1, T}$ to yield an unbiased approximation. Most prior work also"}, {"title": "3 Background: Inference using Diffusion Models", "content": "3.1 General Inverse Problems\nIn many real-life scenarios, we encounter partial observations y derived from an underlying x. Typically, these observations are modeled as y = f(x) + v, where f represents a known linear degradation model and v is Gaussian noise with $v \\sim N(0, \u03c3^2I)$. For instance, in an image inpainting task, y might represent a masked imaged y = x \u2299 m, where m is a binary mask indicating missing pixels.\nIn cases where the degradation of x is significant, exactly recovering x from y is challenging, because many x could produce the same observation y. To express the resulting posterior p(x | y) given a DM prior, we can adapt the Markov generative process in Eq. (2) as follows:\n$P_\u03b8(x|y) = \\int P_\u03b8(x | z_0, y) \\prod_{t=1}^T \\left[ \\int p_\u03b8(z_{t-1} | z_t, y) dz_t \\right] P_\u03b8(z_T | y) dz_T$.\nExactly evaluating this predictive distribution is infeasible due to the non-linear noise prediction (and decoder) network, and the intractable posteriors of latent codes $p(z_{t\u22121} | z_t, y)$ for all t.\nBlended methods [Song et al., 2022, Wang et al., 2023b] define a procedural, heuristic approximation to the posterior and is tailored for image inpainting. They first generate unconditional samples $z_{t-1}$ from the prior using the learned noise prediction network, and then incorporate y by replacing the corresponding dimensions with the observed measurements. RePaint [Lugmayr et al., 2022] attempts to reduce visual inconsistencies caused by blending via a resampling strategy. A \u201ctime travel\u201d operation is introduced, where images from the current time step $z_{t-1}$ are first blended with the noisy version of the observed image $y_{t\u22121}$, and then used to generate images in the $(t \u2212 1) + r, (r > 1)$ time step by applying a one-step forward process and following the Blended denoising process.\nSampling Methods. Motivated by the goal of addressing more general inverse problems, Diffusion Posterior Sampling (DPS) [Chung et al., 2023] uses Bayes' Rule to sample from $p_\u03b8(z_{t-1}|z_t, y) \u221d P_\u03b8(z_{t-1}|z_t)P_\u03b8(y|z_{t-1})$. Instead of directly blending or replacing images with noisy versions of the observation, DPS uses the gradient of the likelihood $log p_\u03b8(y|z_t)$ to guide the generative process at every denoising step t. Since computing $\u2207_{z_t} log p_\u03b8(y|z_{t\u22121})$ is intractable due to the integral over all possible configurations of $z_{t'}$ for $t' < t \u2212 1$, DPS approximates $p(y|z_{t\u22121})$ using a one-step denoised prediction $z_\u03b8$ using Eq. (3). The likelihood $p(y|x) = N(f(x), \u03c3^2)$ can then be evaluated using these approximate predictions. To obtain the gradient of the likelihood term, DPS require backpropagating gradients through the denoising network used to predict $z_\u03b8$.\nSpecializing to image inpainting, CoPaint [Zhang et al., 2023] augments the likelihood with another regularization term to generate samples $z_{t\u22121}$ that prevent taking large update steps away from the previous sample $z_t$, in an attempt to produce more coherent images. Further, it proposes CoPaint-TT, which additionally uses the time-travel trick to reduce discontinuities in sampled images.\nOriginally designed for pixel-space diffusion models, it is difficult to adopt these works directly to latent diffusion models. Posterior Sampling with Latent Diffusion (PSLD) [Rout et al., 2023] first"}, {"title": "4 VIPaint: Variational Inference of Diffusions for Inpainting", "content": "Given a pre-trained diffusion model, VIPaint constructs a joint posterior that is Markovian over a subset of latents, $z_t \u2208 [T_s, T_e]$, where $0 < T_s < T_e < T$. This introduces a hierarchical structure to the inference process. An overview is provided in Fig. 3. The posterior also incorporates additional parameters, \u03bb, and semantically aligns with the test observation during optimization. Once the model is fit, the second stage involves sampling from the posterior, utilizing DPS gradient updates in the low-range latent space ([0, Ts]). In this section, we provide a detailed explanation of VIPaint's posterior formulation, along with its two-step optimization and sampling strategies.\n4.1 Defining the Variational Posterior\nVIPaint selects K intermediate timesteps in the mid-ranged latent space [$T_s, T_e$] to define its hierarchical variational posterior. This technique offers several advantages over RedDiff, as the posterior: 1) infers the medium-to-high global image semantics in the latent space, induced by the corrupted image y; 2) accounts for uncertainty in the missing regions; and 3) strategically avoids the training instabilities observed in the low-ranged latent space [0, Ts) Yang et al. [2024] and is thus easily extended to latent diffusion models. Through experimentation, we select intermediate timesteps that approximately yield a signal-to-noise ratio $(\u03b1^2_t/\u03c3^2_t)$ in the range [0.2, 0.5] across the different (latent) diffusion models used.\nVIPaint approximates the posterior conditional probability $p_\u03b8(x | y)$ in Eq. (7) as follows:\n$q(x) = q(x|z_{Ts}) \\left[ \\prod_{i=1}^{K-1} q_\u03bb(z_{s(i)} | z_{s(i+1)}) \\right] q_\u03bb(z_{T_e}),$\nwhere K \u2265 2, and timesteps ($T_s, T_e$) define the boundaries of our variational posterior along the diffusion timesteps. For the highest timestep $T_e$, we let our posterior $q_\u03bb(z_{T_e})$ be a simple Gaussian"}, {"title": "4.2 Phase 1: Optimization", "content": "To fit our hierarchical posterior, we optimize the variational lower bound (VLB) of the marginal likelihood of the observation y. The derivation is provided in Appendix B, and the simplified three-term objective is expressed as follows:\n$L(\u03bb) = -E_q[log p_\u03b8(y|z_{T_s})] + \u03b2 \\sum_{i=1}^{K-1} D[q_\u03bb(z_{s(i)}|z_{s(i+1)}) || P_\u03b8(z_{s(i)} | z_{s(i+1)})] + L_{(T_e,T)}(z_{T_e})$.\nVIPaint seeks latent-posterior distributions that assign high likelihood to the observed features y (by minimizing the reconstruction loss), while simultaneously aligning with the medium-to-high noise levels encoding image semantics (hierarchical and diffusion losses) via weight $\u03b2 > 1$ [Higgins et al., 2017, Agarwal et al., 2023]. We approximate L(A) with M Monte Carlo samples from the hierarchical posterior distribution in Eq.12 and use automatic differentiation to compute gradients with respect to A. We follow ancestral sampling to draw $z_{T_e}^{(m)} \\sim q_\u03bb(z_{T_e}), \\{z_{s(i-1)}^{(m)} \\sim q_\u03bb(z_{s(i-1)} | z_{s(i)}) \\}_{i=1}^K$ and evaluate L(X) as:\n$\\frac{1}{M} \\sum_{m=1}^M \\left[-log p_\u03b8(y|z_{T_s}^{(m)}) +\u03b2 \\left( \\sum_{i=1}^{K-1} \\left[ D[q_\u03bb(z_{s(i)}|z_{s(i+1)}) || P_\u03b8(z_{s(i)} | z_{s(i+1)})] \\right] + L_{(T_e,T)}(z_{T_e}^{(m)}) \\right)\\right]$"}, {"title": "4.3 Phase 2: Sampling", "content": "After optimization, the hierarchical posterior $\u220f_{i=1}^K q_\u03bb(z_{s(i-1)}|z_{s(i)})q_\u03bb(z_{T_e})$ is now semantically aligned with the observation. We employ ancestral sampling on our K level hierarchical posterior starting from $T_e$ to $T_s$, yielding samples $z_{T_e} \\sim \\prod_{i=1}^{K-1} q_\u03bb(z_{s(i-1)} | z_{s(i)}) q_\u03bb(z_{T_e})$. This posterior sampling step gradually adds more semantic details in samples as shown in Fig. 3. Further, VIPaint refines $z_{T_s}$ using the prior denoising model at every step $t < T_s$. Similar to DPS Chung et al. [2023], we update the samples using gradient of the likelihood $log p_\u03b8(y | z_t), t < T_s$. This ensures fine-grained consistency to our final inpaintings.\nTime Complexity. The time taken by VIPaint scales primarily with the number of denoising network calls. Each optimization step for a K-step posterior (K \u226a T) involves O(K) calls to sample $z_{T_e} \\sim q_\u03bb(z)$, and one to compute the diffusion prior loss, resulting in O(K) function calls per step. Thus, for I optimization steps, the overall complexity is O(KI). For example, when VIPaint-2 is optimized over 50 iterations, it requires only 50 * (2 + 1) = 150 denoising network calls to infer global image semantics. Progress in fitting VIPaint's posterior is shown in Fig. 4. Sampling from this posterior requires iterative refinement with the denoising diffusion network, for an additional $T_s$ calls per sample."}, {"title": "5 Experiments & Results", "content": "5.1 Experimental Setup\nWe conduct experiments across 3 image datasets: LSUN-Church [Yu et al., 2015], ImageNet-64 and ImageNet-256 [Deng et al., 2009]. For ImageNet-64, we use the class-conditioned pixel-space \"EDM\" diffusion model [Karras et al., 2022] with the pre-trained score network provided by the authors. For LSUN-Churches256 and ImageNet256 we use the pre-trained latent diffusion models"}]}