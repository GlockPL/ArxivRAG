[{"title": "VIPaint: Image Inpainting with Pre-Trained Diffusion Models via Variational Inference", "authors": ["Sakshi Agarwal", "Gabriel Hope", "Erik B. Sudderth"], "abstract": "Diffusion probabilistic models learn to remove noise that is artificially added to\nthe data during training. Novel data, like images, may then be generated from\nGaussian noise through a sequence of denoising operations. While this Markov\nprocess implicitly defines a joint distribution over noise-free data, it is not simple\nto condition the generative process on masked or partial images. A number of\nheuristic sampling procedures have been proposed for solving inverse problems\nwith diffusion priors, but these approaches do not directly approximate the true\nconditional distribution imposed by inference queries, and are often ineffective for\nlarge masked regions. Moreover, many of these baselines cannot be applied to latent\ndiffusion models which use image encodings for efficiency. We instead develop a\nhierarchical variational inference algorithm that analytically marginalizes missing\nfeatures, and uses a rigorous variational bound to optimize a non-Gaussian Markov\napproximation of the true diffusion posterior. Through extensive experiments with\nboth pixel-based and latent diffusion models of images, we show that our VIPaint\nmethod significantly outperforms previous approaches in both the plausibility and\ndiversity of imputations, and is easily generalized to other inverse problems like\ndeblurring and superresolution.", "sections": [{"title": "1 Introduction", "content": "Diffusion models [Ho et al., 2020a, Song et al., 2021a, Nichol and Dhariwal, 2021, Song and Ermon,\n2019] and hierarchical variational autoencoders (VAEs) [Child, 2021, Vahdat and Kautz, 2020,\nS\u00f8nderby et al., 2016] are generative models in which a sequence of latent variables encode a rich\ndata representation. For diffusion models, this latent structure is defined by a diffusion process that\ncorrupts data over time\u201d via additive Gaussian noise. While each step of hierarchical VAE training\nrequires end-to-end inference of all latent variables, diffusion models estimate stochastic gradients\nby sampling a few timesteps, and learning to incrementally denoise corrupted data. Given a learned\ndenoising network, synthetic data is generated by sequentially refining Gaussian noise for hundreds\nor thousands of time steps, producing deep generative models that have advanced the state-of-the-art\nin natural image generation [Dhariwal and Nichol, 2021, Kingma et al., 2021a, Karras et al., 2022].\nDiffusion models for high-dimensional data like images are computationally intensive. Efficiency\nmay be improved by leveraging an autoencoder [Kingma and Welling, 2019, Rombach et al., 2022a,\nVahdat et al., 2021] to map data to a lower-dimensional encoding, and then training a diffusion model"}, {"title": "2 Background: Diffusion Models", "content": "The diffusion process begins with clean data x, and defines a sequence of increasingly noisy versions\nof x, which we call the latent variables zt, where t runs from t = 0 (low noise) to t = T (substantial\nnoise). The distribution of latent variable zt conditioned on \u00e6, for any integer time t \u2208 [0, T], is\n$q(z_t | x) = \\mathcal{N}(z_t | a_t x, \\sigma_t^2 I),$\nwhere at and \u03c3\u03c4 are strictly positive scalar functions of t. This noise implicitly defines a Markov\nchain for which the conditional q(zt | Zt\u22121) is also Gaussian. Also, q(Zt-1 | Zt, x) is Gaussian (see\nAppendix A.1) with mean equal to a linear function of the input data x and the latent sample zt.\nThe signal-to-noise ratio [Kingma et al., 2021b] induced by this diffusion process at time t equals\nSNR(t) = \u03b1/\u03c37. The SNR monotonically decrease with time, so that SNR(t) < SNR(s) for\nt > s. Diffusion model performance is very sensitive to the rate at which SNR decays with time, or\nequivalently the distribution with which times are sampled during training [Nichol and Dhariwal,\n2021, Karras et al., 2022]. This DM specification includes variance-preserving diffusions [Ho et al.,\n2020b, Sohl-Dickstein et al., 2015] as a special case, where at = \u221a1 \u2013 07. Another special case,\nvariance-exploding diffusions [Song and Ermon, 2019, Song et al., 2021a], takes at = 1.\nImage Generation. The generative model reverses the diffusion process outlined in Eq. (1), result-\ning in a hierarchical generative model that samples a sequence of latent variables zt before sampling\nx. Generation progresses backward in time from t = T to t = 0 via a finite temporal discretization\ninto T\u2248 1000 steps, either uniformly spaced as in discrete diffusion models [Ho et al., 2020b], or\nvia a possibly non-uniform discretization [Karras et al., 2022] of an underlying continuous-time\nstochastic differential equation [Song et al., 2021a]. Denoting t - 1 as the timestep preceding t, for\n0 < t < T, the hierarchical generative model for data x is expressed as follows:\n$p(x) = \\int p(z_T | x) p(x | z_0) \\prod_{t=1}^T p(z_{t-1} | z_t) dz.$\nThe marginal distribution of z\u012b is typically a spherical Gaussian p(zt) = N(z\u03c4 | 0,01). Pixel-\nbased diffusion models take p(x | zo) to be a simple factorized likelihood for each pixel in x, while\nLDMs define p(x | zo) using a decoder neural network. The conditional latent distribution maintains\nthe same form as the forward conditional distributions q(zt\u22121 | zt, x), but with the original data x\nsubstituted by the output of a parameterized denoising model zo as\n$p_\\theta(z_{t-1} | z_t) = q(z_{t-1} | z_t, z_0 = z_0(z_t, t)), \\text{where} \\qquad z_0(z_t, t) = \\frac{z_t - \\sigma_t \\epsilon_\\theta(z_t, t)}{a_t}.$\nThis denoising model \u0109e (zt, t) typically uses variants of the UNet architecture [Ronneberger et al.,\n2015] to predict the noise-free latent 20 from its noisy counterpart 2t.\nThe Gaussian diffusion implies that po(zt\u22121 | 2t) = N(zt\u22121 | C1(t)zt + C2(t)20(zt, t), \u00f5\u00ee\u22121I), so\nthe mean is a linear combination of the latent zt and the prediction 20, with constants determined\nfrom the diffusion hyperparameters as detailed in Appendix A.1. Our VIPaint approach flexibly\naccommodates multiple parameterizations of the denoising model, including the EDM model's direct\nprediction of 20 for higher noise levels [Karras et al., 2022].\nTraining Objective. The variational lower bound (VLB) of the marginal likelihood is given by\n$- \\text{log } p(x) \\leq -E_{q(z_0|x)} [\\text{log } p_\\theta (x|z_0)] + D[q(z_T|x_0)||p(z_T)] + L_{(0,T)} (z_0),$\nwhere D is the Kullback-Leibler (KL) divergence. The reconstruction loss, usually L1, can be\nestimated stochastically and differentiably using standard reparametrization techniques [Kingma and\nWelling, 2019]. The prior loss is a constant because p(zr) is a Gaussian with fixed parameters. Ho\net al. [2020a] express the diffusion loss for finite time T as follows:\n$L_{(0,T)} (z_0) = \\sum_{t=1}^T E_{q(z_t|z_0)} D[q(z_{t-1}|z_t, z_0)||p_\\theta(z_{t-1}|z_t)].$\nTo boost training efficiency, instead of summing the loss over all T times, timesteps are sampled\nfrom a uniform distribution t ~ U{1, T} to yield an unbiased approximation. Most prior work also"}, {"title": "Latent Diffusion Models", "content": "To encourage resource-efficient diffusion models, Rombach et al.\n[2022a], Vahdat et al. [2021] utilize an encoder q$(20|x) to map high-dimensional data RD into a\nlower-dimension space Rd (d < D), and a decoder py(x|zo) to (approximately) invert this mapping.\nTogether with an L1 reconstruction loss, the training loss for the autoencoder employs a combination\nof the perceptual loss [Zhang et al., 2018] and a patch-based adversarial objective [Rombach et al.,\n2022b] to encourage realism and reduce blurriness. Given this autoencoder, one can train a diffusion\nmodel in the space of low-dimensional encodings. The diffusion process is the same as defined in\nEq. (1), but now corrupts 20 ~ q$(20 | x) samples in the lower-dimensional space. Generation uses\nthe reverse diffusion process to sample from po(20) via the time-dependent noise prediction function\n\u0109o (zt, t), and the decoder py(x | zo) to map the synthesized encodings zo to data space."}, {"title": "3 Background: Inference using Diffusion Models", "content": "General Inverse Problems\nIn many real-life scenarios, we encounter partial observations y derived from an underlying x.\nTypically, these observations are modeled as y = f(x) + v, where f represents a known linear\ndegradation model and v is Gaussian noise with v ~ N(0, 2). For instance, in an image inpainting\ntask, y might represent a masked imaged y = x \u2299 m, where m is a binary mask indicating missing\npixels.\nIn cases where the degradation of x is significant, exactly recovering x from y is challenging, because\nmany x could produce the same observation y. To express the resulting posterior p(x | y) given a\nDM prior, we can adapt the Markov generative process in Eq. (2) as follows:\n$P_\\theta(x|y) = \\int P_\\theta (z_T | y) P_\\theta(x | z_0, y) \\prod_{t=1}^T P_\\theta(z_{t-1} | z_t,y) dz.$\nExactly evaluating this predictive distribution is infeasible due to the non-linear noise prediction (and\ndecoder) network, and the intractable posteriors of latent codes p(zt\u22121 | zt, y) for all t.\nBlended methods [Song et al., 2022, Wang et al., 2023b] define a procedural, heuristic approximation\nto the posterior and is tailored for image inpainting. They first generate unconditional samples\nZt-1 from the prior using the learned noise prediction network, and then incorporate y by replacing\nthe corresponding dimensions with the observed measurements. RePaint [Lugmayr et al., 2022]\nattempts to reduce visual inconsistencies caused by blending via a resampling strategy. A \u201ctime travel\u201d\noperation is introduced, where images from the current time step zt-1 are first blended with the noisy\nversion of the observed image Yt\u22121, and then used to generate images in the (t \u2212 1) + r, (r > 1) time\nstep by applying a one-step forward process and following the Blended denoising process.\nSampling Methods. Motivated by the goal of addressing more general inverse problems, Diffusion\nPosterior Sampling (DPS) [Chung et al., 2023] uses Bayes' Rule to sample from po(Zt-1|zt, y) x\nPo(Zt-1|zt)Po(Y|zt\u22121). Instead of directly blending or replacing images with noisy versions of the\nobservation, DPS uses the gradient of the likelihood log pe (y|zt) to guide the generative process at\nevery denoising step t. Since computing \u2207z\u0142 log p(y|zt\u22121) is intractable due to the integral over all\npossible configurations of zt' for t' < t \u2212 1, DPS approximates p(y|zt\u22121) using a one-step denoised\nprediction 2 using Eq. (3). The likelihood p(y|x) = N(f(x), \u03c32) can then be evaluated using these\napproximate predictions. To obtain the gradient of the likelihood term, DPS require backpropagating\ngradients through the denoising network used to predict 2.\nSpecializing to image inpainting, CoPaint [Zhang et al., 2023] augments the likelihood with another\nregularization term to generate samples 2t\u22121 that prevent taking large update steps away from the\nprevious sample zt, in an attempt to produce more coherent images. Further, it proposes CoPaint-TT,\nwhich additionally uses the time-travel trick to reduce discontinuities in sampled images.\nOriginally designed for pixel-space diffusion models, it is difficult to adopt these works directly to\nlatent diffusion models. Posterior Sampling with Latent Diffusion (PSLD) [Rout et al., 2023] first"}, {"title": "3.2 Red-Diff: Variational Inference via Feature Posteriors", "content": "RedDiff [Mardani et al., 2023] approximates the true complex posterior p(x | y) (Eq. 7) by a simple\nGaussian distribution qx(x) = N(\u03bc, \u03c3\u00b2), where \u03bb = {\u03bc, \u03c3} represents the variational parameters.\nMinimizing the KL divergence D(qx(x)||p(x|y)) guides the distribution q to seek the mode in the\nposterior distribution over all possible images that align with the observation y:\n$D(q_x(x)||p(x|y)) = -E_{q_x(x)} [\\text{log } p(y|x)] + D(q_x(x)||p(x))$\n$ = -E_{q_x(x)} [\\frac{||y - f(x)||^2}{2\\sigma_v^2}] + E_{t,\\epsilon} (\\frac{g(t)^2}{2\\sigma_t^2})[||\\epsilon - \\epsilon_\\theta(z_t, t)||^2]$\nwhere g(t) is the loss-weighing term, zt denotes samples generated by drawing x from qx(x) and\napplying the forward process in Eq. (1). However, RedDiff assumes a small constant variance for the\nvariational distribution (0 \u2248 0), which further simplifies the optimization problem to\n$\\text{min}_{\\mu} ||y - f(\\mu)||^2 + E_{t,\\epsilon} (\\frac{g(t)^2}{2\\sigma_t^2})[||\\epsilon - \\epsilon_\\theta(z_t, t)||^2],$\nwhere 2t = \u03b1\u03c4\u03bc + \u03c3\u03c4\u03b5. RedDiff seeks an image \u00b5 that reconstructs the observation y according to\nthe measurement model f, while having a high likelihood under the diffusion prior (second term).\nThe expectation in the second term averages over many time steps, but in practice they find optimiza-\ntion with such a loss to be difficult. Moreover, RedDiff observes that annealing time from t = T to\nt = 0, as in standard backward diffusion samplers, yields better performance rather than directly\noptimizing the variational bound through random time sampling. Some visual examples are provided\nin Fig. 7 for a comparison between RedDiff-V, which uses random-time sampling as justified by\nthe correct variational bound, and RedDiff which employs descending time from T to 0. Red-Diff\nformulates the loss function at each time step t as follows:\n$||y - f(\\mu)||^2 + w_t (sg[\\epsilon - \\epsilon_\\theta(z_t, t)])^T \\nabla_\\mu z_t,$\nwhere wt is a loss weighing term adjusted by Red-Diff and sg denotes a stop-gradient operator used\nby Red-Diff to prevent optimization instability, which may arise due to the denoising function's lack\nof smoothness at low-noise levels [Yang et al., 2024].\nBecause Red-Diff employs a simple variational posterior that directly optimizes an image at the noise\nfree (t = 0) level only, it is inherently incapable to capture uncertainty in x, due to its mode-seeking\nbehaviour. Additionally, its optimization process is biased because it relies on annealing time during\nthe diffusion process rather than randomly sampling time points. We demonstrate that in contrast, our\nVIPaint framework better models posterior uncertainty, enables stable optimization of an unbiased\nvariational bound, and can be applied to both pixel-based and latent DMs."}, {"title": "4 VIPaint: Variational Inference of Diffusions for Inpainting", "content": "Given a pre-trained diffusion model, VIPaint constructs a joint posterior that is Markovian over a\nsubset of latents, zt \u2208 [Ts, Te], where 0 < Ts < Te < T. This introduces a hierarchical structure to\nthe inference process. An overview is provided in Fig. 3. The posterior also incorporates additional\nparameters, \u5165, and semantically aligns with the test observation during optimization. Once the model\nis fit, the second stage involves sampling from the posterior, utilizing DPS gradient updates in the\nlow-range latent space ([0, Ts]). In this section, we provide a detailed explanation of VIPaint's\nposterior formulation, along with its two-step optimization and sampling strategies."}, {"title": "4.1 Defining the Variational Posterior", "content": "VIPaint selects K intermediate timesteps in the mid-ranged latent space [Ts, Te] to define its hierar-\nchical variational posterior. This technique offers several advantages over RedDiff, as the posterior:\n1) infers the medium-to-high global image semantics in the latent space, induced by the corrupted\nimage y; 2) accounts for uncertainty in the missing regions; and 3) strategically avoids the training\ninstabilities observed in the low-ranged latent space [0, Ts) Yang et al. [2024] and is thus easily\nextended to latent diffusion models. Through experimentation, we select intermediate timesteps that\napproximately yield a signal-to-noise ratio (\u03b1\u03be/\u03c37) in the range [0.2, 0.5] across the different (latent)\ndiffusion models used.\nVIPaint approximates the posterior conditional probability po(x | y) in Eq. (7) as follows:\n$q(x) = \\int q(x | z_{T_e}) (\\prod_{i=1}^{K-1} q_x(z_{s(i)}| z_{s(i+1)})) q_x(z_{T_e}) dz,$\nwhere K \u2265 2, and timesteps (Ts, Te) define the boundaries of our variational posterior along the\ndiffusion timesteps. For the highest timestep Te, we let our posterior qx(ZT) be a simple Gaussian"}, {"title": "4.2 Phase 1: Optimization", "content": "To fit our hierarchical posterior, we optimize the variational lower bound (VLB) of the marginal\nlikelihood of the observation y. The derivation is provided in Appendix B, and the simplified\nthree-term objective is expressed as follows:\n$L(\\lambda) = -E_q[\\text{log } p_\\theta (y|z_{T_e})] + \\beta (\\sum_{i=1}^{K-1} D [q_x(z_{s(i)} | z_{s(i+1)})||P_\\theta(z_{s(i)} | z_{s(i+1)})] + L_{(T_e,T)}(z_{T_e})),$\nVIPaint seeks latent-posterior distributions that assign high likelihood to the observed features y (by\nminimizing the reconstruction loss), while simultaneously aligning with the medium-to-high noise\nlevels encoding image semantics (hierarchical and diffusion losses) via weight \u03b2> 1 [Higgins et al.,\n2017, Agarwal et al., 2023]. We approximate L(A) with M Monte Carlo samples from the hierarchical\nposterior distribution in Eq.12 and use automatic differentiation to compute gradients with respect\nto \u03bb. We follow ancestral sampling to draw $z_{T_e}^{(m)} \\sim q_x(z_{T_e}), {z_{s(i-1)}}^{(m)} \\sim q_x (z_{s(i-1)}|z_{s(i)})\\}_{i=1}^K$ and\nevaluate L(A) as:\n$\\frac{1}{M} \\sum_{m=1}^M [-\\text{logo } p_\\theta (y|z_t^m) +\\beta (\\sum_{i=1}^{K-1} D [q_x (z_{s(i)}^{(m)} | z_{s(i+1)}^{(m)})||P_\\theta(z_{s(i)} | z_{s(i+1)}^{(m)})] + L_{(T_oT)}(z_{T_e}^{(m)}))].$"}, {"title": "Reconstruction Loss", "content": "This term guides the samples from the posterior $z_{T_e}^{(m)}$ to be closer to the\nobservations y. We employ a one-step expected mean prediction $E[z_0| z_{T_e}^{(m)}]$ as in Eq. (3) to\napproximate 20. Because Ts is closer to t = 0, this approximation is accurate enough to guide\nsamples z\u012b to be consistent with y. Then, for latent diffusion models, we use decoder upsampling to\nproduce image 2. We follow the L1 reconstruction loss that was used to pre-train the diffusion models.\nFor latent diffusion models specifically for the task of image inpainting, we add the perceptual loss\n[Zhang et al., 2018] that was also originally used to train the decoder. Fig. 11 (Appendix) shows an\nablation that adding such a term helps avoid blurry reconstructions."}, {"title": "Diffusion Loss", "content": "We derive the diffusion loss in Appendix B). Essentially, $L_{(T_e,T)}(z_{T_e}^{(m)})$ is a\nrestriction of Eq. (5) to a small set of high-ranged times (Te, T). Intuitively, this loss term regularizes\nthe samples $z_{T_e}^{(m)} \\sim q(z_{T_e})$ in high-level image semantics while being consistent with the observation\ny. Following prior work, instead of summing this loss over all t > Te, we sample timesteps\nt ~ U(Te, T) defined on a non-uniform discretization [Karras et al., 2022], yielding an unbiased\nestimate of the loss as (see App. B) :\n$L_{(T_e,T)}(z^{(m)}) = \\frac{T-T_E}{2} E_{t\\sim U(T_e,T),q(z_t|z_{T_e}^{(m)})} D[q(z_{t-1}|z_t, z_{T_e}^{(m)})||P_\\theta(z_{t-1}|z_t)].$"}, {"title": "Hierarchical Loss", "content": "The KL terms across the K 1 intermediate critical times in the hierarchy is\ncomputed in closed form (an analytic function of the means and variances) between the posterior and\nprior conditional Gaussian distributions. Intuitively, this term further regularizes posterior samples\n${z_{s(i)}}^{(m)}\\}_{i=1}^{K-1}$ to capture high-to-medium level image details learned by the prior diffusion model in\nthe range [Ts, Te].\nHence, all the loss terms in Eq. (13) are stochastically and differentiably estimated based on samples\nfrom the hierarchical posterior, enabling joint optimization. Progress in fitting VIPaint's posterior\nis shown in Fig. 4; the number of optimization steps may be reduced to more quickly give approx-\nimate posteriors. From Eq. (13), if the posterior is only defined on the noise-free level zo as in\nRed-Diff [Mardani et al., 2023], the VIPaint objective reduces to an objective mentioned in their\nwork. However, VIPaint strategically avoids low noise levels in its posterior and decreases training\ninstabilities as observed by RedDiff and extends to Latent Diffusion priors."}, {"title": "4.3 Phase 2: Sampling", "content": "After optimization, the hierarchical posterior $\\prod_{i=1}^K q_x (z_{s(i-1)}|z_{s(i)})q_x(z_{T_e})$ is now semantically\naligned with the observation. We employ ancestral sampling on our K level hierarchical posterior\nstarting from Te to Ts, yielding samples $z_{T_e} \\sim \\prod_{i=1}^K q(z_{s(i-1)}|z_{s(i)})q_x(z_{T_e})$. This posterior\nsampling step gradually adds more semantic details in samples as shown in Fig. 3. Further, VIPaint\nrefines z\u012b using the prior denoising model at every step t < Ts. Similar to DPS Chung et al.\n[2023], we update the samples using gradient of the likelihood log pe(y | zt), t < Ts. This ensures\nfine-grained consistency to our final inpaintings.\nTime Complexity. The time taken by VIPaint scales primarily with the number of denoising network\ncalls. Each optimization step for a K-step posterior (K \u226a T) involves O(K) calls to sample\nZT. ~ qx(z), and one to compute the diffusion prior loss, resulting in O(K) function calls per step.\nThus, for I optimization steps, the overall complexity is O(KI). For example, when VIPaint-2 is\noptimized over 50 iterations, it requires only 50 * (2 + 1) = 150 denoising network calls to infer\nglobal image semantics. We show the progress in fitting VIPaint's posterior is shown in Fig. 4.\nSampling from this posterior requires iterative refinement with the denoising diffusion network, for\nan additional Ts calls per sample."}, {"title": "5 Experiments & Results", "content": "Experimental Setup\nWe conduct experiments across 3 image datasets: LSUN-Church [Yu et al., 2015], ImageNet-64\nand ImageNet-256 [Deng et al., 2009]. For ImageNet-64, we use the class-conditioned pixel-space\n\"EDM\" diffusion model [Karras et al., 2022] with the pre-trained score network provided by the\nauthors. For LSUN-Churches256 and ImageNet256 we use the pre-trained latent diffusion models"}, {"title": "VIPaint solves general linear inverse problems", "content": "A quantitative analysis is reported in Table 4 and\nqualitative results in Fig. 14, 15, and 16. In addition to the LPIPS scores, we also compute the PSNR\nmetrics, averaged over 10 random samples for each of 100 test images. We see that VIPaint shows\nstrong advantages over ReSample and DPS for complex image datasets like ImageNet."}, {"title": "VIPaint enforces consistency with large masking ratios", "content": "Table 1 reports LPIPS scores for the\ntask of image inpainting with large masking ratios using pixel and latent-based diffusion models,\nrespectively. For pixel-based diffusion models, we see that RED-Diff and DPS perform poorly."}, {"title": "VIPaint yields multiple plausible reconstructions in the case of high uncertainty", "content": "We compare\nVIPaint with the best performing baseline, CoPaint across multiple sample inpaintings in Fig. 8, a\nmore comprehensive comparison is in Appendix (Fig 18-19). We observe that VIPaint produces\nmultiple visually-plausible imputations while not violating the consistency across observations. We\nshow diversity in possible imputations using different class conditioning using VIPaint in Fig. 9."}, {"title": "VIPaint smoothly trades off time and sample quality", "content": "VIPaint-2, utilizing a two-step hierarchy\nnaturally is the fastest choice for any k in VIPaint-K. It is comparable with other baselines with\nrespect to time (for a more detailed analysis, please refer to Appendix E). However, from Tables 1,\nwe see a remarkable gain in performance when compared with other baselines. VIPaint-4 converges\na bit more slowly (Fig. 12), but ultimately reaches the best solutions."}, {"title": "6 Conclusion", "content": "We present VIPaint, a simple and a general approach to adapt diffusion models for image inpainting\nand other inverse problems. We take widely used (latent) diffusion generative models, allocate varia-\ntional parameters for the latent codes of each partial observation, and fit the parameters stochastically\nto optimize the induced variational bound. The simple but flexible structure of our bounds allows\nVIPaint to outperform previous sampling and variational methods when uncertainty is high."}, {"title": "A Diffusion Models: Definition & Training Procedure Recap", "content": "The background and expressions on forward diffusion process is taken from Kingma et al. [2021b]\nand included here for completeness. Re-iterating Eq. 1, we have the forward diffusion as:\n$q(z_t | x) = \\mathcal{N}(a_t x, \\sigma_t^2 I).$\nThe distribution q(zt|zs) for any t > s are also Gaussian, and\nfrom Kingma et al. [2021b], we can re-write as\n$\\mathcal{N}(a_{t|s}s, \\sigma_{t|s}^2 I)$\nwhere, $a_{t|s} = a_t/A_s,$\nand, $\\sigma_{t|s}^2 = \\sigma_t^2 - a_{t|s}^2 \\sigma_s^2.$\nThe posterior q(zs|zt, x) from Kingma et al. [2021b] can be\nwritten as:\n$q(z_s|z_t,x) = \\mathcal{N}(\\mu_q(z_t, x; s, t), \\sigma_q^2(s,t)I)$\nwhere, $\\sigma_q^2(s,t) = \\sigma_t^2 \\sigma_s^2/\\sigma_t^2$\nand, $\\mu_Q (z_t, x; s,t) = \\frac{ a_s \\sigma_t^2 z_t + a_t \\sigma_s^2 x}{\\sigma_t^2}.$\nHere, we describe in detail the conditional reverse model distributions pe(zs|zt) for the two cases\nof variance-exploding and variance preserving diffusion process. Given these formulations, it is\nstraightforward to compute the KL distance between our posterior qx (zs|zt, y) and the prior pe(zs|t)\nin our loss objective (Eq. 13) since both are conditionally Gausian distributions and computing the\nKL between two Gaussians can be done in closed form.\nIn this case, at = 1 and ot is usually in the range\n[0.002, 50] Song et al. [2021a]. We follow the ancestral sampling rule from the same work to define\nour prior conditional Gaussian distributions pe(ZsZt):\n$P_\\theta(z_s|z_t) = \\mathcal{N}(\\mu_\\theta(z_t; s, t), \\sigma_\\theta^2(s,t)I)$\nwhere, $\\sigma_\\theta^2(s,t) = \\frac{\\sigma_s^2(\\sigma_t^2 - \\sigma_s^2)}{\\sigma_t^2}$\nand, $\\mu_\\theta (z_t; s, t) = \\frac{\\sigma_s^2}{\\sigma_t^2} z_t + \\frac{\\sigma_t^2 - \\sigma_s^2}{\\sigma_t^2} x_\\theta(z_t, t)$\nwhere $x_\\theta (z_t, t) = z_t - \\sqrt{(\\sigma_t^2 - \\sigma_s^2)} * \\epsilon_\\theta(z_t, t)$\nIn this case, at = $\\sqrt{1 - \\sigma_t^2}$ and ot is usually in the\nrange [0.001, 1] Ho et al. [2020a]. We follow the DDIM sampling rule Song et al. [2021b] to define\nour prior conditional Gaussian distributions pe(zs|zt). This sampling rule is widely used to generate\nunconditional samples in small number of steps, and naturally becomes a key design choice of our\nprior. Here,"}, {"title": "A.3 Derivation of Objective for training Diffusion Models: $L_{(0,T)} (z_0)$", "content": "The usual variational bound on the negative loglikehood on data x:\n$\\mathbb{E"}, ["text{log } p_\\theta(x)"], "le E_q [-\\text{log } \\frac{p_\\theta (z_{0:T})}{q(z_{1:T}|x_0)}}"], "follows": "n$\\mathcal{L"}, {"frac{q(z_{1": "T"}, {"p_\\theta(z_{0": "T"}, {}]