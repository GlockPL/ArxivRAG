{"title": "KBQA-01: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search", "authors": ["Haoran Luo", "Haihong E", "Yikai Guo", "Qika Lin", "Xiaobao Wu", "Xinyu Mu", "Wenhao Liu", "Meina Song", "Yifan Zhu", "Luu Anh Tuan"], "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-01, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration's performance and search space. With heuristic exploration, KBQA-01 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-01 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo. Our code is publicly available at https://github.com/LHRLAB/KBQA-01.", "sections": [{"title": "1. Introduction", "content": "Knowledge Base Question Answering (KBQA) leverages a large-scale structured knowledge base (KB), such as Free-base or Wikidata, as a reference to answer questions in nat-ural language, widely applied in fields such as search en-gines, medical consultations, and legal analysis. Typically, KBs are stored in graph databases and accessed using graph queries, such as SPARQL, which sup-port multi-hop and logical queries to acquire knowledge in-formation. To answer natural language questions, language models are usually leveraged to convert the questions into logical forms, such as S-expression, and then transform them into executable graph queries to obtain answers from KB, as shown in Figure 1.\nWith the emergence of LLMs, two main types of KBQA methods appear, as shown in Figure 2. On the one hand, end-to-end methods generate logical forms directly from natural language questions and utilize retrieval before or after gen-eration for improvement. On the other hand, step-by-step methods alternate be-tween generation and retrieval for stepwise thinking on KB, performing in a Chain-of-Thought (CoT) or Tree-of-Thoughts (ToT) manner.\nHowever, three main challenges remain: (1) Poor aware-ness of the KB environment in end-to-end methods. Re-lying on direct logical form generation by language models, end-to-end KBQA methods struggle with limited logical form schemas and unseen en-tities and relations that existed in KB, making it difficult to fully capture the KB environment. (2) Local optima or large search space in step-by-step methods. Com-pared to end-to-end methods, CoT-based step-by-step meth-ods in the search process can lead to"}, {"title": "2. Related Work", "content": "Knowledge Base Question Answering. Before the rise of LLMs, the KBQA methods could be divided into information-retrieval-based (IR-based) and semantic-parsing-based (SP-based). In the era of LLMS, LLM-based KBQA methods can be divided into two cate-gories: end-to-end and step-by-step. End-to-end methods take advantage of in-context learning (ICL) or fine-tuned open-source LLMs to equip LLMs with the ability to generate queries. The step-by-step methods follow a reasoning process on the graph to gradually find the answers. In this paper, we propose the first heuristic KBQA method.\nLLMs and LLM-powered Agent. LLMs have shown significant advantages in semantic understanding, genera-tion, and reasoning. Powered by CoT, guiding LLMs to think step by step can further enhance reasoning capabilities. ReAct introduces a prompt-based agent, using tools to interact with the environment. Lightman et al. verifies the CoT by rewarding the process and the outcome. On the other hand, to expand the thought space in every step, ToT is designed to reason in a tree-like manner. RAP employs MCTS, a heuristic algo-rithm applied in AlphaGo, AlphaGeometry and the OpenAI's large reasoning model GPT-01."}, {"title": "3. Preliminaries", "content": "Definition 1: Knowledge Base. A knowledge base (KB) is a large-scale knowledge graph G = (E, R, T), composed of an entity set E, a relation set R, and a triple set T. Relations in the relation set r \u2208 R are used to connect two entities. Each triple (s, r, o) \u2208 T is formed by (entity, relation, en-tity), thus T = {(s, r, o)|s \u2208 E, r \u2208 R, o \u2208 E}.\nDefinition 2: Logical Form. The logical form F is a multi-hop expression that can convert equally to a graph query q = Convert(F). Each logical form can be divided into a stepwise list of functions F = [fi] with l steps.\nProblem Statement. In the KBQA task, given a natural language question Q and a KB G, the goal is to first convert Q into a logical form F, and then an executable graph query q. Once executed, the result A = Exec(q, G) is a set of entities in the KB A C E that answer the question Q."}, {"title": "4. Method: KBQA-01", "content": "In this section, we introduce the three components of KBQA-01: agent initialization, heuristic environment exploration using MCTS with policy and reward models to optimize the agent process, and incremental fine-tuning with auto-annotated data to improve low-resource performance."}, {"title": "4.1. Agent Initialization", "content": "KBQA-01 follows a ReAct-based agent prompt, with KB environment, the agent state space and exploration space, targeting to generate logical forms.\nKB Environment G. We consider KB a critical environment that provides guides in generating the logical form at each step of the agent, for example, by providing the candidate relations connected to the current state of the logical form.\nThe Agent State Space H. The agent state ht \u2208 His defined by its exploration history as ht = (ho, e1, ..., et).\n\u2022 Initial State (ho): We create an initial prompt, consist-ing of the task description and the given question Q, as the initial state ho, as shown in Appendix A.1.\n\u2022 State Update (ht): At each step t, the state incorpo-rates the latest exploration step, which is prompted as ReAct-based Thought-Action-Observation tuple et = (eth\u2081, eact, eobs) as shown in Appendix A.2, to update: ht = ht\u22121 + et.\n\u2022 State Representation (Fh\u2081): Besides prompt-based history, the cumulative Observations in the trajectory (ebs,..., eobs) determine the function list, which is equivalent to the logical form Fh\u2081 = [et].\nThe Agent Exploration Space Exp(H, G). The agent ex-ploration et \u2208 Exp(ht\u22121, G) is dynamically determined by the last state ht\u22121 \u2208 H and the KB environment G. Each exploration comprises the following components:\n\u2022 Tool Selection (etht): Based on ht\u22121, the agent selects one of the eight atomic query tools as shown in Table 1.\n\u2022 Argument Determination (eact): The agent identifies the appropriate arguments for the selected tool, lever-aging the candidates provided by the KB environment. For example:\nCalling Extract entity requires specifying an entity name existed in G as the argument.\nCalling Find_relation involves specifying a relation name that has a connection with Fht-1.\n\u2022 Function Writen (ebs): Based on the selected tool and arguments, the agent writes down the correspond-ing target function in Observation, referred to Table 1.\nThe Agent Target (h\u0131, Fhi, An\u2081). The agent explores the exploration space until calling the Finish tool or when the length of the function list exceeds the maximum allowable length l < L. The ultimate target of the agent is to find a complete state h\u2081, which forms a logical form Fh\u2081 and the final answers executed An\u2081 = Exec(Convert(Fn\u2081), G).\nProposition 4.1. The agent's awareness of the environment makes it more effective in generating optimal logical forms compared to end-to-end methods.\nProof. We provide quantitative experimental results in Sec-tion 5.4 and qualitative proofs in Appendix B.1."}, {"title": "4.2. Heuristic Environment Exploration", "content": "As shown in Figure 3, to address the issue of the step-by-step agent falling into local optima or large search spaces, we design an MCTS-based heuristic environment exploration method, driven by a policy model and a reward model."}, {"title": "4.2.1. THE POLICY MODEL", "content": "The policy model aims to provide the agent with a forward-looking capability. We use the last state at each step ht-1 from the annotated training set Da as input, and the steps from the current state to the conclusion (et, ..., er) as output, forming SFT data for training the policy model \\(\\pi_{\\text{policy}}\\):\n\\(L_{\\text{SFT}}(\\pi_{\\text{policy}}, D_a) = -E_{D_a}\\sum_{i=t}^{r} \\log \\pi_{\\text{policy}}(e_i | h_{t-1})\\)"}, {"title": "4.2.2. THE REWARD MODEL", "content": "The reward model aims to assess the entire trajectory by scoring the final logical form. We use the question Q as input, and the logical form Fh, from the annotated training set Da as output, forming SFT data for training the reward model \\(\\tau_{\\text{reward}}\\):\n\\(L_{\\text{SFT}}(\\tau_{\\text{reward}}, D_a) = -E_{D_a} [\\log \\tau_{\\text{reward}} (F_{h_l} | Q)],\\)\nMoreover, we employ a scoring method R that uses the logits from the LLM \\(\\pi\\), which can be \\(\\pi_{\\text{policy}}\\ or \\(\\tau_{\\text{reward}}\\), to evaluate the likelihood of an output y, given an input x:\n\\(R(y|x) = \\beta + \\alpha \\log \\pi (y|x).\\)\nwhere \\(\\beta\\) is the defined full score, set as 100, and \\(\\alpha\\) is a positive temperature to control the disparity of scores."}, {"title": "4.2.3. \u039c\u039fNTE CARLO TREE SEARCH OVER KB", "content": "MCTS is a heuristic search algorithm in the form of a tree, where each node represents a state in the agent process. Starting from the initial state (the root node), the algorithm uses four stages of selection, expansion, simulation, and back-propagation to explore and enrich the search tree it-eratively. MCTS conducts a total of N search rollouts. In the n-th rollout (n = 1, ..., N), the agent process can be represented by the trajectory of agent states {[hn)]}=1}=1\nSelection. When a new MCTS rollout begins, the agent process starts from the root node and progressively searches down through the child nodes of the already explored tree until it reaches a leaf node. At each level, the UCT (Upper Confidence Bound applied to Trees) algorithm is used to select the next child node:\n\\(e_{t} = \\text{arg } \\max_{e_{t-1}^{(n)}}\\frac{Q(h_{t-1}^{(n)} + e)}{N(h_{t-1}^{(n)} + e)} + w \\sqrt{\\frac{\\ln N(h_{t-1}^{(n)})}{N(h_{t-1}^{(n)}+e)}}\\)\nwhere N(.) is the visit counts of the agent state during the MCTS process, E(.) is the candidate expansion of Thought-Action-Observation explorations, derived from Equation (7), and Q(.) is the Q-value of the agent state, which will be updated by back-propagation. UCT balances the selection of high-scoring nodes with the exploration of unvisited ones. The variable w controls the tendency towards exploration. A larger w encourages exploration of nodes with fewer visits, while a smaller w biases nodes with higher scores.\nExpansion. Once the selection process reaches a leaf node but not in a terminal Finish state and is not beyond the maximum depth L, the policy model \\(\\pi_{\\text{policy}}\\ generates several possible next states by beam search:\n\\({e_b^{(i)}}\\}_{b=1}^{B} \\sim \\pi_{\\text{policy}}(e | h_{\\text{beam}}),\\)\nwhere B is the beam size. To engage with KB environment, we utilize an unsupervised retrieval model, SimCSE, to match the generated explorations with the exploration options e \u2208 Exp(h), G) that are executable over KB G when connected with the last state hn):\n\\({e_k^{(i)}}\\}_{k=1}^{k} \\leftarrow \\text{arg max}_{e \\in \\text{Exp}(h), G)} \\text{SimCSE}(\\{e_b^{(i)}\\}_{b=1}^{B} \\{e}).\\)\nFor example, if the ground truth is 'Find_relation[ file.actor.film ]' the model might initially generate '[ film.actor ]'. Then, we select the most semantically related action options from Exp(h), G) such as \u2018[ file.actor.film ]' and '[tv.tv_actor.starring_roles ]' to filter the top k explo-rations {e}\\}_{i=1}^{(i) k}.\nThen, the policy model \\(\\pi_{\\text{policy}}\\ scores the k candidates based on the previous state h), and selects the top d candidates as expanded options E(h), which are added as child nodes to the leaf node, thus expanding the tree:\n\\(E(h) = \\{e_d^{(i)}\\}_{d=1}^{d} \\leftarrow \\text{arg max} \\text{Rpolicy} (\\{e_k^{(i)}\\}_{k=1}^{k}|h).\\)\nSimulation. After the nodes are expanded, the policy model assigns scores to all newly added child nodes. The node with the highest prospective score is selected:\n\\(e_{t} \\leftarrow \\text{arg max}_{e} (\\eta(n^{(2)}) R_{\\text{policy}} (e|h^{(1)},\\)\nand the simulation continues to explore the process until the final Finish state, producing a complete logical form generation trajectory.\nBack-propagation. Once the final state is reached, the reward model \\(\\tau_{\\text{reward}}\\ evaluates the entire trajectory by as-sessing the corresponding logical form combined with the policy model's score from the last step to compute the over-all Q-value of the final state:\n\\(Q(h^{(n)}) \\leftarrow \\delta  R_{\\text{policy}} (e_t | h_t) + (1-\\delta ) \\text{Reward} (F_{h_l^{(i)}} | Q),\\)\nwhere \\(\\delta\\) is a ratio from (0, 1) to balance the process score and overall score. The algorithm then back-propagates the score by updating the Q-values of all nodes along the trajec-tory, from the leaf back to the root:\n\\(Q(h^{(n)}) \\leftarrow \\frac{\\sum_{j=1}^{|E(h^{(n)})|}Q(h_j^{(n)})}{|E(h^{(n)})|},\\)\nwhere the Q-values of parent nodes are updated to the max-imum average Q-value from all child nodes along the trajectory. Meanwhile, the visit count of each node along the trajectory N(h) adds 1, and then the next rollout begins."}, {"title": "4.2.4. FINAL TRAJECTORY CHOSEN", "content": "After MCTS completes its exploration for N rollouts with parameter set \\(\\theta\\), we select the trajectory h\\(^\\theta\\) \u2208 {h(n)}=1 with the highest Q-value in every state in the expanded search tree as the optimal trajectory of question Q:\n\\((\\hat{h}^{\\theta}, \\hat{F}^{\\theta}, \\hat{A}^{\\theta}) = \\text{MCTS}_{\\theta} (Q, \\pi_{\\text{policy}}, \\tau_{\\text{reward}}),\\)\nwhere F^\\theta is corresponding logical form of \u0125, and \u00c2\u00ba = Exec(Convert(\u00ce\u00ba), G) is the executed answers.\nProposition 4.2. The MCTS-based heuristic method bal-ances the effectiveness and size of the search space better than CoT-based and ToT-based step-by-step methods.\nProof. We provide quantitative experimental results in Sec-tion 5.4 and qualitative proofs in Appendix B.2."}, {"title": "4.3. Incremental Fine-Tuning", "content": "In addition to training on annotated data, KBQA-01 em-ploys MCTS with exploration incentives dexp for heuristic exploration on unannotated questions Q \u2208 Dn:\n\\{(\\hat{h}^{\\theta}, \\hat{F}^{\\theta}, \\hat{A}^{\\theta})\\}_{Q \\in D_i} = {\\text{MCTS}_{\\theta \\text{exp}} (Q, \\pi_{\\text{policy}}, \\tau_{\\text{reward}})}\\}_{Q \\in D_n}\\)\nThen, we discard the annotation by choosing if the answer set is not empty and the reward score of logical form does not exceed a threshold \\(\\gamma^*\\):\n\\(\\hat{R}^{\\theta} = R-\\text{reward} (\\hat{F}^{\\theta} | Q),\\)\n\\(D_{i} = D_a \\cup { (Q, \\hat{F}^{\\theta}, \\hat{A}^{\\theta}) | \\hat{A}^{\\theta} \\neq 0 \\land \\hat{R}^{\\theta} > \\gamma^*}\\}_{Q \\in D_n} \\)\nCombined with the original annotated data Da, the incre-mental data Di is then used for incremental fine-tuning of the policy and reward models:\n\\(L_{\\text{SFT}}(\\pi_{\\text{policy}}, D_i) = -E_{D_i} \\sum_{i=t}^{r} \\log \\pi_{\\text{policy}} e_i|h_{t-1}\\),\\)\n\\(L_{\\text{SFT}} (\\tau_{\\text{reward}}, D_i) = -E_{D_i} [\\log \\tau_{\\text{reward}} (F_{h_l} | Q)] .\\)\nThrough incremental fine-tuning, the policy and reward models gain enhanced understanding of the environment and a preference for high-reward logical form trajectories. Finally, we perform testing Dt under efficiency-focused MCTS parameter settings deff, yielding final answers \u00c2\u00ba:\n{\\hat{h}^{\\theta}, \\hat{F}^{\\theta}, \\hat{A}^{\\theta}}\\}_{Q \\in D_i} = {\\text{MCTS}_{\\theta \\text{eff}} (Q, \\pi_{\\text{policy}}, \\tau_{\\text{reward}})}\\}_{Q \\in D}\nProposition 4.3. There exists a reward threshold \\(\\gamma^*\\) < \\(\\beta\\) such that incremental fine-tuning data, under the effect of the KB, can improve model performance.\nProof. We provide quantitative experimental results in Sec-tion 5.5 and qualitative proofs in Appendix B.3."}, {"title": "5. Experiments", "content": "This section presents the experimental setup, results, and analysis. We answer the following research questions (RQs):\nRQ1: Does KBQA-01 outperform other KBQA methods?\nRQ2: Does the main component of KBQA-01 work? RQ3: Does KBQA-01 address the corresponding challenges com-pared to end-to-end and step-by-step KBQA methods? RQ4: How does incremental fine-tuning gradually improve low-resource KBQA performance?"}, {"title": "5.1. Experimental Setup", "content": "Datasets. All experiments are conducted on three standard KBQA datasets in low-resource setting: GrailQA, WebQSP, and"}, {"title": "5.2. Main Result (RQ1)", "content": "As shown in Tables 2, 3, and 4, KBQA-01 enables open-source LLM to outperform previous low-resource KBQA methods based on GPT-3.5-turbo, with limited labeled data. In more complex data sets such as GrailQA, KBQA-01 improves the overall EM performance of the Llama-3.1-8B model by 28.1 percentage points and boosts F1 scores by 30.0 percentage points over the previous best meth-ods. In compositional and zero-shot evaluations, KBQA-01 even outperforms fully supervised KBQA methods, which demonstrates its strong capabilities for generalization, en-vironment exploration, and handling complex logical ques-tions. Moreover, KBQA-01 is plug-and-play, allowing inte-gration with various open-source 7B-72B LLMs, expected to improve further with future open-source LLM updates."}, {"title": "5.3. Ablation Study (RQ2)", "content": "As shown in Table 5, we conduct an ablation study on the Llama-3.1-8B-based KBQA-01 methods. Five ablation set-tings are tested: removing the ReAct-based agent prompt, removing environment feedback, excluding the initial su-pervised fine-tuning (SFT) with a small amount of labeled data, removing MCTS optimization, and omitting the incre-mental fine-tuning stage, respectively. Comparisons of the evaluation results reveal that all modules contribute to over-all performance, underscoring the importance of designed agent initialization, heuristic environment exploration, and incremental fine-tuning as key components of KBQA-01."}, {"title": "5.4. Comparison Analysis (RQ3)", "content": "To explore how KBQA-01 addresses the challenges of end-to-end and step-by-step methods mentioned in Section 1, we construct six variants. We design two end-to-end variants: one is a retrieve-then-generate method (RG-E2E) based on DECAF, and the other is a generate-then-retrieve method (GR-E2E), based on ChatKBQA. Furthermore, we design two step-by-step variants: one is based on CoT (CoT-SbS) as QueryAgent, and the other is based on ToT (ToT-SbS) as ToG. Finally, we implement an MCTS-optimized variant without incremental fine-tuning and the full KBQA-01 method after incremental fine-tuning."}, {"title": "5.5. Analysis of Incremental Improvement (RQ4)", "content": "To explore the impact of incremental fine-tuning on improv-ing the performance of KBQA-01 in low-resource KBQA scenarios, we examine three key factors: the impacts of the exploration samples, the reward threshold, and the explo-ration weights on performance and efficiency.\nImpact of Exploration Samples. As shown in Figure 5(a), we gradually increase the number of unlabeled exploration samples and observe a steady improvement in the F1 and EM scores after incremental fine-tuning. This suggests that"}, {"title": "6. Conclusion", "content": "In this work, we propose KBQA-01, an agentic KBQA method with Monte Carlo Tree Search (MCTS) for efficient exploration. By combining a ReAct-based agent process with incremental fine-tuning, it improves logical form gen-eration and reduces reliance on annotated data. Experiments on three KBQA datasets show that KBQA-01 outperforms previous low-resource methods and rivals fully supervised models, demonstrating its scalability and effectiveness."}, {"title": "Impact Statement", "content": "This work introduces KBQA-01, a novel agentic KBQA method with MCTS. However, it has limitations, including the need for more fine-grained policy and reward mecha-nisms, and challenges in large-scale transfer. To address these, we propose four future directions: (1) exploring re-inforcement learning methods such as DPO for continual learning, (2) enriching logical operators for broader reason-ing capabilities, (3) adapting the framework to specialized domains like medicine and law, and (4) expanding to mul-timodal, multilingual, and multi-agent settings, detailed in Appendix K. This work does not raise ethical concerns, as it uses publicly available datasets and focuses on advancing KBQA technology, with positive societal impacts such as supporting decision-making in critical domains."}, {"title": "Appendix", "content": "A. Prompts Used in KBQA-01"}, {"title": "A.1. Initial Prompt", "content": "Figure 6 illustrates the initial prompt used in the KBQA-01 agent process. This prompt defines the structure and steps for solving KBQA tasks through interleaving Thought, Action, and Observation stages. The prompt guides the agent to perform specific actions, such as entity extraction, relation finding, expression merging, ordering, numerical comparisons, and adding time constraints. These predefined actions are essential for generating logical forms step by step from natural language questions. The diagram highlights the structured input format (<input> for the question and <output> for logical expressions) and the scratchpad used for intermediate reasoning steps during the task."}, {"title": "A.2. Example Agent Process", "content": "Figure 7 provides an example of the complete agent process in KBQA-01 for the query. The figure demonstrates how the agent iteratively constructs the logical form using the Thought-Action-Observation prompt. Each step involves reasoning about the current context (Thought), performing a specific operation (Action), and observing the resulting logical expression (Observation). The process begins with extracting the topic entity followed by finding relevant relations and applying numerical and time constraints. The agent merges expressions to form a complete logical form, which is then executed to retrieve the answer from the knowledge base. The final logical output form is shown at the bottom, showcasing the agent's ability to generate structured and executable queries systematically."}, {"title": "B. Proof", "content": "B.1. Proof of Proposition 4.1"}, {"title": "B.2. Proof of Proposition 4.2", "content": "Proposition 4.2. The MCTS-based heuristic method balances the effectiveness and size of the search space better than CoT-based and ToT-based step-by-step methods."}, {"title": "C. MCTS Algorithm Details", "content": "Figure 8 and Algorithm 1 illustrate the Monte Carlo Tree Search (MCTS) process in KBQA-01. The figure highlights the four stages of MCTS: Selection, where nodes are chosen using the Upper Confidence Bound for Trees (UCT) to balance exploration and exploitation; Expansion, where candidate actions are generated by the policy model, filtered for relevance to the knowledge base, and added as child nodes; Simulation, where the most promising path is explored to produce a complete logical form and compute rewards; and Back-propagation, where rewards are propagated back to update Q-values and visit counts. The pseudocode formalizes this process, iteratively performing rollouts that follow the four stages. It ensures efficient exploration by selecting nodes with high potential, expanding with semantically relevant actions, simulating logical forms, and updating scores through back-propagation. This approach enables KBQA-01 to navigate large search spaces effectively and generate high-quality logical forms.\nComplexity Analysis. The time complexity of the MCTS process in KBQA-01 can be analyzed based on its four stages: Selection, Expansion, Simulation, and Back-propagation. In the Selection stage, the algorithm traverses the search tree up to depth L, selecting the best node using the Upper Confidence Bound for Trees (UCT), leading to a complexity of O(kL) per rollout, where k is the number of possible actions per step. The Expansion stage generates B beam candidates, which are filtered based on knowledge base similarity, adding O(wk) complexity. The Simulation stage explores paths up to depth L, contributing O(k \u00b7 L) . Finally, the Back-propagation stage updates the rewards along the path, requiring O(L) ."}, {"title": "D. Dataset Details", "content": "Table 6 provides an overview of the dataset statistics used in the KBQA-01 experiments across different settings: I.I.D (Independent and Identically Distributed), Compositional, Zero-shot, and three datasets (GrailQA, WebQSP, and GraphQ). The table includes the following rows: #Train is the number of training examples used for each dataset. #Exploration is the number of exploration queries generated during the heuristic exploration phase for each dataset. The numbers indicate the extensive exploration. #Test is the number of testing examples for evaluation in each setting."}, {"title": "E. Atomic Query Tool Details", "content": "As shown in Table 1, KBQA-01 introduces eight atomic query tools designed to facilitate logical form generation for KBQA. These tools are tailored to systematically convert natural language queries into logical forms by leveraging the structural properties of KB. Each tool serves a specific function, ensuring precise interactions with the KB to retrieve or manipulate information. Below is an explanation of each tool:\nExtract_entity is used to identify and extract a specific entity from the knowledge base. It takes an entity as an argument and initializes the logical form with this entity. The target function is represented as START('entity'), and the corresponding logical form is simply the entity itself. For instance, extracting \"Taylor Lautner\u201d initializes the logical form with the entity identifier for the actor.\nFind_relation identifies a relation connected to the current logical form. It takes a relation as an argument and appends it to the existing logical form using the JOIN operation. The resulting logical form is (JOIN relation (expression)). For example, finding the film.actor.film relation connects an actor to their associated films.\nMerge combines two logical expressions into a single conjunctive expression. It takes two arguments, expression1 and expression, and returns AND(expression1, expression) as the target function. The equivalent logical form is (AND"}, {"title": "F. Baseline Details", "content": "The six full-resource baselines provide a comprehensive evaluation framework for KBQA. These methods leverage fully annotated datasets and advanced reasoning mechanisms to achieve high performance on various KBQA tasks. Below is a summary of each baseline:\nRnG-KBQA is a retrieve-and-generate framework that first retrieves relevant knowledge from the knowledge base (KB) and then generates executable logical forms for answering questions. It focuses on leveraging retrieval to enhance logical form generation accuracy.\nDecAF employs multi-granular retrieval strategies to ensure robust KBQA performance. By focusing on progressively refining retrieved knowledge, DecAF addresses the complexity of large-scale KBs and supports more accurate logical reasoning.\nTIARA is a multi-stage retrieval method designed for large-scale KBs. It enhances robustness by retrieving candidate knowledge in multiple stages and integrating it into logical form generation, improving its ability to handle complex queries.\nSPARQA uses a skeleton-based semantic parsing approach to generate logical forms for complex questions. It simplifies question processing by extracting a structural skeleton, which is then converted into a complete logical form.\nBERT+Ranking integrates a BERT-based encoder with a ranking mechanism to select the most relevant answers from candidate entities. It enhances the precision of entity linking and relation matching in complex KBQA scenarios.\nArcaneQA combines dynamic program generation with contextualized encoding to address complex reasoning tasks. Its innovative design enables it to process multi-hop reasoning queries with high accuracy.\nThe three low-resource baselines are designed to address the challenges of KBQA in scenarios. Unlike fully supervised methods, these approaches focus on GPT API for in-context-learning. Below is a detailed summary of each method:\nKB-BINDER leverages GPT-3.5-turbo to perform KBQA with minimal supervision. It adopts a structured in-context learning approach, where logical form templates guide the generation process. This method effectively balances efficiency and accuracy in low-resource settings.\nKB-Coder employs code-style in-context learning to improve logical form generation. By treating logical form generation as a code-writing task, this method enhances reasoning consistency and adaptability, even with limited training data."}, {"title": "G. Hyperparameter Settings", "content": "Table 7 presents the hyperparameter configurations for the KBQA-01 across three datasets: GrailQA, WebQSP, and GraphQ. These parameters are categorized into four stages: Initial Few-shot SFT, MCTS Exploration Stage, Incremental Fine-tuning, and MCTS Prediction Stage, each designed to optimize the KBQA framework's performance for different tasks.\nIn the Initial Few-shot SFT stage, the policy and reward models are fine-tuned using a small labeled dataset to initialize the framework. Both models adopt the DoRA architecture, optimized for reasoning tasks. The batch size is set to 4, ensuring stability in training with manageable memory usage. A fixed learning rate of 5e-5 controls weight updates, providing a balance between convergence speed and training stability. The number of epochs varies by dataset, with 100 for the policy model in GrailQA and 50 for WebQSP and GraphQ. The reward model undergoes more training, with 300 epochs for GrailQA and 100 for WebQSP and GraphQ, to ensure the robustness of the reward function.\nThe MCTS Exploration Stage employs Monte Carlo Tree Search to explore logical forms by simulating reasoning paths within the KB. Each rollout performs 6 iterations of exploration. A beam size of 2 limits the number of candidate paths considered. TopK and TopD parameters further refine candidate selection, with GrailQA and GraphQ using TopK=10, while WebQSP uses TopK=3. Similarly, GrailQA and WebQSP have TopD=3, whereas GraphQ sets TopD=5. An exploration weight of 50 balances exploration of new paths with exploitation of known high-quality paths. The reward ratio, fixed at 0.5, ensures equal contribution from the policy and reward models. Dataset-specific reward thresholds (-100 for GrailQA, 30 for WebQSP, and -50 for GraphQ) filter out low-quality paths, tailoring the exploration process to the characteristics of each dataset.\nThe Incremental Fine-tuning stage further refines the policy and reward models based on insights gained during the exploration stage. The models retain the DoRA architecture, with a batch size of 4 and a learning rate of 5e-5. Both models are fine-tuned for 10 epochs across all datasets, ensuring improved generalization.\nIn the MCTS Prediction Stage, the refined models are used to generate final logical forms for KBQA tasks. 6 rollouts are performed, consistent with the exploration stage, but the beam size is reduced to 1 to focus on the most promising candidate paths. TopK and TopD parameters mirror those in the exploration stage to maintain consistency in candidate selection. The exploration weight is reduced to 10, prioritizing exploitation of high-quality paths while allowing limited exploration during prediction. The reward ratio remains fixed at 0.5 across all datasets, maintaining a consistent balance between contributions from the policy and reward models."}, {"title": "H. Open-source LLMs used in KBQA-01", "content": "The KBQA-01 framework is designed with a plug-and-play architecture, allowing seamless integration of different open"}]}