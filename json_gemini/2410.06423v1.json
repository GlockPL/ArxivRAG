{"title": "FAIREDU: A Multiple Regression-Based Method for Enhancing Fairness in Machine Learning Models for Educational Applications", "authors": ["Nga Pham", "Minh Kha Do", "Tran Vu Dai", "Pham Ngoc Hung", "Anh Nguyen-Duc"], "abstract": "Fairness in artificial intelligence and machine learning (AI/ML) models is becoming critically important, especially as decisions made by these systems impact diverse groups. In education, a vital sector for all countries, the widespread application of AI/ML systems raises specific concerns regarding fairness. Current research predominantly focuses on fairness for individual sensitive features, which limits the comprehensiveness of fairness assessments. This paper introduces FAIREDU, a novel and effective method designed to improve fairness across multiple sensitive features. Through extensive experiments, we evaluate FAIREDU's effectiveness in enhancing fairness without compromising model performance. The results demonstrate that FAIREDU addresses intersectionality across features such as gender, race, age, and other sensitive features, outperforming state-of-the-art methods with minimal effect on model accuracy. The paper also explores potential future research directions to enhance further the method's robustness and applicability to various machine-learning models and datasets.", "sections": [{"title": "1. Introduction", "content": "With the increasing application of Machine Learning (ML) systems across various industries and sectors of society [1], ensuring the quality of these systems is becoming more important. In the software industry, AI/ML algorithms are potentially transforming how software is developed and operated [2]. As AI/ML takes on a greater role in decision-making processes, particularly with decisions affecting diverse groups, fairness has emerged as a critical concern [3,2]. Unfair outcomes in AI/ML systems are often viewed as \"fairness bugs,\" and substantial research has been dedicated to detecting and mitigating these biases [4, 5, 6, 7, 8, 9, 10, 11]. ML algorithms, for example, can introduce biases linked to sensitive features like gender [12, 13] or race [11, 12, 14], disadvantaging historically marginalized groups.\nIn education, fairness in ML systems extends beyond technical challenges, requiring solutions that address deep-rooted social and structural inequalities [15]. Scholars have long studied disparities in educational access and outcomes, particularly focusing on issues like school segregation and achievement gaps [16, 17, 18, 19, 20]. For instance, it is unfair if students from low-income families consistently score lower due to limited access to resources, or if teacher evaluations and algorithmic grading systems contain biases [21, 22]. Addressing multiple social factors\u2014such as gender, race, socioeconomic status, and disability\u2014is essential for achieving fairness [23]. However, this is a complex issue, as different subgroups face varying degrees of privilege or disadvantage [24]. Moreover, there is often a trade-off between fairness and model performance [10, 25, 26, 27, 28], and the extent to which current methods balance these two aspects remains unclear, especially when considering multiple sensitive features.\nExisting fairness methods fall into three main categories: pre-processing, in-processing, and post-processing [29, 30]. Pre-processing methods, like Reweighting (RW) [31] and Disparate Impact Remover (DIR) [32], adjust the data before model training. In-processing methods, such as Meta Fair Classifier (META) [33], Adversarial Debiasing (ADV) [34], and PR (Prejudice Remover) [35], intervene during model training. Post-processing methods, like Equalized Odds Processing (EOP) [36], Calibrated Equalized Odds (CEO) [37], and ROC (Reject Option Classification) [38] adjust the model's predictions. Additionally, methods combining multiple stages have been proposed, such as Fair-SMO\u03a4\u0395 [6], \u039c\u0391\u0391\u03a4 [39], and FairMask [40]. While effective, these methods often focus on a single sensitive feature, which limits their ability to address fairness across intersecting features.\nIn 2022, Yanhui Li et al. introduced LTDD, a linear-regression-based Training Data Debugging method that enhances fairness by eliminating dependencies between features and sensitive features, making it a simple yet effective solution for real-world applications [41]. However, LTDD is limited to handling one feature at a time, which can improve fairness for a specific feature while potentially reducing it for others [30]. Recently, a few studies have focused on fairness for multiple sensitive features."}, {"title": null, "content": "For instance, Zhenpeng Chen et al. proposed a solution to improve fairness by forming sensitive features by combining different sensitive features into subgroups [30]. Although the combination is quite simple, it provides a solution to address fairness research on a single sensitive feature.\nThis work proposes a novel method, FAIREDU, that is both simple and effective in addressing fairness across intersectional features within the educational context. Four research questions (RQs) are derived from the research objective:\nRQ1 - Is there a systematic bias present among sensitive features within educational datasets?\nRQ2 - Does the level of fairness vary across different machine learning models?\nRQ3 - How does FAIREDU manage multiple sensitive features compared to current state-of-the-art methods?\nRQ4 - How effectively does FAIREDU balance fairness and model performance relative to state-of-the-art methods?\nFAIREDU works as follows: the method detects the dependency of remaining features on sensitive features based on a multivariate regression model and then removes the dependency to create a new dataset that ensures fairness for all features without reducing model performance. We highlight the key characteristics of our method:\nFAIREDU addresses fairness across multiple sensitive features.\nFAIREDU handles multiple sensitive features very well.\nIt applies to both discrete and continuous sensitive features.\nThe rest of this paper is structured as follows. Section 2 presents the background and related work in fairness in ML. Section 3 introduces the FAIREDU method in details. Section 4 describes the experimental setup and methodology used to evaluate FAIREDU. Section 5 presents the research results. Section 6 provides a detailed discussion, where we address the research questions and show the limitations of our approach. Finally, Section 7 concludes the paper and suggests future research directions."}, {"title": "2. Background", "content": "Fairness has been a topic of extensive philosophical debate for centuries, with no universally accepted definition due to differing perspectives and cultural contexts. As artificial intelligence (AI) and machine learning (ML) systems become increasingly embedded in various aspects of life, they now play a significant role in decision-making processes that directly affect individuals [21]. These systems, however, are susceptible to biases, often reflecting the values and prejudices of their human designers. Saxena et al. (2022) note that \"fairness in decision-making can be understood as the absence of bias or prejudice against individuals or groups based on inherent characteristics\" [42]. While the precise definition of fairness in AI/ML remains contested, Hutchinson and Mehrabi offer several prominent interpretations that highlight the diversity of thought in this area [43, 42]. These definitions, summarized in Table 1, provide a foundation for understanding how fairness is applied in AI/ML systems."}, {"title": null, "content": "The fairness literature primarily focuses on characteristics of individuals [51, 7, 52]. To prevent discrimination during tasks like classification or prediction, certain personal characteristics must be protected; these are known as protected attributes or sensitive features. Common sensitive features include sex, race, age, religion, disability status, and national origin. In real-world applications, ML systems often need to account for multiple sensitive features simultaneously. Based on the values of these sensitive features, individuals can be divided into privileged and unprivileged groups. Typically, the privileged group is associated with favorable labels, while the unprivileged group is more likely to receive unfavorable labels [30]. The most common sensitive features in the education context are summarized in Table 2."}, {"title": "2.3. Detecting and fixing fairness bugs for AI/ML systems", "content": "Detecting and addressing fairness bugs in AI/ML systems involves a range of strategies, which are broadly categorized into three main approaches: pre-processing, in-processing, and post-processing methods. Pre-processing methods focus on modifying the training data to eliminate biases before the model is trained. Methods in this category include reweighting, resampling, and data transformation to ensure that the dataset does not favor any particular group. In-processing methods integrate fairness considerations directly into the model training process. These methods involve adjusting the learning algorithms to minimize bias, such as through adversarial debiasing, fairness constraints, or incorporating fairness-aware loss functions. Post-processing methods aim to adjust the model's predictions after training to achieve fair outcomes. This can involve methods like equalized odds processing, and reject option classification, which modify the decision thresholds to ensure fairness across different groups\nPre-processing methods:\nRW (Reweighting) [31] employs differential weighting of training data for each combination of groups and labels to achieve fairness.\nDIR (Disparate Impact Remover) [32] adjusts feature values to enhance fairness while preserving the rank-ordering within groups\nIn-processing methods:\nMETA (Meta Fair Classifier) [33] employs a meta-algorithm to optimize fairness regarding protected attributes.\nADV (Adversarial Debiasing) [34] uses adversarial methods to minimize the presence of protected attributes in predictions, while concurrently maximizing prediction accuracy.\nPR (Prejudice Remover) [35] incorporates discrimination-aware regularization to mitigate the influence of protected attributes.\nPost-processing methods:\nEOP (Equalized Odds Processing) [36] uses linear programming to calculate probabilities for adjusting output labels, aiming to optimize equalized odds concerning protected attributes."}, {"title": "3. FAIREDU - A regression-based method for fairness of multiple sensi-tive features in Education", "content": "Assumed that we have an AI/ML model that does classification or produces binary value, denoted as $S_{ML}$ in Formula 1, can be defined as a function that maps domain feature vectors $x = [x_1, x_2,...,x_d] \\in R^d$ to class labels $y \\in \\{0,1\\}$, i.e.,\n$S_{ML}: R^d \\rightarrow \\{0,1\\}.$ (1)\nTypically, for a new input x, y represents the actual label, while $\\hat{y} = S_{ML}(X)$ denotes the label predicted by the ML software.\nBuilding on the effective solution to fairness challenges presented by Li et al. with the LTDD method [41], we developed FAIREDU to address scenarios involving multiple sensitive features. Pre-processing methods like LTDD allow for the correction of biases directly within the dataset, ensuring that the data used to train machine learning models is fair and unbiased from the outset. This method is particularly advantageous because it is model-agnostic [53], meaning it can be seamlessly integrated"}, {"title": null, "content": "with various types of machine learning algorithms without requiring modifications to the model architecture or training procedures.\nIn diference to the LTDD method, this approach specifically addresses the intersectionality of sensitive attributes such as gender, race, age, and disability, denoted as $X_1,...,X_k$, we will use multivariate regression to simultaneously eliminate the dependencies of each non-sensitive feature on all sensitive features. Mathematically, this is as defined in Formula 2\n$x_i = \\beta_0 + \\beta_1\\cdot X_1 + \\beta_2 \\cdot X_2 + \\cdots + \\beta_k \\cdot X_k + \\epsilon$ (2)\nBy employing this multivariate regression model, FAIREDU effectively detects and removes the dependencies of the remaining features on all specified sensitive features, thereby enhancing the fairness of machine learning systems in educational contexts. This method ensures a balanced consideration of multiple sensitive features, addressing the complexities introduced by intersectionality and reducing the risk of bias across different groups."}, {"title": "3.2. Overall architecture of FAIREDU", "content": "The architecture in Figure 1 represents the overall workflow of the FAIREDU model, which is designed to improve fairness in machine learning systems by addressing the dependencies between sensitive features and other features in the dataset."}, {"title": null, "content": "Here's a breakdown of how this figure works in combination with the previously generated explanation:\nDataset Preparation:\nThe process starts with the full dataset, which contains both sensitive and non-sensitive features.\nThe dataset is divided into two parts:\nTraining Set (85%): Used for training the model.\nTest Set (15%): Reserved for testing and evaluating the trained model on unseen data.\nRemove Association and Train (Training Set):\nIn the training set, the FAIREDU algorithm applies multivariate regression to identify and remove dependencies between non-sensitive features and multiple sensitive features (e.g., gender, race, age).\nWe assume have k sensitive features $X_1,...,x_k$. For each non-sensitive feature $x_i, k+1 \\leq i \\leq d$, we evaluate the association between the sensitive features $x_1,...,x_k$ and $x_i$ in the training dataset. It is worth noting that, since the association between some non-sensitive features and the sensitive feature may be trivial, we employ the Wald test with t-distribution to check whether the null hypothesis (that the slope b of the linear regression model is zero) holds. Specifically, we introduce the p-value of the Wald test to avoid unnecessary removing steps, i.e., consider \u201cp-value < 0.05\" as a precondition. If \u201cp-value < 0.05\" holds, we calculate the estimates $\\hat{a_i}$ and $b_i$ of the Multivariate-regression model, which are sorted in $E_a$ and $E_b$.\nThe multiple regression model is mathematical as defined in Formula 2:\nThe goal is to eliminate these dependencies and generate a new, bias-reduced dataset $X_{inew}$, such that: with each i, k + 1 \\leq i \\leq d then $X_{inew}$ as defined in Formula 3\n$X_{inew} = X_i - (\\beta_0 + \\beta_1\\cdot x_1 + \\cdots + \\beta_k \\cdot X_k)$ (3)\nThe adjusted training set is then used to train the machine learning model, resulting in a trained model.\nRemove Association (Test Set):"}, {"title": null, "content": "The same multivariate regression is applied to the test set, where associations between sensitive and non-sensitive features are removed before the model is tested. This ensures that the model does not learn biased relationships and can make fair predictions.\nEvaluation:\nThe trained model is evaluated on the bias-adjusted test set to assess both fairness and performance.\nThis step is critical to determine whether the removal of bias has maintained or improved the model's performance and whether it generalizes fairness across various sensitive features."}, {"title": "3.3. Algorithm", "content": "Based on the multivariate regression model, we propose the FAIREDU fair debugging algorithm, shown in Algorithm 1. FAIREDU method includes the following three steps:\nUsing multivariate regression, Identify the biased features and estimate their biased parts by evaluating the association between each insensitive feature and all sensitive features (lines 5 to 8)\nExclude the biased parts from the training samples. In this step, for any training sample, we perform the following two operators to remove bias: remove sensitive features (line 16) and modify insensitive feature values (lines 17 to 20)\nApply the same modification on the testing samples (lines 22 to 25), and use SML to predict the label of $x^{te}$ (line 26)."}, {"title": "4. Experiment Setup", "content": "In this section, we describe the data preparation for the experiment as well as the general experiment setup. The details of each dataset, including its name, number of variables/features, total valid data points, list of sensitive variables/ features, and their distribution are shown in Table 3."}, {"title": "4.1. Selection of Datasets", "content": "In this article, we use six popular data sets taken from Kaggle 1 and one data set collected from the IT department of Dai Nam University (DNU), Hanoi, Vietnam 2. Characters of these seven datasets are presented below.\nAdult dataset. This data set contains 48,842 samples with 14 features. The goal of the data set is to determine whether a person's annual income can be larger than 50k. This dataset has two sensitive features Gender and race [54].\nCOMPAS dataset. COMPAS is the abbreviation of Correctional Offender Management Profiling for Alternative Sanctions, which is a commercial algorithm for evaluating the possibility of a criminal defendant committing a crime again. The dataset contains the features used by the COMPAS algorithm to score the defendant and the judgment results within two years. There are over 7000 rows in this dataset, with two sensitive features Gender and race [55].\nDefault of Credit Card Clients (Default for short) dataset. This dataset aims to determine whether customers will default on payment through customers' information. It contains 30,000 rows and 24 features, including two sensitive features Gender and age [56].\nPredict students' dropout and academic success data set. This dataset contains data from a higher education institution on various features related to undergraduate students, including demographics, socioeconomic factors, and academic performance, to investigate the impact of these factors on student dropout and academic success. This dataset has two sensitive features Gender and Debtor. It contains 4,425 rows and 34 features. [57].\nStudent Performance dataset. This data approaches student achievement in secondary education of two Portuguese schools. The data features include student grades, demographic, social, and school-related features and it was collected by using school reports and questionnaires. This dataset has two sensitive features Gender and Health. It contains 395 rows and 33 features [58].\nOulad dataset. It contains data about courses, students, and their interactions with the Virtual Learning Environment (VLE) for seven selected courses (called modules). Course presentations start in February and October, marked by \u201cB\u201d and \u201cJ,\u201d respectively. The dataset consists of tables connected using unique identifiers. All tables are stored in the CSV format. This dataset contains 32,593 rows and 12 features, and it has two sensitive features Gender and Disability [59].\nDNU dataset. The data collected spanning over 11 courses, the 11 datasets collected belong to 3 different training programs, so the number of credits for each program and the courses within each program also vary. We have selected similar courses, using equivalent courses to replace different ones. After performing these steps, the new dataset includes 59 features and 411 samples. The normalized dataset consists of 42 features: 6 features about the identity information of students, and 33 features about their score, the remaining 3 features include average score, rating, and prediction labels (safety and risk). All features related to scores are the average scores of courses on a 10-point scale. This dataset has three sensitive features Gender, Birthplace (Zone), and Date of Birth."}, {"title": "4.2. Selection of models", "content": "In this paper, we conduct experiments on widely used machine learning models in educational applications, including Logistic Regression, Decision Trees, and Random Forests [60, 61, 62].\nLogistic regression (LR): is a statistical method used for binary classification problems, where the goal is to predict one of two possible outcomes. It's a type of regression analysis where the dependent feature is categorical [7].\nDecision Tree (DT): algorithm is a popular machine-learning method for classification and regression tasks. It operates by partitioning the dataset into smaller subsets and constructing a decision tree based on decision rules. Each node in the tree represents a feature, and each edge represents a value of that feature. The leaves of the tree correspond to labels or predicted values [63].\nRandom Forest (RF): algorithm is a structured machine-learning approach based on the concept of decision trees. However, instead of using a single"}, {"title": "5. Results", "content": "This section presents the experimental results aimed at addressing all the research questions outlined in, including RQ1 (Section 5.1), RQ2 (Section 5.2), RQ3 (Section 5.3), and RQ4 (Section 5.4)."}, {"title": "5.1. RQ1 - Is there a systematic bias present among sensitive features within educa-tional datasets?", "content": "The results of fairness levels, measured by $|1 \u2013 DI|$, for various sensitive features, including Gender, Race, Age, Disability (Disab.), Health, Debtor, and Birthplace were presented in Figure 2. The figure shows that the Gender feature shows the widest range of values, indicating significant variability in fairness across different contexts or datasets. The fairness value of Race and Age does not vary much. We can only collect one value point for each feature: Disability, Health, Debtor, and Birthplace. However, it can be seen that there are no patterns regarding the order of biasness among these sensitive features. To ensure a thorough evaluation of fairness, it is essential to take into account all sensitive features present in the dataset."}, {"title": "Note 5.1: Sensitive features in the educational datasets", "content": "The analysis reveals no consistent bias across sensitive features within the educational datasets. In other words, no single sensitive feature consistently demonstrates greater unfairness than the others."}, {"title": "5.2. RQ2 - Does the level of fairness vary across different machine learning methods?", "content": "Table 5 displayed the average fairness value across seven datasets for each sensitive feature with each ML method. We present four figures according to four fairness metrics, which are $|1 \u2013 DI|$, SPD, AOD, and EOD. In the first figure, we compared the $|1 - DI|$ value among three ML models: logistic regression, random forest, and decision tree. The result shows that overall logistic regression leads to a higher level of bias across the dataset. This observation is the same with different measures of fairness, including SPD, AOD, and EOD in the next figures. Decision Tree is the model with the lowest level of bias across different sensitive features.\nLogistic regression tends to be more sensitive to the presence of biased data because it applies the same linear weights across all instances. If the training data reflects historical biases or unequal distributions, the model will inherently reproduce and potentially amplify these biases. Moreover, the linear nature of logistic regression makes it prone to capturing and amplifying relationships between sensitive features, such as gender or race, and the target feature, leading to unfair outcomes among different groups. For example, if \"gender\" strongly influences the outcome, this model will reflect that difference."}, {"title": null, "content": "Decision trees split data at each node based on the optimal feature threshold that maximizes information gain (or minimizes impurity), considering local patterns. This flexibility allows decision trees to capture non-linear relationships and adapt to different contexts within the data. As a result, decision trees can better handle complex scenarios where biases might manifest differently in various subsets of the data, leading to lower overall bias levels.\nRandom forests, by using multiple decision trees, can offer similar or even better fairness, as they mitigate the influence of bias if any individual tree is skewed by a sensitive feature.\nWe also compare fairness measures. with $|1 \u2013 DI|$, debtor is the feature with the highest level of bias. However, with SPD, AOD, and EOD, disability is the feature with the highest level of bias. The debtor status shows the highest bias under $|1 \u2013 DI|$ likely because The \"debtor\" feature shows the highest level of bias because this measure is sensitive to the disparity in the proportion of positive outcomes between groups. In this case, the proportion of debtors (503/4425) is significantly lower compared to non-debtors, and the large difference in positive outcomes between these groups leads to a higher $|1 \u2013 DI|$. One note that, \"debtor\u201d only appears in the Student-Dropout-Predict dataset.\nOn the other hand, with SPD, AOD, and EOD, the \"disability\u201d feature exhibits the highest bias. This is likely due to the significant disparity in the ratio of disabled and non-disabled individuals (3164/32594), and the considerable difference in the model's ability to correctly predict positive outcomes for these two groups.\nThis highlights that different fairness metrics capture different aspects of fairness. 1- DI focuses on the distribution of positive outcomes across groups, while SPD evaluates the difference in positive prediction rates between groups, without considering accuracy. AOD and EOD, however, consider the true positive rate (TPR) and true negative rate (TNR), reflecting how balanced the model's predictions are across different groups. The disability feature shows the highest bias under SPD, AOD, and EOD because these measures capture different types of biases that go beyond simple outcome distributions:\nSPD (Statistical Parity Difference): Indicates a disparity in the overall likelihood of receiving a positive prediction between groups. If individuals with disabilities are less likely to receive positive outcomes regardless of their actual qualifications, SPD will detect this bias.\nAOD (Average Odds Difference): Evaluates the difference in error rates (false positives and false negatives) between groups. If a model is more likely to misclassify individuals with disabilities, this would lead to a high AOD.\nEOD (Equal Opportunity Difference): Focuses on the difference in true positive rates between groups. If individuals with disabilities who qualify for a positive outcome (e.g., job suitability or creditworthiness) are less likely to actually receive it, EOD will be high.\nAlternatively, Table 5 demonstrates that, for the same model, different fairness metrics yield varying results. For instance, the Disability feature exhibits greater fairness than the Debtor feature when evaluated using Disparate Impact (DI) and Statistical Parity Difference (SPD). However, when assessed through Average Odds Difference (AOD) and Equal Opportunity Difference (EOD), the Disability feature is found to be less fair.\nDisparate Impact (DI) and Statistical Parity Difference (SPD) assess fairness by comparing the percentage of favorable outcomes between groups, without considering the accuracy of predictions. In contrast, Average Odds Difference (AOD) and Equal Opportunity Difference (EOD) focus on the quality of predictions, evaluating fairness based on true positive or true negative rates across groups. As a result, optimizing fairness according to one metric can potentially compromise fairness according to another. For instance, improving fairness in terms of Equal Opportunity may require lowering the model's overall accuracy by adjusting decision thresholds to equalize true positive rates between groups.\nThis highlights the importance of selecting fairness measures that align with the specific context and goals of the analysis. If the objective is to ensure an equitable distribution of favorable outcomes, metrics like $|1-DI|$ or SPD would be appropriate. However, if the emphasis is on the accuracy of positive predictions for different groups, AOD and EOD offer a more meaningful evaluation. Given the potential trade-offs between these measures, the choice of fairness metric should be guided by the particular fairness goals in a given scenario."}, {"title": "Note 5.2: Fairness different across different ML methods", "content": "There are differences in bias level for different ML models. The LR model shows a greater risk of bias than the RF and DT models.\nThe Order of fairness level for different sensitive features differs for different fairness measures. If the focus is on the distribution of favorable outcomes across groups, DI and SPD serve as appropriate metrics. Contrary, if the objective is to examine the balance of predictions across outcome groups, AOD and EOD are more suitable.\nTo draw comprehensive conclusions about fairness, it is crucial to consider multiple metrics."}, {"title": "5.3. RQ3 - How does FAIREDU manage multiple sensitive features compared to cur-rent state-of-the-art methods?", "content": "To assess the fairness improvement of the FAIREDU method, we compared it against other state-of-the-art methods such as Reweighing, DIR, Fairway, FairSmote, and LTDD [41] across multiple machine learning models, including Logistic Regression, Random Forest, and Decision Tree. Table 6 presents the comparison across different methods and datasets presented in LTDD study [41]. The comparison results with the original model and other state-of-the-art models show that FAIREDU outperforms in most cases across the Adult, COMPAS, Default, and Student datasets. However, for $1 \u2013 DI|$ on the Compas_sex and Default_sex features, we fall slightly behind LTDD, but the difference is not significant (less than 0.1). Similarly, for SPD, our results are only marginally lower than LTDD, with a difference of less than 0.01."}, {"title": null, "content": "datasets, evaluating a total of 15 sensitive features. Each scenario was run 100 times to obtain average results, ensuring statistical significance and minimizing the impact of random fluctuations. We use the colored boxes to highlight the results (better or worse than the baseline). We also report the difference in percent change. In total, across 60 fairness comparisons (win/tie/loss), FAIREDU achieved 35 wins and 25 losses against LTDD. These results demonstrate that FAIREDU provides superior performance in most situations when compared to LTDD."}, {"title": null, "content": "Besides that, the results of applying the Random Forest (RF) model, include without intervention and with the Fairedu or LTDD interventions, are summarized in Figure 3. In this figure, color lines indicate fairness metric outcomes, where the lower line represents more improved fairness. As shown in Figure 3, the Fairedu intervention either outperformed or matched the performance of LTDD and the original model in most cases. Specifically, Fairedu surpass LTDD in 11 out of 15 cases based on the SPD measure, which covers the majority of scenarios tested across 7 datasets and 15 sensitive features listed in Table 3.\nSimilarly, for the Decision Tree (DT) model, we experiments conducted using the same 7 datasets, which are mentioned in Table 3, in cases without intervention, as well as with Fairedu and LTDD interventions. The results of the fairness metrics $|1 \u2013 DI|$ demonstrate that Fairedu outperformed LTDD in 9 out of 15 cases, which results are summaried in Figure 4, again constituting the majority of experimental scenarios.\nIn summary, across all three models: Logistic Regression (LT), Random Forest (RF), and Decision Tree (DT) the Fairedu method consistently yields positive results, showing superiority over the previous LTDD method in most cases."}, {"title": "Note 5.3: The Fairness of FAIREDU and state-of-the-art methods", "content": "FAIREDU has demonstrated superior fairness compared to other fairness enhancement methods in two key areas:\nSimultaneously addressing and improving fairness across multiple sensitive features within the dataset.\nFAIREDU has improved the equity indicators after the intervention, specifically:\n$|1 \u2013 DI|$ reduced up to 96.7% (wrt. Gender in Oulad set)\nSPD reduced up to 88.55% (wrt. Gender in Adult set)\nAOD reduced up to 85.79% (wrt. Race in Compas set)\nEOD reduced up to 84.87% (wrt. Gender in Oulad set)"}, {"title": "5.4. RQ4 - How effectively does FAIREDU balance fairness and model performance relative to state-of-the-art methods?", "content": "To evaluate how well FAIREDU balances fairness and model performance, we conducted statistical analyses on three models: Logistic Regression, Random Forest, and Decision Tree, applied across seven datasets and seven sensitive features. The comparison was made between three methods: no intervention (Origin), an intervention using LTDD, and the fairness intervention using FAIREDU, as shown in Table 8.\nModel Performance Impact: As shown in Table 8, FAIREDU's intervention aimed to improve fairness across all sensitive features, and the results indicate that overall model performance did not significantly decline. For accuracy (ACC), FAIREDU outperformed both Origin and LTDD in 9 out of 45 cases and tied in 13 cases. While it underperformed in 23 cases, the performance reduction was minimal, with the highest deviation being 5.71% in the Decision Tree model on the DNU-BP dataset. In this case, the accuracy decreased by no more than 0.056, highlighting that the performance drop was relatively small. For recall, FAIREDU outperformed the other methods in 4 cases, tied in 15 cases, and underperformed in 26 cases. Similar to accuracy, the deviations were not significant, with the largest reduction being 9.6% in the Logistic Regression model on the Adult dataset, where recall decreased by 0.096 at most.\nFairness-Performance Tradeoff: Despite these performance fluctuations, the application of FAIREDU demonstrated its effectiveness in improving fairness while minimally affecting model accuracy and recall. The small deviations in performance suggest that FAIREDU manages to maintain a balance between fairness and model effectiveness, which is crucial when implementing fairness interventions in practical applications. While fairness-focused interventions can sometimes lead to significant reductions in performance, FAIREDU shows that it is possible to enhance fairness with minimal compromises."}, {"title": "Note 5.4: The performance of FAIREDU and state-of-the-art methods", "content": "FAIREDU demonstrated no significant performance trade-off compared to the original model and other augmentation methods."}, {"title": "6. Discussion", "content": "This section summarizes how the results presented in Section 5 address each of the research questions posed in this paper. The findings provide valuable insights into the evaluation of fairness across multiple dimensions within educational datasets and demonstrate the effectiveness of FAIREDU as a fairness intervention. Below is a detailed breakdown of the answers to each research question:\nRegarding RQ1 (addressing in Subsection 5.1), the results confirm the absence of significant bias among sensitive features in the educational datasets. Despite the presence of features like disability, health status, debtor status, and birthplace in only a single dataset, features such as gender, race, and age do not exhibit consistent bias across datasets. This emphasizes the need to assess all sensitive features for their potential impact on fairness and highlights the importance of developing interventions that can address multiple sensitive features simultaneously within a single dataset.\nRegarding RQ2 (addressing in Subsection 5.2) show that different machine learning models yield varying fairness evaluations, even when applied to the same dataset and fairness metric. Decision Tree and Random Forest models, for example, demonstrate higher fairness compared to Logistic Regression models. Additionally, different fairness metrics produce different outcomes across models, highlighting the necessity of selecting appropriate fairness indices for each machine learning model to ensure accurate fairness assessments."}, {"title": "6.2. Limitations", "content": "While FAIREDU demonstrates promising capabilities in enhancing fairness across multiple sensitive features within educational datasets, several limitations concerning internal validity, external validity, construct validity, and conclusion validity must be acknowledged [66, 67, 68]. To ensure the validity of this study, we adhered to the validity guidelines from Runeson [67]."}, {"title": "6.2.1. Internal Validity", "content": "FAIREDU relies on multivariate linear regression to detect and eliminate dependencies between features and sensitive features. This linear assumption may limit the method's ability to capture non-linear relationships inherent in certain datasets, potentially leaving some residual biases unaddressed. In a relevant work by Li et al. [36], the authors compare the results of the linear regression and polynomial regression, showing a significantly better performance of linear regression than that of polynomial regressions.\nBesides, our evaluation focused on specific fairness metrics, and while these are widely recognized, they may not encompass all fairness dimensions relevant to every"}, {"title": "6.2.2. External Validity", "content": "The evaluation of FAIREDU was conducted using datasets specific to the education sector. Although chosen to represent various educational contexts, these datasets may not fully capture the diversity of real-world educational environments, leaving the effectiveness of FAIREDU in more diverse settings uncertain. Since our study relies on traditional ML algorithms (LR, RF, RT), the generalizability of our findings to more modern ML/AI approaches, such as Neural Networks, Deep Learning, etc, is limited."}, {"title": "6.2."}]}