{"title": "Exploring visual language models as a powerful tool in the diagnosis of Ewing Sarcoma", "authors": ["\u00c1lvaro Pastor-Naranjo", "Pablo Meseguer", "Roc\u00edo del Amor", "Jose Antonio Lopez-Guerrero", "Samuel Navarro", "Katia Scotlandi", "Antonio Llombart-Bosch", "Isidro Machado", "Valery Naranjo"], "abstract": "Ewing's sarcoma (ES), characterized by a high density of small round blue cells without structural organization, presents a significant health concern, particularly among adolescents aged 10 to 19. Artificial intelligence-based systems for automated analysis of histopathological images are promising to contribute to an accurate diagnosis of ES. In this context, this study explores the feature extraction ability of different pre-training strategies for distinguishing ES from other soft tissue or bone sarcomas with similar morphology in digitized tissue microarrays for the first time, as far as we know. Vision-language supervision (VLS) is compared to fully-supervised ImageNet pre-training within a multiple instance learning paradigm. Our findings indicate a substantial improvement in diagnostic accuracy with the adaption of VLS using an in-domain dataset. Notably, these models not only enhance the accuracy of predicted classes but also drastically reduce the number of trainable parameters and computational costs.", "sections": [{"title": "1 Introduction", "content": "Sarcomas are neoplasms that arise in the bone or soft tissues and represent more than 20% of pediatric malignant neoplasms [1]. Specifically, Ewing's sarcoma (ES) is a type of malignant bone tumor characterized by its rapid spread and aggressiveness. Although it is a cancer with a low incidence worldwide, its social impact is considerable, as it mainly affects children and adolescents [2]. The 5-year survival rate for ES is highly dependent on the extent of the tumor, decreasing from 68% in localized tumors to 39% when metastases are present. These statistics highlight the need for early diagnosis of ES tumors as their prognosis differs from other types of sarcomas, requiring specialized treatment [3].\nHistologic analysis of biopsies is a fundamental tool in the diagnosis of ES. The histology of ES predominantly comprises round cells, with smaller proportions of small ovoid and spindle cells arranged in a homogeneous pattern with few stromal tissue. Features are also present in the small round cell tumor (SRCT) family. The histopathologic study can be performed using tissue microarrays (TMA), including multiple circular micrometric sections of cylindrical biopsies (named cores), allowing their comparative analysis. This technique differs from whole biopsy sections that enable detailed observation of the entire tissue. With advances in digital pathology and the advent of specialized scanners, the digitization of histologic tissue samples from biopsies has allowed the implementation of computer vision algorithms based on artificial intelligence (AI).\nPreviously, deep learning algorithms have been applied to detect Ewing's sarcoma in pediatric radiography [4]. Regarding histological images, two recent works used machine learning to generate predictive models for the major histological subtypes of rhabdomyosarcoma (a subtype of sarcoma) [5], [6], [7]. In the realm of Ewing Sarcoma research, emphasis primarily revolves around classifications in binary tasks and prognostic evaluations of the cancer [8] rather than its classification into a multiclass problem. With this in mind, our contribution holds significant value as it fills this crucial gap. In concrete, this work proposes a new paradigm based on multiple instance learning (MIL) using histological images as the main source of data to accurately assess the diagnosis of Ewing's sarcoma on tissue microarrays and differentiate it from three other soft tissue or bone sarcomas: chondrosarcoma, gastrointestinal stroma tumor spindle or epithelioid variants and rhabdomyosarcoma. For this purpose, a frozen vision-language model specialized for computational pathology is used to extract high-level feature representations from patches corresponding to the cores. After that, a powerful embedding aggregator based on transformer is used to obtain the prediction at the core level. This methodology is compared with feature extractors that require training, such as the VGG-based architectures, showing the competitiveness of the proposed method with the added value of a highly reduced number of trainable parameters."}, {"title": "2 Related work", "content": null}, {"title": "2.1 Visual language models", "content": "Contrastive learning has emerged as a powerful pretraining technique for learning task-agnostic visual features from language supervision. This technique has shown to be a highly effective and scalable strategy to pre-train dual-encoder image-text models that can excel at a range of downstream visual recognition tasks. Representative works such as CLIP [9] and ALIGN [10] showed that by scaling to large, diverse web-source datasets of paired images and captions, we can train models capable of exhibiting fairly robust zero-shot transfer capabilities through the use of prompts that exploit the cross-modal alignment between image and text learned by the model during pretraining. In medical imaging, ConVIRT [11] considered paired chest X-ray images and reports for learning aligned visual language representation. Recent methods CONCH [12] and PLIP [13] have been used to learn visual representations for histopathology images using large datasets containing histopathology image-caption pairs from pathology textbooks, PubMed research articles and Twitter."}, {"title": "2.2 Multiple instance learning", "content": "In the Multiple Instance Learning (MIL) framework, instances are organized into bags, with labels only assigned at the bag level. According to the standard MIL assumption, a bag is considered positive if it contains at least one instance belonging to the positive class. Among other tasks, it has been used to detect breast cancer [14] and grade local patterns in prostate cancer [15]. This assumption makes sense when the labels at the pixel/region label directly affect the image label. However, there are cases where this is not true. In this case, the whole image outcome combines features among the different patches [16], called the bag-embedding MIL. The most common technique is to obtain the bag-level representation by instance-level aggregation of features extracted from each instance by a backbone. The feature extraction is frequently performed with pre-trained networks, transfer learning [17], and, more interestingly, following contrastive learning. Then, the aggregation of the patch features results in the bag embedding. The most straightforward and non-trainable aggregation techniques are batch global average (BGAP) [18] and batch global max pooling (BGMP) [14]. Other aggregation techniques include trainable parameters, such as weighted embeddings based on attention [19] or recurrent neural networks (RNN) [20]. Recently, authors in [21] proposed a Transformed-based correlated MIL (TransMIL) that considers the morphological and spatial correlation between instances."}, {"title": "3 Dataset", "content": "The database used in this study comprises circular micrometric sections of cylindrical biopsies extracted from patients with a genetic confirmation. These samples, followed by histological techniques, were placed in tissue microarrays and digitized with digital pathology scanners to obtain digitized histological images. The database is constituted of different types of sarcomas, including Ewing's sarcoma (EWING), chondrosarcoma (COND), gastrointestinal stroma tumor (GIST) and Rhabdomyosarcoma (RHABDO)."}, {"title": "4 Methods", "content": "An overview of our proposed method is depicted in Figure 2. In the following, we describe the problem formulation and each proposed component.\nProblem Formulation. Under the paradigm of MIL, instances are grouped in bags $X = {X_n}_{n=1}^{N}$ that exhibit neither dependency nor ordering among them, and its number N is arbitrary for each bag. In the multi-class scenario, each bag is a member of one of K mutually exclusive classes, such that $Y_k \\in {0,1}$. Note that, in contrast to other MIL formulations, the individual instances do not have an associated label, but rather, the label of the bag is determined by the combination of features of the different instances."}, {"title": "Patch-level feature extraction.", "content": "In the MIL paradigm, the initial step involves patch-level feature extraction. Typically, convolutional neural networks (CNN) architectures, such as VGG16 pre-trained on ImageNet, emerge as the most commonly employed algorithms for this task [16]. However, the domain in which these networks are pre-trained significantly differs from the histological domain, see Figure 2 (A.). Fine-tuning such networks for histopathological tasks requires large annotated datasets, which can be challenging to obtain in the medical field due to the time-consuming nature of annotation processes. This underscores the practical limitations of adapting pre-trained networks to histological image analysis tasks within the medical domain. To overcome this limitation, we use Pathology language-image pretraining (PLIP), a multimodal artificial intelligence with both image and text understanding, which is trained on OpenPath, a dataset of paired histopathology image-captions [13]. Integrating comprehensive natural language annotations into the learning process increases the capacity to understand image-based semantic knowledge, empowering it to perform a wide range of downstream tasks, see Figure 2 (B.).\nLet us denote a feature extractor, $f_\\theta() : X \\rightarrow Z$, which projects instances $x \\in X$ to a lower dimensional manifold $z \\in F^{C \\times R^d}$, with d the embedding dimension."}, {"title": "Embedding-based MIL", "content": "We aim to train a model capable of predicting bag-level labels using a combination of embedding extracted at the instance level. This learning strategy falls under the embedding-based MIL paradigm. We define an aggregation, $f_a(\\cdot)$, which combines the instance-level projections into a global embedding, Z. To take into account morphological and spatial information, we use a transformer-based aggregator [21]. Finally, a neural network classifier, $f_i() : Z \\rightarrow S$, is in charge of predicting softmax bag-level class scores, $S_k$, such that $S_k \\in [0,1]$. The optimization of the model parameters is driven by the minimization of standard categorical cross-entropy loss between the reference labels and predicted scores such that:\n$L_{ce} = -\\frac{1}{K} \\sum_{k=1}^{K} Y_k log(S_k)$ (1)"}, {"title": "5 Results and Discussion", "content": null}, {"title": "5.1 Implementation details", "content": "The experiments were conducted on the NVIDIA DGX A100 system, with all settings aimed at minimizing cross-entropy during training, incorporating weights that were inversely proportional to the number of instances per class. This approach was aimed at accentuating the impact of misclassifications on minority class images, thereby addressing the challenge of class imbalance. As for the optimizer, AdamW and a learning rate $\\eta$ between 1-5 and 5-5 were used, depending on the configuration. In terms of batch size, 1 sample per batch was chosen. The database partitioning into training, validation, and test sets was conducted using the following proportions: 60% for training, 15% for validation, and 25% for testing. Model evaluation was carried out employing metrics including sensitivity (SEN), precision (PREC), accuracy (ACC), and the F1-score (F1S)."}, {"title": "5.2 Ablation experiments", "content": null}, {"title": "1. Comparative Analysis of Feature Extractors", "content": "In this section, we present a comparative analysis between PLIP (Pathology Language and Image Pre-Training) and VGG16 regarding their feature representation capabilities. In Figure 3, it is shown that feature extraction using PLIP yields notably enhanced class separability compared to VGG16. PLIP's ability to leverage both visual and textual modalities appears to provide a more discriminative representation space, thus facilitating clearer boundaries between different classes. This emphasizes the potential of vision-language models in enhancing feature representation and classification performance in diverse image analysis tasks."}, {"title": "1. Feature Aggregation Techniques", "content": "We explore various embedding aggregation strategies within the MIL framework: batch global average pooling (BGAP), max pooling (BGMP), attention mechanisms (MILAtt), and a transformer-based aggregator (TransMIL), see Figure 4.\nAcross experiments employing three feature extraction models, fine-tuned VGG (VGG FT), frozen VGG16 (VGG F), and PLIP, the latter consistently demonstrated superior performance when integrated with a transformer-based aggregation method incorporating spatial information. Notably, even with fine-tuning on target domain data, VGG failed to surpass PLIP's efficacy. Our findings underscore a significant advantage: the ability to maintain a frozen feature extractor while focusing training efforts on a robust aggregator. This approach not only enhances computational efficiency but also reduces the time and cost associated with training algorithms on cores. Considering that the batch size used is one sample and that the most powerful aggregators were used in each case, the fine-tuned VGG takes 0.112 seconds to process the image and update the weights, while using PLIP, this value is reduced to 0.0155 seconds, almost 10 times less. This relation is also true for trainable parameters, 29.7 million for VGG with BGAP versus 2.6 million for PLIP with TransMIL. This methodological insight contributes to advancing MIL tasks and offers practical implications for resource-efficient model training in histopathological image analysis."}, {"title": "5.3 Test Results", "content": "In the test evaluation, we present the performance results for each feature extractor utilizing their optimal aggregation methods determined during the validation phase: VGG16 (Fine-Tuned) with BGAP, VGG16 with BGAP, and PLIP with TransMIL, see Table 2 and Figure 5."}, {"title": "6 Conclusion", "content": "In this study, we pioneered the identification of various sarcomas, including Ewing's sarcoma, using histological images through a multiple-instance learning methodology. Furthermore, our investigation highlights the potential of Vision-Language models, which integrate image and textual information during training to improve visual feature learning and efficient model adaptation. We have demonstrated that leveraging such models as frozen patch-level feature extractors and powerful embedding aggregators, such as those based on transformers, surpasses traditional supervised approaches like fine-tuning. Additionally, future lines aim to further advance this field by integrating data augmentation techniques within the MIL framework."}]}