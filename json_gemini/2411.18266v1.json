{"title": "Wearable intelligent throat enables natural speech in stroke patients with dysarthria", "authors": ["Chenyu Tang", "Shuo Gao", "Cong Li", "Wentian Yi", "Yuxuan Jin", "Xiaoxue Zhai", "Sixuan Lei", "Hongbei Meng", "Zibo Zhang", "Muzi Xu", "Shengbo Wang", "Xuhang Chen", "Chenxi Wang", "Hongyun Yang", "Ningli Wang", "Wenyu Wang", "Jin Cao", "Xiaodong Feng", "Peter Smielewski", "Yu Pan", "Wenhui Song", "Martin Birchall", "Luigi G. Occhipinti"], "abstract": "Wearable silent speech systems hold significant potential for restoring communication in patients with speech impairments. However, seamless, coherent speech remains elusive, and clinical efficacy is still unproven. Here, we present an AI-driven intelligent throat (IT) system that integrates throat muscle vibrations and carotid pulse signal sensors with large language model (LLM) processing to enable fluent, emotionally expressive communication. The system utilizes ultrasensitive textile strain sensors to capture high-quality signals from the neck area and supports token-level processing for real-time, continuous speech decoding, enabling seamless, delay-free communication. In tests with five stroke patients with dysarthria, IT's LLM agents intelligently corrected token errors and enriched sentence-level emotional and logical coherence, achieving low error rates (4.2% word error rate, 2.9% sentence error rate) and a 55% increase in user satisfaction. This work establishes a portable, intuitive communication platform for patients with dysarthria with the potential to be applied broadly across different neurological conditions and in multi-language support systems.", "sections": [{"title": "Main", "content": "Neurological diseases such as stroke, amyotrophic lateral sclerosis (ALS), and Parkinson's disease frequently result in dysarthria\u2014a severe motor-speech disorder that compromises neuromuscular control over the vocal tract. This impairment drastically restricts effective communication, lowers quality of life, substantially impedes the rehabilitation process, and can even lead to severe psychological issues [1, 2, 3, 4]. Augmentative and alternative communication (AAC) technologies have been developed to address these challenges, including letter-by-letter spelling systems utilizing head or eye tracking [5, 6, 7, 8] and neuroprosthetics powered by brain-computer interface (BCI) devices [9, 10, 11, 12]. While head or eye tracking systems are relatively straightforward to implement, they suffer from slow communication speeds. Neuroprosthetics, while transformative for severe paralysis cases, often rely on invasive, complex recordings and processing of neural signals. For individuals retaining partial control over laryngeal or facial muscles, a strong need remains for solutions that are more intuitive and portable (SNote 1).\n\nA promising solution lies in wearable silent speech devices that capture non-acoustic signals, such as subtle skin vibrations [13, 14, 15, 16, 17] or electrophysiological signals from the speech motor cortex [18, 19, 20, 21]. These technologies offer non-invasiveness, comfort, and portability, with potential for seamless daily integration. Yet, despite their promise, current systems remain in their infancy, achieving reliable, discrete word decoding in healthy users but showing limited success in patient trials [13, 14, 15]. More critically, these systems fall short of delivering truly natural communication\u2014requiring both delay-free expression and consistent contextual coherence, capabilities essential for fully effective and meaningful interactions.\n\nTo advance wearable silent speech systems for real-world dysarthria patient use, we developed an AI-driven intelligent throat (IT) system that captures extrinsic laryngeal muscle vibrations and carotid pulse signals, integrating silent speech and emotional states analysis in real-time. The system generates personalized, contextually appropriate sentences that accurately reflect patients' intended meaning (Figure 1). It employs ultrasensitive textile strain sensors, fabricated using advanced printing techniques, to ensure comfortable, durable, and high-quality signal acquisition [14, 22]. By analyzing speech signals at the token level (~100ms), our approach outperforms traditional time-window methods, enabling continuous, fluent word and sentence expression in real time. Knowledge distillation further reduces computational latency by 76%, significantly enhancing communication fluidity. Large language models (LLMs) serve as intelligent agents, automatically correcting token classification errors and generating personalized, context-aware speech by integrating emotional states and environmental cues. Pre-trained on a dataset from 10 healthy individuals, the system achieved a word error rate (WER) of 4.2% and a sentence error rate (SER) of 2.9% when fine-tuned on data from five dysarthric stroke patients. Additionally, the integration of emotional states and contextual cues further personalizes and enriches the decoded sentences, resulting in a 55% increase in user satisfaction and enabling dysarthria patients to communicate with fluency and naturalness comparable to that of healthy individuals. STable 1 provides a comprehensive comparison between the IT system and state-of-the-art wearable silent speech systems."}, {"title": "Results", "content": "The intelligent throat system\n\nThe IT system consists primarily of hardware (a smart choker embedding textile strain sensors and a wireless readout printed circuit board (PCB)) and software components (machine learning models and LLM agents). Silent speech signals generated in real time by the user's silent expressions are decoded by a token decoding network and synthesized into an initial sentence by the token synthesis agent (TSA). Simultaneously, pulse signals are collected from the smart choker device and processed by an emotion decoding network to determine the user's real-time emotional status. The sentence expansion agent (SEA) intelligently expands the TSA-generated sentence, incorporating personalized emotion labels and objective contextual background data to produce a refined, emotionally expressive, and logically coherent sentence that captures the user's intended meaning (Fig. 1, SVideo 2). Each component of the IT system is elaborated upon in the following sections.\n\nFig. 2a shows the structure of the strain sensing choker screen-printed on an elastic knitted textile. The choker features two channels located at the front and side of the neck, designed to monitor the strain applied to the skin by the muscles near the throat and the carotid artery (SFig. 1). The graphene layer printed on the textile forms ordered cracks along the stress concentration areas of the textile lattice to detect subtle skin vibrations [14]. Silver electrodes are connected to the integrated PCB on the choker. A rigid strain isolation layer with high Young's modulus is printed around each channel to reduce crosstalk between the two channels and the variable strains caused by wearing. Due to the difference in Young's modulus between the elastic textile substrate and the strain isolation layer, less than 1% of external strain is transmitted to the interior when wearing the choker, while the internal sensing areas remain soft and elastic (SFig. 2) [22]. For uniaxial stretching from 1-10 Hz, the printed textile-based graphene strain sensor shows good linear behaviour, producing a response over 10% to subtle strains of 0.1%, and maintains a gauge factor (GF) over 100 during high-frequency stretching (Fig. 2b). Furthermore, our previous studies have confirmed the reliability of the printed textile-based strain sensors with high robustness, durability and washability, as well as high levels of comfort, biocompatibility and breathability [14, 22].\n\nTo operate the system and enable wireless communication between the IT choker and server, the PCB was designed for bi-channel measurements (i.e., silent speech and carotid pulse signals), enabling simultaneous acquisition of speech and emotional cues. The PCB integrates a low-power Bluetooth module (Fig. 2c) for continuous data transmission while optimizing energy efficiency for extended use. Key components of the PCB include an analog-to-digital converter (ADC) for high-fidelity signal digitization and a microcontroller unit (MCU) that manages data processing and transmission (Fig. 2e, SFig. 4, and SFig. 5). Power supply, operational amplifiers, and the reference voltage chip are configured to ensure stable signal amplification, catering to the sensitivity requirements of both strain and pulse sensors. For the energy management system, a comprehensive power budget analysis reveals that the designed PCB operates with a total power consumption of 76.5 mW (Fig. 2f). The main power-consuming components are the Bluetooth module (29.7 mW) and amplification circuits (31.9 mW). To extend operational time and support portable use, a 1800 mWh battery was incorporated, providing sufficient capacity for continuous operation thoughout an entire day without recharging.\n\nToken-level speech decoding"}, {"title": "Token-level speech decoding", "content": "Current wearable silent speech systems operate by recognizing discrete words or predefined sentences and lack the ability for continuous, real-time expression analysis typical of the human brain [45]. This limitation arises because these systems rely on fixed time windows (typically 1-3 seconds) for word decoding, requiring users to complete each word within a set interval and pause until the next window to continue [13-21]. Such constraints lead to fragmented expression and unnatural user experience. To address this, we developed a high-resolution tokenization method for signal segmentation (Fig. 2f), dividing speech signals into fine-grained ~100ms segments for continuous word label recognition. This granular segmentation ensures that each token accurately corresponds to a specific part of a single word and is labeled accordingly. This setup enables users to speak fluidly without worrying about timing constraints, as the system continuously classifies and aggregates tokens into coherent words and sentences. Our optimization determined that a token length of 144 ms offers the ideal balance: it minimizes boundary confusion from longer tokens while avoiding the increased computational demands associated with shorter tokens.\n\nWhile high-resolution tokenization improves fluidity, shorter tokens inherently contain limited context, making them less effective for accurate word decoding. Temporal machine learning models, like recurrent neural networks (RNN) or transformers, could capture contextual dependencies, but their complexity and computational cost render them suboptimal for wearable silent speech systems [23, 24, 25], which prioritize real-time operation. To balance context awareness and computational efficiency, we implemented an explicit context augmentation strategy (Fig. 3a), where each sample consists of N tokens: N-1 preceding tokens provide context, and the current token determines the sample's label. For initial tokens, any missing preceding tokens are padded with blank tokens to ensure completeness. We found N=15 tokens to be optimal (Fig. 3c), with accuracy initially increasing as tokens accumulate, then declining due to insufficient context at lower counts and gradient decay or information loss at higher counts [26]. This strategy enables the use of efficient one-dimensional convolutional neural networks (1D-CNNs) instead of computationally intensive temporal models for token decoding [27, 28]. Attention maps reveal that signals from preceding regions indeed contribute to token decoding, validating the effectiveness of the explicit context augmentation strategy (SFig.10).\n\nTo further enhance model efficiency and accuracy on patients' data, we designed the training pipeline shown in Fig. 3b. The model was pre-trained on a larger dataset from healthy individuals and then fine-tuned on the limited patients' data, leveraging shared signal features to enhance patient-specific decoding. After only 25 repetitions per word in few-shot learning, the model achieved a token classification accuracy of 92.2% (Fig. 3d). In contrast, a model trained from scratch using solely patients' data could only reach an accuracy of 79.8%. Additionally, we employed response-based knowledge distillation [29] to transfer knowledge from a larger 1D ResNet-101 model to a smaller 1D ResNet-18, reducing computational load by 75.6% while maintaining high accuracy, with only a 0.9% drop from the teacher model, achieving 91.3% (Fig. 3e). Fig. 3f and Fig. 3g display the confusion matrix and UMAP feature visualization for token decoding [30]. Over 90% of the classification errors involved confusion between class 0 (blank tokens) and neighbouring word tokens. As shown in later analyses of the LLM agent's performance, such boundary errors can be effectively corrected during token-to-word synthesis by the token synthesis agent (TSA)."}, {"title": "Decoding of emotional states", "content": "To enrich sentence coherence by providing emotional context, we decode emotional states from carotid pulse signals. Emotional state recognition can typically be achieved through a variety of methods, including analysis of facial images from cameras, audio speech signals, and various physiological indicators such as heart rate and blood pressure [31, 32, 33]. In line with our objective of creating a highly integrated wearable system, we chose carotid pulse signals as a biomarker for emotional decoding. Using 5-second windows, we segmented patients' pulse signals into samples to construct a dataset, focusing on three common emotion categories for stroke patients: neutral, relieved, and frustrated (data collection protocol detailed in Methods). Fig. 4a shows the discrete Fourier transform (DFT) distributions for each emotion, highlighting distinct frequency characteristics among these emotional states. Accordingly, we incorporated DFT frequency extraction into the decoding pipeline shown in Fig. 4b, where removal of the DC component, Z-score normalization, and DFT are sequentially applied before feeding the values into a classifier for categorization. Fig. 4c illustrates the performance of different classifiers with and without DFT frequency extraction. The results show a significant improvement in decoding accuracy with DFT. The optimal model was the 1D-CNN with DFT, achieving an accuracy of 83.2%, with its confusion matrix displayed in Fig. 4d. The SHAP values reveal that the emotion decoding model primarily focuses on low-frequency signals in the 0-2 Hz range, which is consistent with the pulse signal range demonstrated by the DFT (SFig. 11).\n\nIn addition to the silent speech and carotid pulse signals analyzed in this study, various physiological activities generate distinct vibrational signals in the neck area, which can introduce artefacts hindering analysis [34, 35]. Fig. 4e shows the frequency and magnitude distributions of several prominent signals in this region. Our observations revealed that silent speech exhibits a relatively strong magnitude, and in applications with the IT, vibration can propagate transversely from the throat center to the carotid artery, introducing crosstalk in the pulse signal. Due to the considerable frequency overlap between silent speech and pulse signals, digital filters are non-ideal for effective artefacts suppression [36]. While adding reference channels could theoretically help, it does not align with the goal of a highly integrated IT [37]. To address this issue, we employed a stress isolation treatment using a polyurethane acrylate (PUA) layer, as shown in Fig. 2a, to prevent strain crosstalk propagation along the IT. The theoretical basis of this isolation strategy has been thoroughly discussed in our previous study [22]. Fig. 4f compares pulse signals with and without strain isolation treatment when silent speech occurs concurrently (the vowel \u201ca\u201d introduced at 2.5s), demonstrating significant crosstalk resilience in the treated IT."}, {"title": "LLM agents for sentence synthesis and intelligent expansion", "content": "To naturally and coherently synthesize sentences that accurately reflect the patient's intended expression from the decoded token and emotion labels, we introduced two LLM agents based on the GPT-40-mini API (Fig. 5a): the token synthesis agent (TSA) and the sentence expansion agent (SEA). The TSA merges token labels directly into words silently expressed by the patient and combines them into sentences (left). The SEA, on the other hand, leverages emotion labels and objective information, such as time and weather, to expand these basic sentences into logically coherent, personalized expressions that better capture the patient's true intent. Through a simple interaction (in this study, two consecutive nods), the IT system enables seamless switching between the direct output and the enriched, expanded sentence.\n\nTo optimize the performance of the TSA, we refined the prompt design [38]. First, we optimized the prompt length (Fig. 5b), observing a trend where both WER and SER improved with increasing prompt length up to 400 words before eventually deteriorating for higher lengths. We attribute this trend to the fact that longer prompts provide clearer synthesis instructions, but overly lengthy prompts dilute the model's focus ability. Additionally, we compared performance with and without example cases, where the agent was provided with five examples of token label sequences and their corrected word outputs. Including examples significantly improved synthesis accuracy (Fig. 5c). Finally, we evaluated the effect of providing empirical constraints, which specify typical token counts for words of various lengths. Performance improved considerably when constraints were included. Under optimal prompt conditions, TSA achieved its best performance with a WER of 4.2% and an SER of 2.9%.\n\nWe also assessed and refined the performance of the SEA. Patient satisfaction with the expanded sentences was evaluated through a questionnaire (see STable 4 for criteria details). Following Chain-of-Thought (CoT) optimization [39] and the inclusion of patient-provided expansion examples, the expanded sentences scored significantly higher across multiple criteria (Fig. 5f). Contribution analysis revealed that emotion labels made a substantial impact on emotion accuracy, while objective information notably improved fluency, jointly contributing to the overall satisfaction with the expanded sentences compared to the basic word-only output (Fig. 5e). Under optimal prompt conditions, the SEA-generated expanded sentences resulted in a 55% increase in overall patient satisfaction compared to the TSA's direct output, raising satisfaction from \u201csomewhat satisfied\u201d to \u201cfully satisfied\u201d levels (SFig. 12 and SFig. 13).\n\nIn both operating modes, sentences generated by the TSA and SEA agents are sent to an open-source text-to-speech model [44], which synthesizes audio that matches the patient's natural voice for playback. In real-world applications, the delay between the completion of the user's silent expression and the sentence playback is approximately 1 second (SNote 2). This low latency effectively supports seamless and natural communication in practical settings."}, {"title": "Discussion", "content": "In this work, we introduce the IT, an advanced wearable system designed to empower dysarthric stroke patients to communicate with the fluidity, intuitiveness, and expressiveness of natural speech. Comprehensive analysis and user feedback affirm the IT's high performance in fluency, accuracy, emotional expressiveness, and personalization. This success is rooted in its innovative design: ultrasensitive textile strain sensors capture rich and high-quality vibrational signals from the laryngeal muscles and carotid artery, while high-resolution tokenized segmentation enables users to communicate freely and continuously without expression delays. Additionally, the integration of LLM agents enables intelligent error correction and contextual adaptation, delivering exceptional decoding accuracy (WER < 5%, SER < 3%) and a 55% increase in user satisfaction. The IT thus sets a new benchmark in wearable silent speech systems, offering a naturalistic, user-centered communication aid.\n\nFuture efforts in several key areas will guide the continued development of the IT system. First, expanding its adaptability to a wider range of neurological conditions and demographic groups will make the technology more inclusive. Second, enhancing its linguistic diversity and multilingual support will allow for more personalized communication across language barriers. Finally, miniaturizing the system within an edge computing framework will facilitate seamless integration into real-world settings, boosting usability and accessibility.\n\nLooking ahead, the advantages of the IT extend beyond enhancing everyday communication; they contribute to the holistic health of neurological patients, encompassing both physical and psychological well-being. The regained fluency in communication allows patients to re-engage in social interactions, reducing isolation and the associated risk of depression. Moreover, effective communication facilitates real-time, personalized adjustments by rehabilitation therapists, supporting patients' recovery from motor impairments like hemiplegia. Together, these capabilities position the IT as a comprehensive tool for restoring independence and improving quality of life for individuals with neurological conditions."}, {"title": "Methods", "content": "Materials\n\nTIMREX KS 25 Graphite (particle size of 25 \u03bcm) was sourced from IMERYS. Stretchable conductive silver ink was obtained from Dycotec Materials Ltd. Ethyl cellulose was purchased from SIGMA-ALDRICH. Flexible UV Resin Clear was acquired from Photocentric Ltd. The textile substrate, composed of 95% Polyester and 5% spandex, was procured from Jelly Fabrics Ltd.\n\nInk formulation\n\nThe graphene ink for screen printing was prepared following a reported method. Briefly, 100g of graphite powder and 2g of ethyl cellulose (EC) were mixed in 1L of isopropyl alcohol (IPA) and stirred at 3000 rpm for 30 minutes. The mixture was then added into a high-pressure homogenizer (PSI-40) at 2000 bar pressure for 50 cycles to obtain graphene dispersion. The graphene dispersion is centrifuged at 5000g for 30 min to remove unexfoliated graphite.\n\nFabrication of textile strain sensor\n\nThe textile substrate was washed with detergent, thoroughly dried, and then treated with UV-ozone for 5 minutes to clean the surface. Screen printing was performed using a 165T polyester silk screen on a semi-automatic printer (Kippax & Sons Ltd.) set with a squeegee angle of 45 degrees, a spacer of 2 mm, a coating speed of 10 mm/s, and a printing speed of 40 mm/s. Graphene ink, silver paste, and PUA were successively printed to form the sensing layer, electrodes, and strain isolation layer, respectively. After printing the PUA, the textile was exposed to UV light for 5 minutes. After each printing pass, the textile was air-dried. Following printing, the sensor was dried at 80 \u00b0C overnight. A biaxial strain of approximately 10% was then applied to induce the formation of ordered cracks.\n\nCharacterization\n\nScanning Electron Microscopy (SEM) images were taken with a Magellan 400, after sputtering the textile samples with a 5 nm layer of gold to enhance conductivity. Optical images were captured using an Olympus microscope. Tensile properties of the textile strain sensors were evaluated using a Deben Microtest 200N Tensile Stage and an INSTRON universal testing system. Electrical signals were recorded concurrently with a potentiostat (EmStat4X, PalmSens) and a multiplexer (MUX, PalmSens). Copper tape was crimped onto the contact pads of the samples, supplemented with a small amount of silver paste to improve electrical contact.\n\nWireless PCB for data readout\n\nA custom wireless PCB was developed for efficient, continuous data acquisition and transmission within the IT system. Powered by a TP4065 lithium charger and a 3.3V regulator, the PCB ensures stable operation via battery or USB. The STM32G431 microcontroller captures silent speech and carotid pulse signals through two ADC channels, with an OPA2192 operational amplifier for high-precision signal conditioning, amplifying low-level signals and enhancing overall data fidelity. A BLE module (BLE-SER-A-ANT) transmits real-time data via UART, enabling seamless, delay-free communication.\n\nSilent speech data acquisition\n\nWe recruited 10 healthy subjects (mean age: 25.3 \u00b1 4.1 years; 6 males, 4 females) and 5 stroke patients with dysarthria (mean age: 43.9 \u00b1 8.3 years; 4 males, 1 female) for silent speech signal collection, in compliance with Ethics Committee approval from the First Affiliated Hospital of Henan University of Chinese Medicine, approval no. 2023HL-142-01. A corpus was developed consisting of 47 Chinese words commonly used by stroke patients in daily communication, along with 20 sentences constructed from these words (see STable 2 and STable 3). For the healthy subject dataset, we collected 100 repetitions per word and 50 repetitions per sentence. For the patient dataset, we gathered 50 repetitions per word and 50 per sentence.\n\nThe healthy subject data serves as a critical baseline for initial model training, enabling the model to establish foundational patterns in silent speech signals. This pre-training facilitates improved generalization and performance when later fine-tuning the model on the limited data from dysarthric patients, ultimately enhancing decoding accuracy and robustness in patient-specific applications. The silent speech signals were segmented into tokens at 144 ms intervals. Each token was combined with the preceding 14 tokens to form a sample, allowing the model to incorporate context. The sample's label corresponds to the word of the current token. The signals were originally recorded at a sampling rate of 10 kHz and subsequently downsampled to 1 kHz before tokenization. Before neural network analysis, each sample was uniformly preprocessed with detrending and z-score normalization.\n\nProtocol for emotion data collection\n\nEmotional pulse data was collected concurrently with silent speech signals, ensuring synchronized datasets that capture both speech-related and underlying physiological responses. To achieve accurate labeling, each emotion\u2500neutral, relieved, and frustrated was elicited through a carefully structured protocol involving audio-induced emotional states [40, 41, 42]. The emotions were induced via the international affective digitized sounds (2nd Edition; IADS-2) [43]. The three emotions were chosen as they are the most frequently encountered emotions in dysarthric patients' daily communication. Labeling was verified through collaboration between the participants and the therapist to ensure the successful and reliable induction of each target emotion. To balance sufficient information within each window and achieve the necessary resolution for emotion detection, pulse signals were segmented into 5-second samples. A 50% window overlap was applied to increase the training set size, enhancing model learning and generalization. The signals were originally recorded at a sampling rate of 10 kHz and subsequently downsampled to 200 Hz before analysis.\n\nSoftware environment and model training\n\nSignal preprocessing was performed on a MacBook Pro equipped with an M1 Max CPU. Network training was conducted using Python 3.8.13, Miniconda 3, and PyTorch 2.0.1 in a performance-optimized environment. Training acceleration was enabled by CUDA on NVIDIA A100 GPU. The detailed training parameters for all models can be found in SFig. 8 and SFig. 9."}]}