{"title": "Advice for Diabetes Self-Management by ChatGPT Models: Challenges and Recommendations", "authors": ["Waqar Hussain", "John Grundy"], "abstract": "Given their ability for advanced reasoning, extensive contextual understanding, and robust question-answering abilities, large language models have become prominent in healthcare management research. Despite adeptly handling a broad spectrum of healthcare inquiries, these models face significant challenges in delivering accurate and practical advice for chronic conditions such as diabetes.\n\nWe evaluate the responses of ChatGPT versions 3.5 and 4 to diabetes patient queries, assessing their depth of medical knowledge and their capacity to deliver personalized, context-specific advice for diabetes self-management. Our findings reveal discrepancies in accuracy and embedded biases, emphasizing the models' limitations in providing tailored advice unless activated by sophisticated prompting techniques. Additionally, we observe that both models often provide advice without seeking necessary clarification, a practice that can result in potentially dangerous advice. This underscores the limited practical effectiveness of these models without human oversight in clinical settings.\n\nTo address these issues, we propose a commonsense evaluation layer for prompt evaluation and incorporating disease-specific external memory using an advanced Retrieval Augmented Generation technique. This approach aims to improve information quality and reduce misinformation risks, contributing to more reliable AI applications in healthcare settings. Our findings seek to influence the future direction of AI in healthcare, enhancing both the scope and quality of its integration.", "sections": [{"title": "Introduction", "content": "Diabetes affects over one million people in Australia and represents a significant global health challenge that necessitates daily self-management [3]. Effective self-management and patient-centered education improve clinical outcomes and substantially reduce healthcare costs related to diabetes and its complications [7,17,32]. Artificial intelligence-based applications are known to support self-management by advising on exercise, diet control, and glycemic control, thereby improving patient outcomes [16, 31, 40, 44].\n\nThe advancements and proliferation of Large Language Models (LLMs) such as ChatGPT, with advanced conversational abilities, extensive medical knowledge, and proficiency in scenario-based learning, mark them as promising tools for patient advice in health management. ChatGPT-4, in particular, has shown strong performance on medical benchmarks like the United States Medical Licensing Examination (USMLE), the Medical Knowledge Self-Assessment Program (MKSAP) exams, and MultiMedQA, showcasing a solid foundation in healthcare knowledge [37]. Recent studies underscore ChatGPT's capabilities, comparable to those of a third-year medical student, providing consistent triage recommendations across diverse patient demographics [14,24]. This performance highlights the significant potential of LLMs in clinical settings, particularly in diabetes management where they can offer treatment recommendations, address patient queries, and personalize treatment plans [10, 27, 42, 44, 49].\n\nWe put these claims to test and evaluate the latest versions of ChatGPT, including GPT-4, GPT-40, and GPT-40 mini, on their advice on diabetes-related patient questions. Our evaluations note only a slight improvement in the quality of advice and performance of these latest models compared to their predecessor, ChatGPT 3.5. We also expose both the technical and ethical limitations of ChatGPT's patient advice for diabetes. We note that the previous critique of advice from ChatGPT 3.5 mostly holds on the latest models and, in comparison with other LLMs like Claude 3.5 Son-"}, {"title": "ChatGPT in Healthcare", "content": "ChatGPT's integration into healthcare has enhanced patient care, clinical processes, and communication, demonstrating both promising outcomes and some limitations [45]. Performance on benchmarks like the USMLE and MultiMedQA has established the efficacy of ChatGPT models, particularly ChatGPT 4, which has excelled beyond its predecessor GPT-3.5 and other models such as Google Bard and Med-PaLM. A study by Lim et al. [34] illustrates this by noting that 80.6% of ChatGPT 4's responses to myopia-related queries were rated as 'good' by pediatric ophthalmologists, significantly higher than those of GPT-3.5 (61.3%) and Google Bard (54.8%). This performance highlights ChatGPT 4's enhanced ability in medical query precision and reliability."}, {"title": "Answering Patient Queries", "content": "Beyond benchmarks, ChatGPT's role in direct patient interactions has shown mixed results in the quality and empathy of healthcare communication. According to Ayers et al. [4], while patients often prefer ChatGPT's responses to those of physicians, with 78.5% rated as good or very good compared to 22.1% for physicians, these findings come with significant caveats. The high rating may reflect the more polished and reassuring language used by ChatGPT rather than the clinical accuracy of its responses.\n\nVaishya et al. [47] evaluated ChatGPT's effectiveness in healthcare by testing its response accuracy on medical queries. Their study highlighted that while ChatGPT can rapidly generate responses, these answers may contain potential inaccuracies due to its reliance on outdated data. Additionally, they noted that ChatGPT often provides generic answers, which may not always be suitable for individual patient needs."}, {"title": "Medical Reports Simplification", "content": "LLMs like Chat GPT are being evaluated to streamline healthcare documentation, such as radiology, X-ray reports and preauthorization letters with promising results. In their exploratory study Jeblick et al. [25] report that the medical experts considered Chat GPT-simplified radiology reports to be \"factually correct, complete and not potentially harmful\". Similarly, in another study Lyu et al. [35] note that radiologists' assessments indicate that ChatGPT effectively converts radiology reports into understandable language, achieving an average rating of 4.27 out of 5. The evaluation noted an average of 0.08 instances of missing information and 0.07 instances of inaccurate information per report. These types of efficiencies can result in saving valuable clinicians' time and directing it more towards patient care [25,43]."}, {"title": "EHR Interpretability", "content": "With electronic health records (EHR) being one of the most extensive and rapidly expanding data sources, which currently suffer from limited interpretability due to their lack of standardization, LLMs offer a solution to navigate these complexities [19,46]. In a notable study, GatorTron, an advanced clinical language model, demonstrated its efficacy across multiple clinical NLP tasks with notable accuracy improvements (9.6% in Natural Language Inference and 9.5% in Medical Question and Answer). This highlights GatorTron's capacity to utilize LLMs for interpreting complex EHR data, showcasing its potential to enhance medical A\u0399 system precision and utility in handling unstructured EHR content [33]."}, {"title": "Medical Student Training", "content": "LLMs could potentially be used to enhance medical education by creating comprehensive practice questions and breaking down complex medical topics for students [23,26], a particularly valuable tool for diabetes educa-"}, {"title": "Diabetes Education and Management", "content": "Growing research into ChatGPT and similar Large Language Models (LLMs) for diabetes management shows promise in improving patient engagement, offering personalized advice, and streamlining healthcare processes [10, 42]. These applications highlight the educational and clinical potential of LLMs [12,28,44]. Barlas et al.'s study on obesity assessment in type 2 diabetes found ChatGPT aligned well with clinical guidelines for assessment but fell short in treatment advice, indicating its role as a supplementary tool rather than a replacement for expert care [5].\n\nYang et al. [49] tested ChatGLM in diabetes management, noting its ability to generate accurate treatment recommendations, including lab tests and medications. However, it proved less reliable for patients with complex medical histories, emphasizing that it cannot fully replace physician judgment.\n\nAbbasian et al. [1] developed a conversational health agent (CHA) infused with American Diabetes Association dietary guidelines and Nutritionix data. Their CHA demonstrated excellence in generating nutrient-related queries from a sample of 100 diabetes-related questions. In another study, Hulman et al. [22] assessed ChatGPT's responses to diabetes-related questions against human experts in a Danish diabetes center. Despite a high response rate, participants correctly identified ChatGPT's answers 59.5% of the time, demonstrating its potential to mimic human response quality.\n\nThese studies highlight the impact of LLMs in enhancing healthcare interactions, education, decision-making, and public health communications. However, LLMs cannot replace human oversight, particularly for complex cases, as evidenced by Chat GPT's sometimes inaccurate dietary recommendations for patients with multiple conditions [39]. Effective integration of LLMs into healthcare demands continuous research, expert validation, and ethical oversight."}, {"title": "Study Methodology", "content": "Building on the work of Sng et al. [44], this study investigates the capabilities and limitations of ChatGPT versions 3.5 and 4 within Diabetes Self-Management Education and Support (DSMES). Our goal is to assess their evolution and ability to address shortcomings in medical interpretation and clinical classification, enhancing understanding of AI advancements in healthcare and optimizing AI applications in clinical settings. We adapted and extended the methodology of Sng et al. [44]. We posed 20 unstructured diabetes-related queries to both ChatGPT 3.5 (trained on data up to January 2022) and ChatGPT-4 (training data cut-off in April 2023). The questions covered four DSMES domains: diet and exercise, hypoglycemia and hyperglycemia education, insulin storage, and administration, and were asked in a conversational manner without prompt engineering [37].\n\nA critical aspect was applying the same critiques from April 2023 [44] to both ChatGPT versions, allowing direct comparison. For ChatGPT-4, we focused on the text-based 'GPT-4 (no vision)' model [2, 29, 37]. While GPT-4 is a general-purpose model not specifically trained for medical tasks, its enhanced language understanding and recent training data suggested potential improvements in addressing these critiques [2,37].\n\nWe evaluated the responses from ChatGPT based on three primary metrics: consistency, reliability, and accuracy. These metrics were detailed to emphasize relevance to the query, factual correctness, and practical applicability within the context of Diabetes Self-Management Education and Support (DSMES). The assessment was conducted by two healthcare professionals a General Practitioner and a Dietician who critically reviewed the responses. Our analysis of ChatGPT advice, yielded an in-depth understanding of the AI models' effectiveness and potential risks in delivering contextually appropriate and clinically relevant advice."}, {"title": "Results and Analysis", "content": "We conducted a detailed analysis of responses from ChatGPT 3.5 and ChatGPT 4 on diabetes management, employing the tripartite evaluation metrics of consistency, reliability, and accuracy. Our comparative assessment systematically scrutinized each AI model's response across several dimensions: Accuracy and Depth, Clarity and User-Focused Communication, Consistency and Content Evolution, Error Reduction and Ethical Adherence, Comparative Performance and Feedback Incorporation, and Intertextuality."}, {"title": "ChatGPT Diabetes Advice", "content": "Our analysis compares qualitative improvements from ChatGPT 3.5 to ChatGPT 4, focusing on their practical utility in providing patient advice and communication effectiveness. We evaluated the models based on the 20 questions as noted in [44],due to space constraints however, we present the comparison for only two questions here, summarized in Tables 1 and 2. This analysis reveals how each model handles the following diabetes related patient queries,\n\n\u2022 \"How often should I rotate injection sites?\"\n\n\u2022 \"Is the ketogenic diet safe for someone with diabetes?\"\n\nA comprehensive evaluation of both models based on all questions is summarized towards the end of this section."}, {"title": "Injection Site Rotation Advice", "content": "Based on the comparative analysis presented in the Table 1, ChatGPT-4 demonstrates superior capabilities in providing diabetes self-management education and support (DSMES) compared to ChatGPT-3.5. Here's a concise summary of how GPT-4 outperforms GPT-3.5 across various critical aspects:\n\nSpecificity and Clarity: While GPT-3.5 often provides more specific medical terminology, GPT-4's general terms may be more accessible to a broader audience. Importantly, GPT-4 offers clearer and more specific guidelines regarding injection spacing and techniques, which are crucial for effective diabetes management.\n\nDetailed Guidance: GPT-4 excels in offering detailed, actionable advice on injection site details, such as specific anatomical areas to use or avoid. This level of detail helps patients avoid common mistakes and enhances the effectiveness of their self-management practices.\n\nSystematic Approach: GPT-4 advocates for a systematic rotation pattern for injection sites, adding structure to the self-management process, which can lead to more consistent and reliable self-care practices.\n\nPractical Tools: GPT-4 suggests practical tools for tracking injection sites, such as apps or written records, directly supporting patients in maintaining an organized approach to their diabetes care.\n\nEnhanced Readability and Structure: GPT-4's responses are noted for their logical structuring and coherence, making the guidance more accessible and easier to follow for patients.\n\nUser-Focused Communication: GPT-4's communication style is more user-focused, providing detailed advice that not only adheres to ethical standards but is also tailored to enhance patient understanding and engagement.\n\nComprehensive Self-Management Support: Overall, GPT-4's responses are characterized by a greater depth of advice, which covers a broad range of practical aspects of diabetes care. This comprehensive support is crucial for effective DSMES, as it addresses both the technical and practical sides of diabetes management."}, {"title": "Ketogenic Diet", "content": "Based on the detailed comparison presented in Table 2 regarding responses on the ketogenic diet for diabetes, ChatGPT-4 shows several enhancements over ChatGPT-3.5 including:\n\nFocused Relevance to Diabetes: ChatGPT-4 specifically tailors its responses to the needs of people with diabetes, focusing directly on how the ketogenic diet affects diabetes management. In contrast, ChatGPT-3.5 discusses the diet in a broader context of weight loss and various health conditions, which might dilute the focus needed for diabetes-specific dietary advice.\n\nComprehensive Risk Assessment: ChatGPT-4 provides a broader range of potential risks associated with the ketogenic diet, such as hypoglycemia, nutritional deficiencies, and ketoacidosis, offering a more comprehensive view than ChatGPT-3.5. This inclusivity in potential risks equips patients with a more complete understanding of what to consider before adopting the diet.\n\nMedical and Nutritional Supervision: Both models emphasize the necessity of medical supervision, but ChatGPT-4 extends this to include consulting dietitians, which highlights a multidisciplinary approach to managing health through diet. This could help ensure that dietary advice is not only medically sound but also nutritionally balanced.\n\nEnhanced Monitoring and Adaptation: ChatGPT-4 advises on monitoring both blood sugar levels and ketones, along with necessary medication adjustments. This dual monitoring is crucial in managing diabetes effectively when on a ketogenic diet and provides a more thorough framework than the monitoring suggested by ChatGPT-3.5, which focuses more narrowly on blood sugar and insulin adjustments.\n\nNutritional Planning: Both versions recognize the need for nutritional planning, but ChatGPT-4 emphasizes the importance of focusing on healthy fats and considering nutrient supplementation. This advice can help ensure that patients receive a balanced intake of nutrients, which is essential when restricting certain food groups on a ketogenic diet.\n\nPersonalized Guidance: While ChatGPT-3.5 explicitly states the need for individualized dietary plans, ChatGPT-4's implication of personalized guidance based on individual health profiles subtly suggests a customized approach without overemphasizing it. This can make the guidance appear less daunting and more accessible."}, {"title": "Overall Evaluation", "content": "We conducted a comparative analysis of ChatGPT 3.5 and ChatGPT 4 across 20 questions, evaluating key performance metrics. ChatGPT 4 generally shows improvements over its predecessor in terms of accuracy, depth, and structure, particularly in complex medical scenarios. It effectively simplifies complex medical information into clear, user-friendly explanations, thereby enhancing user engagement. Both versions deliver consistently reliable information, yet ChatGPT 4 provides added depth and practical utility, indicative of advanced content development. ChatGPT 4's advanced error-checking mechanisms enhance its reliability and ethical compliance, though it faces challenges. The model adeptly adapts to specific medical symptoms and user feedback, offering targeted and comprehensive dietary advice for diabetes patients, notably improving over ChatGPT 3.5 and suggesting its potential superiority for Diabetes Self-Management Education and Support (DSMES). However, it struggles with the nuances of personalized, culturally sensitive advice, as our critiques indicate.\n\nOverall, ChatGPT 4's refined dietary and exercise recommendations mark progress but also reveal significant unresolved issues. This underscores its role as an evolving, user-focused health information tool for diabetes management, necessitating continual refinements to overcome its limitations."}, {"title": "Critique of ChatGPT Advice", "content": "Despite advancements, ChatGPT 4, like its predecessor, faces significant challenges in providing nuanced diabetes management advice, as identified in Microsoft and OpenAI research [2,37]. Both versions often deliver generalized advice, failing to meet the individualized needs and specific requirements crucial for effective diabetes self-management. This limitation manifests across the board in their suggestions and advice on a wide array of queries related to DSMES, which were previously critiqued, underscoring the models' inadequate comprehension of the disease's intricacies and the personalized approach required for effective treatment.\n\nIn our analysis, we revisit the 2023 critiques by Sng et al. concerning the responses of ChatGPT versions 3.5 and 4 to key diabetes management questions, reassessing their validity in 2024. Table 4 systematically reviews each critique and its impact on patient care. Most critiques from the earlier study still hold, indicating ongoing gaps in the models' ability to provide personalized and medically precise advice. This reevaluation reaffirms the critiques' relevance and highlights where ChatGPT models still require improvements to better support diabetes care. For specific insights, readers can refer to Table 4 which validates these findings and pinpoints areas needing enhancement in the provided advice."}, {"title": "Generalized advice for Snacking", "content": "Our findings confirm that both ChatGPT versions continue to offer generalized advice insufficient for the nuanced demands of diabetes self-management. For instance, their universal snack inclusion in meal plans lacks necessary customization for individual diabetic needs, as detailed in Table 4. This assessment echoes the 2023 critiques and persists across meal planning and other areas. Despite advancements in language process-"}, {"title": "Errors in Insulin Regime Recognition", "content": "The inability to differentiate between various insulin regimens and to recognize the nuances of medical scenarios remains a significant gap in both versions. As evidenced by the fact that both versions inadequately clarify the distinctions between basal/premixed and multiple-daily injection insulin regimens, a key factor in advising on diabetic snacking see Table 4. This limitation is critical in diabetes management, where the type of insulin therapy significantly impacts diet and lifestyle choices. The fact that this critique, initially made for ChatGPT 3.5, still holds for ChatGPT 4 underscores a lack of progress in the AI's capacity to grasp and articulate these subtleties."}, {"title": "Error in Blood Sugar Measurement Unit", "content": "Both versions of ChatGPT consistently assume blood glucose readings in mg / dL, commonly used in the USA, without clarifying the units. This assumption could lead to dangerous medical advice, especially in severe cases of hypoglycemia or hyperglycemia, since regions vary in their units for blood glucose measurement (see Table 4). This issue underscores the necessity for AI models to be trained on diverse, global datasets to accurately respond to regional practices."}, {"title": "Misdiagnosis of Pseudo-hypoglycemia", "content": "When presented with symptoms such as sweating and shaking alongside a blood sugar level of around 5 mmol/L, both ChatGPT 3.5 and ChatGPT 4 replicated the same misclassification error identified in a critique from 2023. Specifically, they incorrectly diagnosed the condition as hypoglycemia unawareness rather than pseudo hypoglycemia. This persistence of error, nearly a year later, across two versions, accentuates the ongoing relevance of the 2023 critique. It underscores a critical area where both ChatGPT 3.5 and 4 need to enhance their diagnostic accuracy and adapt their responses to reflect a deeper understanding of medical conditions."}, {"title": "Gaps in Insulin Storage Advice", "content": "In response to a question regarding the storage of insulin pens post-opening, ChatGPT 3.5 and ChatGPT 4 both demonstrated a lack of specificity in addressing the distinct storage needs of regular insulin versus insulin analog pens. This recurring issue, first highlighted in the 2023 critique, remains unaddressed in the subsequent versions, emphasizing the critique's ongoing significance. The persistent gap underscores the"}, {"title": "The Risks of AI Assumptions", "content": "Clinicians' informed decisions are based on a comprehensive grasp of patient information, a skill that involves probing deeper when faced with uncertainties\u2014a capability that ChatGPT lacks, as it doesn't seek further clarification and often fills gaps with assumptions, differing markedly from the human clinical approach [5]. Howard et al. highlight ChatGPT's lack of situational awareness, inference-making based on assumptions, and consistency deficits in responses as significant barriers to its healthcare adoption [21]. Although capable of generating convincing responses, these intrinsic limitations call for a cautious approach to ChatGPT's healthcare integration, emphasizing the need for human-computer collaboration to mitigate risks and boost patient care. The use of AI"}, {"title": "Ethical Issues of ChatGPT Advice", "content": "This section explores significant ethical issues related to employing ChatGPT for diabetes patient advice, with a focus on cultural and economic sensitivity, language support disparities, and the effects of transitioning from free to paid subscription models. This underscores the importance of designing language models that are technologically adept and attuned to global socio-economic and cultural realities, promoting equitable and inclusive access."}, {"title": "Cultural and Economic Insensitivity", "content": "Both ChatGPT versions frequently provide meal plans that reflect a Western dietary preference, such as wholemeal wheat English muffins, almond butter, and unsweetened almond milk. These choices are not suitable for countries like Pakistan where socio-economic conditions and dietary customs differ significantly. This indicates a potential skew in the models' training data towards affluent, Western-centric dietary habits, thus overlooking global dietary diversity.\n\nTo make the meal plans relevant for economically less advantaged countries like Pakistan, ChatGPT-4 was prompted to revise the meal plan, considering local economic conditions, ingredient availability, and dietary customs. This prompt adjustment was informed by the use of external information resources like the UN Country Annual Results Report for Pakistan and economic data from the World Bank, which reveals that nearly 4.9% of Pakistanis live on less than US$2.15 a day amidst 30% inflation."}, {"title": "Non-English Support and Info Quality", "content": "Our comparative analysis of the responses provided by ChatGPT in English and Roman Urdu regarding diabetes management diets reveals significant disparities in content quality and accessibility. The English response provides a detailed and comprehensive guide, addressing various aspects of diet and their impact on blood sugar control, designed for an audience familiar with nutritional concepts. Conversely, the Urdu response, though linguistically accessible, lacks crucial information on meal timing, portion control, and the integration of food items into a balanced diabetic diet plan.\n\nThis discrepancy not only reflects the models' limitations in language support but also underscores broader issues of AI accessibility and equity. The efficacy of such models often declines sharply for languages other than English, posing significant barriers for non-English speakers, particularly in third-world countries. Economic constraints and educational disparities further complicate accessibility, as users need to be technologically savvy and proficient in English to effectively leverage these AI technologies.\n\nThe current design and operation of these tools predominantly serve the needs of affluent, English-speaking populations, sidelining those from diverse linguistic and economic backgrounds. This situation highlights the critical need for AI systems that are adaptable and inclusive, ensuring that the benefits of advanced technologies are accessible across different linguistic contexts and contribute to narrowing, rather than widening, global healthcare and educational disparities."}, {"title": "Model Subscriptions and Global Equity", "content": "The shift to a subscription-based model like ChatGPT-4 underscores significant ethical considerations about global access and equity in AI-driven healthcare. While offering advanced capabilities, such models could inadvertently widen the gap in global healthcare education and support, particularly affecting individuals in economically disadvantaged or resource-constrained environments who may not afford these personalized AI services. This situation not only highlights the urgent need for equitable access to AI tools but also emphasizes the imperative for AI systems to adapt to global disparities. Ensuring that AI advancements are universally accessible and practical across diverse economic"}, {"title": "Implications for DSMES:", "content": "Our findings highlight a crucial need for human oversight and expert intervention, especially in complex and personalized healthcare domains like Diabetes Self-Management Education and Support (DSMES). While AI models like ChatGPT can provide general guidance and information, their current capabilities are insufficient for nuanced, individual-specific medical advice. The lack of substantial progress in addressing previously identified critiques in ChatGPT 4 implies that reliance solely on AI for critical healthcare information remains risky. Some known general challenges facing ChatGPT's integration into healthcare include its sensitivity to prompt phrasing, the need for new reliability and confidence assessment standards, and the potential societal and professional impacts. The model's output can significantly vary with slight prompt modifications or through its ongoing updates, highlighting the necessity for healthcare-specific evaluation metrics. Furthermore, the perception of AI's role could alter professional dynamics in healthcare, emphasizing the importance of addressing over-reliance and ensuring AI complements human expertise. Ethical considerations and transparent communication about AI's limitations are crucial to mitigate risks and foster responsible use in enhancing patient care.\n\nIt is important to consider that the limitations of LLMs are systemic and that GPT-4 operates within the limitations of general-purpose language models. Its marked improvements in data processing and reasoning capabilities highlight its potential in specialized domains like healthcare. As argued by [6,18], simply scaling up training data sizes and increasing the number of model parameters to create future versions of the same model architectures will not adequately address these shortcomings. Instead, this approach may amplify existing limitations [6], necessitating a more nuanced and strategic development of LLMs to truly harness their potential in complex healthcare domains like diabetes management."}, {"title": "Recommendations", "content": "Recent benchmarking shows significant performance improvements in GPT-40 over earlier versions such as ChatGPT 3.5 and GPT-4. For example, in the ARC-Easy-Hausa benchmark, which evaluates the model's ability to answer common sense grade-school science questions, accuracy improved from 6.1% with GPT 3.5 Turbo to 71.4% with GPT-40. Similarly, on TruthfulQA-Yoruba, accuracy increased from 28.3% to 51.1% [38]. These advances suggest progress in multilingual support, though gaps between English and other languages persist [38]. However, despite these technological gains, limitations remain in providing common-sense, personalized, culturally-aware, and linguistically inclusive medical advice, as indicated by data in Figure 1, Table 3, and Table 4."}, {"title": "Commonsense Evaluation Layer", "content": "To enhance accuracy and relevance in healthcare contexts, it is essential to augment the System Cards approach with a common-sense evaluation layer [38]. This additional layer should include more robust benchmarks than those currently employed, such as TriviaQA for knowledge-centric tasks and HellaSwag and Lambada for common sense-centric or text-continuation tasks [38]. Implementing this layer, especially in high-risk and emergency contexts, is crucial to mitigate the risks of misinformation and misguidance. Systematic checks and clarifying questions prior to generating responses can significantly boost the safety and reliability of AI models in complex healthcare settings. The integration of GPT-4 into healthcare should follow a structured, risk-tiered approach, guided by user-interaction principles such as the EU's Human-Centered AI and Risk-Based Approach [30]. This framework categorizes AI interactions based on their risk levels, enhancing patient safety and ethical compliance in DSMES and other healthcare applications. For high-risk scenarios, such as medical diagnoses and treatment plans, GPT-4 must include systematic validation processes that require direct medical supervision and mandatory validation before any AI-generated advice is given to patients."}, {"title": "Advice Improvements with RAG", "content": "Retrieval Augmented Generation (RAG) models, a novel stride in natural language processing, aim to refine the prowess of Large Language Models (LLMs). These models confront challenges like hallucination, outdated knowledge reliance, and opaque reasoning, which RAG addresses by fusing LLMs' inherent knowledge with dynamic, updated external databases, enhancing accuracy, credibility, and information relevance, particularly in knowledge-intensive areas.\n\nIn healthcare, especially in diabetes management, Advanced RAG offers an innovative paradigm. It seeks to marry authoritative resources with advanced retrieval tactics to surpass the constraints of Naive RAG and static-query LLMs. By dynamically interfacing with the latest medical literature, guidelines, and studies, Advanced RAG aspires to furnish healthcare practitioners and patients with dynamic, precise, and context-aware information, mirroring the latest medical protocols and insights.\n\nImagine harnessing external knowledge from authentic sources like the National Institutes of Health (NIH) and the Centers for Disease Control and Prevention (CDC) to empower Advanced RAG in anchoring diabetes management advice firmly in the latest research and health data. This strategy could offer personalized care solutions, adeptly addressing the unique dietary, insulin, and exercise requirements essential in navigating the complexities of diabetes.\n\nIn clinical settings, Advanced RAG could emerge as a cost-efficient tool, offering immediate access to current data, thereby economizing the time healthcare workers dedicate to research. This could streamline care and uplift healthcare quality, marking a substantial advancement over conventional methods and aiding in health-"}, {"title": "Limitations", "content": "While informative, the study's reliance on simulated patient inquiries may not fully capture real-world interactions, suggesting a pivot to live scenarios in future research to better assess ChatGPT's real-time applicability. The scope of diabetes-related queries, though focused, was not exhaustive. Future studies should expand the query range and explore underlying causes of ChatGPT's limitations to enhance its contextual and personalized advice capabilities. Our text-centric analysis may overlook the benefits of visual aids in DSMES, highlighting the potential value of investigating multimodal ChatGPT versions (like Dalle). Additionally, insights from clinical experts have enriched our study, emphasizing the importance of ongoing collaboration to ensure AI-generated advice aligns with medical standards and patient needs."}, {"title": "Conclusion", "content": "This study underscores the dual nature of AI advancements in healthcare, particularly in Diabetes Self-Management Education (DSME). While AI models like ChatGPT demonstrate improved capabilities in medical knowledge and language processing, they continue to face significant challenges. These include limitations in providing personalized advice, cultural and dietary sensitivity, and the translation of test performance to clinical settings. The research highlights the necessity of human oversight and expert intervention in AI integration, emphasizing the importance of ethical and informed applications. It stresses the need for a balanced approach that combines technological innovation with human clinical expertise, ensuring safe and effective A\u0399 deployment in healthcare. This study marks a pivotal step in understanding and harnessing AI's potential in DSME while acknowledging and addressing its inherent limitations."}]}