{"title": "Evaluating Research Quality with Large Language Models: An Analysis of ChatGPT's Effectiveness with Different Settings and Inputs", "authors": ["Mike Thelwall"], "abstract": "Evaluating the quality of academic journal articles is a time consuming but critical task for national research evaluation exercises, appointments and promotion. It is therefore important to investigate whether Large Language Models (LLMs) can play a role in this process. This article assesses which ChatGPT inputs (full text without tables, figures and references; title and abstract; title only) produce better quality score estimates, and the extent to which scores are affected by ChatGPT models and system prompts. The results show that the optimal input is the article title and abstract, with average ChatGPT scores based on these (30 iterations on a dataset of 51 papers) correlating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is slightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest that article full texts might confuse LLM research quality evaluations, even though complex system instructions for the task are more effective than simple ones. Thus, whilst abstracts contain insufficient information for a thorough assessment of rigour, they may contain strong pointers about originality and significance. Finally, linear regression can be used to convert the model scores into the human scale scores, which is 31% more accurate than guessing.", "sections": [{"title": "Introduction", "content": "Evaluating the quality of academic research is necessary for many national research evaluation exercises, such as in UK and New Zealand (Buckle & Creedy, 2024; Sivertsen, 2017), as well as for appointments, promotions, and tenure (Pontika et al., 2022; Tierney & Bensimon, 1996). These evaluations are expensive and reduce the time that academics can spend conducting research, so automated alternatives or support are desirable. The need for shortcuts has given rise to the field of bibliometrics/scientometrics, which has (controversially: MacRoberts & MacRoberts, 2018) developed a wide range of academic impact or quality indicators, typically based on citation counts (Moed, 2006). Attempts to use bibliometric information to directly score journal articles for research quality have obtained mixed results, however, with article level Pearson correlations between traditional machine learning (e.g., extreme gradient boost) predictions varying between 0.028 (Art and Design) and 0.562 (Clinical Medicine) for REF2021 (Thelwall et al., 2023). It is therefore logical to assess whether LLMs can be more accurate at predicting quality scores, given that they are relatively accurate at a wide range of language processing tasks (Bang et al., 2023; Koco\u0144 et al., 2023), and have been proposed for scientometric purposes (Bornmann & Lepori, 2024). In fact, one funder is now using LLMs to support the initial triage of biomedical grant proposals, creating a pool of apparently weaker submissions for human reviewers to consider for rejection before the full peer review process. The experimental results showed that LLMs were not reliable enough to be used without human judgements but had an above random chance of identifying weak submissions and could save time by flagging these for human triage (Carbonell Cort\u00e9s, 2024).\nMany previous studies have assessed the extent to which LLMs can give useful feedback on academic works, usually to help reviewers/editors analysing conference/journal submissions or to support authors with text revisions. These have tended to show that LLMs can make useful suggestions and their comments may even extensively overlap with those of human reviewers (Liang et al., 2024b). The main current challenge is that LLM-generated reviews are plausible enough that busy reviewers may cheat by using them. Because of this threat to the integrity of the peer review process and for copyright infringement, some publishers, like Elsevier (2024), instruct reviewers and editors not to use LLMs. Alternative approaches include detecting LLM-authored reviews (Liang et al., 2024a) or harnessing LLM outputs to build a reward system for reviewers creating high quality reports (Lu et al., 2024).\nWhilst the extent to which LLMs can make useful comments on manuscripts is now supported by substantial evidence, their ability to make overall quality judgements for unreviewed and reviewed papers is less clear. In this direction, ChatGPT 4 can identify two characteristics of papers (novelty and engagement; accessibility and understandability) that correlate positively with citation counts (de Winter, 2024). Similar to the current paper, one study analysed unreviewed conference papers from the PeerRead dataset of 427 reviews of International Conference on Learning Representations 2017 submissions (Kang et al., 2018). It used the prompt, \"You are a professional reviewer in computer science and machine learning. Based on the given review, you need to predict the review score in several aspects. Choose a score from [1,2,3,4,5], higher score means better paper quality\u201d with ChatGPT 2.5 or ChatGPT 3.5-16k, probably with the ChatGPT API. It found that reviewer scores of 1-5 weakly correlated with ChatGPT 3.5 predictions when only submission abstracts were entered (Spearman: 0.282) but not when full texts were entered (Spearman: 0.091) (Zhou et al., 2024). It is not clear if multiple reviewer scores were aggregated for the same paper, however. For (mainly) reviewed papers, one previous study of 51 information science papers from a single author (me) asked ChatGPT 4 (web interface) to score articles (full text PDFs or Word documents) for research quality using the instructions given to assessors in the UK Research Excellence Framework (REF) 2021. It elicited a score of 1*,2*,3* or 4* for each paper, repeating the process 15 times. Whilst the ChatGPT 4 scores correlated weakly (0.20) with the author's self-evaluations, when the 15 scores for each article were averaged, the correlation rose to 0.51 (Thelwall, 2024). The article did not compare different LLMs, inputs, or prompts.\nThe current article addresses a few gaps left by the above research. It seeks to verify and extend the prior suggestion that analysing abstracts gives better results than analysing full texts (Zhou et al., 2024). The focus is on ChatGPT rather than alternatives because ChatGPT has been shown to perform well on a wide variety of text processing tasks, as reviewed above, and there is insufficient evidence yet to prefer Google Gemini (e.g., Buscemi & Proverbio, 2024). This article also specifically investigates the relationship between the number of ChatGPT iterations and the accuracy of the prediction, extending a prior study that only compared 1 and 15 iterations (Thelwall, 2024). It also compares different ChatGPT models for research quality evaluation for the first time. Also for the first time, it directly assesses the accuracy of the scores and compares different system prompts. The overall purpose is to gain more insights into how to obtain the most accurate estimates from ChatGPT."}, {"title": "Methods", "content": "The overall research design was to run a series of experiments guided by the research questions on a set of 51 articles with quality scores, using the ChatGPT API environment. For each experiment, the ChatGPT completion requests were carried out consecutively and then repeated a further 29 times to give 30 scores for each article. This seems like a large enough number (and double the previously published maximum) to reveal the relationship between the number of iterations and the usefulness of the average score predictions. The API environment is necessary to run the high volume of tests for the proposed experiments. It also has the added advantage that ChatGPT promises not to use data submitted within the API for training its models (\u201cWe do not train our models on inputs and outputs through our API\", OpenAI, 2024), so each score should be independent of all the others. The queries were all submitted in July 2024."}, {"title": "Data", "content": "The raw data for this paper is a set of 51 information science journal articles that have either been published or prepared for submission and subsequently rejected or not submitted. All were written by the author, who has copyright, and were scored by him using the REF quality scale of 1*, 2*, 3* or 4* (REF, 2019), with which he is familiar. Papers were given a mid-score (e.g., 3.5*) if they seemed to fall between two score levels. The scores given to these papers have never been disclosed to anyone else or uploaded to any Artificial Intelligence (AI) system. This dataset is not ideal since (a) it is part of a single author's output and therefore not representative even of its field, (b) the author's scores are less relevant than the scores of more independent and less expert (on this topic) senior researchers, who would be the ones forming the evaluations in the most important context (e.g., the REF or promotion/appointment). Nevertheless, the papers are relatively homogeneous in topic, giving more scope for Al to differentiate quality differences from topic differences.\nThe REF quality criteria encompass the core dimensions of rigour, significance and originality (Langfeldt et al., 2020). Whilst the specific instructions are unique, the goals are nevertheless general in this sense. Quality criteria can differ between contexts, however, for example by emphasising local research needs (e.g., Barrere, 2020).\nAll articles were available in PDF format or as Word Documents. The Word documents were converted to text with Word's Save As text feature. The PDF documents were converted to text with PyMuPDF in Python (Convert_academic_pdf_to_jsonl.py in https://github.com/MikeThelwall/Python_misc). PyMuPDF discards images and converts the text into blocks, each of which is typically a paragraph or sentence. It guesses paragraphs, frequently making mistakes, and does not attempt to format tables. A program was written to clean the results, merging sentences into paragraphs, when appropriate, and removing page headers and footers (in the Webometric Analyst program free online at https://github.com/MikeThelwall/Webometric_Analyst: Services | PDF text file processing | Clean Fitz/PyMuPDF converted PDF text files from headers). These texts were then manually checked for paragraph structure and additional headers/footers, requiring extensive cleaning in each case. They were then processed to convert into Jsonl format for input into the ChatGPT API (Webometric Analyst: Services | Convert set of texts (e.g., cleaned PDFs) in folder to single Jsonl file). This generates less rich representations than the original PDFs/Word documents previously used (Thelwall, 2024) due to the lack of images and appropriately formatted tables and formulae, but has the advantage of being better structured in terms of paragraphs.\nSeveral different datasets were generated to try different extents of imput.\nTruncated dataset: This consisted of the full text files without the reference list (not strongly relevant), the contents of tables (difficult to process by an LLM), the authors, and the keywords. This was designed to contain all the key information needed for an evaluation, given that core information in tables and figures would be referred to in the full text and the reference list is not essential to an evaluation, even though it can sometimes be statistically helpful in citation prediction (Kousha & Thelwall, 2024; Qiu & Han, 2024). Thus, in the spirit of document simplification (Zhou et al., 2024) it is probably a better input than a full text PDF, which the API would not accept anyway.\nAbstract dataset: This consisted of the title and abstract alone after removing the authors, keywords and the remaining text.\nTitle dataset: This consisted of article titles alone."}, {"title": "GPT prompt setup", "content": "In machine learning it is typical to use separate development and training sets to allow an Al system to be configured on data that it is not tested on. This was not done in this case although there was a configuration exercise. This exercise consisted: of (a) fruitless tests with different prompts to try to get the score prediction to be reported more consistently, and (b) fruitless experiments with attempts to get score predictions from DOls or full text URLs. In both cases, the results did not change the initial setup and evaluations were based solely on the format of the results and not their accuracy.\nThe main system prompt used was similar to that used in the previous paper and consists of the REF guidelines for the research area (Main Panel D) containing the information science in the REF (REF, 2019). The full prompt (Strategy 6) is in Appendix 1. The main changes are in the words at the start to cast it as an instruction to ChatGPT rather than information for REF assessors. The changes were motivated by prior research about prompting strategies (Yang et al., 2024) and an expectation that language similar to that reported by OpenAl in its documentation would be the most effective because it might reflect the format of the instructions fed to ChatGPT in its instruction following training phase.\nEach score request was a single API call, specifying a ChatGPT model, including the system instructions, as in Appendix 1, and with the prompt, \"Score the following journal article: \", followed by the article title/abstract/truncated text, as relevant. The maximum temperature parameter was set to 1, the default, the top_p parameter was also set to its default of 1, and the max_tokens parameter was set to 1000, which seems adequate for the typical reports written by ChatGPT in the previous study (Thelwall, 2024)."}, {"title": "GPT system prompt variations", "content": "Six variations of the basic REF prompt were tested to assess whether alternative prompts might give better results. In particular, the basic system prompt is lengthy and complex so it seems plausible that a simpler system prompt would be more effective. For this purpose, five truncated versions of the basic prompt were created by cutting it down at natural points (Appendix 1). A seventh prompt was created that did not request a score justification (Appendix 2) to test whether simpler results could be safely obtained from ChatGPT in contexts when only the score would be needed."}, {"title": "GPT score extraction", "content": "Perhaps because of the length of the REF guidelines in the system prompt, I did not find a way to get ChatGPT to report the score predictions systematically. Thus, each report contained an analysis of the article that (usually) contained a score assessment within it in different formats. Each ChatGPT model used a wide variety of structures to report its assessment and the models tested had their own structures and styles to some extent. A program was written to use all the patterns found to extract the score from the reports (Webometric Analyst: Al | ChatGPT: extract REF scores from report). For example, one rule was to extract the number between \"Overall Score**: **\" and \"*\". When three separate scores were given for rigour, originality, and significance, but not an overall score, these three scores were averaged to three decimal places. When ChatGPT reported a score range of two (usually, e.g., 3* to 4*) or three (occasionally, e.g., between 2* and 4*) the midpoint was taken. When the rules failed to find a score, the program prompted human input (me) to enter the score. In some cases (usually for titles or abstracts), scores were given for originality and significance but not for rigour. In such cases, the average of the originality and significance was used.\nFor some ChatGPT reports on titles and once for reports on abstracts, it did not give a score, instead reporting that it had insufficient information to make a judgement. These reports were registered as missing and ignored for the analysis. For example, if two of the 30 scores for article 1 were missing, then the article 1 average score would be the average of the remaining 28."}, {"title": "Analysis", "content": "Spearman correlations were used to assess the extent to which the 51 human scores agreed with the ChatGPT scores, without assuming that they are on the same scale. This assesses the extent to which the ChatGPT scores are in the same rank order as the human scores.\nSince, as expected (Thelwall, 2024), the scores for the same article differed between iterations, averaging them provides more stable predictions. To test the averaging, it seems reasonable to assume that the order in which the 30 iterations for each experiment was conducted would be irrelevant to the score. Thus, to assess the likely range of correlations for single ChatGPT run, averaging was used, although with four different strategies (Webometric Analyst: Tab-sep|Stats|Randomly average 2-n columns and correlate with col n+1):\nNo averaging (or averaging 29 runs): Correlations were calculated for each run (or for each set of 29 runs) and the 30 correlations were averaged. The standard deviation was calculated to estimate confidence intervals with the t distribution.\nAveraging 2 or 28 runs. In both cases there are 30x29=870 permutations of sets of runs to average, so all 870 correlations were calculated, and the mean and standard deviation calculated for the average and confidence intervals.\nAveraging 3-27 runs. In these cases, there are over 30x29x28=87028 permutations of sets of runs to average, so a random set of 1000 permutations was selected (with replacement) and correlations were calculated for these. The mean and standard deviation was again calculated for the average and confidence intervals.\nAveraging all 30 runs. In this case no permutations were possible and so no confidence intervals could be calculated. It is possible to calculate confidence intervals through bootstrapping with replacement, but this seems unnecessarily complex.\nThe accuracy of predictions was assessed with Mean Absolute Deviation (MAD) to record the average difference between the predictions and human scores. Since model scores are not necessarily on the same scale as the human scores, linear regression models were fitted to transform them to the same scale in the simplest way. MAD calculations on the linear regression predictions therefore give a better estimate of the underlying accuracy of the predictions. The software used for this is correlation_and_regression.py in https://github.com/MikeThelwall/Python_misc."}, {"title": "Results", "content": "Input and averaging length comparisons: ChatGPT 3.5-turbo on truncated texts, abstracts, and titles\nThe ChatGPT reports were always well structured and usually plausible, often giving explicit reasons for the scores or making general statements like, \u201cRigour: [description]: this aligns with the criteria for a 3*\". Reports on the titles alone were often short and without a prediction, stating instead that it was impossible to evaluate an article based on its title. For example, \"Since the actual content and impact of the article [] is unknown, this evaluation remains speculative.\u201d (ChatGPT 4). These title-based reports were sometimes slightly inconsistent in that they would make conditional statements in the early parts, such as \"If the methods align with expectations, then []\u201d but in later parts give a more definitive evaluation, such as \"Thus, this article is assigned a score of 3*\".\nIn terms of the report scores, averaging more iterations of ChatGPT 3.5-turbo increases the correlation with human scores, whatever the input (Figure 1). The rate of score increases tends to diminish as the number of iterations increases, however.\nThe optimal input for ChatGPT 3.5-turbo seems to be article abstracts (with titles) (Figure 1). Predictions based on these have the highest correlation with human scores. Extending abstracts to truncated full texts decreases the correlation but restricting the input text to article titles substantially decreases it. Nevertheless, score predictions from titles alone are surprisingly non-trivial, having a 0.46 correlation with human scores despite this minimal input."}, {"title": "Model comparison: ChatGPT 3.5-turbo, ChatGPT 4o and ChatGPT 4o-mini on abstracts", "content": "Unsurprisingly, the most powerful model (ChatGPT 4o) seems to give the best predictions but the difference between the models is not large. There is little difference between the relatively old ChatGPT 3.5-turbo and the new (at the time of writing) ChatGPT 4o-mini. Given that API calls with ChatGPT 4o are ten times more expensive than calls with ChatGPT 3.5-turbo and twenty times more expensive than ChatGPT 4o-mini calls, this suggests that the cheaper models are good alternatives unless the highest accuracy is needed."}, {"title": "Comparison of prompt strategies", "content": "Although all seven system prompts gave positive results, in the sense of correlations substantially above zero, the most complex strategy was the most effective (Figure 4). By far the worst strategy was the one not requesting feedback in the ChatGPT report (Strategy 0), suggesting that asking for an analysis helps ChatGPT to judge scores better. This is despite ChatGPT occasionally giving its score before its analysis in response to other system prompts. The second-best system prompt is the single paragraph Strategy 2, and adding more information to it makes it worse (Strategies 3-5) until the quality level definitions are added (Strategy 6). If Strategy 2 is combined with the part of Strategy 6 that is not in Strategy 5 then this performs slightly worse than strategy 6, however (correlation 0.641 after averaging 30) so the full instructions seem to be the optimal choice."}, {"title": "Individual score level accuracy", "content": "Despite the correlations, the prediction scores (Direct in Table 3) are, on average, closer to a wrong score than a correct score. They are nevertheless slightly better than the baseline strategy of assigning all articles the average score (2.75). The accuracy of the predictions can be improved with linear regression (Regression in Table 3) to the extent that they improve on the baseline strategy by up to 31% (i.e., they are 31% closer to the correct value than the 2.75 guesses are, on average). The regression MAD values are probably underestimates, however, due to overfitting \u2013 calculating accuracy on the data used to create the model.\nThe optimal prediction strategy in terms of accuracy for an article REF score is therefore to run its title and abstract 30 times through GPT-4o, then multiply the average score by 2.05 and subtract 3.4. If the predicted scores need to be rounded to whole numbers, then the MADs do not change much (not shown in table), although the GPT-3.5 turbo: Abstracts MAD decreases from 0.51 to 0.45."}, {"title": "Discussion", "content": "This study has many limitations, the most important of which is the restriction to a relatively small number of articles written by single person. The results could easily be substantially different for other fields and correlations are likely to be lower for sets of articles with a narrower quality range (probably including most REF submissions). It is also possible that correlations would be weaker for more mixed sets of articles, since LLMs might find quantitative research easier to classify or, more generally, they might use different scales for different types of research. The results may also be better with future, larger models or with more precise system instructions than those listed in the Appendices. The results could also be better with other ChatGPT temperature and top_p parameters (which control different aspects of randomness), although the performance did not improve (for the abstracts dataset with GPT-4o-mini) if temperature was set to 0.1,0.5, 1.5 or 2, or if top_p was set to 0.25, 0.5, or 0.75. Finally, alternative current LLMs might perform better.\nComparisons with prior work\nThe results improve on the prior attempt to predict REF scores on the same set of articles with ChatGPT 4 using the web interface and uploading PDFs or Word documents (Thelwall, 2024). Although this prior paper only averaged over 15 iterations, a graph of its results (Figure 5) shows that it is less accurate at this point than the three models tested in the current paper with abstracts as inputs (Figure 2) and it is also less accurate than the weaker ChatGPT 3.5-turbo with either abstracts or truncated full texts as inputs. This tends to confirm that inputting full text documents is a relatively ineffective approach for obtaining quality scores. The results also confirm that averaging over multiple runs is a substantially more effective strategy than single runs of ChatGPT. Whilst ChatGPT can ingest documents that mix text and images, this comparison suggests that it is not powerful enough to leverage this capability for quality score prediction."}, {"title": "Alternative input strategies", "content": "In theory, it might also be possible to request a review of an article from ChatGPT by referencing the article's DOI or by submitting an URL of a public archive page or a free text online PDF. For this, ChatGPT API access does not allow content to be fetched from the web, but the ChatGPT web interface might. Experiments at the start of this project with these were unsuccessful. Web ChatGPT either reported that it could not find the document at the URL or wrote a plausible report on an imaginary article, sometimes citing the correct URL. This suggests currently that pointing ChatGPT to an article online is insufficient for an analysis and that it relies on the information directly entered.\nFor example, using the web interface and ChatGPT 4o, the prompt \"Score the following journal article: https://arxiv.org/abs/2104.09138\" (an URL for the paper, \u201cCures, Treatments and Vaccines for Covid-19: International differences in interest on Twitter\" assessed an unrelated article (\"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\"), probably picking this article as a citation classic and therefore frequently met in its database. Repeating the same query produced assessments of similarly unrelated articles (e.g., \"Electronically Programmable Photonic Molecule\", \"High-Resolution Image Synthesis with Latent Diffusion Models\"). One repeated attempt identified the correct article, but did not write a report about it, and a follow-up identical query again identified the correct article and wrote a report about it. Thus, using URLs instead of article information is error prone.\nFollowing on from the above, ChatGPT 4o could be encouraged to hallucinate by entering partial titles. For example, the query \"Score the following journal article: An experiment to ...\" generated a fictional article title (ChatGPT 4o: \u201cAssuming the journal article is titled \"An Experiment to Improve Teaching Methods in Higher Education\":\") and a report evaluating it at 3* (ChatGPT 4: \u201cThis article could be rated 3* because it is internationally excellent in terms of originality, significance, and rigour but might fall short of the highest standards needed for a 4* rating due to limited global adoption or transformative impact.\"). Thus, entering short titles has additional risks."}, {"title": "Conclusion", "content": "Albeit on a single limited sample, the results suggest that the optimal strategy for estimating the REF-like research quality of an academic journal article is to use a system prompt derived from the unabbreviated REF2021 instructions, then feed the article title and abstract into ChatGPT 4o thirty times, then apply a regression formula to transform it into the human scale. This gives a prediction that is on the same scale as the human evaluator and has a high rank correlation with it (0.673). The prediction is still inaccurate, however, being likely to deviate from the true score by 0.5, on average, despite the limited range of 1* to 4*. Others wishing to use this approach should calibrate the linear regression equation to their own quality standards by repeating the above experiment with 50+ articles that they have scored. Of course, article titles and abstracts can only be uploaded to ChatGPT if copyright permits. The web version of ChatGPT might use the input for training, so the copyright must allow this, and this probably excludes uploading CC BY copyright articles since ChatGPT will not generate copyright statements when the model trained on the data creates new content.\nThe ChatGPT accuracy, at the level of correlation, is the highest yet found for a quality prediction (e.g., Thelwall et al., 2023; Thelwall, 2024) and has the advantage of not requiring citation information as an input. Nevertheless, at the individual article level, this is unacceptably inaccurate for important applications. In particular, it should not be used for peer review of conference papers or journal articles and also not for promotion and hiring decisions. It might be plausible to use average scores to compare sets of articles, as in the REF, or for comparing departments, if this did not introduce systematic biases (e.g., against qualitative research). More analysis is needed to understand how the predictions are made by ChatGPT to give insights into the types of biases that are likely."}]}