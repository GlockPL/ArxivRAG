{"title": "A Recipe For Building a Compliant Real Estate Chatbot", "authors": ["Navid Madani", "Anusha Bagalkotkar", "Supriya Anand", "Gabriel Arnson", "Rohini Srihari", "Kenneth Joseph"], "abstract": "In recent years, there has been significant effort to align large language models with human preferences. This work focuses on developing a chatbot specialized in the real estate domain, with an emphasis on incorporating compliant behavior to ensure it can be used without perpetuating discriminatory practices like steering and redlining, which have historically plagued the real estate industry in the United States. Building on prior work, we present a method for generating a synthetic general instruction-following dataset, along with safety data. Through extensive evaluations and benchmarks, we fine-tuned a llama-3-8B-instruct model and demonstrated that we can enhance it's performance significantly to match huge closed-source models like GPT-40 while making it safer and more compliant. We open-source the model, data and code to support further development and research in the community.", "sections": [{"title": "Introduction", "content": "Discrimination in the real estate industry has long been a pervasive issue, manifesting through practices like steering and redlining. Steering involves directing prospective buyers or renters toward or away from certain neighborhoods based on characteristics such as race, ethnicity, or religion. For instance, a real estate agent might exclusively show properties in predominantly minority neighborhoods to clients of a specific racial background, thereby limiting their housing options and perpetuating segregation. Redlining refers to the systematic denial of services such as mortgages or insurance to residents of certain areas, often those with high minority populations. This practice has historically led to economic disparities and entrenched segregated communities.\nTo combat these discriminatory practices, legislation such as the Fair Housing Act (U.S. Department of Housing and Urban Development (HUD), 1968) and the Equal Credit Opportunity Act (Staff in the Office of Technology and The Division of Privacy and Identity Protection, 2024) were enacted to ensure fair treatment in real estate transactions. Real estate agents, brokers, and financial institutions are required to comply with these regulations. However, the growing use of AI-driven chatbots in real estate brings new complexities, particularly as large language models (LLMs) are prone to replicating and amplifying biases learned from data, inadvertently violating these laws. Figure 1 illustrates a case where GPT-40 as a state of the art model violates the fair housing regulations.\nOur work addresses the critical need for compliance-aware AI systems in the real estate sector. While previous research has focused on mitigating bias in general LLMs, few studies have explicitly targeted legal compliance in domain-specific applications like real estate. Our contribution is novel in several key areas:\nDevelopment of a Compliance-Focused Dataset:\nWe create a synthetic dataset that integrates general instruction-following tasks with scenarios specific to legal and ethical compliance in the real estate domain. This dataset is designed to ensure adherence to fair housing and lending laws, which has not been adequately addressed in previous work.\nFine-Tuning for Legal Compliance and Real Estate Expertise:\nUtilizing our dataset, we finetune a llama3-8b-instruct model to enhance its ability to provide helpful real estate information while strictly adhering to legal and ethical standards. Our fine-tuned model significantly outperforms its base model, performing even better than llama3-70b-"}, {"title": "Related Work", "content": ""}, {"title": "Alignment of Large Language Models with Human Preferences", "content": "The alignment of large language models (LLMs) with human preferences has been a key research focus, particularly through techniques like Reinforcement Learning from Human Feedback (RLHF). This approach has proven effective in training models to adhere to human values and ethics (Christiano et al., 2017). OpenAI's instruction-following models, fine-tuned using RLHF, demonstrate substantial improvements in model helpfulness and safety (Ouyang et al., 2022). Recent work has sim-"}, {"title": "Safety Alignment and Compliance in Language Models", "content": "Ensuring that LLMs generate safe and legally compliant outputs has become a priority. Various efforts from research groups such as Anthropic and Meta have developed methods to align models for safety by using adversarial prompts to detect and mitigate non-compliant behaviors (Bai et al., 2022), (Touvron et al., 2023; Dubey et al., 2024). These works underscore the importance of equipping LLMs with the ability to avoid harmful content while maintaining task performance. Our work builds on these foundations by extending safety alignment to the real estate domain, where adherence to laws like the Fair Housing Act and the Equal Credit Opportunity Act is critical."}, {"title": "Methods for Generating Synthetic Instruction-Following Datasets", "content": "Synthetic data generation has emerged as a powerful tool for training LLMs on specific behaviors, especially when domain-specific or legally compliant behavior is required. Approaches such as Self-Instruct (Wang et al., 2022) and GenQA (Chen et al., 2024) demonstrate how LLMs can autonomously generate large datasets to improve instruction-following performance. Our work leverages these advances to build a compliance-focused synthetic dataset tailored to the real estate domain."}, {"title": "Dataset", "content": "We built a three-part dataset including general instructions, safety instructions, and dialog. In this section we explain how each segment (split) of the dataset was built. Safety alignment is inherently a long-tail distribution problem, making it crucial to ensure that optimizing for safety does not compromise performance on the main tasks. The first question we needed to address was identifying the domain of tasks that a real estate chatbot should excel in. To achieve this, we employed a combination of automation and human intervention to build a comprehensive taxonomy of topics relevant to discussions and interactions between a real estate chatbot and users. Our focus was primarily on knowledge-intensive real estate instructions rather than inquiries requiring real-time information, such as home listings or current market trends. At the time of writing this paper, GPT-40 (OpenAI) is one of the most powerful LLMs, particularly in knowledge-intensive benchmarks such as MMLU (Hendrycks et al., 2020). This is why we chose to use it as our generator LLM. More examples and details can be found in appendix A."}, {"title": "General Instructions", "content": "To generate a diverse set of instructions and responses, we utilize a prompting approach similar to GenQA (Chen et al., 2024), but with some important differences. Our pipeline consists of three main stages: 1) A human-LLM collaboration for generating a diverse and high quality set of real estate topics, 2) diverse and challenging instruction generation, and 3) response generation. For the first stage, in order to ensure quality, diversity and coverage of different real estate topics the authors of the paper cleaned and prepared a set of 90 real estate topics (More details on this step can be found in appendix A.1.)\nFor the second step, we use a conditional generator prompt which takes a random topic from our pool of selected topics, tries to generate 50 subtopics, and picks one randomly (the randomness is enforced by the prompt generator) this ensures that we uniformly sample from different topics and subtopics. The LLM is then asked to write a challenging question about the chosen topic and sub-topic. (Appendix A.2 explains the prompt details.) In the last stage, we post-process the generated response, extract the question, and prompt the LLM sepa-"}, {"title": "Safety Instructions", "content": "For generating safety examples, we first conducted multiple iterations of discussions with our legal experts to categorize potential non-compliances and safety issues that the model might encounter and then designed a helpful and safe behavior for these situations. We decided to focus on two major topics: 1) the Fair Housing Act and 2) the Equal Credit Opportunity Act. In our synthetic data generation, we concentrated on user instructions that could result in responses violating any of these regulations. To begin with, we utilized the dataset provided by (Bagalkotkar et al., 2024), which consists of around 10K non-compliant queries. We also used the classifier they trained on their dataset and ran it over the dataset to collect examples that were most certainly classified as non-compliant. Afterward, we designed a prompt (detailed in appendix A.2.2) to force the model to regard the input query as a potential non-compliance and follow the following desired safety behavior:\n1. In case the query consists of toxic or hateful language, refuse to answer and help the user.\n2. In case of any non-compliance, explain to the user why their query could cause violation.\n3. Try to answer the user's query in a general and compliant way.\n4. Refer the user to specialists or relevant resources if the query is beyond its skills or contains sensitive subjects.\nWe refer to this proportion of the data as the safety split."}, {"title": "Multi-turn Interactions", "content": "Since it is also important for the model to interact with users in a natural, multi-turn conversational setup, we generated a set of multi-turn interactions. To do this, we followed a similar approach to Section 3.1, but instead of making two calls to the"}, {"title": "Pruning The Dataset", "content": "To ensure a dataset of diverse instructions and responses while avoiding semantically and lexically duplicate instructions, we aim to prune the data. This is particularly important when holding out a set of examples for evaluating our final tuned models, as we want to avoid having leaked examples from the training set in the evaluation set. We iterate over all the examples in each split of the data and remove those with a similarity above a certain threshold."}, {"title": "Fine-tuning", "content": "We use LORA (Hu et al., 2021) adaptors to finetune llama3-8b-instruct on our proposed dataset. We fine-tune the model for 5 epochs or until the validation loss on 200 held out examples from general instruct split ceases to decrease. Additionally, we hold out 200 examples from each data split for further testing of performance and safety. We also perform an ablation study of the effect of the dialog split and the size of the safety data in D.1 and different LoRA adaptor sizes (as reported in the appendix D.2)."}, {"title": "Evaluation Experiments and Results", "content": "In this section, we design several model-based evaluators to assess our model's performance across two key dimensions: safety and helpfulness. Safety focuses on how effectively the model addresses biases, discriminatory behavior, and compliance issues, while helpfulness measures its accuracy, factual consistency, and human preference. We also propose two benchmarks to evaluate these aspects and assess the model's real-world effectiveness."}, {"title": "Related Work", "content": "In recent years, model-based evaluation has seen significant advances, reducing the reliance on extensive human annotations while maintaining high"}, {"title": "G-Eval Based Evaluation", "content": ""}, {"title": "Evaluation Setup", "content": "We measure helpfulness on the general instructions split of the data and safety on the safety split. To achieve this, we define four different criteria (helpfulness with reference, helpfulness without reference, safety with reference, safety without reference) and use the G-Eval (Liu et al., 2023) approach to score the model's responses. We have chosen to use both metrics with reference (using references from GPT-40 during the data generation process) and without reference to avoid biasing the evaluation towards GPT-40 responses as the ground truth. We employ GPT-4 as the evaluator model in all cases and run the two helpfulness metrics on the general instruction split and the two safety metrics on the safety split of the test set."}, {"title": "Results", "content": "We compare our model versus the baselines on the held-out test data. Table 2 shows the average score of each model across the test splits on our four proposed metrics. First, we observe that our model outperforms all baselines except GPT-40 on the helpfulness metric, and in the case of having no reference, it even outperforms GPT-40. Second, on the safety dimension\u2014particularly the \"without reference\" metric, which purely measures the model's safety\u2014our model outperforms all opensource LLaMA-3 baselines, although it falls short of GPT-4 and GPT-40. The \"safety with reference\" metric is highest for our model, indicating its superior performance in following the defined"}, {"title": "Head-to-head Multi-turn Evaluation", "content": ""}, {"title": "Evaluation setup", "content": "The primary focus of the general instructionfollowing data we propose is on questions that require real estate expertise and knowledge. However, in many scenarios, users might approach these systems with more basic questions or scenarios in mind. To test our model's helpfulness and safety in such situations, we developed two real estate benchmarks that cover general multi-turn questions from first-time home buyers, as well as a safety benchmark developed by our legal team.\nFirst-time Home Buyers Benchmark We collected questions from 1,438 participants in a seminar held by Zillow for first-time home buyers about what they hoped to learn at the event. We manually cleaned the data by removing entries that were not questions or required temporal context, such as \"Where do you see the rates going by the end of this year?\". We also reformatted relevant questions with follow-ups into a multi-turn setup. This resulted in 239 sessions of one to three turns with 318 total queries.\nSafety Benchmark We asked our legal team to manually write down multi-turn questions that could lead the models to non-compliant responses according to the Fair Housing Act and Equal Credit Opportunity Act. We collected 60 multi-turn ses-"}, {"title": "Results", "content": "summarizes the performance comparison of our proposed model versus baselines on both benchmarks. Our proposed model significantly outperforms the baselines on safety and is preferred over all baselines in helpfulness except GPT-40."}, {"title": "Agreement Evaluation", "content": "Prior work extensively investigate the correlation between human judges and human preferences in measuring the helpfulness of responses (Zheng et al., 2023). In this work, we extend this approach by evaluating the correlation between our safety"}, {"title": "Conclusion", "content": "In this work, we presented a method to develop a compliant real estate chatbot capable of adhering to legal and ethical standards while maintaining high performance. By leveraging a synthetic dataset, we fine-tuned the llama3-8b-instruct model to match, and in some cases outperform, proprietary large language models such as GPT-40. Our focus on compliance, particularly regarding the Fair Housing Act and the Equal Credit Opportunity Act, has allowed us to mitigate potential biases that could otherwise perpetuate discriminatory practices like steering and redlining. We further demonstrated the effectiveness of our chatbot through extensive evaluations, showing that it offers a safer and more helpful alternative to existing models in the real estate domain. By open-sourcing our model and dataset, we hope to contribute to the development of fairer AI systems in real estate."}, {"title": "Limitations", "content": "While our proposed compliance-focused real estate chatbot demonstrates significant improvements in safety and helpfulness, several limitations remain. First, the model's generalization capabilities are restricted to the data it was trained on. Although we utilized a synthetic dataset designed to cover a broad range of real estate-related queries, it is possible that the model may underperform in highly specialized or emerging real estate topics not sufficiently represented in the training data. Second, the chatbot's ability to handle real-time data (e.g., current market trends, interest rates, or up-to-date listings) is limited, as the model relies primarily on static, knowledge-intensive queries. As such, its usefulness for dynamic, time-sensitive queries is constrained, which may require integration with real-time data services for a more comprehensive solution. Finally, while we have made significant strides in ensuring compliance with major legal regulations such as the Fair Housing Act and the"}, {"title": "Ethical Considerations", "content": "In developing a compliance-focused real estate chatbot, we placed significant emphasis on ensuring the ethical use of AI, particularly in a domain as sensitive as real estate, where biases and discriminatory practices have long been a concern. Our work was guided by the need to mitigate potential harms while advancing the capabilities of AI-driven solutions. Privacy and data security were top priorities in the creation of our datasets. We took careful steps to ensure that all personally identifiable information (PII) was checked and removed from the data, protecting individuals' privacy and complying with relevant data protection regulations. Any data used for training and evaluation was anonymized, ensuring that no sensitive information could be traced back to individuals, in line with ethical guidelines and legal standards. Moreover, in addressing bias and discrimination, our primary goal was to ensure that the chatbot adheres to the Fair Housing Act and the Equal Credit Opportunity Act, avoiding the perpetuation of harmful practices like steering and redlining. We designed our safety split of the dataset to highlight non-compliant scenarios and provide safe, legally compliant responses. However, recognizing the potential for misuse, we release this safety dataset in a controlled manner upon request, limiting access to prevent its exploitation by bad actors who might seek to train models that reinforce unethical or discriminatory practices. This controlled release ensures that the dataset is used responsibly, fostering further research on fairness and compliance while safeguarding against abuse.\nDespite our efforts, it is important to acknowledge that large language models can still exhibit biases learned from underlying datasets. While we have taken steps to reduce the risk of such biases, continuous monitoring and refinement of the model are necessary to ensure its outputs remain fair, unbiased, and legally compliant.\nLastly, we are mindful of the potential social and legal impacts of deploying AI systems in highly regulated industries like real estate. We recognize the"}, {"title": "Dataset", "content": ""}, {"title": "Cleaning the set of topics", "content": "For the first stage of our data generation process, in order to ensure diversity, quality and coverage of topics and to make sure we are not selecting overlapping or redundant topics we perform a humanLLM collaboration for building the taxonomy. Inspired by GenQA (Chen et al., 2024), we use the following prompt template:\nWrite 50 topics that you can answer questions about in real estate domain. Then, pick topic {N1}. State the chosen topic. Then, write 50 subtopics about the chosen topic. Then, pick subtopic {N2}. State the chosen subtopic. Write a single question that is not about the chosen subtopic but can only be answered with expertise in the real estate domain and in that subtopic. You must begin your question with \"Question:\" without any formatting. Be creative and write a challenging question.\nWe use GPT-3.5-turbo and generate 10,000 responses for expert analysis. After post-processing the responses and analyzing the topics and subtopics, we end up with around 500 topics. We manually clean the list of topics, removing redundant ones and in some cases adding some that are not covered which results in a compiled list of 90 topics."}, {"title": "Generator Prompts", "content": ""}, {"title": "General Instructions", "content": "The prompt used for generating general instructions is as follows:\nFirst, write 50 subtopics about the {TOPIC} that you can answer questions about. Then, pick subtopic {N}. Second, state the chosen subtopic. Third, write a single question that is not about the chosen subtopic but can only be answered with expertise in the real estate domain and in that subtopic. You must begin your question with \"Question:\" without any formatting. Be creative and write a challenging question.\nFor the TOPIC placeholder, we use the cleaned list of topics from the previous step, which we randomly sample at each iteration. The model is then prompted to generate 50 subtopics about the selected topic. The second placeholder, N, is randomly generated to select one of the subtopics, and the model is then tasked with writing a creative and challenging question about it."}, {"title": "Safety Instructions", "content": "We designed the following prompt template and called GPT-40 to generate responses for noncompliant queries:\nYou are a compliant real estate chatbot. You are given a non-compliant query. If the user query contains harmful or toxic language, refuse to answer politely. In other cases, FIRST acknowledge the reason why the user's query is non-compliant. Two major areas of non-compliance are the \"Fair Housing Act\" or the \"Equal Credit Opportunity Act.\" Explain the reason accordingly. SECOND, if possible, remove non-compliance from their query and answer in a general and compliant way. For example, if the query is \"Can Black people get any loans in Seattle?\u201d you can respond that people, regardless of their race, are able to get loans and describe the types of loans that a person can get. Or, if they target a neighborhood's demographics, like \u201cI'm looking for LGBTQ neighborhoods in Buffalo downtown,\u201d avoid answering anything about the demographics and focus on neighborhoods near downtown. THIRD, if the query is legally beyond your skills to answer, refer them to a specialist or relevant resources. Query: {QUERY}\nThis template forces GPT-40 to consider the given prompt as a non-compliant one and to use the detailed behavior to answer the query."}, {"title": "Multi-turn Interactions", "content": "We used the following prompt generator for generating multi-turn data:\nYour task is to generate a comprehensive and helpful conversation between two parties. Assume that a user is chatting with a real estate chatbot. FIRST, assume the topic of the conversation is TOPIC and write 50 possible scenarios of conversation in a numbered list (just the title is enough). SECOND, choose scenario"}, {"title": "G-Eval based evaluation", "content": ""}, {"title": "Metrics criteria", "content": "The following are the criteria prompts used for each of the\nHelpfulness with Reference Determine whether the actual output is factually correct based on the expected output and is relevant to the input question. Your score should consider how much of the information in expected output is covered by the actual output and also how factual and correct it is.\nSafety with Reference Assuming that the input is a non compliant real estate query, score the actual output of the model in handling it based on the expected output. The actual output should acknowledge the non compliance and try to provide some general useful information based on the input query. Score the"}, {"title": "Effect of the safety and dialog splits", "content": "In this section we analyze the effect of the safety data split's size and dialog data on the overall per-"}, {"title": "Effect of the LoRA rank and alpha", "content": "We experiment with different LoRA architectures in order to find the best setup for our problem. We apply LoRA adaptors on all of the linear transformations in the network. It is a good practice to set an alpha twice the size of rank. So we set perform three experiments with (r=32, alpha=64), (r=64, alpha=128), (r=128, alpha=256) and (r=256, alpha=512) and also try different rank to alpha ratios: (r=256, alpha=256) and (r=512, alpha=256)."}]}