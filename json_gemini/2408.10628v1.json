{"title": "Finding the DeepDream for Time Series: Activation Maximization for Univariate Time Series", "authors": ["Udo Schlegel", "Daniel A. Keim", "Tobias Sutter"], "abstract": "Understanding how models process and interpret time series data remains a significant challenge in deep learning to enable applicability in safety-critical areas such as healthcare. In this paper, we introduce Sequence Dreaming, a technique that adapts Activation Maximization to analyze sequential information, aiming to enhance the interpretability of neural networks operating on univariate time series. By leveraging this method, we visualize the temporal dynamics and patterns most influential in model decision-making processes. To counteract the generation of unrealistic or excessively noisy sequences, we enhance Sequence Dreaming with a range of regularization techniques, including exponential smoothing. This approach ensures the production of sequences that more accurately reflect the critical features identified by the neural network. Our approach is tested on a time series classification dataset encompassing applications in predictive maintenance. The results show that our proposed Sequence Dreaming approach demonstrates targeted activation maximization for different use cases so that either centered class or border activation maximization can be generated. The results underscore the versatility of Sequence Dreaming in uncovering salient temporal features learned by neural networks, thereby advancing model transparency and trustworthiness in decision-critical domains.", "sections": [{"title": "1. Introduction", "content": "Techniques such as Activation Maximization [1] and DeepDream [2] have emerged as approaches to make the complex inner workings of deep neural networks (DNNs) more transparent. These methods illuminate the black box of neural networks by visualizing what neural networks learn, significantly improve model interpretability, and provide valuable insights into diagnosing model behavior [3]. Activation Maximization focuses on identifying the input patterns that maximize the response of specific neurons or layers, revealing the features and patterns a model perceives as most salient [1]. Meanwhile, DeepDream leverages the layers of neural networks to generate intricate, dream-like (unreal-looking) images that highlight the learned features in a visually compelling way [2]. Together, these techniques enhance our understanding of how neural networks process information and guide the development of more transparent, effective, and interpretable AI systems. Through the lens of Activation Maximization and DeepDream,\nwe can unravel the intricacies of neural networks, paving the way for advancements in AI that are comprehensible [3].\nIn the evolving landscape of neural network interpretation, adapting Activation Maximization for time series data extends the understanding of how deep learning models perceive and process temporal information. Sequence Dreaming enables visualization of the intricate temporal features and dynamics the network has learned to recognize by manipulating time series data to amplify the patterns that maximally or targeted activate specific neurons within a model. This method sheds light on the model's decision-making process and unveils the temporal sequences and patterns deemed most significant by the neural network. In doing so, Sequence Dreaming bridges the gap between the opaque decision-making of deep learning models and the tangible insights they derive from sequential data, offering another lens through which to interpret and refine models trained on time series similar to shapelet learning [4]. This approach promises to enhance model transparency, facilitate diagnostic analysis, and inspire the development of models for handling the complexities of sequential data analysis.\nIn this paper, we introduce Sequence Dreaming, an adaptation of the activation maximization techniques for DeepDream [2, 5] explicitly tailored for time series data. By applying this method, we aim to enhance the interpretability of deep learning models that process time series data, shedding light on the temporal dynamics or patterns these models capture. To refine and control the generation of artificial time series that maximize neuron activations, we extend Sequence Dreaming with a suite of regularization techniques, including an a-norm, total variation, time point smoothness, Gaussian smoothing, and random noise reinitiation to produce more realistic and informative visualizations. These modifications are designed to mitigate overfitting to noise and emphasize the underlying patterns critical for model decisions. We test our approach on a time series classification dataset tackling predictive maintenance. Our methodology advances the field of model interpretability for time series analysis and offers a framework for improving the transparency and trustworthiness of models deployed in critical decision-making domains.\nSource code for Sequence Dreaming and results of the experiments are online available at:\nhttps://github.com/visual-xai-for-time-series/sequence-dreaming"}, {"title": "2. Related Work", "content": "In explainable artificial intelligence (XAI), elucidating the operational intricacies of machine learning algorithms, particularly for time series data, is imperative for advancing the field's theoretical and practical applications. Theissler et al. [4] delineate a comprehensive framework for examining time series XAI, emphasizing the necessity of addressing interpretability at multiple analytical levels. This multi-tiered perspective is instrumental in unraveling the complex dynamics and temporal dependencies inherent in time series data, thereby facilitating a deeper understanding of model behavior.\nActivation Maximization, within this context, emerges as a crucial methodology for probing the internal representations developed by neural networks during the training process [1]. It accomplishes this by optimizing input signals to obtain maximal responses from specific neurons or layers, effectively revealing the features and patterns deemed most significant by the model for its decision-making processes. DeepDream enhances Activation Maximization by"}, {"title": "3. Activation Maximization", "content": "Activation Maximization emerges as a promising technique designed to reveal the features most salient to individual neurons within a trained neural network, enabling interpretability [2]. Post-training, the focus shifts to decoding the learned representations by identifying the inputs that provoke maximal activation in specific neurons. This process, integral to Activation Maximization, aims to uncover the features or patterns to which a neuron is most responsive by iteratively refining the input to maximize a neuron's activation, providing deeper insights into the model's internal representations [1]. However, applying this technique to time series data introduces novel challenges necessitating regularization strategies.\nIn time series classification, we can define a time series by $ts = (t_1, t_2,...,t_m) \\in R^{m\\times d}$ as an ordered set of m real-valued observations (or time steps), with dimensionality d [4]. For univariate time series, we have d = 1 and thus our $ts \\in R^{m}$. Given a trained model M and a target class c, the activation maximization is described as follows: Consider a score function $S_c: R^m \\rightarrow R$ and let $S_c(I)$ denote the score of the class c from M computed by the classification layer of the model for an input $I = (x_1, x_2,...,x_m) \\in R^m$, e.g., I = ts and thus $S_c(ts)$. Thus, our previous time series ts works as an input for the model M. Next, we want to find an L2-regularised input, such that the score $S_c$ is maximized, i.e.,\n$$\\max_{ts} S_c(ts) \u2013 \\lambda||ts||^2,$$\nwhere $\\lambda > 0$ is a regularisation parameter. By employing the back-propagation technique, we can identify an input, referred to as ts, that is locally optimal in terms of the model's criteria. Rather than modifying the network's weights, we hold them constant at the values established during the training phase and instead focus our optimization on the input ts itself. In our case, we focus on $S_c(ts)$ and, in the first step, remove the L2-regularised input, focusing directly on the maximization of the activation score of the class, i.e., $\\lambda = 0$. For this approach, we use a gradient ascent to fine-tune the input to increase the activation."}, {"title": "4. Regularization Tricks", "content": "Regularization plays a crucial role in Activation Maximization, primarily to ensure the generation of interpretable, meaningful, and visually coherent inputs [6]. It prevents the optimization process from overfitting to noise, which would otherwise lead to unrecognizable patterns that maximize neuron activation but lack relevance. By introducing regularization, such as Gaussian blur [6], the process is guided towards producing aesthetically pleasing inputs closer to the distribution of natural inputs, enhancing the interpretability of the results. This ensures that the generated inputs reflect genuine features the network learns from real-world data, offering clearer insights into its internal representations and improving our understanding of its decision-making processes.\nExploring regularization techniques that do not alter the loss function presents an approach in optimization we first want to explore, particularly within Activation Maximization using gradient ascent. Thus, we collect and reformulate the approaches from the literature [6, 9, 11] to time series. We focus on the following approaches:\nClamping to borders, as described by Yosinski et al. [6], ensures that the optimized input does not venture beyond the predefined input space, maintaining realism and coherence. This technique effectively keeps the activation maximization process within the bounds of the training data distributions.\n$L_2$ decay, another technique highlighted by Yosinski et al. [6], imposes a regularization term that penalizes high-frequency noise in the generated input, promoting smoother and more interpretable features. This regularization helps focus on the essential features that the neuron detects rather than artifacts.\nRandom scaling introduces variability in the time points of the input during the optimization process, encouraging the network to identify and amplify scale-invariant time points. This technique enriches the diversity of patterns that activate the neurons, showcasing the model's robustness to scale variations.\nMoving average smoothing is applied to the input to mitigate rapid fluctuations, ensuring that the input generation progresses smoothly toward enhancing meaningful patterns. This approach helps stabilize the time points, making it less susceptible to local optima.\nExponential smoothing, when applied to the input during the optimization process, em- phasizes the significance of smoother inputs by assigning them greater weights. This method adapts the input dynamically, ensuring that the activation maximization is finely tuned based on the most recent trends and patterns in the data, thereby fostering a more responsive and effective approach to highlighting the neuron's preferences.\nGaussian blur filter, as employed by Yosinski et al. [6], aids in reducing high-frequency noise across the optimization iterations, thereby focusing the model's attention on broader, more significant patterns. This technique contributes to the production of more visually appealing and interpretable inputs.\nRandom reinitiation of the input, if the optimization process shows no significant change, acts as a reset mechanism to escape from potential plateaus in the activation landscape. This"}, {"title": "4.2. With regularization on the loss", "content": "After we adopt a regularization approach without modifying the loss function, utilizing gradient ascent as demonstrated by Yosinski et al. [6], we transition the approach imposing regularization directly on the loss function, thereby altering our optimization strategy. We no longer perform a full activation maximization but aim to approximate a specific target activation in advance. Thus, we shift from gradient ascent to gradient descent, employing traditional optimizers and thus integrating a specific loss term with, for example, stochastic gradient descent with momentum to refine our optimization process further.\nIn extending Equation 1, we incorporate total variation regularization, as outlined by Si- monyan et al. [1], to enhance the visual clarity and reduce noise in the generated input, which also enables smoothing for the input time series. To optimize this extended formulation, we employ the Adam optimizer [12], known for its efficiency in handling sparse gradients and adaptive learning rates, complemented by weight decay for improved regularization on the input, following the approach suggested by Mahendran and Vedaldi [5]. Additionally, we meticulously adjust the normalization parameters for the $L_2$-Regularization (weight decay on the input) and Total Variation (on the input) and fine-tune the weighting of the loss terms to identify the most effective settings for our optimization objectives, ensuring a balanced and nuanced approach to maximizing activation as closely to our target as possible.\nThus, we get the formula from Mahendran and Vedaldi [5]\n$$\\min_{ts} \\frac{|S_c(ts) \u2013 S_c(T)|^2}{|S_c(T)|^2} + \\lambda_{\\alpha} \\cdot ||ts|| + \\lambda_{\\beta} \\cdot TV(ts, \\beta),$$\nwith $S_c(ts)$ as the score of the class c for an input ts and $S_c(T)$ as the score of the class c for a target T. The target T and $S_c(T)$ as a reference for the activation we want to achieve, as mentioned before. The parameter a changes the input range to be encouraged to stay within an interval if set to large values > 2, commonly set to a = 6 [5]. The parameter $\\beta$ controls the total variation factor and, thus, how similar the inputs in the neighborhood should be. The parameter is normally $\\beta > 1$ so that the weighting of the error between the time points can be adjusted. In our case, we want a confident prediction of a sample toward the class c after the softmax, e.g., M(T) = [0, 1] for two classes and c = 2 for $c \\in \\{1,2\\}$.\nTV corresponds to the total variation adapted and approximated with\n$$TV(ts, \\beta) = \\sum_{i=1}^{m}(t_{i+1} - t_i)^{\\beta}$$"}, {"title": "4.3. Sequence Dreaming combining both", "content": "Sequence Dreaming combines the most promising regularization techniques from previous work on images with specialized ones on time series to generate an approach for time series deep learning models. By extending Equation 2 with an additional smoothness factor similar to TV but scaled for time series lengths, Sequence Dreaming achieves a more refined regularization process. This method employs a combination of the new loss extension, Gaussian blur, clamping, and random noise to ensure effective regularization, particularly when the loss is not significantly changing. Furthermore, Sequence Dreaming transitions from using Adam to a gradient descent method without momentum or other improvements, necessitating a few more optimization steps to achieve the desired results.\nFirst, we extend Equation 2 by adding an additional regularization term\n$$\\min_{I} \\frac{S_c(ts) \u2013 S_c(T)^2}{S_c(T)^2} + \\lambda_{\\alpha} \\cdot ||ts|| + \\lambda_{\\beta} \\cdot TV(ts, \\beta) + \\lambda_{sm} \\cdot SM(ts),$$\nwhere (SM)oothness SM(ts) is defined as\n$$SM(ts) = \\frac{1}{m-1}\\sum_{i=1}^{m-1}|t_{i+1} - t_i|.$$\nSM takes the input, calculates the discrete difference between time points, uses the absolute value of the difference, and sums these up. The normalization of the length enables an equal weighting between every time point.\nWhy SM? - The inclusion of two smoothing factors, despite TV's inherent smoothing capabilities, arises from their distinct focuses and benefits. TV primarily aims to minimize errors between individual time points, emphasizing the coherence of each point in the time series. In contrast, the second smoothing factor, SM, targets the entire time series, enhancing overall normalization and providing a broader perspective on data regularization. While TV hones in on reducing local discrepancies, SM ensures a holistic approach, balancing the series as a whole and easing the optimization path in the loss landscape. This dual-faceted approach"}, {"title": "5. Evaluation of Results and Discussion", "content": "Evaluation is crucial because different approaches can generate activation maximization samples that are not always plausible for our dataset. Outside the projected data, these samples may still maximize neuron activation but in a different region, as illustrated in Figure 1b. Ellis et al. [11] assess their activation maximization approach by comparing the activation distribution against the data, performing a visual evaluation, analyzing the frequency domain, and examining frequency importance. Similarly, we will use a visual evaluation to present the generated time series. Subsequently, we will employ measures for outlier detection to assess the plausibility of the generated activation maximization input toward the model and the data. Finally, we will integrate out-of-distribution analysis and visual evaluation with distribution plots to compare Sequence Dreaming with other approaches."}, {"title": "5.2. Out-of-distribution evaluation", "content": "After visually comparing the activation maximization results, we aim to assess their performance using various quantitative methods. The distributions of the class' activations for the training data are particularly interesting in this context to compare them to the generated time series. Different methods exist for evaluating data based on distributions, though we focus on outlier analysis to assess the quality of the generated time series towards the training data.\nIn our case, we employ outlier analysis using the Mahalanobis distance [14] on time series data and activations. The Mahalanobis distance measures the distance between a point and a distribution, considering the correlations between variables. It is calculated by determining how many standard deviations away a point is from the mean of the distribution, considering the covariance matrix to account for the data's spread and orientation. This makes it a useful approach for identifying outliers, as points that are far from the mean in terms of the Mahalanobis distance are likely to be anomalies. We believe that applying this measure directly to the time series data is not ideal due to its diversity; only existing time series or those generated in the frequency domain would fit within the distribution. However, we do think this approach can effectively demonstrate how well the generated time series fit into the activations, revealing whether they are more on the border with high activation or more centered."}, {"title": "5.3. Visual out-of-distribution evaluation", "content": "After inspecting the raw numbers and uncovering some interesting findings, we combine the visual observations' results and the numbers from the outlier detection into distribution plots.\nFirst, we use violin plots to show activations and the results of the generated time series, similar to Ellis et al. [11]. Next, we use projections (PCA) of the activations, as seen in Figure 1a."}, {"title": "6. Conclusion and Future Work", "content": "In conclusion, we conducted a study on effective regularization for time series classification data without altering the loss function, using only gradient ascent and a modified loss function to achieve good results. Based on our observations, we introduced Sequence Dreaming, an adap- tation of previous methods, incorporating enhanced regularization with increased smoothness. The results demonstrate that adding regularization terms to data transformation and the loss function can be effective. We learned that activation maximization with regularization terms is highly parameter-sensitive but can produce convincing results comparable to those of frequency domain approaches, which inherently have advantages. Our PCA projections of the activations particularly highlight how our approach can generate precise time series corresponding to specific activations with smooth properties.\nFor future work, several promising directions are poised to refine the Sequence Dreaming process for time series data. Among these, exploring advanced regularization techniques, such as introducing perturbations directed towards less contributing time points informed by attributions, offers a nuanced method for enhancing model interpretability while minimizing information loss. Additionally, the adoption of genetic or evolutionary algorithms, inspired by the works of Nguyen et al. [7] and Xiao and Kreiman [15], presents an intriguing avenue for optimizing activation maximization through a process that mimics natural selection, potentially uncovering novel and highly effective stimuli. Moreover, incorporating wavelet transforms, akin to Fourier transformations, could provide a more comprehensive analysis of time series by capturing both the frequency and location in time of significant features, thereby offering a richer representation of the data for time series activation maximization generation. Also, incorporating attributions more heavily in the process could lead to more plausible activation maximization time series as these can regularize the generation process. However, selecting a working attribution technique is not trivial and needs another careful consideration [16]."}]}