{"title": "Do Large Language Models Truly Grasp Mathematics? An Empirical Exploration", "authors": ["Wei Xie", "Shuoyoucheng Ma", "Zhenhua Wang", "Enze Wang", "Baosheng Wang", "Jinshu Su"], "abstract": "Despite their proficiency in math tasks, the mechanisms underlying LLMs' mathematical reasoning abilities remain a subject of debate. Recent studies suggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning by encouraging LLMs to employ human-like logical reasoning (System 2), enabling them to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs genuinely possess System 2-like logical reasoning, we introduced targeted modifications to CRT problems. Our findings reveal that, despite the use of CoT prompts, mainstream LLMs, including the latest ol-preview model, continue to exhibit a significant error rate. Further analysis indicates that they predominantly rely on System 1-like intuitive reasoning and pattern matching derived from training data, rather than demonstrating mastery of mathematical thinking. This discovery challenges the prevailing notion that LLMs possess genuine logical reasoning abilities and that CoT can enhance them. Consequently, this work may temper overly optimistic projections regarding LLMs' advancement toward artificial general intelligence.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are hailed as the dawn of Artificial General Intelligence (AGI) [1]. LLMs such as ChatGPT[2], GPT-4[1], Claude[3], Gemini [4], GLM[5], and ol-preview [6] have garnered significant attention from both academia and industry, have demonstrated tremendous potential in various fields such as education[7], healthcare [8, 9], coding[10], and social governance[11]. One of the key reasons for this is the 'emergence phenomenon\u2019[12], enabled through massive training data and vast parameter counts: LLMs exhibit unexpected competence in tasks they were not specifically trained for.\nUsing mathematical skills as an illustration, not all LLMs have received specific training for solving complex mathematical problems during their development. Nonetheless, most of them have shown remarkable abilities to tackle these issues. Moreover, by using the CoT method, the ability of LLMs to solve mathematical problems can be further enhanced [13, 14, 15, 16, 17]. However, due to the interpretability challenges posed by large-scale neural networks, there is no scientific consensus yet on the origins of the mathematical capabilities of LLMs. Meanwhile, recent research has questioned the effectiveness of the CoT method, suggesting that CoT provides strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks[18].\nSpecifically, Thilo Hagendorff et al. [19] have demonstrated through experiments that the CoT method can effectively help LLMs handle the pitfalls in Cognitive Reflection Test (CRT) problems[20]. CRT problems are some well-crafted math or logic problems that human testers often get wrong due to intuitive thinking (System 1[21, 22, 23, 24]). By using the CoT method, LLMs are prompted to rely more on human-like logical reasoning (System 2[21, 22, 23, 24]), enhancing their accuracy in solving these problems. Advanced models like GPT-4 have even achieved higher accuracy than humans in these CRT tasks. However, the authors also raised speculative questions in their work, suggesting, \"It is possible that some models encountered enough examples in their training to solve them 'from memory' \" [19]. Our study aims to investigate this speculation through empirical research.\nWe repeated and improved upon the experiment conducted by the previous study [19], making targeted (yet as simple as possible) modifications to the original CRT problems. If LLMs genuinely possess the intrinsic capability to comprehend mathematical logic, as is widely hypothesized, their accuracy in responding to the modified queries should not experience a marked decrement. However, our experiments revealed a distinctly opposing result: even when employing the CoT approach, prominent LLMs, including the latest ol model, continued to manifest a considerable error rate for the modified problems. Further analysis indicates that LLMs may have not developed a logic reasoning proficiency akin to System 2, nor have they acquired comprehensive mathematical cognitive abilities, including principles of mathematical operations and algorithm selection. They predominantly resort to a methodology reminiscent of System 1, which involves matching and producing responses based on the similarity between user inquiries and training data during the process of text generation. This investigation may serve to temper the overly optimistic anticipations regarding the effectiveness of CoT and the competencies of LLMs in approximating AGI."}, {"title": "2 Experiment I: Changing the numbers in a problem without altering its description and principle", "content": "We first replicated the experiment on the CRT3 dataset from Thilo Hagendorff's study[19], which comprises 50 mathematical test problems. We introduced three types of modifications to the test problems, as delineated in Table 1. Type A represents the original problems in the CRT3 test, characterized as typical exponential growth problems, and incorporates three numbers in the problem statement. Type B modifies two of these numbers, while type C modifies all three. Type D replaces two numbers with letters, transforming the arithmetic problem into an algebraic one. To prevent LLMs from being unsure how to handle algebraic symbols, we appended a suffix for guidance: \"X and Y are both numbers, you can use them to represent the final answer.\" The modifications in Types B, C, and D do not alter the description of the original problem (Type A). Thus, the underlying mathematical principle of the original problem remains unchanged. To uncover the problem-solving strategies of LLMs, we appended explicit instructions ('Please think step by step') to the end of each of the four types of problems, encouraging the use of the CoT method."}, {"title": "Result Analysis of Experiment I", "content": "The modifications in types B, C, and D only altered the specific numbers in the original problems without changing the problems' mathematical principles and computational rules. For a computer program, merely changing the input parameters without modifying the computation process should not lead to a decrease in accuracy. However, the performance of LLMs differs significantly from that of computer programs. We analyzed all incorrect responses provided by the five LLMs for each type of problem and found that in type B, errors arising from mathematical operations (e.g., 16/2=4) constituted 6.78%, while those due to errors in calculation steps (e.g., omitting a step or altering the original calculation method) accounted for 83.47%, and errors caused by both factors accounted for 9.75%. In type C, errors solely due to mathematical operations accounted for 5.11%, errors due to calculation steps accounted for 85.51%, and errors caused by both factors accounted"}, {"title": "3 Experiment II: Modifying the principle of a problem while maintaining similarity in its description", "content": "In this experiment, we substantially altered the fundamental principles of the mathematical problems, endeavoring to preserve descriptions that closely mirrored the original versions. Subsequently, we investigated whether LLMs persisted in utilizing their prior problem-solving strategies or adapted their methods to the modified problem contexts throughout the problem-solving process. We conducted three distinct sub-experiments corresponding to the CRT1, CRT2, and CRT3 datasets."}, {"title": "3.1 For CRT1: Transforming Additively Separable Problems into Non-Additively Separable Problems", "content": "For instance, Figure 2 illustrates the crucial distinction between an original CRT1 problem and its modified version. In the original problem, the total cost is simply the sum of the prices of two items. However, in the modified version, reaching the two items from the starting point through a shared segment must be deducted from the overall distance calculation. We use the string similarity calculation function within the difflib module of Python to quantify the degree of similarity in textual expression between the modified problem and the original one, resulting in an average similarity of 75.91%. Despite the similar wording in the problem statements, the underlying mathematical principles are fundamentally distinct. To reveal how LLM approaches these problems, we included a CoT prompt for both the original and the modified problems."}, {"title": "3.2 For CRT2: Transforming Problems Related to the number of Individuals into Problems Independent of Individual Count", "content": "As illustrated in Figure 4, we take the first problem in the original CRT2 dataset as an example. In the original problem, all workers work together, and the total amount of product is related to the number of workers. In the modified problem, working together is changed to walking together, therefore, the total distance walked is independent of the number of people. We calculated the average similarity using the same function as in the previous sub-experiment, yielding 83.67%. Although the descriptions of the original problem and the modified one are similar, the underlying mathematical principles are entirely different. To uncover the problem-solving strategies of LLMs, we added CoT prompts to both the original and the modified problems."}, {"title": "3.3 For CRT3: Transforming Exponential Growth Problems into Linear Growth Problems", "content": "Figure 6 presents the first problem in the CRT3 dataset as an illustrative example. In the original problem, the number of viruses doubles each day, resulting in exponential growth of the total number. In contrast, the modified problem features a constant daily increase in the number of viruses, which leads to linear growth. We calculated the average similarity using the same function, yielding a result of 88.89%. To further elucidate the problem-solving strategies of the LLM, we added a CoT prompt for both the original and the modified problem."}, {"title": "Result Analysis of Experiment II", "content": "The modifications to CRT1, CRT2, and CRT3 altered the mathematical principles underlying these problems with a effort to preserve the similarity of their statements. The experimental findings reveal that LLMs continue to rely on their inherent problem-solving methodologies, even when provided with CoT prompts as guidance. This observation further substantiates our hypothesis that generative LLMs operate similarly to a word chain game; they can address some mathematical issues probably because their training datasets include analogous problems. Consequently, LLMs tend to adopt problem-solving strategies"}, {"title": "4 Experiment III: Replicating Experiment I & II on the Latest ol Model", "content": "Given OpenAI's latest release of the ol-preview model, which focuses on enhancing logical reasoning capabilities [6], we have repeated the above experiments with o1. The results are shown in Figure 8.\nIn terms of replicating Experiment I, changing the numbers in the problem statements did not significantly affect the accuracy of ol's answers, which aligns with expectations. This observation suggests that (1) o1 has enhanced numerical computation capabilities compared to previous LLMs, and (2) the specific number given in the problem statement does not influence the method ol chooses to solve the problem. This might imply that o1 has potentially incorporated prompts like \"list the equations before solving\" into its built-in thought process. Nevertheless, this speculation cannot be officially confirmed, as OpenAI has not released technical details regarding ol's improved reasoning capabilities.\nHowever, when replicating Experiment II, the average accuracy rate of ol was only 10%, which was even lower than that of most previous LLMs. After making minimal textual modifications while altering the mathematical principles underlying the original CRT1, CRT2, and CRT3 problems, ol persisted in selecting problem-solving approaches based on the mathematical principles corresponding to the original problems. The errors stemming from this persistence constituted 100%, 68.42%, and 75.86% of the incorrect responses to the three types of CRT problems, respectively. This may indicate that technologies such as CoT, prompt engineering, and even fine-tuning tailored to specific datasets, cannot fundamentally enhance the ability of LLMs to comprehend mathematical problems. The reason for this lies in the fact that the learning paradigm of LLMs has not undergone substantial changes (such as those involving word chains), leading to their thought patterns being deeply ingrained."}, {"title": "5 Conclusion", "content": "This paper draws on the classic CRT problems from human psychology to conduct an empirical study on the \"emergence\u201d of mathematical capabilities in mainstream LLMs. By constructing forward experiments (Experiment 1) and reverse experiments (Experiment 2), the conclusions were starkly different from mainstream views. Specifically:\n(1) LLMs tend to match problem-solving strategies based on textual similarity rather than truly understanding the principles of mathematical problems.\n(2) Even with the introduction of the CoT reasoning or improving reasoning ability through specialized training (e.g., ol), such methods cannot entirely adjust the LLMs' inherent but incorrect problem-solving approaches.\nThis paper focuses on mathematical ability, which is the foundation of all scientific research, as its central point of study. It provides a counterexample to the overexpectation of the \"emergence phenomenon\" of LLM capabilities and the CoT method, both of which are currently receiving much attention."}]}