{"title": "Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition", "authors": ["Erlygin Leonid", "Alexey Zaytsev"], "abstract": "Accurately estimating image quality and model robustness improvement are critical challenges in unconstrained face recognition, which can be addressed through uncertainty estimation via probabilistic face embeddings [25]. Previous research mainly focused on uncertainty estimation in face verification, leaving the open-set face recognition task underexplored. In open-set face recognition, one seeks to classify an image, which could also be unknown. Here, the low variance of probabilistic embedding does not imply a low error probability: an image embedding could be close to several classes in a gallery, thus yielding high uncertainty.\nWe propose a method aware of two sources of ambiguity in the open-set recognition system: (1) the gallery uncertainty caused by overlapping classes and (2) the uncertainty of the face embeddings. To detect both types, we use a Bayesian probabilistic model of embedding distribution, which provides a principled uncertainty estimate. Challenging open-set face recognition datasets, such as IJB-C, serve as a testbed for our method. We also propose a new open-set recognition protocol for whale and dolphin identification. The proposed approach better identifies recognition errors than uncertainty estimation methods based solely on image quality.", "sections": [{"title": "1. Introduction", "content": "The open-set face recognition (OSFR) problem arises in many practical applications [10]. The key distinction between the OSFR and a face classification problem is the possibility of occurrence of previously unseen subjects. During the inference, the OSFR system must accept known subjects, identify them by finding the most similar subjects in the gallery of known people, and reject subjects from outside the gallery, i.e., those not seen previously. An established baseline solution to OSFR is to train a face embedding model that produces discriminative feature vectors and uses cosine distance between them to estimate the semantic difference between face images [4,26]. The test image is rejected if its distance to each subject in the gallery is higher than a certain threshold. Otherwise, it is accepted, and the closest gallery subject label is assigned.\nHowever, in face recognition deterministic embeddings are not robust to corrupted input images. When critical visual features are missing, the embeddings can shift unpredictably, leading to recognition errors [25]. To address this, probabilistic embedding models, which account for data uncertainty, have gained attention [11, 17, 25]. Probabilistic embeddings enable uncertainty-aware distance computation, improving template aggregation by producing a single embedding from multiple images of the same person and facilitating low-quality image filtering [25]. These uncertainty-aware models have been tested on face verification tasks and have outperformed deterministic embedding models.\nThe existing OSFR literature typically overlooks input uncertainties, focusing primarily on reducing recognition errors. This approach is taken because applying existing uncertainty estimates directly lacks the essential properties needed for a robust OSFR uncertainty estimator. For instance, an OSFR system should exhibit high uncertainty when embedding a test image close to several classes in the gallery, as shown in Figure 1. In such cases, the probabilistic embedding uncertainty predicted by SCF [17] is relatively low and fails to reflect the uncertainty related to identification ambiguity adequately. Introducing such effects into the uncertainty estimate necessitates dealing with complex Bayesian integrals and diverse uncertainty estimates from different sources that lack consistency and this issue hasn't been resolved yet.\nOur work introduces the holistic approach to estimating uncertainty for OSFR. Based on possible error types, the estimate assigns low confidence in three cases: low image quality, image embedding close to several gallery classes, and those near the decision boundary between rejection and acceptance. For the last two types of errors, we present an illustration in Figure 2. The most complete declaration of presented uncertainties is the posterior probability represented as the integral over the embedding space. We use Laplace approximation to transform the integral into the sum of two terms. The first term refers to the uncertainty related to the relative image position c.t. the training sample; the second term deals with estimating the image quality.\nTo consider information about face embedding relative position, we reformulate the open-set face recognition problem as a classification problem by adding an out-of-gallery class corresponding to unknown subjects. In this way, our Bayesian model defines the categorical distribution of possible decisions in the OSFR system for each test image. We utilize the von Mises-Fisher (vMF) [6] and Power [3] distributions with a density defined on a sphere. In such a formulation, the class with the maximum probability is predicted, and the probability of this class is used as a measure of recognition uncertainty. For the image quality estimation, we adopt existing SCF [17] or PFE [25] approaches. Finally, we estimate the true uncertainty after calibrating both scores [15]. To our knowledge, this work is the first to combine information about face embedding relative position with information about embedding quality to obtain a robust uncertainty estimation method in OSFR. By considering all the components, we received superior quality scores.\nThe key contributions of our work are as follows:\n1.  A gallery-aware probabilistic method for uncertainty estimation in an open-set recognition problem that produces calibrated decision probabilities, naturally incorporating information about face embedding's relative positions.\n2.  A method to combine multiple uncertainty estimates. Based on this approach, we combine a gallery-aware uncertainty measure with a neural network-based face image quality estimator to cover all sources of possible errors.\n3.  A new Open Set Recognition protocol for the Happywhale dataset [2] to test if our method can be used in a broader than face domain range.\n4.  A careful experimental evaluation of the proposed uncertainty estimation approach shows that our method outperforms uncertainty estimators based solely on face image quality information."}, {"title": "2. Related Work", "content": "Open-set face recognition problem The amount of conducted research in open-set recognition (OSR) demonstrates the importance of this topic [9]. The OSR task differs from the classification task because during inference, previously unseen (out-of-gallery) images are present. Existing works have focused on developing OSR algorithms that can accurately reject out-of-gallery test samples, leaving the uncertainty estimation and the probability calibration tasks poorly explored. Current solutions to the OSR problem for the face domain are no exception: most works use simple cosine distance-based rejection rule and focus instead on the improvement of the quality of the embedding model by incorporating recent advances in the field, including deep learning models trained with utilization of loss functions like ArcFace [4] and CosFace [27].\nUncertainty estimation in open-set recognition Uncertainty estimation can provide benefits for face recognition problems - both in terms of the final recognition accuracy and decision-making based on the estimated level of confidence [25]. There are two primary sources of uncertainty: model and data uncertainty [1].\nIn the case of face recognition, model uncertainty is caused by imperfect embedding model training. This may result in noticeable embedding vector variation for different face images of the same person, leading to OSFR errors. Data uncertainty describes uncertainty in input face images, which can be caused by image corruption or the coverage of critical visual features, such as nose or mouth [25]. There are three types of OSR errors: false acceptance, false rejection, and misidentification [26], and an ideal recognition system should predict high uncertainties when it is likely to make any of these errors. Although uncertainty in OSR has been studied [22, 23], to our knowledge, it was never used as the likelihood of OSR errors. Instead, its usage focused on finding out-of-gallery samples. There are some ad hoc methods for constructing open-set decision probability distribution. However, they are not calibrated and cannot be used for model uncertainty estimation [18].\nData uncertainty in face recognition Numerous works are dedicated to predicting probabilistic face embedding [11, 17, 25], focusing on a common problem. In contrast to deterministic face recognition approaches, such as Arc-Face [4], they predict the whole distribution of embeddings. The variance of predicted distribution can be used to measure the ambiguity of the input face image. Images of low quality or with occluded distinctive face parts would have high variance. These ambiguous images can lead to OSFR errors. Thus, their identification can help to make face recognition more robust. The SCF [17] predicts the concentration parameter for von Mises-Fisher (vMF) distribution on sphere [6] to model embedding uncertainty. We elaborate on this idea and use vMF and Power Spherical [3] distributions with fixed concentration parameters for all gallery classes to build a Bayesian model of embedding distribution of known subjects, which allows for natural model uncertainty estimation. In our work, we use a pretrained face embedding model and thus deal only with data uncertainty. The proposed Bayesian model captures data uncertainty related to possible gallery classes overlapping and the presence of out-of-gallery samples that look like some gallery classes. The SCF model predicts data uncertainty related to face embedding ambiguity. It yields high uncertainty for falsely rejected samples due to poor image quality.\nEnsembling for uncertainty estimation A natural idea for uncertainty estimation is to obtain an ensemble of deep learning models and use a disagreement between predictions of ensemble members as an uncertainty estimation [16]. A subsequent study proposes using a collection of weights from the optimization trajectory to estimate the variance for predictions [19]. A prominent work [29] proposes using an ensemble of vMF-based probabilistic models, improving quality and uncertainty for the face recognition problem. Their work uses specific properties of the vMF distribution to derive the analytical uncertainty estimation for an ensemble of models, making this approach difficult to apply to two general models with general uncalibrated uncertainty estimations. Moreover, such an approach requires constructing multiple deep-learning models, leading to prohibitive computational costs [8]. A prospective remedy is to use approximations like dropout-based MCMC [7], while often these methods compromise accuracy for efficiency [24]. Despite the richness of the literature devoted to the ensembling of uncertainty estimation, little attention has been paid to the ensembling of uncertainty estimates, given that they are uncalibrated and can differ from each other in the number of parameters and architecture.\nResearch gap To our knowledge, no present method has used the data uncertainty of a calibrated OSFR probabilistic model together with the data uncertainty caused by corrupted face images. We construct a Bayesian model of embedding distribution of known subjects, which yields OSFR decision probabilities and natural model uncertainty, and calibrate it on validation data using beta calibration. Combining this model uncertainty with data uncertainty produced by SCF, we get an uncertainty estimator that can cover all sources of possible OSFR errors."}, {"title": "3. Methods", "content": "This section is divided into two parts: an introduction and a description of our proposed method. In the introductory part, we define the open-set face recognition (OSFR) problem, following the framework outlined in the Handbook of Face Recognition [26]. We also discuss the task of uncertainty estimation in OSFR and present the metrics used to assess both recognition and uncertainty estimation quality. The second part begins with a description of baseline approaches. We then introduce our proposed method, detailing how it improves on existing techniques."}, {"title": "3.1. Open-set face recognition problem statement", "content": "An OSFR system has access to a gallery G of images of known subjects. Each subject may have several images in the gallery. The gallery is structured as a set of templates  $\\mathcal{G} = \\{g_1,..., g_K\\}$, where each template $g_i$ consists of available images of a subject with index i. So, the gallery has K distinct classes of images associated with known people.\nAt inference time, the system receives a test template p with several images of a person  $\\left\\{x_1,...,x_m\\right\\}$ with, in most cases, m = 1. The system should correctly answer two questions:\n1.  Does a template associated with the subject depicted on an image p exist in the gallery  $\\mathcal{G}$?\n2.  If such a template $g^*$ exists, then what is its index $\\hat{i}(g^*) \\in \\{1,..., K\\}$?"}, {"title": "3.2. Uncertainty in open-set face recognition", "content": "A robust face recognition system should be capable of estimating the uncertainty of its predictions. High uncertainty should correlate with a high probability of error, prompting the system to refuse to recognize the test template p to avoid potential misrecognition. In applications, the recognition system may suggest a user to retake the photo and try again.\nUncertainty arises when the visual features of faces in the presented template are occluded or corrupted. Additionally, the system experiences uncertainty in its recognition decision when the test template embedding is close to multiple classes in the gallery or lies on the decision boundary between acceptance and rejection. For instance, when two similar-looking identities are present in the gallery, as depicted in Figure 1, it becomes unclear which class should be assigned to p if it is accepted.\nUncertainty estimation can be formalized by introducing uncertainty score u(p) that takes a template p as an input. If u(p) is higher than a threshold t, the system asks the user to redo the photo, hoping to decrease the overall recognition error rate. Below, we propose our uncertainty score and a few baselines."}, {"title": "3.3. Quality metrics for uncertainty evaluation in OSFR", "content": "OSFR quality To define OSFR errors, we consider three sets of face image templates [26]:\n*   $\\mathcal{G}$ - a gallery,\n*   $\\mathcal{P}_g$ - a set of test templates which have a corresponding class in the gallery,\n*   $\\mathcal{P}_N$ - a set of negative test templates which have no corresponding class in the gallery.\nFor each test template in $\\mathcal{P}_g$ and $\\mathcal{P}_N$, the OSFR predictions described in the previous section are produced. The OSFR system should reject templates from $\\mathcal{P}_N$ set, accept and correctly identify templates from $\\mathcal{P}_g$ set. So, we can have three kinds of OSFR errors: False Acceptance, False Rejection, and Misidentification.\nTo get a unified recognition quality measure, we follow [23] and compute the $F_1$ score:\n$\\begin{equation}F_1 = 2 \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}},\\end{equation}$\n$\\begin{equation}\\text{precision} = \\frac{TP}{TP + FP}, \\quad \\text{recall} = \\frac{TP}{TP + FN}\\end{equation}$\nA test template p from $\\mathcal{P}_g$ is True Positive (TP), if p is accepted and $\\hat{i}(p) = i(p)$, and False Negative (FN) otherwise. We count a test template p as False Positive (FP), when $p \\in \\mathcal{P}_N$, and p is accepted. In the formulae, we refer to abbreviations TP, FN, and FP as the number of objects in the corresponding sets.\nWe also use more common in face recognition Detection & Identification rate (DIR) and False alarm rate (FAR) metrics:\n$\\begin{equation}\\text{DIR} = \\text{recall}, \\quad \\text{FAR} = \\frac{FP}{|\\mathcal{P}_N|}\\end{equation}$\nWe minimize FAR and maximize DIR and $F_1$. We also can fix FAR by varying a threshold and report DIR or another metric for a fixed FAR.\nUncertainty estimation quality Two natural metrics for estimating the quality of obtained uncertainties are the area under a rejection curve and the quality of templates obtained by aggregation based on uncertainty [5].\nAt first, we may filter test templates with uncertainty scores higher than some threshold t and compute recognition accuracy metrics, e.g., $F_1$ score, to compare different uncertainty estimation methods. We obtain a prediction rejection (PR) curve by varying the percentage of the rejected templates. The area under the PR curve is a quality measure: the higher the curve, the better we identify erroneous templates [11]. We consider uncertainty estimation methods that yield higher gains in target metrics more effective. In our study, we compare uncertainty estimation methods using the Prediction Rejection Ratio (PRR) [5]:\n$\\begin{equation}\\text{PRR} = \\frac{\\text{AUCPR}_{unc} - \\text{AUCPR}_{rnd}}{\\text{AUCPR}_{oracle} - \\text{AURCPR}_{rnd}},\\end{equation}$\nwhere $\\text{AUCPR}_{unc}$ area under the PR curve for an uncertainty estimation method, $\\text{AUCPR}_{rnd}$, $\\text{AUCPR}_{oracle}$ areas under random and optimal PR curves."}, {"title": "3.4. Holistic uncertainty estimation", "content": "Our primary goal is to obtain the class distribution $p(c|x)$ conditional on the face image x:\n$\\begin{equation}p(c|x) = \\int p(c|z)p(z|x)dz,\\end{equation}$\nwhere c is an ID or out-of-gallery label, x is a single-image template p, and z is the embedding of p. So, $p(z|x)$ is the distribution of embeddings given x, and $p(c|z)$ is the distribution of classes given embeddings. The magnitude of $p(c|x)$ would correspond to the uncertainty of the model.\nThe integral is intractable, so previously introduced approaches focused on modeling a single term: either $p(c|z)$ or $p(z|x)$. Below, we introduce ways to model $p(c|z)$ and $p(z|x)$, presenting the final solution based on a Laplace approximation of this integral, suitable for the OSFR, that takes into account both terms."}, {"title": "3.4.1 Models for separate terms", "content": "Prediction-based baseline for uncertainty estimate The simplest OSFR method involves the computation of acceptance score s(p), where s(*) is a decision function. If s(p) exceeds a threshold $\\tau$ (typically around 0.5), the test template p is accepted and classified. A common decision function is the cosine similarity with the closest gallery class [26]:\n$\\begin{equation}s(p) = \\max_{\\mu_c \\in \\mathcal{G}} \\mu_c^{\\top} z,\\end{equation}$\nwhere z is the aggregated embedding of the test template p; $\\mu_c$ is the aggregated embedding of a gallery template $g_c$. Low values of s(p) let the system assume that the test template is out of the gallery and thus has to be rejected. In this way, we obtain a two-step algorithm:\n1.  If $s(p) \\geq \\tau$, p is accepted.\n2.  If p is accepted, $\\hat{i}(p) = \\arg \\max_{c \\in \\{1,...,K\\}} \\mu_c^{\\top} z$.\nFor this OSFR method, we propose an ad-hoc uncertainty score:\n$\\begin{equation}U_{baseline}(p) = -|s(p) - \\tau| + |1 - \\tau|.\\end{equation}$\nThe recognition system will likely make false acceptance or rejection errors for test templates p with an acceptance score near the decision threshold $\\tau$. With this function, uncertainty is at its maximum when $s(p) = \\tau$, and when $s(p) = 1$, the uncertainty is zero.\nProbabilistic embeddings baseline $p(z|c)$ for uncertainty estimate Another approach to uncertainty estimation in metric learning is to estimate the posterior density $p(z|c)$. In this case, $s(p) = \\max_{c \\in \\{1,...,K\\}} p(z|c)$.\nWe test two alternative distributions for $p(z|c)$, defined on a d-dimensional unit sphere $S^{d-1} \\subset \\mathbb{R}^d$, to model class conditional embedding distribution $p(z|c)$ for each gallery class $c \\in \\{1,...,K\\}$: von Mises-Fisher (vMF) [6] and Power Spherical distributions [3]:\n$\\begin{equation}p_{VMF}(z|c) = C_d(\\kappa) \\exp (\\kappa \\mu^{\\top} z),\\end{equation}$\n$\\begin{equation}C_d(\\kappa) = \\frac{\\kappa^{d/2-1}}{(2\\pi)^{d/2} I_{d/2-1}(\\kappa)},\\end{equation}$\n$\\begin{equation}p_{Power}(z|c) = \\frac{\\Gamma(\\alpha + \\beta)}{\\alpha + \\beta \\pi^{\\frac{d}{2}} \\Gamma(\\alpha)} (1 + \\mu^{\\top} z)^{\\alpha},\\end{equation}$\n$\\begin{equation}\\alpha = \\frac{d-1}{2} + \\kappa, \\quad \\beta = \\frac{d-1}{2},\\end{equation}$\nwhere $\\kappa$ is a concentration parameter constant for all classes c, $I_a$ is a modified Bessel function of the first kind of the order a. The closed forms for the posterior class probabilities are available in the Appendix 6.2.\nGallery-aware uncertainty estimation $p(c|z)$ The acceptance score-based method described above ignores information about the relative embedding position of classes in the gallery, which is essential for meaningful uncertainty estimates. To deal with this problem, we introduce a new uncertainty estimate for the OSFR.\nWe reformulate the OSFR decision process, defined in Section 3.1, as a K + 1 class classification problem by adding a new out-of-gallery class for test templates with no corresponding class in the gallery. We mark the label for this class as N = K + 1. In the N-class classification problem, a posterior class probability for class c has the following form:\n$\\begin{equation}p(c|z) = \\frac{p(z|c)p(c)}{p(z)}\\end{equation}$\n$\\begin{equation}p(z) = \\sum_{c=1}^N p(z|c)p(c),\\end{equation}$\nwhere p(c) is the prior probability of the class c; p(z|c) is the class-conditional embedding distribution.\nWe assume that the prior probability of the out-of-gallery class is $\\beta$, and the gallery class probabilities are equal:\n$\\begin{equation}p(c) = \\begin{cases} \\beta, & c = N, \\\\ \\frac{1-\\beta}{N-1}, & c \\in \\{1, ..., N - 1\\}. \\end{cases}\\end{equation}$\nIn our experiments, we set $\\beta = 0.5$. For out-of-gallery samples prior p(z|N), it is meaningful to select the uniform distribution on the unit d-dimensional sphere:\n$\\begin{equation}p(z|N) = \\frac{1}{S_{d-1}}, \\quad S_{d-1} = \\frac{2 \\pi^{\\frac{d}{2}}}{\\Gamma(\\frac{d}{2})},\\end{equation}$\nwhere $I$ is the Gamma function. For other classes, we define p(z|c) via a power or vMF distribution from the section above.\nWith this, the test template is rejected if an out-of-galley class with the label N = K + 1 has the highest probability:\n1.  If $\\arg \\max_{c \\in \\{1,...,K+1\\}} P(c|z) \\neq K + 1$, p is accepted.\n2.  If p is accepted, $\\hat{i}(p) = \\arg \\max_{c \\in \\{1,...,K\\}} P(c|z)$.\nThis decision rule is equivalent to the baseline method mentioned in Section 3.4.1 with a specific function f:\n$\\begin{equation}\\tau = f(\\kappa, \\beta, K).\\end{equation}$\nIn the Appendix 6.3, we derive the exact form of the function f for vMF and Power Spherical distributions.\nAlthough predictions of this method are equivalent to the baseline solution with appropriate $\\tau$, the proposed approach provides a natural way to estimate the uncertainty of face recognition."}, {"title": "3.4.2 Joint uncertainty estimate", "content": "To produce the final uncertainty estimates we propose to final steps: a Laplace approximation for the posterior distribution p(c|x) and calibration of terms inside the Laplace approximation.\nLaplace approximation Now, with (12), we can write class distribution conditional on the face image:\n$\\begin{equation}p(c|x) = \\int p(c|z)p(z|x)dz\\end{equation}$\nThis integral is intractable for our distributions, so we use Laplace's approximation [13] at the point of maximum vMF concentration $z_0 = \\mu(x)$ to compute uncertainty:\n$\\begin{aligned} h(z) &= \\log[p(c|z)p(z|x)] \\\\ p(c|x) &= \\int e^{h(z)} dz = \\int e^{h(z_0) + o(||z-z_0||)} dz \\\\ &= p(c|z_0)p(z_0|x) \\int e^{o(||z-z_0||)} dz \\approx p(c|z_0)p(z_0|x). \\end{aligned}$\nThis approximation may seem coarse, but it gives us a simple way to combine two types of data uncertainty: $p(c|z_0)$ indicates gallery ambiguity and $p(z_0|x)$ facial features ambiguity. When either of the terms is low, the probability of the predicted class is low. Thus, uncertainty is high. We can decompose uncertainty into a sum of two terms by taking logarithm:\n$\\begin{equation}\\log p(c|x) \\approx \\log p(c|z_0) + \\log p(z_0|x)\\end{equation}$\n$\\begin{equation}p(Z_0 | X) = p_{VMF} (Z_0; \\mu(x), \\kappa(x)) = \\\\ = C_d(\\kappa(x)) \\exp (\\kappa(x) \\mu^{\\top} (x) z_0)\\end{equation}$\n$\\begin{equation}z_0 = \\mu(x) = p(z_0|x) = C_d(\\kappa(x)) \\exp (\\kappa(x))\\end{equation}$\nFinally, we have the following estimate with $\\log p(c|\\mu(x))$ from (12):\n$\\begin{equation}\\log p(c|x) \\approx \\log p(c|\\mu(x)) + \\log C_d(\\kappa(x)) + \\kappa(x)\\end{equation}$\nCalibration The straightforward usage of the formula above fails due to uncalibrated probabilities of different scales, which are incorrect to sum. So, instead, we sum calibrated confidence estimates to get the final uncertainty score u(x, c):\n$\\begin{equation}-u(x, c) = \\log \\gamma_{\\text{beta}} (p(c \\mu(x)) + \\log \\gamma_{\\text{logistic}} (\\log C_d(\\kappa(x)) + \\kappa(x)),\\end{equation}$\nwhere c is the class label with maximum $p(c \\mu(x))$; $\\gamma_{\\text{beta}}$, $\\gamma_{\\text{logistic}}$ are calibration transformations.\nWe use beta calibration for gallery confidence $p(c \\mu(x))$ because it is the top-performing approach for probabilistic input [15] $\\gamma_{\\text{beta}}(t) = \\frac{t^a}{(1-t)^b}$ where a, b, c are the calibration parameters. For image confidence, we use the logistic calibration because it is suited to calibrate unbounded scores t with $\\gamma_{\\text{logistic}}(t) = \\frac{1}{1+1/exp(s(t-m))}$, where m, s are the calibration parameters. We train calibration parameters by maximizing the log-likelihood of correct OSFR predictions on a validation set. Appendix 6.5 provides the validation set construction details."}, {"title": "3.4.3 Summary", "content": "We propose an uncertainty estimate based on evaluating probabilities for the list of classes augmented with a class for out-of-gallery images. On top of these probabilities, we define the uncertainty estimate based on the maximum of calculated probabilities. We use SCF probabilistic face embeddings to account for uncertainty caused by image quality. Finally, to combine both sources of data uncertainty, we calibrate both these scores and provide the sum of calibrated uncertainties as our final score."}, {"title": "4. Experiments", "content": "In this section, we report the results of our numerical experiments with the proposed uncertainty estimation approach. The main competitors are image quality-based SCF uncertainty and probabilistic baseline."}, {"title": "4.1. Compared methods", "content": "First, we use a single component of our uncertainty estimation: either only vMF or Power distributions defined. Another baseline is SCF, the top performer in uncertainty estimation for metric learning [17]. We also compare our methods to simple ad-hoc uncertainty measure Baseline, described in Section 3.4.1. Uncertainty estimation methods, covering two sources of uncertainty from 3.4.2, are named METHOD + SCF. The PR curves of Random and Oracle rejection methods are shown for reference.\nNote that in our filtering experiments, we compute recognition metrics, e.g., $F_1$, on the remaining part of the test data. So, the metric of the Random method remains almost constant as we throw away more test points. In the Oracle method, we filter OSFR errors first in random order.\nWe test our approaches using common IJB-B [28] and IJB-C [20] datasets, universally acknowledged for such purposes in the face recognition literature. In addition, we consider a different dataset for the whale identity prediction. The dataset is based on the Happawhale Kaggle competition [2]. We train ArcFace [4] and SCF [17] models on it to perform risk-controlled recognition. A detailed description of the OSR protocol for the whale dataset is presented in Appendix 6.4."}, {"title": "4.2. Risk-Controlled Open-set Face Recognition", "content": "The main experiment is the comparison of rejection curves and $F_1$ values after rejection for different approaches and quality metrics. The higher the curve is, the better. In all tables, we filter 5% of test data to compute PRR. We compute average embedding vectors for test and gallery template aggregation in all experiments, so all approaches start from the same point on the right side of the plots.\nOur methods outperform image quality-based uncertainty estimation, SCF, and simple ad-hoc rejection/acceptance boundary-aware Baseline. In IJB-B and Whale datasets, the difference between Power and Power+ SCF is relatively small. In the case of the IJB-B dataset, the images are quite simple, and adding SCF uncertainty changes little. Another cause may be imperfect calibration due to the domain shift between calibration and test data. We calibrate Power and SCF on subset of MS1MV2 dataset [4] (see Appendix 6.5), which has images of decent quality. We selected this validation dataset for proof-of-concept and left the question of validation dataset selection for future work. On the Whale dataset, we suspect that the SCF estimates perform poorly, and its addition to Power also gives minuscule improvement.\nThis evidence is further support with the analysis of rejection curves even for the most interesting IJB-C dataset. SCF underperforms in this scenario, providing lower filtering capabilities. This behavior happens because SCF poorly detects false accepted images, as shown in Figure 3b. Indeed, false acceptance occurs mainly due to the large gallery and test set sizes: in the vast test set, there are probably individuals who look like one of the many subjects in the gallery set. In this case, test images should not necessarily be of low quality. Contrarily, Power and Baseline methods can detect false accepted images because their acceptance score of such images is close to the accept/reject decision boundary due to their imperfect similarity to some subjects in the gallery. Method Power + SCF is overall superior because SCF uncertainty enhances false reject error detection capabilities of Power, see Figure 3c. SCF accurately detects \"false reject\" errors because they mainly happen when an image is of poor quality and its embedding lies far from all gallery classes. Bayesian model-based uncertainty estimators struggle with such cases because they classify such samples as out-of-gallery with high confidence. A combination of uncertainty estimation methods Power and SCF allows for the detection of \"false accept\" and \"false reject\" errors equally well. False identification PR curve in Figure 3d shows that the Bayesian model-based uncertainty estimator, Power, is better than Baseline at detecting identification errors. This behavior is expected because unlike Baseline, Power considers information about test image embedding position relative to gallery classes (see Figure 2)."}, {"title": "4.3. Ablation studies", "content": "Table 2 compares vMF and Power Spherical distribution-based Bayesian OSFR models. We see that the performance of these methods is almost equal, so we use Power in our experiments."}, {"title": "4.4. Uncertainty calibration", "content": "Another related issue for uncertainty estimates is their calibration properties: how well these estimates correspond to the actual error probabilities.\nWe follow the protocol from [18] to compare calibrations of provided estimates before and after beta calibration for the Whale dataset. See Figure 4 for calibration plots and ECE (expected calibration error) values. The uncertainty estimates are from the Power uncertainty estimate. After calibration, the probabilities on the test part of the dataset are close to the true values. Thus, this calibration method can obtain single-scale probabilities ready for aggregation. Similar behavior holds for the logistic calibration of SCF scores."}, {"title": "5. Conclusion", "content": "We presented a method for uncertainty estimation in open-set recognition that distinguishes itself from others by providing a way to cover multiple sources of uncertainty. A gallery-aware method for the open-set recognition problem provides a fully probabilistic treatment of the problem at hand and gives a natural uncertainty estimate. Combining the gallery-aware uncertainty estimate with a measure of face image quality yields additional fundamental improvement, as the latter accounts for false rejection errors. The core idea for aggregation is to calibrate both estimates first, then aggregate them linearly.\nThe gallery-aware uncertainty estimate by itself gives noticeable improvement over existing image quality based uncertainty estimation approaches. When applied to widely used datasets in the face recognition literature, our method shows a clear improvement in risk-controlled metrics over simple image quality-based filtering, providing evidence of its superiority. We provide an open-set recognition protocol for the Whale dataset and show that our method's superiority also holds in this challenging task."}, {"title": "6. Appendix", "content": "The appendix contains the following sections that complement the main body of the paper:\n*   a qualitative comparison of the Bayesian model and image quality-based uncertainty estimators;\n*   exact computation of posterior probabilities;\n*   proof that our gallery-aware uncertainty is equivalent to a prediction-based baseline in terms of OSFR predictions;\n*   description of the whale dataset, face validation set, and corresponding testing protocols."}, {"title": "6.1. Qualitative comparison of the proposed uncertainty estimates", "content": "This section provides illustrative examples to grasp the capabilities of our gallery-aware method and prove that it produces reasonable uncertainty scores.\nFor most blended and thus ambiguous test images in the middle, our Power Spherical distribution-based uncertainty estimate has the highest value. The SCF-based data uncertainty notices only the quality of test images and thus cannot detect decision ambiguity of images in the middle.\nOur combined approach relies also on the quality of the input image. If a test image is corrupted, the image embedding may be far from its true position, and the recognition system decision can not be trusted. An extreme case of image corruption is the completely black test image. Its embedding will likely be far from any gallery class, and the black image will be classified with high confidence as an out-of-gallery example. In this example, we have iteratively applied a Gaussian kernel with $\\sigma = 3$ and kernel size equal to 10% of the image height/width. You can see how, in Figure 6, a posterior probability-based uncertainty (green) rises until the out-of-gallery class has the highest probability. After this, the probability rises, and the Power spherical distribution-based Max Prob uncertainty decreases as we add more noise. On the other hand, SCF uncertainty (orange) is rising because it is specifically aimed at detecting low-quality images. To remedy the pathological behavior of the posterior probability-based approach, we use it with the data uncertainty estimator, SCF, and get a reasonable uncertainty estimate (blue), which produces significantly higher uncertainty for blurred images than for clear ones."}]}