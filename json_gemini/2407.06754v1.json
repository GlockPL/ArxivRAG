{"title": "Threats and Defenses in Federated Learning Life Cycle: A Comprehensive Survey and Challenges", "authors": ["Yanli Li", "Jifei Hu", "Zhongliang Guo", "Nan Yang", "Huaming Chen", "Dong Yuan", "Weiping Ding"], "abstract": "Federated Learning (FL) offers innovative solutions for privacy-preserving collaborative machine learning (ML). Despite its promising potential, FL is vulnerable to various attacks due to its distributed nature, affecting the entire life cycle of FL services. These threats can harm the model's utility or compromise participants' privacy, either directly or indirectly. In response, numerous defense frameworks have been proposed, demonstrating effectiveness in specific settings and scenarios. To provide a clear understanding of the current research landscape, this paper reviews the most representative and state-of-the-art threats and defense frameworks throughout the FL service life cycle. We start by identifying FL threats that harm utility and privacy, including those with potential or direct impacts. Then, we dive into the defense frameworks, analyze the relationship between threats and defenses, and compare the trade-offs among different defense strategies. Finally, we summarize current research bottlenecks and offer insights into future research directions to conclude this survey. We hope this survey sheds light on trustworthy FL research and contributes to the FL community.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, smart devices have become integral to people's daily lives. While engaging with these applications or smart services, users generate tens of billions of data points per second. This data, regarded as a valuable resource for fueling data-driven models, but remains underutilized due to the centralized nature of traditional training paradigms [1], [2]. In particular, centralized data collection necessitates stable network connections and high-speed broadband, conditions that often conflict with the realities of edge devices. Even worse, transferring data from the user's domain raises significant privacy concerns, particularly when dealing with sensitive financial, medical, and social media information. As phrased by authors in [3], \"data remains isolated within individual owners' hands, creating data silos.\"To mitigate the problem of data silos, federated learning (FL) was introduced by Google in 2017 [4]. This new machine learning (ML) paradigm allows participants to collaborate in a loosely federated manner while retaining a degree of \"autonomy\" [3]. Under the FL paradigm, participants locally train their model and only upload model updates for aggregation. Due to its privacy-preserving characteristics, FL has attracted significant attention from both academia and industry, leading to the deployment of numerous real-world applications. Notable examples include Google's smart keyboard Gboard [5], Apple's intelligent voice assistant Siri [6], and WeBank's finance risk prediction service for reinsurance [7]. A typical FL service life cycle [1], [3], [4] begins with a service provider publishing a task and selecting participants. Each selected client then receives the global model, trains it locally, uploads the gradients, and participates in the aggregation process. Once the global model achieves the desired performance, the server distributes rewards and releases the application to the public.Despite its promise, FL is vulnerable to various attacks due to its distributed nature [8]. Based on the capacity of adversaries, these threats can completely [9]\u2013[11] or partially [12]\u2013[14] harm the utility of model, break the privacy of participants [15]\u2013[17], and could happen at any phase during the FL service life cycle. In response, numerous defense frameworks [18]\u2013[20] have been proposed and shown effectiveness across different settings and scenarios. Given the extensive security research, the FL community urgently needs comprehensive reviews to provide a clear understanding of the current research landscape.Although some reviews exist [21]\u2013[23], their scope is generally limited to threats leading to direct, serious harm and around the local training phase, failing to provide a comprehensive picture of the research field. To mitigate the research gap, we present this review, which aims to comprehensively investigate all threats throughout the FL life cycle, along with their corresponding defenses. Unlike existing reviews, our study considers both direct and potential threats and expands the scope to encompass the entire FL life cycle.In summary, the main contributions of this survey are as follows:\u2022 Providing the entire life cycle of FL services, from the learning task publish to service delivery; identifying each learning phase's vulnerabilities, the potential threats' impact, and actors.\u2022 Identifying and comparing the utility-harmed threats in the FL environment, providing their attack and poisoning strategies, ranging from those with the potential to direct impact.\u2022 Identifying and comparing the privacy-harmed threats in the FL environment, ranging from the application to the network layer.\u2022 Classifying the existing defense frameworks, summarizing their effectiveness when defending against various utility-harmed or privacy-harmed threats, and discussing their assumptions and trade-offs.\u2022 Discussing the existing bottlenecks of trustworthy FL, identifying the emerging requirements, and providing insights about future trends.The rest of this paper is organized as follows: We provide the background information of FL in Section II, including its life cycle, vulnerabilities, and categories. Sections III and IV review the most representative and state-of-the-art threats targeting the utility and privacy of FL systems. In Sections V and VI, we summarize the defense frameworks for addressing FL privacy concerns and enhancing FL robustness, compare their effectiveness, and discuss their trade-offs. Section VII offers insights into future research trends, and Section VIII concludes the survey. For better readability, we present a diagram in Figure 1, illustrating the organization of the survey."}, {"title": "II. FEDERATED LEARNING BACKGROUND", "content": "In this section, we introduce the FL service life cycle, different types of FL, and FL threat models to provide the survey's background."}, {"title": "A. Federated Learning Life cycle", "content": "0. Learning task bidding and participants selection: The life cycle of FL begins when service providers publish a learning task within FL communities or mention it in an application's user terms. Interested clients submit bids to participate based on the specified requirements and promised rewards. These bids include commitments of their computing resources, bandwidth, network capabilities, and training data. Service providers then review these commitments and select the participants for the learning task.1. Global model broadcasting: The server, which can be the service provider or another third-party computing center, broadcasts the current global model to all selected participants. During the initialization of the learning task (the first learning round), the server also specifies the conditions for concluding both local and global learning iterations.2. Participants' local training: Once the current global model is received, each participant trains the model locally using their private data. This training process continues for several rounds until preset conditions, such as a specified number of learning rounds or a desired model accuracy, are met. Subsequently, the participants send their local model updates back to the server, completing the local training phase.3. Aggregation and global model update: The server aggregates the collected local model updates using a predefined aggregation rule. The resulting outcome serves as the gradient to update the global model. If the end condition has not been reached, the updated global model is then broadcast to all participants for the next round of training.4. Incentive distribution: When the global model achieves the expected performance, the server stops broadcasting, and the training task is finalized. The service provider then evaluates each participant's contribution and distributes incentives accordingly, which could be either profit or non-profit.5. Model publish, and user access: Finally, the service provider publishes the FL model or FL-based service online. Users can then access and utilize the application for various purposes, such as querying, making predictions, and more."}, {"title": "B. Federated learning Categories", "content": "The FL can be categorized based on the participants' data distribution or the system architecture [24].Based on the distribution of data features and data samples, FL can be classified as horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning (FTL). Specifically, HFL is applicable to cases in which participants own different data samples but the same feature spaces. For example, in rare disease research, hospitals may have distinct patients but their records own highly similar features. Contrary to HFL, the VFL participants overlap in sample spaces but differ in feature spaces. For example, banks and telecom operators in a city may keep records of the same citizens, but these records contain different features. FTL integrates the concepts of FL and transfer learning, where transfer learning leverages knowledge acquired from one domain to enhance performance in another domain. FTL addresses the challenge of limited overlapping samples and features by training a model on a large public dataset.On the other hand, FL can be divided into centralized federated learning (CFL) and decentralized federated learning (DFL) based on the system architecture. Under CFL, a server organizes the whole learning process, including models broadcast, collection and aggregation. In contrast, DFL does not involve the central server. Following a preset rule, participants can directly exchange their models and automatically perform aggregation. Without further notification, we use FL to represent CFL in this survey."}, {"title": "C. Threat Models in Federated Learning Life Cycle", "content": "In FL, each participant has a certain degree of autonomy and can independently conduct local training. While the federated architecture potentially enhances participants' privacy, it also introduces new attack surfaces. Beyond the training phase attacks that directly degrade FL model accuracy and compromise client privacy, dishonest behavior in other phases of the FL life cycle can also negatively impact the model. For instance, dishonest self-reporting during participant selection can lead to insufficient computing or data resources during training, resulting in lower-than-expected model performance. In light of these scenarios, we propose a broader definition of threats and attackers in this survey:Threats (i.e., attacks) are the misbehavior that may directly or potentially harm FL models' performance (i.e., utility) or privacy.Attackers are the actors who intentionally launch threats.Vulnerabilities in FL: Today, numerous studies have demonstrated that attacks can occur at any stage throughout the entire FL life cycle. Initially, a dishonest client may exaggerate its capabilities to secure a spot in the selection process and join the learning task. Under the local training, the malicious clients may use modified training data to train the model, or directly perturb the trained model (or gradients) to degrade, even fully break the model performance. During server-client communication, the information transmitted can be replaced or tampered with by a \"middleman.\" Once the training is complete, the dishonest participants can forge their consumption to defraud high returns. When the FL-based application is published, an attacker can use a crafted data item to mislead the application to generate unexpected outcomes. Although the server is assumed to be trustworthy in most cases, the server is potentially curious or hijacked during the training process. In those cases, the server may act maliciously and infer sensitive information from individual updates collected. We summarize and provide detailed discussions of the utility-harmed and privacy-harmed threats in Section III and IV."}, {"title": "III. UTILITY-HARMED ATTACKS", "content": "Compared to data-centralized collection learning schemes, FL provides a different way of training models collaboratively [3], [4]. Instead of relying on a visible and centralized collection dataset, FL trains the model across a potentially vast array of unreliable devices, each equipped with private, uninspectable datasets. Furthermore, the local training process also introduces a new attack surface and brings opportunities for potential attacks [11], [27]\u2013[29]. Specifically, as data remains on the devices and only model updates are shared, adversaries could attempt to exploit this setup to compromise the model's utility or performance. We demonstrate and compare different FL utility-harmed attacks in Figure 5."}, {"title": "A. Poisoning Attacks", "content": "As one of the most representative adversarial attacks, poisoning attacks attempt to degrade the model performance or totally destroy the global model. Depending on the stage at which the poisoning occurs, poisoning attacks can be further categorized into model poisoning attacks and data poisoning attacks.1) Model poisoning Attacks: Model poisoning attacks [30] refer to adversaries directly manipulating reports (local model updates) submitted to the service provider. One of the most straightforward strategies for executing model attacks involves introducing a fixed perturbation or substituting the benign gradient parameters with fixed values. For instance, Reverse Attack [9], and Random Attack [10] generate the malicious gradient by reversing and randomly replacing the original gradients. Under the Partial Drop Attack [10], the adversary replaces a preset percentage of the benign gradient parameters as 0; the variants of Partial Drop Attack enlarge the attack effectiveness by using other malicious values if the benign gradients naturally carry a large amount of 0. Although these attack strategies can effectively degrade the global model performance and decrease the testing accuracy, the malicious gradients are unavoidably away from the benign gradients, which can be identified and discarded by the byzantine resilient FL methods.To enhance the attacking performance and avoid being identified by the defender, some existing attacks [11], [27], [28] design more dynamic attack strategies. For instance, Little is Enough Attack [31] first normally trains the model through the data of the clients controlled. Then, the adversary statistics the mean value and standard deviation (Std) of the gradients owned. Based on the statistics, the adversary subsequently generates the crafted gradient by adding a scaled Std on the mean value. The Fall of Empires Attack [32] follows a similar strategy, which generates the malicious gradient by timing the mean value with a preset scalier. To maintain the stealth of model poisoning attack, research [27] proposes an optimization-based model poisoning attack, which only manipulates a small fraction of the local model updates instead of all gradient parameters. Under the study [27], adversary training is organized into two tasks: the main task and the adversarial task. In the main task, the adversary trains the model in the usual manner. Conversely, in the adversarial task, the adversary embeds adversarial features into the neural network's redundant space to enhance the attack's persistence.Recently, Local Model Poisoning Attack [11] proposes a more flexible attacking algorithm that adopts the attack strategy for different defense frameworks. Based on the information owned (partial or full knowledge), this attack first infers the optimal global model updates and subsequently formulates the optimization problem that aims to reverse the global model update the most along the infers. This work [11] applies the attack in Krum [33], Trimmed-mean [34], and Median [34] frameworks and consequently generates the Krum Attack and Trim Attack. The study [28] targets the Krum framework with an emphasis on evasion detection, specifically through covert model poisoning (CMP). Research [28] first formulates the model poisoning as an optimization problem, aiming to minimize the Euclidean distance between the manipulated model and a designated one, all within the constraints set by Krum. Based on the solution to the optimization problem, [28] subsequently formulates CMP algorithms to counter the Krum framework. To improve the practicability, a low-complexity CMP algorithm is introduced and achieves the optimization complexity reduction. We summarize various model poisoning strategies"}, {"title": null, "content": "$$\\begin{equation} \\arg \\min_{x^{\\prime}, y^{\\prime}}||G^{\\prime}-G||^{2} \\tag{1} \\end{equation}$$"}, {"title": null, "content": "$$\\begin{equation} \\arg \\min_{x^{\\prime}}(\\frac{G^{\\prime}G}{||G^{\\prime}||\\cdot||G||}+ \\alpha\\cdot TV(x^{\\prime})) \\tag{2} \\end{equation}$$"}, {"title": "2) Data poisoning Attacks:", "content": "Different from model poisoning attacks that directly introduce the perturbation to the gradient, data poisoning attacks generate crafted gradients by training models based on poisoned data. Label Flipping Attack is one of the most representative data poisoning attacks. In Label Flipping Attacks, the adversary mismatches the training labels and training data to generate the malicious gradient, aiming to degrade the attacked class learning performance of the joint model. Study [29] considers the support vector machine (SVM) algorithm and aims to find a labelflipping combination under a given budget so that a classifier trained on such data will have maximal classification error. Research [37] introduces the Label Flipping Attack in the deep learning scenarios and evaluates the key conditions for successfully performing attacks, including sufficient adversarial participants and continuous participation. To enhance the flexibility of the attacks, study [38] proposes the On-Off Label Flipping strategy. Under the On-Off Label Flipping strategy, the adversary first acts as a benign client for a period of time and builds a positive expectation in the defense system to strengthen the impact of the following updates. Then, the adversary betrays the system and performs an attack for a subsequent period of time. As the betrayal behavior lowers the reputation, the adversary may toggle back to acting as the benign client again after the One-Off attack and recover its reputation.Label Flipping Attacks also show significant effectiveness in real-world applications. For instance, Study [19] implements this attack within a malware detection task for Android platforms, employing a Silhouette Clusteringbased Label Flipping Attack (SCLFA). This method involves attackers calculating the silhouette scores of data samples and targeting those with negative values for generating polluted data through label flipping. Similarly, research [39] illustrates the potency of Label Flipping Attacks in compromising spam filtering systems. By executing entropy-based flipping attacks, their study managed to elevate the false negative rate of Naive Bayes classifiers in the presence of label noise without impairing the classification of legitimate emails. Furthermore, study [40] explores the impact of Label Flipping Attacks on hardware Trojan detection systems, demonstrating how model performance could be substantially degraded through a stochastic hill-climbing search-based flipping strategy, incurring minimal costs for label manipulation. These instances underscore the critical need for robust defenses against Label Flipping Attacks in diverse application areas."}, {"title": "3) Backdoor Attacks:", "content": "Backdoor Attack is a special type of poisoning attack, which aims to lead the global model to misbehave only on a selected minority of examples while maintaining good overall accuracy on all other examples [12], [13]. Recent studies indicate that the Backdoor Attack can be executed either by poisoning the clean training data or by introducing crafty perturbations to benign gradients. In other words, the Backdoor Attack can be categorized as either a model poisoning attack or a data poisoning attack based on the poisoning strategy.When performing Backdoor Attacks through the data poisoning strategy, the adversary should first decide on a trigger (backdoor) and consequently assign a selected label for the trigger carrier to form the poisoning data. This backdoor could be semantic or artificial. For semantic backdoor, study [14] leverages the samples with uncommon attributes as the backdoor, such as the cars with unconventional colors (e.g., green), scenes containing a unique object (e.g., a striped pattern), or trigger sentences in word prediction problems that conclude with an attacker-selected target word. Similarly, research [41] proposes an attack strategy to introduce a backdoor in the FL-based IoT intrusion detection system. In this scenario, the adversary targets packet sequences originating from specific malware-driven malicious traffic. Different from the semantic backdoor that replies on existing share properties, the artificial backdoor manually poisons benign samples by introducing triggers. Study [1] demonstrates the adversary can manually add pattern \"L\" at the images' corner to activate the backdoor, where the pattern \"L\" does not naturally exist in the benign samples. A recent study [42] extends the scope of backdoor attacks to real-world settings. The research reveals that beyond digital triggers, physical triggers such as sunglasses, tattoos, and earrings can also serve as triggers for organizing backdoor attacks.Within the Vanilla FL framework, the most straightforward method to execute a backdoor attack using a model poisoning strategy is scaling the malicious updates containing backdoor information to dominate updates from benign clients. Study [42] first employs the replace method, in which the attacker seeks to substitute the new global model with a poisoned one by a wisely chosen factor. While the scaling-replacement method proves effective in averaging aggregation, straightforward scaling appears to be naive to success under clipping and restricting defenses. To enhance the stealth of model poisoning attacks, research [43] proposes a projected gradient descent (PGD) attack strategy, which projects the crafted model on a small ball centered around the global model from the previous iteration."}, {"title": "B. Promise breaking", "content": "During the FL life cycle, clients may pledge to provide their training data or computing resources to gain participation opportunities or claim their contributions to receive incentives. However, malicious clients can exploit the FL system to gain advantages, such as access to the joint model and incentives, without fulfilling their promises [44], [45]. Although these attackers do not deliberately poison the model, promise-breaking behaviors can lead to insufficient computing resources and training data, as well as unfair incentive distribution. This, in turn, may degrade learning performance and diminish participant motivation.Research [45] introduces a Free-riding Attack framework based on the Vanilla FL and shows that the adversary can perform a Free-riding Attack in each learning iteration. In particular, the Free-riding attacker does not perform local training but adds noise to the constructed parameter updates and applies Stochastic Gradient Descent (SGD) to maximize similarity with updates from other benign clients. Research [44] proposes a novel Free-riding Attack in which the model is trained locally using a small dataset masqueraded as a large dataset to obtain more incentive rewards. Study [46] introduces a Free-riding attack strategy, namely Random Weights Attack. Under this attack, the adversary creates a gradient update matrix with dimensions matching the received global model by randomly sampling each parameter from a uniform distribution within a pre-designed range.Studies [47]\u2013[49] investigate dishonest reporting during intensive distribution. To cheat the reward system, malicious participants may falsely report their resource consumption or claim to have trained the joint model with high-quality data while actually using low-quality data, such as spam."}, {"title": "C. Sybil Attacks", "content": "The Sybil Attack refers to a single attacker (or a small amount attackers) joining the system with multiple colluding identities to enhance the stealth and effectiveness of an adversarial attack. Given that some traditional FL permits selected participants to freely join and exit during training tasks, it becomes inherently susceptible to Sybil Attacks.The research [50] first introduces Sybil Attacks within the FL context. It demonstrates Sybil Attacks can significantly enhance the impact of other attacks (such as the Label Flipping Attack) by misleading the global model into classifying handwritten digits \u201c1\u201d as \u201c7\u201d through only two sybils. Study [51] builds upon the findings of [50], delving deeper into the vulnerability of FL to Sybil Attacks, as well as exploring associated attack strategies and objectives. This work [51] categorizes and names the Denial-of-Service (DoS) attacks instigated by Sybil Attacks as Training Inflation and demonstrates Sybil Attacks can be performed from various FL dimensions. From the data distribution perspective, Sybils can use identical datasets, different datasets, or synthetic data to train local models and generate crafted updates [52]. On the other hand, to dominate the benign clients and amplify their influence on the system, Sybils might coordinate amongst themselves before producing the subsequent series of model updates, achieving different levels of coordination."}, {"title": "D. Communication Layer Attacks", "content": "FL continuously updates the joint model, which relies on communications between the server and multiple participants. However, the large volume of data exchange and frequent communication drives a significant communication bottleneck and brings new attack interfaces for adversaries.The man-in-the-middle (MiTM) attacks [53], as one of the most representative network layer attacks, has been introduced in FL by recent studies [2], [54]. As FL leverages the single server to aggregate model updates and organize the learning process, performing MiTM between the server and participants can effectively block the server and lead to a single point of failure. Research [55] explores attacking the FL communication bottleneck by decreasing the bandwidth, delaying the response, and increasing the struggle possibility. Through performing attacks on the communication of FL, [55] shows the model convergence and performance can be degraded significantly."}, {"title": "IV. PRIVACY-HARMED ATTACKS", "content": ""}, {"title": "A. Evasion Attacks", "content": "In evasion attacks, the adversary deliberately adds slight malicious perturbations to input samples during the inference phase, causing the classifier to misclassify the sample with a high probability. One of the most representative works of the evasion attack has been introduced by [56], which is a so-called adversarial example. By adding unnoticeable pixel perturbation on a panda image, [56] successfully changes the classification of GoogLeNet [57] from panda to gibbon with 99.3% confidence.Based on the information owned by the adversary, the evasion attacks could be further divided into white-box and black-box attacks. In the white-box setting, the adversary has full knowledge of the learning algorithm and model parameters and can craft adversarial examples based on the information. For instance, studies [58], [59] generate adversarial examples by maximizing the loss function, subject to a norm constraint, using constrained optimization techniques like projected gradient ascent. On the other hand, the blackbox setting refers to the attackers having no knowledge about the algorithm and parameter information of the FL system and generating adversarial samples through the interaction process or query access with the system. Considering the black-box setting, [60] proposes a zeroth order optimization (ZOO) based attack to estimate the target model updates and generate adversarial examples consequently. Similarly, Boundary Attack [61] has been designed in the same scenario. When performing Boundary Attack [61], the adversary first identifies the boundary separating adversarial and nonadversarial samples, then subtly introduces perturbations along this boundary to produce adversarial examples.A recent work [62] introduces an Evasion Attack in the Vertical FL (VFL) scenarios and proposes the Adversarial Dominating Inputs Attack. Unlike the aforementioned adversarial samples that domain the whole feature space, the Adversarial Dominating Inputs Attack [62] can dominate the inputs of other clients by controlling a fraction of the feature inputs and lead to the misclassification of specific inputs. In the meanwhile, the Adversarial Dominating Inputs Attack can also result in reduced and unfair incentive rewards for impacted participants due to their decreasing contributions.While FL is designed to protect the privacy of participants, recent studies have shown that privacy-harmed attacks still exist in FL systems. These attack objectives include identifying members (MIA) or properties (PIA) of victim training data, generating representative or (RIA) original samples (Inversion Attacks) of victim training data, and eavesdropping on transmissions between the victim and the server. We demonstrate and compare different FL privacyharmed attacks in Figure 6."}, {"title": "B. Inference Attacks", "content": "During the training process of FL, the model will inadvertently learn the latent information of private data. Therefore, an honest but curious server or any other user can launch an inference attack to recover the original training data without any prior knowledge. Inference Attacks include model extraction attacks, attribute inference attacks, and membership inference attacks.1) Membership Inference Attack (MIA): Similar to MIAs in traditional machine learning scenarios, malicious clients or users can execute MIAs in FL applications to infer whether a data sample belongs to its training dataset. Study [15] focuses on black-box within Machine Learning as a Service (MLaaS) settings. By accessing multiple generated shadow models, the study achieves high accuracy in inferring membership. ML-Leaks [70] reduces the cost of launching attacks and demonstrates the effectiveness of MIA using only a single (even without) shadow model. Study [16] considers the white-box settings and proposes the gradient ascent MIA strategy to magnify the presence of data points in others' training sets. It demonstrates that all algorithms trained using Stochastic Gradient Descent (SGD) are susceptible to privacy vulnerabilities from MIAs. The study [71] further extends MIAs to NLP learning tasks and effectively infers whether a specific text segment belongs to the training dataset.2) Property Inference Attacks (PIA): Property inference attacks aim to infer private features or characteristics (e.g., value, distribution, etc.) of the training data without direct exposure to the data. Although some surveys [24], [72] categorize threats aimed at reconstructing training data as PIAs, we consider PIAs to primarily focus on inferring properties of the training data, rather than the data itself. Therefore, in this survey, we differentiate threats aimed at data reconstruction from PIAs and classify them as Inversion Attacks.Attackers in the study [73] use auxiliary data and trained classifiers to determine if observed model updates are generated from a dataset that includes the target property. Considering fully connected neural networks (FCNNs) are invariant under the permutation of nodes when represented using matrices, study [74] introduces PIAs in white-box settings and shows good inference performance in shallow-FCNNs. The study [75] further extends PIAs to logistic regression (LR), long short-term memory networks (LSTM), and graph convolutional networks (GCN) across multiple domains. It demonstrates that even in black-box settings, PIAs can still achieve accuracies exceeding 60%.3) Representative Inference Attacks (RIA): Representative Inference Attacks indicate the malicious generate dummy outcome that is representative of the victim's training data [76], [77]. The study [76] first introduces record-level RIAS in collaborative learning scenarios and demonstrates that GAN-based attacks can effectively compromise against Distributed Selective SGD (DSSGD [78]). However, subsequent research [77] reveals its performance degradation in FL settings due to the averaging of updates and introduces the mGAN-AI framework. This framework enhances GANS with a multitask discriminator capable of simultaneously assessing the category, reality, and client identity of input samples. Compared to [76], mGAN-AI enables the generator to recover representative private data for specific users."}, {"title": "C. Inversion Attacks", "content": "Although FL allows clients to only upload gradients during model training, it has been found that some properties of the training data are implicitly carried by these gradients, which could be revealed through inverse attacks. DLG [64] first introduces gradient inversion in FL computer vision areas. Specifically, the DLG generates random dummy input samples x' and labels y' and corresponding dummy gradients G'. x' and y' are Iteratively optimized through L-BFGS to minimize the difference between dummy and benign gradient. Formally, the objective function of DLG shown as follows:"}, {"title": null, "content": "Considering the high-dimensional direction of gradients carried important information, study [65] introduces cosine similarity in the loss function to find images that lead to a similar change in model prediction as the ground truth instead of finding images with a gradient that best fits the observed gradient. The objective function is shown as follows, where TV represents the Total Variance regularization."}, {"title": "D. Eavesdropping", "content": "Network eavesdropping involves an attacker intercepting and capturing data packets transmitted over a network using their own devices, with the intent to analyze the contents of these packets to get private information [80]. Since model training under the FL scheme requires regular communication between the server and clients, FL is naturally vulnerable to eavesdropping attacks. This vulnerability is particularly pronounced when the communication between participants and the server is in plaintext or employs a weak encryption method [23], [81]. Since the messages transmitted are usually models or gradients, the attacker may further leverage inference or inversion attacks to decipher the eavesdropped messages to understand the private information [82]\u2013[84]. Study [85] considers the eavesdropping attacks in unmanned aerial vehicles (UAVs) scenarios, demonstrating that eavesdroppers can deduce the raw data from the shared parameters and compromise participants' privacy."}, {"title": "V. DEFENCE AGAINST UTILITY-HARMED ATTACKS", "content": "As the diversity and complexity of adversarial attacks against FL increase, new defenses are being developed to counteract their malicious impacts. These defense frameworks generally follow the setting that the server is honest and can play the defender role. As some defense methods are effective against multiple types of attack categories, we survey these FL defense frameworks from different technologies instead of grouping them according to the attack category defended."}, {"title": "A. Data Sanitization", "content": "Data sanitization [86]\u2013[88] has been introduced to remove malicious and suspicious data before the training process and consequently mitigate crafted-data-related threats, including data poisoning and evasion attacks. The research [86] introduces a novel training pipeline that incorporates a data sanitization phase. During each iteration, this phase creates multiple models, termed micro-models, based on subsets of the training data. These micro-models are used to produce provisional labels for each training input and consequently combined in a voting scheme to determine which parts of the training data may be potentially attacked. To filter out noise-labeled data, FedDiv [89] maintains a global filter. This filter is trained, exchanged, and aggregated similarly to the global model and supports local data filtration. From the data shuffling perspective, study [88] proposes a byzantine-resilient matrix-vector (MV) multiplication and further proposes an algorithm based on the data encoding process and error correction over real numbers to combat various adversarial attacks.Although data sanitization can filter out some potential attacked data and keep training data clean, it relies on directly accessing the local data of clients, which may raise privacy issues. A recent work [90] indicates that data sanitization can only work in cross-silo FL with strong authentication and be performed locally by the client itself. However, this framework [90] assumes the trustworthiness of the clients, which may not be applicable to the adversarial FL environment."}, {"title": "B. Anomaly Detection", "content": "To perform threats, attackers unconsciously exhibit different patterns from benign clients, including abnormal behaviors or data patterns. To this end, anomaly detection has been proposed to identify suspicious through statistical or behavior analysis technology [91]\u2013[93]. We follow the study [24] and divide the anomaly detection of FL in client and data from two perspectives.A) Client Anomaly DetectionUnder FL, Client Anomaly Detection could be performed based on the information from the communication (IP address, User ID, Time Stamp [94]) or application layer, namely client model updates. By employing a pre-trained auto-encoder model, study [91] introduces Auto-encoderBased Anomaly Detection to detect abnormal model weight updates from the clients and consequently erase the negative impact carried by the potential attackers. Inspired by spectral anomaly detection that distinguishes abnormal data instances by capturing the normal data features, research [95] designs a novel robust FL framework. The spectral anomaly detection model is trained through a centralized training process and applied in the following learning iteration. Since the detection threshold is adjusted dynamically in each communication round, it becomes challenging for the adversary to understand the framework, ensuring they can be effectively excluded from the aggregation. Recent research [92] presents a visualization scheme for anomaly detection, introducing VADAF. Specifically, VADAF captures the intricate dynamics of the FL training process, aiding in investigating various client issues and assessing the severity of anomalous clients.B) Data Anomaly DetectionAnomalous data, potentially introduced during the data sampling process, can adversely affect the model's training performance. Consequently, data anomaly detection has been introduced to identify outliers in the dataset or values that are far from the normal data feature values. Research [96] proposes a two-phase iterative adversarial detection method to identify software samples in the malicious application detection system that have been subjected to poisoning attacks. Research [97] proposes an anomaly detection method for time series datasets based on recursive auto-encoding, which reduces the impact of overfitting to anomalies. To further improve robust and efficient anomaly detection in unsupervised time series scenarios, a variational recurrent encoder model [98] can separate anomalies from normal data without relying on anomaly labels. However, this operation requires direct access to clients' data or transferring trust to participants, leading to limitations similar to those in Data Sanitization."}, {"title": "C. Adversarial Training", "content": "During the model training process, a minor and welldesigned perturbation can be manually introduced to enhance the model's robustness, namely Adversarial Training.Research [99] first introduces ensemble adversarial training in centralized learning settings, which augments training data through transferring perturbations from other pre-trained models. Studies [93], [100], [101] further extend the adversarial training in decentralized settings. Specifically, study [100] applies adversarial training in the FL environment to reduce model drift and accelerate model convergence. Nevertheless, as the perturbation introduction inevitably affects the classification accuracy, the performance carried by adversarial training may not be stable against complex black-box attacks. On the other hand, adversarial training relies on a massive volume of training datasets, which increases the cost of computation resources on the local devices. Especially in cross-device FL environments with multi-participants, lightweight clients can not afford the high costs of adversarial training. Research [93] proposes a novel learning scheme that propagates adversarial robustness from high-resource users, who can undertake adversarial learning, to low-resource users throughout the FL process. Research [101] investigates the effectiveness of defencing Evasion Attacks through adversarial learning. Specifically, Gaussian noise is used to smooth the training data by including adversarial data in the training dataset."}, {"title": "D. Model Compression", "content": "The large model size enables the DL model to achieve a good learning performance but also introduces both communication challenges and a broadened attack surface in the FL environment. To reduce communication overhead and parameter redundancy, model compression technology has been widely"}]}