{"title": "Threats and Defenses in Federated Learning Life Cycle: A Comprehensive Survey and Challenges", "authors": ["Yanli Li", "Jifei Hu", "Zhongliang Guo", "Nan Yang", "Huaming Chen", "Dong Yuan", "Weiping Ding"], "abstract": "Federated Learning (FL) offers innovative solutions for privacy-preserving collaborative machine learning (ML). Despite its promising potential, FL is vulnerable to various attacks due to its distributed nature, affecting the entire life cycle of FL services. These threats can harm the model's utility or compromise participants' privacy, either directly or indirectly. In response, numerous defense frameworks have been proposed, demonstrating effectiveness in specific settings and scenarios. To provide a clear understanding of the current research landscape, this paper reviews the most representative and state-of-the-art threats and defense frameworks throughout the FL service life cycle. We start by identifying FL threats that harm utility and privacy, including those with potential or direct impacts. Then, we dive into the defense frameworks, analyze the relationship between threats and defenses, and compare the trade-offs among different defense strategies. Finally, we summarize current research bottlenecks and offer insights into future research directions to conclude this survey. We hope this survey sheds light on trustworthy FL research and contributes to the FL community.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, smart devices have become integral to people's daily lives. While engaging with these applications or smart services, users generate tens of billions of data points per second. This data, regarded as a valuable resource for fueling data-driven models, but remains underutilized due to the centralized nature of traditional training paradigms [1], [2]. In particular, centralized data collection necessitates stable network connections and high-speed broadband, conditions that often conflict with the realities of edge devices. Even worse, transferring data from the user's domain raises significant privacy concerns, particularly when dealing with sensitive financial, medical, and social media information. As phrased by authors in [3], \"data remains isolated within individual owners' hands, creating data silos.\"\nTo mitigate the problem of data silos, federated learning (FL) was introduced by Google in 2017 [4]. This new machine learning (ML) paradigm allows participants to collaborate in a loosely federated manner while retaining a degree of \"autonomy\" [3]. Under the FL paradigm, participants locally train their model and only upload model updates for aggregation. Due to its privacy-preserving characteristics, FL has attracted significant attention from both academia and industry, leading to the deployment of numerous real-world applications. Notable examples include Google's smart keyboard Gboard [5], Apple's intelligent voice assistant Siri [6], and WeBank's finance risk prediction service for reinsurance [7]. A typical FL service life cycle [1], [3], [4] begins with a service provider publishing a task and selecting participants. Each selected client then receives the global model, trains it locally, uploads the gradients, and participates in the aggregation process. Once the global model achieves the desired performance, the server distributes rewards and releases the application to the public.\nDespite its promise, FL is vulnerable to various attacks due to its distributed nature [8]. Based on the capacity of adversaries, these threats can completely [9]\u2013[11] or partially [12]\u2013[14] harm the utility of model, break the privacy of participants [15]\u2013[17], and could happen at any phase during the FL service life cycle. In response, numerous defense frameworks [18]\u2013[20] have been proposed and shown effectiveness across different settings and scenarios. Given the extensive security research, the FL community urgently needs comprehensive reviews to provide a clear understanding of the current research landscape.\nAlthough some reviews exist [21]\u2013[23], their scope is generally limited to threats leading to direct, serious harm and around the local training phase, failing to provide a comprehensive picture of the research field. To mitigate the research gap, we present this review, which aims to comprehensively investigate all threats throughout the FL life cycle, along with their corresponding defenses. Unlike existing reviews, our study considers both direct and potential threats and expands the scope to encompass the entire FL life cycle.\nIn summary, the main contributions of this survey are as follows:\n\u2022 Providing the entire life cycle of FL services, from the learning task publish to service delivery; identifying each learning phase's vulnerabilities, the potential threats' impact, and actors.\n\u2022 Identifying and comparing the utility-harmed threats in the FL environment, providing their attack and poisoning strategies, ranging from those with the potential to direct impact.\n\u2022 Identifying and comparing the privacy-harmed threats in the FL environment, ranging from the application to the network layer.\n\u2022 Classifying the existing defense frameworks, summarizing their effectiveness when defending against various utility-harmed or privacy-harmed threats, and discussing their assumptions and trade-offs.\n\u2022 Discussing the existing bottlenecks of trustworthy FL, identifying the emerging requirements, and providing insights about future trends.\nThe rest of this paper is organized as follows: We provide the background information of FL in Section II, including its life cycle, vulnerabilities, and categories. Sections III and IV review the most representative and state-of-the-art threats targeting the utility and privacy of FL systems. In Sections V and VI, we summarize the defense frameworks for addressing FL privacy concerns and enhancing FL robustness, compare their effectiveness, and discuss their trade-offs. Section VII offers insights into future research trends, and Section VIII concludes the survey. For better readability, we present a diagram in Figure 1, illustrating the organization of the survey."}, {"title": "II. FEDERATED LEARNING BACKGROUND", "content": "In this section, we introduce the FL service life cycle, different types of FL, and FL threat models to provide the survey's background."}, {"title": "A. Federated Learning Life cycle", "content": "0. Learning task bidding and participants selection:\nThe life cycle of FL begins when service providers publish a learning task within FL communities or mention it in an application's user terms. Interested clients submit bids to participate based on the specified requirements and promised rewards. These bids include commitments of their computing resources, bandwidth, network capabilities, and training data. Service providers then review these commitments and select the participants for the learning task.\n1. Global model broadcasting: The server, which can be the service provider or another third-party computing center, broadcasts the current global model to all selected participants. During the initialization of the learning task (the first learning round), the server also specifies the conditions for concluding both local and global learning iterations.\n2. Participants' local training: Once the current global model is received, each participant trains the model locally using their private data. This training process continues for several rounds until preset conditions, such as a specified number of learning rounds or a desired model accuracy, are met. Subsequently, the participants send their local model updates back to the server, completing the local training phase.\n3. Aggregation and global model update: The server aggregates the collected local model updates using a predefined aggregation rule. The resulting outcome serves as the gradient to update the global model. If the end condition has not been reached, the updated global model is then broadcast to all participants for the next round of training.\n4. Incentive distribution: When the global model achieves the expected performance, the server stops broadcasting, and the training task is finalized. The service provider then evaluates each participant's contribution and distributes incentives accordingly, which could be either profit or non-profit.\n5. Model publish, and user access: Finally, the service provider publishes the FL model or FL-based service online. Users can then access and utilize the application for various purposes, such as querying, making predictions, and more. Figure 2 illustrates the life cycle of FL service."}, {"title": "B. Federated learning Categories", "content": "The FL can be categorized based on the participants' data distribution or the system architecture [24].\nBased on the distribution of data features and data samples, FL can be classified as horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning (FTL). Specifically, HFL is applicable to cases in which participants own different data samples but the same feature spaces. For example, in rare disease research, hospitals may have distinct patients but their records own highly similar features. Contrary to HFL, the VFL participants overlap in sample spaces but differ in feature spaces. For example, banks and telecom operators in a city may keep records of the same citizens, but these records contain different features. FTL integrates the concepts of FL and transfer learning, where transfer learning leverages knowledge acquired from one domain to enhance performance in another domain. FTL addresses the challenge of limited overlapping samples and features by training a model on a large public dataset. Figure 3 illustrates the clients' data distribution of HFL, VFL and FTL.\nOn the other hand, FL can be divided into centralized federated learning (CFL) and decentralized federated learning (DFL) based on the system architecture. Under CFL, a server organizes the whole learning process, including models broadcast, collection and aggregation. In contrast, DFL does not involve the central server. Following a preset rule, participants can directly exchange their models and automatically perform aggregation. Without further notification, we use FL to represent CFL in this survey. Figure 4 illustrates the system architectures of CFL and DFL."}, {"title": "C. Threat Models in Federated Learning Life Cycle", "content": "In FL, each participant has a certain degree of autonomy and can independently conduct local training. While the federated architecture potentially enhances participants' privacy, it also introduces new attack surfaces. Beyond the training phase attacks that directly degrade FL model accuracy and compromise client privacy, dishonest behavior in other phases of the FL life cycle can also negatively impact the model. For instance, dishonest self-reporting during participant selection can lead to insufficient computing or data resources during training, resulting in lower-than-expected model performance. In light of these scenarios, we propose a broader definition of threats and attackers in this survey:\nThreats (i.e., attacks) are the misbehavior that may directly or potentially harm FL models' performance (i.e., utility) or privacy.\nAttackers are the actors who intentionally launch threats.\nVulnerabilities in FL: Today, numerous studies have demonstrated that attacks can occur at any stage throughout the entire FL life cycle. Initially, a dishonest client may exaggerate its capabilities to secure a spot in the selection process and join the learning task. Under the local training, the malicious clients may use modified training data to train the model, or directly perturb the trained model (or gradients) to degrade, even fully break the model performance. During server-client communication, the information transmitted can be replaced or tampered with by a \"middleman.\" Once the training is complete, the dishonest participants can forge their consumption to defraud high returns. When the FL-based application is published, an attacker can use a crafted data item to mislead the application to generate unexpected outcomes. Although the server is assumed to be trustworthy in most cases, the server is potentially curious or hijacked during the training process. In those cases, the server may act maliciously and infer sensitive information from individual updates collected. We summarize and provide detailed discussions of the utility-harmed and privacy-harmed threats in Section III and IV.\nAttacker's Knowledge and Capacity: Attacker's knowledge represents how much information of the FL system can be inspected by the adversary. From the perspective of model parameters, the attacker's knowledge can be categorized as black-box, grey-box, or white-box [22], [25]. These three settings correspond to scenarios where the adversary has no access to the FL model, has the ability to inspect the model of a particular state (possibly a historical version), and can inspect the FL model parameters of the current learning round, respectively. Considering participants receive the global model to perform local training each iteration, the grey box is the most general setting in adversarial FL contents. As some FL algorithms introduce the byzantine-robust aggregation rule as a defense mechanism, the attacker's knowledge is also considered as to whether the aggregation rule is exposed and reviewed by adversaries.\nThe attacker's capacity in the FL context generally refers to the level of collusion among adversaries. Malicious clients are often assumed to constitute a certain percentage of the overall participants, and these attackers can collude to agree on the same attack strategy to maximize attack efficiency. In a non-collusion setting, the adversaries are considered to have different attacking objectives or strategies, resulting in a weaker attack effect. We note that some recent works [26] classifies the attacker's capacity as passive or active, depending on whether the adversary can directly interfere with the global model or simply inspect it. However, we believe that the terms passive and active do not accurately reflect the capacity of the attackers but rather the characteristics of different phases in the FL life cycle. Specifically, participants can arbitrarily modify their local model during the training phase, while they can only access or visit the FL application during the pre- and post-training phases. Thus, in this survey, we do not discuss the passive or active nature of adversaries from their perspective. Instead, we emphasize the interaction methods of various parties to the FL system at different phases in the FL life cycle.\nIn some attack strategies, attackers may need to perform large-scale grid searches or complex computations to generate the desired crafted parameters. Under these scenarios, the attacker's capacity extends to computing resources, which could be at the average level of the overall clients or potentially unlimited."}, {"title": "III. UTILITY-HARMED ATTACKS", "content": "Compared to data-centralized collection learning schemes, FL provides a different way of training models collaboratively [3], [4]. Instead of relying on a visible and centralized collection dataset, FL trains the model across a potentially vast array of unreliable devices, each equipped with private, uninspectable datasets. Furthermore, the local training process also introduces a new attack surface and brings opportunities for potential attacks [11], [27]\u2013[29]. Specifically, as data remains on the devices and only model updates are shared, adversaries could attempt to exploit this setup to compromise the model's utility or performance. We demonstrate and compare different FL utility-harmed attacks in Figure 5."}, {"title": "A. Poisoning Attacks", "content": "As one of the most representative adversarial attacks, poisoning attacks attempt to degrade the model performance or totally destroy the global model. Depending on the stage at which the poisoning occurs, poisoning attacks can be further categorized into model poisoning attacks and data poisoning attacks.\n1) Model poisoning Attacks: Model poisoning attacks [30] refer to adversaries directly manipulating reports (local model updates) submitted to the service provider. One of the most straightforward strategies for executing model attacks involves introducing a fixed perturbation or substituting the benign gradient parameters with fixed values. For instance, Reverse Attack [9], and Random Attack [10] generate the malicious gradient by reversing and randomly replacing the original gradients. Under the Partial Drop Attack [10], the adversary replaces a preset percentage of the benign gradient parameters as 0; the variants of Partial Drop Attack enlarge the attack effectiveness by using other malicious values if the benign gradients naturally carry a large amount of 0. Although these attack strategies can effectively degrade the global model performance and decrease the testing accuracy, the malicious gradients are unavoidably away from the benign gradients, which can be identified and discarded by the byzantine resilient FL methods.\nTo enhance the attacking performance and avoid being identified by the defender, some existing attacks [11], [27], [28] design more dynamic attack strategies. For instance, Little is Enough Attack [31] first normally trains the model through the data of the clients controlled. Then, the adversary statistics the mean value and standard deviation (Std) of the gradients owned. Based on the statistics, the adversary subsequently generates the crafted gradient by adding a scaled Std on the mean value. The Fall of Empires Attack [32] follows a similar strategy, which generates the malicious gradient by timing the mean value with a preset scalier. To maintain the stealth of model poisoning attack, research [27] proposes an optimization-based model poisoning attack, which only manipulates a small fraction of the local model updates instead of all gradient parameters. Under the study [27], adversary training is organized into two tasks: the main task and the adversarial task. In the main task, the adversary trains the model in the usual manner. Conversely, in the adversarial task, the adversary embeds adversarial features into the neural network's redundant space to enhance the attack's persistence.\nRecently, Local Model Poisoning Attack [11] proposes a more flexible attacking algorithm that adopts the attack strategy for different defense frameworks. Based on the information owned (partial or full knowledge), this attack first infers the optimal global model updates and subsequently formulates the optimization problem that aims to reverse the global model update the most along the infers. This work [11] applies the attack in Krum [33], Trimmed-mean [34], and Median [34] frameworks and consequently generates the Krum Attack and Trim Attack. The study [28] targets the Krum framework with an emphasis on evasion detection, specifically through covert model poisoning (CMP). Research [28] first formulates the model poisoning as an optimization problem, aiming to minimize the Euclidean distance between the manipulated model and a designated one, all within the constraints set by Krum. Based on the solution to the optimization problem, [28] subsequently formulates CMP algorithms to counter the Krum framework. To improve the practicability, a low-complexity CMP algorithm is introduced and achieves the optimization complexity reduction.", "equations": ["G' = -G", "G' = Gi", "G' = G. M, M ~ Bernoulli(P)", "G' = G. M, M ~ Random(P)", "G' = \u03bc - zo or \u03bc + 2\u03c3", "G' = -zu or zu", "| G' \u2208 (\u03bc + 3\u03c3, \u03bc + 40) or (\u03bc \u2013 4\u03c3, \u03bc \u2013 3\u03c3)", "| G'\u2208 (Gmax, z. Gmax) or (z. Gmin, Gmin)", "G' = favg(G) + y\u2207,", "arg max max||G \u2013 \u0120i||2 \u2264 max||\u0120i - Gj||2", "\u0e0a", "G' = favg(G) + y\u2207,", "arg max ||G \u2013 Gi|| < ||\u0120i \u2013 Gj||"]}, {"title": "2) Data poisoning Attacks:", "content": "Different from model poisoning attacks that directly introduce the perturbation to the gradient, data poisoning attacks generate crafted gradients by training models based on poisoned data. Label Flipping Attack is one of the most representative data poisoning attacks. In Label Flipping Attacks, the adversary mismatches the training labels and training data to generate the malicious gradient, aiming to degrade the attacked class learning performance of the joint model. Study [29] considers the support vector machine (SVM) algorithm and aims to find a labelflipping combination under a given budget so that a classifier trained on such data will have maximal classification error. Research [37] introduces the Label Flipping Attack in the deep learning scenarios and evaluates the key conditions for successfully performing attacks, including sufficient adversarial participants and continuous participation. To enhance the flexibility of the attacks, study [38] proposes the On-Off Label Flipping strategy. Under the On-Off Label Flipping strategy, the adversary first acts as a benign client for a period of time and builds a positive expectation in the defense system to strengthen the impact of the following updates. Then, the adversary betrays the system and performs an attack for a subsequent period of time. As the betrayal behavior lowers the reputation, the adversary may toggle back to acting as the benign client again after the One-Off attack and recover its reputation.\nLabel Flipping Attacks also show significant effectiveness in real-world applications. For instance, Study [19] implements this attack within a malware detection task for Android platforms, employing a Silhouette Clusteringbased Label Flipping Attack (SCLFA). This method involves attackers calculating the silhouette scores of data samples and targeting those with negative values for generating polluted data through label flipping. Similarly, research [39] illustrates the potency of Label Flipping Attacks in compromising spam filtering systems. By executing entropy-based flipping attacks, their study managed to elevate the false negative rate of Naive Bayes classifiers in the presence of label noise without impairing the classification of legitimate emails. Furthermore, study [40] explores the impact of Label Flipping Attacks on hardware Trojan detection systems, demonstrating how model performance could be substantially degraded through a stochastic hill-climbing search-based flipping strategy, incurring minimal costs for label manipulation. These instances underscore the critical need for robust defenses against Label Flipping Attacks in diverse application areas."}, {"title": "3) Backdoor Attacks:", "content": "Backdoor Attack is a special type of poisoning attack, which aims to lead the global model to misbehave only on a selected minority of examples while maintaining good overall accuracy on all other examples [12], [13]. Recent studies indicate that the Backdoor Attack can be executed either by poisoning the clean training data or by introducing crafty perturbations to benign gradients. In other words, the Backdoor Attack can be categorized as either a model poisoning attack or a data poisoning attack based on the poisoning strategy.\nWhen performing Backdoor Attacks through the data poisoning strategy, the adversary should first decide on a trigger (backdoor) and consequently assign a selected label for the trigger carrier to form the poisoning data. This backdoor could be semantic or artificial. For semantic backdoor, study [14] leverages the samples with uncommon attributes as the backdoor, such as the cars with unconventional colors (e.g., green), scenes containing a unique object (e.g., a striped pattern), or trigger sentences in word prediction problems that conclude with an attacker-selected target word. Similarly, research [41] proposes an attack strategy to introduce a backdoor in the FL-based IoT intrusion detection system. In this scenario, the adversary targets packet sequences originating from specific malware-driven malicious traffic. Different from the semantic backdoor that replies on existing share properties, the artificial backdoor manually poisons benign samples by introducing triggers. Study [1] demonstrates the adversary can manually add pattern \"L\" at the images' corner to activate the backdoor, where the pattern \"L\" does not naturally exist in the benign samples. A recent study [42] extends the scope of backdoor attacks to real-world settings. The research reveals that beyond digital triggers, physical triggers such as sunglasses, tattoos, and earrings can also serve as triggers for organizing backdoor attacks.\nWithin the Vanilla FL framework, the most straightforward method to execute a backdoor attack using a model poisoning strategy is scaling the malicious updates containing backdoor information to dominate updates from benign clients. Study [42] first employs the replace method, in which the attacker seeks to substitute the new global model with a poisoned one by a wisely chosen factor. While the scaling-replacement method proves effective in averaging aggregation, straightforward scaling appears to be naive to success under clipping and restricting defenses. To enhance the stealth of model poisoning attacks, research [43] proposes a projected gradient descent (PGD) attack strategy, which projects the crafted model on a small ball centered around the global model from the previous iteration."}, {"title": "B. Promise breaking", "content": "During the FL life cycle, clients may pledge to provide their training data or computing resources to gain participation opportunities or claim their contributions to receive incentives. However, malicious clients can exploit the FL system to gain advantages, such as access to the joint model and incentives, without fulfilling their promises [44], [45]. Although these attackers do not deliberately poison the model, promise-breaking behaviors can lead to insufficient computing resources and training data, as well as unfair incentive distribution. This, in turn, may degrade learning performance and diminish participant motivation.\nResearch [45] introduces a Free-riding Attack framework based on the Vanilla FL and shows that the adversary can perform a Free-riding Attack in each learning iteration. In particular, the Free-riding attacker does not perform local training but adds noise to the constructed parameter updates and applies Stochastic Gradient Descent (SGD) to maximize similarity with updates from other benign clients. Research [44] proposes a novel Free-riding Attack in which the model is trained locally using a small dataset masqueraded as a large dataset to obtain more incentive rewards. Study [46] introduces a Free-riding attack strategy, namely Random Weights Attack. Under this attack, the adversary creates a gradient update matrix with dimensions matching the received global model by randomly sampling each parameter from a uniform distribution within a pre-designed range.\nStudies [47]\u2013[49] investigate dishonest reporting during intensive distribution. To cheat the reward system, malicious participants may falsely report their resource consumption or claim to have trained the joint model with high-quality data while actually using low-quality data, such as spam."}, {"title": "C. Sybil Attacks", "content": "The Sybil Attack refers to a single attacker (or a small amount attackers) joining the system with multiple colluding identities to enhance the stealth and effectiveness of an adversarial attack. Given that some traditional FL permits selected participants to freely join and exit during training tasks, it becomes inherently susceptible to Sybil Attacks.\nThe research [50] first introduces Sybil Attacks within the FL context. It demonstrates Sybil Attacks can significantly enhance the impact of other attacks (such as the Label Flipping Attack) by misleading the global model into classifying handwritten digits \u201c1\u201d as \u201c7\u201d through only two sybils. Study [51] builds upon the findings of [50], delving deeper into the vulnerability of FL to Sybil Attacks, as well as exploring associated attack strategies and objectives. This work [51] categorizes and names the Denial-of-Service (DoS) attacks instigated by Sybil Attacks as Training Inflation and demonstrates Sybil Attacks can be performed from various FL dimensions. From the data distribution perspective, Sybils can use identical datasets, different datasets, or synthetic data to train local models and generate crafted updates [52]. On the other hand, to dominate the benign clients and amplify their influence on the system, Sybils might coordinate amongst themselves before producing the subsequent series of model updates, achieving different levels of coordination."}, {"title": "D. Communication Layer Attacks", "content": "FL continuously updates the joint model, which relies on communications between the server and multiple participants. However, the large volume of data exchange and frequent communication drives a significant communication bottleneck and brings new attack interfaces for adversaries. The man-in-the-middle (MiTM) attacks [53], as one of the most representative network layer attacks, has been introduced in FL by recent studies [2], [54]. As FL leverages the single server to aggregate model updates and organize the learning process, performing MiTM between the server and participants can effectively block the server and lead to a single point of failure. Research [55] explores attacking the FL communication bottleneck by decreasing the bandwidth, delaying the response, and increasing the struggle possibility. Through performing attacks on the communication of FL, [55] shows the model convergence and performance can be degraded significantly."}, {"title": "IV. PRIVACY-HARMED ATTACKS", "content": "In evasion attacks, the adversary deliberately adds slight malicious perturbations to input samples during the inference phase, causing the classifier to misclassify the sample with a high probability. One of the most representative works of the evasion attack has been introduced by [56], which is a so-called adversarial example. By adding unnoticeable pixel perturbation on a panda image, [56] successfully changes the classification of GoogLeNet [57] from panda to gibbon with 99.3% confidence.\nBased on the information owned by the adversary, the evasion attacks could be further divided into white-box and black-box attacks. In the white-box setting, the adversary has full knowledge of the learning algorithm and model parameters and can craft adversarial examples based on the information. For instance, studies [58], [59] generate adversarial examples by maximizing the loss function, subject to a norm constraint, using constrained optimization techniques like projected gradient ascent. On the other hand, the black-box setting refers to the attackers having no knowledge about the algorithm and parameter information of the FL system and generating adversarial samples through the interaction process or query access with the system. Considering the black-box setting, [60] proposes a zeroth order optimization (ZOO) based attack to estimate the target model updates and generate adversarial examples consequently. Similarly, Boundary Attack [61] has been designed in the same scenario. When performing Boundary Attack [61], the adversary first identifies the boundary separating adversarial and non-adversarial samples, then subtly introduces perturbations along this boundary to produce adversarial examples.\nA recent work [62] introduces an Evasion Attack in the Vertical FL (VFL) scenarios and proposes the Adversarial Dominating Inputs Attack. Unlike the aforementioned adversarial samples that domain the whole feature space, the Adversarial Dominating Inputs Attack [62] can dominate the inputs of other clients by controlling a fraction of the feature inputs and lead to the misclassification of specific inputs. In the meanwhile, the Adversarial Dominating Inputs Attack can also result in reduced and unfair incentive rewards for impacted participants due to their decreasing contributions."}, {"title": "B. Inference Attacks", "content": "During the training process of FL, the model will inadvertently learn the latent information of private data. Therefore, an honest but curious server or any other user can launch an inference attack to recover the original training data without any prior knowledge. Inference Attacks include model extraction attacks, attribute inference attacks, and membership inference attacks.\n1) Membership Inference Attack (MIA): Similar to MIAs in traditional machine learning scenarios, malicious clients or users can execute MIAs in FL applications to infer whether a data sample belongs to its training dataset. Study [15] focuses on black-box within Machine Learning as a Service (MLaaS) settings. By accessing multiple generated shadow models, the study achieves high accuracy in inferring membership. ML-Leaks [70] reduces the cost of launching attacks and demonstrates the effectiveness of MIA using only a single (even without) shadow model. Study [16] considers the white-box settings and proposes the gradient ascent MIA strategy to magnify the presence of data points in others' training sets. It demonstrates that all algorithms trained using Stochastic Gradient Descent (SGD) are susceptible to privacy vulnerabilities from MIAs. The study [71] further extends MIAs to NLP learning tasks and effectively infers whether a specific text segment belongs to the training dataset.\n2) Property Inference Attacks (PIA): Property inference attacks aim to infer private features or characteristics (e.g., value, distribution, etc.) of the training data without direct exposure to the data. Although some surveys [24], [72] categorize threats aimed at reconstructing training data as PIAs, we consider PIAs to primarily focus on inferring properties of the training data, rather than the data itself. Therefore, in this survey, we differentiate threats aimed at data reconstruction from PIAs and classify them as Inversion Attacks.\nAttackers in the study [73] use auxiliary data and trained classifiers to determine if observed model updates are generated from a dataset that includes the target property. Considering fully connected neural networks (FCNNs) are invariant under the permutation of nodes when represented using matrices, study [74] introduces PIAs in white-box settings and shows good inference performance in shallow-FCNNs. The study [75] further extends PIAs to logistic regression (LR), long short-term memory networks (LSTM), and graph convolutional networks (GCN) across multiple domains. It demonstrates that even in black-box settings, PIAs can still achieve accuracies exceeding 60%.\n3) Representative Inference Attacks (RIA): Representative Inference Attacks indicate the malicious generate dummy outcome that is representative of the victim's training data [76], [77]. The study [76] first introduces record-level RIAs in collaborative learning scenarios and demonstrates that GAN-based attacks can effectively compromise against Distributed Selective SGD (DSSGD [78]). However, subsequent research [77] reveals its performance degradation in FL settings due to the averaging of updates and introduces the mGAN-AI framework. This framework enhances GANs with a multitask discriminator capable of simultaneously assessing the category, reality, and client identity of input samples. Compared to [76], mGAN-AI enables the generator to recover representative private data for specific users."}, {"title": "C. Inversion Attacks", "content": "Although FL allows clients to only upload gradients during model training, it has been found that some properties of the training data are implicitly carried by these gradients, which could be revealed through inverse attacks. DLG [64] first introduces gradient inversion in FL computer vision areas. Specifically, the DLG generates random dummy input samples x' and labels y' and corresponding dummy gradients G'. x' and y' are Iteratively optimized through L-BFGS to minimize the difference between dummy and benign gradient. Formally, the objective function of DLG shown as follows:\nConsidering the high-dimensional direction of gradients carried important information, study [65] introduces cosine similarity in the loss function to find images that lead to a similar change in model prediction as the ground truth instead of finding images with a gradient that best fits the observed gradient. The objective function is shown as follows, where TV represents the Total Variance regularization.\nAlthough DLG [64], IG [65] show effectiveness on single image and small batch size training scenarios, their inversion performance degrades when facing large batch size (> 8). To further improve the attack effectiveness, GGL [68] leverages a generative model trained on public datasets as the learned natural image prior to ensure the reconstructed image quality. Additionally, GradInversion (GI) [67] integrates Fidelity and Group Consistency regularization into the objective function and effectively increases the inversion batch size to 48. These are designed to ensure that the generated images closely resemble real images and to penalize any dummy images that deviate from a \"consensus\" image, respectively. GradViT [69] extends the GI approach to Vision Transformer (ViT) models, demonstrating that ViTs are significantly more vulnerable to inversion attacks than the previously studied CNNs due to the attention mechanism."}, {"title": "D. Eavesdropping", "content": "Network eavesdropping involves an attacker intercepting and capturing data packets transmitted over a network using their own devices, with the intent to analyze the contents of these packets to get private information [80]. Since model training under the FL scheme requires regular communication between the server and clients, FL is naturally vulnerable to eavesdropping attacks. This vulnerability is particularly pronounced when the communication between participants and the server is in plaintext or employs a weak encryption method [23], [81]. Since the messages transmitted are usually models or gradients, the attacker may further leverage inference or inversion attacks to decipher the eavesdropped messages to understand the private information [82]\u2013[84]. Study [85] considers the eavesdropping attacks in unmanned aerial vehicles (UAVs) scenarios, demonstrating that eavesdroppers can deduce the raw data from the shared parameters and compromise participants' privacy."}, {"title": "V. DEFENCE AGAINST UTILITY-HARMED ATTACKS", "content": "As the diversity and complexity of adversarial attacks against FL increase, new defenses are being developed to counteract their malicious impacts. These defense frameworks generally follow the setting that the server is honest and can play the defender role. As some defense methods are effective against multiple types of attack categories, we survey these FL defense frameworks from different technologies instead of grouping them according to the attack category defended."}, {"title": "A. Data Sanitization", "content": "Data sanitization [86]\u2013[88] has been introduced to remove malicious and suspicious data before the training process and consequently mitigate crafted-data-related threats, including data poisoning and evasion attacks. The research [86] introduces a novel training pipeline that incorporates a data sanitization phase. During each iteration, this phase creates multiple models, termed micro-models, based on subsets of the training data. These micro-models are used to produce provisional labels for each training input and consequently combined in a voting scheme to determine which parts of the training data may be potentially attacked. To filter out noise-labeled data, FedDiv [89] maintains a global filter. This filter is trained, exchanged, and aggregated similarly to the global model and supports local data filtration. From the data shuffling perspective, study [88] proposes a byzantine-resilient matrix-vector (MV) multiplication and further proposes an algorithm based on the data encoding process and error correction over real numbers to combat various adversarial attacks.\nAlthough data sanitization can filter out some potential attacked data and keep training data clean, it relies on directly accessing the local data of clients, which may raise privacy issues. A recent work [90] indicates that data sanitization can only work in cross-silo FL with strong authentication and be performed locally by the client itself. However, this framework [90] assumes the trustworthiness of the clients, which may not be applicable to the adversarial FL environment."}, {"title": "B. Anomaly Detection", "content": "To perform threats"}, {"title": "Threats and Defenses in Federated Learning Life Cycle: A Comprehensive Survey and Challenges", "authors": ["Yanli Li", "Jifei Hu", "Zhongliang Guo", "Nan Yang", "Huaming Chen", "Dong Yuan", "Weiping Ding"], "abstract": "Federated Learning (FL) offers innovative solutions for privacy-preserving collaborative machine learning (ML). Despite its promising potential, FL is vulnerable to various attacks due to its distributed nature, affecting the entire life cycle of FL services. These threats can harm the model's utility or compromise participants' privacy, either directly or indirectly. In response, numerous defense frameworks have been proposed, demonstrating effectiveness in specific settings and scenarios. To provide a clear understanding of the current research landscape, this paper reviews the most representative and state-of-the-art threats and defense frameworks throughout the FL service life cycle. We start by identifying FL threats that harm utility and privacy, including those with potential or direct impacts. Then, we dive into the defense frameworks, analyze the relationship between threats and defenses, and compare the trade-offs among different defense strategies. Finally, we summarize current research bottlenecks and offer insights into future research directions to conclude this survey. We hope this survey sheds light on trustworthy FL research and contributes to the FL community.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, smart devices have become integral to people's daily lives. While engaging with these applications or smart services, users generate tens of billions of data points per second. This data, regarded as a valuable resource for fueling data-driven models, but remains underutilized due to the centralized nature of traditional training paradigms [1], [2]. In particular, centralized data collection necessitates stable network connections and high-speed broadband, conditions that often conflict with the realities of edge devices. Even worse, transferring data from the user's domain raises significant privacy concerns, particularly when dealing with sensitive financial, medical, and social media information. As phrased by authors in [3], \"data remains isolated within individual owners' hands, creating data silos.\"\nTo mitigate the problem of data silos, federated learning (FL) was introduced by Google in 2017 [4]. This new machine learning (ML) paradigm allows participants to collaborate in a loosely federated manner while retaining a degree of \"autonomy\" [3]. Under the FL paradigm, participants locally train their model and only upload model updates for aggregation. Due to its privacy-preserving characteristics, FL has attracted significant attention from both academia and industry, leading to the deployment of numerous real-world applications. Notable examples include Google's smart keyboard Gboard [5], Apple's intelligent voice assistant Siri [6], and WeBank's finance risk prediction service for reinsurance [7]. A typical FL service life cycle [1], [3], [4] begins with a service provider publishing a task and selecting participants. Each selected client then receives the global model, trains it locally, uploads the gradients, and participates in the aggregation process. Once the global model achieves the desired performance, the server distributes rewards and releases the application to the public.\nDespite its promise, FL is vulnerable to various attacks due to its distributed nature [8]. Based on the capacity of adversaries, these threats can completely [9]\u2013[11] or partially [12]\u2013[14] harm the utility of model, break the privacy of participants [15]\u2013[17], and could happen at any phase during the FL service life cycle. In response, numerous defense frameworks [18]\u2013[20] have been proposed and shown effectiveness across different settings and scenarios. Given the extensive security research, the FL community urgently needs comprehensive reviews to provide a clear understanding of the current research landscape.\nAlthough some reviews exist [21]\u2013[23], their scope is generally limited to threats leading to direct, serious harm and around the local training phase, failing to provide a comprehensive picture of the research field. To mitigate the research gap, we present this review, which aims to comprehensively investigate all threats throughout the FL life cycle, along with their corresponding defenses. Unlike existing reviews, our study considers both direct and potential threats and expands the scope to encompass the entire FL life cycle.\nIn summary, the main contributions of this survey are as follows:\n\u2022 Providing the entire life cycle of FL services, from the learning task publish to service delivery; identifying each learning phase's vulnerabilities, the potential threats' impact, and actors.\n\u2022 Identifying and comparing the utility-harmed threats in the FL environment, providing their attack and poisoning strategies, ranging from those with the potential to direct impact.\n\u2022 Identifying and comparing the privacy-harmed threats in the FL environment, ranging from the application to the network layer.\n\u2022 Classifying the existing defense frameworks, summarizing their effectiveness when defending against various utility-harmed or privacy-harmed threats, and discussing their assumptions and trade-offs.\n\u2022 Discussing the existing bottlenecks of trustworthy FL, identifying the emerging requirements, and providing insights about future trends.\nThe rest of this paper is organized as follows: We provide the background information of FL in Section II, including its life cycle, vulnerabilities, and categories. Sections III and IV review the most representative and state-of-the-art threats targeting the utility and privacy of FL systems. In Sections V and VI, we summarize the defense frameworks for addressing FL privacy concerns and enhancing FL robustness, compare their effectiveness, and discuss their trade-offs. Section VII offers insights into future research trends, and Section VIII concludes the survey. For better readability, we present a diagram in Figure 1, illustrating the organization of the survey."}, {"title": "II. FEDERATED LEARNING BACKGROUND", "content": "In this section, we introduce the FL service life cycle, different types of FL, and FL threat models to provide the survey's background."}, {"title": "A. Federated Learning Life cycle", "content": "0. Learning task bidding and participants selection:\nThe life cycle of FL begins when service providers publish a learning task within FL communities or mention it in an application's user terms. Interested clients submit bids to participate based on the specified requirements and promised rewards. These bids include commitments of their computing resources, bandwidth, network capabilities, and training data. Service providers then review these commitments and select the participants for the learning task.\n1. Global model broadcasting: The server, which can be the service provider or another third-party computing center, broadcasts the current global model to all selected participants. During the initialization of the learning task (the first learning round), the server also specifies the conditions for concluding both local and global learning iterations.\n2. Participants' local training: Once the current global model is received, each participant trains the model locally using their private data. This training process continues for several rounds until preset conditions, such as a specified number of learning rounds or a desired model accuracy, are met. Subsequently, the participants send their local model updates back to the server, completing the local training phase.\n3. Aggregation and global model update: The server aggregates the collected local model updates using a predefined aggregation rule. The resulting outcome serves as the gradient to update the global model. If the end condition has not been reached, the updated global model is then broadcast to all participants for the next round of training.\n4. Incentive distribution: When the global model achieves the expected performance, the server stops broadcasting, and the training task is finalized. The service provider then evaluates each participant's contribution and distributes incentives accordingly, which could be either profit or non-profit.\n5. Model publish, and user access: Finally, the service provider publishes the FL model or FL-based service online. Users can then access and utilize the application for various purposes, such as querying, making predictions, and more. Figure 2 illustrates the life cycle of FL service."}, {"title": "B. Federated learning Categories", "content": "The FL can be categorized based on the participants' data distribution or the system architecture [24].\nBased on the distribution of data features and data samples, FL can be classified as horizontal federated learning (HFL), vertical federated learning (VFL), and federated transfer learning (FTL). Specifically, HFL is applicable to cases in which participants own different data samples but the same feature spaces. For example, in rare disease research, hospitals may have distinct patients but their records own highly similar features. Contrary to HFL, the VFL participants overlap in sample spaces but differ in feature spaces. For example, banks and telecom operators in a city may keep records of the same citizens, but these records contain different features. FTL integrates the concepts of FL and transfer learning, where transfer learning leverages knowledge acquired from one domain to enhance performance in another domain. FTL addresses the challenge of limited overlapping samples and features by training a model on a large public dataset. Figure 3 illustrates the clients' data distribution of HFL, VFL and FTL.\nOn the other hand, FL can be divided into centralized federated learning (CFL) and decentralized federated learning (DFL) based on the system architecture. Under CFL, a server organizes the whole learning process, including models broadcast, collection and aggregation. In contrast, DFL does not involve the central server. Following a preset rule, participants can directly exchange their models and automatically perform aggregation. Without further notification, we use FL to represent CFL in this survey. Figure 4 illustrates the system architectures of CFL and DFL."}, {"title": "C. Threat Models in Federated Learning Life Cycle", "content": "In FL, each participant has a certain degree of autonomy and can independently conduct local training. While the federated architecture potentially enhances participants' privacy, it also introduces new attack surfaces. Beyond the training phase attacks that directly degrade FL model accuracy and compromise client privacy, dishonest behavior in other phases of the FL life cycle can also negatively impact the model. For instance, dishonest self-reporting during participant selection can lead to insufficient computing or data resources during training, resulting in lower-than-expected model performance. In light of these scenarios, we propose a broader definition of threats and attackers in this survey:\nThreats (i.e., attacks) are the misbehavior that may directly or potentially harm FL models' performance (i.e., utility) or privacy.\nAttackers are the actors who intentionally launch threats.\nVulnerabilities in FL: Today, numerous studies have demonstrated that attacks can occur at any stage throughout the entire FL life cycle. Initially, a dishonest client may exaggerate its capabilities to secure a spot in the selection process and join the learning task. Under the local training, the malicious clients may use modified training data to train the model, or directly perturb the trained model (or gradients) to degrade, even fully break the model performance. During server-client communication, the information transmitted can be replaced or tampered with by a \"middleman.\" Once the training is complete, the dishonest participants can forge their consumption to defraud high returns. When the FL-based application is published, an attacker can use a crafted data item to mislead the application to generate unexpected outcomes. Although the server is assumed to be trustworthy in most cases, the server is potentially curious or hijacked during the training process. In those cases, the server may act maliciously and infer sensitive information from individual updates collected. We summarize and provide detailed discussions of the utility-harmed and privacy-harmed threats in Section III and IV.\nAttacker's Knowledge and Capacity: Attacker's knowledge represents how much information of the FL system can be inspected by the adversary. From the perspective of model parameters, the attacker's knowledge can be categorized as black-box, grey-box, or white-box [22], [25]. These three settings correspond to scenarios where the adversary has no access to the FL model, has the ability to inspect the model of a particular state (possibly a historical version), and can inspect the FL model parameters of the current learning round, respectively. Considering participants receive the global model to perform local training each iteration, the grey box is the most general setting in adversarial FL contents. As some FL algorithms introduce the byzantine-robust aggregation rule as a defense mechanism, the attacker's knowledge is also considered as to whether the aggregation rule is exposed and reviewed by adversaries.\nThe attacker's capacity in the FL context generally refers to the level of collusion among adversaries. Malicious clients are often assumed to constitute a certain percentage of the overall participants, and these attackers can collude to agree on the same attack strategy to maximize attack efficiency. In a non-collusion setting, the adversaries are considered to have different attacking objectives or strategies, resulting in a weaker attack effect. We note that some recent works [26] classifies the attacker's capacity as passive or active, depending on whether the adversary can directly interfere with the global model or simply inspect it. However, we believe that the terms passive and active do not accurately reflect the capacity of the attackers but rather the characteristics of different phases in the FL life cycle. Specifically, participants can arbitrarily modify their local model during the training phase, while they can only access or visit the FL application during the pre- and post-training phases. Thus, in this survey, we do not discuss the passive or active nature of adversaries from their perspective. Instead, we emphasize the interaction methods of various parties to the FL system at different phases in the FL life cycle.\nIn some attack strategies, attackers may need to perform large-scale grid searches or complex computations to generate the desired crafted parameters. Under these scenarios, the attacker's capacity extends to computing resources, which could be at the average level of the overall clients or potentially unlimited."}, {"title": "III. UTILITY-HARMED ATTACKS", "content": "Compared to data-centralized collection learning schemes, FL provides a different way of training models collaboratively [3], [4]. Instead of relying on a visible and centralized collection dataset, FL trains the model across a potentially vast array of unreliable devices, each equipped with private, uninspectable datasets. Furthermore, the local training process also introduces a new attack surface and brings opportunities for potential attacks [11], [27]\u2013[29]. Specifically, as data remains on the devices and only model updates are shared, adversaries could attempt to exploit this setup to compromise the model's utility or performance. We demonstrate and compare different FL utility-harmed attacks in Figure 5."}, {"title": "A. Poisoning Attacks", "content": "As one of the most representative adversarial attacks, poisoning attacks attempt to degrade the model performance or totally destroy the global model. Depending on the stage at which the poisoning occurs, poisoning attacks can be further categorized into model poisoning attacks and data poisoning attacks.\n1) Model poisoning Attacks: Model poisoning attacks [30] refer to adversaries directly manipulating reports (local model updates) submitted to the service provider. One of the most straightforward strategies for executing model attacks involves introducing a fixed perturbation or substituting the benign gradient parameters with fixed values. For instance, Reverse Attack [9], and Random Attack [10] generate the malicious gradient by reversing and randomly replacing the original gradients. Under the Partial Drop Attack [10], the adversary replaces a preset percentage of the benign gradient parameters as 0; the variants of Partial Drop Attack enlarge the attack effectiveness by using other malicious values if the benign gradients naturally carry a large amount of 0. Although these attack strategies can effectively degrade the global model performance and decrease the testing accuracy, the malicious gradients are unavoidably away from the benign gradients, which can be identified and discarded by the byzantine resilient FL methods.\nTo enhance the attacking performance and avoid being identified by the defender, some existing attacks [11], [27], [28] design more dynamic attack strategies. For instance, Little is Enough Attack [31] first normally trains the model through the data of the clients controlled. Then, the adversary statistics the mean value and standard deviation (Std) of the gradients owned. Based on the statistics, the adversary subsequently generates the crafted gradient by adding a scaled Std on the mean value. The Fall of Empires Attack [32] follows a similar strategy, which generates the malicious gradient by timing the mean value with a preset scalier. To maintain the stealth of model poisoning attack, research [27] proposes an optimization-based model poisoning attack, which only manipulates a small fraction of the local model updates instead of all gradient parameters. Under the study [27], adversary training is organized into two tasks: the main task and the adversarial task. In the main task, the adversary trains the model in the usual manner. Conversely, in the adversarial task, the adversary embeds adversarial features into the neural network's redundant space to enhance the attack's persistence.\nRecently, Local Model Poisoning Attack [11] proposes a more flexible attacking algorithm that adopts the attack strategy for different defense frameworks. Based on the information owned (partial or full knowledge), this attack first infers the optimal global model updates and subsequently formulates the optimization problem that aims to reverse the global model update the most along the infers. This work [11] applies the attack in Krum [33], Trimmed-mean [34], and Median [34] frameworks and consequently generates the Krum Attack and Trim Attack. The study [28] targets the Krum framework with an emphasis on evasion detection, specifically through covert model poisoning (CMP). Research [28] first formulates the model poisoning as an optimization problem, aiming to minimize the Euclidean distance between the manipulated model and a designated one, all within the constraints set by Krum. Based on the solution to the optimization problem, [28] subsequently formulates CMP algorithms to counter the Krum framework. To improve the practicability, a low-complexity CMP algorithm is introduced and achieves the optimization complexity reduction."}, {"title": "2) Data poisoning Attacks:", "content": "Different from model poisoning attacks that directly introduce the perturbation to the gradient, data poisoning attacks generate crafted gradients by training models based on poisoned data. Label Flipping Attack is one of the most representative data poisoning attacks. In Label Flipping Attacks, the adversary mismatches the training labels and training data to generate the malicious gradient, aiming to degrade the attacked class learning performance of the joint model. Study [29] considers the support vector machine (SVM) algorithm and aims to find a labelflipping combination under a given budget so that a classifier trained on such data will have maximal classification error. Research [37] introduces the Label Flipping Attack in the deep learning scenarios and evaluates the key conditions for successfully performing attacks, including sufficient adversarial participants and continuous participation. To enhance the flexibility of the attacks, study [38] proposes the On-Off Label Flipping strategy. Under the On-Off Label Flipping strategy, the adversary first acts as a benign client for a period of time and builds a positive expectation in the defense system to strengthen the impact of the following updates. Then, the adversary betrays the system and performs an attack for a subsequent period of time. As the betrayal behavior lowers the reputation, the adversary may toggle back to acting as the benign client again after the One-Off attack and recover its reputation.\nLabel Flipping Attacks also show significant effectiveness in real-world applications. For instance, Study [19] implements this attack within a malware detection task for Android platforms, employing a Silhouette Clusteringbased Label Flipping Attack (SCLFA). This method involves attackers calculating the silhouette scores of data samples and targeting those with negative values for generating polluted data through label flipping. Similarly, research [39] illustrates the potency of Label Flipping Attacks in compromising spam filtering systems. By executing entropy-based flipping attacks, their study managed to elevate the false negative rate of Naive Bayes classifiers in the presence of label noise without impairing the classification of legitimate emails. Furthermore, study [40] explores the impact of Label Flipping Attacks on hardware Trojan detection systems, demonstrating how model performance could be substantially degraded through a stochastic hill-climbing search-based flipping strategy, incurring minimal costs for label manipulation. These instances underscore the critical need for robust defenses against Label Flipping Attacks in diverse application areas."}, {"title": "3) Backdoor Attacks:", "content": "Backdoor Attack is a special type of poisoning attack, which aims to lead the global model to misbehave only on a selected minority of examples while maintaining good overall accuracy on all other examples [12], [13]. Recent studies indicate that the Backdoor Attack can be executed either by poisoning the clean training data or by introducing crafty perturbations to benign gradients. In other words, the Backdoor Attack can be categorized as either a model poisoning attack or a data poisoning attack based on the poisoning strategy.\nWhen performing Backdoor Attacks through the data poisoning strategy, the adversary should first decide on a trigger (backdoor) and consequently assign a selected label for the trigger carrier to form the poisoning data. This backdoor could be semantic or artificial. For semantic backdoor, study [14] leverages the samples with uncommon attributes as the backdoor, such as the cars with unconventional colors (e.g., green), scenes containing a unique object (e.g., a striped pattern), or trigger sentences in word prediction problems that conclude with an attacker-selected target word. Similarly, research [41] proposes an attack strategy to introduce a backdoor in the FL-based IoT intrusion detection system. In this scenario, the adversary targets packet sequences originating from specific malware-driven malicious traffic. Different from the semantic backdoor that replies on existing share properties, the artificial backdoor manually poisons benign samples by introducing triggers. Study [1] demonstrates the adversary can manually add pattern \"L\" at the images' corner to activate the backdoor, where the pattern \"L\" does not naturally exist in the benign samples. A recent study [42] extends the scope of backdoor attacks to real-world settings. The research reveals that beyond digital triggers, physical triggers such as sunglasses, tattoos, and earrings can also serve as triggers for organizing backdoor attacks.\nWithin the Vanilla FL framework, the most straightforward method to execute a backdoor attack using a model poisoning strategy is scaling the malicious updates containing backdoor information to dominate updates from benign clients. Study [42] first employs the replace method, in which the attacker seeks to substitute the new global model with a poisoned one by a wisely chosen factor. While the scaling-replacement method proves effective in averaging aggregation, straightforward scaling appears to be naive to success under clipping and restricting defenses. To enhance the stealth of model poisoning attacks, research [43] proposes a projected gradient descent (PGD) attack strategy, which projects the crafted model on a small ball centered around the global model from the previous iteration."}, {"title": "B. Promise breaking", "content": "During the FL life cycle, clients may pledge to provide their training data or computing resources to gain participation opportunities or claim their contributions to receive incentives. However, malicious clients can exploit the FL system to gain advantages, such as access to the joint model and incentives, without fulfilling their promises [44], [45]. Although these attackers do not deliberately poison the model, promise-breaking behaviors can lead to insufficient computing resources and training data, as well as unfair incentive distribution. This, in turn, may degrade learning performance and diminish participant motivation.\nResearch [45] introduces a Free-riding Attack framework based on the Vanilla FL and shows that the adversary can perform a Free-riding Attack in each learning iteration. In particular, the Free-riding attacker does not perform local training but adds noise to the constructed parameter updates and applies Stochastic Gradient Descent (SGD) to maximize similarity with updates from other benign clients. Research [44] proposes a novel Free-riding Attack in which the model is trained locally using a small dataset masqueraded as a large dataset to obtain more incentive rewards. Study [46] introduces a Free-riding attack strategy, namely Random Weights Attack. Under this attack, the adversary creates a gradient update matrix with dimensions matching the received global model by randomly sampling each parameter from a uniform distribution within a pre-designed range.\nStudies [47]\u2013[49] investigate dishonest reporting during intensive distribution. To cheat the reward system, malicious participants may falsely report their resource consumption or claim to have trained the joint model with high-quality data while actually using low-quality data, such as spam."}, {"title": "C. Sybil Attacks", "content": "The Sybil Attack refers to a single attacker (or a small amount attackers) joining the system with multiple colluding identities to enhance the stealth and effectiveness of an adversarial attack. Given that some traditional FL permits selected participants to freely join and exit during training tasks, it becomes inherently susceptible to Sybil Attacks.\nThe research [50] first introduces Sybil Attacks within the FL context. It demonstrates Sybil Attacks can significantly enhance the impact of other attacks (such as the Label Flipping Attack) by misleading the global model into classifying handwritten digits \u201c1\u201d as \u201c7\u201d through only two sybils. Study [51] builds upon the findings of [50], delving deeper into the vulnerability of FL to Sybil Attacks, as well as exploring associated attack strategies and objectives. This work [51] categorizes and names the Denial-of-Service (DoS) attacks instigated by Sybil Attacks as Training Inflation and demonstrates Sybil Attacks can be performed from various FL dimensions. From the data distribution perspective, Sybils can use identical datasets, different datasets, or synthetic data to train local models and generate crafted updates [52]. On the other hand, to dominate the benign clients and amplify their influence on the system, Sybils might coordinate amongst themselves before producing the subsequent series of model updates, achieving different levels of coordination."}, {"title": "D. Communication Layer Attacks", "content": "FL continuously updates the joint model, which relies on communications between the server and multiple participants. However, the large volume of data exchange and frequent communication drives a significant communication bottleneck and brings new attack interfaces for adversaries. The man-in-the-middle (MiTM) attacks [53], as one of the most representative network layer attacks, has been introduced in FL by recent studies [2], [54]. As FL leverages the single server to aggregate model updates and organize the learning process, performing MiTM between the server and participants can effectively block the server and lead to a single point of failure. Research [55] explores attacking the FL communication bottleneck by decreasing the bandwidth, delaying the response, and increasing the struggle possibility. Through performing attacks on the communication of FL, [55] shows the model convergence and performance can be degraded significantly."}, {"title": "IV. PRIVACY-HARMED ATTACKS", "content": "In evasion attacks, the adversary deliberately adds slight malicious perturbations to input samples during the inference phase, causing the classifier to misclassify the sample with a high probability. One of the most representative works of the evasion attack has been introduced by [56], which is a so-called adversarial example. By adding unnoticeable pixel perturbation on a panda image, [56] successfully changes the classification of GoogLeNet [57] from panda to gibbon with 99.3% confidence.\nBased on the information owned by the adversary, the evasion attacks could be further divided into white-box and black-box attacks. In the white-box setting, the adversary has full knowledge of the learning algorithm and model parameters and can craft adversarial examples based on the information. For instance, studies [58], [59] generate adversarial examples by maximizing the loss function, subject to a norm constraint, using constrained optimization techniques like projected gradient ascent. On the other hand, the black-box setting refers to the attackers having no knowledge about the algorithm and parameter information of the FL system and generating adversarial samples through the interaction process or query access with the system. Considering the black-box setting, [60] proposes a zeroth order optimization (ZOO) based attack to estimate the target model updates and generate adversarial examples consequently. Similarly, Boundary Attack [61] has been designed in the same scenario. When performing Boundary Attack [61], the adversary first identifies the boundary separating adversarial and non-adversarial samples, then subtly introduces perturbations along this boundary to produce adversarial examples.\nA recent work [62] introduces an Evasion Attack in the Vertical FL (VFL) scenarios and proposes the Adversarial Dominating Inputs Attack. Unlike the aforementioned adversarial samples that domain the whole feature space, the Adversarial Dominating Inputs Attack [62] can dominate the inputs of other clients by controlling a fraction of the feature inputs and lead to the misclassification of specific inputs. In the meanwhile, the Adversarial Dominating Inputs Attack can also result in reduced and unfair incentive rewards for impacted participants due to their decreasing contributions."}, {"title": "B. Inference Attacks", "content": "During the training process of FL, the model will inadvertently learn the latent information of private data. Therefore, an honest but curious server or any other user can launch an inference attack to recover the original training data without any prior knowledge. Inference Attacks include model extraction attacks, attribute inference attacks, and membership inference attacks.\n1) Membership Inference Attack (MIA): Similar to MIAs in traditional machine learning scenarios, malicious clients or users can execute MIAs in FL applications to infer whether a data sample belongs to its training dataset. Study [15] focuses on black-box within Machine Learning as a Service (MLaaS) settings. By accessing multiple generated shadow models, the study achieves high accuracy in inferring membership. ML-Leaks [70] reduces the cost of launching attacks and demonstrates the effectiveness of MIA using only a single (even without) shadow model. Study [16] considers the white-box settings and proposes the gradient ascent MIA strategy to magnify the presence of data points in others' training sets. It demonstrates that all algorithms trained using Stochastic Gradient Descent (SGD) are susceptible to privacy vulnerabilities from MIAs. The study [71] further extends MIAs to NLP learning tasks and effectively infers whether a specific text segment belongs to the training dataset.\n2) Property Inference Attacks (PIA): Property inference attacks aim to infer private features or characteristics (e.g., value, distribution, etc.) of the training data without direct exposure to the data. Although some surveys [24], [72] categorize threats aimed at reconstructing training data as PIAs, we consider PIAs to primarily focus on inferring properties of the training data, rather than the data itself. Therefore, in this survey, we differentiate threats aimed at data reconstruction from PIAs and classify them as Inversion Attacks.\nAttackers in the study [73] use auxiliary data and trained classifiers to determine if observed model updates are generated from a dataset that includes the target property. Considering fully connected neural networks (FCNNs) are invariant under the permutation of nodes when represented using matrices, study [74] introduces PIAs in white-box settings and shows good inference performance in shallow-FCNNs. The study [75] further extends PIAs to logistic regression (LR), long short-term memory networks (LSTM), and graph convolutional networks (GCN) across multiple domains. It demonstrates that even in black-box settings, PIAs can still achieve accuracies exceeding 60%.\n3) Representative Inference Attacks (RIA): Representative Inference Attacks indicate the malicious generate dummy outcome that is representative of the victim's training data [76], [77]. The study [76] first introduces record-level RIAs in collaborative learning scenarios and demonstrates that GAN-based attacks can effectively compromise against Distributed Selective SGD (DSSGD [78]). However, subsequent research [77] reveals its performance degradation in FL settings due to the averaging of updates and introduces the mGAN-AI framework. This framework enhances GANs with a multitask discriminator capable of simultaneously assessing the category, reality, and client identity of input samples. Compared to [76], mGAN-AI enables the generator to recover representative private data for specific users."}, {"title": "C. Inversion Attacks", "content": "Although FL allows clients to only upload gradients during model training, it has been found that some properties of the training data are implicitly carried by these gradients, which could be revealed through inverse attacks. DLG [64] first introduces gradient inversion in FL computer vision areas. Specifically, the DLG generates random dummy input samples x' and labels y' and corresponding dummy gradients G'. x' and y' are Iteratively optimized through L-BFGS to minimize the difference between dummy and benign gradient. Formally, the objective function of DLG shown as follows:\nConsidering the high-dimensional direction of gradients carried important information, study [65] introduces cosine similarity in the loss function to find images that lead to a similar change in model prediction as the ground truth instead of finding images with a gradient that best fits the observed gradient. The objective function is shown as follows, where TV represents the Total Variance regularization.\nAlthough DLG [64], IG [65] show effectiveness on single image and small batch size training scenarios, their inversion performance degrades when facing large batch size (> 8). To further improve the attack effectiveness, GGL [68] leverages a generative model trained on public datasets as the learned natural image prior to ensure the reconstructed image quality. Additionally, GradInversion (GI) [67] integrates Fidelity and Group Consistency regularization into the objective function and effectively increases the inversion batch size to 48. These are designed to ensure that the generated images closely resemble real images and to penalize any dummy images that deviate from a \"consensus\" image, respectively. GradViT [69] extends the GI approach to Vision Transformer (ViT) models, demonstrating that ViTs are significantly more vulnerable to inversion attacks than the previously studied CNNs due to the attention mechanism."}, {"title": "D. Eavesdropping", "content": "Network eavesdropping involves an attacker intercepting and capturing data packets transmitted over a network using their own devices, with the intent to analyze the contents of these packets to get private information [80]. Since model training under the FL scheme requires regular communication between the server and clients, FL is naturally vulnerable to eavesdropping attacks. This vulnerability is particularly pronounced when the communication between participants and the server is in plaintext or employs a weak encryption method [23], [81]. Since the messages transmitted are usually models or gradients, the attacker may further leverage inference or inversion attacks to decipher the eavesdropped messages to understand the private information [82]\u2013[84]. Study [85] considers the eavesdropping attacks in unmanned aerial vehicles (UAVs) scenarios, demonstrating that eavesdroppers can deduce the raw data from the shared parameters and compromise participants' privacy."}, {"title": "V. DEFENCE AGAINST UTILITY-HARMED ATTACKS", "content": "As the diversity and complexity of adversarial attacks against FL increase, new defenses are being developed to counteract their malicious impacts. These defense frameworks generally follow the setting that the server is honest and can play the defender role. As some defense methods are effective against multiple types of attack categories, we survey these FL defense frameworks from different technologies instead of grouping them according to the attack category defended."}, {"title": "A. Data Sanitization", "content": "Data sanitization [86]\u2013[88] has been introduced to remove malicious and suspicious data before the training process and consequently mitigate crafted-data-related threats, including data poisoning and evasion attacks. The research [86] introduces a novel training pipeline that incorporates a data sanitization phase. During each iteration, this phase creates multiple models, termed micro-models, based on subsets of the training data. These micro-models are used to produce provisional labels for each training input and consequently combined in a voting scheme to determine which parts of the training data may be potentially attacked. To filter out noise-labeled data, FedDiv [89] maintains a global filter. This filter is trained, exchanged, and aggregated similarly to the global model and supports local data filtration. From the data shuffling perspective, study [88] proposes a byzantine-resilient matrix-vector (MV) multiplication and further proposes an algorithm based on the data encoding process and error correction over real numbers to combat various adversarial attacks.\nAlthough data sanitization can filter out some potential attacked data and keep training data clean, it relies on directly accessing the local data of clients, which may raise privacy issues. A recent work [90] indicates that data sanitization can only work in cross-silo FL with strong authentication and be performed locally by the client itself. However, this framework [90] assumes the trustworthiness of the clients, which may not be applicable to the adversarial FL environment."}, {"title": "B. Anomaly Detection", "content": "To perform threats, attackers unconsciously exhibit different patterns from benign clients, including abnormal behaviors or data patterns. To this end, anomaly detection has been proposed to identify suspicious through statistical or behavior analysis technology [91]\u2013[93]. We follow the study [24] and divide the anomaly detection of FL in client and data from two perspectives.\nA) Client Anomaly Detection\nUnder FL, Client Anomaly Detection could be performed based on the information from the communication (IP address, User ID, Time Stamp [94]) or application layer, namely client model updates. By employing a pre-trained auto-encoder model, study [91] introduces Auto-encoderBased Anomaly Detection to detect abnormal model weight updates from the clients and consequently"}, {"title": "V. DEFENSE AGAINST PRIVACY-HARMED ATTACKS", "content": "Unlike utility-oriented defense frameworks that generally assume a trusted central server, defenses against privacy-oriented attacks can be implemented by either the server or the clients, depending on different settings. Today, various defense frameworks have been proposed to mitigate privacy concerns, ranging from cryptography-based methods to fully decentralized architectures. In this section, we survey these defense solutions, introduce their techniques, and compare their advantages and limitations."}, {"title": "A. Differential Privacy", "content": "Differential Privacy (DP) has been proposed as a privacy protection data-sharing method. By introducing artificial noise", "146": ".", "CDP)": "When the server is trusted, noise can be added centrally"}]}]}