{"title": "Text-to-Image GAN with Pretrained Representations", "authors": ["Xiaozhou You", "Jian Zhang"], "abstract": "Generating desired images conditioned on given text descriptions has received lots of attention. Recently, diffusion models and autoregressive models have demonstrated their outstanding expressivity and gradually replaced GAN as the favored architectures for text-to-image synthesis. However, they still face some obstacles: slow inference speed and expensive training costs. To achieve more powerful and faster text-to-image synthesis under complex scenes, we propose TIGER, a text-to-image GAN with pretrained representations. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. (i) The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision models to enhance model performance. Unlike previous works, we explore stacking multiple pretrained models in our discriminator to collect multiple different representations. (ii) The high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). And the HFBlock contains several deep fusion modules and a global fusion module, which play different roles to benefit our model. Extensive experiments demonstrate the outstanding performance of our proposed TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets, which obtain a new FID 5.48 (COCO) and 9.38 (CUB). On the zero-shot text-to-image synthesis task, we achieve comparable performance with fewer model parameters, smaller training data size and faster inference speed. Additionally, more experiments and analyses are conducted in the Supplementary Material.", "sections": [{"title": "1 Introduction", "content": "Recently, Artificial Intelligence Generated Content (AIGC) has become a hot topic in the academic community. Among them, text-to-image synthesis is one of the most attractive areas. Among them, text-to-image synthesis is one of the most attractive areas due to its potential value in many fields, such as computer-aided design, virtual scene generation, and photo editing. It aims to generate desired images conditioned on given text descriptions. To reach this, many methods have been proposed, including generative adversarial networks [Xu et al., 2018], diffusion models [Gu et al., 2022], variational auto-encoders [Kingma and Welling, 2014], autoregressive models [Ramesh et al., 2021], etc. In the early years, GAN was the main tool to realize text-to-image synthesis.\nDue to GAN's unstable training and weak performance under complex scenes, some researchers try to adopt diffusion models (DF) and autoregressive models (AR) to accomplish the task. Based on large-scale datasets and massive parameters, they have demonstrated outstanding expressivity and gradually replaced GAN as the favored architecture for text-to-image synthesis. However, they still suffer from two limitations. First, large-scale models have slow inference speed. The progressive denoising of diffusion models and the token-by-token generation of autoregressive models make their speed far behind GAN [Tao et al., 2023]. For example, to generate an image, GLIDE (DM) costs 15s, Parti (AR) costs 5s, and Lafite (GAN) only costs 0.02s. Second, the training of large-scale models requires massive training data and parameters, which brings huge training burden and computing requirements. For example, DALL-E [Ramesh et al., 2021] has 12B parameters and is trained on 250M image-text pairs. These characteristics seriously hinder their further application.\nTo address them, we revisit GAN. Compared with diffusion model and autoregressive model, GAN has faster inference speed but weaker performance under complex scenes. Pointed out by some works [Kumari et al., 2022; Sauer et al., 2021; Tao et al., 2023], pretrained vision models have better complex scene understanding and domain generalization capabilities, such as DINO [Caron et al., 2021], CLIP [Radford et al., 2021], Swin [Liu et al., 2021]. Motivated by this, we decided to introduce representations from pretrained vision models into GANs.\nIn this work, we propose TIGER, a text-to-image GAN with pretrained representations, which aimed to build a more powerful and faster text-to-image model. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. The vision-empowered discriminator absorbs the complex scene understanding ability and the domain generalization ability from pretrained vision model to enhance the performance under complex scenes. Unlike previous works [Sauer et al., 2021; Tao et al., 2023], we explore to collect multiple representations from different models, which means we stack multiple pretrained models in our discriminator. And we freeze the parameters of pretrained vision models in order to maintain their original characteristics. Besides, we propose a high-capacity generator, aiming to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks. And the high-capacity fusion block contains several deep fusion modules and a global fusion module. The goal of the deep fusion module is to achieve efficient text-image fusion, and the goal of the global fusion module is to enhance model expressivity both on spatial and channel dimensions.\nAs shown in Fig. 2, extensive experiments demonstrate the outstanding performance of TIGER both on standard and zero-shot text-to-image synthesis tasks. On the standard text-to-image synthesis task, TIGER achieves state-of-the-art performance on two challenging datasets. On the zero-shot text-to-image synthesis task, we achieve a comparable performance with fewer model parameters, smaller training data and 120\u00d7 faster inference speed than LDM [Rombach et al., 2022] and Parti-350M [Yu et al., 2022]."}, {"title": "2 Related Works", "content": "Text-to-Image Synthesis.\nReed et al. first proposed employing conditional generative adversarial networks to generate images under text conditions in 2016 [Reed et al., 2016a], which opened the door to text-to-image synthesis. To further improve the quality of generated images, Zhang et al. proposed stacking multiple generator-discriminator pairs to gradually generate high-quality images from coarse to fine under text conditions [Zhang et al., 2017]. During training, multiple generator-discriminator pairs are required to coordinate to generate high-quality images. After that, Xu et al. proposed AttnGAN [Xu et al., 2018] to achieve word-level fine-grained generation by introducing a word-level attention mechanism.\nTo overcome the limitations of stacked architecture, Ming et al. proposed DF-GAN [Tao et al., 2020], which aims to employ single generator to complete text-to-image synthesis. In addition, he also proposed utilizing MA-GP to generate text-matching images. The following SSA-GAN [Liao et al., 2022] and RAT-GAN [Ye et al., 2022] also adopted single-stage architecture. SSA-GAN proposes semantic-spatial condition batch normalization, which employs mask map to overcome the problem of insufficient spatial fusion in DF-GAN. RAT-GAN proposes Recurrent Affine Transformation to model long-range dependencies between fusion blocks.\nLarge-Scale Text-to-Image Models.\nRecently, based on large-scale pre-training, diffusion models and autoregressive models have started to show their influence on zero-shot text-to-image synthesis. DALL-E [Ramesh et al., 2021] is the first large-scale zero-shot text-to-image model, which trains a 12B Transformer based on autoregressive model. Similar to DALL-E, CogView [Ding et al., 2021] trains a 4B parameter autoregressive model and finetunes it for various downstream tasks. Then, more large-scale text-to-image generative models are proposed, further demonstrating their expressiveness, such as CogView2 [Ding et al., 2022], Muse [Chang et al., 2023], GLIDE [Nichol et al., 2021], and eDiff [Balaji et al., 2022]. The Parti [Yu et al., 2022] attempts to treat text-to-image synthesis as a translation task, which proposes a sequence-to-sequence autoregressive model. Latent Diffusion Models (LDM) [Rombach et al., 2022] introduce the latent space to diffusion models to enable the training on limited computational resources."}, {"title": "3 Methods", "content": "In this paper, we propose TIGER, a text-to-image GAN with pretrained representations, which aimed to build a more powerful and faster text-to-image model. We will introduce the novel components of TIGER in this section, including: high-capacity fusion block and vision-empowered discriminator."}, {"title": "3.1 Model Overview", "content": "The overall architecture of TIGER is shown in Fig. 1. Unlike previous works [Sauer et al., 2021; Tao et al., 2020], we explore introducing multiple pretrained vision models in our network, and the goal is to utilize the representations from pretrained models to further improve model performance. Specifically, we propose an innovative vision-empowered discriminator and an innovative high-capacity generator. The vision-empowered discriminator consists of several sub discriminators, each of which processes the extracted representation from different pretrained vision model. The overall adversarial loss of the vision-empowered discriminator is the sum of sub discriminators' loss. The architecture of the sub discriminator is shown in Fig. 4 (a).\nThe architecture of the high-capacity generator is shown in Fig. 3 (a). In order to increase the model capacity and achieve effective cross-mode text-image fusion, we propose a novel high-capacity fusion block (HFBlock) in our proposed high-capacity generator. First, we generate a 100-dimensional random noise sampled from the Gaussian distribution. And the random noise is used as the input of first layer. Then, the text descriptions are encoded to get sentence vector by a pretrained text encoder. We concatenate random noise with sen-"}, {"title": "3.2 High-Capacity Fusion Block", "content": "As shown in Fig. 3 (b), the High-Capacity Fusion Block consists of several deep fusion modules and a global fusion module. The goal of the deep fusion module is to achieve efficient cross-modal text-image fusion, and the goal of the global fusion module is to enhance the model expressivity both on spatial and channel dimensions. We describe them in detail next.\nDeep fusion Module.\nTo achieve effective cross-modal text-image fusion, we introduce several deep fusion modules into our network [Tao et al., 2020]. Compared with the previous attention-based method [Xu et al., 2018], the deep fusion module has a lower computational cost, which can benefit the training of model. Compared with the previous concatenation method [Reed et al., 2016b], the deep fusion module can more effectively complete the text-image fusion [Liao et al., 2022]. Specifically, the deep fusion module consists of two affine modules and a 3 \u00d7 3 convolutional neural network. The architecture of affine module is shown in Fig. 3 (c). After the affine module, we employ a ReLU function to add nonlinearity to fusion process. And we stack two deep fusion modules in our HFBlock (n = 2 by default).\nGlobal fusion Module.\nSince the affine module only fuses the text condition separately for each channel, it lacks information fusion across channel dimensions. To enhance the model capacity both on spatial and channel dimensions, we propose the global fusion module [Guo et al., 2022]. As shown in Fig. 3 (b), the global fusion module consists of a 3 \u00d7 3 depth-wise convolutional network, a 5 \u00d7 5 dilated depth-wise convolutional network with dilation being 3, and a 1 \u00d7 1 convolutional network. The 3 \u00d7 3 depth-wise convolutional network focuses on modeling and enhancing information fusion on the spatial dimensions. The 5 \u00d7 5 dilated depth-wise convolutional network is aimed to extract long-range dependencies. And the 1 \u00d7 1 convolutional network is employed to facilitate cross-modal fusion and explicitly model the inter-dependencies between channels."}, {"title": "3.3 Vision-Empowered Discriminator", "content": "As pointed out by previous works [Sauer et al., 2021; Kumari et al., 2022], GAN has weak performance under complex scenes and faces some obstacles, such as the lack of diversity and unstable training. To build a powerful GAN, we propose a vision-empowered discriminator, in which we explore introducing multiple representations from different pretrained vision models. These models are pretrained on huge and complex datasets, which include many different scenarios [Radford et al., 2021; Caron et al., 2021]. We argue that these pretrained model has outstanding complex scene understanding ability and domain generalization ability [Tao et al., 2023], which helps to enhance the performance of GAN under some complex datasets.\nUnlike previous works, we integrate multiple pretrained representations in the discriminator instead of one. We believe that integrating multiple pretrained representations from different vision networks in the discriminator can be beneficial to improve performance. Specifically, the vision-empowered discriminator consists of multiple sub discriminators, each of which mainly processed the representations from different pretrained vision models. The framework of the sub discriminator is shown in Fig. 4 (a). For generated images or real images, we exploit the selected pretrained network to extract representation. Then, the extracted representation are processed by an adapter to obtain vision features. For single-level representation (such as VGG and Swin), we employ two sequential convolutional networks as adapter. For multi-level representation (such as CLIP and DINO), we try two different adapters to improve performance (Fig. 4 (b) and (c)). The text description is encoded by a pretrained text encoder to obtain text features. Then, the replicated text features and vision features are concatenated together. Finally, the image assessor consisting of several convolutional networks evaluates the quality of the image. And its result is the adversarial loss of the sub discriminator. The total discriminator adversarial loss is the sum of all sub discriminator adversarial losses."}, {"title": "3.4 Loss Function", "content": "Semantic Contrastive Loss.\nTo promote the semantic consistency between the generated image and text description, we introduce semantic contrastive loss in our model. Specifically, we compute the cosine similarity between text features and image features extracted by the CLIP model [Radford et al., 2015]. The mathematical form is as follows:\n$L_{CLIP} = Cos(I, T)$,\nwhere Cos is the cosine function, I and T are the encoded image features and text features extracted by the CLIP model, respectively.\nOverall Loss.\nFor the ith sub discriminator in our vision-empowered discriminator, we use hinge loss with MA-GP [Tao et al., 2020] as the sub discriminator loss $L_{adv}^{Di}$. The specific mathematical form is as follows:\n$L_{adv}^{Di} = \\mathbb{E}_{x \\sim P_{data}}[max(0, 1 - D_{i}(x, s))] \\\\\n+ \\frac{1}{2} \\mathbb{E}_{x \\sim P_{G}} [max(0, 1 + D_{i}(\\hat{x}, s))] \\\\\n+ \\frac{1}{2} \\mathbb{E}_{x \\sim P_{data}}[max(0, 1 + D_{i}(x, \\hat{s}))]$,\nwhere x is real image, $\\hat{x}$ is generated image, s is matched sentence, $\\hat{s}$ is unmatched sentence, $D_{i}$ is the ith sub discriminator, respectively. The overall adversarial loss $L_{D}$ of vision-empowered discriminator is the sum of the sub discriminator loss. The specific mathematical form is as follows:\n$L_{D} = \\sum_{i=1}^{M} \\lambda_{i} L_{adv}^{Di}$,\nwhere $\\lambda_{i}$ is is the corresponding hyperparameter, M is the total number of sub discriminators, respectively. The training loss of generator $L_{G}$ is as follows:\n$L_{G} = \\lambda_{CLIP} \\cdot L_{CLIP} + L_{adv}$,\n$L_{adv} = \\sum_{i=1}^{M} \\lambda_{i} \\cdot \\mathbb{E}_{x \\sim P_{data}} [D_{i}(\\hat{x}, s)]$.\nwhere $\\lambda_{CLIP}$ is a hyper-parameter, respectively."}, {"title": "4 Experiments", "content": "In this section, we introduce the datasets, training details, and evaluation details. Then, we conduct our experiments to verify the effectiveness and superiority of our proposed TIGER.\nDatasets. We evaluate the proposed model on three challenging datasets, i.e., CUB bird [Wah et al., 2011], COCO [Lin et al., 2014] and CC12M [Changpinyo et al., 2021]. The COCO datasets contain 80k images for training and 40k images for testing. Each image has five language descriptions. Besides, the COCO dataset is always employed in recent works to evaluate the performance of complex image synthesis. The CUB bird datasets (200 categories) contain 8855 training images and 2933 testing images. Each image has 10 text descriptions. The CC12M datasets contain about 12 million text-image pairs, and it's always adopted for pre-training and to evaluate the zero-shot performance of the text-to-image model.\nTraining Details. Our model is implemented in PyTorch. The Adam optimizer [Kingma and Ba, 2015] with $\\beta_{1}$ = 0.0 and $\\beta_{2}$ = 0.9 is used in the training. The learning rate is set to 1 \u00d7 10-4 for generator and 4 \u00d7 10-4 for discriminator according to TTUR [Heusel et al., 2017]. The hyper-parameters $\\lambda_{CLIP}$ is set to 4. And the proposed vision-empowered consists of 2 sub discriminators (CLIP, DINO). The corresponding hyper-parameters $ \\lambda_{1},  \\lambda_{2}$ are set to 1 and 0.001, respectively. And we use the CLIP text encoder as our text encoder.\nEvaluation Details. The Fr\u00e9chet Inception Distance (FID) [Heusel et al., 2017] and top-1 R-precision [Xu et al., 2018] are used to evaluate the performance of our work. For FID, it computes the Fr\u00e9chet distance between the distribution of the generated images and real-world images in the feature space of a pre-trained Inception v3 network [Szegedy et al., 2016]. Lower FID means model achieves better performance. For R-precision, we use CLIP [Radford et al., 2021] to calculate the cosine similarity between original image and given description. Following previous works [Liao et al., 2022; Tao et al., 2022], we do not use IS [Salimans et al., 2016] on COCO and CUB datasets because it can't evaluate the image quality well. Moreover, we evaluate the number of parameters (Param), inference speed (Speed) and training data size (Data) to compare with current methods."}, {"title": "4.1 Quantitative Evaluation", "content": "As shown in Tab. 1, we conduct the quantitative comparison between our proposed TIGER and state-of-the-art methods on standard text-to-image synthesis task, including: AttnGAN [Xu et al., 2018], DM-GAN [Zhu et al., 2019], XMC-GAN [Zhang et al., 2021], DAE-GAN [Ruan et al., 2021], RAT-GAN [Ye et al., 2022], DF-GAN [Tao et al., 2022], Lafite [Zhou et al., 2022], VQ-diffusion [Gu et al., 2022] and GALIP [Tao et al., 2023]. On COCO datasets, TIGER reaches a new state-of-the-art FID 5.48. Compared with the single-stage architecture baseline DF-GAN, TIGER decreases the FID metric from 19.32 to 5.48 and improves the R-precision metric from 0.278 to 0.348, which shows that our GALIP obtains a huge performance boost. And Compared with the diffusion model VQ-diffusion, TIGER decreases the FID metric from 13.86 to 5.48. Especially, compared with the recent Lafite, TIGER decreases the FID metric from 8.21 to 5.48 and improves the R-precision metric from 0.332 to 0.348, which demonstrates that integrating pretrained representations is effective. On CUB datasets, TIGER also achieves leading results. Compared with the recent GALIP, TIGER decreases the FID metric from 10.08 to 9.38 and improves the R-precision metric from 0.325 to 0.341.\nAs shown in Tab. 2, we conduct the quantitative comparison between our proposed TIGER and influential methods on zero-shot text-to-image synthesis task, including DALL-E [Ramesh et al., 2021; Ramesh et al., 2022], CogView2 [Ding et al., 2022], Muse [Chang et al., 2023], GLIDE [Nichol et al., 2021], eDiff [Balaji et al., 2022], GiGaGAN [Kang et al., 2023], and StyleGAN-T [Sauer et al., 2023] etc. Compared with autoregressive models (AR) and diffusion models (DF), our TIGER achieves competitive performance with faster speed, smaller training data size and fewer model parameters. Especially, compared with LDM (DF), TIGER obtains slight performance improvement (FID: 11.96 vs. 12.63) with 3% training data size, ~23% model parameters and ~120\u00d7 inference speed; compared with Parti-350M (AR), TIGER obtains decrease FID from 14.10 to 11.96 with less training data size and ~130\u00d7 inference speed. Besides, compared with other GAN methods, our TIGER achieves outstanding results."}, {"title": "4.2 Qualitative Evaluation", "content": "As shown in Fig. 5, we conduct the qualitative comparison between our proposed TIGER, the single-stage method DF-GAN [Tao et al., 2022] and the stacked method AttnGAN [Xu et al., 2018]. Compare with other works, our generated images are more photo-realistic and text-matching. For example, in the 1st column, given the text \u201cA blue and red train leaving the station on a sunny day\", our TIGER generates the desired images which match the attributes \"blue and red\", \"train\", \"station\", and \"sunny\". While the image generated by AttnGAN doesn't meet the requirements seriously, and the image generated by DF-GAN only saw the red and blue train and doesn't meet other attributes. In the 6th column, given the text \"This orange and black small bird has a straight pointed beak\", the image generated by TIGER has all the mentioned attributes. However, the image generated by DF-GAN does not reflect \"straight pointed beak\" and the image generated by AttnGAN is a little blurry. In the 8th column, TIGER produces the desired image, but other methods don't produce clear enough images to meet all attributes."}, {"title": "4.3 Ablation Study", "content": "As shown in Tab. 3, to verify the superiority of each component in our proposed TIGER, we deploy our experiments on the COCO test set [Lin et al., 2014]. These components include pretrained vision models, adapter, global fusion module and CLIP and DINO Layer Selection. The VE-D stands for our vision-empowered discriminator and the GFM stands for global fusion module.\nBaseline. The baseline is our proposed a single-stage text-to-image GAN [Tao et al., 2020], which has a CNN-based generator/discriminator and is trained from scratch.\nPretrained Vision Models. As shown in Fig. 6, we conduct the ablation experiments of different pretrained vision models, include: VGG [Simonyan and Zisserman, 2015], Swin-T (Moby, Segment, Dectet) [Liu et al., 2021], CLIP [Radford et al., 2021], DINO [Caron et al., 2021], U-Net [Schonfeld et al., 2020], etc. We experimentally found that multi-level pretrained representations (DINO, CLIP) can significantly improve model performance. In the 1st round, we add the first pretrained representation to the discriminator, which means our vision-empowered discriminator only has one sub discriminator. The CLIP model performs best in the 1st round. In the 2nd round, we keep the selection from the previous round unchanged and add the second pretrained vision model to our discriminator. Next, the DINO model is selected for our model. In the 3rd round, our model trains slowly and fails to converge, perhaps due to our limited computing resources. So, we only add two pretrained models (CLIP, DINO).\nAdapter. To maximize our model performance, we experimented with two different adapter architectures (shown in Fig. 4 (b) and (c)). The adapter A and B are used to process multi-level representations (DINO, CLIP). Experiments show that different adapter architectures have an important impact on model performance, and the best adapter pair decreases FID from 7.94 to 5,48 and improves R-precision from 0.308 to 0.348. At last, we equipped first sub discriminator (CLIP) with adapter A and equipped second sub discriminator (DINO) with adapter B.\nGlobal Fusion Module. We ablate the operations in our proposed global fusion module. First, we try to delete the global fusion module in our TIGER but get worse performance, which indicates the global fusion module can enhance our model capacity. Besides, we conduct experiments to verify the effectiveness of 3\u00d73 depth-wise convolutional network, 1\u00d71 convolutional network and 5\u00d75 dilated convolutional network. The ablation experiments confirm they play a positive role in our global fusion module.\nCLIP and DINO Layer Selection. Inspired by GALIP [Tao et al., 2023], different layers in CLIP/DINO have performance impact. Generally, first few layers contain some low-level general visual features, while last few layers contain some low-level abstract concepts [Kumari et al., 2022]. To further improve performance, we experimented with various layer combination strategies. In the end, we chose the 2nd, 5th, 9th layer of CLIP and the 1st, 5th, 9th layer of DINO."}, {"title": "5 Conclusion", "content": "In this paper, we propose TIGER, a text-to-image GAN with pretrained representations, which aimed to build a more powerful and faster text-to-image model. To be specific, we propose a vision-empowered discriminator and a high-capacity generator. The vision-empowered discriminator absorbs the complex scene understanding ability and domain generalization ability from pretrained vision model to enhance the performance. And the high-capacity generator aims to achieve effective text-image fusion while increasing the model capacity. The high-capacity generator consists of multiple novel high-capacity fusion blocks (HFBlock). Extensive experiments demonstrate the outstanding performance of our proposed TIGER. In the future, we hope to try to pretrain TIGER on a larger dataset to further enhance the performance of zero-shot text-to-image synthesis."}]}