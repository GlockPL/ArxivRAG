{"title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs", "authors": ["Joao Pedro Fernandes Torres", "Catherine Mulligan", "Joaquim Jorge", "Catarina Moreira"], "abstract": "The growing volume of academic publications poses significant challenges for researchers conducting timely and accurate Systematic Literature Reviews, particularly in fast-evolving fields like artificial intelligence. This growth of academic literature also makes it increasingly difficult for lay people to access scientific knowledge effectively, meaning academic literature is often misrepresented in the popular press and, more broadly, in society. Traditional SLR methods are labor-intensive and error-prone, and they struggle to keep up with the rapid pace of new research. To address these issues, we developed PROMPTHEUS: an AI-driven pipeline solution that automates the SLR process using Large Language Models. We aimed to enhance efficiency by reducing the manual workload while maintaining the precision and coherence required for comprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR process, including systematic search, data extraction, topic modeling using BERTopic, and summarization with transformer models. Evaluations conducted across five research domains demonstrate that PROMPTHEUS reduces review time, achieves high precision, and provides coherent topic organization, offering a scalable and effective solution for conducting literature reviews in an increasingly crowded research landscape. In addition, such tools may reduce the increasing mistrust in science by making summarization more accessible to laypeople.", "sections": [{"title": "1 Introduction", "content": "The exponential growth of academic publications poses a significant challenge for researchers attempting to stay current with developments across numerous fields. Over 2.5 million papers are published annually"}, {"title": "2 Background and Related Work", "content": "Systematic Literature Reviews are crucial for synthesizing research, identifying knowledge gaps, and shaping future directions across various domains. The traditional SLR process, defined by the PRISMA guidelines, consists of four phases: Planning, Selection, Extraction, and Execution. Despite its rigor, this method faces increasing challenges due to the sheer volume of academic publications. The manual nature of SLRs makes them labor-intensive, prone to error, and difficult to scale, particularly as research outputs grow exponentially."}, {"title": "2.1 Advances in Automating Systematic Literature Reviews", "content": "Recently, machine learning (ML) and natural language processing have emerged as powerful tools that can assist with these challenges by automating various SLR process stages. Large language models such as T5, GPT-3.5, GPT-40, and GPT-01 have been integrated into the SLR workflow, particularly for tasks like literature search, data extraction, and summarization. These AI approaches, including Technology-Assisted Review (TAR) systems, apply NLP and ML techniques to automate the search and screening phases, significantly reducing manual effort by iteratively refining models to prioritize relevant studies. This automation extends to data extraction, where NLP techniques ensure consistency and synthesis, where models such as T5 and GPT generate coherent summaries of research findings, enhancing accuracy and readability.\nMoreno-Garcia et al (2023) propose a novel AI-based framework that leverages ensemble learning techniques to improve the accuracy and efficiency of study selection and data extraction processes. Their model demonstrates how ensemble techniques, when applied to AI-assisted systematic reviews, can enhance the precision and recall of study identification while reducing manual effort. This work contributes to AI-driven SLR tools by highlighting the potential for combining multiple AI models to tackle the inherent variability and challenges in automating complex tasks like data extraction and synthesis.\nBolanos et al (2024) conducted a comprehensive review of AI-integrated SLR tools, highlighting the efficiency improvements AI brings while emphasizing usability-related challenges. The authors highlight"}, {"title": "2.2 Limitations of Current Automated SLR Systems", "content": "Despite the progress in AI-assisted SLRs, several limitations remain. Many systems struggle with handling complex queries, often relying on simple keyword searches that fail to capture the depth and specificity needed for comprehensive reviews. Additionally, the criteria for inclusion and exclusion are frequently poorly defined, leading to the retention of irrelevant or low-quality studies. Existing research highlights the need for more sophisticated search algorithms, improved Boolean logic integration, and better strategies for managing large datasets without sacrificing accuracy and relevance .\nMachine learning tools, such as Abstrackr, have effectively automated SLRs' title and abstract screening stages. In the study by Gates et al (2020), Abstrackr reduced manual effort by up to 35%, significantly saving time while maintaining a high accuracy level in identifying relevant studies. However, the tool missed some important studies during the screening process, underscoring a critical limitation of AI-assisted systems. Despite the efficiency gains, these tools still require human oversight to ensure that essential research is not inadvertently excluded. This highlights the balance between leveraging AI to reduce workload and ensuring that expert validation is in place to preserve the comprehensiveness and quality of the review process.\nTo support these findings, Cierco Jimenez et al (2022) reviewed a range of ML tools for automating the SLR process, noting that many tools lacked user-friendly interfaces for researchers without programming"}, {"title": "2.3 Advanced NLP and LLM Techniques", "content": "Recent studies demonstrate the potential of advanced NLP techniques in addressing some of the limitations of current automated SLR systems. For instance, Kharawala et al (2021) explored using zero-shot classification combined with ML algorithms to automate abstract screening, demonstrating high precision and recall. Similarly, Dennst\u00e4dt et al (2024) tested LLMs for title and abstract screening in the biomedical domain, showing high sensitivity but noting challenges related to resource demands and biases.\nTo enhance Al's role in systematic reviews, Hamel et al (2021) developed a framework for integrating AI into the title and abstract screening phases of SLRs, stressing the importance of robust training sets and transparent reporting. In parallel, Masoumi et al (2024) demonstrated the effectiveness of BioBERT, a variant of BERT fine-tuned for biomedical texts, in automating the abstract review process in medical research, showing that such models can significantly reduce manual workloads while maintaining high accuracy."}, {"title": "2.4 Challenges of Rapid Reviews and Methodological Shortcuts", "content": "AI-based approaches have also been applied to rapid reviews (RRs), often employing methodological shortcuts to expedite the review process. Guo et al (2024) examined the impact of these shortcuts, showing that while they improve efficiency, they can introduce biases and reduce comprehensiveness. Speckemeier et al (2022) echoed these concerns, calling for more rigorous methodologies to balance the need for speed with the maintenance of review quality.\nMoreover, O'Connor et al (2019) examined the cultural and practical challenges of adopting automation tools in systematic reviews, particularly in healthcare. Their study emphasized better collaboration between AI systems and human experts to ensure these tools are effectively integrated into existing workflows."}, {"title": "2.5 Addressing the Gap", "content": "Building on the limitations identified in existing automated systems, our work presents PROMPTHEUS.\nThis fully automated SLR pipeline system enhances the review process by addressing critical limitations such as inadequate inclusion/exclusion criteria and complex query handling. PROMPTHEUS automates the Selection, Extraction, and Synthesis phases, allowing researchers to manage the Planning phase while leveraging advanced NLP techniques like BERTopic for topic modeling and Sentence-BERT for sentence similarity. By incorporating LLMs like GPT and T5 for summarization and post-editing, PROMPTHEUS ensures that the generated summaries are accurate and coherent."}, {"title": "3 PROMPTHEUS: A Framework for AI-Driven SLRs", "content": "Despite significant advancements in AI-assisted SLRs, challenges remain in ensuring automated systems' accuracy, scalability, and relevance. Our proposed framework, PROMPTHEUS, introduces an integrated and fully automated SLR framework that enhances SLRs' Selection, Extraction, and Synthesis phases while maintaining human oversight during the Planning phase. PROMPTHEUS leverages advanced NLP techniques such as BERTopic for topic modeling and Sentence-BERT for sentence similarity to improve the precision and relevance of selected studies. Our system also integrates LLMs like GPT and T5 for summarization and post-editing, ensuring the generated summaries are accurate and coherent.\nBy introducing early-stage inclusion and exclusion criteria, PROMPTHEUS improves the rigor of study selection and reduces the likelihood of including irrelevant papers. This approach addresses the shortcomings identified by O'Connor et al (2019) and de la Torre-L\u00f3pez et al (2023), who emphasized the importance of integrating AI tools that improve efficiency without compromising the accuracy and comprehensiveness of systematic reviews, and also the challenges highlighted by Affengruber et al"}, {"title": "3.1 General Overview", "content": "Our automated SLR pipeline architecture is organized into three interconnected phases: (1) Systematic Search and Screening, which identifies and selects relevant academic papers; (2) Data Extraction and Topic Modeling, which categorizes and organizes the selected studies; and (3) Synthesis and Summarization, which generates coherent summaries and integrates the findings into a structured review document. Each module employs specialized NLP techniques and LLMs, producing an efficient and scalable SLR process."}, {"title": "3.2 Systematic Search and Screening Module", "content": "The Systematic Search and Screening Module is the foundation of the automated Systematic Literature Review process, which automates retrieving and filtering academic papers based on a user-defined research question or topic. This module addresses the limitations of traditional literature search methods, which often require extensive manual effort, by using LLMs and advanced NLP techniques to enhance the"}, {"title": "Research Topic Expansion.", "content": "The module begins with the user providing a research question or topic as input. The system leverages an LLM (GPT-3.5, GPT-4 or GPT-40) to expand the initial input into a more detailed and semantically rich set of keywords and phrases to ensure the search captures a comprehensive range of relevant studies. This expansion is guided by a carefully crafted prompt that instructs the model to retain the core focus of the research topic while adding appropriate keywords and terms to cover variations and related concepts. Part of the prompt used for this task is:\nSystem: \"You are a knowledgeable AI specializing in generating expanded titles for research topics. Your expanded titles should be concise and focus on capturing the core semantic meaning of a topic, suitable for creating informative embeddings for tasks like similarity comparisons.\u201d\nUser: \"Task: Generate a slightly expanded title for the following research topic, keeping the core focus while potentially adding 1-2 highly relevant terms for improved semantic representation.\nTopic: title\nGuidelines:\n* Include essential keywords directly related to the topic.\n* If necessary, add 1-2 closely related terms to capture topic variations.\n* Avoid introducing new concepts or significantly altering the original title's meaning.\n* Keep the expanded title concise and focused on the core meaning.\nOutput format:\n* Provide the expanded title only. Do not include any additional explanations or commentary.\u201d\nFor instance, given the input topic \"AI-based literature review\", the LLM might generate an expanded version such as \"AI-based literature review, automated systematic reviews, natural language processing for academic research synthesis\". This expanded set of keywords ensures that the subsequent search covers a broader scope, capturing essential variations and closely related studies that might be overlooked."}, {"title": "Automated Query Generation.", "content": "After expanding the research topic, the system constructs a structured search query tailored to the arXiv repository. This step is guided by another prompt instructing the LLM to craft a precise and targeted search query incorporating all relevant keywords and phrases. The model is asked to include fields such"}, {"title": "Relevance Filtering Using Sentence Similarity.", "content": "The module employs a similarity-based mechanism using Sentence-BERT embeddings to filter the most pertinent papers from the initial search results. It computes vector embeddings for both the expanded research topic and the cleaned abstracts of the retrieved papers. The cosine similarity between these embeddings is then calculated to assess the relevance of each paper. The top 200 papers with the highest similarity scores are selected for further analysis, ensuring the final literature set is focused and comprehensive. This structured approach significantly"}, {"title": "3.3 Data Extraction and Topic Modeling Module", "content": "The Data Extraction and Topic Modeling Module automates organizing and categorizing selected academic papers into meaningful topics based on semantic content. The module leverages topic modeling and language generation techniques to create a structured literature representation, making identifying key research themes and subtopics easier. The module's core components include Topic Modeling and Document Clustering, Keyword Extraction and Title Generation, and Topic Report Generation."}, {"title": "Topic Modeling and Document Clustering.", "content": "Once the most relevant documents are selected from the initial screening phase, this module initiates by creating embeddings for the textual content of each document using a Sentence-BERT model. These embeddings capture the semantic information of the documents, allowing for an effective clustering of papers based on their conceptual similarities. Topic modeling uses the BERTopic algorithm, which groups documents into coherent clusters reflecting the selected literature's primary themes. The number of topics and the minimum topic size are dynamically adjusted based on the size and content of the dataset to ensure that the generated topics are both meaningful and interpretable."}, {"title": "Keyword Extraction and Title Generation.", "content": "After clustering the documents into distinct topics, the system extracts keywords for each topic, summaris- ing the main themes in that cluster. The keywords are input into a language model, such as GPT-3.5, GPT-4, or GPT-40, to generate concise and descriptive titles for each topic. This process is guided by a structured prompt instructing the language model to create topic titles that accurately represent the essence of the keywords while maintaining clarity and relevance. The prompt used for this task is:"}, {"title": "Topic Report Generation.", "content": "After generating the titles, the system compiles a comprehensive report that includes the list of documents under each topic, the topic keywords, and the generated titles. This hierarchical organization of literature enhances the comprehensiveness and accessibility of the review, as it delineates different research themes and subtopics, making it easier for researchers to identify key trends and gaps in the literature. The module's process is further supported by a series of iterations and parameter adjustments to refine the topic modeling. If the initial number of topics is too few or too many, the system dynamically tunes the parameters, such as the number of topics or the minimum size of a topic, to achieve optimal clustering. Overall, this module significantly enhances the efficiency and effectiveness of the systematic literature review process by automating the categorization of papers and generating meaningful insights into the core themes of the literature. It automates the categorization of documents, offering researchers valuable insights into the core themes of the literature and simplifying the identification of key trends and gaps."}, {"title": "3.4 Synthesis and Summarization Module", "content": "The Synthesis and Summarization Module generates concise and coherent summaries for each identified topic cluster, significantly reducing the manual effort typically required in literature review processes. This module utilizes transformer-based models, such as T5, to summarize abstracts and GPT-based models for post-editing, ensuring that the resulting content is well-structured and easy to understand."}, {"title": "Abstract Summarization with T5.", "content": "The process begins by generating summaries for individual abstracts within each topic cluster using a transformer-based model like T5. This model is specifically configured to produce short yet comprehensive summaries that capture each document's key contributions and findings. The generated summaries retain essential details while significantly reducing the length of the original abstracts, making it easier to synthesize large volumes of research."}, {"title": "Topic-Level Summarization and Aggregation.", "content": "After individual summaries are generated, they are aggregated into a comprehensive summary for each identified topic. This step synthesizes the insights from multiple papers within the same topic, offering a holistic view of the research contributions, trends, and open questions. The aggregated summaries provide a structured narrative highlighting the most significant findings across multiple studies."}, {"title": "Post-Editing and Refinement with GPT.", "content": "To enhance the clarity, coherence, and flow of the aggregated summaries, a GPT-based model is employed for post-editing. The refinement process involves using a predefined prompt instructing GPT to improve readability and structure while preserving critical information. This step ensures that the final summaries are well-organized and suitable for inclusion in a structured literature review document. The following prompt is used for post-editing:"}, {"title": "Document Compilation and Report Generation.", "content": "The final step is compiling the generated summaries and topics into a coherent literature review document. This module integrates all the synthesized content into a structured LaTeX document, which includes an introduction, background information, detailed literature synthesis for each topic, and a conclusion. The system also generates a BibTeX file with the references for all included papers, ensuring proper citation and academic integrity.\nThe document generation process uses GPT, ensuring the final output is professionally formatted and adheres to the desired layout and style. The module supports various formats for exporting the final report, including LaTeX and PDF, providing researchers with a polished, ready-to-use literature review."}, {"title": "4 Experimental Setup", "content": "The proposed automated SLR framework was evaluated using a comprehensive experimental setup to assess its performance across different stages of the review process. We used five distinct research topics for the experiments: \"Explainable Artificial Intelligence (\u03a7\u0391\u0399),\" \"Virtual Reality (VR),\" \"Blockchain,\" \"Large Language Models (LLMs),\" and \"Neural Machine Translation (NMT).\" Each experiment focused on a specific phase of the proposed SLR framework: Systematic Search and Screening, Data Extraction and Topic Modeling, and Synthesis and Summarization."}, {"title": "Datasets.", "content": "We conducted experiments using five different research topics, each representing a unique area of academic research: Explainable Artificial Intelligence, Virtual Reality, Blockchain, Large Language Models, and Neural Machine Translation. We collected the papers for each research topic from the arXiv database. We retrieved papers based on search queries generated by GPT-3.5 and GPT-40 models, with a maximum limit of 3000 papers per query."}, {"title": "Experiments.", "content": "We designed four experiments to assess the system's performance across different phases: Systematic Search and Screening, Data Extraction and Topic Modeling, Synthesis and Summarization, and Document Compilation and Report Generation. We reported the results using various metrics, including topic coherence, ROUGE scores, readability scores, and cosine similarity."}, {"title": "Readability Analysis.", "content": "We evaluated the readability of the generated summaries and final LaTeX documents using the Flesch Reading Ease Score (FRES). The Flesch Reading Ease Score  provides insight into how easily a text can be read and understood. Higher FRES scores indicate simpler reading material, while lower scores denote more complex and challenging passages. We computed FRES at different stages of the summarization and document generation process to assess how readability changes as the content is processed through T5 summarization, GPT post-editing, and final document generation."}, {"title": "Metrics.", "content": "To evaluate the quality and robustness of the proposed framework, we used the following metrics:\n\u2022 Topic coherence. Measures the semantic similarity between words in a topic, indicating how well the generated topics represent coherent and interpretable concepts. A higher coherence score suggests that the words within each topic are more closely related, making the topics more useful and understandable for further analysis .\n\u2022 ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It compares an automatically produced summary or translation against a set of reference summaries (typically human-produced). ROUGE evaluates various aspects, such as the overlap of n-grams, word sequences, and word pairs between the machine-generated output and the reference.\n\u2022 ROUGE-1 measures the overlap of unigrams (single words) between the generated and reference abstracts. ROUGE-1 is particularly useful for evaluating summarization techniques because it captures the essential content and ensures that key information from the original text is retained in the summary.\n\u2022 Precision for ROUGE-1 measures the fraction of relevant instances among the retrieved cases, indicating how much of the generated summary is present in the reference text, which is the abstract in our case."}, {"title": "Number of Papers Retrieved", "content": "\u2022 Recall for ROUGE-1 measures the fraction of instances retrieved over the total number of cases in the reference, indicating how much of the reference abstract is covered by the generated summary.\n\u2022 F1-Score for ROUGE-1 is the harmonic mean of precision and recall, providing a balance between the two metrics.\n\u2022 Cosine Similarity measures the similarity between two non-zero vectors of an inner product space, effectively capturing the semantic closeness between the generated text and the reference text. Cosine similarity was used to evaluate the semantic alignment of abstracts with expanded topics during the Systematic Search and Screening phase.\n\u2022 Flesch Reading Ease Score (FRES)  provides insight into how easily a piece of text can be read and understood. The FRES formula considers sentence length and syllable count, with higher scores indicating simpler and more accessible text. We computed the FRES for three stages: T5-generated summaries, GPT post-edited sections, and the final LaTeX document. The formula is as follows:\n$FRES = 206.835 - 1.015 ( \\frac{total words}{total sentences} )-84.6 (\\frac{total syllables}{total words})$\nThis formula provides a measure of how easy a text is to read. Higher scores indicate easier-to-read material, while lower scores denote more difficult passages.\n\u2022 Number of Papers Retrieved indicates the coverage of the search query and its ability to find relevant literature.\n\u2022 Number of Papers Filtered reflects the number of papers that passed an initial relevance filter based on the research topic.\n\u2022 Total CPU Time is the computational time required for generating queries, retrieving papers, and filtering results."}, {"title": "Hardware.", "content": "Experiments were conducted on a Google Colab environment using an Intel Xeon CPU @ 2.20GHz (2 cores, 56MB cache), with 12.7 GB of RAM and 107.7 GB of disk space."}, {"title": "4.1 Experiment 1: Systematic Search and Screening", "content": "This experiment evaluated the effectiveness of GPT-3.5 and GPT-40 in generating queries for retrieving research papers from the arXiv repository. Given their capabilities in generating structured and contextually rich queries, we sought to compare the two models regarding their retrieval performance, efficiency, and computational cost. The experiment aimed to identify which model performs better across various research topics."}, {"title": "4.2 Experiment 2: Data Extraction and Topic Modelling", "content": "This experiment evaluated the quality of topics generated during the data extraction and topic modeling phase. The goal was to determine how well the BERTopic algorithm organized the retrieved literature into meaningful and coherent themes.\nWe used the topic coherence metric from Gensim to measure the quality of the generated topics. Topic coherence quantifies the semantic similarity between words within a topic, indicating how well the topics represent coherent and interpretable concepts. This measure has been validated as a reliable method for assessing topic models in previous work by R\u00f6der et al (2015). Their study evaluated over 237,912 coherence measures across six benchmark datasets and demonstrated that specific combinations of coherence metrics correlate highly with human ratings, setting a standard for evaluating topic models. Our experiment applied Gensim's implementation of the coherence metric to assess the topics generated from documents retrieved using GPT-3.5 and GPT-40 queries. This metric ensures that the topics produced are statistically sound and interpretable to human evaluators."}, {"title": "4.3 Experiment 3: Synthesis and Summarization", "content": "This experiment assessed the performance of the Synthesis and Summarization phase of our automated literature review framework. We evaluated the quality of the generated summaries using ROUGE scores to determine their relevance and content retention. Additionally, the readability of each summary was analyzed using the Flesch Reading Ease metric. The primary goal was to determine how effectively the system condenses and synthesizes information from multiple research papers while maintaining coherence and relevance."}, {"title": "ROUGE Score Analysis.", "content": "We used the ROUGE-1 metric to compare the content overlap between the machine-generated summaries and the abstracts of the selected research papers, which served as reference texts. ROUGE-1 measures the degree of overlap in unigrams (single words) between the generated summaries and reference texts, making it suitable for evaluating content retention and relevance.\nThe evaluation was conducted in three stages:\nAbstract Generated Summaries using T5: These serve as the baseline summaries generated by the T5 model, which captures the core content of the abstracts.\nPost-Edited Generate Summaries using GPT: GPT-based models refine these summaries to enhance readability, coherence, and overall structure.\nDocument Compilation and Report Generation using LaTeX: Comprehensive sections formatted as LaTeX documents that integrate information from multiple summaries, providing a cohesive and structured literature overview.\nWe computed ROUGE-1 precision, recall, and F1 scores for each stage. While all three metrics provide valuable insights, we focused primarily on precision. High precision indicates that the summaries retain the most pertinent information from the reference abstracts, minimizing irrelevant details."}, {"title": "Post-Editing Stage Results", "content": "The post-editing phase is crucial in refining the machine-generated summaries produced by the T5 model. This stage utilizes GPT-based models to enhance the initial summaries' clarity, coherence, and structure. The objective is to condense and reorganize the content while preserving the most relevant information. Post-editing is essential for transforming raw summaries into well-structured sections that align with the broader context of an SLR."}, {"title": "4.4 Experiment 4: Readability Score", "content": "We used the Flesch Reading Ease Score to evaluate the readability of the generated summaries and final documents at different stages of the document generation process. This metric provides insights into how accessible the text is to a general audience, with higher scores indicating easier-to-read content. Readability was evaluated for the T5-generated summaries, GPT post-edited summaries, and the final LaTeX documents."}, {"title": "4.5 Experiment 5: Sentence Similarity", "content": "To further evaluate the effectiveness of our automated systematic literature review (SLR) system, we computed cosine similarity scores at various stages of document generation. This analysis quantifies how closely the generated summaries and final documents align with the original input queries, providing a measure of content retention and relevance. For comparison, a baseline (Random) was included, representing a document generated with random words, to serve as a control.\nCosine similarity measures the cosine of the angle between two vectors in a multidimensional space here, these vectors represent text embeddings derived from the documents. Higher cosine similarity scores indicate greater alignment between the generated texts and the original input queries."}, {"title": "Abstract Filtering.", "content": "The filtered abstracts generated by GPT-3.5 and GPT-40 exhibit high cosine similarity scores, demonstrating that the initial search and screening phase effectively identifies documents closely related to the input queries. Both models perform well at this stage, confirming the robustness of the search process."}, {"title": "T5-Generated Summaries.", "content": "The cosine similarity scores decrease slightly for the T5-generated summaries, which is expected. Summarization inherently condenses content and may omit some details, leading to a lower similarity with the full abstracts. However, the core information relevant to the input query remains retained, ensuring that the generated summaries focus on the main topics."}, {"title": "GPT Post-Edited Summaries.", "content": "The cosine similarity scores increase after the post-editing process by GPT. This improvement suggests that the GPT-based post-editing refines the structure and readability and enhances alignment with the original input. The post-editing process ensures that the key content is retained while improving the coherence of the generated sections. GPT-40 generally outperforms GPT-3.5 in maintaining content similarity, indicating that GPT-40 is more effective at preserving relevant information."}, {"title": "Final LaTeX Documents.", "content": "The final documents generated by GPT continue to exhibit high similarity scores, indicating that the synthesis and summarization process effectively retains the relevance of the content. The structured nature of LaTeX documents ensures that core themes from the input queries are well-represented. GPT-40 again shows slightly better performance than GPT-3.5, further suggesting that"}, {"title": "Random Baseline:", "content": "As expected, the cosine similarity scores for the randomly generated document are very low. This serves as a control, validating the significance of the similarity scores observed for the generated summaries and final documents."}, {"title": "4.6 Experiment 6: Finding the Optimal Number of Papers for SLR", "content": "This section explores several key performance metrics to determine the optimal number of papers to include in the SLR process. The metrics analyzed include CPU time, number of topics identified, topic coherence, ROUGE scores, readability scores, and cosine similarity scores. These metrics are used to assess the impact of different document limits on the quality and efficiency of the SLR. We recommend the most effective document limit that balances performance and computational resources based on the analysis results."}, {"title": "CPU Time.", "content": "CPU Time. shows how computational requirements scale with the number of documents processed. As the document count increases, CPU time rises significantly, with GPT- 40 consistently requiring more time than GPT-3.5. This indicates that although GPT-40 can potentially offer more accurate results, it demands more computational resources, which is a trade-off to consider when processing large volumes of documents."}, {"title": "Number of Topics Found.", "content": "Number of Topics Found. In Figure 2-(b), BERTopic identifies an increasing number of topics as more documents are processed. GPT-40 consistently identifies more topics than GPT-3.5 across all document limits. This suggests that GPT-40 is more adept at detailed clustering, potentially offering a more nuanced breakdown of the literature. However, after a certain threshold, the increase in topics may not necessarily translate to better quality but rather more fragmented groupings."}, {"title": "Topic Coherence.", "content": "The Topic Coherence metric measures the semantic similarity within the topics identified, providing insight into the quality of the generated clusters"}, {"title": "ROUGE Scores for Summarization Quality.", "content": "The ROUGE scores measure how well the generated summaries align with reference abstracts, focusing on content retention. Figure 2-(d) shows that as the number of documents increases, the ROUGE scores improve, peaking around 200. This suggests that the system becomes better at generating summaries that capture the core content of the papers as more documents are processed. However, beyond the 200-document threshold, the improvement in ROUGE scores plateaus, indicating that additional documents do not contribute significantly to better summariza- tion. This implies that while increasing the document count improves the system's ability to summarize effectively, there is little benefit to going beyond 200 documents regarding content retention and quality."}, {"title": "Cosine Similarity for Content Alignment.", "content": "Cosine Similarity for Content Alignment. The Cosine Similarity scores measure how closely the generated documents align with the input queries, indicating relevance and focus. shows that GPT-3.5 and GPT-40 achieve high similarity scores across all document limits, stabilizing around 200 documents. This indicates that 200 documents provide sufficient information to produce outputs well- aligned with the original research query without overwhelming the system with excess data. The plateau"}, {"title": "Readability Scores.", "content": "Readability Scores. We use the Flesch Reading Ease metric to evaluate how accessible and easy to read the generated summaries are. indicates that the readability scores increase as more documents are processed, reaching their highest point, around 200. This suggests that the generated summaries become clearer and easier to read as the system processes more documents, possibly due to having a more comprehensive pool of content to draw from. However, readability scores decline slightly after 200 documents, indicating that the system might introduce more complex or fragmented language as the document count grows. This highlights that 200 documents offer the best balance for generating summaries that are both informative and easy to read."}, {"title": "Optimal Number of Papers", "content": "Based on the analysis of the above metrics, 200 documents emerge as the optimal document limit for the SLR process. At this threshold, the system provides high-quality summaries, maintains strong topic coherence, and produces readable and relevant outputs without excessive computational resources. Using over 200 documents leads to diminishing returns, particularly regarding topic coherence, readability, and cosine similarity. Thus, we recommend 200 documents as the ideal balance between performance and efficiency for conducting automated systematic literature reviews."}, {"title": "5 Discussion", "content": "The results presented in this study demonstrate the potential of the proposed automated SLR framework to streamline and enhance the process of conducting literature reviews. By integrating advanced NLP techniques and LLMs such as GPT-3.5 and GPT-40, the framework automates systematic search, data extraction, topic modeling, and summarization stages. However, a critical analysis of the results reveals both strengths and areas for improvement."}, {"title": "GPT-40 retrieves more papers than GPT-3.5.", "content": "GPT-40 retrieves more papers than GPT-3.5. The experiments revealed that GPT-40 consistently outperformed GPT-3.5 in retrieving a larger number of papers across all research topics. This suggests that GPT-40 is better at generating more comprehensive and contextually rich search queries. The ability of GPT-40 to retrieve more papers is beneficial in scenarios where exhaustive literature coverage is important, such as systematic reviews and meta-analyses, as it ensures that a wider array of relevant research is considered. However, this improved retrieval capacity may also introduce more irrelevant or"}, {"title": "High ROUGE-1 precision scores for both models.", "content": "High ROUGE-1 precision scores for both models. Both GPT-3.5 and GPT-40 demonstrated high ROUGE-1 precision scores during the summarization phase, indicating that the generated summaries retained a significant amount of relevant content from the reference abstracts. This suggests that the models effectively focus on the most important information when creating summaries, which is critical in systematic reviews where maintaining the relevance of summarized content is paramount. However, the relatively lower recall scores reflect that some content was omitted during summarization, which may be intentional to avoid overwhelming the reader with excessive detail. The high precision with lower recall suggests a bias toward conciseness, which can be advantageous in certain contexts but may require adjustment depending on the goals of the review."}, {"title": "Post-editing improved precision but reduced recall.", "content": "Post-editing improved precision but reduced recall. The post-editing phase significantly improved the precision of the generated summaries but reduced recall, indicating that while the content became more concise and focused, some relevant details were omitted. This aligns with the goal of post-editing, which is to refine and streamline the summaries for clarity and coherence. GPT-40 demonstrated higher recall than GPT-3.5 in this phase, suggesting it more effectively retained content during post-editing. This slight advantage highlights GPT-40's ability to balance relevance and conciseness better, making it more suitable for generating comprehensive yet readable summaries in systematic reviews."}, {"title": "GPT-40 achieved higher recall in final LaTeX documents.", "content": "GPT-40 achieved higher recall in final LaTeX documents. The final LaTeX documents generated by GPT-40 achieved higher recall scores than those generated by GPT-3.5, indicating that GPT-40 was more successful in incorporating additional relevant content while maintaining coherence. This makes GPT-40 particularly advantageous for use cases that require comprehensive literature coverage, as the final documents generated by GPT-40 were better at synthesizing information from multiple sources. However, this increase in recall may come at the cost of readability, as the additional content could make the final documents more complex and challenging to navigate. Future work could explore optimizing the balance between recall and readability in final document generation."}, {"title": "Readability improved through post-editing and final document generation.", "content": "Readability"}]}