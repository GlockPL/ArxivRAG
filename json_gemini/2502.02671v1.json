{"title": "On Teacher Hacking in Language Model Distillation", "authors": ["Daniil Tiapkin", "Daniele Calandriello", "Johan Ferret", "Sarah Perrin", "Nino Vieillard", "Alexandre Ram\u00e9", "Mathieu Blondel"], "abstract": "Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.", "sections": [{"title": "1. Introduction", "content": "Distillation for post-training LMs. Language models (LMs) have achieved remarkable success across a wide range of natural language processing tasks, such as translation, summarization, and reasoning. Notably, large LMs demonstrate impressive generalization capabilities, but their high computational cost poses a significant challenge, particularly when deployed on resource-constrained devices. Efficiency considerations motivate the training of smaller LMs, that would ideally provide similar performance at a fraction of the computational cost. To this end, the most popular approach is knowledge distillation (KD) (Hinton et al., 2015), in which a smaller student LM is trained to imitate the larger teacher LM. Distillation is increasingly studied (Agarwal et al., 2024; Gu et al., 2024; Kim et al., 2024) and used, notably for the post-training pipelines of LMs (as demonstrated by Zephyr (Tunstall et al., 2023), Gemma-2 (Riviere et al., 2024), and DeepSeek-V3 (Liu et al., 2024a)), just before the final reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022) phase.\nTeacher as an imperfect proxy. However, a key under-studied limitation of KD is that the teacher model does not represent the ground-truth distribution but instead acts as"}, {"title": "2. Preliminaries", "content": "Let X and Y denote the spaces of all possible prompts and responses, assumed to be sentences in the vocabulary \u03a3. For two sets A and B, \u0394(A) denotes the space of probability measures over A, and \u0394(A|B) represents the space of conditional probability measures over A given B. An auto-regressive language model \u03c0\u2208 LM\u2211(X) is defined as the conditional probability of the next token or the end-of-sequence token given a prompt x \u2208 X and a partial generation y \u2208 Y, expressed as \u03c0(w|x, y), where w \u2208 \u03a3. In practice, the probability \u03c0(w|x, y) is defined using a softmax with temperature \u03c0(w|x, y) x exp(-z(w|x, y)), where z(x, y) are the logits output by the neural network and T is a temperature parameter.\nFor any language model \u03c0, the induced distribution over responses is given by p\u03c0(y|x) \u225c \u03a0y\u1d62 \u03c0(y\u1d62|x, y<i), where |y| denotes the length of the response y, i.e., the number of tokens (non-empty characters) in y, and y<i \u225c (y\u2081, y\u2082,..., y\u1d62\u208b\u2081).\nDistances between language model distributions. Let d\u2208 \u0394(X) be a distribution over prompts induced by the particular task dataset. To measure the convergence of one distribution, induced by a language model \u03c0, to a distribution induced by a language model \u03c0', we use the (expected) forward and reverse KL between the corresponding conditional measures p\u03c0 and p\u03c0', defined as KLseq(\u03c0, \u03c0') \u225c Ex\u223cd(\u00b7) [KL(p\u03c0(\u00b7|x), p\u03c0'(\u00b7|x))]. By the properties of the KL divergence, the sequence-level divergence can be estimated very efficiently using token-level KL divergences. Additionally, we use a sequence-level Jensen-Shannon divergence JSseq(\u03c0, \u03c0'). We provide more details in Appendix B.\nSupervised fine-tuning. Let p\u2208 \u0394(Y|X) be a conditional response distribution that encodes the ground-truth response distribution. Solving downstream tasks such as summarization, translation, reasoning, or instruction following is fundamentally equivalent to approximating p. Therefore, the ultimate goal of any post-training pipeline is to approximate p in order to address these tasks effectively.\nOne of the common approaches to this problem is supervised fine-tuning (SFT). Let us assume that we have a dataset of pairs (x, y) for x ~ d(\u00b7) and y ~ p(\u00b7|x). Then, to find a language model \u03c0 such that its conditional distribution p\u03c0 approximates p, it is common to use a simple log-loss\n$$LSFT(\u03c0) \u225c Ex\u223cd(\u00b7),y~p(\u00b7|x) [\u2212 log p\u03c0 (y|x)].$$\nThis loss is equal, up to a constant factor, to an expected sequence-level forward KL divergence between p and p\u03c0: Ex~d(\u00b7) [KL(p, p\u03c0)]."}, {"title": "Language model distillation.", "content": "We suppose that we have access to a teacher language model, denoted \u03c0t \u2208 LM\u2211(X), such that it approximates the ground-truth distribution p, that is, Pt = p\u03c0t \u2248 \u03c1. The goal of language model distillation is to train a student language model, denoted \u03c0s, so as to approximate the teacher model, that is, ps = p\u03c0s \u2248 Pt. We emphasize that the teacher-induced distribution pt is not equal to a ground-truth p but only approximates it.\nWe usually distinguish between hard and soft distillation. In hard distillation, the student is only trained from the teacher's predicted next tokens, i.e., the student only sees the most likely tokens according to the teacher. In soft distillation, the student is trained from the teacher's predicted next token distributions, giving much more information to the student. In this work, we focus on soft distillation, and the loss function for this procedure takes the form\n$$LKD(\u03c0s) \u225c \\frac{1}{|Y|} E{\\substack{x \\sim d(\\cdot) \\ y \\sim \\nu(\\cdot|x)}} \\left[\\sum_{i=1}^{|y|} ltoken (p{\\pi_s} (\\cdot|x, y{:<i}), p{\\pi_t} (\\cdot|x, y{:<i}))\\right],$$\nwhere x ~ d(\u00b7) and y ~ \u03bd(\u00b7|x), \u03bd\u2208 \u0394(Y|X) is a data source, and ltoken is a token-level loss function between two distributions over the vocabulary.\nThe token-level loss and the data source should satisfy two assumptions: (i) it is non-negative and satisfies the property ltoken (p, q) = 0 if and only if p = q for any two distributions p, q \u2208 \u0394(\u03a3), and (ii) the support of \u03bd(\u00b7|x) includes the support of teacher-induced conditions measure pt(x) for almost all x, i.e. d(x) > 0. Given these two assumptions, it is easy to show that a language model \u03c0 minimizes the loss LKD(\u03c0s) = 0 if and only if \u03c0s = \u03c0t.\nIn particular, considering \u03bd(x) induced by an offline dataset generated by a teacher and using ltoken(p, q) = KL(p, q), we achieve the same expected loss as in the case of SFT. This approach corresponds to the works of (Hinton et al., 2015; Sanh et al., 2020). However, we could consider different token-level losses, such as reverse KL divergence (Gu et al., 2024), generalized Jensen-Shannon divergence (Agarwal et al., 2024), or skewed KL divergence (Ko et al., 2024). The data source can be induced by sampling online from the teacher model (Kim & Rush, 2016), sampling online from the student model (Gu et al., 2024), or combining offline and online data (Lin et al., 2020; Agarwal et al., 2024; Ko et al., 2024)."}, {"title": "Offline vs. online data sources.", "content": "In this paper, we distinguish between two different types of data sources: offline and online. Offline data sources are based on a fixed dataset, denoted as Doffline = {(xi, yi)}N\u1d62\u208c\u2081, where xi is sampled from Dprompt, and yi ~ pt(\u00b7|xi) are responses generated by the teacher model. Importantly, the dataset Doffline does not need to have a one-to-one correspondence between prompts"}, {"title": "Teacher hacking.", "content": "As we already mentioned, the main goal of the post-training pipeline is to approximate the ground-truth distribution p by the student model. This goal could be achieved by SFT given access to sufficiently many samples from p. However, recent works have shown that distilling from a teacher model can actually work better since the whole next-token distribution contains much richer information than only sampled tokens. An understudied problem is that the teacher model is only an imperfect proxy for p. This may lead to a problematic situation where the student LM learns to imitate the teacher, not by better approximating the true data distribution but by exploiting imperfections in the teacher. We call this phenomenon teacher hacking, and give a more formal definition below.\nDefinition 1 (Teacher hacking). Let {p\u209b\u207d\u1d4f\u207e}\u2096\u208c\u2081 be a sequence of conditional response distributions induced during the training of a student model, pt the distribution induced by the teacher model, and p the target human expert conditional distribution. We say that {p\u209b\u207d\u1d4f\u207e}\u2096\u208c\u2081 exhibits the teacher hacking phenomenon with respect to a distance measure dist: \u0394(Y|X) \u00d7 \u0394(Y|X) \u2192 R+ if, as k \u2192 +\u221e, dist(p\u209b\u207d\u1d4f\u207e, pt) decreases while dist(p\u209b\u207d\u1d4f\u207e, p) increases.\nA simple example where this would occur is when the student model is initially closer to the target distribution p than the teacher model. However, in more realistic scenarios that are closer to real-world applications, the teacher model is larger and provides a better approximation of p than the student model."}, {"title": "3. Methodology", "content": "To analyze the effect of teacher hacking, we require a method to estimate the distance between the student model and the ground-truth distribution. For this purpose, we introduce the oracle model, denoted as \u03bc\u2208 LM\u2211(X), which is assumed to induce the target distribution p, i.e., p\u00b5 = \u03c1.\nGolden and proxy metrics. As outlined in Definition 1, evaluating the teacher hacking phenomenon requires computing two sets of metrics.\nGolden metrics are computed using the oracle model and reflect the performance with respect to the true objective. Specifically, we use three types of divergences: the forward KL divergence KLseq(\u03bc, \u03c0s), the reverse KL divergence KLseq(\u03c0s, \u03bc), and a Jensen-Shannon-like distance JSseq(\u03c0s, \u03bc), which is closely related to the Jensen-Shannon divergence between the conditional distributions over the response space. These metrics are estimated using a held-out validation set of prompts, with sampling performed from the respective models."}, {"title": "Training.", "content": "The training procedure for our experiments consists of two stages.\nIn the first stage, supervised fine-tuning is performed on both the teacher and student models using a small oracle-generated dataset Doracle = {(xi, yi)}N\u1d62\u208c<subscript>oracle</subscript>. The prompts xi are sampled from the task distribution d(\u00b7), and the responses yi ~ p(xi) are generated using the oracle model. Our setup is semi-synthetic, in the sense that prompts are sampled from real datasets but responses (seen as labels) are sampled from the oracle LM to learn the teacher, and from the teacher LM to learn the student. This stage is the only place where direct information from the oracle model is propagated to the teacher and student models. The fine-tuning process optimizes a usual SFT loss (1) that in expectation equals to a sequence-level distance KLseq(\u03bc, \u03c0t). The best checkpoint is selected based on the estimate of this quantity over the validation set.\nIn the second stage, distillation is conducted from the teacher to the student model by optimizing the soft-distillation loss (2). The distillation process uses a training dataset of unlabeled prompts Dprompt = {xi}N\u1d62\u208c\u2081, where N> Noracle and different data sources that define a distribution of yi ~ \u03bd(\u00b7|x) in the loss."}, {"title": "Evaluation.", "content": "To investigate the teacher hacking phenomenon, we analyze two key types of curves: (1) the dependence of the training loss, proxy metrics, and golden metrics on the number of epochs completed, and (2) the proxy-golden curve, which illustrates the relationship between the golden metric (only accessible in our controlled experimental setup) and the proxy metric. Both proxy and golden metrics are computed using a held-out validation set of prompts. The epoch-dependence plots provide insights into scaling law phenomena (Kaplan et al., 2020) and help to understand overall training dynamics. Proxy-golden curves are crucial for visually assessing the presence of teacher hacking: a U-shaped curve serves as a clear indicator. Indeed, we expect the proxy metric to be reduced during training, and if the golden metric first decreases and then increases, it directly shows teacher hacking. These proxy-golden plots can be compared to plots from Gao et al. (2023), with one essential"}, {"title": "4. Experimental results", "content": "Oracle, teacher, and student models. Our experiments use a family of encoder-decoder language models based on T5 (Raffel et al., 2020; Roberts et al., 2022). The oracle model is the Flan-T5-XL (Chung et al., 2024), a 3B-parameter model fine-tuned on the Flan dataset (Wei et al., 2021; Longpre et al., 2023) for instruction-based tasks. For the teacher and student models, we use pretrained checkpoints of T5-1.1 in three configurations: small (77M parameters), base (250M parameters), and large (800M parameters). We always use temperature sampling with a temperature parameter T = 1 for generations from any model.\nDatasets. Our experiments use three datasets for training and evaluation: the XSum summarization dataset (Narayan et al., 2018), the WMT-14 en-de translation dataset (Bojar et al., 2014), and the instruction-following dataset Natural Instructions (Mishra et al., 2022; Wang et al., 2022). In alignment with our experimental setup, we use only the prompt data from these datasets, supplemented with task-specific instructions as needed for each task.\nFor the first stage of the training pipeline, where the oracle dataset is build and used for SFT, we use Noracle = 25 000, 50000, and 100000 prompts from the XSum, WMT-14 en-de, and Natural Instructions datasets, respectively. For the second stage, which involves the knowledge distillation procedure, we use N = 200 000, 450000, and 500000 prompts from these datasets. A single epoch is defined as one complete pass through all N examples, corresponding to [N/B] training steps, where B denotes the batch size. For XSum and WMT-14 en-de, we use batch size B = 32; for Natural Instructions, we use batch size B = 64."}, {"title": "4.1. Does teacher hacking appear?", "content": "We begin by investigating whether teacher hacking appears.\nSetup. For the first experiment, we use only offline data sources: responses are pre-generated as yi ~ pt(xi) for all xi \u2208 Dprompt, and the dataset remains fixed throughout training. The learning rate for optimization is selected via a grid search over {10\u207b\u2074,3 \u00d7 10\u207b\u2074,10\u207b\u00b3}.\nThe distillation procedure starts from the SFT checkpoints of the teacher and student models. Training is carried out over 50 epochs to analyze long-term convergence behavior.\nResults. The results of distilling the T5-large teacher model into the T5-base student on the XSum dataset, using forward KL loss, along with the corresponding golden and proxy metrics, are shown in Figure 4.\nIn this plot, the x-axis represents the optimization progress in terms of the distance to the teacher model (from left to right), while the y-axis shows the golden metric. The scatter plot shows the exact values of proxy and golden metrics, where the color demonstrates at which epoch this measurement was performed. The curve itself shows a relationship between smoothed values of the proxy and"}, {"title": "Observation 1. Teacher hacking exists and emerges after extended training on a fixed offline dataset.", "content": null}, {"title": "4.2. When does teacher hacking appear?", "content": "In this subsection, we investigate the conditions under which teacher hacking occurs.\nSetup. For this experiment, we evaluate three distinct data sources: (1) Offline data source; (2) Online teacher data source: for each batch of prompts sampled from Dprompt, a new response yi ~ pt(xi) is dynamically generated by the teacher model; (3) Online student data source: responses are generated on-the-fly as yi ~ ps(xi) using the current student model.\nAs in the previous experiment, we use the forward KL divergence as a token-level loss. We refer to Appendix A for different token-level loss functions, such as reverse KL divergence and Jensen-Shannon divergence.\nWe analyze the dynamics of proxy and golden metrics for each data source. For the proxy and golden metrics, we apply Gaussian smoothing to smooth the noisy behavior of the curves. We would also like to emphasize that the difference in scaling between the training loss and proxy/golden"}, {"title": "(i) In the offline data scenario,", "content": "we validate the presence of teacher hacking: the proxy metric decreases while the golden metric increases after a certain point. This phenomenon does not occur with online data sources."}, {"title": "(ii) We notice that the behavior of all curves for online and offline data sources is different.", "content": "Overall, the training loss for the offline data sources decreases faster since the training loss is optimized multiple times over the same data, but the performance on the proxy/golden metrics is worse overall."}, {"title": "(iii) The proxy metric for online data sources follows a linear trend on the log-log scale,", "content": "indicating a polynomial convergence law. In contrast, teacher hacking in offline data coincides with deviations in the proxy metric compared to online data. It gives a mechanism to detect teacher hacking using only the proxy metric, that is measurable even in real scenarios (not only in our controlled experimental setup)."}, {"title": "4.3. How to mitigate teacher hacking?", "content": "In the next experiment, we evaluate different methods for modifying the diversity and amount of offline data, in order to investigate how the properties of the offline data affect the teacher hacking phenomenon.\nSetup. For this experiment, we evaluate different approaches to constructing an offline data source. We define an ordinary offline data source as one that uses all available prompts and a single generation for each prompt: Doffline = {(xi, yi) | yi ~ pt(\u00b7|xi)}N\u1d62\u208c\u2081, where Dprompt = {xi}N\u1d62\u208c\u2081.\nOur first objective is to study how the diversity of the offline dataset impacts the teacher hacking phenomenon under a fixed dataset generation budget. Let k \u2208 N be a natural number. To construct a dataset with reduced diversity, we sub-sample [N/k] prompts from Dprompt and generate k responses for each sampled prompt using the teacher model. The resulting dataset maintains the same generation budget of N total responses but exhibits reduced diversity because the k generations for the same prompt x\u1d62 are closer to each"}, {"title": "Observation 2. Employing online data generation or limiting training to a few epochs effectively prevents teacher hacking.", "content": null}, {"title": "Observation 3. Prioritize Prompt Diversity.", "content": "When the generation budget for the distillation dataset is fixed, focusing on increasing the diversity of prompts can help reduce the impact of teacher hacking."}, {"title": "Observation 4. Expand the Dataset with Multiple Completions.", "content": "If the prompt dataset is fixed, increasing the generation budget by generating multiple completions per prompt also helps diminish the effects of teacher hacking."}, {"title": "5. Related work", "content": "Goodhart's law states: \u201cWhen a measure becomes a target, it ceases to be a good measure\" (Strathern, 1997). In particular, it manifests itself as reward hacking in RLHF (Amodei et al., 2016; Gao et al., 2023; Weng, 2024). A line of works studied reward hacking under controlled experimental setups (Gao et al., 2023; Rafailov et al., 2024). Our setup closely resembles that of Gao et al. (2023), where two types of reward models (RMs) are used: a golden reward model, which substitutes the ground-truth reward function, and a proxy reward model, trained on golden RM-preferred generations as ground-truth preferences. Specifically, we employ oracle and teacher models in the same roles as the golden and proxy RMs, respectively: the teacher model is trained on oracle-generated data, while the final student model is trained using the teacher's next-token distribution.\nGiven possible negative consequences of reward hacking (Hendrycks et al., 2021; Wen et al., 2024), another line of research attempts to mitigate its effects through better reward modeling or more robust training procedures (Chen et al., 2024; Ram\u00e9 et al., 2024; Liu et al., 2024b)."}, {"title": "6. Conclusion", "content": "In this paper, we introduce and examine the phenomenon of teacher hacking in language model distillation by designing a semi-synthetic controlled experimental setup. This allows us to measure its effects, and validate experimentally its presence when using a fixed offline dataset for the distillation procedure.\nFortunately, as a practical outcome of our study, we were"}, {"title": "Impact statement", "content": "This paper presents work on language model distillation, which is actively used in the training of many modern language models. We identify a possible shortcoming of existing distillation procedures, called teacher hacking, that can lead to the transfer of unsafe behaviors from teacher to student. Additionally, we proposed several strategies to reduce the effect of this phenomenon. We believe that understanding and identifying such issues have positive societal consequences and allow the development of more reliable and safe language models."}, {"title": "(i) utilize online generations during the distillation process,", "content": null}, {"title": "(2) when the generation budget is fixed,", "content": "prioritize increasing the diversity of the prompt dataset, and (3) if the prompt dataset is fixed and online generations are not feasible, generate multiple offline completions per prompt ahead of time to expand the dataset. We hope that these practical and methodological insights provide valuable guidance in extending the applicability and effectiveness of language model distillation in real-world scenarios."}]}