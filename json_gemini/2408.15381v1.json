{"title": "On Stateful Value Factorization\nin Multi-Agent Reinforcement Learning", "authors": ["Enrico Marchesini", "Andrea Baisero", "Rupali Bathi", "Christopher Amato"], "abstract": "Value factorization is a popular paradigm for designing scalable multi-agent re-\ninforcement learning algorithms. However, current factorization methods make\nchoices without full justification that may limit their performance. For example,\nthe theory in prior work uses stateless (i.e., history) functions, while the practical\nimplementations use state information-making the motivating theory a mismatch\nfor the implementation. Also, methods have built off of previous approaches,\ninheriting their architectures without exploring other, potentially better ones. To\naddress these concerns, we formally analyze the theory of using the state instead\nof the history in current methods-reconnecting theory and practice. We then\nintroduce DuelMIX, a factorization algorithm that learns distinct per-agent utility\nestimators to improve performance and achieve full expressiveness. Experiments\non StarCraft II micromanagement and Box Pushing tasks demonstrate the benefits\nof our intuitions.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in multi-agent reinforcement learning (MARL) have led to impressive results\nin many complex cooperative tasks (Samvelyan et al., 2019a; Mordatch and Abbeel, 2017; Aydeniz\net al., 2023). Many of these MARL methods use Centralised Training with Decentralised Execution\n(CTDE) (Lowe et al., 2017), which allows them to train in a centralized fashion but still execute in\na decentralized manner.\nThe dominant form of CTDE in value-based MARL is value factorization (Sunehag et al., 2017;\nRashid et al., 2018, 2020; Wang et al., 2021b; Marchesini and Farinelli, 2021, 2022). These methods\nfactor a (centralized) joint action value into (decentralized) per-agent utilities conditioned on local\ninformation. The resulting approaches ensure the greedy action selection from each agent's local\nutility is the same as greedy action selection over the centralized value function (i.e., the argmax over\nthe local utilities is the same as a joint argmax over the centralized value function)\u2014the Individual\nGlobal Max (IGM) principle (Son et al., 2019). IGM provides decentralization and scalability since\nalgorithms no longer need to perform costly joint maximization over all agents. Earlier forms of\nfactorization used strong constraints to ensure IGM (e.g., linearly or monotonically combining the\nlocal utilities in VDN (Sunehag et al., 2017) and QMIX (Rashid et al., 2018)) which limited their\nexpressiveness. More recent methods such as QPLEX (Wang et al., 2021b) can, in theory, represent\nthe full set of IGM factorizations.\nDespite the merits of value factorization, there is a mismatch between the theory and practice of\nthese methods. In particular, the theory behind the methods assumes history information at the local\nand centralized levels while most practical implementations replace history with (ground truth) state\nin some places. While replacing history information with state information is tempting to exploit\nadditional information during centralized training, it has been shown to be unsound in partially\nobservable settings, as recently shown in the actor-critic CTDE case (Lyu et al., 2023).\nTo address the gap between theory and practice in value factorization, we extend the theory to\nthe stateful case that combines state and history information. We show that IGM still holds for\nmost of the methods (VDN, QMIX, and QPLEX) but not necessarily for the weighted version of\nQMIX (WQMIX) (Rashid et al., 2020). We also show that QPLEX's practical implementation\ncan not represent the full IGM function class due to the use of state information instead of history\ninformation-losing one of its main benefits.\nWhile there are many architectures that would satisfy IGM, previous approaches, such as QPLEX,\nmade choices based on earlier work without exploring other alternatives that could improve perfor-\nmance. For example, unlike dueling networks, which typically learn separate history and advantage\nvalue functions at the agent level, QPLEX learns a Q-function and assumes the V-function is a max\nof it. For this reason, we introduce DuelMIX. DuelMIX maintains separate estimators at the agent\nlevel-instead of computing them from the agents' Q-functions. Such a separation has been shown to\nlearn better value approximations, which enhance performance and sample efficiency in single-agent\nscenarios (Wang et al., 2016).\nOur work makes the following contributions: we (i) formalize IGM over the centralized stateful\nfunctions used in practical implementations; (ii) analyze that the state does not introduce bias into\nQMIX (Rashid et al., 2018) for IGM, and QPLEX (Wang et al., 2021b) for Advantage-IGM; (iii)\nempirically show that using other sources of information during factorization (i.e., constant and\nrandom vectors) could lead to performances comparable or better than using the state, contrasting\nthe common belief that the state allows higher performance; (iv) present DuelMIX, a factorization\nscheme integrating dueling networks at a per-agent level, and combining joint history-state values in\na weighted fashion to achieve full expressiveness. This learning stream separation leads to significant\nbenefits in cooperative scenarios where optimal joint policies often hinge on specific actions.\nTo validate our intuitions, we test the methods in the highly partially observable Box Pushing (BP)\nscenario, where the optimal behavior is contingent on a specific agent's action (Xiao et al., 2020),\nand micromanagement tasks in the recent StarCraft II Lite (SMACLite) suite (Michalski et al., 2023).\nOur results on BP show the benefits of separate value learning, allowing DuelMIX to achieve good\nperformance where previous approaches fail. Moreover, SMACLite experiments show that DuelMIX\noutperforms previous factorization methods and significantly improves sample efficiency."}, {"title": "2 Preliminaries", "content": "We model fully cooperative multi-agent tasks as Decentralized Partially Observable Markov\nDecision Processes (Dec-POMDPs) (Oliehoek and Amato, 2016), represented as tuple\n$(N,S,U,T_s,r, \\mathcal{O},T_o,\\gamma)$. N is a finite set of agents; S is a finite set of states; $U = (U_i)_{i\\in N}$\nand $\\mathcal{O} = (O_i)_{i\\in N}$ are the finite sets of joint actions and observations respectively, while $U_i, O_i$ are\nthe individual ones. At each time step, every agent i chooses an action, forming the joint action\n$u = (u_i)_{i\\in N} \\in U$. After performing u, the environment transitions from a state s to a new s',\nfollowing a transition probability function $T_s : S \\times U \\times S \\rightarrow [0, 1]$ ($T_s(s, u, s') = Pr(s' | s, u)$),\nand returning a joint reward $r: S \\times U \\rightarrow \\mathbb{R}$ for being in state s$\\in S$ and taking actions u$\\in U$.\nIn a partially-observable setting, agents receive an observation $o = (o_i)_{i\\in N} \\in \\mathcal{O}$ according to\nan observation probability function $T_o : S \\times U \\times \\mathcal{O} \\rightarrow [0,1]$ ($T_o(u,s', o) = Pr(o | u,s')$),\nand each agent maintains a policy $\\pi_i(u_i | h_i)$ mapping local action-observation histories $h_i =$\n$(o_{i,0}, u_{i,0}, o_{i,1},..., o_{i,t}) \\in H_i$ to actions. In finite-horizon tasks, we aim to find a joint policy\n$\\pi(u | h)$ maximizing the expected discounted episodic return $E_{\\pi} [\\sum_t \\gamma^t r_t]$, where $\\gamma\\in [0, 1)$ is the\ndiscount and $h_t = (o_0, u_0,..., o_t) \\in H$ is the joint action-observation history."}, {"title": "2.1 Value Factorization", "content": "In this section, we summarize the stateless theoretical framework presented by seminal value factor-\nization works. In Section 3, we will discuss how most algorithms use the state during factorization.\nAs such, we argue the following preliminaries do not correctly reflect most of the published literature."}, {"title": "3 Stateful Value Factorization", "content": "Practical implementations of QMIX, WQMIX, and QPLEX often use state values in their centralized\nmodels usually through a mixing network-in ways that are neglected in their theoretical analysis\n(see Appendix A for a formal summary of stateless and stateful QMIX and QPLEX variants). For\nthe first time (to our knowledge), we analyze if the theory of these methods extends correctly to\nthe stateful case, or whether using state values introduces any learning bias. Such biases can easily\nhappen by improper uses of state in single and multi-agent partially observable control problems as\nrecently shown by Baisero and Amato (2022); Baisero et al. (2022); Lyu et al. (2023).\nWe begin by adjusting the notation of centralized value models that use state, effectively resulting\nin history-state values Q(h, s, u). Given that consistent history and history-state values are related\nby $Q(h, u) = E_{s|h} [Q(h, s, u)]$, we correctly reformulate the IGM principle in a marginalized\nhistory-state form as follows:\nProposition 3.1 (History-State IGM). For a joint $Q : H \\times S \\times U \\rightarrow \\mathbb{R}$ and individuals$\\langle Q_i:\nH_i\\times U_i \\rightarrow \\mathbb{R}\\rangle_{i\\in N}$ such that the following holds:\n$\\arg \\max_{u\\in U} E_{s|h}[Q(h, s, u)] = \\underset{i \\in N}{\\Big(arg \\max_{u_i \\in U_i} Q_i(h_i, u_i)\\Big)}$,\n$(Q_i(h_i, u_i))_{i\\in N}$ are said to satisfy History-State IGM for Q(h, s, u).\nRole of the State in QMIX. The question now becomes whether the mixing model of QMIX (see\nAppendix A.1.2 for details) satisfies the History-State IGM principle. Given QMIX's hyper-network\narchitecture and the way the state is used, the monotonicity constraint of Equation (3) holds for every\nstate even for history-state values, resulting in a non-marginalized version of History-State IGM:\n$\\arg \\max_{u\\in U}  Q(h, s, u) = \\underset{i \\in N}{\\Big(arg \\max_{u_i \\in U_i} Q_i(h_i, u_i)\\Big)}$\nNotably, Equation (10) is distinct from Equation (9) in ways that may have problematic or even\ncatastrophic consequences in partially observable control, e.g., the maximal action of a centralized"}, {"title": "4 DuelMIX", "content": "To overcome the implementation drawbacks of previous factorization algorithms, we present a novel\nfactorization scheme, DuelMIX. Our approach leverages the dueling networks estimator at a per-agent\nlevel and introduces a weighted mixing mechanism that estimates a joint history value. Following\nour intuitions on stateful factorization, we introduce DuelMIX using stateful functions.\nAlgorithm. The overall architecture of DuelMIX is detailed in the following sections. In particular,\nFigure 1 shows the overall architecture of DuelMIX, which is composed of the following modules:\nAgent Dueling Utility (yellow). Each agent $i \\in N$ employs a recurrent Q-network taking its previous\nhidden state $h^{-1}$, previous action $u^{-1}$, and current observation $o$ as input to ensure decentralized\nexecution. In contrast to previous factorization methods that produce a local utility $Q_i (h_i, u_i)$ through\na single-stream estimator, DuelMIX utilizes two separate streams and outputs history and advantage\nutilities, denoted as $V_i(h_i)$ and $A_i (h_i, u_i)$, respectively. Crucially, building upon the insights of Wang\net al. (2016), each centralized update influences the $V_i$ stream, enhancing the approximation of the\nhistory value. In single-agent scenarios, enhancing the approximation of value functions has led to\nhigher performance and sample efficiency (Wang et al., 2016; Marchesini and Amato, 2023; Marzari\net al., 2024), and also proves to be particularly advantageous in cooperative tasks. In these tasks, the\noptimal joint policy often hinges on specific actions taken in particular histories (as demonstrated\nin the Box Pushing experiments of Section 5). Simultaneously, advantage utilities are employed\nfor decentralized action selection, such as with an $\\epsilon$-greedy policy. In particular, the advantage\nstream outputs $A_i(h_i, \u00b7) - \\max_{u_i} A_i(h_i, u_i)$, forcing advantages to be zero for the chosen action and\nnon-positive (< 0) for the others (which becomes necessary when transforming the advantage values\nwith positive weights).\nTransformation (green). DuelMIX incorporates the transformation network used by Qatten and\nQPLEX. In detail, the transformation network combines local utilities $(V_i(h_i), A_i(h_i, u_i))_{i\\in N}$ with\nstate information $(V_i(h_i, s), A_i(h_i, s, u))_{i\\in N}$. This module consists of a multi-layer perceptron\n(MLP) taking the state s as input, and outputs a set of biases and positive weights $(b_i(s), w_i(s) >\n0)_{i\\in N}$. These weights and biases transform the local utilities as in Equation (11). Unlike QPLEX, the\nDuelMIX transformation module transforms the $V_i(h_i), A_i(h_i, u_i)$ learned by the agent, instead of\nbeing obtained by decomposition from $Q_i (h_i, u_i)$ following their optimal definition as in Equation (4).\nMixing (blue). The centralized mixing network employed by DuelMIX uses the same efficient multi-\nhead attention mechanism as QPLEX to estimate positive importance weights $(\\lambda_i(s, u) > 0)_{i\\in N}$.\nMaintaining positivity is essential for action-selection consistency. These weights, combined with the\nper-agent transformed advantages, yield the stateful joint advantage value:\n$A(h, s, u) = \\sum_{i}^{|N|}\\lambda_i(s, u) A_i (h_i, s, u_i)$.\nIn contrast to prior works, we use an MLP taking as input the transformed history utilities and the\nstate to estimate weights $(w(h, s))_{i\\in N}$ that are used to factorize the joint history value as:\n$V(h, s) = \\sum_{i}^{N}w(h, s)V_i(h_i, s)$.\nUnlike the positive weights in the joint advantage factorization, our design of $w(h, s)$ is driven\nby two key motivations: they (i) can assume arbitrary values since $V (h, s)$ does not influence the\naction-selection process; (ii) allow DuelMIX to have full expressiveness over the class of functions\nsatisfying IGM since they are a function of the joint history h and are not limited by monotonic\nconstraints. Hence, this value stream can correct for the discrepancies between the centralized joint\naction value function and the monotonic advantage combination.\nThe joint history-state-action value driving the centralized learning process then follows by definition:\n$Q(h, s, u) = V(h, s) + A(h, s, u)$."}, {"title": "4.1 Representational Complexity", "content": "The positive weights in DuelMIX, the non-positive advantage utilities, and the joint history used to\nestimate $w'(h, s)$, enable our method to satisfy IGM and achieve full expressiveness. A formal proof\nis provided in Appendix C.\nProposition 4.1. The function class that DuelMIX can realize is equivalent to what is induced by\nHistory-State Advantage-IGM (Equation (33)).\nRole of the State in DuelMIX. We conclude our analysis of DuelMIX by including a result analogous\nto those in Section 3, on the bias that may be introduced by a potentially improper use of stateful\nmodels. As in the case of both QMIX and QPLEX, we are able to determine the following proposition,\nwhose formal proof is provided in Appendix B.\nProposition 4.2 (DuelMIX State Bias). A stateful implementation of DuelMIX does not introduce\nany additional state-induced bias compared to a stateless implementation."}, {"title": "5 Experiments", "content": "This section presents a comprehensive evaluation of the performance of DuelMIX in comparison to\nexisting factorization methods, namely VDN (Sunehag et al., 2017), QMIX (Rashid et al., 2018),\nQatten (Yang et al., 2020), and QPLEX (Wang et al., 2021b).\nOur experiments address the following key questions: (i) Does the agent's dueling utility network\nlearn a more effective representation of the state value? (ii) Does DuelMIX exhibit better performance\nover previous algorithms? (iii) Is the state crucial for performance in value factorization? To answer\nthese questions, we conduct experiments using the well-known Box Pushing task (Seuken and\nZilberstein, 2007) and standard micromanagement tasks based on Starcraft II (Michalski et al., 2023).\nImplementation Details. Data collection is performed on Xeon E5-2650 CPU nodes with 64GB of\nRAM, using existing implementations for the baselines (Sunehag et al., 2017; Rashid et al., 2018;\nWang et al., 2021b; Yang et al., 2020). Hyperparameters are in Appendix D and we report the average\nreturn smoothed over the last ten episodes of ten runs per method. Shaded regions represent the\nstandard error. This number of independent trials surpasses the typical 3-5 runs used in previous\nworks (Sunehag et al., 2017; Rashid et al., 2018; Wang et al., 2021b; Yang et al., 2020). Considering\nthe computational resources used for our experiments, Appendix E addresses the environmental\nimpact and our strategy to offset estimated CO2 emissions."}, {"title": "5.1 Environments", "content": "To demonstrate the benefits of learning per-agent separate utilities, we consider the BP task (Seuken\nand Zilberstein, 2007) with a grid size of 30 (BP-30). In this Dec-POMDP task, two agents must\ncollaborate to move a large box to the goal. Notably, agents can individually push small boxes, while\nmoving the large box requires synchronized effort. Agents have very limited visibility observing\nonly the cell in front of them, making high-dimensional scenarios considerably challenging. We\nalso test in the SMACLite decentralized micromanagement tasks (Michalski et al., 2023). SMAC"}, {"title": "5.2 Results", "content": "Box Pushing. This challenging Dec-POMDP task allows us to visualize how much importance the\ndueling utility network gives to the input features. Figure 2 shows results for stateful factorization\nalgorithms. Overall, previous methods learn very sub-optimal policies, where both agents roam\naround in the grid for a few steps, before one of them successfully pushes a small box to the goal.\nDuelMIX learns a more effective policy that enables the two agents to navigate directly to small\nboxes and push them simultaneously to the goal. This confirms DuelMIX's capability to achieve\nhigher returns in challenging scenarios.\nState Stream Representation. We show the saliency maps of Simonyan et al. (2013), employing\nthe Jacobian of the trained network, which allows us to visualize salient parts of the input as seen\nby the model's weights, showing which input features are more relevant for the weights. For clarity,\nthis requires re-training our policies in a fully observable setup, where agents take as input their\npositions, their orientations as a one-hot encoding, and the position of the boxes. It is thus possible\nto plot the \"importance\" that the state value stream of DuelMIX and QPLEX gives at each input.\nThe idea is to show that DuelMIX's state value stream gives significantly less importance to entities\nthat are not relevant to achieving a good payoff, while QPLEX's state value is not able to do so.\nFigure 3 overlaps the saliency map onto a reduced-size BP view. Blue cells indicate the features\n(cells) are not relevant to the agent so they will not affect its decisions relevantly. In contrast, green\ncells indicate the agent gives a high importance to those features. So we would like to see our policies\ngiving more importance (i.e., green) to the only cells leading to a positive reward. We show the"}, {"title": "5.3 Influence of the State on Performance", "content": "We explore the performance of using different centralized information during factorization. Injecting\nthe state in the mixer has become a standard de facto, but its use can not be supported by the theory.\nWe investigate the performance of factorization algorithms (except VDN, which does not use the state)\non representative SMACLite tasks. Following our stateful analysis in Section 3, the state information\ngets marginalized out so it is not clear that any information is actually being used from it. In both\nscenarios, the impact of weights and biases outputted by the factorization modules introduce noise in\nthe joint estimation with the result of increasing exploration and, potentially, robustness. As such, we\nexpect different sources of centralized information to work well with factorization algorithms."}, {"title": "6 Conclusions", "content": "This work effectively addressed an important gap in the theoretical and practical underpinnings\nof value function factorization. We analyzed the relationship between theoretical frameworks and\npractical implementations and proposed a novel efficient factorization algorithm.\nFrom a theoretical standpoint, we formally analyze the mismatch between the stateless theoretical\nframework presented in prior research and the actual stateful algorithms. Our experiments further\nquestioned the conventional use of the state during the factorization process. Contrary to common\npractice, where the state is employed as centralized information, our results suggest there could be\nbetter solutions. In particular, even simplistic forms of centralized random noise or zero vectors,\nexhibit comparable or superior performance compared to stateful algorithms in certain scenarios.\nOn the practical front, we introduced DuelMIX, a factorization scheme designed to learn more\ngeneral and distinct per-agent utility estimators. Furthermore, DuelMIX combines history values in\na weighted manner to refine the estimation of the joint history value. Experiments on StarCraft II\nmicromanagement and complex coordination tasks demonstrate the benefits of our intuitions.\nOur empirical insights not only contribute significantly to the current understanding of MARL but\nalso lay a principled foundation for future research in this domain."}, {"title": "A Value Factorization Method Variants", "content": "In this section, we formalize variants of QMIX and QPLEX that use different types of information\nin their key components, e.g., nothing, joint history, state, or joint history and state. Each variant\nemploys per-agent utilities $Q_i(h_i, u_i)$ and combines them (optionally integrating joint history and/or\nstate information) to obtain a joint action value function that takes the form of $Q(h, u)$ for stateless\nvariants, and $Q(h, s, u)$ for stateful variants."}, {"title": "A.1 QMIX", "content": "We categorize the QMIX variants in terms of which auxiliary input is provided to the mixing hyper-\nnetwork in addition to the individual agent utilities. In that regard, QMIX (Rashid et al., 2018)\nis originally formalized directly in a state variant, while the IGM principle is formalized without\nany sort of auxiliary information. This discrepancy is not directly addressed, and may potentially\nundermine the validity of the theoretical guarantees for the practical implementation."}, {"title": "A.1.1 Plain-QMIX", "content": "The Plain-QMIX variant is a direct implementation of the IGM principle, with no auxiliary infor-\nmation provided to set the hyper-network weights. This variant uses a mixing network to com-\nbine the per-agent utilities $(Q_i(h_i, u_i))_{i\\in N}$ into a joint history-action value function $Q(h, u) =$\n$f(Q_1(h_1, u_i),..., Q_n (h_n, u_n))$ that satisfies monotonic constraints between the joint Q and the\nindividual $Q_i$. This variant trivially satisfies the IGM principle."}, {"title": "A.1.2 State-QMIX", "content": "The State-QMIX variant uses ground truth state information to set the hyper-network weights, and\nit is the main variant proposed in its respective paper (see Figure 5). This variant uses a mixing\nhyper-network to combine the per-agent utilities $(Q_i(h_i, u_i))_{i\\in N}$ and the ground truth state into\na joint history-action value function $Q(h, s, u) = f(Q_1(h_1, u_1), ..., Q_n (h_n, u_n), s)$ that satisfies\nmonotonic constraints between the joint Q and the individual $Q_i$. The original work by Rashid et al.\n(2018) performs an empirical evaluation concerning the role of state. However, it does not formally\ndemonstrate that a stateful mixing model still satisfies the stateless IGM principle."}, {"title": "A.2 QPLEX", "content": "We categorize the QPLEX variants in terms of the inputs to the weight, bias, and attention models. In\nthat regard, the derivation and implementation of QPLEX (Wang et al., 2021b) differ in ways that\nundermine the validity of the theoretical guarantees for the practical implementation. In the following\nsubsections, we formalize each variant clearly, highlighting the differences. The discussion of why\nthese differences matter can be found in the main document of this submission. The history variant\ncorresponds to the theoretical formalization of QPLEX, the state variant corresponds to its practical\nimplementation, while the history-state variant is our attempt at bridging the gap between theory and\npractice."}, {"title": "A.2.1 History-QPLEX", "content": "History-QPLEX corresponds to the theoretical stateless formulation derived in the corresponding\npaper. It decomposes the per-agent utilities $(Q_i(h_i, u_i))_{i\\in N}$ into individual values $\\langle V_i(h_i)\\rangle_{i\\in N}$ and\nadvantages $(A_i(h_i, u_i))_{i\\in N}$ according to:\n$V_i(h_i) = \\max_u Q_i (h_i, U_i),$\n$A_i(h_i, U_i) = Q_i(h_i, U_i) - \\max_u Q_i (h_i, u)$.\nSuch values are then conditioned on the joint history using a monotonic transformation. This module\ntakes as input the joint history h and outputs biases and positive weights $(b_i(h), w_i(h) > 0)_{i\\in N}$:\n$V_i(h) = w_i(h)V_i(h_i) + b_i(h),$\n$A_i(h, u_i) = w_i(h) A_i(h_i, U_i)$.\nAfter this transformation, History-QPLEX factorizes the joint value function as:\n$Q(h, u) = \\sum_i V_i(h) + \\lambda_i(h, u) A_i(h, u_i)$,\nwhere $(\\lambda_i(h, u) > 0)_{i\\in N}$ are weights computed with an attention module to enhance credit assign-\nment. Both the transformation and the attention module use positive weights to maintain action\nselection consistency over the advantage values. Naturally, this positivity enforces monotonicity in\nthe advantage factorization."}, {"title": "A.2.2 State-QPLEX", "content": "State-QPLEX corresponds to the stateful practical implementation used in the empirical evaluation in\nthe corresponding paper (see Figure 6). The main difference compared to History-QPLEX is that the\ntransformation and attention modules replace the joint history for the underlying ground truth state.\nState-QPLEX decomposes the per-agent utilities $(Q_i(h_i, u_i))_{i\\in N}$ into individual values $\\langle V_i(h_i)\\rangle_{i\\in N}$\nand advantages $(A_i(h_i, U_i))_{i\\in N}$ according to:\n$V_i(h_i) = \\max_u Q_i (h_i, U_i),$\n$A_i(h_i, U_i) = Q_i(h_i, U_i) - \\max_u Q_i (h_i, u)$.\nSuch values are then conditioned on the ground truth state using a monotonic transformation. This\nmodule takes as input the state s and outputs biases and positive weights $(b_i(s), w_i(s) > 0)_{i\\in N}$:\n$V_i(h_i, s) = w_i(s)V_i(h_i) + b_i(s),$\n$A_i(h_i, s, u_i) = W_i(s)A_i(h_i, u_i)$.\nAfter this transformation, State-QPLEX factorizes the joint value function as:\n$Q(h, s, u) = \\sum_i V_i(h_i, s) + \\lambda_i(s, u) A_i (h_i, s, u_i)$,\nwhere $(\\lambda_i(s, u) > 0)_{i\\in N}$ are weights computed with an attention module to enhance credit assign-\nment. Note that the joint value function is now also a function of state, due to the dependence on the\nstateful transformation and attention modules."}, {"title": "A.2.3 History-State-QPLEX", "content": "History-State-QPLEX is our proposed attempt at unifying the stateless theoretical derivation with\nthe stateful practical implementation. The main difference compared to previous variants is that the\ntransformation and attention modules employ both the joint history and the underlying ground truth\nstate.\nState-QPLEX decomposes the per-agent utilities $(Q_i(h_i, u_i))_{i\\in N}$ into individual values $\\langle V_i(h_i)\\rangle_{i\\in N}$\nand advantages $(A_i(h_i, U_i))_{i\\in N}$ according to:\n$V_i(h_i) = \\max_u Q_i (h_i, U_i),$\n$A_i(h_i, U_i) = Q_i(h_i, U_i) - \\max_u Q_i (h_i, u)$.\nSuch values are then conditioned on the joint history and the ground truth state using a monotonic\ntransformation. This module takes as input the joint history h and state s and outputs biases and\npositive weights $(b_i(h, s), w_i(h, s) > 0)_{i\\in N}$:\n$V_i(h, s) = w_i(h, s)V_i(h_i) + b_i(h, s),$\n$A_i(h, s, u_i) = w_i(h, s) A_i(h_i, U_i)$.\nAfter this transformation, History-State-QPLEX factorizes the joint value function as:\n$Q(h, s, u) = \\sum_i V_i(h, s) + \\lambda_i(h, s, u) A_i(h, s, u_i)$,\nwhere $(\\lambda_i(h, s, u) > 0)_{i\\in N}$ are weights computed with an attention module to enhance credit\nassignment. As in the previous case, the joint value function is now also a function of state, due to\nthe dependence on the stateful transformation and attention modules."}, {"title": "B Stateful Value Factorization Proofs", "content": "In this section, we formally present the proofs for Propositions 3.2, 3.3 and 4.2.\nA common theme among all methods and proofs is the conclusion that the constraints that guarantee\nthe mixing model's monotonicity are so restrictive that they intrinsically do not allow state information\nto alter the identity of the maximal action, leading to the same actions being taken during both training\nand execution. While the state does influence the values that are being learned, actions are always\nobtained in a way that is intrinsically consistent with the stateless context of partial observability. This\nis not to say that the methods do not present any other type of bias; in fact, it would be more accurate\nto say that the modeling bias of the mixing model, which specifically prevents it from becoming a\nuniversal function approximator, expands widely enough to subsume any possible bias associated\nwith the use of the state."}, {"title": "B.1 Proof of Proposition 3.2", "content": "Proposition (QMIX State Bias). A stateful implementation of QMIX does not introduce any additional\nstate-induced bias compared to a stateless implementation.\nWe begin by adjusting the notation of centralized value models to show the use of state", "u)": "the IGM principle is equivalently written in a marginalized\nhistory-state form as follows:\n$\\arg \\max_{u\\in U"}]}