{"title": "VIDEOREPAIR: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement", "authors": ["Daeun Lee", "Jaehong Yoon", "Jaemin Cho", "Mohit Bansal"], "abstract": "Recent text-to-video (T2V) diffusion models have demonstrated impressive generation capabilities across various domains. However, these models often generate videos that have misalignments with text prompts, especially when the prompts describe complex scenes with multiple objects and attributes. To address this, we introduce VIDEOREPAIR, a novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling a T2V diffusion model to perform targeted, localized refinements. VIDEOREPAIR consists of four stages: In (1) video evaluation, we detect misalignments by generating fine-grained evaluation questions and answering those questions with MLLM. In (2) refinement planning, we identify accurately generated objects and then create localized prompts to refine other areas in the video. Next, in (3) region decomposition, we segment the correctly generated area using a combined grounding module. We regenerate the video by adjusting the misaligned regions while preserving the correct regions in (4) localized refinement. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VIDEOREPAIR substantially outperforms recent baselines across various text-video alignment metrics. We provide a comprehensive analysis of VIDEOREPAIR components and qualitative examples.", "sections": [{"title": "1. Introduction", "content": "Recent text-to-video (T2V) diffusion models [3, 8, 11, 14, 41, 47, 54] have shown impressive photorealism and versatility across diverse domains. However, these models often struggle to generate videos that accurately follow text prompts, particularly when the prompt includes multiple objects and attributes, such as incorrect number of objects or attribute bindings (see Fig. 3). Such misalignment problems largely discourage practical applications.\nSeveral recent work has studied enhancing text alignments of diffusion models via training-free iterative refinements [29, 52]. Prompt optimization [29] iteratively searches for better prompts, by creating multiple variations of prompts with LLM and choosing videos with the highest score (e.g., DSG [6]), as described in Fig. 2 (a). However, as there is no explicit feedback for misalignment, such method is sensitive to initial noise [1, 30, 37, 45] and thus requires many iterations (e.g., 30) to reach a prompt that improves alignment. In a different approach, SLD [52] proposes a refinement framework with more explicit guidance, as described in Fig. 2 (b). SLD first generates a bounding-box level plan with an LLM, then runs a set of refinement operations (e.g., object addition, deletion, reposition) following the plan. However, the object addition requires an external layout-guided object"}, {"title": "2. Related Works", "content": "Text-to-video generation with diffusion models. Text-to-video (T2V) diffusion models [3, 8, 11, 12, 14, 26, 41, 47, 49, 51, 54, 56] aim to produce videos describing given text prompts. These methods train a denoising model that can gradually generate clear videos from noisy videos, where the noises are added via diffusion process [10]. The denoising is commonly performed in a compact latent space of an autoencoder [39] for computational efficiency. VideoCrafter2 [4] synthesizes low-quality videos with high-quality images through a joint training design of spatial and temporal modules, obtaining high-quality videos. T2V-turbo [18] presents a distilled video consistency model [42, 48] for improved and rapid video generation. A line of recent work also studies LLM-guided planning frameworks, where an LLM first generates an overall plan (e.g., list of bounding boxes) then video diffusion models render the scene following the plan [21, 22, 27]. However, even the recent T2V diffusion models suffer from misalignment problems. In the following, we discuss the research direction of refining the image/video diffusion models, including VIDEOREPAIR.\nAutomatic refinement for image/video diffusion models. Recent works propose refinement frameworks that automatically improve diffusion models' text alignment [17, 29, 43, 52]. A line of work studies training-based refinement, where they detect errors of a diffusion model, generate training data, and then finetune the model to improve alignment [17, 43]. However, training-based methods are expensive and can often make the model overfit to specific domains of generated training data. Another line of work proposes training-free refinement [29, 52]. OPT2I [29] presents iterative prompt optimization, where an LLM provides various variations of text prompts, T2I diffusion models generate images from the prompts, and the images are ranked with a T2I alignment score (e.g., DSG [6]) to provide the final image. Since no explicit feedback is given to the backbone generation model, it usually takes long iterations (e.g., 30 LLM calls) to find a prompt that provides improved alignment, making the framework expensive to use in practice. SLD [52] proposes a refinement framework with more explicit guidance,"}, {"title": "3. VIDEOREPAIR: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement", "content": "We propose VIDEOREPAIR, a novel automatic refinement framework for T2V generation. VIDEOREPAIR is designed with three key questions in mind: (i) which prompt elements are misaligned in the video?; (ii) which objects are valuable to be preserved?; (iii) which video areas should be modified?; (iv) how can we update the video? In VIDEOREPAIR, we explicitly detect, localize, and address the fine-grained errors of T2V generation models in a four-stage process: (1) video evaluation (Sec. 3.1), (2) refinement planning (Sec. 3.2), (3) region decomposition (Sec. 3.3), and (4) localized refinement (Sec. 3.4). Below, we describe the problem definition and details of each stage."}, {"title": "3.1. Video Evaluation with Misalignment Detection", "content": "Generating evaluation questions. Our primary goal is to generate a video that achieves improved alignment with text prompts, using a pre-trained T2V diffusion model without requiring additional fine-tuning. Given an input text prompt p and initial noise $\\epsilon_0 \\sim \\mathcal{N}(0, I)$, we first generate an initial video $V_0 = f(p, \\epsilon_0)$. To identify which elements of the prompt are misaligned with the generated video, we create a list of evaluation questions designed to be answered with \"yes\" or \"no.\" Specifically, we generate object-centric questions by providing manually written in-context examples to the LLM.\nFollowing DSG [6], we first define semantic categories consisting of entities, attributes, and relationships. Each element is represented as a semantic tuple $\\mathcal{T}$: attributes are expressed in 2-tuples (entity, its attribute, (e.g., {bed, blue}), and relationships are in 3-tuples (subject entity, object entity, and their relationship, (e.g., {people, pizza, make}). Based on $\\mathcal{T}$, which covers all scene-relevant information, we generate questions $\\mathcal{Q}$ using the LLM (e.g., GPT-4).\nNote that although DSG includes \"count\" questions, they are only generated when there is more than one object class; i.e., there is no penalty about object counts when there is a single object in the prompt. For example, given a prompt 'there is a bear', DSG only generates an evaluation question"}, {"title": "3.2. Refinement Planning: what to keep and refine", "content": "Identifying visual content to be retained. As mentioned earlier, the initially generated video may suffer from insufficient text-video alignment due to incorrect or distorted generation of objects and their attributes. However, this does not mean that all components are mis-generated; some key concepts or objects may have been generated precisely in certain areas. VIDEOREPAIR aims to retain these accurately generated portions while focusing on correcting only the mis-generated regions to ensure improved text-video alignment. To this end, we first identify the key object $O^*$ and determine the number of its instances to be preserved. First, to select which object (i.e., object name) should be retained, we prompt GPT-40 with question-answer pairs and the initial video as input, allowing it to identify the most suitable object to preserve. Next, to determine the number of instances of $O^*$ to retain, we define the count of $O^*$ as $N^*$ based on the previous triplet $A^{O} = \\{b_{O^*}, n_{O^*}, n'_{O^*}\\}$ as follows:\n\n$N^* = \\begin{cases} n_{O^*} \\quad \\text{if } n'_{O^*} < n_{O^*} \\\\ n'_{O^*} \\quad \\text{otherwise,} \\end{cases}$ (1)\n\nwhere $n'_{O^*} < n_{O^*}$ indicates the need to remove excess instances of the object, and $n'_{O^*} > n_{O^*}$ suggests that additional instances are required. For example, in step 2 of Fig. 4, if $O^*$ represents a bear with $n_{O^*} = 1$ and $n'_{O^*} = 2$, we set $N^* = 1$, preserving only one bear because there is an extra bear in the generated content ($n'_{O^*} <n_{O^*}$), requiring the deletion of an extra instance. This approach effectively removes the additional instance, ensuring accurate object representation.\nPrompt regeneration for regions requiring refinement. We additionally generate a local prompt for refinement to enable distinct control over different regions during generation. To this end, we prompt an LLM to produce a refinement-oriented prompt, $p_r$, based on $\\mathcal{Q}$ but excluding any questions related to $O^*$. As illustrated in Fig. 4, steps 2 and 4, this regenerated local prompt will be used to guide the denoising process for specific areas to be refined during video generation in a later stage (will be discussed in Sec. 3.4)."}, {"title": "3.3. Region Decomposition", "content": "Given the errors identified in earlier stages, we localize the video regions corresponding to the errors to create concrete guidance in the following refinement stage. Similarly, SLD [52] uses an open-vocabulary object detector to detect localization errors through bounding boxes. However, unlike T2I generation, T2V generation often introduces complex distortions (e.g., attribute mixing) across scenes, making it difficult to capture all cases of distortion using only a model-based detector. Additionally, segmenting each object area with separate bounding boxes complicates interactions between objects in the scene. To address these challenges, we propose an alternative approach: identifying and retaining correctly generated objects while regenerating misaligned objects in the remaining areas.\nTo localize the correctly generated $O^*$ area, we generate a local mask for the area to be refined using Molmo [7], a VLM that can localize specific objects by referring 2d points in an image given a text-pointing prompt, and Semantic-SAM [15]. As illustrated in step 3 of Fig. 4, we first construct a pointing prompt $p_p$ using predefined $N^*$ and $O^*$, \u201cPoint $\\{N^*\\} \\{O^*\\}$\u201d (e.g., \u201cPoint 1 bear\u201d) and obtain the referring 2D coordinates for $O^*$ with Molmo. Then, using this point as input, we employ Semantic-SAM to segment the specified area and finally represent it as a binary segmentation mask $M \\in \\mathbb{R}^{H \\times W}$, where H and W denotes the height and width of the video frame, respectively."}, {"title": "3.4. Localized Refinement", "content": "Localized noise re-initialization. At this stage, we refine the video to achieve a more accurately aligned output video. Inspired by region-based text-to-image generation framework [2, 19], we adopt a mask-based segmentation approach to control specific regions within the video generation process. Given the region to refine $M$ (obtained in Sec. 3.3), we introduce a selective noise re-sampling process to enable controlled regeneration of specific regions. Specifically, different from MultiDiffusion [2], we preserve the partial region of the initial noise map $\\epsilon_0$ detected by $M$ and re-initialize the rest areas with a newly sampled noise $\\epsilon_\\delta \\sim \\mathcal{N}(0, I)$ to expect different generation tendency from initial video. This aims to keep the masked areas in the video consistent while allowing the unmasked areas to be refined based on the updated prompt.\nTo process the pixel-level mask $M$ in the latent space, we transform $M$ from pixel space to latent space through block averaging (i.e., pooling). Specifically, we take the mean of each $H/c \\times W/c$ submatrix within $M$, where $c$ denotes the downsampling scale factor, effectively reducing the spatial resolution of the mask. We apply this transformed mask consistently across the entire temporal domain. The combined noise map $\\epsilon$ is then computed as follows:\n$\\epsilon = (\\epsilon_0 \\odot pool(M, c)) + (\\epsilon_\\delta \\odot (1 \u2013 pool(M, c))),$ (2)\nwhere $pool(\\cdot, c): \\mathbb{R}^{H \\times W} \\rightarrow \\mathbb{R}^{\\mid H/c\\mid \\times \\mid W/c \\mid}$ denotes the block averaging (i.e., pooling) operation with scale factor $c$, and $\\odot$ represents element-wise multiplication, which preserves the initial noise map structure in the masked regions. Using this hybrid noise map $\\epsilon$ as input to a frozen video diffusion model with corresponding localized prompts, we achieve targeted refinement within designated regions, resulting in high-fidelity video with controlled updates that align with the intended modifications.\nLocalized text guidance. We apply distinct text prompts to regions based on their noise re-initialization status, using $1 \u2013 M$ for re-initialized areas and $M$ for preserved regions. For the re-initialized regions, we guide generation in the"}, {"title": "4. Experiments", "content": "We compare VIDEOREPAIR and recent refinement methods on different text-to-video generation benchmarks. Below we provide the experiment setups (Sec. 4.1), quantitative evaluation with baselines (Sec. 4.2), qualitative examples (Sec. 4.3), and additional analysis of VIDEOREPAIR components (Sec. 4.4)."}, {"title": "4.1. Experiment Setups", "content": "Benchmarks and evaluation metrics. We adopt two text-to-video generation benchmarks: EvalCrafter [25] and T2V-CompBench [44], which extensively evaluate text-to-video alignment with various types of prompts. For EvalCrafter, we split prompts by attributes according to their official metadata.\u00b9 In our experiments, we use 'count', 'color', and 'ac-\n\u00b9https://github.com/evalcrafter/EvalCrafter/\nblob/master/metadata.json"}, {"title": "4.2. Quantitative Results", "content": "EvalCrafter: VIDEOREPAIR improves T2V alignments, outperforming existing refinement methods. Tab. 1 shows the evaluation results on EvalCrafter, measured with text-video alignment and visual quality. We observe that SLD and OPT2I only improve the baseline diffusion backbones minimally or even hurt the performance. Specifically, SLD SLD significantly deteriorates the alignment of action and count categories when applied to VideoCrafter2. We expect that this is because SLD is designed to merge denoised latent features of each object for each frame, resulting in challenges in maintaining consistent object counts and locations for video generation tasks. OPT2I improves the alignment on action category by extensively searching for optimized prompts through sequential processes of prompt candidate generation, ranking via DSG, and selection. However, its overall improvements are limited as it cannot provide fine-grained localized guidance beyond the text space.\nAlternatively, VIDEOREPAIR surpasses all baselines in the text-video alignment metric (evaluated using CLIP, BLIP2, and SAM-Track) across all four splits by a significant margin, achieving relative improvements of +2.87% and +11.09% over VideoCrafter2 and T2V-turbo on initial"}, {"title": "4.3. Qualitative Results", "content": "Fig. 5 visualizes videos generated by the original T2V-turbo and refinement frameworks (OPT2I, SLD, and VIDEOREPAIR) on T2V-turbo. These examples clearly illustrate how effectively VIDEOREPAIR addresses object and attribute misalignment issues (as discussed in Fig. 3) compared to T2V-turbo and other refinement methods. In the leftmost example, VIDEOREPAIR precisely generates the specified color attribute (blue apple), while other methods incorrectly produce pink apples blended with the pink tree. In the middle example, VIDEOREPAIR improves on T2V-turbo by accurately generating three distinct three dogs, whereas other baselines either fail to do (OPT2I) so or introduce artificial distortions (SLD). In the rightmost example, VIDEOREPAIR successfully captures spatial relationships among different objects (sandcastle on the left of a beach umbrella) without compromising multi-object generation.\nIn addition, we validate the potential of VIDEOREPAIR for iterative refinement. While a single refinement step of VIDEOREPAIR may not fully achieve precise alignment with the initial prompt, we explore an iterative refinement process to progressively enhance alignment and address any residual discrepancies. As shown in Fig. 6, the initial refinement process partially resolves misalignments between the video and the prompt (generating a scene depicting a night of camping under the stars) but misses the family. but omits the presence of the family. Through subsequent refinement iterations, VIDEOREPAIR successfully achieves precise alignment with the text prompt. Similarly, the example at the bottom of Fig. 6 demonstrates the generation of seven cute puppies after iterative refinements.\nPlease also see the appendix for additional qualitative"}, {"title": "T2V-Compbench: VIDEOREPAIR improves T2V alignments, also outperforming strong T2V baselines.", "content": "Tab. 2 shows the evaluation results on T2V-Compbench's three dimensions: consistent attribute binding, spatial relationship, and generative numeracy. We observe that VIDEOREPAIR improves initial videos from both T2V models (VideoCrafter2 and T2V-turbo) in all three splits, achieving relative improvements of +8.76% and +5.40%, respectively. While SLD with T2V-turbo demonstrates strong per-"}, {"title": "4.4. Additional Analysis", "content": "VIDEOREPAIR components. We compare different components of VIDEOREPAIR using T2V-turbo backbone, on three splits (Count+Color+Action) of EvalCrafter. For evaluation questions (Sec. 3.1), we compare the original DSG question and our modified DSGObj. For the selection of the most prominent object O* (Sec. 3.2/Sec. 3.3), we compare selecting via GPT-4o to randomly selecting from all objects from the DSGObj semantic tuples. For scoring methods for video ranking (Sec. 3.4), we compare DSGObj with CLIPScore [9] and BLIP-BLEU [16, 34], which are parts of metrics used in EvalCrafter. Tab. 3 shows that the combination of DSGObj for evaluation question, GPT-4o for object selection, and DSGObj for video ranking achieves the best performance overall. We use these components for the default setting of VIDEOREPAIR.\nAblations on # Videos candidates. We quantitatively analyze the impact of the video ranking in VIDEOREPAIR using a subset of EvalCrafter (Count, Color, and Action categories). As shown in Tab. 4, VIDEOREPAIR already obtain superior performance without video ranking (i.e., K = 1), compared to strong baselines (LLM paraphrasing: 44.7, SLD: 44.5, OPT2I: 45.7 in average), highlighting the effectiveness of VIDEOREPAIR refinement process. Furthermore, we enhance text-video alignment by incorporating video ranking based on DSGObj.\nImpact of Iterative Refinement. We experiment with iteratively performing VIDEOREPAIR to further improve the text-video alignments. We monitor the DSGObj score and terminate the iterative refinement when the DSGObj reaches 1.0 (max score), and use video ranking with K=5 candidates. As illustrated in Fig. 7, iterative refinement benefits all three prompt splits (count / color / action) of EvalCrafter. Additional iterative refinement examples are provided in Fig. 17."}, {"title": "5. Conclusion", "content": "We introduce VIDEOREPAIR, a novel model-agnostic, training-free video refinement framework that automatically identifies fine-grained text-video misalignments and generates explicit spatial and textual feedback, enabling a T2V diffusion model to perform targeted, localized refinements. VIDEOREPAIR consists of four stages: In (1) video evaluation, we detect misalignments by generating evaluation questions and answering the questions with MLLM. In (2) refinement planning, we identify accurately generated objects and create a local prompt for refinement. In (3) region decomposition, we segment areas in a video to preserve and refine using a combined grounding module. Finally, in (4) localized refinement, we regenerate the video by adjusting the misaligned regions while preserving the correct regions. On two popular video generation benchmarks (EvalCrafter and T2V-CompBench), VIDEOREPAIR substantially outperforms recent baselines across various text-video alignment metrics. We provide comprehensive analysis on VIDEOREPAIR components. We hope that our work encourages future advancements in automatic refinement frameworks in visual generation tasks."}, {"title": "A. VIDEOREPAIR Implementation Details", "content": "For generating DSGObj, we follow DSG [6] mainly but revise in-context examples. Given the limitation of DSG as described in Fig. 8, we change all \u2018entity-whole' tuples of DSG to 'count' attribute tuples to capture the exact number of objects."}, {"title": "A.2. Visual Question Answering", "content": "To evaluate the generated videos, we utilize GPT-4o to answer both count-related (Q) and attribute-related (Q) questions, as illustrated in Fig. 20. For Q prompts, we guide GPT-40 through four steps: reasoning, answering, counting the predicted number of objects (ng), and verifying the true count (no). These steps yield an answer triplet A = {b, n, n}. To ensure valid responses, we account for dependencies among questions, following the methodology of DSG [6]. Each question is posed to GPT-4o sequentially, and a DSG score is calculated after processing all VQA tasks. This DSG score determines whether the VIDEOREPAIR process should continue. If the DSG score reaches"}, {"title": "A.3. Key Object Extraction", "content": "To extract the key concept O* from initial videos Vo, we provide the first frame of Vo and the list of question-answer pairs for each object to GPT4o as shown in Fig. 21. Here, we prioritize selecting objects with a higher number of 1.0 scores. Moreover, we force GPT4o to select 'object' instead of 'background' elements to improve the accuracy of region decomposition by pointing."}, {"title": "A.4. Refinement Prompt Generation", "content": "To produce a refinement prompt p\", we use GPT4 with instruction as shown in Fig. 22. After getting O*, we can decompose the whole question set Q as Q and others depending on whether the O* keyword is included in the question. To generate p from specific question sets, we utilize five manually crafted in-context examples to ensure the accuracy of the generation process. If the DSG score is 0.0 (indicating a complete failure from VQA) and the key object O* cannot be identified, we consider the T2V model to have failed in generating any object correctly. In such cases, we paraphrase Q directly into p using a large language model (LLM).\""}, {"title": "B. Additional Baseline Details", "content": "LLM Paraphrasing. Following [29], we compare VIDEOREPAIR with paraphrasing prompts from LLM. Here, we ask GPT4 to generate diverse paraphrases of each prompt, without any context about the consistency of the images generated from it. The prompt used to obtain paraphrases is provided in Fig. 23.\nOPT2I. Since OPT2I [29] aims to improve text-image consistency for T2I models, we reimplement OPT2I for T2V setup. Specifically, we replace the original T2I model part with T2V models (T2V-Turbo and VideoCrafter2) to generate outputs. Using GPT-40, we then pose DSG questions to these outputs. For prompts, we directly adopt the ones provided in the original OPT2I paper. For LLM, we use GPT4 as VIDEOREPAIR. Finally, we perform iterative refinement, running 10 iterations for T2V-Turbo and 5 iterations for VideoCrafter2, with five video candidates per iteration.\nSLD. To adapt SLD [52] to the T2V setup, we apply their official code to individual video frames and maintain their default setup. Note that SLD is a GLIGEN [19]-based T2I model, which poses challenges for direct extension to video generation. Since SLD operates using DDIM inversion, we use the initial videos generated by T2V-Turbo and VideoCrafter2 as inputs, enabling the implementation of"}, {"title": "C. Additional Evaluation Details", "content": "EvalCrafter. To evaluate the effectiveness of VIDEOREPAIR across different prompt dimensions, we decompose EvalCrafter [25] using the official metadata.json. Specifically, we utilize the attributes key for each prompt and categorize the dataset into 'count', 'color', 'action', 'text', 'face', and 'amp (camera motion)'. Prompts without explicit attributes are grouped into an 'others' category. Among these dimensions, we focus on 'count', 'color', 'action', and 'others', excluding 'text', 'face', and 'amp'. This decision is based on our observation that video errors related to text prompts (e.g., \"the words \u2018KEEP OFF THE GRASS\"\"), face prompts (e.g., \u201cKanye West eating spaghetti\u201d), and amp prompts (e.g., \"A Vietnam map, large motion\u201d) cannot be reliably detected through GPT-40 question-answering, therefore hard to proceed VIDEOREPAIR.\nFor evaluation metrics, we mainly adopt the average text-video alignment score they proposed. Among their all text-video alignment scores (CLIP-Score, SD-Score, BLIP-BLEU, Detection-Score, Count-Score, Color-Score, Celebrity ID Score, and OCR-Score) we exclude Celebrity ID Score and OCR-Score since they are related to 'face' and 'text' categories. Therefore, we calculate text-video alignment score as Avg(CLIP-Score, SD-Score, BLIP-BLEU, DetectionScore, CountScore, ColorScore). For overall video quality, we directly adopt their metrics including Inception Score [40] and Video Quality Assessment (VQAA, VQAT) [50].\nT2V-Compbench. Since VIDEOREPAIR has strength in compositional generation, we adopt T2V-Compbench [44] and evaluate three dimensions: spatial relationships, generative numeracy, and consistent attribute binding. 'Spatial relationships' requires the model to generate at least two objects while maintaining accurate spatial relationships (e.g. 'to the left of', 'to the right of', 'above', 'below', 'in front of') throughout the dynamic video. 'Generative numeracy' specifies one or two object types, with quantities ranging from one to eight. 'Consistent attribute binding' contains color, shape, and texture attributes among two objects.\nFollowing [44], we adopt Video LLM-based metrics for consistent attribute binding and detection-based metrics for spatial Relationships and numeracy.\""}, {"title": "D. Additional Quantitative Analysis", "content": "In this section, we present additional quantitative results to provide a deeper understanding. Specifically, we demonstrate that VIDEOREPAIR achieves superior efficiency in"}, {"title": "D.1. Inference Time", "content": "To validate the efficiency of VIDEOREPAIR, we compare its inference time against other baselines. The evaluation is conducted using a single NVIDIA A100 80GB GPU on the 'count' section of EvalCrafter. We report the average inference time per video, the total inference time for 50 videos, and the text-video alignment score. As shown in Table 5, VIDEOREPAIR demonstrates the highest efficiency among all baselines while also achieving superior text-video alignment scores. Notably, even with just one iteration, VIDEOREPAIR can refine a single video in only 59 seconds."}, {"title": "D.2. Increasing # of Video Candidates", "content": "To evaluate the impact of video ranking, we vary the number of video candidates as K = 1, 5, 10, and 20 during the ranking process. The variation among video candidates arises from different random seeds used to initialize 6. For example, video ranking is not applied when K = 1, and only one refinement is produced using a single random seed noise 6. For ranking metrics, we rely on DSGObj across all ablation studies. As depicted in Fig. 9, higher K values (5, 10, and 20) consistently yield higher scores across all categories than K = 1. This trend is particularly prominent in"}, {"title": "E. Additional Qualitative Examples", "content": "We present additional qualitative comparisons with baseline methods (OPT2I [29], SLD [52], and Vico [53]) in Figs. 10 to 16. These examples address a variety of failure cases commonly observed in T2V models, including inaccuracies in object count and attribute depiction, as highlighted in our main paper. Figs. 10 to 13 correspond to results from T2V-Turbo, while Figs. 14 to 16 showcase examples from VideoCrafter2. Additionally, we provide binary segmentation masks that identify preserved areas (in black) and updated areas (in white).\nAcross these examples, VIDEOREPAIR effectively preserves the O* areas while refining the remaining regions using p. For instance, in Fig. 10, the camel from the original T2V-Turbo video is preserved, and a snowman is successfully added. In contrast, while SLD also leverages DDIM inversion to preserve objects, it often fails to integrate new objects seamlessly."}, {"title": "E.2. Iterative Refinement", "content": "We also demonstrate the results of iterative refinement in Fig. 17, showing the initial video alongside the first and second refinements generated from T2V-Turbo. Overall, VIDEOREPAIR progressively enhances text-video alignment with each refinement step.\nFor numeracy-related cases (e.g., six dancers and five cows), VIDEOREPAIR iteratively adds or removes specific objects, ensuring alignment with the given prompts. In cases of missing objects (e.g., biologists and ducks), VIDEOREPAIR successfully generates additional biologists and multiple ducks while preserving the context of the initial video. Additionally, for attribute-related prompts (e.g., yellow umbrella and blue cup), VIDEOREPAIR effectively refines object attributes, such as adding a wooden handle to the umbrella and enhancing the cup's blue color. These results demonstrate the ability of VIDEOREPAIR to iteratively improve both object count and attribute alignment with high fidelity."}]}