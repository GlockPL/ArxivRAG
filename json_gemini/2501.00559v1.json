{"title": "AraSTEM: A Native Arabic Multiple Choice Question Benchmark\nfor Evaluating LLMs Knowledge In STEM Subjects", "authors": ["Ahmad Mustapha", "Hadi Al-Khansa", "Hadi Al-Mubasher", "Aya Mourad", "Ranam Hamoud", "Hasan El-Husseini", "Marwah Al-Sakkaf", "Mariette Awad"], "abstract": "Large Language Models (LLMs) have shown remark-\nable capabilities, not only in generating human-like\ntext, but also in acquiring knowledge. This high-\nlights the need to go beyond the typical Natural Lan-\nguage Processing downstream benchmarks and asses\nthe various aspects of LLMs including knowledge and\nreasoning. Numerous benchmarks have been devel-\noped to evaluate LLMs knowledge, but they predom-\ninantly focus on the English language. Given that\nmany LLMs are multilingual, relying solely on bench-\nmarking English knowledge is insufficient. To ad-\ndress this issue, we introduce AraSTEM, a new Ara-\nbic multiple-choice question dataset aimed at evaluat-\ning LLMs knowledge in STEM subjects. The dataset\nspans a range of topics at different levels which re-\nquires models to demonstrate a deep understanding\nof scientific Arabic in order to achieve high accuracy.\nOur findings show that publicly available models of\nvarying sizes struggle with this dataset, and under-\nscores the need for more localized language models.\nThe dataset is freely accessible on Hugging Face.", "sections": [{"title": "Introduction", "content": "Language models are traditionally evaluated on var-\nious Natural Language Processing (NLP) tasks such\nas text generation, sentiment analysis, translation,\nand Part of Speech (POS) tagging, using metrics like\nthe BLEU score. However, with the advent of Large\nLanguage Models (LLMs), these metrics and bench-\nmarks have become outdated. Unlike conventional\nlanguage models, LLMs exhibit zero-shot and few-\nshot learning abilities, enabling them to become bet-\nter across many existing benchmarks. These mod-\nels have demonstrated the ability to perform tasks\nthat require not only language comprehension but\nalso knowledge acquisition and reasoning. In essence,\nLLMs function as multi-faceted agents, and their ca-\npabilities must be assessed from multiple angles, in-\ncluding knowledge, reasoning, and alignment with\nhuman preferences.\nWhile numerous benchmarks have been proposed,\nthe majority of them is English-centric, despite the\nfact that several open-source and proprietary mod-\nels claim robust multilingual capabilities. To make\nthese models more accessible for various languages,\nit is essential to create language-specific benchmarks\nthat evaluate their performance in these languages.\nOne language that lacks sufficient and comprehensive\nevaluation benchmarks is Arabic. Arabic ranks fifth\nin the world's league table of languages, with over 200"}, {"title": "Related Work", "content": "Because LLMs have shown superior performance on\nmany conventional NLP benchmarks, many proposed\ncontemporary benchmarks that are more challenging\nfor LLMs. Hendrycks [4] developed the Massive Mul-\ntitask Language Understanding (MMLU) benchmark\nwhich tests against world knowledge along 57 differ-\nent topics. Zellers [5] on the other hand produced a\nbenchmark that tests human common sense by gener-\nating entries using adversarial filtering which makes\nthem challenging for models but easy for humans.\nSakaguchi [6] compiled WinoGrande which is a scaled"}, {"title": "The Dataset", "content": "The AraSTEM dataset contains 11,637 multiple-\nchoice questions covering a wide range of knowledge\nareas. It includes questions from elementary and sec-\nondary level math and science, as well as advanced\ntopics in biology, physics, and medicine. The dataset\nwas compiled from various sources, and in response\nto the issues recently raised about data traceability\nand governance [22], we opted to cite the source for\neach individual question."}, {"title": "Data Sources and Data Collection", "content": "The data was collected from multiple sources, each\nemploying a distinct collection process. Scraping. We scraped two publicly accessible\nMCQ websites, beadaya.com and alloschool.com, us-\ning Python scripts with libraries like BeautifulSoup\nand Requests. Many of the questions collected from\nthese sites included images and mathematical equa-\ntions, which were removed during the cleaning pro-\ncess. To maintain traceability, each question from\nthese sources includes a link in the dataset that di-\nrects to its original source. The scraped questions\ncover a range of topics, from simple science questions\nfor primary school students to more advanced physics\nand chemistry questions at the secondary school level.\nManual Extraction. A portion of the questions\nwas manually collected from two reference books that\ncontain thousands of questions and answers in biology\nand chemistry. While the majority of these questions\nwere not originally in multiple-choice format, we were\nable to extract some MCQs. Moreover, we gener-\nated additional MCQs by transforming other types of\nquestions. For example, \"definition\" questions were\nturned into MCQs by rearranging possible answers,\nand \"fill in the blanks\" questions were similarly con-\nverted by offering multiple answer options, including\nthe correct one. To ensure traceability, each entry in\nthe dataset includes a link to a public version of the\nbook. Additionally, several publicly available MCQs\nwere extracted manually by the authors from various\nonline sources, primarily featuring college-level ques-"}, {"title": "Data Characteristics", "content": "Distribution Per Level and Subject. The ques-\ntions in the dataset are classified into three ed-\nucational levels: primary, secondary, and college.\nAnd it encompasses a broad array of subjects, in-\ncluding math, science, physics, chemistry, biology,\ninformation technology, dentistry, pharmacy, and\nmedicine. The \"science\" category pertains specifi-\ncally to primary-level questions that includes physics,\nchemistry, and biology but was labeled as \"science\"\nfrom the source. Distribution Per Number of Options. The\nnumber of options per question varies. Some ques-\ntions provide four options, while others offer only two.\nWord Count Distribution. An important as-\npect to note is the length of the questions. To evalu-\nate this, we calculated the word count for each ques-"}, {"title": "Experiments", "content": "To evaluate the difficulty of the dataset, we tested\nthe performance of several language models. These\nmodels ranged from smaller ones with around 500\nmillion parameters to medium-sized models with 7\nbillion parameters, and up to larger models with 30\nto 40 billion parameters."}, {"title": "Prompt Engineering", "content": "We designed the prompt based on recent research\nfindings. First, we wrote the prompt in English,\nfollowing Koto [15], who found that using English\nfor the body of the prompt - even for Arabic ques-\ntions - improves performance across multiple models.\nSecond, we applied Chain-of-Thought (CoT) prompt-\ning, as suggested by the latest research [25]. In this\napproach, the model is instructed to carefully ana-\nlyze the question, identify the key concepts related\nto the subject, and then proceed to answer. The ex-"}, {"title": "Zero Shot Performance", "content": "Table 5 presents the results of this experimental\nsetup. The best-performing model, on average, is\nJais 30B Chat, achieving an accuracy of 56%, fol-\nlowed closely by the Jais 13B chat and Llama 3.1 8B\ninstruct models. This high score highlights the chal-\nlenging nature of our dataset.\nIn the table, we bolded the top four accuracies per\nsubject for each model group. A clear trend emerges:\nJais, Llama 3.1, Bloomz, and AceGPT consistently\nachieve the highest accuracies across subjects, posi-\ntioning them as the top-performing group on aver-\nage. This pattern is notable, as Bloomz and AceGPT\nmodels outperform popular models like Llama 2 and\nFalcon. The results suggest the value of incorporat-\ning substantial Arabic text in the models' training\ndatasets; indeed, the top performers have all cited\nArabic as part of their training data except for Lama\n3.1. Specifically, Jais reported that 28% of its train-"}, {"title": "Models' Characteristics", "content": "In this section, we explored how model characteristics\naffect performance on the AraSTEM dataset. Specif-\nically, we examined two key characteristics: model\nsize and whether the model was fine-tuned for in-\nstruction following. The figure reveals that three models - Jais, Bloomz,\nand AceGPT - only experience a performance boost\nwhen their model size increases. As mentioned in sec-\ntion 4.2, these models have incorporated Arabic into\ntheir training datasets [1, 3, 26]. In contrast, other\nmodels like XGLM, Llama 2, and Falcon do not show\na significant improvement in performance as their size\nincreases. It's worth noting that Llama 3.1 achieved\na high score despite its relatively smaller size. We hy-\npothesize that this success is likely due to the model\nbeing trained on a substantial amount of Arabic to-\nkens, even though this is not explicitly stated in the\nmodel's paper [27].\nThe trend presented in the figure can be explained\nby the well-known scaling law for large language mod-\nels, known as the Chinchilla law [28], which suggests\nthat for optimal performance, we should scale the\ndataset size alongside the model size. In our case,\nthis would mean scaling the inclusion of Arabic in\nthe training data to see better results as the model\nsize grows."}, {"title": "Models' Calibration", "content": "In this section we study the models' confidence\nagainst its accuracy. To investigate, we computed\ncalibration plots for the four top-performing mod-\nels: Jais, Llama 3.1, Bloomz, and AceGPT. The results show that for\npredictions with high confidence, the actual accuracy\nfalls below the perfect calibration line for all models\nexcept for Llama 3.1. This indicates that the models\ntend to be overconfident in their predictions, high-\nlighting the need for calibration to better match their\nconfidence levels with their true performance. On the\nother hand, Llama 3.1 shows a calibrated behavior as\nit follows the perfect calibration line for predictions\nwith high confidence."}, {"title": "Analysis of Hard Failures", "content": "We analyzed the set of questions that none of the\ntested models could answer correctly. This subset\nis relatively small, consisting of only 193 questions\n(0.16% of the dataset), predominantly in dentistry\nand medicine, with a few questions in math and sci-\nence. This indicates that the models' predictions are\ncomplementary, as each model excels in answering\ndifferent questions.\nTo explore this further, we created an upset plot to\nexamine the overlap in correctly answered questions\namong five models: Jais, Bloomz, AceGPT, Falcon,\nand Llama 3.1."}, {"title": "Analysis of Subject-Wise Under- standing", "content": "Figure 9 illustrates the performance of selected mod-\nels on the AraSTEM dataset, broken down by sub-\nject. The results reveal three distinct performance\nclusters:\nTop-performing models: Jais, Llama 3.1,\nBloomz, and AceGPT compose this cluster, follow-\ning a similar performance pattern. These models\nstruggle with the dentistry dataset but excel in infor-\nmation technology, showing a noticeable performance\nspike. Among them, Jais stands out for its superior\nperformance in the medicine and pharmacy subjects.\nLlama 3.1 slightly beats Jais in math.\nModerate performers: Falcon and Llama 2 be-\nlong to this cluster, also displaying a similar perfor-\nmance trend. Both achieve relatively good results in\nbiology and information technology but generally lag\nbehind the top models.\nLow performers: The final cluster comprises\nmodels that perform close to random guessing, indi-\ncating significant challenges in handling the dataset.\nThese patterns highlight the strengths and limi-\ntations of current models across various STEM sub-\njects."}, {"title": "Conclusion", "content": "To address the lack of benchmarks for evaluating\nLLM knowledge in Arabic, we introduced in this\npaper, the AraSTEM dataset. It comprises 11,637\nmultiple-choice questions across STEM subjects such\nas information technology, math, physics, chemistry,\nbiology, and medicine. AraSTEM has proven to\nbe both diverse and challenging for state-of-the-art\nopen-source Arabic and non-Arabic LLMs. The re-\nsults highlight the importance of training on sub-\nstantial amounts of Arabic text for achieving strong\nperformance on Arabic benchmarks, emphasizing the\nneed for greater localization in selecting training\ndatasets. As a future work we aim to investigate\napplying explainability techniques [29] on best per-\nforming models."}]}