{"title": "Mathematical Models of Computation in Superposition", "authors": ["Kaarel H\u00e4nni", "Jake Mendel", "Dmitry Vaintrob", "Lawrence Chan"], "abstract": "Superposition - when a neural network represents more \"features\" than it has dimensions - seems to pose a serious challenge to mechanistically interpreting current AI systems. Existing theory work studies representational superposition, where superposition is only used when passing information through bottlenecks. In this work, we present mathematical models of computation in superposition, where superposition is actively helpful for efficiently accomplishing the task.\nWe first construct a task of efficiently emulating a circuit that takes the AND of the $\\binom{m}{2}$ pairs of each of $m$ features. We construct a 1-layer MLP that uses superposition to perform this task up to $\\varepsilon$-error, where the network only requires $O(m^3)$ neurons, even when the input features are themselves in superposition. We generalize this construction to arbitrary sparse boolean circuits of low depth, and then construct \"error correction\" layers that allow deep fully-connected networks of width $d$ to emulate circuits of width $\\tilde{O}(d^{1.5})$ and any polynomial depth. We conclude by providing some potential applications of our work for interpreting neural networks that implement computation in superposition.", "sections": [{"title": "1. Introduction", "content": "Mechanistic interpretability seeks to decipher the algorithms utilized by neural networks (Olah et al., 2017; Elhage et al., 2021; R\u00e4uker et al., 2023; Olah et al., 2020; Meng et al., 2023; Geiger et al., 2021; Wang et al., 2022; Conmy et al., 2024). A significant obstacle is that neurons are polysemantic - activating in response to various unrelated inputs (Fusi et al., 2016; Nguyen et al., 2016; Olah et al., 2017; Geva et al., 2021; Goh et al., 2021). As a proposed explanation for polysemanticity, Olah et al. (2020) introduce the 'superposition hypothesis' (see also Arora et al. (2018); Elhage et al. (2022)): the idea that networks represent many more concepts in their activation spaces than they have neurons by sparsely encoding features as nearly orthogonal directions.\nPrevious work has studied how networks can store more features than they have neurons in a range of toy models (Elhage et al., 2022; Scherlis et al., 2022). However, previous models of superposition either involve almost no computation (Elhage et al., 2022) or rely on some part of the computation not happening in superposition (Scherlis et al., 2022). Insofar as neural networks are incentivized to learn as many circuits as possible (Olah et al., 2020), they are likely to compute circuits in the most compressed way possible. Therefore, understanding how networks can undergo more general computation in a fully superpositional way is valuable for understanding the algorithms they learn.\nIn this paper, we lay the groundwork for understanding computation in superposition in general, by studying how neural networks can emulate sparse boolean circuits.\n\u2022 In Section 2, we clarify existing definitions of linearly represented features, and propose our own definition which is more suited for reasoning about computation.\n\u2022 In Section 3, we focus our study on the task of emulating the particular boolean circuit we call the Universal AND (U-AND) circuit. In this task, a neural network must take in a set of boolean features in superposition, and compute the pairwise logical ANDs of these features in a single layer with as few hidden neurons as possible. We present a construction which allows for many more new features to be computed than the number of hidden neurons, with outputs represented natively in superposition. We argue that real neural networks may well implement our construction in the wild by proving that randomly initialised networks are very likely to emulate U-AND.\n\u2022 In Section 4 we demonstrate a second reason why this task is worth studying: it is possible to modify our construction to allow a wide range of large boolean circuits to be emulated entirely in superposition, provided that they satisfy a certain sparsity property.\nWe conclude with a discussion of the limitations of our formal models, including the fact that our results are asymptotic and deal with only boolean features, and provide directions of future work that could address them."}, {"title": "2. Background and setup", "content": "2.1. Notation and conventions\nAsymptotic complexity and $\\tilde{O}$ notation We make extensive use of standard Bachmann-Landau (\u201cbig O\") asymptotic notation. We use $\\tilde{O}$ to indicate that we are ignoring polylogarithmic factors:\n$\\tilde{O}(g(n)) := O(g(n) \\log^k n)$ for some $k \\in \\mathbb{Z}$.\n(And so forth for $\\tilde{E}, \\tilde{\\Omega}$, etc.)\nFully connected neural networks We use $M_w: \\mathcal{X} \\rightarrow \\mathcal{Y}$ to denote a neural network model parameterized by $w$ that takes input $x \\in \\mathcal{X}$ and outputs $M_w(x) \\in \\mathcal{Y}$. In this work, we study fully-connected networks consisting of $L$ MLP layers with ReLU activations:\n$\\begin{aligned} a^{(0)}(x) &= x\\\\ a^{(l)}(x) &= \\text{MLP}^{(l)}(a^{(l-1)}(x))\\\\ &= \\text{ReLU} (W_{\\text{in}}^{(l)} a^{(l-1)}(x) + W_{\\text{bias}}^{(l)})\\\\ M_w(x) &= W_{\\text{out}}a^{(L)},\\end{aligned}$\nwhere $\\text{ReLU}(x) = \\text{max}(0, x)$ with max taken elementwise. We assume that our MLPs have width $d$ for all hidden layers, that is, $a^{(l)} \\in \\mathbb{R}^d$ for all $l \\in \\{1, ..., L\\}$. For simplicity's sake we will be dropping $l$ whenever we only talk about a single layer at a time.\n2\nFeatures and feature vectors Following previous work in mechanistic interpretability (e.g. Tamkin et al. (2023); Rajamanoharan et al. (2024)), we suppose that the activations of a model can be thought of as representing $m > d$ boolean features $f_k: \\mathcal{X} \\rightarrow \\{0, 1\\}$ of the input in superposition. That is,\n$a(x) = \\sum_{i=1}^m \\phi_k f_k(x)$\nfor some set of feature vectors $\\phi_1, ..., \\phi_m \\in \\mathbb{R}^d$ and features $f_1, ..., f_m: \\mathcal{X} \\rightarrow \\{0, 1\\}$. Equivalently,\na^{(1)}(x) = \\Phi b\nwhere $\\Phi = (\\phi_1,..., \\phi_m)$ is the $d \\times m$ feature encoding matrix with columns equal to the feature vectors and $b \\in \\{0, 1\\}^m$ is the boolean vector with entries $b_k = f_k(x)."}, {"title": "2.2. Strong and weak linear representations", "content": "Given the activations of a neural network at a particular layer $a^{(l)}: \\mathcal{X} \\rightarrow \\mathbb{R}^d$, we can also ask what features are linearly represented by $a^{(l)}$. In this section, we present three definitions for a feature being linearly represented by $a^{(l)}$, which we illustrate in Figure 2.\nThe standard definition of linear representation is based on whether or not the representations of positive and negative examples can be separated by a hyperplane:\nDefinition 1 (Weak linear representations). We say that a binary feature $f_k$ is weakly linearly represented by $a: \\mathcal{X} \\rightarrow \\mathbb{R}^d$ (or linearly separable in $a$) if there exists some $r_k \\in \\mathbb{R}^d$ such that for all $x_1, x_2 \\in \\mathcal{X}$ where $f_k(x_1) = 0$ and $f_k(x_2) = 1$, we have:\nr_k a(x_1) < r_k a(x_2).\nOr, equivalently, the sets $\\{x|f_k(x) = 0\\}$ and $\\{x|f_k(x) = 1\\}$ are separated by a hyperplane normal to $r_k$.\nThat being said, features being linearly separable does not mean a neural network can easily \"make use\" of the features. For some weakly linearly represented features $f_1$ and $f_2$, neither $f_1 \\land f_2$ nor $f_2 \\lor f_2$ need to be linearly represented, even if their read-off vectors $r_1, r_2$ are orthogonal (Figure 3). In fact, a stronger statement is true: it might not even be possible to linearly separate $f_1 \\land f_2$ or $f_2 \\lor f_2$ in MLP \\circ a, that is, even after applying an MLP to the activations (see Theorem 9 in Appendix C).\nAs a result, in this paper we make use of a more restrictive notion of a feature being linearly represented:\nDefinition 2 ($\\varepsilon$-linear representations). Let $\\mathcal{X}$ be a set of inputs and $a: \\mathcal{X} \\rightarrow \\mathbb{R}^d$ be the activations of a neural network (in a particular position/layer in a given model). We say that $f_1,..., f_m$ are linearly represented with interference $\\varepsilon$ (or $\\varepsilon$-linearly represented from these activation vectors) if there exists a read-off matrix $R \\in \\text{Mat}_{m \\times d}$ with rows $r_1,...,r_m \\in \\mathbb{R}^d$ such that for all $k \\in \\{1, ..., m\\}$ and all $x \\in \\mathcal{X}$, we have\n|r_k a(x) - f_k(x)| < \\varepsilon.\nWe refer to $r_k$ as a read-off vector for the feature $f_k$. It follows that if $a(x) = \\sum \\phi_k f_k(x)$, then we have:\n||R\\Phi - Id_m|| < \\varepsilon\nwhere $Id_m$ is the $m \\times m$ identity matrix\u00b9.\nFor brevity's sake, we very slightly abuse notation here to include the bias term in $r_k$. This is equivalent to assuming that one of $a$'s outputs is a constant, that is, $a_i(x) = c$ for all $x$ for some $i \\in \\{1, ..., d\\}$ and some $c \\in \\mathbb{R}$.\nIn contrast to features that are merely linearly separable, features that are $\\varepsilon$-linearly represented are easy to linearly separate, as we show in Figure 3. We formalize and prove this in Theorem 10 in Appendix C.\nComparison with Anthropic's Toy Model of Superposition Finally, Elhage et al. (2022) and Bricken et al. (2023) consider a definition of linearly represented feature that involves using a ReLU to remove negative interference:\nDefinition 3 (ReLU-linear representations). A set of $m$ binary features $\\mathcal{F} = (f_1, ..., f_m)$ is ReLU-linearly represented in $a: \\mathcal{X} \\rightarrow \\mathbb{R}^d$ with error $\\varepsilon$ if there exists a read-off matrix $R \\in \\text{Mat}_{m \\times d}$ such that\n$\\mathbb{E}_{x \\in \\mathcal{X}} ||F(x) - \\text{ReLU} (Ra(x)) ||_2 < \\varepsilon$.\n\u00b9In some cases if the feature vectors satisfy $|\\Phi^T\\Phi - Id_m| < \\mu$, that is, if the feature vectors are almost orthogonal with interference $\\mu$, then the features vectors can function as their own readoffs."}, {"title": "3. Universal ANDs: a model of single-layer MLP superposition", "content": "We start by presenting one of the simplest non-trivial boolean circuits: namely, the one-layer circuit that computes the pairwise AND of the input features. Note that due to space limitations, we include only proof sketches in the main body and may ignore some regularity conditions in the theorem statement. See Appendix D for more rigorous theorem statements and proofs.\nDefinition 4 (The universal AND boolean circuit). Let $b \\in \\{0, 1\\}^m$ be a boolean vector. The universal AND (or U-AND) circuit has m inputs and $\\binom{m}{2}$ outputs indexed by unordered pairs $k, l$ of locations and is defined by\nC_{UAND}(b)_{k,l} := b_k \\land b_l.\nIn other words, we apply the AND gate to all possible pairs of distinct inputs to produce $\\binom{m}{2}$ outputs.\nWe will build our theory of computation starting from a single-layer neural net that emulates the universal AND when the input $b$ is $s$-sparse for some $s \\in \\mathbb{N}$ (this implies that the output has sparsity $O(s^2)$).\n3.1. Superposition in MLP activations enables more efficient U-AND\nFirst, consider the naive implementation, where we use one ReLU to implement each AND using the fact that for boolean $x_1, x_2$:\n$\\text{ReLU}(x_1 + x_2 - 1) = x_1 \\land x_2$.\nThis requires $\\binom{m}{2} = O(n^2)$ neurons, each of which is monosemantic in that it represents a single natural feature. In contrast, by using sparsity, we can construct using exponentially fewer neurons (Figure 1):\nTheorem 1 (U-AND with basis-aligned inputs). Fix a sparsity parameter $s \\in \\mathbb{N}$. Then for large input length $m$, there exists a single-layer neural network $M_w(x) = \\text{MLP}(x) = \\text{ReLU}(W_{\\text{in}}x + W_{\\text{bias}})$ that $\\varepsilon$-linearly represents the universal AND circuit $C_{\\text{UAND}}$ on $s$-sparse inputs, with width $d = \\tilde{O}(m^{1/2})$ (i.e. polylogarithmic in $m$).\nProof. (sketch) To show this, we construct an MLP such that each neuron checks whether or not at least two inputs in a small random subset of the boolean input $b$ are active (see also Figure 1). Intuitively, since the inputs are sparse, each neuron can be thought of as checking the ANDs of any pair of input variables $b_{k_1}, b_{k_2}$ in the subset, with interference terms corresponding to all the other variables. That is, we can write the preactivation of each neuron as the sum of the AND of $b_{k_1}, b_{k_2}$ and some interference terms:\n$-1 + b_{k_1} + b_{k_2} + \\sum_{k' \\neq k_1, k_2} b_{k'}$\nb_{k_1} \\land b_{k_2}\ninterference terms\nWe then use the sparsity of inputs to bound the size of the interference terms, and show that we can \"read-off\" the AND of $b_{k_1}, b_{k_2}$ by averaging together the value of post-ReLU activations of the neurons connected to $b_{k_1}, b_{k_2}$. We then argue that this averaging reduces the size of the interference terms to below $\\varepsilon$.\nSpecifically, we construct input weights $W_{\\text{in}} \\in \\text{Mat}_{d \\times m}$ such that the input to each neuron is connected to the $k$th entry of the input $b_k$ with weight 1 with probability $p = \\frac{\\log^2 m}{\\sqrt{d}}$, and weight 0 otherwise. We set the bias of each neuron to -1.\nLet $\\Gamma(k)$ be indices of neurons that have input weight 1 for $b_k$, and $\\Gamma(k_1,k_2)$ be the indices of neurons that have input weight 1 for $b_{k_1}, b_{k_2}$, $\\Gamma(k_1,k_2, k_3)$ be the indices of neurons reading from all of $b_{k_1}, b_{k_2}, b_{k_3}$, and so forth. By construction, $\\Gamma(k_1)$ has expected size $O(\\frac{\\log^2 m}{\\sqrt{d}})$, $\\Gamma(k_1, k_2)$ has expected size $(\\frac{\\log^4 m}{d})$, and $\\Gamma(k_1,k_2,k_3)$ has expected size $O(\\frac{\\log^6 m}{\\sqrt{d^3}})$. In general, the set of indices for n such inputs has expected size $O(\\frac{\\log^{2n} m}{d^{(n/2-1)}})$.\nOur read-off vector $r$ for the AND $b_{k_1} \\land b_{k_2}$ will have entries:\n$\\hat{r}_i = \\begin{cases} \\frac{1}{|\\Gamma(k_1, k_2)|} & i \\in \\Gamma(k_1,k_2)\\\\ 0 & \\text{otherwise} \\end{cases}$\nWe then check that $r \\cdot \\text{MLP}(b)$ gives the correct output in each of three cases. Note that for any input, $r \\cdot \\text{MLP}(b) \\geq b_{k_1}b_{k_2}$, so it suffices to upper bound the average number of non-$k_1, k_2$ inputs that are non-zero, divided by the total number of neurons in $\\Gamma(k_1, k_2)$.\n\u2022 When $b_{k_1} = b_{k_2} = 0$, the interference terms in each read-off neuron have value at most $s$, and there are at most\n$\\sum_{b_{k'} \\neq b_{k''} = 1} |\\Gamma(k_1,k_2, k', k'')| = O(s^2 \\cdot \\frac{\\log^8 m}{d^3})$\nsuch neurons outputting non-zero values. So the error is bounded above by\n$\\frac{s \\cdot \\sum_{k' \\neq k_1, k_2} |\\Gamma(k_1,k_2,k', k'')|}{|\\Gamma(k_1, k_2)|} = O(\\frac{s^3 \\log^4 m}{d \\sqrt{d}})."}, {"title": "3.2. Neural networks can implement efficient U-AND even with inputs in superposition", "content": "Note that in Theorem 1, we assume that the network gets $m$ basis-aligned inputs (that is, not in superposition). However, it turns out that we can extend the result in Theorem 1 to inputs in superposition.\nTheorem 2 (U-AND with inputs in superposition). Let $s \\in \\mathbb{N}$ be a fixed sparsity limit and $\\varepsilon < 1$ a fixed interference parameter. There exists a feature encoding $\\Phi$ and single-layer neural net $M_w(x) = \\text{MLP}(x) = \\text{ReLU}(W_{\\text{in}}x + W_{\\text{bias}})$ with input size and width $d = \\tilde{O}(\\sqrt{m/s^2})$, where $M_w \\circ \\Phi$ $\\varepsilon$-linearly represents $C_{\\text{UAND}}$ on all $s$-sparse inputs $b$.\nProof. (sketch) By picking almost orthogonal unit-norm vectors $\\Phi = (\\phi_1,..., \\phi_m)$, we can recover each feature up to error $\\varepsilon$ using readoffs $R = \\Phi^T$. Take the input weight $W_{\\text{in}} \\in \\text{Mat}_{d \\times m}$ for the MLP constructed in the proof of Theorem 1. Using $W_{\\text{in}}' = W_{\\text{in}}R$ and $w_{\\text{bias}}' = w_{\\text{bias}}$ suffices, as this gives us\n$\\begin{aligned}M_w \\circ \\Phi(b) &= \\text{ReLU}(W_{\\text{in}}R\\Phi b + W_{\\text{bias}})\\\\ &\\approx \\text{ReLU}(W_{\\text{in}}b + W_{\\text{bias}}),\\end{aligned}$\nwhich is just the model from Theorem 1, which $\\varepsilon$-linearly represents $C_{\\text{UAND}}$ as desired. Carefully tracking error terms shows that we need $d = \\tilde{O}(\\sqrt{m})$ neurons."}, {"title": "3.3. Randomly initialized neural networks linearly represent U-AND", "content": "While the results in previous section show that there exist some network weights that $\\varepsilon$-linearly represents the U-AND circuit $C_{\\text{UAND}}$, there still is a question of whether neural networks can learn to represent many ANDs starting from the standard initialization. In this section, we provide some theoretical evidence \u2013 namely, that sufficiently wide randomly initialized one-layer MLPs $\\varepsilon$-linearly represent $C_{\\text{UAND}}$.\nTheorem 3 (Randomly initialized MLPs linearly represent U-AND). Let $\\text{MLP}: \\mathbb{R}^m \\rightarrow \\mathbb{R}^d$ be a one-layer MLP with $d = \\Omega(1/\\varepsilon^2)$ neurons that takes input $b$, and where $W_{\\text{in}}$ is drawn i.i.d from a normal distribution $N(0, \\sigma^2)$ and $W_{\\text{bias}} = 0$. Then this MLP $\\varepsilon$-linearly represents $C_{\\text{UAND}}$ on $s$-sparse inputs outside of negligible probability.\nProof. (Sketch) We prove this by constructing a read-off vector $r$ for each pair of features $k_1, k_2$. Let $\\sigma$ be the sign function\n$\\sigma(x) = \\begin{cases} +1 & x > 0 \\\\ 0 & x = 0 \\\\ -1 & x < 0 \\end{cases}$\nand let $W_{i,k}$ be the contribution to the preactivation of neuron $i$ from $b_k$.\nWe construct $r$ coordinatewise (that is, neuron-by-neuron). In particular, we set the $i$th coordinate of $r$ to be\n$\\hat{r}_i = N_i (\\frac{1{\\sigma(W_{i,k_1}) = \\sigma(W_{i,k_2}) - 1}{\\sigma(W_{i,k_1}) \\neq \\sigma(W_{i,k_2})}}).\nThat is, if $k_1$ and $k_2$ contribute to the neuron preactivations with the same sign, then $r_i = \\eta_i$, else, $r_i = -\\eta_i$. Here, $N_i$ is a scaling parameter of size $\\Theta(\\sqrt{s/d})$ used to scale the read-off to be 1 when $b_{k_1} = b_{k_2} = 1$\nWhen $b_{k_1} = 0$ or $b_{k_2} = 0$, the expected value of $r \\cdot M_w (b)$ is zero, while the error terms have size $\\tilde{O}_m(1/\\sqrt{d})$. So setting $d = \\Omega(1/\\varepsilon^2)$ suffices to get error below $\\varepsilon$ with high probability.\nWhen $b_{k_1} = b_{k_2} = 1$, the contribution from each neuron $i$ to $r \\cdot M_w(b)$ with $\\sigma(W_{i,k_1}) = \\sigma(W_{i,k_2})$ will be in expectation larger than those with $\\sigma(W_{i,k_1}) \\neq \\sigma(W_{i,k_2})$ (as the standard deviation of the sum of two weights with equal signs is larger than the sum of two weights with different signs, and we apply a ReLU). By setting $\\eta$ to be the reciprocal of the difference in expected contributions, we have that this value has expectation 1. Again, as the error terms have size $\\tilde{O}_m(1/\\sqrt{d})$, it follows that setting $d = \\Omega(1/\\varepsilon^2)$ suffices to get error below $\\varepsilon$ with high probability, as desired."}, {"title": "4. MLPs as representing sparse boolean circuits", "content": "In the previous section we showed variants of computation in superposition at a single layer, for one of the simplest non-trivial boolean circuits. In this section, we extend these results to show that neural networks can efficiently represent arbitrary sparse boolean circuits.\nAs in Section 3, we include only proof sketches in the main body due to space limitations, and may also ignore some regularity conditions in our theorem statements. See Appendix D for more rigorous theorem statements and proofs.\n4.1. Boolean circuits in single layer MLPS\nWe start by extending these results from Section 3 to ANDS of more than two variables.\nLet $C_{UAND}^{(n)}$ be the boolean circuit of depth $L = \\log(n)$ that computes the ANDs of each n-tuple of elements in $b$.\nLemma 5 (\"High fan-in\u201d U-AND). For each $n \\in \\mathbb{N}$, there exists a one-layer neural network $M_w = \\text{MLP}: \\mathbb{R}^M \\rightarrow \\mathbb{R}^d$ with width $d = \\tilde{O}(n/\\varepsilon^2)$ such that $M_w(b)$ $\\varepsilon$-linearly represents $C_{\\text{COMAND}}$ on $s$-sparse inputs.\nProof. (sketch) We can extend the construction in the proof of Theorem 1 to allow for ANDs of exactly $n$ variables, by considering index sets $I(k_1,k_2, ..., k_n)$ of $n$ variables, and changing the bias of each neuron from -1 to -n + 1. The expected size of an index set of $n$ variables is $E[I(k_1,k_2,..., k_n)|] = p^n d, and we require this expected value to be (\\log^4m) to ensure that the index set is non-empty outside negligible probability (using the normal Chernoff and Union bounds). Therefore, we have to scale up the probability that any given value in $W_{\\text{in}}$ is 1: $p=\\frac{\\log^2 m}{d^{1/n}}$ suffices. A similar argument to the one found in the proof of Theorem 1 shows that all the interference terms are o(1).\nAs illustrated in Figure 4, Lemma 5 allows us to construct MLPs that $\\varepsilon$-linearly represents arbitrary small circuits:\nTheorem 6. For any $s$-sparse circuit $C$ of width $m$ and depth $L$, there exists a feature encoding $\\Phi \\in \\text{Mat}_{d \\times m}$ and a single-layer neural network $M_w(x) = \\text{ReLU}(W_{\\text{in}}x + W_{\\text{bias}})$ of width $d = \\tilde{O}(\\sqrt{m})$ such that $M_w(\\Phi b)$ $\\varepsilon$-linearly represents $C(b)_k$ for all $k \\in \\{1, ...,m\\}$ for some $\\varepsilon = \\tilde{O}(m^{-1/3})$.\nProof. (sketch) First, apply the construction in Theorem 2 to show that there exists one-layer MLPs of width $d = \\tilde{O}(\\sqrt{m})$ that compute $C_{\\text{UAND}}^{(n)}$ when the inputs are in superposition, where $n \\in \\{2, 3, ..., 2^L\\}$.\nNext, concatenate together the $2^{L - 1}$ networks of width $d = \\tilde{O}(\\sqrt{m})$ that $\\varepsilon$-linearly represent each $C_{\\text{UAND}}^{(n)}$ for $n \\in \\{2, 3, ..., 2^L\\}$, when the inputs are in superposition. Since the output of any boolean circuits of depth $L$ can be written as a linear combinations of ANDs of maximum fan-in $2^L$, it follows that the concatenated network $\\varepsilon'$-linearly represents any boolean circuit of depth $L$, for some $\\varepsilon'$ dependent on how many ANDs need to be added together to compute the circuit, as desired."}, {"title": "4.2. Efficient boolean circuits via deep MLPS", "content": "The one-layer MLP in Theorem 6 has width that is exponential in the depth of the circuit. However, by combining pairwise U-AND layers (which linearly represent any one-layer boolean circuit) with \"error correction\" layers, we can construct deeper neural networks with sublinear width and depth linear in the depth of the circuit.\nLemma 7. Assume that $m = \\tilde{O}(d^{1.5})$, and $c$ is some large polylog constant. Then for sufficiently small input interference $\\varepsilon = \\tilde{O}(1/\\sqrt[4]{d})$ there exists a 1-layer MLP $M_w: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ that takes as input a boolean vector of length $m$ encoded in $d$-dimensions using superposition and returns (outside negligible probability) an encoding of the same boolean vector with interference $\\varepsilon/c$.\nProof. See Theorem 21 in Appendix D.4."}, {"title": "5. Related Work", "content": "The idea that neural networks could or should make use of distributed or compositional representations has been a mainstay of early neural network research (Rosenblatt, 1961; Holyoak, 1987; Fodor & Pylyshyn, 1988). Arora et al. (2018) were the first in the modern deep learning context to discuss that neural networks could store many features in superposition. Olah et al. (2020) developed this idea into the 'superposition hypothesis': the conjecture that networks use the same neurons for multiple circuits to maximise the number of circuits they can learn.\nMany of our results are similar in flavor to those from the fields of sparse dictionary (Tillmann, 2014) and hyperdimensional computing (Zou et al., 2021), as all rely on useful properties of high-dimensional spaces. In addition, many of our boolean circuit results on randomly-initialized MLP layers are similar in flavor to universality results on randomly initialized neural networks with different non-linearities (Rahimi & Recht, 2008a;b). However, these results consider cases where there are fewer \"true features\" than there are dimensions, while the superposition hypothesis requires that the number of \"true features\" exceeds the dimensionality of the space. Randomized numerical linear algebra (Murray et al., 2023) studies the use of random projections to perform efficient computation, but in the context of reducing the cost of linear algebra operations such as linear regression or SVD with inputs and outputs represented in an axis-aligned fashion.\nSuperposition has been studied in a range of idealised settings: Elhage et al. (2022) provided the first examples of toy models which employed superposition to achieve low loss and Henighan et al. (2023) further explored superposition in a toy memorisation task. Notably, they study features that are ReLU-linear represented. (See Section 2.2 for more discussion.) Scherlis et al. (2022) study a model of using a small number of neurons with quadratic activations to approximately compute degree two polynomials. The models studied in all of these papers require sparse features of declining importance. In contrast, our model allows for sparse features that are equally important. More importantly, none of these listed works study performing computation with inputs in superposition."}, {"title": "6. Discussion", "content": "6.1. Summary\nIn this work", "interpretability": "nUnused features The implementation of U-AND by random matrices (Theorem 3) suggests that certain concepts may be detectable through linear probes in a network's activation space without being actively utilized in subsequent computations. This phenomenon could explain the findings of Marks (2024), who observed that arbitrary XORS of concepts can be successfully probed in language models. Furthermore, it implies that successfully probing for a concept and identifying a direction that explains a high percentage of variance (e.g., 80%) may not constitute strong evidence of the model's actual use of that concept. Consequently, there is reason to be cautious about how many of the features identified by Sparse Autoencoders (Cunningham et al., 2023; Bricken et al., 2023; Bloom, 2024; Templeton et al., 2024) are actively employed by the model in its computation.\nRobustness to noise This research underscores the critical role of error correction in networks performing computations in superposition. Effective error correction mechanisms should enable networks to rectify minor perturbations in their activation states, resulting in a nonlinear response in output when activation vectors are slightly altered along specific directions. Expanding on this concept, Heimersheim & Mendel (2023) conducted follow-up investigations, revealing the presence of plateaus surrounding activation vectors in GPT2-small (Radford et al., 2019). Within these plateaus, model outputs exhibit minimal variation despite small changes in activation values, providing weak evidence for an error correcting mechanism in the model's computation.\n6.3. Limitations and future work\nThat being said, there are a number of ways in which the computational framework presented in this work is very likely to miss the full richness of computation happening in any given real neural network.\nFirstly, this work studies computation on binary features. It is plausible that other kinds of features \u2013 in particular, discrete features which take on more than 2 distinct values, or continuous-valued features occur commonly in real neural networks. It would be valuable to extend the understanding developed in this work to such non-binary features.\nSecondly, though we do not require features to have declining importance, we do require features to be sparse, with each data point only having a small number of active features. It is plausible that not all features are sparse in practice (given the present state of empirical evidence, it even appears open to us whether a significant fraction of features are sparse in practice)"}]}