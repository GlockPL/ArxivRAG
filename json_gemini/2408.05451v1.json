{"title": "Mathematical Models of Computation in Superposition", "authors": ["Kaarel H\u00e4nni", "Jake Mendel", "Dmitry Vaintrob", "Lawrence Chan"], "abstract": "Superposition - when a neural network represents more \"features\" than it has dimensions - seems to pose a serious challenge to mechanistically interpreting current AI systems. Existing theory work studies representational superposition, where superposition is only used when passing information through bottlenecks. In this work, we present mathematical models of computation in superposition, where superposition is actively helpful for efficiently accomplishing the task.\nWe first construct a task of efficiently emulating a circuit that takes the AND of the $\\binom{m}{2}$ pairs of each of $m$ features. We construct a 1-layer MLP that uses superposition to perform this task up to $\\varepsilon$-error, where the network only requires $O(m^3)$ neurons, even when the input features are themselves in superposition. We generalize this construction to arbitrary sparse boolean circuits of low depth, and then construct \"error correction\" layers that allow deep fully-connected networks of width $d$ to emulate circuits of width $\\tilde{O}(d^{1.5})$ and any polynomial depth. We conclude by providing some potential applications of our work for interpreting neural networks that implement computation in superposition.", "sections": [{"title": "1. Introduction", "content": "Mechanistic interpretability seeks to decipher the algorithms utilized by neural networks (Olah et al., 2017; Elhage et al., 2021; R\u00e4uker et al., 2023; Olah et al., 2020; Meng et al., 2023; Geiger et al., 2021; Wang et al., 2022; Conmy et al., 2024). A significant obstacle is that neurons are polysemantic - activating in response to various unrelated inputs (Fusi et al., 2016; Nguyen et al., 2016; Olah et al., 2017; Geva et al., 2021; Goh et al., 2021). As a proposed explanation for polysemanticity, Olah et al. (2020) introduce the 'superposition hypothesis' (see also Arora et al. (2018); Elhage et al. (2022)): the idea that networks represent many more concepts in their activation spaces than they have neurons by sparsely encoding features as nearly orthogonal directions.\nPrevious work has studied how networks can store more features than they have neurons in a range of toy models (Elhage et al., 2022; Scherlis et al., 2022). However, previous models of superposition either involve almost no computation (Elhage et al., 2022) or rely on some part of the computation not happening in superposition (Scherlis et al., 2022). Insofar as neural networks are incentivized to learn as many circuits as possible (Olah et al., 2020), they are likely to compute circuits in the most compressed way possible. Therefore, understanding how networks can undergo more general computation in a fully superpositional way is valuable for understanding the algorithms they learn.\nIn this paper, we lay the groundwork for understanding computation in superposition in general, by studying how neural networks can emulate sparse boolean circuits.\n\u2022 In Section 2, we clarify existing definitions of linearly represented features, and propose our own definition which is more suited for reasoning about computation.\n\u2022 In Section 3, we focus our study on the task of emulating the particular boolean circuit we call the Universal AND (U-AND) circuit. In this task, a neural network must take in a set of boolean features in superposition, and compute the pairwise logical ANDs of these features in a single layer with as few hidden neurons as possible. We present a construction which allows for many more new features to be computed than the number of hidden neurons, with outputs represented natively in superposition. We argue that real neural networks may well implement our construction in the wild by proving that randomly initialised networks are very likely to emulate U-AND.\n\u2022 In Section 4 we demonstrate a second reason why this task is worth studying: it is possible to modify our construction to allow a wide range of large boolean circuits to be emulated entirely in superposition, provided that they satisfy a certain sparsity property.\nWe conclude with a discussion of the limitations of our for-"}, {"title": "2. Background and setup", "content": "2.1. Notation and conventions\nAsymptotic complexity and $\\tilde{O}$ notation We make extensive use of standard Bachmann-Landau (\u201cbig O\") asymptotic notation. We use $\\tilde{O}$ to indicate that we are ignoring polylogarithmic factors:\n$\\tilde{O}(g(n)) := O(g(n) \\log^k n)$ for some $k \\in \\mathbb{Z}$.\n(And so forth for $\\tilde{E}, \\tilde{\\Omega}$, etc.)\nFully connected neural networks We use $M_w : X \\rightarrow Y$ to denote a neural network model parameterized by $w$ that takes input $x \\in X$ and outputs $M_w(x) \\in Y$. In this work, we study fully-connected networks consisting of $L$ MLP layers with ReLU activations:\n$a^{(0)}(x) = x$\n$a^{(l)}(x) = MLP^{(l)}(a^{(l-1)}(x))$\n$MLP^{(l)}(x) = ReLU (W_{in}^{(l)} a^{(l-1)}(x) + W_{bias}^{(l)})$\n$M_w(x) = W_{out}a^{(L)}$,\nwhere $ReLU(x) = max(0, x)$ with max taken elementwise. We assume that our MLPs have width $d$ for all hidden layers, that is, $a^{(l)} \\in \\mathbb{R}^d$ for all $l \\in \\{1, ..., L\\}$. For simplicity's sake we will be dropping $l$ whenever we only talk about a single layer at time."}, {"title": "2.2. Strong and weak linear representations", "content": "Given the activations of a neural network at a particular layer $a^{(l)} : X \\rightarrow \\mathbb{R}^d$, we can also ask what features are linearly represented by $a^{(l)}$. In this section, we present three definitions for a feature being linearly represented by $a^{(l)}$, which we illustrate in Figure 2.\nThe standard definition of linear representation is based on whether or not the representations of positive and negative examples can be separated by a hyperplane:\nDefinition 1 (Weak linear representations). We say that a binary feature $f_k$ is weakly linearly represented by a: $X \\rightarrow \\mathbb{R}^d$ (or linearly separable in a) if there exists some $r_k \\in \\mathbb{R}^d$ such that for all $x_1, x_2 \\in X$ where $f_k(x_1) = 0$ and $f_k(x_2) = 1$, we have:\n$r_k \\cdot a(x_1) < r_k \\cdot a(x_2)$.\nOr, equivalently, the sets $\\{x | f_k(x) = 0\\}$ and $\\{x | f_k(x) = 1\\}$ are separated by a hyperplane normal to $r_k$.\nThat being said, features being linearly separable does not mean a neural network can easily \"make use\" of the features. For some weakly linearly represented features $f_1$ and $f_2$, neither $f_1 \\land f_2$ nor $f_2 \\lor f_2$ need to be linearly represented, even if their read-off vectors $r_1, r_2$ are orthogonal (Figure 3). In fact, a stronger statement is true: it might not even be possible to linearly separate $f_1 \\land f_2$ or $f_2 \\lor f_2$ in $MLP \\circ a$, that is, even after applying an MLP to the activations (see Theorem 9 in Appendix C).\nAs a result, in this paper we make use of a more restrictive notion of a feature being linearly represented:\nDefinition 2 ($\\varepsilon$-linear representations). Let $X$ be a set of inputs and $a: X \\rightarrow \\mathbb{R}^d$ be the activations of a neural network (in a particular position/layer in a given model). We say that $f_1,..., f_m$ are linearly represented with interference"}, {"title": "3. Universal ANDs: a model of single-layer MLP superposition", "content": "We start by presenting one of the simplest non-trivial boolean circuits: namely, the one-layer circuit that computes the pairwise AND of the input features. Note that due to space limitations, we include only proof sketches in the main body and may ignore some regularity conditions in the theorem statement. See Appendix D for more rigorous theorem statements and proofs.\nDefinition 4 (The universal AND boolean circuit). Let $b \\in \\{0, 1\\}^m$ be a boolean vector. The universal AND (or U-AND) circuit has $m$ inputs and $\\binom{m}{2}$ outputs indexed by unordered pairs $k, l$ of locations and is defined by\n$C_{UAND}(b)_{k,l} := b_k \\land b_l$.\nIn other words, we apply the AND gate to all possible pairs of distinct inputs to produce $\\binom{m}{2}$ outputs.\nWe will build our theory of computation starting from a single-layer neural net that emulates the universal AND when the input $b$ is $s$-sparse for some $s \\in \\mathbb{N}$ (this implies that the output has sparsity $O(s^2)$).\n3.1. Superposition in MLP activations enables more efficient U-AND\nFirst, consider the naive implementation, where we use one ReLU to implement each AND using the fact that for boolean $x_1, x_2$:\n$ReLU(x_1 + x_2 - 1) = x_1 \\land x_2$.\nThis requires $\\binom{m}{2} = O(n^2)$ neurons, each of which is monosemantic in that it represents a single natural feature. In contrast, by using sparsity, we can construct using exponentially fewer neurons (Figure 1):\nTheorem 1 (U-AND with basis-aligned inputs). Fix a sparsity parameter $s \\in \\mathbb{N}$. Then for large input length $m$, there exists a single-layer neural network $M_w(x) = MLP(x) = ReLU(W_{in}x + W_{bias})$ that $\\varepsilon$-linearly represents the universal AND circuit $C_{UAND}$ on $s$-sparse inputs, with width $d = \\tilde{O}m^{1/2}$ (i.e. polylogarithmic in m).\nProof. (sketch) To show this, we construct an MLP such that each neuron checks whether or not at least two inputs in a small random subset of the boolean input $b$ are active (see"}, {"title": "3.2. Neural networks can implement efficient U-AND even with inputs in superposition", "content": "Note that in Theorem 1, we assume that the network gets $m$ basis-aligned inputs (that is, not in superposition). However, it turns out that we can extend the result in Theorem 1 to inputs in superposition.\nTheorem 2 (U-AND with inputs in superposition). Let $s \\in \\mathbb{N}$ be a fixed sparsity limit and $\\varepsilon < 1$ a fixed interference parameter. There exists a feature encoding $\\Phi$ and single-layer neural net $M_w(x) = MLP(x) = ReLU(W_{in}x + W_{bias})$ with input size $d = \\tilde{O}(\\sqrt{m}/s^2)$, where $M_w \\circ \\Phi$ $\\varepsilon$-linearly represents $C_{UAND}$ on all $s$-sparse inputs $b$.\nProof. (sketch) By picking almost orthogonal unit-norm vectors $\\Phi = (\\phi_1,..., \\phi_m)$, we can recover each feature up to error $\\varepsilon$ using readoffs $R = \\Phi^T$. Take the input weight $W_{in} \\in Mat_{d \\times m}$ for the MLP constructed in the proof of Theorem 1. Using $W_{in}' = W_{in}R$ and $w_{bias}' = w_{bias}$ suffices, as this gives us\n$M_w \\circ \\Phi(b) = ReLU(W_{in}R\\Phi b + W_{bias})$\n$\\approx ReLU(W_{in}b + W_{bias})$,\nwhich is just the model from Theorem 1, which $\\varepsilon$-linearly represents $C_{UAND}$ as desired. Carefully tracking error terms shows that we need $d = \\tilde{O}(\\sqrt{m})$ neurons."}, {"title": "3.3. Randomly initialized neural networks linearly represent U-AND", "content": "While the results in previous section show that there exist some network weights that $\\varepsilon$-linearly represents the U-AND circuit $C_{UAND}$, there still is a question of whether neural networks can learn to represent many ANDs starting from the standard initialization. In this section, we provide some theoretical evidence \u2013 namely, that sufficiently wide randomly initialized one-layer MLPs $\\varepsilon$-linearly represent $C_{UAND}$.\nTheorem 3 (Randomly initialized MLPs linearly represent U-AND). Let $MLP : \\mathbb{R}^m \\rightarrow \\mathbb{R}^d$ be a one-layer MLP with $d = \\Omega(1/\\varepsilon^2)$ neurons that takes input $b$, and where $W_{in}$ is drawn i.i.d from a normal distribution $N(0, \\delta^2)$ and $W_{bias} = 0$. Then this $MLP$ $\\varepsilon$-linearly represents $C_{UAND}$ on $s$-sparse inputs outside of negligible probability.\nProof. (Sketch) We prove this by constructing a read-off vector $r$ for each pair of features $k_1, k_2$. Let $\\sigma$ be the sign function\n$\\sigma(x) =$\n$\\begin{cases}\n+1 & x > 0\\\\\n0 & x = 0\\\\\n-1 & x < 0\n\\end{cases}$\nand let $w_{i,k}$ be the contribution to the preactivation of neuron $i$ from $b_k$.\nWe construct $r$ coordinatewise (that is, neuron-by-neuron). In particular, we set the $i$th coordinate of $r$ to be\n$r_i = \\eta \\cdot (1_{\\{\\sigma(w_{i,k_1}) = \\sigma(w_{i,k_2})\\}} - 1_{\\{\\sigma(w_{i,k_1}) \\neq \\sigma(w_{i,k_2})\\}})$.\nThat is, if $k_1$ and $k_2$ contribute to the neuron preactivations with the same sign, then $r_i = \\eta_i$, else, $r_i = -\\eta_i$. Here, $\\eta_i$ is a scaling parameter of size $\\Theta(\\sqrt{s/d})$ used to scale the read-off to be 1 when $b_{k_1} = b_{k_2} = 1$\nWhen $b_{k_1} = 0$ or $b_{k_2} = 0$, the expected value of $r \\cdot M_w(b)$ is zero, while the error terms have size $\\tilde{O}m(1/\\sqrt{d})$. So setting $d = \\Omega(1/\\varepsilon^2)$ suffices to get error below $\\varepsilon$ with high probability.\nWhen $b_{k_1} = b_{k_2} = 1$, the contribution from each neuron $i$ to $r \\cdot M_w(b)$ with $\\sigma(w_{i,k_1}) = \\sigma(w_{i,k_2})$ will be in expectation larger than those with $\\sigma(w_{i,k_1}) \\neq \\sigma(w_{i,k_2})$ (as the standard deviation of the sum of two weights with equal signs is larger than the sum of two weights with different signs, and we apply a ReLU). By setting $\\eta$ to be the reciprocal of the difference in expected contributions, we have that this value has expectation 1. Again, as the error terms have size $\\tilde{O}m(1/\\sqrt{d})$, it follows that setting $d = \\Omega(1/\\varepsilon^2)$ suffices to get error below $\\varepsilon$ with high probability, as desired."}, {"title": "4. MLPs as representing sparse boolean circuits", "content": "In the previous section we showed variants of computation in superposition at a single layer, for one of the simplest non-trivial boolean circuits. In this section, we extend these results to show that neural networks can efficiently represent arbitrary sparse boolean circuits.\nAs in Section 3, we include only proof sketches in the main body due to space limitations, and may also ignore some regularity conditions in our theorem statements. See Appendix D for more rigorous theorem statements and proofs.\n4.1. Boolean circuits in single layer MLPS\nWe start by extending these results from Section 3 to ANDS of more than two variables.\nLet $C_{UAND}^{(n)}$ be the boolean circuit of depth $L = \\log(n)$ that computes the ANDs of each n-tuple of elements in b.\nLemma 5 (\"High fan-in\u201d U-AND). For each $n \\in \\mathbb{N}$, there exists a one-layer neural network $M_w = MLP : \\mathbb{R}^M \\rightarrow \\mathbb{R}^d$ with width $d = \\tilde{O}(n/\\varepsilon^2)$ such that $M_w(b)$ $\\varepsilon$-linearly represents $C_{UAND}^{(n)}$ on $s$-sparse inputs.\nProof. (sketch) We can extend the construction in the proof of Theorem 1 to allow for ANDs of exactly $n$ variables, by considering index sets $I(k_1,k_2, ..., k_n)$ of $n$ variables, and changing the bias of each neuron from -1 to -n + 1. The expected size of an index set of $n$ variables is $E[I(k_1,k_2,..., k_n)] = p^n d$, and we require this expected value to be $(\\log^4 m)$ to ensure that the index set is non-empty outside negligible probability (using the normal Chernoff and Union bounds). Therefore, we have to scale up the probability that any given value in $W_{in}$ is 1: $p = \\frac{\\log^2 m}{d^{1/n}}$ suffices. A similar argument to the one found in the proof of Theorem 1 shows that all the interference terms are o(1).\nAs illustrated in Figure 4, Lemma 5 allows us to construct"}, {"title": "4.2. Efficient boolean circuits via deep MLPS", "content": "The one-layer MLP in Theorem 6 has width that is exponential in the depth of the circuit. However, by combining pairwise U-AND layers (which linearly represent any one-layer boolean circuit) with \"error correction\" layers, we can construct deeper neural networks with sublinear width and depth linear in the depth of the circuit.\nLemma 7. Assume that $m = \\tilde{O}(d^{1.5})$, and c is some large polylog constant. Then for sufficiently small input interference $\\varepsilon = \\tilde{O}(1/\\sqrt[4]{d})$ there exists a 1-layer MLP $M_w : \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ that takes as input a boolean vector of length $m$ encoded in $d$-dimensions using superposition and returns (outside negligible probability) an encoding of the same boolean vector with interference $\\varepsilon/c$.\nProof. See Theorem 21 in Appendix D.4."}, {"title": "5. Related Work", "content": "The idea that neural networks could or should make use of distributed or compositional representations has been a mainstay of early neural network research (Rosenblatt, 1961; Holyoak, 1987; Fodor & Pylyshyn, 1988). Arora et al. (2018) were the first in the modern deep learning context to discuss that neural networks could store many features in superposition. Olah et al. (2020) developed this idea into the 'superposition hypothesis': the conjecture that networks use the same neurons for multiple circuits to maximise the number of circuits they can learn.\nMany of our results are similar in flavor to those from the fields of sparse dictionary (Tillmann, 2014) and hyperdimensional computing (Zou et al., 2021), as all rely on useful properties of high-dimensional spaces. In addition, many of our boolean circuit results on randomly-initialized MLP layers are similar in flavor to universality results on randomly initialized neural networks with different non-linearities (Rahimi & Recht, 2008a;b). However, these results consider cases where there are fewer \"true features\" than there are dimensions, while the superposition hypothesis requires that the number of \"true features\" exceeds the dimensionality of the space. Randomized numerical linear algebra (Murray et al., 2023) studies the use of random projections to perform efficient computation, but in the context of reducing the cost of linear algebra operations such as linear regression or SVD with inputs and outputs represented in an axis-aligned fashion.\nSuperposition has been studied in a range of idealised settings: Elhage et al. (2022) provided the first examples of toy models which employed superposition to achieve low loss and Henighan et al. (2023) further explored superposition in a toy memorisation task. Notably, they study features that are ReLU-linear represented. (See Section 2.2 for more discussion.) Scherlis et al. (2022) study a model of using a small number of neurons with quadratic activations to ap-"}, {"title": "6. Discussion", "content": "6.1. Summary\nIn this work, we have presented a mathematical framework for understanding how neural networks can perform computation in superposition, where the number of features computed can greatly exceed the number of neurons. We have demonstrated this capability through the construction of a neural network that efficiently emulates the Universal AND circuit, computing all pairwise logical ANDs of input features using far fewer neurons than the number of output features. Furthermore, we have shown how this construction can be generalized to emulate a wide range of sparse, low-depth boolean circuits entirely in superposition. This work lays the foundation for a deeper understanding of how neural networks can efficiently represent and manipulate information, and highlights the importance of considering computation in superposition when interpreting the algorithms learned by these systems.\n6.2. Practical Takeaways for Mechanistic Interpretability\nOur primary motivation for undertaking this work was to glean insights about the computation implemented by neural networks. While we provide more potential takeaways in Appendix B, here we discuss what we think are two salient takeaways for interpretability:\nUnused features The implementation of U-AND by random matrices (Theorem 3) suggests that certain concepts may be detectable through linear probes in a network's activation space without being actively utilized in subsequent computations. This phenomenon could explain the findings of Marks (2024), who observed that arbitrary XORS"}, {"title": "6.3. Limitations and future work", "content": "That being said, there are a number of ways in which the computational framework presented in this work is very likely to miss the full richness of computation happening in any given real neural network.\nFirstly, this work studies computation on binary features. It is plausible that other kinds of features \u2013 in particular, discrete features which take on more than 2 distinct values, or continuous-valued features occur commonly in real neural networks. It would be valuable to extend the understanding developed in this work to such non-binary features.\nSecondly, though we do not require features to have declining importance, we do require features to be sparse, with each data point only having a small number of active features. It is plausible that not all features are sparse in practice (given the present state of empirical evidence, it even appears open to us whether a significant fraction of features are sparse in practice) \u2013 for instance, perhaps real neural networks partly use more compositional representations with dense features.\nThirdly, in this work, we have made a particular choice regarding what it takes for a feature to be provided in the input and to have been computed in the output: $\\varepsilon$-linear representation (Definition 2). Future empirical results or theoretical arguments could call for revising this choice \u2013 for instance, perhaps an eventual full reverse-engineering"}, {"title": "A. Mathematical definitions", "content": "Here, we list and define the mathematical terms that we use throughout this work.\nX\nset of inputs\nY\nset of outputs\n$M_w: X \\rightarrow Y$\nneural network with ReLU activations, parameterized by w\n$a^{(l)}(x) \\in \\mathbb{R}^d$\nthe activations of a neural network at layer l, l \\in \\{0, ..., L\\}\n$MLP^{(l)}: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$\nthe lth MLP layer, $MLP^{(l)}(x) = ReLU (W_{in}^{(l)} x + W_{bias}^{(l)})$\n$f_k: X \\rightarrow \\{0, 1\\}$\nboolean feature of the input, k = 1, . . ., m\n$F: X \\rightarrow \\{0, 1\\}^m$\nthe concatenation of m boolean features\n$r_k \\in \\mathbb{R}^d$\nvector linearly representing the kth boolean feature\n$\\Phi \\in \\mathbb{R}^{d \\times m}$\nthe feature embedding matrix, $\\Phi = (\\phi_1, ..., \\phi_m)$\n$b = b(x) \\in \\{0, 1\\}^m$\na boolean vector of length m associated to an input/activation\n$b_k = b_k(x) \\in \\{0, 1\\}$\nthe kth entry in the boolean vector, equal to $f_k(x)$\n$||b(x)||_1$\n\"sparsity\u201d, a.k.a. number of bits that are \"on\" for the boolean vector b,\nequal to $\\sum_{k=1}^m f_k(x)$.\n$C: \\{0, 1\\}^m \\rightarrow \\{0, 1\\}^{m'}$\na boolean circuit\n$C_l : \\{0, 1\\}^m \\rightarrow \\{0, 1\\}^{m'}$\nlayer l of the boolean circuit C, consisting of m' boolean gates of fan-in at\nmost two.\nWe also use the following conventions for clarity:\ni, j \\in \\{1, ..., d\\}\nindices for neurons\nk, l, p \\in \\{1, ..., m\\}\nindices for features\n$\\mu$\namount of interference between near-orthogonal vectors\n$\\varepsilon$\nerror in the read-off of a boolean feature\n$s$\nA bound on the \u201csparsity\u201d; we require $||b(x)||_1 \\leq s \\forall x \\in X$.\nWe assume our terms satisfy the following asymptotic relationships in terms of the principal complexity parameter m (the number of features):\nd is polynomial in m\nso $d = \\sum{m\\choose k}$, $d = \\tilde{O}(m^\u03b1)$ for some finite exponents (0 < \u03b1 < \u03b1_ < \u221e.\ns is at worst polynomial in m\nso $s = \\tilde{O}(m^\u03b2)$. Note that this is different from the body,\nwhere we assumed s is a constant (so \u03b2 = 0).\n$s = \\tilde{O}(d^{1/3})$\nThis is a technical \"sparsity\u201d condition that will be useful for us."}, {"title": "B. Potential takeaways for practical mechanistic interpretability", "content": "Our motivation for studying these mathematical models is to glean insights about the computation implemented by real networks, that could have ramifications for the field of mechanistic interpretability, particularly the subfield focussed on taking features out of superposition in language models using sparse dictionary learning (Cunningham et al., 2023; Bricken et al., 2023; Tamkin et al., 2023; Bloom, 2024; Braun et al., 2024; Templeton et al., 2024). In order to render the models mathematically tractable, we have had to make idealising assumptions about the computation implemented by the networks.\n1. Early work on superposition (Elhage et al., 2022) suggested that it may be possible to store exponentially many features in superposition in an activation space. On the other hand, early sparse dictionary learning efforts (Cunningham et al., 2023; Bricken et al., 2023; Bloom, 2024) learn dictionaries which are smaller than even the square of the dimension of the activation space. Our work suggests that the number of features that can be stored in superposition and computed with is likely to be around $\\tilde{O}(d^2)$ (this is also the information-theoretic limit). We think that by using a dictionary size that scales quadratically in the size of the activations, while computationally challenging, this will likely lead to better performance on downstream tasks. We are heartened by more recent work by Templeton et al. (2024) which works with dictionaries that are closer to this size, and would encourage more systems-oriented work to scale to ever larger dictionaries.\n2. The current mainstream sparse autoencoder (SAE) architecture used by Cunningham et al. (2023); Bricken et al. (2023); Bloom (2024); Templeton et al. (2024) and others uses ReLUs to read off feature values, in accordance with the toy model of superposition of Elhage et al. (2022) and features being ReLU-linearly represented. Our work suggests that networks may be more expressive when storing features $\\varepsilon$-linearly. If so, this suggests that future work should consider sparse dictionary learning with alternative activation functions that only allow for removing errors of size $\\varepsilon$, such as a noise-filtering nonlinearity\n$NF_\\varepsilon(x) = \\begin{cases} x & x > \\varepsilon\\\\\n0 & |x| \\leq \\varepsilon\\\\\nx & x < -\\varepsilon\\end{cases}$\nor nonlinearities that filter all but the k largest positive and largest negative preactivations. Notably, recent work by Rajamanoharan et al. (2024); Taggart (2024) finds suggestive evidence that the ProLU activation:\n$ProLU(x) = \\begin{cases} x & x > \\varepsilon\\\\\n0 & |x| \\leq \\varepsilon\\end{cases}$\n outperforms the standard ReLU activation SAEs, which accords with the predictions in this work.\n3. Previous work by Gurnee et al. (2023) found some features that were represented on a small set of neurons, even when they weren't represented on any singular particular neuron. In our constructions, feature representations end up distributed over a larger range of neurons. We expect that networks which employ superposition heavily to maximise their expressiveness are unlikely to have many sparse features that are localised to one or even a few neurons."}, {"title": "C. Additional discussion of various feature definitions", "content": "C.1. Formal statements and proofs for facts referenced in main body\nWe present formal statements and proofs that we referred to in Section 2.1. Note that without loss of generality, we can include the activation function a into our input set X, so we omit the use of a in this section.\nTheorem 9 (Composition of linearly separable features). There exist a set of inputs X and two features $f_1, f_2$ weakly linearly represented in X such that there exists no MLP layer MLP such that either $f_1 \\land f_2$ or $f_1 \\lor f_2$ are linearly separable in MLP(x).\nProof. (sketch) Let $X = [-1,1]^2$ be the unit square in $\\mathbb{R}^2$, and let $f_1(x) = 1(x_1 > 0)$ and $f_2(x) = 1(x_2 > 0)$ be the indicator functions of whether the first and second coordinates are greater than zero. There exists no MLP layer $MLP : X \\rightarrow \\mathbb{R}^d$ of any width d such that $f_1 \\land f_2$ is linearly separable in MLP(X).\nTo show this, it suffices to notice that any MLP layer has finite Lipschitz coefficient, and that any function weakly linearly representing $f_1 \\land f_2$ or $f_1 \\lor f_2$ will need to have arbitrarily high Lipschitz coefficient (since there exist points that are arbitrarily close to the separating hyperplanes of $f_1$ and $f_2."}, {"title": "Theorem 10 (Composition of $\\varepsilon$-linearly represented features). For any set X and features $f_1, f_2$ that are $\\varepsilon$-linearly represented in X, there exists a two neuron MLP $MLP : X \\rightarrow \\mathbb{R}^2$ such that $f_1 \\land f_2$ and $f_1 \\lor f_2$ are $\\varepsilon'$-linearly represented in MLP(X) for some $\\varepsilon'$.", "content": "Proof. (sketch) We use an MLP with two neurons $MLP_1, MLP_2$ with input weights equal to the read-off vectors of $r_1, r_2$. To read off $f_1 \\land f_2$, we use the read-off vector $r_{1\\land 2}$ defined by $r_{1\\land 2}(x) = MLP_1(x) + MLP_1(x) - 3/4$. Similarly, to read off $f_1 \\lor f_2$, we use the read-off vector $r_{1\\lor 2}(x) = MLP_1(x) + MLP_1(x) - 1/4$.\nIn fact, by allowing for wider MLPs, it is fairly easy to construct an MLP $MLP : X \\rightarrow \\mathbb{R}^d$ such that $f_1 \\land f_2$ and $f_1 \\lor f_2$ are also $\\varepsilon$-linearly represented in MLP(X) (that is, with equal error). We leave the construction of this MLP as an exercise for the reader."}, {"title": "D. Precise statements and proofs of theorems", "content": "Let m be a parameter associated to the length of a boolean input. For the remainder of this section", "interference)": "n$m=\\Omega(d^\\alpha)$ (1)\n$\\varepsilon_{in} = \\Omega(m^{-\\beta_{in}})$ (2)\n$\\varepsilon_{out} = \\tilde{O}(m^{-\\beta_{out}})$ (3)\n$s = \\tilde{O}(m^\\gamma)$ (4)\nMore precisely, we assume that a large parameter m is given and the O(polylog(m)) scaling factors implicit in the $\\tilde{O}, \\tilde{\\Omega}$ asymptotics"}]}