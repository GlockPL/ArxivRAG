{"title": "Brotherhood at WMT 2024: Leveraging LLM-Generated Contextual Conversations for Cross-Lingual Image Captioning", "authors": ["Siddharth Betala", "Ishan Chokshi"], "abstract": "In this paper, we describe our system under the team name Brotherhood for the English-to-Lowres Multi-Modal Translation Task. We participate in the multi-modal translation tasks for English-Hindi, English-Hausa, English-Bengali, and English-Malayalam language pairs. We present a method leveraging multi-modal Large Language Models (LLMs), specifically GPT-40 and Claude 3.5 Sonnet, to enhance cross-lingual image captioning without traditional training or fine-tuning. Our approach utilizes instruction-tuned prompting to generate rich, contextual conversations about cropped images, using their English captions as additional context. These synthetic conversations are then translated into the target languages. Finally, we employ a weighted prompting strategy, balancing the original English caption with the translated conversation to generate captions in the target language. This method achieved competitive results, scoring 37.90 BLEU on the English-Hindi Challenge Set and ranking 1st and 2nd for English-Hausa on the Challenge and Evaluation Leaderboards, respectively. We conduct additional experiments on a subset of 250 images, exploring the trade-offs between BLEU scores and semantic similarity across various weighting schemes.", "sections": [{"title": "1 Introduction", "content": "Machine translation (MT) is a classic sub-field in NLP that investigates the usage of computer software to translate text or speech from one language to another without human involvement (Yang et al., 2020). Machine translation (MT) has seen remarkable advancements in recent years, primarily due to the success of neural approaches (Bahdanau, 2014; Vaswani, 2017). However, these improvements have been predominantly observed in high-resource language pairs, leaving low-resource languages significantly behind (Sennrich and Zhang, 2019; Costa-juss\u00e0 et al., 2022). The challenges in low-resource MT are multifaceted, including limited parallel corpora, lack of linguistic diversity in training data, and the absence of specialized tools and resources.\nOne promising direction to address these challenges is the incorporation of visual information into the translation process, known as multimodal machine translation (MMT) (Elliott et al., 2016; Specia et al., 2016; Calixto et al., 2017). The underlying hypothesis is that visual context can provide crucial disambiguating cues, especially for languages with limited textual resources. This approach aligns with the human cognitive process of language understanding, which often relies on multiple sensory inputs (Beinborn et al., 2018).\nThe Workshop on Machine Translation (WMT) 2024 has presented a shared task on English-to-Low-Resource Multi-Modal Translation, focusing on Hindi (Parida et al., 2019), Bengali (Sen et al., 2022), Malayalam\u00b9, and Hausa (Abdulmumin et al., 2022). This task utilizes variants of the Visual Genome (Krishna et al., 2017) dataset, adapted for these target languages. While these datasets provide a valuable resource for research, they also present unique challenges. First, the quality of translations in low-resource languages can be inconsistent (see Table 1), potentially introducing noise into the training process. Second, the limited size of these datasets makes it difficult to train robust neural models without overfitting. Lastly, the cultural and linguistic nuances of these languages may not be fully"}, {"title": "2 Dataset", "content": "We utilized only the datasets specified by the organizers for the related tasks. However, our use of the GPT-40 and Claude 3.5 Sonnet models places our submissions in the unconstrained track. The provided datasets contain captions in English and the target language, describing rectangular regions in images of various scenes. The task involves generating captions in the target language using either the text, the image, or both. Across all languages, the training set consists of 29,000 examples. The dataset is complemented by three test sets: development (D-Test), evaluation (EV-Test), and challenge (CH-Test). Our submissions were evaluated on the EV-Test and CH-Test sets, which contain approximately 1,600 and 1,400 examples, respectively. The development set comprises around 1,000 examples."}, {"title": "3 Methodology", "content": "The overall pipeline of our approach is shown in Figure 1."}, {"title": "3.1 Preprocessing", "content": "For the text data, all utterances are converted to lowercase, and punctuation is removed. The dataset includes images of complete scenes along with the coordinates of the bottom-left corner and the dimensions of the rectangular region corresponding to each caption. This information is used to crop the relevant rectangular regions from the images. Since these images are later used as part of prompts for LLMs, base64 encodings of all images in the EV and CH sets are generated."}, {"title": "3.2 Multi-Model Context Generation in English with a Fusion approach", "content": "In this step, we leverage the capabilities of two large language models (LLMs) - GPT-40 and Claude 3.5 Sonnet - to generate rich, contextual conversations about the input image and its associated English caption. This process involves two key stages: individual LLM processing and conversation fusion.\nWe separately prompt GPT-40 and Claude 3.5 using Prompt-1 from Table 4. The format of the conversation and prompt design is inspired by an example prompt from Liu et al. (2024). Both models are given the same input: the cropped image and its English caption. This parallel processing allows us to capitalize on the unique strengths of each model.\nAfter obtaining separate conversations from GPT-40 and Claude 3.5, we employ a fusion model, implemented using the GPT-40 API, to integrate these conversations. This fusion process is designed to create a single, coherent dialogue that encompasses three response types: (1) Short QA pairs, (2) Detailed descriptions, and (3) Complex reasoning QA. The prompt has been designed to generate such types of responses to ensure that the responses can capture the context effectively. The fusion prompt is carefully crafted to ensure that the final conversation retains the most relevant and insightful elements from both initial conversations.\nThis fusion approach is aimed at using complementary strengths of the models for error mitigation and rich context consolidation. Recent work has shown the advantage of such fusion and ensembling approaches in mitigating hallucination across tasks such as machine translation, definition modeling, and paraphrase generation (Mehta et al., 2024). Additionally, such an approach has a flexibility advantage as it allows for future integration of additional LLMs or specialized models."}, {"title": "3.3 Translation of Context to Target Languages", "content": "This stage involves translating the rich contextual information generated in English to the target languages. This step is essential for preserving the nuanced understanding developed in the previous stages while adapting it to the linguistic and cultural context of each target language. For the translation of the generated conversations, we employ different approaches based on the target language:\n\u2022 Hindi, Bengali, and Malayalam: For these Indic languages, we utilize the IndicTrans2 (Gala et al., 2023) model. This state-of-the-art translation model is specifically designed for Indian languages and has demonstrated strong performance in multilingual translation tasks.\n\u2022 Hausa: Due to the limited availability of specialized translation models for Hausa, we leverage the GPT-40's translation capabilities."}, {"title": "3.4 Weighted Prompt-Based Caption Generation", "content": "The final stage of our pipeline employs a weighted prompting strategy to generate the target language caption, balancing fidelity to the original English caption with the rich contextual information derived from our LLM-generated conversations. We utilize GPT-40 API for this crucial step, employing a carefully crafted prompt (Prompt-2, detailed in Table 5) that takes two primary inputs along with the weight value:\n1. The original English caption (weight: 100-x%)\n2. The translated conversation in the target language (weight: x%)\nThe weighting mechanism allows us to control the influence of each input on the final caption. This approach offers flexibility in balancing between direct translation fidelity and contextual enrichment. For our submissions we provide equal weightage to the given English caption and the generated conversation in the target language."}, {"title": "4 Results", "content": "The BLEU score serves as the primary metric for evaluating model performance on the leaderboard, complemented by the RIBES metric for a more comprehensive assessment."}, {"title": "4.1 Weight Tuning Analysis and Performance Metrics", "content": "To thoroughly evaluate our weighted prompt-based approach, we conducted extensive experiments across different weight combinations for each target language. This section presents our findings and analyzes the impact of various weight distributions on caption quality and semantic preservation. We employed three primary metrics to assess the performance of our model under different weight configurations:\n\u2022 BLEU Score: Bilingual Evaluation Understudy (BLEU) measures the similarity between the generated caption and the reference caption. It provides a quantitative measure of translation quality.\n\u2022 Semantic Similarity (Sem. Sim.):\nWe use cosine similarity between sentence embeddings to measure the semantic closeness of the generated caption to the reference caption in the target language. This metric is calculated using the Sentence Transformer's SentenceBERT model 'distiluse-base-multilingual-cased-v1', which provides multilingual sentence embeddings.\n\u2022 Normalized Semantic Similarity (Norm. Sem.): This metric compares the semantic similarity of the generated caption to the English source with that of the reference translation to the English source as the ratio of the two. It helps assess how well the generated caption preserves the meaning of the original English caption relative to the reference translation.\nTo ensure a comprehensive evaluation of our approach across various scenarios, we conducted an in-depth analysis on a diverse subset of the corpus. This subset comprises 250 image-caption pairs, randomly selected from the training, development, evaluation, and challenge sets. Generally, BLEU scores decrease as more weight is given to the translated conversation. This suggests that higher weights on the original caption produce translations more closely aligned with the reference.\nThe semantic similarity between the generated and reference captions in the target language tends to decrease with increasing weight on the translated conversation. This indicates that while the generated captions may become more descriptive, they may deviate from the reference in terms of semantic content.\nInterestingly, the normalized semantic similarity remains relatively stable across weight distributions. This suggests that our approach consistently preserves the semantic relationship between the English source and the generated caption, regardless of the weight distribution.\nWhile the highest BLEU scores are generally achieved with lower weights on the translated conversation, there's a trade-off between BLEU score and the richness of the generated caption. A balanced approach (e.g., 50-50 weighting) often provides a good compromise between translation accuracy and contextual enrichment.\nNotably, setting a 100% weight for the original English caption allows us to evaluate GPT-4's zero-shot cross-lingual transfer capabilities in direct translation tasks from English to Hindi, Bengali, Malayalam, and Hausa. Conversely, assigning a 100% weight to the additional context - which consists of the translated LLM-generated conversation in the target language\u2014enables us to assess the model's abstractive summarization abilities in these non-English languages. This analysis provides insights into the models' multilingual competence and their capacity for language understanding and generation across diverse linguistic contexts."}, {"title": "5 Conclusion", "content": "Key strengths of our approach include its training-free nature, which avoids propagating errors from potentially flawed datasets, and its flexibility in balancing source fidelity with enhanced descriptiveness through a weighted prompting strategy. The method's multilingual capability and rich context generation offer promising avenues for dataset enrichment and improvement in low-resource languages. We demonstrated how our weighted prompting strategy serves as a probe for assessing LLMs' capabilities in zero-shot cross-lingual transfer for direct translation tasks, as well as their abstractive summarization abilities in low-resource target languages. However, we acknowledge limitations such as reliance on LLM APIs, potential for hallucination, and computational intensity. The challenge of evaluating enhanced descriptions with traditional metrics like BLEU also highlights the need for more comprehensive evaluation methods. Future work should focus on:\n\u2022 Conducting human evaluations to better assess caption quality and appropriateness.\n\u2022 Analyzing specific cases of significant improvements or detractions from original captions.\n\u2022 Exploring applications in dataset error correction and enhancement.\n\u2022 Investigating performance across diverse image types and caption complexities.\nBy addressing these areas, we aim to further refine and expand the capabilities of our approach, potentially leading to more robust and versatile multimodal translation systems. This work represents a step towards bridging the gap between high-resource and low-resource languages in multimodal machine translation, opening new possibilities for cross-lingual image understanding and dataset enrichment."}]}