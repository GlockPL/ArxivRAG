{"title": "Dy-mer: An Explainable DNA Sequence Representation Scheme using Sparse Recovery", "authors": ["Zhiyuan Peng", "Yuanbo Tang", "Yang Li"], "abstract": "DNA sequences encode vital genetic and biological information, yet these unfixed-length sequences cannot serve as the input of common data mining algorithms. Hence, various representation schemes have been developed to transform DNA sequences into fixed-length numerical representations. However, these schemes face difficulties in learning high-quality representations due to the complexity and sparsity of DNA data. Additionally, DNA sequences are inherently noisy because of mutations. While several schemes have been proposed for their effectiveness, they often lack semantic structure, making it difficult for biologists to validate and leverage the results. To address these challenges, we propose Dy-mer, an explainable and robust DNA representation scheme based on sparse recovery. Leveraging the underlying semantic structure of DNA, we modify the traditional sparse recovery to capture recurring patterns indicative of biological functions by representing frequent K-mers as basis vectors and reconstructing each DNA sequence through simple concatenation. Experimental results demonstrate that Dy-mer achieves state-of-the-art performance in DNA promoter classification, yielding a remarkable 13% increase in accuracy. Moreover, its inherent explainability facilitates DNA clustering and motif detection, enhancing its utility in biological research.", "sections": [{"title": "1. Introduction", "content": "DNA sequences harbor the genetic and biological information imperative for the development and functioning of organisms, dictating traits and orchestrating biochemical processes Dictionary [2002]. Typically, DNA adopts a double helix structure composed of two intertwined alternating deoxyribonucleotide strands. A nucleotide comprises a sugar (deoxyribose), a phosphate group, and one of four nucleobases-adenine (A), cytosine (C), guanine (G), or thymine (T). Biologists assert that permutations of nucleotides encode vital biological information crucial for RNA transcription and protein synthesis. However, raw DNA sequences are inherently variable in length, which cannot be inputted into common data mining models. Hence, researchers design various DNA representation schemes to transform DNA sequences into numerical representations of equal length and decipher DNA sequences for multiple DNA-related tasks, including DNA similarity analysis Jin et al. [2017] and classification Sun et al. [2021a].\nThough designing a representation scheme is crucial, it also presents several challenges. DNA sequences inherently possess a high-dimensional structure characterized by their diverse composition, permutations, and varying lengths. However, high-quality DNA datasets are often limited. It is challenging for DNA representation schemes to extract abundant features from such complex and sparse data. Furthermore, DNA sequences are inherently noisy, because of frequent mutations, such as insertions, deletions, and point mutations. Therefore, the DNA representation scheme should be effective and robust, otherwise, it could significantly undermine representation quality.\nEven with such challenges present, many works have recently been proposed. For instance, A scheme employed Empirical Mode Decomposition to extract features from original nucleotide permutations, enabling the identification of characteristic segments within DNA sequences Bai et al. [2011]. Le et al. Le et al. [2019] utilized sliding windows to obtain subsequences, known as K-mers Chor et al. [2009], Mapleson et al. [2017] and applied FastText N-Grams to convert them into vectors. Finally, the model aggregates all vectors to represent each DNA sequence. Although existing representation schemes demonstrate promising performance in downstream tasks, they frequently lack interpretable data structures, instead relying on attention or gradient mechanisms of application-specific data-mining algorithms Xiao et al. [2021], Santorsola and Lescai [2023] to implicitly measure each feature's contribution. It hinders biologists' ability to validate and leverage the insights derived from these representations.\nIn this paper, we propose Dy-mer, an explainable and robust DNA representation scheme based on sparse recovery Cand\u00e8s et al. [2006], Donoho [2006], Chu and Stormo [2022] to address these challenges. The main idea of sparse recovery is to recover the underlying sparse structure from raw data and obtain the sparse representation, under the assumption that the original signal can be sparsely represented by a well-designed basis. While classic sparse recovery methods Cui et al. [2023], Zhou et al. [2021], Xie et al. [2018] enhance the robustness of representations by leveraging various mathematical techniques to ensure the low rank and sparsity of the representation based on a randomly initialized basis, their interpretability is often compromised for obtaining unstructured and dense basis.\nOur method innovates upon the traditional sparse recovery framework. In our approach, basis vectors represent a subset of K-mers encoding potential DNA underlying structure and biological information. Basis vectors are initialized with frequent K-mers observed in real-world settings and are optimized to capture the most frequent and semantically meaningful ones. This idea originates from the observation of patterns in DNA sequences: Biologists have identified several recurring K-mers consistently performing analogous functions, such as transcription factor binding sites (TFBS). These recurring patterns, represented as motifs D'haeseleer [2006], Schneider [2002], play a crucial role in DNA regulation. Therefore, it is appropriate to reconstruct DNA sequences as concatenations of semantic local K-mers. For batch DNA processing, Dy-mer introduces a tensor-based data structure and corresponding mathematical operations. The tensor records the positional assignment relationship of K-mers and the operations are designed to simulate the concatenation.\nHowever, in the previous sparse recovery based representation scheme, a well-constructed basis is essential to obtain a high-quality representation for the accurate reconstruction of original DNA sequences and the elucidation of their intrinsic semantic meaning. However, in the sparse recovery based approach, the basis is sampled frequent K-mers from DNA sequences, which is heavily influenced by the configuration of hyperparameters, which introduces uncertainty regarding the attainment of an optimal basis and may affect the quality of learned representations. Sampling too many K-mers can lead to redundant and dependent K-mers such as AAAAAA and AAAA, unnecessarily increasing the basis size and representation dimensionality. Conversely, sampling too few may fail to capture the complexity of DNA sequences. Moreover, the sampling process is both time-intensive and space-consuming, compromising efficiency. In addition, the SR-based scheme may struggle to obtain a high-quality basis and generate effective representations in small-scale datasets or resource-limited scenarios. To address this limitation, we propose another method based on the sparse dictionary learning algorithm. Sparse dictionary learning algorithm aims at learning a dictionary from the input data, to map each data into its corresponding sparse representation. Many works have already utilized sparse dictionary learning to obtain the optimal sparse numerical representation Zheng et al. [2015]. For instance, Castro Castro [2023] utilized a dictionary learning algorithm to learn an optimal dictionary for the sparse recovery on the image superresolution task. Inspired by sparse dictionary learning, we could regard the sampled static basis as a learnable dictionary, which allows for the simultaneous optimization of the dictionary and sparse representations. Since we don't limit the dictionary elements to specific K-mers, these elements could be viewed as position weight matrices of motifs associated with particular biological functions. Motif, as a recurring pattern in DNA sequences could represent K-mers of similar biological functions and analogous structure. It is clear that the motif is more representative and generalizable than specific K-mers and is more effective and efficient for DNA representation learning compared to the previous method. Additionally, our experiments demonstrate that SDL-based schemes excel on limited-size datasets, effectively handling data scarcity. SDL-based scheme could generate lower-dimensional representations with comparable effectiveness to higher-dimensional SR-based representations, along with a more compact dymer dictionary. Moreover, SDL significantly enhances computation efficiency by eliminating the data preprocessing section and obtaining a more compact dictionary.\nOur approaches have demonstrated their effectiveness in downstream applications, notably achieving state-of-the-art performance in DNA promoter classification. By enhancing the interpretability of robust DNA representations, our methods also facilitate tasks like DNA clustering and motif detection.\nIn summary, the main contributions of our research are as follows:\n\u2022 We leverage a modified Sparse Recovery framework to establish an effective and explainable DNA representation scheme by using frequent K-mers as the basis and representing each DNA sequence as the concatenation of K-mers.\n\u2022 We utilize a sparse dictionary learning algorithm to develop a more computationally efficient and effective DNA representation scheme on limited-resource settings by optimizing the dictionary and representation together.\n\u2022 We formulate the problem using tensor formats to boost batch computational efficiency and preserve the semantic structures of DNA.\n\u2022 We demonstrate the effectiveness of our DNA representations through their successful application in various downstream tasks.\nThe subsequent sections of this article are structured as follows: Section 2 is the literature review of DNA representation schemes. Section 3 provides essential background knowledge necessary for our representation scheme. Section 4 introduces the explainable representation structure of our scheme. Sections 5 and 6 elaborate on our methodology based on sparse recovery and sparse dictionary learning, respectively. Finally, we offer conclusions and insights derived from our research in Section 7."}, {"title": "2. Literature Review", "content": "DNA representation schemes serve a pivotal role in computational biology, transforming dynamic-length DNA sequences into fixed-length representations compatible with data-mining models. Different schemes are designed for specific applications, but their common objective is to extract meaningful features-such as chemical properties, composition, and permutation of nucleotides-to generate effective representations. Biologists have concluded several criteria to judge whether a representation scheme is well-designed, including accuracy, robustness, succinctness, and adaptability Jin et al. [2017]. Adhering to these criteria entails avoiding information loss, artifacts, or inconsistencies; extracting invariants to represent specific genetic information across different functions or species; effectively compressing the expression to facilitate downstream applications; and accommodating various DNA sequence lengths while considering both local and global features. All DNA representation schemes can be broadly categorized into graphical and numerical schemes.\nGraphical schemes aim at mapping important biochemical features to different geometrical alternatives, including 2-dimensional, 3-dimensional, and higher-dimensional graphical schemes. For example, a 2-dimensional model Randi\u0107 et al. [2003] was proposed to map the four DNA nucleotides to the Cartesian coordinate axes. Despite excelling in visualization, graphical schemes become less practical as sequences grow longer and more features are selected.\nOn the contrary, numerical schemes encode sequence information mathematically, offering effectiveness and scalability in high-dimensional and high-throughput settings. Traditionally, researchers have focused on individual nucleotides, dinucleotides, or trinucleotides, mapping them into numerical representations. For instance, integer encoding maps each nucleotide type to a numeric value Afreixo et al. [2011], Cristea [2001], Kwan and Arniker [2009]. Subsequently, a frequency-based encoding method Alaku\u015f [2023] substitutes the mapped integer with the corresponding nucleotide's frequency, leading to about 10% increase in accuracy in the DNA enhancer classification task. However, these single-nucleotide mapping methods are too simple to represent the complex and high-dimensional structure within the DNA sequences. Many advanced approaches aim to capture more complex subsequences for more comprehensive representations of DNA sequences. For instance, a method employing Empirical Mode Decomposition obtains several intrinsic mode functions from primitive permutations as subcomponents indicating DNA features Bai et al. [2011]. Additionally, the emergence of deep learning methods provides a powerful tool for DNA representation learning. For example, Le et al. Le et al. [2019] utilized sliding windows to obtain K-mers of varying lengths and applied FastText N-Grams to convert them into DNA representations. dna2vec Ng [2017], utilizes the word embedding model word2vec on DNA sequences to obtain representations by capturing meaningful subsequences.\nHigh-quality DNA representation schemes not only contribute to decoding the biochemical information embedded within DNA sequences but also facilitate various DNA-related tasks. For example, researchers designed an efficient coding technique inspired by Huffman coding to compute DNA sequence similarity Jin et al. [2016]. The performance of downstream applications is one of the common evaluation metrics for the effectiveness of representation schemes. Although existing representation schemes have achieved promising performance in many DNA-related applications, there still exist some challenges left unsolved. DNA sequences present inherent challenges in representation learning due to their high dimensionality and complexity, stemming from diversity in composition, length, and permutations. Compounding this complexity is the difficulty in acquiring high-quality datasets, which are essential for training effective representations. This scarcity of data further complicates the task of learning meaningful representations from DNA sequences. Additionally, DNA sequences are inherently noisy due to frequent mutations. To address these challenges, a robust and effective representation scheme is crucial. Such a scheme should be able to extract semantic features from noisy and sparse data, enabling its applicability in important downstream tasks such as DNA classification Li and Lin [2006], Sun et al. [2021b], Le et al. [2019], DNA clustering Randi\u0107 et al. [2003], Randi\u0107 et al. [2003], lan Bai et al. [2007], motif detection Chu and Stormo [2022], and more. By accurately capturing the underlying structure and meaning of DNA sequences, the representation scheme can provide valuable insights into biological processes and facilitate various genomic analyses."}, {"title": "2.2. Explainability in DNA Representation Schemes", "content": "Explainability in DNA representation learning refers to interpreting the biochemical semantic meaning under captured features, which helps biologists comprehend and leverage the knowledge from the extracted features and validate the process of the decision behind the data-mining algorithm.\nPrevious DNA representation schemes primarily relied on explainable artificial intelligence (XAI) to enhance the explainability, such as post-hoc techniques Santorsola and Lescai [2023]. For instance, Li Xue et al. Xue et al. [2019] developed a convolution neural network to predict single-guide RNA function, utilizing convolutional kernel visualization to identify RNA motifs. Besides, models incorporating attention techniques demonstrate effectiveness. AttCRISPR Xiao et al. [2021] incorporates attention modules into the model to indicate the model's decisions on global and local level, which offers great interpretability to the DNA representations. Additionally, some metrics have also been designed to measure the contribution of each element in the representation towards the result of specific applications Lin et al. [2014]. These measurements could be viewed as an indicator of the importance of each feature.\nDespite these advancements, many existing models generate representations that lack an interpretable data structure. Instead, they often rely on attention mechanisms or gradient-based approaches within downstream data-mining algorithms. These complicated techniques challenging for biologists to validate explanations and comprehend insights from the representations. In contrast, our scheme aims to provide DNA sequence representations with an explainable structure by reconstructing DNA sequences as the concatenation of extracted semantic K-mers, which facilitates biologists' understanding and utilization of captured features."}, {"title": "3. Preliminary", "content": "Our main objective is to develop robust and explainable representations derived from DNA sequences. As highlighted by Badri et al. Badri et al. [2014], integrating sparsity constraints significantly improves the robustness of representations, especially in handling outliers and noise inherent in the data. Therefore, we provide concise explanations of two Two widely utilized sparse representation learning frameworks sparse recovery and sparse dictionary learning"}, {"title": "3.1. Sparse Recovery", "content": "In the beginning, Sparse Recovery serves as a potent technique for representing signals of interest by harnessing sparse vectors derived from a restricted set of measurements or observations Cand\u00e8s et al. [2006], Donoho [2006]. In other words, it entails identifying and reconstructing a signal $\\mathbf{y} \\in \\mathbb{R}^{d_y}$ characterized by a sparse representation $\\mathbf{x} \\in \\mathbb{R}^{d_x}$ within a high-dimensional space $\\mathbf{\\Phi} \\in \\mathbb{R}^{d_y*d_x}$ : $\\mathbf{\\Phi} = [\\phi_1,...,\\phi_{\\alpha},]$. Essentially, the problem could be conceptualized into an optimization problem:\n$\\begin{aligned}\n\\arg \\underset{\\mathbf{x} \\in \\mathbb{R}^{d_x}}{min} \\Psi(\\mathbf{x}) \\\\\n \\text { s.t. } \\mathbf{\\Phi x = y}\n\\end{aligned}$\nHere, $\\Psi(\\mathbf{x})$ represents a sparsity constraint on $\\mathbf{x}$.\nOptimization algorithms can be broadly classified into four categories Crespo Marques et al. [2019], Arjoune et al. [2017]: Convex Relaxation, Non-convex Optimization, Greedy Tropp and Gilbert [2007], Wang et al. [2011], Blumensath and Davies [2008], Kumar and Sahoo [2022], and Bayesian approaches Arjoune et al. [2017], Ji and Carin [2007], Subramaniam et al. [2015]. Convex Relaxation category methods are usually efficient in computation leveraging the optimization instruments. For example, Basis Pursuit (BP) Chen and Donoho [1994] employs the $l_1$ norm as a sparsity constraint to acquire the optimal sparse representation, as formulated below:\n$\\begin{aligned}\n&\\arg \\underset{\\mathbf{x} \\in \\mathbb{R}^{d_x}}{min} ||\\mathbf{x}||_1 \\\\\n &\\text { s.t. } \\mathbf{\\Phi x = y}\n\\end{aligned}$\nAlternatively, the original formulation can be transformed using the Lagrange multiplier, and then be solved using either Karush-Kuhn-Tucker (KKT) conditions or gradient descent."}, {"title": "3.2. Sparse Dictionary Learning", "content": "Another important sparse representation learning framework is Sparse Dictionary Learning (SDL), which aims at learning a well-constructed dictionary $\\mathbf{D} \\in \\mathbb{R}^{d_y*d_x}$ to map each input data $\\mathbf{y} \\in \\mathbb{R}^{d_y}$ as a linear combination of dictionary elements. A high-quality dictionary should consist of $n$ elements $\\mathbf{D} = [\\mathbf{d_1}, ..., \\mathbf{d_n}]$ that effectively approximate the original data and transform each input data into a sparse representation $\\mathbf{x} \\in \\mathbb{R}^{d_x}$ Zheng et al. [2015], which could be formulated as the following optimization problem:\n$\\begin{aligned}\n&\\arg \\underset{\\mathbf{D} \\in C, \\mathbf{x} \\in \\mathbb{R}^{d_x}}{min} ||\\mathbf{y \u2013 Dx}||_2 + \\lambda\\Psi(\\mathbf{x}), \\\\\n&\\text { s.t. } \\lambda > 0 \\\\\n&\\text { where } C = {\\mathbf{D} \\in \\mathbb{R}^{d_y*d_x} : ||\\mathbf{d_i}||_2 \\leq 1, \\forall i = 1, ..., n}\n\\end{aligned}$\nMany researchers Hongyi et al. [2021], Tang et al. [2023], Xu et al. [2017] have leveraged sparse dictionary learning for image denoising, representation learning, and so on. Various algorithms have been devised to optimize the original multivariate optimization problem alternatively Zheng et al. [2015]. Among the alternating optimization techniques, Coordinate Gradient Descent (CGD) Wright [2015], Boyd and Vandenberghe [2004], Nesterov [2012], Tseng and Yun [2009] stands out for its simplicity and ability to handle large-scale problems efficiently. The coordinate descent iteratively updates the solution in one dimension at a time, holding the other dimensions constant.\n$\\mathbf{x_i^{(k)} = x_i^{(k-1)} - \\alpha_k i \\frac{\\partial f(x_1^{(k)}, x_2, ...,x_i^{(k)}..., x_{i-1}, x_i, x_{i+1}, ..., x_n^{(k-1)})}{\\partial x_i}}$"}, {"title": "4. Explainable DNA Representation Structure", "content": "In the previous sections, we clarified that the pursuit of an effective and explainable representation scheme for DNA sequences is paramount but challenging. To achieve this, Dy-mer firstly introduces an explainable representation structure for DNA by extracting and preserving the underlying semantic structure. In this section, we first clarify some imperative concepts. Later, the explainable representation structure is designed.\nA DNA sequence usually is a string of $l$ permuted nucleotides $b_i \\in \\{A, T, G, C\\}, 0 \\leq i \\leq l$. We could use one-hot code to represent DNA as a matrix $\\mathbf{d} = [\\delta(b_1) \\delta(b_2) ... \\delta(b_l)]_{4xl}$. Here, $\\delta(b_i)$ denotes the one-hot mapping. For example, DNA AATTCGAT could be written as a matrix d, where each row represents a type of nucleotide and each column represents a position.\n$\\begin{bmatrix}\n1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\n\\end{bmatrix}$\nIn biology, the subsequence of a DNA is denoted as a K-mer. A substring of length $k$ within a DNA d is called a K-mer $b_1b_2...b_k$ where $k \\in \\mathbb{Z}^+$\nSimilarly, each K-mer could be written as a matrix $\\phi = [\\delta(b_1) \\delta(b_2) ... \\delta(b_k)]_{4xk}$. For example, a 4-mer in AATTCGAT can be AAT, the matrix $\\phi$ is\n$\\begin{bmatrix}\n1 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}$\nStudies indicate that biological functions are often associated with K-mers, with similar K-mers typically sharing comparable biochemical properties and functions. Furthermore, researchers have summarized patterns of similar K-mers into motifs. Biologists typically represent motifs using a position weight matrix (PWM) $P \\in [0, 1]^{4xl}$, that each row represents a nucleotide"}, {"title": "5. Methodology", "content": "This section mainly introduces the optimization algorithm. Firstly, we relax the original optimization into a real-number optimization and obtain the optimal solution through coordinate gradient descent. After obtaining the optimal dictionary, a thresholding technique could select frequent dymer from the learned dictionary and we could use the dymer dictionary to represent unseen DNA sequences."}, {"title": "5.1.2. Obtain the Relaxed Solution with Coordinate Gradient Descent", "content": "As mentioned before, sparse recovery tries to find a sparse representation of the original data based on a basis, which could enhance the robustness of the representation. It is equivalent to constructing a K-mer basis and obtaining the sparse assignment matrix of each DNA sequence in our representation scheme. Hence, given a DNA sequence d, we could rewrite the classic sparse recovery framework for DNA representation learning,\n$\\begin{aligned}\n&\\mathbf{A_d^* = \\arg \\underset{A_d}{min} \\Psi(A_d)} \\\\\n &\\text { s.t. } \\hat{\\mathbf{d}} = \\mathbf{d}\n\\end{aligned}$\nHere, $\\Psi$ is a sparse penalty on the assignment matrix, and $\\hat{\\mathbf{d}}$ means the reconstructed DNA sequences. The constraint $\\hat{\\mathbf{d}} = \\mathbf{d}$ requires that the K-mers in the basis accurately reconstruct the DNA sequence. Mathematically, this constraint implies that the squared second norm of their difference is minimized, effectively approaching zero.\n$\\begin{aligned}\n&\\mathbf{A_d^* = \\arg \\underset{A_d}{min} \\Psi(A_d)} \\\\\n &\\text { s.t. } ||\\hat{\\mathbf{d}} - \\mathbf{d}||_2^2 = 0\n\\end{aligned}$\nLater, we can employ the method of Lagrange multipliers to derive the unconstrained optimization problem.\n$\\begin{aligned}\n&\\mathbf{A_d^* = \\arg \\underset{A_d}{min} \\Psi(A_d) + \\lambda ||\\hat{\\mathbf{d}} - \\mathbf{d}||} \\\\\n &\\text { s.t. } \\lambda > 0\n\\end{aligned}$\nNext, we can extend the optimization problem to generate the representation of a batch of DNA sequences simultaneously. Given m DNA sequences {$\\mathbf{d_1,d_2, ...,d_m}$} and a well-constructed basis $\\O$ consisting of n semantic K-mers. The optimal assignment matrices $\\mathbf{A_{d_i}} \\in {\\mathbf{A_{d_1},A_{d_2},...,A_{d_m}}}$ could be obtained by,"}, {"title": "5.1.2.2. Problem Formulation.", "content": "To enhance the computational efficiency for batch data processing, we decide to use tensor notation.\nGiven m DNA sequences matrices {$\\mathbf{d_1,d_2,...,d_m}$}, we can stack them into a 3-dimensional measurement tensor $\\mathbf{X} \\in {0, 1}^{m x 4 x L_1}$, where 4 represents the four different nucleotides (A, T, G, C) and $L_1$ represents the index number in DNA sequences. Each layer along the first dimension of this tensor corresponds to the DNA sequence $\\mathbf{d_i}$ with padding 0.\nThe basis also could be represented as a 3-dimensional tensor $\\mathbf{\\Phi} \\in {0, 1}^{n x 4 x L_2}$, where $L_2$ denotes the maximum length among all K-mers. Each layer along the first dimension of these tensors corresponds to the K-mer matrix $\\phi$ with padding 0.\nSimilarly, all the assignment matrices could be stacked into a 3-dimensional tensor $\\mathbf{A} \\in {0, 1}^{n x m x L_3}$, where n is the number of different K-mers, m represents the number of DNA sequences and $L_3$ represents the maximum index where a K-mer can be placed of any DNA sequences. Each layer along the first dimension of this tensor corresponds to the matrix $A_{\\phi_i}$ with padding 0. Each layer along the second dimension of this tensor corresponds to the assignment matrix $\\mathbf{A_{d_i}}$ with padding 0.\nIn addition, we need a tensor shaped as $\\mathbf{X}$ to store the reconstructed DNA sequences $\\hat{\\mathbf{X}} \\in {0,1}^{m x 4 x L_1}$. According to the rules of convolution, we assume $L_1 = L_2 + L_3 \u2013 1."}, {"title": "5.1.2.3. Accuracy Constraint.", "content": "Initially, it's crucial to represent DNA sequences accurately by selecting the appropriate K-mers with the assignment matrix. Therefore, the disparity between the reconstructed DNA sequence matrix and the original DNA sequence matrix should be minimized. This is achieved through a loss function that quantifies the distance between the two matrices, serving as the accuracy constraint. We denote the set of m original DNA sequences as $\\mathbf{X} = {\\mathbf{d_1,d_2,..., d_m}}$ and the set of reconstructed DNA sequences as $\\hat{\\mathbf{X}} = {\\hat{\\mathbf{d_1}, \\hat{\\mathbf{d_2}}, ...,\\hat{\\mathbf{d_m}}}}$. The distance between $\\mathbf{X}$ and $\\hat{\\mathbf{X}}$, $dist(X, Y)$, is defined as the Reconstruction Loss $L_{RC}$.\n$\\begin{aligned}\nL_{R C} &= dist(\\mathbf{X}, \\hat{\\mathbf{X}})\\\\\n &= \\sum_{i=1}^m ||\\mathbf{d_i - \\hat{d_i}}||^2 = \\sum_{i=1}^m ||\\mathbf{d_i \u2013 A_{d_i} \\odot \\phi}||^2\\\\ \n &= \\sum_{i=1}^m ||\\mathbf{d_i - \\sum_{\\phi \\in \\Phi} A_{d_i} \\odot \\phi_b}||^2 \n\\end{aligned}$\nHere, $||\\bullet||^3$ means the squared second norm.\nThe tensor-based formulation of $L_{RC}$ is\n$\\begin{aligned}\nL_{RC} &= ||\\mathbf{X \u2013 \\hat{X}}||^2 = ||\\mathbf{X \u2013 A \\odot \\Phi}||^2 \\\\\n& \\text { where } X = A \\odot \\Phi \\Leftrightarrow X_{i j l}=\\sum_{k} \\sum_{s} A_{k i s} \\bullet \\Phi_{k j(l-s)}\n\\end{aligned}$"}, {"title": "5.1.2.4. Sparsity Constraints.", "content": "However, given multiple assignment matrices can meet the accuracy constraint, meaningless and insignificant K-mers may be selected due to the complex and noisy DNA data. By employing sparsity constraints, we can handle outliers and noise and capture semantic K-mers to enhance the robustness of representations. Based on the two sparsity constraints can be designed based on the mentioned objectives.\nTo ensure the selection of significant K-mers and exclude noisy or redundant ones, a sparsity constraint termed Basis Complexity ($L_{BC}$) is introduced. This constraint quantifies the count of frequent and important K-mers in the basis $\\Phi$. Its purpose is to penalize insignificant noises or redundant K-mers, thus contributing to a compact and low-rank basis consisting of significant K-mers crucial for biological functions.\nThe significance of a K-mer is assessed based on its utilization in representing DNA sequences, as recorded in the assignment matrix. Alternatively, Basis Complexity can be computed according to the assignment matrix. For a given DNA sequence $\\mathbf{d_i}$, each entry of the assignment matrix $\\mathbf{A_{d_i}}$ serves as a binary variable indicating whether the corresponding K-mer is utilized at each position for reconstructing the DNA sequence. Alternatively, we can determine the significance of a K-mer by examining the maximum value in the corresponding row, indicating whether the DNA sequence utilizes this K-mer in any position. Extending this analysis to all DNA sequences, the maximum value of $\\mathbf{A_{\\phi_j}}$ serves as an indicator of whether the K-mer $\\phi_j$ is utilized across the dataset.\nGiven a basis $\\Phi$, the Basis Complexity is,\n$\\begin{aligned}\nL_{B C} = |\\Phi| = \\sum_{\\phi \\in \\Phi} max \\mathbf{A_{\\phi}}\n\\end{aligned}$\nHowever, the scheme may prioritize short and repetitive K-mers to reduce the Basis Complexity. For example, a basis comprising only A, T, G, C as K-mers greatly decreases the Basis Complexity but overlooks many semantic K-mers. Another constraint on the K-mers used for each DNA sequence becomes necessary. This constraint aims to enforce sparsity in the representation, utilizing as few K-mers as possible to ensure their representativeness and informativeness.\nTo achieve this goal, we define the Average Representation Loss $L_{RE}$, which quantifies the average number of K-mers used to reconstruct DNA sequences. The assignment matrix records binary elements indicating whether a K-mer contributes to sequence reconstruction. Summing these binary elements across the matrix yields the total number of K-mers used for sequence reconstruction. Alternatively, given m assignment matrices for each DNA sequence $A_{d_1}, A_{d_2}, ..., A_{d_m}$, we can then compute the Average Representation Loss by averaging the $L1$ norms of these matrices.\n$\\begin{aligned}\nL_{RE} = \\frac{\\sum_1^m ||A_{d_i}||_1}{m}\n\\end{aligned}$\nHere, $||\\bullet||_1$ means the $L1$ norm of a matrix.\nHaving defined the constraints as three distinct losses, we can now formulate the element-wise optimization problem. Given m original DNA sequences $\\mathbf{X} = {\\mathbf{d_1,d_2,...,d_m}}$, it is expected to reconstruct DNA sequences $\\hat{\\mathbf{X}} = {\\hat{\\mathbf{d_1}, \\hat{\\mathbf{d_2}}, ...,\\hat{\\mathbf{d_m}}}}$ with an well-constructed basis $\\O$ and obtain optimal assignment matrices {$\\mathbf{A_{d_1^*}, A_{d_2^*},..., A_{d_i^*}}$}. The element-wise objective function is,\n$\\begin{aligned}\n\\mathbf{A_{d_i^*}} &= \\arg \\underset{A_{d_i} \\in A, A_{d_i} \\in {0,1}^{n x l}}{min} (\\alpha_{RC} L_{RC} + \\alpha_{BC} L_{BC} + \\alpha_{RE} L_{RE}) \\\\\n&= \\arg \\underset{A_{d_i} \\in A, A_{d_i} \\in {0,1}^{n x l}}{min} \\alpha_{RC} ( di - \\sum_{\\phi \\in \\Phi} \\Phi_{\\phi_b} * A_{\\phi}^{1d} ||_2^2) \\\\\n&+ \\alpha_{BC} \\sum_{\\Phi \\in \\Phi} max \\mathbf{A_{\\phi}} + \\alpha_{RE} \\frac{\\sum |A||_1}{m}\\\\\n \\text { s.t. } \\alpha_{RC} > 0, \\alpha_{BC} > 0, \\alpha_{RE} > 0\n\\end{aligned}$\nHere, I denotes the maximum length of DNA sequences. $\\alpha_{RC}$ refers to the $A$ in the previous framework. $\\alpha_{BC}$ and $\\alpha_{RE}$ are the weights to regulate between two different sparsity constraints.\nUsing tensor notation, the Basis Complexity cost is\n$\\begin{aligned}\nL_{B C} &= \\sum_{j=1}^m max A_{\\phi_j::} \\\\\n&=\\min ((\\mathbf{A \\times_1 \\iota_{|1|2}} ) \\times_1 \\iota_{0,1_m}) \\times_1 \\iota_{1_m}\\\\\n& where \\iota_m = \\begin{bmatrix}\n1 & 1 & ... & 1 \\\\\n1 & 1 & ... & 1\n\\end{bmatrix}\\\\\n& \\iota_{0,1_m} = \\begin{bmatrix}\n1 & 0 & ... & 0 \\\\\n0 & 1 & ... & 0 \\\\\n... & ... & ... & ... \\\\\n0 & 0 & ... & 1\n\\end{bmatrix}\\\\\n& \\iota_{n \\times n} = E_n \\odot \\iota_{0,1_m}\n\\end{aligned}$\nmeans the Kronecker product and $\\times_1$ means the left multiplication between a matrix and a mode-1 Matricization tensor.\nThen, we could compute the representation cost loss $L_{RE}$.\n$\\begin{aligned}\nL_{RE} &= \\frac{\\sum ||A_{d::}||_1}{m} = \\frac{||A||_1}{m}\n\\end{aligned}$\nFinally, the objective function could be adapted into a tensor format. Given DNA sequences $\\mathbf{X}$, and a basis $\\mathbf{\\Phi}$, we want to find the optimal assignment matrix $\\mathbf{A^*}$. The objective function is:\n$\\begin{aligned}\n\\mathbf{A^*} &= arg \\underset{A \\in {0,1}^{n \\times m \\times L_3}}{min} (\\alpha_{RC} L_{RC} + \\alpha_{BC} L_{BC} + \\alpha_{RE} L_{RE}) \\\\\n&= arg \\underset{A \\in {0,1}^{n \\times m \\times L_3}}{min} (\\alpha_{RC} ||X \u2013 A \\odot \\Phi||^2 \\\\\n&+ \\$\\alpha_{BC} \\sum_{j=1}^n max A_{\\phi j::} + \\alpha_{RE} \\frac{\\sum_{j=1}^m ||A::j||_1}{m})\n\\end{aligned}$\nSince Assignment tensors are binary, the original formulation (??) is a Combinatorial Optimization. Similar to the sparse recovery based method, if the motif in the dictionary is fixed, the optimization problem is reducible to a known NP-hard problem, the knapsack problem. Hence, we relax the problem into a real-number optimization by converting the assignment tensor into a real-number tensor within [0, 1]. Each element of the real-number assignment tensor indicates the likelihood of assigning a motif at a specific position for a DNA sequence. After the relaxation, we could solve the real-number multivariate optimization problem to get the optimal dictionary and sparse representation.\nAs mentioned in Section 3.4, the joint optimization of the dictionary and the assignment tensor is nearly infeasible. Consequently, we propose an alternative optimization approach to tackle this multivariate optimization challenge. At each time step t, we begin by fixing the dictionary $\\mathbf{D_{t-1}}$ from the previous iteration and then determine the optimal assignment tensor $\\mathbf{A_t}$. Then, the dictionary $\\mathbf{D_t}$ is updated by fixing the assignment tensor. This iterative process is repeated until the loss converges or both the dictionary and the assignment tensor become stable. Finally, we get the optimal dictionary $\\mathbf{D^*}$ and assignment tensor $\\mathbf{A^*}$.\nThe proposed alternative optimization algorithm encompasses two primary steps: sparse representation learning and dictionary learning. This approach allows for a more manageable optimization of the complex multivariate problem by sequentially updating the dictionary and assignment tensor, thus facilitating convergence to the optimal solution."}, {"title": "5.1.2.5. Sparse representation learning.", "content": "In each timestamp t, we treat the dictionary as fixed and focus on optimizing the assignment tensor. This is achieved by solving the following optimization problem:\n$\\begin{aligned}\n\\mathbf{A_t} &= arg \\underset{A_t \\in [0,1]^{n \\times m \\times L_3}}{min} \\alpha_{RC} ||X \u2013 A_{t-1} \\odot D_{t-1}||^2 \\\\\n&+ \\alpha_{DC} \\min((\\mathbf{A_{t-1} \\times_1 \\iota_{|1|2}} ) \\times_1 \\iota_{0,1_m}) \\times_1 \\iota_{1_m} + \\alpha_{RE} \\frac{||A_{t-1}F}{m}\\\\\n \\text { s.t. } \\alpha_{RC} > 0, \\alpha_{DC} > 0, \\alpha_{RE} > 0\n\\end{aligned}$"}, {"title": "5.1.2.6. Dictionary Update.", "content": "After updating the assignment tensor $\\mathbf{A_t}$, we then fixed it to optimize the dictionary. This involves solving the following optimization problem:\n$\\begin{aligned}\n\\mathbf{D_t} &= arg \\underset{D_t \\in [0,1]^{n \\times m \\times L_3}}{min} \\alpha_{RC} ||X \u2013 A_{t} \\odot D_{t-1}||^2 \\\\\n \\text { s.t. } \\alpha_{RC} > 0\n\\end{aligned}$\nAfter solving the optimization problem, we get an updated dictionary $\\mathbf{D_t}$. As discussed in the previous scheme, the convolution operation constitutes a linear computation among the elements of each operand. Consequently, the optimization problem can be simplified to a quadratic optimization, which is convex.\nHowever, it is not necessary to solve the optimal solution in each iteration, because the time complexity will greatly increase as data volume grows. For simplicity, we use gradient descent as a method to alternately update the assignment tensor and dictionary. We use the coordinate gradient descent, mentioned in section 2.5, as a substitution. The coordinate gradient descent only updates parameters in one dimension or a small subset of dimensions and fixes other dimensions. Especially when dimensions are independent or approximately independent, the coordinate gradient descent method can be very efficient in computation and convergence. Hence, this strategy enhances the computational efficiency, without the necessity of finding the absolute optimal solution at every iteration."}, {"title": "5.1.3. Obtain Frequent dymer Dictionary by Thresholding", "content": "Similarly, frequent motifs could be selected from optimal dictionary $\\mathbf{D^*}$ according to the assignment tensor $\\mathbf{A^*}$ as a dymer dictionary.\nFirstly, we compute the average likelihood $L_{P_j}$ of a motif $P_j$ and construct a vector of likelihoods for all motifs $\\mathbf{L} = [L_{P_1}, L_{P_2}, ..., L_{P_n}]$.\n$\\begin{aligned}\nL_{P_i} = \\frac{\\sum_{i=1}^m (A_{P_i})}{m}\n\\end{aligned}$\nThen, we set a threshold and use the indicator function $\\Delta_{\\epsilon}$ to transfer the likelihood vector into a binary indicator vector $\\mathbf{I} = \\Delta_{\\epsilon}(\\mathbf{L})$. Finally, we extract the frequent motifs as an $\\epsilon$-frequent dymer dictionary $\\epsilon$\\-D*.\n$\\begin{aligned}\n&\\mathbf{D^*} = D \\times_1 (I 1_n)\n\\end{aligned}$\nHere, $\\mathbf{1_n}$ represents a vector of length n with all elements being 1."}, {"title": "5.1.4. Represent Unseen DNA Sequence by Frequent Dymer Dictionary", "content": "Since we have obtained $\\epsilon$-frequent dymer dictionary $\\epsilon$\\-D*, we can represent new DNA sequences with these semantic motifs.\nTo proceed, we first get the tensor of new DNA sequences, denoted as $\\mathbf{X_{new}}$.\n$\\begin{aligned}\n\\mathbf{A^*} &= arg \\underset{A}{min} (\\alpha_{RC} L_{RC} + \\alpha_{DC} L_{DC} + \\alpha_{RE} L_{RE}) \\\\\n &\\text { s.t. } \\alpha_{RC} > 0, \\alpha_{DC} > 0, \\alpha_{RE} > 0, \\\\\n &D = D^*, X = X_{new}\n\\end{aligned}$\nSubsequently, we solve the optimization problem to obtain the representation for unseen DNA sequences. In this section, we evaluate the sparse dictionary learning based representation scheme by its performance on DNA promoter classification. Then, we compare the time complexity and space complexity between the sparse recovery based representation scheme and the sparse dictionary based scheme.\nWith the constraints defined and the optimization problem formulated, the next step is to solve it to derive the optimal assignment matrix for each DNA sequence $d_i$, serving as the DNA representation."}, {"title": "5.2. Experiments", "content": "In this section, we evaluate the effectiveness and explainability of our sparse recovery-based representation scheme through a critical DNA-related task: DNA promoter classification. Subsequently, we can capitalize on the explainability of our representation scheme for various downstream applications, including DNA clustering and motif detection."}, {"title": "5.2.1. Experiment Setup", "content": "DNA promoters play a crucial role in gene expression regulation and transcription, making accurate identification of these segments essential for understanding various biological processes.\nResearchers have identified several validated motifs that commonly appear in promoters, including CpG islands Jabbari and Bernardi [2004], CCAAT box Romier et al. [2002], TATA Box Lifton et al. [1978], GC box Lundin et al. [1994] and transcriptional initiator Javahery et al. [1994]. These motifs are typically represented using the IUPAC one-letter codes, where A, T, G, C denote their respective nucleotides. R denotes purine(A/G), and Y signifies Pyrimidines(T/C) and W represents Weak interaction (A/T). Due to the inherent mutability of DNA, variations such as insertions, deletions, and point mutations are common. Therefore, their motifs exhibit diversity. For instance, the motifs of the CCAAT box may include patterns like GGCCAATCT or simply CCAAT, among other variations. Similarly, the TATA box typically manifests as motifs such as TATAWAW or other similar forms. The most common motif for GC box is GGGCGG, while the transcription initiator motif often appears as YYANWYY. Furthermore, promoters, especially strong ones, tend to be rich in GC regions compared to non-promoters.\nAn effective and explainable DNA representation scheme should be capable of selecting various semantic K-mers corresponding to the aforementioned motifs as indicators of promoters, thereby facilitating DNA promoter classification. Therefore, we evaluate the effectiveness and explainability of our representation scheme through the DNA promoter classification task."}, {"title": "5.2.2. Metrics and Model Structure", "content": "The analysis involves two primary tasks: firstly, classifying promoters from the remaining non-promoter sequences; secondly, categorizing the promoters into strong promoters and weak promoters.\nWe use 4 indicators to evaluate the model performance.\n$\\begin{aligned}\nDefinition 2.&(i) Sensitivity(Sens) = 1 - \\frac{N^-}{N^+}, 0 \\leq Sen \\leq 1\\\\\n &(ii) Specificity (Spec) = 1 - \\frac{N^+}{N^-}, 0 \\leq S pec \\leq 1\\\\\n &(iii) accuracy (Acc) = 1-\\frac{N_{err}}{N_{total}}, 0 \\leq Acc \\leq 1\\\\\n &(iv) Matthews Correlation Coefficient (MCC) \\\\\n&=\\frac{\\left(N_{++} N_{--}-N_{+-} N_{-+}\\right)}{\\sqrt{\\left(N_{++}+N_{+-}\\right)\\left(N_{++}+N_{-+}\\right)\\left(N_{--}+N_{+-}\\right)\\left(N_{--}+N_{-+}\\right)}},-1 < MCC \\leq 1\n\\end{aligned}$\nInitially, we transform all DNA sequences into numerical representations using our DNA representation scheme. Once we obtain the DNA representations, the next step involves designing a classification model for the DNA promoter classification task. Given that we represent each DNA sequence with a"}, {"title": "5.2.3. Hyperparameter Selection", "content": "Several hyperparameters need to be determined for the experiments. Firstly, it's crucial to set a suitable frequency threshold F and establish maximal and minimal restrictions on the length of K-mers to construct a high-quality basis. Additionally, choosing the optimal Lagrange multipliers for the optimization problem is essential. The optimal hyperparameters are selected by a 10-fold cross-validation on the training set with a learning rate of 0.01 and an epoch number of 500. After obtaining the DNA representations, we utilize the accuracy of the first classification task as the metric. The learning rate and epoch number of the classification model are set as 0.01 and 25, respectively.\nWe collect the classification accuracy for various basis sizes in Observations reveal that the optimal number of K-mers in the basis is 1,400, which strikes a balance between being neither too large nor too small. A large basis may include noisy K-mers, while a small basis may overlook important ones. Therefore, we set the frequency threshold F to 6% and restrict the length of K-mers to between 3 and 12 base pairs.\nThen, we explore various configurations of the Lagrange multipliers during the process of learning DNA representations with each classification accuracy summarized in Table 7. Based on the results, we set the Lagrange multipliers as $\\alpha_{RC} : \\alpha_{BC} : \\alpha_{RE} = 1:1:5$"}, {"title": "5.2.4. Numerical Result", "content": "After determining all the hyperparameters, we conducted multiple trials to compare the results of our representation scheme with other existing methods. Initially, we learned the DNA representations by setting a learning rate of 0.01 and an epoch number of 2,500. Subsequently, these DNA representations were split into a training set and a testing set to serve as inputs for the classification models for two tasks, respectively. Finally, we collected the results of our scheme and existing schemes to compare their effectiveness. According to Table 1, our scheme consistently outperforms all previous methods on all metrics on the benchmark dataset. Additionally, through 10-fold cross-validation detailed in Table 2, our model exhibits great stability and effectiveness.\nFurthermore, in terms of interpretability, we examined the top50 frequent K-mers from the dymer dictionary extracted by our method in strong promoters, weak promoters, and non-promoters, respectively. This analysis aimed to determine whether the scheme extracts semantic K-mers to represent DNA sequences.\nHere are some solid findings:\n(i) In promoters, both weak and strong, there is a notable preference for GC-rich regions (GC Box), especially in strong promoters. For example, K-mers like GCGCGC, CGCGCG are 3 times more prevalent in promoters compared to non-promoters. CGCGCGC is even 15 times more frequent in promoters. Additionally, K-mers such as GGCCCG, GCCGCA, GGGCGG exhibit a higher likelihood in strong promoters compared to weak promoters.\n(ii) In promoters, the TATA box region is characterized by frequent occurrences of K-mers like TATAAAA, TATAAAT, TATAAA and TATAAT. Similarly, the CCAAT box region exhibits enrichment with K-mers such as GC-CAAT, CAATCT.\n(iii) Furthermore, there are K-mers such as TCAC\u0422\u0421\u0421, TTAAATT, TTATTTT, CCATTTT and CTATTTT that resemble the initiator elements motif YYANWYY. These K-mers appear at least 3 times more frequently in promoters than in non-promoters.\nBased on the results mentioned, it's evident that our representation scheme can generate effective and explainable representations for DNA sequences."}, {"title": "5.2.5. Ablation Experiments", "content": "In this section, we conducted ablation experiments on the three losses in our representation scheme, assessing their impact on classification accuracy for the first task. Initially, we obtained DNA representations using different objective functions of the optimization problem, treating each as a distinct model. We used a learning rate of 0.01 and an epoch number of 500 for this purpose. Subsequently, we inputted the representations of these different models into the classification model for the first task, employing a learning rate of 0.01 and 500 epoch numbers. Finally, we collected the classification accuracy data presented in Table 6. The table indicates that these two sparsity constraints indeed contribute to the effectiveness of the representation."}, {"title": "5.2.6. DNA Clustering and Phylogenetic Tree", "content": "DNA, as the carrier of evolution-related biological information, elucidates relationships among species through similarities. Our representation scheme reconstructs the DNA sequence with K-mers possessing explanatory ability and biochemical meaning. As a result, it facilitates the hierarchical clustering of DNA sequences and enables the generation of a phylogenetic tree, which illustrates the evolutionary relationships among different species."}, {"title": "5.2.6.3. Experiments.", "content": "We compute the similarity between m different DNA representations $\\mathbf{A_{\\phi_i}}$, and obtain the distance matrix $\\mathbf{D_{is}} \\in \\mathbb{R}^{m*m}$. Here, we use the cosine similarity. Then, hierarchical clustering is conducted based on the distance matrix $\\mathbf{D_{is}}$, and the resulting clusters are visualized in a phylogenetic tree."}, {"title": "5.6.1. Performance on Small-scale Dataset", "content": "To compare the effectiveness of these two schemes on datasets of different sizes, we employed the same learning rate and epoch number for both. We learned DNA expression through both schemes on a range of DNA datasets, varying in size. Then, we inputted the obtained representations into the classification model and measured the accuracy of the first classification task through 10-fold cross-validation. Additionally, we compared the effects of the two methods under different sizes of K-mers/Motifs for generalizability. The results are illustrated in Figure 12."}, {"title": "5.6.2. Computational Efficiency", "content": "Additionally, the SDL-based method enhances the efficiency of representation learning by extracting motifs to represent DNA sequences. These motifs reduce the dictionary size and representation dimensionality due to their generalizability. Figure 13 illustrates the performance of the two schemes in terms of accuracy for the first classification task, with representations of different dimensionalities learned from each scheme on the same dataset, using the same learning rate and epoch number. From Figure 13, it displays that the SDL-based scheme generates representations with significantly lower dimensionality compared to the SR-based scheme while achieving similar or even better performance. For instance, the 800-dimensional representation from the SDL-based scheme competes favorably with the 1,900-dimensional representation from the SR-based scheme. This is attributed to the fact that motifs are more representative than K-mers, with a single motif capable of representing several similar K-mers. As a result, the SDL-based method constructs a more compact dictionary, achieving comparable performance in DNA promoter classification relative to the SR-based method.\nTherefore, the SDL-based method notably boosts model efficiency by eliminating the data preprocessing section and reducing the dictionary size compared to the SR-based method. The SR-based approach heavily relies on hyperparameters for sampling frequent K-mers as the basis, which requires obtaining the frequency distribution of all possible K-mers from the DNA sequences' K-mer spectrum before sampling-a process that incurs significant time expenditure.\nTransitioning to a learnable dictionary in the SDL-based method circumvents the need for sampling, substantially reducing the average running time. As indicated in Table ??, the SDL-based representation scheme significantly enhances model efficiency by obviating preprocessing time. The time complexity in sampling is O(mn). This means that the time consumption will increase at a rate with the product of the number of K-mers n and the amount of DNA m. Additionally, by employing merely 800 motifs, the SDL-based method achieves representations that are comparable to those learned from 1,900 K-mers.\nEfficiency improvement is crucial given the typically long and dynamic nature of DNA sequences. Excessive time spent on data preprocessing and model training significantly undermines the effectiveness and feasibility of our representation scheme. Moreover, high-dimensional DNA representations could complicate models in downstream applications. Hence, the remarkable model efficiency of the SDL-based model not only improves training efficiency but also provides low-dimensional DNA representations, facilitating downstream applications.\nFrom all the experimental results, we could conclude that:\ni. If the dataset contains a large number of DNA sequences and there are ample storage resources, with no urgency for time constraints, the SR-based scheme can generate excellent DNA representations. However, it may require a long training time.\nii. On the other hand, if the dataset is limited in size or if space and time resources are constrained, and there is a need for relatively effective DNA representations, the SDL-based scheme is a preferable choice."}, {"title": "6. Conclusion", "content": "In this study, we have proposed an innovative DNA sequence representation scheme, Dy-mer, inspired by Sparse Recovery, to address the limitations of existing methods in capturing the explainable and high-dimensional structure of DNA sequences. By formulating the problem as a tensor-based optimization, we successfully extract a frequent dymer dictionary comprising dynamic-length K-mers, enabling the reconstruction of DNA sequences through simple concatenation. Dy-mer offers a transparent framework for interpreting biological information encoded in DNA, ensuring both robustness and explainability in DNA representation.\nThrough extensive experiments, we have demonstrated the effectiveness of Dy-mer in various biological applications, particularly in DNA promoter classification. Our approach achieved a remarkable 13% increase in accuracy compared to existing methods. Furthermore, Dy-mer proves invaluable in DNA clustering and motif detection, providing rich interpretability essential for advancing biological research.\nThe success of Dy-mer underscores the importance of developing innovative and explainable DNA representation schemes to fully harness the potential of machine learning in deciphering biological data. By providing both robustness and interpretability, Dy-mer paves the way for advancements in understanding DNA sequences and their role in biological processes. In future endeavors, we aim to explore the adaptation of dictionary learning methods to enhance the performance of DNA representation further."}]}