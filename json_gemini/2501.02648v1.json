{"title": "Representation Learning of Lab Values via Masked AutoEncoder", "authors": ["David Restrepo", "Chenwei Wu", "Yueran Jia", "Jaden K. Sun", "Jack Gallifant", "Catherine G. Bielick", "Yugang Jia", "Leo A. Celi"], "abstract": "Accurate imputation of missing laboratory values in electronic health records (EHRs) is critical to enable robust clinical predictions and reduce biases in AI systems in healthcare. Existing methods, such as variational autoencoders (VAEs) and decision tree-based approaches such as XGBoost, struggle to model the complex temporal and contextual dependencies in EHR data, mainly in underrepresented groups. In this work, we propose Lab-MAE, a novel transformer-based masked autoencoder framework that leverages self-supervised learning for the imputation of continuous sequential lab values. Lab-MAE introduces a structured encoding scheme that jointly models laboratory test values and their corresponding timestamps, enabling explicit capturing temporal dependencies. Empirical evaluation on the MIMIC-IV dataset demonstrates that Lab-MAE significantly outperforms the state-of-the-art baselines such as XGBoost across multiple metrics, including root mean square error (RMSE), R-squared (R2), and Wasserstein distance (WD). Notably, Lab-MAE achieves equitable performance across demographic groups of patients, advancing fairness in clinical predictions. We further investigate the role of follow-up laboratory values as potential shortcut features, revealing Lab-MAE's robustness in scenarios where such data is unavailable. The findings suggest that our transformer-based architecture, adapted to the characteristics of the EHR data, offers a foundation model for more accurate and fair clinical imputation models. In addition, we measure and compare the carbon footprint of Lab-MAE with the baseline XGBoost model, highlighting its environmental requirements.", "sections": [{"title": "1. Introduction", "content": "Laboratory values play a pivotal role in real-time clinical care by demonstrating a patient's baseline physiology, generating a differential diagnosis for acute or chronic illnesses, and guiding prognosis. The effectiveness of machine learning (ML) models in leveraging laboratory data from electronic health records (EHRs) is often hampered by the prevalence of missing values Luo (2022); Austin et al. (2021), which can severely affect model performance and introduce harmful bias in clinical implementation Riley et al. (2024). In addition to technical challenges, the social patterning inherent in the data generation, often drives missing data in clinical datasets. Factors such as socioeconomic status, access to healthcare, and systemic biases can significantly influence the availability of laboratory results, affecting underrepresented groups and introducing skew into clinical datasets Teotia et al. (2024). Optimal handling of missing data in this context is a critical challenge, as it directly affects the reliability of clinical models in healthcare settings.\nConventional imputation techniques, such as mean and standard deviation-based substitutions, are not well-suited to clinical tasks due to highly contextualized and individualized interpretation. These simplistic approaches fail to capture the intricate temporal and inter-variable dependencies present in high-dimensional physiological data Li et al. (2021). For example, the clinical importance of high Creatinine values in the hospitalized setting depends heavily on the patient's own baseline values and the presence of any of the potential causes of acute kidney injury, including sepsis, hemorrhage, iatrogenic causes, urinary tract obstruction, and more. Some advanced methods, such as training multiple tabular models such as XGBoost Chen and Guestrin (2016) for individual lab values, have been used to capture these data relations in lab values Zhang et al. (2020); Chen and Guestrin (2016). However, even these models often struggle to fully leverage the available information, leading to suboptimal solutions that may overlook valuable contextual details Waljee et al. (2013); Luo et al. (2016).\nRecent advances in self-supervised learning have opened new avenues for handling missing data by learning robust representations from the available data itself. Previous work on data imputation in EHR datasets has predominantly employed Variational Autoencoders (VAE) Kingma (2013) due to their ability to model latent distributions and generate plausible imputations Zamanzadeh et al. (2021). However, recent studies have shown that Masked Autoencoders (MAEs) offer notable improvements over VAEs, particularly in their ability to reconstruct high-dimensional data with fewer assumptions on the latent space and greater capacity to learn complex feature dependencies directly from the data He et al. (2022); Bao et al. (2021). These advances highlight the potential of MAEs to deliver more accurate and context-aware imputations for clinical data, motivating the development of our proposed framework.\nIn addition, transformer-based models have shown promise in fields such as natural language processing Devlin (2018); Vaswani (2017); Renc et al. (2024) and computer vision Dosovitskiy (2020); Parvaiz et al. (2023) due to their ability to model complex patterns and relationships within the data. Despite all the performance and results shown by transformer models in fields such as computer vision and natural language processing, the creation of foundation models and the training of transformer models for tabular data is a field that still needs to be further explored van Breugel and van der Schaar (2024). Some recent previous works have shown that deep learning models can improve classical models such as XGBoost Chen and Guestrin (2016) on tabular data tasks Kadra et al. (2021). Additionally, attention-based methods have demonstrated significant promise in tabular data imputation Lee and Kim (2023); Wu et al. (2020). Kowsar et al. Kowsar et al. (2024) proposed an attention-based missing value imputation framework that leverages self-attention and between-sample attention mechanisms to reconstruct missing data. Their method surpasses classical machine learning approaches, such as decision-tree-based imputation, and achieves superior performance on several EHR datasets. The potential of self-supervised pretraining for clinical data is immense, especially when combined with the transformer architecture, and techniques such as masked autoencoding, which aim to learn from the inherent structure of the data"}, {"title": "2. Methods", "content": null}, {"title": "2.1. Datasets", "content": "The dataset used in this study is derived from the MIMIC-IV database Johnson et al. (2020, 2023), which contains de-identified health records of patients admitted to critical care units at the Beth Israel Deaconess Medical Center between 2008 and 2019. Our focus was on the top 100 most common lab values, selected based on their occurrence in patient records. The cohort comprises data of 1,417,738 stays for training and 100,000 stays for evaluation. These cohorts were extracted and processed using Google BigQuery."}, {"title": "2.2. Data Processing", "content": "We utilized SQL queries through Google BigQuery to extract lab event data from the labevents table of MIMIC-IV Johnson et al. (2020, 2023). The data includes unique hospital admission IDs (hadm_id), patient race information for a further fairness and bias evaluation, lab test item IDs (itemid), lab test timestamps (charttime), and corresponding numerical lab values (valuenum). The dataset was preprocessed to remove invalid lab values (e.g., negative values) and filtered to include only valid, positive measurements.\nFor each patient admission, the earliest recorded timestamp for each lab test was used as a reference point. Additional columns were computed to represent the difference between each lab test's timestamp and this reference. The numerical lab values were then normalized using quantile normalization to limit extreme outliers.\nWe also calculated follow-up values for each lab test (denoted as npval_last_id) by tracking subsequent tests performed within the same admission. For each lab test and follow-up, a corresponding time difference column was generated (denoted as nptime_id), representing the time elapsed since the reference point. The extracted data was further partitioned into training and test sets based on the timestamp, with admissions prior to the year 2179 used for training and those afterward for testing to avoid data leakage during evaluation.\nThe data preprocessing resulted in a train set of 1,417,738 rows used for training and an independent test set of 100.000 rows. Each dataset contains 3"}, {"title": "2.3. Foundation Lab-MAE Architecture", "content": "We base our foundation laboratory imputation model (Lab-MAE) on a Masked Autoencoder architecture, inspired by the Remasker framework Du et al. (2023). This architecture utilizes a Transformer backbone to capture complex correlations between lab values over time, providing robust imputation of missing data in medical records. The model is composed of an encoder-decoder structure that is trained in a self-supervised manner by masking portions of the input and reconstructing the masked values.\nThe model uses learned positional encodings to represent the unique lab IDs, and timestamps ensuring that each lab value and timestamp is always passed to the model in the same positional slot in the input sequence. This approach allows the model to consistently interpret each lab test and time, regardless of missing values or the presence of other tests. Specifically, the lab values are placed in predefined positions, and the timestamps corresponding to those lab tests are placed in the following position. This design enables the model to capture temporal relationships between the lab values and their corresponding times.\nIn this sense, let $x \\in \\mathbb{R}^{L \\times d}$ represent the input sequence, where $L$ is the sequence length (including both lab values and timestamps) and $d$ is the embedding dimension of each token. The learned positional encodings $P \\in \\mathbb{R}^{L \\times d}$ are added to the input sequence as follows:\n$z_0 = x + P$ (1)\nwhere $z_0$ is the input to the encoder. Positional encodings align lab values with corresponding timestamps, capturing temporal relationships.\nThen, the encoder consists of multiple layers of Transformer blocks, where each block includes multi-head self-attention and feed-forward layers. The self-attention mechanism is defined as in the original \"attention is all you need\" paper Vaswani (2017):\n$\\text{Attention}(Q, K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$ (2)\nwhere $Q$, $K$, and $V$ represent the query, key, and value matrices, respectively, and $d_k$ is the dimensionality of the key vectors. To address the specific challenge of missing values in our dataset, we introduce a missing value attention mask, which prevents missing values from influencing the attention computation. Let $M \\in \\{0,1\\}^{L \\times L}$ represent the attention mask, where $M_{ij} = 0$ indicates a missing value, and the corresponding attention score is masked out:\n$A_{ij} = \\begin{cases} A & \\text{if } M_{ij} = 1\\\\ -\\infty, & \\text{if } M_{ij} = 0 \\end{cases}$ (3)\nThe decoder reconstructs the masked values by aggregating learnable masked tokens in the masked positions from the latent representation generated by the encoder. To ensure that missing values do not bias the model's predictions, we introduce modifications to the loss function. Specifically, the reconstruction loss is only calculated for observed values (current and masked values), while missing values are ignored. Given the predicted values $\\hat{x}$ and true values $x$, the loss function $L$ is defined as:\n$L = \\frac{1}{L}\\sum_{i=1}^{L} (1-m_i)(1-m_i)(x_i - \\hat{x_i})^2$ (4)\nwhere $m_i$ is the missingness indicator for each value. This ensures that the model focuses on reconstructing only the available data, and prevents the overfitting to missing values."}, {"title": "2.4. Lab-MAE Training", "content": "The Lab-MAE model was trained using a self-supervised learning approach with a focus on imputing missing values in clinical lab data. The training process involved several stages, including data preprocessing, model setup, optimization, and evaluation, designed to maximize the model's ability to predict missing lab values from the MIMIC-IV dataset Johnson et al. (2020, 2023).\nBefore training, the dataset underwent to a preprocessing to handle missing values and remove irrelevant or redundant data points. Rows with fewer than 17 non-missing lab and time values were excluded from the training set. The remaining data"}, {"title": "2.5. Lab-MAE Imputation Evaluation", "content": null}, {"title": "2.5.1. BASELINE MODELS", "content": "To benchmark the performance of our proposed foundation Lab-MAE model for lab value imputation, we implemented a set of baseline models using XGBoost.\nXGBoost was selected for its well-known capabilities in handling tabular data tasks, including its ability to directly manage missing values without requiring extensive preprocessing. A total of 100 separate XGBoost models, value were trained, one for each lab value, to provide a robust comparison against our foundation Lab-MAE model.\nThe training of these XGBoost models involved a hyperparameter optimization process using GridSearchCV to identify the best-performing configuration for each lab-specific model. The hyperparameter search was conducted using the following grid: Learning rate: [0.01, 0.1, 0.2], Max depth: [3, 4, 5], Number of estimators: [50, 100, 200].\nGridSearchCV was applied with three-fold cross-validation to evaluate different parameter combinations, aiming to minimize the MSE of the lab value of interest. After identifying the optimal hyperparameters, each model was retrained using the entire training dataset to ensure the best possible predictive performance.\nThese trained XGBoost models served as our baseline for comparison with the Lab-MAE model. The key objective was to leverage XGBoost's capability in handling missing data and its structured tree-based learning mechanism to predict individual lab values based on the contextual data."}, {"title": "2.5.2. FOUNDATION LAB-MAE MODEL VS. XGB EVALUATION SETUP", "content": "Both the Lab-MAE and XGBoost models were evaluated using a cohort of 100,000 data points extracted from our independent test set. To ensure a fair comparison, the same data points were used across both models during evaluation. We computed three primary metrics for each lab value: RMSE, R2, and the WD. The R2 metric was used as the main reference due to its ability to assess the correlation strength and its capacity to avoid overfitting to mean or most common values. The WD was used to assess how well the model captures the overall distribution of lab values, including extreme values, unlike RMSE and R2, which emphasize overall accuracy and variance explained. WD complements RMSE and R2 by highlighting the quality of predictions in capturing not only central tendencies but also the full range of values, particularly the extremes. More information about the metrics is avaiable in appendix A."}, {"title": "2.5.3. TEST SET EVALUATION PROCESS", "content": "The evaluation was conducted lab-by-lab to ensure comprehensive and robust performance assessment. For each lab value, we simulated missing data by masking its existing values in the test dataset, effectively challenging the models to predict these masked values using the remaining available context, including other lab values and the associated timestamps.\nDuring inference with the Lab-MAE model, we ensured that no gradients were calculated, and the random masking ratio was set to zero, focusing purely on the prediction task.\nSimilarly, the XGBoost models were applied in a lab-specific manner, where each lab's missing values were predicted using the corresponding XGBoost model trained for that specific lab. This approach allowed the XGBoost models to leverage their tree-based structure to effectively utilize the available data for predictions."}, {"title": "2.6. Fairness and Bias Analysis", "content": "The fairness and bias analysis in our study aimed to evaluate how well the Lab-MAE model performs across different demographic groups, particularly focusing on racial differences, and to assess the impact"}, {"title": "2.6.1. LAB-MAE MODEL FAIRNESS ACROSS RACE GROUPS", "content": "To ensure that the Lab-MAE model's predictions are equitable across different racial groups, we conducted an analysis of its performance for five race groups: White, Black, Hispanic, Asian, and Others. We compared the performance of the Lab-MAE model with the baseline XGBoost models for each race group using the same metrics.\nFor each race, the imputation model's performance was calculated by comparing the predicted values to the actual lab values, using the following approach:\n\u2022 Filter the test dataset to include only the records for the race group being evaluated.\n\u2022 Mask the lab values to simulate missing data and use the Lab-MAE model to predict these values.\n\u2022 Calculate the metrics WD, RMSE, and R2 for each lab value to quantify the model's prediction performance for the respective race group.\nThe results were consolidated into a single dataframe to allow a comparative analysis of the model's fairness across racial categories."}, {"title": "2.6.2. CARBON FOOTPRINT MEASUREMENT", "content": "We assessed the carbon footprint of the Lab-MAE and baseline XGBoost models using the CodeCarbon library Courty et al. (2024). Emissions were estimated during the inference process for batch sizes of 1, 32, and 64 across multiple geographic locations, including Colombia, USA, France, Uganda, Philippines, and Australia. These locations were chosen to represent diverse geographic profiles in Asia, Africa, North America, South America, Europe, and Australia.\nThe models were evaluated by performing inference on subsets on the test dataset comparing the performance of the Lab-MAE model vs the set of XGBoost models. For each location, emissions were measured using simulated conditions based on the country's emission factor, reflecting the environmental impact of running machine learning models in different parts of the world.\nMore details about the methodology used to calculate carbon emissions can be found in Appendix B."}, {"title": "3. Results", "content": null}, {"title": "3.1. Comparison of Lab-MAE vs. XGBoost for Data Imputation", "content": "In this section, we present the results of our comparison between the Lab-MAE and XGBoost models for the task of data imputation, evaluated using three metrics: WD, RMSE, and R2. These metrics were calculated on the entire set of 100 lab values and also on the top 20 most frequently occurring lab values to provide a more focused analysis."}, {"title": "3.1.1. OVERALL ANALYSIS OF ALL LAB VALUES", "content": "The comparison between the Lab-MAE and XGBoost models across all 100 lab values demonstrates that the Lab-MAE generally outperforms XGBoost. The average performance metrics calculated over the entire dataset indicate a lower WD and RMSE for the Lab-MAE model, and higher R2 values, suggesting that the Lab-MAE model has a better predictive capability and is less prone to error."}, {"title": "3.1.2. FOCUSED ANALYSIS ON THE TOP 20 LAB VALUES", "content": "To provide a clearer understanding of the models' performance on the most relevant lab values, we analyzed the top 20 most frequently occurring lab values. This focused analysis highlights the significant differences in prediction accuracy between Lab-MAE and XGBoost for these high-impact features.\nThe Lab-MAE model consistently demonstrates superior performance, achieving lower RMSE and WD values, as well as higher R2 scores for most lab values.\nFor example, for Creatinine, the Lab-MAE model achieved an RMSE of 0.233 compared to 0.261 for XGBoost, indicating a significant reduction in the average error. Furthermore, the WD for Lab-MAE was 0.034, which is lower than the 0.039 obtained by XGBoost. The R\u00b2 score for Lab-MAE was 0.943, outperforming XGBoost's 0.929, demonstrating a stronger predictive capability.\nThis trend is not isolated to Creatinine; similar patterns are observed across other lab values. For Sodium, Lab-MAE showed a reduction in RMSE from 1.035 to 0.883 and in WD from 0.309 to 0.220, with an improvement in R2 from 0.921 to 0.943, highlighting the model's robustness in reducing both large and small prediction errors.\nIn some cases, such as lab Platelet Count, both models presented challenges due to the complexity of the predictions. Nevertheless, Lab-MAE still showed slight improvements, with an RMSE of 39.861 versus 41.464 for XGBoost and an WD of 7.568 compared to 9.349. The R2 values for this lab remained relatively high for both models, with Lab-MAE marginally outperforming XGBoost (0.866 versus 0.855).\nThe detailed comparison provided underscores the strengths of the Lab-MAE model, suggesting that its ability to leverage the temporal and contextual information encoded in the dataset is a significant advantage over the XGBoost approach. These findings validate the hypothesis that a Transformer-based architecture, when properly trained and fine-tuned, can substantially improve data imputation tasks over classical machine learning methods.\nThe analysis confirms that the Lab-MAE model not only performs better overall but also excels particularly in the top 20 most frequently ordered lab values, making it a superior choice for data imputation tasks in clinical settings."}, {"title": "3.2. Fairness Analysis Across Race Groups", "content": "To gain deeper insights into the bias and fairness of the Lab-MAE and XGBoost models, we conducted a focused analysis on the top 20 most frequently occurring lab values, evaluating their performance across different racial groups. This detailed analysis reveals notable patterns in how each model performs for specific lab values, highlighting both strengths and potential biases.\nThe analysis reveals that the model's performance varies significantly depending on both the lab value and the racial group, indicating potential biases that warrant further attention.\nOne notable pattern is the consistent higher performance of the Lab-MAE model for Asian and White groups in certain lab values, which indicates a disparity in the model performance. This disparity in the performance is dependent on the lab value and metric used, highlighting the need of multiple metrics and domain specific metrics."}, {"title": "3.3. Carbon Footprint Results", "content": "After running experiments on inference of the Lab-MAE model versus the XGBoost models in batches of 1, 32 and 64 data points, and collecting the results, we averaged across six geographic locations spanning Asia, Africa, Australia, Europe, North America, and South America. Table 3 shows the mean values of duration, emissions, emissions rate, CPU power, GPU power, and RAM power for each model and batch size. Overall, Lab-MAE shows lower or comparable carbon footprints, particularly at lower batch sizes, whereas XGBoost sometimes demands a higher CPU"}, {"title": "4. Discussion", "content": "Our study introduces Lab-MAE, a novel transformer-based architecture that fundamentally advances the field of clinical data imputation while maintaining algorithmic fairness. Through an extensive empirical evaluation of the MIMIC-IV dataset Johnson et al. (2020, 2023), we demonstrate that Lab-MAE achieves superior performance compared to traditional approaches and exhibits remarkable consistency between demographic groups a critical consideration for healthcare applications."}, {"title": "4.1. Clinical and Technical Implications", "content": "Our LAB-MAE demonstrates a greater ability to learn real-world distributions, which may be multipolar or characterized by extreme values. This is highlighted by consistent R2 above XGB but reduced EMD/Wasserstein values overall and across subgroups. The model's performance stability across demographic groups challenges the expected trade-off between accuracy and fairness in machine learning systems. This suggests that architectural innovations focused on capturing temporal and contextual relationships can simultaneously advance both objectives.\nAnother critical advantage of Lab-MAE lies in its design as a single foundation model capable of processing complex sequences of data, as opposed to the XGBoost approach that requires a separate model for each feature. This structural difference not only contributes to an improved energy efficiency, but also makes it a more scalable and practical solution for real-world applications."}, {"title": "4.2. Fairness and Equity Considerations", "content": "Previous approaches to laboratory value imputation have largely relied on traditional machine learning methods or simplified time-series models. Our model builds upon recent work by Bellamy et al. (2023) for modeling laboratory data, and Du et al. (2023) with the ReMasker framework for tabular data imputation, while addressing their limitations, particularly in handling temporal dependencies and maintaining performance across diverse patient populations. The significant improvement over XGBoost, especially in complex laboratory parameters, aligns with emerging evidence that properly architected deep learning models can overcome the traditional advantages of tree-based methods in tabular data van Breugel and van der Schaar (2024). However, the observed disparities in performance across racial groups reflect the social patterning of data generation Teotia et al. (2024), where factors such as systemic inequalities in healthcare access contribute to missingness patterns. Addressing this requires not only technical innovations in imputation models but also systemic efforts to improve data equity"}, {"title": "4.3. Connection to Foundation Models", "content": "The success of Lab-MAE reflects a broader paradigm shift in medical AI, where foundation model architectures are being successfully adapted to specialized clinical tasks. Similar to how large language models have revolutionized natural language processing, our results suggest that transformer-based architectures can effectively capture clinical data's complex temporal and interdependent nature. The robust performance of Lab-MAE across diverse patient populations suggests that foundation model architectures, when properly adapted to clinical domains, can help bridge the gap between general and specialized medical AI applications.\nBy consolidating multiple tasks into a single, cohesive model, Lab-MAE reduces the redundancy and overhead associated with training and deploying separate models for each feature, as required by XGBoost. This capability is particularly advantageous in healthcare settings like hospitals, where complex data environments demand robust yet streamlined solutions. Lab-MAE's foundation model design highlights its potential to adapt effectively to diverse clinical scenarios, offering a powerful combination of scalability and environmental responsibility."}, {"title": "4.4. Limitations and Future Directions", "content": "Despite Lab-MAE's promising results, several limitations merit attention. First, our evaluation was conducted on a single, albeit large, academic medical center dataset, potentially limiting generalizability. Second, while we demonstrated fairness across major demographic groups, future work should investigate intersectional fairness and rare subpopulations. Key directions for future research include: (1) extending Lab-MAE to incorporate structured medical knowledge, (2) investigating transfer learning capabilities across different healthcare settings, and (3) developing interpretability methods specifically designed for temporal clinical predictions."}, {"title": "5. Conclusion", "content": "Lab-MAE represents a significant advance in clinical data imputation, demonstrating that foundation model architectures can be effectively adapted for specialized healthcare tasks while maintaining fairness across demographic groups. Our results suggest a promising path forward for developing robust, equitable healthcare AI systems that can handle the complexity of real-world clinical data. As healthcare continues to digitize and generate increasingly complex datasets, approaches like Lab-MAE will be crucial for ensuring both high performance and algorithmic fairness in clinical decision support systems."}, {"title": "Appendix C. Impact of shortcut features", "content": null}, {"title": "C.1. Methods: Impact of follow-up data as a shortcut feature", "content": "In addition to analyzing fairness across races, we also examined the Lab-MAE model's performance in scenarios where follow-up data was present versus when it was absent. This analysis aimed to understand if the availability of follow-up data serves as a shortcut feature, potentially inflating the model's performance by providing additional context.\nFor this analysis, the following procedure was adopted:\n\u2022 For each lab value, we identified the corresponding follow-up values (denoted as npval_last) and separated the test samples into two groups: those with follow-up values and those without.\n\u2022 The imputation model's performance was then evaluated separately for each group to determine how the presence or absence of follow-up data affected its predictive performance of the model on data imputation.\n\u2022 Metrics such as WD, and R\u00b2 were calculated for each group to quantify the performance variations between the two scenarios.\nThis experiment allowed us to observe whether the model's predictive performance disproportionately relies on the availability of follow-up data, which could lead to biased predictions when such data is not present.\nThe findings from both the fairness analysis across racial groups and the follow-up data evaluation are crucial for understanding the Lab-MAE model's robustness and its potential biases. This analysis also aids in identifying areas where the model could be further improved to ensure more equitable performance across diverse patient populations."}, {"title": "C.2. Results: Impact of follow-up data as a shortcut feature", "content": "In this section, we analyze the impact of the Lab-MAE and XGBoost models to shortcuts by comparing their imputation performance with and without the presence of follow-up values. Follow-up values represent additional lab measurements taken after the initial test, providing a temporal context that could serve as a shortcut for predicting the target lab values.\nTo do so, we evaluated the performance of both models using the R2 metrics for the lab values with and without follow-up data. The comparison of the distributions for each lab value and the WD values is available in the supplementary materials."}, {"title": "Appendix D. R2 metrics comparison", "content": null}, {"title": "D.1. R2 metrics comparison of Lab-MAE vs XGBoost for the top 20 lab values", "content": "Figure 2 provides a visual representation of the performance metrics for the top 20 lab values, highlighting the consistent improvement of the Lab-MAE model over XGBoost. This visual analysis further supports the numerical results, emphasizing the Lab-MAE model's enhanced ability to predict missing lab values accurately."}, {"title": "D.2. R2 metrics comparison per race of Lab-MAE for the top 20 lab values", "content": "Figure 3 visually depicts the variation in R2 scores for the top 20 lab values across different racial groups. This visualization clearly demonstrates the disparities in model performance, with certain lab values consistently showing higher predictive accuracy for specific races while underperforming for others."}, {"title": "Appendix F. Lab value distribution of Lab-MAE and XGBoost", "content": "Prediction vs real distributions in test set [here]"}, {"title": "Appendix G. Lab value distributions per race", "content": "\u2022 Prediction vs real distributions per race for Lab-MAE model in test set [here]\n\u2022 Prediction vs real distributions per race for XGBoost model in test set [here]"}, {"title": "Appendix H. Lab value distributions with vs without follow-up values", "content": "\u2022 Prediction vs real distributions with vs without follow-up value in test set for Lab-MAE [here]\n\u2022 Prediction vs real distributions with vs without follow-up value in test set for XGBoost [here]"}]}