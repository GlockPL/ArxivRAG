{"title": "Decoding Drug Discovery: Exploring A-to-Z In silico Methods for Beginners", "authors": ["Hezha O. Rasull", "Dlzar D. Ghafour", "Bakhtyar K. Aziz", "Bryar A. Hassan", "Tarik A. Rashid", "Arif Kivrak"], "abstract": "The drug development process is a critical challenge in the pharmaceutical industry due to its time- consuming nature and the need to discover new drug potentials to address various ailments. The initial step in drug development, drug target identification, often consumes considerable time. While valid, traditional methods such as in vivo and in vitro approaches are limited in their ability to analyze vast amounts of data efficiently, leading to wasteful outcomes. To expedite and streamline drug development, an increasing reliance on computer-aided drug design (CADD) approaches has merged. These sophisticated in silico methods offer a promising avenue for efficiently identifying viable drug candidates, thus providing pharmaceutical firms with significant opportunities to uncover new prospective drug targets. The main goal of this work is to review in silico methods used in the drug development process with a focus on identifying therapeutic targets linked to specific diseases at the genetic or protein level. This article thoroughly discusses A-to-Z in silico techniques, which are essential for identifying the targets of bioactive compounds and their potential therapeutic effects. This review intends to improve drug discovery processes by illuminating the state of these cutting-edge approaches, thereby maximizing the effectiveness and duration of clinical trials for novel drug target investigation.", "sections": [{"title": "1. Introduction", "content": "The search for new chemical entities with therapeutic potential is known as drug discovery. One of the main objectives of drug development programs is the identification of novel molecular entities that may be effective in treating diseases with specific medical needs. Currently, the medications on the market represent a relatively modest number of pharmacological target categories [1]. There are many moving parts in the drug development process. At its most fundamental level, the process can be broken down into two distinct phases, as shown in Figure 1. The first, known as drug discovery, is conducting experiments and investigations to find a single chemical that can be used in clinical practice after identifying a biological target and related disease state. Target identification, lead discovery, and optimization are three distinct phases that follow the drug discovery phase [2]. Drug discovery relies on establishing a causal relationship between a biological target and a disease state model that closely mimics the human illness state at each stage. Target progression and validation is a strategy that utilizes molecular probes to identify a set of compounds that can modulate the activity of a specific biological target [3]. Target selection can be simplified by employing existing compounds; these \u2018known' compounds can then be subjected to lead discovery and optimization, resulting in 'new' compounds. Lead discovery involves biologically screening many compounds to identify sets of structurally related compounds with the required biological activity [4]. The lead optimization process starts once a candidate series has been chosen. This phase aims to find a single chemical that can advance into the drug development stage by analyzing structural analogs in a lead series. In lead discovery and optimization processes, many lead series are often found through iterative rounds of experimentation [5], [6]. In many instances, the lead discovery and lead optimization phases overlap since a typical drug discovery program creates multiple groups of related compounds with the potential to find candidates that might move into therapeutic development. Drug development typically follows the lead discovery phase, which concludes when in vivo efficacy is established in an appropriate animal model using a molecule with physical and chemical properties consistent with a future clinical trial [6]."}, {"title": "2. Artificial Intelligence in Drug Discovery", "content": "The pharmaceutical industry has experienced a significant surge in data digitalization in the past several years. It might be demanding to gather, investigate, and use the knowledge brought about by digitization to tackle complex therapeutic difficulties [11]. This challenge motivates using artificial intelligence, which can handle enormous data sets more efficiently through increased automation. Synthesizing human intelligence, AI uses a wide range of cutting-edge computing resources and computational networks. AI uses systems and software to read and learn from input data to make autonomous decisions to achieve particular goals. Still, it is not a comprehensive threat to replace human physical presence. Machine learning (ML) is a subfield of AI that uses statistical methods to acquire knowledge through or independently of explicit programming [12], [13], [14]. ML employs algorithms to spot patterns in already- organized data [15], [16], [17], [18]. Supervised, unsupervised, and reinforcement learning are the three main sub-types of ML. Classification and regression methods are used to construct predictive models from input and output data while learning is being supervised. Disease diagnosis is an example of supervised ML's subgroup classification output. In contrast, pharmaceutical efficacy and adverse drug reaction (ADMET: Absorption, Distribution, Metabolism, Excretion, and Toxicity) prediction is an example of supervised ML's subgroup regression output [19]. Feature discovery and data clustering based solely on input data are examples of unsupervised learning techniques [20]. Illness subtype discovery via clustering and disease target discovery with feature- finding approaches possible with unsupervised ML [21]. Reinforcement learning, often known as the 'science of decision-making trial and error', is a method for optimizing performance in a particular setting by implementing decisions. The results of this type of machine learning can be used in various contexts, such as de novo drug design for decision-making or experimental design for implementation. Modeling and quantum chemistry help achieve these goals [22] (Figure 2)."}, {"title": "3. Drug Discovery and Drug Design", "content": "Small molecules, peptides, antibodies, and emerging modalities like short RNAs and cell therapies are the backbone of drug discovery because of their ability to influence the activity of a biological target and, hence, affect the disease state [27]. Two leading causes of drug failure in clinical trials are ineffectiveness and safety concerns. Therefore, target selection and validation are essential in creating a new drug. The term 'target' is broad enough to encompass a wide range of biological entities, including but not limited to proteins, genes, and RNA. One of the essential qualities of a good target is that it is 'druggable' or amenable to therapeutic intervention. Whether the putative drug molecule is small or large, the term 'druggable' refers to a target it can access. The biological reaction can be observed in vitro and in vivo after binding to the target. The finding of a target with a convincing therapeutic hypothesis that modifying the target would modulate the disease state is still necessary for initiating a drug development program, notwithstanding the current resurgence of phenotypic screens. Target identification and prioritization refer to determining and ranking the essential targets accordingly [28]. Target identification and validation can help us predict the strength of the link between the target and the disease. It allows us to examine whether modulating the target will have unwanted consequences due to the underlying mechanism [29]. A naturally existing"}, {"title": "3.1 Target discovery", "content": "Small molecules, peptides, antibodies, and emerging modalities like short RNAs and cell therapies are the backbone of drug discovery because of their ability to influence the activity of a biological target and, hence, affect the disease state [27]. Two leading causes of drug failure in clinical trials are ineffectiveness and safety concerns. Therefore, target selection and validation are essential in creating a new drug. The term 'target' is broad enough to encompass a wide range of biological entities, including but not limited to proteins, genes, and RNA. One of the essential qualities of a good target is that it is 'druggable' or amenable to therapeutic intervention. Whether the putative drug molecule is small or large, the term 'druggable' refers to a target it can access. The biological reaction can be observed in vitro and in vivo after binding to the target. The finding of a target with a convincing therapeutic hypothesis that modifying the target would modulate the disease state is still necessary for initiating a drug development program, notwithstanding the current resurgence of phenotypic screens. Target identification and prioritization refer to determining and ranking the essential targets accordingly [28]. Target identification and validation can help us predict the strength of the link between the target and the disease. It allows us to examine whether modulating the target will have unwanted consequences due to the underlying mechanism [29]. A naturally existing (genetic) mutation or a well-designed experimental intervention must be proved to cause the disease to establish that the target is regulated. Increased target identification is primarily attributable to data mining of available biological data. Phenotypic screening, chemo-proteomics, imaging, gene association studies, biomarkers, and transgenic organisms are just a few methods to identify potential therapeutic targets (Figure 4). Using a computational technique to find, prioritize, and select prospective disease targets is what 'data mining' means in this context [30]. However, to produce statistically effective models that can generate predictions for target identification, these multidimensional data sets need the proper analytical techniques, and this is where ML may be used. Products can be validated using various methods, from in vitro instruments and full animal models to modifying a target in patients with the disease. Although clinical trials remain the gold standard for validating targets, early-stage validation using omics technologies and preclinical models enables the efficient allocation of resources toward promising targets [31]."}, {"title": "3.1.1. Genomics", "content": "Genomic data allow researchers to identify mutations or genetic variations associated with diseases, such as single nucleotide polymorphisms (SNPs) [32], copy number variations (CNVs) [33], and other mutations that are implicated in diseases. Techniques such as Genome-Wide Association Studies (GWAS) [34] and whole-genome sequencing have played pivotal roles in identifying genes linked to specific diseases [35], [36]. GWAS studies map genetic loci associated with diseases by scanning thousands of genomes. The studies have proven valuable in uncovering loci associated with complex diseases such as Alzheimer's, cancer, and cardiovascular diseases [34]. CRISPR-Cas9-based screening technologies have also revolutionized functional genomics, enabling researchers to knock out or modify specific genes to determine their role in disease mechanisms [37]. CRISPR screening allows the high-throughput identification of disease-related genes by systematically perturbing the genome. Recent studies have employed CRISPR screens to identify essential cancer genes as potential therapeutic targets [38], [39]."}, {"title": "3.1.2. Proteomics", "content": "Proteomics plays a pivotal role in identifying differentially expressed proteins in diseased versus healthy tissues. It focuses on the large-scale study of proteins, particularly their structures and functions. Proteins are the functional molecules that execute most cellular processes, and changes in protein expression, post- translational modifications, and interactions often indicate disease states [40]. Mass spectrometry (MS) [41] and protein-protein interaction (PPI) networks [42] are critical technologies used in this domain. Mass spectrometry-based proteomics provides high-resolution data on protein abundance, modifications (phosphorylation), and degradation. Integrating proteomic data with genomic and transcriptomic data allows researchers to understand how genetic mutations translate into altered protein functions [43]. PPI networks help elucidate how proteins interact within the cellular environment. Integrating PPI data with disease phenotypes has uncovered networks of dysregulated proteins that could serve as therapeutic targets [44]."}, {"title": "3.1.3. Transcriptomics", "content": "Transcriptomics involves studying the RNA transcripts produced by the genome, providing insights into gene expression patterns under different conditions. RNA sequencing is a widely used tool for transcriptomic analysis and can reveal how diseases affect gene expression. It allows for quantifying RNA transcripts and identifying differentially expressed genes between diseased and healthy states. These genes can then be linked back to the pathways that may be driving the disease, providing targets for therapeutic intervention. Recent studies have integrated transcriptomics with other omics approaches to identify essential regulatory genes that control disease progression, such as transcription factors involved in cancer metastasis [45], [46]."}, {"title": "3.1.4. Metabolomics", "content": "Metabolomics measures the dynamic changes in small molecules (metabolites) within cells, tissues, or organisms. Metabolomic profiling can reveal biochemical changes accompanying disease, providing additional data for target identification. Nuclear magnetic resonance (NMR) [47] and mass spectrometry- based metabolomics are critical for measuring metabolite concentrations. Changes in the metabolome often reflect underlying genetic or protein-level changes and can point to disrupted disease pathways [48], [49], [50]."}, {"title": "3.1.5. Integrative Multi-Omics Approaches", "content": "Combining these omics technologies, referred to as multi-omics integration, allows for a holistic view of disease mechanisms [51]. Researchers can more precisely pinpoint novel drug targets by combining genomic mutations with proteomic changes, transcriptomic dysregulation, and metabolomic shifts [52]. Bioinformatics platforms such as STRING (Search Tool for the Retrieval of Interacting Genes/Proteins) [53], Cytoscape [54], and MetaboAnalyst [55]facilitate the integration of omics data and provide visual representations of complex biological networks. Network-based systems biology approaches have also been increasingly adopted to understand how molecular alterations at different levels (genes, proteins, and metabolites) contribute to disease [56]. Such approaches can predict drug-target interactions and suggest new therapeutic avenues."}, {"title": "3.1.6. Protein Structure Prediction", "content": "Protein structure prediction is critical in understanding protein function and interactions, especially when experimental 3D structures are unavailable. When 3D structures are accessible through resources like the Protein Data Bank (PDB) [57]and the EMDataBank for cryo-electron microscopy structures [58]. These data provide valuable insights into protein function, ligand-binding sites, and molecular interactions, essential for structure-based drug design (SBDD) [59]. The availability of high-resolution structures often determined through X-ray crystallography [60]or NMR spectroscopy [61], enables detailed analyses of protein-ligand interactions, facilitating the rational design of small-molecule inhibitors or modulators. In the context of drug discovery, known protein structures allow for the application of molecular docking and virtual screening techniques, which can be used to predict the binding affinity of various ligands. Furthermore, structural data can inform the design of small molecules with enhanced binding specificity and potency, providing a robust framework for lead optimization and rational drug design [59], [62]."}, {"title": "3.1.6.1. Known 3D Protein Structures", "content": "Protein structure prediction is critical in understanding protein function and interactions, especially when experimental 3D structures are unavailable. When 3D structures are accessible through resources like the Protein Data Bank (PDB) [57]and the EMDataBank for cryo-electron microscopy structures [58]. These data provide valuable insights into protein function, ligand-binding sites, and molecular interactions, essential for structure-based drug design (SBDD) [59]. The availability of high-resolution structures often determined through X-ray crystallography [60]or NMR spectroscopy [61], enables detailed analyses of protein-ligand interactions, facilitating the rational design of small-molecule inhibitors or modulators. In the context of drug discovery, known protein structures allow for the application of molecular docking and virtual screening techniques, which can be used to predict the binding affinity of various ligands. Furthermore, structural data can inform the design of small molecules with enhanced binding specificity and potency, providing a robust framework for lead optimization and rational drug design [59], [62]."}, {"title": "3.1.6.2. Unknown Protein Structures", "content": "In cases without an experimentally determined 3D protein structure, computational methods are utilized to predict protein structures based on sequence data. Several approaches have been developed to address this challenge, each offering varying degrees of accuracy depending on the availability of homologous structures and the complexity of the target protein [59]."}, {"title": "3.1.6.2.1. Homology Modeling", "content": "Homology modeling, also known as comparative modeling, is predicated on the assumption that proteins with similar sequences adopt similar tertiary structures. This method involves identifying a homologous protein with a known structure as a template, followed by alignment of the target sequence to the template to generate a model [63], [64]. Homology modeling is particularly effective when high-sequence similarity exists between the target and the template, typically requiring at least 30% sequence identity for accurate structural prediction. Tools such as SWISS-MODEL [65] and Modeller [66] are widely used."}, {"title": "3.1.6.2.2. Threading (Fold Recognition)", "content": "When sequence similarity is insufficient for homology modeling, threading can be employed. Threading methods predict the 3D structure of the target protein by mapping its sequence onto a library of known protein folds, identifying the most likely structural conformation. This approach can be valuable when the target protein lacks close sequence homologs but shares structural features with known proteins [67]. Commonly used tools for threading-based predictions include Phyre2 [68] and I-TASSER [69], which leverage sequence-structure compatibility to produce accurate models for proteins with low sequence identity to known structures."}, {"title": "3.1.6.2.3. AlphaFold Protein Structure Prediction", "content": "The recent development of AlphaFold, a deep learning-based algorithm, has significantly advanced the field of protein structure prediction. AlphaFold predicts protein structures directly from amino acid sequences by modeling accurate inter-residue distances and angles. This approach has proven particularly effective for proteins with no homologous structures, providing reliable structural predictions in cases where traditional methods such as homology modeling and threading fall short. AlphaFold is now transforming how unknown protein structures are modeled, making it an indispensable tool for studying protein function and facilitating drug discovery [70], [71]."}, {"title": "3.2. Lead discovery", "content": "After target identification, the lead discovery step of the small molecules' translational route involves the search for compounds that can interact with the target. Pharmaceutical chemists look for substances that might interact with the target. During lead discovery, scientists look for a drug-like biological or small chemical treatment that can be put through preclinical studies, clinical trials, and, if everything goes well, into the market. The term 'development candidate' describes this class of drugs [72]."}, {"title": "3.3. Hit identification", "content": "Finding 'hits' or molecules that interact with the target despite being poorly optimized, is the first step in the lead discovery process. To determine which functions are most effectively modified, this method considers tiny molecules as hits for target binding [72]. One of the essential milestones in preclinical drug discovery is the identification of drug-target interactions. A drug's therapeutic effects depend on its interaction with its intended target, while off-target protein interactions can lead to unwanted side effects and drug repositioning [73]. Furthermore, due to the limited number of currently available drugs, it is not easy to experimentally examine the entire chemical space of chemicals for druggable target proteins. Since the conceivable organic molecules may be enumerated computationally in chemical space, it is possible to identify novel and high-quality molecules [74]."}, {"title": "3.3.1. Drug repurposing", "content": "Drug repurposing aims to discover new uses for already approved pharmaceuticals by investigating their potential for new effects or targets. It is considered that drug repurposing offers significant advantages over the typical method of drug development (de novo drug discovery) that involves hunting for a novel active ingredient with time and money-saving effectiveness. Drug repositioning potentials could be developed more rapidly, reducing development risks due to applying existing drug knowledge [78], [79], [80], [81], [82]. Other terms for this practice include therapeutic switching, drug reuse, drug recycling, drug re-tasking, and drug reprofiling. Typical drug repositioning research focuses on finding drugs with similar effects [83] and mechanisms of action, looking at shared properties such as chemical structures and adverse effects [73], establishing links between diseases and medications [84], or selecting new targets from the present pharmacopeia to screen for novel medication indications [85]. Figure 5 shows how drug repurposing assists in the discovery of novel drugs."}, {"title": "3.3.2. High-throughput screening (HTS)", "content": "Drug discovery methodologies have significantly modified and included numerous new ideas during the past few decades. HTS emerged from efficiently screening manageable library sizes. Whether using a cell- based assay or a more complicated assay method that relies on many targets and, as a result, requires additional tests to confirm the site of action of drugs, screening is the process of assessing a large number of compounds for their activity against a therapeutic target [92]. This screening strategy uses advanced laboratory automation but does not assume (based on prior information) the type of chemotype that will be most effective at the target protein. High throughput is developed and implemented to find compounds interacting with the therapeutic target. Chemical programs are carried out to boost the potency. The molecule's selectivity and physiochemical characteristics support the premise that a therapeutic target intervention will benefit the illness state. Research in the pharmaceutical business and, increasingly, in academic institutions is focusing on this sequence of activities to identify potential molecules for clinical development. To find targets, collect compound libraries, and set up the necessary infrastructure to screen those compounds, pharmaceutical companies have developed massive organizations to identify hit molecules from HTS and optimize those screening 'hits' into clinical prospects [29], [93]. Due to the high cost of HTS screens, the HTS assay needs to be optimized and reduced in size. The standard number of wells on a screening plate is 1536 [94], indicating that HTS enables a rapid evaluation of several chemicals, typically highly automated. The 'hit compounds' activity will be examined at various concentrations, or a dosage response, after they have been identified. The design and synthesis of extensive and varied libraries of small organic compounds via combinatorial chemistry have cost the pharmaceutical industry much. An ever-growing amount of data is produced when these chemical libraries are evaluated using HTS techniques against potential therapeutic targets. This data must be handled smartly and highly automated to find and select novel hits and lead candidates [95]. Currently, chemoinformatic methods help medicinal chemists and molecular pharmacologists examine and organize data and comprehend connections between observed biological data and drug attributes. The effectiveness of HTS depends on the choice of the most probable lead compounds, the prompt and systematic removal of suspected \u2018false positives', and the detection of 'false negatives'. The properties of the compounds selected for different lead-finding processes should ideally be druglike in several pharmacokinetic, physicochemical, and structural criteria. Chemoinformatic approaches play a significant role in identifying and implementing these factors to reduce the size of data sets because in vitro and in vivo screening cannot handle the generally high number of primary HTS hits [96], [97], [98]. Several diseases, such as multiple myeloma, glioblastoma, and pediatric tumors, have benefited from recent HTS developments in the design of customized therapy. Several drugs have been studied for their potential to provide targeted treatment, including the mTOR inhibitor hemisodium, the ALK inhibitor ceritinib, and the PLK1 inhibitor BI2536; the proteasome inhibitor bortezomib; the Bcl-2 inhibitor ABT-263; and the mTOR inhibitor AZD-8055 [99], [100], [101]."}, {"title": "3.3.3. Virtual screening (VS)", "content": "To reduce the time, money, and resources needed to find novel compounds, virtual screening techniques have revolutionized the discovery of new compounds with specific bioactivity by analyzing massive structural libraries against a bioreceptor or biological system [102]. VS can be implemented by docking a known medication into many diverse target structures or by docking a database of licensed drugs into a single intended specialized target. These techniques use iterative and hierarchical steps to narrow the search space to only those compounds that meet specific criteria for their pharmacokinetics, pharmacodynamics, and physicochemical properties. Hit compounds pass through every filter of the VS, and to verify their biological activity, they must undergo experimental testing in the lab. A virtual screening process includes five fundamental tasks; the first four steps are performed computationally, while the last is experimental (Figure 6). The first is library preparation, which entails accumulating chemical structures of substances (ligands and receptors), converting files to usable formats like PDBQT (Protein Data Bank, Partial Charge (Q), & Atom Type (T)), SMILES (simplified molecular-input line-entry system), SDF (structure data file), and MOL2 (MDL Mol file) [103], [104], [105], [106]; conformers creation, stereochemical correction, solvent and receptor-associated binding substances removal, polar hydrogens and charges addition are among the processes [107]. The data is pre-processed and filtered in the second step, and drug-likeness properties are examined in the third step. The fourth stage employs computational methods [108], docking studies, molecular dynamics (MD) simulation, and molecular mechanics calculation to filter the desired chemicals. However, the last phase involves experimental validation employing in vitro and in vivo assays, such as cell lines and enzymatic inhibition studies [109], [110]. In the early stages of drug development, VS approaches are often used to expand the initial library with active compounds, much as HTS. Compared to HTS, VS can handle thousands of compounds quickly, reducing the number of compounds that must be manufactured, purchased, and analyzed. Additionally, VS can be used to create virtual libraries of substances, increasing the chemical space. While a VS strategy occasionally produces highly active compounds, its primary objective may be to generate structurally diverse lead compounds that can be optimized further during the hit-to-lead and lead- optimization phases [111]. Virtual screening strategies have been applied using a variety of computer methodologies that have been developed over time and draw on an understanding of molecular modeling [112], [113], [114], [115], [116], probability and statistics [117], [118], [119], [120], [121], and artificial intelligence [122], [123], [124], [125], [126]. The success of discovering novel bioactive chemicals increases when these techniques are combined with experimental procedures [127], [128], [129], [130]."}, {"title": "3.3.3.1. Ligand-based virtual screening", "content": "LBVS, researchers compare compounds in a library to reference compounds that are effective against a target or possess the desired characteristics. Johnson and Maggiora initially proposed similar properties [131], according to which similar compounds have similar properties and serve as the foundation for various strategies. As a result, substances with a high degree of resemblance to reference substances are more likely to exhibit similar behavior or function via equivalent mechanisms and produce comparable effects. Since medications can have many pharmacological effects, similarity has been widely used in lead identification and optimization [132]. Different methodologies utilize various similarity measurements when comparing two compounds because similarity is subjective. Compared to structure-based methods, the computational cost of ligand-based VS approaches is lower because no macromolecules are required. This aspect means they are most useful at the outset of the VS procedure when the number of chemicals in the initial library is the biggest. The LBVS technique only utilizes assessments of the basic properties of the compound structure, for example, physicochemical, topological, structural, and electronic aspects of a molecule that are connected to its molecular activity, a group of chemicals with empirically demonstrated biological action as a starting point [133], [134], [135]. The LBVS approach uses various computational methods [108], [136]. The three most common techniques are Ligand-based Pharmacophores, which are constructed by superimposing a set of active compounds to determine the standard chemical features that promote their activity [137]. Similarity searching involves collecting molecules from a database with user-defined query substructures comparable to active substances [138]. Machine learning methods are commonly employed in quantifying structure-activity relationships (QSAR) [139]."}, {"title": "3.3.3.2. Structure-based virtual screening", "content": "The structure-based virtual screening (SBVS) method designs and finds new bioactive compounds by using knowledge about the ligand's molecular recognition in the bioreceptor structure as a starting point. This information also includes the bioreceptor's geometry, the molecule's surface charge, intermolecular interactions, and the chemical composition of the residue at the binding site [130], [140], [141]. These techniques require the receptor's 3D structure to be fully understood and, ideally, combined with the bioactive substance. The 3D structure determines the bioactive ligands' structural conformation and molecular binding location. Molecular docking, molecular dynamics simulation, and structure-based pharmacophore modeling are examples of computational techniques used in the SBVS methodology [108], [142]. Public databases like the RCSB Protein Data currently contain more than two million 3D structures of molecular targets that have been determined. SBVS techniques investigate information on the target protein structure when choosing compounds expected to interact favorably. Due to its knowledge-based nature, SBVS places a high value on the quantity and quality of information accessible on the system under study. Regardless of the protein type selected as a molecular target, it is essential to consider the target's draggability, the selection of the most pertinent shape, the receptor's flexibility, the correct allocation of protonation states, and the existence of molecules of water in the binding site [143], [144]. The various steps involved in the LBVS and SBVS are briefly shown in Figure 7."}, {"title": "3.3.4. Network Pharmacology", "content": "Network pharmacology is a cutting-edge approach to drug discovery that moves away from the traditional idea of 'one drug, one target'. Instead, it focuses on understanding the complexity of human diseases at a broader systems level \u2018multi-target, multi-drug' model. By combining systems biology, computational modeling, and experimental methods, network pharmacology aims to map out the complex interactions in biological networks, such as protein interactions, gene regulation, and metabolic pathways. This approach helps scientists find and affect multiple targets within a network, which is especially useful for diseases like cancer, neurodegenerative disorders, and metabolic syndromes, where disruptions across many pathways drive the disease forward [145], [146]. A key concept in network pharmacology is identifying 'network hubs' or important nodes in these biological networks that control several pathways. Targeting these hubs allows drugs to have a more powerful effect compared to traditional single-target drugs, which may only address one part of a disease. For instance, in cancer treatment, the interconnected nature of signaling pathways can cause other pathways to compensate when a single protein is blocked. Network pharmacology addresses this issue by developing multi-target therapies that block several critical pathways simultaneously, lowering the chance of resistance and improving treatment results [147]. Recent advances in computational tools have greatly improved the use of network pharmacology. Tools like STRING, Cytoscape, and STITCH help build and visualize complex interaction networks, making identifying key targets and druggable points easier. These tools combine data from multiple omics layers, such as genomics, proteomics, and metabolomics, to provide a complete picture of the molecular mechanisms behind diseases (as described in Section 3.1.5. Integrative Multi-Omics Approaches) [54]. New algorithms for network-based drug repositioning allow researchers to find new uses for existing drugs by analyzing their effects across different biological networks. These discoveries speed up the drug discovery process and reduce the cost and time needed to develop new therapies [148], [149]. The benefits of network pharmacology are significant. It gives a more complete understanding of how diseases work, enabling the creation of drugs that can target multiple areas with greater precision and effectiveness. By considering the interconnected nature of biological systems, network pharmacology reduces the risk of side effects and increases the likelihood of finding drugs with safer profiles. It also offers a strong framework for drug repurposing, helping researchers find new treatments for existing drugs by examining how they influence different molecular networks. As the field evolves, incorporating AI and machine learning is expected to further speed up the discovery of multi-target drug candidates, leading to more personalized and effective treatments in the future [150], [151], [152]."}, {"title": "3.4. Hit to lead", "content": "Lead generation refers to the first phase of drug development. Once a prospective pharmacological target has been discovered, the next step in the research process is identifying molecules that can engage with the target to produce the desired biological effects. A hit compound is a molecule that reaches the required activity level in a screening assay. The hit discovery and hit-to-lead selection processes rely heavily on developing pharmacologically relevant screening assays. By refining the screening parameters, the most potential molecules in a pool of hits are singled out as potential lead compounds [72], [153], [154]. In a typical hit-to-lead process, the verified hit is chemically modified to increase its affinity for the target and transform it into a lead compound. These optimization phases can be finished using a trial-and-error approach, and it typically takes several cycles to find the suitable affinity. In addition to off-target effects, physicochemical properties and ADME features can be screened using the secondary assays used in the lead selection process. This discovery process step aims to improve each successful series to create more powerful and selective drugs with pharmacokinetic (PK) characteristics enough to determine their effectiveness in any accessible in vivo models. The critical factors for hits obtained using various hit-finding algorithms may differ remarkably [155]. It is standard procedure to conduct in-depth structure-activity relationships (SAR) studies around each central chemical structure and then to use these data to ascertain the activity and selectivity of the molecules in question. This process must be carried out systematically when structural knowledge of the target is available. Structure-based drug design methods, including molecular modeling and techniques like X-ray crystallography and nuclear magnetic resonance, can be utilized to build the SAR more rapidly and precisely. This action typically leads to discovering new binding sites on specific proteins [29], [72], [153], [156]. There are a variety of computer-based tactics designed to circumvent hit-to-lead ratios. For instance, by utilizing accessible experimental data and derived descriptors, ligand-based approaches like the Quantitative Structure-Activity Relationship (QSAR) can be performed to improve a sequence of compounds. To optimize a set of compounds, for instance, ligand-based techniques like the QSAR can use the existing experimental observations and derived descriptors [157]. By using the binding interactions of the confirmed fragment as an initial point, de novo design methods can also be used to sample the cavity and generate potential optimum molecules [158], [159]."}, {"title": "3.4.1. Quantitative structure-activity relationship", "content": "The hit-to-lead optimization technique used QSAR analysis to identify possible lead compounds from hit analogs with the prediction of bioactivity analogs [160]. It is a computer program used to calculate the strength of a correlation between the chemical structures of a set of substances and an observed chemical or biological reaction. Comparable structural or physiochemical properties lead to relative activity; this is the core idea behind the QSAR [161], [162]. QSAR models provide a mathematical explanation of how a ligand's structural characteristics influence the behavior of a target after it binds to it. Electronic, hydrophobic, steric, and sub-structural molecular characteristics may be applied to build QSAR models [163]. The first step is to identify a set of chemical building blocks or lead molecules that display the target biological activity. There is a robust quantitative relationship between the physicochemical properties of the active compounds and their biological activity. The developed QSAR model is then used to enhance the biological activity of the active compounds. Next, the anticipated chemicals are tested in the lab to see if they have the expected action. As a result, the QSAR method can be used as a map to locate alterations to a chemical that result in a noticeable increase in activity. QSAR approach has advanced primarily regarding the variety of molecular descriptors utilized and their relationship to activity [164]. To construct a reliable QSAR model, a set of sequential processes is required [72], [164], [165], [166] (Figure 8): (i) Find ligands that exhibit the desired biological action (a minimum of 20 active chemicals) at experimentally observed levels. Though they should have a good chemical variety to display a wide range of activities; (ii) Choose and identify the molecular descriptors that correspond to the different construction and physicochemical characteristics of the molecules under examination; (iii) Analyse the data set to find chemical descriptors that correlate with biological activity and provide an explanation for the observed variance in activity; (iv) Examine the QSAR model's statistical applicability and predictivity performance."}, {"title": "3.4.2. De novo drug design", "content": "De novo design tools use predetermined collections of substructures and rules guiding their linking to automatically create compounds 'from scratch' within a binding site with a known 3D structure of the receptor. Target receptor information is provided, but no leads can interact with it. Molecular modeling methods determine the lead target complexes' structural properties and modify the lead [158", "159": "."}]}