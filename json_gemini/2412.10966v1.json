{"title": "FlowDock: Geometric Flow Matching for Generative Protein-Ligand Docking and Affinity Prediction", "authors": ["Alex Morehead", "Jianlin Cheng"], "abstract": "Powerful generative models of protein-ligand structure have recently been proposed, but few of these methods support both flexible protein-ligand docking and affinity estimation. Of those that do, none can directly model multiple binding ligands concurrently or have been rigorously benchmarked on pharmacologically relevant drug targets, hindering their widespread adoption in drug discovery efforts.\nIn this work, we propose FLOWDOCK, a deep geometric generative model based on conditional flow matching that learns to directly map unbound (apo) structures to their bound (holo) counterparts for an arbitrary number of binding ligands. Furthermore, FLOWDOCK provides predicted structural confidence scores and binding affinity values with each of its generated protein-ligand complex structures, enabling fast virtual screening of new (multi-ligand) drug targets. For the commonly-used PoseBusters Benchmark dataset, FLOWDOCK achieves a 51% blind docking success rate using unbound (apo) protein input structures and without any information derived from multiple sequence alignments, and for the challenging new DockGen-E dataset, FLOWDOCK matches the performance of single-sequence Chai-1 for binding pocket generalization. Additionally, in the ligand category of the 16th community-wide Critical Assessment of Techniques for Structure Prediction (CASP16), FLOWDOCK ranked among the top-5 methods for pharmacological binding affinity estimation across 140 protein-ligand complexes, demonstrating the efficacy of its learned representations in virtual screening.", "sections": [{"title": "Introduction", "content": "Interactions between proteins and small molecules (ligands) drive many of life's fundamental processes and, as such, are of great interest to biochemists and drug discoverers. Historically, elucidating the structure, and therefore the function, of such interactions has required that considerable intellectual and financial resources be dedicated to determining the interactions of a single biomolecular complex. For example, techniques such as X-ray diffraction and cryo-electron microscopy have traditionally been effective in biomolecular structure determination, however, resolving even a single biomolecule's crystal structure can be extremely time and resource-intensive. Recently, new machine learning (ML) methods such as AlphaFold 3 [Abramson et al., 2024] have been proposed for directly predicting the structure of an arbitrary biomolecule from its primary sequence, offering the potential to expand our understanding of life's molecules and their implications in disease, energy research, and beyond.\nAlthough powerful models of general biomolecular structure are compelling, they currently do not provide one with an estimate of the binding affinity of a predicted protein-ligand complex, which may indicate whether a pair of molecules truly bind to each other in vivo. It is desirable to predict both the structure of a protein-ligand complex and the binding affinity between them via one single ML system [Dhakal et al., 2022]. Moreover, recent generative models of biomolecular structure are primarily based on noise schedules following Gaussian diffusion methodology which, albeit a powerful modeling framework, lacks interpretability in the context of biological studies of molecular interactions. In this work, we aim to address these concerns with a new state- of-the-art hybrid (structure & affinity prediction) generative model called FLOWDOCK for flow matching-based protein-ligand structure prediction and binding affinity estimation, which allows one to interpretably inspect the model's structure prediction trajectories to interrogate its common molecular interactions and to screen drug candidates quickly using its predicted binding affinities."}, {"title": "Related work", "content": "Molecular docking with deep learning. Over the last few years, deep learning (DL) algorithms (in particular geometric variants) have emerged as a popular methodology for performing end-to-end differentiable molecular docking. Models such as EquiBind [St\u00e4rk et al., 2022] and TankBind [Lu et al., 2022] initiated a wave of interest in researching graph-based approaches to modeling protein-ligand interactions, leading to many follow-up works. Important to note is that most of such DL- based docking models were designed to supplement conventional modeling methods for protein-ligand docking such as AutoDock Vina [Eberhardt et al., 2021] which are traditionally slow and computationally expensive to run for many protein-ligand complexes yet can achieve high accuracy with crystal input structures.\nGenerative biomolecular modeling. The potential of generative modeling in capturing intricate molecular details in structural biology such as protein-ligand interactions during molecular docking [Corso et al., 2022] has recently become a research focus of ambitious biomolecular modeling efforts such as AlphaFold 3 [Abramson et al., 2024], with several open-source spin-offs of this algorithm emerging [Discovery et al., 2024, Wohlwend et al., 2024].\nFlow matching. In the machine learning community, generative modeling with flow matching [Lipman et al., 2023, Chen and Lipman, 2024, Tong et al., 2024] has recently become an appealing generalization of diffusion generative models [Ho et al., 2020, Karras et al., 2022], enabling one to transport samples between arbitrary distributions for compelling applications in computer vision [Esser et al., 2024], computational biology [Klein et al., 2024], and beyond. As a closely related concurrent work (as our method was developed for the CASP16 competition starting in May 2024), Corso et al. [2024b] recently introduced and evaluated an unbalanced flow matching procedure for pocket-based flexible docking. However, the authors' proposed approach mixes diffusion and flow matching noise schedules with geometric product spaces in an unintuitive manner, and neither source code nor data for this work are publicly available for benchmarking comparisons. In Section 3.3, we describe flow matching in detail.\nContributions. In light of such prior works, our contributions in this manuscript are as follows:\n\u2022 We introduce a simple yet state-of-the-art hybrid generative flow model capable of quickly and accurately predicting protein-ligand complex structures and their binding affinities, with source code and model weights freely available.\n\u2022 We rigorously validate our proposed methodology using standardized benchmarking data for protein-ligand complexes,\n\u2022 with our method ranking as a competitive structure predictor compared to (single-sequence) Chai-1.\n\u2022 Our method ranked as a top-5 binding affinity predictor for the 140 pharmaceutically relevant drug targets available in the CASP16 ligand prediction competition.\n\u2022 We release one of the largest ML-ready datasets of apo-to-holo protein structure mappings based on high-accuracy predicted protein structures, which enables training new models on comprehensive biological data for distributional biomolecular structure modeling."}, {"title": "Methods and materials", "content": "The goal of this work is to jointly predict protein-ligand complex structures and their binding affinities with minimal computational overhead to facilitate drug discovery. In Sections 3.1 and 3.2, we briefly outline how FLOWDOCK achieves this and how its key notation is defined. We then describe FLOWDOCK's training and sampling procedures in Sections 3.3-3.6."}, {"title": "OVERVIEW", "content": "Figure 1 illustrates how FLOWDOCK uses geometric flow matching to predict flexible protein-ligand structures and binding affinities. At a high level, FLOWDOCK accepts both (multi-chain) protein sequences and (multi-fragment) ligand SMILES strings as its primary inputs, which it uses to predict an unbound (apo) state of the protein sequences using ESMFold [Lin et al., 2023] and to sample from a harmonic ligand prior distribution [Jing et al., 2024] to initialize the ligand structures using biophysical constraints based on their specified bond graphs. Notably, users can also specify the initial protein structure using one produced by another bespoke method (e.g., AlphaFold 3 which we use in certain experiments). With these initial structures representing the complex's state at time t 0, FLOWDOCK employs conditional flow matching to produce fast structure generation trajectories [Liu et al., 2023]. After running a small number of integration timesteps (e.g., 20 in our experiments), the complex's state arrives at time t = 1, i.e., the model's estimate of the bound (holo) protein-ligand heavy-atom structure. At this point, FLOWDOCK runs confidence and binding affinity heads to predict structural confidence scores and binding affinities of the predicted complex structure, to rank-order the model's generated samples."}, {"title": "Notation", "content": "Let xo denote the unbound (apo) state of a protein-ligand complex structure, representing the heavy atoms of the protein and ligand structures as $x_p \\in \\mathbb{R}^{N_P \\times 3}$ and $x_l \\in \\mathbb{R}^{N_L \\times 3}$, respectively, where NP and NL are the numbers of protein and ligand heavy atoms. Similarly, we denote the corresponding bound (holo) state of the complex as x1. Further, let $s_p \\in \\{1,...,20\\}^{S_P}$ denote the type of each amino acid residue in the protein structure, where SP represents the protein's sequence length. To generate bound (holo) structures, we define a flow model $v_{\\theta}$ that integrates the ordinary differential equation (ODE) it defines from time t = 0 to t = 1."}, {"title": "Riemannian manifolds and conditional flow matching", "content": "In manifold theory, an n-dimensional manifold M represents a topological space equivalent to Rn. In the context of Riemannian manifold theory, each point x \u2208 Mona Riemannian manifold is associated with a tangent space TxM. Conveniently, a Riemannian manifold is equipped with a metric $g_x: T_xM \\times T_xM \\rightarrow \\mathbb{R}$ that permits the definition of geometric quantities on the manifold such as distances and geodesics (i.e., shortest paths between two points on the manifold). Subsequently, Riemannian manifolds allow one to define on them probability densities $M \\int p(x)dx = 1$ where p: M\u2192 R+ are continuous, non-negative functions. Such probability densities give rise to interpolative probability paths pt: [0,1] \u2192 P(M) between probability distributions p0, p1 \u2208 P(M), where P(M) is defined as the space of probability distributions on M and the interpolation in probability space between distributions is indexed by the continuous parameter t.\nHere, we refer to $ \\psi_t : M \\rightarrow M$ as a flow on M. Such a flow serves as a solution to the ODE: $\\frac{d \\psi_t(x)}{dt} = u_t(\\psi_t(x))$ [Mathieu and Nickel, 2020] which allows one to push forward the probability trajectory p0 \u2192 p1 to pt using $ \\psi_t$ as $p_t = [\\psi_t]_{\\#}(p_0)$, with $ \\psi_0(x) = x$ for u : [0, 1] \u00d7 M \u2192 M (i.e., a smooth time-dependent vector field [Bose et al., 2024]). This insight allows one to perform flow matching (FM) [Lipman et al., 2023] between po and p1 by learning a continuous normalizing flow [Papamakarios et al., 2021] to approximate the vector field ut with the parametric $v_{\\theta}$. With p0 = pprior and p1 = pdata, we have that pt advantageously permits simulation-free training. Although it is not possible to derive a closed form for ut (which generates pt) with the traditional FM training objective, a conditional flow matching (CFM) training objective remains tractable by marginalizing conditional vector fields as $u_t(x) := \\int_Z U_t(x/z) P_t (\\@_t z)q(z) dz$, where q(z) represents one's chosen coupling distribution (by default the independent coupling $q(z) = q(x_0)q(x_1)$) between xo and x1 via the conditioning variable z. For Riemannian CFM (RCFM) [Chen and Lipman, 2024], the corresponding training objective, with t ~ U(0, 1), is:\n$L_{RCFM}(\\Theta) = E_{t,q(z),p_t(x \\mid z)} ||v_{\\Theta} (x_t, t) - u_t(x_t | z)||_2^2$,\nwhere Tong et al. [2024] have fortuitously shown that the gradients of FM and CFM are identical. As such, to transport samples of the prior distribution po to the target (data) distribution p1, one can sample from po and use $v_{\\theta}$ to run the corresponding ODE forward in time. In the remainder of this work, we will focus specifically on the 3-manifold $\\mathbb{R}^3$."}, {"title": "Prior distributions", "content": "With flow matching defined, in this section, we describe how we use a bespoke mixture of prior distributions ($p_f^p$ and $p_f^l$) to sample initial (unbound) protein and ligand structures for binding (holo) structure generation targeting our data distribution of crystal protein-ligand complex structures p1. In Section 4.1, we ablate this mixture to understand its empirical strengths.\nESMFold protein prior. To our best knowledge, FLOWDOCK is among the first methods-concurrently with Corso et al. [2024b]- to explore using structure prediction models with flow matching to represent the unbound state of an arbitrary protein sequence. In contrast to Corso et al. [2024b], we formally define a distribution of unbound (apo) protein structures using the single-sequence ESMFold model as $p^p_f(x) \\propto ESMFold(s^P) + \\epsilon, \\epsilon \\sim \\mathcal{N}(0, \\sigma)$, which encourages our model to learn more than a strict mapping between protein apo and holo point masses. Based on previous works developing protein generative models [Dauparas et al., 2022], during training we apply $ \\epsilon \\sim \\mathcal{N}(0, \\sigma = le-4)$ to both $x_0^p$ and $x_1^p$ to discourage our model from overfitting to computational or experimental noise in its training data. It is important to note that this additive noise for protein structures is not a general substitute for generating a full conformational ensemble of each protein, but to avoid the excessively high computational resource requirements of running protein dynamics methods such as AlphaFlow [Jing et al., 2024] for each protein, we empirically find noised ESMFold structures to be a suitable surrogate.\nHarmonic ligand prior. Inspired by the FlowSite model for multi-ligand binding site design [Stark et al., 2024], FLOWDOCK samples initial ligand conformations using a harmonic prior distribution constrained by the bond graph defined by one's specified ligand SMILES strings. This prior can be sampled as a modified Gaussian distribution via $p^l_f(x) \\propto exp(-x^T L x)$ where L denotes a ligand bond graph's Laplacian matrix defined as L = D-A, with A being the graph's adjacency matrix and D being its degree matrix. Similarly to our ESMFold protein prior, we subsequently apply $ \\epsilon \\sim \\mathcal{N}(0, \\sigma = 1e^{-4})$ to $x_l^0$ during training."}, {"title": "Training", "content": "We describe FLOW DOCK'S structure parametrization, optimization procedure, and the curation and composition of its new training dataset in the following sections. Further, we provide training and inference pseudocode in Appendix A.\nParametrizing protein-ligand complexes with geometric flows. Based on our experimental observations of the difficulty of scaling up intrinsic generative models [Corso, 2023] that operate on geometric product spaces, FLOWDOCK instead parametrizes 3D protein-ligand complex structures as attributed geometric graphs [Joshi et al., 2023] representing the heavy atoms of each complex's protein and ligand structures. The main benefit of a heavy atom parametrization is that it can considerably simplify the optimization of a flow model $v_{\\theta}$ by allowing one to define its primary loss function as simply as a CondOT path [Pooladian et al., 2023, Jing et al., 2024]:\n$L_{\\mathbb{R}^3}(\\Theta) = E_{t,q(z),p_t(x \\mid z)} ||v_{\\Theta} (x_t, t) - x_1||^2,$\nwith the conditional probability path pt chosen as\n$p_t(x | z) = p_t(x | x_0,x_1) = (1 - t) \\cdot x_0 + t \\cdot x_1, x_0 \\sim p_0(x_0)$\nThe challenge introduced by this atomic parametrization is that it necessitates the development of an efficient neural architecture that can scalably process all-atom input structures without the exhaustive computational overhead of generative models such as AlphaFold 3. Fortunately, one such architecture, recently introduced by Qiao et al. [2024] with the NeuralPLexer model, satisfies this requirement. As such, inspired by how the AlphaFlow model was fine-tuned from the base AlphaFold 2 architecture using flow matching, to train FLOWDOCK we explored fine-tuning the NeuralPLexer architecture to represent our vector field estimate $v_{\\theta}$. Uniquely, we empirically found this idea to work best by fine-tuning the architecture's score head, which was originally trained with a denoising score matching objective for diffusion-based structure sampling, instead using Eqs. 2 and 3. Moreover, we fine-tune all of NeuralPLexer's remaining intermediate weights and prediction heads including a dedicated confidence head redesigned to predict binding affinities, with the exception of its original confidence head which remains frozen at all points during training.\nPDBBind-E Data Curation. To train FLOWDOCK with resolved protein-ligand structures and binding affinities, we prepared PDBBind-E, an enhanced version of the PDBBind 2020-based training dataset proposed by Corso et al. [2024a] for training recent DL docking methods such as DiffDock-L. To curate PDBBind-E, we collected 17,743 crystal complex structures contained in the PDBBind 2020 dataset and 47,183 structures of the Binding MOAD [Hu et al., 2005] dataset splits introduced by Corso et al. [2024a] and predicted the structure of these (multi-chain) protein sequences in each dataset split using ESMFold. To optimally align each predicted protein structure with its corresponding crystal structure, we performed a weighted structural alignment optimizing for the distances of the predicted protein residues' Ca atoms to the crystal heavy atom positions of the complex's binding ligand, similar to [Corso et al., 2024a]. After dropping complexes for which the crystal structure contained protein sequence gaps caused by unresolved residues, the total number of PDBBind and Binding MOAD predicted complex structures remaining was 17,743 and 46,567, respectively.\nGeneralized unbalanced flow matching. We empirically observed the challenges of naively training flexible docking models like FLOWDOCK without any adjustments to the sampling of their training data. Accordingly, we concurrently developed a generalized version of unbalanced flow matching [Corso et al., 2024b] by defining our coupling distribution q(z) as\n$q(x_0,x_1) \\propto q_0 (x_0) q_1 (x_1) \\mathbb{I}_{c(x_0,X_1) \\in C_A}$,\nwhere CA is defined as a set of apo-to-holo assessment filters measuring the structural similarity of the unbound (apo) and bound (holo) protein structures (n.b., not simply their binding pockets) in terms of their root mean square deviation (RMSD) and TM-score [Zhang and Skolnick, 2004] following optimal structural alignment (as used in constructing PDBBind-E). Effectively, we sample independent examples from qo and 91 and reject these paired examples if c(x0, X1) < CATM or c(x0,X1) > CARMSD (n.b., we use CATM = 0.7 and CARMSD = 5\u00c5 as well as other length-based criteria in our experiments, please see our code for full details)."}, {"title": "Sampling", "content": "By default, we apply i = 40 timesteps of an Euler solver to integrate FLOWDOCK's learned ODE $v_{\\theta}$ forward in time for binding (holo) structure generation. Specifically, to generate structures, we propose to integrate a Variance Diminishing ODE (VD-ODE) that uses $v_{\\theta}$ as\n$x_{n+1} = clamp(\\frac{1-\\eta}{1-t}) \\cdot x_n + clamp(\\frac{\\eta}{1-t}) \\cdot (x_t + v_{\\theta}(x_t, t)),$\nwhere n represents the current integer timestep, allowing us to define t = $\\frac{n}{i}$ and s = $\\frac{n+1}{i}$; \u03b7 = 1.0 in our experiments; and clamp ensures both the LHS and RHS of Eq. 5 are lower and upper bounded by le-6 and 1-le-6, respectively. We experimented with different values of \u03b7 yet ultimately settled on 1.0 since this yielded FLOWDOCK's best performance for structure and affinity prediction. Intuitively, this VD-ODE solver limits the high levels of variance present in the model's predictions $v_{\\theta}$ during early timesteps by sharply interpolating towards $v_{\\theta}$ in later timesteps."}, {"title": "Results", "content": "PoseBench Protein-Ligand Docking\nPoseBusters Benchmark set. In Figures 2 and 3, we illustrate the performance of each baseline method for protein-ligand docking and protein conformational modification with the commonly-used PoseBusters Benchmark set [Buttenschoen et al., 2024], provided by version 0.6.0 the PoseBench protein-ligand benchmarking suite [Morehead et al., 2024], which consists of 308 distinct protein-ligand complexes released after 2020. It is important to note that this benchmarking set can be considered a moderately difficult challenge for methods trained on recent collections of data derived from the Protein Data Bank (PDB) [Bank, 1971] such as PDBBind 2020 [Liu et al., 2015], as all of these 308 protein-ligand complexes are not contained in the most common training splits of such PDB-based data collections [Buttenschoen et al., 2024] (with the exception of AlphaFold 3 which uses a cutoff date of September 30, 2021). Moreover, as described by Buttenschoen et al. [2024], a subset of these complexes also have low protein sequence similarity to such training splits.\nFigure 2 shows that FLOWDOCK consistently improves over the original NeuralPLexer model's docking success rate in terms of its structural and chemical accuracy (as measured by the RMSD < 2\u00c5 & PB-Valid metric [Buttenschoen et al., 2024]) and inter-run stability (as measured by the error bars listed). Notably, FLOWDOCK achieves a 10% higher docking success rate than NeuralPLexer without any structural energy minimization driven by molecular dynamics software [Eastman et al., 2017], and with energy minimization its docking success rate increases to 51%, achieving second-best performance on this dataset compared to single-sequence Chai-1 [Discovery et al., 2024] (which is a 10x larger model and is trained on more than twice as much data in the PDB deposited up to 2021). Additionally, FLOWDOCK outperforms the hybrid flexible docking method DynamicBind [Lu et al., 2024], which is a comparable model in terms of its size, training, and downstream capabilities for drug discovery. Our results with an ablated version of FLOWDOCK trained instead with a protein harmonic prior (FLOWDOCK-HP) highlight that the protein ESMFold prior the base FLOWDOCK model employs has imbued it with meaningful structural representations for accurate ligand binding structure prediction.\nA surprising finding illustrated in Figure 3 is that no baseline method, including FLOWDOCK, can consistently improve the binding pocket RMSD of AlphaFold 3's initial protein structural conformations, which contrasts with the results originally reported for flexible docking methods such as DynamicBind which used structures predicted by AlphaFold 2 [Jumper et al., 2021] in its experiments. From this figure, we observe that DynamicBind and NeuralPLexer both infrequently modify AlphaFold 3's initial binding pocket structure, whereas FLOWDOCK often modifies the pocket structure during ligand binding. The former two methods occasionally improve largely-correct initial pocket conformations by ~1\u00c5, whereas FLOWDOCK primarily does so for mostly-incorrect initial pockets."}, {"title": "DockGen-E set", "content": "To assess the generalization capabilities of each baseline method, in Figures 4 and 5, we report each method's protein-ligand docking and protein conformational modification performance for the novel (i.e., naturally rare) protein binding pockets found in the new DockGen-E dataset from PoseBench. Each of DockGen-E's protein-ligand complexes represents a distinct binding pocket that facilitates a unique biological function described by its associated ECOD domain identifier [Corso et al., 2024a]. As our results for the DockGen-E dataset show in Figure 4, most DL-based docking or structure prediction methods have likely not been trained or overfitted to these binding pockets, as this dataset's best docking success rate achieved by any method is approximately 4%, much lower than the 68% best docking success rate achieved for the Pose Busters Benchmark set seen in Figure 2. We find further support for this phenomenon in Figure 5, where we see that all DL-based flexible docking methods find it challenging to avoid degrading the initial binding pocket state predicted by AlphaFold 3 yet all methods can restore a handful of AlphaFold 3 binding pockets to their bound (holo) form. This suggests that all DL methods (some more so than others) struggle to generalize to novel binding pockets, yet FLOWDOCK achieves top performance in this regard by tying with single-sequence Chai-1. To address this generalization issue, in future work, training new DL methods on diverse binding pockets such as those contained in the new PLINDER dataset [Durairaj et al., 2024] may be necessary to discourage models from capturing only the most common protein-ligand binding patterns in the PDB."}, {"title": "Computational resouces", "content": "To formally measure the computational resources required to run each baseline method, in Table 1 we list the average runtime (in seconds) and peak CPU (GPU) memory usage (in GB) consumed by each method when running them on a 25% subset of the Astex Diverse dataset [Hartshorn et al., 2007] (baseline results taken from Morehead et al. [2024]). Here, we notably find that FLOWDOCK provides the second lowest computational runtime and GPU memory usage compared to all other DL methods, enabling one to use commodity"}, {"title": "PDBBind Binding Affinity Estimation", "content": "In this section, we explore binding affinity estimation with FLOWDOCK using the PDBBind 2020 test dataset (n=363) originally curated by [St\u00e4rk et al., 2022], with benchmarking results shown in Table 2. Popular affinity prediction baselines listed in Table 2 such as TankBind [Lu et al., 2022] and DynamicBind [Lu et al., 2024] demonstrate that accurate affinity estimations are possible using hybrid DL models of protein-ligand structures and affinities. Here, we find that, as a hybrid deep generative model, FLOWDOCK provides the best Pearson and Spearman's correlations compared to all other baselines including FLOWDOCK-HP (a fully harmonic variant of FLOWDOCK) and produces compelling root mean squared error (RMSE) and mean absolute error (MAE) rates compared to the previous state-of-the-art method DynamicBind. Referencing Table 1, we further note that FLOWDOCK's average computational runtime per protein-ligand complex is more than 3 times lower than that of DynamicBind, demonstrating that FLOWDOCK, to our best knowledge, is currently the fastest binding affinity estimation method to match or exceed DynamicBind's level of accuracy for predicting binding affinities using the PDBBind 2020 dataset.\nIn Figure 6, we provide an illustrative example of a protein-ligand complex in the PDBBind test set (6167) for which FLOWDOCK predicts accurate complex structural motions and binding affinity values, importantly recognizing that the right-most protein loop domain should be moved further to the right to facilitate ligand binding. One should note that, for historical reasons, our experiments with this PDBBind-based test set employed protein structures predicted by ESMFold (not AlphaFold 3). In the next section, we explore an even more practical application of FLOWDOCK's fast and accurate structure and binding affinity predictions in the CASP16 ligand prediction competition."}, {"title": "CASP16 Protein-Ligand Binding Affinity Prediction", "content": "In Figure 7, we illustrate the performance of each predictor group for blind protein-ligand binding affinity prediction in the ligand category of the CASP16 competition held in summer 2024, in which pharmaceutically relevant binding ligands were the primary focus of this competition. Notably, FLOWDOCK is the only hybrid (structure & affinity prediction) ML method represented among the top-5 predictors, demonstrating the robustness of its knowledge of protein-ligand interactions. Namely, all other top prediction methods were trained specifically for binding affinity estimation assuming a predicted or crystal complex structure is provided. In contrast, in CASP16, we demonstrated the potential of using FLOWDOCK to predict both protein-ligand structures and binding affinities and using its top-5 predicted structures' structural confidence scores to rank-order its top-5 binding affinity predictions. Ranked 5th for binding affinity estimation, these results of the CASP16 competition demonstrate that this dual approach of predicting protein-ligand structures and binding affinities with a single DL model (FLOWDOCK) yields compelling performance for virtual screening of pharmaceutically interesting molecular compounds."}, {"title": "Conclusion", "content": "In this work, we have presented FLOWDOCK, a state-of-the-art deep generative flow model for fast and accurate (hybrid) protein-ligand binding structure and affinity prediction. Benchmarking results suggest that FLOWDOCK achieves comparable results to state-of-the-art single-sequence co-folding methods such as Chai-1 and outperforms existing hybrid models like DynamicBind across a range of binding ligands. Lastly, we have demonstrated the pharmaceutical virtual screening potential of FLOWDOCK in the CASP16 ligand prediction competition, where it achieved top-5 performance. Future work could include retraining the model on larger and more diverse clusters of protein-ligand complexes, experimenting with new ODE solvers, or scaling up its parameter count to see if it displays any scaling law behavior for structure or affinity prediction. As a deep generative model for structural biology made available under an MIT license, we believe FLOWDOCK takes a notable step forward towards fast, accurate, and broadly applicable modeling of protein-ligand interactions."}, {"title": "Conflict of interest", "content": "No conflicts of interest are declared."}, {"title": "Author contributions statement", "content": "A.M. and J.C. conceived the research. A.M. conducted the experiment(s). J.C. acquired funding to support this work. A.M. and J.C. analyzed the results and wrote the manuscript."}, {"title": "Funding", "content": "This work was supported by a U.S. NSF grant (DBI2308699) and a U.S. NIH grant (R01GM093123) awarded to J.C. Additionally, this work was performed using computing infrastructure provided by Research Support Services at the University of Missouri-Columbia (DOI: 10.32469/10355/97710)."}, {"title": "Geometric Flow Matching Training and Inference", "content": "We characterize FLOWDOCK's training and sampling procedures in Sections 3.5 and 3.6 of the main text, respectively. To further illustrate how training and inference with FLOWDOCK work, in Algorithms 1 and 2 we provide the corresponding pseudocode. For more details, please see our accompanying source code."}], "algorithms": [{"title": "Algorithm 1 Training", "content": "Require: Training examples of binding site-aligned apo (holo) protein (ligand) structures, protein sequences, ligand SMILES strings, and binding affinities {(X, X, X, Si, Mi, Bi)} \n1: for all (X, X, X, Si, Mi, Bi) do \n2: Extract x, x \u2190 HeavyAtoms(XP,XL) \n3: Sample xf - ESMFold(Si) + \u20ac, \u03b5 ~ N(0, \u03c3 = 1e-4) \n4: Sample xf\u2190 HarmonicPrior (Mifrag), \u2200frag\u2208 Mi \n5: Sample t~U(0, 1) \n6: Concatenate xo = Concat(xf,x) \n7: Concatenate x1 = Concat(x1,x) \n8: Interpolate xt \u2190 t. x1 + (1-t) xo \n9: Predict Xh\u2081 \u2190 NeuralPLexer(Si, Mi, xt, t) \n10: Predict B\u2081 \u2190 ESDMa f f (Si, Mi, StopGrad(Xh\u2081)) \n11: Optimize losses Lx := 1x \u00b7FAPE(Xh\u2081, Xh\u2081) + LB := \u03bb\u03b2\u00b7 MSE(Bi, Bi), \u03bbx = 0.2, \u03bb\u03b2 = 0.1 \n12: end for"}, {"title": "Algorithm 2 Inference", "content": "Require: Protein sequences and ligand SMILES strings (S, M) \nEnsure: Sampled top-5 heavy-atom structures X with confidence scores \u0108 and binding affinities B \n1: Sample xf\u2190 ESMFold(S) + \u20ac, \u03b5 ~ \u039d\u0384(0, \u03c3 = 1e-4) \n2: Sample x\u2190 HarmonicPrior(Mfrag), frag \u2208 M \n3: Concatenate xo = Concat(x,x) \n4: forn 0 to i do \n5: Lett and s\u2190 n+1 \n6: Predict X NeuralPLexer (S, M, xn, t) \n7: if ni- 1 then \n8: Predict \u0108 \u2190 ESDMcon f (S, M, X\u00c2) # Pre-trained \n9: Predict B ESDMa f f (S, M, X) \n10: Rank top-5 X and B using \u0108 \n11: return X, C, B \n12: end if \n13: Extract 21 \u2190 HeavyAtoms(X) \n14: Align In RMSDAlign(xn, 21) \n15: Interpolate xn+1 = clamp(\u00b7)\u00b7xn+clamp((1\u22121) \u03b7)\u00b7 21, \u03b7 = 1 \n16: end for"}]}