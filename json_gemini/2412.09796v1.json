{"title": "AutoPatent: A Multi-Agent Framework for Automatic Patent Generation", "authors": ["Qiyao Wang", "Shiwen Ni", "Huaren Liu", "Shule Lu", "Guhong Chen", "Xi Feng", "Chi Wei", "Qiang Qu", "Hamid Alinejad-Rokny", "Yuan Lin", "Min Yang"], "abstract": "As the capabilities of Large Language Models (LLMs) continue to advance, the field of patent processing has garnered increased attention within the natural language processing community. However, the majority of research has been concentrated on classification tasks, such as patent categorization and examination, or on short text generation tasks like patent summarization and patent quizzes. In this paper, we introduce a novel and practical task known as Draft2Patent, along with its corresponding D2P benchmark, which challenges LLMs to generate full-length patents averaging 17K tokens based on initial drafts. Patents present a significant challenge to LLMs due to their specialized nature, standardized terminology, and extensive length. We propose a multi-agent framework called AutoPatent which leverages the LLM-based planner agent, writer agents, and examiner agent with PGTree and RRAG to generate lengthy, intricate, and high-quality complete patent documents. The experimental results demonstrate that our AutoPatent framework significantly enhances the ability to generate comprehensive patents across various LLMs. Furthermore, we have discovered that patents generated solely with the AutoPatent framework based on the Qwen2.5-7B model outperform those produced by larger and more powerful LLMs, such as GPT-40, Qwen2.5-72B, and LLAMA3.1-70B, in both objective metrics and human evaluations. We will make the data and code available upon acceptance\u00b9.", "sections": [{"title": "1 Introduction", "content": "As a representative of Intellectual Property (IP), a patent is an exclusive right granted for an invention, which can benefit inventors by providing them with legal protection of their inventions\u00b2. The inventor should draft a patent and submit it to a national or regional intellectual property (IP) office, such as the United States Patent and Trademark Office (USPTO) or the European Patent Office (EPO), to obtain a patent grant (Toole et al., 2020). The patent will be examined by a patent examiner for patentability. The examiner decides whether the proposed invention is useful, non-obvious, and statutory, and searches for prior arts within the technology field of the invention to confirm whether it is novel (USTPO, 2020; EPO, 1994). Therefore, inventors should draft a detailed, embodied patent and maximize the legal scope of protection for the invention without infringing on other patents.\nA patent typically consists of a title, abstract, background, summary, detailed description and claims (World Intellectual Property Organization, 2022). The drafting of patent is usually carried out by a human patent agent who is familiar with patent law and has passed the patent bar exam. Patent covers various technical fields, which requires human patent agents to possess a broad knowledge base. However, this process is still entirely conducted manually which results in high labor and time costs and lowering efficiency.\nWe introduce a novel real-world task named Draft2Patent for converting an inventor's draft into a complete patent, as shown in Figure 1. We construct a challenging benchmark named D2P for"}, {"title": "2 Related Work", "content": "Patent Writing. Researchers have processed the text structure in patent domain with multiple nature language processing methods. (Ni et al., 2024; Wang et al., 2024) focus on the application of patent law-related question-answering task in real-world scenario of intellectual property field. (Jiang and Goetz, 2024) summarizes the patent-related tasks into two types: patent analysis and patent generation. The patent generation task typically includes summarization (Souza et al., 2019; Sharma et al., 2019), translation (Wirth et al., 2023; Heafield et al., 2022), simplification (Casola et al., 2023), and patent writing.\nThe patent writing task previously focused on the internal conversion of a patent. (Lee and Hsiang, 2020) preliminarily validated the feasibility of using GPT-2-based (Radford et al.) language models to construct patent claims. (Lee, 2020) converted patent abstract to claims through fine-tuning transformer-based models. (Jiang et al., 2024) introduce a task for generating claims based on detailed description and constructed a benchmark to test this capability of LLMs. (Zuo et al., 2024) use LLMs to convert claims into abstract and generate subsequent independent or dependent claims from existing claims.\nBut these tasks did not focus on writing a complete patent, (Knappich et al., 2024) constructed a dataset with paper-patent pairs based on chunk-based, outline-guided method to convert papers into patents. But in real-world scenarios, patent granting is affected by previously published papers. Our Draft2Patent task focuses more on interaction between inventors and patent agents, aiming to generate a complete and high-quality patent that can even be submitted to the IP office.\nLLMs-based Multi-Agent Framework for Long"}, {"title": "3 Draft2Patent Task", "content": "The IP office only publishes granted patents of inventors, while inventors and patent agents never make their drafts public. In this section, we introduce our novel patent drafting task Draft2Patent and the agent-based method we used to construct the D2P benchmark, in detail."}, {"title": "3.1 Task Definition", "content": "We define Draft2Patent task by simulating real-world scenarios, as shown in Figure 1. In real scenario, inventors usually deliver a patent technical draft, which contains the most comprehensive information, to a human patent agent, and ask them to draft a high-quality patent. The patent agent will review and rewrite the patent in terms of patentability, terminology standardization, terminology consistency, claim drafting and legal compliance. This process typically costs a lot of time before the patent is submitted to the IP office.\nWe leverage an agent-based method to simulate the interaction between inventors and patent agents, designing five questions q1, q2, ..., q5 that encompass all the relevant information about an invention. We combine them with the inventors' answers a1, a2, ..., a5 to form the patent draft D and then we can use it to generate the patent P:\nD = {(q1, a1), (q2, a2), ..., (q5, a5)} \\qquad (1)\nWhere five questions q1, q2, ..., q5 are as shown in Appendix A.1. And where the P consists of the title, abstract, background, summary, claims, and detailed description of a patent."}, {"title": "3.2 D2P Dataset Construction", "content": "Draft Construction. We use the HUPD (Suzgun et al., 2023) dataset constructed based on USPTO's publicly available patent as data source. We randomly select granted patent samples labeled with decision as ACCEPTED, ensuring that the contents of them are complete. For the patent P, we simulate GPT-40-mini as the inventor, asking it the five questions 91, 92, ..., 95. The corresponding answers a1, a2, ..., a5 are then combined to form the draft.\nDraft Quality Review. After obtaining 2,000 patent drafts, we establish a standard for assessing the quality of the answers to each question qi to ensure they contain sufficient information about the invention. We simulate GPT-40 as a patent examiner agent to evaluate whether the answers fully address these questions, the concrete prompt as shown in Appendix A.2. Finally, we obtain 1,933 patent drafts that meet the quality standard through the collaborative assessment of the LLM patent agent and human patent agent. These drafts are then divided into a training set of 1,500, a validation set of 133, and a test set of 300.\nOther Metadata Construction. Our D2P dataset not only contains draft-patent pairs, but also includes a fine-tuning dataset for short components generation and patent writing guideline tree (PGTree) generation. We combine the metadata in HUPD with drafts to create paired data, such as draft-title pairs. For patent P, we simulate GPT-40-mini as a assistant to summarize each part of the P's description, using it as a PGTree W for writing the detailed description.\nWe calculate the average length of the 1,933 drafts, patents, PGTrees, and other metadata in D2P benchmark using GPT-40-mini's tokenizer, as shown in Table 1. The average length of a complete patent exceeds 17K tokens, with the detailed description averaging over 14K tokens and accounting for more than 80% of the total, while each remaining section averages less than 2K tokens."}, {"title": "4 AutoPatent Framework", "content": "We propose an automatic multi-agent patent drafting framework named AutoPatent for Draft2Patent, as shown in Figure 2. We design a specialized pipeline with eight agents and three steps to simulate the process of patent drafting in real-world scenario. We use five specialized agents to generate various sections of a patent, assigning a dedicated writer agent to each short component. And we assign a planning agent to generate a two layers, multi-way PGTree that instruct the reference-review-augmented generation (RRAG) process. We also assign an examiner agent to evaluate the quality of the generated subsections and provide modification suggestions.\nIn this section, we introduce each agent and the workflow of our AutoPatent framework for drafting the patent P in detail, given the draft D. We also provide Algorithm 1, which outlines the key steps of AutoPatent framework."}, {"title": "4.1 Agents", "content": "We define each agent A as a sequence-to-sequence model that takes text as input and generates text as output. In AutoPatent framework, we design eight agents, categorized into three types: writer, planner, and examiner. Each agent has its own task and set of instructions.\nWriter Agent. We categorize six writer agents into two types: short component writer and description writer. Different parts of a patent exhibit significant stylistic differences, with the abstract typically being a single short paragraph and the claims often being lengthy and structured with numbered points. As shown in Table 1, all the average length of component is less than 2K tokens, except detailed description.\nWe define these agents as Ai, where i \u2208 {T, A, B, S, C, D}, representing the corresponding component writers responsible for generating"}, {"title": "4.2 Framework Workflow", "content": "As shown in the blue section of Figure 2, we divide the workflow of the AutoPatent framework into three steps, simulating the real-world scenario of patent drafting.\nShort Components Generation. In step I, we leverage different agents to generate various short components of a patent based on draft D, considering differences in style. For open-source models with a parameter size of less than 14B, we fine-tune them using the D2P training set to enhance their ability to generate high-quality short components, while zero-shot prompting is used for commercial or larger models. The concrete prompt for supervised fine-tuning of short component generation is shown in Appendix B.1. We then combine the generated title, abstract, background, summary, and claims with the draft D to form the reference R, which can offer useful information for detailed description generation.\nPatent Writing Guideline Tree (PGTree) Building. In step II, given draft D, we fine-tune the planning agent for smaller models and use zero-shot prompting for others, to generate a PGTree W for a detailed description. Assuming that the a PGTree consist of m sections, the i-th section contains ti subsections, as shown in Figure 3. The generated PGTree is structured as a two-layer multi-way tree, dividing the description generation task into two levels of outlines. The first level of the outline provides an overview of the section, while the second level offers concrete instructions for the description writer to generate concise content for that part of the description. The prompt as shown in Appendix B.2.\nReference-Review-Augmented Generation (RRAG). In step III, given the guideline of nij, the description writer retrieves useful information rij from reference R. This process connects the planning agent with other short component writers, enhancing consistency throughout the entire patent and reducing the difficulty of tasks like writing claims in the detailed description. The prompt for retrieval as shown in Appendix B.1.\nAfter obtaining rij, we combine it with nij and the PGTree W to instruct the description writer in generating the corresponding part dij of the description. The examiner agent then actively intervenes to review the quality of dij and provides feedback for its refinement through multi-turn interactions with the description writer until the examiner agent deems the output acceptable. After traversing the PGTree for the m sections, all the accepted dij are concatenated to form the complete detailed description d. Finally, we combine all the generated text to form a complete patent P. The description gen-"}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Evaluation Metric", "content": "We use the n-gram-based metric, BLEU (Papineni et al., 2002), the F1 scores of ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) as the objective metrics. (Sai et al., 2022) indicated that n-gram-based metrics exhibit a preference for repeated n-grams and short sentences. We propose a new metric, termed IRR (Inverse Repetition Rate), to measure the degree of sentence repetition within the patent P = {si|1 \u2264 i \u2264 n}, which consists of n sentences. The IRR is defined as:\nIRR(P,t) = \\frac{C_2^n}{\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} f(s_i, s_j) + \\varepsilon} \\qquad (2)\nWhere \\varepsilon is a small value added for smoothing to prevent division by zero, and t is threshold for determining whether two sentences, si and sj, are considered repetitions based on their Jaccard similarity J, calculated after removing stop words. The function f(si, sj) is defined as:\nf(s_i, s_j) = \\begin{cases} 1, & \\text{if } J(s_i, s_j) \\geq t, \\\\ 0, & \\text{if } J(s_i, s_j) < t. \\end{cases} \\qquad (3)\nWe invite three experts who are familiar with the patent law and patent drafting to evaluate the quality of generated patent using a single-bind review. We provide them with the review standards"}, {"title": "5.2 Compared Method", "content": "We compare our AutoPatent framework with two baseline methods: zero-shot prompting and supervised fine-tuning. Both of these methods take the draft as input and output a complete patent in an end-to-end manner.\nZero-shot Prompting Generation. We use the zero-shot prompt shown in Appendix C.1 to instruct commercial models, including GPT-40 and GPT-40-mini, and open-source models, including different sizes of LLAMA3.1, Qwen2.5 and Mistral to generate complete patents directly. We set the maximum token limit to all models, such as 16,384 for the GPT series models and 32,768 for the LLAMA3.1 series models. The temperature is set to 0.5 and top p to 0.9 for all models to improve the stability of the output.\nSupervised Fine-Tuning Generation. We use 1,500 draft-patent pairs from D2P's training set for fully supervised fine-tuning of the LLAMA3.1-8B,"}, {"title": "5.3 Result", "content": "Objective Metric Results. We report the objective metric results on D2P's test set and the average length of generated patent in Table 2. Observing the average length in the results, all models generate patents with an average length of less than 3,000 tokens using the zero-shot prompt. While generated with AutoPatent, the average length of the patent exceeds 10K tokens. For n-gram-based metric, our AutoPatent framework achieves the higher performance both within the same base models. We observed that when leveraging Qwen2.5-7B as the base model in the AutoPatent framework surpasses the performance of GPT-40-mini.\nThe average IRR score across all real patents in the test set is 91.33 when t is 0.2 and 98.57 when t is 0.4. This phenomenon is primarily attributed to the stylistic characteristics of patent language, such as the inclusion of claims within the description. The patents generated using the supervised fine-tuning method exhibit significant repetition errors, with an IRR score of 49.17 for LLAMA3.1-8B+SFT, 71.18 for Qwen2.5-7B+SFT and 62.49 for Mistral-7B+SFT when the threshold is set to 0.2. These severe repetition errors lead to over-rewarding in n-gram-based metrics, resulting in the best scores for SFT, despite its actual quality being poor.\nHuman Evaluation Results. We report the human evaluation results in Figure 4, comparing Qwen2.5-7B+AutoPatent with zero-shot prompting generation (denoted as GPT-40, GPT-40-mini, Qwen2.5-7B, Qwen2.5-72B, LLAMA3.1-70B) and SFT generation (denoted as Qwen2.5-7B+SFT) for 50 generated patents. The three human experts all agree that the quality of the com-"}, {"title": "6 Analysis", "content": ""}, {"title": "6.1 Ablation Study", "content": "We conduct three types of ablation experiments. Two of these use GPT-40-mini as the base model to evaluate the AutoPatent framework without PGTree or RRAG module. The third experiment uses LLAMA3.1-8B as the base model to evaluate the performance of the short components writer and planning agent without fine-tuning. The results are reported in Table 3 and Table 4.\nAblation on PGTree. We conduct an ablation experiment on PGTree. After the different short component writers generate the corresponding parts of the patent, the description writer completes the full detailed description in a single pass without utilizing the PGTree. Observing the results in Table 3, the average length drops below 2,000 tokens, and all objective metrics decrease significantly, with BLEU experiencing an almost 15-times reduction.\nAblation on RRAG. We conduct an ablation experiment on RRAG. When the description writer generates a subsection dij using the guideline nij, it simply adds it to the list of description candidates without considering advice from the examination agent. Table 3 shows that removing the RRAG results in a 4.8% reduction in the BLEU score, along with declines in all other objective metrics. Without the reference and the supervision of the examiner agent, repetition errors slightly increase, as reflected by a minor decrease in the IRR score.\nAblation on Fine-tuning Agent. We conduct an ablation experiment on generating short components without fine-tuning. Due to differences among patents, the base model exhibits varying"}, {"title": "6.2 Case study", "content": "We carefully review all the generated patents based on different methods and conduct a detailed analysis. The generated patent using SFT exhibits significant repetition errors, resulting in meaningless content and even leading to the failure to generate complete content. This phenomenon results in high ROUGE scores for SFT, but human evaluation highlights its shortcomings. As shown in Appendix D.1, we present a comparison between SFT and our AutoPatent framework.\nThe patents generated using AutoPatent exhibit greater comprehensiveness, with no missing parts, and are even capable of generating flowcharts. Other methods often fail to generate complete content, such as missing descriptions, as shown in Appendix D.2. The AutoPatent framework also improves consistency in generated patents, such as ensuring alignment between claims in the descrip-"}, {"title": "7 Conclusion", "content": "In this work, we introduce a novel and practical task, Draft2Patent and its corresponding D2P benchmark containing 1,933 draft-patent pairs, which requires LLMs to generate full patent documents with an average length of 17K tokens. Due to the specialized nature of patents, standardized terminology, and long length, mainstream LLMs perform poorly. We propose an innovative multi-agent framework, AutoPatent, which capitalizes on the collaborative efforts of a LLM-based planning agent, six writing agents, and a review agent to produce high-caliber patent documents with novel PGTree and RRAG method.\nOur experimental results indicate that AutoPatent markedly enhances the capacity of various LLMs to generate full patents. Moreover, we have discovered that patents generated exclusively with the AutoPatent framework, utilizing the Qwen2.5-7B model, surpass those produced by more extensive and potent LLMs, such as GPT-40, Qwen2.5-72B, and LLAMA3.1-70B, in both objective metrics and human evaluations. Remarkably, the quality of patents generated by AutoPatent rivals that of human authorship. We hope that AutoPatent will revolutionize the way patents are generated and managed, simplifying the process and lowering the barriers to innovation."}, {"title": "Limitations", "content": "The patent evaluation task is highly challenging, involving intricate legal and technical standards that demand meticulous review by human experts. This results in low efficiency and high costs in human evaluation, and we will explore a concise method for automated patent evaluation in the future. Due to limitations in computing resources, we do not fully fine-tune LLMs with a parameter size of 14B or larger."}, {"title": "Ethics Statement", "content": "All patent data used in this paper are obtained from publicly accessible sources. The purpose of the Draft2Patent task is to improve the efficiency of patent agents in drafting applications before submission to the IP office. We do not encourage people to use our method to generate fake or meaningless patents that would burden the IP office's examination department. We acknowledge that the patents generated by our method are not yet sufficient to be submitted directly to the IP office. They still require modification by a patent agent to ensure they meet patent law and technical standards."}]}