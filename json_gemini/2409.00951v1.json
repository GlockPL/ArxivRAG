{"title": "Semantically Controllable Augmentations for Generalizable Robot Learning", "authors": ["Zoey Chen", "Zhao Mandi", "Homanga Bharadhwaj", "Mohit Sharma", "Shuran Song", "Abhishek Gupta", "Vikash Kumar"], "abstract": "Generalization to unseen real-world scenarios for robot manipulation requires exposure to diverse datasets during training. However, collecting large real-world datasets is intractable due to high operational costs. For robot learning to generalize despite these challenges, it is essential to leverage sources of data or priors beyond the robot's direct experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. These generative models encompass a broad range of real-world scenarios beyond a robot's direct experience and can synthesize novel synthetic experiences that expose robotic agents to additional world priors aiding real-world generalization at no extra cost.\nIn particular, our approach leverages pre-trained generative models as an effective tool for data augmentation. We propose a generative augmentation framework for semantically controllable augmentations and rapidly multiplying robot datasets while inducing rich variations that enable real-world generalization. Based on diverse augmentations of robot data, we show how scalable robot manipulation policies can be trained and deployed both in simulation and in unseen real-world environments such as kitchens and table-tops. By demonstrating the effectiveness of image-text generative models in diverse real-world robotic applications, our generative augmentation framework provides a scalable and efficient path for boosting generalization in robot learning at no extra human cost.", "sections": [{"title": "Introduction", "content": "While robot learning has often focused on the search for plausible policies (Levine et al. 2015; Nagabandi et al. 2019) or motions plans (Qureshi et al. 2018) in specific scenarios, the benefits of learning methods in robotics come from the prospect for generalization. Going beyond policy optimization in highly controlled situations such as warehouses or factories, robot learning methods have the potential for widespread generalization across tasks, environments, and objects. While techniques such as imitation learning methods circumvent the challenges of exploration, teaching a robot various skills requires a large amount of experience and diverse data sources. Different from collecting vision and language data, robot demonstration data requires active interaction with the scene. This is thus expensive, and prior works have indeed spent years collecting large robot manipulation datasets for imitation, through techniques like tele-operation Brohan et al. (2022). Beyond the total quantity of data, the rigidity of most robotics setups makes it non-trivial to collect diverse data in a wide variety of scenarios. As a result, many robotics datasets involve a single setup with just a few hours of robot data.\nThe limitation of data diversity has been a challenge for the field of machine learning in general. Training reliably effective models primarily hinges on access to datasets that comprehensively represent the target environment. Beyond the challenges of scale, informational diversity while pivotal is hard to capture. These limitations often impede the model's ability to generalize effectively to unseen scenarios. To get around these challenges, data augmentation techniques, such as color adjustments, Gaussian blur, and cropping, have traditionally been exploited to enhance the generalization capabilities of machine learning models.\nThese techniques have also proven effective in the field of robot learning for handling minor variations in appearances (color, lighting, etc). They however fall short in addressing structured variations in the scene such as the introduction of distractors, alterations in the background, or changes in the object's visual appearance. These limitations arise from their inability to provide augmentations introducing diverse, realistic, and semantic alterations in the data, which are crucial for training robust policies capable of adapting to diverse unseen real-world scenarios. These considerations are particularly important in the field of robotics, where the availability of data is often constrained due to operational and safety challenges. The ability to simulate a wide range of realistic and semantically meaningful scenarios is therefore crucial for generalization.\nIn this work, we introduce a framework for semantic data augmentation, which aims at automatically and significantly enabling broad robot generalization, by leveraging pre-trained generative models. While on-robot data can be limited, the"}, {"title": "Related Work", "content": "Variance Injection into Learning The concept of injecting invariance into learning models has been employed in prior works. Domain randomization, for instance, exposes physics invariances but relies on access to parametric models of the environment. Our work focuses on visual generalization\u2014a domain where access to such environmental parameters is often not feasible. The most widely used technique for injecting visual variance is various forms of data augmentation (Shorten and Khoshgoftaar 2019a), such as cropping, shifting, noise injection and rotation. These methods have been used in many robot learning approaches and provide a significant improvement in data efficiency (Benton et al. 2020; Cubuk et al. 2018; Shorten and Khoshgoftaar 2019b; Kostrikov et al. 2020). For example, (Zafar et al. 2022) investigate different augmentation modes in Meta-learning settings. In addition, several methods attempt to enforce geometric invariance through architectural innovations such as (Wang et al. 2022) and (Deng et al. 2021). While these methods can provide a local notion of robustness and invariance to perceptual noise, they do not provide generalization to novel object shapes or scenes. More recently out-of-domain models have started making their way into robot learning. For example - (Kapelyukh et al. 2022) uses large text-image models like Dall-e (Ramesh et al. 2022) to generate favorable image goals for robots.\nThese approaches while helpful in task specification, provide limited benefit for robots to generalize to entirely unseen situations. In contrast, our framework induces semantic changes to the observations thereby helping acquire behavior invariance to new scenes.\nAlternate Data Sources in Robotics. The recent advancements in self-supervised methods across language and vision fields have shown the benefits of utilizing extensive datasets. A lot of recent works have studied the use of pre-trained visual representations trained mainly on datasets of non-robot interactions (Grauman et al. 2022; Deng et al. 2009b), for learning control policies (Nair et al. 2022a; Parisi et al. 2022; Shridhar et al. 2022; Majumdar et al. 2023; Shah and Kumar 2021). Many works focus on single-task settings (Nair et al. 2022a; Parisi et al. 2022; Sharma et al. 2023; Hansen et al. 2022), or simulated robot environments (Hansen et al. 2022; Majumdar et al. 2023). Given challenges with collecting large real-world robotics datasets, some works focus on alternate data sources such as language (Tellex et al. 2011; Lynch and Sermanet 2020; Stepputtis et al. 2020; Brohan et al. 2023), human videos (Nguyen et al. 2018;"}, {"title": "Background and Formulation", "content": "Here, we describe the problem statement we consider in our semantic data augmentation technique Generative Augmentation and show how generative models can conceptually be used to inject semantic invariances into the robot learning frameworks. Shown in Figure 1, we aim to bootstrap an initial small offline dataset using generative augmentation, and train a robot policy that generalizes widely on unseen environments and tasks. In this section, we will first formulate the problem of learning from demonstrations, followed by the proposed method of leveraging generative models for data augmentation."}, {"title": "Problem Formulation", "content": "Our work considers general robotic decision-making problems and we specifically focus on robot manipulation. Our setup considers a robot arm that receives sensory observations $o\\in O$ such as camera observations, and outputs appropriate action $a \\in A$ (e.g. where to move the robot arm for picking up an object). Our goal is to learn a model (a policy) $f_\\theta: O \\rightarrow \\Delta A$ (where $\\Delta A$ denotes the simplex over actions) that predicts a distribution over actions such that the action $a \\sim f_\\theta(.|o)$ can accomplish a task when executed in the environment. In this work, we restrict our consideration to supervised learning methods for learning $f_\\theta(.0)$. We assume a human expert provides a dataset of demonstrations $D = \\{(o_0, a_0), (o_1, a_1),..., (o_N,a_N)\\}$ for solving different tasks. We use maximum likelihood training to learn optimal policies for the provided demonstrations (Zeng et al. 2020; Shridhar et al. 2021):\n$\\max_\\theta E_{(o,a)\\sim D} [\\log f_\\theta(a|o)]$\nAs noted above, our training process is limited to the demonstration dataset D collected by the human supervisor. Since collecting large-scale human demonstration data is hard, the dataset size |D| is most often quite limited. Data augmentation techniques are often used to increase the dataset size. Data augmentation methods apply augmentation functions $q: O\\times A\\times Z \\rightarrow O \\times A$ which generate augmented data $(o', a') = q(o, a, z); z \\sim p(z)$, where different noise vectors z generate different augmentations. This could include augmentations like Gaussian noise, cropping, and color jitter amongst others (Benton et al. 2020; Cubuk et al. 2018; Shorten and Khoshgoftaar 2019b; Perez and Wang 2017). Using this augmentation function we can sample a large number of different augmentations to create an augmented dataset"}, {"title": "Leveraging Generative Models for Data Augmentation", "content": "While data augmentation methods typically hand-define augmentation functions $(o', a') = q(o, a, z); z \\sim p(z)$, the generated data $(o', a')$ may not be particularly relevant to the true distribution of real-world data. Since most of these generated variations do not appear in the real-world distribution it is unclear if generating such a large augmented dataset $D_{aug}$ helps learned predictors $f_\\theta$ generalize in real-world settings. By contrast, the key insight in our framework is that pre-trained text-to-image generative models such as Stable Diffusion (Rombach et al. 2022a)\nare trained on the distribution $p_{real}(o)$ of real images (including real scenes that a robot might find itself in). This lends them the ability to generate (or modify) the training set observations o in a way that corresponds to the distribution of real-world scenes instead of a heuristic approach such as described in (Perez and Wang 2017). We will use this ability to perform targeted data augmentation for improved generalization of the learned predictor $f_\\theta$.\nWe formalize our augmentation setting by assuming access to generative models $g : T \\times O \\times Z \\rightarrow O$, which map from an image o, a text description t, and a noise vector z to a modified image $o' = g(o, t, z); z \\sim p(z)$. This includes commonly used text-to-image inpainting models such as Make-A-Video (Singer et al. 2022), DALL-E 2 (Ramesh et al. 2022), Stable Diffusion (Rombach et al. 2022b) and Imagen (Saharia et al. 2022).\nWhile these generative models excel at creating novel visual observations o, they do not inherently generate new actions a. Instead, their strength lies in the potential to enforce semantic invariance in the learned model $f_\\theta$, ensuring that varied but semantically related observations $o, g(t_1, o, z_1), g(t_2, o, z_2), ..., g(t_M,o,z_M)$ correspond to the same action a. To harness this potential in pre-trained text-to-image generative models for semantic data augmentation, we can generate sets of semantically equivalent observation-action pairs $(o, a), (g(t_1, o, z_1),a),...$ for each $(o, a) \\in D$, ensuring the generated observations maintain semantic equivalence with the original action a.\nThis enables generating a diverse dataset of seman-tically meaningful augmentations while still performing the specific task in the respective trajectories. Unlike typical data augmentation with the hand-defined shifts described above, the generated augmented observations $\\{g(t_1, o, z_1), g(t_2, o, z_2), ..., g(t_M, o, z_M)\\}$ have a high like-lihood under the distribution of real images $P_{real}(o)$ that a robot may encounter on deployment. This ensures that the model generalizes to a wide variety of novel scenes, making it significantly more practical to deploy in real-world scenarios since it will be robust to changes in objects, distractors, backgrounds, and other characteristics of an environment. Although our approach allows us to create a large set of relevant augmentations it still has few limitations. First, our augmentations can only create new observations for provided actions and cannot generate novel actions a, Second, generating new observations without care can also sometimes lead to physically inaccurate augmentations, e.g., inaccurate contacts leading to collisions between objects, physical inconsistencies such as objects in air. In the next section we use a common table-top robotic manipulation to discuss our method in more detail."}, {"title": "Generative Augmentations for Robot Learning", "content": "We describe our framework for generative augmentations in robot learning, that enables training policies through behavior cloning for generalization to environments and tasks beyond the original demonstrations."}, {"title": "Framework Overview", "content": "Semantic Augmentation In the initial phase, the pre-collected dataset is expanded by generating a variety of seman-tic augmentation to the robot's existing experiences. This process transforms a single or limited robotic demonstration into multiple versions, each containing different semantic elements like objects, textures, and backgrounds, without requiring additional human demonstrations. This approach of enriching data with real-world semantic variances enhances the multi-task agent's generalization to unforeseen, out-of-distribution scenes that the robot might encounter during test time.\nPolicy Learning \u2013 The second phase focuses on learning robust robot skills using a small amount of robot data. This is achieved by adapting design choices from previous works that were usually limited to single-task environments and applying them to achieve larger-scale adaptability across various multi-task and multi-scene manipulation tasks. In addition to the single-task policies, we introduce a multi-task, language-driven policy framework. These are designed to train versatile agents capable of acquiring a range of skills from diverse, multi-modal datasets."}, {"title": "Semantic Data Augmentation", "content": "We consider two different regimes for our semantic augmentation. The first is a low-data regime that is controllable over the structure of semantic augmentations, including the use of 3D meshes and segmentation masks. This gives more control over augmentations and allows more physically plausible augmentations. However, its manual aspects limit its scalability and make it less feasible to augment extensive datasets. The second is a regime with large data, with a framework for completely automatic augmentations using pre-trained models. This approach enables us to scale up to datasets comprising thousands of trajectories, facilitating the learning of extensive multi-task policies, which can be deployed in diverse scenarios based on a specified goal. We detail each regime in the following sections.\nStructure-Aware Augmentation for Low-Data Regime In this setting, we focus on how to perform controllable augmentation that is structure-aware and physically plausible on both RGB and depth images. By maintaining consistency between the augmented RGB images and their corresponding"}, {"title": "Scalable Augmentation for Multi-Task Data", "content": "In order to augment large multi-task datasets at scale, we develop an automatic augmentation strategy that doesn't require any manually specified parameters such as object masks or object meshes and doesn't require training or fine-tuning any model. Starting with an initial collection of robotic behaviors, we bootstrap this dataset by generating multiple semantically-augmented versions of it, while keeping the robot's behavior consistent in each trajectory. These semantic alterations are produced by implementing frame-by-frame augmentations within each trajectory. that is fully automatic. In particular, we implement two types of scene augmentations on RGB images: Object Augmentation: Using the robot's joint angles in a specific trajectory frame, we apply forward kinematics to derive both the robot's mask and the position of its end-effector. The end-effector's location is used to guide SegmentAnything (Kirillov et al. 2023a) in generating a mask for the object being manipulated. This object is then altered through inpainting, based on textual prompts. To ensure temporal consistency, we employ TrackAnything (Yang et al. 2023a) to track the object across the trajectory. Background Augmentation: Segment Anything (Kirillov et al. 2023a) is utilized to select a group of background objects that don't intersect with either the robot's mask or the mask of the interacting object. We then inpaint these background areas using an overall mask created from the aggregation of all object masks identified by Segment Anything. This approach allows for varied background alterations in the scene."}, {"title": "Policy Learning", "content": "In this section, we first explain how our augmentation pipeline can benefit single-task policies that rely on RGBD data, then show how to extend our policy learning to multi-task domains more efficiently with larger-scale datasets.\nSingle-Task Policy Learning Our single-task learning network is based on Transporter Network (Zeng et al. 2020) and CLIPort (Shridhar et al. 2021). The transporter network learns spatial correspondences between visual"}, {"title": "System Setup", "content": ""}, {"title": "Task Overview", "content": "To demonstrate the effectiveness of generative augmentation, We consider evaluations at different generalization levels by applying randomization on a scene. In particular, we define 4 types of unseen environments to evaluate how well generative augmentation can help with generalization at"}, {"title": "Generative Augmentation", "content": "As described in Semantic Data Augmentation, we present two types of generative augmentation.\nThe Structure-Aware Augmentation generates RGBD augmentation which requires object 3D meshes to generate cross-category augmentations and distractors. To perform this augmentation, we use 40 object meshes from the GoogleScan dataset (Downs et al. 2022) and Free3D (Free3D). Of these, we choose 11 objects to augment the original object of interest and 12 objects to augment the target receptacle. Any of the remaining 38 objects are then randomly chosen as distractors. During augmentation, we randomly select which components (table, object texture, shape, distractors) to change to generate the augmented training dataset. For each demonstration, we apply augmentation 100 times resulting in 1000 augmented environments per task. The augmented data is then passed into Cliport (Shridhar et al. 2022) to learn a language-conditioned policy for Pick-and-Place tasks.\nScalable Augmentation as described in Scalable Aug-mentation for Multi-Task Data, improves the augmentation efficiency of the structure-aware method, which is automatic and does not require any manual effort in specifying masks, object meshes etc. Since this type of augmentation only operates on RGB images, we train a transformer-based visual policy that only takes RGB observations."}, {"title": "Real-world Setup", "content": "For the Pick-and-Place tasks, we use the 6 DoF xArm5 with a vacuum gripper manipulator and control it directly in the end-effector space. Figure 9 shows our overall setup. We use an XArm mounted on a table in a well-lit room. We use a tripod with a depth camera mounted on top and position the tripod such that it has a full view of the robot as well as any objects on the table. We use an automated setup to collect demonstrations for our pick-place task. Specifically, we use the image captured by the frontal camera and project it into a 2D top-down image and height map. We use this image and have users annotate pick-and-place locations on it. We then convert these pick-and-place image coordinates to world coordinates and use an inverse kinematics controller to reach these positions and perform the pick-place gripper actions. Overall, we collect data for 10 tasks and for each task we collect 10 demonstrations. All data is collected in one single environment as shown as \"Demo Environment\" in Figure 9. Appendix A provides further details on each task.\nTo guide the robot to complete the new tasks in the cluttered environment, we largely build on the architecture and training scheme of CLIPort (Shridhar et al. 2022), which combines the benefits of language-conditioned policy learning with transporter networks (Zeng et al. 2020) for data-efficient imitation learning in tabletop settings. CLIPort (Shridhar et al. 2022) requires RGBD observations as input, which is obtained from an Intel RealSense Camera (D435i). We then manually label the object masks for the collected demonstrations and apply structure-aware augmentation to generate a larger dataset of augmented RGBD data. The input observations are then projected to a top-down view which CLIPort takes, together with language prompts, and predict where to pick and place (Zeng et al. 2020). Going beyond simple Pick-and-Place tasks, we additionally conduct experiments on multi-task kitchen environments. Figure 10 shows our overall setup. We use a kitchen setup that consists of common everyday objects, a Franka Emika Panda arm with a two-finger Robotiq gripper fitted with Festo Adaptive Fingers. We mount three static cameras (top, left, right) to provide a full view of the scene. We further use a wrist camera mounted for more precise motions. Our multi-camera setup provides an exhaustive view of the workspace, which allows for robust policy learning."}, {"title": "Baseline Definition", "content": ""}, {"title": "Experiments", "content": "We evaluate the effectiveness of our framework in both the real world and simulation. Our goal is to: (1) demonstrate our framework is practical and effective for real-world robot learning, (2) compare our method with other baselines in end-to-end vision manipulation tasks. We will first show our results in a real-world setting for both single-task learning and multi-task learning, followed by an in-depth analysis and discussions. In addition, all simulation results are detailed in Appendix B."}, {"title": "Real-World Experiments", "content": "We conduct a thorough evaluation of the efficacy of our approach focusing on two different learning settings. We first evaluate its performance in simple pick-and-place tasks using single-task language-conditioned policies. This evaluation serves as a benchmark for the fundamental capabilities of our framework in a low-data regime. We then extend our experiments beyond the pick-and-place tasks, aiming to demonstrate the generalization and adaptability of our method. In particular, we show how our approach performs when learning multi-task policies for behavior cloning tasks with video trajectories.\nPick-and-Place (L3 and L4 Generalization) To show the generalization capability of a model trained with generative augmentation, we first collect demonstrations of 10 tasks in one single environment and create different styles of test environments such as \"Playground\", \"Study Desk\", \"Kitchen"}, {"title": "Ablations", "content": ""}, {"title": "Discussion", "content": "Trade-off between structure consistency and scalability"}, {"title": "Failure Cases", "content": "Failures in Generative Augmentation We observe two typical failure modes during generative augmentation. Shown in Figure 20, (1) When applying in-painting diffusion models, smaller mask regions usually lead to invalid augmentation. (2) depth-guided augmentation results to unrealistic augmentation when the prompt is not specific, such as \"a mouse\" instead of \"a computer mouse\".\nFailures in Robot Experiments We observe failure cases usually occur when the background color is similar to the pick or place object. Or one of a few distractors has a very bright color or similar color. We expect this can be improved by increasing the number of augmentations in the training"}, {"title": "Limitations", "content": "Action Assumption Despite showing promising visual diversity, our work does not augment action labels and reason about physics parameters such as material, friction, or deformation, thus it assumes the same action still works on the augmented scenes. For the augmented cluttered scenes, we assume the same action trajectory is not colliding with the augmented objects. Augmentation Speed It usually takes about 30 seconds to complete all the augmentations for one scene, which might not be practical for some robot learning approaches such as on-policy RL. Long-horizon Learning An important constraint in our work is the focus solely on isolated skills for each task. A promising avenue for future research lies in devising methods that can autonomously compose these skills. This development would be crucial for tackling tasks that require extended planning and execution horizons."}, {"title": "Conclusion and Future Work", "content": "We present Generative Augmentation, a novel system for augmenting real-world robot data. Our approach leverages a data augmentation approach that bootstraps a small number of human demonstrations into a large dataset with diverse and novel objects. By automatically growing an initial small robotics dataset using semantic scene augmentations, we train a language-conditioned policy using the augmented dataset and demonstrate that generative augmentation can enable a robot to generalize to entirely unseen environments and objects. For future work, we are interested in developing a more scalable augmentation approach that is consistent and fast while still maintaining physical plausibility. Additionally, whether a combination of language models and vision-language models can yield impressive scene generations"}, {"title": "Appendix A: Augmented Dataset in Simulation", "content": "Given demonstrations from a task collected in simulation, we apply augmentation 100 times for each demonstration. We visualize examples of the augmented dataset in Figure 22.\nWe also observe diverse visual augmentation on the same object template, as shown in Figure 23. Given different text prompts, our method is able to generate different and realistic textures."}, {"title": "Learning from out-of-domain priors", "content": "In addition, we investigate whether learning from a pretrained out-of-domain visual representation can improve the zero-shot capability on challenging unseen environments. In particular, we initialize the network with pre-trained R3M (Nair et al. 2022b) weights and finetune it on our dataset.\nWe use baselines described above with two imitation learning methods: TransportNet (Zeng et al. 2020) and CLIPort (Shridhar et al. 2021). Since all the baselines cannot update the depth of the augmented images, we only use RGB images instead of RGBD used in the original TransporterNet and CLIPort. For each baseline, we train 5 tasks in simulation and report their average success rate in Table 3. We observe our work notably outperforms other approaches in most of the tasks. One interesting observation is that randomly copying and pasting segmented images or replacing the background images can provide reasonable robustness in unseen environments but are not able to achieve similar performance as ours at unseen objects. This indicates generating semantically meaningful and physically plausible scenes is important.\nVisualization of Baseline Data augmentation We visualize some examples of randomly copying and pasting segmented images from LVIS dataset (Gupta et al. 2019), as shown in Figure 25.\nWe observe this baseline often results in unrealistic, low-quality image generation, which does not usually match observations during test time in both the real-world and simulation."}, {"title": "Multi-Task Kitchen Tasks", "content": "In addition to simple tabletop pick and place tasks, we also perform in-depth experiments on multi-task kitchen tasks. In this experiment, instead of operating augmentation using generative models, we hope to conduct a simple test and see how much augmentation would improve a multi-task robot policy. We utilize the advantage of simulation and directly perform augmentation to change layout, texture, and lighting conditions.\nTask Configuration The simulation setting for multi-task kitchen tasks comprises 18 distinct tasks involving 8 primary objects, such as activating a light switch, opening a cabinet's left door, and adjusting a knob. Alongside these semantic tasks, we've crafted 100 unique kitchen layouts through randomization. The 8 objects include four stove knobs, a light switch, a kettle, two types of cabinets, and a microwave. We use a standard on-policy RL algorithm, namely NPG (Kakade 2001), to train a fleet of single-task, single-layout expert policies \u03c0(st) from state-based input observations st. For each task T, we define a reward function rt, and the expert policy \u3160\u315c receives the current simulator state st = {robot, object}pos,vel as observation at time-step t. We generate 100 layout variations for each of the 18 tasks and trains an expert policy for each layout, hence resulting in a total of 900 policies. The process of training experts is cost-effective and can be efficiently parallelized. In practical terms, we start a substantial number of training runs simultaneously. Then, we identify and select converged policies as experts by applying a success rate threshold of 90%.\nTraining Details We employ a 43-dimensional context vector to inform a versatile, multi-task policy model. A task is considered successful if the manipulated object's final position closely aligns with its intended goal position, within a specified margin of error. This embedding is meticulously designed to encode both the specific pose of the target object for each task and the unique layout configuration. To test the"}, {"title": "Appendix B: Simulation Experiments", "content": "Single-Task Pick and Place Tasks"}, {"title": "Simulation", "content": "Pick-and-place environment To further study the effectiveness of our work, we conduct in-depth experiments with other baselines focusing on pick and place tasks in simulation. In particular, we organize baseline methods as (1) in-domain augmentation methods and (2) learning from out-of-domain priors, as described below.\nIn-domain augmentation methods (1) \"No Augmenta-tion\" does not use any data augmentation techniques. (2) \"Spatial Augmentation\" randomly transforms the cropped object image features to learn rotation and translation equivariance, as introduced in TransporterNet (Zeng et al. 2020). (3)\"Random Copy Paste\" randomly queries objects and their segmented images from LVIS dataset (Gupta et al. 2019), and places them in the original scene. This includes adding distractors around the pick or place objects or replacing them. Further visualization of this approach can be found in Appendix C. (4)\"Random Background\" does not modify the pick or place objects but replaces the table and background with images randomly selected from LVIS dataset. (5)\"Random Distractors\" randomly selects segmented images from LVIS dataset as distractors."}, {"title": "Appendix C: More Real-World Experiments", "content": "To further show the effectiveness of GenAug, we compare our approach with CLIPort trained without GenAug, shown"}, {"title": "Appendix D: Computational Cost", "content": "For generating the augmented dataset, the average speed for augmenting one mask is about 4 seconds on a 2080Ti GPU, so per-image augmentation is about 10 seconds to"}, {"title": "Appendix E: Potential Applications", "content": "Our framework is versatile and general and can be potentially applied to other robotics domains such as locomotion, indoor navigation, and articulated object manipulation. Figure 29"}]}