{"title": "UNVEILING THE SECRET RECIPE: A GUIDE FOR SUPERVISED FINE-TUNING SMALL LLMS", "authors": ["Aldo Pareja", "Nikhil Shivakumar Nayak", "Hao Wang", "Krishnateja Killamsetty", "Shivchander Sudalairaj", "Wenlong Zhao", "Seungwook Han", "Abhishek Bhandwaldar", "Guangxuan Xu", "Kai Xu", "Ligong Han", "Luke Inglis", "Akash Srivastava"], "abstract": "The rise of large language models (LLMs) has created a significant disparity: industrial research labs with their computational resources, expert teams, and advanced infrastructures, can effectively fine-tune LLMs, while individual developers and small organizations face barriers due to limited resources to effectively explore the experiment space. In this paper, we aim to bridge this gap by presenting a comprehensive study on supervised fine-tuning of LLMs using instruction-tuning datasets spanning diverse knowledge domains and skills. We focus on small-sized LLMs (3B to 7B parameters) for their cost-efficiency and accessibility. We explore various training configurations and strategies across four open-source pre-trained models. We provide detailed documentation of these configurations, revealing findings that challenge several common training practices, including hyperparameter recommendations from TULU (Wang et al., 2023b) and phased training recommended by Orca (Mitra et al., 2023).\nKey insights from our work include: (i) larger batch sizes paired with lower learn-ing rates lead to improved model performance on benchmarks such as MMLU, MTBench, and Open LLM Leaderboard; (ii) early-stage training dynamics, such as lower gradient norms and higher loss values, are strong indicators of better final model performance, allowing for early termination of sub-optimal runs and significant computational savings; (iii) through a thorough exploration of hyper-parameters like warmup steps and learning rate schedules, we provide guidance for practitioners and find that certain simplifications do not compromise performance; and (iv) we observed no significant difference in performance between phased (sequentially training on data divided into phases) and stacked (training on the entire dataset at once) strategies, but stacked training is simpler and more sample efficient. With these findings holding robustly across datasets as well as model families and sizes, we hope this study serves as a guide for practitioners fine-tuning small LLMs and promotes a more inclusive research environment for LLM development.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) are growing in size, but bigger is not always better. Small-sized LLMS (3B to 7B parameters) become increasingly popular among developers with limited resources and are emerging as the backbone of enterprise AI systems due to their adaptability and efficiency"}, {"title": "RELATED WORK", "content": "Instruction Tuning Data. Instruction tuning with diverse, large-scale datasets can effectively improve LLM performance across downstream tasks (Wang et al., 2023c; Honovich et al., 2023; Chung et al., 2024; Isik et al., 2024; Cheng et al., 2024). Recent studies have found that large-scale instruction tuning data focusing on knowledge and skills is particularly beneficial for adapting LLMs to customized domains or applications, improving factual recall and reducing hallucinations (Cheng et al., 2023; Allen-Zhu & Li, 2023; Yang et al., 2024). This observation has led to a growing body of research introducing novel instruction tuning datasets. For instance, several works leveraged larger, more powerful LLMs (e.g., ChatGPT (OpenAI, 2023; 2022) and Mistral models (Jiang et al., 2023; 2024)) to distill instruction tuning data from them using seed examples provided by users (Mitra et al., 2024; Xu et al., 2023; Ding et al., 2023a; Peng et al., 2023; Mukherjee et al., 2023). GLAN (Li et al., 2024) and LAB (Sudalairaj et al., 2024) further advanced this area by proposing taxonomy-driven frameworks to enhance the diversity of synthetic instruction tuning data. Building on these datasets, many studies explored strategies to optimize dataset composition, select representative data subsets, and evaluate data quality before incorporating them into model training (Ivison et al., 2023; Liu et al., 2023; Li et al., 2023; Xie et al., 2023). While these advancements have driven rapid progress in instruction-tuned LLMs, limited work has focused on how to effectively use such data during training to achieve optimal performance, or how training outcomes vary with different compute budgets (e.g., GPUs and TPUs). In this paper, we fill this gap by conducting an extensive set of experiments to investigate various training strategies and hyperparameters for customizing small LLMs on these datasets, analyzing how different configurations interact with available compute resources to impact the downstream performance of fine-tuned models.\nTraining Dynamics. Training configurations and hyper-parameter setups play a pivotal role in training LLMs, as they directly influence model performance, convergence stability, and resource efficiency. Most research has focused on the pre-training phase, as it is the most resource-intensive part of LLM development (Yang et al., 2022; H\u00e4gele et al., 2024; Bi et al., 2024; Kaplan et al., 2020; Rosenfeld et al., 2019; Gunter et al., 2024; Dubey et al., 2024). For example, Sardana et al. (2024); Hoffmann et al. (2022) introduced scaling laws to determine optimal model sizes for given datasets and H\u00e4gele et al. (2024) proposed novel learning rate schedulers as alternatives to conventional cosine decay. Additionally, recent research proposed to incorporate instruction tuning data alongside pre-training data as part of a decay phase in pre-training, linking to the body of research on continual pre-training (Hu et al., 2024; Ibrahim et al., 2024; Lesort et al., 2021; Scialom et al., 2022). In contrast, our work shifts the focus to customizing pre-trained LLMs through instruction tuning, highlighting under-explored challenges in training strategies and hyper-parameter configurations for this stage. Many instruction tuning studies either omit the reporting of hyperparameters altogether (Mukherjee et al., 2023) or only provide a selective set of hyperparameters used in successful runs (Wang et al., 2023b; Ivison et al., 2023; Xu et al., 2023), often without disclosing failed experiments or alternative configurations explored during their research. In contrast, we conduct extensive experiments, exploring a range of hyper-parameters and training strategies. Our findings challenge several widely used practices, including TULU, and we hope that our work can serve as a valuable reference for practitioners and spark discussions on a deeper understanding of training dynamics for fine-tuning LLMs.\nTraditional Wisdom in Neural Networks Training. Identifying effective training configurations to improve model generalization has been an active area of research long before the rise of LLMs (Zhang et al., 2017; Srivastava et al., 2014; Ioffe & Szegedy, 2015). For example, Jiang et al."}, {"title": "EXPERIMENTAL SETUP", "content": "This section outlines the pre-trained LLMs, the datasets curated for fine-tuning these models, the training strategies used, and the hyper-parameters tested in our experiments. Details on the training infrastructure and optimization techniques used in our experiments can be found in Appendix A.3. We directly present the experiments and results in the following sections. For readers interested in the detailed experimental design and hypotheses, please refer to Appendix A.4."}, {"title": "BASE MODELS AND DATASETS", "content": "We conduct experiments using four open-source, small-sized LLMs: Granite 3B\u00b9, Granite 7B, Mistral 7B, and LLAMA 3B. The Granite models (Mishra et al., 2024), developed by IBM Research, are decoder-only architectures designed for enterprise applications, with the \"3B\" and \"7B\" designations indicating their parameter counts of 3 billion and 7 billion, respectively. The Mistral 7B model (Jiang et al., 2023), created by Mistral AI, is a dense, decoder-only transformer model with 7 billion parameters, optimized for high performance relative to its size. While our primary focus is on the Granite and Mistral models, given their permissive Apache-2.0 licensing, we include the LLaMA model (Touvron et al., 2023) in specific experiments to test the generalizability of our findings. These experiments further validating the robustness of our conclusions across architectures and model sizes within the small-sized LLM category.\nAs detailed in our previous work on LAB Sudalairaj et al. (2024), we curated a comprehensive dataset designed to progressively enhance the base models' capabilities in instruction following (phase 00), foundational knowledge (phase 05), and complex skills (phase 10). This dataset is organized into three phases, each targeting specific aspects of language understanding and generation (see Appendix A.2 for details). We also explored an alternative dataset partitioning based on task difficulty, where phases are defined by sentence length as a proxy for difficulty (further detailed in Appendix A.5.1). We also conducted experiments with the TULU dataset (Wang et al., 2023b; Ivison et al., 2023), a diverse mix of complex, instruction-tuning data from human and GPT-4 sources; details are provided in the main results section. Finally, we test our findings on a synthetically generated Math, Reasoning, and Code dataset, similar to our other datasets, with a focus on tasks in these domains to ensure they hold for domain-specific datasets."}, {"title": "TRAINING STRATEGIES", "content": "We explore two training strategies\u2014sequential phased training and stacked training. Phased training follows the approach adopted by recent instruction tuning research (Sudalairaj et al., 2024; Mitra et al., 2023; Pang et al., 2024), where the base model is fine-tuned on different data types in a predetermined sequence. This strategy aims to mitigate catastrophic forgetting and allows the model to build progressively on knowledge and skills acquired in earlier stages. In our experiments, models are fine-tuned in multiple phases, each focusing on a specific type of data (see Appendix A.2 for details on the datasets used in each phase). At the end of each phase, the best-performing checkpoint is"}, {"title": "HYPERPARAMETERS", "content": "Our experiments explore various hyperparameter configurations to analyze their impact on training dynamics and model performance.\n\u2022 Batch Size. We investigate effective batch sizes of 128 (small), 3,840 (medium), and 7,680 (large) samples. The effective batch size is achieved through a combination of micro-batch sizes and gradient accumulation steps. For instance, on 64 GPUs, we can process a batch of 3,840 samples in a single micro-batch, whereas on 1 GPU or 8 GPUs, we use gradient accumulation to approximate the same batch size. We confirm that gradient accumulation on a single node produces equivalent results to multi-node distributed training, with details in Appendix A.5.10.\n\u2022 Learning Rate and Warmup Steps. We experiment with various goal learning rates: 1 \u00d7 10-6, 5 \u00d7 10-6, 2 \u00d7 10\u22125, 3 \u00d7 10\u22125, 4 \u00d7 10\u22125, 6 \u00d7 10\u22125, 8 \u00d7 10\u22125, and 1 \u00d7 10\u22124. Warmup steps are varied among 0, 25, and 100, corresponding to different numbers of samples processed before reaching the goal learning rate. The learning rate schedule typically involve a linear warmup to the goal learning rate, followed by either a constant learning rate or the cosine decay schedule.\n\u2022 Training Configurations. We consider three main hyperparameters configurations: LAB (Sudalairaj et al., 2024), TULU (Wang et al., 2023b; Ivison et al., 2023), and a new configuration introduced in this paper, TULU++. Details of these configurations are provided in Table 1.\nWe used the LAB hyperparameter configuration for all experiments where we varied a single factor (e.g., batch size, learning rate, learning rate schedule, training strategy) while keeping all other settings constant to isolate its effect. For comparisons between TULU and LAB, we directly used the respective configurations. LAB and TULU were chosen as primary configurations due to their prominence: TULU is widely regarded as a gold standard for fine-tuning LLMs with high-quality instruction datasets, while LAB introduces a multi-phase tuning framework leveraging knowledge and skills data to reduce reliance on human annotations. This dual comparison allowed us to systematically evaluate established practices and propose actionable guidelines."}, {"title": "EVALUATION METRICS", "content": "Benchmarks. To assess the models' performance and ability to generalize, we use two primary benchmarks: MMLU (Hendrycks et al., 2020) and MTBench (Zheng et al., 2023). MMLU assesses the models' knowledge and reasoning across a wide range of subjects. It includes questions from 57 subjects spanning STEM, humanities, social sciences, and more, testing the model's ability to recall factual knowledge and apply reasoning skills to answer multiple-choice questions. MTBench evaluates multi-turn conversational abilities and generalization to unseen tasks. It measures the quality of responses in dialogue settings, focusing on coherence, relevance, informativeness, and adherence to instructions. The benchmark covers diverse tasks such as reasoning, coding, mathematics, and other skill-based domains. Additionally, we evaluated our models on MMLU-Pro, GPQA, MuSR, MATH, IFEval, and BBH from the Open LLM Leaderboard v22. For the comparison with the TULU dataset, we used the same benchmarks as in the TULU paper (Wang et al., 2023b; Ivison"}, {"title": "MAIN RESULTS", "content": "In this section, we present the empirical findings of our experiments, focusing on the impact of different training strategies, batch sizes, and hyperparameter configurations on the fine-tuning performance of LLMs. We present results using the Granite 7B model and provide additional experiments to other model sizes and architectures (Granite 3B, LLaMA 3B, and Mistral 7B models) in Appendix A.5.8 to validate the robustness and generalizability of our findings. Additionally, we conducted experiments on a domain-specific Math, Reasoning, and Code (MRC) dataset to evaluate the applicability of our findings to specialized fine-tuning scenarios, with further details and results provided in Appendix A.5.7. We include baseline scores for the Granite and LLaMA base pretrained models in applicable tables to facilitate easier interpretation of fine-tuned performance. MTBench scores are not provided for baseline models, as these benchmarks evaluate instruction-following and conversational capabilities not present in base models."}, {"title": "STACKED TRAINING VS. SEQUENTIAL PHASED TRAINING", "content": "We conducted a comprehensive comparison between stacked training and sequential phased training to evaluate their effectiveness in fine-tuning small sized LLMs. The analysis was performed using the Granite 7B model and evaluated on the MMLU, MTBench, ARC, GSM8K, and Leader-board (BBH, MATH, MuSR) benchmarks. We observed that stacked training slightly outperformed sequential phased training and is more sample efficient across all batch sizes \u2013 128 and 3,840. The detailed comparison of performance across batch sizes is presented in Appendix A.5.1, along with corresponding figures."}, {"title": "IMPACT OF BATCH SIZE", "content": "We investigated the effect of batch size on model performance by experimenting with effective batch sizes of 128, 3,840, and 7,680 samples. The experiments were conducted using the Granite 7B model and evaluated on the MMLU and MTBench benchmarks. To ensure a fair comparison, we ran each experiment for approximately the same number of gradient steps.\nObservations. Larger batch sizes lead to better final performance but may require more computational resources and training samples. For stacked training, larger batch sizes uniformly resulted in improved performance on both MMLU and MTBench. The performance gains observed with larger batch sizes can be attributed to reduced statistical error in gradient estimation during optimization. Since each gradient step is computed by averaging over more training examples, the variance of the estimate decreases proportionally to $\\frac{1}{n}$ (where n is the batch size). This reduction in estimation noise enables more consistent parameter updates that better align with the true gradient of the loss function, potentially leading to more efficient optimization trajectories. This reduced noise in the gradients likely enables the optimization process to progress toward a minimum with fewer fluctuations, allowing the model to settle near the pre-trained parameters (Hoffer et al., 2017). We hypothesize that this effect minimizes deviation from the pre-trained state, as also observed by Keskar et al. (2016), helping the model adapt to new data without significant departure from its initial configuration, thereby reducing forgetting. For phased training, the batch size of 3,840 samples outperformed the smaller batch size of 128 samples. While larger batch sizes still yield better overall performance, the impact is less pronounced compared to stacked training. This could be because, in phased training, each phase focuses on a specific type of data, resulting in batches where the data is more homogeneous; therefore, the benefits of reduced gradient noise from larger batch sizes are less significant, and the impact of larger batch sizes is less pronounced compared to stacked training."}, {"title": "EFFECT OF LEARNING RATE SCHEDULES ON LARGE BATCH SIZES", "content": "We explored whether using a cosine decay learning rate schedule improves model performance when training with large batch sizes. Cosine decay is often thought to facilitate convergence by allowing higher initial learning rates and gradually reducing them. It can be particularly beneficial when training with large batches that may require larger steps to make meaningful progress. We conducted experiments using the Granite 7B model with an effective batch size of 3,840 samples. We compared two learning rate schedules: a constant learning rate and a cosine decay schedule. The learning rate tested was 2 \u00d7 10-5.\nObservations. As shown in Table 4, the models trained with a constant learning rate (no decay) performed on par with those trained with a cosine decay schedule on MMLU and even outperformed them on MTBench. Detailed plots are provided in Appendix A.5.3.\nAnalysis. Our findings suggest that, contrary to common practice, cosine decay may not improve model performance when fine-tuning small-size LLMs with large batch sizes. Instead, a constant learning rate ensures consistent progress throughout training, under the assumption that the initial rate is suitable for stable training. For practitioners, this implies that using a constant learning rate could simplify the training process without compromising performance, and may even offer slight improvements."}, {"title": "TULU vs. LAB", "content": "We compared the TULU and LAB hyperparameter configurations to assess their effectiveness in enhancing the model's memorization and generalization capabilities. Memorization was evaluated using a subset of the MMLU benchmark focused on factual knowledge domains, while generalization was assessed using the MTBench benchmark, which tests the model's ability to perform diverse and complex tasks requiring various skills. Detailed plots and scores over all checkpoints during training are provided in Appendix A.5.4, along with performance results for the Leaderboard (BBH, MATH Lvl 5, MuSR), ARC, and GSM8K benchmarks. Tables 8 and 9 show that LAB outperforms TULU across all benchmarks.\nCross-Dataset Evaluation with the TULU Dataset. To further investigate the impact of batch size on fine-tuning performance, we conducted an experiment using the TULU dataset (Wang et al., 2023b; Ivison et al., 2023). This dataset is a refined mixture of instruction-tuning data, integrating both human and GPT-4-generated instructions that are complex and cover various domains. We"}, {"title": "EFFECT OF LEARNING RATE", "content": "We examined how different learning rates impact the model's downstream performance, using Granite 7B as a base model. We used the LAB hyperparameter configuration since it outperformed TULU. We conducted a learning rate sweep from 2 \u00d7 10\u22125 to 1 \u00d7 10\u22124. All other hyperparameters were kept constant to isolate the effect of the learning rate. We evaluated on MMLU, MTBench, Leaderboard (BBH, MuSR), ARC, and GSM8K benchmarks after the final phase of phased training. As shown in Table 6, the lowest learning rate of 2 \u00d7 10-5 yielded the best performance on most benchmarks and comparable performance on the rest. As the learning rate increased, there was a consistent decline in benchmark performance. This trend suggests that lower learning rates enhance the model's ability to generalize to unseen tasks requiring knowledge, complex reasoning, and instruction following."}, {"title": "EFFECT OF WARMUP STEPS", "content": "We investigated the impact of the number of warmup steps on the training process and final model performance. The warmup phase is traditionally considered crucial for stabilizing training, especially when using higher learning rates, by gradually increasing the learning rate from a small value to its target value over a specified number of steps (Goyal et al., 2017). We ran experiments with the Granite 7B model in the stacked setting using LAB hyperparameters\u2014our best configuration\u2014across three warmup setups: 0, 25, and 100 warmup steps, corresponding to approximately 0, 96,000, and 384,000 samples processed before reaching the target learning rate, respectively. As shown in Appendix A.5.6, the model trained without warmup steps achieved better performance on the MMLU benchmark and similar performance on MTBench compared to models trained with 25 or 100 warmup steps. The training curves for all configurations followed a similar trajectory, converging to comparable performance levels within approximately the same number of training steps, indicating that omitting warmup steps does not negatively affect the final model performance."}, {"title": "EARLY TRAINING DYNAMICS AS PREDICTORS OF FINAL PERFORMANCE", "content": "We consistently observed that models exhibiting lower gradient norms and higher training loss values during training achieved better final performance on MMLU and MTBench. Figures 1 and 2 illustrate the correlation between early training dynamics\u2014gradient norms and loss values\u2014and final benchmark performances.\nTULU vs. LAB Phase 10 Training (Figure 1). The LAB configuration achieved better final performance with lower gradient norms compared to the TULU configuration.\nLAB Learning Rate Sweep Experiments. Models trained with a learning rate of 2 \u00d7 10-5 demonstrated lower gradient norms initially, which increased toward the end of training, and higher loss throughout, ultimately resulting in superior final performance compared to models trained with higher learning rates. For the most effective learning rates, the gradient norm started at its lowest value and increased towards the end of training (Figure 2). Despite the higher gradient norms in the later stages, the associated loss remained higher throughout the entire training for these rates. This is consistent with the use of lower learning rates, which typically result in higher training loss but better generalization. Figure 2 shows the gradient norms and loss values for different learning rates, along with the final performance on MTBench. The lowest learning rates delivered superior results. Smaller learning rates may enable the model to stabilize the learning process initially and then gradually explore more challenging regions of the loss landscape as training progresses, leading to better generalization and final performance. We hypothesize that lower gradient norm values at the start of training contribute to a smoother and more stable optimization process, preventing the model from overfitting too quickly. This allows for gradual learning, which we hypothesize facilitates better exploration of the loss landscape as training progresses. The subsequent increase in gradient norm during later training stages may indicate that the model is delving into more complex regions of the parameter space, enhancing its ability to generalize. MMLU results are provided in Appendix A.5.9.\nThe correlation between early training dynamics and final performance holds across different batch sizes, warmup steps, and learning rate schedules (see Appendix A.5.9 for additional results)."}, {"title": "DISCUSSION, GUIDELINES FOR PRACTITIONERS, AND LIMITATIONS", "content": "Balancing Performance and Efficiency. Our results show a trade-off between performance and computational cost. Configurations such as higher batch sizes or lower learning rates achieve better final performance but take longer to converge. In contrast, hyperparameters yielding lower final performance often dominate early on before plateauing. For those with limited resources, smaller batch sizes or higher learning rates may be more efficient. For example, in stacked training, a 4k batch size outperforms 8k initially, and higher learning rates offer faster learning in early stages. Moreover, we encourage practitioners to monitor early training dynamics, such as gradient norms and loss values, as they correlate strongly with final model performance. Observing lower gradient norms and higher loss values during the initial phases of training can serve as reliable indicators of better generalization capabilities. This allows for early termination of suboptimal runs, conserving computational resources.\nTraining Strategy Recommendations. Based on our empirical evidence, we advocate for stacked over sequential phased training. This recommendation is supported by consistent performance gains and improved sample efficiency observed in Granite 3B, Granite 7B, and LLaMA 3B models. Stacked training simplifies the fine-tuning process and eliminates the need for phase-wise data management.\nHyperparameter Selection. We offer guidance on selecting batch sizes, learning rates, warmup steps, and learning rate schedules. Larger batch sizes (e.g., 4k and 8k) are recommended, as they have demonstrated superior performance across model sizes compared to smaller batch sizes like 128. Low learning rates are crucial for optimal performance. We found that 2 \u00d7 10-5 works well for Granite models, while 1 \u00d7 10-6 performs best for Mistral. Lower learning rates allow for more precise adjustments to the model weights, preventing overshooting in the optimization landscape. Practitioners should start with these values and, if necessary, perform a localized search by testing slightly higher or lower learning rates to find the optimal setting for their specific model. This approach significantly reduces the search space. Our experiments indicate that omitting warmup steps and using a constant learning rate instead of cosine decay does not negatively impact performance, simplifying the training process without sacrificing model quality.\nLimitations.\nOur experiments focused on small (3B to 7B parameters) LLMs and were conducted on two model architectures: Granite (based on the Llama architecture) and Mistral. While our findings are promising, they may not directly generalize to larger models or other architectures. Future work should explore whether these observations hold for larger models and across a broader range of architectures, such as Gemma or others. Additionally, we did not investigate parameter-efficient fine-tuning strategies, such as LoRA, or explore how different pre-training objectives, tokenizer configurations, or optimizers (as we focused solely on Adam) might affect the applicability of our fine-tuning strategies. Furthermore, our evaluation centered on synthetic datasets generated from a comprehensive taxonomy covering various knowledge and skills, using benchmarks such as MMLU, MTBench, and LLM Leaderboard v2. We also explored the TULU dataset to understand fine-tuning across diverse datasets. However, confirming these findings across additional datasets and evaluation metrics would further strengthen the generalizability of our conclusions. Finally, we acknowledge that our experiments were conducted using a single seed due to computational constraints, which may introduce some noise into the observations."}, {"title": "APPENDIX", "content": null}, {"title": "MODEL DETAILS", "content": "Granite 3B is composed of transformer layers (encoder blocks) that include multi-head self-attention mechanisms and feed-forward networks. It has a smaller hidden size and fewer attention heads, making it less computationally intensive and faster for both training and inference.\nGranite 7B has more transformer layers and increased hidden dimensions, offers greater representational capacity. It also includes more attention heads, enabling it to capture more complex language patterns and long-range dependencies.\nLlama 3.2 3B employs a scaled-down transformer architecture with fewer layers and a reduced hidden size compared to larger models in the Llama family. It maintains the core design principles of its larger counterparts, including rotary positional embeddings and optimized attention mechanisms, while balancing performance and efficiency for resource-constrained environments.\nMistral 7B utilizes advanced attention mechanisms, including multi-query attention and Sliding Window Attention, which enhance efficiency and reduce memory usage during inference. With 32 layers and 32 attention heads, it is designed for improved performance on benchmarks, particularly in logical reasoning and commonsense tasks, while maintaining competitive resource demands."}, {"title": "DATASETS DETAILS", "content": "The datasets were curated using a taxonomy-driven approach to ensure comprehensive coverage of instruction-following, foundational knowledge, and compositional skills. The taxonomy hierarchically organizes tasks into three main branches\u2014knowledge, foundational skills, and compositional skills\u2014each further divided into granular subcategories. For each subcategory, manually written instruction-response pairs served as seed examples. These examples guided synthetic data generation using teacher models (e.g., Mixtral-7x8B) to expand the dataset while maintaining high quality and diversity. For knowledge data, reliable sources such as textbooks and technical manuals provided a grounding for synthetic questions and responses. Foundational skills data were drawn from public datasets covering essential areas like mathematics, coding, and reasoning. Compositional skills were synthesized using a taxonomy-guided approach to combine knowledge and foundational skills for complex tasks, such as writing detailed emails or generating logical arguments. We provide details about the datasets we used in Table 7."}, {"title": "TRAINING INFRASTRUCTURE AND OPTIMIZATION", "content": "To handle large batch sizes and optimize computational efficiency, we use an optimized training infrastructure."}, {"title": "EXPERIMENTAL DESIGN", "content": "We investigate how training strategies, batch sizes, learning rates, and warmup steps influence LLM fine-tuning. We systematically vary these factors while holding other parameters constant to isolate their individual effects.\nImpact of Batch Size and Training Strategies. We examine how different batch sizes influence model performance and training dynamics in both stacked and phased training settings. Our hypothesis is that larger batch sizes will improve model performance in stacked training by ensuring sufficient data diversity within each batch, allowing for more robust gradient updates. In contrast, their impact on phased training may be less pronounced. On the other hand, this improvement may come at the cost of reduced sample efficiency. While larger batch sizes may achieve better final performance, they typically require more training samples and computational resources due to the higher number of samples used per gradient step. Conversely, smaller batch sizes could achieve comparable performance with fewer samples, especially in phased training, where each batch is inherently constrained to a specific data type, limiting intra-batch diversity. We formalize these hypotheses below and explore the trade-offs between performance gains and computational efficiency in our experiments.\n\u2022 Hypothesis 1. Stacked training may underperform at smaller batch sizes due to insufficient diversity within each batch. A smaller batch may not capture the wide range of data types present in the combined dataset, leading to less effective learning. In contrast, larger batch sizes in stacked training could match or surpass phased training by capturing a wider range of signals in each gradient update.\n\u2022 Hypothesis 2. While the stacked approach simplifies the training pipeline by eliminating the need for phase selection of data, it can be less sample efficient. Learning all types of data simultaneously could require more steps for the model to adequately learn the complex and diverse patterns in the combined dataset. This translates to worse sample efficiency, as the model may need more gradient updates to converge."}, {"title": "GRADIENT ACCUMULATION EQUIVALENCE TO FULL BATCH TRAINING", "content": "We investigated whether using gradient accumulation on a single node with a large batch size is equivalent to distributed training across multiple nodes with the same effective batch size. Theoretically, both methods should yield identical training dynamics and result in the same fine-tuned model if implemented correctly.\nIn our experiments, we compared two setups:\n\u2022 Single Node with Gradient Accumulation. We utilized a single node with gradient accumulation to achieve an effective batch size corresponding to 60,000 tokens.\n\u2022 Multi-Node Distributed Training. We employed distributed training across four nodes, maintaining the same effective batch size of 60,000 tokens without gradient accumulation.\nWe evaluated both setups by comparing their training loss curves, as well as performance on the MMLU and MTBench benchmarks. The results showed that the loss trajectories were virtually identical between the two methods. Additionally, the final performances on MMLU and MTBench were the same within experimental variance. These findings confirm that gradient accumulation on a single node can replicate the training dynamics and outcomes of full-batch distributed training across multiple nodes. This equivalence provides flexibility for practitioners with limited computational resources, allowing them to achieve the same model quality using gradient accumulation on fewer GPUs."}, {"title": "ADDITIONAL RESULTS", "content": null}, {"title": "STACKED TRAINING VS. SEQUENTIAL PHASED TRAINING", "content": "Contrary to our initial hypothesis that stacked training might underperform at smaller batch sizes due to insufficient gradient stability, our results show that stacked training achieves better or comparable performance to phased training consistently at both 128 and 4,000 batch sizes. The performance comparison across MTBench and MMLU benchmarks indicates that stacked training slightly out-performs phased training at each batch size, suggesting that batch size does not significantly impact the difference between the two training strategies. Instead, stacked training's exposure to the entire dataset in each epoch, even at smaller batch sizes, may help maintain stability in learning, effectively supporting generalization across diverse types of data without requiring phased partitioning.\nIn Figure 3, we compare the performance of both training strategies using the LAB hyperparameter configuration, which provided the best overall results for both approaches. Figure 3a shows the final MTBench performance, where stacked training outperformed phased training by 0.01 points. Figure 3b illustrates that stacked training is also more sample-efficient, with the best performance points annotated by the number of samples required to reach them. Note that the line for phased training begins partway through, as samples from Phases 00 and 05 were already included. This applies consistently to all similar figures presented in this paper."}, {"title": "IMPACT OF BATCH SIZE", "content": "Figure 8 shows the performance of different batch sizes in stacked and phased training on the MT-Bench benchmark. Similarly, Figure 7 highlights the impact of batch size on model performance in both stacked and phased training on the MMLU benchmark. These results consistently demonstrate that larger batch sizes lead to better final performance. While smaller batch sizes initially reach higher performance levels more quickly, they plateau earlier, allowing larger batch sizes to surpass them with extended training."}, {"title": "EFFECT OF LEARNING RATE SCHEDULES ON LARGE BATCH SIZES", "content": "As shown in Figure 9, the models trained with a constant learning rate (no decay) performed on par with those trained with a cosine decay schedule on both MMLU and MTBench, and in some cases even outperformed them, particularly on MTBench."}, {"title": "TULU vs. LAB", "content": "Memorization and Generalization. We focused on Phase 05 in the sequential phased training strategy, which is designed to augment the model's foundational knowledge and memorization of facts. We evaluated the models using specific MMLU subjects related to memorization, including history, law, and science domains. We compared the performance of models starting from both the base Granite model and the best checkpoint obtained from Phase 00 training. Table 8 shows that models trained with LAB hyperparameters outperform those trained with TULU hyperparameters on the memorization-focused MMLU tasks. We evaluated the models' generalization abilities after Phase 10 in the sequential phased training strategy, which focuses on complex skills and compositional tasks. The MTBench benchmark was used to assess performance on tasks requiring reasoning, problem-solving, and adaptation to unseen scenarios. As shown in Table 8, the model trained with LAB hyperparameters significantly outperforms the one trained with TULU hyperparameters on MTBench."}, {"title": "EFFECT OF LEARNING RATE", "content": "As shown in Figure 11, the lowest learning rate of 2 \u00d7 10-5 yielded the best performance on the MTBench Benchmark.\nWe investigated whether larger batch sizes necessitate higher learning rates, based on the premise"}]}