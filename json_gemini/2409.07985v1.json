{"title": "Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols", "authors": ["Charlie Griffin", "Louis Thomson", "Buck Shlegeris", "Alessandro Abate"], "abstract": "To evaluate the safety and usefulness of deployment protocols for untrusted AIs, AI Control uses a red-teaming exercise played between a protocol designer and an adversary. This paper introduces AI-Control Games, a formal decision-making model of the red-teaming exercise as a multi-objective, partially observable, stochastic game. We also introduce methods for finding optimal protocols in AI-Control Games, by reducing them to a set of zero-sum partially observable stochastic games. We apply our formalism to model, evaluate and synthesise protocols for deploying untrusted language models as programming assistants, focusing on Trusted Monitoring protocols, which use weaker language models and limited human assistance. Finally, we demonstrate the utility of our formalism by showcasing improvements over empirical studies in existing settings, evaluating protocols in new settings, and analysing how modelling assumptions affect the safety and usefulness of protocols.", "sections": [{"title": "1 Introduction", "content": "An increasing range of products aim to automate tasks using Al systems based on large foundation models. For example, some chatbots are given access to tools, such as code interpreters (OpenAI 2023; Adept AI 2024). Ensuring that AI systems behave safely across deployment settings will become increasingly challenging if the variety of use cases, scale of systems, and complexity of tasks increase. While some previous research has focused on increasing our confidence in model behaviour, for example by training robustly harmless models (Casper et al. 2024) or by detecting harmful behaviour with interpretability tools (MacDiarmid et al. 2024), in this paper we consider a complementary approach: evaluating plans for safely deploying 'untrusted' AI systems about which we have an unacceptable degree of uncertainty.\nFollowing Greenblatt et al. (2024), we study AI-Control protocols: practical plans for safely deploying an untrusted AI system (U) utilising various resources including non-AI software, human labour, or a weaker, trusted AI system. For example, one protocol for controlling a chatbot might involve a pre-deployment evaluation where humans utilise other language models to generate prompts that induce harmful completions (Shah et al. 2023). This paper focuses on another case study, Trusted Monitoring: constantly monitoring U's behaviour using a trusted model (T), with human auditing of suspicious behaviour.\nGreenblatt et al. (2024) operationalise safety as the probability that an adversarially chosen untrusted AI system would be incapable of causing some concrete unacceptable outcome. One example of an unacceptable outcome would be revealing some sensitive information, such as model weights (Clymer et al. 2024). We evaluate the safety of protocols under the worst-case assumption that U is 'scheming', meaning it behaves as though it knows the protocol and is trying to subvert it to cause the unacceptable outcome.\nA major challenge to evaluating the safety of a protocol with respect to a scheming AI is realistically replicating the potential capabilities of such a system: as a proxy for a schemer, AI Control uses a human red-teaming process to evaluate safety. The red team can intervene in U in some predefined set of ways to induce it to subvert the AI-Control protocol and violate the safety property. For example, the red team might intervene in an untrusted language model by finetuning weights or modifying prompts.\nSince one safe protocol is 'never use U', we must also evaluate a protocol's usefulness, which is its ability to utilise U to complete a task under the assumption that U is 'honest' (not scheming). Our proxy evaluations model an 'honest' untrusted model by not allowing the red team to intervene.\nWe begin in section 2 by introducing a running example: using Trusted Monitoring protocols to control a programming assistant. We then move onto our contributions:\n\u2022 Section 3 introduces AI-Control Games, the first formal model of AI Control: a partially observable, multi-objective, leader-follower, stochastic game.\n\u2022 Section 4 demonstrates how our formalism can model Trusted Monitoring, and leverage it to give expressions for the safety and usefulness of a wide range of protocols.\n\u2022 Section 5 presents algorithms for finding Pareto-optimal protocols by reducing to single-objective zero-sum games.\n\u2022 Section 6 we use our methods for evaluating and synthesising protocols to demonstrate Pareto-improvements on existing protocols and analyse the safety-usefulness trade-off in novel settings.\nFinally, we conclude with a comparison to existing literature in section 7, and a discussion of potential future work in section 8. All of our code is available online."}, {"title": "2 Example: Setting and Protocol", "content": "Throughout this paper, we will introduce AI-Control Games using the example deployment setting from Greenblatt et al. (2024), and a generalisation of some of their protocols.\nDeployment setting. In the APPS backdooring setting a language model is used as a programming assistant (Greenblatt et al. 2024). The protocol is asked to solve 1000 challenging programming tasks, presented sequentially from the Automated Programming Progress Standard (APPS) dataset (Hendrycks et al. 2021). The protocol cannot itself write code, but may utilise three resources to do so: the untrusted AI (U) is a powerful language model capable of solving programming problems; the trusted AI (T) is a less powerful language model incapable of solving them\u00b2; and the human (H) is trusted and capable but can only be used on 2% of problems. In this setting, a protocol's usefulness is its ability to submit correct programming solutions. Since it is assumed that only U can solve the problems, usefulness is the proportion of solutions submitted that were generated by U.2 The unacceptable outcome (that the red team is trying to cause, but the blue team is trying to avoid) is that any one of the 1000 solutions submitted is backdoored, meaning it contains an exploitable vulnerability.3\nTrusted Monitoring Protocol. As an example, we focus on a set of Trusted Monitoring protocols and red team interventions, drawn in fig. 1a. After receiving a problem, the"}, {"title": "3 Formalising AI-Control Games", "content": "To analyse the properties of AI-Control evaluations, we formalise them mathematically. An AI-Control Game is a form of two-player game that is played between Blue (the protocol designer) and Red (the red team acting as a proxy for a scheming model). Episodes (play-throughs) of an AI-Control Game have two modes: \u2018scheming' or 'honest'. The 'scheming' mode represents the case in an AI-Control evaluation where the red team is able to interfere with U (which itself is a worst-case proxy for if the untrusted model were trying to subvert the protocol). In a scheming game, both players are active: Red is trying to maximise the probability of an unacceptable outcome (minimise safety), whereas Blue is trying to minimise this probability (maximise safety). The honest game represents the benign case, where we assume the untrusted AI system is not trying to cause unacceptable outcomes. In an honest game, Blue effectively plays alone and is only assessed on its usefulness score. Because Blue tries to maximise safety and usefulness, the game is multi-objective. A key to the game is that Blue does not know which mode it is in (honest or scheming) and for at least this reason the game is partially observable. Blue may try to deduce whether the game is in scheming mode, while Red might choose actions to conceal this fact.\nRed represents the red team that acts as a proxy for an untrusted AI system that runs on servers controlled by the protocol. Crucially, the protocol will have access to the state of the untrusted AI system and, therefore, we model Red as memoryless (its actions depend only on the most recent observation).5 We model the game as leader-follower: the protocol is suggested, and then the red team chooses the best response. Combining all of these requirements, we model an AI-Control Game as a tuple:\nDefinition 3.1 (AI-Control Game). An AI-Control Game G is a two-player (Blue and Red), two-objective, observable, finite-horizon game defined by a tuple:\n$(S, S_1, A_B, A_R, \\Omega_B, \\Omega_R, I, P, R_{use}, H)$\nHere S is a state space. $S_1 \\subset S$ is a set of terminal states representing 'unacceptable outcomes'. As is standard in decision processes, $A_B$ and $A_R$ are finite sets of actions for Blue and Red, respectively. $A_R$ must contain a privileged action, $a_{null}$, which represents Red not interfering in the game. The tokens $\\oplus$ and $\\ominus$ represent the honest and scheming conditions. In honest ($\\oplus$) games, Red's action is replaced by $a_{null}$. Whether a game is honest or scheming can be chosen non-deterministically and is stored in the variable $\\Theta$. The usefulness reward $R_{use} : S \\times A_B \\times S \\rightarrow \\mathbb{R}$ rewards usefulness on a given honest turn. $\\Omega_B, \\Omega_R$ are finite sets of observations for Blue and Red: the signals that the agents receive from the environment that give evidence about the state of the game. The initial state/observation distribution is $I \\in \\Delta(S \\times \\Omega_B \\times \\Omega_R)$ and the transition function $P : S \\times A_B \\times A_R \\rightarrow \\Delta (S \\times \\Omega_B \\times \\Omega_R)$ gives a distribution over potential next states and observations.78 Finally, all episodes are H time-steps long.\nProposition 3.2. For any AI-Control Game G, there exist distance functions in $\\Pi_B$ and $\\Pi_R$ such that: $Safety(\\pi_B, \\cdot), Safety(\\cdot, \\pi_R)$, and $Use(\\cdot)$ are continuous in their domains. 10\nProposition 3.3. For all $\\pi_B$, $Safety(\\pi_B, \\cdot)$ attains its minimum at some $\\pi_R$, and $\\min_{\\pi_R} Safety(\\cdot, \\pi_R)$ is continuous.\nOur goal is to find protocols that maximise worst-case safety and usefulness. This can be modelled as a multi-objective variant of a Stackelberg equilibrium:\nDefinition 3.4. The pair $(\\pi_B, \\pi_R)$ is in an AI-Control Equilibrium iff $[\\min_{\\pi_R} Safety (\\pi_B, \\pi_R), Use(\\pi_B)]^T$ is weakly Pareto optimal and $\\pi_R$ minimises safety for $\\pi_B$."}, {"title": "4 Example: Modelling the protocol", "content": "With our general framework from section 3, we can define a formal model of the Trusted Monitoring protocol for the APPS backdoor setting, as outlined in Greenblatt et al. (2024). For the full definition please see appendix B; next we present the key points.\nWe define a Trusted Monitoring AI-Control Game tuple $G_{TM} = (S,..., H)$ step-by-step. As always, whether the game is scheming ($\\ominus$) or honest ($\\oplus$) is determined exogenously and non-deterministically. If the game is $\\ominus$, for any given problem, Red can choose either for the untrusted model to attempt a backdoor or to let it submit a safe solution ($A_R := \\{(Backdoor), (Safe)\\})$. If the game is $\\oplus$, Red's null action, to not backdoor, is chosen ($a_{null} = (Safe)$). Given a solution by the untrusted AI, Blue has three available actions: to ask the human to audit the solution, to defer to the trusted AI to rewrite the solution, or to run the solution immediately. Therefore define $A_B := \\{(Audit), (Defer), (Run)\\}$. Apart from initial and terminal states, the state space, S, consists of triples $s = (m,c,b)$, where m represents the number of problems left to solve. The value of m is M (the total number of problems) at the start of the game, and decreases by 1 at each timestep. The variable c represents the number of 'audit coins' left: the number of times the protocol can ask the human to audit a solution. At the start, $c = C = 0.02 * M$ (for a 2% audit proportion) and then c decreases by 1 each time the protocol chooses to audit. If the protocol attempts to audit when no audit coins are left, the code is run instead. The last variable, b, is a boolean that is True when the current problem is backdoored (when Red has chosen (Backdoor) in the previous timestep).\nIn honest games ($\\oplus$), where only usefulness matters to Blue, Red's action is always substituted for $a_{null} = (Safe)$. Therefore, b is always False. In honest games, Blue scores 1/M usefulness points for each solution by the untrusted model it submits - giving a maximum score of 1 across the whole episode.\nIn scheming games ($\\ominus$), where only safety matters to Blue, Red may have chosen (Backdoor) last turn, and so b might be True. If so, and Blue chooses to run the code, the safety property is violated, and the game ends with an unacceptable outcome. If Blue chooses to audit, we assume the human will always spot the backdoor, and the game ends safely (we enter a special 'safe' self-looping state). If Blue chooses to defer, then the game continues. The safety score for a policy pair is the probability that a scheming game ends without violating the safety property.\nThroughout the games, Red receives only the null observation ($O_{null}$),11 whereas at each turn Blue receives observa-"}, {"title": "4.1 Evaluating Memoryless Protocols", "content": "We generally model protocols (Blue policies) as functions from the observation-action history to a distribution over actions. However, all of the Trusted Monitoring policies in Greenblatt et al. (2024) are memoryless. We can restrict to a simple and interpretable subset of policies that generalises those by Greenblatt et al. (2024). Remember that, in our model, Blue's observation is (m, c, x), where x is a noisy observation of b, sampled from the Trusted Monitor. Define a set of memoryless threshold protocols, characterised by the false positive rates (FPRs) for (Audit) and (Defer) ($q_{m,c}$ and $q_{m,c}$), for each m, c. Let g(q) be the threshold that gives an FPR of q: $P [x_t > g(x) | b_t = False] = q$.12 Then, define the policy:\n$\\pi_B(m, c, x) = \\begin{cases} (Audit) & \\text{If } x \\geq g(q_{m,c}) \\\\ (Defer) & \\text{If } g(q_{m,c}) > x \\geq g(q_{m,c}) \\\\ (Run) & \\text{otherwise} \\end{cases}$ (3)\nThis class of policies generalises the policies in Greenblatt et al. (2024), and is a dominant subset of memoryless policies. These policies' safety and usefulness values can be computed in O(MC) multiplications by calculating the values of the following relation using dynamic programming.\nProposition 4.1. For any $\\beta := \\pi_R((Backdoor) | O_{null})$, the safety of the memoryless threshold protocol $\\pi_B$ is given by the value of $z_0^M$, where $\\Lambda_{m,c}$ depend only on $q_{m,c}$ and $\\Gamma$:\n$z_m := (\\Lambda_{m,c} + \\Gamma_{m,c}) z_{m-1} + \\Lambda_{m,c} (1 - \\beta) z_{m-1} + \\Gamma_{m,c} + \\Lambda_{m,c}(1 - \\beta)$\nwhere $z_m^0 := (1 - \\beta + \\Lambda_{m,c}) z_{m-1}^0$ and $z_0^0 = 1$.\nCorollary 4.2. Expanding $z_M$ gives a polynomial, p(\u03b2), in \u03b2 of degree M. For any fixed memoryless threshold policy, the optimal backdoor rate is $\\beta^* := \\min_{\\beta \\in [0,1]} p(\\beta)$.\nA similar expression for usefulness, which does not depend on \u03b2, can be found in appendix C. The safety of a finite mixture of memoryless threshold policies can be found by taking a weighted average. Although we implemented a root-finding algorithm based on theorem 4.2, we found that it was faster and more stable in practice to compute the values in fig. 2 using a combination of grid and ternary search."}, {"title": "5 Finding Pareto-optimal Protocols", "content": "In many cases, AI-Control Games are useful for evaluating hand-picked protocols, for example the protocols explored by Greenblatt et al. (2024) that were found via trial and error. However, it may also be useful to utilise the AI-Control Games to find the Pareto-frontier of existing and new protocols. This is a multi-objective maximin optimisation problem over a large space. This section presents tools for finding Pareto-optimal protocols, beginning with a reduction from a multi-objective to a single-objective problem.\nLemma 5.1. A Blue policy $\\pi_B$ is in an AI-Control Equilibrium if, and only if, for some w \u2208 [0, 1], it maximises $\\min_{\\pi_R} (w \\cdot Safety(\\cdot, \\pi_R) + (1 \u2212 w) \\cdot Use(\\cdot))$.\nThe proof uses the mappings from $\\Pi_B$ and $\\Pi_R$ to two simplexes introduced in appendix A. Lemma 5.1 implies that each safety/usefulness point corresponding to an AI-Control Equilibrium can be found by equivalently solving a, possibly easier, single-objective optimisation problem. Next, we leverage this result to further reduce the game to a single-objective, partially observable stochastic game (ZSPOSG) (see definition A.3).\nLemma 5.2. For every AI-Control Game G, and weight w\u2208 [0,1], there is a single-objective, zero-sum, partially observable stochastic game $G_w$ with expected return (for Blue) $V_G^w$ such that:\n$\\mathbb{E}_{\\pi_B, \\pi_R} : V_G^w(\\pi_B, \\pi_R) = V(\\pi_B, \\pi_R)$.\nThis proof is constructive and a full version can be found in appendix A. Intuitively, we create ZSPOSGs by adding a variable to each state. Each $G_w$ has reward $R_w$, where $R_0$ rewards usefulness in $\\oplus$ games and $R_1$ rewards safety in $\\ominus$ games. Since $\\pi_R$ has no effect on honest games, and therefore on usefulness, we can set Red to minimise the usefulness, making the game zero-sum without changing optima. Combined with lemma 5.1, this allows us to find any AI-Control Equilibrium by solving the right ZSPOSG.\nTheorem 5.3. A policy $\\pi_B$ is in an AI-Control Equilibrium if, and only if, for some ZSPOSG $G_w$ with value function $V_G$, $\\pi_B$ maximises $\\min_{\\pi_R \\in \\Pi_R} V_G(\\cdot, \\pi_R)$ over $\\Pi_B$.\nAlgorithm 1 uses theorem 5.3 to iteratively fill out the frontier by solving a series of single-objective maximin problems. Here FindNextWeight first returns $w_0$ = 0 and $w_1$ = 1, and then chooses weights that maximise the difference between known points on the Pareto frontier.14\nLine 3 of algorithm 1 requires finding the value of $\\max_{\\Pi_B \\in \\Pi_B} \\min_{\\pi_R \\in \\Pi_R} V_G(\\pi_B, \\pi_R)$. Although ZSPOSGs are previously studied, existing solvers are incompatible with a mem-"}, {"title": "6 Example: Results", "content": "In this section, we present example novel evaluations of protocols: evaluating existing and novel protocols across existing and novel settings. We also present the first Pareto"}, {"title": "6.1 Assumptions and Trends", "content": "Formalising an AI-Control Game requires making a number of modelling assumptions; the results below outline how varying each assumption impacts the safety and usefulness scores for a given blue-team policy.\nAuditing Budget. The auditing budget corresponds to the amount of human labour invested in overseeing the protocol. Protocols that utilise increases to the audit budget, get significantly safer and more useful as the budget increases, especially when the budget is small.\nNumber of Problems. For fixed FPRs, safety and usefulness are not significantly affected by the number of prob-"}, {"title": "7 Related Work", "content": "Although the application of a formal game-theoretic framework to Greenblatt et al. (2024)'s AI Control framework is novel, and the resulting type of game is unique, there are analogues to existing work. In this section, we compare AI-Control Games to existing models.\nThe AI-Control Games studied are most similar to the work of Chang, Erera, and White (2015), who model a security problem for food manufacturing. Like AI-Control Games, they consider a leader-follower equilibrium, where the leader tries to maximise productivity and minimise risk, and the follower tries to hide an attack. To select the leader strategy against multiple objectives, they use a genetic algorithm (as do Lins et al. (2013)). Their model differs in its factored state space, infinite time horizon, and the lack of 'scheming/honest' game modes. However, the largest difference is their consideration of a memoryful-follower.\nGiven our reduction in section 5, methods for finding equilibria in ZS-POSGs are particularly relevant. However, we find that our requirement for a memoryful-leader, memoryless-follower equilibrium precludes the use of many existing solvers. For example, Delage et al. (2023), which finds Nash equilibria in ZS-POSGs using a dynamic programming approach that solves subgames for each timestep from H down to 1. Other works attempt to solve subclasses of ZS-POSGs, with \u2018public observations' (Hor\u00e1k and Bo\u0161ansk\u00fd 2019) or 'on-sided partial-observability' (Zheng, Jung, and Lin 2022; Hor\u00e1k et al. 2023). AI-Control Games do not in-general fit into these subclasses, and each of these"}, {"title": "8 Future Work", "content": "In this work, we introduced a novel model for analysing the safety and usefulness of AI-Control protocols. We used our model and solution techniques to evaluate a much broader range of protocols and settings, but there is significant room to apply them further. In the Trusted Monitoring setting, future work can utilise algorithm 2 with POMDP-based best-response oracles to explore the space of memory-dependent protocols. Our model of the Trusted Monitoring protocol is also generalisable to any deployment setting using a question-answer format and any 'unacceptable outcome' that is a function of U's answer. Future work can explore such settings by empirically studying the interaction of the untrusted AI and the trusted monitor to generate distributions like those in fig. 1b and analyse the resulting frontiers."}, {"title": "A General results and proofs", "content": "This appendix presents proofs of the results from section 3 and section 5 that apply to all AI-Control Games. To do so, we first introduce a useful lemma which maps the optimisation problem into a vector space."}, {"title": "A.1 Mapping to vector space", "content": "We will define a mapping from $\\Pi_B$ to a simplex X, and another from $\\Pi_R$ to a simplex Y. Then, we will show that these mappings can be used to calculate safety and usefulness.\nWe begin by defining the simplex Y and a mapping $\\mu_R : \\Pi_R \\rightarrow Y$. Recall that: $\\Pi_R := {\\pi_R | \\pi_R : \\Omega_R \\rightarrow A_R}$. Define n := $|A_R||\\Omega_R|$ and enumerate the n observation-action pairs such that each dimension in $\\mathbb{R}^n$ corresponds to one pair. Consider the simplex Y $\\subset \\mathbb{R}^n$ such that all values are > 0 and, for each observation, the sum across all actions is 1. Then, define the mapping such that $\\mu_R(\\pi_R) = y$ if, and only if, for all j, $\\pi_R(a_R | o_R) = y_j$ for j'th observation-action pair $(o_R, a_R)$. Note that $\\mu_R$ is bijective.\nBlue's mapping is more complex. Define the finite set of Blue observation-action histories as $H_B := \\Omega_B \\times \\cup_{t=1}^{\\mathbb{H}}(A_B \\times \\Omega_B)^t$. Since the sets $H_B$ and $A_B$ are finite, we could use a similar strategy to the one we used for Y to define finite-dimensional vector space X that represents $\\Pi_B := {\\pi_B | \\pi_B : H_B \\rightarrow A_B}$. However, an alternative approach gives a useful linearity.\nBecause Blue has perfect recall, any behaviourally stochastic policy has an equivalent representation as a mixed strategy over the finite set of deterministic policies. This is the reverse direction of Kuhn's Theorem (Kuhn 1953), sometimes stated informally with 'and vice versa'. This exact result is mentioned in Maschler, Solan, and Zamir (2013, Theorem 6.26 on p. 242) as part of a more general proof about infinite-horizon games, and an informal proof is given by Osborne and Rubinstein (2006, p. 214). The other direction (which will also be useful), is given explicitly in Maschler, Solan, and Zamir (2013, Theorem 6.15 on p. 232):\nTheorem (Kuhn (1953)). In every game in extensive form, if player i has perfect recall, then for every mixed strategy of player i there exists an equivalent behavior strategy.\nNote that the equivalence stated here is in terms of the probability of reaching any vertex in an extensive-form tree which is more than sufficient for our purposes (Maschler, Solan, and Zamir 2013, see Definition 6.5 on p. 223).\nWe will use the fact that an AI-Control game is a finite extensive form game, and the 'vice versa' of Kuhn's Thoerem to give a mapping from behavioural to mixed policies.\nLet $\\mathbb{I}_B$ be the set of deterministic policies for Blue. Then, for each behaviorally stochastic $\\pi_B$, there is an equivalent mixture over deterministic policies: $\\sigma_B \\in \\Delta(\\mathbb{I}_B)$. Let m := $|\\mathbb{I}_B| = |A_B|^{|H_B|}$ and consider the simplex $X \\subset \\mathbb{R}^M$:\nX :=\\left\\{ x \\mid x \\in X, x_i \\geq 0, \\sum_{i=1}^{m} x_i = 1 \\right\\}\nEnumerate the m deterministic policies for Blue and assign each policy to a dimension in $\\mathbb{R}^M$. Let $\\pi_B^i$ be the i'th deterministic policy for Blue.\nDefine the mapping $\\mu_B(\\pi_B) := x$ where $x_i := \\sigma_B(\\pi_B^i)$, ie. the probability that $\\pi_B^i$ is chosen from the mixture $\\sigma_B$ that is equivalent to $\\pi_B$. Further, note that by the main direction of Kuhn's theorem, for any mixed stochastic policy $(\\pi_B)$, there is also a behaviourally stochastic policy $(\\bar{\\pi_B})$ that is equivalent but it's not immediate that $\\mu_B$ is bijective, since there may be two mixed stochastic policies that correspond to the same behaviourally stochastic policy.\nFrom here on, we drop the explicit use of $\\mu_B$ and $\\mu_R$, and instead use $x_{\\pi_B} := \\mu_B(\\pi_B)$ and $x_{\\pi_R} := \\mu_R(\\pi_R)$.\nLemma A.1. For any G, and any full-length trajectory h, there is a function $p_h : X \\times Y \\rightarrow [0,1]$ that is linear in it's first argument, continuous in its second, and satisfies the following for all $\\pi_B, \\pi_R$:\np_h(x_{\\pi_B}, y_{\\pi_R}) = \\mathbb{P} [h | \\ominus, \\pi_B, \\pi_R]\np_h(x_{\\pi_B}, y_{null}) = \\mathbb{P} [h | \\oplus, \\pi_B, \\pi_R] \nwhere $a_{null}$ is the policy that always chooses $a_{null}$.\nProof. Let $\\mathcal{H}$ be the set of all possible full-length trajectories (or joint-state-observation-action histories):\nh := (\\Theta, s_0, o_0^B, o_0^R, a_0^B, a_0^R, s_1, ..., o_{\\mathbb{H}-1}^B, o_{\\mathbb{H}-1}^R, a_{\\mathbb{H}-1}^B, a_{\\mathbb{H}-1}^R, s_{\\mathbb{H}})\nWe will define $p_h$ such that, for all $\\pi_B, \\pi_R$:\n\\mathbb{P} [h | \\Theta, \\pi_B, \\pi_R] = p_h(x, y)\nThen, we can write the probability of each history explicitly, in terms of $\\pi_B$ and $\\pi_R$:\n\\mathbb{P} [h | \\Theta, \\pi_B, \\pi_R] = I(s_0, o_0^B, o_0^R) \\cdot \\pi_B (a_0^B | o_0^B) \\cdot \\pi_R(a_0^R | o_0^R)\n\\cdot P(s_1, o_1^B, o_1^R | s_0, a_0^B, a_0^R)\n*...* P(s_{\\mathbb{H}},,, | s_{\\mathbb{H}-1},,)\n= c_h \\prod_{t=0}^{\\mathbb{H}-1} \\pi_B(a_t^B | o_{0:t}^B, a_{0:t-1}^B) \\pi(a_t^R | o_t^R)\nHere $o_{0:t}^B$ denotes the tuple $(o_0^B,...,o_t^B)$, and similar for $a_{0:t-1}^B$ etc. The value $c_h$ is a constant with respect to x and y"}, {"title": "A.2 Proving results from section 3", "content": "Using the lemma above, we can immediately prove some of the results from section 3.\nProposition (Proposition 3.2 Restated). For any AI-Control Game G, and appropriate distance functions in $\\Pi_B$ and $\\Pi_R$:\nSafety(,), Safety(, $\\pi_R$), and Use() are continuous in their domains.\nProof. These results follow fairly straightforwardly from corollary A.2, using the Blue distance measure $d(\\pi_B, \\pi_B) = ||x_{\\pi_B} - x_{\\pi_B}||$ and Red distance measure $d(\\pi_R, \\pi_R) = ||y_{\\pi_R} - y_{\\pi_R}||$. For fixed $\\pi_R$ and $\\epsilon$, $\\delta(\\epsilon)$ such that if $d(\\pi_B, \\pi_B) = ||x_{\\pi_B} - x_{\\pi_B}|| < \\delta$, then by corollary A.2, |s(x,y) - s(x,y)| = |Safety(\\pi_B, \\pi_R) - Safety(\\pi_B, \\pi_R)| < $\\epsilon$, and therefore Safety is continuous. The proofs for Safety($\\cdot, \\pi_R$) and Use($\\cdot$) are similar.\nProposition (Proposition 3.3 Restated). For all $\\pi_B$, Safety($\\pi_B$,) attains its minimum at some $\\pi_R$, and $\\min_{\\pi_R}$ Safety($\\cdot$, $\\pi_R$) is continuous.\nProof. For any $\\pi_B$, the minimiser y to s($x_{\\pi_B}$, ) always exists by corollary A.2. By the fact that $\\mu_R$ is a bijection, there is some $\\pi_R$ such that $\\mu_R(\\pi_R) = y$ and this $\\pi_R$ must minimise Safety($\\pi_B$, ) since s(x, y) = Safety($\\pi_B, \\pi_R$) for all $\\pi_B$ and $\\pi_R$. That $\\min_{\\pi_R} Safety(\\cdot, \\pi_R)$ is continuous at all $\\pi_B \\in \\Pi_B$ follows directly from the claim that $\\min_{y} s(\\cdot, y)$ is continuous."}, {"title": "A.3 Results from section 5", "content": "The mapping defined in lemma A.1 is also useful for proving the results in section 5", "1": "it maximises $\\min_{\\pi_R"}, "w \\cdot Safety(\\cdot, \\pi_R) + (1 \u2212 w) \\cdot Use(\\cdot))$.\nProof. The claim that a Blue policy $\\pi_B$ is in an AI-Control Equilibrium if, and only if, it is in a maximin point for $V_w$ can be shown using the equivalent claim from theorem A.3: that x is weakly Pareto optimal if, and only if, it is optimal for some $v_w$. A Blue policy $\\pi_B$ is in an AI-Control Equilibrium if, and only if, $(x_{\\pi_B})$ is weakly Pareto optimal, if and only if, there exists some w for which $x_{\\pi_B}$ is optimal for $v_w$.\nFor what remains, it is sufficient to show that, for any W, $x_{\\pi_B}$ is optimal for $v_w$ if, and only if, $\\pi_B$ maximises $\\min_{\\pi_R} (w \\cdot Safety(\\cdot, \\pi_R) + (1 \u2212 w) \\cdot Use(\\cdot))$.\n($\\Rightarrow$) If some $\\pi_{\\hat{B}}$ beat $\\pi_B$ in $\\min_{\\pi_R} (w \\cdot Safety(\\cdot, \\pi_R) + (1 \u2212 w) \\cdot"]}