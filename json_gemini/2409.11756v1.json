{"title": "Synthesizing Evolving Symbolic Representations for Autonomous Systems", "authors": ["Gabriele Sartor", "Angelo Oddi", "Riccardo Rasconi", "Vieri Giuliano Santucci", "Rosa Meo"], "abstract": "In recent years, Artificial Intelligence (AI) systems have made remarkable progress in various tasks. Deep Reinforcement Learning (DRL) is an effective tool for agents to learn policies in low-level state spaces to solve highly complex tasks. Recently, researchers have introduced Intrinsic Motivation (IM) to the RL mechanism, which simulates the agent's curiosity, encouraging agents to explore interesting areas of the environment. This new feature has proved vital in enabling agents to learn policies without being given specific goals.\nHowever, even though DRL intelligence emerges through a sub-symbolic model, there is still a need for a sort of abstraction to understand the knowledge collected by the agent. To this end, the classical planning formalism has been used in recent research to explicitly represent the knowledge an autonomous agent acquires and effectively reach extrinsic goals. Despite classical planning usually presents limited expressive capabilities, Probabilistic Planning Domain Definition Language (PPDDL) demonstrated usefulness in reviewing the knowledge gathered by an autonomous system, making explicit causal correlations, and can be exploited to find a plan to reach any state the agent faces during its experience.\nThis work presents a new architecture implementing an open-ended learning system able to synthesize from scratch its experience into a PPDDL representation and update it over time. Without a predefined set of goals and tasks, the system integrates intrinsic motivations to explore the environment in a self-directed way, exploiting the high-level knowledge acquired during its experience. The system explores the environment and iteratively: (a) discover options, (b) explore the environment using options, (c) abstract the knowledge collected and (d) plan. This paper proposes an alternative approach to implementing open-ended learning architectures exploiting low-level and high-level representations to extend its own knowledge in a virtuous loop.", "sections": [{"title": "1 Introduction", "content": "In the last few years, new AI systems have solved incredible tasks. These tasks include real-world games, such as chess [1] and Go [2-4], videogames such as Atari [5], Dota [6], and different robotics tasks [7-10]. These results have been mostly achieved through the intensive use of Reinforcement Learning (RL, [11]) with the rediscovered tech- nology of neural networks and deep learning [12]. Usually, \"standard\" RL focuses on acquiring policies that maximise the achievement of fixed assigned tasks (through reward maximisation) with a predefined collection of skills. New approaches have been proposed to enrich RL, allowing the agent to extend its initial capabilities over time inspired by neuroscience and psychology. Indeed, studies on animals [13-15] and humans [16-18] have explored the inherent inclination towards novelty, which is further supported by neuroscience experiments [19-21]. The field of intrinsically motivated open-ended learning (IMOL [22]) tackles the problem of developing agents that aim at improving their capabilities to interact with the environment without any specific assigned task. More precisely, Intrinsic Motivations (IMs [23, 24]) are a class of self- generated signals that have been used to provide robots with autonomous guidance for several different processes, from state-and-action space exploration [25, 26], to the autonomous discovery, selection and learning of multiple goals [27-29]. In general, IMs guide the agent in acquiring new knowledge independently (or even in the absence) of any assigned task to support open-ended learning processes [30]. This knowledge will then be available to the system to solve user-assigned tasks [31] or as a scaffold- ing to acquire new knowledge cumulatively [32-34] (similarly to what has been called curriculum learning [35]).\nThe option framework has been combined with IMs and \"curiosity-driven\" approaches to drive option learning [32] and option discovery [36-39]. In the hierar- chical RL setting [40], where agents must chunk together different options to properly achieve complex tasks, IMs have been used to foster sub-task discovery and learning [41-43], and exploration [26]. Autonomously learning and combining different skills is a crucial problem for agents acting in complex environments, where task solving consists of achieving several (possibly unknown) intermediate sub-tasks that are dependent on each other. An increasing number of works are tackling this problem [29, 44, 45], most focused on low-level, sub-symbolic policy learning [46], in turn combined in a hierar- chical manner using some sort of meta-policy [47]. While promising, these approaches necessarily face the problem of exploration, which becomes slower and less efficient as the space of states and actions increases."}, {"title": "2 Background", "content": "To reach a high level of autonomy, an agent acting in the low-level space, sensing the environment with its sensors and modifying it through its actuators must implement a series of layers of abstraction over its state and action spaces. As human beings reason over both simple and complex concepts to perform their activities, so robots should be able to build their own abstract representation of the world to deal with the increased complexity, using labels to refer to actions and events to be recognized and reasoned upon. In this paper, two level of abstractions are applied: the first one, from primitive actions to options [11, 62] and the second one, from options to classical planning [49]."}, {"title": "2.1 From primitives to options", "content": "As discussed before, at the lowest level, the agent sees the world with its sensor's values and changes it through the movement of its actuators. The most common formalism at this stage to deal with this type of representation is the Markov Decision Process (MDP), which models the environment as the tuple:\n$(S, A, R, T, \\gamma)$,   (1)\nin which S represents the set of possible high-dimensional states where each $s \\in S$ is described by a vector of real values returned by the agent's sensors, A describes the set of low-level actions $a \\in A$ in some cases also called primitives, R the reward function where $R(s, a, s')$ is a real value returned executing a from state s achieving s', T the transition function describing for $T(s's, a)$ the probability of reaching the state s' executing a from s, and the discount factor $\\gamma\\in (0,1]$ describing the agent's preference for immediate over future rewards. Usually, in this setting, the goal is to maximize return, defined as\n$R = \\sum_{i=0}^{oo} \\gamma^{i}R(s_i, a_i, s_{i+1})$.   (2)\nHowever, dealing with the state and action spaces of the formulation (1) is, in certain cases, impractical due to the high dimensional spaces considered. An effective formalism introduced to reduce the complexity of the problems is the option framework [62]. The option is a temporally-extended action definition which employs the following abstracted representation:\n$o = (I_o, \\pi_o, \\beta_o)$,   (3)\nwhere the option o is defined by an initiation set $I^o$ = {s|o \u2208 O(s)} representing the set of states in which o can be executed, a termination condition $ \\beta_o(s) \\rightarrow [0,1]$ returning the probability of termination upon reaching s by o, and a policy $ \\pi_o$ which can be run from a state s \u2208 I, and terminated reaching s' such that the probability $ \\beta_o(s')$ is sufficiently high. A policy is a function defining the behavior of the agent, mapping the perceived state of the environment to the action to be taken [11]. It is worth noting that options create a temporally-extended definition of the actions [62]."}, {"title": "2.2 Options and Classical Planning", "content": "The option formalism and its way of abstracting the dynamics of an environment share common characteristics with classical planning, in which the world is described in a simplified formal description considering only the aspects necessary to solve the agent's task [49]. Planning is the field of research studying formal methods to automatically find solutions, also called plans, to tasks requiring a sequence of actions [\u03b11,..., \u03b1\u03c0] to reach a goal state sg from an initial state sinit. The plan w is obtained by giving in input a model of the environment dynamics and the problem definition to a planner, returning a solution applying general optimization algorithms."}, {"title": "2.3 Intrinsic Motivation", "content": "The impulse to drive the agent away from the monotony of its usual activities, which psychologists and cognitive scientists have studied under the name of intrinsic moti- vation, is one of the most important elements enabling Open Ended Learning (OEL). The research in the field of Intrinsic Motivation (IM) concerns the study of human behaviors not influenced by external factors but characterized by internal stimuli (i.e. curiosity, exploration, novelty, surprise). In the case of artificial agents, we can summa- rize such aspect as anything that can drive the agent's behavior which is not directly dependent on its assigned task.\nThe insights provided by the IMs gave the researchers new ideas to model the stimuli of the agent (e.g. curiosity). Indeed, some models have been implemented using the prediction error (PE) in anticipating the effect of agent's actions (and more precisely the improvement of the prediction error [64, 65]) as an IM signal. A formal definition of agent driven by its curiosity has been formulated by Schmidhuber [64] as simply maximizing its future success or utility, which is the conditional expectation\n$u(t) = E_\\mu\\left[\\sum_{\\tau=t+1}^{T} \\gamma^{(\\tau-t)} r(\\tau) | h(\\leq t) \\right]$,   (10)\nover t = 1,2,..., T time steps, receiving in input a vector x(t), executing the action y(t), returning the reward r(t) at time t, taking into consideration the triplets h(t) =\n[x(t), y(t), r(t)] as the previous data experienced until time step t (also called history). The conditional expection $E_\\mu(\u00b7|\u00b7)$ assume an unknown probability distribution \u03bc from M representing possible probabilistic reactions of the environment. To maximize (10), the agent also has to build a predictor p(t) of the environment to anticipate the effects of its actions. The reward signal is defined as follows\nr(t) = g(rext(t), rint(t)),   (11)\nwhich is a certain combination g of an external reward rext(t) and an intrinsic reward rint(t). In particular, rint(t + 1) is seen as surprise or novelty in assessing the improvements in the results of p at time t + 1\nrint(t+1) = f|C(p(t), h(\u2264 t + 1)), C(p(t + 1), h(\u2264 t + 1))|,   (12)\nwhere C(p, h) is a function evaluating the performance of p on a history h and f is a function combining its two parameters (e.g. in this case, it could be simply the improvement f(a,b) = a b). It is important to notice that, as a baby does, an intrinsically motivated agent needs to find regularities in the environment to learn something. Consequently, if something does not present a pattern, there is nothing to learn and this becomes boring for both an agent and a human being.\nIn literature, IMs have also been categorized into different typologies [66-68]. An important discriminant aspect is the kind of signal received by the agent which can be of two types: knowledge-based (KB-IMs), which depends on the prediction model of the world (e.g. [64]), and competence-based (CB-IMs), which depends on the improvement of the agent's skills (e.g. [27]). In the framework presented in the next section, both these typologies are employed. CB-IM is used at a lower level to learn new skills and KB-IM at higher level to push the system to focus on the frontier of the visited states, from which it is more likely to discover novel information (e.g., find new states and learn new actions)."}, {"title": "3 System Overview", "content": "This section presents a new framework of an open-ended learning agent which, starting from a set of action primitives, is able to (i) discover options, (ii) explore the environ- ment using them, (iii) create a PPDDL representation of the collected knowledge and (iv) plan to improve its exploration while reaching a high-level objective set by the game.\nThe aim of this study is to assess the potential of abstraction in autonomous systems and propose a new approach for planning systems, extending them with learn- ing capabilities and behaviors driven by IMs. IMs are employed for discovering new options in a surprise-based manner at low-level and continuously exploring new states driven by curiosity at high-level. By the term abstraction, we simply mean mapping a certain problem space into a simpler one (e.g. converting a continuous domain into a discrete domain). In the proposed architecture, the abstraction is applied at two levels: a) passing from the primitive action space to the options action space and b) converting low-level data collected during the exploration into a high-level domain rep- resentation suitable for high-level planning, thus from raw sensors' data to a PPDDL representation."}, {"title": "3.1 Architecture description", "content": "As depicted in Figure 2, the system can be seen as a three-layered architecture: (i) the higher level contains the explicit agent's knowledge, (ii) the middle layer maps the high-level actions to their controllers and convert the raw sensors data into an explicit representation, and (iii) the lower level containing the components to sense the environment and interact with it. Mainly, the system executes the following pipeline:\n1. Option Discovery: using a set of primitives A = {a1, a2,...} belonging to the agent, the system combines them to create higher level actions, in this case, the options;\n2. Exploration: the set of options O = {01, 02, ...} discovered in the previous step is used to explore the environment. In the meantime, the visited low-level states data are collected into two datasets: the initiation data ID and transition data TD containing, respectively, the samples of states in which an option can be run and the transition between states executing it;\n3. Abstraction: datasets ID and TD of the visited low-level states until that moment are processed by the algorithm of abstraction, generating a PPDDL domain D of the knowledge collected;\n4. Planning: the PPDDL representation D is used to assess whether the final goal of the task sg can be reached with the currently synthesized knowledge, and to gener- ate a plan WEX to explore interesting areas of the domain. This plan is suggested by the Goal Selector, function included in the Long-Term Autonomy (LTA) module.\n5. The system, coordinated by the LTA, will execute again the loop from step 1, exploiting WEX to improve the Option Discovery and Exploration phases.\nIn this setting, the agent is initially only endowed with a set of primitive movements A = {ao,..., am}, and the world s\u2208 S is represented in terms of a vector (vo, ..., Un) of low-level variables vi \u2208 R, whose values as retrieved by the agent's sensors. The iterative utilization of this framework allows the synthesis of an emerging abstract representation of the world from the raw data collected by the agent, which continuously undergoes a refinement process over time, as it gets enriched with new"}, {"title": "3.1.1 Option Discovery", "content": "In this section, we will analyze the Option Discovery module (Algorithm 1, line 8) in greater detail. As previously anticipated, the discovery of new options is considered to be driven by the agent's surprise in finding out that new primitives are available for execution, during the agent's operations. When the agent encounters a change in the availability of its primitive abilities, it stores this event as a low-level skill that can be re-used later to explore the surrounding environment.\nBy executing the algorithm, the agent can discover a set of options O from scratch; such options are generated by repeatedly executing a certain primitive a \u2208 A among the available ones and collecting the produced changes in the environment. This pro- cedure is intentionally implemented in a simplified way, given that the focus of this work is on the architecture for extensible symbolic knowledge to be reusable to reach intrinsic and extrinsic goals autonomously; more sophisticated stategies to discover new policies are left to future works.\nIn more detail, the agent creates new options considering the following modified definition of option:\no(ap, at, I,\u03c0, \u03b2),   (13)\nwhere ap and at are primitive actions such that: (i) a\u00ba \u2260 at, ap is used by the execution of \u03c0, (ii) at stops the execution of when it becomes available, (iii) \u03c0is the policy applied by the option, consisting in repeatedly executing ap until it can no longer be executed or at becomes available, (iv) I is the set of states from which ap can run; and (v) \u03b2 is the termination condition of the option, corresponding to the availability of the primitive at or to the impossibility of further executing ap. For the sake of simplicity, in the remainder of the paper the option's definition will follow the more compact syntax\no(ap, at)   (14)\nmeaning that I is the set of states in which ap can run, \u03b2 checks the following two conditions: at becomes available or ap is no longer available, and is the policy corresponding to repeatedly executing an until \u03b2 verifies. Algorithm 2 describes in further details the option discovery procedure previously described. At the beginning of each discovery episode, the plan wEX is executed to reach a new area where to start learning new options (line 7-9). Then, for a maximum number of episodes and steps, the agent saves the current state s and randomly selects a primitive ap which can be executed in s (line 10-12). ap is repeatedly executed towards reaching the state s' until either ap is no longer available or new primitives beyond ap become available. If s \u2260 s', the procedure creates a new option o where o = o(ap, at) if a new primitive at has become available, or o = o(ap, {}) in the opposite case. In either way, in case o has not been discovered before, it is added to the other collected options. It is important to note that the options are independent on the state where the agent is, and are defined by the primitives' availability. This definition makes options reusable on different floors and with different objects, just"}, {"title": "3.1.2 Exploration", "content": "After discovering a set of valid options O as explained in the previous section, the system exploits them to explore the environment, collecting data about the reached low-level states (Algorithm 1, line 10). Considering that the abstracted representation of the world does not change significantly with a small amount of new data, the func- tion Collect_Data() is in charge of executing d_steps options for d_eps episodes, in which the agent starts its exploration from the initial configuration of the environment.\nAt each timestep, the agent attempts to perform an option o\u2208 O from a certain low-level state s \u2208 S. The selection of the action o during the exploration can follow different strategies, which are described in the subsection 3.1.4. In case the execution of"}, {"title": "3.1.3 Abstraction", "content": "The datasets collected in the previous step are then used as input for the function Create_PPDDL() (Algorithm 1, line 13), returning a symbolic representation D of the agent's current knowledge expressed in PPDDL formalism (PPDDL domain). The main advantage of the obtained PPDDL representation is that it makes explicit the causal correlations between operators that would have remained implicit at the option level. In the following, we provide a summary description of the abstraction procedure; for further details, the reader is referred to the original article [59].\nThe abstraction procedure executes the following five steps:\n1. Options partition: this step is dedicated to partitioning the learned options into abstract subgoal options, a necessary assumption of the abstraction procedure. Abstract subgoal options are characterized by a single precondition and effect set. However, given the uncertainty of the actions' effects in the environment, the opera- tors' effects will be modelled as mixture distributions over states. This phase utilizes the transition dataset TD collected before, as it captures the information about the domain segment the option modifies. Basically, the transition dataset is divided into sets of transition tuples presenting the same option o and mask m. Then, for each set, the partitions are ultimately obtained by performing clustering on the final states s' through the DBSCAN algorithm [69]. If some clusters overlap in their\n2. Precondition estimation: this step is dedicated to learning the classifiers that will identify the preconditions of the PPDDL operators. In order to have negative examples of the initiation set classifier for each operator, this operation utilizes the initiation dataset ID considering all the samples with option o and f(s, 0) = False. The positive examples comprise instead the initial states s taken from TD tuples belonging to the same partition. The initiation set classifier of the option is computed using the Support Vector Machines (SVM) [70]. The output of this phase is the set of all the initiation set classifiers of all the operators.\n3. Effect estimation: analogously, this step is dedicated to learning the symbols that will constitute the effects of the PPDDL operators. The effects distribution is modelled through the Kernel density estimation [71, 72], taking in input the final states s' of each partition.\n4. PPDDL Domain synthesis: finally, this step is dedicated to synthesising the PPDDL domain, characterized by the complete definition of all the operators asso- ciated with the learned options in terms of preconditions and effect symbols. This step entails the simple mapping of all the data structures generated during the previous steps in terms of symbolic predicates to be used as preconditions and/or effects for every operator.\nThe produced PPDDL domain can be potentially used to reach any subgoal that can be expressed in terms of the available generated symbols at any point during the Discovery-Plan-Act (DPA) loop. One interesting aspect of the proposed DPA frame- work is that the semantic precision of the abstract representation and its expressiveness increase as the DPA cycles proceed, as will be described in the experimental section."}, {"title": "3.1.4 Goal Selector", "content": "This module aims at simulating the intrinsic motivations driving the agent towards interesting areas to satisfy its curiosity and optimize its exploration. In particular, one of the most fascinating aspects of this system is the capability of setting its high- level goals, which potentially could be a combination of symbols defining a state that the agent has never experienced before. In other words, abstract reasoning could be the driving criterion for using the imagination to explore an unknown environment. Despite the previous goal is rather ambitious and still the object of future work, we will demonstrate in this work that the abstract reasoning can indeed be used for the more \"down-to-earth\" task of devising rational criteria to make more efficient the exploration of unexplored parts of the environment.\nThe selection of the target state $S_{target} \\in S$ to be reached in the next exploration cycle is performed at line 14 of Algorithm 1, calling the procedure Get_Target_State(). The Goal Selector suggests such state to the system, following an internal strategy which can be, in this case, Action Babbling, Goal Babbling and Distance-based Goal Babbling.\nAction Babbling is the simplest strategy of the system for the exploration, consist- ing of a pure random walking of the agent. This strategy returns $S_{target}$ = NULL, so no plan wEX is generated and executed in the exploration phase on the subsequent cycle. Goal Babbling and Distance-based Goal Babbling differ in the way they imple- ment IMs (such as curiosity). More specifically, in our system curiosity is formalised as an interest to reach (i) a random goal among those already achieved, and (ii) the border of the already acquired knowledge.\nThe first strategy is represented by Goal Babbling, consisting in randomly selecting a low-level state in the environment and trying to reach it [73]. Usually, the assumption of Goal Babbling is that all the goals which can be set belong to the world's low-level states that are reachable; each goal is formalised as the configuration of joints or posi- tion to be reached with the robot's actuators [27]. Since in general not all the states s\u2208 S are valid (e.g. the agent can't move inside the wall), this strategy selects a ran- dom state $S_{target}$ among the visited ones. Subsequently, $S_{target}$ is translated into a set of propositional symbols {$\u03c31,..., \u03c3k$}, as described in the following subsection 3.1.5, which represent the high-level goal to be reached using an off-the-shelf PPDDL planner. The capability of translating low-level states into symbols gives the agent a chance to reason on causal dependencies and, consequently, plan. It is important to notice that a pure Goal Propositional Babbling, consisting of the selection of a random subset of high-level symbols, would not be an effective strategy because only a limited number of combinations of symbols conjunctions are valid goals. Consequently, Goal Propositional Babbling is not taken into consideration.\nThe second strategy (Distance-based Goal Babbling) is implemented as a modified version of the Goal Babbling, and models the curiosity towards the less explored states as being influenced by the goal's distance from its starting location Sinit. In this case,"}, {"title": "3.1.5 Translating low-level states into symbols", "content": "The peculiarity and, simultaneously, the biggest challenge of this software architecture is thus to use an abstract symbolic representation to manage the evolution of the agent's knowledge. Using a symbolic representation to describe a desired environment configuration is a powerful tool for an efficient exploration of the world.\nAfter selecting $S_{target}$, the purpose of the planning module is twofold. On the one hand, it can be employed as usual to verify the reachability of the goal sg of the environment (i.e. get the treasure and bring it \"home\" in the Treasure Game, in line 17) and, on the other hand, it can be exploited to generate a plan driving the agent towards states, in our case $S_{target}$, relevant to extend the knowledge of the system (line 16). In both cases, to take advantage of planning it is necessary to transform a low-level state into a high-level one. This operation requires finding the combination of propositional symbols\n$\\sum_{target} = {\\sigma1,...,\\sigma k}$   (19)\nthat best represent the portion of state space including $S_{target}$. In other words, we look for a subset of symbols $\\sum_{target}$ whose grounding is $S_{target}$. These symbols make it possible to generate the definition of a planning problem (line 15) which, together with the domain definition, can be used to perform planning and solve the problem (line 16).\nIn order to select the right symbols conjunction, the system creates the classifier $Cl_{target} \\sim p(S_{target})$ approximating a distribution over $S_{target}$. $Cl_{target}$ is a SVM"}, {"title": "3.1.6 Planning", "content": "At the end of each cycle, the planning process generates a plan to reach either $S_{target}$ and sg. In both cases, in order to create a PDDL problem P, it is necessary to find the set of symbols $\\sum_{init}$, $\\sum_g$ and $\\sum_{target}$, describing the most suitable high-level state representation for Sinit, Sg and $S_{target}$ respectively, as described in the previous sub- section 3.1.5. Indeed, the couples ($E_{init}$, $E_g$) and ($E_{init}$, $E_{target}$) define the problems Pg and $P_{target}$. At line 15 of Algorithm 1, $P_{target}$ is generated as previously discussed and the plan to solve it, $wEX$, is generated by the planner (line 16).\nBefore moving to the next cycle, the system tries to solve also the problem Pg, performing the function Check_PPDDL_Validity (line 17). The resulting plan w9 is only used internally by the system to keep track of the success ratio of the planner with the evolution of the synthesized knowledge of the agent."}, {"title": "4 Experiment and Results", "content": "This section, dedicated to the experimental analysis, will first describe the dynam- ics of the environment, followed by an example of the system's cycle execution with its outputs and, finally, the overall results collected over different environment configurations."}, {"title": "4.1 Environment setup", "content": "The implemented system has been tested in the so-called Treasure Game domain [59]. In such an environment, an agent can explore the maze-like space by moving through\ns = (Xagent, Yagent, \u03b81, \u03b82, ..., Xkey, Ykey, Xbolt, Xtreasure, Ytreasure)   (22)\nin which Xagent, Yagent is the (x,y) position of the agent, \u03b8\u2081 is the angle of the lever i, Xkey, Ykey is the (x,y) location of the key, Xbolt is the state of the bolt (1 if open and 0 if locked) and Xtreasure, Ytreasure is the (x,y) location of the treasure."}, {"title": "4.2 The cycle", "content": "For exemplificatory purposes, a complete execution cycle of the system is briefly described in the following, showing the output of each phase of an intermediate cycle (cycle n. 10) performed on domain3 (see Figure 4c) in the Goal Babbling strategy case.\nOption Generation\nAt the beginning of each cycle, the agent executes Algorithm 2 to collect some options exploiting the agent's primitives, before the exploration can commence. In the Treasure Game environment selected for this work, the agent executes d_eps = 1 episodes, composed by d_steps = 200 primitive actions. After the execution of the plan WEX, the random exploration is reprised using the primitives contained in A (see Section 2.1). The result is the following set of learned options (11 in total) following the formalization (14):\nO = {(go_up, {}), (go_down,{}), (go_left,{}), (go_left,go_up), (go_left, go_down), (go_left, interact), (go_right,{}), (go_right,go_up), (go_right,go_down), (go_right, interact), (interact, {}) }.\nIt is important to note that, in general, the discovered options are not all the options that may be possibly discovered in the environment, but only those experienced by the agent during the exploration. This procedure is incremental, adding options to the set O each iteration of the Algorithm 1. As described in section 3.1.1, this procedure leverages IMs at low level, capturing the curiosity of the agent when it discovers to have new available primitives to exploit."}, {"title": "4.3 Results", "content": "In this section, the overall results of the system over different domain instances are described. First, the setting of the environment is discussed, then the adopted baseline, as well as the other strategies enabling the planning exploration. Subsequently, some charts are presented, that highlight the system's performance in terms of success ratio on the planning task. Finally, some issues worth being underscored are commented, and the limitations of the employed technologies are discussed.\nFor our purposes, the Treasure Game has been used with five different mazes (see Figure 4), to focus on the performances of the system on smaller domains, highlighting pros and cons of the symbolic approach proposed. The system has been executed, following the workflow described in Algorithm 1, in the cited five mazes configurations using parameters suitable to solve the tasks, which are described later. To summarize,"}, {"title": "4.4 Discussion and Future Work", "content": "As shown in Figure 9, the results are not deterministic and change over time. On the one hand, the system is continuously evolving in terms of knowledge and, on the other hand, ML techniques introduce further stochasticity to the final representation gen- erated. During the abstraction procedure, described in section 3.1.3, some statistical tools are employed to create data structures to represent preconditions, effects and symbols.\nFor instance, an erroneous clustering phase could generate unexpected effects on symbols and operators. An example highlighting this fact is the noisiness of some symbols, interfering with the generation of a correct formalization of D, P and, conse- quently, the resulting plan w. In Figure 10 it can be seen the graphical representation of the symbol \"on the highest y-axis position\", meaning \"on the top ladder\" because it is the only possible state with such y-axis value (it is not allowed to move inside the walls). In some cases, it could happen that the initial state of the game is interpreted\nas being at the top floor under the ladder and, consequently, it is not necessary to climb down the ladder to execute the agent's task. Then, although almost the whole plan w is correct to complete the game, without the option of climbing down the lad- der as the first action, the plan is incomplete, and the trial is considered unsuccessful. On the one hand, this is a drawback of using ML tools, which can introduce noise in the symbolic representation. On the other hand, it is the strong point of the proba- bilistic approach, always proposing solutions, even though presenting a certain degree of error.\nThe system proposed is limited by the propositional logic representation, which does not permit the inclusion of arguments in operators and symbols. Such limitation could be overtaken by developing a new abstraction procedure following other log- ics (e.g. First Order Logic (FOL)). Recently, an implementation of an object-centric abstraction procedure has been proposed [76], demonstrating that it is possible to create a lifted representation suitable for transfer the knowledge to new tasks.\nAnother aspect to be analyzed deeper is the management of the knowledge data. Unfortunately, the abstraction module has no real incremental nature, but the abstrac- tion procedure is executed over all the data, each cycle from scratch. This way of operating is not computationally efficient and does not reflect the learning process of the human being. However, given that in our domains all the necessary knowledge is discovered in a certain amount of time and does not change, a first improvement to be applied to the system could be maintaining a maximum of transition and initiation data discarding over-sampled transitions. Therefore, a sort of filter in the knowledge acquisition could stop the growth in terms of the abstraction procedure's time in stationary and limited domains but also reduce it for all the other cases.\nThen, the abstraction time seems being linear with respect to the number transition tuples (in the worst-case exponential according to Konidaris [59]). Consequently, it would be fundamental to find solutions to mitigate this aspect. Possible actions could be to filter the acquired data or structure the collected knowledge in another form more efficiently for the abstraction process and, eventually, produce different complementary representations which are used according to the task to be tackled.\nFinally, a natural evolution of this work lies in the neuro-symbolic approach. The integration of explicit knowledge inside sub-symbolic systems makes the learning pro- cess more effective and efficient, taking advantage of both the approaches. One of most promising extension of this work should embrace this new techniques, as some other preliminar works have already done [54, 55, 57]."}, {"title": "5 Conclusions", "content": "In this paper, a novel approach for open-ended learning in autonomous agents based on intrinsically motivated planning is presented. This approach integrates two powerful paradigms, intrinsic motivation and classical planning, to enable agents to continuously learn and improve their knowledge and skills without relying on external supervision or rewards.\nThis work suggests an alternative or complementary approach to the advanced and popular sub-symbolic methods, demonstrating interesting features. First, it allows agents to explore and learn in a self-directed and open-ended manner without being limited to a predefined set of goals or tasks. Second, it enables agents to represent and reason about their knowledge and skills in a structured and formal way, which can facilitate planning and generalization to new situations. Third, it can incorporate intrinsic motivations that drive the agent to explore and learn beyond extrinsic goals, which can enhance the agent's adaptability, robustness, and creativity. However, there are"}]}