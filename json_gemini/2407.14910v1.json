{"title": "Visual Geo-Localization from images", "authors": ["Rania SAOUD", "Slimane LARABI"], "abstract": "This paper presents a visual geo-localization system capable of determining the geographic\nlocations of places (buildings and road intersections) from images without relying on GPS\ndata. Our approach integrates three primary methods: Scale-Invariant Feature Transform\n(SIFT) for place recognition, traditional image processing for identifying road junction types,\nand deep learning using the VGG16 model for classifying road junctions. The most effective\ntechniques have been integrated into an offline mobile application, enhancing accessibility\nfor users requiring reliable location information in GPS-denied environments.", "sections": [{"title": "1. Introduction", "content": "Geo-localization involves determining the precise location of an image or video in the\nreal world, typically using latitude and longitude coordinates obtained through technologies\nsuch as GPS, Wi-Fi, and IP address tracking. Algorithms process this data to pinpoint exact\ncoordinates[11][12]. Geo-localization is important for organizing and analyzing large volumes\nof imagery data, as demonstrated by systems like the US Geological Survey (USGS), which\nclassify and locate satellite and drone images to streamline data collection and analysis.\nSocial media platforms like Instagram use geo-localization to tag photos with specific loca-\ntions, enabling users to explore location-based content[11]. Despite its significance, many\nimages and videos lack geo-localization data, particularly those collected in the past or by\ndevices without GPS capabilities[12]. This presents challenges in organizing and analyzing\nsuch images based on their locations. Visual geo-localization addresses these issues by iden-\ntifying the geographic position of an image using visual cues within the image itself, rather\nthan relying on external metadata like GPS tags[12]. As defined by Brejcha and Cadik,\nvisual geo-localization involves finding the geographic coordinates (and possibly the camera\norientation) for a given query image[12]. This technique uses attributes such as building\nfacades, points of interest, and geographical information to compare the query image with\na database of geo-referenced images[11]. The goal is to provide accurate location predic-\ntions in situations where conventional methods, such as GPS, are ineffective, inaccurate, or\nunavailable. Advanced computer vision and deep learning algorithms play a critical role\nin visual geo-localization by analyzing visual data to identify unique features or patterns\nassociated with specific locations. This technology enhances applications like augmented\nreality, self-driving cars, and location-based services. However, geo-localization faces tech-\nnical challenges due to the vast amount of visual content shared online. Issues such as\nambiguity from geographical features appearing in multiple locations and the need for pre-\ncision and accuracy in geotagging processes arise. Additionally, many images lack geo-tags,\ncomplicating initial location estimation. The dynamic nature of physical surroundings, in-\nfluenced by factors like construction and seasonal changes, affects the visual indicators used\nfor geo-localization. Poor photographic conditions, such as inadequate lighting and camera\ndistortions, further complicate accurate location identification[11]. To address these chal-\nlenges, scalable systems capable of processing and matching large quantities of data with\nextensive geographic databases are necessary. Such systems must achieve high performance\nand accuracy to be useful in applications like city navigation or emergency services, where\npinpointing locations within a few meters is important. Efficiency is essential for real-time\napplications, which depend on quick data processing. Robustness against various conditions\nand low impact from domain shift, where query images significantly deviate from reference\ndata, are also critical[13]. These requirements underscore the need for innovative methods\ncombining advanced deep learning and computer vision techniques, with a strong emphasis\non user privacy, to analyze visual information and accurately predict geographic locations."}, {"title": "2. Related Works", "content": "This paper introduces a novel geo-localization solution developed at the University of Science\nand Technology Houari Boumediene. To enhance the system's efficiency, we have collected\ndata from various locations over time. Our project explores three methods for visual geo-\nlocalization: using SIFT, identifying road junctions through image processing, and applying\ndeep learning techniques. After evaluating each method, we selected the most effective ap-\nproach. This method has been developed into an offline mobile app, providing users with\nconvenient and reliable location identification without the need for an internet connection.\nThe field of image-based geo-localization has seen various innovative approaches aimed at\nimproving the accuracy of determining geographic locations from images. For instance, the\nmethod presented in [1] uses sequences of ground photos compared against geo-tagged aerial\nimages to handle changes in perspectives and sequence differences effectively. Similarly, the\nTransVLAD: Multi-Scale Attention-Based Global Descriptors for Visual Geo-Localization\n[2] introduces an efficient technique that significantly enhances location accuracy, which is\nespecially useful for autonomous driving and robot navigation. Urban localization accuracy\nis improved in [3] by focusing on buildings and using advanced matching techniques to handle\ndifferent viewing conditions and perspectives. Another novel approach is presented in [4],\nwhich mimics human navigational methods to efficiently determine locations from photos.\nIn [5], a sophisticated method enhances the accuracy of photo localization by ensuring that\nvisual features match well with those in a large database. Cross-view Image Geo-localization\n[6] introduces a streamlined model for matching street-level photos with aerial images, sim-\nplifying the process and enhancing accessibility. The study in [7] shows how selectively\nusing image features for matching can improve the efficiency and accuracy of localization,\nintroducing a robust method for feature encoding. Large-scale Image Geo-Localization Us-\ning Dominant Sets [8] uses an advanced clustering approach to select the most coherent and\ncompact sets of features matching the query image, significantly speeding up the localization\nprocess. The work in [9], RK-Net, addresses the challenge of localizing images from different\nviewpoints by focusing on the most informative parts of images, simplifying the learning"}, {"title": "3. Proposed System", "content": "Our system aims to enhance geo-localization accuracy using visual information from im-\nages. By integrating advanced techniques in image processing, machine learning, and com-\nputer vision, we can pinpoint geographic locations independently of GPS data. We use three\nprincipal approaches: the Scale-Invariant Feature Transform (SIFT), deep learning with the\nVGG16 model, and traditional image processing methods for analyzing road junction types.\nInstead of using traditional image processing techniques to identify types of road junctions\ndirectly from image, road junctions and its time-consuming nature, we used a deep learning\napproach using the VGG16 model. In addition, we used the SIFT approach to match images\nwith the model images. This section details the steps of our proposed visual geo-localization\nsolution: We start with an existing map that provides a basic layout of the geographical area\nof interest. This map serves as the reference for aligning and comparing our image-based\nfindings. The coordinates of the starting position are obtained from the name of the starting\nplace.\nUsing SIFT for Place Recognition:\nWe use the Scale-Invariant Feature Transform (SIFT) to analyze panoramic images [18].\nThis method identifies the closest match to a given location by comparing features from new\nimages with those in our panoramic dataset.\nDeep Learning for Junction Identification:\nIn this advanced method, we use the VGG16 deep learning model to classify road junctions\nfrom images. Once we identify these junctions, we match them with the map to verify and\nenhance geographical accuracy."}, {"title": "3.1. Place Recognition SIFT descriptor", "content": "In this approach, we focus on place recognition by comparing input images against a\ndataset of panoramic images using SIFT descriptors. First, we create the dataset by ex-\ntracting frames from videos, which are preferred over still images due to their continu-\nous and comprehensive environmental capture. This continuity is important for seamless\nstitching, so we ensure a significant overlap-typically around 30-50%-between consecutive\nframes. Videos must be recorded with stable movement and consistent speed. After creat-\ning panoramic views, we refine them by cropping and applying the Scale-Invariant Feature\nTransform (SIFT) technique to extract distinctive features. Subsequently, we use the Detec-\ntron2's panoptic segmentation to filter the input images, retaining only those where buildings\nand their surroundings occupy a significant portion of the content (more than 60%). We\nthen apply the SIFT algorithm to these selected images to match their features against the\npanoramic dataset using the FLANN matcher. This step identifies the most similar place for\neach image. Finally, a voting process determines the most likely place based on the results\nfrom all analyzed images.\n1) Dataset Creation :\nThe initial step in our dataset creation involves extracting frames from video recordings\nat specific intervals determined by the skip frames parameter. This method balances the\ncomprehensiveness of the data against redundancy, ensuring the frames are representative\nyet manageable in size. To construct panoramic images, we stitch multiple frames together\nusing algorithms that align overlapping areas based on shared features. This process helps\novercome challenges such as changes in lighting and camera movement, and we enhance the\naccuracy of the stitch through techniques like color normalization and feature alignment.\nAfter stitching, we crop unnecessary black edges from the panoramic images to focus on\nrelevant content, thereby optimizing the data for further processing. Using the SIFT tech-\nnique, we then extract key features from the images that remain consistent despite changes\nin scale or rotation, making it easier to match images from different scenes. These features"}, {"title": "3.2. Road junction Identification using deep learning", "content": "The second approach is to identify road junctions using a deep learning method with the\nVGG16 model, pre-trained on a large database of images. We start by collecting diverse\nimages of roads, including features like T-junctions, X-junctions, Y-junctions, and round-\nabouts. These images are labeled to highlight specific road features. The VGG16 model\nis then fine-tuned to focus on road junctions, ensuring it performs well with new, unseen\nimages. After training, the model is tested with new images to evaluate its capability in\nidentifying and classifying different road features.\nModel Preparation and Training:\nPreprocessing: Images were resized to 224x224 pixels and normalized to suit the input\nrequirements of the VGG16 model.\nClass Weight Calculation: To address dataset imbalances, class weights were calculated\nand applied during training to ensure equitable learning across less frequent classes.\nModel Architecture Modifications: The base VGG16 model was customized by removing\nthe top layer and adding dense layers with ReLU activation and dropout layers to enhance\nfeature learning capabilities.\nModel Training and Validation:\nTraining Set Preparation:The dataset was split into training, validation, and test sets,\nwith augmentation techniques such as random flipping and brightness adjustment applied\nto enhance generalization.\nTraining Process: Initially, only the top layers of the model were trained. Techniques\nlike batch normalization and dropout were used to improve training stability and prevent\nover-fitting.\nFine-Tuning and Evaluation:\nFine-Tuning: After the initial training, deeper layers of the VGG16 were gradually\nunfrozen and the model was fine-tuned with a reduced learning rate to refine its ability to\nclassify junction types accurately.\n- Model Evaluation: The fine-tuned model was tested against a separate set of images to\nassess its performance and ensure it could reliably classify road features in varied real-world\nconditions.\nOur deep learning strategy harnesses the capabilities of the pretrained VGG16 model,\nwhich we fine-tune to classify various road junctions and features.\nAdvantages:\nGeneralization: Once trained, the model can effectively identify a broad spectrum of road\nfeatures, adapting seamlessly to new, encountered scenarios.\nEfficiency: Compared to real-time computer vision analysis, deep learning models require\nless computational power during inference, making them faster and more cost-effective.\nEase of Integration: These models can be effortlessly integrated into various applications\ndue to their flexibility and self-contained nature.\nChallenges:\nData Dependency: The precision of predictions heavily depends on the quantity and\nquality of the training data.\nOpacity in Decision-Making: Unlike rule-based techniques, deep learning models do not\neasily reveal how decisions are made, which can complicate troubleshooting and refinement.\nConsidering the limitations of the Computer Vision approach, particularly its depen-\ndency on specific imaging conditions and the complexity of adapting to diverse scenarios, we\ntransitioned to using a Deep Learning approach. This change significantly enhances our sys-\ntem's ability to generalize and scale. By expanding our dataset and continuously retraining\nthe model, we can accommodate a wider array of road features and intersections without the\nneed for extensive manual algorithm adjustments. This strategic pivot not only speeds up\nthe process but also broadens the potential applications of our model in real-world scenarios."}, {"title": "3.3. Integration and Map Matching", "content": "This section outlines the methodology for integrating geographic data with a graph-based\nmap system and the subsequent application for route mapping and validation.\n1) Creation of Graph-based Road Map Dataset\nDefining the Area of Interest with JOSM: Initially, we define the geographic area of\ninterest using JOSM, a robust tool for editing OpenStreetMap data. This tool enables us\nto tailor the map data precisely by adding or removing roads, buildings, and intersections,\nthus facilitating the map-matching process.\nAutomated Road Feature Analysis and Classification: This process involves converting\ndetailed road data into a structured graph. We use GeoJSON files to represent roads as\nline strings, which aids in constructing a graph where intersections are nodes and road\nsegments are edges, enhancing our analysis of road connectivity. We classify junction types\nby identifying nodes where roads intersect, with different configurations indicating various\njunction types like T-junctions, X-junctions or Y-junctions (see Figure 4).\nIntegration and Validation: Integrating the data into a unified graph-based map involves\nconsistency checks to ensure data accuracy and representation. Validation occurs through\nsystematic verification of node adjacency's and junction classifications, ensuring the integrity\nof the geographic data within our application."}, {"title": "2) Sequence Matching with Graph-Based Map", "content": "In this phase, we use the graph-based road map to identify potential routes by matching\na sequence of junction types to the graph data. Starting with intersections close to a given\nlocation, we calculate distances using the Haversine formula to maintain focus on relevant\nintersections. We then apply a depth-first search algorithm to trace paths that match the\ndefined sequence of junction types. This method not only aids in route planning but also\nenhances our navigation system's reliability by ensuring the paths adhere to expected road\npatterns.\nExample of path finding result :\nIn this section, we present an example of the pathfinding results generated by our algo-\nrithm. Figure 5 illustrates a sequence of real images taken from intersections 1 to 5. Each\nimage represents a critical point in the route, showcasing different types of road junctions\nencountered along the way.\nThe pathfinding algorithm processes these images to determine the most probable route\non the map. By using the intersection types and their sequences, the algorithm identifies the\npath through the road network. The resultant path is depicted in the map shown in Figure\n5.\n3) Validation and Correction Mechanism\nOnce a route is identified, we conduct a detailed validation of each junction of the sequence\nto confirm alignment with the expected path. Discrepancies are addressed by exploring\nalternative routes that provide a better match. This process is important for maintaining\nthe accuracy and reliability of the navigation system, ensuring that the route suggestions\nare practical and reflect real-world conditions."}, {"title": "4. Experimental results", "content": "In this section, we present the results of our experiments conducted to evaluate the\nperformance of our visual Geo-localization system. We will detail the performance metrics for\neach approach, compare the effectiveness of traditional image processing and deep learning\nmethods, and discuss the results of our mobile application implementation."}, {"title": "4.1. Dataset Preparation and Annotation", "content": "We captured 282 images of road junctions using a smartphone, focusing on T-junctions,\nY-junctions, X-junctions, and roundabouts. These images were taken from various distances\nand angles, ensuring a diverse dataset,a selection of images is displayed in Figure 6."}, {"title": "4.2. Data Splitting and Balancing", "content": "The data was split into training (70%), validation (15%), and test (15%) sets. To ad-\ndress class imbalances, we used SMOTE (Synthetic Minority Over-sampling Technique) to\ngenerate synthetic samples."}, {"title": "4.2.1. Data Augmentation", "content": "To enhance the model's generalization, we used Keras' ImageDataGenerator for data\naugmentation. This technique artificially creates variations of our existing images by ap-\nplying random transformations such as shifting, rotating, flipping, and altering brightness.\nThese transformations increase the diversity of the dataset, helping the model to generalize\nbetter to new, unseen images. Additionally, SMOTE was used to create synthetic samples\nfor the minority classes. SMOTE works by generating new samples along the line segments\njoining existing minority class samples. This further balances the dataset and ensures that\nall classes are equally represented during training. Combining ImageDataGenerator and\nSMOTE significantly increased the number of training images. While the exact number of"}, {"title": "4.3. Training and Fine-Tuning of the VGG16 Model", "content": "augmented images is not directly counted, the process resulted in more than 20,000 image\nvariations over the training epochs, greatly enhancing the dataset's diversity.\nWe used the Adam optimizer and categorical cross-entropy loss function for training\nthe VGG16 model. The accuracy metric was used to measure the model's performance\nduring training. The initial training was conducted over 50 epochs with a batch size of\n32. For fine-tuning, we unfroze the last eight layers of the VGG16 model, training for an\nadditional 30 epochs with a reduced learning rate to improve accuracy. We used callbacks\nlike ReduceLROnPlateau and EarlyStopping to enhance training efficiency and prevent over-\nfitting."}, {"title": "4.4. Results", "content": "We conducted tests where users uploaded images of buildings and road intersections.\nThe app processed these images to determine the user's location, prioritizing intersection\nimages for better accuracy. Figure 8 shows an example of three query images with a correct\nclassification. Table 01 the values of precision, recall and F1-measure.\nBy addressing the imbalance in the dataset, the model's performance on the test set\nimproved by 3%. Below, the table 02 illustrates the changes in loss and accuracy."}, {"title": "5. Conclusion", "content": "The experimental results demonstrate the effectiveness of our proposed Geo-localization\nmethod using a deep learning approach. The proposed system which is implemented on mo-\nbile successfully integrates these methods, providing a practical tool for users in GPS-denied\nenvironments. Future work will focus on expanding the dataset, further improving model\naccuracy, and adapting the system for augmented reality (AR) and indoor applications."}]}