{"title": "STEERDIFF: STEERING TOWARDS SAFE TEXT-TO-IMAGE DIFFUSION MODELS", "authors": ["Hongxiang Zhang", "Yifeng He", "Hao Chen"], "abstract": "Text-to-image (T2I) diffusion models have drawn attention for their ability to generate high-quality images with precise text alignment. However, these models can also be misused to produce inappropriate content. Existing safety measures, which typically rely on text classifiers or ControlNet-like approaches, are often insufficient. Traditional text classifiers rely on large-scale labeled datasets and can be easily bypassed by rephrasing. As diffusion models continue to scale, fine-tuning these safeguards becomes increasingly challenging and lacks flexibility. Recent red-teaming attack researches further underscore the need for a new paradigm to prevent the generation of inappropriate content. In this paper, we introduce SteerDiff, a lightweight adaptor module designed to act as an intermediary between user input and the diffusion model, ensuring that generated images adhere to ethical and safety standards with little to no impact on usability. SteerDiff identifies and manipulates inappropriate concepts within the text embedding space to guide the model away from harmful outputs. We conduct extensive experiments across various concept unlearning tasks to evaluate the effectiveness of our approach. Furthermore, we benchmark SteerDiff against multiple red-teaming strategies to assess its robustness. Finally, we explore the potential of SteerDiff for concept forgetting tasks, demonstrating its versatility in text-conditioned image generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Text-to-image (T2I) diffusion models have attracted attention for their out-of-the-box functionality and the superior quality of their generated images. Using models like Stable Diffusion (Rombach et al., 2022; 2021) or DALL-E (Ramesh et al., 2022), users can use simple natural language descriptions as input to generate high-quality images with precise text alignment. This capability is largely contributed by pre-trained language models (Hammoud et al., 2024; Jin et al., 2020; Dosovitskiy, 2020) that learn and reflect the underlying syntax and semantics, as well as by extensive multimodal training datasets that encompass a wide range of text-to-image aligned content. However, these training methods also introduce risks of generating inappropriate images to the models. Thus, preventing the generation of inappropriate images is both critical and urgent.\nTo this end, existing T2I diffusion models have integrated several safety strategies to prevent in-appropriate content generation. Stable diffusion incorporates a Not Safe For Work (NSFW) post-generation filter and ad-hoc filter during training to avoid generating unsafe images (Schuhmann et al., 2022; Schramowski et al., 2023; Rando et al., 2022). However, existing text-based attacks (Li et al., 2018; Jin et al., 2020; Garg & Ramakrishnan, 2020; Maus et al., 2023) can mislead the classification mechanisms in the filter by rephrasing \"A photo of a billboard showing a naked man.\" into \"A photo of a billboard showing an LGBT man in an explicit position\". Additionally, users report that removing explicit images and other subjects from training data may have had a negative impact on the output quality, thus harming the utility of the models (Rombach et al., 2022)."}, {"title": null, "content": "The prevention of inappropriate content generation in AI models has primarily been approached through concept removal, which aims to control the generation process itself. Two primary strategies have been explored: (1) using ControlNet-like (Zhang et al., 2023a) structures to guide the diffusion process (Schramowski et al., 2023; Rando et al., 2022), and (2) identifying and pruning weights within the diffusion model (Gandikota et al., 2023). However, these methods face significant challenges as generative models grow in scale and complexity. The increasing size of models makes these approaches computationally expensive and impractical (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022). Additionally, simply guiding the generative process introduces vulnerabilities, leaving models open to jailbreaking attempts (Zhang et al., 2023b; Qu et al., 2023). These limitations highlight the need for more efficient and robust solutions.\nAlthough the aforementioned safety mechanisms have shown effectiveness according to their respective evaluation schemes, recent red-teaming studies demonstrate their potential flaws (Zhang et al., 2023b; Qu et al., 2023). Chin et al. (2023) shows that approximately half of the prompts, which were originally mitigated by existing safety mechanisms, can be manipulated by their Prompt-ing4Debugging (P4D) to become problematic. Similarly, Unsafe Diffusion (Qu et al., 2023) found that 14.56% of generated images across four state-of-the-art T2I models and their four prompt datasets were unsafe, underscoring the vulnerability of these models to generating harmful content. Moreover, black-box jail-breaking approaches Jailbreak Prompt Attack (JPA) and SneakyPrompt (Ma et al., 2024; Yang et al., 2024) successfully attack both online services and offline T2I models with the current safety mechanisms.\nIn this work, we propose SteerDiff, a two-stage lightweight adaptor model for text-conditioned diffusion models that focuses on guiding text prompt embeddings rather than controlling the generative process. Our method constructs a semantic boundary that maximally distinguishes between safe and unsafe content. We then project potentially unsafe embeddings toward the safe region while preserving the original semantics and maintaining the diffusion model's generative capabilities. This approach offers three key advantages: efficiency, effectiveness, and versatility. By operating at the prompt embedding level, our method eliminates the need for computationally intensive model re-training while preserving the original semantics. Moreover, by preventing unsafe content at the earliest stage of generation, we can block the formation of unsafe latent representations more directly and reliably. Lastly, our lightweight approach can be easily trained and applied to various concept removal tasks.\nWe benchmark our prototype SteerDiff against state-of-the-art concept removal techniques, including Erased Stable Diffusion (ESD) (Gandikota et al., 2023) and Safe Latent Diffusion (SLD) (Schramowski et al., 2023; Rando et al., 2022), for removing inappropriate content. Experimental results demonstrate that SteerDiff significantly reduces inappropriate content generation while preserving image quality and semantic fidelity. Furthermore, we evaluate its robustness by defending against red-teaming methods such as P4D and SneakyPrompt, demonstrating the effectiveness of our approach in mitigating various forms of adversarial attacks on text-to-image generation systems."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 TEXT-TO-IMAGE DIFFUSION MODELS", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Rombach et al., 2022) are a type of probabilistic generative model that learns a data distribution by gradually transforming a simple distribution into a complex target distribution. Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020) model the data generation process as a sequence of denoising steps. Given an image, DDPMS progressively add noise sampled from Gaussian distribution to generate an intermediate noisy image $x_t$ at each time step $t$ called forward diffusion steps. The noisy image $x_t$ can be expressed in closed form as a function of the original image $x_0$, the time step $t$, and noise $\\epsilon$ sampled from a Gaussian distribution $N(0, I)$. The model is then trained using the reverse process, where the objective is to learn a model parameterized by $\\theta$ that predicts $\\epsilon$. This objective is defined as:\n$L = E_{t, x_0, \\epsilon} [||\\epsilon - \\epsilon_\\theta(x, t)||_2]$"}, {"title": "2.2 TOWARDS SAFE IMAGE GENERATION", "content": "Current approaches to prevent undesirable images from generation generally follow two main strategies. The first involves removing undesired images from the training set, such as excluding all human figures (Nichol et al., 2021) or selectively omitting specific undesirable image categories in the dataset (Schuhmann et al., 2022; DAL, 2022; Rombach & Esser, 2022). However, this approach is costly as it necessitates retraining the model, and removing certain data classes often degrades the overall quality of the generated images (O'Connor, 2022). The second strategy is post-hoc, includes using blacklists for blocking unsafe concepts (DAL, 2022; mid, 2024; Leo, 2023; Markov et al., 2023), editing images after generation, or fine-tuning diffusion models to guide the inference process (Schramowski et al., 2023). Although blacklists are straightforward to implement, they can be easily bypassed. Image editing and diffusion process manipulation methods are effective but still require image synthesis, adding computational overhead. Other approaches (Park et al., 2024; Zhang et al., 2024) use inpainting to mask unsafe content or attempt to unlearn inappropriate concepts either within the diffusion model (Zheng & Yeh, 2023) or the textual encoder (Poppi et al., 2023). These methods involve expensive fine-tuning, while SteerDiff offers an intermediary solution that operates between the input prompt and the diffusion model without additional training. Similarly, Latent Guard (Liu et al., 2024) identifies inappropriate concepts prior to diffusion by learning a latent space on top of the T2I model's text encoder. However, while Latent Guard is capable of detecting inappropriate concepts, it lacks the ability to remove them. In this work, we compare the naive blacklist method commonly used in commercial platforms, Erased Stable Diffusion (Gandikota et al., 2023), the state-of-the-art model-editing approach, and Safe Latent Diffusion (Schramowski et al., 2023), a guidance-based model. Our focus is to introduce a new approach that identifies and manipulates inappropriate concepts in the text embedding space to ensure safer image generation."}, {"title": "3 METHODOLOGY", "content": "Determining what constitutes inappropriate imagery is highly subjective and varies based on context, setting, cultural and social predispositions, and individual factors. Additionally, Qu et al. (2023) observes that unsafe content can be shared through memes. In the context of T2I models, these new concepts are embedded within the words of a prompt. Consequently, adding new concepts to block T2I generation without retraining the diffusion model is impractical. To overcome these limitations, we formalize the problem by identifying unsafe concepts in text embedding space and then projecting potentially unsafe embeddings toward safe regions. This projection preserves the original semantics while maintaining the diffusion model's generative capabilities. Our approach allows us to define blacklisted concepts at test time, enabling greater flexibility."}, {"title": "3.1 TRAINING DATA COLLECTION", "content": "Directly classifying safe/unsafe prompts requires large-scale annotated datasets (Markov et al., 2023). Following Safe Latent Diffusion, we base our definition of inappropriate content on the work of Gebru et al.: \"[data that] if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety\" (Gebru et al., 2021; Schramowski et al., 2023). Specifically, we consider an image as inappropriate if it contains any concept $c \\in C_{unsafe}$, where\n$C_{unsafe} = \\{hate, harassment, violence, self-harm, sexual content, \\newline shocking\\ images,\\ illegal\\ activity\\}.$"}, {"title": "3.1.1 IDENTIFIER DATASET", "content": "As mentioned previously, we aim to detect inappropriate concepts in prompts to avoid inappropriate content generation. The first step in our pipeline is to construct the dataset with unsafe terms capturing the concepts from $C_{unsafe}$. To achieve this, we start by collecting open-sourced blacklisted phrases. Midjourney (mid, 2024) employs a blacklist of words and phrases that includes phrases associated with violence, hate speech, explicit sexual content, illegal activities, and other categories considered inappropriate by the platform. Additionally, Latent Guard (Liu et al., 2024) introduces the CoPro dataset, which includes safe and unsafe prompts centered around blacklisted concepts. We build our blacklist based on Midjourney and CoPro dataset.\nAlthough we can collect numerous NSFW terms from open-source datasets, these datasets may be imbalanced and could lack some categories defined in $C_{unsafe}$. For example, the number of shocking images or illegal activity is lower compared to images in other categories. To tackle this, we leverage an LLM to generate related terms $t_c$ centered around one sampled concept $c$ in the blacklist of $C_{unsafe}$ as illustrated in Figure 1, similarly to Hammoud et al. (2024) and Liu et al. (2024). This allows us to create a set $T_{unsafe} \\{t_c | c \\in C_{unsafe}\\}$. The unsafe terms in $T_{unsafe}$ simulate typical unsafe terms that a malicious user may input. In addition to the unsafe terms, we combine randomly selected 500 prompts (Blue block on the left) from the COCO 30k dataset (Lin et al., 2014) and the following mentioned paired safe phrases dataset (Green block on the left) to serve as our safe prompt dataset. All phrases within these prompts are considered safe."}, {"title": "3.1.2 STEER MODEL DATASET", "content": "In our subsequent steering transformation training procedure, we synthesize additional safe terms to steer unsafe embeddings toward safe ones. The core idea is to associate each unsafe term $t_c \\in T_{unsafe}$ with a corresponding safe term $t'_c$ of similar meanings, allowing us to convert unsafe concepts into safe alternatives while preserving the original semantic intent of the prompt. For example, consider the term \"killed\" in the prompt \"A man got killed.\" which represents a violent visual scene linked to the concept of \"violence\". We use an LLM to eliminate unsafe concepts in the input term $t_c$. In this case, a possible safe term $t'_c$ would be \"saved\", transforming the meaning of the prompt to something like \u201cA man got saved\". By processing all elements in $T_{unsafe}$, we generate paired safe phrases dataset $T_{safe}$, comprising M safe terms $t'_c \\in T_{safe}$. For more details of the synthesized dataset, please refer to subsection A.2."}, {"title": "3.2 INAPPROPRIATE CONCEPTS IDENTIFIER", "content": "To ensure the generation of safe and appropriate content, we detect and mitigate undesirable concepts within user prompts before they are processed by the text-to-image diffusion model. The core idea is that the prompt embeddings can be leveraged to represent individual terms or phrases, enabling precise identification of inappropriate content.\nThis task can be framed as classifying phrases as either appropriate or inappropriate. We employ a lightweight multi-layer perceptron model (MLP) to classify the embeddings of individual terms. Intuitively, we want to identify the phrase that precisely includes the inappropriate concept. For example, in the prompt \"a man got shot.\" only the term \u201cshot\" relates to the concept of violence. Additionally, the phrase \"screw you\u201d is related to the inappropriate concept \u201chate\u201d, but neither \u201cscrew\u201d nor \"you\" is related to the concept \"hate\". To address such cases, we utilize a sliding window technique to identify single terms and phrases that may collectively express inappropriate concepts.\nAs shown in Figure 1, the safe and unsafe phrases are first embedded by a text encoder and then used to train the identifier. We expect the identifier to detect the unsafe concepts defined in Equation 1. Let $T_{unsafe}$ represent the set of unsafe phrases. Specifically, the safe phrases set is composed of two subsets: $T_{safe}$ (synthetic set) and $T'_{safe}$ (collected from open-source resources). The MLP outputs the probability of a given phrase belonging to unsafe or safe. Denote the input phrase embeddings as $E_i$, the MLP output as $\\hat{y}_i$, and the true label as $y_i \\in \\{0, 1\\}$, where $y_1 = 1$ indicates an unsafe phrase and $y_i = 0$ indicates a safe phrase. The classification loss $L$ is defined as follows:\n$L = -\\frac{1}{N} \\sum_{i=1}^{N} (y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i))$"}, {"title": "3.3 STEERING TOWARD SAFE CONTENT", "content": "To steer the inappropriate prompts toward generating safe images, we propose to learn a linear transformation to the embedding of identified unsafe prompts. This transformation shifts unsafe concepts toward safe ones while preserving original semantic meaning. The intuition behind this approach is that linear transformations of word embeddings can effectively steer the generation"}, {"title": null, "content": "style of language models (Han et al., 2023). Since SteerDiff operates in word embedding space, applying these transformations enables us to adjust unsafe embeddings and steer the diffusion model toward producing safe outputs. Specifically, let $E$ denote the embedding of a word, we define the transformation as follows:\n$E_{steered} = \\epsilon W E_{unsafe} + (1 - \\epsilon) E_{unsafe}$"}, {"title": null, "content": "In this equation, $E_{steered}$ represents the adjusted embedding after steering, $E_{unsafe}$ refers to the original embedding of the unsafe phrases in $T_{unsafe}$, $\\epsilon$ is a steering parameter that controls the intensity of the transformation, and $W$ is a linear transformation matrix learned during training.\nThe key concept is to map the embedding of inappropriate content to safe content through a linear transformation. By applying Equation 3, we shift the embedding within the semantic space, guiding it toward regions associated with safe imagery. The steering parameter $\\epsilon$ offers fine-grained control over the degree of transformation, allowing flexibility in balancing semantic preservation and safety. To learn the transformation matrix $W$, we employ a supervised learning method using a paired dataset of unsafe phrases and their corresponding safe phrases, as described in subsection 3.1. The training process minimizes the following loss function:\n$L = |E_{safe} - W \\cdot E_{unsafe}|^2$"}, {"title": "3.4 INFERENCE", "content": "Once SteerDiff is trained, it can be seamlessly integrated into diffusion models without additional fine-tuning. In practical applications, SteerDiff can effectively detect the presence of blacklisted concepts and steer inappropriate prompts toward generating safe images."}, {"title": "4 EXPERIMENT", "content": null}, {"title": "4.1 EXPERIMENTAL SETUP AND METRICS", "content": "To evaluate the reduction of inappropriate content, we used Stable Diffusion (SD) v1.4 as the base model to generate three images for each prompt. We selected a general concept set $C_{unsafe}$, applicable across all categories of Inappropriate Image Prompts (I2P) dataset (Schramowski et al., 2023), as outlined in Equation 1. We evaluated inappropriate image content by combining two classifiers. First, Q16 classifier (Schramowski et al., 2022), which is also used for annotating LAION-5B dataset (Schuhmann et al., 2022), was utilized to detect a broad range of inappropriate content. Second, we applied NudeNet (Praneeth, 2019) to identify sexually explicit content. For nude content, we classified exposed buttocks, exposed breasts, exposed genitalia, and exposed anus content as inappropriate, while allowing otherwise provocative images (Schramowski et al., 2023; Wu et al., 2024; Ma et al., 2024). Unless otherwise specified, an image was classified as inappropriate if at least one of the classifiers flagged it as containing such content.\nFor the following RQ1, we evaluated SteerDiff on the I2P dataset and compared the probability of generating inappropriate images with state-of-the-art defense model Safe Latent Diffusion (SLD) under its MAX and STRONG configurations. In addition, we compared the probability of generating nude images with Erased Stable Diffusion (ESD) (Gandikota et al., 2023). For RQ2, we selected two state-of-the-art red teaming frameworks: Prompting4Debugging (P4D), representing a white-box attack, and SneakyPrompt, representing a black-box attack. Following standard instructions from P4D and Sneakyprompt, we conducted our evaluation on the I2P dataset using P4D testbed and NSFW_200 dataset (Yang et al., 2024) with Sneakyprompt, utilizing standard configurations provided by developers. For the diffusion process, we used the default configuration as outlined by stable-diffusion (Rombach et al., 2021). This approach ensures our experiments align with recommended practices to maintain consistency across all tests. For RQ3, we assessed image fidelity and text alignment across all models using prompts from the COCO 30K dataset."}, {"title": "4.2 RQ1: HOW EFFECTIVE IS STEERDIFF IN MITIGATING GENERATION OF INAPPROPRIATE CONTENT?", "content": "To investigate the ability of SteerDiff in identifying and steering inappropriate concepts, we started by demonstrating its effectiveness in reducing the generation of explicit content. We compared SteerDiff against ESD, SLD STRONG, and SLD MAX. Next, we expanded the scope of inappropriate concepts to $C_{unsafe}$ to investigate whether SteerDiff could effectively identify and steer prompts containing a wider range of inappropriate content toward generating safe images. In this evaluation, we evaluated SteerDiff with Stable Diffusion (SD) v1.4, naive blacklist, SLD STRONG, and SLD MAX approach. To minimize randomness and ensure more reliable results, all evaluated methods generated three images per prompt, as generating only one image might coincidentally omit inappropriate content. Notably, we used inappropriate terms from SteerDiff's training set as the naive blacklist."}, {"title": "4.2.1 EXPLICIT CONTENT REMOVAL", "content": "We evaluated the performance of SteerDiff and competitors on the I2P dataset, focusing on the sexual content category. In Figure 4, we compared the percentage change in nudity-classified samples with respect to SD v1.4. Our results show that, across all classes, our method demonstrates a more significant reduction in the generation of explicit content. Specifically, SD v1.4 generated 637 im-"}, {"title": "4.2.2 INAPPROPRIATE CONTENT REMOVAL", "content": "We further investigated a more comprehensive inappropriate set $C_{unsafe}$ defined in Equation 1. We began our evaluation by demonstrating the inappropriate generation of SD v1.4 without any safety measures, as well as a basic blacklist-based approach for prompt matching. Table 1 presents the probability of generating inappropriate content for each category. Varying from different categories, SD v1.4 generated inappropriate content with probabilities ranging from 18.16% to 46.29%. The naive blacklist approach slightly reduced probability, but inappropriate content was still generated in 17.86% of cases across all categories. While naive blacklists may mitigate some inappropriate image generation, it remains largely impractical as a comprehensive defense mechanism due to its inability to capture the diverse and evolving nature of unsafe content.\nNext, we demonstrated the probability of generating inappropriate content using SteerDiff, SLD STRONG, and SLD MAX across different I2P concepts. As shown in Table 1\u00b9, both SteerDiff and SLD MAX demonstrate the strongest performance, ranking first and second, respectively. Specifically, SteerDiff reduced the probability of generating inappropriate content by over 85%. In particular, SteerDiff outperformed its closest competitor, SLD MAX, in categories of violence, sexual content, and overall inappropriate content. As a result, only 5% of images generated by SteerDiff were still classified as inappropriate. However, it is worth noting that the Q16 and Nudenet classifiers tend to flag images as inappropriate even when problematic content has been significantly reduced. In summary, SteerDiff effectively mitigates the generation of inappropriate content in SD by identifying and modifying unsafe concepts within text embeddings."}, {"title": "4.3 RQ2: CAN STEERDIFF DEFEND AGAINST RED-TEAMING ATTACKS?", "content": "While SteerDiff was the state-of-the-art model when evaluating the I2P dataset, the robustness against red-teaming attacks is also a necessary factor of successful defense. Therefore, we com-"}, {"title": "4.4 RQ3: CAN STEERDIFF MAINTAIN HIGH IMAGE FIDELITY AND TEXT ALIGNMENT?", "content": "We have demonstrated the effectiveness of SteerDiff in mitigating generation of inappropriate content in T2I diffusion models. However, maintaining high image fidelity and ensuring strong alignment between generated images and input text prompts are equally important. Ideally, SteerDiff should have minimal or no impact on prompts that are already safe. To assess these aspects, we evaluated SteerDiff with COCO FID-30K score for image fidelity and CLIP score for measuring alignment between generated images and input text prompts.\nAs shown in Table 3, SteerDiff achieved a lower FID-30K score (15.45) compared to baseline mod-els\u00b2, indicating better image fidelity. While CLIP score (0.78) is slightly lower than SD1.4 and ESD,"}, {"title": "5 DISCUSSION ON CONCEPT FORGETTING", "content": "In this section, we explore the potential of SteerDiff to erase specific concepts during image generation. For this experiment, SteerDiff was applied to remove references to \u201cElon Musk\u201d and \u201cJoe Biden\u201d from user inputs. As shown in Figure 5, the first row illustrates that SteerDiff successfully removed these concepts from the generated images. Notably, SteerDiff effectively removed target concepts while preserving some attributes of the original concepts, such as hairstyle and distinctive clothing style. We also evaluated the method's effect on unrelated concepts, including \"Johnny Depp\", \"Emma Watson\u201d, \u201cTom Cruise\u201d, and \u201cMickey Mouse\u201d. Ideally, SteerDiff should have little to no impact on unrelated concepts. As outlined in the last two rows, SteerDiff preserved these unrelated concepts. This demonstrates that our approach can be effectively applied to selective concept forgetting without affecting other content."}, {"title": "6 CONCLUSION", "content": "The evolution of diffusion models in generating intricate images highlights both their potential and associated risks. Although recent unlearning methods for diffusion models have made notable progress in mitigating inappropriate content generation, red-teaming studies reveal that these defenses can still be bypassed. Moreover, many defense strategies rely on fine-tuning the model to avoid generating inappropriate content, which becomes increasingly challenging as diffusion models grow larger. In this paper, we present SteerDiff, a lightweight method that acts as an intermediary between the user's input and the diffusion model, ensuring that generated images comply with ethical"}, {"title": "7 ETHICS STATEMENT", "content": "In this work, we introduce a paradigm to identify inappropriate concepts within input prompts and steer their embeddings to mitigate the generation of unsafe content. Unlike previous methods that primarily focus on post-hoc prevention or concept removal, SteerDiff operates directly on the embeddings of the input prompts, prior to the diffusion process. By intervening earlier in the generation pipeline, we aim to more effectively prevent the propagation of unsafe content.\nHowever, the real-world application of SteerDiff requires carefully defining what constitutes inappropriate concepts, which may vary depending on the application domain. Defining these concepts is a non-trivial task and is likely to require input from human experts, potentially leading to subjective biases. These biases may stem from the social and cultural context in which the system is deployed, as the definition of inappropriateness is highly subjective and dependent on societal norms, which differ across regions and communities. Moreover, since the notion of inappropriateness is largely defined by social norms, the system's performance may vary depending on whose norms are reflected in the training data. This introduces the risk of reinforcing the biases present in the data, as SteerDiff may disproportionately represent the values of the social groups most prominent in the training set.\nOur testbed for evaluating inappropriateness is limited to specific, predefined concepts, which may not fully capture the diversity of opinions and sentiments regarding what is considered inappropriate. As societal norms evolve, so too must the definitions of inappropriateness used by SteerDiff. This necessitates regular updates to the training data and model parameters to ensure the continued relevance and fairness of the system.\nBeyond the identification and mitigation of inappropriate content, we believe that SteerDiff can be applied to other areas, such as concept or artistic style removal. As discussed in section 5, SteerDiff has the potential to be extended to various applications, including the removal of specific artistic styles or other undesired concepts in generative models. However, such applications must also carefully consider the ethical implications of content modification, as indiscriminate use of these techniques could lead to censorship or the suppression of artistic expression.\nIn summary, while SteerDiff offers a promising approach to mitigating unsafe content generation, its reliance on subjective definitions of inappropriateness and the potential for reinforcing societal biases limit its scope and fairness."}, {"title": "8 REPRODUCIBILITY STATEMENT", "content": "Upon acceptance of this paper, all relevant code and data used in our experiments will be made publicly available. The repository will include the source code for SteerDiff, as well as the datasets and instructions necessary to reproduce the results. This will ensure transparency and encourage further research in this domain."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 BACKGROUND", "content": null}, {"title": "A.1.1 TEXT-TO-IMAGE DIFFUSION MODEL", "content": "Text-to-image (T2I) generation has recently seen significant advancements with the advent of diffusion models (Rombach et al., 2022; 2021; DAL, 2022), which have shown remarkable ability to synthesize high-quality images from textual descriptions. Diffusion models operate by gradually denoising a noisy image, starting from pure Gaussian noise, until a coherent image is formed. The integration of textual information into the diffusion process differentiates T2I diffusion models from traditional diffusion models (Ho & Salimans, 2022), enabling the generation of images that align closely with a given textual input.\nIn T2I diffusion models, a pre-trained language model, such as CLIP (Radford et al., 2021), is commonly employed to convert text prompts into embeddings. These embeddings are then incorporated into the noise prediction model at various stages of the denoising process. By conditioning the image generation process on these embeddings, the model learns to generate images that not only match the content described in the prompt but also capture finer details of the semantics conveyed by the text. The use of large-scale pre-trained models allows for generalization across a wide range of prompts, enabling the synthesis of highly detailed and contextually appropriate images.\nMoreover, recent advances have introduced techniques such as classifier guidance (Ho & Salimans, 2022) and score-based models, which further improve the control over the generated images by adjusting the noise gradients based on the provided text. These methods have made T2I diffusion models not only capable of generating realistic images but also flexible in terms of creative control, making them highly valuable for various generative applications."}, {"title": "A.1.2 RED-TEAMING TEXT-TO-IMAGE DIFFUSION MODELS", "content": "While T2I diffusion models have demonstrated impressive capabilities, they also pose potential risks related to the generation of harmful or inappropriate content. Red-teaming (Ganguli et al., 2022; Perez et al., 2022; Li et al., 2024), the practice of stress-testing AI systems to uncover vulnerabilities, has become a crucial area of research in ensuring the safety and ethical use of these models. In the context of T2I diffusion models, red-teaming (Chin et al., 2023; Yang et al., 2024; Ma et al., 2024; Zhang et al., 2023b; Zhuang et al., 2023; Tsai et al., 2023) focuses on detecting and mitigating the generation of undesirable or unsafe content, such as offensive imagery or representations of inappropriate concepts.\nGiven that T2I diffusion models rely heavily on textual prompts to guide image generation, vulnerabilities often arise from the misalignment between the text embeddings and the visual content produced. Adversaries could potentially exploit these models by crafting malicious prompts that subtly bypass content filters. As a result, recent work has explored methods to steer these models away from generating unsafe outputs, such as by applying transformations to the prompt embeddings (Han et al., 2023), or by integrating ethical constraints into the generative process. The challenge lies in maintaining the model's ability to generate diverse and creative images while ensuring that the output adheres to safety guidelines. Addressing this issue is essential for deploying these models in real-world applications, particularly in sensitive domains such as media, art, and content moderation."}, {"title": "A.2 TRAINING DATA GENERATION", "content": "Following the description in subsubsection 3.1.1, we start from defining a concept set $C_{unsafe}$ and generate related terms $t_c$ entered around one sampled concept $c$ in the blacklist of $C_{unsafe}$. As introduced in the paper, we do this by prompting an LLM for prompt generation. We follow Latent Guard (Liu et al., 2024; Fan & Tao, 2024; Fan et al., 2024) in conditioning the T2I prompt. For a concept $c$, we prompt an LLM with the following text to generate $t_c$:"}, {"title": "A.3 RED-TEAMING ATTACK ANALYSIS", "content": null}, {"title": "A.4 VISUALIZATION OF STEERING", "content": "Figure 7 illustrates the outcome of applying the SteerDiff, which learns to project unsafe phrases into a safer latent space. In this 3D PCA visualization, the red points represent the original unsafe phrases, while the green points correspond to the steered versions of these phrases after transformation by SteerDiff."}, {"title": "A.5 LIMITATION", "content": "In addition to the ethical concerns discussed earlier, SteerDiff has certain technical limitations. One key limitation is the use of a hard-coded steering degree during the experiments, which may lead to over-steering in some cases. This is particularly problematic for prompts containing mildly inappropriate content, where excessive steering could result in unnecessary modifications, thereby altering the intended meaning of the prompt. A more dynamic and context-aware approach could mitigate this issue by adjusting the steering degree based on the severity of the unsafe content, ensuring more precise control without compromising the prompt's original intent."}, {"title": "A.6 EXAMPLES OF GENERATED IMAGES", "content": "In our evaluation of different models-SD v1.4, SteerDiff, ESD, SLD STRONG, and SLD MAX. We observe clear trade-offs between the ability to mitigate inappropriate content and the quality of generated images.\nSteerDiff consistently demonstrates superior mitigation of inappropriate content while preserving the generative capabilities of the diffusion model. The images produced by SteerDiff maintain higher fidelity, with details and clarity closely resembling those from the baseline SD v1.4, making it the most balanced approach in terms of both safety and image quality. On the other hand, ESD and SLD STRONG offer competitive image quality but occasionally fail to fully suppress inappropriate content generation. These models generate visually appealing images with relatively high quality, especially in complex textures and objects, but their inconsistency in filtering undesirable elements presents a notable shortcoming in safety-critical applications.\nSLD MAX achieves a mitigation performance comparable to SteerDiff in terms of defense against inappropriate content generation."}]}