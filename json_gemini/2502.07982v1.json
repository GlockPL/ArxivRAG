{"title": "Deep Semantic Graph Learning via LLM based Node Enhancement", "authors": ["Chuanqi Shi", "Yiyi Tao", "Hang Zhang", "Lun Wang", "Shaoshuai Du", "Yixian Shen", "Yanxin Shen"], "abstract": "Graph learning has attracted significant attention due to its widespread real-world applications. Current main-stream approaches rely on text node features and obtain initial node embeddings through shallow embedding learning using GNNs, which shows limitations in capturing deep textual seman-tics. Recent advances in Large Language Models (LLMs) have demonstrated superior capabilities in understanding text seman-tics, transforming traditional text feature processing. This paper proposes a novel framework that combines Graph Transformer architecture with LLM-enhanced node features. Specifically, we leverage LLMs to generate rich semantic representations of text nodes, which are then processed by a multi-head self-attention mechanism in the Graph Transformer to capture both local and global graph structural information. Our model utilizes the Transformer's attention mechanism to dynamically aggregate neighborhood information while preserving the semantic richness provided by LLM embeddings. Experimental results demonstrate that the LLM-enhanced node features significantly improve the performance of graph learning models on node classification tasks. This approach shows promising results across multiple graph learning tasks, offering a practical direction for combining graph networks with language models.", "sections": [{"title": "I. INTRODUCTION", "content": "Graphs are ubiquitous structures found across various do-mains, from protein molecule prediction to anomaly detection. Among the three most common types of graph attributes - text, image, and audio - textual attributes are particularly accessi-ble. For instance, in citation networks, node features can be derived from frequently occurring words in specific research domains. Given the widespread application of Text-Attributed Graphs (TAGs) in social network analysis and natural language processing tasks, developing effective methods for processing these graphs is crucial.\nCurrent approaches to handling TAGs primarily fall into two categories: structure-based random walks and deep neural net-works represented by Graph Neural Networks (GNNs[1]). The random walk-based methods, similar to word2vec, and GNNS establish non-contextual, surface-level connections between structure and attributes for node classification. However, recent studies indicate that these shallow, non-contextual embeddings often underperform in downstream tasks due to insufficient se-mantic information capture, particularly in handling polysemy and semantic relationships between words. Recent advances in Natural Language Processing (NLP[2]) have introduced contextual word embeddings like BERT[3] and DeBERTa[4]. The remarkable success of Large Language Models (LLMs[5]) like ChatGPT has revolutionized various NLP tasks. Unlike traditional shallow semantic feature extraction, LLMs are pretrained on vast text corpora, enabling rich semantic knowl-edge acquisition that can address the limitations of semantic information capture.\nThis raises an important question: Can we leverage LLMs as feature enhancers for textual information, and which GNN-based models would optimize node classification performance when combined with such enhanced features? In this work, we explore these questions, aiming to expand LLM applications in graph deep learning. We evaluate various LLMs against tra-ditional shallow embedding methods[6,7] to investigate which information better facilitates understanding and utilization of graph structure and textual attributes. Our primary contribu-tion lies in implementing and validating LLMs as attribute enhancers for node text properties through comprehensive experimental analysis.\nWe propose a framework that utilizes LLMs as attribute enhancers for node text properties, demonstrating how this enhancement can improve the quality of node repre-sentations.\nThrough comprehensive experimental analysis, we iden-tify that Graph Transformer-based models perform opti-mally among GNN-based methods when combined with LLM-enhanced features.\nWe provide empirical evidence showing how LLM-enhanced text attributes can better capture semantic rela-tionships compared to traditional shallow embeddings."}, {"title": "II. PRELIMINARIES", "content": "TAGS\nA text-attributed graph can be formally represented as G = (V, E, X), where: V = {v_1, v_2, ..., v_n} represents the set of nodes with |V| = n. E \u2286 V \u00d7 V denotes the set of edges. X \u2208 R^{nxd} is the text attribute matrix, where each row x_i \u2208 R^d represents the textual features associated with node v_i. For TAGs, each node v_i is associated with textual content, which could be derived from various sources such as document descriptions, user profiles, or article abstracts. These textual attributes play a crucial role in understanding node characteristics and their relationships within the graph structure. The adjacency matrix A \u2208 R^{nxn} of graph G is defined such that A_{ij} = 1 if there exists an edge between nodes v_i and v_j, and A_{ij} = 0 otherwise.\nGNNS.\nGNNs operate through an iterative process of message passing and node updating. The basic formulation can be expressed by two key equations. First, for each node i at layer l, the message passing (or aggregation) step collects information from its neighboring nodes N(i) according to:\nm_i^{(l)} = AGGREGATE^{(l)}(\\{h_j^{(l-1)} : j\u2208N(i)\\})\nFollowing this, the node update step combines the aggre-gated message with the node's previous representation:\nh_i^{(l)} = UPDATE^{(l)}(h_i^{(l-1)}, m_i^{(l)})\nHere, h_i^{(l)} denotes the feature vector of node i at layer l, while m_i^{(l)} represents the aggregated message. The AGGRE-GATE function can take various forms such as mean, sum, or max pooling operations, while the UPDATE function typically implements a neural network transformation. Through these operations, each node iteratively updates its representation by incorporating information from its local neighborhood struc-ture, enabling the network to learn increasingly sophisticated node representations that capture both node features and graph topology.\nNode Classification on TAGS\nConsider a set of nodes V_s with their corresponding labels Y_i, where y_i represents the true label of node v_i. Our objective is to construct a model based on the set V_s and their text attributes to predict the labels y_u for the remaining unlabeled node set V_u. To illustrate this approach, we utilize the Cora citation network dataset. In this network, each node represents an academic paper from one of seven domains, and each paper is characterized by a binary feature vector. These features are derived from the presence of specific keywords in the text: a value of 1 indicates the presence of a keyword, while 0 indicates its absence. The edges in the network represent citation relationships between papers. The primary task is to accurately classify these papers into their respective domains.\nLLMS\nIn this study, we concentrate on LLMs, defined as transformer-based architectures pre-trained on massive-scale text corpora. At the core of these models is the self-attention mechanism, defined as:\nAttention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\nwhere Q, K, and V represent Query, Key, and Value matrices respectively, and d_k is the dimension of the key vectors."}, {"title": "III. FEATURE-LEVEL ENHANCEMENT", "content": "This section explores the integration of LLMs and GNNs at the feature level. Specifically, we propose a framework where GNNs utilize LLM-generated embeddings as initial node features to produce classification results. We begin our discussion with a detailed description of the datasets. Datasets\nIn this study, we adopt Cora, Pubmed[12], two popular bench marks for node classification. We establish two distinct data splitting protocols to investigate the impact of differ-ent partitioning strategies on model performance. The first protocol implements a widely-adopted low-label ratio setting, where 20 nodes per class are randomly selected for training, 500 nodes for validation, and 1000 nodes for testing from the remaining pool. The second protocol adopts a high-label ratio setting, with a 60-20-20 split for training, validation, and test sets, respectively. The models were trained for 300 epochs with early stopping (patience=10). We used Adam optimizer (lr=0.01, weight-decay=5e-4), 4-layer architecture with 64 hidden dimensions and dropout=0.5. All experiments were repeated across 5 random seeds to ensure statistical significance. All experimental results reported in subsequent sections represent the average performance across three inde-pendent runs.\nBaseline\nIn this study, we investigate the feature-level enhancement of node attributes using LLMs through three key compo-nents: (1) GNN architecture selection, (2) LLM selection, and (3) integration framework design. For GNN architectures, we implement GCN and Graph Transformer on both Cora and Pubmed datasets, while including Multi-Layer Percep-tron (MLP) baselines to evaluate embedding quality without graph aggregation. For LLM selection, we consider three categories of models with accessible embeddings: First, fixed PLMs/LLMs without fine-tuning, including DeBERTa and LLaMA[13] (implemented via LLaMA-cpp3, using [EOS] token embeddings); Second, local sentence embedding models, specifically Sentence-BERT[14] (a popular lightweight model) and E5-large[15] (current state-of-the-art on MTEB); Third, online sentence embedding services, namely OpenAI's text-ada-embedding-002[16] and Google's PaLM-Cortex-001[17]. For baseline comparison, we include non-contextualized shal-low embeddings, using TF-IDF for Pubmed's raw text at-tributes. The performance results of various combinations between text encoders and GNNs are presented in Tables 1 to 4, where MLP performance metrics provide insights into the intrinsic quality of text embeddings prior to graph aggregation."}, {"title": "IV. NODE CLASSIFICATION PERFORMANCE COMPARISON", "content": "As show in Table 1 to 4, the experimental evaluation presents a nuanced examination of how LLMs enhance node representations in graph learning tasks, with results reveal-ing complex interactions between model architectures, data availability, and embedding approaches. Our analysis spans two influential benchmark datasets, PubMed and Cora, under distinct label ratio settings, comparing various models across GCN, MLP, and Graph Transformers. In the high-label ratio setting (60-20-20 split), LLM-enhanced features demonstrate superior performance across both datasets, highlighting the advantages of leveraging pre-trained language models for node representation. On PubMed, Google's language model achieves the highest accuracy of 81.38% with Graph Trans-former architecture, substantially outperforming traditional Word embeddings (68.84%). This significant performance gap of 12.54 percentage points underscores the value of rich semantic understanding provided by LLMs. SBERT and ADA also show impressive results, reaching 79.92% and 80.27% respectively, suggesting that different LLM architectures can effectively capture domain-specific semantic information. The Cora dataset exhibits similar patterns, with SBERT and ADA achieving top performances of 82.31% and 82.20% respec-tively using Graph Transformer architecture, reinforcing the generalizability of LLM-enhanced features across different scientific document networks. In low-label settings (20 nodes per class), performance decreases notably across all models, with SBERT-GCN achieving 65.90% accuracy on PubMed. Interestingly, the Graph Transformer's superiority becomes less pronounced and sometimes reverses, as demonstrated by SBERT-GCN (65.90%) outperforming its Graph Transformer counterpart (64.32%). This suggests that complex architectures like Graph Transformers may struggle to fully utilize their capacity when training data is limited, despite their theoret-ical advantages. The comparative analysis shows that Graph Transformers perform best with abundant labels but struggle with limited data, while GCNs maintain consistent perfor-mance across data scenarios when paired with LLM features. MLPs consistently underperform, highlighting the value of graph structural information. Regarding LLM performance, Google's model excels with abundant data but degrades under constraints, while SBERT maintains consistent performance across scenarios. Traditional Word embeddings consistently underperform, demonstrating their limitations in capturing complex document relationships. These findings suggest that while LLMs enhance graph learning tasks, their effectiveness depends on data availability. Architecture selection should be guided by application context and data constraints, with simpler models often being more suitable for low-resource scenarios."}, {"title": "V. CONCLUSION", "content": "Our study demonstrates the effectiveness of Large Language Models (LLMs) in enhancing graph learning through improved node representations. While LLM-enhanced features consis-tently outperform traditional word embeddings, their optimal utilization depends on data availability. In high-label settings, Graph Transformers with LLM features achieve superior per-formance, as shown by Google's model reaching 81.38% accuracy on PubMed. However, in low-label scenarios, simpler architectures like GCN often prove more effective. These findings suggest that successful integration of LLMs in graph learning requires careful consideration of both architectural choices and data constraints, opening new avenues for future research in this promising direction."}]}