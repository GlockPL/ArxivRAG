{"title": "AN EVOLVED UNIVERSAL TRANSFORMER MEMORY", "authors": ["Edoardo Cetin", "Qi Sun", "Tianyu Zhao", "Yujin Tang"], "abstract": "Prior methods propose to offset the escalating costs of modern foundation mod-\nels by dropping specific parts of their contexts with hand-designed rules, while\nattempting to preserve their original performance. We overcome this trade-off\nwith Neural Attention Memory Models (NAMMs), introducing a learned network\nfor memory management that improves both the performance and efficiency of\ntransformers. We evolve NAMMs atop pre-trained transformers to provide differ-\nent latent contexts focusing on the most relevant information for individual layers\nand attention heads. NAMMs are universally applicable to any model using self-\nattention as they condition exclusively on the values in the produced attention ma-\ntrices. Learning NAMMs on a small set of problems, we achieve substantial per-\nformance improvements across multiple long-context benchmarks while cutting\nthe model's input contexts up to a fraction of the original sizes. We show the gen-\nerality of our conditioning enables zero-shot transfer of NAMMs trained only on\nlanguage to entirely new transformer architectures even across input modalities,\nwith their benefits carrying over to vision and reinforcement learning. Our source\ncode is available at https://github.com/SakanaAI/evo-memory.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformer architectures have be-\ncome the golden standard in deep\nlearning, with ubiquitous applica-\ntions in the design of modern foun-\ndation models, exhibiting exceptional\nperformance and scalability (Achiam\net al., 2023; Das et al., 2023; Team\net al., 2023; Dosovitskiy et al., 2020;\nChen et al., 2021a; Brohan et al.,\n2023; Gur et al., 2023). The out-\nputs of a transformer are exclusively\nconditioned on a recent context of in-\nput tokens, which for language mod-\nels (LMs) generally correspond to a\nwindow of preceding words. Thus,\naddressing the challenge of extend-\ning this context window is critical\nto enable tackling long-range tasks\nand is currently a focal area of re-\nsearch (Huang et al., 2023). However, long contexts also immediately impact training and inference\ncosts, with modern foundation models being increasingly resource-hungry and expensive. Many\nrecent methods proposed to partially offset these costs by studying how to heuristically quantify the\nimportance of each token stored in the model's latent memory, i.e., stored in its Key-Value (KV)\ncache. Then, by simply evicting the least important tokens with hand-designed strategies, they have\nshown early success at reducing memory size while limiting performance losses (Luohe et al., 2024).\nOur research aims to go beyond these hand-designed strategies as we hypothesize that shaping the\nlatent memory KV cache of transformers entails new opportunities to improve their capabilities in"}, {"title": "2 BACKGROUND AND PRELIMINARIES", "content": "Attention and transformers. Transformers are neural network architectures designed specifically\nfor efficiently processing input sequences. These models take as input a stream of tokens (e.g.,\nembeddings of words, image patches, robotic states, etc.) and, produce a set of latents with the same\nlength within their layers. Multi-headed dot product attention (Vaswani et al., 2017), or simply"}, {"title": "3 NEURAL ATTENTION MEMORY MODELS", "content": "An immediate limitation of transformers is the quadratic costs associated with computing the atten-\ntion matrix A. To partially address this issue, during auto-regressive generation, the latents for the\nkeys and values of the tokens generated at the previous steps are usually stored in what is referred to\nas the KV cache. This object can be regarded as being analogous to the memory of the transformer,\nwhich now, at each step, only needs to compute the query, key, and value of the latest token and\nperform attention over a horizontal vector by exploiting causal ordering. In this section, we describe\nthe feature extraction, architecture, and optimization of NAMMs, which have been designed to act\non the KV cache to improve both the performance and practicality of this powerful class of models."}, {"title": "3.1 ATTENTION SPECTROGRAMS FOR MODEL-AGNOSTIC FEATURE EXTRACTION", "content": "The feature extraction framework of NAMMs is designed to be agnostic to the parameterization of\nthe base transformer they are applied for. In particular, we build a representation for each token in\nthe current KV cache memory from its corresponding column vector in the attention matrix \u0391\u03b3. \u03a4\u03bf\nmeaningfully compress this unbounded vector signal, we process it via an STFT with a fixed-sized\nHann window. This operation produces a spectrogram representation of the attention\ncolumns w, representing the frequencies with how the queries attend to each of the stored key\ntokens (indexed by i) on a compressed time-axis (indexed by t). Thus, this representation exposes\nprecisely the knowledge of how each token's relative importance varies across all past queries in a\ncompact form factor, discarding all other information specific to the learned transformer weights.\nAs NAMMs rely only on the attention values for their input, they are universally applicable to any\nlayer producing an attention matrix. This property is crucial, enabling us to avoid learning individual\nmemory models for the different layers of a transformer, thus, greatly limiting the number of total\noptimized parameters. Furthermore, it also allows efficient training on top of smaller foundation\nmodels for targeted problems, and later transferring the resulting models zero-shot at test-time to\nlarger architectures and arbitrary applications."}, {"title": "3.2 \u039c\u0395\u039cORY MODEL DESIGN AND CROSS-TOKEN COMMUNICATION", "content": "NAMMS parameterize a small neural network m\u00f8 to output a scalar\nselection score si = mp(w:T) for each ith token in the KV cache.\nFirst, to obtain a consistent input dimension, we reduce the atten-\ntion spectrogram into a smaller feature vector wi by compressing\nthe time-axis via an element-wise exponentially moving average\n(EMA: Wi = \u03a3\u03c4\u03b3\u03c9; Figure 2, center). We then append po-\nsitional encodings and feed the vector wi to the memory model's\nnetwork mo to produce the score si. Finally, we evict from the KV\ncache memory all latent tokens with si < 0, effectively treating the\nproblem as a binary classification task. We repeat this process with\na fixed interval, every set number of new input tokens, Nup.\nBackward attention memory models (BAM). For the design of\nmo, we posit that sharing information from all tokens in memory\ncould be key for assessing their importance. A particularly motivat-\ning scenario in LMs arises when considering the case of repeated\nwords or sentences, where learning a diversity measure that com-\npares different tokens would allow preventing redundancies in the\nKV cache. Corroborating this intuition, even from a biological per-\nspective, memory formation and retention appear to adhere to mod-\nels of neuronal competition (Han et al., 2007).\nBased on these considerations, we design the backward attention\nmemory architecture (BAM) for parameter-efficient sharing of in-\nformation while making use of the powerful inductive biases en-\nabled by the masked self-attention operation. In particular, we im-\nplement mo via an initial self-attention layer with a counter-causal mask M, which we refer to as\nbackward. This design serves to introduce a purposeful asymmetric relationship, allowing\nto distinguish between older and newer tokens. We then output si from a final linear operation:", "equations": ["attentionM (Q, K, V) = AV, where, A = softmax M \\frac{QKT}{\\sqrt{d}} (1)", "w*[n] = \\sum_{t'=0}^{T}v[t']w[t - t']e^{w}. (2)", "Oi = attention(\u039a\u03a9, Vn, Qn), si = linear(or). (3)"]}, {"title": "3.3 TRAINING NAMMS WITH INCREMENTAL EVOLUTION", "content": "We evolve the network weights of our NAMMs to directly optimize the performance on a subset of\nlong-context language modeling tasks from LongBench (Bai et al., 2023). As we share a single mo\nacross all layers, even with our largest NAMM we only evolve about 4000 total parameters. We use\nthe seminal CMA-ES optimization algorithm (Hansen, 2006) and apply NAMM atop a Llama 3 8B\nbase model (Dubey et al., 2024) with a context extended from 8192 to 32768 tokens via NTK-aware\npositional interpolation (bloc97, 2023). Due to the inference costs of LMs with long inputs, we\nsample a subset of different prompts from each task in each generation and propose training in an\nincremental fashion: starting from a single task, and adding additional tasks at later training stages.\nEmpirically, we found both these choices to provide effective regularization, improving generaliza-\ntion (see Appendix C). The performance of modern LMs on LongBench varies considerably across\ntasks, and even across different task prompts. Hence, instead of using the raw scores, we opt to max-\nimize normalized performance relative to the vanilla base model's stored evaluation performance on\neach same subset of prompts, retaining all tokens in its KV cache memory.\nWe choose three tasks from different LongBench categories across both English and Chinese where\nthe Llama 3 base model seems to particularly struggle: PassageRetrieval-en, DuReader, and Narra-\ntiveQA. We evolve our NAMM for 300 generations in its first incremental phase, 250 in its second,\nand 120 in its third. We diminish the number of generations to counteract the increasing costs\nwith each additional phase and make more efficient use of computational resources. At the end of\neach phase, we resume from the best previous checkpoint. We provide training curves of our main\nbackward-attention model in Figure 4, showing the average and standard deviation of the normalized\nbatch performance across the population (left), together with the normalized per-task and average\nperformance on all samples of the optimized mean from CMA-ES (right). We refer to Appendix A\nfor additional architectural and optimization details, together with the set of hyper-parameters. We\nalso provide additional statistics and training curves for other memory model designs in Appendix C."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "In this section, we evaluate and analyze evolved NAMMs as compared to full-context transformers\nand two recent hand-designed methods for KV cache management: H2O (Zhang et al., 2024c) and\nL2 (Devoto et al., 2024). We compare each method in terms of absolute and normalized perfor-\nmance and also provide the resulting average cache size recorded at the end of each prompt. We\nfirst consider three long-context language modeling benchmarks spanning 36 diverse tasks in three\nlanguages, using the same Llama 3 8B base transformer from training. Then, we evaluate the capa-\nbilities of zero-shot transferring NAMMs to other unseen transformers and task domains. In particu-\nlar, we not only consider transfer to larger LMs, but also transformers with tokens constructed from\nmodalities other than language. Across all these settings, we also compare BAM with a simpler 2-\nlayer MLP architecture and provide summarized results after every stage of incremental evolutions.\nWe refer to Appendix C additional evaluations, ablation studies, and all learning curves. Lastly, we\nperform targeted qualitative analysis to understand the behavior of our new memory framework."}, {"title": "4.1 LONG-CONTEXT LANGUAGE UNDERSTANDING", "content": "Longbench. In Table 2, we provide results across all LongBench tasks (Bai et al., 2023). Our\nNAMM yields concrete improvements to the Llama 3 8B transformer both when considering the full\nset or exclusively the held-out set of test tasks that were not used for evolution, with improvements\nof 11% and 7% respectively. At the same time, our NAMM also yields efficiency side benefits,\nnotably reducing the context-extended KV cache size. Instead, while both H2O and L2 produce even\nsmaller cache sizes, they both come with some performance costs - in line with their stated objective\nof retaining rather than improving the original full-context performance. These results indicate how\ntwo different hand-designed extremes, either retaining all tokens or aggressively dropping them,\ncome with their own set of downsides for either efficiency or performance. On the other hand,\nNAMMs are the only methods making consistent improvements from the base model across both\naxes, highlighting how end-to-end evolutionary optimization can open new orthogonal directions\nbeyond what is feasible with manually-designed heuristics.\nInfiniteBench. In Table 3, we provide results across the InfiniteBench tasks (Zhang et al., 2024a). In\nthis benchmark, the average prompt length is close to 200K tokens making it extremely challenging,\nespecially for LMs that were not expensively finetuned for very long context understanding. In fact,\nas reported by Zhang et al. (2024a), even GPT4 (Achiam et al., 2023) cannot exceed a performance\nof 1% on some of its problems. In line with these results, the full-context Llama 3 together with\nH2O and L2 obtain near-zero performance on most tasks. Instead, our NAMM provides outstanding\nimprovements, bringing overall benchmark performance from 1.05% to 11%. We also observe that\nwhile our NAMM's memory size is larger than for LongBench, it is considerably lower in relation to\nthe base model's (now only 40%). This result suggests that NAMMs emergently learned a scalable\nmemory strategy, forgetting redundant and detrimental information at an increasing rate with longer\ncontexts without requiring the hand-designed hard cache limits enforced by L2 and H2O.\nChouBun. Our new\nbenchmark focuses on\ntasks designed exclu-\nsively in Japanese, a novel\nlanguage unseen during\nNAMMs training. We hope\nthis benchmark might itself"}, {"title": "4.2 ZERO-SHOT TRANSFER ACROSS ARCHITECTURES AND MODALITIES", "content": "Cross-scale adaptation. In Table 5, we provide results zero-shot transferring our NAMM from the\nLlama 3 8B to the Llama 3 70B model on LongBench. Across all tasks, we find performance to be\nvery close to the full-context baseline with an overall gap of less than 1% even for the test subset.\nWhile NAMMs are not able to improve the overall full-context performance in this first transfer\nsetting outside specific task categories (e.g., coding and few-shot learning), they still outperform\nboth H2O and L2 baselines and retain a similar efficiency as with their original training transformer.\nVision Language Understanding. In Ta-\nble 6, we provide results zero-shot trans-\nferring to the computer vision domain,\nevaluating NAMMs with a Llava Next\nVideo 7B model (Zhang et al., 2024b)\non LongVideoBench (Wu et al., 2024)\nand Multi-Task Long Video Understand-\ning (MLVU) (Zhou et al., 2024). As when evaluated atop Llama 8B, our NAMM is the only method\nrecording gains over the full-context base transformer in both benchmarks. Furthermore, we find\nthat NAMMs learns to forget almost exclusively parts of redundant video frames rather than the\nlanguage tokens describing the final prompt, even though they were never faced with such modal-\nity during training. This result validates that our NAMM recovered a domain-agnostic memory\nmanagement strategy, further highlighting their flexibility.\nReinforcement learning. In Table 7, we provide our zero-shot transfer results for the offline re-\ninforcement learning setting, where we apply NAMMs atop a decision transformer (Chen et al.,\n2021b) and consider the canonical continuous-control tasks from the D4RL benchmark (Fu et al.,\n2020). We find our NAMM improves the base transformer quite considerably in this domain across\neight out of nine offline tasks with over 9% overall gains, opposing the performance loss of the other\nefficient baselines. We posit that since the nature of the decision transformer optimization is closely\ntied to behavior cloning, the ability to discard part of the context is likely to allow NAMMs to forget"}, {"title": "4.3 UNDERSTANDING NEURAL ATTENTION MEMORY MODELS", "content": "Influence of layer depth. We begin analyzing NAMMs by focusing on the final amount of retained\ntokens and their oldness. At the top of Figure 5, we provide these normalized metrics as a function\nof layer depth. Interestingly, our learned NAMM does not appear to affect the KV cache uniformly,\nretaining visibly more and older tokens for some of the early-middle layers of the base transformer."}, {"title": "5 RELATED WORKS", "content": "Devoto et al. (2024) and Yao et al. (2024) try to identify the least important tokens to evict using\nheuristics such as L2 magnitude and entropy. Alternative strategies include considering simple\nstatistics from the attention matrix (Liu et al., 2024; Oren et al., 2024; Zhang et al., 2024c). Ge\net al. (2024) and Li et al. (2024b) build on these ideas by applying multiple strategies based on\nmatching specific attention patterns. Unlike this prior work, our approach uniquely employs a black-box model to learn token-level memory management and shows potential for improving both the\nperformance and efficiency of transformers. We refer to App. E for references and connections to\nthe wider literature, including efficient architectures, memory, and evolution."}, {"title": "6 DISCUSSION AND FUTURE WORK", "content": "This work introduced Neural Attention Memory Models, providing a new framework to enhance the\nperformance of transformers while significantly reducing memory footprint. By evolving NAMMS\non top of pre-trained LMs, we demonstrated their effectiveness across diverse long-context tasks\nin three languages, significantly surpassing previous hand-designed KV cache eviction frequently\nhindering performance, and the original model relying on costly full-context conditioning. Our\ncarefully designed approach also enabled NAMMs, trained solely on language tasks, to achieve\nzero-shot transferability across architectures, input modalities, and task domains. This work has\nonly begun to explore the design space of our memory models, which we anticipate might offer many\nnew opportunities to advance future generations of transformers. In this regard, we believe NAMMS\nshould not be viewed as a replacement for gradient-based optimization, but rather an orthogonal\nframework that could be combined and alternated with parameter fine-tuning. Such an extension\nhas the potential to unlock efficient long-context training, drawing parallels to the iterative process\nof learning and evolution that shaped human memory."}, {"title": "7 AUTHOR CONTRIBUTIONS", "content": "Edoardo Cetin initiated the project, led the design and implementation of NAMMs, and provided\nmajor contributions to writing. Qi Sun designed and implemented the zero-shot transfer experiments\nwith Llama 3 70B and Llava Next Video 7B, and provided contributions and feedback to writing.\nTianyu Zhao devised and implemented ChouBun, and provided contributions and feedback to writ-\ning. Yujin Tang coordinated the project, gave key advice for the design of NAMMs, and provided\nmajor contributions to writing."}, {"title": "A IMPLEMENTATION DETAILS", "content": "A.1 MODEL SPECIFICS AND NAMMS EXECUTION\nWe evolve our Neural Attention Memory Models on top of a context-extended Llama 3 8B (Dubey\net al., 2024) base model. In particular, we employ the NTK-aware positional interpolation strategy\n(bloc97, 2023) to extend the context by four times from 8192 to 32768. Unlike prior strategies that\nrequire further gradient fine-tuning to avoid performance collapse (Chen et al., 2023), NTK-aware\npositional interpolation has been shown to produce sensible results even when applied zero-shot. In\ncase the length of a task prompt still exceeds 32768 we perform mid-sentence cropping (Xiao et al.,\n2023; Jin et al., 2024), as standard in long-context LM evaluation (Bai et al., 2023; Zhang et al.,\n2024a).\nWhen applying NAMMs, we only affect the execution of the base model with a fixed frequency,\nonce every nup = 512 steps. When feeding longer prompts to our model, we simply split the tokens\ninto nup-sized chunks. We note that due to modern frameworks being bound primarily by memory\nconstraints, input-splitting in itself has minimal effects on running time, with similar approaches\nbeing already performed under the hood by established kernel procedures (Dao et al., 2022).\nA.2 FEATURE EXTRACTION AND ARCHITECTURE DETAILS\nOur new feature extraction framework is a key component for enabling the transfer properties of\nNAMMs. In practice, we extract the attention spectrogram from the real-valued attention matrix\nusing a Hann window of size nw = 32, resulting in just seventeen complex-values frequencies that"}, {"title": "A.3 ZERO-SHOT TRANSFER", "content": "For our zero-shot transfer experiments, we consider a Llama 3 transformer with 70B parame-\nters (Dubey et al., 2024), a Llava Next Video transformer with 7B parameters (Zhang et al., 2024b),\nand a decision transformer (Chen et al., 2021b) with about 1M parameters. For our 70B experiments,\nwe follow the exact same setup as when evaluating our 7B Llama model used in training. For our\nvideo-language model, we extract 12 \u00d7 12 image tokens from 48 uniformly sampled frames, 6912\nin total. We also slightly shift the selection score threshold by 5, to counteract the lower number"}, {"title": "C ADDITIONAL RESULTS", "content": "C.1 PERFORMANCE ACROSS INCREMENTAL STAGES AND ARCHITECTURES\nWe provide additional results and analysis to the summarized one, complementing Section 4, with\nthe detailed performance across different NAMMs, evaluating the best checkpoints after each stage\nof incremental training stage, and ablating the BAM architecture with an MLP.\nExtended language modeling results. We report our results for LongBench, InfiniteBench, and\nChouBun in Tables 12, 13, 14. First, we note that even training on a single task with our simple\nMLP architecture impressively improves performance across all benchmarks. Additionally, perfor-\nmance across benchmarks sees near-monotonic further improvements with each stage of our incre-\nmental evolution recipe. Comparing our implementations, we note that the performance benefits\nfrom the memory models with backward attention are consistently superior to the fully connected\nvariant in both initial stages of incremental training, empirically validating our hypothesis about the\nimportance of global KV cache information for determining the importance of each token. Lastly,\non ChouBun. we observe that the performance with BAM sees a notable upswing after the second\nstage of incremental training, which might be associated with the introduction of another ideogram-\nbased language in the training set. The same improvement not occurring with the MLP-based\nNAMMs might be further evidence of architectural performance saturation, highlighting once again\nthe effectiveness of our main implementation design.\nExtended zero-shot transfer results. We report our extended zero-shot transfer results for the 70B\nmodel and the offline RL setting in Tables 15, 16, and 17. We see the benefits from NAMMs again\nincrease as we incorporate backward attention, and with each stage of incremental training to a sim-\nilar extent as with the language modeling tasks. These results further highlight the potential benefits\nof scaling up the architecture of our memory model and increasing the number of incremental stages.\nC.4 INCREMENTAL TRAINING ABLATION\nWe provide a full set of ablations results for our incremental training strategy, training a Neural\nAttention Memory Model with the BAM architecture from scratch on both the PassageRetrieval-en\nand DuReader tasks, as employed during the second stage of incremental learning. We evolve this\nNeural Attention Memory Model for 360 consecutive generations and provide training curves in\nFigure 12. In the left sub-plot, we show the average and standard deviation of the normalized batch\nperformance across the population, in the center sub-plot, we show the normalized per-task and\naverage performance on all samples of the optimized mean from CMA-ES, and on the right subplot\nwe show the corresponding memory size. Furthermore, in Table 18, we provide the full LongBench\nevaluation results for this baseline, also showing our original incremental model's performance for\nease of comparison. Interestingly, the non-incremental NAMM obtained a notably higher score on\nthe training tasks with a normalized performance of 1.57, in contrast to the normalized performance\nof 1.41 achieved by the best checkpoint from the second incremental training stage. Yet, outside\nthe PassageRetrieval-en and DuReader tasks, its performance is notably inferior and very close to\nthe original performance of the base model. These results appear to indicate that the usefulness of\nincremental training goes beyond the faster evolution provided by reducing the number of evaluation\nprompts to assess performance and that this strategy plays an important role in regularizing evolution\nand making Neural Attention Memory Models effectively generalize to new tasks."}, {"title": "D ADDITIONAL ANALYSIS", "content": "D.1 SENSITIVITY TO ATTENTION FREQUENCIES AND POSITIONAL ENCODINGS\nWe analyze the magnitudes of the gradients of the token scores si with respect to each dimension\nin the token feature vectors. This procedure quantifies how varying each dimension in our attention\nspectrogram representation locally affects the output score of NAMMs, thus, providing a heuristic\nmeasure of its relevance (since scores determine which tokens get discarded). In Figure 13, we plot\nthe distribution of magnitudes for all the seventeen features up to the Nyquist frequency (0 to 16)\nin the attention spectrogram. All frequency distributions seem to cover a wide range of values, with\neach mean being close to the global mean, seemingly indicating NAMMs learn to make use of all\navailable spectrogram information for at least some of the tokens. Additionally, we note that many\nof the higher frequencies have distributions with higher means and larger tails than the 'ground\nfrequency' at dimension 0. Furthermore, as shown in the rightmost-lower subplot, NAMMs appear\nvisibly less sensitive to recency information provided by the concatenated positional embeddings,\nwith a lower total influence than frequency information on token scores. Overall, these observations\nseem to further validate the importance of going beyond simple hand-designed methods solely based\non token recency and the sum of the attention values, which has so far been considered a strong\nestablished recipe for KV cache management."}, {"title": "E EXTENDED RELATED WORKS", "content": "Similar to our NAMMs implementation, memory management through token eviction has been ex-\nplored mostly to reduce memory constraints and enable querying LMs with longer contexts (Luohe\net al., 2024). Commonly, strategies entail simply cropping input prompts to a shorter length, of-\nten more effective when done from the middle rather than the ends (Xiao et al., 2023; Jin et al.,\n2024). More advanced, several heuristic strategies have been proposed to identify and evict the least\nimportant tokens in the KV cache, selectively pruning it to a fixed size for each layer. These strate-\ngies assess token relevance using metrics like L2 magnitude or entropy , or analyze statistics from the attention matrix, such as value magnitude or cumulative\nsums (Liu et al., 2024; Oren et al., 2024; Zhang et al., 2024c). Building on these ideas, Ge et al.\n(2024) and Li et al. (2024b) apply multiple strategies simultaneously, choosing the best fit for each\nlayer by matching them with specific attention patterns. However, unlike previous work, our ap-\nproach uniquely employs a black-box model to learn KV cache management, aiming to enhance\nefficiency and boost performance.\nMany other methods to reduce memory consumption, affecting the KV cache, are mostly orthogonal\nand likely complementary to our approach. For instance, MQA and GQA propose merging different attention heads during the training of LLMs, either fully or\npartially, to improve deployment-time throughput. Brandon et al. (2024), pushed these strategies fur-\nther, attempting to merge heads even across different layers. GQA is commonly employed in many\nmodern LMs, including the LLama 3 family of models which we use to train and evaluate NAMMS\non language tasks (Dubey et al., 2024). Furthermore, several methods have looked at KV cache\ncompression through either quantization of the keys and values (Hooper et al., 2024; Dong et al.,\n2024a;b) or even the whole hidden states (DeepSeek-AI et al., 2024). Similarly to the aforemen-\ntioned prior work concerning KV cache pruning, these methods considered mainly hand-designed\nstrategies, such as employing different quantization rates based on heuristically recognizing impor-\ntant tokens. We note that using evolution to optimize for which channels to merge or compress could\nalso yield new interesting unexplored approaches, combining these orthogonal directions with some\nof the principles introduced by NAMMs.\nThere has also been much research interest in exploring new architectures to explicitly model com-\nponents of a memory system or to address key challenges of reasoning over longer contexts. For\ninstance, past work has looked at incorporating neural models of memory within neural networks by\nimplementing different reading and writing operations - either directly replacing their layers (We-\nston et al., 2014; Sukhbaatar et al., 2015), or introducing new auxiliary components (Rae et al., 2016;\nLample et al., 2019). In relation to transformers, more recent works have been proposed rethinking\nthe ingredients of the self-attention operation, mostly in the context of LMs. These works looked\nat either efficient linear approximation to self-attention to overcome quadratic costs (Beltagy et al.,\n2020; Katharopoulos et al., 2020; Wang et al., 2020; Peng et al., 2021), or introducing new kinds of\npersistent tokens and storage to extend information propagation (Dai et al., 2019; Munkhdalai et al.,\n2024; Hwang et al., 2024). However, as also noted by Dao et al. (2022), none of these methods and\napproximations have managed to replace standard approaches so far. We take a different approach\nthat can be integrated in a zero-shot manner even without any fine-tuning.\nLastly, methodologically related to NAMMs, there have been other prior methods making use of\nevolution for or with transformer models. For example, Tang & Ha also trained a small\nattention-based model through evolution, exploiting the inherent parameter efficiency behind these\noperations. Furthermore, So et al. (2019) proposed using evolution to meta-optimize the basic build-\ning of transformers via neural architecture search, while Akiba et al. (2024) focused on evolving\ndifferent merging strategies across layers belonging to LMs with different capabilities. As for these\nworks, we note that evolution plays a critical role for NAMMs, allowing us to directly optimize for\ntarget performance and overcome the inherent non-differentiability underlying our new framework."}, {"title": "F LIMITATIONS AND FUTURE EXTENSIONS", "content": "F.1 EXPLORING THE DESIGN SPACE OF NEURAL ATTENTION MEMORY MODELS\nIn this work, we introduced Neural Attention Memory Models and showed their efficacy and poten-\ntial to improve the performance and efficiency of transformers, even when evaluated zero-shot for\nunseen architectures and domains. However, given the novelty of our framework, we note that our\ndesign choices were mostly motivated by simplicity and practicality rather than quantitative empiri-\ncal evidence. Thus, there is an extremely large design space in terms of the implementation, training,\nand deployment of these models that should be explored beyond this work, which is likely to yield\nfurther improvements.\nFor instance, while our current feature extraction, based on computing the spectrogram of the atten-\ntion matrix, enables capturing global frequency information about the attention values of each token,\nit might fall short of modeling local information with enough granularity. This hypothesized limita-\ntion inherently comes from a few design choices we made with the purpose of limiting the input size\nand corresponding parameter count of our memory models. In particular, our spectrogram features\nonly consider the real components of a short-time Fourier transform with a small Hann window of\nsize thirty-two. Thus, we only provide NAMMs information about a relatively limited number of\nthirty-two frequencies, losing any notion of the phase of the attention matrix that would be captured\nby the full complex-valued Fourier coefficients. Consequently, the representations of tokens with\nhigh attention values for entirely non-overlapping queries occurring with the same frequency would\nbe indistinguishable to our models. Moreover, our exponentially moving average reduction over the\ntime dimension of the spectrograms provides an additional layer of heavy compression inevitably\ntrading off expressivity for simplicity.\nTo partially address these concerns, an alternative design we explored entailed delaying the ini-\ntial element-wise exponentially moving average reduction. Concretely, this involved computing T\ndifferent scores, feeding mo all feature vectors w\u2021 for t = 1,2,...,T, across the attention spec-\ntrogram's compressed time axis, only then reducing the resulting scores sT via EMA. While, in\nprinciple, this alternative ordering would allow for additional expressivity without adding to the pa-\nrameter count, in practice, when evaluated with an initial version of the simple 2-layer MLP model,\nwe found no significant performance difference and opted for the former lighter option. However,\nintroducing cross-token interactions with the improved BAM design and further scaling is likely to\nintroduce a need of re-evaluating this choice.\nOne further limitation comes from the current reliance on the exact values of the attention matrix.\nThis reliance precludes NAMMs training from making use of fast kernel algorithms developed to\naccelerate inference by foregoing materializing attention values . While the main\nfocus of this work has been to introduce NAMMs and display its potential to improve transformers\nacross different domains, more scalable parameterizations and efficient backend integrations remain\nexciting open challenges for future research.\nF.2 IMPROVING LONG-CONTEXT SPARSE RETRIEVALS\nOne notable example exemplifying some of the aforementioned limitations, comes from the canoni-\ncal Needle In A Haystack task , which has been used to qualitatively evaluate LLMs\nfor their ability to remember sparse information over long noisy horizons. We provide results on\nthis task using the best-performing NAMM after three stages of incremental training with the BAM"}]}