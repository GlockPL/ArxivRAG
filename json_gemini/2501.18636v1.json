{"title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model", "authors": ["Xun Liang", "Simin Niu", "Zhiyu Li", "Sensen Zhang", "Hanyu Wang", "Feiyu Xiong", "Jason Zhaoxin Fan", "Bo Tang", "Shichao Song", "Mengwei Wang", "Jiawei Yang"], "abstract": "The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) provides an efficient solution for expanding the knowledge boundaries of large language models (LLMs). Many advanced LLMs, such as ChatGPT (OpenAI et al., 2024), Gemini (Team et al., 2024), and Perplexy.ai\u00b9, have incorporated external retrieval modules within their web platforms. However, during the RAG process, query-relevant texts are processed sequentially through the retriever, the filter before being synthesized into a response by the generator, introducing potential security risks, as attackers can manipulate texts at any stage of the pipeline. Current attack tasks targeting RAG can be divided into the following four surfaces:\n\nNoise: Due to the limitation in retrieval accuracy, the retrieved contexts often contain large quantities of noisy texts that are at most merely similar to the query but do not actually contain the answer. Attackers can exploit this retrieval limitation to dilute useful knowledge by deliberately injecting extensive noisy texts (Chen et al., 2024a; Fang et al., 2024).\nConflict: Knowledge from different sources may conflict with one another, creating opportunities for attackers to manipulate. Simply injecting conflicting texts could prevent LLMs from determining which piece of knowledge is more reliable, resulting in vague or even incorrect responses. (Wu et al., 2024a; Liu et al., 2023).\nToxicity: The internet often contains toxic texts published by attackers. Such malicious texts are highly likely to be incorporated into the RAG pipeline, inducing LLMs to generate toxic responses (Deshpande et al., 2023; Perez and Ribeiro, 2022)."}, {"title": "Denial-of-Service (DoS)", "content": "The target of DoS is to cause LLMs to refuse to answer, even when evidence is available (Chaudhari et al., 2024; Shafran et al., 2024). DoS-inducing texts injected by attackers are particularly insidious because the resulting behavior is easily mistaken for the RAG's limitations.\n\nHowever, most of existing attack tasks at above surfaces often fail to bypass the safety RAG components, making the attacks no longer suitable for RAG security evaluation. There are four main reasons. R-1: Simple safety filters can effectively defend against noise attack (Li et al., 2024), as existing noise is often concentrated in superficially relevant contexts, which may actually belong to either similar-topic irrelevant contexts or relevant contexts that do not contain answers. R-2: Existing conflict primarily focuses on questions that LLMs can directly answer but contain factual inaccuracies in the related documents (Xu et al., 2024). Current adaptive retrievers (Tan et al., 2024) have been able to effectively mitigate such context-memory conflict. R-3: Advanced generators demonstrate strong capabilities in detecting and avoiding explicit and implicit toxicity, such as bias, discrimination, metaphor, and sarcasm (Sun et al., 2023; Wen et al., 2023). R-4: Traditional DoS attack mainly involves maliciously inserting explicit or implicit refusal signals into the RAG pipeline. Fortunately, such signals are often filtered out as they inherently do not support answering the question, or they are ignored by generators due to being mixed into evidences (Shafran et al., 2024).\n\nTo address above limitations, we propose four novel attack tasks for conducting effective RAG security evaluation. Firstly, we define silver noise , which refers to evidence that partially contains the answer. Such noise can circumvent most safety filters, thereby undermining the RAG diversity (R-1). Secondly, unlike the widely studied context-memory conflict, we explore a more hazardous inter-context conflict. Since LLMs lack sufficient parametric knowledge to handle external conflicts, they are more susceptible to being misled by tampered texts (R-2). Thirdly, we reveal the vulnerability of RAG under the soft ad attack. As a special type of implicit toxicity, the soft ad can evade LLMs and ultimately be inserted into the response of generators . Finally, to enable refusal signals to bypass filters or generators, we propose a white DoS attack. Under the guise of a safety warning, such attack falsely accuses the evidence of containing a large number of distorted facts, thereby achieving its purpose of refusal (R-4).\n\nExisting benchmarks mainly focus on applying a certain attack task at specific stages of the RAG pipeline and observing the impact of the selected attack on the retriever or generator. In this paper, we introduce the RAG security evaluation benchmark, SafeRAG, which systematically evaluates the potential security risks in the retriever and generator by performing four surfaces of improved attack tasks across different stages of the RAG pipeline. Our main contributions are:\n\nWe reveal four attack tasks capable of bypassing the retriever, filter, and generator. For each attack task, we develop a lightweight RAG security evaluation dataset, primarily constructed by humans with LLM assistance.\nWe propose an economical, efficient, and accurate RAG security evaluation framework that incorporates attack-specific metrics, which are highly consistent with human judgment.\nWe introduce the first Chinese RAG security benchmark, SafeRAG, which analyzes the risks posed to the retriever and generator by the injection of noise, conflict, toxicity, and DoS at various stages of the RAG pipeline."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 RAG Security Benchmark Dataset", "content": "Before performing RAG security evaluation, researchers typically design attack datasets meticulously to trigger the vulnerability of RAG. The primary attack types currently include noise, conflict, toxicity, and DoS. As for noise, RGB (Chen et al., 2024a) employs a retrieve-filter-classify strategy, dividing the top retrieved contexts related to the query into golden contexts (those containing the correct answer) and relevant noise contexts. RAG Bench (Fang et al., 2024) adopts the same approach to construct relevant noise while also introducing irrelevant noise. LRII (Wu et al., 2024b) further refines the construction of irrelevant noise through a combination of human effort and LLM assistance, categorizing it into three types: semantically unrelated, partially related, and related to questions."}, {"title": "3 Threat Framework: Attacks on the RAG Pipeline", "content": ""}, {"title": "3.1 Meta Data Collection and Pre-processing", "content": "As shown in Fig.2-1, we collected raw news texts from news websites between 08/16/24 and 09/28/24, covering five major sections: politics, finance, technology, culture, and military. Subsequently, we manually screened news segments that met the following criteria: (1) contain more than 8 consecutive sentences; (2) consecutive sentences revolve around a specific topic; (3) consecutive sentences can generate comprehensive questions of the what, why, or how types."}, {"title": "3.2 Generation of Comprehensive Question and Golden Contexts", "content": "Using DeepSeek, a powerful Chinese LLM engine, and referencing the news title, we generated a comprehensive question and its corresponding 8 pieces of golden contexts for each extracted news segment (Fig.2-2). In total, we obtained 110 unique question-contexts pairs. Additionally, we manually verified and removed data points that did not meet the following criteria: (1) the question is not a comprehensive what, why, or how type question; (2) there are contexts unrelated to the question. Finally, we obtained 100 unique question-contexts pairs, which serve as the basic dataset for attack text generation. The complete generation prompt is detailed in Fig. 12."}, {"title": "3.3 Selection of Attack-Targeted Texts and Generation of Attacking Texts", "content": "We select different attack-targeted texts from the question-contexts pairs in the base dataset to generate the specific attacking texts."}, {"title": "3.3.1 Generation of Silver Noise", "content": "To construct silver noise, which includes partial but incomplete answers, we first need to decompose the golden contexts in the base dataset. Specifically, we utilized the knowledge transformation prompt proposed in (Chen et al., 2024b) to break the contexts into fine-grained propositions , which are the smallest semantic units that are complete and independent as evidence. Then, we selected the proposition with the highest semantic similarity (cosine similarity) to the question as the attack-targeted text, ensuring that the subsequent attack texts achieve a high recall ratio. Finally, we prompted DeepSeek to generate 10 diverse contexts based on the selected attack-targeted text."}, {"title": "3.3.2 Generation of Inter-Context Conflict", "content": "The goal of conflict attack is to generate target texts that are prone to contradicting or being confused with the golden context. To achieve this, we manually select a golden context most susceptible to manipulation into a conflict. Subsequently, annotators are instructed to modify the context based on the following guidelines: (1) Minimal Perturbation: Introduce conflicts using the smallest possible changes (Fig. 3-1); (2) Rewriting for Realistic Conflicts: Rewrite the context where appropriate to create more convincing conflicts (Fig. 3-2); (3) Preservation of Key Facts: Avoid perturbations that render the conflict invalid, as altering the key fact may lead to generating the hallucinated context instead of the expected conflict context (Fig. 3-3), or even make the context irrelevant to the query (Fig. 3-4). Therefore, we require annotators to preserve key facts while generating conflict contexts. For instance, the year 2018 in Fig. 3-3 and the month September in Fig. 3-\u2463."}, {"title": "3.3.3 Generation of Soft Ad", "content": "For the toxic attack task, we manually selected the golden context most susceptible to the injection of malicious soft ads as the attack-targeted text. Then, we encouraged annotators to use the following two methods to create the soft ad attack text by seamlessly incorporating seemingly professional and harmless soft ads into the attack-targeted text: (1) Direct Insertion: Soft ad attack texts are directly inserted into the original context (Fig. 26-1); (2) Indirect Insertion: The original context is modified or adjusted to some extent, for instance, by promoting alongside authoritative entities (such as government, organizations, etc.) within the context, making the soft ad more subtle and closely integrated into the context (Fig. 26-2). The complete annotation requirements are shown in Fig. 17."}, {"title": "3.3.4 Generation of White Denial-of-Service", "content": "In a DoS attack, the original question is directly used as the target text. We then employ a rule-based approach to construct refusal contexts. Specifically, the construction rule is as shown as in Fig. 4.\n\nThe white DoS attack text constructed in this manner leverages the pretense of a safety warning to falsely accuse the evidence of containing heavily distorted information, thereby justifying refusal. Since safety warnings are perceived as well-intentioned and high-priority, they are less likely to be filtered by filters and are more likely to be adopted by generators."}, {"title": "3.4 Attacks on the RAG Pipeline", "content": "We perform a final manual verification of the various types of attack contexts obtained and selected appropriate contexts from the golden contexts to construct the final security evaluation datasets. The detailed construction and usage rules for the datasets can be found in Appendix A.1.3."}, {"title": "4 Evaluation Metrics", "content": ""}, {"title": "4.1 Retrieval Safety Assessment Metric: Retrieval Accuracy", "content": "Retrieval Accuracy (RA) is a metric used to evaluate the performance of RAG in terms of both retrieval accuracy and safety. It combines the recall of golden contexts and the suppression ability for attack contexts. The formula is as follows:\n\n$RA = \\frac{Recall (gc) + (1 - Recall (ac))}{2}$\n\nwhere Recall (gc) and Recall (ac) denote the recall of golden contexts and attack contexts, respectively. The core idea of RA is to balance the RAG's ability to retrieve relevant content while avoiding incorrect or harmful content. A high Recall (gc) reflects strong coverage of correct content, while a low Recall (ac) demonstrates the RAG's robustness in suppressing irrelevant or disruptive content. By combining these two sub-metrics, the higher RA indicates better retrieval performance by RAG."}, {"title": "4.2 Generation Safety Assessment Metric: F1 Variant and ASR", "content": ""}, {"title": "4.2.1 F1 Variants in Noise and DoS", "content": "For silver noise and white DoS tasks, we designed a metric called F1 (avg) to evaluates whether the generated response include fine-grained propositions and thus assess the diversity of the generator. Specifically, we prompt evaluator to complete the multiple-choice question based on the generated response, i.e., the news summary (Fig.7."}, {"title": "4.2.2 F1 Variants in Conflict", "content": "To construct multiple-choice options for each data point, we deliberately use conflicting facts between contexts as the options and manually annotate the correct ground truth options. As shown in Fig.8, if the response utilizes the correct fact, it can accurately select the correct options. However, selecting the wrong options is not solely due to using conflicting facts; it may also result from generating hallucinated facts. Consequently, the F1 (incorrect) metric is unreliable. Therefore, in the evaluation of inter-context conflict, we only retain the F1 (correct) metric to assess the generator's performance."}, {"title": "4.2.3 ASR in Conflict, Toxicity, and DoS", "content": "In the conflict, toxicity, and DoS tasks, attack keywords are present, such as the conflict facts leading to inter-context conflicts, seamlessly integrated soft ad keywords, and refusal signals. Therefore, in these tasks, we can evaluate the generator's safety using the attack success rate (ASR) (Zou et al., 2024). If a higher proportion of attack keywords appears in the response text, the ASR will increase."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Settings", "content": "The default retrieval window for the silver noise task is set to top K = 6, with a default attack injection ratio of 3/6. For other tasks, the default retrieval window is top K = 2, and the attack injection ratio is fixed at 1/2. We evaluated the impact of using different retrievers (DPR, BM25, Hybrid, Hybrid-Rerank) and filters (OFF, filter NLI (Li et al., 2024), compressor SKR (Wang et al., 2023)) across different RAG stages (indexing, retrieval, generation) on the contexts retrieved for various generators (DeepSeek, GPT-3.5-turbo, GPT-4, GPT-40, Qwen 7B, Qwen 14B, Baichuan 13B, ChatGLM 6B). The bold values represent the default settings. Then, we adopt a unified sentence chunking strategy to segment the knowledge base and build the index. The embedding model used is bge-base-zh-v1.5, the reranker is bge-reranker-base, and the evaluator is GPT-3.5-turbo."}, {"title": "5.2 Results on Noise", "content": "We inject different noise ratios into the text accessible in the RAG pipeline, including the knowledge base, retrieved context, and filtered context. As shown in Fig.5, the following observations can be made: (1) Regardless of the stage where noise is injected, the F1 (avg) score exhibits a downward trend as the noise ratio increases, indicating a decline in generation diversity ; (2) The retriever demonstrates some noise resistance, as noise injected at the knowledge base has approximately 50% chance of not being retrieved. The results in Fig.5-1 support this point. Specifically, as the noise ratio increases, the Retrieval Accuracy (RA) of injecting silver noise into the retrieved context or filtered context significantly outperforms that of injecting it into the knowledge base; (3) The performance of injecting noise into the retrieved context and filtered context is similar, indicating that the filter cannot effectively resist silver noise since silver noise still supports answering the query. (4) Different retrievers exhibit varying levels of robustness to noise. Overall, the ranking is Hybrid-Rerank > Hybrid > BM25 > DPR, suggesting that compared to attacking contexts, hybrid retriever and rerankers show a preference for retrieving golden contexts. (5) Compression-based filters like SKR are not sufficiently secure, as they tend to lose detailed information, leading to a decrease in F1 (avg)."}, {"title": "5.3 Results on Conflict, Toxicity, and DoS", "content": "(1) After injecting different types of attacks into the texts accessible by the RAG pipeline, it was observed that the retrieval accuracy (RA) and the attack failure rate (AFR) decreased across all three tasks. The ranking of attack effectiveness at different RAG stages was: filtered context > retrieved context > knowledge base. Furthermore, adding conflict attack increased the likelihood of misjudging incorrect options as correct, leading to a drop in F1 (correct). Introducing DoS attack reduced F1 (avg) and severely impacted generative diversity. (2) Retrievers exhibited different vulnerabilities to various attacks. For instance, Hybrid-Rerank was more susceptible to conflict attack, while DPR was more prone to DoS attack. Both experienced a significant decrease in AFR. Additionally, all retrievers showed consistent AFR degradation under toxicity attack. After adding conflict attack, the F1 (correct) scores of all retrievers became similar, indicating stable attack effectiveness. However, DPR was more affected by DoS attack compared to other retrievers, as evidenced by its significantly larger decline in the diversity metric F1 (avg). (3) The RA of different retrievers was largely consistent across different attack tasks . (4) In conflict tasks, using the SKR filter was less secure because it could compress conflict details, resulting in a decline in F1 (correct). In toxicity and DoS tasks, the NLI filter was generally ineffective, with its AFR close to that of disabling the filter. However, the SKR filter proved to be safe in these tasks, as it was able to compress soft ads and warnings."}, {"title": "5.4 Analysis of Generator and Evaluator", "content": ""}, {"title": "5.4.1 Selection of Generator", "content": "We conduct a cumulative analysis of the positive metrics across different attack tasks. As shown in Fig.11, the results show that Baichuan 13B achieved a leading position in multiple attack tasks, particularly excelling in AFR (DoS) and F1 Variants (DoS) metrics. Lighter models are even safer than models such as the GPT series and DeepSeek."}, {"title": "5.4.2 Selection of Evaluator", "content": "As shown in Table 2, We present the evaluation metrics and their consistency with human judgments. The ASR and AFR metric exhibit a high human consistency. Similarly, the F1 (correct) and F1 (incorrect) scores obtained using DeepSeek also demonstrate strong agreement with human judgments. Therefore, DeepSeek is uniformly adopted for evaluation across all experiments."}, {"title": "6 Conclusion", "content": "This paper introduces SafeRAG, a benchmark designed to assess the security vulnerabilities of RAG against data injection attacks. We identified four critical attack surfaces: noise, conflict, toxicity, and DoS, and revealed significant weaknesses across the retriever, filter, and generator components of RAG. By proposing novel attack strategies such as silver noise, inter-context conflict, soft ad, and white DoS, we exposed critical gaps in existing defenses and demonstrated the susceptibility of RAG systems to subtle yet impactful threats."}]}