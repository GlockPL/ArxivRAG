{"title": "Online Learning from Strategic Human Feedback in LLM Fine-Tuning", "authors": ["Shugang Hao", "Lingjie Duan"], "abstract": "Reinforcement learning from human feedback (RLHF) has become an essential step in fine-tuning large language models (LLMs) to align them with human preferences. However, human labelers are selfish and have diverse preferences. They may strategically misreport their online feedback to influence the system's aggregation towards their own preferences. Current practice simply averages labelers' feedback per time and fails to identify the most accurate human labeler, leading to linear regret $O(T)$ for T time slots. To our best knowledge, we are the first to study online learning mechanisms against strategic human labelers in the LLM fine-tuning process. We formulate a new dynamic Bayesian game and dynamically adjust human labelers' weights in the preference aggregation, ensuring their truhtful feedback and sublinear regret $O(T^{1/2})$. Simulation results demonstrate our mechanism's great advantages over the existing benchmark schemes.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) such as ChatGPT and SORA have succeeded in handling a number of tasks such as text and video generation. To better meet users' demands for specific applications, pre-trained LLMs are fine-tuned to be customized using task-oriented datasets (e.g., [1]). Traditional supervised learning methods fail to align with human preferences because of the difficulty in acquiring a significant number of question-answer paired data (e.g., [2], [3]). Reinforcement learning from human feedback (RLHF) has emerged as a promising approach to tackle this human preference alignment problem (e.g., [4], [5]). It queries online human feedback (e.g., in a weekly cadence [6], [7], [8]) to obtain a human-annotated preference dataset, which will then be used to train and update the learning policy. RLHF has become an essential training step in LLM fine-tuning due to its effectiveness in aligning with human preferences.\nHowever, human labelers are selfish in the RLHF loop to have diverse preferences and they may strategically misreport their online feedback to influence the system's aggregation towards their own preferences (e.g., [3], [9]). For example, a user in an LLM rating system may strategically give an extreme response rating of 0 or 10 in the range of [0, 10] to maximally influence the overall rating toward his actual rate (e.g., [10]). Besides, there is a renowned \u201cwet bias\" where a weather forecaster as human labeler or predictor may deliberately report an exaggerated probability of precipitation to increase the influence of his forecast in the system's final prediction (e.g., [11]). Current practice of LLM fine-tuning largely ignores human labelers' misreporting and simply averages human feedback in the preference aggregation (e.g., [4], [5], [12], [13], [14], [15], [16]) and we wonder its actual performance. Our first question naturally arises:\n\u2022 Q1. How bad is the current practice of average feedback aggregation for LLM fine-tuning performance?\nLater we prove that the average feedback aggregation scheme fails to identify the most accurate human labeler in the online learning process and incurs a non-vanishing regret $O(T)$ overtime. This motivates us to propose new schemes for truthful human feedback and vanishing regret. In the recent RLHF literature, Sun et al. (2024) in [3], Park et al. (2024) in [17], Soumalias et al. (2024) in [9] and Dubey et al. (2024) in [18] focus on monetary payment-based mechanism design to reward and enable strategic human labelers' truthful preference feedback. In practice, monetary mechanisms involve complicated billing issues and may not be easy to implement. Furthermore, these works assume a one-shot or offline preference feedback setting and do not consider human labelers' online feedback. In the online setting, human labelers have more room to strategically misreport and play with the RLHF system for long-term influence.\nIn the related literature of algorithmic game theory, there are relevant non-monetary mechanism studies on facility location games (e.g., [19], [20], [21]), where the system aims to incentivize customers' truthful reporting of their locations to optimize facility placement. There each customer can strategically misreport his location to mislead the facility placement as close to his location (preference) as possible. The popular \u201cmedian\u201d scheme (e.g., [10], [22]) to aggregate multi-agent reports is widely used to return customers' truthful reporting. Yet, later we prove that it is no longer truthful for our online problem and also incurs a non-vanishing regret as the average feedback aggregation scheme. Our second question is thus:\n\u2022 Q2. How to design an efficient and truthful mechanism for online learning from strategic human feedback in LLM fine-tuning?\nA natural non-monetary mechanism approach is to deploy\""}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "First, we introduce our system model. Then, we formulate a new dynamic Bayesian game and give desired properties for guiding our late mechanism design."}, {"title": "A. System Model of LLM Fine-Tuning from Online Human Feedback", "content": "We consider an LLM fine-tuning process of aggregating online feedback from $N\u22652$ strategic human labelers overtime. It starts from fine-tuning a pre-trained language model with supervised learning based on a task-orientated dataset (e.g., paragraph summary) to obtain a reference policy $\\pi_{ref}$. Then, the system iterates the RLHF process in T time slots (e.g., a weekly cadence [6], [7], [8]), where each time slot $t \\in [T] := {1,\\ldots,T}$ contains the following three stages. The current practice of LLM fine-tuning only involves Stages I and II with uniform weights in the aggregation and we introduce Stage III to dynamically adjust human labelers' weights according to their online feedback accuracy.\nStage I. Online Preference Feedback: The system draws $m_t$ prompts $\\{x_j\\}_{j=1}^{m_t}$ from the context space $\\mathcal{X}$ and generates $m_t$ pairwise responses $\\{(y_{ij}, y_{i,x})\\}_{j=1}^{m_t}$ from the response space $\\mathcal{Y}$ according to the last slot policy $\\pi_{t-1}$ with $\\pi_0 = \\pi_{ref}$ (e.g., [8]). It then shares $\\{(x_j, y_{ij}, y_{i,x})\\}_{j=1}^{m_t}$ with N human labelers for their preference feedback. Each human labeler $i\\in[N]$ independently realizes his continuous private preference of response $y_{ij}$ over $y_{i,x}$, as $P_i(y_{ij} > y_{i,x}) \\in [0,1]$ for each $j\\in [m_t]$ and he believes that the ground-truth preference $p_j \\sim Bernoulli(P_i(y_{ij} > y_{i,x}))$, where realization $p_j = 1$ means response $y_{ij}$ is preferred over $y_{i,x}$ and $p_j = 0$ otherwise. He wants to influence the system's aggregation toward his own preference and may feedback another continuous $P'_i(y_{ij} > y_{i,x}) \\in [0,1]$ different from his actual $P_i(y_{ij} > y_{i,x})$ to the system (e.g., [3], [9]). The system and other human labelers are uncertain of his $P'_i(y_{ij} > y_{i,x})$ realization."}, {"title": "Stage II. Online Feedback Aggregation and Policy Optimization", "content": "After receiving each human labeler i's feedback $\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_t}$ for $i \\in [N]$, the system aggregates according to the weight $w_i^t$ for each prompt $j \\in [m_t]$ as\n$$P(y_{ij}, y_{i,x}, x_j) = \\frac{\\sum_{i=1}^{N} w_i^t P'_i(y_{ij}, y_{i,x})}{\\sum_{i=1}^{N} w_i^t}$$\\nwith a uniform weight $w_i^1$=1 for all $i\\in[N]$ in the first time slot. The aggregated preference $\\{P(y_{ij} > y_{i,x})\\}_{j=1}^{m_t}$ will be included to construct the human preference dataset $D_t := \\{P(y_{ij} > y_{i,x})\\}_{j=1}^{m_t}$. Based on the preference dataset $D_t$, the system then learns a policy $\\pi_t$ using direct preference optimization (DPO) to solve a KL-regularized optimization problem against the reference policy $\\pi_{ref}$ (e.g., [12]):\n$$\\min_{\\pi_t} \\min_{\\sigma_{t(.)}} -E_{(x,y,y') \\sim D_t} [\\ln \\sigma(\\pi_t(y|x), \\pi_t(y'|x)) - \\beta \\ln \\frac{\\pi_t(y|x)}{\\pi_t(y'|x)} + \\beta \\ln \\frac{\\pi_{ref}(y'|x)}{\\pi_{ref}(y'|x)}]$$\nwhere $\\sigma(.)$ denotes the logistic function and $\\beta$ is a parameter of evaluating the deviation from the reference policy $\\pi_{ref}$."}, {"title": "Stage III. Reweighing Human Labelers", "content": "In the RLHF literature, the current practice of LLM fine-tuning simply averages human labelers' feedback in the preference aggregation using uniform $w_i^t$=1 for all $i\\in[N]$ and $t\\in[T]$ in (1) (e.g., [4], [5], [12]). To our best knowledge, we are the first to consider dynamic adjustment of each human labeler's weight in the online RLHF process, where the system determines each\n$$w_i^{t+1} = f_i(\\{\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_t}\\}_{i=1}^N, \\{p_j\\}_{j=1}^{m_t})$$\nfor the next slot $t + 1$'s aggregation according to feedback $\\{\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_t}\\}_{i=1}^N$ and the realized preference $\\{p_j\\}_{j=1}^{m_t}$. The system implements and tests the obtained policy $\\pi_t$ for customers' practical usage and learns the realized binary preference $p_j \\in \\{1,0\\}$ for each prompt $j \\in [m_t]$ according to its customers' realized experience, where $p_j = 1$ if $y_{ij}$ is preferred than $y_{i,x}$, and 0 otherwise (e.g., [23], [24], [25]).\nEach selfish human labeler wants to use $P'_i(y_{ij}, y_{i,x})$ to influence the system's aggregation towards his own preference (e.g., [3], [9]). Thus, he wants to obtain a large weight in the system's feedback aggregation in (1) in each time slot $t \\in [T]$ and aims to maximize his long-term influence benefit as the cumulative weight over the whole T time slots as follows:\n$$u_i(\\{\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_{t-1}}\\}_{t=1}^T) = \\sum_{t=1}^T w_i^t(\\{\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_{t-1}}\\}, \\{p_j\\}_{j=1}^{m_{t-1}})$$\nOn the other hand, the system wants to improve the feedback accuracy in the aggregation by assigning the largest weight to the most accurate human feedback. However, the best human labeler is unknown in the online iteration. It then turns to reducing the regret between online weighted aggregation and offline choice of the best human labeler in\n$^1$One can also use $w_i^1$=1/N without any change for the aggregation result."}, {"title": "B. Dynamic Bayesian Game Formulation", "content": "Based on our system model above, we formulate the multi-agent online learning as a new dynamic Bayesian game:\n\u2022 In Stage I of each time slot $t\\in[T]$, each human labler $i$ with his private preference $\\{P_i(y_{ij} > y_{i,x}|x_j)\\}$ determines his preference feedback $\\{P'_i(y_{ij}, y_{i,x})\\}$ to maximize his accumulative weight in (3).\n\u2022 In Stage III of each time slot $t\\in[T]$, the system updates each human labeler's weight $w_i^{t+1}=f_i(\\{\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_t}\\}_{i=1}^N, \\{p_j\\}_{j=1}^{m_t})$ for reducing regret in (4).\nNote that there is no strategic decision for human labelers or the system in Stage II. We need to carefully design an online aggregation mechanism for ensuring each human labeler's truthful preference feedback and a vanishing regret in time. We define the desired properties as below.\nDefinition 1 (Truthfulness for Human Feedback): An online weighted aggregation mechanism $\\mathcal{M}$ is truthful if each human labeler $i\\in[N]$ obtains a larger long-term influence in (3) over the whole T time slots though truthful preference feedback instead of misreporting in the mean time, i.e.,\n$$\\mathbb{E}\\left[\\sum_{t=2}^{T-1} w_i^{t+1}(\\{\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_t}\\}, \\{p_j\\}_{j=1}^{m_t}, \\{\\{P'_k(y_{ij} > y_{i,x})\\}_{j=1}^{m_t}\\}_{k=1,k\\neq i}^N)\\right] > \\mathbb{E}\\left[\\sum_{t=2}^{T-1} w_i^{t+1}(\\{\\{P'_i(y_{ij}, y_{i,x})\\}_{j=1}^{m_t}\\}, \\{p_j\\}_{j=1}^{m_t}, \\{\\{P'_k(y_{ij} > y_{i,x})\\}_{j=1}^{m_t}\\}_{k=1,k\\neq i}^N)\\right].$$\nDefinition 2 (High Efficiency in Sublinear Regret R(T) in (4)): An online weighted aggregation mechanism $\\mathcal{M}$ is efficient if its time-average regret $R_{\\mathcal{M}}(T)/T$ is vanishing in the time slot number T, i.e., $\\lim_{T \\rightarrow \\infty} \\frac{R_{\\mathcal{M}}(T)}{T} = 0$"}, {"title": "III. TWO BENCHMARK SCHEMES: AVERAGE FEEDBACK AGGREGATION AND MEDIAN AGGREGATION", "content": "In this section, we analyze two common schemes used in the literature of both RLHF and algorithmic game theory, serving as two fair benchmarks for our mechanism to compare later."}, {"title": "A. Benchmark 1: Average Feedback Aggregation", "content": "The current practice of LLM fine-tuning simply averages human labelers' feedback in the preference aggregation using uniform $w_i^t = 1$ for all $i \\in [N]$ and $t \\in [T]$ (e.g., [4], [5], [12]). Unfortunately, such an average feedback aggregation scheme can lead to a non-vanishing regret as shown below.\nLemma 1: The system's regret in (4) under the benchmark 1 of average aggregation is $R_1(T)=O(T)$, leading to a non-vanishing time-average regret $\\lim_{T \\rightarrow \\infty} \\frac{R_1(T)}{T} > 0$.\nBenchmark 1 fails to dynamically adjust human labelers' aggregation weights according to their online feedback accuracy. Thus, the most accurate human labeler cannot receive the largest weight in the online learning process, leading to a non-vanishing time-average regret even if $T \\rightarrow \\infty$. We are motivated to find other schemes to reduce the system's regret."}, {"title": "B. Benchmark 2: Median Aggregation Scheme", "content": "In the algorithmic game theory literature, the popular \u201cmedian\u201d scheme is widely used to motivate strategic agents' truthful reporting (e.g., [10], [22]). We define it below.\nDefinition 3 (Median Aggregation Scheme): The system first re-organizes human labelers' preference feedback $\\{P'_i(y_{ij} > y_{i,x})\\}_{i=1}^N$ in an increasing order as $P_{1,j}^t \\le P_{2,j}^t \\le...\\le P_{N,j}^t$ for each prompt $j \\in [m_t]$ in each time slot $t \\in [T]$. It then chooses the median $P_{s,j}^t$ as its preference aggregation, where the index s = N/2 if N is even and s = (N+1)/2 otherwise. Since the benchmark 2 independently commits to the median feedback for each prompt in each time slot, at the equilibrium, all the human labelers' feedback will converge to one common point for an equal probability to be the median. We summarize the equilibrium in the following.\nProposition 1: At the equilibrium of the benchmark 2, all the human labelers' feedback $P^*_i(y_{ij}, y_{i,x})$ will converge to an arbitrary point $P(y_{ij} > y_{i,x}) \\in [0, 1]$ for $j\\in[m_t], i\\in[N]$ and $t\\in[T]$, which may not be the same as their own preference. The median scheme is not truthful since a human labeler may be committed with a positive probability by misreporting than no probability by truthful feedback. Further, it leads to a non-vanishing time-average regret in T in the following.\nLemma 2: The system's regret in (4) under the untruthful median scheme is $R_2(T) = O(T)$, leading to a non-vanishing time-average regret $\\lim_{T \\rightarrow \\infty} \\frac{R_2(T)}{T} > 0$.\nThe median feedback can lead to a total aggregation loss of O(T) over the T time slots. Yet, there can exist a human labeler holding $P_k (y_{ij}, y_{i,x}) = p_j$, incurring zero aggregation loss of the best human labeler in hindsight. The average regret is then non-vanishing even if the time slot number $T \\rightarrow \\infty$. Given non-vanishing regret of both benchmark schemes, we are well motivated to develop a truthful mechanism to substantially reduce the system's regret."}, {"title": "IV. ONLINE WEIGHTED AGGREGATION MECHANISM: NEW DESIGN, ANALYSIS, AND SIMULATIONS", "content": "In this section, we first present our mechanism design and its regret bound. We then run simulations for verification."}, {"title": "A. Mechanism Design and Theoretical Analysis", "content": "Unlike benchmark 1, in Stage III of each time slot, we dynamically adjust each human labeler's weight based on his feedback accuracy and assign a larger weight if his feedback is closer to the realized binary preference. We need to carefully design the online mechanism in (2) to ensure that each obtains the largest long-term influence in Definition 1 only with truthful feedback. We define our mechanism as below.\nDefinition 4 (Online Weighted Aggregation Mechanism): At Stage III of each time slot $t \\in [T \u2013 1]$, the system updates each human labeler's weight $w_i^{t+1}$ in (2) based on his feedback $P'_i(y_{ij}, y_{i,x})$ and the realized binary preference $p_j$:\n$$w_i^{t+1} = w_i^t - \\alpha (1 - \\frac{1}{m_t} \\sum_{j=1}^{m_t} (P'_i(y_{ij}, y_{i,x}) - p_j)^2),$$\nwhere $\\alpha > 0$ is the step-size parameter.\nIntuitively, our mechanism determines each human labeler's weight in time slot t + 1 based on his feedback accuracy in the previous time slot t. If the squared difference between his feedback and the realized binary preference $(P'_i(y_{ij}, y_{i,x})-p_j)^2$ is small, his weight $w_i^{t+1}$ will be only reduced by a small value from $w_i^t$. Though all human labelers' weights are decreasing over time, we care about the relative weighted aggregation as in (1).\nSince each human labeler holds a Bernoulli belief on $p_j$, our mechanism satisfies the truthful property as shown below.\nProposition 2: Our mechanism in Definition 4 is truthful, i.e., $P'_i(y_{ij}, y_{i,x}) = P_i(y_{ij}, y_{i,x})$ for any prompt $j \\in [m_t]$, human labeler $i \\in [N]$ and time slot $t \\in [T]$. Further, our mechanism is efficient and incurs a vanishing time-average regret in T in the following.\nTheorem 1: Our mechanism in Definition 4 incurs the sublinear regret $R_{\\mathcal{M}}(T)=O(\\sqrt{T})$ by choosing step-size $\\alpha = \\sqrt{\\frac{21nN}{T}}$, leading to zero time-average regret with $\\lim_{T \\rightarrow \\infty} \\frac{R_{\\mathcal{M}}(T)}{T} = 0$.\nAccording to Theorem 1, our mechanism obviously im-proves from benchmarks 1 and 2 by distinguishing the best human labeler in the online process as $T \\rightarrow \\infty$. As N increases, the system may find a more accurate human labeler in hindsight. Thus, it chooses a larger step-size $\\alpha$ to punish inaccurate human labelers more in the weighted aggregation to retire them. As T increases, the system is more patient to choose a smaller $\\alpha$ for selecting the best human labeler in hindsight with more time slots and samples."}, {"title": "B. Simulation Results for Verification", "content": "In this subsection, we run simulations to show our mecha-nism's great improvement from the two benchmark schemes.\nBased on human-written form post summaries with TRLX framework (e.g., [28]) for the RLHF process, we consider a task of paragraph summarization (e.g., [12]) and use a supervised fine-tuning model to obtain the reference policy $\\pi_{ref}$. We use the Reddit TL;DR summarization dataset in [29] to draw each promptx as a form post from Reddit, and"}, {"title": "V. CONCLUSION", "content": "In this paper, we are the first to study online learning from strategic human feedback in LLM fine-tuning. We design an efficient truthful mechanism to achieve zero time-average regret, greatly improving from non-vanishing regrets of the av-erage feedback aggregation and median schemes, respectively. Finally, simulation results demonstrate our mechanism's great advantages over the two benchmark schemes."}, {"title": "A. Proof of Lemma 1", "content": "We will prove $R_1(T) = O(T)$ with a possible sequence of human labelers' preferences. In particular, we consider $P_k(y_{w_j} > y_{i,x}|x) = p_j$ holds for one particular $k \\in [N]$ with any $j \\in [m_t]$ and $t \\in [T]$. For the remaining human labelers, we consider $\\sum_{i=1, i\\neq k}^N (P_i(y_{w_j} > y_{i,x}) - p_j)^2 = c$ for each $i \\neq k, i \\in [N], j \\in [m_t]$ and $t \\in [T]$, where $c \\in [\\frac{1}{2}, 1]$. Accordingly, we have the best-fixed human labeler in hindsight is $i^* = k$, which brings\n$$\\min_{i \\in[N]} \\frac{1}{m_t} \\sum_{t=1}^{T} \\sum_{j=1}^{m_t} (P_{i^*}(y_{w_j} > y_{i,x}) - p_j)^2 = 0.$$\nHowever, with the system's uniform weight scheme, we have the cumulative aggregation loss over T slots as follows:\n$$\\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{1}{N} \\sum_{i=1}^{N} w_iP_i(y_{w_j} > y_{i,x}) - p_j)^2 = \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{1}{N} w_k P_k(y_{w_j} > y_{i,x}) + \\frac{1}{N} \\sum_{i=1, i\\neq k}^{N} P_i(y_{w_j} > y_{i,x}) - p_j)^2$$\n$$= \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{1}{N} Np_j + \\frac{1}{N} \\sum_{i=1, i\\neq k}^{N} P_i(y_{w_j} > y_{i,x}) - p_j)^2$$\n$$= \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{1}{N} \\sum_{i=1, i\\neq k}^{N} (\\frac{1}{N} P_i(y_{w_j} > y_{i,x}) - p_j))^2$$\n$$= \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{N-1}{N} \\sum_{i=1, i\\neq k}^{N} (\\frac{1}{N} c_t))^2$$\n$$= \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{1}{N} c_t)^2 = O(T),$$\nwhere the last equality holds because each $c_t \\in [\\frac{1}{2}, 1]$ and $ \\sum_{t=1}^T \\frac{1}{m_t} \\sum_{j=1}^{m_t} c_t^2 $ does not vanish as $T \\rightarrow \\infty$. Finally, we have the regret for bechmark 1 of average aggregation as follows:\n$$R_1(T) = \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{\\sum_{i=1}^{N} w_iP_i (y_{w_j} > y_{i,x})}{\\sum_{i=1}^{N} w_i} - p_j)^2 - \\min_{i \\in[N]} \\frac{1}{m_t} \\sum_{t=1}^{T} \\sum_{j=1}^{m_t} (P_{i^*}(y_{w_j} > y_{i,x}) - p_j)^2 = O(T),$$\nwhich indicates that $ \\lim_{T \\rightarrow \\infty} \\frac{R_1(T)}{T} > 0 $. We then finish the proof."}, {"title": "B. Proof of Proposition 1", "content": "We want to prove that $P'_i(y_{w_j} > y_{i,x}) = P_i(y_{w_j} > y_{i,x})$ for $j \\in[m_t], i \\in[N]$ and $t \\in[T]$ is an equilibrium. Note that the system independent determines the aggregation as the median feedback for each prompt $j \\in [m_t]$ in each time slot $t \\in [T]$, each human labeler $i \\in [N]$ aims to maximize each $w_i^t$ to obtain a largest possible accumulative weight overtime. Since the system always commits to the median feedback, given the other $N \u2013 1$ human labelers except for i choose $P_k (y_{w_j} > y_{i,x} |x)= P(y_{w_j} > y_{i,x} |x), k\\neq i, k \\in [N]$, the human labeler i feedback any $P'_i (y_{w_j} > y_{i,x} |x) \\neq P (y_{w_j} > y_{i,x} |x)$ will lead to his weight $w_i^t = 0$ because such feedback cannot be the median. Thus, he will feedback consistently as $P'_i(y_{w_j} > y_{i,x}) = P(y_{w_j} > y_{i,x})$ for an equal chance to be the median and will never deviate from this feedback strategy. We then finish the proof."}, {"title": "C. Proof of Lemma 2", "content": "We want to prove $R_2(T) = O(T)$ with a possible sequence of human labelers' preferences. In particular, we consider $P_k(y_{w_j} > y_{i,x} |x) = p_j$ holds for one particular $k \\in [N]$ with any $j \\in [m_t]$ and $t \\in [T]$. Further, we consider $(P^t_{k,m} - p_j)^2 = c$ for $j \\in [m_t]$ and $t \\in [T]$, where $P^t_{k,m}$ denotes the median of human labelers' feedback $\\{P'_i(y_{w_j} > y_{i,x})\\}_{i=1}^N$ and $c \\in [\\frac{1}{2}, 1]$. Accordingly, we have the best-fixed human labeler in hindsight is $i^* = k$, which brings\n$$\\min_{i \\in[N]} \\frac{1}{m_t} \\sum_{t=1}^{T} \\sum_{j=1}^{m_t} (P_{i^*}(y_{w_j} > y_{i,x}) - p_j)^2 = 0.$$"}, {"title": "However, with the system's median scheme, we have the cumulative aggregation loss over T slots as follows", "content": "$$\\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{\\sum_{i=1}^{N} w_iP_i (y_{w_j} > y_{i,x})}{\\sum_{i=1}^{N} w_i} - p_j)^2$$\n$$= \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{\\sum_{i=1}^{N}  P_i (y_{w_j} > y_{i,x})}{\\sum_{i=1}^{N} } - p_j)^2 =  \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} ( P^t_{k,m} - p_j)^2 $$\n$$= \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} ( P^t_{k,m} - p_j)^2 $$\n$$ = \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} c_j = O(T),$$\nwhere the last equality holds because each $c_t \\in [\\frac{1}{2}, 1]$ and $ \\sum_{t=1}^T \\frac{1}{m_t} \\sum_{j=1}^{m_t} c_t^2 $ does not vanish as $T \\rightarrow \\infty$. Finally, we have the regret of the median scheme as follows:\n$$R_2(T) = \\sum_{t=1}^{T} \\frac{1}{m_t} \\sum_{j=1}^{m_t} (\\frac{\\sum_{i=1}^{N} w_iP_i (y_{w_j} > y_{i,x})}{\\sum_{i=1}^{N} w_i} - p_j)^2 - \\min_{i \\in[N]} \\frac{1}{m_t} \\sum_{t=1}^{T} \\sum_{j=1}^{m_t} (P_i(y_{w_j} > y_{i,x}) - p_j)^2 = O(T).$$\nWe then finish the proof."}, {"title": "D. Proof of Proposition 2", "content": "Note that each human labeler believes that $p_j \\sim Bernoulli(P_i(y_{w_j} > y_{i,x}|x))$, we have expectation on $w_i^{t+1}$ in (5) over $p_j$ is\n$$\\mathbb{E}[w_i^{t+1}", "2": "n$$=w_i^t - \\frac{\\alpha}{m_t} \\sum_{j=1}^{m_t} [1 - \\alpha (P'_i(y_{w_j} > y_{i,x}) - P_i (y_{w_j} > y_{i,x}))^2 - (P_i(y_{w_j} > y_{i,x}) - P_i (y_{w_j} > y_{i,x})"}]}