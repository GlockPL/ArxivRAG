{"title": "SoK: Mind the Gap\u2014On Closing the Applicability Gap in Automated Vulnerability Detection", "authors": ["Ezzeldin Shereen", "Madeleine Dwyer", "Dan Ristea", "Sanyam Vyas", "Chris Hicks", "Shae McFadden", "Vasilios Mavroudis"], "abstract": "The frequent discovery of security vulnerabilities in both open-source and proprietary software underscores the urgent need for earlier detection during the development lifecycle. Initiatives such as DARPA's Artificial Intelligence Cyber Challenge (AIxCC) aim to accelerate Automated Vulnerability Detection (AVD), seeking to address this challenge by autonomously analyzing source code to identify vulnerabilities.\nThis paper addresses two primary research questions: (RQ1) How is current AVD research distributed across its core components? (RQ2) What key areas should future research target to bridge the gap in the practical applicability of AVD throughout software development? To answer these questions, we conduct a systematization over 79 AVD articles and 17 empirical studies, analyzing them across five core components: task formulation and granularity, input programming languages and representations, detection approaches and key solutions, evaluation metrics and datasets, and reported performance.\nOur systematization reveals that the narrow focus of AVD research-mainly on specific tasks and programming languages-limits its practical impact and overlooks broader areas crucial for effective, real-world vulnerability detection. We identify significant challenges, including the need for diversified problem formulations, varied detection granularities, broader language support, better dataset quality, enhanced reproducibility, and increased practical impact. Based on these findings we identify research directions that will enhance the effectiveness and applicability of AVD solutions in software security.", "sections": [{"title": "1. Introduction", "content": "Traditionally, vulnerability detection relied on manual code reviews and security audits, practices that are both labor-intensive and time-consuming. However, in the past decade, researchers have developed increasingly sophisticated approaches to automate this process. Automated Vulnerability Detection (AVD) now represents a potentially transformative advancement in software security, aiming to identify and support the remediation of security flaws before they reach the codebase. Due to rapid advancements in the field, initiatives such as DARPA's Cyber Grand Challenge (CGC) in 2016 [1] and more recently the Artificial Intelligence Cyber Challenge (AIxCC) in 2024 [2] have sought to catalyze this change, pushing AVD from research prototypes to practical solutions ready for production.\nThis work systematizes current approaches in AVD in response to growing complexity and fragmentation within the field. As AVD research has evolved swiftly from rule-based methods [3], [4] to sophisticated machine learning [5], [6] and language model-driven techniques [7], [8], [9], the field has become increasingly divided, with studies often focused on isolated tasks, limited language support, and inconsistent evaluation methodologies. For example, prior works have noted the variability in dataset quality [10] and poor open science practices [11]. Overall, this rapid progress, while impressive, has potentially led the field away from a unified approach and hindered AVD's broader applicability in real-world software development, risking misalignment with the ultimate goal of proactive and reliable security integration. We aim to systematically investigate this by answering the following research questions:\nRQ1. How is current AVD research distributed across its core components: solved task, expected input, used approach, performed evaluation, and reported performance?\nRQ2. What key areas should future research target to bridge the gap in the practical applicability of AVD throughout software development?\nTo answer the research questions, we conduct a systematization of the literature in the field of AVD, consolidate the current state of the art based on the core components of AVD, and perform a meta-analysis of reported performance.\nOur findings after analyzing 79 AVD articles and 17 empirical studies reveal that, despite notable progress, the AVD field faces substantial obstacles that limit its practical applicability. AVD research remains overly narrow, with a significant portion of studies focused on binary classification"}, {"title": "2. Scope", "content": "Our study focuses on AVD within the context of the Software Development Life Cycle, where security measures can be integrated at all stages to address vulnerabilities as early as possible. By embedding vulnerability analysis directly into the development process, security becomes a continuous aspect of software creation, supporting more secure applications throughout their life cycles. In this context, we distinguish between three primary tasks: detection, exploitation, and repair.\nDetection\u00b9 is the task of identifying whether code contains a vulnerability. This task forms the foundational layer of vulnerability analysis, as accurate detection is essential for addressing security flaws before they are exploited.\nExploitation is the task of generating an input that triggers a vulnerability, thereby demonstrating its potential\nThis work focuses on the detection task, as it is the initial and arguably the most critical step in the vulnerability analysis pipeline. By focusing on detection, we aim to enable early identification of vulnerabilities, particularly within the software development lifecycle, which in turn facilitates downstream tasks of exploitation and repair. In particular, having access to the source code supports white-box exploitation, which is more efficient than black-box approaches [13], and most white-box exploitation techniques, such as code-guided fuzzing [14], depend on AVD to pinpoint targets within the software. In addition, automated program repair (APR) approaches [15] require AVD to identify specific vulnerable components to be patched."}, {"title": "3. Background", "content": "The AVD literature uses diverse input representations and program analysis methods, which have increasingly included machine learning. We briefly introduce these as a foundation for our analysis of the included articles."}, {"title": "3.1. Program Analysis", "content": "Vulnerability detection is a branch of program analysis, the process of determining a program's properties. Program analysis is divided into static, dynamic, and hybrid analysis.\nStatic analysis examines a program without executing it, thus it can be applied to source code before compilation. Its result are generally applicable across program executions and inputs. However, the static analysis of complex programs can be slow and statically determining if a program is free of vulnerabilities is undecidable [16]. Therefore, static AVD methods require approximation to be practical, leading to diverse approaches with different trade-offs. As universal approximators [17], neural networks have been increasingly applied to this setting.\nDynamic analysis executes the program to examine its behavior at run-time. It only provides information about executed code paths and inputs so it cannot prove general properties, but it can find violations of properties. It can be augmented by introducing instrumentation into the program during compilation (e.g., sanitizers [18]) which provides additional information at the cost of execution speed.\nHybrid analysis combines static and dynamic methods to improve results and lower computation requirements.\ncommon traditional program analysis methods in AVD literature are:\n1) Symbolic execution statically creates a tree of execution paths recording which constraints variables must satisfy for the path to be executed [19]. Finding inputs that result in an execution path thus becomes a satisfiability problem.\n2) Taint analysis tracks potentially malicious inputs and any values derived from them, tracing their usage through either static or dynamic analysis. It identifies instances where tainted values are used in high-risk code, such as memory operations.\n3) Program slicing extracts subsets of code that behave identically to the full program with respect to a specified behavior. This simplifies further analysis by excluding unrelated code. It can be static or dynamic."}, {"title": "3.2. Program Representation", "content": "How software is represented determines which AVD approaches can be applied and their effectiveness, as each representation offers different levels of abstraction and detail.\nSource code is the fundamental representation from which other forms are derived. The input granularity at which source code is analyzed \u2013 from individual statements and lines to larger constructs like functions or entire projects influences AVD detection granularity, especially as input and output granularities are frequently the same. It also introduces trade-offs, such as balancing the retention of long-range dependencies in the code with reducing input size to improve performance. This leads to the use of program slices, non-contiguous sections of program containing only code executed for a specified behavior; these include code gadgets, abstracted slices intended for AVD training. ML-based AVD solutions must additionally vectorize their input source code to a numeric representation. In recent research, the most prevalent vectorization method is generating embeddings at the word, sub-word, or sentence level, to capture code semantics. Embeddings can be learned from scratch through training, or directly generated from a pre-trained embedding model (e.g., word2vec [20], Glove [21], sent2vec [22], and doc2vec [23]).\nBinary code is a program representation produced by compiling the source code. It is primarily used by dynamic methods, though static analysis of binary code is possible [24].\nIntermediate representations (IRs) were developed to aid analysis of source code by compilers but have become widely-used outside of compilation. IRs can be instruction sets (such as bytecode) or specialized abstract data structures. The common IR data structures in the AVD literature are:\n1) Abstract syntax tree (AST): captures the syntactic structure of source code; nodes represent language constructs (e.g., variables, if statements) and edges show that the parent node operates on the sub-trees rooted at the child nodes (e.g., if statements will typically connect to their condition, then-branch, and else-branch).\n2) Control flow graph (CFG): captures all paths that a program may execute; nodes represent continuous blocks of code connected by directed edges that represent jumps (e.g., if statements, function calls).\n3) Data flow graph (DFG): captures how data moves between program elements modeling dependencies between values; nodes represent expressions that hold values and edges show the flow of data.\n4) Program dependency graph (PDG): captures both data and control dependencies in a single graph.\n5) Code property graph (CPG): combines information from the AST, CFG, and PDG into a single structure.\nIRs for AVD are typically generated using off-the-shelf tools, such as ANTLRv4 [25] and astminer [26] for ASTs or Joern [27] for C/C++ CPGS."}, {"title": "3.3. Machine Learning Architectures", "content": "The AVD literature uses a variety of ML techniques, including the following common neural network architectures.\n1) Convolutional Neural Networks (CNN) are neural networks developed for grid-like data (e.g., images). CNNs apply learnable filters (called kernels) to sliding windows of the data and match patterns through a convolution operation. CNNs are very efficient, as the kernels' parameters are shared across dimensions, significantly reducing the number of learnable parameters.\n2) Recurrent Neural Networks (RNN) are neural networks primarily designed for sequential data. The core component of an RNN is the recurrent cell. It keeps a hidden state summarizing past inputs, enabling RNNs to learn which information to retain or forget. However, basic RNNs [28] tend to easily forget inputs early in the sequence, so several improvements have been proposed, such as long short-term memory (LSTM) [29] and gated recurrent units (GRU) [30]. In addition, bi-directional RNNs [31], such as bi-directional LSTMs (BLSTM) and bi-directional GRUS (BGRU), are RNN variants in which the sequences can communicate both forward and backward. These are particularly useful when information later in an sequence can affect earlier information.\n3) Graph Neural Networks (GNN) are neural networks that support learning from graph structures, where the information of each node is represented as a vector. GNNs combine messages from neighboring nodes using an aggregation function, such as sum, mean, or max. The aggregated information is then transformed to produce richer context-aware representations of nodes. The aggregation-transformation step is repeated for a number of layers to obtain representative node embeddings. GNNs can be applied to graphs irrespective of their dimensions (e.g., the number of nodes or edges).\n4) Large Language Models (LLM) are transformer-based [32] neural networks that model natural language with a high number of parameters, from hundreds of millions [33] to trillions [34]. The primary innovation behind LLMs lies in the attention mechanism, which allows the model to selectively focus on different parts of the text. LLMs can be general purpose (e.g. ChatGPT [34]) or specialized in a field (e.g., Code-BERT [35]). LLMs are initially pre-trained on large text corpora in order to generate coherent and high-quality text. Next, pre-trained models can be adapted to a task through fine-tuning on a task-specific dataset."}, {"title": "4. Methodology", "content": "Our research methodology comprises an article collection phase and multiple screening phases."}, {"title": "4.1. Article Collection", "content": "We rched four research databases that cover the vast majority of research venues in both the security and machine learning fields: IEEEXplore, ACM Digital Library, ScienceDirect, and arXiv. This methodology ensures a broader and more representative sample than including only articles from a list of selected top venues [36], as done by other SoKs [37]. We applied the following search terms: 1) The article title must include either \u201cvulnerability detection\" or \"vulnerability discovery\", and 2) The body of the article must include \"software\". To include relevant articles that may have been missed by the above search or were not available in the considered databases, we conducted a manual search based on examining existing AVD surveys. The collection process resulted in 965 articles, and is summarized in Figure 2."}, {"title": "4.2. Article Screening", "content": "To ensure the relevance and impact of the included articles, we performed three rounds of screening: topic-based, citation-based, and full-text screening."}, {"title": "4.2.1. Topic-Based Screening", "content": "We used Rayyan [38] to conduct automatic article deduplication, as well as title and abstract-based screening. Articles were included if they satisfied the following criteria:\n1) Proposed a novel AVD solution (excluding surveys, empirical studies, or datasets).\n2) Focused on detection, not exploitation or repair.\n3) Addressed security vulnerabilities, excluding syntactic or semantic bugs.\n4) Targeted vulnerabilities in software applications, not communication networks or smart contracts.\n5) Excluded detection of malicious software.\n6) Were full papers, not theses, posters, or tutorials.\nUsing these criteria, we reduced the selection to 395 articles. In addition, this screening phase identified 76 empirical studies, which will be studied separately in Section 6."}, {"title": "4.2.2. Citation-Based Screening", "content": "To only include articles with a significant impact in the field, we further filter the above 395 articles on citation count adjusted for their age to fairly compare research of varying ages [39]. We calculate citation frequency $R_c$ for each article as $R_c = \\frac{N_c}{A}$, where $N_c$ is the number of citations achieved by an article as recorded on the 5th of September 2024, and $A$ is the article's age in years. The age is computed as $A = Y_n - Y_p$, where $Y_n = 2024.67$ is the current year (corresponding to the 5th of September), and $Y_p$ is the year of publication for the article. As cited computer science articles had a mean of five citations after one year from publication [40], we set a threshold of $R_c \\geq 5$, resulting in 97 articles."}, {"title": "4.2.3. Full-Text Screening", "content": "The third and final screening phase is based on the full text of the remaining 97 articles. In this phase, we manually excluded duplicate articles that were not automatically detected by Rayyan, as well as articles deemed irrelevant based on the full text (e.g., exploitation solutions without a detection component). Figure 3 presents a histogram illustrating the distribution of the 79 included articles published between 2006 and 2024."}, {"title": "5. Analysis of AVD Research", "content": "Our systematization of the 79 selected articles follows Figure 4, encompassing various components of the AVD pipeline: task definition (Section 5.1), input software (Section 5.2), detection approach (Section 5.3), evaluation (Section 5.4), and performance (Section 5.5). For each component we present our insights and recommendations."}, {"title": "5.1. AVD Tasks", "content": "We analyze the different tasks tackled by AVD research, covering the formulation of the AVD problem and the granularity at which detection occurs."}, {"title": "5.1.1. Problem Formulation", "content": "The surveyed literature studies three formulations of AVD as a classification problem:\nBinary Classification determines if the given software contains a security vulnerability, without providing additional information about the vulnerability.\nBinary CWE Classification determines if the given software contains a specific security vulnerability represented by its common weakness enumeration identifier (CWE-ID). A classification as \"not vulnerable\" does not rule out susceptibility to other vulnerabilities.\nMulti-Class CWE Classification determines which vulnerability type (i.e., CWE-ID) the software contains, with an additional \"not vulnerable\" class.\nTable 1 shows the distribution of the above formulations within AVD research. Note that some articles study multiple formulations. The table shows that Binary classification dominates the literature with 89.9% of the articles, while only 10.1% address Multi-class CWE classification, and just one article [108] focuses on Binary CWE classification. The latter two tasks are closely related: an ensemble of binary CWE classifiers can serve as a multi-class CWE classifier. Importantly, both formulations provide information about the vulnerability types. This knowledge is of significant practical value during the software development life-cycle, directly supporting the patching process. Nevertheless, these two formulations are underrepresented in AVD research, likely due to the increasing complexity of classification problems as the number of vulnerability types grows (the number of CWE-IDs exceeds 600 [114]), requiring more sophisticated solutions and diverse datasets. Consequently, we recommend that future AVD research explore finding a practical number of classes that strikes a balance between performance and interpretability in multi-class formulations. This can be done, for instance, by grouping similar vulnerability types into core categories, along the lines of [110]."}, {"title": "5.1.2. AVD Detection Granularity", "content": "The software classified by an AVD solution can have one of the granularity levels defined in Section 3.2. Note that we refer here to the detection granularity not that of the input software, though these two are oftentimes the same. The distribution of the detection granularity in AVD research is illustrated in Table 1. The table shows that AVD solutions with coarser granularities (i.e., project, commit, and file) are uncommon, totaling 11 articles (13.9%). Furthermore, extremely fine-grained line-level AVD solutions are also rare (6.3%), with no representation in multi-class AVD formulations. Medium granularities snippet, function, and slice represent the majority of AVD literature, among which, function-level detection is by far the most common. This is likely because it is easier to extract and annotate vulnerable and non-vulnerable functions from any software, compared to extracting say, code gadgets or commits.\nWe argue that different detection granularities can be more suitable for different stages of software development. For instance, finer-grained detection (e.g., line-level) are most informative to flag potential issues while the code is being written by the developer, but are typically less accurate [8]. The issue of alert fatigue can then be mitigated by calibrating for low FPR. At later stages, commit-level AVD can be extremely useful for attribution, allowing a vulnerable commit to be easily reverted. As a result, recent initiatives like DARPA's AIxCC competition [2] highlight the potential of commit-level AVD, encouraging the development of suitable AVD datasets and solutions. For the above reasons, we recommend further research in developing both line-level and commit-level AVD."}, {"title": "5.2. Input", "content": "We analyze the types of input to AVD solutions with regards to programming languages and input representations."}, {"title": "5.2.1. Programming Languages", "content": "Programming languages vary in susceptibility to vulnerabilities, and prevalence in software development.Therefore, we conduct a comprehensive, multi-faceted analysis of how the current AVD research landscape maps to the real-world with regards to programming languages. We consider the popularity of languages in real-world software development, and the relative susceptibility of a language to vulnerabilities.\nFigure 5 compares the languages across multiple metrics. The first metric is the distribution of languages among AVD research, which shows that the vast majority of articles focuses on C/C++ (72%). This may be explained in part by the high proportion of widely used AVD datasets comprising of C/C++ code (c.f., Section 5.4.2). Additionally, C and C++ are notorious for memory-based vulnerabilities, which make up 40% of the MITRE 2023 top-25 dangerous software weaknesses [115]. As such, C/C++ may be more prone to vulnerabilities than other popular languages. This leads to our second metric, the proportion of MITRE's 2023 Top-25 dangerous software weaknesses [115] to which a language is vulnerable. The top languages (C/C++, Java and PHP), are all equally vulnerable.\nHowever, some vulnerabilities are much more prevalent in real-world software. The Top-25 weaknesses are determined using a danger score that combines the detection frequency of a vulnerability and its severity. Therefore, the Top-25 inherently includes prevalence, but the score discrepancy is wide, with scores of more than 60 and less than 4. This motivates our third metric, which captures the overall severity of a language's vulnerabilities. For each language, we compute a combined danger score by adding the danger scores of its vulnerabilities, then normalizing it by the sum of all the Top-25 vulnerabilities. From this, PHP is the most vulnerable language, followed by C/C++, Java, and JavaScript. Yet, PHP currently makes up less than 10% of AVD research, disproportionate to its relative vulnerability.\nFor capturing the prevalence of a language, we use the 2024 IEEE Spectrum Top Programming Languages ranking [116]. Languages are ranked as a proportion of the most popular language, Python, with C/C++ ranked second, and Java third. Despite this, Python is only represented in less than 10% of AVD research. Another metric for prevalence is the relative proportion of GitHub pushes from Q1 2024. According to this metric, JavaScript is the most prevalent, followed by Python, and then C/C++.\nThe above multi-faceted analysis uncovers obvious discrepancies between AVD research and practical software development needs. Future research should diversify its focus to include languages such as Python and JavaScript, due to their significant popularity, along with Java and PHP due to their susceptibility to vulnerabilities. Expanding research into these languages would increase the relevance and applicability of AVD solutions in real-world projects. However, it is not completely surprising why C/C++ dominates current AVD research, given both its susceptibility to vulnerabilities and its widespread use in open-source projects."}, {"title": "5.2.2. Input Representation", "content": "We categorize the AVD literature according to the most commonly employed input representations in Table 2. Source code is used by almost all the surveyed articles (71/79 \u21d2 89.9%), either alone or in conjunction with other representations. Among these, 28 articles (35.4%) used only raw source code, without any other representations.\nIntermediate representations (IRs) (c.f., Section 3.2) are also commonly-used in AVD solutions. Abstract syntax trees are the second most common representation among the included articles (35.4%). They are predominantly used in concert with GNNs [6], [58], [61], [77], where the AST acts as (part of) the graph underlying the message-passing mechanism. ASTs have also been used as input to RNN-based AVD solutions [49], [67], [68], [104]. Finally, ASTs have been used as an additional context in the prompt of LLM-based AVD solutions [60]. Individual AST nodes are vectorized using embeddings (c.f., Section 3.2), and apart from when used with GNNs, the whole AST can be serialized using depth-first or breadth-first traversal.\nSimilar to ASTs, control flow graphs were primarily used in AVD as the underlying graph of GNNs [6], [61], [77], as a part of the data pre-processing pipeline [49], or as an input to an ML model in a vectorized form [65]. Program dependency graphs were used with GNN-based AVD solutions [57], [58], [105] and as part of pre-processing [63], [110]. The code property graph is another commonly-used graph representation, and has been used with GNNs [55], or as additional context in the prompt of LLM-based AVD solutions [73]. Other AVD works also used data flow graphs in GNN-based AVD solutions [61], or in the prompt of LLM-based AVD solutions [60].\nFinally, a few works used binary code. Features extracted from binary code can be used as the sole input to different ML classifiers (e.g., CNN [43] and RNN [24]), or can be combined with dynamic features extracted from execution traces [41]. Alternatively, the binary code can be analyzed using symbolic execution [42].\nAlongside source-code, we argue that more research into using binaries can be of significant practical importance. First, analyzing binaries allows for language-agnostic AVD solutions. Furthermore, solutions based solely on source code will fail to detect vulnerabilities and bugs that manifest themselves with specific compilers or operating systems [117]."}, {"title": "5.3. AVD Approaches and Solutions", "content": "This section categorizes the most common approaches utilized for solving AVD tasks and presents a selected list of frequently cited AVD solutions.\n5.3.1. Approaches. Several approaches have been applied to solve AVD tasks, including both traditional non-learning approaches and more recent ML approaches. The distribution of the most common approaches is shown in Table 3. Furthermore, Figure 6 demonstrates the trend in popularity of these approaches in AVD research. From Table 3, GNNs top the list of the most popular approaches with 31.6% of the articles. Starting to appear in 2019, and virtually dominating the field since 2021, GNNs excel at capturing the code structure (often utilizing an IR, c.f., Section 5.2.2). Among GNN approaches, common variants include the seminal graph convolution networks (GCNs) [118], gated graph neural networks (GGNNs) [119] (combining GNNs and RNNs), relational GCNS (RGCNs) [120] (allowing for different edge types), and graph attention networks (GATs) [121] (utilizing the attention mechanism).\nRNNs have also been extensively used for AVD (22.8%), and have been particularly popular between 2017 and 2023. RNNs are good at modeling sequential data, but their inability to use parallel processing introduces bottlenecks. Among RNN approaches, bi-directional variants (e.g., BLSTM and BGRU) are prevalent, capturing code dependencies in both the forward and backward directions. The third most popular approach, despite its recency, is LLMs, starting to gain popularity in 2022. Relying on the transformer architecture, LLMs can both capture long-range code dependencies, and support fast parallel processing. Pre-trained LLMs can be adapted to AVD tasks either through prompt engineering, or through fine-tuning on task-specific AVD datasets.\nOther less commonly-used approaches include CNNs, in addition to a few non-learning approaches. These include computing function signatures (typically based on hash values) for evaluating code similarity, and taint analysis.\nOverall, software developers can benefit from AVD approaches that are more informative and interpretable. LLMs can provide explainability through textual elaboration for their predictions, but the accuracy of these explanations is hard to evaluate. GNN approaches naturally lend themselves to interpretability through highlighting nodes and edges that are more responsible for its output [122], [123]. However, current research on the interpretability and explainability of AVD approaches is insufficient, constituting a significant barrier to practical deployment."}, {"title": "5.3.2. Influential Solutions", "content": "The articles we surveyed use a set of influential AVD solutions as baselines. Overall, our 79 articles used a median of 3 baselines per article. Table 4 catalogs the most common baselines, distinguishing between learning-based and non-learning based solutions. The most commonly used baselines are either RNN-based [5], [49], GNN-based [6], [55], [58], or rule-based [3], [4], [124]. However, rule-based solutions are recently being used by fewer articles as baselines, with only one article [98] since 2023. Moreover, the top-3 baselines (VulDeePecker [5], Devign [6], and SySeVR [49]) are used in at least 20 articles (25.3%), which shows their importance to the field despite their age. A notable recent solution is Linevul [8], which is commonly used as a baseline despite being proposed in 2022. We recommend that new AVD research compares proposed solutions to a wide variety of learning-based (the aforementioned four) and non-learning-based solutions (e.g., Flawfinder [3]), to ensure the validity of their results. Doing so would increase confidence in the results reported, which in turn encourages applicability in software development."}, {"title": "5.4. Evaluation Metrics and Datasets", "content": "AVD solutions are evaluated on different datasets and report heterogeneous metrics. To aid consistent evaluation in AVD, we systematize these aspects."}, {"title": "5.4.1. Evaluation Metrics", "content": "Figure 7 shows the most commonly-used metrics to evaluate AVD solutions. Due to the ubiquity of binary classification in AVD (c.f., Section 5.1.1), the majority of the metrics are specific to that problem formulation:he fundamental quantities in classification are: Accuracy, Recall (True Positive Rate), False Positive Rate (FPR), and the area under the ROC curve (ROC-AUC). Due to the typical imbalance between vulnerable and non-vulnerable code in practice, AVD research also uses metrics that are less common in other domains: the Area under the Precision-Recall curve (PR-AUC), and the Matthews correlation coefficient (MCC). Some recent works [50], [66] have used Informedness (IFN = Recall - FPR) and Markedness (MKD = Precision - FNR). These are unbiased versions of Recall and Precision, respectively, that consider the number of true negatives (TN) [136].\nNevertheless, with the exception of ROC-AUC and PR-AUC, the values of the aforementioned metrics are dependent on an arbitrary value of a detection threshold, which hinders a fair and precise comparison between solutions. In addition, an AVD solution can be more sensitive to high FPR during software development. Therefore, we believe that the Recall value at a chosen FPR (denoted as Recall@FPR) is a practical metric of the AVD solutions' performance, and we thus encourage new research to use it along with the widely-used metrics (e.g., F\u2081). Recall@FPR should be reported for a comprehensive range of FPRs."}, {"title": "5.4.2. Evaluation Datasets", "content": "The included AVD articles exhibit wide variability in their evaluation datasets. The majority (72.2%) either created new datasets from scratch or extracted subsets of openly-available vulnerability databases, while 32.9% used existing benchmark datasets (one article may use both dataset types). Out of the 13 identified, the nine most frequently used evaluation benchmark datasets are shown in Table 5. From the table, the most common datasets are all function or slice-level C/C++ datasets (similar to the most common solutions, c.f., Section 5.3.2). The function-level datasets are typically built by extracting the code changes from security-related commits in open-source projects (e.g., Devign [127]: FFMPeg+Qemu, Reveal [128]: Linux Debian kernel and Chromium, Big-Vul: 348 projects, D2A: OpenSSL, FFMpeg, httpd, NGINX, libtiff, and libav). On the other hand, most slice-level datasets [130], [132], [135] are extracted from two sources maintained by the US National Institute of Standards and Technology (NIST): the National Vulnerability Database (NVD) [137], which contains real-world programs, and the Software Assurance Reference Dataset (SARD) [138], which contains both real and synthetic programs.\nWe observed that most existing AVD datasets lack fixed train-test splits, which are essential for a consistent and fair evaluation of solutions. Only two datasets [131], [134] in Table 5 include fixed splits, we recommend that future AVD datasets should include fixed splits, and preferably, keep a hidden test set (with a public leaderboard) in order to prevent data leakage and memorization issues. In that regard, it is worth mentioning that the CodeXGLUE [134] defects dataset is, in essence, a fixed train-test split of the aforementioned Devign dataset.\nMoreover, since all the datasets in Table 5 contain C/C++ programs at either the function (55.6%) or slice level (44.4%), future AVD datasets should represent a wider variety of programming languages (Section 5.2.1) and granularities (Section 5.1.2). Furthermore, the table shows that two common datasets (i.e., D2A and draper) utilize static analyzers for annotation. Although this facilitates the rapid creation of larger datasets, this methodology has come under sharp criticism due to incorrect annotation [55]. Finally, the aforementioned datasets are slightly outdated, at least three years old, and therefore lack knowledge of recently discovered vulnerabilities. More recent datasets, such as PrimeVul [139], MegaVul [140] VulBench [141], and DiverseVul [142], should be used in conjunction with the classical datasets, for evaluating AVD solutions."}, {"title": "5.5. Performance Meta-analysis", "content": "We quantitatively evaluate the progress of the AVD field towards practical applicability by comparing the performance of existing AVD solutions (as reported in the articles). We consider the three most-used AVD datasets: Devign [127", "128": "and Big-Vul [129", "reasons": 1, "6": "which is the article that originally introduced the dataset. Subsequent work [55"}, {"6": "were not reproducible. Apart from that", "are": 1, "92": "F\u2081 = 67.7), a cross-domain solution leveraging GGNN and domain adaptation (DA) [143"}]}