{"title": "Time is on my sight: scene graph filtering for dynamic environment perception in an LLM-driven robot", "authors": ["Simone Colombani", "Luca Brini", "Dimitri Ognibene", "Giuseppe Boccignone"], "abstract": "Robots are increasingly being used in dynamic environments like workplaces, hospitals, and homes. As a result, interactions with robots must be simple and intuitive, with robots' perception adapting efficiently to human-induced changes.\nThis paper presents a robot control architecture that addresses key challenges in human-robot interaction, with a particular focus on the dynamic creation and continuous update of the robot's state representation. The architecture uses Large Language Models to integrate diverse information sources, including natural language commands, robotic skills representation, real-time dynamic semantic mapping of the perceived scene. This enables flexible and adaptive robotic behavior in complex, dynamic environments.\nTraditional robotic systems often rely on static, pre-programmed instructions and settings, limiting their adapt-ability to dynamic environments and real-time collaboration. In contrast, this architecture uses LLMs to interpret complex, high-level instructions and generate actionable plans that enhance human-robot collaboration.\nAt its core, the system's Perception Module generates and continuously updates a semantic scene graph using RGB-D sensor data, providing a detailed and structured representation of the environment. A particle filter is employed to ensure accurate object localization in dynamic, real-world settings.\nThe Planner Module leverages this up-to-date semantic map to break down high-level tasks into sub-tasks and link them to robotic skills such as navigation, object manipulation (e.g., PICK and PLACE), and movement (e.g., GOTO).\nBy combining real-time perception, state tracking, and LLM-driven communication and task planning, the architecture enhances adaptability, task efficiency, and human-robot collaboration in dynamic environments.", "sections": [{"title": "1. Introduction", "content": "Immediacy is crucial in assistive robotics [1, 2, 3]. In a typical human-robot interaction scenario, users may provide commands in natural language, such as \u201cPick the blue bottle on the table and bring it to me\". To such aim, the use of Large Language Models (LLM) allows robots to interpret natural language requests and \u201ctranslate\u201d instructions into plans to achieve specific goals; yet, these models need to know the environment in which they operate so to generate accurate plans [4]. The need for translation arises from the complexity of human language and the variability in instructions. Users may express commands differently or exploit ambiguous terms that the robot must comprehend. To address these challenges, robotic architectures must integrate natural language processing with environmental understanding.\nThe chief concern of the work is to exploit scene graphs as semantic maps providing a structured representation of spatial and semantic information of robot's environment. This enables LLMs to generate plans based on this information. Indeed, via scene graphs robots can map the relationships between objects, their properties, and their spatial arrangements.\nHere we address such limitations by representing the environment as a graph endowed with updatable"}, {"title": "2. Related works", "content": "A scene graph captures detailed scene semantics by explicitly modeling objects, their attributes, and the relationships between paired objects (e.g., \u201cblue bottle on the table\") [5]. 3D scene graphs [6] extend this concept to three-dimensional spaces, representing environments like houses or offices, where each piece of furniture, room, and object is a node. The edges between these nodes describe their relationships, such as a vase on a table or a chair in front of a sofa.\nRecent works, such as [7] and [8] have proposed to generate 3D scene graphs from RGB-D images, combining geometric and semantic information to create detailed environmental representations. Scene graphs have been widely used in computer vision and robotics to improve scene understanding, object detection, and task planning. For example, SayPlan [9] integrates 3D scene graphs and LLMs for task navigation and planning, performing semantic searches on the scene and instructions to create accurate plans, further refined through scenario simulations. DELTA [10] utilizes 3D scene graphs to generate PDDL files, employing multiple phases to prune irrelevant nodes and decompose long-term goals into manageable sub-goals, enhancing computational efficiency for execution with classical planners. SayNav [11] constructs scene graphs incrementally for navigation in new environments, allowing the robot to generate dynamic and appropriate navigation plans in unexplored spaces by passing the scene graph to a LLM, thus facilitating effective movement and execution of user requests.\nIn a crude summary, the main limitations of the above mentioned approaches to build environment representations lie in their reliance on computationally heavy vision-language models (VLMs) and computer vision models. Such models are not designed for precision and often demand significant resources, while lacking the ability to be updated in real time, and thus limiting their practical application."}, {"title": "3. Architecture", "content": "Our system is based on two components:\nPerception Module: it is responsible for sensing and interpreting the environment and building a semantic map in the form of a directed graph that integrates both geometric and semantic information. Its architecture is explained in detail below.\nPlanner Module: it takes the information provided by the Perception Module to formulate plans and actions that allow the robot to perform specific tasks. It is composed by the following:\nTask Planner: Translates user requests, expressed in natural language, into high-level skills.\nSkill Planner: Translates high-level skills into specific, low-level executable actions.\nExecutor: Executes the low-level actions generated by the Skill Planner.\nController: Monitors the execution of actions and manages any errors or unexpected events during the process.\nExplainer: Interprets the reasons of execution failures by analyzing data received from the Controller and provides suggestions to the Task Planner on how to adjust the plan.\nThese components interact to allow the robot to understand its environment and act accordingly to satisfy user requests. In what follows we specifically address the Perception Module while details on the planner will be provided in a separate article."}, {"title": "3.1. Perception module", "content": "The Perception Module is the component responsible for building a representation of the environment, which the robot can use for task planning. The representation takes the form of a semantic map, a graph that integrates both geometric and semantic information about the environment. To generate the semantic map, the perception module uses data from various sensors. It requires RGB-D frames obtained from the camera which are then processed using a scene graph generation model, such as PSGTR [12] to extract objects masks, label and relationships. Also it uses data on the camera position relative to the geometric map to determine the location of the objects identified by the model. More formally, a Semantic Map is represented as a directed graph Gm = (Vm, Em) where:\nA node v \u2208 Vm can be one of the following types:\nRoom node: Defines the different semantic areas of the environment, such as \"kitchen,\" \"living room,\u201d or \u201cbedroom.\u201d Each room node contains information about its geometric boundaries and the object nodes it contains;\nObject node: Represents physical objects in the environment, such as \u201ctable,\u201d \u201cchair,\u201d or \"bottle.\" Each object node contains information about its 3D position, semantic category, dimensions, and other relevant properties:\nAn edge e \u2208 Em can represent:\nThe relationship between two objects;\nThe connection between two rooms;\nThe belonging of an object to one and only one room.\nThe presence of room nodes is important because it facilitates the categorization of objects based on their respective rooms, which helps distinguish between objects with the same name and enhances the natural language description of the task, while room nodes enable the application of graph search algorithms for planning paths to objects. Room nodes are created based on the geometric map, while object nodes are generated following the steps explained below.\nAs to edges, more specifically:\nEdges between rooms directly connect two rooms and facilitate navigation between them.\nEdges between objects represent the relationships between objects and are directed, the direction capturing the influence of one object on another; the label associated with each edge is derived from the inferences made by the PSGTR model.\nGenerating and updating the semantic map The scene graph generation process is based on the PSGTR model, a single-stage model built on the Transformer architecture [13]. This model generates a graph representation of a scene given its panoptic segmentation. PSGTR does not achieve the highest quality in panoptic segmentation compared to better models, but it provides reasonable inference times for real-time applications, taking about 400 ms to process a 480p image on a machine with access to an NVIDIA T4 GPU.\nThe Perception Module uses the result of PSTGR and builds the semantic map following the steps below:\nReading RGB-D frames: The video frames from the robot's cameras are sent to the model to be analyzed and used to generate the scene graph."}, {"title": "4. Graph construction", "content": "This step involves extracting data from the object returned by the model and computing values dependent on the robot system, such as the position of objects. At a finer level it consists of three sub-steps:\na) Node construction: Classes and masks of detected objects are extracted. Next, the 3D position of each object is computed, starting in the pixel coordinate system, then transform-ing to the camera system, and finally to the robot's map coordinate system. Nodes for the semantic scene and the semantic map are instantiated using the appropriate 3D coordinates. A distance-based filter is applied to prune objects that are too far from the robot to avoid issues with object detection and tracking.\nb) Edge construction: Data about relationships between objects are extracted. For each relationship, the source and target object indices are identified. If both objects meet distance constraints and the relationship probability exceeds a defined threshold, an edge is created between the corresponding nodes.\nc) Inference improvement through Particle Filter (PF): As the model's output is not accurate regarding mask inference, this leads to errors in calculating the object's centroid for obtaining its position relative to the map. A PF based on previous observations is applied to improve the accuracy of the result.\nAt the end of the process, the semantic map is updated with the new information, and the semantic scene is generated and provided to the planner module.\nThe PF is used to track the object masks in real-time, provided as output by the PSGTR model, and to improve the estimation of their position in space. During the update process, the filter uses information from frames acquired to refine the position estimate of the objects. The last object masks identified by the PSGTR model are compared with previous ones using the Intersection over Union (IoU) metrics and by applying the motion model, which can be defined as a transformation of the camera position relative to the map between two time instances. Denote the transformation matrices describing the camera position at time t 1 and at subsequent time t, Tt\u22121 and Tt, respectively; then, the change in position and orientation can be expressed by the transformation matrix \u2206T = T\u0141T7_11. To associate objects between successive frames, we use an IoU matrix computed over segmentation masks. For two masks A and B, IoU is defined as IoU(A, B) = \\frac{| A\u2229B|}{|AU B|}, where | A\u2229B| represents the area of intersection between masks A and B, and |AU B| represents the area of their union. To compare segmentation"}, {"title": "4. Conclusions", "content": "Scene graphs provide a structured representation that captures geometric and semantic information about the environment. This comprehensive understanding enables improved task planning with large"}, {"title": "Algorithm 1 Semantic Map update using Particle Filter", "content": "1: for each frame t do\n2: for each object k do\n3: Apply transformation: M\u22121 = \u2206T\u00b7 Mt-1\n4: end for\n5: Compute IoU(M-1, Mt) = \\frac{M\u22121\u2229Mt}{M\u22121\u222aMt}\n6: for each object k do\n7: if IoU > Iou then\n8: Update weights: d = ||s \u2212 Snew ||2, w = \\frac{1}{1+d}\n9: Normalize: w =\n10: Estimate: St = \u03a3i=1N \u03c9i Si\n11: end if\n12: end for\n13: for each unmatched observation do\n14: Init new object: s ~ \u039d(\u03bc\u03bf, \u03a3\u03bf)\n15: Update semantic map with st\n16: end for\n\u03c9t\n\u03a3i=1 \u03c9i\n\u25b6 Transform previous masks\n\u25b6 Compute IoU between nodes and inference results\n17: end for"}, {"title": "5. Online Resources", "content": "More information about RoBee and Oversonic Robotics are available:\n\u2022 RoBee,\n\u2022 Oversonic Robotics"}]}