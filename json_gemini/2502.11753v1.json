{"title": "HINTSOFTRUTH: A Multimodal Checkworthiness Detection Dataset with Real and Synthetic Claims", "authors": ["Michiel van der Meer", "Pavel Korshunov", "S\u00e9bastien Marcel", "Lonneke van der Plas"], "abstract": "Misinformation can be countered with fact-checking, but the process is costly and slow. Identifying checkworthy claims is the first step, where automation can help scale fact-checkers' efforts. However, detection methods struggle with content that is (1) multimodal, (2) from diverse domains, and (3) synthetic. We introduce HINTSOFTRUTH, a public dataset for multimodal checkworthiness detection with 27K real-world and synthetic image/claim pairs. The mix of real and synthetic data makes this dataset unique and ideal for benchmarking detection methods. We compare fine-tuned and prompted Large Language Models (LLMs). We find that well-configured lightweight text-based encoders perform comparably to multimodal models but the first only focus on identifying non-claim-like content. Multimodal LLMs can be more accurate but come at a significant computational cost, making them impractical for large-scale applications. When faced with synthetic data, multimodal models perform more robustly.", "sections": [{"title": "1 Introduction", "content": "Online misinformation spreads rapidly via social networks and deceptive websites posing as legitimate news sources (Del Vicario et al., 2016; Rocha et al., 2021; Ecker et al., 2024). This influences voting behavior (Ribeiro et al., 2017) and pollutes the digital information space (Greenspan and Loftus, 2021; Sharma et al., 2019). Misinformation tactics include decontextualization (e.g., wrongly presenting image-based evidence) and providing incomplete information (Kreps et al., 2022). Generative AI, like ChatGPT (OpenAI, 2023) for text and Midjourney (Midjourney, Inc., 2023) for images, has worsened the issue by enabling large-scale alteration or fabrication of news narratives (Zhou et al., 2023; Chen and Shu, 2023). Given these developments, continuous verification of multimodal information is a key challenge (Abdelnabi et al., 2021; Singh and Sharma, 2022).\nMedia gatekeepers, including news publishers and fact-checking services, verify content veracity, but manual fact-checking is costly and time-consuming (Nakov et al., 2021). Therefore, selecting which claims to fact-check is a major challenge, as misinformation far exceeds verification capacity. Automated approaches can help by identifying checkworthy claims (Nakov et al., 2018; Konstantinovskiy et al., 2021), see Figure 1 for an example, or in other stages in the fact-checking pipeline (Figure 2).\nHowever, existing automated checkworthiness detection methods (1) have poor support for multimodal content, (2) have only been tested in a handful of domains, (3) have unknown capabilities on synthetic media, and (4) do not consider compute cost as a factor. (Akhtar et al., 2023). First, modern misinformation often includes mixed forms of media, such as images or videos (Dufour et al., 2024), yet it is unclear if detection methods effectively integrate visual data (Alam et al., 2023). Second, strategies for misinformation detection vary by domain (Ecker et al., 2022; Chen et al., 2021; Lasser et al., 2023), raising concerns about generalizability, especially, for practical applications (Jiang and Wilson, 2018; Monteith et al., 2024). Third, ubiquitous access to generative models is reshaping misinformation (Xu et al., 2023), warranting the evaluation of detection methods on synthetic content. Lastly, while Large Language Models (LLMs) perform well, their high compute cost may render large-scale checkworthiness detection impractical (Augenstein et al., 2024), though the exact tradeoffs are unknown.\nThis paper introduces HINTSOFTRUTH, the first publicly available multimodal dataset of image-text pairs containing both real-world and synthetically generated checkworthy and non-checkworthy claims. We source real claims from datasets like 5Pils (Tonglet et al., 2024), Multiclaim (Pikuliak et al., 2023), Flickr30K (Hodosh et al., 2013), and SentiCap (Sharma et al., 2018). Synthetic images and text are generated using Flux (Black Forest Labs, 2024), StableDiffusion 3.5 (Stability.ai, 2024, SD), Llava (Li et al., 2024), and BLIP (Li et al., 2022). We evaluate recent text and image models, from lightweight ones like TinyBERT (Jiao et al., 2020) for scalability to large, multimodal models like Pixtral (Mistral, 2024). These evaluations reveal model limitations and guide practical decisions in checkworthiness detection."}, {"title": "Contributions", "content": "We present: (1) HINTSOFTRUTH, a novel dataset for multimodal checkworthiness detection from diverse sources, with an established connection between images and textual claims, that can be used as a benchmark for checkworthiness detection models, (2) synthetic counterparts of images and claims in the dataset, which has not been explored in the context of checkworthiness, and (3) an extensive set of experiments demonstrating the limits of state-of-the-art detection methods."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Human-Centered Fact-Checking", "content": "Recent research on human-centered AI has emphasized developing tools that augment humans (Akata et al., 2020; Nakov et al., 2021). In the field of fact-checking, such tools would complement human fact-checkers in their work (Micallef et al., 2022; Graves, 2017), allowing experts to control what and how to fact-check (Das et al., 2023)."}, {"title": "2.2 Misinformation in the Age of LLMs", "content": "LLMs play a significant role in both detecting and generating misinformation. Recent work integrates LLMs into fact-checking frameworks (Geng et al., 2024), although methods are shown to generalize poorly across time (Stepanova and Ross, 2023). Nonetheless, LLMs look promising when applied to text-based checkworthiness detection (Majer and Snajder, 2024). Reviews of LLM-generated multimedia highlight the open challenges (Lin et al., 2024; Augenstein et al., 2024). For instance, large amounts of synthetic misinformation have the potential to impact the quality of future LLMs (Pan et al., 2023), and misinformation generated by GPT-4 may be harder to detect than that written by humans (Chen and Shu, 2023)."}, {"title": "2.3 Multimodal Resources", "content": "Existing work on fact-checking emphasizes empirical research, which involves extensively benchmarking fact-checking methods (Schlichtkrull et al., 2023; Papadopoulos et al., 2024), often using distant supervision (Nakamura et al., 2020; Zlatkova et al., 2019). Most multimodal datasets investigate the out-of-context use of images and claims (Luo et al., 2021; Tonglet et al., 2024), or whether claims are reflected in an image between claim and image (Yoon et al., 2024; Papadopoulos et al., 2023). Few datasets exist that (1) check whether the image contributes new information (Liu et al., 2024), or (2) contain synthetically generated data (Xu et al., 2023; Seow et al., 2022). The few efforts on multimodal checkworthiness indicate that textual data, whether through OCR or by focusing on claims only, is sufficient for state-of-the-art performance (Frick and Vogel, 2023). More extensive experiments on varied types of data with complex image use, across domains, are needed to further examine this finding."}, {"title": "3 Method", "content": "We introduce the multimodal checkworthiness task definition, how we obtain the real-world data underlying HINTSOFTRUTH, and how we generate synthetic samples to augment our dataset."}, {"title": "3.1 Task Definition: Multimodal Checkworthiness Detection", "content": "Given a textual claim $c$ and an image published alongside the claim $i$, predict whether the pair is worthy of fact-checking $p(i, c) = 1$. In check-worthiness detection, the following questions are answered: (Q1) Does the text contain a verifiable factual claim? (Q2) Is the claim potentially harmful, urgent, and up-to-date? The task definition is derived from Barr\u00f3n-Cedeno et al. (2020), which also formed the basis for the canonical dataset for multimodal checkworthiness detection, CheckThat! 2023 Task 1A (Alam et al., 2023).\nTo establish that an image provides meaningful context to a claim and is necessary for assessing the pair's checkworthiness, we also consider the following contextualized questions: (Q3) Is the content of the claim reflected in the image? (Q4) Does the image contribute extra information to the claim? These two questions help identify complex image use, which will test the multimodal capabilities of checkworthiness detectors (Dufour et al., 2024)."}, {"title": "3.2 Getting Checkworthy Image/Claim Pairs", "content": "We set out to obtain image/claims pairs that we can deem checkworthy. We rely on data stemming from fact-checking articles, as claims in these articles have already been checked. Fact-checking articles are written by experienced fact-checkers and contain rich contextual information. In practice, claims are often sourced from social media platforms. We obtain our data from two sources:\n5Pils (Tonglet et al., 2024). 5Pils contains extracted images, claims, and contextual questions about claims from news sources in India, Kenya, and South Sudan. Through the use of contextual questions, images in this dataset are ensured to have a relationship with the claim.\nMulticlaim (Pikuliak et al., 2023) contains URLs to a wide array of fact-checking articles and their respective claims but needs to be scraped and filtered for images. We retain those claims for which (1) we find images in close proximity, and (2) explicitly refer to visual information. See Appendix A.1 for additional details."}, {"title": "3.3 Non-checkworthy Image/Claim Pairs", "content": "We also need image/text pairs that are not check-worthy. We resort to strategies derived from the task definition for obtaining negative instances. We select samples from datasets that we consider not checkworthy because they answer 'no' to any of the guiding questions posed in Section 3.1. The strategies select:\nNon-factual (Q1) claims, such as subjective opinions or facts that cannot be verified using external information. The dataset representing this strategy is SentiCap (Sharma et al., 2018).\nNon-relevant (Q2) statements that are not harmful, not about breaking news, not up-to-date, or not relevant to news topics. The dataset representing this strategy is Flickr30K (Hodosh et al., 2013), though there are many other resources containing arbitrary image-text pairs (see Appendix C.3).\nNo cross-modal connection (Q3) images - we know have a deep connection with a text but with the image swapped to no longer make sense. The dataset representing this strategy is Fakeddit (Nakamura et al., 2020).\nTo incorporate the fourth guiding question (Q4), we filter out claims from any of the stated datasets that do not explicitly refer to multimodal content. This way, we encourage that the samples with basic image use (i.e., those pairs where the claim does not refer to the image) are excluded from our dataset. We combine the checkworthy and non-checkworthy samples into HINTSOFTRUTH, our novel multimodal checkworthiness dataset that spans multiple domains."}, {"title": "3.4 Generating Synthetic Samples", "content": "Given the risks of synthetic misinformation (Dufour et al., 2024; Papadopoulos et al., 2023; Zhou et al., 2023), we augment our dataset with additional claims and images generated using various publicly accessible models. Specifically, we employ two image generators to create images from claims and two multimodal models to generate claims from images. Our approach follows a simple cross-modality generation: models freely generate corresponding text or images without adversarial prompts (Perez and Ribeiro, 2022). This allows us to examine how checkworthiness detection models respond to synthetic data. Labels are adjusted accordingly: synthetic claims are deemed non-checkworthy, as models primarily generate non-relevant captions (see Q2 in Section 3.1), while synthetic images retain their original label, ensuring consistency with the claim's content. Checkworthy claims remain unchanged."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Data", "content": "See Table 1 for the datasets used in this paper. We use the canonical CheckThat! 2023 Task 1A dataset ('CheckThat' henceforth, Alam et al., 2023) as training dataset and reference benchmark using its predefined train/test split, which represents the in-distribution scenario (models are trained and tested on the same dataset). CheckThat has a label ratio of .66/.34 between non-checkworthy and checkworthy samples. We use the test set of our dataset, HINTSOFTRUTH (label ratio of .62/.38) for testing the detection methods fine-tuned on CheckThat, representing the cross-distribution scenario, which should be more challenging than in-distribution. We will make our dataset publicly available upon publication."}, {"title": "4.2 Multimodal Checkworthiness Methods", "content": "We experiment with various state-of-the-art text-based, image-based, and multimodal encoders for checkworthiness detection, see Table 2. We use different model sizes to investigate the tradeoff between compute cost and task performance. We include single-modality models to identify whether both modalities are needed (i.e., checkworthiness can be assessed without leveraging cross-modal information). In addition, we distinguish between encoder-only and decoder-only models, to determine the difference between fine-tuning models on multimodal checkworthiness and In-Context Learning (Dong et al., 2024). Below, we describe the experimental setup for each type of approach. Additional information is available in Appendix B."}, {"title": "Fine-Tuning (FT)", "content": "To fine-tune models for multimodal checkworthiness detection, we update all model parameters @ when predicting $p_e(i, c)$. We instantiate the models using pretrained versions, adding a single linear classification layer with a two-node output over their embeddings. We fine-tune our models with data from CheckThat using its predefined train/val/test split. Additionally, we tune a threshold parameter on the positive class probability, similar to a single-neuron sigmoid output (Zou et al., 2016; Korshunov and Marcel, 2019)."}, {"title": "In-Context Learning (ICL)", "content": "We evaluate the impact of n-shot learning (with n = {0, 1, 2, 5}) and prompt verbosity. The verbose prompt instructions include guiding questions Q1 through Q4 (see Section 3.1), while the succinct prompt only asks for an overall checkworthiness label. We experiment with two models: (1) Llava (Liu et al., 2023), using a Mistral-7B backend with a 32K token context. (2) Pixtral (Mistral, 2024), with a context size of 1024K. Both models are chosen for their compatibility with standard hardware (up to a single H100 with 80GB VRAM) and accessibility, excluding non-local proprietary LLMs. Our experiments focus on zero-shot ICL with concise instructions to minimize token usage, though multiple setups are explored in Section 5.1."}, {"title": "4.3 Research Questions", "content": "Based on the criteria discussed in Section 1, we conduct four experiments to address: (Q1) Does combining modalities influence checkworthiness detection performance? (Q2) How well do models generalize across domains? (Q3) How do models fare on synthetic data? (Q4) What is the tradeoff between compute cost and task performance?"}, {"title": "5 Results", "content": "Table 3 shows the in-distribution experiments results on CheckThat, and Table 4 demonstrates the results on the non-synthetic, real part of HINTSOFTRUTH, illustrating the cross-distribution experiments. We answer each research question individually step by step."}, {"title": "5.1 Cross-modality Performance", "content": "On the CheckThat dataset, the strongest models are multimodal BLIP and BLIP2, which form the upper bound (see Table 3). Interestingly, the accuracies of text-only encoders are close to those of BLIP and BLIP2 (up to 93% of accuracy), suggesting that little visual information is required for accurate checkworthiness detection, in line with results found in Frick and Vogel (2023). Image-only encoders also achieve only 14% lower accuracy than the upper bound. The narrow gap between single and multimodal models shows the dataset's limited suitability for assessing multimodal capabilities. ICL-based methods perform surprisingly poorly, with considerable false positive rates of 30% and 39% for Llava and Pixtral.\nFor real data of HINTSOFTRUTH, Pixtral forms the upper performance bound (see Table 4). The gap between text-only models and the upper bound is larger than in CheckThat (-14% vs. -7%). Surprisingly, TinyBERT outperforms larger text-only models and is second to only Pixtral. This suggests that a small model with a well-tuned classification threshold can be effective, which poses an interesting venue for smaller organizations with limited compute capacity. Image-only encoders perform worse (-35% vs. upper bound) than others. Among multimodal models, BLIP2 excels, followed by Llava, aligning performance with parameter count (i.e., the bigger the model, the better its performance).\nWe investigate the impact of n-shot learning on Llava and Pixtral by varying the prompting setup for these ICL models in Figure 3. Performance on HINTSOFTRUTH reveals that Pixtral and Llava have contrary behavior with an increase in context: (1) Adding more examples with few-shot learning aids Llava but hurts the Pixtral model, and (2) Llava can benefit from a long prompt in zero-shot cases, but Pixtral generally benefits from short prompts. This is a surprising finding as additional examples should inform a model better. Like before, we observe an oversensitivity to predicting a checkworthy label. The wide context of Pixtral may have it confuse which image/claim pair is currently under scrutiny. While CheckThat shows this behavior partially, HINTSOFTRUTH provides a clearer pattern, attesting to the usefulness of our dataset."}, {"title": "5.2 Domain Generalization", "content": "Evaluating performance on each of the HINTSOFTRUTH subsets shows (see Table 3) that while fine-tuned (FT) models are trained on only the three domains using CheckThat, they consistently generalize to the subsets of HINTSOFTRUTH, which suggests an effective knowledge transfer. However, performance varies based on experiment characteristics such as modalities used, pretraining setup, and model size.\nAmong FT models, TinyBERT is robust across most datasets but struggles on Fakeddit, likely due to the linguistic differences between CheckThat and Fakeddit; Text data in the latter stems from user-submitted post titles, which are less grammatically correct. Larger BERT models perform well on Fakeddit but worse on SentiCap and Multiclaim. Since TinyBERT is distilled from these models, constraining model size may enhance generalization but influence error modes.\nICL performance also varies: Llava achieves the second-highest accuracy on Fakeddit, while Pixtral excels on all other subsets. Llava's training on noisy user-generated ShareGPT4V data (Chen et al., 2024) may explain its behavior, while Pixtral may favor syntactically correct texts. This difference between two models highlights noisy data as a unique generalization challenge. Finally, BLIP excels on Flickr30K, despite it not being pretrained on it, raising data leakage concerns (Balloccu et al., 2024)."}, {"title": "5.3 Performance on Synthetic Data", "content": "We investigate performance on the synthetic part of HINTSOFTRUTH, using images generated by Flux (Black Forest Labs, 2024) and Stable Diffusion 3.5 (Stability.ai, 2024), and textual claims by Llava (Li et al., 2024) and BLIP (Li et al., 2022). To the human eye, synthetic samples appear distinct from real-world samples (see Figure 6 for some examples). Our goal is to determine whether models can reliably detect synthetic data and differentiate between various generative methods. To achieve this, we cross-check with the same models used for classification to assess whether they can identify their own synthetic generations. We evaluate a subset of models, including the smallest (TinyBERT, ResNet) and largest (BLIP2, Llava, Pixtral), to analyze compute/accuracy tradeoffs. Figure 4 provides an overview of the positive checkworthiness prediction rate (PPR) per generation method, highlighting false positive and false negative rates.\nResults TinyBERT is accurate on regular 5Pils data but shows mixed results on synthetic data. It misclassifies over half of BLIP-generated texts as checkworthy and produces a high false positive rate (\u00b10.54) on Flickr30K, increasing fact-checkers' workload unnecessarily. ResNet, on the other hand, shows a significant false negative rate on 5Pils. Its PPR is higher than TinyBERT's for synthetic images-by 32% for Flux and by 14% for SD. On Flickr30K, ResNet shows an unexpected sensitivity to synthetic content (66% higher PPR for Flux and 54% for SD). Llava generates many false negatives on 5Pils while obtaining a high false positive rate on Flickr30K, suggesting that the model may misunderstand the task instructions. The high PPR on Llava-generated texts for 5Pils reveals an oversensitivity to synthetic texts generated by itself.\nBLIP2 behaved more in line with expectations, with a lower PPR for synthetic texts while sustaining a high PPR for synthetic images in 5Pils. On Flickr30K, it maintained a minimal PPR across all synthetic data, likely benefiting from pretraining on synthetic captions. The origin of the synthetic text (BLIP vs. Llava) had little impact on its performance. Pixtral mirrors BLIP2's results, except that images generated by Flux were 11% less often identified as checkworthy by Pixtral, suggesting that as newer, higher-quality image generators emerge, Pixtral's accuracy might decline."}, {"title": "5.4 Compute Budget", "content": "Unsurprisingly, models with more parameters generally perform better at checkworthiness detection. However, running large models like Pixtral demands substantial compute resources. Since checkworthiness detection serves as a prefiltering task, such resources may not be available to media organizations or outpaced by new content. To explore the trade-off between model size and performance, we visualize the compute budget in FLOPs (Hassid et al., 2024) compared to final accuracy in Figure 5. FLOPs usage and wall time are estimated using the calflops library (Ye, 2023), averaging over 100 random samples from HINTSOFTRUTH, measured on a node with a single H100 GPU.\nResults The best-performing model, Pixtral, requires at least two orders of magnitude more compute than FT models, even with zero-shot ICL. BLIP2 offers a balanced trade-off, ranking third in accuracy at a reasonable compute cost. However, in wall time, it closely matches ICL models\u2014on average, BLIP2 runs as long as 1-shot Pixtral, while Pixtral 0-shot is up to 36% faster per sample (see App C.4 for details). TinyBERT emerges as the most balanced, delivering competitive accuracy at significantly lower cost and runtime (four orders of magnitude in FLOPS, two in wall time). This suggests that tuning a small model can achieve strong performance, raising questions about the role of visual information in checkworthiness detection."}, {"title": "6 Conclusions", "content": "HINTSOFTRUTH provides key insights into the challenges and opportunities in multimodal checkworthiness detection and the unclear role that visual content plays in misinformation. Our findings indicate that while multimodal models outperform image-only approaches, their advantage over text-only models remains unclear. Well-tuned text-based models achieve nearly the same accuracy (up to 86%), raising uncertainty about the extent to which visual content contributes to the checkworthiness of real-world image/claim pairs.\nUnlike many other areas of NLP, our experiments reveal that the syntactic and grammatical structure of the claims rather than their domain impacts generalization. Larger models, like Pixtral, demonstrate high adaptability but may unexpectedly fail to transfer. When confronted with synthetic data, lightweight models become oversensitive, often misclassifying images as checkworthy. This increases fact-checkers' workload by requiring manual filtering of false positives. Fine-tuning models on synthetic samples could help but risks turning into an adversarial race with evolving image generators (Corvi et al., 2023). Our analysis of the computational trade-offs reveals that large models come at compute costs of four orders of magnitude larger than smaller models like TinyBERT. These small models show surprising efficacy when carefully configured, and thus lightweight solutions may practically be better suited as checkworthiness detection methods.\nFuture work should shift checkworthiness detection to a ranking-based approach, helping fact-checkers prioritize claims. Explain why a claim needs verification can further help fact-checkers communicate decisions (McCright and Dunlap, 2017). Techniques like Learning to Defer (Madras et al., 2018; Khurana et al., 2024) and Active Learning (van der Meer et al., 2024) assist in efficient data collection."}, {"title": "Limitations", "content": "Several limitations have an impact on the findings of our work. First, our study is conducted entirely on English data, whereas misinformation has impacts across many different languages and cultures. However, some of the resources used in our work could be exploited to generate instances in other languages. Second, we do not incorporate retrieval-augmented generation (RAG) systems in our experiments. While such systems could potentially enhance checkworthiness detection by retrieving relevant fact-checks (Singal et al., 2024), they are sensitive to temporal leakage when past fact-checks are accessible (Glockner et al., 2022), skewing the results, and require even further resources than the models in this paper.\nFinally, we do not conduct a human evaluation of checkworthiness predictions. While crowd annotators are often employed for such tasks, their ability to accurately judge checkworthiness remains uncertain. Fact-checking services often employ expert journalists who draw on their intuition and experience to decide what to fact-check and may take up to a couple of days to write fact-checking articles. Whether lay crowd annotators can reliably annotate checkworthiness in an online annotation study is therefore unclear. Parallel crowd and expert evaluation studies, such as expert assessments or real-world fact-checking use cases, could provide deeper insights into annotator behavior."}, {"title": "Ethical Considerations", "content": "The development of multimodal fact-checking datasets, like HintsOfTruth, involves several critical ethical considerations to ensure societal benefit."}, {"title": "Bias Mitigation", "content": "Data is sourced from diverse domains, including social media (e.g., Multiclaim) and datasets that focus on underrepresented cultures (e.g., 5Pils). However, since we primarily reuse existing datasets, our corpus remains limited in size and inclusivity. As a result, geographic and cultural biases may persist."}, {"title": "Anonymization", "content": "To protect user privacy, real-world data from social media and fact-checking articles is anonymized. Image/claim pairs are stripped of personally identifiable information (PII), and no additional contextual information is introduced. We adhere strictly to the licensing terms of the publicly available datasets we use."}, {"title": "Misinformation Risks", "content": "As our system contributes to the fact-checking pipeline, it is designed to help combat misinformation. However, synthetic data generation tools have the potential for misuse. To mitigate this risk, our study explicitly avoids introducing adversarial prompts that could be exploited for harmful purposes."}, {"title": "Resource Accessibility", "content": "We prioritize lightweight models, such as TinyBERT, to enhance scalability and ensure that organizations with limited computational resources can access misinformation detection tools. Additionally, all models used in our research, including the largest ICL models, are freely available on the HuggingFace Hub."}, {"title": "A Data", "content": ""}, {"title": "A.1 Multiclaim", "content": "To extract images and claims from the Multiclaim dataset, we followed these steps:\n1. Obtain fact-checking article from URL.\n2. Filter claims for multimodal terms (e.g. \"photo\", \"image\", etc.).\n3. Filter out articles not written in English.\n4. Obtain the image associated with the claim based on the HTML in the article. We look for the image tag that is closest to the claim in the HTML tree.\n5. Filter out some erroneously obtained images based on their URL, such as repeated entries (usually website logos), or specific image dimensions (image too small or aspect ratio too distorted)."}, {"title": "B Experimental Details", "content": "Computational resources Experiments were largely run between August 2024 and February 2025. Training and inference were performed on a cluster with heterogeneous computing infrastructure, including RTX3090, V100, and H100 GPUs. We fine-tuned a total of eight models for checkworthiness detection, which took up to two hours per model. For all our experiments, we use the Huggingface transformers library (Wolf et al., 2019), with default values unless otherwise mentioned.\nModel versions See Table 5 for the specific checkpoints used to instantiate the FT and ICL models."}, {"title": "B.1 Fine-tuning", "content": "We use hyperparameters shown in Table 6 when fine-tuning the models on the training set of CheckThat. Non-mentioned parameters are set using the default values in the Huggingface library. During training, we keep track of the accuracy on the validation set, and at the end of training all epochs, we use the model at the step that obtained the best accuracy on the validation set."}, {"title": "B.2 In-Context Learning", "content": "For the short prompt, see Prompt 1. For the verbose prompt, see Prompt 2. In cases of few-shot learning where $n > 0$, the orange examples are repeated n times, once for each randomly retrieved example. The blue text is filled during inference for each sample in the evaluation set."}, {"title": "C Additional Experiments", "content": ""}, {"title": "C.1 Threshold tuning for fine-tuned models", "content": "See Figures 7 and 8 for the ROC and Precicion-Recall curves for CheckThat and HINTSOFTRUTH, respectively. We selected a False Positive Rate (FPR) of 0.3 as the threshold point to prefer recall over precision. The final threshold values are reported in Table 5."}, {"title": "C.2 Single-logit Output", "content": "Opposed to the two-logit setup used in the experiments in this paper, one could use a single-logit setup to perform checkworthiness detection. In this setup, we would turn the softmax and cross-entropy loss into a sigmoid activation and a binary cross-entropy loss. However, we found this led did not let the model learn better-than-random accuracy, even though its loss was going down more smoothly when training on the CheckThat data, see Figure 9."}, {"title": "C.3 Image captions as Negative Samples", "content": "In this set of experiments, we investigate whether models for multimodal checkworthiness are likely to label generic image captions as checkworthy wrongly. This acts as an additional sanity check that our models do not rely on spurious features or other shortcuts (Geirhos et al., 2020)."}, {"title": "Approach", "content": "We take models trained on the CheckThat dataset, and apply them across various image caption datasets. Since image captions (1) do not contain verifiable claims, or (2) are not (potentially) harmful, they can be considered non-checkworthy. We perform experiments on the following datasets: (1) Flickr30K (Hodosh et al., 2013) (2) VizWiz (Gurari et al., 2018) (3) Conceptual Captions (Sharma et al., 2018) (4) Coyo (Byeon et al., 2022) (5) PixelProse (Singla et al., 2024). For each dataset, impose similar filtering on the textual claims as for the scraped datasets mentioned in Section 3. Furthermore, since these datasets are of significant size, we downsample them to 6K samples each before accessing the image URLs. Table 7 denotes the final sizes of each dataset. We further split this into training/test/validation sets using a 70/20/10 ratio. We then apply each checkworthiness detection approach (as described in Section 4.2) to this task and report the classification accuracy."}, {"title": "C.4 Compute Wall Time", "content": "We compute the average wall time per sample and its standard deviation over 100 random samples. See Figure 10 for the results, including the various n-shot ICL models. For the FT image-based and multimodal models, the standard deviation is considerable, due to having to resize images at inference time to be fed as input to the model. Pixtral has larger standard deviations than Llava, possibly due to the larger context size in combination with KV caching."}, {"title": "C.5 Prompt Error Rates", "content": "We prompt the LLMs to produce a response according to a fixed format and include some additional manual response interpretation steps to ensure we can extract the prediction from the model. However, even with this generous interpretation, the model occasionally erroneously reverts to a different response format or fails to provide a prediction label. We consider those responses as errors, and plot the error rates in Figure 11.\nThe error rates are generally low, especially for the Pixtral model. For Llava, in a one-shot setup, the error rate is largest, both for CheckThat and HINTSOFTRUTH. Qualitative observations revealed that reasons for the models failing to respond in almost all cases are due to the model refusing to answer and immediately generating an"}]}