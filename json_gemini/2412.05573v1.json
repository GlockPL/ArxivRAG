{"title": "Neighborhood Commonality-aware Evolution Network for Continuous Generalized Category Discovery", "authors": ["Ye Wang", "Yaxiong Wang", "Guoshuai Zhao", "Xueming Qian"], "abstract": "Continuous Generalized Category Discovery (C-GCD) aims to continually discover novel classes from unlabelled image sets while maintaining performance on old classes. In this paper, we propose a novel learning framework, dubbed Neighborhood Commonality-aware Evolution Network (NCENet) that conquers this task from the perspective of representation learning. Concretely, to learn discriminative representations for novel classes, a Neighborhood Commonality-aware Representation Learning (NCRL) is designed, which exploits local commonalities derived neighborhoods to guide the learning of representational differences between instances of different classes. To maintain the representation ability for old classes, a Bi-level Contrastive Knowledge Distillation (BCKD) module is designed, which leverages contrastive learning to perceive the learning and learned knowledge and conducts knowledge distillation. Extensive experiments conducted on CIFAR10, CIFAR100, and Tiny-ImageNet demonstrate the superior performance of NCENet compared to the previous state-of-the-art method. Particularly, in the last incremental learning session on CIFAR100, the clustering accuracy of NCENet outperforms the second-best method by a margin of 3.09% on old classes and by a margin of 6.32% on new classes. Our code will be publicly available at https://github.com/xjtuYW/NCENet.git.", "sections": [{"title": "1. Introduction", "content": "Category Discovery (CD) [44, 16] aims to discover novel classes in unlabelled images partially based on the knowledge learned from labelled images. This task has numerous applications in real-world scenarios, such as novel disease detection in medical images, new species discovery, and automatic image data annotation, etc. This paper focuses on a specific setting of Continuous Generalized Category Discovery (C-GCD) [48], i.e., given a sequence of unlabelled image sets, we need to continually discover novel categories from each unlabelled image set while maintaining performance on old categories. This task is quite challenging from many perspectives, such as the old training set being inaccessible during incremental learning sessions, the incremental image set being unlabeled, and the number of categories being unknown.\nDirectly applying the conventional CD methods can not solve this task well due to the following two reasons:\n1) Labelled data reliance. In existing CD methods, labelled data are often required to guide the learning of discovering novel classes in unlabelled data.\n2) Catastrophic forgetting issue. C-GCD is an incremental task that consists of multiple incremental learning sessions. As the learning process proceeds, the absence of old data in incremental learning sessions will drop the clustering performance of some CD methods significantly.\nIn light of this, the pioneering C-GCD method [48] proposes to exploit meta-learning to learn a satisfactory initial model with less forgetting. Specifically, the proposed meta-learning strategy sets the goal of C-GCD as the meta-learning optimization objective and constructs pseudo-incremental tasks to optimize the model by mimicking the real incremental settings. However, despite achieving superior performance, it needs complicated data curation to construct the pseudo-incremental task. Importantly, it overlooks the learning of representational differences between instances of different classes which plays a key role in discovering novel classes.\nGenerally speaking, a good representation should possess the following two characteristics: 1) it should effectively express semantics highly relevant to its category, and 2) it should suppress semantics that are irrelevant to its category. As a result, we can obtain a discriminative feature space for the clustering/discovery of novel categories. To this end, from the perspective of methodology, 1) a line of studies [37, 54, 2] propose to incorporate clustering with contrastive learning. However, clustering algorithms are often computationally intensive or need to take the category number as a prior [2] or other complicated and task-specific pre-processing operations [37, 54], rendering such a solution less suitable for C-GCD. 2) Another line of studies [13, 47, 45] propose to exploit the self-distillation technique, wherein the sharpened prediction distribution of one augmentation view is utilized as the pseudo-label to supervise the learning of another augmentation view of the same instance. However, though clustering is not required, the prediction distributions are not meaningful enough because they are often generated by a randomly initialized classification head, which will compromise the model's performance on novel classes (see Section 6.2).\nTo address these issues, we incorporate the self-distillation technique with local commonalities and propose a novel Neighborhood Commonality-aware Representation Learning (NCRL) module. As shown in Figure 1, our motivation is that each class consists of a set of local similar semantics (commonalities). Meanwhile, instances within neighborhoods often share similar semantics. For instance, tabby cats exhibit analogous pointed ears and a striped pattern. These characteristics imply that we can use local commonalities derived from neighborhoods to guide the learning of representational differences between instances of different classes. Therefore, our proposed NCRL first perceives local commonalities by harnessing the average features of neighbors. Subsequently, NCoR conducts representation learning by self-distillation, where the prediction distributions are generated by exploiting the obtained local commonalities. In such a way, NCRL can generate more meaningful prediction distributions. Meanwhile, the prediction distributions, which represent the relationships between instances and semantics embodied in different classes, can help the model learn discriminative representations, thereby leading to a satisfactory clustering performance on novel classes. Furthermore, the commonality perception and representation learning are performed in a mini-batch, thus the learning process with NCRL is also efficient and not necessary to take the category number as a prior.\nHowever, we find that only focusing on the novel class representation learning will degenerate the model's representation ability for old classes as the learning process proceeds, which in turn leads to the notorious catastrophic forgetting problem. To mitigate this issue, a natural idea is to perform knowledge distillation to maintain the learned knowledge. In general, knowledge distillation is achieved by KL divergence [52, 28, 41] or MSE [49, 29, 15]. However, the representation-related knowledge is structured [42], making KL divergence or MSE have a limited effect on maintaining such knowledge. Considering the inherent advantage of contrastive learning in representing such knowledge, we further propose a Bi-level Contrastive Knowledge Distillation (BCKD) module to achieve old knowledge retention. Concretely, our proposed BCKD leverages contrastive learning to perceive both the learning and learned representational knowledge and perform knowledge distillation. By knowing what is learning and what is learned, BCKD can achieve holistic representational knowledge retention with less compromising the learned new knowledge (see Section 6.3).\nOverall, taking NCRL and BCKD together, our proposed method dubbed Neighborhood Commonality-aware Evolution Network (NCENet) achieves competitive performance on three C-GCD benchmark datasets. Our contributions are summarized as follows:\n\u2022 A new C-GCD learning framework. We propose a NCENet, a new C-GCD learning framework that solves the task of C-GCD from the perspective of novel class representation learning and old class representation degeneration confrontation.\n\u2022 Neighborhood Commonality-aware Representation Learning (NCRL) module. NCRL incorporates local commonalities derived from neighborhoods with the self-distillation technique to guide the learning of representational differences between instances of different classes, making NCENet able to output discriminative representations for novel classes.\n\u2022 Bi-level Contrastive Knowledge Distillation (BCKD) module. BCKD explores the utilization of contrastive learning in C-GCD and exploits contrastive learning to perform knowledge distillation, making NCENet could maintain the representation ability for old classes.\n\u2022 Competitive performance on three C-GCD benchmark datasets."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Category Discovery", "content": "Category Discovery (CD) [17, 44] aims to dynamically assign labels to unlabelled data partially based on the knowledge learned from labelled data. Contemporary research inc CD can be roughly divided into two groups, Novel Category Discovery (NCD) [16, 21, 55] and Generalized Category Discovery (GCD) [10, 38, 37]. NCD operates under the premise that the label space of the unlabeled data is entirely separate from that of the labeled data. In contrast, GCD generalizes the NCD by considering a scenario where the unlabeled data encompasses known and previously unseen classes. Despite the differences between the two tasks, one of the key challenges is representation learning. In light of this, supervised contrastive learning [23] and unsupervised contrastive learning [44] serve as a baseline solution. To further enhance representation learning, recent studies can be roughly grouped into neighborhood-based, clustering-based, and self-distillation methods. Considering that the number of negative samples in unsupervised contrastive learning is dominant, it will undermine the performance of representation learning. The neighborhood-based methods [50, 55, 11] introduce neighbors to mitigate this issue. For example, NCL [55] utilizes instances within the neighborhood of the anchor sample as positive samples and mines hard negative samples from a memory buffer, while CMS [11] leverages mean-shifted embeddings derived from neighborhoods as contrastive samples. Unlike the neighborhood-based methods, the clustering-based methods [2, 54, 37] argue that unsupervised contrastive learning can not underline relationships between instances of the same classes. To address this issue, the clustering-based methods leverage various clustering algorithms, such as GMM [54] or Infomap [37], and prototypical contrastive learning to learn representation. In contrast to the clustering-based methods, the self-distillation-based methods [13, 47, 45, 46] perform representation learning by minimizing the prediction distributions of two augmentation view of the same instance, where a random initialized classification head is used to generate prediction distributions.\nIn contrast to these offline methods, our proposed method focuses on solving both the representation learning of sequential unlabelled data and the catastrophic forgetting problem that occurs in the continuous learning process. More concretely, our proposed NCRL is most similar to self-distillation-based methods, but our NCRL exploits commonalities derived from neighborhoods to output more meaningful prediction distributions."}, {"title": "2.2. Incremental Category Discovery", "content": "Incremental Category Discovery (ICD) aims to continuously discover novel classes from unlabelled data while maintaining the ability for old classes. Recent studies [39, 22, 51, 53, 48] mainly engage in four ICD tasks, class-incremental Novel Class Discovery (class-iNCD) [39, 22], Continuous Category Discovery (CCD) [51], Incremental Generalized Category Discovery (IGCD) and Continuous Generalized Category Discovery (C-GCD). In these tasks, class-iNCD and CCD mainly focus on the incremental learning of NCD, while IGCD and C-GCD focus on the incremental learning of GCD. Further, class-iNCD only sets one incremental stage while CCD sets multiple incremental stages. Meanwhile, except for C-GCD, the other tasks utilize the same data to train and evaluate the model. To address the task of class-iNCD, FROST [39] retrains the prototypes of labelled data and replays them in incremental sessions followed by a feature-level distillation loss to prevent the forgetting problem. ADM [8] sets a base branch to maintain the previously learned knowledge and a novel branch to discover novel classes. At the end of each learning session, ADM merges the two branches with an adaptive module to prevent the growth of the model's parameters. To solve the task of CCD, GM [51] presents a learning framework consisting of a growing phase and a merging phase. In the growing phase, GM first detects novel samples and then sets an additional dynamic branch to perform the NCD task with the detected novel samples and previously learned static branch. In the merging phase, GM first learns class-level discriminative features and then merges the two branches in an EMA manner. For the task of IGCD, Zhao et al. [53] provide a baseline by adapting the SimGCD to this task. For the challenging task of C-GCD, MetaGCD [48] introduces a meta-learning framework that solves C-GCD from the perspective of model initialization.\nIn this paper, we focus on solving the challenging task of C-GCD. Unlike MetaGCD, we solve C-GCD from the perspective of representation learning. More concretely, our proposed method leverages local commonality derived from neighborhood to learn representations for novel classes and contrastive learning to mitigate the representation degeneration of old classes."}, {"title": "2.3. Knowledge distillation", "content": "Knowledge distillation [19] aims to transfer \"Dark Knowledge\" from a larger model (teacher) to a smaller model (student). The existing KD methods can be roughly divided into two groups, logits distillation and feature distillation.\nIn logits distillation, TAKD [33] introduces several teacher assistants with a gradual reduction of model size to achieve progressive knowledge transfer. DGKD [40] improves TAKD by gathering logits of previous teacher assistants. DIST [20] proposes to use the Pearson correlation coefficient [35] derived from logits to match the inter- and intra-correlations between teacher and student. GLD [24] proposes to add an additional local logits distillation branch to further transfer spatial knowledge. DKD [52] splits logits into the target and non-target parts and performs knowledge distillation in a decoupled manner. LSDK [41] proposes a logits standardization method to help the student model capture key information of the teacher model. CTKD [28] sets the knowledge distillation temperature to be trainable and proposes a learning curriculum to control the difficulty of learning tasks.\nIn feature distillation, a line of works engage in the designation of various feature-oriented distillation knowledge, such as intermediate features [1, 18], cross-layer fusion features [9], relationships [30, 43, 36] between instances, attention maps [49, 29, 15]. In the above methods, KL divergence and MSE are usually used to perform knowledge distillation. In contrast to these methods, CRD [42] argues that representational knowledge is structured and proposes to leverage contrastive learning to achieve representational knowledge transfer, where a memory buffer is set to store negative samples.\nInspired by CRD [42], our proposed method leverages the contrastive learning technique to conquer the representation degeneration issue. But unlike CRD, our proposed BCKD performs KD in a bi-level contrastive manner to achieve comprehensive knowledge retention."}, {"title": "2.4. Representation Learning with self-distillation", "content": "In addition to the methods introduced in Section 2.1, a line of works in Semi-Supervised Learning [4, 5] and Self-Supervised Learning [14, 7, 12, 3] also utilize self-distillation for representation learning. When it comes to generating prediction distributions, these methods can be categorized into two types: instance-based [12, 4] and prototype-based [6, 7, 3]. Instance-based methods either use labeled support instances [4] from sampled classes or random instances [12] to produce predictions. In prototype-based methods, the prototypes are typically set to be trainable. Unlike these methods, our proposed NCRL uses local commonalities derived from instances within different neighborhoods to generate prediction distributions."}, {"title": "3. Preliminaries", "content": "Task Definition. In C-GCD, a base session and several incremental sessions come in sequence. The base session provides sufficient labelled data, whereas the incremental sessions only provide unlabelled data. The goal of C-GCD is to continually discover novel classes without forgetting old classes. Formally, let D\u2070 denotes the base session and D\u1d57(t>0) denotes the incremental session. The label spaces of different sessions satisfy Y\u1d57\u207b\u00b9 \u2282 Y\u1d57, which means that data of incremental session t comes from seen and unseen categories. The training data of D\u2070 and D\u1d57(t > 0) satisfy Dtrain \u2229 Dtrain = \u2205. In incremental learning session t, only Dtrain is available. When finishing the training, the model is evaluated with test data accumulated until session t, i.e., the test set Dtest of incremental session t is constituted by {Dtest, ..., Dtest}.\nArchitecture. Following [48], our model architecture f = g \u25e6 h consists of an encoder g and a projection head h. In the training stage, our goal is to optimize parameters of f using provided training data. In the inference stage, we use g to encode corresponding test data and clustering accuracy on encoded features to evaluate the model's performance.\nLearning Startup. In the base session, we follow the common practice [44, 47, 37] to combine supervised and unsupervised contrastive learning to train the model. Formally, let z\u1d62 and z\u2c7c denote projected features obtained by passing two augmentation views of the same instance into f. The supervised contrastive loss Lsup is calculated by:\nLsup = 1/|B\u02e1| \u03a3\u1d62\u2208B\u02e1 \u03a3\ud835\udc5e\u2208\ud835\udca9(\ud835\udc56) \u2212log (exp(z\u1d62 \u00b7 z\ud835\udc5e / T\u1d63) / \u03a3\u2c7c\u2260\u1d62 exp(z\u1d62 \u00b7 z\u2c7c / T\u1d63)  ) (1)\nwhere |B\u02e1| denotes the number of labeled data in a mini-batch, \ud835\udca9(\ud835\udc56) denotes indices of other instances with the same label as instance i and T\u1d63 is the scaling factor. The unsupervised contrastive loss Lunsup is defined as:\nLunsup = 1/|B| \u03a3\u1d62\u2208B \u2212log (exp(z\u1d62 \u00b7 z\u2c7c / T\u1d63) / \u03a3\u2c7c\u2260\u1d62 exp(z\u1d62 \u00b7 z\u2c7c / T\u1d63) ) (2)\nwhere |B| denotes the batch size. After obtaining Lsup and Lunsup, the overall objective in the base session is represented as:\nL = \u03b2Lsup + (1 \u2013 \u03b2) Lunsup (3)\nwhere \u03b2 is a hyperparameter used to control the contribution of Lsup and Lunsup."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Overview", "content": "As depicted in Figure 2, our proposed NCENet comprises two key components: the Neighborhood Commonality-aware Representation Learning (NCRL) module (Section 4.2) and the Bi-level Contrastive Knowledge Distillation (BCKD) module (Section 4.3). The NCRL module is primarily responsible for discriminative representation learning of novel classes, while BCKD is mainly designed to preserve the old representational knowledge. Concretely, for the incremental session t + 1, NCENet commences the incremental learning process by generating two augmentation views for each unlabelled instance in a mini-batch. Then, NCENet feeds different augmentation views"}, {"title": "4.2. Neighborhood Commonality-aware Representation Learning", "content": "The core idea of the designation of NCRL is to exploit local commonalities derived from instances within different neighborhoods to guide the learning of representational differences between instances of different classes. To this end, NCRL mainly involves two steps: 1) commonality perception and 2) self-distilled representation learning.\nStep1: commonality perception used to obtain local commonalities to prepare for future commonality learning. Concretely, given features z\u1d57\u207a\u00b9 \u2208 \u211d|\u1d2e|\u00d7\u1d48 encoded by current learning model f\u1d57\u207a\u00b9, where d refers to the feature dimension. NCRL first calculates cosine similarities w\u2208 \u211d|\u1d2e|\u00d7|\u1d2e| between different features. Then, NCRL selects k nearest neighbors \ud835\udca9\ud835\udca9(z\u1d57\u207a\u00b9) \u2208 \u211d\u1d4f\u00d7\u1d48 for each feature based on obtained w. In the end, NCRL computes the local commonalities \u03bc\u2208\u2208 \u211d|\u1d2e|\u00d7\u1d48 by:\n\u03bc\u1d62 = 1/k \u03a3q\u2208\ud835\udca9\ud835\udca9(z\u1d57\u207a\u00b9) zq\u1d57\u207a\u00b9 (5)\nwhere \u03bc\u1d62 denotes the local commonality derived from neighbors of z\u1d62\u1d57\u207a\u00b9.\nStep2: self-distilled representation learning leverages obtained local commonalities to learn discriminative representations for novel classes. Concretely, given features z\u1d57\u207a\u00b9 and \u017e\u1d57\u207a\u00b9 of two augmentation views of the same instance. NCRL first computes prediction distribution \ud835\udc5d of z\u1d57\u207a\u00b9 over \u03bc by:\npj = exp(z\u2c7c\u1d57\u207a\u00b9 \u00b7 \u03bc\u1d62/\u03c4) / \u03a3m exp(z\u2c7c\u1d57\u207a\u00b9 \u00b7 \u03bcm/\u03c4) (6)\nwhere p\u2c7c refers to the probability of z\u2c7c\u1d57\u207a\u00b9 belonging to local commonality \u03bc; and \u03c4 refers to temperature used to sharpen the prediction distribution. Meanwhile, using Eq. 6 and setting \u03c4 to 1, NCRL computes prediction distribution \ud835\udc5d\u0302 of \u017e\u1d57\u207a\u00b9 over \u03bc. After obtaining p and \ud835\udc5d\u0302, the learning objective of NCRL is defined as:\nLncr\u03b9 = 1/|B| \u03a3\u1d62=1 \u03a3\u2c7c p\u1d62 log p\u0302j. (7)\nRemark: Though the local commonalities obtained from a mini-batch are not comprehensive, a wider and more diverse set of instances will compensate for this shortcoming as the learning process proceeds."}, {"title": "4.3. Bi-level Contrastive Knowledge Distillation", "content": "With NCRL, we can improve the model's representation ability for novel classes. However, the absence of old training data will degenerate the model's representation ability for old classes as the learning process proceeds, this phenomenon is also dubbed the dilemma of plasticity and stability [31, 27]. To achieve old representational knowledge retention, an effective solution is to apply the contrastive learning-based knowledge distillation method [42] used for the model compression task to C-GCD. However, the differences between C-GCD and model compression tasks will make such a method suffer from the over-constraint issue. Specifically, in C-GCD, we expect the student model to inherit knowledge from the teacher model without hindering the learning of new knowledge. In model compression tasks, more emphasis is placed on the student model's ability to fully inherit all knowledge from the teacher model. Consequently, directly using existing contrastive knowledge distillation compression methods in C-GCD may result in the student model being overly reliant on the teacher's knowledge, which may undermine the learning of new knowledge to some extent (Section6.3).\nIn light of this, BCKD leverages student-anchored contrastive knowledge distillation and teacher-anchored contrastive knowledge to achieve representational knowledge transportation from teacher to student. Formally, given features z\u1d57\u207a\u00b9 encoded by current learning model f\u1d57\u207a\u00b9 and z\u1d57 encoded by historical model f\u1d57. The student-anchored contrastive knowledge distillation learning objective is defined as:\nLsa = 1/|B| \u03a3\u2c7c \u2212log (exp(z\u2c7c\u1d57\u207a\u00b9 \u00b7 z\u2c7c\u1d57 / Tk) / \u03a3\u1d62 exp(z\u2c7c\u1d57\u207a\u00b9 \u00b7 z\u1d62\u1d57 / Tk)  ) (8)\nwhere Tk refers to the temperature. The teacher-anchored contrastive knowledge distillation learning objective is defined as\nLta = 1/|B| \u03a3\u2c7c \u2212log (exp(z\u2c7c\u1d57 \u00b7 z\u2c7c\u1d57\u207a\u00b9 / Tk) / \u03a3\u1d62 exp(z\u2c7c\u1d57 \u00b7 z\u1d62\u1d57\u207a\u00b9 / Tk) ) (9)\nOverall, the learning objective of BCKD is presented as:\nLbckd = Lsa + Lta / 2 (10)\nBy incorporating student-anchored contrastive knowledge distillation with teacher-anchored contrastive knowledge, BCKD can perceive the learning and learned representational knowledge, thus achieving effective incremental-oriented representational knowledge retention. Additionally, since knowledge distillation is conducted within a mini-batch, BCKD obviates the need for a memory buffer to store negative samples."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Datasets", "content": "We conduct corresponding experiments on three benchmark datasets, including CIFAR10 [25], CIFAR100 [25] and Tiny-ImageNet [26]. Following [48], we split CIFAR10 dataset into 1 base session and 3 incremental sessions. For the CIFAR100, a division is made into 1 base session and 4 incremental sessions. In the case of Tiny-ImageNet, it is structured into 1 base session and 5 incremental sessions. For each dataset, we sample 80% training images from each labelled class for base learning, the remaining data are used for incremental learning. We summarize dataset splits in Table 1.\nIncremental session. For CIFAR10, the training data of each incremental session incorporates 3,000 training images from 1 novel class and 2,000 training images from 7 + (t \u2212 1) \u00d7 1 seen classes, where t refers to the incremental session id. For CIFAR100, 1,500 training images from 5 novel classes and 2,000 training images from 80+(t-1)\u00d75 seen classes are used for incremental learning. For Tiny-ImageNet, we sample 3,000 training images from 10 novel classes and 3,000 training images from 150 + (t \u2212 1) \u00d7 10 known classes to construct the training data."}, {"title": "5.2. Evaluation Protocol", "content": "After finishing the training in each incremental session, we follow [48] to measure the clustering accuracy (ACC) by\nACC = 1/M \u03a3\u1d62=1 I{y* = m(\u0177\u1d62)}, (11)\nwhere M refers to the total number of test images used in the current session, y* indicates the ground truth, \u0177 represents the cluster label given by our model and m refers to the optimal permutation for matching predicted cluster assignment to the ground truth, and I denotes the indicator function. In this paper, we use clustering accuracy on All classes to evaluate the model's entire performance. To decouple the evaluation on forgetting and discovery, we follow [48] to further report clustering accuracy on Old classes and New classes. Concretely, when computing the clustering accuracy on Old/New classes, we only use samples in the test set belonging to Old/New classes."}, {"title": "5.3. Implementation Details", "content": "We use PyTorch [34] to implement our proposed method and conduct all experiments using one NVIDIA GeForce RTX 2080 Ti.\nModel Architecture. Follow [48], we adopt ViT-B/16 pre-trained by DINO [7] as the encoder and take the encoder's output [CLS] token with a dimension of 768 as the feature representation. We build the projection head using three linear layers, where we set the hidden dimension to 2048 and the output dimension to 65536 as [48]. In following training processes, we only finetune the last block of the encoder and the projection head.\nBase Training. We split the provided labelled set into a training set and a validation set used to select the best model. In particular, the training set takes 75% samples, and the validation set takes the remaining 25% samples. We train the model with a batch size of 128 for 50 epochs. We adopt SGD as the optimizer, where the initial learning rate is set to 0.01. We decay the learning rate with the cosine schedule [32]. We set Tr to 0.1 and \u03b2 to 0.35 as [47, 44].\nIncremental Training. We train the model with a batch size of 128 for 20 epochs. We adopt SGD as the optimizer, where the initial learning rate is set to 0.0001 and decayed using the cosine schedule [32]. We set the temperature \u03c4 in NCRL, temperature Tk in BCKD, and hyperparameter \u03bb used to control the contributions of the two modules to 0.07, 0.04, and 0.1, respectively."}, {"title": "5.4. Comparison with State-of-the-Art", "content": "To validate the effectiveness of our proposed NCENet, we compare NCENet with the novel category discovery method (FROST[39]), generalized category discovery method (VanillaGCD[44]), incremental category discovery methods (FROST[39] and GM[51]), and a strong C-GCD baseline (MetaGCD [48]).\nTable 2 shows the clustering accuracy on Old/New/All of each method in each session. On CIFAR10, most methods achieve superior performance. Especially, the previous state-of-the-art method MetaGCD establishes a strong baseline. Compared to MetaGCD, though our proposed NCENet shows no advantage on Old class, NCENet achieves better clustering performance on New and All classes in each incremental session. Particularly, the clustering accuracy on New classes of NCENet surpasses that of MetaGCD by a large margin of 7.26%.\nOn CIAFR100, the clustering accuracy on Old and All classes of our proposed NCENet shows consistent superiority over other methods. Further, though the clustering accuracy on New classes of NCENet is weaker than the second-best method MetaGCD in the first incremental session, NCENet outperforms MetaGCD in the last three incremental sessions. Particularly, in the last incremental session, NCENet outperforms MetaGCD by a margin of 3.09%, 6.32%, and 1.32% on Old, New and All classes, respectively.\nOn Tiny-ImageNet, our proposed NCENet outperforms other methods on New classes in each incremental session. As for the performance on Old classes, compared to MetaGCD, though NCENet shows less competitive performance in the first two sessions, NCENet achieves better performance in the last three incremental sessions. Particularly, in the last session, compared to MetaGCD, NCENet achieves an improvement of 2.58%, 4.92%, and 3.78% on Old classes, New classes, and All classes, respectively."}, {"title": "5.5. Ablation study", "content": "Our proposed NCENet relies on Neighborhood Commonality-aware Representation Learning (NCRL) to enhance the novel class discovery ability and Bi-level Contrastive Knowledge Distillation (BCKD) to mitigate the catastrophic forgetting problem. To validate the effectiveness of each module, we conduct several ablation studies on CIFAR10 and report corresponding clustering accuracy in Table 3. From the table, we can see that compared to the performance given by using both NCRL and BCKD (row 3), though removing NCRL (row 1) leads to a performance improvement on old class clustering accuracy (Old), it drops the new class clustering accuracy (New) by a relatively larger margin in each session, which results in performance degradation on all class clustering accuracy (All). Particularly, the mN/mA without using NCRL is 78.77%/94.57% while that given by using NCRL is 89.36%/92.56%, this indicates that NCRL is pivotal in novel category discovery. Further, though removing BCKD (row 2) improves the clustering accuracy on old classes, it drops the clustering accuracy on new classes and all classes in each session. Especially, removing BCKD drops the mO from 95.87% to 90.24% and mA from 94.57% to 91.93%, this suggests that BCKD plays a key role in old knowledge retention. In summary, experimental results shown in Table 3 show that our proposed NCRL and BCKD are both effective. Further, combining NCRL and BCKD can achieve better entire clustering accuracy than using one of them solely."}, {"title": "6. Discussion", "content": ""}, {"title": "6.1. Discussion about hyperparameter \u03bbb", "content": "To investigate the influence of \u03bb used to balance contributions of NCRL and BCKD, we vary the value of \u03bb across {0.1, 0.3, 0.5, 0.7,0.9} and report the corresponding clustering accuracy on Old/New/All classes in Figure 3. As de"}, {"title": "6.2. Discussion about NCRL", "content": "Temperature \u03c4 In NCRL, we use a temperature \u03c4 to sharpen the prediction distribution of one of the augmentation views. To explore the influence of \u03c4, we change \u03c4 among {0.01, 0.04, 0.07, 0.10} and report corresponding clustering accuracy on Old/New/All classes of last session in Figure 6. From Figure 6(a), we can see that increasing \u03c4 from 0.01 to 0.04 results in a clustering accuracy degradation on Old classes. Conversely, increasing \u03c4 from 0.04 to a larger value boosts the clustering accuracy on Old classes by a relatively larger margin. However, as shown in Figure 6(b), though setting \u03c4 to 0.07 and 0.1 both achieves a satisfactory clustering accuracy on Old classes, setting \u03c4 to 0.07 achieves better clustering accuracy on New classes. Overall, as depicted in Figure 6(c), setting to 0.07 helps our proposed method achieve the best clustering performance.\nThe number of neighbors in NCRL. To explore the influence of different numbers of neighbors on the model's clustering performance, we set k to different values and report the corresponding clustering accuracy on Old/New/All classes of last session in Figure 5. As we can see from Figure 5(a), the clustering accuracy on Old classes is relatively stable across different k values. However, as shown in Figure 5(b), setting k to a relatively larger value achieves better clustering accuracy on New classes. We speculate that using a single instance may inadequately represent the local commonality, thereby compromising the effectiveness of NCRL. Further, compared to other larger k values, changing k from 1 to 5 achieves the most significant clustering accuracy improvement, approximately 5%. The main reason we guess is that a large k value may introduce noise to commonality representation, which also undermines the effectiveness of NCRL. Overall, as shown in Figure 5(c), setting k to 5 helps our proposed method achieve the best clustering accuracy on All classes.\nNeighbor selection strategies. To explore whether using a threshold is more optimal than using a fixed number to select neighbors, we use the performance given by using a fixed number 5 to select neighbors as the baseline, and then switch the neighbor selection strategy to a threshold"}, {"title": "6.3. Discussion about BCKD", "content": "Temperature Tk in BCKD. To investigate the influence of temperature Tk used in our proposed Bi-level Contrastive Knowledge Distillation (BCKD), we change the value of Tk among {0.01, 0.04, 0.07, 0.10} and report the clustering accuracy on Old/New/All classes given by different Tk values in Figure 6. As shown in Figure 6(a), we find that setting a relatively smaller Tk value helps our proposed NCENet achieves better clustering accuracy on Old classes. Conversely, as we can see from Figure 6(b), setting a relatively larger Tk value helps our proposed NCENet achieves better clustering accuracy on New classes. Overall, as shown in Figure 6(c), setting the value of Tk to 0.04 achieves the best clustering accuracy on All classes.\nDifferent knowledge distillation losses. To further validate the effectiveness of our proposed Bi-level Contrastive Knowledge Distillation (BCKD), we compare BCKD with the commonly used knowledge distillation losses, including MSE and KL divergence. As shown in Table 6, using MSE to perform knowledge distillation achieves a better clustering accuracy on Old classes than KL divergence, but the clustering accuracy on New classes is dropped by a relatively larger margin. Using our proposed Lsa/Lta achieves better clustering accuracy on Old classes than MSE, this demonstrates that using contrastive learning to perform knowledge distillation is more helpful for our proposed method to achieve better old knowledge retention. Meanwhile, we observe that using Lta results in a better clustering accuracy on Old classes than using Lsa, but it drops the clustering accuracy on New classes by a relatively larger margin. The main reason may be that only perceiving the learned knowledge leads to an over-constrained issue which undermines the new knowledge learning ability. Ultimately, compared to the clustering performance given by using only Lta, though combing Lsa and Lta drops the clustering accuracy on Old classes slightly, it improves the clustering accuracy on New classes by a relatively larger margin and achieves the best clustering accuracy on All classes, this implies that introducing the learning knowledge can mitigate the over-constrained issue."}, {"title": "7. Conclusion", "content": "In this paper, we solve the challenging C-GCD problem from"}]}