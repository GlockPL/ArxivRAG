{"title": "AFLOW: AUTOMATING AGENTIC WORKFLOW GENERATION", "authors": ["Jiayi Zhang", "Jinyu Xiang", "Zhaoyang Yu", "Fengwei Teng", "Xionghui Chen", "Jiaqi Chen", "Mingchen Zhuge", "Xin Cheng", "Sirui Hong", "Jinlin Wang", "Bingnan Zheng", "Bang Liu", "Yuyu Luo", "Chenglin Wu"], "abstract": "Large language models (LLMs) have demonstrated remarkable potential in solv- ing complex tasks across diverse domains, typically by employing agentic work- flows that follow detailed instructions and operational sequences. However, con- structing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial man- ual setup and fall short of achieving fully automated and effective workflow gen- eration. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFLOW, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refin- ing workflows through code modification, tree-structured experience, and execu- tion feedback. Empirical evaluations across six benchmark datasets demonstrate AFLOW's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFLOW enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code will be avail- able at https://github.com/geekan/MetaGPT.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have emerged as powerful tools for solving complex tasks across various domains, including code generation, data analysis, decision-making, and question answer-ing (Liu et al., 2024; Li et al., 2024a; Zhu et al., 2024; Xie et al., 2024b; Sun et al., 2024; Wang et al., 2024b; Song et al., 2023; Xie et al., 2024a; Zhong et al., 2024a). However, the rapid advancement of LLMs heavily relies on manually designed agentic workflows \u2013 structured sequences of LLM invocations accompanied by detailed instructions. Designing and refining these workflows requires significant human effort, which limits the scalability and adaptability of LLMs to new, complex domains and hinders their ability to transfer skills across diverse tasks (Tang et al., 2024).\nRecent efforts have focused on automating the discovery of effective agentic workflows to reduce the reliance on human intervention (Khattab et al., 2024; Y\u00fcksekg\u00f6n\u00fcl et al., 2024; Liu et al., 2023; Hu et al., 2024). Despite these advancements, full automation has not been achieved. For instance, Khattab et al. (2024) requires manual workflow setup before automated prompt optimization. Similarly, methods proposed by Y\u00fcksekg\u00f6n\u00fcl et al. (2024) and Zhuge et al. (2024) fail to capture the full diversity of workflows necessary for a wide range of tasks (Yu et al., 2023; Yang et al., 2024b; Sun et al., 2023), as their optimization objectives struggle to represent the breadth of possible workflows. The inability to effectively model diverse workflow structures within these automated systems limits their utility and impact. ADAS (Hu et al., 2024) represents workflows using code, achieving a"}, {"title": "RELATED WORK", "content": "Agentic Workflow. Agentic workflow and autonomous agents (Zhuge et al., 2023; Hong et al., 2024a; Zhang et al., 2024; Wang et al., 2023) represent two distinct paradigms of LLM application. The former completes tasks statically through predefined processes with multiple LLM invocations, while the latter solves problems dynamically through flexible autonomous decision-making. Com-pared to autonomous agents that require specific actions and decision patterns designed for the en-vironment, agentic workflows can be constructed based on existing human domain experience and iterative refinement, offering higher potential for automated construction.\nAgentic workflows can be broadly categorized into general and domain-specific types. General workflows emphasize universal problem-solving approaches, such as (Wei et al., 2022; Wang et al., 2022; Madaan et al., 2023; Wang et al., 2024a). Domain-specific workflows focus on building effective processes to solve domain-specific problems, such as code generation (Hong et al., 2024b; Ridnik et al., 2024; Zhong et al., 2024a), data analysis (Xie et al., 2024b; Ye et al., 2024; Li et al., 2024a; Zhou et al., 2023), mathematics (Zhong et al., 2024b; Xu et al., 2024), question answering (Nori et al., 2023; Zhou et al., 2024a). Existing work has manually discovered numerous effective agentic workflows, but it's challenging to exhaust various tasks across different domains, further highlighting the importance of automated workflow generation and optimization.\nAutomated Agentic Optimization. Recent work aims to automate the design of agentic work-flows, categorized into three types: automated prompt optimization, hyperparameter optimization, and automated workflow optimization. Prompt optimization (Fernando et al., 2024; Y\u00fcksekg\u00f6n\u00fcl et al., 2024; Yang et al., 2024a; Khattab et al., 2024) uses LLMs to optimize prompts within fixed workflows. Hyperparameter optimization (Saad-Falcon et al., 2024) focuses on optimizing prede-fined parameters. While these approaches improve performance, they are limited in generalization to new tasks and often require moderate human effort for task-specific designs.\nAutomated workflow optimization (Li et al., 2024b; Zhou et al., 2024b; Zhuge et al., 2024; Hu et al., 2024) aims to optimize entire workflow structures, offering more potential for fully automated generation. Recent works explore diverse representations and methods. GPTSwarm (Zhuge et al., 2024) uses graph structures with reinforcement learning, but struggles to represent workflows with conditional states due to graph structure limitations. ADAS (Hu et al., 2024) utilizes code structures to represent workflows and stores historical workflows in a linear list structure, aligning closely with our goals. However, it is constrained by the efficiency of its search algorithm as it relies on overly simplistic representations of experiences in the searching process, making it challenging to discover effective workflows.\nAFLOW also uses code to represent workflows, but goes further by providing a more fundamen-tal structure called named node. This structure encompasses various LLM invocation parameters, allowing for more detailed workflow representation. We also introduce operators that implement predefined node combination functions. Simultaneously, AFLOW employs a specially designed MCTS algorithm for automated workflow optimization, leveraging the tree-structured experience and execution feedback to efficiently discover effective workflows."}, {"title": "PRELIMINARY", "content": "In this section, we will first formulate the automated agentic workflows generation problem in Sec-tion 3.1 and then discuss design considerations of our AFLOW in Section 3.2. For the core concept of this section, we provide an example explanation in Figure 2."}, {"title": "PROBLEM FORMULATION", "content": "Agentic Workflow. We define an agentic workflow W as a sequence of LLM-invoking nodes, denoted as N = {N1, N2, ..., N\u2081 ...}. Each node N\u2081 represents a specific operation performed by an LLM and is characterized by the following parameters. The code abstraction of the node is shown in Appendix A.2.\n\u2022 Model M: The specific language model invoked at node Ni.\n\u2022 Prompt P: The input or task description provided to the model at each node.\n\u2022 Temperature \u03c4: A parameter controlling the randomness of the LLM's output at node Ni.\n\u2022 Output format F: The format in which the model's output is structured (e.g., xml, json,markdown, raw). The node in workflow should provide different output formats, inspired by the Tam et al. (2024).\nThese nodes are connected by edges E, which represent the connection between the nodes, govern-ing the sequence of execution. The edges E can represent various structures, such as:\n\u2022 Graph Zhuge et al. (2024): A flexible structure representing hierarchical, sequential, or par-allel relationships between nodes, allowing for complex branching workflows.\n\u2022 Neural Network (Liu et al., 2023): A structure that can represent complex, non-linear re-lationships between nodes, allowing for adaptive and learnable workflows based on input andfeedback.\n\u2022 Code (Hu et al., 2024): A comprehensive representation that can express linear sequences,conditional logic, loops, and incorporate graph or network structures, offering the most precisecontrol over workflow execution.\nAutomated Workflow Optimization. Given a task T and an evaluation function G, the goal of workflow optimization is to discover a workflow W that maximizes G(W, T). This can be formu-lated as a search process where an algorithm A explores the search space S to determine the optimal workflow configuration. The search space S for a workflow optimization problem encompasses all possible configurations of node parameters and edge structures:\nS = {(N, E) | E \u2208 E},\nwhere N = {N(\u039c,\u03c4, P, F) | M\u0454 \u041c, \u03c4\u2208 [0, 1], P \u2208 P, F \u2208 F}, with M, P, F, E representing the sets of possible language models, prompts, output formats, and edge configurations, respectively."}, {"title": "AFLOW OVERVIEW", "content": "Limitations of Previous Methods. Previous approaches Y\u00fcksekg\u00f6n\u00fcl et al. (2024); Khattab et al. (2024); Zhuge et al. (2024) to workflow optimization have primarily been constrained by the limited scope of their search spaces, based on problem definition in Section 3.1. Another related work, ADAS (Hu et al., 2024), searches in a larger space comprising a combination of prompts N(P,T) and edges E, but fails to discover effective workflows due to the efficiency limitations of its linear heuristic search algorithm.\nFormulation. To address the limitations of previous methods, we propose AFLOW, a novel frame-work that leverages Large Language Models (LLMs) as optimizers within a variant of Monte CarloTree Search (MCTS) to search for optimal workflows. By representing nodes N and edges Ethrough code, AFLOW can explore the full range of possible agentic workflows, ensuring com-pleteness in the search space. Specifically, as shown in Figure 3, AFLOW uses a variant of MCTS toiteratively explore the workflow search space, evaluate different configurations, and backpropagateexperiences to refine the workflow optimization process.\nTo enhance search efficiency in practice, we simplify the search space by fixing key parameters such as the model M, temperature \u03c4, and format F. This simplification allows AFLOW to focus"}, {"title": "THE DESIGN DETAILS OF AFLOW", "content": "The core concept of the AFLOW is to employ Large Language Models (LLMs) as optimizers to modify code-represented workflows within a search structure based on a Monte Carlo Tree Search (MCTS) variant. It operates through an iterative process of soft mixed probability selection, LLM-based optimization expansion, execution evaluation, experience backpropagation, and dynamic con-vergence assessment until reaching maximum iterations or meets convergence criteria. A simplified illustration is shown in Figure 3, with a detailed algorithm process presented in Appendix 1.\nExisting workflow optimization methods iteratively use past workflow structures to prompt LLMs to discover new structures. However, due to information loss during accumulation (as input tokens increase), this approach struggles to guide LLMs towards specific performance metrics. Combined with the vast search space of code, this reduces search efficiency. Our key idea is to leverage the tree structure of MCTS to preserve node-based exploration experiences in workflow optimization. When a node is revisited, we accurately reuse past successful experiences and avoid failures, en-abling effective workflow generation and improving search efficiency. To prevent local optima, we introduce a special selection mechanism allowing generation from a blank template at any round.\nNext, we will introduce the complete process of AFLOW, as shown in Algorithm 1."}, {"title": "EXPERIMENTS", "content": "EXPERIMENTAL SETUP\nDatasets. We utilized six public benchmarks for our experiments. Following established prac-tices (Saad-Falcon et al., 2024; Hu et al., 2024) in workflow optimization, we divide the data intovalidation and test sets using a 1:4 ratio. Specifically, we use the full datasets for GSM8K (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and MBPP (Austin et al., 2021). For HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019), we randomly select 1,000 samples each, in line with (Hu et al., 2024; Shinn et al., 2023). For the MATH (Hendrycks et al., 2021) dataset, we follow (Hong et al., 2024a) in selecting 617 problems from four typical problem types (Combinatorics & Proba-bility, Number Theory, Pre-algebra, Pre-calculus) at difficulty level 5."}, {"title": "EXPERIMENTAL RESULTS AND ANALYSIS", "content": "Main Results. The main experimental results, as shown in Table 1, demonstrate the effectiveness of AFLOW. Workflows optimized by AFLOW outperform all manually designed methods by an av-erage of 5.7% and surpass contemporary automated workflow optimization work by 19.5%. Across six datasets in QA, Code, and Math domains, AFLOW achieves an average performance of 80.3%, marking the capability and usability of this method. Notably, compared to similar works, AFLOW performed better on more challenging tasks, improving over ADAS on MATH1v5* and MBPP tasks by 57%, showcasing the robustness of the model on complex datasets."}, {"title": "Cost Analysis.", "content": "We demonstrate the comparison of performance and cost between the baselines and the top three workflows found by AFLOW using GPT-4o-mini and DeepSeek-V2.5 as executionLLMs. The comparison is made across four models with different capabilities and price points.Results demonstrate that AFLOW can identify workflows that allow weaker models to outperformstronger models on the pareto front of cost-effectiveness. This breakthrough effectively removesbarriers to the widespread application of agentic workflows across various domains. By automatingthe design of effective agentic workflows, AFLOW eliminates the human labor costs previouslyrequired. Moreover, the ability to achieve superior performance at lower costs compared to strongermodels opens up further possibilities for widespread adoption."}, {"title": "Ablation Study.", "content": "To enhance search efficiency, we introduce operators into the search space, which can be viewed as a form of human design. To explore AFLOW's performance with zero human intervention, we conducted an ablation study on the GSM8K dataset. Results shown in Figure 5 demonstrate that AFLOW with operators discover better-performing workflows within the same number of iterations, exhibiting a trend of multiple small improvements. This indicates that op-erators effectively boost search efficiency by introducing a constrained search space. Even without operators, AFLOW still achieves 93.1% performance, surpassing other manually designed methods. Notably, in the experiment without operators, AFLOW autonomously develops a structure similar to the ensemble. This demonstrates its advantage as an optimizer for searching code-represented edges, enabling it to independently design efficient structures for problems, thereby taking a significant step towards fully automated workflow optimization. Details is shown in Appendix B."}, {"title": "Case Study.", "content": "AFLOW demonstrates a clear iteration process, as shown in Figure 6, illustrating howit evolves from a blank template (containing only a single Node without prompts) to the structurepresented in Figure 5(B). In each iteration, AFLOW employs a single-step modification, meaningit either adds one operator (rounds 2, 3) or makes a targeted modification to a prompt (rounds 8,10). Among the unsuccessful exploration nodes, AFLOW introduced a custom review node that di-rectly modified answers generated through complex processes without additional reasoning (round5), which decreased accuracy. In round 14, AFLOW attempted to rephrase the problem but overlyfocused on 'discount' information, leading to a decrease in accuracy. This iteration process show-"}, {"title": "CONCLUSION", "content": "This paper has introduced AFLOW, a novel framework for automated workflow optimization. We have comprehensively formulated the automated workflow optimization problem, establishing a foundational structure for future research. AFLOW has leveraged Monte Carlo Tree Search and code-represented workflows to navigate the vast search space of possible workflows efficiently.Our experiments across six benchmarks demonstrate the effectiveness of AFLOW, which has out-performed manually designed methods and existing automated optimization approaches. Ablation studies have shown that AFLOW can autonomously discover effective structures, even without pre-defined operators. Importantly, AFLOW has enabled weaker models to outperform stronger ones on the Pareto front of cost-effectiveness, potentially revolutionizing the adoption of agentic workflows across various domains. These results have highlighted AFLOW's potential for enhancing LLMs' problem-solving capabilities while optimizing computational costs."}, {"title": "A APPENDIX", "content": ""}, {"title": "LLM BASED EXPANSION: PROMPT FOR LLM OPTIMIZER", "content": ""}, {"title": "BASIC STRUCTURE OF NODE", "content": ""}, {"title": "BASIC STRUCTURE OF WORKFLOW", "content": ""}, {"title": "OPERATORS", "content": ""}, {"title": "MCTS ALGORITHM OF AFLOW.", "content": ""}, {"title": "B CASE STUDY", "content": ""}, {"title": "CASE STUDY OF AFLOW", "content": "Alpha Codium like workflow for MBPP\nAFLOW demonstrates its ability to reduce human effort by evolving from an empty workflow to asolution highly similar to manually designed workflows like Ridnik et al. (2024) in the code gener-ation scenario. This showcases AFLOW's capability to generate efficient workflows comparable toexpert designs with minimal human intervention."}, {"title": "The optimal workflow generated for MATH", "content": "This optimal workflow generated for the MATH task showcases the model's ability to generate com-plex, task-specific solutions from task-agnostic initial settings. It combines programmatic solutionswith various reasoning strategies, culminating in an ensemble selection process, and spontaneouslyformats the answer into the required form. This adaptation demonstrates the model's flexibility intailoring workflows to different problem domains, while maintaining sophisticated problem-solvingstructures."}, {"title": "The optimal workflow generated for MBPP", "content": "The optimal workflow generated for the MBPP task simply combines operators with an ingeniousFIX-CODE PROMPT, achieving the optimal workflow in the iteration at the fourteenth round. Al-though this workflow is simple, its score is extremely high and stable, demonstrating AFLOW'spotential to find the optimal cost-performance balance."}, {"title": "The optimal workflow generated for HotpotQA", "content": "The optimal workflow generated for the HotpotQA task demonstrates the effectiveness of executionfeedback. Apart from logical reasoning, another factor affecting QA problem scores is effectiveformatting. AFLOW can effectively identify the correct format and automatically perform formattingthrough learning from execution feedback, showcasing the efficacy of this design."}, {"title": "An ensemble structure that emerged in the GSM8K ablation experiment", "content": ""}, {"title": "CASE STUDY OF ADAS", "content": ""}], "equations": [{"equation": "S = {(N, E) | E \u2208 E},"}, {"equation": "W = A(S,G,T),"}, {"equation": "W* = arg max G(W, T),"}, {"equation": "Pmixed(i) = x. - + (1 \u2212 x)\u00b7\\frac{\\exp(a. (si - Smax))}{\\sum_{j=1}^{n}\\exp(\u03b1\u00b7 (sj \u2013 Smax))}"}]}