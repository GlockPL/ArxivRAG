{"title": "KnowledgeSG: Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server", "authors": ["Wenhao Wang", "Xiaoyu Liang", "Rui Ye", "Jingyi Chai", "Siheng Chen", "Yanfeng Wang"], "abstract": "The success of large language models (LLMs) facilitate many parties to fine-tune LLMs on their own private data. However, this practice raises privacy concerns due to the memorization of LLMs. Existing solutions, such as utilizing synthetic data for substitution, struggle to simultaneously improve performance and preserve privacy. They either rely on a local model for generation, resulting in a performance decline, or take advantage of APIs, directly exposing the data to API servers. To address this issue, we propose KnowledgeSG, a novel client-server framework which enhances synthetic data quality and improves model performance while ensuring privacy. We achieve this by learning local knowledge from the private data with differential privacy (DP) and distilling professional knowledge from the server. Additionally, inspired by federated learning, we transmit models rather than data between the client and server to prevent privacy leakage. Extensive experiments in medical and financial domains demonstrate the effectiveness of KnowledgeSG.", "sections": [{"title": "1 Introduction", "content": "The world has witnessed the tremendous success of large language models (LLMs) across a variety of tasks (Touvron et al., 2023b; OpenAI, 2023). Such success has attracted numerous parties to fine-tune their customized LLMs by leveraging their local private data (Wu et al., 2023; Xue et al., 2023; Zhou et al., 2024; Singhal et al., 2023). Nonetheless, training such LLMs on private data could cause significant privacy concerns, since LLMs are shown to memorize sensitive information from the training data (Carlini et al., 2021; Lukas et al., 2023).To address this privacy issue, a series of methods have been proposed to circumvent the direct"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Privacy Concerns with Fine-tuning on Private Data", "content": "Fine-tuning large language models is crucial for enhancing their instruction following ability and im-"}, {"title": "2.2 Synthetic Text Generation", "content": "Two widely adopted approaches for generating private synthetic text in practice are In-Context Learning (ICL) (Dong et al., 2022; Chen et al., 2024; Ye et al., 2024a) and Self-Instruction (Wang et al., 2022). Largely relying on prompt design and the base model's comprehension, they suffer from either low data fidelity yielded by the base model, or privacy concerns requesting API servers. What makes it worse, with private data included directly in prompts, these methods pose an additional risk of revealing sensitive information.\nRecently, researchers have recognized the feasibility and effectiveness of the DP generator method (Yu et al., 2024; Yue et al., 2023; Kurakin et al., 2024). This approach first trains an LLM on private data with DP, and then repeatedly samples the DP-finetuned model to generate synthetic text sequences. Although proved to gain improvements in distribution similarity, previous works primarily concentrate on generating diverse synthetic instructions. They ignore or skip the practical scenarios where responses are equally crucial for instruction tuning of LLMs. Moreover, current DP generator methods only focus on general knowledge, lead-"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Setup", "content": "Let $\\mathcal{D}_{pri}$ represent the private dataset possessed by the client, which contains privacy from patients. $W_{Loc}$ is the local base model pre-trained on general data that needs to acquire medical knowledge from $\\mathcal{D}_{pri}$. $W_{pro}$ refers to the professional model hosted by the server which is relatively larger than $W_{Loc}$ and is assumed to have extensive knowledge of the medical domain. To formalize our problem setup, we assume that $\\mathcal{D}_{pri}$ used for instruction tuning consists of two components: $Instruction$ and $Response$, both of which contain Personal Identifiable Information (PII), e.g. patients' names. Therefore, $\\mathcal{D}_{pri}$ can not be directly transmitted over networks due to privacy concerns. We present a detailed definition of PII in Appendix D.\nOur ultimate objective is to generate a synthetic dataset $\\mathcal{D}_{Syn}$ that maintains high data quality while containing no trace of PIIs. This allows us to fine-tune $W_{Loc}$ on $\\mathcal{D}_{Syn}$ to facilitate improvements in privacy-performance trade-off."}, {"title": "3.2 System Overview", "content": "We introduce a novel client-server framework called KnowledgeSG (Knowledge-based Synthetic data Generation), which aims to improve synthetic data quality and further promote model performance without violating privacy.\nWe attribute the quality gap between synthetic data and original private data to the comprehension deficiency of the local model $W_{Loc}$ used for generation. Due to privacy concern, previous works place all generation on the client side without involving the server. To compensate for the aforementioned comprehension deficiency, we further extend previous setting into a client-server framework to leverage the knowledge from the server-side professional model $W_{pro}$. We give further elaboration of the quality gap in Appendix E.\nThe client-server framework of KnowledgeSG involves learning local knowledge from private data on the client side and acquiring knowledge distillation from the professional model on the server side. We also design a convenient transmitting unit to mitigate potential eavesdropping. In this way, we manage to achieve superior performance results while preventing memorization or leakage of the private dataset $\\mathcal{D}_{Pri}$"}, {"title": "3.3 Client Side", "content": "On the client side, our framework is primarily designed to extract knowledge from the private data $\\mathcal{D}_{Pri}$ without memorization and subordinately de-"}, {"title": "DP-based Local Learning", "content": "Due to its direct access to $\\mathcal{D}_{pri}$, the client side must comply with strict privacy constraint while still enabling effective knowledge learning from the private dataset $\\mathcal{D}_{pri}$. To achieve this primary goal, we adopt Differentially Private SGD (DP-SGD) (Abadi et al., 2016).\nDP-SGD is a privacy-preserving optimization algorithm that improves upon traditional Stochastic Gradient Descend (SGD) by adding noise to the gradients during training. This noise ensures that the inclusion or exclusion of any individual data sample has a minimal impact on the resulting fine-tuned model $W_{DP}$, offering strong privacy guarantees. We follow the first step of previous works (Yu et al., 2022; Kurakin et al., 2024; Yue et al., 2023) and adopt DP-SGD as our local training approach. The local base model $W_{Loc}$ pre-trained on general corpora, is fine-tuned through DP-SGD, i.e. DP-finetuned on $\\mathcal{D}_{pri}$ to gain local knowledge under a privacy budget $(\\epsilon, \\delta)-DP$. This budget theoretically guarantees the process of DP-finetuning without any leakage of private information, providing the basis for us to transmit the fine-tuned model $W_{DP}$ to the server later."}, {"title": "LORA Adaptation", "content": "The second characteristic of the client side in Knowledge is lightweight, since we do not expect the client to have substantial hardware resources compared to the server. Therefore, we minimize the workload on the client by shifting the resource-intensive data generation process to the server.\nBesides, we apply Low-Rank Adaptation (LoRA) (Hu et al., 2021) using the implementation of Wutschitz et al. (2022), as our training approach. LORA is an efficient fine-tuning technique for large language models. It reduces the number of trainable parameters by introducing low-rank decomposition into the weight matrices of the model, allowing for faster and more resource-efficient adaptation to new tasks.\nEven when considered relatively \"small\", the full size of the base model such as Llama2-7B, still occupies a significant amount of storage. The resulting inconvenience for transmitting the full model weights of $W_{DP}$ is plain to see. In contrast, LORA adaptation significantly reduces the transmission burden by allowing us to send only the LORA adapter $A_{DP}$, resulting in a far more manageable model size."}, {"title": "3.4 Server Side", "content": "The server side of KnowledgeSG is designed to improve data quality beyond what can be achieved by relying solely on the client. It operates through three stages: raw synthetic data generation, refinement of raw synthetic data and normal fine-tuning of local model."}, {"title": "Synthetic Instructions Generation", "content": "The first step on the server side is to recover the full model $W_{DP}$ from $A_{DP}$, assuming the server has the same base model $W_{Loc}$ as the client prior to communication. Afterward, we prompt the DP-finetuned model $W_{DP}$, which has knowledge of the private data $\\mathcal{D}_{pri}$, to generate raw synthetic instructions.\nThe post-processing property of DP (Dwork and Roth, 2014) ensures that once the model $W_{Loc}$ has been fine-tuned with DP, sampling from the fine-tuned model $W_{DP}$ incurs no extra privacy loss. As a result, when the LoRA adapter $A_{DP}$ is uploaded to the server, it can generate synthetic data without exceeding the privacy budget $(\\epsilon, \\delta)-DP$."}, {"title": "Synthetic Instruction Filtration", "content": "During the second stage, to realize optimal results, we apply two compatible filtration methods distinguished by whether assistance from the professional model $W_{Pro}$ is required.\nFiltration without $W_{pro}$ uses similarity deduplication via the BLEU score (Papineni et al., 2002). Bilingual Evaluation Understudy (BLEU) is a widely used automated evaluation metric for measuring the similarity between machine translation outputs and reference translations to assess translation quality. We adopt it to determine if an synthetic instruction is too similar to any example from the private dataset $\\mathcal{D}_{pri}$ to raise possibilities of leaking privacy. This method is much faster compared with the other model-based method.\nFor the filtration method involving $W_{Pro}$, we prompt the raw instructions into $W_{pro}$ for judgements. If the instruction is domain-specific, $W_{Pro}$ assesses whether it is relevant to its domain. If it is domain-specific, $W_{pro}$ judges an instructions"}, {"title": "Efficient Knowledge Distillation", "content": "Without the need to derive loss from $W_{pro}$ (Flemings and Annavaram, 2024), we use a convenient method of knowledge distillation by feeding top instructions into $W_{pro}$ to generate preferable responses corresponding to these instructions after filtration (Xu et al., 2023; Wang et al., 2022; Jiang et al., 2023). This step is crucial as the knowledge is embedded in these responses which are subsequently distilled into the local model $W_{DP}$ through fine-tuning.\nFinally, we use the generated instructions and responses sorted by the IFD score (Li et al., 2023a) to normally (non-DP) fine-tune $W_{DP}$ and obtain the desired model $W_{Target}$. Further details and results regarding the IFD score are presented in Section 4.5. At this stage, DP-finetuning is not needed, as we assume the refined synthetic data contains no sensitive information."}, {"title": "3.5 Communication between Client & Server", "content": ""}, {"title": "Federated Model Transmission", "content": "Although synthetic data is supposed to contain no privacy, i.e. PIIs, two non-negligible concerns remain: (1) The size of the data prepared for fine-tuning are relatively larger than that of the LoRA adapter $A_{DP}$. (2) Leakage of synthetic data can potentially reveal approximate data distribution or other sensitive information.\nTherefore, inspired by federated fine-tuning of language models (Wei et al., 2020; Ye et al., 2024b), we propose to apply transmitting the fine-tuned version of model into our new setting which only has one client and one server, rather than directly transmitting data."}, {"title": "Proposed Transmitting Unit", "content": "Moreover, to reduce the potential risk of eavesdropping, i.e. an unauthorized party intercepts and steals the transmitted model, we introduce an efficient transmitting unit. Note that this unit is compatible and optional if the client using KnowledgeSG has no concerns about eavesdropping.\nWe start by sampling a small amount of data from public datasets, e.g. Alpaca (Taori et al., 2023), as the seed dataset $\\mathcal{D}_{Seed}$, which is agreed and shared by the client and server at the beginning. Then we fine-tune the original base model $W_{Loc}$ on $\\mathcal{D}_{Seed}$ to create a full adaption of model weights and replace original $W_{Loc}$ with the new"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Basic Setups", "content": "Models and Datasets. If not otherwise mentioned, our base model is pre-trained Llama2-7B (Touvron et al., 2023b). We choose FinGPT (Yang, 2023) and AlpaCare (Zhang et al., 2023) as our professional models for financial and medical domains respectively. The dataset sample is kept to 500 for any comparison except the ablation study in Section 4.6. We use the name substitution technique in Appendix B.2 to pre-process datasets, preventing inaccurate evaluation on privacy.\nBaselines. Our baselines comprise one None-Private approach, one private approach with DP-SGD (Abadi et al., 2016), and six private approaches using synthetic data generation, i.e. ICL (Dong et al., 2022), Self-Instruct (Wang et al., 2022), Self-Instruct-ICL, DP-Gene (Kurakin et al., 2024), DP-Instruct (Yu et al., 2024) and DP-Instruct-ICL. The detailed comparison of baselines is shown in Table 14 in Appendix F.3."}, {"title": "4.2 Privacy Evaluation", "content": "Setups. We study the privacy leakage of LLM by measuring the reconstruction rates following Lukas et al. (2023). In this approach, the attacker is given a sentence with multiple masked pieces of PII and asked to reconstruct the target PII from given candidates. The reconstruction rate is then calculated as the success ratio over attempt times.\nIn practice, for each sample in our training dataset, we mask all individual names and randomly choose one as the target. Then we use the PII reconstruction attack (Lukas et al., 2023) to predict the targeted individual name from a list of candidates and report the average prediction accuracy. Concretely, each time we sample 64 names as candidates from our datasets, making sure one of them is correct, and decode from the model using top-k sampling with k set to 40. We employ Flair models (Akbik et al., 2018) to tag individual names in the datasets."}, {"title": "4.3 Financial Benchmarks", "content": "Setups. We use the financial sentiment analysis dataset as the training dataset (Yang et al., 2023). During the evaluation, we employ the code from Yang et al. (2023) and consider four financial sentiment analysis benchmarks, including FPB (Malo et al., 2014), FIQA-SA (Maia et al., 2018), TFNS (Magic, 2022), and NWGI (Yang, 2023), where both accuracy and F1 score are measured. Besides, we also report the performance of GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) for reference. Since NWGI cannot be measured using GPT-3.5/4, we report the average metric of the first three and four evaluation datasets for an overall comparison.\nResults. Table 3 demonstrates the results of our method and six other baselines using synthetic data generation on financial benchmarks. From the table, we can conclude that: (1) KnowledgeSG out-"}, {"title": "4.4 Medical Free-Form Evaluation", "content": "Setups. We utilize the HealthCareMagic-100k dataset (Li et al., 2023c) as our training dataset, since it contains many individual names (e.g. see Fig 4). This dataset consists of real conversations between patients and doctors collected from the HealthCareMagic website.\nFollowing Zhang et al. (2023), we conduct free-form evaluation by employing GPT-3.5-turbo (Zheng et al., 2023) to serve as a judge. For each instruction in the test dataset, the judge pairwise compares two responses resulting from the target model and THE reference model, respectively. We"}, {"title": "4.5 Data Quality Measurement", "content": ""}, {"title": "Embedding Distribution Similarity", "content": "As shown in Yue et al. (2023), the similarity of synthetic data to the original data implicitly indicates its quality. Unlike typical natural language generation"}, {"title": "4.6 Ablation on Dataset Size", "content": "Setups. We perform an ablation study on dataset size to investigate its impact on the model's fi- nal performance through synthetic data genera- tion. The training and evaluation setups are the"}, {"title": "4.7 Transmitting Unit", "content": "Setups. We employ alpaca (Peng et al., 2023) and randomly select 50 samples to form our seed dataset $\\mathcal{D}_{Seed}$. We first fine-tune Llama2-7B on $\\mathcal{D}_{Seed}$, then replace the original model with its fine-tuned version. We assume the attacker only has access to the transmitting process, meaning he can intercept the LoRA adapter fine-tuned on the new base model. Without access to $\\mathcal{D}_{Seed}$, the attacker can only attempt to merge the adapter with the original base model, i.e. open-sourced Llama2-7B, thus unable to reproduce the full performance of our model Relative Drop is calculated by $Relative Drop = \\frac{(KnowledgeSG - Attacker)}{KnowledgeSG}$\nResults. Results in Table 7 show that the performance of model stolen by the attacker drops significantly compared to KnowledgeSG. This demonstrates that our model is not compromised, confirming the efficacy of proposed transmitting unit."}, {"title": "5 Discussions", "content": ""}, {"title": "5.1 Why not Scrubbing", "content": "The most intuitive way of privacy-preserving is PII scrubbing. PII scrubbing is a dataset curation technique that removes PII from text, relying on Named Entity Recognition (NER) to tag PII. In practice, using scrubbing to mask or add noise to"}, {"title": "5.2 Why not DP-SGD only", "content": "Fine-tuning models to satisfies DP can only address the risk of memorization. There is no protec- tion during the data collection stage where the user instructions are exposed to human annotators for response generation (Yu et al., 2024). Moreover, using DP-SGD to prevent memorization by adding noise into the training process is destined to sac- rifice performance. As proved in our experiments in Table 11, employing DP-SGD alone leads to considerable performance drop."}, {"title": "6 Conclusions", "content": "This paper addresses the challenge of preserving privacy while fine-tuning large language models on sensitive data. To improve the quality of synthetic data, an aspect often overlooked in previous works, we introduce a novel client-server framework called KnowledgeSG. Specifically, KnowledgeSG leverages knowledge distillation from a professional server, by prompting it to provide judgments and corrections for raw synthetic data generated by the DP-finetuned base model. Inspired by federated learning, KnowledgeSG transmits models rather than data through a specially designed transmitting unit to ensure privacy. We conduct extensive experiments, and the results validate the effectiveness of KnowledgeSG. The framework achieves a relative improvement of 120.39% compared to the Non-Private training, as measured by medical free-form evaluation. Additionally, KnowledgeSG significantly reduces the reconstruction rate from 97.13 to 0.87, demonstrating its strong privacy-preserving capabilities."}, {"title": "7 Limitations", "content": "While KnowledgeSG offers best privacy and perfor- mance trade-off across various domain-specific sce- narios, its effectiveness on general tasks remains to be fully explored. Further experiments are needed to test its generalizability in broader contexts.\nAlso, KnowledgeSG involves more communication and computation cost than Non-Private fine- tuning, as it requires DP-finetuning the base model and leveraging a professional model for knowledge distillation. However, we believe these costs are justified, given the significant reduction in memorization concerns and the substantial performance improvements.\nFor future directions, we plan to conduct experiments on more general tasks and seek ways to optimize communication and computation costs. Additionally, we aim to make the deployment of KnowledgeSG more compatible and lightweight."}, {"title": "A Privacy Analysis", "content": ""}, {"title": "A.1 Potential Privacy Risks", "content": "There is a potential privacy concern that the base model may have already encountered the private dataset $\\mathcal{D}_{Pri}$ during pre-training. If this is the case, synthetic data generated by the base model $W_{Loc}$ or its DP-finetuned variant $W_{DP}$ may still violate pri- vacy requirements (Igamberdiev et al., 2022). Additionally, if the professional model $W_{Pro}$ has been trained on $\\mathcal{D}_{Pri}$, it could inadvertently produce sen- sitive information such as individual names, when we utilize it to distill knowledge and improve the synthetic data generated by $W_{DP}$.\nTo address this concern in KnowledgeSG, we will provide both theoretical elaborations and ex- perimental results. It is important to note that the likelihood of private datasets being leaked and pre- trained by models is minimal in real-world appli- cations. Our work focuses on preventing further memorization when using sensitive data, rather than reversing any memorization that has already occurred."}, {"title": "A.2 Theoretical Privacy Elaborations", "content": "Interchangeability of Models. In our frame- work, both the base model and professional model are interchangeable. KnowledgeSG is not depen- dent on any specified LLM, e.g. Llama2-7B. The clients using KnowledgeSG can select any other LLM that has not been pre-trained on their private datasets to mitigate the risk.\nTheoretical Guarantee of Differential Privacy. Based on previous works, we assert the privacy-preserving nature of our framework is justified by differential privacy theory. First, on the client side, we follow Abadi et al. (2016); Yue et al. (2023) to DP-fintuned the base model $W_{Loc}$. This provides us with a strong theoretical guarantee against memorization within the privacy budget $(\\epsilon, \\delta)-DP$. Second, on the server side, the post-processing property of DP (Dwork and Roth, 2014) ensures that once the model $W_{Loc}$ has been fine-tuned with DP, sampling from the fine-tuned model $W_{DP}$ does not result in extra privacy loss. Therefore, when the LoRA adapter $A_{DP}$ is uploaded to the server, it can generate synthetic data without exceeding the privacy budget, mitigating associated privacy risks."}, {"title": "A.3 Experimental Results", "content": "Setups. To further validate the effectiveness of KnowledgeSG and ensure that no private data has been accessed by either the base model or the pro- fessional model, we conducted additional experi- ments using the ai-medical-chatbot dataset, which was collected and released six months later than Llama2-7B and AlpaCare. We adhere to the exper- imental setups described in Section 4.4 and also employ Llama2-7B as the base model.\nResults. The results presented in Table 8, reaf- firm the effectiveness of KnowledgeSG, regardless of whether the models had access to the private dataset. It also shows that KnowledgeSG can gen- eralize well across different datasets. Additionally,"}, {"title": "B Additional Techniques", "content": ""}, {"title": "B.1 Filtration with Models", "content": "As mentioned in Section 3, filtration with model means that we prompt the professional model $W_{Pro}$ with raw instructions for judgments. Then we filter out subpar instructions based on judge- ments.\nFor domain-specific settings such as the medi- cal domain, the judgements are mainly based on whether the tested instructions are related to partic- ular medical knowledge. We first prompt AlpaCare using the template written in Figure 10, then ex- tract judgements from the model outputs. In exper- iments, we also try GPT-3.5-turbo as the domain classifier of instructions and receive acceptable re- sults."}, {"title": "B.2 Name Substitution", "content": "In order to discard the possibility that the pre- trained model has already seen those individual names (e.g. John, Trump) in our training datasets $\\mathcal{D}_{Pri}$, we ask GPT-4 (OpenAI, 2023) to generate hundreds of unique names (e.g. Anastasija, Melan- gell) to substitute the original names. This tech- nique addresses the potential privacy risk discussed in Appendix A and pave the groundwork for accu- rate experiments in Section 4.2.\nTo evaluate the name substitution technique, we follow the experimental setups in Section 4.2, and compare reconstruction rates of different baselines before and after name substitution. The results in Table 9 reveal the effectiveness of our approach. Before name substitution, there is no distinguished gap between the different models. After name sub- stitution, as expected, the pre-trained Llama2 ex- hibits no memorization, while the Non-private ap- proach shows high memorization because of fine- tuning over private data. And the memorization issue is addressed through synthetic text genera- tion."}, {"title": "C Additional Experiments", "content": ""}, {"title": "C.1 Medical Benchmarks", "content": "Setups. We evaluate the same models as Sec- tion 4.4 on 3 medical question answering bench- marks including MedQA (Jin et al., 2021), Pub- MedQA (Jin et al., 2019), and MedMCQA (Pal et al., 2022). We follow the code base of LMflow and use the prompt shown in Figure 6 to inference answers.\nResults. From Table 10, we can conclude that: (1) Compared to free-form evaluation in Section 4, the results on medical benchmarks are more ran- dom. Along with the limit of performance ceiling, the gap between different methods are narrowed especially on MedQA and MedMCQA. (2) Our method still performs the best on average."}, {"title": "Distinctions in Medical Evaluations", "content": "Compared to the benchmark results in Table 10, the gap be- tween different baselines is much more pronounced and noticeable in the free-form evaluation in Table 4, aligning more closely with expectations. We at- tribute the reasons as: (1) For MedQA and MedM- CQA, the dataset we use is HealthCareMagic, whose purpose is to provide patients with consul- tant. This may not correspond with the nature of benchmarks to choose the right answer to a medicine-related question. (2) Benchmark results"}, {"title": "C.2 DP-SGD Performance Evaluation", "content": "We follow the details for DP-finetuning in Ap- pendix F.1 and evaluate its performance on the financial domain, same as Section 4.3.\nFrom the results in Table 11, we can conclude that relying on DP-SGD only results in a consid- erable decline of performance, necessitating our approach of synthetic data generation with knowl- edge distillation from server."}, {"title": "C.3 Generalizability in Other Domains", "content": "Setups. To evaluate the generalizability of KnowledgeSG, we conduct additional experiments in the mathematical and code domains.\nFor the experimental setup of mathematical do- main, we utilize 500 samples from the lighteval/- MATH dataset, employing MAmmoTH-7B (Yue et al., 2024) as the professional model and Llama2- 7B as the base model. Following Yue et al. (2024), we evaluate models on the GSM8K dataset (Cobbe et al., 2021) using the public benchmark MAm- moTH. For the code domain, we utilize the PythonCodeInstructions-18k dataset, employing Llama3-8B-Instruct as the professional model. We evaluate models on HumanEval dataset (Chen et al., 2021) using the bigcode-evaluation-harness benchmark (Ben Allal et al., 2022).\nWe compare three representative methods: Non- Private fine-tuning, In-Context Learning (ICL), and a simplified version of KnowledgeSG that replaces the synthetic responses in ICL with those generated by the professional model $W_{Pro}$"}, {"title": "D Definition of PII", "content": "There are various definitions of Privacy catering to different privacy concerns in different scenarios. A LLM can know your preference by digging into your search histories. It can also infer that you have a girlfriend from your recent query of buying flowers on Valentine's day. In this work, we mainly research on one of the definitions of privacy, i.e. PII which is well-studied by the community.\nPII is short for Personal Identifiable Information,"}, {"title": "E Differences of Domain-Specific Data from General Data", "content": ""}, {"title": "E.1 Illustration", "content": "We give additional illustration in this section to explain the performance discrepancies of domain- specific data and general data after synthetic data generation.\nDeploying an LLM to generate new synthetic data from the original private data is just like asking a student to read an examination question and try to create a new copy of it. Naturally, the quality of the rewritten question is highly dependent on how the student understands the original question, and how he may generalize. As illustrated in Fig 5, a Ph.D. student will behave well on general questions, e.g. Alpaca (Taori et al., 2023). But if you ask a kindergarten student to create a new calculus test"}, {"title": "E.2 Gap Ratio", "content": "For the purpose of quantifying the gap between domain-specific data and general data and provid- ing better understanding of the proposed problem, we heuristically define a ratio called Gap Ratio. We choose GPT-4 (OpenAI, 2023) to be the da- tum model as we assume it is an all-around player that behaves well both on general tasks and domain- specific tasks. And the Gap Ratio is calculated by the ratio of target model results and GPT-4 results on the same evaluation benchmark. For example, from Table 13, Llama2-7B's Gap Ratio is 0.8722 on Chatbot Arena and 0.7007 on general bench- marks on average.\nNo matter what the absolute value is in different measurements of model performance, we can ap-"}, {"title": "F Implementation Details", "content": ""}, {"title": "F.1 Training Details", "content": "For normal fine-tuning (not DP), we follow the codebase of (Ye et al., 2024b) and use the local training algorithm to train the model for 100 rounds in total. For each round, we train for 10 steps with batch-size set to 5 using AdamW (Loshchilov and Hutter, 2018) optimizer. This means each sample in the training dataset is iterated for 10 times on average, equal to training the model for 10 epochs without setting max-steps. We apply a cosine learn- ing rate schedule according to the round index. The initial learning rate in the first round is 5e \u2013 5, and the final learning rate in the last round is le - 6.\nFor DP fine-tuning, we follow the codebase of dp-transformers library (Wutschitz et al., 2022), which is a wrapper around Opacus (Yousefpour et al., 2021). We train the model for 4 epochs for the first stage of generation, and 10 epochs for fair comparison between training on private data with DP and training on synthetic data. The target epsilon is set to 8 and maximum per-sample gradient norm is set to 1.0 for differentially private training. The privacy budget we use is (,\\delta) = (8,). According to (Lukas et al., 2023), these values are close to established DP deployments such as Apple's QuickType and Google's models. The max sequence length is set to 512 for training in both normal and DP fine-tuning. All the train-"}, {"title": "F.2 Inferencing Details", "content": "We use VLLM (Kwon et al., 2023) for faster in- ferencing and set the max-model-len to as long as 2048 to obtain more information. The inferencing experiments are mostly conducted on A100 40G. We set temperature to 0.7 to encourage diversity. We follow in-context learning (Dong et al., 2022) and self-instruct (Wang et al., 2022) to formulate our prompts. The prompt templates we employ are shown in Figure 7 and 8. To make sure we have enough instructions for subsequent filtering, the generation times are set two times of the original dataset size. To ensure sufficient instructions for subsequent filtering, the generation count is set to twice the size of the original dataset. For instruc- tion extraction and pre-processing, we extract the first instruction the model generates and filter those shorter than 2 tokens."}, {"title": "F.3 Baselines", "content": "To give a detailed comparison between different baselines in our experiments, we elaborate on three aspects in Table 14, ranging from the model used for generating instructions, whether the base- line first generates instructions then responses and whether the baseline requires few-shot examples to generate response if it is twp-step. DP-Instruct- ICL and Self-Instruct-ICL are different from DP- Instruct and Self-Instruct in that they require few- shot examples from original dataset to produce bet- ter responses during the second stage of generation while the others do not. Theoretically, DP-Instruct performs better than Self-Instruct and DP-Gene"}, {"title": "G Deployment Guidance", "content": "To"}]}