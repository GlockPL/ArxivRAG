{"title": "Noise-Agnostic Multitask Whisper Training for Reducing False Alarm Errors in Call-for-Help Detection", "authors": ["Myeonghoon Ryu", "June-Woo Kim", "Minseok Oh", "Suji Lee", "Han Park"], "abstract": "Keyword spotting is often implemented by keyword classifier to the encoder in acoustic models, enabling the classification of predefined or open vocabulary keywords. Although keyword spotting is a crucial task in various applications and can be extended to call-for-help detection in emergencies, however, the previous method often suffers from scalability limitations due to retraining required to introduce new keywords or adapt to changing contexts. We explore a simple yet effective approach that leverages off-the-shelf pretrained ASR models to address these challenges, especially in call-for-help detection scenarios. Furthermore, we observed a substantial increase in false alarms when deploying call-for-help detection system in real-world scenarios due to noise introduced by microphones or different environments. To address this, we propose a novel noise-agnostic multitask learning approach that integrates a noise classification head into the ASR encoder. Our method enhances the model's robustness to noisy environments, leading to a significant reduction in false alarms and improved overall call-for-help performance. Despite the added complexity of multitask learning, our approach is computationally efficient and provides a promising solution for call-for-help detection in real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "ASR technology has emerged as a pivotal component in contemporary human-computer interaction, enabling applications ranging from virtual assistants to assistive solutions for individuals with hearing impairments. Among the various sub-tasks in ASR, keyword spotting (KWS) has gained considerable attention due to its critical role in enabling voice-activated systems [4, 9]. While previous KWS studies has shown promise in various applications [4, 19, 23, 27, 33], its effectiveness in detecting call-for-help in hazardous environments remains underexplored. We aim to address this gap by leveraging ASR and KWS concepts to develop an effective system for call-for-help detection in real-world scenarios [6].\nKWS specifically involves detecting predefined words or phrases within a continuous speech stream, serving as a trigger for executing predetermined actions, such as activating smart speakers or initiating command sequences [31]. Typically, a keyword classification head is attached to the encoder of an acoustic model to identify predefined keywords. While this method has demonstrated efficacy in finite keyword settings, such as query-by-example [5, 11, 14, 28, 32], it frequently encounters challenges when deployed in real-world environments. One key limitation is scalability. Previous approaches require retraining when new keywords are introduced or when the system needs to adapt to different contexts. This becomes particularly problematic in dynamic environments where new keywords may emerge. Consequently, this constant retraining can lead to inefficiencies for KWS service. To address these issues, studies on Open-Vocabulary KWS system were emerged [3, 19, 21, 33], including few-shot trials [20], leveraging speech embeddings [17] and text-audio embeddings [23, 27, 29]. Despite advancements in scalability, their works often involve complex network architectures and rely on cascaded ASR systems for KWS, which can introduce challenges for user implementation.\nThis paper proposes a simple yet effective approach for KWS, particularly focused on detecting calls-for-help in emergency situations. By fine-tuning an off-the-shelf large-scale Whisper [25], on a small dataset specific to emergency-related calls for help, we aim to develop a more adaptive and scalable KWS system. Our method leverages ASR outputs to enable quick adaptation to new keywords without the need for retraining, making it highly suitable for real-world deployment.\nHowever, deploying systems in real-world environments is a challenging tasks, especially due to noise [5, 11, 12, 22, 34]. Microphones in devices like smartphones and smart speakers often pick up substantial background noise, which can significantly impair system performance. For the call-for-help in emergency situations, where both accuracy and speed of ASR are critical, false alarms triggered by noisy conditions can have serious consequences.\nWe introduce a multitask learning approach that integrates a noise classification head into the Whisper encoder. This multitask setup enables the model to learn noise patterns that commonly trigger false alarms, enhancing its robustness in noisy environments while using only a small number of additional model parameters. By simultaneously training the model to recognize both keywords and environmental noise types, our model can be a noise-agnostic call-for-help detection system in practical scenarios. Experimental results validate that our proposed method improves the model's robustness in noisy environments, significantly reducing false alarms and enhancing overall performance in real-world scenarios. Our contributions are summarized below:\n\u2022\tWe present a simple yet effective approach to call-for-help detection in emergency situations. By leveraging off-the-shelf Whisper for fine-tuning, we address the scalability issues present in previous KWS systems.\n\u2022\tWe propose a multitask learning approach for a noise-agnostic call-for-help detection system that requires only a few additional model parameters. Our method significantly reduces false alarms while enhancing overall performance in noisy environments, offering a practical solution for emergency response.\n\u2022\tTo validate the effectiveness of our method in real-world scenarios, we recruited participants and recorded call-for-help situations. As a result, our method significantly reduced false alarms in diverse environments. Finally, we provide the code to the KWS community to support further research and development."}, {"title": "II. METHOD", "content": "A. Fine-tuning Whisper for call-for-help detection\nWhisper [25] is a large-scale pretrained ASR model trained on 680K hours of multilingual speech data. Besides, Whisper is further trained on various speech processing tasks using a multitask supervision approach, such as translation, voice activity detection, and language identification, etc. By leveraging the pretrained Whisper model, it can be easily fine-tuned to extend its rich knowledge for several downstream tasks.\nWe use the Whisper model for fine-tuning on call-for-help detection tasks, which are closely related to keyword spotting. Unlike previous approaches that rely heavily on predefined keywords or acoustic models, we categorize call-for-help scenarios and utilize ASR results for classification. In this work, we define emergency scenarios as: 1) shouting for help in life-threatening situations 2) crying out for assistance when personal safety is at risk and 3) other similar situations, though not limited to these. Based on the speech and its corresponding text, e.g., help me or save me, we classify these as emergency tasks. All other results are categorized as others.\nB. Multitask training for reducing false alarm errors\nTo alleviate the false alarm errors in noisy real-world scenarios, we introduce multitask learning for noise classification. As shown in Fig. 1, noise classifier is added to the Whisper encoder, therefore the ASR encoder is sharable for noise classification as well as speech recognition. Therefore, the final loss $\\mathcal{L}_{Multi}$ can be formularized as:\n$\\mathcal{L}_{Multi} = \\mathcal{L}_{Noise} + \\mathcal{L}_{Seq2Seq}$"}, {"title": "C. Evaluation stage for call-for-help detection", "content": "To mitigate the computational costs in real-world scenarios, our evaluation stage for call-for-help detection is illustrated in Fig. 2. The proposed system commences by acquiring audio data from a microphone. First, input waveform from microphones are resampled and converted to the Mel-spectrogram format. The output audio representations from ASR encoder are then fed into a noise classifier to determine the presence of noise within the speech signal. In the event of noise detection, the system terminates processing, thereby conserving computational resources when the noise predictor exhibits high accuracy. Conversely, if speech is identified, the extracted audio features are forwarded to the ASR decoder, adhering to conventional ASR protocols. Subsequently, the transcribed output is categorized according to predefined emergency tasks, facilitating the detection of call-for-help requests."}, {"title": "III. EXPERIMENTS", "content": "A. Setting\nEmergency dataset: We used the public Korean Emergency Audio and Speech dataset\u00b9, which consists of 16 emergency situations and approximately 3,500 hours of 44 kHz audio. The dataset includes 422K training and 55K test samples across various emergency scenarios, such as call-for-help, fire, gas accidents, forced harassment, theft, robbery, and violent crime. In this study, we focused on six call-for-help scenarios, leading to final training and test sets comprising 186K and 18K samples. Specifically, for the keywords saveme and helpme, the training set contains 5,250 and 2,720 samples, while the test set includes 338 and 1,513 samples, respectively.\nNoise dataset: For multitask learning with noise classification, we employed CochlScene [13] acoustic scene dataset, which consists of 76K samples collected across 13 distinct acoustic scenes. We also consider the MS-SNSD [26] dataset for out-of-domain noise classification performance evaluation.\nReal-world recording dataset: We aimed to collect audio data that accurately represents typical real-world environments, focusing on tasks such as ASR and call-for-help detection. We collected a total of 547 audio samples, comprising both speech data and background noise. The speech data comprised 202 recordings of the phrase saveme (six distinct utterances in Korean) and 145 recordings of the phrase helpme (three distinct utterances in Korean), spoken by eight participants with an equal gender ratio in diverse environments, including indoor spaces, outdoor areas, and public restrooms. We assume that the recorded samples have an SNR of approximately 20 dB. To ensure a comprehensive evaluation of call-for-help detection model in noisy conditions, we also gathered 200 background noise recordings from offices, restrooms, and public spaces. All samples were captured using Spon TS-905E (omnidirectional), Spon TS- 715E (directional and omnidirectional), and Spon TS-929E (unidirectional) microphones.\nTraining details: We fine-tuned the Whisper-tiny model for 10 epochs, employing the AdamW optimizer with a learning rate of 1e-4, cosine learning rate scheduling, and a batch size of 32. For multitask noise classification training, the output of the Whisper encoder was processed by averaging along the time axis. To ensure stable training, momentum updates with a coefficient of 0.5 were applied to all learnable parameters. This fine-tuning setting was conducted under both single-task and multitask learning paradigms.\nMetrics: We report the call-for-help detection accuracy across three classes (saveme, helpme, others). We also provide results for noise classification accuracy and F1-score."}, {"title": "IV. CONCLUSION", "content": "This paper focused on a novel KWS-based system designed to accurately identify call-for-help detection in real-world scenarios characterized by challenging noisy acoustic conditions. To this end, we proposed a multitask learning framework for a noise-robust system. Our empirical findings demonstrated that this approach is both straightforward and effective for call-for-help detection in real-world environments. Nevertheless, future research will be required to enhance the system's noise classification capabilities across a broader spectrum of scenarios."}]}