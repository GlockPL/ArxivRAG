{"title": "PLDDT-PREDICTOR: HIGH-SPEED PROTEIN SCREENING USING\nTRANSFORMER AND ESM2", "authors": ["Joongwon Chae", "Zhenyu Wang", "Peiwu Qin"], "abstract": "Recent advancements in protein structure prediction, particularly AlphaFold2, have revolutionized\nstructural biology by achieving near-experimental accuracy. However, the computational intensity\nof these models limits their application in high-throughput protein screening. Concurrently, large\nlanguage models like ESM (Evolutionary Scale Modeling) have demonstrated the potential to extract\nrich structural information directly from protein sequences. Despite these advances, a significant\ngap remains in rapidly assessing protein structure quality for large-scale analyses. We introduce\nPLDDT-Predictor, a high-speed protein screening tool that bridges this gap by leveraging pre-trained\nESM2 protein embeddings and a Transformer architecture to accurately predict AlphaFold2's PLDDT\n(predicted Local Distance Difference Test) scores. Our model addresses the critical need for fast,\naccurate protein structure quality assessment without the computational burden of full structure\nprediction.By combining the evolutionary information captured in ESM2 embeddings with the\nsequence-wide context modeling of Transformers, pLDDT-Predictor achieves a balance between\nstructural insight and computational efficiency. Our experimental results, conducted on a diverse\ndataset of 1.5 million protein sequences, demonstrate that pLDDT-Predictor can classify more than\n90 percent of proteins with a pLDDT score above 70, closely matching AlphaFold2's confidence\nlevel.", "sections": [{"title": "Introduction", "content": "The determination of protein structure was once a task requiring extensive experimental validation, such as X-ray\ncrystallography[1] and cryo-electron microscopy, which made it both time-consuming and resource-intensive. The\nidea that computers could predict three-dimensional protein structures by calculating interatomic distances, angles,\nbond lengths, and hydrophobic interactions was considered nearly impossible. This remained the case until the\ngroundbreaking development of AlphaFold[2], which revealed patterns in protein structures that were previously\nelusive.\nAlphaFold's success demonstrated that deep learning models could, in fact, predict protein structures with near-\nexperimental accuracy. It also established the predicted Local Distance Difference Test (pLDDT) as a reliable metric\nfor assessing the confidence in these predictions. With this, the landscape of protein structure prediction was forever\nchanged. This suggested that the protein sequence itself holds critical information for accurate structure prediction,\nunderscoring the importance of sequence-based approaches."}, {"title": "Methodology", "content": "The pLDDT-Predictor is an advanced deep learning model designed to predict protein structure quality scores. It\nconsists of four primary components: an ESM2[12] Embedding Layer, a Transformer Encoder, Fully Connected Layers,\nand a Global Mean Pooling operation. This architecture leverages the power of pre-trained language models and\nattention mechanisms to capture complex protein sequence patterns and predict structure quality."}, {"title": "Network Architecture", "content": "We utilize the pre-trained ESM2 model (Evolutionary Scale Modeling) to provide rich, evolutionary-scale features for\neach amino acid in the sequence. Specifically, we employ the ESM2-t6-8M-UR50D variant, which offers a balance\nbetween model size and performance. This model has been trained on a vast corpus of protein sequences, allowing it to\ncapture intricate patterns and relationships within protein sequences.\nThe embedding process begins with tokenizing the input amino acid sequence using the ESM2 vocabulary, mapping\neach residue to a corresponding integer. This tokenized sequence is then passed through the ESM2 model to extract\nembeddings. We use the final layer's output for each amino acid, resulting in a 320-dimensional vector per residue.\nThese embeddings encapsulate complex evolutionary and structural information about each residue in its sequence\ncontext.\nTo capture both local and long-range dependencies within the sequence, we integrate a Transformer encoder[23] into\nthe model. The Transformer encoder consists of 6 layers, each with 8 attention heads and a hidden dimension of 1024."}, {"title": "", "content": "The multi-head attention mechanism in our Transformer enables the model to focus on different parts of the sequence in\nparallel, capturing complex relationships across multiple subspaces of the representation. This is particularly beneficial\nfor modeling various types of residue-residue interactions within proteins, such as local interactions in secondary\nstructures and long-range interactions in tertiary structures.\nAfter processing the sequence through the Transformer encoder, the output is passed through two fully connected layers.\nThe first layer (FC1) is a linear transformation with ReLU activation, maintaining the hidden dimension of 2048. This\nlayer allows the model to learn non-linear combinations of the features extracted by the Transformer encoder.\nThe second layer (FC2) reduces the dimensionality to a single scalar per residue, representing the predicted PLDDT\nscore. This per-residue prediction allows the model to capture local variations in structure quality across the protein.\nOnce per-residue pLDDT scores are computed, we apply global mean pooling to aggregate the scores across the entire\nprotein sequence. This operation computes the mean of the individual residue scores, resulting in a single scalar value\nrepresenting the overall protein structure confidence score. This global score provides a comprehensive measure of the\npredicted quality of the entire protein structure.\nFor model training, we employ the Huber loss function, also known as smooth L1 loss. This loss function is designed to\nhandle outliers while maintaining sensitivity to small errors. The Huber loss is defined as:\n$L_{\\delta}(y, \\hat{y}) = \\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n\\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2 & \\text{otherwise}.\n\\end{cases}$\nwhere y is the true value, \u0177 is the predicted value, and 8 is a hyperparameter that determines the transition point between\nthe quadratic and linear parts of the loss. We set 8 = 1.0 in our experiments, which balances the behavior of the loss\nfunction between Mean Squared Error (MSE) for smaller errors and Mean Absolute Error (MAE) for larger errors,\nmaking it robust to outliers.\nWe use the Adam optimizer with a learning rate of 0.0001 and weight decay of le-5 for training. To improve convergence\nand generalization, we implement a CosineAnnealingLR scheduler, which gradually reduces the learning rate over the\ncourse of training. To handle the computational demands of training on large protein datasets, we implement distributed\ndata parallel (DDP) training across 8 GPUs. This approach allows us to process larger batch sizes and accelerate\ntraining time. We use a batch size of 32 per GPU, resulting in an effective batch size of 256 across all GPUs.\nTo further optimize training efficiency, we employ mixed precision training using PyTorch's automatic mixed precision\n(AMP) feature. This technique uses lower precision (FP16) computations where possible, reducing memory usage and\nincreasing computational speed, while maintaining model accuracy through the use of a dynamic loss scaling factor.\nThe dataset is split into training (80%), validation (10%), and test (10%) sets. We use a DistributedSampler to ensure\neven distribution of data across GPUs during training. Data loading is optimized using DataListLoader with 2 worker\nprocesses per GPU, enabling efficient parallel data loading and preprocessing.\nDuring training, the model's performance is evaluated on the validation set after each epoch. We use the validation loss\nas the primary metric for model selection, saving the model with the lowest validation loss as the best model."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setup", "content": "All experiments were conducted on a distributed setup using eight NVIDIA RTX 3090 GPUs. This setup allowed for\nefficient parallel processing and reduced overall training time. We implemented our model using PyTorch for efficient\ndistributed training. The ESM2 model was utilized through the fair-esm library. We optimize the model using the Adam\noptimizer with a learning rate of 0.0001, \u03b2\u2081 = 0.9, \u03b22 = 0.999, \u0454 = 1e \u2013 8, and a weight decay of le-5. A cosine\nannealing learning rate scheduler is utilized to gradually decrease the learning rate throughout the training process. The\ninference process involves generating ESM2 embeddings for the input sequence, passing these embeddings through\nthe Transformer encoder, mapping the encoder output to per-residue scores via fully connected layers, and finally\naggregating the scores using global mean pooling. The final output is scaled back to the original pLDDT score range,\nproviding a structural confidence score for the entire protein. This architecture allows the PLDDT-Predictor to perform\nefficient and accurate predictions, making it suitable for large-scale protein screening tasks. By combining the power of\npre-trained protein language models with the flexibility of Transformer architectures, our model achieves a balance\nbetween capturing evolutionary information and modeling complex sequential dependencies in protein structures."}, {"title": "Dataset", "content": "We used a large-scale dataset of 1.5 million protein sequences for training and evaluation. This dataset was created by\nselecting diverse protein sequences from AlphaFold Database[2] to generate their corresponding pLDDT scores. The\ndataset was split into training (80%), validation (10%), and test (10%) sets.\nTo manage computational resources and ensure consistency, we truncated sequences to a maximum length of 2048\namino acids. To stabilize training and improve convergence, we normalize the target pLDDT scores to the range [0, 1]\nby dividing the original scores by 100. During inference, these predictions are scaled back to the original pLDDT range\nof 0-100."}, {"title": "Evaluation Metrics", "content": "We employed several metrics to evaluate our model's performance:\n\u2022 Mean Squared Error (MSE)\n\u2022 Mean Absolute Error (MAE)\n\u2022 Pearson Correlation Coefficient\n\u2022 Spearman Rank Correlation Coefficient\n\u2022 Classification accuracy for high-confidence structures (pLDDT > 70)"}, {"title": "Results and Analysis", "content": "In this study, we explored three different approaches for predicting pLDDT (predicted Local Distance Difference Test)\nscores: a Transformer-based model, a Graph Attention Network[24], and our proposed PLDDT Predictor. Our aim was\nto develop a method that could accurately predict protein structure quality without the computational overhead of full\nstructure prediction."}, {"title": "Performance Comparison", "content": "We evaluated the performance of each model on a comprehensive test set consisting of 10000 medium-sized protein\nsequences, each approximately 300-400 amino acids in length. Table 1 summarizes the quantitative results."}, {"title": "Analysis of Model Performance", "content": ""}, {"title": "Transformer and GAT Models", "content": "Initially, we implemented Transformer and GAT models, which have shown promise in various protein-related tasks.\nHowever, our experiments revealed limitations in their ability to accurately predict pLDDT scores."}, {"title": "PLDDT Predictor", "content": ""}, {"title": "Detailed Analysis of pLDDT Predictor", "content": "Given the superior performance of the pLDDT Predictor, we conducted a more detailed analysis of its results.\n1. Confusion Matrix: The model demonstrated high accuracy in classifying proteins as high-confidence (pLDDT\n> 70) or low-confidence. It correctly identified 3,849 high-confidence proteins (True Positives) and 1,098\nlow-confidence proteins (True Negatives), with relatively low misclassifications (277 False Positives and 192\nFalse Negatives)."}, {"title": "Inference Time Comparison", "content": "One of the key advantages of our pLDDT Predictor is its computational efficiency. We compared the inference time of\nour model with AlphaFold2 and ESMFold, two state-of-the-art protein structure prediction models. In this analysis, we\nused 10000 medium-sized protein sequences, each approximately 300-400 amino acids in length, to benchmark the\ninference times. Table 2 presents the average inference times for each model on an RTX 4090 GPU."}, {"title": "Conclusion and Discussion", "content": "In this paper, we introduced PLDDT-Predictor, a novel approach for rapid and accurate prediction of protein structure\nquality using pLDDT scores. Our method leverages pre-trained protein language models (ESM2) and Transformer\narchitectures to achieve a balance between accuracy and computational efficiency.\nKey findings of our study include:\n\u2022 High accuracy in pLDDT score prediction, with a Pearson correlation of 0.78 with AlphaFold2-generated\nscores.\n\u2022 Significant speed improvement, processing approximately 100 proteins per second on a single GPU.\n\u2022 Robust performance across various protein families and structures, as evidenced by our large-scale evaluation\non 10000 sequences.\nThe success of pLDDT-Predictor demonstrates the potential of combining transfer learning from protein language\nmodels with task-specific architectures. This approach allows us to capture both evolutionary information and complex\nsequential dependencies in protein structures efficiently.\nHowever, we acknowledge several limitations of our current model:\n\u2022 Performance degradation for very long sequences (>1000 amino acids).\n\u2022 Reliance on AlphaFold2-generated PLDDT scores for training, which may introduce biases.\n\u2022 Limited interpretability of the model's predictions.\nFuture work should address these limitations and explore the following directions:\n\u2022 Incorporating additional structural features to improve accuracy and generalization.\n\u2022 Developing methods for better handling of long protein sequences.\n\u2022 Investigating model compression techniques to further reduce inference time.\n\u2022 Exploring the application of our approach to other protein structure quality metrics.\nIn conclusion, pLDDT-Predictor represents a significant step towards enabling rapid, large-scale assessment of protein\nstructure quality. By bridging the gap between the accuracy of state-of-the-art structure prediction methods and the\nneed for high-throughput screening, our work opens new avenues for research in structural biology, drug discovery, and\nprotein engineering.\nPLDDT-Predictor represents a significant step forward in bridging the gap between the accuracy of state-of-the-art\nprotein structure prediction methods and the need for rapid, large-scale structural quality assessment. By enabling fast\nand accurate prediction of PLDDT scores, our work opens new avenues for high-throughput structural biology research\nand has the potential to accelerate discoveries across various fields, from basic science to applied biomedical research.\nAs we continue to refine and expand this approach, we anticipate that tools like pLDDT-Predictor will play an\nincreasingly important role in unraveling the complex relationship between protein sequence, structure, and function."}]}