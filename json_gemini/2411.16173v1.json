{"title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis", "authors": ["Junho Kim*", "Hyunjun Kim*", "Hosu Lee", "Yong Man Rot"], "abstract": "Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence. In this paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process. We address two main challenges to achieve it: (i) We present the SceneWalk dataset, a high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context. (ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries. Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses. Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Large Language Models (LLMs) [22, 43, 44] have brought us one step closer to achieving Artificial General Intelligence (AGI). Next following step, the current trend is shifting toward modular systems that integrate various multi-modality, leveraging the exceptional generalization and reasoning capabilities of LLMs to evolve into Large Multi-modal Models (LMMs). Accordingly, users can unrestrictedly interact with the models across various modalities beyond text, expanding the scope of machine understanding and enhancing user engagement. Especially, considering the widespread adoption of long-form videos across various web platforms, the importance of understanding long, untrimmed video has become increasingly prominent in the multi-modal domain.\nAfter the pioneer works [14, 37, 38] utilizing visual instruction tuning to augment vision perception into LLMs, remarkable strides [11, 17, 65] have been made in aligning cross-modal consistency-especially between vision and language domains. Albeit more recent models [29, 69] integrate various vision modalities all at once, current approaches still face significant challenges in understanding untrimmed and long-form video content. The main challenge is attributed to the limited context length of LMMs, which is an inherent structural limitation that restricts the models to process only a finite number of tokens as the input sequences. We can exemplify that LLaVA series [29, 30], when processing a video data, require 144 visual tokens per each frame, where numerical approximation is only maximum of ~56 frames using 8K max context length LMMs, which is still limited to handle long sequnce data.\nAccordingly, current video-LMMs [12, 35, 39] rely on (i) sparse frame sampling to represent entire videos [26, 72], (ii) dense compression of visual tokens into a smaller size to manage the excessive number of frames [34, 41], and (iii) adaptive pooling strategies [59, 60] based on the Slow-Fast approach [19], all aimed at fitting the long video sequences within the limited context window of LMMs. Several studies focusing on the long video understanding task have presented memory-augmented generation [23, 53] utilizing an additional buffer to embed long-term information, or have extended the context using RoPE-based frequency extension during the training [70]. Despite of such endeavors, when handling massive video frames, previous works still confront restricted context size and significant memory overhead, which leads to substantial visual information loss. As critical events may be overlooked by the models, this hinders their ability to fully capture context changes in lengthy videos, resulting in inaccurate and irrelevant responses for the user queries.\nStarting from the intuitive insight outlined below, in this paper, we propose a retrieval-driven approach for long video understanding with LLMs. Analogous to the recent Retriever-Augmented Generation (RAG) systems [28] (widely adopted in LLMs), which retrieve relevant information from external factual knowledge, humans naturally employ similar strategies when seeking specific information, efficiently locating and referring necessary materials to answer targeted questions-e.g., imagine that we are taking open-book exams or searching for a certain recipe in a cookbook. Given a long and untrimmed video, mirroring the targeted retrieval processes, we introduce a novel framework, Segment-Augmented LOng Video Assistant (SALOVA) to effectively handle the long sequence visual inputs by retrieving the relevant video segments.\nTo construct our video segments retrieval framework, central challenge hinges on establishing two main components: (i) Densely captioned video data, which consists of video-caption pairs with progressively-captioned descriptions that change throughout each video used to train the model to accurately identify relevant video segments. (ii) Dynamic routing mechanism, which selects pertinent video segments for the queries, followed by being connected to LLMs. To address that, our approach is outlined as follows.\nData. (\u00a73) Recently several video-text paired datasets [4, 6, 8, 10, 56, 62] have been released, but they are inadequate for handling long and untrimmed video data, where only partial video moments are described with limited word length as compared in Fig. 1(a). To handle such insufficiency of detailed descriptions within the videos and the short durations of both videos and texts, we introduce the SceneWalk dataset, a new high-quality video dataset with thorough captioning for each video. It includes dense and detailed descriptions for every video segment across the entire scene context. The SceneWalk dataset, sourced from long and untrimmed 87.8K YouTube videos (avg. 486 seconds each), features frequent scene transitions across a total of 11.8K hrs video duration and 1.3M massively segmented video clips. Each video segment in the dataset is provided with a detailed description (avg. 137.5 word length), generated by combining pre-trained models [54, 73] and manual curation from human.\nArchitecture. (\u00a74) Utilizing the constructed video dataset, SALOVA learns to identify relevant video segments for the given queries within each video source and then auto-regressively predicts the next token. To do so, we present two architectural designs to seamlessly incorporate the retrieved segments in an end-to-end training: Spatio-Temporal Connector and Segment Retrieval Router. By focusing on the relevant segments, our framework can perform deeper reasoning without being constrained by context length limitations. Additionally, we present FocusFast approach, which intensively analyzes the selected segments for detailed comprehension (focus pathway), while quickly accessing overall contextual information with routing tokens obtained from the entire video segments (fast pathway). The strategy ensures SALOVA to maintain comprehensive video understanding while prioritizing details where it is most needed, effectively enhancing long and untrimmed video interpretation.\nThrough extensive experiments and analyses, we corroborate that competitive performance of SALOVA to the existing video-LMM models in understanding complex long-form videos. Also, our results show significant reductions in the loss of crucial visual information and a lower risk of omitting important events, demonstrating the effectiveness of our proposed method across various video benchmarks.\nOur contribution can be summarized into three-fold:\n\u2022 We introduce the SceneWalk dataset, a high-quality and densely-captioned video dataset with detailed segment-level descriptions from 87.8K long and untrimmed video sources. The proposed dataset provides rich context and scene continuity, enabling effective training for long-form video understanding.\n\u2022 We propose Segment-Augmented LOng Video Assistant (SALOVA), a novel video-LMM framework designed to enhance long video comprehension by targeting relevant video segments in lengthy videos, optimizing the model's focus on essential segment targets for the given queries.\n\u2022 Through extensive evaluation, we validate that SALOVA improves overall long video understanding capabilities by effectively integrating relevant video segments, thus optimizing to handle long and untrimmed video content."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Large Multi-modal Models", "content": "After the emergence of LLMs [5, 55], which can actively interact with users through back-and-forth conversations, as a next leap, various research efforts [2, 25, 31] integrate different modalities into the LLMs, utilizing their core reasoning and zero-shot capabilities. Building on the open-sourced models [13, 55], seminal works [14, 38, 65] have bridged image and text modality under the visual instruction tuning and presented multi-modal assistant models that possess visual perception and QA capabilities. Since then, numerous research studies have been introduced to (i) enhance vision understanding with advanced architectures [17] or higher resolutions [30, 37], (ii) implement more sophisticated alignment layers [7, 42] between modalities, and (iii) train the models with more high-quality data and larger model parameters.\nRecent focus has shifted towards more unified modality processing following the release of omnivorous models [46]. Some recent omni-versions of LMMs [29, 64] can handle combinatorial subsets from various modality sources, such as images, videos, audio, speech, and depth. However, the current LMMs for video [32, 35, 41] still lack of capturing the necessary details to effectively process video information due to their sparse frame sampling strategy. While such approach is seemingly adequate for relatively shorter videos, it may fail to capture comprehensive spatio-temporal information, potentially compromising the accuracy of model responses to user queries. In this paper, SALOVA first retrieves relevant video segments, then concentrates on more granular video cues. Such targeted focus allows the model to effectively understand complex analysis within the videos, significantly improving its ability to provide contextual-aware and accurate responses."}, {"title": "2.2. Long Video Understanding", "content": "In parallel, we detailedly introduce video-specialized models [12, 23, 53] integrated with LLMs, which have also been widely explored these days to enhance video understanding and reasoning. Here, the most challenging part of current video-LMMs lies in handling long video sequences, mainly due to the limited context length of the LLMs. This limitation compels the models to sparsely sample the video frames in only limited sizes (e.g., typically 8 or 16 frames), potentially missing important spatial and temporal information. To address this, several studies have focused on compressing visual tokens into a more manageable size, proposing aggregation [34, 41] or pooling methods [35, 59] with advanced vision encoder structures [67, 73]. In addition, memory-augmented methods [23, 53] first stored long-term information in a memory bank, then responded to specific queries by loading memory features from the stored buffer. On the other hand, among more recent approaches, Li et al. [70] have directly extended the LLMs' context length by exploiting RoPE-based frequency interpolation, and Xue et al. [61] have introduced sequence parallelism that can be implemented on multiple GPUs by modifying backend systems. However, we argue that current approaches inherently cannot be free from the fixed context length and provoke intensive memory demands when processing more longer videos. Instead, by focusing on the relevant segments within the entire video, SALOVA can efficiently handle the limited context length, enabling targeted processing of key moments without the need for excessive memory consumption, thereby enhancing performance on longer video sequences."}, {"title": "3. Scene Walk Dataset", "content": "In this section, we elaborate on how we collected the SceneWalk dataset. The overall pipeline for building the dataset and summarized statistics are illustrated in Fig. 1. While several video SFT datasets [32, 41] are widely used during the instruction tuning stage, they often fail to capture comprehensive details within the scenes. This stems from the nature of instruction-type questions, which provide only partial information, and the brief lengths of both videos and texts in QAs. In contrast, the SceneWalk dataset offers densely captioned video-text pairs that cover long sequence videos in full details, as shown in Fig. 1(a). For further detailed data statistics, please see Appendix A."}, {"title": "3.1. Data Gathering and Processing", "content": "Video Source & Filtering. For the long and untrimmed video sources, we primarily focus on three key aspects to build densely captioned video dataset: (i) extensive video length with diverse video source categories, (ii) high-quality video contents, (ii) frequent scene transitions within each video. Accordingly, our data collection is mainly sourced from YouTube, ensuring rich dynamic content that better reflects real-world complexities experienced by global users-here, because our main goal for video gathering is on complex scene understanding, we exclude low-quality and user-uploaded aesthetic videos (e.g., WebVid, Pixabay, Pexels, and etc..) that are rather beneficial for video generation tasks, despite their merits for easy collection. We have collected YouTube urls from [27] and downloaded the whole video in untrimmed states. Among the total 32 coarse and diverse video categories YouTube API provided, we selectively curated 10 categories, excluding categories such as News & Politics, Classics, and Documentary, due to their static nature, which provides only sparse temporal information in the videos. We further supplemented the dataset with additional Movie & Drama videos sourced from [21, 53], totaling 87,867 video sources with 11.87 Khrs video duration (avg. 486.5-seconds).\nSegmenting Video into Clips. Next, for the collected long and untrimmed video sources, we cut the lengthy videos into small segments to densely caption the entire video in next phase. Instead of adopting the bottom-up approach used in the ShareGPT4Video dataset [8], which segments videos into fixed time intervals (2-seconds) in advance and then merges adjacent frames based on their CLIP similarity [48], we directly employed PysceneDetect 1 to segment the videos, dynamically adjusting the threshold based on the raw-level video information to reliably detect scene changes. At the end, the total number of 1.29M of video segments with 33.11-seconds average video length is extracted from the original video sources."}, {"title": "3.2. Captioning and Scoring", "content": "Dense Segment Captioning. After obtaining the massive video segments, our next goal is to caption each segment with visual details and narrative context to capture the scene-specific explanations, which can enrich scene-level interpretation. To achieve this, we plan to utilize pre-trained LMMs to generate detailed descriptions for the partial video segments. As the captioner, we empirically found that VILA-1.5 (13B) [36] shows competent descriptive quality than other open-sourced models, and used to generate dense captions for each video segment with randomly sampled instructions for detailed descriptions. As a result, we acquire 1.29M pairs of detailed descriptions corresponding to the video segments, each description with average 137.5 word length. Please see instruction details and qualitative examples of generated captions in Appendix A.\nScoring Video-Text Correspondence. Lastly, we score the correspondence between the video segments and the paired dense descriptions, which will later be used as explicit supervision to robustly train our retrieval framework. What we must not overlook here is that the paired video-text relationship is not solely a one-to-one correspondence but is more akin to generalized bipartite matching. That is, within the long and untrimmed video source, each video segment can be connected to other descriptions with additional edges. Therefore, for the N\u2082 number of video segments and their paired segments, we can construct a {N}2 correspondence matrix between video-text (V2T). To measure each correspondence, we employ LanguageBind [73] due to its competitive alignment capabilities across various modalities. In addition, we build another {N}2 matrix to provide a doubly robust measure for the correspondence scores among adjacent descriptions (T2T) by comparing similarity within the textual context using the SBERT model [54]."}, {"title": "4. Segment-Augmented LOng Video Assistant", "content": "Network Overview. For a given set of N video segments sampled at 1 FPS v={v}N, where each segments Vi \u2208 RTi\u00d7H\u00d7W\u00d7C has varying video length (summing up to the total time T of a long and untrimmed video), SALOVA consists of four main architecture components as illustrated in Fig. 2:\n\u2022 Vision Encoder: We use CLIP-ViT-L-336px [48] to extract visual features, followed by 2x2 average pooling, resulting in 144 visual tokens for each frame.\n\u2022 Spatio-Temporal Connector: To handle spatio-temporal features of varying lengths from the vision encoder, we employ the Perceiver Resampler [2], which consists of 2-layer Transformer architecture followed by a 2-layer MLP with GELU activation as projector. This resampler embed each video segment feature into fixed size latent features that are connected to LLMs.\n\u2022 Segment Retrieval Router: For the given textual queries, a retrieval structure (2-layer Transformer) gathers representative information (i.e., routing tokens) from each video segment and then routes the query-relevant video features into the LLMs. Note that the router architecture is trained in an end-to-end manner.\n\u2022 Large Language Model: We select two open-sourced LLMs with varying parameter sizes, LLaMA-3.2 (3B) [18], Phi-3.5 (3.8B) [1] and Qwen-2.5 (7B) [63], both of which are instruction-tuned models that possess QA assistant capabilities."}, {"title": "4.1. Long Video Processing and Pipeline", "content": null}, {"title": "4.1.1. Spatio-Temporal Connector", "content": "The first component of our model, Spatio-Temporal Connector, efficiently handles long and variable-length input video segments by extracting each segment's visual semantics in a fixed-size latent vector. As illustrated in Fig. 2(b), we first sample video frames at 1 FPS from each video segments, then visual features are acquired with 2 \u00d7 2 pooling (thus, 144 tokens from each frame). After that, the visual features are flattened and fed into the ST-Connector with additional positional and temporal encoding. Here, when the long video is processed, the number of unfolded patch tokens becomes extremely large, leading to exhaustive computations. To address this, we employ a dynamic token drop technique to reduce computational load.\nDynamic Token Drop. To effectively manage long video sequences, the token drop has been utilized in video generation tasks [16, 40]. Expanding such approach, in our framework, the dropout rate is dynamically adjusted based on the length of the input sequence T\u1d62 in the input visual feature f\u1d65 ~ T\u00a1 \u00d7 H\u209aW\u209a \u00d7 d, which allows for more efficient processing of longer sequences by reducing computational demands, while still preserving dense visual semantics in shorter videos. Additionally, to retain spatio-temporal information from the dropped patches, we add positional embeddings separately along the spatial and temporal axes. This enables more refined extraction of spatio-temporal visual semantics even after reducing the number of tokens."}, {"title": "4.1.2. Segment Retrieval Router", "content": "Next, the key to conveying the pertinent video information to LLMs is retrieving relevant video segments by querying sentence. To densely cue the similarities between the video and sentence information, we introduce a routing framework, Segment Retrieval Router, which consists of 2-layer Transformer as illustrated in Fig. 2(c). After obtaining the routing tokens R={Ri}Nv \u2208 RN\u03c5\u00d7D from entire video segments, we aggregate them and feed into the SR-Router as queries. For the given sentence, we employ the same text encoder used for the vision encoder and project it into the shared embedding space to obtain sentence features S\u2208 RN\u0141\u00d7D, where Nt indicates textual length.\nUsing the cross attention mechanism (q: R; k/v: S'), we can estimate similarity scores between the video segments and given sentence queries (i.e., V-T similarity). The scores enable the SR-Router to prioritize and select the most relevant video segments that align with the sentence query.\nRetrieval Objective. To seamlessly train the SR-Router with the mainstream flows of SALOVA in an end-to-end manner, we have designed a similarity loss function Lsim that minimizes the distance between the high-dimensional embeddings of the video segments and sentence queries. Here, we use the correspondence scores (aforementioned in Sec. 3.2) as a retrieval supervision signal yi, after applying one-hot encoding. We incorporate a simple margin-based loss, commonly used in contrastive learning settings [33], which enables the model to learn off-diagonal relaxation in the correspondence matrices between video segments and sentences. As mentioned earlier, the relationship between paired videos and sentences is closer to generalized bipartite matching than to one-to-one matching, so relaxation learning helps to accommodate the inherent complexity in aligning correspondence. In conclusion, with the binary cross-entropy loss and the score margin loss, we can formulate the similarity loss as follows:\nL_{sim} = L_{bce}(y_i, s_i) + \\frac{1}{N_{pairs}} \\sum_{i=1}^{N_{v}} max(0, \\delta - (s_i^p - s_i^n)),"}, {"title": "4.1.3. FocusFast Pathways: Integration to LLMs", "content": "Using the routing tokens, we can calculate the similarities of each video segment for the given query. Leveraging the similarities, SALOVA efficiently retrieves the specific video features that exhibit the highest relevance score to the textual query, where the indexed video features are then directly integrated into the LLM architecture. Here, extending the SlowFast pathways concept [19], we present the FocusFast mechanism to effectively manage the processing pathway for the retrieved video segments: (i) Focus pathway concatenates the top-K most pertinent features to construct a comprehensive video representation, capturing local details across retrieved segments and enabling detailed interactions with textual queries to enhance handling complex video information. (ii) Fast pathway focuses on the more broader-level context by employing segment-wide routing tokens as the condensed global representation. It effectively contains dynamic spatio-temporal changes throughout the video stream, thereby allowing SALOVA to understand the overall video content and scene-level continuity awareness.\nOnce the most pertinent features are retrieved, they are delivered to the LM backbone for the final processing as in Fig. 2(a), integrating video-specific details into the models' responses. By effectively handling long and untrimmed videos with the proposed retrieval and routing mechanism, SALOVA can maintain the flow of salient information without the processing overhead for less related data, thus generating more context-aware responses."}, {"title": "4.2. Training Strategies", "content": "The current training strategies for LMMs predominantly consist of two-step training: (i) cross-modal alignment and (ii) visual instruction tuning. Recently, Li et al. [29] have emphasized the importance of high-quality knowledge learning between the two training stages (thus stage 1.5), pointing out that the models cannot enoughly learn necessary knowledge during the alignment with the low-qualitative web-scale image-text data. As the similar approach of using rephrased descriptions for additional knowledge learning [29], we employ the newly collected SceneWalk dataset as the parametric knowledge injection step, which enables the SALOVA to learn detailed spatial and temporal representation from the long sequence video data before the instruction tuning.\nAccordingly, our training recipes and data configuration can be divided into three steps as follow (Please see the training details in Appendix B):\nStage 1: Cross-modality Alignment. For the initial step in modality alignment, we utilize 790K image/video-text paired dataset: (i) 558K image-text pairs from the CC3M dataset [52], filtered by LLaVA [38] and (ii) video-text pairs sampled from the WebVid 2.5M subset [4]. We freeze vision encoder and LLMs during the training, and mainly focus on optimizing the connector and router to map the visual information into the textual space.\nStage 1.5: Long Video Knowledge Injection. As an intermediate training step, we use the SceneWalk dataset to train the SALOVA, unfreezing all trainable parameters except for the vision encoder. During training, we input the long and untrimmed video instances and follow the processing pipeline shown in Fig. 2. By training the model with densely captioned video-description pairs, it acquires high-quality parametric knowledge of both spatial and temporal information. In addition, through the aforementioned retrieval process, the model learns to target video segments that are mostly relevant to the video description.\nStage 2: Video Instruction Tuning. To possess QA capabilities in SALOVA, we use extensive video instruction-tuning data as the final training step. The instruction data are mainly sourced from four different datasets: LLaVA-Video-178K [71], NeXT-QA [58], ActivityNetQA [66], and PerceptionTest [47] comprising a total of 1.4M video-instruction QA data, including caption entries, open-ended QA, and multiple-choice QA. Note that we train all the network parameters during this stage and auto-regressively update the instruction-following assistant's response for the next word prediction."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Experimental Details", "content": "Implementation. For the vision encoder and text encoder of the SR-Router, we utilize the CLIP-ViT-L model [48] with a resolution size of 336. We employ a 2-layer transformer with a head size of 2 for the ST-Connector, which has a latent dimension of 256. The token drop mechanism is dynamically applied according to video length, with varying maximum drop rates for each training stage-Stage 1 has no token drop, Stage 1.5 up to 0.7, and Stage 2 up to 0.4. For the configuration of SR-Router, we set 2-layer of transformers with a single head, and top-K number is set to 5 during the stage 2. Following [30, 37], the projector layer consists of 2-layer MLP with GELU. Our LLM backbones are (i) 3B: Llama-3.2-3B [18], (ii) 3.8B: Phi-3.5-mini [1], and (iii) 7B: Qwen2.5-7B [63].\nTraining Details. For the each training stage, we train SALOVA for 1 epoch with 1 node of 8 A100 GPUs. The total training hours for 3B/3.8B and 7B models roughly take 5 and 7 days, respectively. We employ FlashAttention-2 [15], gradient checkpointing [9], and ZeRO-2 [49] to minimize the memory footprint associated with model components (i.e., gradient, activation, and optimizer states). Additionally, we fine-tune the trainable parameters at each step without employing LoRA [24]. For the extended training configuration, we have attached the details in Appendix C."}, {"title": "Evaluation Benchmarks", "content": "We evaluate our model using two types of video analysis benchmarks-long video understanding and general video understanding, categorized based on the video length. For the long video benchmark, we primarily utilize Video-MME [20] and LongVideoBench [57], both of benchmarks includes videos up to two hours long duration. As the general video analysis evaluations, we employ various benchmarks such as ActivityNetQA [66], VideoChatGPT [41], and MVBench [32]. Note that the same pipeline is used to obtain video segments for each benchmark, and all benchmarks are sampled at 1 FPS without token drop during inference. As a comparison baseline, considering academic budget constraints, we evaluate against models that have similar parameter size."}, {"title": "5.2. Experimental Results", "content": "Results on Long Video Understanding. Video-MME [20] evaluates LMMs with a focus on video analysis across a variety of video types and durations. We primarily compare the benchmark results in settings without subtitles, relying solely on video frames. Therefore it can assess the LMMs' visual comprehension capabilities rigorously, based purely on visual content. Also, LongVideoBench [57] is designed to assess LMMs' understanding of long-duration videos up to two hours. It includes a diverse collection of videos, challenging the models' ability to process and interpret extensive visual and contextual information across a variety of themes. As shown in Tab. 1, our model shows competent video understanding performance across all video length distributions in Video-MME and lengthy video instances in LongVideoBench. Notably, we highlight that SALOVA achieved significant performance in the medium (average 562.7 seconds) and long (average 2385.8 seconds) length categories in Video-MME benchmark, even with more smaller size of backbone LM parameters compared with the baseline models.\nSuch performance gains in long video instances are attributed to our model's dynamic capability to retrieve and process only the relevant video segments, enabling it to handle lengthy video content efficiently without being constrained by the limited context length. Especially, the routing mechanism in SALOVA strategically prioritizes video segments that are likely to contain crucial visual and contextual cues relevant to the query. This selective routing mechanism reduces the computational load and minimizes the information loss that commonly occurs in current video-LMMs trying to process extensive video data in entirety.\nResults on General Video Understanding. Using benchmarks such as ActivityNetQA [66], VideoChatGPT [41], and MVBench [32], SALOVA was evaluated across various video types to assess its general video understanding capabilities. As shown in Tab. 2, SALOVA demonstrated competent performance, comparable to existing video-LMMs, especially in dynamic and shorter video sequences. On ActivityNetQA, the model effectively utilized its segment retrieval strategy to provide focused and contextually appropriate responses, which helped maintain accuracy. This approach was similarly effective in the multi-modal settings of VideoChatGPT and MVBench, where SALOVA showed consistent performance in handling dialogues and visual cues. These outcomes highlight SALOVA's capability to process general video content efficiently through its dynamic routing mechanism, offering a reliable solution that balances computational resources with output quality."}, {"title": "5.3. Additional Analyses on SALOVA", "content": "Ablation Study. We conduct ablation studies on three components as follows: (i) different video frame sampling strategies, (ii) intermediate training stage for long video knowledge injection, and (iii) the FocusFast mechanism to understand branched local-global representation in videos.\nAs shown in Table 3, we first observe that using more frames significantly enhances performance, particularly in understanding long-form videos. This aligns with our key insights on managing long videos, suggesting that a higher frame count can provide more spatio-temporal information and improve the model's response without losing contextual information within the video. Additionally, we compare with a baseline trained with stage 1-2 (skipping stage 1.5). Here, we highlight the effectiveness of the SceneWalk dataset as an intermediate training step to enhance parametric knowledge for the long video analysis by allowing the model to learn from high-quality and densely captioned scene-level information, which is crucial for adapting to various lengths and contexts. Lastly, we conduct an analysis on the FocusFast method and demonstrate its efficacy in analyzing not only local details from relevant video segments but also in understanding the global video context through the simultaneous use of routing tokens, thereby facilitating a more comprehensive understanding of video content.\nAnalysis of Retrieving Segments. By retrieving relevant video segments for the given queries, SALOVA can effectively target salient information in the long video and retain long context information. To further demonstrate the model's targeting capabilities beyond numerical performance in long video analysis, we explore our model's application in the Visual Needle-In-A-Haystack (V-NIAH) task [70], which extends the Needle-in-a-Haystack (NIAH) evaluation for LLMs to a vision-level benchmark. This task is particularly challenging as it requires models to not only detect but also precisely retrieve the sparse yet crucial visual cues scattered across lengthy videos.\nAs shown in Fig. 3, we compare our model to a baseline trained on sparsely sampled frames (16 frm, without SR-Router). Our framework effectively identifies and extracts relevant video segments from densely packed content, even when handling long context lengths. These results highlight SALOVA's robustness in managing complex, long-form videos, maintaining contextual continuity and relevance by strategically focusing on critical segments in response to user queries."}, {"title": "6. Discussion and Conclusion", "content": "Discussion. Despite SALOVA's competence in handling extended video sequences, it is important to recognize scenarios where its complex architecture may not be necessary. Specifically, for shorter videos where sparse sampling suffices to capture essential spatio-temporal information, simpler models could potentially outperform the efficiency of SALOVA without necessitating its extensive processing capabilities. This suggests a future avenue for integrating a hybrid approach based on our framework by dynamically adjusting the complexity of the retrieval and processing mechanisms based on the video length and content density.\nConclusion. In this paper, we introduce SALOVA, a novel framework designed to enhance the comprehension of long and untrimmed video by leveraging a retrieval-driven approach with new densely captioned dataset, the SceneWalk dataset. SALOVA strategically targets and processes only the relevant video segments, effectively addressing the structural limitations of current Video-LMMs with its Spatio-Temporal Connector and Segment Retrieval Router. Through extensive evaluation on various benchmarks, SALOVA exhibits its robust performance in interpreting complex video content, enhancing efficiency, and improving the understanding of extended videos."}, {"title": "A. Details of Scene Walk Dataset", "content": null}, {"title": "A.1. Detailed Data Statistics", "content": "We provide a comprehensive analysis of the proposed SceneWalk dataset, focusing on detailed data statistics, including video duration, categorical distribution, and segment-level descriptions. The information emphasizes the versatility and diversity of the dataset, ensuring its applicability for training our video-LLM.\nDataset Composition. The SceneWalk dataset comprises 87,867 long-form video sources, spanning a total of 11.87 Khrs (average video duration: 486.5 seconds). The video sources are collected from a curated selection of 10 diverse categories as in Fig. 1 sourced primarily from YouTube, with additional contributions from Movie & Drama datasets [21, 53]. This ensures a wide range of real-world scenarios, avoiding static categories."}, {"title": "A.2. Pipeline for Dense Caption", "content": "Splitting into Video Segments To divide untrimmed and long video sources into a massive 1.29M video segments, we directly utilize PySceneDetect with the AdaptiveDetector using the default adaptive threshold (3.0), which compares the difference in content between adjacent frames similar using a rolling average of adjacent frame changes. This can help mitigate false detections in situations such as fast camera motions.\nInstructions of Dense Segment Captioning. To generate detailed descriptions for each video segment obtained from the above process, we mainly use a pre-trained LMM (VILA-1.5-13B [36])."}, {"title": "B. Training Details of SALOVA", "content": "Training Config. In this section, we elaborate the training process of SALOVA. All variations of SALOVA undergo training with unified settings, though per-device batch sizes differ slightly due to hardware limitations. To equalize the global batch size across these variations, gradient accumulation is implemented, facilitating a consistent training timeline for each variant. The detailed training configuration for each step can be found in Tab. 5, which optimizes the use of available GPU memory for batch sizing and ensures efficient training dynamics with limited hardware resource."}, {"title": "C. Architecture Details of SALOVA", "content": "Network Config. Here, we explain our network configurations in detail. For the first part of our architecture, the Spatio-Temporal Connector, we employ the Perceiver Resampler [2"}]}