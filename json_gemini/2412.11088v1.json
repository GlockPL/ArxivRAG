{"title": "Seeing the Forest and the Trees: Solving Visual Graph and Tree Based Data Structure Problems Using Large Multimodal Models", "authors": ["Sebastian Gutierrez", "Irene Hou", "Jihye Lee", "Kenneth Angelikas", "Owen Man", "Paul Denny", "Sophia Mettille", "Stephen MacNeil"], "abstract": "Recent advancements in generative AI systems have raised concerns about academic integrity among educators. Beyond excelling at solving programming problems and text-based multiple-choice questions, recent research has also found that large multimodal models (LMMs) can solve Parsons problems based only on an image. However, such problems are still inherently text-based and rely on the capabilities of the models to convert the images of code blocks to their corresponding text. In this paper, we further investigate the capabilities of LMMs to solve graph and tree data structure problems based only on images. To achieve this, we computationally construct and evaluate a novel benchmark dataset comprising 9,072 samples of diverse graph and tree data structure tasks to assess the performance of the GPT-40, GPT-4 with Vision (GPT-4V), Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 1.0 Pro Vision, and Claude 3 model families. GPT-40 and Gemini 1.5 Flash performed best on trees and graphs respectively. GPT-40 achieved 87.6% accuracy on tree samples, while Gemini 1.5 Flash, achieved 56.2% accuracy on graph samples. Our findings highlight the influence of structural and visual variations on model performance. This research not only introduces an LMM benchmark to facilitate replication and further exploration but also underscores the potential of LMMs in solving complex computing problems, with important implications for pedagogy and assessment practices.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative AI is increasingly prevalent in computing education, with diverse applications that range from code generation [2, 13, 21, 55, 68] and high-quality code explanations [4, 5, 41, 46, 47, 68] to debugging code [43, 50]. Consequently, many students now report using them as a primary help resource [34, 53, 70]. However, the capabilities of generative AI extend beyond mere coding support; students can also leveraging these technologies to answer multiple-choice questions [59, 61] and solve text-based programming problems [25, 33, 56]. The emergence of multimodal models has further transformed this landscape by enabling the comprehension and generation of visual data, thereby enhancing the models' ability to engage with complex educational tasks. While this advancement presents new opportunities for learning, it simultaneously raises significant concerns among researchers and educators regarding assessment integrity and the potential for misuse in academic settings [40, 44, 52, 53, 70].\nTo deter unsanctioned use of large language models (LLMs) among students, researchers have proposed solutions such as human-proctored exams [36, 53, 58, 64, 70] and visual-based questions that challenge the text-only modalities of LLMs [15, 22, 50]. However, human-proctored exams come with inherent limitations, including biases in test-taking and scalability challenges that can hinder their effectiveness [17, 20, 32, 69]. Although AI-proctored systems offer a potential solution to scalability issues for online exams, they introduce a range of new concerns, such as breaches of privacy and autonomy, barriers to accessibility, and the risk of negative biases [11, 16, 28, 30]. While representing programming problems visually-through diagrams or images-has historically limited LLMs' problem-solving capabilities [22, 50], the advent of multimodal models that can process both images and text poses a significant challenge to this approach. Recent studies, such as those by Hou et al., have demonstrated these models' remarkable performance on visual programming problems, including Parsons problems [33]. This development underscores the urgent need for further investigation into the capabilities of multimodal models in visual problem-solving contexts. As these models become increasingly proficient in handling complex computational tasks, the potential for academic dishonesty grows, complicating traditional assessment strategies.\nThis paper builds upon previous research regarding the vision capabilities of large multimodal models (LMMs) by systematically evaluating the performance of several advanced models, including GPT-40, GPT-4V, Gemini 1.5 Flash, Gemini 1.5 Pro, Gemini 1.0 Pro Vision, and the Claude 3 model family. To mitigate data leakage, we computationally constructed a benchmark dataset comprising 9,072 diverse graph and tree data structure problems, designed to assess how these models handle variations in both aesthetic features (such as edge width and node color) and structural characteristics (including density, layout, and the number of nodes and edges). Through this evaluation, we aim to gain insights into the strengths and limitations to inform computing education pedagogy and assessment.\nWe investigate the following research questions:\nRQ 1 How well do large multi-modal models perform operational and representational tasks on graph and tree data structures?\nRQ 2 How do variations in the graph and tree's structural features influence the accuracy of large multimodal models?\nRQ 3 How do variations in the graph and tree's aesthetic features influence the accuracy of large multimodal models?\nOur results indicate that these multimodal models are capable of performing operational and representational tasks on graphs and trees to varying degrees, depending on structural and aesthetic variations. GPT-40 was a top-performer on tree-based problems with 87.6% accuracy, but only 44.7% on graph problems. This study highlights both the challenges that models face and the growing concerns for educators, particularly as LMMs are expected to continue improving. To facilitate replication and further research with new models, we also contribute an open-source repository for generating graph and tree problems. We conclude with a discussion on the implications for pedagogy and assessment practices. We offer the following contributions:\n\u2022 Empirical evaluation of multimodal models on data structure tasks: We provide a comprehensive analysis of multiple multimodal models' performance on visually represented graph and tree problems, highlighting their varying abilities to handle different structural and aesthetic aspects.\n\u2022 A benchmark dataset for assessing multimodal model capabilities: We introduce a novel dataset of 9,072 data structures problems that vary across structural and visual attributes.\n\u2022 An open-source tool for generating graph and tree problems: We contribute a tool to create diverse graph and tree problems, supporting further research and the development of new benchmarks as multimodal models evolve."}, {"title": "2 RELATED WORK", "content": "2.1 Graphs and Trees in Computing Education\nGraph and tree data structures are core components of most computing education curricula [35, 62]. Considered foundational topics in CS2 data structures and algorithm courses, students are expected to be able to represent and operate various tasks on graphs and trees [35]. Common examples of graph and tree tasks focus on traversals, insertions, deletions, and efficiency calculations on derivatives such as binary trees (BT), binary search trees (BST), directed graphs (DG), and undirected graphs (UG) [35]. Previous work emphasizes the value of teaching graph theory concepts to students of all ages [26]; these concepts are often important across multiple domains and industries [57, 67].\nWhile critical to computer science, graph theory can be complex to grasp [18]. Prior research has identified popular student misconceptions and errors, such as assuming binary search trees are balanced by default or performing insertions at the wrong node [37, 71, 71]. Students also struggle with traversing BSTs [49], understanding the distinction between heaps and trees, [19], and implementing error-free recursive code for operations [29]. Students especially need help with algorithmic implementation [48].\nGiven that graphs and tree problems are notoriously challenging for students, pedagogical strategies and visualization techniques have been devised to support the representation of these concepts. For example, students' understanding can be improved with visualizations, drawing attention to the relationships between nodes, branches, and structure [8, 10, 51, 63, 72].\nBased on this difficulty and visual complexity, these problems are an effective benchmark for testing the spatial reasoning capabilities of mulitmodal models in computing education. Prior work demonstrated that these models can solve text-based Parsons problems based only on images, though those tasks required limited spatial reasoning ability [33]. However, graphs can be represented in multiple styles with nodes and links placed in varied ways.\n2.2 Generative AI Threats to Assessment\nThe emergence of LLMs and LMMs has sparked widespread concern among educators regarding effective assessment methods in computing education. Educators are particularly worried about academic integrity and the potential for students to overly rely on generative AI tools, potentially compromising genuine learning [3, 40, 45, 53, 70]. These concerns have only been amplified by recent research on the wide-ranging capabilities of these models in computing contexts such as solving programming problems [13, 25, 33, 55, 56, 68] and performing as well as students on multiple choice quizzes [59, 60].\nWhile some instructors initially leaned towards prohibiting AI models in the classroom, many believe that 'resistance is futile' and that regardless of sanctions, students would use them [40, 70]. Most computing students report being aware of these models and seeking help from them on a regular basis [7, 34]. In response, some practitioners are adopting more structured uses of AI tools in classrooms, allowing students guided access to these models [38, 42], while others are developing problems to circumvent student usage [22]. Discourse on circumventing LLM usage also includes possibilities of returning to proctored or oral exams [36, 64, 70], despite known challenges and biases [11, 16, 28, 30]. Despite a potential 'arms race' between AI capabilities and instructor interventions, and capabilities continue to improve. More recently, multimodal models have emerged which can process text and image data. A recent paper [33] showed that multimodal models can even solve Parsons problems based only on an image of the problem."}, {"title": "3 METHODOLOGY", "content": "Building on previous research that benchmarks the performance of LLMs and LMMs in computing education [12, 33, 56], we evaluated the capabilities of LMMs, such as GPT-4V, in solving visual programming problems.\nRecognizing that image-based problems can help mitigate cheating in computing education contexts [22], and noting the limited research on evaluating visual problem-solving skills [33], we investigate the ability of LMMs to solve tree and graph data structures problems. We hypothesize that these problems will present unique challenges due to their reliance on spatial reasoning. Our goal is to uncover insights into the interpretative and spatial reasoning abilities of LMMs, representing a significant advancement in understanding their applications in educational contexts. This section describes the creation of our dataset and our evaluation process.\n3.1 Creating the Benchmark Dataset\nData leakage presents significant challenges when evaluating the performance of LLMs and LMMs. These models are typically trained on extensive datasets sourced from the internet, which raises the risk that they may encounter similar problems during both training and testing phases. If the test dataset includes problems that already exist online, the model could leverage its prior knowledge to solve these problems, rather than demonstrating its true problem-solving capabilities. This situation can result in inflated performance metrics, ultimately undermining the validity of the evaluation.\nTo mitigate this issue, we developed a benchmark dataset comprising graph and tree problems that do not exist on the internet. We created a Python script to programmatically generate these graphs and trees, ensuring diverse yet systematic variations. By guaranteeing that the problems are unique and not previously encountered by the model, we significantly reduce the risk of data leakage and enhance the reliability of our assessments.\nIn our commitment to advancing the research community, we have made both the generator and the benchmark dataset publicly available on GitHub under an MIT license\u00b9. By providing these resources, we aim to foster collaboration, facilitate replication, and encourage the extension of our findings, thereby establishing a solid foundation for future investigations into the capabilities of large multimodal models in tackling complex programming challenges.\n3.1.1 Task Criteria. We establish a rigorous process for obtaining graph and tree data structures vision tasks. We seek to create a standard, repeatable method to ensure consistent and reliable task selection through clear inclusion and exclusion factors.\n(1) Curricular Grounding: Subject areas were sourced from the ACM computer science curriculum guidelines [65] to minimize bias and ensure the selection of relevant topics.\n(2) Alignment with Core Topics and Learning Objectives: Evaluation tasks are selected based on their relevance to the core topics and learning objectives outlined in the curriculum guidelines, ensuring that the tasks are contextually appropriate and aligned with educational goals.\n(3) Adherence to Task Standards: Each task requires multimodal reasoning, including object reasoning, spatial relations, and counting. We designed tasks to (a) maximize open-endedness, (b) minimize unnecessary image prompt details, and (c) reduce dependence on explicit output formats, aligning with best practices for vision-language tasks [1].\n(4) Image Specifications: Images are standardized to a 1:1 aspect ratio and 512x512 resolution, which represents the minimum resolution recommended by the models' API guides at the time of publication. This standardization allows for a systematic assessment of model performance.\nThis comprehensive approach allows a diverse and robust dataset that enables in-depth analysis of model capabilities on graph and tree data structures vision-language tasks.\n3.1.2 Task Selection. The selection of samples is guided based on the Curriculum Guidelines for Undergraduate Degree Programs in Computer Science. Specifically, we focus on the topics classified under 'Core Tier 1' in the 'Fundamental Data Structures and Algorithms' course of the 'Algorithms and Complexity (AL)' domain. We are particularly concerned with the following tasks:\nBinary search trees: Common operations on binary search trees such as select min, max, insert, delete, iterate over tree.\nGraphs and graph algorithms: Representations of graphs (e.g., adjacency list, adjacency matrix) and traversals (e.g.: breadth- and depth-first).\nWe also ensure that the tasks are aligned with core curriculum standards and the learning outcomes for data structures courses. We are particularly focused on the eighth learning outcome:\nSolve problems using fundamental graph algorithms, including depth-first and breadth-first search. [Usage]\nBased on these curricular topics and learning objectives, we are able to justify our epistemic decisions. We adopted four structures: binary tree, binary search tree, undirected graph, and directed graph. We also focused on key tasks, such as traversals. This selection process ensures educational relevance for our work.\n3.1.3 Process for Constructing the Benchmark Dataset. To construct our benchmark dataset, we followed the process outlined in Figure 1. Initially, three authors sourced problems from academic textbooks, ensuring alignment with our previously established criteria. This step enabled us to identify key variations in the representation of graphs and trees, including node values, node colors, and edge widths. Leveraging these variations, we programmatically generated our benchmark dataset. We created a Python script that used graph libraries (i.e. networkx and matplotlib) to systematically create relevant images for the vision-language tasks."}, {"title": "3.2 Prompt Engineering", "content": "We establish a standard prompt method across all tasks as a zero-shot imperative task to the model in reference to the paired image. The schema consists of a user message with the text and base64 image prompts, followed by the model response as a prediction. We do not include a system message as the Gemini models currently do not support it. These base prompts are templates that contain two keys to insert information related to the given image prompt. One key is for the given data structure, such as a binary tree, and the other is a vertex node for graph prompts to instruct a model where to start a search traversal.\nThese prompts align with our task criteria, where no step-by-step instructions are given to perform the task correctly, remaining open-ended. The prompts also do not indicate the data structure's content other than the identification of the structure's type, minimizing image details. The only exceptions to this are depth-first search and breadth-first search tasks where a starting vertex was provided. The prompt does not explicitly mention the programming language, but the type annotation is in Python. We specify such a format to allow the prediction to be extracted using regular expressions.Three base text prompts take this described form:\nProvide a list[int] of vertices representing the {pre-order, in-order, post-order traversal} for the image of the structure}.\nProvide a dict[int, list[int]] of vertices and their edges representing the adjacency list for the image of the structure}.\nProvide a list[int] of vertices representing the {depth-first, breadth-first} search traversal for the image of the {structure} starting from vertex {vertex}."}, {"title": "3.3 Model Selection", "content": "To select models for this study, we aimed to align with established research practices while incorporating diversity and leveraging the most state-of-the-art models. Consequently, we first used OpenAI's GPT-4V model due to its widespread use in LMM benchmarks and popularity among both researchers and consumers (via ChatGPT). Initially, we conducted preliminary tests comparing GPT-4V with Google's Bard (prior to its rebranding as Gemini) to broaden our investigation. During these tests, we observed that Bard demonstrated inferior performance in our evaluations. However, with the subsequent release of Google's Gemini Pro 1.0 and 1.5 models-which officially replaced Bard-we observed performance comparable to GPT-4V in our pilot studies, prompting us to include both GPT-4V and Gemini as primary systems in our research. Just before the first analysis, the Claude 3 model family was released with impressive benchmarks which prompted us to include it. After the first analysis, GPT-40 was also released and we repeated the analysis for that model. Finally, all parameter values were left to default since not all models shared them, except for temperature. We elected a temperature of 1.0 to be used for the set of models since this was the most common default temperature. This set of models aims to constitute a good representation of current state-of-the-art research and consumer multimodal systems.\nA given multimodal model $M_i$ from a set of models $M$ is a function to process a given vision-language task, mapping the input pair of an image and its text prompt to an invoked prediction. Formally, for a task $(I_i, Q_i, A_{true})$, the model function is applied as $A_{model} = M_i(I_i, Q_i)$ where $A_{model}$ is the model's predicted answer. The objective of $M_i$ is to return an accurate prediction for $(I_i, Q_i)$. See Appendix B for a further breakdown of performance."}, {"title": "3.4 Evaluation", "content": "The first evaluation took place on April 17, 2024 and included the Claude family of models and GPT-4V. As other models-such as GPT-40, Gemini 1.5 Flash, and Gemini 1.5 Pro-became available, a second evaluation was completed for those models on June 13th, 2024. OpenAI models had few rate limitations, and the evaluation was completed in around 45 minutes. Gemini 1.5 Pro was the earliest released model in a preview state and, therefore, was slower to complete, at around three days. Gemini 1.0 Pro Vision was completed over two hours. Finally, the Claude 3 family of models completed each task quickly, but due to token rate limitations, it ended up taking around the same time as Gemini 1.0 Pro Vision at around two hours each. Results were saved to CSV files, where cleaning and parsing were done to ensure accurate measurements.\nTo assess the accuracy of the model responses, we define a performance metric, $P$, which quantitatively evaluates the correspondence between $A_{model}$ and $A_{true}$. The metric $P$ is a function $P(A_{true}, A_{model})$ that outputs a binary true or false value representing the correctness of the model-generated answer to the ground truth. In practice, our program uses regular expressions to extract the model's answer to compare with the ground truth. The performance metric for each model is measured in both pass@1 and pass@3 accuracy, which shows if the model answered correctly on the first attempt or within three attempts respectively.\nOur analysis requires a systematic comparison across models and task categories. The comparative function is thus articulated to discern the model that exhibits superior performance on the designated tasks. For each category of graph G and tree T tasks, we compute the mean performance across all relevant instances, delineating specific measures for undirected graphs UG, directed graphs DG, binary trees BT, and binary search trees BST. These measures facilitate a nuanced comparison across models, revealing their strengths and weaknesses. Thus, let $M = {M_1, M_2, ..., M_n}$ be the set of models we compare. We form separate performance matrices for each category and subcategory with graph tasks being $P_{model}(type)$, where type can be UG or DG, and with tree tasks being $P_{model}(type)$ where type can be BT or BST. Finally, function C takes the performance matrices and compares them to find the best-performing model for each set of tasks:\n$C(M, type) = arg max_{M_i} M P_{type}$\n3.4.1 Feature Importance. To address RQ2 and RQ3, we trained a classification model on evaluation results to examine the influence of specific features on model performance. The feature set included experimentally varied aesthetic aspects (e.g. edge width, node colors, and node value variations) and structural features (e.g. number of nodes, node values, graph density, etc.) to capture the visual diversity of graph and tree representations. Additionally, we used Deep Graph Learning (DGL) to compute augmented features, such as graph density and degree, which may impact accuracy in structurally complex tasks. Finally, the task type, determined by the six prompt categories, was included as a feature to understand how different tasks affect multimodal model performance.\nTo minimize overfitting and enhance model interpretability, we applied Principal Component Analysis (PCA) to image features captured with EfficientViT (efficientvit_13.r384_in1k), reducing noise and eliminating less relevant features. Finally, we fit a logistic regression model to the refined feature set for each multimodal model, achieving F1 scores ranging from 0.693 to 0.843, with an average score of 0.791."}, {"title": "4 RESULTS", "content": "We measure accuracy for undirected and directed graphs in addition to binary and binary search trees via pass@k performance. Our findings indicate GPT-40 performs at a higher overall accuracy than any other model in the selection for trees, followed by Gemini 1.5 Flash for graphs. See Appendix A for a detailed response analysis.\n4.1 RQ 1 Overall Performance\n4.1.1 Tree Performance. GPT-40 is the top performer on trees with 77.8% pass@1 and 87.6% pass@3 accuracy, which is nearly tied with GPT-4V performance at 77.8% pass@1 and 84.5% pass@3 accuracy. GPT-40 surpasses the average range of performance between models on trees, operating at roughly 20% higher accuracy than the next best after GPT-4V. Additionally, Gemini 1.5 Pro is only more accurate than Gemini 1.5 Flash by a small margin, with scores of 71.1% (+0.8%) pass@3. On the lower end, Claude 3 Haiku, the smallest model out of the family, achieves greater pass@1 performance than its siblings at 41.0%, an average of +3.5% higher than its siblings. However, pass@3 accuracy is surpassed by Claude 3 Opus with 49.4%, an average of +2.0% higher than its siblings.\n4.1.2 Graph Performance. The top performer on the set of graphs is split between Gemini 1.5 Flash, which achieves a top pass@3 accuracy of 56.2%, and Gemini 1.5 Pro, which reaches a top pass@3 accuracy of 53.8%. GPT-40 is the next best performer, reaching a top pass@3 accuracy of 44.7%, slightly ahead of GPT-4V's pass@3 accuracy of 40.0%. For the Claude 3 model family, Sonnet obtains the highest performance across pass@k of 26.5% pass@1, an average of +6.6% higher than its siblings, and 33.6% pass@3, an average of +7.7% higher than its siblings.\n4.2 RQ 2 Structural Features\nThe structural features were often the most important features. See Appendix C for feature engineering details.\n4.2.1 Number of Edges. The number of edges was the most important feature in terms of model performance. Further analysis of the coefficients suggests a positive influence at a fewer edges and approaches a negative influence as the number of edges increases.\n4.2.2 Degree Histograms. The distribution of in-degree vertices across all vertices in the graph had a strong influence with a slight positive influence at lower distributions and negative influence at higher distributions. Similarly, the distribution of out-degree vertices exhibit less influence than in-degree features, but they generally adhere to a similar trend as the in-degree features.\n4.2.3 Number of Nodes and Layout. The number of nodes and layout of a graph or tree exhibited a trend of positive coefficients at a smaller number of nodes and negative coefficients at a higher number of nodes, which aligns with what is observed in Figure 3.\n4.2.4 Density. The number of edges determines a graph's density compared to the maximum number of edges possible for a given graph. Our analysis suggests a positive influence at lower and a negative influence at higher densities.\n4.3 RQ 3 Aesthetic Features\nBased on our logistic regression analysis, the aesthetic features were ranked less important than structural features. The edge width varied in influence and showed inconsistent impact across width levels. This suggests that models are relatively robust in recognizing the edge connections. The node color is represented as three features that form the RGB value as a color channel. Results show that, similar to edge width, there is a limited influence on models."}, {"title": "5 DISCUSSION", "content": "Recent research has shown that generative AI tools like ChatGPT can tackle a broad range of programming tasks, especially at introductory levels (CS1) [14, 24, 33, 39, 50, 59, 60, 68]. These trends have important implications for pedagogy and assessment. Our study extends this body of work in two key ways: by examining advanced data structures problems and by evaluating the spatial reasoning capabilities of multimodal models (LMMs).\nOur findings show that multimodal models consistently solve tree problems across a range of visual presentations, including variations in structure and aesthetics, achieving performance levels sufficient to pass traditional assessments. This raises critical concerns about students' ability to leverage these tools as a shortcut, bypassing the development of conceptual understanding. While graph problems showed lower accuracy rates, the use of a simple zero-shot prompting approach suggests a lower bound. Savy students could refine the prompts or use more advanced prompting techniques. Importantly, minimal prompting can still yield strong results, suggesting students don't need extensive prompt-engineering skills to benefit from these tools [9, 22, 54, 68].\n5.1 Transforming Traditional Assessment\nOur findings challenge the prevailing reliance on traditional assessment methods in computing education, particularly as generative AI continues to evolve. While recent work has suggested that diagram-based and visual problems could counteract students' over-reliance on text-based AI tools [15, 22, 27], our results suggest that this strategy is unsustainable. Multimodal models already demonstrate significant spatial reasoning capabilities, and their ability to process and solve complex visual problems, such as tree structures, suggests that these stopgap measures will soon lose their efficacy. Even video-based problems, which have been proposed as a more robust alternative [15], are unlikely to hold up as models improve. In parallel, some educators have turned to AI detection tools in an attempt to identify when students misuse generative AI. These tools rely on statistical methods that compare human and AI writing patterns. However, this approach is flawed. Statistical resemblance alone is not reliable evidence of authorship. Just as writing that mimics the style of a famous author does not constitute plagiarism, content that resembles AI-generated patterns does not necessarily indicate Al authorship. In the context of computing, the prevalence of design patterns, coding conventions, and commonly used libraries only increases the likelihood of false positives.\nBoth of these attempts to counter AI's influence on assessment-whether through visual-based problems or detection tools-offer only fleeting solutions. Educators are now, perhaps reluctantly, being forced to adopt what some have long advocated for-innovative assessment models like ungrading [6] or project-based learning that encourage active, meaningful engagement and prioritize mastery over performance. This shift, once considered an ideal, is now becoming a necessity in the face of rapidly evolving AI tools. By reimagining assessment to emphasize curiosity and growth, we can create a more resilient approach to learning driven by purpose. This is not merely a pedagogical shift, but a moral imperative. Preparing the next generation of computing professionals requires assessments that promote critical engagement with AI systems, enabling students to navigate their limitations and biases.\n5.2 New Opportunities for Pedagogy\nWhile the focus of this work is on the implications for assessment, LMM capabilities could also provide benefits for computing students and educators. As CS2 topics like data structures are particularly challenging [37, 71] and often contribute to 'weeding out' students who lack prior experience in computer science, disproportionately impacting traditionally underrepresented students [31]. Enhanced AI tools could improve access to complex computing concepts like trees and graphs, potentially offering these students an extra layer of support. As these models advance, they could also facilitate the creation of digital teaching assistants that deliver high-quality scaffolded help [23]. More specifically, future work could investigate the potential for LMMs to produce code based on a graph or tree. This would provide help to students that struggle with graph and tree algorithms [48]."}, {"title": "6 LIMITATIONS", "content": "There are several limitations to our work. First, the tree and graph data structures used do not cover all possible data structures, and the operations evaluated are limited to core topics typically found in fundamental data structures courses. Consequently, these findings may not be generalized to more advanced topics in data structure. Second, the generated data structures may not encompass the full range of relevant attributes for graphs and trees. For instance, exploring additional edge widths and node colors could provide further insights. Therefore, investigating a broader array of structural and aesthetic features is warranted. In addition, while advanced prompting methods-such as self-critique, chain-of-thought, prompt chaining, and few-shot learning-can enhance performance in reasoning tasks, our study focused on demonstrating how students might use models with basic zero-shot approaches. This choice reflects the assumption that students may not be familiar with these advanced techniques. The model parameters also remained consistent across the evaluations, representing the default values that can be found when using these models for generalized purposes. It is almost certain that the performance varies greatly depending on the modification of certain parameters such as temperature and sampling coefficients across a matrix of these possible combinations. Finally, the models assessed in this study are continually evolving. To facilitate ongoing evaluation of future models, we have open-sourced our benchmarks and generation tools."}, {"title": "7 CONCLUSION", "content": "In this paper, we investigated the capabilities of large multimodal models (LMMs) to solve complex, visually represented data structure problems. Through the development and evaluation of a diverse benchmark dataset of graph and tree structures, we assessed the performance of various LMMs, including the GPT-4 and Gemini models, in tackling spatially and visually challenging tasks. Our results reveal that while certain models, such as GPT-40 and Gemini 1.5 Flash, performed well on tree and graph problems respectively, there remain considerable variations in accuracy that are influenced by structural complexity, such as edge density. These findings underscore the evolving capabilities of LMMs in processing visual and spatially complex problems, which has important implications for computing education. As generative AI continues to advance and reshape computing education, it becomes essential for educators to adapt assessment methods and foster environments that promote intrinsic motivation."}, {"title": "C FEATURE ENGINEERING", "content": "This section details the process and rationale behind the extraction and preparation of the features used to evaluate the performance of the multimodal models. Each feature is derived from various properties of each vision language task sample. The feature engineering process used in the study involves several techniques that transform the data into a suitable format for classification model training. Below is a detailed breakdown of these techniques.\nC.1 Graph Features\nGraph features are extracted from the adjacency list of each graph, including the number of nodes and edges, graph density, and degree statistics. These features are one-hot encoded and binned to capture structural nuances in the graph data.\nC.1.1 Data Structure Type. Each structure category, BT, BST, G, and DG is one-hot encoded into a vector.\nC.1.2 Node Value Variation. The variation identifier representing a unique set of node values for a given graph is one-hot encoded into a vector.\nC.1.3 Generation. The generation identifier representing a unique spatial graph layout is one-hot encoded into a vector.\nC.1.4 Node Count. Each graph's node count is converted into a one-hot encoded vector. The length of this vector is determined by the maximum number of nodes across the set of graphs.\nC.1.5 Edge Count. Similar to node count, the number of edges is also one-hot encoded, where each position in the vector indicates the presence of a specific number of edges, up to the maximum found in the dataset.\nC.1.6 Graph Density. The density of a graph is calculated as the ratio of the number of edges to the number of possible edges in the graph. Density is calculated differently for directed and undirected graphs. For directed graphs, the formula used is\n$\\frac{edges}{edges \\cdot (nodes - 1)}$\nand for undirected graphs it is\n$\\frac{2 \\cdot edges}{edges \\cdot (nodes - 1)}$\nThe resulting density measure is binned into predefined categories and one-hot encoded.\nC.1.7 Average Node Degree. The average degree of nodes in the graph represents the average number of connections each node has. This is calculated by dividing the total number of edges by the number of nodes. The result is binned into predefined intervals and transformed into a one-hot encoded vector.\nC.1.8 In-Degree and Out-Degree Histograms. These histograms represent the distribution of connections that go into and exit nodes, providing information on the directional flow of the graph. Each histogram uses different calculations.\na. In-degree is calculated by counting the incoming edges to each node and categorizing these counts into bins. The counts are then compiled into a histogram.\nb. Out-degree similarly counts and categorizes the outgoing edges from each node into a histogram.\nC.2 Image Features\nImage features are extracted using EfficientViT. The images are resized, normalized, and then processed to obtain average-pooled features. Dimensionality reduction is applied using Principal Component Analysis (PCA), reducing the feature space to the top components that capture the most variance.\nC.3 Text Features\nText data from prompts is processed using a TF-IDF vectorizer followed by a Truncated Singular Value Decomposition (SVD) to reduce the dimensionality while capturing latent semantic information. The number of components retained is based on the model's explanatory power on the variance observed in the text data.\nC.4 Interaction Features\nInteraction terms are created to model the interaction between different types of features, such as the interaction between the number of nodes and the width of the edges. These features are expected to provide insights into how structural properties might influence the overall characteristics of the graphs.\nC.5 Edge Width Features\nThe set of possible edge width parameter values are converted into one-hot encoded vectors.\nC.6 Node Color Features\nEach channel of a node color's RGB values are split and added as continuous features.\nC.7 Encoding and Normalization\nCategorical variables such as graph types and task identifiers are one-hot encoded. Continuous features are normalized or standardized to ensure they contribute equally to the model training process.\nC.8 Feature Reduction\nPost-feature extraction and engineering, features undergo a selection process in which constant features are removed and the remaining features are used for model training. This step ensures that only the most informative features are considered, improving model performance and computational efficiency.\nC.9 Matrix Construction\nFinally, all engineered features are combined into a single feature matrix, along with target labels for classification tasks. This matrix serves as an input for the subsequent model training phase."}, {"title": "D CLASSIFICATION", "content": "The classification framework is trained to predict ground truth values from engineered features derived from text-image paired prompts. The"}]}