{"title": "On the Interplay Between Sparsity and Training in Deep Reinforcement Learning", "authors": ["Fatima Davelouis", "John D. Martin", "Michael Bowling"], "abstract": "We study the benefits of different sparse architectures for deep reinforcement learning. In particular, we focus on image-based domains where spatially-biased and fully-connected architectures are common. Using these and several other architectures of equal capacity, we show that sparse structure has a significant effect on learning performance. We also observe that choosing the best sparse architecture for a given domain depends on whether the hidden layer weights are fixed or learned.", "sections": [{"title": "Introduction", "content": "A fundamental principle of deep learning is that neural networks with many connections can represent more functions than sparse networks with fewer connections. However, this increased representational capacity comes at the cost of more computational resources, both in terms of storage (for network weights) and time (for running a forward pass). While computationally efficient, sparse networks also provide statistical efficiency when their connection assignments (i.e. sparse structure) accord with the dependencies expressed in the data distribution (i.e. dependence structure). This is why convolutional networks are thought to generalize well in settings with natural, spatial imagery (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; LeCun et al., 2015; Szegedy et al., 2015).\nIn deep reinforcement learning, prior knowledge of the environment guides which connections to represent. For instance, convolutional networks are common in domains with spatial dependence structure; these networks exploit the co-variability of nearby pixels by connecting neighboring inputs. Of course an environment's dependence structure is not always known, and in such cases one is assumed, or a fully-connected network is used. Efficiency suffers in the latter case as additional updates are needed to shrink the weights of weakly-related inputs. While these approaches underpin many performant learning systems (Mnih et al., 2015; Silver et al., 2017b; Hessel et al., 2018), they are both far from ideal in the general setting.\nMethods to learn sparse structure have been studied. Graesser et al. (2022) recently benchmarked various sparse-training techniques in reinforcement learning (RL). Consistent with previous studies, they found that an agent's performance depends on sparse structure when the full network is learned end-to-end. Other work from RL reaches a similar result using fixed networks with random weights (Gaier & Ha, 2019; Martin & Modayil, 2021). These findings collectively motivate an investigation of the interplay between sparse structure and the learning strategy. A key question is whether the effects of sparsity are consistent across networks with weights fixed to random values and networks with learned weights.\nOur paper presents two main findings. First, we confirm that when controlling for network capacity, sparse structure has a significant effect on an agent's performance. This result establishes previ-"}, {"title": "Problem Setting", "content": "This work studies deep reinforcement learning problems. The setting is characterized by an agent and environment that interact through an interface of actions A and image-based observations O. At every time-step t\u2208 N, the agent chooses an action at \u2208 A based on its current observation ot \u2208 O. It takes the action then receives the next observation Ot+1 along with a scalar reward rt+1. A history of interaction is denoted h = a101, a202,\u2026, with length-n histories coming from the set Hn = (A \u00d7 O)\", and all finite-length histories from H = U_1Hn. Furthermore, the agent is assumed to observe samples from a distribution\ne: H \u00d7 \u0391 \u2192 \u0394(\u039f \u00d7 R), which conditions on the current history and action; this is the environment. In our setting, it is assumed that the environment allows histories to be repeated in an episodic fashion.\nThe goal is to learn a policy, \u03c0 : \u0397 \u2192 \u0394(A) that maximizes the expected sum of future discounted rewards. For a given discount factor \u03b3\u2208 [0,1), the action-value, q\u33a2(h, a), expresses the utility of taking action a from the history h and following \u03c0 for all time-steps thereafter:\n$q_\\pi(h, a) = E_{\\pi, \\epsilon}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... | H_t = h, A_t = a]$.\nIn many deep RL settings, it is common for the agent to follow an e-greedy policy; this selects uniform-random actions with probability e and otherwise selects actions that maximize the current action-value.\nThe full history requires an unbounded amount of memory to represent, Therefore, the agent maintains a finite internal state s \u2208 S. Following prior work (Dong et al., 2022; Sutton, 2022; Abel et al., 2023), we define the internal state recursively, as st+1 = f(st, at, Ot+1), for all time-steps t, with f : S \u00d7 A \u00d7 O \u2192 S as the state-update function. At any moment, st is assumed to provide sufficient context for the agent's present circumstances in the environment, i.e. st = ht. The terms \"state\" and \"internal state\" are henceforth used interchangeably.\""}, {"title": "RL with a Deep Q-Network", "content": "The Deep Q-Network (DQN) is a method for learning an approximate action-value function represented as a deep neural network (Mnih et al., 2015). Its state-update function f: Rd \u2192 S uses a common set of hidden-layer weights \u03a6 to map input images, represented as d-dimensional vectors, to internal states: f(st,at, Ot+1) = f(ot+1; \u03a6). In our work, f is a neural network with a single hidden layer and ReLU activation functions. Furthermore, action-values are computed with a linear combination of the state and final-layer weights wa, for each a \u2208 A:\n$\\hat{q}(o, a; \\theta) = w_a^T f(o; \\Phi)$.\nThe full collection of parameters is denoted 0 = {\u03a6,wada \u2208 A}. Network parameters are learned by minimizing the following loss, averaged over a minibatch D of experience:\n$L(\\theta) = \\frac{1}{|D|} \\sum_{(o_t, a_t, r_{t+1}, o_{t+1}) \\in D} [(r_{t+1} + \\gamma \\arg \\max_{a' \\in A} \\hat{q}(o_{t+1}, a'; \\theta) - \\hat{q}(o_t, a_t; \\theta))^2]$.\nThe target term, $\\hat{q}(o_{t+1}, a_{t+1}; \\theta)$, is computed with a separate network of identical architecture but different parameters $\\theta$. This design prevents gradients from affecting the update and promotes optimization stability (Asadi et al., 2022). Every few cycles, $\\theta$ is assigned the current values of $\\theta$. In our study, we use Adam (Kingma & Ba, 2014) to optimize network parameters."}, {"title": "Sparse Deep Q-Networks", "content": "Using DQN, our study examines the performance benefits of different sparse structures. We encode sparse structure as a binary matrix M\u2208 {0,1}dxn and apply it by taking an element-wise product with the hidden-layer weights \u03a6\u2208 Rd\u00d7n, denoted M \u03a6. Preactivations are computed by taking the dot-product of the sparse weight matrix and an observation o. For a single-layer architecture, a forward pass is given by\n$\\hat{q}(o, a) = w_a^T f((M\\Phi)o)$. We assume that the sparse structure is fixed throughout learning, so M is not affected by optimization.\nThere are different ways to arrive at the sparse configuration of the binary masks M: some can be hand-crafted based on the designer's domain knowledge, while others can be learned. For instance, one of our base-lines relies on L\u2081-regularization to induce sparsity in an end-to-end manner (Hastie et al., 2009; Hernandez-Garcia & Sutton, 2019; Ma et al., 2019; De & Doostan, 2022). In machine learning, L\u2081-regularization is a commonly used technique in which we add a term to the loss function and weight it by a regularization coefficient \u03b2\u2208 [0,1):\n$L(\\theta) = L(\\theta) + \\beta ||\\theta||_1$\nAs \u03b2 approaches 1, the loss penalizes weights that are non-zero. Although the weights are not guaranteed to reach zero exactly due to finite steps of optimization and floating point approximation, at the end of training we can take sufficiently small weights in \u03a6 and zero out their corresponding entries in the mask M. The rationale is that the smallest weights in \u03a6 likely correspond to those that contribute less in generating useful features."}, {"title": "Related Work", "content": "Sparse Networks in Supervised Learning. Sparse networks are studied extensively in the context of supervised learning. Frankle & Carbin (2018) demonstrate the existence of sparse networks that perform as well as fully-connected networks. Others study methods for obtaining such lottery tickets, as they are known, by pruning connections with small weights and regrowing connections with large gradients (Evci et al., 2020). Sokar et al. (2022) introduce the Dynamic Sparse Training (DST) method, which periodically prunes connections with small weight magnitudes and randomly adds new connections during training. In this setting, network sparsity has also been learned end-to-end by including connection masks as a learnable parameter (Liu et al., 2020). In the time-series classification problem, Xiao et al. (2022) applied DST to the kernels of a convolutional neural network. A later work studied the performance benefits of various pruning techniques on other challenging classification tasks (Xiao et al., 2024); surprisingly, they found that sparse networks can sometimes surpass their dense counterparts. Despite their successful results, none of these works studied the performance of sparse architectures when the final topology is fixed and the weights are learned.\nSparse Networks in Reinforcement Learning. Sparse networks have received comparatively less attention in RL. A line of work has applied DST with various pruning heuristics (Sokar et al., 2022; Graesser et al., 2022; Grooten et al., 2023). Graesser et al., conducted an extensive empirical study that surveyed various techniques for adapting the network connectivity of DQN (Graesser et al., 2022); they established the conditions when sparse networks perform best in Atari and MuJoCo. These works studied the effects of sparse structure while the full network was learned end-to-end. In another line of work, sparse structure was studied using fixed networks of random weights (Gaier & Ha, 2019). Martin & Modayil (2021) adapt the network structure based on predictions of the input observations. Modayil & Abbas (2023) extended this technique to large-scale control settings. However, these works do not explore the performance benefits of sparsity when controlling for the learning process fixing versus learning the hidden layer weights.\nConnection between Representations and the environment. Previous studies dealt with the problem of learning representations with no a priori knowledge of the environment's dependence structure. For instance, there are existing techniques for selecting subsets of sensor readings based on correlations in order to form a low dimensional embedding (Modayil, 2010). More importantly, this work showed that we can"}, {"title": "Empirical Study", "content": "We show supporting evidence for the claims in Section 1, namely that (1) sparse structure has a significant effect on an agent's performance when controlling for network capacity, and (2) the sparse structure that enables the highest performance depends on whether the hidden layer weights are fixed or learned. Comparisons are made measuring the return across time-steps, averaged over thirty independent trials after sweeping over the step-sizes. For complete details of our methodology, please refer to the Appendix."}, {"title": "The MinAtar Environment", "content": "MinAtar is a suite of simplified Atari 2600 video games (Young & Tian, 2019). Similar to the Arcade Learning Environment (Bellemare et al., 2013), MinAtar provides image observations, joystick commands, and game-score rewards. The games most relevant to our study of sparse structure are Breakout and Space-Invaders, as their object dynamics appear to have sparse, spatial relationships.\nBreakout requires an agent to control a paddle and deflect a moving ball into a wall of bricks. A brick is destroyed whenever the ball contacts it, and the goal is to destroy as many bricks as possible. If the ball moves past the paddle, then the game is reset, and the score goes to zero. Observations are images with four channels, showing pixels of the paddle, ball, previous ball location, and bricks.\nOn the other hand, Space-Invaders is comparatively more complex. Here the agent commands a spaceship: controlling its horizontal position and cannon. The cannon fires munitions upward in straight lines. Above the ship is a row of aliens who move side to side and fire their cannons downward. If the ship is hit, the game resets and the score goes to zero. The goal is to shoot as many aliens as possible while avoiding their attacks. Image observations have six channels that display locations of the ship, aliens, and munitions. For more information about both games, see the Appendix and the paper by Young & Tian (2019).\nFor our purposes, these games provide experiential data with spatially-dependent observations. In these domains, we expect spatially-biased architectures, whose topology hard-codes nearest-neighbor relationships, to perform best. In Breakout, for instance, the previous position of the ball is predictive of where it will be at the next time step. Thus, one expects connections from nearby pixels to be relevant, and pixels from distal parts of the image to be extraneous."}, {"title": "Architecture Baselines", "content": "We consider several sparse baseline architectures while controlling for the amount of sparsity and network capacity. Each architecture has the same number of learnable parameters and hidden layer dimensionality. In all the experiments, the degree of sparsity was fixed to 91% relative to a fully-connected architecture. This was controlled by setting the same number of zeroes in each architecture's binary mask. See the Appendix for further details.\nThe first architecture, Random, imposes sparse connections in the hidden layer uniformly at random. Out-performing Random with another sparse architecture suggests that the underlying data distribution imposes non-random, sparse relationships among the observation components.\nWe also consider a spatially-biased baseline, Spatial. This architecture is similar to a convolutional layer in how it forms receptive fields with nearby pixels. However, to control for the effects of sparse structure and control for the number of learnable parameters, Spatial does not impose weight sharing; each kernel contains its own learnable weights.\nA third architecture, Predictive, establishes connections with the Prediction Adapted Networks algorithm (Martin & Modayil, 2021), which uses a measure of temporal relevance to assign its sparse connections. This baseline represents an architecture with non-random and non-spatial structure. Previous work has established its relevance in RL-based domains (Martin & Modayil, 2021; Modayil & Abbas, 2023).\nAnother baseline we consider, L\u2081-Reg, uses L\u2081-regularization to induce a sparse structure. It serves as an example for how an end-to-end algorithm can be used to obtain a sparse hidden layer structure. We controlled for the amount of sparsity in L\u2081-Reg by sweeping over the regularization coefficient until it matched the other architectures. Specifically, a weight is zeroed out if its final value is smaller than the average.\nThe final architecture we consider is fully-connected (Dense), meaning that each input influences all features in the hidden layer. No binary matrix is imposed onto the hidden layer weight matrix; thus this architecture has nearly ten times the number of active hidden layer weights as the sparse architectures.\nRandom, Spatial, and Predictive generate each pre-activation from the same number of inputs: 36 inputs (out of 400) generate each feature in Breakout, and 54 inputs (out of 600) in Space-Invaders. In contrast, the number of inputs used per feature by the L\u2081 baseline can vary, since it tries to maximize sparsity in aggregate, over the entire network. The Appendix provides more architectural details as well as visualizations for each type of sparse neighborhood in each environment."}, {"title": "Case Study: Fixed Hidden Weights and Topology", "content": "This experiment reveals that network sparsity plays an important role in the agent's performance when the hidden layer weights are randomly initialized and then held fixed. Moreover, these results also show that no architecture is strictly better across both environments in this setting. These are surprising results because, in practice, a single architecture is often thought to apply equally well across an entire suite of games, like MinAtar, ALE (Bellemare et al., 2013), Go, chess, shogi (Silver et al., 2017a; Schrittwieser et al., 2020), and Gran-Turismo (Wurman et al., 2022).\nIn this experiment we initialized the hidden layer weights at random and held them fixed-only allowing the final layer to be learned through back-propagation. We expected that the Spatial architecture would perform best, since it is similar to the sparse structure imposed by convolutional layers which are widely used in domains like MinAtar. Furthermore, as discussed in the previous section, these domains have a distinct spatial structure.\nResults from Breakout are shown in Figure 1 (left). Our results suggest that Dense achieves the highest performance. We believe this is due to its larger representational capacity (more hidden layer connections). The DQN architectures that achieved the second highest performance were Predictive and Random, although these do not perform significantly different from Dense. Surprisingly, Spatial yields the lowest performance, nowhere near the aforementioned three baselines. The fact that Predictive and Random perform nearly on par with Dense while using significantly less parameters suggests that there are no statistical benefits to"}, {"title": "Case Study: Learned Hidden Weights and Fixed Topology", "content": "Does hidden layer topology affect the agent's performance when the weights are learned end-to-end? In this case study, we investigate this question by randomly initializing the network and updating the weight magnitudes via back-propagation. We observe that the degree to which performance improves when also learning the weights varies across sparse architectures. This gives evidence for our second claim-sparse structures that enable the highest performance depend on whether we fix or learn the hidden layer weights.\nResults for Breakout are shown in Figure 2 (left). While Spatial remains the least useful in this domain, we see a change from the learning curves in the previous section: now the average return of Predictive is statistically higher than Random. Learning the weight magnitudes almost doubled the performance of Dense and Predictive (from an average return of nearly 5.5 and 5 to 11 and 10 respectively). Meanwhile, Random"}, {"title": "Case Study: Learned Hidden Weights and Topology", "content": "In our final case study, we investigate the performance of L\u2081-Reg, an architecture whose topology is learned end-to-end via L\u2081-regularization. Please refer to the appendix for more details on how this type of network sparsity was generated and how we ensured that it has nearly the same number of active connections as the other baselines.\nWe perform two experiments: we compare the average return of L1-Reg to all other sparse architectures in two scenarios: (1) when the hidden layer weights are randomly initialized and held fixed, and (2) when the latter are learned end-to-end. Due to the greater flexibility that L\u2081-regularization has to mask out weights in a non-uniform fashion throughout the hidden layer, we hypothesize that L\u2081-Reg will yield a higher performance than all the other sparse networks in both environments.\nFigure 1 shows the average returns on Breakout (left) and Space-Invaders (right) for all sparse DQN architectures with fixed hidden layer weights, with L\u2081-Reg shown in yellow. Clearly, our hypothesis is refuted here: on average, L1-Reg performs significantly worse than Random and Predictive in Breakout and on par with Random in Space-Invaders. Our results suggest that L\u2081-sparsity does not benefit the agent when the hidden layer weights are fixed.\nWe also investigate how L1-Reg performs when the hidden layer weights are learned, as shown by the yellow learning curves in Figure 2 for Breakout (left) and Space-Invaders (right). In both environments, we find that L\u2081-Reg performs better than Random and statistically on par with Dense. In this learning regime, there are no benefits to training a fully-connected network, since a network sparsified through L\u2081-regularization can perform just as well.\nWhy do we observe that L1-Reg performs better than Random, Predictive and Spatial when the hidden layer weights are learned? Recall that in order to generate L\u2081-sparsity, we trained a dense network with a regularized loss function. This means that L\u2081-sparsity is optimized for a network that is learned end-to-end through back-propagation. For this reason, L\u2081-Reg yields high performance when the network is trained, which is precisely what Figure 2 shows. In other words, the learning task that generated L\u2081-sparsity is the same task to approximate the action-values in DQN. On the other hand, the method used to arrive at predictive sparsity-the Prediction Adapted Networks algorithm is an auxiliary learning mechanism that generates sparse connections in a way that does not directly optimize DQN's off-policy learning objective."}, {"title": "Conclusion", "content": "When we control for network capacity, our empirical results suggest that network sparsity has a significant effect on an agent's performance, and that the sparse structure that enables the highest performance depends on whether we fix or learn the hidden layer weights. For instance, in our control domains we observed that relative performance among five sparse networks did not remain consistent when the hidden layer weights were fixed versus learned. Interestingly, in domains with presumed spatial dynamics, spatial sparsity is not the most performant.\nFuture work may investigate different behaviour polices in the phase where we generate some of our sparse structures, such as predictive and L\u2081-sparsity. Another possibility is to consider more sparse topologies which are expected to be distinct from those considered. One option may include Dynamic Sparse Training techniques (Grooten et al., 2023; Sokar et al., 2022; Graesser et al., 2022)."}, {"title": "Experimental Details", "content": "To evaluate performance, we compute the average return incurred by each DQN agent over 30 independent trials. More specifically, a trial refers to a random seed used to (1) randomly initialize the DQN weights at the beginning of learning, (2) sample random actions from an e-greedy policy and (3) sample the environment's initial state. In Breakout this amounts to resetting the position of the ball at the start of the game whenever the brick wall is destroyed. On the other hand, in Space-Invaders, the environment is fully deterministic and thus does not depend on a random seed."}, {"title": "Constructing Sparse Architecture Baselines", "content": "None of our baselines use convolutional layers. Therefore, in order to feed the 2D observations into our value network, we first flatten the multi-channel inputs to 1D vectors.\nOur experiment methodology involves two phases. In phase one, we generate sparse hidden layer topologies and encode them as binary mask matrices. Then in phase two, we impose the binary mask onto DQN's hidden layer weights, as described in section 2.2, and train the agent for 5 million time steps. In the case of Spatial, phase one involves hand-crafting spatially-biased sparse connections. In the case of Random, we assign connections uniformly at random. On the other hand, for Predictive and L\u2081-Reg, generating sparsity is more complex as we explain below."}]}