{"title": "A Two-Stage Proactive Dialogue Generator for Efficient Clinical Information Collection Using Large Language Model", "authors": ["Xueshen Li", "Xinlong Hou", "Nirupama Ravi", "Ziyi Huang", "Yu Gan"], "abstract": "Efficient patient-doctor interaction is among the key factors for a successful disease diagnosis. During\nthe conversation, the doctor could query complementary diagnostic information, such as the patient's\nsymptoms, previous surgery, and other related information that goes beyond medical evidence data (test\nresults) to enhance disease diagnosis. However, this procedure is usually time-consuming and less-efficient,\nwhich can be potentially optimized through computer-assisted systems. As such, we propose a diagnostic\ndialogue system to automate the patient information collection procedure. By exploiting medical history\nand conversation logic, our conversation agents, particularly the doctor agent, can pose multi-round clinical\nqueries to effectively collect the most relevant disease diagnostic information. Moreover, benefiting from our\ntwo-stage recommendation structure, carefully designed ranking criteria, and interactive patient agent, our\nmodel is able to overcome the under-exploration and non-flexible challenges in dialogue generation. Our\nexperimental results on a real-world medical conversation dataset show that our model can generate clinical\nqueries that mimic the conversation style of real doctors, with efficient fluency, professionalism, and safety,\nwhile effectively collecting relevant disease diagnostic information.", "sections": [{"title": "Introduction", "content": "Clinical diagnosis is a complex decision-making process that combines evidence-based information collected from multiple resources, including patients' symptoms, previous surgery, medical testing evidence, and other related information such as habits [1, 2]. Existing studies mainly target computer-assisted disease analysis tasks, such as abnormal detection and disease prediction [3, 4, 5, 6, 7], with few studies investigating diagnostic data collection. Currently, the diagnostic-critical information is usually collected manually through patient-doctor/nurse interviews or conventional queries, which is less efficient and time-consuming. Moreover, the extensive questioning required can lead to patient fatigue, increasing the likelihood of inaccuracies in their responses. Automating the medical query process could significantly streamline data collection, enhance the accuracy of information gathered, and improve overall patient experience by reducing the cognitive load on patients during medical interviews. A potential solution to accelerate this process is using questionnaires to guide self-report medical history collection[8, 9, 10]. However, previous studies show that self-administered questionnaires may not be able to provide the same information as clinical interviews, and thus are not recommended for certain diseases [11, 12]. As such, there is a need to develop an intelligent dialogue system to effectively query patient's information to support the diagnostic procedure.\nDespite its practical importance in diagnosis, the development of computer-assisted diagnostic dialogue systems is in the early stages. Directly transferring the solution of question-answering (QA) tasks [13] to diagnostic query generation is challenging. In a typical QA interaction, the model responds to a given query by performing information retrials from its knowledge database. Similar to a search engine [14, 15], it can only passively answer questions, with no query capability to pose questions. By contrast, the diagnostic dialogue system aims to pose questions to effectively collect disease-relevant information, such as symptoms, previous surgery, and other related information, from the patients to enhance the following diagnosis. This requires the model to have the professional knowledge to identify disease features, the logical capability to perform communication, and the incorporation capability to analyze multi-round conversations. Overall, the model should be equipped with reasoning capability to raise diagnostic relevant questions, rather than simply answer a given question.\nNaively formulating the query generation problem as a single query prediction task is not an ideal solution, as it could reduce the flexibility of the query and further hinder the completeness of diagnostic information collection. In natural dialogue, it is possible to have multiple appropriate 'answers' for a given response. This is different from standard classification tasks with one-to-one expected outputs where each input is assigned to a single clear label for prediction. However, limited by the training framework, we could only use one ground truth query in the model optimization. Hence, the flexibility of query generation will be limited due to the lack of exploration designs. Moreover, the prediction from an optimized foundation model is mostly based on its nearest contextual sentences to ensure the logic is coherent. As a result, key diagnostic factors, such as the completeness of disease feature checking and the rationale of disease diagnosis, might be insufficiently performed and considered during the conversation generation. Formulating a recommendation system solution could directly address the above challenges. The importance of key diagnostic factors can be directly enhanced through a carefully designed ranking strategy on query candidate selection. Benefiting from the explore-exploit tradeoff strategy, the model could fully explore the potential mechanism and relevant features for disease diagnosis from the real-world clinical dialogue to improve the completeness of the diagnostic checking.\nIn this paper, we develop a diagnostic system, which consists of a unique two-stage recommendation structure, to proactively collect diagnostic information from patients with the following key features:\n\u2022 We propose a proactive, diagnostic dialogue sys-"}, {"title": "Related Work", "content": "Recent advances in large language models (LLMs), such as PaLM[16], LLaMA[17, 18, 19], GPT family[20, 21], and ChatGLM[22, 23], have pushed the boundaries of natural language processing (NLP) tasks, including text generation, text summarization, and QA. LLMs have demonstrated the potential for computer-assisted diagnosis. Medical LLMs, such as Med-PaLM[24], MEDITRON[25], PMC-LLAMA[26], and BioMedGPT[27], have achieved satisfying scores in the United States Medical Licensing Examination. However, such contribution is limited in disease summarizing given clinical information.\nRecent studies employing large language models (LLMs) [28, 29, 30, 31] for the generation of automatic visual descriptions have shown considerable potential in computer-aided diagnostic applications. Notably, preliminary research has pioneered the use of a medical dialogue system that leverages a foundational model framework, such as ChatGPT[32], to deliver medical advice across three distinct imaging domains [4]. Furthermore, Zhou et al. advanced this field by creating an interactive dermatology diagnostic system [5]. This system was developed by fine-tuning Mini-GPT with an extensive dataset of skin disease images, enabling the generation of detailed reports on various skin conditions. This integration of clinical concepts and physician annotations has significantly enhanced the utility and accuracy of automated diagnostic systems. Although image information is aligned in a pre-trained LLM, those dialogue models still lack the capability of proactive interaction.\nHuatuoGPT[33], Zhongjing[34] have been developed for medical Q&A via conversation interaction. These medical large language models (LLMs) demonstrate strong performance in answering medical questions by leveraging training on medical databases and dialogues. However, they are not specifically optimized for proactively gathering diagnostic information. Moreover, these models employ deterministic generation methods, which lack the adaptability offered by a recommendation system."}, {"title": "Methods", "content": "In this study, we present a proactive LLM-based medical conversation framework to effectively acquire diagnostic-associated information from the patients. Different from classic VQA or QA tasks, our dialogue agents could proactively post questions to effectively collect diagnostic information, rather than passively answering questions. Specifically, our proposed system can professionally retrieve patients' medical histories and collect their health conditions to support follow-up disease diagnosis. This design is motivated by the clinical dialogue between patient and doctor during regular clinical visits.\nThe overall structure of our proposed dialogue system is illustrated in Fig.2 (a). As shown, our proposed proactive dialogue generator consists of three major modules: a doctor agent, a dialogue recommendation system, and a patient agent. The proactive dialogue generator takes the patients' latest interactions as input and generates several disease-relevant queries/answers as response candidates. Based on a novel ranking strategy, the dialogue ranking system selects the most relevant response among the ranked candidates to continue interacting with the patients."}, {"title": "Overall Description", "content": "The development of medical dialogue systems requires a doctor agent with reasoning abilities on disease understanding, diagnosis logic performing, and communication concluding. However, existing LLMs [16, 17, 18, 19, 20, 21, 22, 23, 25, 24, 26, 27] are mainly trained on general text datasets, with limited clinical knowledge in disease diagnosis. In this paper, we finetune an LLM based on a real-world clinical dialogue dataset [35] to confer domain expertise of our model, which is referred to as a doctor agent. This allows the model to have a professional understanding of disease-relevant features, enhancing its reliability on downstream disease diagnostic tasks. In addition, we utilize the medical history generated from [35] in the model finetuning stage (detailed in 3.2) to further strengthen its reasoning and logic capability for query generation. In the ideal case, there should be a real patient who answers the questions from the doctor. However, the real-world dataset only provides fixed answers and follow-up queries from patients for each round of conversation. To reduce the potential inconsistency and logic flaws among multi-round conversations, we developed an interactive patient agent to answer questions and ask follow-up queries from the doctor agent, based on medical history data. In addition to the powerful doctor agent, with the involvement of a patient agent, we can generate a realistic clinical dialogue for LLM training or educational training."}, {"title": "Proactive Dialogue Generator", "content": "In the proactive dialogue generator module, we fine-tune a doctor agent to proactively generate disease-relevant query candidates. We propose to start from a pre-trained LLM on general textural information to fully utilize its reasoning and language abilities and further fine-tune it through the real-world medical dialogue dataset along with medical histories to expand its professional knowledge of disease understanding. \nIn Fig. 2 (b), we present the finetuning diagram of the proposed doctor agent. We only fine-tune the doctor agent for query generation with a fixed patient agent, to ensure a quick and reliable model conver-"}, {"title": "Dialogue Recommendation System", "content": "We propose to design a dialogue recommendation system to perform query generation and selection. Compared with a single end-to-end doctor agent, the proposed dialogue recommendation system can perform a more calibrated query selection and finalization. As shown in Fig. 2 (a), our recommendation system consists of two stages: query candidate generation and candidate ranking. This design allows us to fully explore the candidate query space and select the most relevant queries as the current response. Our candidate ranking algorithm pseudo code is shown in Algorithm 1.\nQuery Candidate Generation. We let the patient agent initial the start of the dialogue. Based on patients' input, we let the doctor agent generate N queries as candidates. Then the patient agent generates a response for each dialogue ni in the queue, where i stands for the index of the dialogue. After that, we generate another N candidate and select the optimal candidate based on the ranking score (detailed below). These steps are repeated until the patient agent stops to generate text.\nCandidates Ranking. To calculate the ranking score, we use a pre-trained LLM to evaluate the quality of the candidates. The LLM is prompted to provide ranking scores within a range of 1 to 10 for multiple aspects. In this paper, we consider the correctness of logic and relevance to medical history, as the two aspects to evaluate the quality of the response generated by the doctor agent. The combination of the two scores is considered to be the final ranking score, with 20 indicating the best quality and 0 indicating the worst quality. Note that our proposed ranking strategy overcomes the black box challenge in general LLM. During the process of candidate ranking, the potential candidates are listed and selected based on explicitly defined criteria, making our proposed solution transparent and explainable."}, {"title": "The Interactive Patient Agent", "content": "Using the candidate ranking strategy, our doctor agent will search for the optimal answer/query based on previous conversation records. We further design a patient agent to provide appropriate responses to the queries posed by the doctor agent. Empowered by LLMs, our patient agent uses the patient's medical history to avoid inconsistency and logical errors between the current and future rounds of the conversation. Based on the medical history, the interactive patient agent will answer the doctor agent's questions or generate new queries that are related to the health conditions. In real-world settings, patients may have diverse backgrounds. As such, we investigate two types of patient agents, one directly using a pre-trained LLM while another fine-tuned by the real-world dialogue dataset [35]. The latter is used to mimic the scenario where patients have basic clinical knowledge for their current visits. The prompts used for the interactive patient agent are shown in Fig. 3. Thus, we set our proposed proactive dialogue generator as a combination of finetuning, candidate ranking, and interactive patient agent. Using finetuning, our framework is familiarized with the medical history and style of clinical conversation between a doctor and a patient. With the candidate ranking strategy, the doctor agent generates the optimal query/answer according to the statement from the patient agent. Using the interactive patient agent, we aim to mitigate the inconsistency and logic flaws among multi-round conversations."}, {"title": "Results", "content": "Experimental settings"}, {"title": "Experimental dataset", "content": "We conduct extensive experiments to validate our model in the real-world medical conversation dataset [35]. The real-world dataset contains multi-round 1,120 doctor-patient dialogues from online consultation medical dialogues, with an official split of 800 for training, 160 for validation, and 160 for testing. We use the official training set to finetune the query generator and the test set to generate queries. For each dialogue, there is a set of labels that serves as diagnostic information. The diagnostic information is formulated by three sections, which are category, items, and status. For the category, there are four subclasses which are symptoms, surgery, test, and other information. The detailed descriptions of the contents in the category are provided in the item section. The status section contains the doctor's diagnosis and patients' self-reporting labels, either positive or negative, for each item in the corresponding category. Based on the diagnostic information, we use ChatGPT-3.5 to generate medical history for each patient."}, {"title": "Evaluation metrics", "content": "We use Bilingual Evaluation Understudy (BLEU) [36] and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) [37] scores to evaluate the performance of dialogue generation using different models. Also, we evaluate high-level metrics of the generated dialogues from the perspectives of Fluency, Professionalism, and Safety. We follow the definitions of the three high-level metrics in [34]. The high-level metrics are calculated using ChatGPT-3.5. Also, we evaluate the performance of our method in the downstream task of extracting diagnostic information. For the real-world and generated medical dialogues, we extract the diagnostic information using ChatGPT-3.5. We calculate the F1 scores of the extracted diagnostic information."}, {"title": "Implementation details", "content": "In this paper, we finetune a Llama-3-8B-Instruct [19] as the doctor agent, based on which we perform candidate ranking. We finetuned the doctor agent on the real-world dataset for 50 epochs. During the candidate ranking, we use Llama-3-8B-Instruct to generate ranking scores based on the correctness of logic and relevance to medical history. For the patient agent, we use another Llama-3-8B-Instruct to serve as the patient and generate responses according to the doctor agent's query. The fine-tuning, candidate ranking, and patient agent are running on a single Nvidia A6000 GPU. We use official implementation and model weights for HuatuoGPT [33] and Zhongjing [34]."}, {"title": "Language styles of LLMs", "content": "In Table 1, we evaluate the generated responses from the proactive dialogue generator with state-of-the-art dialogue generators HuatuoGPT [33], Zhongjing [34], and Llama3 (Meta AI 2024) [19]. Additionally, we report the ablation results with different experimental settings. As shown, our model outperforms baselines in all evaluation metrics. We observe that existing medical Q&A LLMs [33] [34] suffer from low BLEU and ROUGE scores. A possible reason is that these models are mainly designed to generate a response, rather than proactively collecting diagnostic information through posing multi-round queries. These results indicate that our framework could effectively mimic the pattern of real-clinic interactions, where healthcare professionals proactively ask clinical questions to collect comprehensive diagnostic information from patients. Moreover, in our ablation study, we observe an improved overall performance when conducting dialogues with the fine-tuned patient agent. The fine-tuning of the patient agent grants it clinical knowledge of the disease diagnosis, making it respond more professionally to disease-related queries. This aligns well with our intuition and knowledge. That is, the outcome of the diagnosis will likely be improved with effective patient-doctor interactions."}, {"title": "Retrieval of diagnostic information", "content": "We further evaluate the quality of retrieved diagnostic information. Following the definition in [35], we measure the F1 scores of the extracted diagnostic information from the aspects of category, items, and status. The diagnostic information is retrieved using ChatGPT-3.5. The prompts to retrieve diagnostic information from different dialogues are shown in Figure 3. Similar to experiments on language style, we compare with Llama-3 [19], HuatuoGPT [33] and Zhongjing [34]. Since the above baselines do not have a patient agent, we report the results of the proposed model with and without the patient agent to present a fair comparison. Benefiting from the improved dialogue quality, the proposed method achieves the best F1 score for categories and items. In addition to the results reported in Table 2, our proposed method also demonstrates the potential to serve as an effective input for retrieving diagnostic information at a similar level compared to the real-world dataset, which has an F1 score of 0.836 in Status."}, {"title": "Ablation study on quality of responses", "content": "Inspired by [34], we use ChatGPT-3.5 to evaluate the language quality of the generate dialogue using the following evaluation metrics: Fluency, Professionalism, and Safety of the generated dialogues. To avoid the dilemma of using ChatGPT to evaluate ChatGPT-generated data (e.g., HuatuoGPT and Zhongjing), this section is limited to an ablation study on the efficiency of proposed components. The results are reported in Table 3. Benefiting from our proposed candidate ranking strategy, all evaluation metrics are increased by at least 80%. These results confirm that our candidate ranking strategy is efficient and could significantly improve the quality of the generated dialogues. Also, the results show that the interactive patient agent can improve the fluency of the generated dialogue, which reflects the purpose of designing the interactive mode to reduce the logical flaws among multi-round conversations. Besides, we argue that doctor agent strategy alone is not sufficient to generate the optimal candidate. As pointed out by [38], the autoregressive mechanism of LLM for generating text confines the candidate decisions by its token-level decision and left-to-right fashion. Thus, our results indicate the importance of the proposed candidate ranking strategy, which overcomes the limitation of the autoregressive mechanism by searching and ranking among a larger pool of candidates."}, {"title": "Demonstration of proactive query generation", "content": "In Figure 4, we demonstrate four representative cases of single-round dialogue between the patient agent and doctoral agents. Note that all the examples shown in the figure are in the first round of conversation, where two variations of our method (doctor agent+candidate ranking) and (doctor agent+candidate ranking+patient agent) generate the same response. In case (a), our doctor agent+candidate ranking model actively asks for the discomfort (symptoms) of the patient, which is in accord with the real-world dialogue. In case (b), the doctor agent+ranking model further asks for more details about the irregular heartbeat (symptoms) and previous medical testing records (medical evidence data). In case (c), the doctor agent+ranking model questions the symptoms of the patient. In case (d), the doctor agent+ranking model refers to the cardio ultrasound testing (medical testing records). In contrast, the doctor agent model turns to directly answering the query of the patient agent by explaining medical terminologies. The demonstrations indicate that the candidate ranking strategy further boosts the capability of LLMs to proactively ask for and collect diagnostic information as proactive dialogue generator."}, {"title": "Discussion", "content": "The experimental results confirm that the proposed doctor agent, candidate ranking strategy, and patient agent provide clinical dialogues that mimic the clinical conversation. Moreover, our proposed model does not adopt any prior knowledge/assumption of any disease or language. As such, it is generic for different types of diseases and languages. Benefiting from the doctor agent and candidate ranking strategies, the proposed methods achieve higher BLEU and ROUGE scores, as well as high-level metrics such as Fluency, Professionalism, and Safety. Moreover, the interactive patient agent improves the high-level scores among multi-round conversations. Although these high-level metrics are not a part of the ranking criteria, the improved high-level scores demonstrate the effectiveness and robustness of the candidate ranking strategy. Also, we confirm that the generated dialogues by the proactive dialogue generator can provide comprehensive diagnostic information compared to the real-world dataset. It is worth mentioning that in real clinical practice, there should be a patient who addresses the queries from the doctor's agent. Limited by the availability of the dataset and patients' involvement, we developed the interactive patient agent to answer queries from the doctor agent and ask follow-up questions. As suggested by the experimental results, the interactive patient agent reduces the inconsistency and flaws in logic in the multi-round conversations. Also, We observe a further increase in fluency, professionalism, and safety scores, after the patient agent is finetuned.\nThe outcomes of our framework have multiple applications. For example, the generated dialogue can be employed to train natural language models for extracting symptoms and diagnosing diseases. The doctor agent can be used to generate query to patient and extracting diagnostic information in clinics. Additionally, it can be used for educational purposes, such as training avatars to interact with healthcare professional students in the role of standardized patients during clinical examinations. Also, the patient agent can be used for training purposes for clinical professionals to practice clinical interactions.\nBeyond the proposed framework, we believe that more agents can be potentially incorporated into the framework. For example, nursing agents, if fine-tuned in different domains of medicine, can be added to our system to provide detailed suggestions which is tailored by the patients' needs. Also, a supervision agent, focusing on the correctness of the doctor agent's output, can be added to the framework to further improve the quality of the generated dialogues. With incremental data, we can also train a model for specialist, such as cardiologist, neurologist, etc., in clinical diagnosis."}, {"title": "Conclusion", "content": "In this paper, we develop a diagnostic system to proactively collect diagnostic information via interactive conversations between doctor and patient agents. Using the proposed doctor agent, two-stage recommendation structure, and interactive patient agent, we perform comprehensive experiments on a real-world medical dialogue dataset. The BLEU and ROUGE scores show that the proposed method, after fine-tuning and candidate ranking, better mimics the conversation style in real-world dialogue. Moreover, the proposed framework achieves better performance in terms of high-level metrics including Fluency, Professionalism, and Safety. The generated dialogues are capable of providing diagnostic information. In the future, we will conduct a human evaluation to further evaluate our model in real-world settings. Specifically, we will invite patients and physicians to evaluate the performance of the proposed algorithms from multiple aspects, including friendliness, efficiency, and accuracy, over the conversation."}]}