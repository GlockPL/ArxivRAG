{"title": "Enhancing Table Representations with LLM-powered\nSynthetic Data Generation", "authors": ["Dayu Yang", "Natawut Monaikul", "Amanda Ding", "Bozhao Tan", "Kishore Mosaliganti", "Giri Iyengar"], "abstract": "In the era of data-driven decision-making, accurate table-level representations\nand efficient table recommendation systems are becoming increasingly crucial\nfor improving table management, discovery, and analysis. However, existing\napproaches to tabular data representation often face limitations, primarily due\nto their focus on cell-level tasks and the lack of high-quality training data. To\naddress these challenges, we first formulate a clear definition of table similarity in\nthe context of data transformation activities within data-driven enterprises. This\ndefinition serves as the foundation for synthetic data generation, which require a\nwell-defined data generation process. Building on this, we propose a novel synthetic\ndata generation pipeline that harnesses the code generation and data manipulation\ncapabilities of Large Language Models (LLMs) to create a large-scale synthetic\ndataset tailored for table-level representation learning. Through manual validation\nand performance comparisons on the table recommendation task, we demonstrate\nthat the synthetic data generated by our pipeline aligns with our proposed definition\nof table similarity and significantly enhances table representations, leading to\nimproved recommendation performance.", "sections": [{"title": "Introduction", "content": "In an era where data-driven decision-making is increasingly central to business operations, the\nability to efficiently and accurately recommend similar tables across vast datasets has become\nessential [Zhang and Balog, 2018, Habibi et al., 2020, Trabelsi et al., 2022]. As tables remain a\ndominant modality in the data landscape, this capability can greatly enhance both table management\nand discovery processes [Habibi et al., 2020, Deng et al., 2022]. By recommending similar tables,\norganizations can streamline table management through effective deduplication, precise lineage\nprediction, and robust clustering or labeling [Yin et al., 2020, Iida et al., 2021]. These processes\ncontribute to maintaining clean, organized, and well-documented data repositories, which can lead\nto significant cost savings in cloud services [Mahesh et al., 2020]. Furthermore, similar table\nrecommendations play a crucial role in discovery and usage, suggesting complementary tables that"}, {"title": "Related Studies", "content": "offer additional insights [Zhang and Balog, 2018]. This enables data analysts to make more informed\ndecisions and better monitor ongoing processes.\nAlthough similar table recommendation is a crucial task, many existing table similarity meth-\nods [Zhang and Balog, 2018, Habibi et al., 2020, Trabelsi et al., 2022] often lack a clear and\nconsistent definition of \"similarity.\" This ambiguity can make it challenging to apply these methods\nto different use cases, as users may be unsure whether their understanding of similarity aligns with\nthe definitions used in these approaches.\nA single table can contain a large amount of cell data and have a complex structure, making the\nmanual annotation of similar tables a costly process. This contributes to the scarcity of high-quality\ntraining data. Facing the issue, some studies [Yin et al., 2020, Iida et al., 2021, Chen et al., 2023] have\nattempted to address the challenge by developing table representations through unsupervised cell-level\ntasks, such as masked cell value restoration. However, these representations tend to struggle with\ncapturing the global structure of a table, which may result in suboptimal performance in table-level\ntasks, including similar table recommendation, as suggested by our experimental results. Another\nline of research has sought to alleviate the data sparsity challenge [Habibi et al., 2020, Trabelsi et al.,\n2022] by reframing the table recommendation problem as a pairwise table matching task, rather than\nfocusing on pointwise table representation. This approach is generally easier to solve and requires\nless supervision [Melnikov et al., 2016, Yates et al., 2021]. However, while pairwise tasks may\nmitigate some data limitations, they introduce significant computational inefficiencies, shifting the\ntime complexity from linear to quadratic [Melnikov et al., 2016, Zhuang et al., 2024], which can make\nthem impractical for large-scale tabular databases typically found in modern big data environments.\nTo address the limitations of existing methods in table similarity recommendation, we propose a\nstructured approach that begins with a clear definition of table similarity grounded in real-world\nuse cases from data-driven industries. Building upon this foundation, we develop a synthetic table\nsimilarity data generation pipeline leveraging Large Language Models (LLMs). By providing any\nnon-annotated tables as a base, this pipeline can generate large-scale, high-quality table similarity\ndata for enhancing general table representation model and recommendation performance evaluation.\nWe evaluated the quality of our generated synthetic dataset through three approaches: (i) conducting\nhuman validation on a subset of the dataset, confirming its accuracy with respect to our defined\nnotion of similarity; (ii) comparing cosine similarities of embeddings from similar tables in our\ndataset against those from an existing dataset, demonstrating its enhanced potential for developing\ntable-level representations; and (iii) improving table representations using our synthetic dataset for\nthe task of similar table matching, outperforming state-of-the-art embedding models in our out-of-\ndistribution proprietary dataset collected from Capital One internally. These results indicate that our\nmethod can generate high-quality synthetic training data and contribute to real-world similar table\nrecommendation applications."}, {"title": "2.1 Textual Representation Learning", "content": "Text embeddings \u2013 vector representations of natural language that capture semantic content \u2013 have\nbeen widely utilized in various natural language processing (NLP) tasks, including question answer-\ning [Choi et al., 2018, Allam and Haggag, 2012], conversational search [Yang et al., 2023a,b], and\nsemantic textual similarity [Muennighoff et al., 2022]. Sentence-BERT [Reimers and Gurevych,\n2019] is one of the earliest and most popular methods for learning text embeddings by fine-tuning\nBERT [Devlin, 2018] on natural language inference datasets.\nTo further improve the performance and robustness of text embeddings, state-of-the-art methods like\nGTE-series [Li et al., 2023] and BGE-series [Xiao et al., 2023] adopt a more complex multi-stage\ntraining approach. These methods first pre-train on billions of weakly supervised text pairs and then\nfine-tune on several high-quality labeled datasets.\nRecently, with the growing recognition of the strong language understanding capabilities of large\nlanguage models (LLMs), there has been a shift towards using LLMs, particularly those based on\ndecoder Transformers, to generate text embeddings [Lee et al., 2024a, BehnamGhader et al., 2024]. In\naddition to changes in model structure, recent state-of-the-art text representation models have begun\nleveraging larger LLMs to generate synthetic data, providing high-quality supervision, combined"}, {"title": "2.2 Tabular Representation Learning", "content": "Inspired by the success of BERT [Devlin, 2018] in constructing general text representations and\nachieving strong performance across various downstream natural language processing tasks, many\nstudies in table representation [Yin et al., 2020, Iida et al., 2021, Chen et al., 2023] have adopted\na similar training approach \u2013 they aim to build general table representations using masked self-\nsupervised tasks. Specifically, they modify the vanilla Transformer encoder [Vaswani et al., 2017]\nby introducing additional column or row attention mechanisms to better adapt the model to learn\nthe spatial structure of a 2D table. In these works, a large unannotated tabular dataset is used for\npre-training, where approximately 15% of the cells are masked, and the modified Transformer is\ntrained to predict these masked cells.\nRecognizing the success of LLMs such as Llama [Touvron et al., 2023] and Gemma [Team et al.,\n2024] in natural language tasks, some studies have explored their potential applications in tabular data\ntasks, such as tabular data cleaning, cell value lookup, and tabular data classification [Borisov et al.,\n2022]. To apply LLMs directly to tabular data, the table must first be serialized into a natural text\nformat. However, it remains an open question as to which format \u2013 CSV, JSON, XML, Markdown,\nor HTML [Sui et al., 2024].\nOther methods have been developed to directly address the task of estimating table similarity or\nperforming table retrieval [Habibi et al., 2020, Trabelsi et al., 2022]. These methods take a pair of\ntables simultaneously, giving a pairwise representation. However, we seek a pointwise representation\na unique table-level embedding \u2013 that can allow for more efficient table retrieval as well as be used\nfor other downstream tasks involving tabular data."}, {"title": "2.3 Data for Table Similarity Estimation", "content": "Currently, only a few datasets for table similarity have been proposed in the literature. One such\ndataset uses tables from PubMed Central (PMC) Open Access \u2013 pairs of tables were manually\nannotated as similar or dissimilar according to the estimated percentage of similarity or dissimilarity\namong the cell data and captions in the tables [Habibi et al., 2020, Trabelsi et al., 2022]. These tables\nwere drawn from a predominantly biomedical and scientific domain. Another more domain-general\ndataset draws tables from Wikipedia, and table pairs were manually annotated for equivalence or\nsubset relationships based on matching column names [Zhang and Balog, 2018, Fetahu et al., 2019,\nHabibi et al., 2020]. While these datasets are the most relevant to our work, the annotation guidelines\nin both reflect limited definitions of similarity.\nOther datasets for table similarity have been adapted from datasets for table retrieval \u2013 these datasets,\nsuch as WikiTables [Bhagavatula et al., 2015, Zhang and Balog, 2018] and TableArXiv [Gao and\nCallan, 2017], were originally developed for retrieving relevant tables given a natural-language query.\nStudies on table similarity then define a pair of tables as similar if both have been labeled as relevant\nto the same query [Habibi et al., 2020, Trabelsi et al., 2022]. While this definition is useful for\ngrouping tables around shared keywords in a query, relevance judgments are not between the tables\nthemselves, so the sense of table similarity is less targeted.\nWe also note that in the performance evaluation of StruBERT [Trabelsi et al., 2022], a state-of-the-art\nmodel for predicting if a pair of tables is similar, we found a sort of label leakage in the test set.\nAlthough the test set does not contain pairs also present in the training set, the individual tables\noverlap between the sets. Because of this, about 74% of the pairs in the test set could be inferred\ntransitively from relationships in the training set \u2013 for example, if the model learns that tables A and\nB are similar, and B and C are similar from the training set, it could infer that A and C are similar in\nthe test set.\nWe removed these label-leakage pairs from both the training and test sets and trained the StruBERT\nmodel, as well as a vanilla BERT (base) model using a na\u00efve table serialization scheme, on the new\ntraining set. We found the adjusted performance of StruBERT that we calculated on the new test\nset to have an accuracy of 0.874, compared to 0.9942 reported on the original test set. This ends"}, {"title": "Definition of Similarity", "content": "up underperforming compared to the vanilla BERT model (an accuracy of 0.908). Furthermore, we\nfound the average latency incurred by StruBERT performing inference on each sample to be around\nfive times longer than BERT due to the added complexity of the model.\nTaken together, our literature review demonstrates the need for a large-scale, domain-general dataset\nof table pairs that are labeled according to a practical definition of similarity to enable tabular\nrepresentation learning and to efficiently perform and fairly evaluate the task of similar table retrieval\nin data-driven industries.\nIn our work, we define \"similarity\u201d based on two key use cases of table matching systems in industry:\ntable management and complementary information retrieval. In industry, table management systems\nare often designed to identify duplicate tables and those with close lineage. While exact duplicates\ncan be easily identified through hard-coded rule matching, finding tables with close lineage is more\nchallenging. This is because data analysts often modify or transform elements of parent tables,\nrequiring the model to understand the underlying semantic connections among tables.\nAnother critical use case in table recommendation is the retrieval of complementary information. In\nthis context, the goal is to identify tables that, while not identical, offer additional insights or relevant\ndata that can enhance the analysis. This requires the model to recognize nuanced relationships\nbetween tables, such as shared themes, overlapping data points, or related metadata.\nTaking these two aspects into account, we define two tables to be similar if one is the result of the\nother's having undergone one or more data transformations typically performed by a data analyst.\nWe elaborate further on these types of operations in the next section. This definition captures the\nsemantic connections and transformations that occur in real-world data management, ideally resulting\nin table-level representations that allow for effective identification and recommendation of other\ncontextually-related tables."}, {"title": "Synthetic Data Generation Pipeline", "content": "We now introduce our LLM-assisted pipeline for generating tabular data from a given input table\nsuch that the generated table is considered similar to the original table, i.e., the generated table can be\nobtained from some series of transformations on the anchor table.\nWe first assume we are given a table, which we call an anchor table, from which our goal is to\ngenerate similar tables in a way that (1) mimics the behavior of human analysts, (2) encompasses a\ndiverse range of data operations, (3) is efficient. We also assume that this anchor table minimally\ncontains a title, column names, some cell data, and some sort of description that briefly summarizes\nthe contents and purpose of the table.\nWe then perform one or more tabular data operations on the anchor table to generate a new table that\nis similar to the anchor table. We manually constructed a concise list of possible table transformations\ninformed by a study of an industry documentation on data management for data analysts:\n\u2022 Concatenation: Add one or more new columns with new and relevant information.\n\u2022 Edit: Create a new column based entirely on an existing column, using string operations\nlike regular expressions, information extraction, or information refinement.\n\u2022 Reordering: Shuffle the order of columns.\n\u2022 Calculation: Generate a calculated column based on an existing numerical column.\n\u2022 Removal: Remove one or more columns.\n\u2022 Update: Modify the title, description, and column names with respect to any new values, or\nsimply to re-word.\nAn example of these operations performed on a sample table is given in Appendix A.3. We believe\nthese operations cover three major aspects of what a data analyst typically performs on the job:\nadding information (Concatenation), deleting information (Removal), and modifying or synthesizing\ninformation (Edit, Reordering, Calculation, Update). The reordering and update operations also help"}, {"title": "Evaluation", "content": "to ensure the generated tables can promote robust tabular embeddings in terms of order invariance\nand semantic representation.\nWhereas the reordering and removal operations can be performed programmatically, we call on an\nLLM to perform the remaining, more complex operations. After choosing a set of table transforma-\ntions to perform on the anchor table, each transformation is applied sequentially (i.e., to the output\nof the preceding transformation) \u2013 we found that if an LLM is given multiple operations in a single\nprompt, the LLM frequently fails to apply all of the operations.\nWe do not include any row operations because, compared to columns, each row represents an\nobservation, and the number of observations typically exceeds the number of columns (or variables).\nWhen tables are serialized for input into a language model, only a few rows are typically sampled [Yin\net al., 2020, Trabelsi et al., 2022, Sui et al., 2024]. Therefore, we eschew spending significant\ncomputation time on row operations.\nThe output of this pipeline is then multiple tables that can be considered similar to the anchor table,\nwhere each output table is the result of some set of tabular transformations applied to the anchor table\nthat mirror those that a data analyst would apply to tabular data in practice. Then, given a sizable\npool of anchor tables, we can generate a large-scale synthetic dataset of pairs of similar tables that\ncan then be used to train and evaluate embedding models and models for downstream tasks involving\ntabular data."}, {"title": "5.1 Manual Validation", "content": "To empirically verify the validity and utility of our pipeline, we evaluate the output of the pipeline on\na standard set of anchor tables via manual inspection, an investigation of similarity with respect to\nembeddings, and a performance comparison on the downstream task of table retrieval.\nIn our pipeline implementation, we use the Llama3.1-8B-Instruct model as the LLM for per-\nforming tabular transformations. For each anchor table, two random sets of operations are chosen to\nperform on the table, generating two similar tables per anchor. Practical considerations for the order\nof operations are detailed in Appendix A.3.\nWe drew anchor tables from the WikiTables dataset [Bhagavatula et al., 2015], which we chose in\nconsideration of its size (approximately 1.6 million tables) and broad coverage of diverse topics. Each\ntable in the dataset contains a title, column names, and cell data \u2013 because our pipeline also expects a\ndescription for each table, we additionally prompt the same LLM to generate a brief description for\nabout 700,000 randomly-chosen tables.\nThe exact prompts we designed to have the LLM perform each operation is given in Appendix A.3.\nEach table in the dataset is represented in a JSON format, and we serialize these tables for the LLM\nby directly taking the string representation of the JSON table \u2013 details are given in Appendix A.1.\nWe prompt the LLM to also produce JSON-formatted tables, although the output was not always\nin a valid JSON format, so we only save pairs for which the outputs could be properly parsed. The\ngenerated dataset output from our pipeline ultimately contained 140,000 pairs of tables (70,000\nanchor tables with 2 synthetic similar tables each)."}, {"title": "5.2 Embedding-Based Similarity Validation", "content": "An immediate limitation of our pipeline is in the prompt engineering required to ensure the LLM\nproduces sensible tables that truly reflect the instructions in the prompt. While it is possible to use\nthe LLM itself to generate a list of operations or sufficient prompts to instruct itself to product new\ntables, similar to other studies in LLM-based synthetic data generation [Wang et al., 2024, Lee et al.,\n2024b], this approach is inefficient because the list of operations generated by the LLM can be long\nand contain significant overlap. Given that we apply operations sequentially, this would entail calling\non the LLM many more times, which would incur a significant computational cost.\nTo verify that our LLM prompting generated reasonable tables, we randomly selected a subsample of\nthe generated dataset for a round of manual validation. Two professional data analysts inspected each\npair of tables (an anchor table and its corresponding generated table) to determine if the operations"}, {"title": "5.3 Downstream Validation", "content": "that were performed by the LLM were indeed correct. Though multiple operations could be performed\non an anchor table, the annotators only saw the anchor table and the final generated table; thus, the\nannotators were asked to label simply whether or not all of the listed operations had been performed\ncorrectly.\nWe randomly selected 80 table pairs for manual review. The data analysts independently labeled\nthese samples, after which disagreements were discussed to arrive at a final label. We measured\nthe inter-annotator agreement using Cohen's kappa [Cohen, 1960] on the initial labels and found\nmoderately strong agreement between the annotators (k = 0.56) [Landis and Koch, 1977]. Our\nmanual review found that 65% of the synthetic tables were generated correctly given their respective\nsets of operations, with a majority of the incorrect tables being due to the edit operation. These results\nsuggest that our proposed pipeline is able to output tables that reflect typical tabular transformations in\ndata-driven industries, but that further prompt engineering may be required to improve the pipeline's\nabilities on more complex transformations.\nNext, we investigate the potential for our generated dataset to be used to build a robust tabular\nrepresentation model. As is typical for representation learning, we envision an embedding space for\ntabular data in which the vector representations of similar tables are close to each other. Specifically,\nthe similarity of two tables $T_1$ and $T_2$ should be a function of the similarity of their embeddings $E_1$\nand $E_2$ given some embedding model.\nWe compared the cosine similarities of embeddings of pairs of similar tables in our generated dataset\nagainst those pairs of similar tables in the dataset used to train and evaluate StruBERT [Trabelsi\net al., 2022], which is also based on the WikiTables dataset. We seek to show that with our targeted\ndefinition of similarity and our synthetic data generation pipeline developed around that definition,\nthe pairs of tables in our generated dataset exhibit relatively high similarity scores.\nFor a fair comparison, we do not train an embedding model on our generated dataset; rather, we use a\nstate-of-the-art text embedding model \u2013 bge-large-en-v1.5 \u2013 to embed the tables, from which we\ncan compute cosine similarities of pairs of tables:\ncosine similarity($T_1$, $T_2$) = $\\frac{E_1 \\cdot E_2}{\\| E_1 \\|\\| E_2 \\|}$\nWe serialized a table by simply concatenating its title, columns names, and the cell data of a randomly-\nselected row, separated by periods and commas (between cells). The row to include was selected\nrandomly so that the generated table pairs are not given an advantage when the first row of both tables\nare mostly the same. Descriptions were not included since the original WikiTables dataset does not\ncontain them."}, {"title": "5.3.1 Baseline", "content": "Finally, we evaluate the quality of our generated dataset by its utility for a downstream task \u2013\nspecifically, similar table retrieval. An embedding model fine-tuned on a high-quality dataset of\npairs of similar tables should be able to perform better on a similar table retrieval task than other\nembedding models, since the fine-tuned model will have learned better representations of tabular data\nfor determining similarity.\nIn the similar table retrieval task, given a query table $T_q$, the objective is to find similar tables from a\ncorpus of tables $C = \\{T_1, T_2, ..., T_k \\}$, where k is the total number of tables. A typical solution for\nthis task would be to use an embedding model to take $T_q$ as input and output an embedding $E_q$ of $T_q$.\nThe cosine similarity can then be computed between $E_q$ and the embeddings of all other tables; those\nwith the highest similarity scores would be considered candidate similar tables.\nWe first evaluate state-of-the-art embedding models on this task directly to establish a baseline\nof pre-trained model performance. We compare three models: bge-large-en-v1.5 (Short as\nBGE), E5-mistral-7b-instruct [Wang et al., 2024] (a more powerful representation model with\nabout 24x more parameters than bge-large-en-v1.5), and TABBIE [Iida et al., 2021] (a general table\nrepresentation model). For the two text embedding models, we again serialize tables as described in\nAppendix A.1. For TABBIE, we use the [CLS] embedding at the (0, 0) position as the representation\nfor the entire table, as the original authors had also done in their experiments. Since TABBIE does\nnot take into account a table's title or description, to make a fairer comparison, we also embed each\ntable's concatenated title and description using bge-large-en-v1.5 and compute similarity as a\nweighted average between the similarity of the text embeddings and the similarity of the TABBIE\nembeddings. We refer to this method as TABBIE-plus.\nAs metrics, we compute two standard measures of retrieval and ranking performance: recall and\nnDCG. In addition to computing these metrics in the top ten returned results, since each anchor table\nhas exactly two similar tables in the pool, we also compute recall and nDCG in only the top two\nreturned results. These results are given in Table 1. We see that the BGE model outperforms the other\nmodels, despite being smaller and not originally intended for tabular data. Therefore, we chose BGE\nmodel as the base model that further implemented for fine-tuning on our synthetic data."}, {"title": "5.3.2 Fine-Tuning", "content": "With this baseline, we seek to show that fine-tuning the BGE model on our synthetic dataset will\nfurther refine its representations of tabular data such that it will not only perform even better on this\ntest set, but also be able to generalize to a completely different test set of close-lineage tables for an\nindustry use case.\nFor training, we set the batch size to 4. Although contrastive learning typically benefits from larger\nbatch sizes, we limited the batch size to 4 to fit within the memory constraints of a single GPU. The\nmaximum serialized length for all input tables was set to 512 tokens, truncating any table exceeding\nthis length. For each sample, we used 15 hard negatives in addition to the in-batch negatives.\nIn addition to evaluating this fine-tuned model on the test set from our generated dataset, we also\nmeasure its performance on a proprietary dataset from our industry. This dataset contains proprietary\ntabular data that have been labeled for close-lineage relationships, i.e., if one table was produced from\nanother table through some data transformation, making it suitable for evaluating the fine-tuned model\non similar table retrieval. While the Wikipedia tables are more general and easily understandable, the\ntables in this proprietary dataset are more specialized, containing domain-specific terminology. As a\nresult, this dataset is considered out-of-distribution for the model, making it a crucial benchmark for\nassessing the model's value in real-world applications.\nThis dataset consists of about 8,000 tables, from which we sample 1,000 to perform our evaluation.\nThe task then is for the model to generate representations that allow the similar (close-lineage) tables\nto rank as highly as possible among the pool of 8,000 tables for each of the tables in the evaluation.\nWe also note that each table contains a title, description, and column names, but no cell data for\nprivacy purposes; thus, in serializing the tables, we simply leave the cell data blank.\nTable 2 shows the improved performance of the fine-tuned model on the synthetic dataset \u2013 this\nresult is expected, given that the test set shares a similar distribution and similar features as the\ntraining set. The more challenging evaluation involves the proprietary dataset of similar tables, which\ncontains out-of-distribution samples. As also shown in Table 2, the fine-tuned model still improves\nupon the pre-trained BGE model, despite being fine-tuned on synthetic data that is quite different\nfrom the proprietary dataset. This demonstrates that the data generated from our pipeline allows for\nmore robust tabular data representations, enhancing table similarity retrieval performance even for\nout-of-distribution samples."}, {"title": "Conclusion", "content": "We fine-tune the BGE model using contrastive learning. To achieve this, each anchor table and its\ncorresponding similar tables must be accompanied by some hard negative examples \u2013 tables that are\ndissimilar to the anchor table and should therefore have representations that are farther away from the\nanchor table. For each anchor table, hard negatives were sourced from the pool of 700,000 tables\nthrough a hybrid-search pipeline using both semantic and bag-of-words embeddings.\nGiven a pair of similar tables an anchor table $T_a$ and a generated target table $T_t$ the cosine\nsimilarity between their embeddings should be relatively higher than the cosine similarity between\n$T_a$ and a dissimilar table or hard negative. For fine-tuning, we apply the standard InfoNCE loss $L$\nover hard negatives and in-batch negatives:\n$L = -log \\frac{exp(\\varphi(T_a, T_t))}{\\sum_{T_n \\in N} exp(\\varphi(T_a, T_n))}$\nwhere $N$ denotes the set of all negatives, and $T_n$ represent a dissimilar table sampled from $N$. $\\varphi$ is\nthe same temperature-scaled cosine similarity defined above.\nFor training, we set the batch size to 4. Although contrastive learning typically benefits from larger\nbatch sizes, we limited the batch size to 4 to fit within the memory constraints of a single GPU. The\nmaximum serialized length for all input tables was set to 512 tokens, truncating any table exceeding\nthis length. For each sample, we used 15 hard negatives in addition to the in-batch negatives.\nIn addition to evaluating this fine-tuned model on the test set from our generated dataset, we also\nmeasure its performance on a proprietary dataset from our industry. This dataset contains proprietary\ntabular data that have been labeled for close-lineage relationships, i.e., if one table was produced from\nanother table through some data transformation, making it suitable for evaluating the fine-tuned model\non similar table retrieval. While the Wikipedia tables are more general and easily understandable, the\ntables in this proprietary dataset are more specialized, containing domain-specific terminology. As a\nresult, this dataset is considered out-of-distribution for the model, making it a crucial benchmark for\nassessing the model's value in real-world applications.\nThis dataset consists of about 8,000 tables, from which we sample 1,000 to perform our evaluation.\nThe task then is for the model to generate representations that allow the similar (close-lineage) tables\nto rank as highly as possible among the pool of 8,000 tables for each of the tables in the evaluation.\nWe also note that each table contains a title, description, and column names, but no cell data for\nprivacy purposes; thus, in serializing the tables, we simply leave the cell data blank.\nTable 2 shows the improved performance of the fine-tuned model on the synthetic dataset \u2013 this\nresult is expected, given that the test set shares a similar distribution and similar features as the\ntraining set. The more challenging evaluation involves the proprietary dataset of similar tables, which\ncontains out-of-distribution samples. As also shown in Table 2, the fine-tuned model still improves\nupon the pre-trained BGE model, despite being fine-tuned on synthetic data that is quite different\nfrom the proprietary dataset. This demonstrates that the data generated from our pipeline allows for\nmore robust tabular data representations, enhancing table similarity retrieval performance even for\nout-of-distribution samples.\nIn this paper, we enhanced table-level representation for similar table recommendation tasks using\nlarge language models (LLMs). We identified two key challenges in the field -- data sparsity and\nthe ambiguous definition of table similarity and alleviated them by introducing a novel synthetic"}, {"title": "A Appendix", "content": "In this paper, we enhanced table-level representation for similar table recommendation tasks using\nlarge language models (LLMs). We identified two key challenges in the field -- data sparsity and\nthe ambiguous definition of table similarity and alleviated them by introducing a novel synthetic\ndata generation process based on LLMs and clearly defining the table similarity problem. We then\ndemonstrated the quality and utility of our generated dataset through manual validation, comparing\nembeddings to an existing table similarity dataset, and evaluating models using our data on the\ndownstream task of similar table recommendation.\nOur evaluations, conducted on both synthetic and proprietary datasets, comprehensively demonstrate\nthat the proposed method effectively improves table similarity matching, even in scenarios involving\nout-of-distribution samples. The results suggest that our approach has the potential to bridge the gap\nbetween synthetic training data and practical applications, offering a viable solution for similar table\nrecommendation in data-driven environments.\nWhile these findings are promising, further research is needed to explore the scalability of our method\nacross even larger datasets. Additionally, improving the ability of LLMs to generate desired and\ncomplete JSON-formatted tables remains a crucial area for future work."}, {"title": "A.1 Table Serialization", "content": "Our tables are originally represented as JSON objects. When serializing a JSON object to text that\nfits the input format of a language model, we directly convert the JSON object into a string, following\nthe order: cell data, description, title, and column names. For cell data, we randomly sample only\ntwo observations to avoid exceeding the context limit of the language model."}, {"title": "A.2 Tabular Operations", "content": "We defined six operations to exemplify the tabular transformations that a data analyst performs on\ndata on the job. Four of these operations rely on an LLM to automatically transform the table: edit,\nconcatenation, calculation, and update. Figure 2 shows how these operations may look on a sample\ntable.\nThe removal operation is applied only to tables without numerical columns, as there may be cases\nwhere a table has only one numerical column, and removing it would prevent a calculation process\nfrom being executed. The edit operation is applied only to non-numerical tables (those without any\nnumerical columns), while the calculation operation is used exclusively for numerical tables (those\nwith at least one numerical column).\nOur pipeline consists of some combination of these six operations performed in the following order:\nremoval, concatenation, edit, calculation, reordering, and update. This ensures that a newly-created\ncolumn does not subsequently get removed and that tabular transformations are performed before\nupdating any verbiage."}, {"title": "A.3 Prompt Design", "content": "This section details the prompts we used for creating the synthetic data generation pipeline.\nThe system prompt for asking an LLM to accomplish each operation:\nSYSTEM_PROMPT = \"You are a data scientist/analyst who edits tabular data every day.\"\nPrompts for guiding an LLM to accomplish operations: concatenation (concat), edit, calculation\n(calc)"}]}