{"title": "Sample Selection via Contrastive Fragmentation for Noisy Label Regression", "authors": ["Chris Dongjoo Kim", "Sangwoo Moon", "Jihwan Moon", "Dongyeon Woo", "Gunhee Kim"], "abstract": "As with many other problems, real-world regression is plagued by the presence\nof noisy labels, an inevitable issue that demands our attention. Fortunately, much\nreal-world data often exhibits an intrinsic property of continuously ordered correla-\ntions between labels and features, where data points with similar labels are also\nrepresented with closely related features. In response, we propose a novel approach\nnamed ConFrag, where we collectively model the regression data by transforming\nthem into disjoint yet contrasting fragmentation pairs. This enables the training\nof more distinctive representations, enhancing the ability to select clean samples.\nOur ConFrag framework leverages a mixture of neighboring fragments to discern\nnoisy labels through neighborhood agreement among expert feature extractors.\nWe extensively perform experiments on six newly curated benchmark datasets of\ndiverse domains, including age prediction, price prediction, and music production\nyear estimation. We also introduce a metric called Error Residual Ratio (ERR)\nto better account for varying degrees of label noise. Our approach consistently\noutperforms fourteen state-of-the-art baselines, being robust against symmetric and\nrandom Gaussian label noise.2.", "sections": [{"title": "1 Introduction", "content": "Regression is an important task in many disciplines such as finance [Zhang et al., 2017b, Wu et al.,\n2020b], medicine [de Vente et al., 2021, Tanaka et al., 2022], economics [Zhang et al., 2022],\nphysics [Sia et al., 2020, Doi et al., 2022], geography [Liu et al., 2023] and more. However, real-\nworld regression labels are prone to being corrupted with noise, making it an inevitable problem to\novercome in practical applications. In previous research, noisy label regression has been studied\nmuch in age estimation with noise incurred from Web data crawling [Rothe et al., 2018, Yiming\net al., 2021]. Beyond that, the issues of continuous label errors have also been reported in the tasks of\nobject detection [Su et al., 2012, Ma et al., 2022] and pose estimation [Geng and Xia, 2014] as well\nas measurements in hardware systems [Zhou et al., 2012, Zang et al., 2019].\nThe vast amount of noisy label learning research has focused more on classification than regression.\nSome notable approaches include regularization [Wang et al., 2019, Zhang and Sabuncu, 2018], data\nre-weighting [Ren et al., 2018, Shen and Sanghavi, 2019], training procedures [Jiang et al., 2018],\ntransition matrices [Yao et al., 2020, Xia et al., 2020], contrastive learning [Zhang et al., 2021a, Li\net al., 2022b], refurbishing [Song et al., 2019] and sample selection [Lee et al., 2018, Ostyakov et al.,\n2018]. Particularly, sample selection can be further divided into exploring the memorability of neural"}, {"title": "2 ConFrag: Contrastive Fragmentation", "content": "In the noisy label regression problem, we are presented with a dataset denoted as D = {X, Y }; in\neach sample (x, y), x \u2208 Rd is an input, and y \u2208 R is the observed label, which can be possibly noisy.\nWe use yst to denote the groundtruth label. The objective of ConFrag is to sample a clean subset of\nthe data as S \u2286 D. By training on S, we aim to enhance the performance of the regression model.\nAn overview of our ConFrag framework is shown in Fig. 3(a). The framework has the following steps.\nWe divide the dataset into what we refer to as contrastive fragment pairs (\u00a7 2.1), which collectively"}, {"title": "2.1 Contrastive Fragment Pairing", "content": "In order to sample a clean data subset S \u2286 D, we\nneed to learn a robust feature that can distinguish\nclean samples from noisy ones. As one theoreti-\ncal result in [Zhang et al., 2023], the cross-entropy\nloss used in classification is better for learning high\nentropy feature representation than the mean squared\nloss in regression (see Appendix D.1 for details).\nBased on this, we start by discretizing the label space\ninto F continuous fragments, transforming the origi\nnal regression problem into the multi-class classifica\ntion one. This transformation harnesses an inherent\nproperty of regression: data points with similar la\nbels are also represented with closely related features,\nas acknowledged in prior studies [Gong et al., 2022,\nYang et al., 2022b, Yao et al., 2022].\nHowever, instead of training a single feature extractor\non the multi-class classification with F classes, we\nconstruct F/2 maximally contrasting fragment pairs\nand train a smaller expert feature extractor for each pair. The procedure of contrastive fragment\npairing is detailed below with an illustration in Fig. 2:\n1. Divide the range of continuous labels Y into F even number of equal-length fragments. This\nallows to divide the dataset D into F disjoint subsets: D = {D1, ..., DF}, where each Di contains\nthe data samples whose y values are in the i-th fragment label range.\n2. Construct a complete graph g = {D, E}, where each vertex is a fragment Di, and each edge\nweight eij is the distance in the label space between the closest samples of the fragments (Di, Dj).\n3. Compute all possible perfect matchings [Monfared and Mallik, 2016, Gibbons, 1985], where every\nvertex of a graph is incident to exactly one edge in the graph."}, {"title": "4. Find the perfect matching with the largest minimal edge weight:", "content": "P = arg max\u011d\u2208G (min v(\u011d)),\nwhere each \u011f is a perfect matching (graph), and v(\u011d) is the set of edge weights in \u011f. Finally,\nP = {(Di, Dj), . . ., (Dk, D\u2081)} constitutes the maximally contrasting pairs of fragments."}, {"title": "Motivation behind contrastive fragment pairing.", "content": "Formulating the multi-class classification\nproblem into F/2 binary classification problems via contrastive fragment pairing has the following\nadvantages. Firstly, since the distance between fragments in each contrastive fragment pair is large, the\nfeature extractor trained on each contrastive pair can generalize better [Shawe-Taylor and Cristianini,\n1998, Gr\u00f8nlund et al., 2019, 2020]. Fig. 1(c) shows the generalization abilities of the expert feature\nextractors trained on contrastive fragment pairs compared to the single feature extractor trained on all\nfragments. When using a single feature extractor on all fragments (all-frags), the samples selected by\nthe feature extractor tend to become more noisy as the feature extractor overfits, causing the regressor\nto perform worse over time. On the other hand, when using multiple feature extractors trained on\ncontrastive pairs, the performance of the regression model consistently improves, indicating that the\nlearned features are more robust and the selected samples are cleaner. The large distance between\nfragments also explains why contrastive fragment pairing is superior to other fragment pairings, as\nshown in \u00a7 4.3. The analysis of the prediction depth [Baldock et al., 2021] in Appendix D.3 supports\nthe claim, as it shows that the binary classification on contrastive fragment pairs results in lower\nprediction depth, leading to better generalization.\nSecondly, the contrastive fragment pairing transforms some of closed-set label noise (whose ground\ntruth is within the label set) into open-set label noise (whose ground truth is not within the label\nset), as shown in Fig. 1(a). Previous works [Wei et al., 2021, Wan et al., 2024] observe that the\nopen-set noise is less harmful than the closed-set noise and may even benefit generalization and\nrobustness against inherent noisy labels. Indeed, in our experiments, we found similar observations\nwhere injecting open-set label noise is less harmful than closed-set one, as shown in Fig. 1(b).\nThe t-SNE visualization in Fig. 1(a) also supports this observation. Let f and fst be fragments that\nthe observed label y and the groundtruth label yst respectively belong to. Prior to contrastive fragment\npairing, all of the noisy labeled data (f \u2260 fst) are closed-set noise as their ground truth fragment ids\nare within the label set (fgt \u2208 [1-6]) and their features are located in the feature spaces of incorrect\nclasses within the group. After contrastive fragment pairing, much of these noisy labeled data is\ntransformed to open-set noise (fgt \u2209 [1,4] while f \u2208 [1, 4] in case of fragment pair [1, 4]), and their\nlearned features tend to reside outside the feature clusters of the clean samples, thus mitigating the\nadverse effects of the noise."}, {"title": "2.2 Training Feature Extractors for Contrastive Pairs", "content": "Once we obtain the contrastive fragment pairs P, we train F/2 number of expert feature extractors on\nbinary classification p(y|x; \u03b8i,j) with its respective contrastive pair (Di, Dj) \u2208 P, where \u03b8ij denotes\nthe parameter of an expert. That is, it is trained to predict whether a data x is in Di or Dj. Later, the\nfeature extractors play a crucial role in determining whether a sample (x, y) is clean."}, {"title": "2.3 Mixture of Neighboring Fragments", "content": "With the learned expert feature extractors, the next step is to perform sample selection. Given a\nsample (x, y), let f be a fragment close to y and f+ be its contrasting pair. Intuitively, we consider\na sample clean if the expert trained on (Df, Df+) strongly predicts that x belongs to a fragment f.\nHowever, since the expert feature extractor is a binary classifier only trained using a contrasting pair\nof fragments, we utilize all experts' opinions to obtain a more robust prediction. Specifically, we\ndeem a sample as clean if the experts exhibit a consensus response (Neighborhood Agreement) for\nfragments close to y (Fragment Prior).\nBased on this intuition, we formulate Mixture of Experts (MoE) [Jacobs et al., 1991] model, where\nthe sampling probability of a datapoint (x, y) is defined as\np(s|x, y, D1...F; \u0398) = \u2211f pf(y)af(x; D1...F, \u04e8), (1)\nwhere \u0398 denotes parameters of all feature extractors, pf is the fragment prior (mixture weight), and\naf is the neighborhood agreement (a binary vote of whether x belongs to the fragment f). Based on"}, {"title": "Fragment Prior.", "content": "For a sample (x, y), we compute the prior pf(y) of a fragment f, using a softmax\nweighting of each fragment f with respect to its relative distance to y:\npf(y) = \\frac{exp(gf(y))}{\\sum_{f'} exp(gf' (y))}, (2)\nwhere gf (y) = range(Y)/(|y \u2013 \u0176f|), range(Y) = max(Y) \u2013 min(Y) is the label range, and \u0176f is\nthe mean label value of fragment f. Since range(Y) is a constant for a given dataset, gf (y) rapidly\ndecreases when the mean value of fragment f is far from y in the continuous label space. From the\nMoE perspective, the fragment prior can be regarded as soft gating that depends on y."}, {"title": "Neighborhood Agreement.", "content": "Given a sample (x, y) and a fragment f, we need to determine whether x\nbelongs to f. The simplest approach is to use the expert trained using (Df, Df+) to classify whether\nx belongs to f or f+, where f+ is the contrasting fragment of f. Based on the classification output\nh(x;\u03b8f,f+) \u2208 {f, f+}, we define self-agreement as:\nself = [h(x; \u03b8f,f+) = f] (3)\nwhere [A] is the Iverson bracket outputting 1 if A is true, and 0 otherwise. Since training with noisy\nlabels often results in suboptimal calibration [Bae et al., 2022, Zong et al., 2024], we use discrete\nclassification output for a self rather than continuous probabilistic one.\nSince the expert \u03b8f, f+ is only trained to discriminate between f and its contrasting fragment f+, it is\nbetter to utilize other experts to obtain a more robust prediction. For example, consider contrastive\nfragment pairs {(1, 4), (2, 5), (3, 6)} as in Fig. 2. If x is more likely to belong to fragment 2 than 5,\nthen it should be more likely to belong to 1 than 4 and 3 than 6. Thus, we consider agreement of\nneighboring fragments fL (left) and fR (right) to obtain neighborhood agreement af(x; D1...F, \u04e8):\naf(x; D1...F, 0) = \u03b1self \u03b1ngb, where \u03b1ngb = [\u03b1selfL \u2227 \u03b1selfR]. (4)\nIntuitively, af is 1 if the fragment f is more likely for x than f+ (\u03b1self = 1) and either f's left or\nright fragment is more likely for x than its respective contrasting fragment (\u03b1ngb = 1)."}, {"title": "2.4 Neighborhood Jittering", "content": "A potential limitation of mixture models is that the individual expert feature extractor may not fully\nbenefit from the full dataset as they model their own disjoint subsets [Dukler et al., 2023]. Our\nneighborhood jittering mitigates this limitation as a robust regularizer that expands the effective\ncoverage of each contrastive fragment pair during learning. The process is visualized in Fig. 3(d).\nWe bound the ratio of the jittering buffer range within [0, \\frac{2(F-1)}{F}], where F is the fragment number.\nFor every epoch, we shift the label coverage of each fragment by randomly sampling the value\nin this range. Jittering leads to a partially overlapping mixture model [Heller and Ghahramani,\n2007b, Hinton, 2002] as some data belong to multiple, neighboring fragments and thus the effective\ncoverage per each expert is expanded. Such regularization inhibits feature extractors from overfitting\nto potentially noisy samples and promotes learning of more robust features, even those that can be\ngeneralizable to overlapping parts of neighboring fragments.\nFig. 4(a) shows that with jittering, the feature extractor exhibits higher accuracy on the clean test\ndata due to its regularization effect. In the sample selection stage (Fig. 4(b)), the feature extractor\ntrained without jittering easily overfits the noise, resulting in over-selection and higher ERR (\u00a7 4.2).\nIn contrast, the jittered feature extractor achieves a relatively low selection rate with halved ERR,\nindicating that the noisier samples are filtered out. Better sample selection due to jittering subsequently\nleads to significantly better performance in regression (Fig. 4(c)). In Appendix G.9, we compare\nneighborhood jittering to other regularizations, demonstrating its efficacy."}, {"title": "3 Related Works", "content": "We review prior works on learning with noisy labels and defer a comprehensive survey to Appendix\nE. We organize them into those utilizing prediction, representation, and combination of the two.\nPrediction-based Methods. This approach has been the focus of much existing research and covers\na wide array of topics: (i) the small loss selection by exploring the pattern of memorization in\nneural networks [Han et al., 2018, Arazo et al., 2019], (ii) relying on the consistency of predictions\nto select or refurbish the samples [Liu et al., 2020, Huang et al., 2020], (iii) estimating the noise\ndistribution [Patrini et al., 2017, Hendrycks et al., 2018], (iv) introducing auxiliary parameters or\nlabels [Pleiss et al., 2020, Hu et al., 2020], (v) using unlabeled data with semi-supervised learn\ning [Li et al., 2020a, Bai et al., 2021, Karim et al., 2022], and (vi) designing a noise-robust loss\nfunction [Menon et al., 2020, Wang et al., 2019].\nRepresentation-based Methods. This approach has seen a recent surge in interest, including (i)\nclustering based selection [Mirzasoleiman et al., 2020, Wu et al., 2020a], (ii) feature eigendecomposi-\ntion filtering [Kim et al., 2021], (iii) using neighbor information to sample and refurbish with clean\nvalidation [Li et al., 2022a, Gao et al., 2016], and (iv) generative models of features for sampling [Lee\net al., 2019].\nCombination. Some works have also studied the combination of representation and prediction spaces.\nWang et al. [2022] formulate a penalized regression between the network features and the labels for"}, {"title": "4 Experiments", "content": "We compare ConFrag with fourteen strong baselines adapted for noisy label regression. Due to the\nscarcity of benchmark datasets, we update existing datasets for the study of noisy labels."}, {"title": "4.1 Settings", "content": "Curation of Benchmark Datasets. We create six benchmark datasets for noisy labeled regression to\nencompass a sufficient quantity of balanced data, span multiple domains, and present a meaningful\nlevel of complexity. (i) Age Prediction from an image is a well-studied regression problem [Li\net al., 2019, Shin et al., 2022, Lim et al., 2020]. To address this domain, we acquire four datasets\nof AFAD [Niu et al., 2016], IMDB-Clean [Yiming et al., 2021], IMDB-WIKI [Rothe et al., 2018],\nand UTKFace [Zhifei et al., 2017]. Notably, IMDB-WIKI contains real-world label noise stemming\nfrom the automatic web crawling process [Yiming et al., 2021]. We use a ResNet-50 backbone for\nall datasets. (ii) Commodity Price Prediction is a vital real-world task [Wen-Huang et al., 2021].\nWe opt for the SHIFT15M dataset [Kimura et al., 2021] due to the diversity and scale of this\ndomain. This dataset is provided as the penultimate feature of the ImageNet pre-trained VGG-16\nmodel. Consequently, we use a three-layer MLP architecture for all experiments [Papadopoulos\net al., 2022, Kimura et al., 2021]. (iii) Music Production Year Estimation uses the tabular MSD\ndataset [Bertin-Mahieux et al., 2011]. This dataset is identified as one of the most intricate and\nchallenging datasets, based on the test R2 score [Grinsztajn et al., 2022]. We adopt a tabular ResNet\nproposed by Gorishniy et al. [2021]. The suffix \"-B\" is appended to the dataset name (e.g., AFAD-B)\nto indicate that it is a curated version of the original dataset. To focus on the noisy label problem, we\ntake measures to balance the datasets as elaborated in Appendix F.1.\nExperimental Design. For all datasets except for IMDB-WIKI-B which contains real-world label\nnoise, we inject symmetric and Gaussian noise into the labels, as done in prior literature [Yao et al.,\n2022, Yi and Wu, 2019, Wei et al., 2020]. These types of noise can simulate a low-cost (human-free)\ncontrolled setting. Symmetric noise mimics randomness such as Web crawling or annotator errors,\nand Gaussian noise is often used for modeling the regression label noise. While Yao et al. [2022]\ninject a fixed 30% standard deviated Gaussian noise for every label, we make it more realistic by\nrandomizing the standard deviation up to 30% or 50% of the domain's range. For our ConFrag\nexperiments, we fix the fragment number (F) as four. See Appendix F.3 for further training details.\nBaselines. There are many existing methods of noisy labeled learning for classification. We assess\nfourteen baselines from the three branches that are naturally adaptable to regression with minor\nor no updates. (i) Small loss selection: CNLCU-S,H [Xia et al., 2022], Sigua [Han et al., 2020],\nSPR [Wang et al., 2022], BMM [Arazo et al., 2019], DY-S [Arazo et al., 2019], SuperLoss [Castells\net al., 2020]. (ii) Regularization: C-mixup [Yao et al., 2022], RDI [Hu et al., 2020], CDR [Xia et al.,\n2021], D2L [Ma et al., 2018]. (iii) Refurbishment: AUX [Hu et al., 2020], Selfie [Song et al., 2019],\nCo-Selfie [Song et al., 2019]. Appendix F.2 comprehensively details these baselines."}, {"title": "4.2 Evaluation Metrics", "content": "We mainly report the Mean Relative Absolute Error (MRAE) following prior works. The MRAE\nis computed as (e/p) \u2013 1, where e is the model's Mean Absolute Error (MAE) performance under\nvarying conditions (noise type, severity) and p is the noise-free model's MAE. We express MRAES\nin percentage for better comprehensibility. The traditional MAE values are also reported in Ap\npendix G.12. In addition, we report the Selection rate (a.k.a prevalence), which is a metric often seen"}, {"title": "4.3 Results and Discussion", "content": "Overall performance. Table 1 compares the MRAE values to the noise-free trained model between\nConFrag and the baselines. We evaluate six types of noise: four symmetric and two random Gaussian\nnoises. ConFrag and Co-ConFrag achieve the strongest performance in all experiments compared\nto the fourteen baselines. Notably, Co-ConFrag mixes co-teaching during the regression learning\nphase by assuming that S still contains 25% noise. The results on UTKFace-B dataset can be found\nin Appendix G.1.\nSelection/ERR/MRAE comparison. Fig. 5 compares ConFrag to five selection and refurbishment\nbaselines of CNLCU-H, BMM, DY-S, AUX, Selfie on IMDB-Clean-B using the selection rate, ERR,\nand MRAE. Ideally, a model should attain a high selection rate and a low ERR. It is worth noting that\nthe relative importance of ERR and selection rate may vary depending on the dataset and the task.\nConFrag achieves the lowest ERR while maintaining above-average selection rates, resulting in the\nbest MRAE. Appendix G.10 includes comparison results for all noise types with more baselines.\nFragment pairing. Fig. 6(a) compares contrastive pairing to alternative pairings using MRAE\nas a metric. The contrastive fragment pairing demonstrates superior performance to other pairing\nmethods. Notably, the performance is poorest when both the average and minimum distance between\nfragments are smallest ([1,2], [3,4] when F = 4, [1,2], [3, 4], [5,6] when F = 6). While the\npairings of [1, 4], [2, 3] and [1, 6], [2, 5], [3, 4] have the same average distance between fragments as\nthe contrastive pairings, their minimum distances between fragments are smaller, resulting in poorer\nperformances than contrastive pairings. This result shows the effectiveness of contrastive fragment\npairing for selecting clean samples. See Appendix G.4 for more details.\nFragment number. ConFrag introduces a hyperparameter F, the number of fragments. While we\nsimply set F = 4 for all experiments, we conduct analysis on the effect of using different F, as\nshown in Fig. 6(b). On SHIFT15M-B dataset, the performance is relatively stable across different\nfragment numbers. On IMDB-Clean-B, a small declining trend in performance is observed as the\nnumber of fragments increases. This decrease is likely attributed to a finer division of the training data\namong feature extractors, ultimately leading to overfitting and reduced generalization capabilities.\nAppendix G.2 provides further analysis of the fragment number.\nAblation analysis on mixture of neighboring fragments. In Table 2, we conduct an ablation\nanalysis of the Mixture of neighboring fragments (\u00a7 2.3). When evaluating neighborhood agreement"}, {"title": "ERR", "content": "d\nERR = \\frac{1/|C| \\sum_{y_{gt} \\in C} |y - y_{gt}| }{1/|D| \\sum_{y_{gt} \\in D} |y - y_{gt}|}, (5)\nwhere C is a set of cleaned (selected or refurbished) samples. The numerator is the average cleaned\nerror that serves as an indicator of the precision of the cleaned data, while the denominator is the\naverage dataset error that normalizes it for standardized assessment. The ERR, along with the\nselection rate and regression metrics (e.g., MSE, MRAE), provides a deeper insight into the model\nperformance. Ideally, a method with a high selection rate coupled with low ERR and regression error\ncan be deemed as closer to the upper bound."}, {"title": "5 Conclusion", "content": "To address the problem of noisy labeled regression, we introduce the Contrastive Fragmentation\nframework (ConFrag). The framework partitions the label space and identifies the most contrasting\npairs of fragments, thereby training a mixture of feature extractors over contrastive fragment pairs.\nThis mixture is leveraged for clean selection based on neighborhood agreements. Extensive exper\niments on six curated datasets on three domains with different levels of symmetric and Gaussian\nnoise demonstrate that our framework performs superior selection and ultimately leads to a better\nregression performance than fourteen state-of-the-art models. Given its foundation in the Mixture\nof Experts model, the parameter size of ConFrag linearly grows with an increase in the number of\nfragments. We acknowledge this as a potential avenue for future research."}, {"title": "A Appendix: Table of Contents", "content": "The Appendix enlists the following additional materials.\nI. Limitations. \u00a7 B\nII. Broader Impacts. \u00a7 C\nIII. Theory of ConFrag. \u00a7 D\ni. Classification versus Regression for Feature Learning D.1\nii. Fragmentation and Neighborhood Jittering D.2\niii. Prediction Depth Analysis D.3\nIV. Extended Related Work. \u00a7 E\ni. Continuously Ordered Correlation of Labels and Features E.1\nii. Noisy Label in Object Detection E.2\niii. Transition Matrix based Methods E.3\niv. Combination with Contrastive Learning E.4\nV. Experiment Details. \u00a7 F\ni. Dataset Curation Details F.1\nii. Baseline Details F.2\niii. ConFrag Training Details F.3\niv. Random Gaussian Noise F.4\nv. Computation Resource F.5\nVI. Extended Results & Analyses. \u00a7 G\ni. UTKFace Results G.1\nii. Fragment Number Analysis G.2\niii. Hyperparameter Analysis G.3\niv. Fragment Pairing Analysis G.4\nv. Closed-Set versus Open-Set Noise G.5\nvi. Analysis of Samples on the Bounday versus Center of Fragment G.6\nvii. Ablation & Combination Analysis G.7\nviii. Discretized Baselines G.8\nix. Comparison of Neighborhood Jittering and Other Regularization Methods G.9\nx. Extended Selection Rate/ERR/MRAE Comparison and Analysis G.10\nxi. Variance Across Random Seeds G.11\nxii. Standard Mean Absolute Error G.12\nVII. ConFrag Pseudo Code (Algorithm 1)"}, {"title": "B Limitation", "content": "A key limitation of ConFrag lies in its foundational reliance on the Mixture of Experts (MoE)\nmodel [Jacobs et al., 1991]. Specifically, integrating MoEs with deep learning introduces notable\nscalability challenges, both computationally and in memory usage [Zuo et al., 2021, Zoph et al.,\n2022, Zhang et al., 2021b]. To address the memory concern, ConFrag currently employs more\ncompact feature extractors. Nevertheless, a prominent inefficiency stems from expert redundancy in\nMoEs' parameters [Zuo et al., 2021]. Some approaches to mitigate this include distilling into sparse\nMoE models, employing pruning, and subsequently compressing to decrease parameter size [Kim\net al., 2023, Fedus et al., 2021]. There are also emerging strategies centered on parameter sharing,\nleveraging matrix product operators (MPO) decomposition [Gao et al., 2020, 2022] and parameter\nefficient fine-tuning [Zadouri et al., 2023]. Of these, we believe the avenue of parameter sharing\nholds special promise when combined with ConFrag; the inherent positive feature correlation in\nregression problems amplifies the advantages of this approach. Also, as in MoEs, ConFrag introduces\nnew hyperparameter, the number of experts (the number of fragments F in ConFrag's case)."}, {"title": "C Broader Impacts", "content": "In the era of deep learning, the need for large datasets increases, yet it is expensive to obtain large\ndataset with high-quality annotated labels. An alternative solution is to collect labels using automated\nlabeling methods, such as web crawling. However, these methods inevitably introduce noisy labels.\nThis work proposes a method for mitigating the negative effect of such label noise in regression, which\ncan save time and money spent on collecting high-quality labels for many applications, bringing\npositive impact on science, society, and economy. However, since the method reduces the need for\naccurate labeling, it may have potential negative effect on the salaries of label workers."}, {"title": "D Theory of ConFrag", "content": "We present several theoretical justifications that enhance the performance of ConFrag."}, {"title": "D.1 Classification versus Regression for Feature Learning", "content": "During the learning process, deep neural networks aim to maximize the mutual information be\ntween the learned representation, denoted as Z, and the target variable, denoted as Y. The mutual\ninformation between these two variables can be defined as I(Z; Y) = H(Z) \u2013 H(Z|Y). A high\nvalue of I(Z; Y) is indicative of a high marginal entropy H(Z). Achieving this dual objective is\naccomplished in classification [Boudiat et al., 2020].\nHowever, Zhang et al. [2023] have shown that regression primarily focuses on minimizing H(Z|Y)\nwhile disregarding H(Z). This results in a relatively lower marginal entropy for the learned represen\ntation Z and ultimately leads to performance deficits in comparison to classification.\nTo experimentally show that this theoretical result also applies to ConFrag, we replace classification\nbased expert feature extractor learning with regression-based one, where each expert feature extractor\nis trained with regression loss on its respective fragment pair dataset. We name this variant ConFrag\nR. In ConFrag-R, self-agreement is defined using distances to the mean of each fragment in the\ncontrasting pair (f, f+):"}, {"title": "D.2 Fragmentation and Neighborhood Jittering", "content": "ConFrag operates by partitioning data samples into fragments and leveraging trained feature extrac\ntors for sample selection through collective modeling. We conceptualize this as a Mixture-of-Experts\n(MoE) model, wherein individual experts specialize in specific problem subspaces through data\npartitioning [Yuksel et al., 2012, Masoudnia and Ebrahimpour, 2014]. MoEs possess theoretically\nadvantageous properties with respect to computational scalability and reduction of output vari\nance [Yuksel et al., 2012], contributing to the enhancements observed in ConFrag. It is noteworthy\nthat since each network is trained on a distinct training set, MoE effectively mitigates concurrent\nfailures, thereby preventing error propagation among networks and ultimately improving the general\nization performance of ConFrag as well [Sharkey and Sharkey, 1997].\nAdditionally, our Neighborhood Jittering leads to a Partially Overlapping Mixture Model [Heller\nand Ghahramani, 2007a], theoretically enabling the modeling of significantly richer and more"}, {"title": "D.3 Prediction Depth Analysis", "content": "Prediction depth [Baldock et al., 2021] of an example refers to the earliest layer where the layer-wise\nK-nearest neighbor probes of the layer and all the subsequent layers are the same as the model\nprediction. In other words, low prediction depth means that the example is easily distinguishable\nin early layers. For example, a prediction depth of zero means that data can be predicted at the\ninput level only based on its distances to other data. Low prediction depth is positively correlated\nwith better prediction consistency, lower learning difficulty, and larger margin. Due to these traits,\nsome previous works aim at reducing the prediction depth during training for better generalization\nperformance [Zhou et al., 2022, Sarfi et al., 2023]."}, {"title": "E Extended Related Work", "content": ""}, {"title": "E.1 Continuously Ordered Correlation of Labels and Features", "content": "One distinctive characteristic of regression problems is their continuous label space, implying a high\nlikelihood of correlation between regions within the feature and label spaces [Yang et al., 2022b,\nGong et al., 2022, Zha"}]}