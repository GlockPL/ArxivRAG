{"title": "Moto: Latent Motion Token as the Bridging Language for Robot Manipulation", "authors": ["Yi Chen", "Yuying Ge", "Yizhuo Li", "Yixiao Ge", "Mingyu Ding", "Ying Shan", "Xihui Liu"], "abstract": "Recent developments in Large Language Models (LLMs) pre-trained on extensive corpora have shown significant success in various natural language processing (NLP) tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks. Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood. To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Natural Language Processing (NLP) have stemmed from successful autoregressive pre-training on large text corpora via next-word prediction [6, 18, 44, 46, 50]. Pre-trained Large Language Models (LLMs) have shown exceptional performance across various downstream NLP tasks after fine-tuning on smaller datasets. This success opens new opportunity for robotics, which has been limited by the high costs of action-labeled data. Given the abundance of interaction-rich video data [3, 57], we ask: Can we leverage autoregressive pre-training on video data to improve robot learning?\nThe main challenge is finding an appropriate representation for autoregressive pre-training on video data that effectively captures prior knowledge for robot manipulation. Pioneering research in video pre-training for robotics primarily focused on static frames, emphasizing frame-level visual details [9, 19, 54]. However, humans learn skills by observing dynamic environments, focusing on changes in state-what we term motion. Thus, we argue that effective autoregression for robotics should prioritize motion-related knowledge, which aligns closely with low-level robot actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions through fine-tuning.\nIn this work, we introduce Moto, which utilizes Latent Motion Tokens as a bridging \"language\" to model visual motions between video frames in an unsupervised manner. As illustrated in Fig. 1, we first train a discrete Latent Motion Tokenizer to produce compact latent motion tokens that capture dynamics between video frames without external supervision. We then pre-train Moto-GPT using a GPT-based architecture to predict the next latent motion token, absorbing motion priors from videos. These learned priors are subsequently transferred to enhance robot manipulation tasks through a co-fine-tuning strategy.\nSpecifically, as shown in Fig. 2, the Latent Motion Tokenizer encoder employs a VQ-VAE-based architecture [51] to compress two successive video frames into discrete tokens. By regularizing the decoder to reconstruct the second frame from the first frame and the tokens, the tokenizer is trained to effectively capture the changes between video frames, which often arise from motion. Once the tokenizer is trained, we obtain latent motion tokens of every two consecutive frames in a video clip and concatenate them into a sequence to represent the motion trajectory. Subsequently, Moto-GPT is pre-trained on these sequences by predicting the next token based on the initial frame and corresponding language instruction. After this pre-training phase, Moto-GPT is capable of generating plausible trajectories by predicting latent motion tokens autoregressively.\nTo adapt Moto-GPT for downstream robot manipulation tasks, we concatenate action query tokens with latent motion token chunk at each time step for co-fine-tuning on action-labeled robot data. The action query tokens are processed by a learnable module to predict low-level actions, while the motion tokens are fine-tuned using the original next-token prediction mechanism. This co-fine-tuning strategy effectively transfers abstract intentions in learned motion priors into precise action execution, allowing the model to utilize the inherent knowledge of the pre-trained Moto-GPT for successful manipulation.\nWe conduct extensive experiments to validate our claims from various perspectives: (1) Latent Motion Token as an Interpretable Motion Language: Experiments show that latent motion tokens encapsulate compact and expressive representations of motion, effectively reconstructing and understanding motion trajectories in videos. (2) Pre-trained Moto-GPT as a Useful Motion Prior Learner: Results indicate that the pre-trained Moto-GPT achieves promising outcomes in predicting plausible motion trajectories and assessing the rationality of robot trajectories based on output likelihood. (3) Fine-tuned Moto-GPT as an Effective Robot Policy: The fine-tuned Moto-GPT demonstrates significant performance improvements over counterparts trained without motion priors, especially with limited training data, highlighting its effectiveness in transferring learned motion knowledge to robot manipulations.\nIn summary, our contributions are threefold as below:\n\u2022 Introduction of Latent Motion Tokens, which model visual motions between video frames in an unsupervised manner, serving as a bridging \"language\" for autoregressive pre-training to enhance robot learning.\n\u2022 Pre-training of Moto-GPT through next latent motion token prediction on video data, enabling the model to learn useful motion priors without requiring action annotations.\n\u2022 Implementation of a co-fine-tuning strategy to successfully transfer learned motion priors to actual robot manipulations, with the fine-tuned model showing competitive performance on robotic benchmarks.\nWe believe the vast reservoir of interaction-rich knowledge in video data presents a crucial opportunity for advancing robot learning and hope this paper inspires further exploration of effective autoregressive representations for acquiring valuable priors through pre-training, ultimately enhancing robotic capabilities."}, {"title": "2. Related Work", "content": "Vision-Language-Action Models. Recent studies have increasingly employed transformers as unified vision-language-action (VLA) architectures to generate robot actions from sequential observations and language instructions [5, 25, 48]. Inspired by the success of pre-training in vision-language transformers [1, 6, 36, 44], VLA model pre-training has gained traction. One approach fine-tunes policy models from powerful vision-language models pre-trained on large image-text datasets [16, 32, 62]. Another explores training generalist policy models on diverse cross-embodiment robot data with action labels [15, 28, 42, 52]. In contrast, our work aims to enhance VLA models through generative pre-training on video data, which offers richer interaction details than text and images and requires no hardware-specific labels of low-level robot actions. Beyond VLA models, several contributions focus on improving robot manipulation performance. Some extend input observations from single-view RGB images to include multi-perspective views and depth information [8, 35, 59]. Techniques like action chunking and policy diffusion also enhance action precision [13, 22, 27]. Additionally, some works [20, 34] decompose high-level language instructions into latent skills learned through auxiliary training objectives during imitation learning.\nRobot Learning from Videos Videos provide rich knowledge about physical dynamics, making them ideal for robot learning. Early works [38, 43] utilized contrastive learning with egocentric videos to enhance visual representations for manipulation. Some studies [4, 17, 29, 30, 33] generate videos or images as intermediate plans for guiding low-level control. Recent research [9, 23, 54] has shifted towards generative video pre-training followed by fine-tuning to create end-to-end policy models. Escontrela et al. [19] pre-trains an autoregressive video prediction model to provide reward signals for reinforcement learning. These works primarily use pixel values or patch-level tokens of video frames as their pretraining target. In contrast, our approach focuses on latent motion tokens as prediction targets, emphasizing key visual motions while decoupling irrelevant details. Additionally, some studies build world models through action-conditioned video generation [21, 55, 56], facilitating reinforcement learning or serving as interactive environments. Notably, Genie [7] proposes unsupervised learning of latent actions from large-scale videos to create a versatile 2D gaming simulator. Our goal, however, is to train a generalized policy model for robot manipulation, which is more complex than developing a 2D gaming simulation environment. Concurrently, Ye et al. [58] pre-train a policy model to predict one-step future latent actions, while Chen et al. [12] use latent actions as intermediate goals for low-level policies. Our approach differs by pre-training an end-to-end policy model to autoregressively predict a trajectory of latent motion tokens for future video clips."}, {"title": "3. Methodology", "content": "3.1. Overview\nMoto utilizes autoregressive generative pre-training on latent motion token sequences to learn motion priors from videos, followed by co-fine-tuning on action-labeled data for robot control. As illustrated in Figure 2, Moto consists of three stages: 1) unsupervised training of the Latent Motion Tokenizer, 2) pre-training of the generative model Moto-GPT, and 3) co-fine-tuning for robot action policy. In Sec 3.2, we detail the Latent Motion Tokenizer, which encodes visual dynamics into quantized latent motion tokens. We also describe the training procedures for Moto-GPT, including motion token autoregressive pre-training in Sec 3.3 and supervised co-fine-tuning in Sec 3.4. Implementation details can be found in the Supplementary Material.\n3.2. Latent Motion Tokenizer\nThe Latent Motion Tokenizer, as shown in Figure 3, learns a latent \"language\" to capture essential visual motions between successive video frames\u00b9 in an unsupervised manner. The architecture follows a standard auto-encoder design for motion tokenization and detokenization. The tokenization"}, {"title": "3.3. Motion Token Autoregressive Pre-training", "content": "With the Latent Motion Tokenizer, Moto-GPT is allowed to learn about diverse visual motions from videos, using latent motion tokens as a bridging language. As shown in Figure 2, Moto-GPT is pre-trained with a next-motion-token prediction objective. For a video clip [o\u2080, o\u2081, ..., o\u209c], we derive a chunk of latent motion tokens for each pair of consecutive frames, concatenating them chronologically to form a sequence. Moto-GPT employs a GPT-style transformer for autoregression on these motion token trajectories. Additionally, we prepend the text features from the instruction and the visual features from the initial video frame as input prompts. The pre-training objective maximizes the likelihood of the ground-truth latent motion token sequence given the language instruction and the initial video frame:\n$\\mathcal{L}_{motion} = - \\sum_{i=1}^M \\log \\Sigma P(m_i | I, v, m_{<i}; \\Theta), $   (1)\nwhere I and v are text and visual features from the frozen pre-trained T5 [47] and ViT [24] models, respectively. m_{<i}"}, {"title": "3.4. Co-fine-tuning for Robot Manipulation", "content": "After pre-training, Moto-GPT can anticipate future trajectories by generating latent motion tokens based on language instructions and initial observations. This process resembles the policy inference of real robots if we take the codebook of latent motion tokens as an abstract action space. However, a gap remains in achieving precise robot control. To address this, during fine-tuning, we introduce special action query tokens into Moto-GPT's input, enabling the generation of real robot actions through a flexible action head, as illustrated in the right part of Figure 2. Specifically, N query tokens are added after the latent motion token chunk at each time step, where N corresponds to the number of robot actions occurring between two video frames. The fine-tuning stage follows the same causal mask mechanism as pre-training in general. Nevertheless, the latent motion tokens do not attend to the newly inserted action query tokens to stay consistent with the pre-training setting. Besides, we randomly mask 50% of the attention from action query tokens to latent motion tokens, allowing knowledge transfer while reducing dependency on ground-truth conditions. This also improves inference efficiency, enabling direct queries to Moto for real actions without generating latent motion tokens. This can be achieved by using padding tokens as placeholders for latent action tokens, blocking attention from action query tokens to these placeholders.\nAn MLP-based action head projects the output hidden state of each action query token into the real robot action space. We apply Smooth-L1 loss for continuous action components, such as positional (\u2206x) and rotational (\u2206\u03b8) displacements, and Binary Cross Entropy (BCE) loss for binary components, like the gripper's open/close state (Agrip)\u00b2. The total action loss Laction is defined as:\n$L_{action} = L(\\Delta x) + L(\\Delta\\theta) + L(Agrip)   (2)\nWe retain the training objective for latent motion token prediction to ensure Moto-GPT retains the motion priors learned from videos. Thus, the overall loss function for the fine-tuning stage is:\n$L_{ft} = L_{motion} + L_{action}   (3)"}, {"title": "4. Experiment Setup", "content": "4.1. Benchmarks and Datasets\nWe use SIMPLER [31] and CALVIN [40] as the main evaluation benchmarks for robot manipulation.\nSIMPLER. On the SIMPLER benchmark, we focus on three tasks concerning the Google Everyday Robot embodiment: Pick Coke Can, Move Near, and Open/Close Drawer, as illustrated in Figure 4. The \"Pick Coke Can\u201d task involves grasping and lifting the empty coke can in three different orientations: horizontal laying, vertical laying, and standing. The \u201cMove Near\u201d task places 3 (out of 8) objects in a triangle pattern on the tabletop and instructs the robot to move a designated source object near another object as the target. We utilize a subset of Open-X-Embodiment [52] to train the Latent Motion Tokenizer and pre-train Moto-GPT, which consists of 109k real-world trajectory videos [5, 10, 14, 37, 39, 41, 45, 49, 53, 60, 61] across various embodiments. For fine-tuning Moto-GPT, we use 73k action-labeled expert trajectories from the RT-1 Robot Action dataset [5].\nCALVIN. On the CALVIN benchmark [40], we assess long-horizon task completion with the Franka Emika Panda robot, requiring the robot to consecutively complete 5 out of 34 manipulation tasks in each trial. There are four different environments (A, B, C, D), each containing a desk with a sliding door, a drawer, differently colored blocks, a"}, {"title": "4.2. Compared Models", "content": "SIMPLER. On the SIMPLER benchmark, we compare Moto-GPT with four representative models pre-trained with Open-X-Embodiment datasets:\n\u2022 RT-1-X [5] uses a transformer backbone to output tokenized actions with a FiLM EfficientNet to fuse language"}, {"title": "4.3. Training Details", "content": "Latent Motion Tokenizer. The implementation details for the trainable modules of the Latent Motion Tokenizer are summarized in Table 1. We use the hyperparameters listed in Table 2 to train this model on four A100-40G GPUs. To facilitate the learning of latent motion tokens, we downsample the original videos in the training dataset, ensuring that the visual motion between frames is sufficiently distinct. Specifically, for videos from the Open-X-Embodiment datasets, we sample one frame every three frames (i.e., \u0394t = 3) and train the Latent Motion Tokenizer for 350k steps. For videos from the CALVIN dataset, we adopt a sampling rate of one frame every five frames (\u0394t = 5) and train the model for 150k steps.\nMoto-GPT. We present the implementation details of Moto-GPT in Table 3, where the Action Head is included only during the fine-tuning phase. Moto-GPT handles a maximum video length of three frames, and the video downsampling rate applied during both the pre-training and fine-tuning stages is consistent with the rate used for training the Latent Motion Tokenizer. When fine-tuning Moto-GPT across different benchmarks, the number of action"}, {"title": "5. Experiments", "content": "To comprehensively evaluate the effectiveness of Moto, we study three key experimental questions:\n\u2022 Q1 (Interpretability): Does the Latent Motion Tokenizer learn interpretable latent motion tokens that effectively represent visual motions from videos?\n\u2022 Q2 (Motion Priors): Does Moto-GPT gain meaningful prior knowledge of motion trajectories through autoregressive pre-training on latent motion token sequences?\n\u2022 Q3 (Performance): Can the motion priors be transferred to enhance policy performance in robot manipulation benchmarks through efficient fine-tuning?"}, {"title": "5.1. Latent Motion Token as an Interpretable Motion Language", "content": "As illustrated in Figure 6, the next frame reconstructed by the Latent Motion Tokenizer using ground-truth latent motion tokens is authentic, effectively capturing the key dynamics between the initial frame and the ground-truth next frame. This suggests that latent motion tokens can represent fine-grained motion details, and the Latent Motion Tokenizer's decoder serves as a qualified simulator for visualizing environmental changes.\nFigure 7 further explores the controllability and consistency of latent motion tokens. Each row demonstrates that different token chunks produce visual motions with varying orientations and scales relative to the initial frame. Conversely, within each column, identical token chunks yield similar effects on the resulting positions and postures across different starting observations. By concatenating latent motion token chunks for every two consecutive frames from a video, we create a sequential representation of motion trajectories, akin to natural language context. As shown in Figure 8, this representation can be applied to different initial observations, generating contextualized motion trajectories and highlighting the potential of latent motion tokens as a unified language interface for guiding imitation learning.\nTable 5 presents quantitative evidence of the semantic interpretability of latent motion tokens. We trained a video classifier using ViT patch features from the initial frame, alongside concatenated latent motion tokens for the subsequent seven frames to predict semantic labels for 34 tasks from the ABC\u2192D split of the CALVIN dataset. The classifier utilizing latent motion tokens achieved an accuracy of 79.7%, comparable to the performance of a classifier using ViT patch features for all eight frames, despite the former reducing input features for each subsequent frame from 196 tokens to just 8. In contrast, classifiers relying solely on the initial frame or a repeated initial frame sequence struggled, achieving accuracies below 30%. These results indicate that, despite training without text or action labels, latent motion tokens provide a highly compact and expressive representation of visual motions, serving as an interpretable language of motion linked to high-level semantics."}, {"title": "5.2. Moto-GPT as a Useful Motion Prior Learner", "content": "The pre-training stage of Moto-GPT involves autoregression on video data using latent motion tokens, enabling it to predict motion trajectories based on initial observations and various language prompts, as illustrated in Figure 9. Table 6 presents the top-k accuracy of Moto-GPT in predicting ground-truth latent motion tokens from a 128-size codebook on the validation splits of the pre-training datasets. These results demonstrate Moto-GPT's effective acquisition of prior knowledge for motion trajectory prediction, which is crucial for robot action inference based on human instructions. Thus, the learned motion priors hold the potential to benefit downstream robotic tasks.\nAdditionally, latent motion tokens allow Moto-GPT to interpret trajectory videos as compact token sequences and evaluate their rationality through the autoregressive likelihood defined in Eq. 3.3. Figure 10 illustrates the potential"}, {"title": "5.3. Moto-GPT as an Effective Robot Policy", "content": "Overall Performance. After fine-tuning, we evaluated Moto-GPT\u00b3 against baseline models on the SIMPLER and"}, {"title": "6. Conclusion and Discussion", "content": "This paper introduces Moto, a novel method that uses latent motion tokens as a \"language\" interface to bridge generative pre-training on video data with precise robot control. Moto opens several exciting avenues for future work.\nFirstly, Moto demonstrates the feasibility of learning a unified language to interpret diverse visual dynamics from videos, eliminating the need for hardware-specific action labels. The latent motion trajectories tokenized from videos provide a rich resource for models to learn motion priors closely related to low-level actions. While we currently mainly use robot videos to train the Latent Motion Tokenizer, the learned latent motion tokens demonstrate the potential to produce consistent visual motions across varied contexts and embodiments. We believe a similar approach"}]}