{"title": "Hybrid Dynamic Pruning: A Pathway to Efficient Transformer Inference", "authors": ["Ghadeer A. Jaradat", "Mohammed F. Tolba", "Ghada Alsahli", "Hani Saleh", "Mahmoud Al-Qutayri", "Thanos Stouraitis", "Baker Mohammad"], "abstract": "In the world of deep learning, Transformer models have become very significant, leading to improvements in many areas from understanding language to recognizing images, covering a wide range of applications. Despite their success, the deployment of these models in real-time applications, particularly on edge devices, poses significant challenges due to their quadratic computational intensity and memory demands. To overcome these challenges we introduce a novel Hybrid Dynamic Pruning (HDP), an efficient algorithm-architecture co-design approach that accelerates transformers using head sparsity, block sparsity and approximation opportunities to reduce computations in attention and reduce memory access. With the observation of the huge redundancy in attention scores and attention heads, we propose a novel integer-based row-balanced block pruning to prune unimportant blocks in the attention matrix at run time, also propose integer-based head pruning to detect and prune unimportant heads at an early stage at run time. Also we propose an approximation method that reduces attention computations. To efficiently support these methods with lower latency and power efficiency, we propose a HDP co-processor architecture.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformer models, including BERT[1], GPT [2], T5 [3] and others [4] [5], have transformed Natural Language Processing (NLP) with their attention mechanism, achieving top performance in tasks such as question-answering [6], text classification [3], and machine translation [7]. The transformer architecture uses self-attention mechanism [8] and it is highly parallelizable on modern Graphical Processing Units (GPUs), providing major benefits over older models like Long Short Term Memories (LSTMs) and Recurrent Neural Networks (RNNs). This has led to fast progress in NLP, with models like BERT exceeding human performance in difficult tasks [9] and expanding their application to computer vision, including object recognition and detection [10], image classification [11], and segmentation [12].\nDeploying large transformer models on devices with limited resources is challenging due to their high computational and memory requirements. For example, BERT-Base Transformer needs 440 MB of memory and over 176 Giga FLoating Point Operations (GFLOPs) [13] . The computations are particularly difficult because of the complex attention operations and the quadratic computational complexity related to the length of input sequences [14]. Attention operations in transformer models become increasingly dominant as the input sequence length grows. For BERT-Base Transformer deployed on edge platforms, with a sequence length of 512, the attention operations account for about half of the total execution time, and this figure rises to 70% when the sequence length extends to 768 [15]. Therefore, finding efficient ways to handle attention operations is crucial for speeding up transformers.\nMany studies utilized sparsity to mitigate the quadratic time and space complexity issue. Some techniques save computational effort by using fixed or static sparse attention patterns [16] [14] [17], but their performance is limited [18], since the sparse pattern in attention is naturally dynamic, and depends only on the input. Other techniques focus on dynamic sparsity, meaning there's no fixed pattern for which parts are sparse (zero). For example, A\u00b3 [19] uses various approximation methods to skip calculations of near-zero values, aiming to decrease computational demands, but it requires loading all data onto the chip, which doesn't decrease off-chip DRAM access. SpAtten [20] introduces a cascaded token pruning mechanism that gradually eliminates less important tokens to simplify the workload using a Top-K strategy. Despite being tailored for dynamic decision-making in hardware, Top-K requires significant computational cost. Energon [15] uses a mixed-precision, multi-stage filtering technique to mimic the Top-K pruning approach, but it relies on a special unit to handle sparsity. AccelTran [21] prunes values in all transformer matrix multiplications if they fall below certain predetermined threshold. A\u00b3, Energon, and AccelTran leverage unstructured sparsity to realize efficient deployment for Transformers which leads to non uniform data access and lower efficiency. It's hard to predict these sparsity patterns, which can slow down performance.\nOther research efforts have been directed towards the re- moval of attention heads in transformer models, based on the understanding that not all heads contribute significantly to the transformer's performance. Researchers in [22] add trainable gating parameters attached to each head and regularized with LO loss. In [23], the importance of each attention head is gauged based on its sensitivity to the overall loss. This sensitivity is utilized as an indirect measure to determine the significance of each head. In [24], a novel approach termed 'single-shot meta-pruner' is presented. This method involves training a compact convolutional neural network with the specific purpose of identifying and selecting the attention heads that are crucial for preserving the distribution of attention within the model. All of these studies perform pruning at compile time not run time, require retraining to recover the"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "A. Transformer Algorithm\nTransformers have shown state-of-the-art performance in natural language processing field since their initial introduction in 2017 [8]. They are often regarded as alternatives to classic CNNs and RNNs in various sit- uations in real-world due to their exceptional efficiency and generality. Transformers consist of two essential components: the encoder and the decoder, which are formed by layering many transformer blocks. As depicted in Fig. 1, a block comprises three primary elements: linear layers, multi-head self-attention, and the normalization layer. The linear layers include the fully connected Layer (FC), Feed Forward (FFN), and the projection layer. The processes starts with transforming the input vectors into embedding vectors, which are then sequentially passed through a series of processing blocks. Within each block, the input is projected through the projection layer to Query (Q), Key (K) and Value (V) features using the weights WQ, WK and WV. Following this, attention mechanisms are utilized on these features to comprehend the long-term relational dependencies present in the input sequence.\nThe attention mechanism, as shown in Algorithm 1, splits the Q, K and V matrices into multiple smaller sets Qh, Kh, and Vh equivalent to the number of heads H. Each of these sets forms an \"attention head\", see Fig. 1 right side. Inside each head the output is computed as follows:\nAttention(Qh, Kh, Vh) = softmax(\\frac{QhKh^T}{\\sqrt{dk}})Vh.\nThe process starts with the computation of the dot product between Qh and Kh, followed by scaling. This step computes the alignment or similarity between each pair of tokens, the fundamental components of a sequence. The resulting matrix represents each token's relationship with every other token. Then a row-wise softmax is applied"}, {"title": "III. ALGORITHMIC OPTIMIZATION", "content": "In this section, we discuss the algorithmic optimizations that enhance the transformer model's efficiency and performance, focusing on block, head pruning and approximation techniques. These optimizations are key to reducing computational complexity and memory access.\nA. Block Pruning\nFor the attention score matrix, most of the query-key relations are not important and can be pruned safely, many methods have been used to prune these relations. Top-K pruning method [20] is used to prune the attention weights where a whole row can be pruned, but this requires a retraining to recover the accuracy, also it requires a specialized hardware to get the k most significant attention weights. Energon [15] avoided the Top-K selection and used the mean filtering as a practical approximation instead, but it still has a separate unit to perform this operation, also faces data duplication overhead due to the multi-round filtering method employed. In Energon and AccelTran [21] the pruning is done in an element-wise pattern which results in an irregular sparse matrix, where zeros are randomly spread over the matrix and this results in an irregular memory access and stalls in the hardware.\nTo address these challenges we propose integer-based block pruning, where pruning decision is exclusively determined by the integer parts of the numbers. We employ a small block size for pruning to eliminate the necessity for retraining and to guarantee a more organized and hardware-compatible sparsity pattern. Initially, multiplication is conducted only on the integer parts of Q and K to obtain Integer_atten. For each 2 \u00d7 2 block, we calculate its importance, 0, as the absolute sum of the values within the block. For each row of blocks, we determine the block pruning ratio, , using a method similar to that in Energon, which involves calculating the minimum, maximum, and mean importance values, along with a predefined pruning ratio, PB, as shown in Algorithm 2 line 15. Blocks with importance, 0, falls below the row-specific threshold, \u0398, are pruned and the mask value for the block is assigned to 0. When a block is pruned, subsequent computations for that block are omitted. Conversely, if a"}, {"title": "IV. HARDWARE ARCHITECTURE", "content": "Current attention accelerators and traditional CPUs/GPUs lack the capability to execute the proposed hybrid dynamic sparse attention technique efficiently. To address this gap, we are introducing a novel HDP accelerator. HDP is developed to function as a co-processor and is compatible with a variety of neural network accelerators for easy integration.\nA. Architecture Overview\nThe HDP architecture, depicted in Fig. 4, compromises multiple cores, the architecture of individual core is shown in Fig. 4 middle part. Each core is composed of an array of processing elements (PE Array), a Sparsity Engine (SE), an adder, and a softmax unit. The PE Array handles matrix multiplication tasks such as Q \u00d7 KT and attention_prob \u00d7 V, also calculating importance values. The SE is tasked with identifying which blocks to prune and deciding whether a head should be pruned or not.\nWorkflow: Once Q, K, and V are generated and quantized by another processor in fixed point 16 bit format and stored in memory, HDP processes each attention head sequentially. It employs tiled matrix multiplication for these operations. Initially, the integer components of Q and K are retrieved from off-chip memory into on-chip memory for the computation of Integer_Q \u00d7 Integer_K using tiling. The SE then uses the computed importance values for each block to create a mask indicating which blocks are not pruned. This approach prevents unnecessary data fetching for pruned blocks, reducing memory access and computational overhead. Once Integer_Q \u00d7 Integer_K computation is complete, and the head importance is assessed, the SE decides whether a head will be pruned. If so, the remaining computations for that head will be skipped and proceeds to the next head. For heads that remain unpruned, a Fetch Upon Mask (FUM) strategy is utilized. If the mask value is 0, indicating a pruned block, the corresponding K values will not be fetched, and the computation for that block is skipped. If the mask value is 1, the corresponding Q and K values are fetched, and the processing element (PE) array calculates the two remaining fractions (Integer_Q \u00d7 Frac_K and"}, {"title": "V. EVALUATION", "content": "A. Algorithm Evaluation\n1) Evaluation Models and Datasets: To validate the efficacy of our proposed method, HDP, we focused on"}, {"title": "VI. CONCLUSION", "content": "In this work we presented HDP, a novel algorithm- architecture co-design to efficiently run dynamic sparse at- tention models. We first proposed a novel integer-based row- balanced block pruning to prune unimportant blocks in atten- tion matrix and integer based head pruning to prune unimpor- tant heads. Moreover, we propose an approximation method that reduces the computations and performs near-zero pruning. We also implemented this method in 2 co-processors architec- ture, HDP-Edge and HDP-Server, to accelerate algorithm on mobile and sever platforms."}]}