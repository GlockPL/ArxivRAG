{"title": "Towards Fully Autonomous Research Powered by\nLLMS: Case Study on Simulations", "authors": ["Zhihan Liu", "Yubo Chai", "Jianfeng Li"], "abstract": "The advent of Large Language Models (LLMs) has created new opportunities for the automation of scientific research, spanning\nboth experimental processes and computational simulations. This study explores the feasibility of constructing an autonomous\nsimulation agent (ASA) powered by LLM, through sophisticated API integration, to automate the entire research process, from\nexperimental design, remote upload and simulation execution, data analysis, to report compilation. Using a simulation problem\nof polymer chain conformations as a case study, we assessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-40 achieved near-flawless execution on designated research missions,\nunderscoring the potential of LLMs to manage complete scientific investigations autonomously. The outlined automation\ncan be iteratively performed up to twenty cycles without human intervention, illustrating the potential of LLMs for large-scale\nautonomous research endeavors. Additionally, we discussed the intrinsic traits of ASAs in managing extensive tasks, focusing\non self-validation mechanisms and the balance between local attention and global oversight.", "sections": [{"title": "Introduction", "content": "In recent years, the rapid development of artificial intelli-\ngence technology, particularly the emergence of large lan-\nguage models (LLMs), has brought revolutionary changes\nto scientific research[1, 2, 3, 4, 5]. Since the launch of\nChatGPT by OpenAI, LLMs such as GPT-4 and Anthropic's\nClaude have demonstrated outstanding performance in natural\nlanguage processing, text generation, and translation [6, 7].\nThese AI models not only excel in traditional text process-\ning tasks but also show significant potential in scientific re-\nsearch, gradually transforming conventional research meth-\nods and providing scientists with unprecedented tools and\nmethodologies[8, 9, 10, 11].\nThe application of artificial intelligence (AI) in the fields of\nchemistry and materials engineering is continually expanding.\nBy integrating machine learning and automated experimental\ntechnologies, faster experimental design and data analysis can\nbe achieved, leading to the discovery of new materials and\nthe optimization of existing material properties in a shorter\ntimeframe [12, 13, 14]. In the domain of physical chem-\nistry knowledge parsing, Meta's Galactica assists researchers\nwith tasks such as information organization, knowledge in-\nference, and writing, showcasing the immense potential of\nlarge language models in scientific and professional domains\n[13]. Meanwhile, DeepMind's AlphaFold series of algorithms\nenable the prediction of structures for biological molecules,\nfrom proteins and nucleic acids to larger and more complex\ncomplexes [15, 16]. In the fields of chemical synthesis and\ndrug design, AI aids in complex chemical synthesis plan-\nning, chemical property prediction, and the generation of\nnew materials. Integrating LLMs with specialized chemi-\ncal tools and databases can extend their capabilities beyond\ngeneral language tasks [17, 18, 19, 20, 21]. Additionally,\nsome studies have highlighted the crucial role of prompt en-\ngineering in guiding models to produce accurate and useful\noutputs. Techniques such as Chain of Thought, ReAct, and\nTree of Thoughts help structure the model's reasoning process,\nreduce errors, and improve the quality of generated results\n[20, 22, 23, 24].\nRecent research shows that continuous human-AI interac-\ntion can accelerate various stages of scientific research. How-\never, this human-led, AI-assisted model still limits the speed\nand depth of scientific research. Some work has explored\nan AI-dominated research model, such as smart laboratories\n(SDLs) that integrate AI, laboratory automation, and robotics\nto conduct autonomous experiments. These systems can au-\ntomatically adjust and optimize based on intelligent analy-\nsis and decision-making support from experimental results\n[25, 26, 27]. The ultimate goal for future AI scientists is to\nachieve full automation of scientific research processes, from\nexperimental design to data analysis and conclusion genera-\ntion. This end-to-end automation is set to become a major\ndevelopment in chemical research and a primary objective\nin chemical engineering, including both experimental and\nsimulation aspects.\nThis paper explores the AI automation paradigm in polymer\ntheory simulation research. In traditional simulation research,\na supervisor typically discusses and plans the research project\nwith students. The students then write and modify the simula-\ntion program based on physical models, run simulations under\ndifferent parameter conditions on remote servers, collect data,\nanalyze results, and write reports. This paper envisions a new\nresearch paradigm where, after the research has been planned,\nthe subsequent tasks are solely carried out by AI. Using an\nautonomous simulation agent (ASA) powered by LLM, after\nhuman researchers input a research plan (RP), the ASA will"}, {"title": "", "content": "then take on the role of the student to complete the entire\nresearch process. Humans only need to review the AI's results\nat the end, thus achieving a research mode without human\nintervention.\nTo test this research paradigm, we systematically attempted\nASAs powered by different LLMs on a moderately complex\nsimulation problem in polymer physics. This involved an\nAI fully autonomously coding the simulation program, re-\nmote uploading and executing simulations, data organization,\nplotting, and scientific report writing.\nRandom walk and self-avoiding walk models are of signif-\nicant physical importance in describing the spatial configu-\nration of polymer chains. The random walk model assumes\nthat each 'step' of the chain is independent and random, while\nthe self-avoiding walk model considers the volume exclusion\neffects between chain segments. Therefore, they exhibit differ-\nent dynamic behaviors and scaling properties, with different\nscaling relationships between the mean square end-to-end\ndistance and the number of chain segmentss N. This scal-\ning relationship has been theoretically derived by Flory, De\nGennes, and others [28, 29] and validated through simula-\ntions and experiment [30, 31]. This problem is fundamental\nin polymer physics and has been extensively studied. How-\never, for AI, it still presents a novel challenge. The difficulty\nof predicting the scaling exponent makes it neither too easy\nnor prohibitively difficult, thus serving as an excellent model\nproblem for testing AI capabilities. We prompted the AI to\nwrite simulation programs for both random walk and self-\navoiding walk models, compute the mean square end-to-end\ndistance for different chain segment numbers N, fit the results\nto $\\langle R^2 \\rangle \\propto N^{\\nu}$, determine the scaling exponent $\\nu$, plot the\ndata, and write a complete scientific report.\nThe ASA leverages an API automation program (AutoProg)\nto enable multi-round text interactions with LLMs via API\n(Figure 1). Initially, the AutoProg provides an RP witten by\nhuman researchers to an LLM, and the LLM will break the\nplan down into multiple sub-tasks and write Python code to\ncomplete the first sub-task in its response. The AutoProg then\nextracts the Python code, executes it, and returns the execution\nresults and any errors along with the conversation history back\nto the LLM. The LLM then decide whether to modify the\nPython code or to proceed to the following sub-tasks. This\nprocess repeats until the LLM determines that the mission is\ncomplete, at which point the AutoProg stops. Upon mission\ncompletion, humans evaluate its success based on criteria such\nas correct simulation, plotting, etc. The ASA we developed\nis a general system (minor adjustments may be required in\nthe AutoProg for different LLM APIs) that requires only a\nchange in the original RP to be applicable to any simulation\nproblem involving Python programming. In addition to the\nRPs designed for polymer conformation simulations, we have\nalso crafted RPs for solar system planetary orbit simulations\nand asteroid trajectory simulations (Figure 1C and SI).\nWe compared the performance of ASAs powered by various\nLLMs, including GPT-40, and explored common characteris-"}, {"title": "", "content": "tics and issues when LLMs tackle long tasks. These findings\nprovide insights for researchers to better improve and utilize\nLLMs for AI-driven scientific research.\nOverall, this paper presents the first fully autonomous AI-\nconducted simulation research process powered by LLMs. By\nadopting this new research paradigm, we aim to explore the\npotential of AI in highly complex and data-intensive scientific\nresearch, breaking traditional human limitations and advanc-\ning the automation and intelligence of scientific research."}, {"title": "Results", "content": "We designed three simulation missions related to polymer\nchain conformation modeling to demonstrate and test the abil-\nity of ASAs powered by different LLMs to autonomously\ncomplete the entire research process. Each mission includes\nmultiple sub-tasks, covering the full spectrum from conduct-\ning experiments to analyzing data and writing reports. The\nresearch plans (RPs) for these missions are shown in Figure 2.\nAmong these RPs, RP 1 is the simplest involving only basic\nresearch steps for simulations given below.\n\u2022 RP 1 requires generating a Python program to simulate\na random walk, sampling different numbers of chain\nsegments N, deriving the scaling relation $\\langle R^2 \\rangle \\propto N^{\\nu}$,\nsaving chain conformation graphs and scaling relation fit\nplots, and writing a research report.\nWe established several evaluation criteria based on the re-\nquirements of the RP, such as whether graphs and plots were\ngenerated and whether the derived scaling relationships were\ncorrect, to assess task completion. We tested nine ASAS\npowered by different LLMs, including GPT-40, conducting\n20 trials for each RP and recording the achievement of the\nevaluation criteria across these trials, as shown in Figure 3.\nASA-GPT-40 and ASA-GPT-4-Turbo excelled on RP 1,\nachieving near-perfect completion in both simulation and re-\nport writing tasks (Figure 3B). ASA-Claude-3.5 and ASA-\nQwen-2 followed, performing well in simulation tasks but\nsometimes generating reports in response or as text files, fail-\ning to meet RP 1's requirements. Other models performed\nwell in plotting graphs but lacked accuracy in establishing\ncorrect physical models and deriving correct scaling values\nthrough simulations.\nAlthough chain conformation simulations are convenient\nfor comparing different LLMs, they are less convincing due\nto being previously well-studied topics. To make a stronger\ncase, we need more challenging problems. Additionally, to\ndemonstrate that ASA can solve problems across different\nscientific domains, we designed RPs for gravitational simu-\nlation problems corresponding to RP S1 and RP S2 (Figure\nS1 and Figure 1C). The mission of RP S2 is to evaluate the\nrisk posed by a meteorite (or asteroid) to earth, given its ini-\ntial position and velocity and write a report. This mission is\nchallenging even for graduates from physics departments. By"}, {"title": "", "content": "simply inputting RP S2 into the same ASA used for chain con-\nformation problems, the ASA will automatically acquire the\nnecessary parameters of the solar system, code the simulation\nprogram to calculate the asteroid's trajectory, and ultimately\nwrite the report. This result underscores the effectiveness\nof our API programming strategy (see Method section) and\nstrongly demonstrates that ASA can address problems in en-\ntirely different fields.\nRP 2 increases complexity with server interactions and RP\n3 doubles the mission length and simulation difficulty. Brief\nintroductions of these two RPs are shown below (Figure 2).\n\u2022 RP 2 directly provides a random walk simulation pro-\ngram, asking the ASA to modify it, run simulations in\na designated folder on a remote server, download the\nexperimental data, and generate graphs and plots and a\nresearch report.\n\u2022 RP 3 is similar to RP 1 but includes both random walk\nand self-avoiding walk simulations.\nFor RP 2, which involves remote upload and remote simu-\nlations, completion rates across ASAs significantly declined,\nwith some agents consistently failing across multiple trials.\nASA-Claude-3.5 achieved the highest completion rate, suc-\ncessfully completing the entire mission, including generating\nall required graphs in over two-thirds of trials, though it oc-\ncasionally missed displaying some generated graphs in the\nreport. ASA-GPT-40, ASA-GPT-4-Turbo, and ASA-Qwen-2\nalso demonstrated some success in solving complex missions\nover 20 trials (Figure 3).\nRP 3 required simulating a self-avoiding walk based on\nthe random walk simulation in RP 1. Although the pro-\ncess was longer, it was less complex than RP 2, resulting\nin improved performance across agents. ASA-GPT-40, ASA-\nGPT-4-Turbo, ASA-Claude-3.5, and ASA-Gemini-1.5-Pro\nperformed relatively well. Notably, none of the agents pro-\nvided a correct self-avoiding scaling due to incorrect sampling\nmethods during simulation, which will be discussed in detail\nin the Discussion section.\nMoreover, ASA-GPT-40 and ASA-Claude-3.5 demon-\nstrated unique advantages in generating rich content. They\nfrequently produced detailed reports exceeding 1,000 words,\ndelving into the distinctions between RW and SA scaling,\nand occasionally included precise literature citations. ASA-\nClaude-3.5 referenced accurate self-avoiding walk scaling\nrelationships validated through theoretical derivations and\nexperimental verifications, accompanied by brief analyses\nof simulation result deviations. Additionally, ASA-GPT-40\npresented experimental data in enriched formats, such as fre-\nquency distribution plots of end-to-end distances and interac-\ntive web-based charts."}, {"title": "", "content": "Figure 2. Overview of RP 1-3. We designed three RPs, each containing multiple steps such as simulation, plotting, and report\nwriting, and provided them to the ASA. RP 1 and RP 3 required the ASA to generate a complete Python program to simulate\npolymer chains, run the simulation locally, and produce a final report. RP 2 required the ASA to modify the provided simulation\nprogram, run the simulation on a remote server, and then generate a report. The provided simulation program (omitted in the\nfigure) and remote server information were included in RP 2."}, {"title": "Automating Agent Coordination through RP Generation", "content": "In the preceding section, RPs were provided by humans. In\nthis section, we task a Main AI with automatically breaking\ndown a human-provided RP into sub-tasks and distributing\nthem as AI RPs to several Subordinate Als.\nWe provided RP 1 and 2 directly to the Master AI, ad-\njusting the ASA to ensure each sub-task was handled by a\ndistinct Subordinate Al without access to the Main AI's or\nother Subordinate Als' conversation histories. The Main AI\nsolely presents the AI RPs (Figure 4A) and received reports\nupon task completion.\nWe also employed GPT-4-turbo for this experiment. The\nresults (SI-data-2) revealed that for RP 1, the Main AI effec-\ntively decomposes the mission into sub-tasks and successfully\ndistributed them to Subordinate Als, thereby completing RP 1.\nHowever, for RP 2, the Main AI encounters repeated failures.\nThe primary issue lies in precise information transmission.\nRP 2 provides a random walk simulation program and details\nof a remote server, including hostname and username. Despite\nmultiple attempts, the Main AI struggles to convey both the\nsimulation program and server details accurately to the Subor-\ndinate Als (Figure 4B). We attempted to instruct the Main AI\nto include all relevant information in the AI RPs, but this did\nnot notably enhance the accuracy of information transmission.\nAdditionally, we observed instances where AI RPs contained\nirrelevant information, potentially confusing the Subordinate\nAls about the mission's focus."}, {"title": "Automating the Automation: Crafting Multi-tier RPs\nfor Large-Scale Research", "content": "Back in the Automating Basic Simulations section, for each\nASA and each RP, we undertake 20 rounds of 'automation'\nby executing RP 1 and collecting the results including codes\nand files generated by the ASA. Can we automate the entire\nprocess of this study on reaserch automation or can we even\nautomate the automation itself?\nRP 4 is designed for this purpose. It is nested within RP 1,\ninstructing the Primary AI to execute the command \"python\nAutoProg.py -s p1.txt -n i\" 20 times. Each execution generates\nan Agent AI to fulfill RP 1 contained in p1.txt, while the\nPrimary AI collects all generated files and conducts result\nanalysis. The detailed content of RP 4 is depicted in Figure\n4C.\nUsing ASA-GPT-4-Turbo, we conducted tests on RP 4.\nThe results (SI) strikingly demonstrate that the Primary AI\nsuccessfully executed the 20 instances of RP 1, meticulously\norganizing all files into their respective folders and providing\na brief summary after all experiments. During this process, the"}, {"title": "", "content": "AI independently and without interruption wrote 66 programs\n(including 26 error versions), conducted more than 20 simu-\nlations, generated 120 images, and wrote 20 research reports\ntotaling over 5,000 words. Marvelously, all these missions\nwere accomplished without any human intervention. This il-\nlustrates the remarkable capability of multi-tier RP structures\nin handling large-scale missions."}, {"title": "Discussion", "content": "The experiments demonstrate that LLMs, with their robust\ncoding ability [24, 32, 33, 34], can be directly employed in de-\nveloping agents to autonomously complete complex research\nmissions when aided by proper API programming. ASA-\nGPT-40 and ASA-GPT-4-turbo, notably, exhibits near-perfect\ncompletion rates for simpler missions, highlighting their po-\ntential. Multi-tier RP designs effectively handle more complex\nmissions, though accurate information transmission remains\nchallenging for such RPs. Enhancements in RP design and AI\ncoordination are essential for fully harnessing Al's potential\nin automating scientific research. Below, we discuss several\nproblems existing in the current reaserch."}, {"title": "Common Issues in Sequential Task", "content": "Despite varying completion rates among tested ASAs, their\nerrors and failure points share common characteristics, reflect-\ning the inherent logical features of LLMs.\nFirst, some ASAs tend to overlook parts of the RP's re-\nquirements or \"take shortcuts\", not fully following the RP's\ninstructions. For instance, they might skip plotting, fail to\nwrite a report, or ask humans to execute part of the task. These\nissues affect mission completeness and can disrupt the entire\nworkflow. To improve autonomy in AI for science, we need\nto address these issues by continuously generating outputs\ndescribing the current mission status, periodically reviewing\ngenerated data, and confirming strict adherence to mission\nrequirements. These improvements can be integrated through\nAutoProg modifications or by incorporating them into the\n\"system\" prompt input to LLMs or fine-tuning the LLMs.\nSecond, ASAs often underperform on technical aspects of\ntasks due to a lack of specialized domain knowledge. For ex-\nample, when simulating a random walk, the algorithm needs\nto generate unit vectors uniformly distributed on a sphere to\nensure isotropy. Although ASAs can often generate spatially\nrandom unit vectors, they fail to ensure uniform distribution on\nthe sphere. In another example, for simulating self-avoiding\nwalks, ASAs often incorrectly extend the random walk strat-\negy by merely checking distances between points, resulting in\nhigh-energy state samples with large statistical errors. Correct\nsampling should use importance sampling to generate statisti-\ncally significant conformations[35]. To avoid these issues in\nresearch, reliable solutions should be provided in the RPs, or\nrelevant domain knowledge should be used for fine-tuning the\nLLMs to enhance their understanding."}, {"title": "Error Loops in Debugging", "content": "Each AutoProg involves a Python code debugging process.\nWhen bugs are detected, the LLM receives error messages and\nattempts to revise the code, iterating until it executes correctly.\nHowever, we've noted instances where the LLM struggles to\ngenerate functional code, resulting in mission failures. This\ntypically occurs when the LLM becomes \"stuck\" after re-\npeated unsuccessful debugging attempts, cycling through the\nsame erroneous code without progress.\nThis phenomenon may be associated with the LLM's\npropensity to depend heavily on past dialogue content. Re-\nsearch has documented instances where LLMs utilize context\nto deliver responses preferred by users, indicating an exces-\nsive reliance on previous information [36, 37, 38]. While this\ncapability allows LLMs to adapt responses based on short\ndialogue histories, it can also lead to persistent errors.\nIn our experiments, to prevent such deadlocks, we imple-\nmented a maximum debug attempt limit. Once this limit is\nreached, the AutoProg returns a \u201cmission failed\" message. To\nensure the smooth execution of concrete research missions,\nit is crucial to avoid these debugging loops. One solution\ninvolves clearing parts of the memory after a certain number\nof debug attempts or encouraging the LLM to generate varied\ncontent in each iteration (Some LLMs include parameters to\navoid generating duplicate content and to adjust the creativity\nof the models.).\""}, {"title": "Balancing Global Oversight and Local Attention", "content": "In our experiments, ASAs exhibited a significant level of\nglobal oversight, which allows them to track mission progress\nbased on the dialogue history. They can retain details from\nearlier stages of missions even after focusing on lengthy sub-\ntasks. However, as discussed above, ASAs occasionally skip\nsteps in prolonged missions or fail to deliver comprehensive\nsolutions for sub-tasks, indicating a lack of sufficient local\nattention.\nAchieving successful and precise mission completion re-\nquires a delicate balance between global oversight and local\nattention, two perspectives that are normally conflicting.\nIn Automating Agent Coordination section, we designed a\ncollaboration plan where a Main AI and some Subordinate AIs\nhandled different aspects of the mission: the Main AI defined\nsub-task requirements and received reports, while Subordinate\nAls focused on specific sub-tasks. This approach minimized\nredundant information in their dialogue history, enhancing\nthe Main Al's global awareness and the Subordinate AIs'\nlocal attention. While the Main AI effectively completed\nthe mission in RP 1, the Main-Sub AI model encountered\nchallenges in RP 2 due to the failure of precise information\ntransmission. One potential optimization is to involve human-\ndesigned RPs distributed among various Subordinate AIs for\nexecution."}, {"title": "Limitations and Next Steps", "content": "This study demonstrates the capability of LLM powered re-\nsearch agent ASA to independently undertake sophisticated"}, {"title": "Method", "content": "Based on the official documentation of various LLMs, we im-\nplemented the API automation programs (AutoProg) by con-\nfiguring Python API calling methods and setting up the local\nenvironment. This setup included installing necessary Python\npackages like numpy, matplotlib, paramiko, and python-docx\ntailored for our simulation tasks. An AutoProg serves dual\nfunctions: 1) it engages in dialogue with the LLM via the API\nand preserves all historical dialogues, and 2) it extracts and\nexecutes Python programs generated by the LLM in response\nto prompts.\nFor instance, in the case of RP 1, an AutoProg initiates\nthe automated research process by reading mission require-\nments saved in a txt file. It then calls the API to transmit the\nRP to the LLM, instructing it to generate a comprehensive\nPython program. The AutoProg extracts this program from\nthe LLM's response, sends it back to the LLM via the API\nalong with entire dialogue history, and requests error check-\ning and compliance verification against mission requirements.\nSubsequently, the AutoProg executes the validated Python\nprogram, capturing and storing output or errors. In case of\nerrors, the AutoProg sends the prior dialogues and program\nerrors back to the LLM, asking for modifications. This pro-\ncess repeats until the program runs correctly. If errors persist\nbeyond a predefined threshold, the AutoProg halts with a \"mis-\nsion failed\" output. Conversely, upon successful execution, it\nreturns the program output to the LLM, prompting it to check\nthe dialogues and files in the working directory to determine\nthe task progress. If the task remains incomplete, the next\nPython program will be generated by the LLM to continue the\nmission, iterating through checking and debugging processes\nuntil completion. When the mission achieves full execution,\nthe AutoProg concludes with a \"mission complete\" output.\nAutoProg.py and RP.txt, containing predefined instructions,\nare stored in the local working directory. To initiate the auto-\nmated research process for each experiment, users navigate\nto the directory via the command line (CMD) and execute\ncommands such as \u201cpython AutoProg.py -s RP.txt\". Alterna-\ntively, AutoProg can be elevated to a command by creating a\nbatch file (Windows) or shell script (Linux), allowing users\nto simply enter \u201cAutoProg RP.txt\" from any directory to exe-\ncute it. A detailed video recording of the execution process\nis available for reference (SI-video), in which a batch file is\nused to facilitate the execution by running the command \"do"}, {"title": "ASA Scoring via EWM and TOPSIS", "content": "To evaluate the performance of ASAs on designated RPs, we\ndevised specific criteria for RP 1-3, including diagram genera-\ntion, report composition, etc. Through systematic observation\nacross multiple trials, we recorded each agent's fulfillment\nagainst these criteria. For quantitative analysis and scoring,\nwe deployed the Entropy Weight Method (EWM) and TOP-\nSIS (Technique for Order of Preference by Similarity to Ideal\nSolution).\nThe EWM, a well-established approach for determining\nthe relative significance of criteria in decision-making [39],\nwas utilized to assign weights based on data variability. This\nensures criteria with higher variability, and thus more infor-\nmational value, receive higher weights in the final evaluation.\nTOPSIS, originally formulated by Hwang and Yoon [40],\nwas subsequently applied. This method computes the geomet-\nric distance between each agent's performance and both ideal\nand anti-ideal solutions, facilitating a comparative assessment\nof ASA efficacy.\nThe evaluation procedure is delineated as follows.\nInitially, fulfillment data for all ASAs across various cri-\nteria were gathered over 20 trials, encapsulated in matrix X,\nwherein $x_{ij}$ signifies the frequency of criterion j's attainment\nby agent i. Normalization was then performed on $x_{ij}$:\n$r_{ij} = \\frac{x_{ij} - min(x_{ij})}{max(x_{ij}) \u2013 min(x_{ij})},$ (1)\nyielding the normalized matrix $R = [r_{ij}]$. Subsequently, cri-\nterion weights were established utilizing EWM. The relative\nproportion $p_{ij}$ of the criterion j:\n$p_{ij} = \\frac{r_{ij}}{\\Sigma_{i=1}^{m}r_{ij}}$\nThe entropy $e_{j}$ of the criterion j:\n$e_{j} = \u2212k\\Sigma_{i=1}^{m} p_{ij}ln(p_{ij}),$ (3)\nwith $k = 1/ln(m)$, and m denoting the total number of agents.\nThe weight $\\omega_{j}$ of the criterion j:\n$\\omega_{j} = \\frac{d_{j}}{\\Sigma_{i=1}^{n}d_{j}}$\nwith $d_{j} = 1 - e_{j}$ the disparity of the criterion and n the\ntotal number of criteria. Conclusively, agent scores were\nderived via TOPSIS. Multiplication of $r_{ij}$ by its respective\nweight $\\omega_{j}$ yields the weighted normalized matrix V = $[V_{ij}]$"}]}