{"title": "Towards Fully Autonomous Research Powered by LLMS: Case Study on Simulations", "authors": ["Zhihan Liu", "Yubo Chai", "Jianfeng Li"], "abstract": "The advent of Large Language Models (LLMs) has created new opportunities for the automation of scientific research, spanning both experimental processes and computational simulations. This study explores the feasibility of constructing an autonomous simulation agent (ASA) powered by LLM, through sophisticated API integration, to automate the entire research process, from experimental design, remote upload and simulation execution, data analysis, to report compilation. Using a simulation problem of polymer chain conformations as a case study, we assessed the performance of ASAs powered by different LLMs including GPT-4-Turbo. Our findings revealed that ASA-GPT-40 achieved near-flawless execution on designated research missions, underscoring the potential of LLMs to manage complete scientific investigations autonomously. The outlined automation can be iteratively performed up to twenty cycles without human intervention, illustrating the potential of LLMs for large-scale autonomous research endeavors. Additionally, we discussed the intrinsic traits of ASAs in managing extensive tasks, focusing on self-validation mechanisms and the balance between local attention and global oversight.", "sections": [{"title": "Introduction", "content": "In recent years, the rapid development of artificial intelligence technology, particularly the emergence of large language models (LLMs), has brought revolutionary changes to scientific research[1, 2, 3, 4, 5]. Since the launch of ChatGPT by OpenAI, LLMs such as GPT-4 and Anthropic's Claude have demonstrated outstanding performance in natural language processing, text generation, and translation [6, 7]. These AI models not only excel in traditional text processing tasks but also show significant potential in scientific research, gradually transforming conventional research methods and providing scientists with unprecedented tools and methodologies[8, 9, 10, 11].\nThe application of artificial intelligence (AI) in the fields of chemistry and materials engineering is continually expanding. By integrating machine learning and automated experimental technologies, faster experimental design and data analysis can be achieved, leading to the discovery of new materials and the optimization of existing material properties in a shorter timeframe [12, 13, 14]. In the domain of physical chemistry knowledge parsing, Meta's Galactica assists researchers with tasks such as information organization, knowledge inference, and writing, showcasing the immense potential of large language models in scientific and professional domains [13]. Meanwhile, DeepMind's AlphaFold series of algorithms enable the prediction of structures for biological molecules, from proteins and nucleic acids to larger and more complex complexes [15, 16]. In the fields of chemical synthesis and drug design, AI aids in complex chemical synthesis planning, chemical property prediction, and the generation of new materials. Integrating LLMs with specialized chemical tools and databases can extend their capabilities beyond general language tasks [17, 18, 19, 20, 21]. Additionally, some studies have highlighted the crucial role of prompt engineering in guiding models to produce accurate and useful outputs. Techniques such as Chain of Thought, ReAct, and Tree of Thoughts help structure the model's reasoning process, reduce errors, and improve the quality of generated results [20, 22, 23, 24].\nRecent research shows that continuous human-AI interaction can accelerate various stages of scientific research. However, this human-led, AI-assisted model still limits the speed and depth of scientific research. Some work has explored an AI-dominated research model, such as smart laboratories (SDLs) that integrate AI, laboratory automation, and robotics to conduct autonomous experiments. These systems can automatically adjust and optimize based on intelligent analysis and decision-making support from experimental results [25, 26, 27]. The ultimate goal for future AI scientists is to achieve full automation of scientific research processes, from experimental design to data analysis and conclusion generation. This end-to-end automation is set to become a major development in chemical research and a primary objective in chemical engineering, including both experimental and simulation aspects.\nThis paper explores the AI automation paradigm in polymer theory simulation research. In traditional simulation research, a supervisor typically discusses and plans the research project with students. The students then write and modify the simulation program based on physical models, run simulations under different parameter conditions on remote servers, collect data, analyze results, and write reports. This paper envisions a new research paradigm where, after the research has been planned, the subsequent tasks are solely carried out by AI. Using an autonomous simulation agent (ASA) powered by LLM, after human researchers input a research plan (RP), the ASA will"}, {"title": "Results", "content": "then take on the role of the student to complete the entire research process. Humans only need to review the AI's results at the end, thus achieving a research mode without human intervention.\nTo test this research paradigm, we systematically attempted ASAs powered by different LLMs on a moderately complex simulation problem in polymer physics. This involved an AI fully autonomously coding the simulation program, remote uploading and executing simulations, data organization, plotting, and scientific report writing.\nRandom walk and self-avoiding walk models are of significant physical importance in describing the spatial configuration of polymer chains. The random walk model assumes that each 'step' of the chain is independent and random, while the self-avoiding walk model considers the volume exclusion effects between chain segments. Therefore, they exhibit different dynamic behaviors and scaling properties, with different scaling relationships between the mean square end-to-end distance and the number of chain segmentss N. This scaling relationship has been theoretically derived by Flory, De Gennes, and others [28, 29] and validated through simulations and experiment [30, 31]. This problem is fundamental in polymer physics and has been extensively studied. However, for AI, it still presents a novel challenge. The difficulty of predicting the scaling exponent makes it neither too easy nor prohibitively difficult, thus serving as an excellent model problem for testing AI capabilities. We prompted the AI to write simulation programs for both random walk and self-avoiding walk models, compute the mean square end-to-end distance for different chain segment numbers N, fit the results to $\\langle R^2 \\rangle \\propto N^\\nu$, determine the scaling exponent $\\nu$, plot the data, and write a complete scientific report.\nThe ASA leverages an API automation program (AutoProg) to enable multi-round text interactions with LLMs via API (Figure 1). Initially, the AutoProg provides an RP witten by human researchers to an LLM, and the LLM will break the plan down into multiple sub-tasks and write Python code to complete the first sub-task in its response. The AutoProg then extracts the Python code, executes it, and returns the execution results and any errors along with the conversation history back to the LLM. The LLM then decide whether to modify the Python code or to proceed to the following sub-tasks. This process repeats until the LLM determines that the mission is complete, at which point the AutoProg stops. Upon mission completion, humans evaluate its success based on criteria such as correct simulation, plotting, etc. The ASA we developed is a general system (minor adjustments may be required in the AutoProg for different LLM APIs) that requires only a change in the original RP to be applicable to any simulation problem involving Python programming. In addition to the RPs designed for polymer conformation simulations, we have also crafted RPs for solar system planetary orbit simulations and asteroid trajectory simulations (Figure 1C and SI).\nWe compared the performance of ASAs powered by various LLMs, including GPT-40, and explored common characteris-"}, {"title": "Automating Basic Simulations in Research", "content": "tics and issues when LLMs tackle long tasks. These findings provide insights for researchers to better improve and utilize LLMs for AI-driven scientific research.\nOverall, this paper presents the first fully autonomous AI-conducted simulation research process powered by LLMs. By adopting this new research paradigm, we aim to explore the potential of AI in highly complex and data-intensive scientific research, breaking traditional human limitations and advancing the automation and intelligence of scientific research.\nWe designed three simulation missions related to polymer chain conformation modeling to demonstrate and test the ability of ASAs powered by different LLMs to autonomously complete the entire research process. Each mission includes multiple sub-tasks, covering the full spectrum from conducting experiments to analyzing data and writing reports. The research plans (RPs) for these missions are shown in Figure 2. Among these RPs, RP 1 is the simplest involving only basic research steps for simulations given below.\n\u2022 RP 1 requires generating a Python program to simulate a random walk, sampling different numbers of chain segments N, deriving the scaling relation $\\langle R^2 \\rangle \\propto N^\\nu$, saving chain conformation graphs and scaling relation fit plots, and writing a research report.\nWe established several evaluation criteria based on the requirements of the RP, such as whether graphs and plots were generated and whether the derived scaling relationships were correct, to assess task completion. We tested nine ASAs powered by different LLMs, including GPT-40, conducting 20 trials for each RP and recording the achievement of the evaluation criteria across these trials, as shown in Figure 3.\nASA-GPT-40 and ASA-GPT-4-Turbo excelled on RP 1, achieving near-perfect completion in both simulation and report writing tasks (Figure 3B). ASA-Claude-3.5 and ASA-Qwen-2 followed, performing well in simulation tasks but sometimes generating reports in response or as text files, failing to meet RP 1's requirements. Other models performed well in plotting graphs but lacked accuracy in establishing correct physical models and deriving correct scaling values through simulations.\nAlthough chain conformation simulations are convenient for comparing different LLMs, they are less convincing due to being previously well-studied topics. To make a stronger case, we need more challenging problems. Additionally, to demonstrate that ASA can solve problems across different scientific domains, we designed RPs for gravitational simulation problems corresponding to RP S1 and RP S2 (Figure S1 and Figure 1C). The mission of RP S2 is to evaluate the risk posed by a meteorite (or asteroid) to earth, given its initial position and velocity and write a report. This mission is challenging even for graduates from physics departments. By"}, {"title": "Automating Simulations with Remote Server Operations and Volume Exclusion", "content": "simply inputting RP S2 into the same ASA used for chain conformation problems, the ASA will automatically acquire the necessary parameters of the solar system, code the simulation program to calculate the asteroid's trajectory, and ultimately write the report. This result underscores the effectiveness of our API programming strategy (see Method section) and strongly demonstrates that ASA can address problems in entirely different fields.\nRP 2 increases complexity with server interactions and RP 3 doubles the mission length and simulation difficulty. Brief introductions of these two RPs are shown below (Figure 2).\n\u2022 RP 2 directly provides a random walk simulation program, asking the ASA to modify it, run simulations in a designated folder on a remote server, download the experimental data, and generate graphs and plots and a research report.\n\u2022 RP 3 is similar to RP 1 but includes both random walk and self-avoiding walk simulations.\nFor RP 2, which involves remote upload and remote simulations, completion rates across ASAs significantly declined, with some agents consistently failing across multiple trials. ASA-Claude-3.5 achieved the highest completion rate, successfully completing the entire mission, including generating all required graphs in over two-thirds of trials, though it occasionally missed displaying some generated graphs in the report. ASA-GPT-40, ASA-GPT-4-Turbo, and ASA-Qwen-2 also demonstrated some success in solving complex missions over 20 trials (Figure 3).\nRP 3 required simulating a self-avoiding walk based on the random walk simulation in RP 1. Although the process was longer, it was less complex than RP 2, resulting in improved performance across agents. ASA-GPT-40, ASA-GPT-4-Turbo, ASA-Claude-3.5, and ASA-Gemini-1.5-Pro performed relatively well. Notably, none of the agents provided a correct self-avoiding scaling due to incorrect sampling methods during simulation, which will be discussed in detail in the Discussion section.\nMoreover, ASA-GPT-40 and ASA-Claude-3.5 demonstrated unique advantages in generating rich content. They frequently produced detailed reports exceeding 1,000 words, delving into the distinctions between RW and SA scaling, and occasionally included precise literature citations. ASA-Claude-3.5 referenced accurate self-avoiding walk scaling relationships validated through theoretical derivations and experimental verifications, accompanied by brief analyses of simulation result deviations. Additionally, ASA-GPT-40 presented experimental data in enriched formats, such as frequency distribution plots of end-to-end distances and interactive web-based charts."}, {"title": "Automating Agent Coordination through RP Generation", "content": "In the preceding section, RPs were provided by humans. In this section, we task a Main AI with automatically breaking down a human-provided RP into sub-tasks and distributing them as AI RPs to several Subordinate Als.\nWe provided RP 1 and 2 directly to the Master AI, adjusting the ASA to ensure each sub-task was handled by a distinct Subordinate Al without access to the Main AI's or other Subordinate Als' conversation histories. The Main AI solely presents the AI RPs (Figure 4A) and received reports upon task completion.\nWe also employed GPT-4-turbo for this experiment. The results (SI-data-2) revealed that for RP 1, the Main AI effectively decomposes the mission into sub-tasks and successfully distributed them to Subordinate Als, thereby completing RP 1. However, for RP 2, the Main AI encounters repeated failures. The primary issue lies in precise information transmission. RP 2 provides a random walk simulation program and details of a remote server, including hostname and username. Despite multiple attempts, the Main AI struggles to convey both the simulation program and server details accurately to the Subordinate Als (Figure 4B). We attempted to instruct the Main AI to include all relevant information in the AI RPs, but this did"}, {"title": "Automating the Automation: Crafting Multi-tier RPs for Large-Scale Research", "content": "not notably enhance the accuracy of information transmission. Additionally, we observed instances where AI RPs contained irrelevant information, potentially confusing the Subordinate Als about the mission's focus.\nBack in the Automating Basic Simulations section, for each ASA and each RP, we undertake 20 rounds of 'automation' by executing RP 1 and collecting the results including codes and files generated by the ASA. Can we automate the entire process of this study on reaserch automation or can we even automate the automation itself?\nRP 4 is designed for this purpose. It is nested within RP 1, instructing the Primary AI to execute the command \"python AutoProg.py -s p1.txt -n i\" 20 times. Each execution generates an Agent AI to fulfill RP 1 contained in p1.txt, while the Primary AI collects all generated files and conducts result analysis. The detailed content of RP 4 is depicted in Figure 4C.\nUsing ASA-GPT-4-Turbo, we conducted tests on RP 4. The results (SI) strikingly demonstrate that the Primary AI successfully executed the 20 instances of RP 1, meticulously organizing all files into their respective folders and providing a brief summary after all experiments. During this process, the"}, {"title": "Discussion", "content": "AI independently and without interruption wrote 66 programs (including 26 error versions), conducted more than 20 simulations, generated 120 images, and wrote 20 research reports totaling over 5,000 words. Marvelously, all these missions were accomplished without any human intervention. This illustrates the remarkable capability of multi-tier RP structures in handling large-scale missions.\nThe experiments demonstrate that LLMs, with their robust coding ability [24, 32, 33, 34], can be directly employed in developing agents to autonomously complete complex research missions when aided by proper API programming. ASA-GPT-40 and ASA-GPT-4-turbo, notably, exhibits near-perfect completion rates for simpler missions, highlighting their potential. Multi-tier RP designs effectively handle more complex missions, though accurate information transmission remains challenging for such RPs. Enhancements in RP design and AI coordination are essential for fully harnessing Al's potential in automating scientific research. Below, we discuss several problems existing in the current reaserch."}, {"title": "Common Issues in Sequential Task", "content": "Despite varying completion rates among tested ASAs, their errors and failure points share common characteristics, reflecting the inherent logical features of LLMs.\nFirst, some ASAs tend to overlook parts of the RP's requirements or \"take shortcuts\", not fully following the RP's instructions. For instance, they might skip plotting, fail to write a report, or ask humans to execute part of the task. These issues affect mission completeness and can disrupt the entire workflow. To improve autonomy in AI for science, we need to address these issues by continuously generating outputs describing the current mission status, periodically reviewing generated data, and confirming strict adherence to mission requirements. These improvements can be integrated through AutoProg modifications or by incorporating them into the \"system\" prompt input to LLMs or fine-tuning the LLMs.\nSecond, ASAs often underperform on technical aspects of tasks due to a lack of specialized domain knowledge. For example, when simulating a random walk, the algorithm needs to generate unit vectors uniformly distributed on a sphere to ensure isotropy. Although ASAs can often generate spatially random unit vectors, they fail to ensure uniform distribution on the sphere. In another example, for simulating self-avoiding walks, ASAs often incorrectly extend the random walk strategy by merely checking distances between points, resulting in high-energy state samples with large statistical errors. Correct sampling should use importance sampling to generate statistically significant conformations[35]. To avoid these issues in research, reliable solutions should be provided in the RPs, or relevant domain knowledge should be used for fine-tuning the LLMs to enhance their understanding."}, {"title": "Error Loops in Debugging", "content": "Each AutoProg involves a Python code debugging process. When bugs are detected, the LLM receives error messages and attempts to revise the code, iterating until it executes correctly. However, we've noted instances where the LLM struggles to generate functional code, resulting in mission failures. This typically occurs when the LLM becomes \"stuck\" after repeated unsuccessful debugging attempts, cycling through the same erroneous code without progress.\nThis phenomenon may be associated with the LLM's propensity to depend heavily on past dialogue content. Research has documented instances where LLMs utilize context to deliver responses preferred by users, indicating an excessive reliance on previous information [36, 37, 38]. While this capability allows LLMs to adapt responses based on short dialogue histories, it can also lead to persistent errors.\nIn our experiments, to prevent such deadlocks, we implemented a maximum debug attempt limit. Once this limit is reached, the AutoProg returns a \u201cmission failed\" message. To ensure the smooth execution of concrete research missions, it is crucial to avoid these debugging loops. One solution involves clearing parts of the memory after a certain number of debug attempts or encouraging the LLM to generate varied content in each iteration (Some LLMs include parameters to avoid generating duplicate content and to adjust the creativity of the models.)."}, {"title": "Balancing Global Oversight and Local Attention", "content": "In our experiments, ASAs exhibited a significant level of global oversight, which allows them to track mission progress based on the dialogue history. They can retain details from earlier stages of missions even after focusing on lengthy sub-tasks. However, as discussed above, ASAs occasionally skip steps in prolonged missions or fail to deliver comprehensive solutions for sub-tasks, indicating a lack of sufficient local attention.\nAchieving successful and precise mission completion requires a delicate balance between global oversight and local attention, two perspectives that are normally conflicting.\nIn Automating Agent Coordination section, we designed a collaboration plan where a Main AI and some Subordinate AIs handled different aspects of the mission: the Main AI defined sub-task requirements and received reports, while Subordinate Als focused on specific sub-tasks. This approach minimized redundant information in their dialogue history, enhancing the Main Al's global awareness and the Subordinate AIs' local attention. While the Main AI effectively completed the mission in RP 1, the Main-Sub AI model encountered challenges in RP 2 due to the failure of precise information transmission. One potential optimization is to involve human-designed RPs distributed among various Subordinate AIs for execution."}, {"title": "Limitations and Next Steps", "content": "This study demonstrates the capability of LLM powered research agent ASA to independently undertake sophisticated"}, {"title": "Conclusion", "content": "GPT-40 demonstrate high completion rates for missions, a strong grasp of task requirements, and consistent alignment with overall objectives across extensive tasks. Notably, the performance exhibited in completing RP 4, entailing a hierarchy of nested tasks, surpassed expectations in a significant manner. These capabilities underscore the readiness of ASAs to independently manage long-term scientific research missions, offering a new perspective on AI applications in science as well as in chemical engineering."}, {"title": "Method", "content": "Based on the official documentation of various LLMs, we implemented the API automation programs (AutoProg) by configuring Python API calling methods and setting up the local environment. This setup included installing necessary Python packages like numpy, matplotlib, paramiko, and python-docx tailored for our simulation tasks. An AutoProg serves dual functions: 1) it engages in dialogue with the LLM via the API and preserves all historical dialogues, and 2) it extracts and executes Python programs generated by the LLM in response to prompts.\nFor instance, in the case of RP 1, an AutoProg initiates the automated research process by reading mission requirements saved in a txt file. It then calls the API to transmit the RP to the LLM, instructing it to generate a comprehensive Python program. The AutoProg extracts this program from the LLM's response, sends it back to the LLM via the API along with entire dialogue history, and requests error checking and compliance verification against mission requirements. Subsequently, the AutoProg executes the validated Python program, capturing and storing output or errors. In case of errors, the AutoProg sends the prior dialogues and program errors back to the LLM, asking for modifications. This process repeats until the program runs correctly. If errors persist beyond a predefined threshold, the AutoProg halts with a \"mission failed\" output. Conversely, upon successful execution, it returns the program output to the LLM, prompting it to check the dialogues and files in the working directory to determine the task progress. If the task remains incomplete, the next Python program will be generated by the LLM to continue the mission, iterating through checking and debugging processes until completion. When the mission achieves full execution, the AutoProg concludes with a \"mission complete\" output.\nAutoProg.py and RP.txt, containing predefined instructions, are stored in the local working directory. To initiate the automated research process for each experiment, users navigate to the directory via the command line (CMD) and execute commands such as \u201cpython AutoProg.py -s RP.txt\". Alternatively, AutoProg can be elevated to a command by creating a batch file (Windows) or shell script (Linux), allowing users to simply enter \u201cAutoProg RP.txt\" from any directory to execute it. A detailed video recording of the execution process is available for reference (SI-video), in which a batch file is used to facilitate the execution by running the command \"do"}, {"title": "ASA Scoring via EWM and TOPSIS", "content": "AI4S_prj1.txt\u201d.\nTo evaluate LLM performance across RP 1-3, each ASA underwent 20 tests per RP, with all generated files and Auto-Prog outputs saved. Sample results are provided in the data (SI-data-1).\nTo evaluate the performance of ASAs on designated RPs, we devised specific criteria for RP 1-3, including diagram generation, report composition, etc. Through systematic observation across multiple trials, we recorded each agent's fulfillment against these criteria. For quantitative analysis and scoring, we deployed the Entropy Weight Method (EWM) and TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution).\nThe EWM, a well-established approach for determining the relative significance of criteria in decision-making [39], was utilized to assign weights based on data variability. This ensures criteria with higher variability, and thus more informational value, receive higher weights in the final evaluation. TOPSIS, originally formulated by Hwang and Yoon [40], was subsequently applied. This method computes the geometric distance between each agent's performance and both ideal and anti-ideal solutions, facilitating a comparative assessment of ASA efficacy.\nThe evaluation procedure is delineated as follows.\nInitially, fulfillment data for all ASAs across various criteria were gathered over 20 trials, encapsulated in matrix X, wherein $x_{ij}$ signifies the frequency of criterion j's attainment by agent i. Normalization was then performed on $x_{ij}$:\n$r_{ij} = \\frac{x_{ij} - min(x_{ij})}{max(x_{ij}) \u2013 min(x_{ij})}$  (1)\nyielding the normalized matrix R = $[r_{ij}]$. Subsequently, criterion weights were established utilizing EWM. The relative proportion $p_{ij}$ of the criterion j:\n$p_{ij} = \\frac{r_{ij}}{\\Sigma_{i=1}^m r_{ij}}$   (2)\nThe entropy $e_j$ of the criterion j:\n$e_j = -k \\Sigma_{i=1}^m p_{ij}ln(p_{ij})$,   (3)\nwith k = 1/ln(m), and m denoting the total number of agents. The weight $\\omega_j$ of the criterion j:\n$\\omega_j = \\frac{d_j}{\\Sigma_{j=1}^n d_j}$   (4)\nwith $d_j$ = 1 - $e_j$ the disparity of the criterion and n the total number of criteria. Conclusively, agent scores were derived via TOPSIS. Multiplication of $r_{ij}$ by its respective weight $\\omega_j$ yields the weighted normalized matrix V = [$V_{ij}$]"}]}