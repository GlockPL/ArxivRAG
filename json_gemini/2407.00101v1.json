{"title": "Hybrid Approach to Parallel Stochastic Gradient Descent", "authors": ["Aakash Sudhirbhai Vora", "Dhrumil Chetankumar Joshi", "Aksh Kantibhai Patel"], "abstract": "Stochastic Gradient Descent is used for large datasets to train models to reduce the training time. On top of that data parallelism is widely used as a method to efficiently train neural networks using multiple worker nodes in parallel. Synchronous and asynchronous approach to data parallelism is used by most systems to train the model in parallel. However, both of them have their drawbacks. We propose a third approach to data parallelism which is a hybrid between synchronous and asynchronous approaches, using both approaches to train the neural network. When the threshold function is selected appropriately to gradually shift all parameter aggregation from asynchronous to synchronous, we show that in a given time period our hybrid approach outperforms both asynchronous and synchronous approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural networks play an important role in modern computer applications. They are indispensable from day to day life today. They are the backbone of recommender systems, stock trading algorithms, voice assistants, autonomous driving, etc. Neural networks are trained using Gradient Descent, where the motive is to minimize the loss function by improving model parameters, iterating over the training data. Training the neural network on the whole dataset as input once is a slow process since the training dataset could be huge. An alternative to this is Stochastic Gradient Descent, where data is passed in batches to train the network. So, Stochastic Gradient Descent is widely accepted as the method to train neural network because it is more efficient.\nThis process of training neural networks can be accelerated even further by using distributed training where there are multiple worker nodes updating the model parameters in parallel. There are multiple types of parallelism techniques used for training a model. Model parallelism involves splitting the model across different workers, so that each worker control over some parameters of the model. Each worker finds the parameters for the part of model it has and then all the parameters are aggregated to get the final model.Petuum proposed by Xing et al (2015) [10], Project Adam proposed by Chilimbi et. al (2014) [1] and Sandblaster proposed by Dean et. al (2012) [2] all use model parallelism to speed up neural network training.\nAnother approach that is used for parallelizing training is known as pipeline parallelism. Fundamental idea of pipeline parallelism to overcome the shortcoming of model parallelism that when a set of workers compute weights for one layer, other workers should not be sitting idly. So, apart from splitting the model, multiple inputs are supplied to the system so that at each point of time, each worker is computing the weights based on one set of inputs. Each worker node is responsible for computing weights for one or more layers. Once the worker computers parameters for the assigned layer, it passes that information to next set of workers responsible for other layers in the network. The same worker is responsible for computing the weights during both feed forward and backpropagation phases of training the model. Model gets divided among workers in the direction of data flow. GPipe proposed by Huang et. al (2019) [4] and Pipedream proposed by Narayanan et. al (2019) [7] both make use of pipeline parallelism to speed up the training process.\nData parallelism is another approach that people use for speeding up neural network training. Core idea for this technique is that the training dataset is split among different workers and each worker trains the model in parallel based on the dataset it has. Thereafter, the parameters are combined from the workers to get the resultant model. Here for parameters to converge, they need to combined. Currently there are two approaches to applying data parallelism, one is synchronous approach. Here, the workers working in parallel need to communicate among themselves to keep the parameters synchronized. One way of synchronizing parameters is to keep the parameters synchronized after each iteration. Another approach is to synchronize parameters after some fixed number of iterations. Popular implementations of synchronous approach to data parallelism are Petuum proposed by Xing et al (2015) [10], Horovod proposed by Sergeev et al (2018) [9] and Stale Synchronous Parallel Parameter Server proposed by Ho et al (2013) [3]. There is a clear drawback to this approach where faster workers will have to wait for slower workers to catch up. This leads to wastage of resources since faster workers will be sitting idle while slower workers catch up.There is also communication overhead in this approach as workers will coordinate amongst themselves to keep the parameters synchronized. Alternative to this, is the asynchronous approach. Here, workers do not have to communicate amongst each other to keep parameters synchronized. Each worker is allowed to update the parameters independently with little communication needed. This will lead to better resource utilization since the faster workers will not have to wait for slower workers. Popular implementations of this are Hogwild proposed by Niu et al (2011) [8], Project Adam proposed by Chilimbi et al (2014) [1] and Distbelief proposed by Dean et al (2012) [2]. However, there is a major drawback regarding parameter convergence. If the model parameters are not sparse, convergence might take a lot of time since faster workers would frequently update the parameters while slower workers would still be operating on stale parameters.\nWe propose a hybrid approach which incorporates advantages of both approaches. This involves using both synchronous and asynchronous approaches. Initially, each worker is computing parameters asynchronously. However as iterations progress, more and more parameters are allowed to be accumulated and passed onto the workers. This approach allows workers to use an asynchronous approach to get more progress per iteration and a synchronous approach to get more confident progress."}, {"title": "2 RELATED WORK", "content": "To increase the effectiveness of neural network training over large training sets, a number of strategies have been put forward. Due to its effectiveness in handling huge datasets, stochastic gradient descent (SGD) has been largely favored over gradient descent. In order to increase efficiency even more, distributed training techniques have been investigated. These techniques let worker nodes update model parameters concurrently.\nSynchronous parallel gradient descent is one of the current training methods for neural networks. This method synchronizes all worker nodes either instantly or after a predetermined number of iterations. The output parameters are closely matched to those acquired by sequential stochastic gradient descent thanks to this synchronization. This strategy has a big drawback in that, faster workers must wait for slower workers to catch up, which adds a lot of idle time. The work \"More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server\" [3] offers a synchronous parallel gradient descent approach with a parameter server, with updates that are slightly stale. However, the disadvantage is that faster workers need to wait for slower workers to catch up, resulting in idle time. The publication \"Horovod: Fast and Easy Distributed Deep Learning in TensorFlow\" [9] describes a distributed training system that employs synchronous parallel gradient descent in conjunction with a ring-based communication technique called ring-all reduce. With features like gradient compression, it enhances scalability and speed. Fast and efficient training is one of the benefits, although it shares the disadvantage of idle time with slower workers. The paper \"Petuum: A New Platform for Distributed Machine Learning on Big Data\" [10] introduces Petuum, a distributed machine learning platform that uses synchronous parallel gradient descent with a parameter server. It makes use of techniques like model compression and a customized communication protocol. Large-scale machine learning jobs can be handled efficiently, but it also suffers from idle time because of slower workers.\nAsynchronous parallel gradient descent is another contempo-rary strategy in which each worker separately updates parameters in parallel. Because workers do not have to wait for each other, this strategy optimizes the utilization of resources. However, convergence will only be achievable for sparse modifications, and individual worker parameters could become outdated. The paper, \"HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent,\" [8] introduces a technique that uses a lock-free mechanism to parallelize stochastic gradient descent. It enables independent model parameter updates across several threads without explicit synchronization. While this approach effectively parallelized SGD minimizing synchronization overhead and resource usage, the method's assumption of sparse updates makes it potentially inconclusive for dense updates. Additionally, because there is no explicit synchronization, individual threads may use outdated parameters, which could have an effect on the training process convergence and accuracy. \"Project Adam: Building a Scalable and Efficient Deep Learning Training System\" [1] also uses asynchronous parallel SGD to allow worker nodes to update simultaneously. To share information among workers and maintain uniformity, it makes use of a communication protocol. Individual workers' use of out-of-date criteria can have an impact on how well training is performed generally, sometimes producing less-than-ideal results. Existing techniques exhibit various trade-offs in terms of synchronization and convergence assurances."}, {"title": "3 PROBLEM FORMULATION", "content": "Let there exist X which is a set of points sample over some unknown distribution defined over the space of n dimension i.e. R\". Corresponding to each point x \u2208 X, there exists a value y sampled from another set Y which is a subset of Rm. The goal is to find a function f : R\u2033 \u00d7 Rk \u2192 Rm defined on X and parameter 0, where \u03b8 \u2208 Rk such that it can approximate the unknown function fun : Rn \u2192 Rm which maps each point in x to corresponding y. Further, we assume that there exists a differentiable convex function j(x, y, 0) defined over Rn \u00d7 Rm \u00d7 Rk \u2192 R. Let a new function J be defined for any subset of X which applies j to all x \u2208 X and returns a summation of them. The function f can be estimated by finding the value of the parameter theta which minimizes the function J(X, Y, 0). This minimization can be performed by gradient-based methods like Stochastic Gradient Descent.\n$$J(X, Y, \\theta) = \\sum_{x_i\\in X,y_i\\in Y} j(x_i, y_i, \\theta)$$\n$$\\theta = \\underset{\\theta\\in R^k}{argmin}J(X, Y, \\theta)$$\n$$\\theta = \\theta_t - \\eta \\frac{\\partial J(X, Y, \\theta)}{\\partial \\theta}$$\nHowever, we have multiple processors at our disposal that we would like to use for performing the minimization. We assign one of the machines to be in charge of maintaining the current values of the parameter and call it the Parameter server p. Rest of the machines are known as worker machines wi. Each of the worker machines wi has a subset of data (Xi, Yi) from the entire dataset (X, Y). They are responsible for getting the current parameter values, calculating the gradient using (Xi, Yi), and sending them to the Parameter server p. We don't assume any kind of global notion of time among these workers. Furthermore, we assume that the execution speed of each worker is different and there also exists communication delays when workers are communicating with the Parameter Server.\nIn such a setting, it becomes necessary to define whether the policy is used by the Parameter server for updating the parameters from the gradients received from the worker. Two widely used approaches are synchronous and asynchronous. In the synchronous approach, the parameter collects gradients from all workers before updating the parameter and each worker waits till updated parameters are not received. Each update in this approach is calculated by all workers on the same previous values of parameters and hence the updates are less noisy. However, this algorithm is slower since faster workers have to wait for slower ones. On the other hand, in an asynchronous approach, any parameter applies the update as soon as it is received from the worker and provides a new value of the parameter to the worker. Though this approach is simple and works very well in practice during the initial convergence(reach quickly to local minima), as we move closer to local minima, faster workers might end up providing too many stale updates which might lead to noisy updates and slow down convergence. Thus, we ask the question, is there a way that combines good properties from both synchronous and asynchronous algorithms i.e. high confidence updates and faster initial convergence respectively, and gives a new algorithm? We hypothesize that if we start initially with asynchronous updates and switch over time to synchronous updates, it would lead to faster convergence."}, {"title": "4 METHODOLOGY: SMOOTH SWITCH ALGORITHM", "content": "To develop a hybrid algorithm, which we call a smooth switch al-gorithm, for stochastic gradient descent (SGD) algorithms, both synchronous and asynchronous techniques are combined. With this method, asynchronous updates are used to increase iteration throughput while synchronous updates are used to ensure the integrity of forward positive progress. The method defines a smooth transition from the asynchronous to the synchronous approach which is controlled by the threshold function. Asynchronicity will help in achieving more updates per iteration and synchronicity will help improve accuracy per iteration. This two-pronged method offers a fair trade-off between training speed and accuracy by combining the advantages of synchronous and asynchronous updates.\nAs the training process iterates, the threshold gradually increases. During each iteration, multiple gradients (G1, G2, G3, ... Gk) are accumulated in the gradient buffer, and at each iteration, the algorithm evaluates if the gradients accumulated are greater than or equal to the threshold to trigger the transition to synchronous updates to the parameter server. If the threshold has not been reached, asynchronous updates continue. Once the threshold is reached, synchronous gradient updates are performed to the parameter server. The steps are repeated until convergence or a specified number of iterations. The algorithm 1 demonstrates the entire process."}, {"title": "Algorithm 1", "content": "Step 1: Set the initial gradient buffer to the parameter database transfer threshold (K) to a very low value to allow for asynchronous updates (stale reads).\nStep 2:\nwhile (!(convergence) or !(a specified number of iterations)) do\n(1) If the total gradients in the gradient buffer >= threshold K then synchronize all the gradients in the gradient buffer with the Parameter Server (reduction in stale reads).\n(2) If the threshold has not been reached, continue with asynchronous updates to the Parameter Server. (Might cause frequent stale reads)\n(3) Gradually increase the threshold (K) as the iterations progress.\n(4) Accumulate multiple gradients (G1, G2, G3, ... Gn) during each iteration."}, {"title": "5 DATA PREPARATION", "content": "As part of our research experiment, we conducted evaluations to assess the performance and generalization capabilities of our approach to Stochastic Gradient Descent (SGD) using a diverse range of datasets. Among these datasets, we utilized the MNIST dataset [6] figure 2 and the CIFAR-10 dataset [5] figure 3.\nA curated collection of grayscale images showing handwritten numbers makes up the MNIST dataset. It serves as a well-known benchmark dataset for tasks involving image classification in which the goal is to correctly identify the represented digit in each image. The MNIST dataset is a rich source of labeled data with 60,000 training images and 10,000 test images, providing a significant quantity of resources for testing and training machine learning algorithms."}, {"title": "6 IMPLEMENTATION AND EXPERIMENTS", "content": "The goal of our experiment is to validate our hypothesis that the proposed algorithm provides a speedup in convergence as compared to both completely synchronous and asynchronous versions of distributed stochastic gradient descent. Hence, in all our experiments, we ran all three algorithms namely, our proposed algorithm, synchronous and asynchronous algorithms for the same initial conditions, and collected values of training loss, testing loss, and testing accuracy at various time intervals.\nAll our experiments were executed in a clustered environment setup. The cluster was provided with 2 CPUs each with 14 cores which makes 28 cores in total. The CPUs were based on 64-bit x86 architecture and the model name was Intel(R) Xeon(R) CPU E5-2680 v4 with 2.4GHz clock speed. The kernel version was 3.10.0-1160.21.1.el7.x86_64 and the operating system was CentOS Linux 7. The available RAM on the cluster was 263.85 GB.\nThe code for all algorithms was written in Python language with version 3.9.12. For parallel execution and scheduling, we used the ray library with version 2.4.0. Pytorch 2.0.0 was used for model creation and training. For the training of the model, 25 gradient workers were used for calculating the gradient and they passed the updates to a worker which acted as Parameter Server. Further, during the training, to simulate the communication delays and faster/slower workers, we randomly introduced execution delays in 50% gradient workers. The execution delays were sampled randomly from a normal distribution with a mean of 0 and a standard deviation of 0.25 during each gradient calculated by the worker.\nFor testing our approach, we selected MNIST and CIFAR-10 datasets. For our algorithm, we used the step function as our threshold function which updated the threshold with an increase in the number of gradient updates. We executed our experiments for combinations of step sizes in multiples of 3 and 5 of reciprocal of learning rate and batch size of 32 and 64. For each combination, we trained the model for 5 rounds starting from random initialization using our algorithm, asynchronous and synchronous algorithm. For each round, the same initialization values of weights were used for each algorithm. The training was performed for 100 seconds in each round. Since both datasets contain images, CNN was used as the model. For experiments, we fixed the learning rate to 0.01. Since we are creating a model to solve the classification problem, negative log-likelihood loss is used.\nFurthermore, we wanted to analyze the effects of various step sizes, batch sizes, and communication delays on our algorithm. Hence, we repeated our experiments for different combinations of step size, batch size, and communication delays and noted training and testing loss along with testing accuracy during different intervals. For this purpose, we used randomly generated datasets with 20 dimensions and 10 classes containing 10k samples with 80:20 train to test split. A newly sampled dataset was used for each configuration. The reason behind selecting a random dataset was to cover a wide range of classification problems and validate the robustness of our algorithm."}, {"title": "7 RESULTS AND DISCUSSION", "content": "In this section, we present the results of various experiments and analyses of the same. First, we present how our algorithm works on the MNIST and CIFAR-10 datasets. After that, we also present the effects of various choices of step sizes, batch sizes, and communication delays on our algorithm."}, {"title": "7.1 Results on MNIST and CIFAR-10 dataset", "content": "Plots 4 and 5 shows the average values of testing accuracy, testing loss, and training loss for five rounds of training from random initialization on the MNIST dataset. It can be seen clearly that our algorithm maintains the lead in terms of accuracy and loss as compared to both asynchronous and synchronous versions. The same trend is observed for all the combinations of batch sizes and step sizes. However, the speed gain by our algorithm over the asynchronous version is not that significant, we believe that MNIST poses a simple optimization problem that does not bring out problems of asynchronous algorithm effectively. Table 1 shows the difference of the metrics like accuracy and loss between our algorithm and asynchronous algorithm averaged over the entire training interval. For better performance, the difference in accuracy should be positive and that loss should be negative."}, {"title": "7.2 Effect of different batch sizes", "content": "Further, we wanted to understand how different values of batch sizes affect the efficiency of our approach. For each of the batch sizes, we executed 5 rounds of training, each with different initialization of the parameters on the randomly generated dataset. Table 3 shows the difference of the metrics like accuracy and loss between our algorithm and asynchronous algorithm averaged over the entire training interval. We hypothesized that as the batch size increases, the difference should decrease since asynchronous algorithms start providing updates with high confidence. This can be also validated by the trend observed in the plot 8."}, {"title": "7.3 Effect of different step sizes", "content": "We also executed experiments to investigate the effects of step sizes on the performance of our algorithm. Again, we repeated the experiment for various step sizes that are multiples of the reciprocal of the learning rate. Table 4 shows the difference between metrics for our algorithm and asynchronous averaged over the training interval. Ideally, for smaller step sizes, our algorithm performs similarly to the synchronous algorithm. As we increase the step sizes, the behavior shifts towards the asynchronous algorithm. The plot 9 shows the relation between step size and the performance of the algorithm."}, {"title": "7.4 Effect of communication delay", "content": "As mentioned in the experiment section, we introduce communication delays by adding random execution delays to the worker. Communication delays are the major reason for the slowness of the synchronous algorithm. However, such a problem is not observed in the asynchronous algorithm, and for our algorithm, we believe that for a good choice of step size and batch size, it should be resilient to communication delay. The table 5 and plot 10, show the results from the experiment with varying delay distribution (normal distribution with mean 0 and different standard deviation) and we observe that for all values, our algorithm outperformed the asynchronous version."}, {"title": "8 CONCLUSION", "content": "In summary, we can say that our approach looks to combine the best of both worlds. It uses the asynchronous approach to gain speed and synchronous approach for accuracy. And the switch from asynchronous to synchronous is made using a threshold function which takes the learning rate into account for determining how many parameters get aggregated for synchronization. Based on the extensive experimentation, we can draw the conclusion that for the same time period, our approach leads to better accuracy and lower loss than both synchronous and asynchronous approaches. As batch size decreases, which is the norm for large training datasets, our approach performs even better. The step size is currently defined in terms of the learning rate and even if the step size is not selected appropriately, our approach will give better results than the synchronous approach."}, {"title": "9 FUTURE WORK", "content": "This approach can be tested on different CPU architectures with smaller memory size and processing power to see what impact it has on the overall performance. Another challenge is to check whether it would work with a non convex loss function since such a function will have multiple local minimas. We can also test this approach with more complex loss functions and model architectures to check its robustness. It can also be tested with larger datasets to see how an increase in dataset size affects performance. Currently, finding the threshold for aggregating parameters is based upon experimental data. However, a good heuristic can be devised which can form a base for selecting aggregation the threshold for different types of models and datasets. Different monotonically increasing functions can also be used to see if the all such functions can be straightaway plugged in without much change in performance. Formal proofs can also be derived for the convergence of the parameters even though the approach is not completely synchronous."}]}