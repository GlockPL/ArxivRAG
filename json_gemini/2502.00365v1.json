{"title": "What should an AI assessor optimise for?", "authors": ["Daniel Romero-Alvarado", "Fernando Mart\u00ednez-Plumed", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "abstract": "An Al assessor is an external, ideally independent system that predicts an indicator, e.g., a loss value, of another AI system. Assessors can leverage information from the test results of many other Al systems and have the flexibility of being trained on any loss function or scoring rule: from squared error to toxicity metrics. Here we address the question: is it always optimal to train the assessor for the target metric? Or could it be better to train for a different metric and then map predictions back to the target metric? Using twenty regression and classification problems with tabular data, we experimentally explore this question for, respectively, regression losses and classification scores with monotonic and non-monotonic mappings and find that, contrary to intuition, optimising for more informative metrics is not generally better. Surprisingly, some monotonic transformations are promising. For example, the logistic loss is useful for minimising absolute or quadratic errors in regression, and the logarithmic score helps maximise quadratic or spherical scores in classification.", "sections": [{"title": "1. Introduction", "content": "AI models and systems are evaluated with very different metrics, depending on the purpose of application. For instance, metrics as diverse as the BLEU score (Papineni et al., 2002) for translation, 'Bold' toxicity score (Dhamala et al., 2021) for text generation, the area under the ROC curve (Fawcett, 2006) for classification, asymmetric loss (Elliott et al., 2005) for sales prediction (Gogolev & Ozhegov, 2023) or any reward function (Eschmann, 2021) for reinforcement learning, are commonly used. Models can be built or trained to minimise some loss, and then repurposed for a situation where another metric matters more. The most characteristic example today of this process is represented by 'foundation models' (Bommasani et al., 2021), such as language models. Even if the model can produce uncertainty estimates about the next token, and these are well calibrated, the metric of interest may be toxicity, for example. Since the model does not estimate toxicity, we need some external way to do this.\nOne solution to this challenge is the development of assessor models (Hern\u00e1ndez-Orallo et al., 2022). An assessor is a predictive model designed to estimate how well another system, called the base or subject system s, will perform on a given example or problem instance i for a specific validity metric before it is actually deployed. An assessor can estimate the conditional distribution p(v | s, i) or simply (point-wise) map (s, i) \u2192 v. Assessors are related to verifiers (Li et al., 2023) but are anticipatory: rather than simply checking outcomes post-execution, they predict the outcomes in advance (i.e., given a new example i, they can predict the value v of the metric that s is expected to achieve). For instance, consider s a self-driving car and i a specific journey. An assessor could predict the safety outcome v of s for i.\nAssessors are used to anticipate any metric of quality, safety, bias or, in general, validity for any kind of subject system, from RL agents to language models. Assessors can be used to monitor or forecast system performance (Schellaert et al., 2024a), to optimise configurations (Zhao et al., 2024), to do anticipatory reject (Zhou et al., 2022), or to delegate by routing (Hu et al., 2024; Lu et al., 2023; Ding et al., 2024). Assessors are usually trained on test data, capitalising on vast information from results of many systems and examples (Burnell et al., 2023).\nIt may seem natural that the assessor is trained to optimise for the metric we are interested in. For instance, if the subject system s estimates daily energy consumption of households and the metric value v is given by the squared error ($L_{\\frac{1}{2}}$) between actual and estimated consumption values, then one would expect that the assessor should be trained to predict the squared error that the system will incur for each household. However, in this paper we challenge the general assumption that training an assessor to optimise directly for a specific metric L necessarily results in the best optimisation outcome for L. In this example, what if optimising for logistic loss ($L^{\\dagger}$) were better? This situation is illustrated in Figure 1.\nTo start exploring this question, in this paper we will consider the base model is solving a regression problem and we will use generic regression metrics, such as absolute error, squared error and logistic error. We will consider signed and unsigned (absolute) versions of these three metrics, and explore whether optimising for a proxy metric is better than optimising for the target metric. From our experimental analysis we observe some results that may be explained by the distribution of errors (residuals) in the test data of the base subject systems. However, some other results are more surprising, such as the logistic error being the best in all situations. Give the effectiveness of some monotonic transformations for regression, we also explore the case of classification, focusing on monotonic transformations, and find a similar phenomenon. These findings suggest that learning an assessor for one central metric might suffice to optimise a family of monotonically-related metrics."}, {"title": "2. Background", "content": "This work situates itself within a broad spectrum of research on error analysis and the exploration of alternative loss functions for training predictive models. However, the use of assessors resituates this question at the meta-level, as a second-order regression problem, an area that, to our knowledge, has not been explored yet."}, {"title": "2.1. Error analysis in regression", "content": "In regression problems, the choice and optimisation of loss functions is critical to model performance. There is an extensive literature on traditional error measures (Hyndman & Koehler, 2006; Botchkarev, 2018; 2019; Chicco et al., 2021) such as Mean Squared Error (MSE), Absolute Error, and more robust variants such as Huber Loss (Owen, 2007), which falls somewhat in between squared and absolute error, or Tukey's biweight loss (Beaton & Tukey, 1974; Belagiannis et al., 2015), which caps quadratic loss beyond a given point. Optimisation of these loss functions leads to different kinds of bias. For instance, quadratic error leads to estimators that are unbiased for the mean while absolute error leads to estimators that are unbiased for the median.\nBeyond their use in performance evaluation, the analysis of errors and residuals also serves a diagnostic purpose, helping to identify model inadequacies or violations of assumptions, providing a comprehensive understanding of the linear and non-linear relationships captured by regression models. For instance, (Rousseeuw & Leroy, 2005) use regression diagnostics, e.g., outlier diagnosis, to identify problems in both the explanatory and response variable, further refining the understanding of errors in predictive models.\nSome studies have also explored more complex loss functions and their impact on regression model performance. According to (Gneiting & Raftery, 2007), appropriate scoring rules incentivise truthful prediction by optimising prediction distributions. However, as models and tasks become more complex, optimising a single loss function may not always align with the broader objectives of the system. In this regard, research such as (Huber, 1992) experiment with alternative, often non-convex, loss functions designed to improve model training under specific constraints or performance benchmarks."}, {"title": "2.2. Error analysis in classification", "content": "In probabilistic classification, evaluating predicted probabilities is as crucial as evaluating label accuracy. Proper scoring rules (Gneiting & Raftery, 2007) such as the log score (cross entropy), Brier score (quadratic score), and spherical score assess probabilistic forecasts' accuracy and promote honest reporting of uncertainties when classifiers provide probability distributions.\nThe choice of loss functions used during training also affects classification errors. Cross-entropy loss (or log loss) is commonly used to train neural networks for classification tasks because it is consistent with maximising the probability of the correct class (Bishop & Nasrabadi, 2006). Alternative loss functions, such as the hinge loss used in support vector machines (Cortes, 1995), have different properties and can lead to different error patterns.\nIn this paper we explore proper scoring rules, as they are minimised when converging to the true distribution and, more importantly, all of them can be expressed as a a distribution between [0, 1] of all decision threshold (e.g., Brier"}, {"title": "2.3. Assessors", "content": "The concept of assessors was first introduced in (Hern\u00e1ndez-Orallo et al., 2022). (Kadavath et al., 2022) extended this by examining LLM and their role as assessors, finding that larger models tended to be more accurate and consistent in predicting outcomes across multiple tasks, although they acknowledged a lack of generalisation in out-of-distribution scenarios. (Schellaert et al., 2024b) further explored assessors to predict instance-level LLM performance on over 100 BIG-bench (Srivastava et al., 2022) tasks (Schellaert et al., 2024b), outperforming subject systems in confidence and demonstrating scalability across model sizes. Similarly, (Zhou et al., 2022) showed that smaller LLMs can predict the performance of larger models on certain tasks, significantly reducing errors and computational costs. (Pacchiardi et al., 2024) also proposed estimating LLM performance using a limited set of reference instances. Other applications of assessors focus on forecasting system performance (scaling laws) (Schellaert et al., 2024a), team configurations (Zhao et al., 2024), anticipatory reject (Zhou et al., 2022), or delegation (routing) to the best language model depending on the prompt (Lu et al., 2023; Hu et al., 2024; Ding et al., 2024). However, an analysis of the chosen validity metric and its distribution has not been done to date.\nAn assessor is an external, second-order system that predicts the scores of another, first-order system, the subject. It is populational, trained on test data spanning numerous instances and potentially multiple subjects. It operates as a standalone entity, independent of the subject. This attribute allows it to be anticipatory; it can predict the subject's performance solely on the basis of the input and the subject's characteristics, without needing access to the subject's output or the ability to execute it. Furthermore, the standalone nature of assessors offers advantages in terms of accountability and verification, as they can be developed by external auditors or for datasets different from those used to train the original subject. In addition, their use extends to increasing curriculum complexity, as in (Bronstein et al., 2022), or facilitating instance-level model selection, a concept derived from algorithm selection (Kerschke et al., 2019). Finally, a perfect assessor (in an ideal scenario) would completely capture the epistemic uncertainty (error) associated with the subject's performance (H\u00fcllermeier & Waegeman, 2021), with the error of the assessor depending only on the aleatoric error of the subject.\nAssessors must learn from a very specific kind of distribution,"}, {"title": "3. Metrics and Problem Representation", "content": "For the rest of the paper, base subjects $m_s$ are models $m_s : \\mathcal{X} \\rightarrow \\mathcal{Y}$, where $\\mathcal{X} \\subset \\mathbb{R}^d$ is an input feature vector and $\\mathcal{Y} \\subset \\mathbb{R}$ or $\\mathcal{Y} \\subset \\{0, 1\\}$ is the output, depending on whether the task is regression or binary classification. For regression, given the output $\\hat{y} = m_s(x)$ and the ground truth y, we can calculate any metric or loss function $L: \\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}$, denoted as $L(\\hat{y}, y)$. We will consider the following signed loss functions, all as a function of the residual $e = \\hat{y} - y$:\nSigned simple error\n$L(\\hat{y}, y) := \\hat{y} - y$ (1)\nSigned squared error\n$L(\\hat{y}, y) := (\\hat{y} - y) \\cdot |\\hat{y} - y|$ (2)\nSigned logistic error\n$L(\\hat{y}, y) := \\frac{2}{1 + e^{-B(\\hat{y} - y)}} - 1, B = \\frac{\\text{In 3}}{\\text{meany} |\\hat{y} - y|}$ (3)\nthe value of $L_{\\frac{1}{2}}$ is 0.5 when the error in an instance is equal to the mean of the absolute errors of the base model.\nThe corresponding unsigned loss functions, are defined by simply removing the sign, i.e., $L^{+} := |L|$, $L^{+}_{\\frac{1}{2}} := |L_{\\frac{1}{2}}|$ and $L^{\\ddagger} := |L^{\\ddagger}|$. It is easy to see that $L_{\\frac{1}{2}}^{+}, L_{\\frac{1}{2}}^{+}$ and $L^{\\ddagger}$ are mononotically related (they do not lose information between each other), and the same happens between the unsigned versions. Of course, this no longer happens between the signed and unsigned versions, as the unsigned versions lose information. Figure 12 in the Appendix shows the six losses. The signed losses contain information about the magnitude and the direction of the error, whereas their unsigned counterparts only carry the magnitude, hence being less informative. The logistic loss tries to represent a smooth loss function that penalises outliers (mostly of aleatoric character) proportionally less than lower errors. It is hence a non-convex loss that, unlike the Huber Loss, does not fall in between the simple (linear) and squared errors, but goes beyond the linear error. It saturates on high residuals, but unlike Tukey's biweight loss, it is not piecewise, and has non-zero gradient everywhere (Tukey's loss is constant from a value, which is usually chosen to be 4.685 when residuals follow a standard normal distribution) (Belagiannis et al., 2015).\nFor classification, given a forecasted probability vector $m_s(x) = r$, m different classes and $r_0$ as the probability assigned by the model to the observed class, known as the 'principal', we will consider the following proper scoring rules $S: [0, 1]^m \\rightarrow \\mathbb{R}$:\nLogarithmic score\n$S_L(r) := ln(r_0)$ (4)\nQuadratic score\n$S_Q(r) := 2r_0 - r \\cdot r$ (5)\nSpherical score\n$S_S(r) := \\frac{r_0}{||r||}$ (6)\nIn the case of binary classification, we can define r as a two-component vector only in terms of the principal $r_0$, simplifying some of the previous scores, we get:\n$S_Q(r) = -(2r_0^2 - 4r_0 + 1)$ (7)\n$S_S(r) = \\frac{r_0}{\\sqrt{2r_0^2 - 2r_0 + 1}}$ (8)\nWhich now allows for transformations between them:"}, {"title": "4. Methodology and experimental setup", "content": "Generation of base model results Training an assessor for a specific task requires test results from one or more base models. We generate these results using the procedure illustrated in Figure 13 in Appendix B and described formally below. The more data and models we have the more the assessor can generalise. The quality of the assessor would also depend on the parametrisation of x and s. In this regard, we have built a collection of base models as a training resource for the assessor. We used 10 regression and 10 classification datasets of varying number of instances and attributes (see Table 1), as well as different distributions of the target variable. We use different model configurations (i.e., representing the combination of a model and its associated hyperparameters). Training such a model configuration on a specific dataset provides us instance-level results of the predicted and actual values on the test set, as well as additional metrics including training and inference time, and memory usage. These characteristics, paired with different hyperparameter values, define the model parametrisation s.\nIn order to have a homogeneous parametrisation s we train five distinct tree-based algorithms for each of the twenty datasets. Our base models include Decision Trees (Breiman et al., 1984), Random Forests (Ho, 1995), CatBoost (Prokhorenkova et al., 2019), XGBoost (Chen & Guestrin, 2016), and LightGBM (Ke et al., 2017). For each dataset, we generate 255 different unique model configurations (denoted by the system space S) by varying hyperparameters such as maximum depth (3, 5, 7, 9 and 11), learning rate (0.01, 0.05 and 0.1), and the number of estimators (100, 250, 500, 750 and 1000). For decision trees, we used fewer configurations. We partitioned the data using a 70/30 train-test partition, and recorded the performance metrics at the instance level on the test set. Therefore, each row (x, s, \u0177, y) of the test set consists of a task instance representation x and a model configuration s, with the corresponding predicted and actual results. In the case of classification, each row is defined as (x, s, r, y), with x and s having the same definition, r being the predicted vector or probabilites and y being the observed class. These results form the dataset D that will serve as the training (and testing) dataset for the assessors.\nSplitting strategy To prevent contamination and ensure the validity of our evaluation, we split D into assessor training and testing sets (70/30) based on the instance identifiers $x_{id}$. Instances with the same $x_{id}$ are not shared between the assessor's training set $D_{train}$ and testing set $D_{test}$, even across different model configurations, so we ensure that assessors do not see any instance during training that they will be tested on.\nAssessor models and training The training process for the assessors is defined as follows: given a pair of target and proxy losses ($L_{\\infty}$ and $L_{\\circ\\rightarrow}$, respectively), we train two assessors independently:\n1.  The target assessor: this assessor is trained to directly predict the target loss, using the tuple $(x, s, L_{\\infty}(\\hat{y}, y))$. No output post-processing is required.\n2.  The proxy assessor: this assessor is trained to predict the proxy loss $L_{\\circ}$, using the tuple $(x, s, L_{\\circ}(\\hat{y}, y))$. The output is then transformed into the target loss, via the corresponding transformation function $f$.\nWe employ various models as assessors, including XGBoost (Chen & Guestrin, 2016), linear regression (Galton, 1886), feed-forward neural networks (McCulloch & Pitts, 1943), and Bayesian ridge regression (Tipping, 2001), to account for the different strategies these models use to solve tasks (Fabra-Boluda et al., 2020; 2024), testing whether our results hold independently of the choice of assessor model. For the classification tasks, scores are still numerical and continuous, so we also use XGBoost Regressor as an assessor. These assessor models are trained independently of the base models used to generate the losses.\nEvaluation metrics We evaluate the relationship between the target and proxy assessors by calculating the Spearman's correlation coefficient $\\rho$. To assess the statistical significance of the differences in $\\rho$,s we establish 95% confidence intervals using a bootstrapping approach"}, {"title": "5. Results", "content": "5.1. Regression\nFigure 4 (left) shows the scores for all datasets when the assessor model of choice is XGBoost (other assessors in Appendix D show similar results). Some interesting patterns can be seen: mainly, that learning from unsigned losses ($L^{+}$, $L^{+}_{\\frac{1}{2}}$ and $L^{\\ddagger}$) to predict their unsigned counterparts yields worse assessors than learning from $L^{+}$, $L^{+}_{\\frac{1}{2}}$ and $L^{+}$, directly: for instance, when the proxy error is $L^{+}$, and the target error is $L_{\\frac{1}{2}}^{+}$, the final score is -9 (e.g., from the 10 datasets, there is one tie - no significant differences in Spearman correlation - and 9 losses). This contrast is specially sharp with the simple signed error, where, in all 10 datasets, its absolute counterpart yields better results in terms of Spearman correlation $\\rho$. Overall, the most under-performing proxy error is by far the signed squared error,"}, {"title": "5.2. Classification", "content": "Figure 8 (left) shows the scores aggregated over all datasets and assessor models. Similar to regression, we see some scores that are good proxies: when using the logarithmic score $S_L$ as the proxy, the assessor often outperforms the target assessor for both the quadratic score $S_Q$ (8 out 10 datasets) and the spherical score $S_S$ (9 out of 10 datasets). Using the quadratic score $S_Q$ as a proxy to predict $S_S$ also resulted in improved performance, with positive scores in 7 out of 10 datasets. The spherical score $S_S$ was generally a poor proxy for predicting $S_L$ or $S_Q$.\nFigure 8 (right) shows the magnitude of these improvements in Spearman $\\rho$, establishing the logarithmic score as the best score to use as a proxy. Looking at the differences also nuances the disadvantage of using $S_S$ as a proxy to predict $S_Q$, since although the score is -4, the Spearman $\\rho$ difference is -0.006. Finally, Figure 9 illustrates the relationships between scores and their effectiveness as proxy metrics. It reveals patterns similar to those observed in regression, particularly highlighting the transitive nature of the scores."}, {"title": "6. Discussion", "content": "AI assessors represent a second-order estimation problem whose goal is to predict a loss or utility function, for any new example and base subject model. This is much more flexible than uncertainty self-estimation because we can choose the metric of the assessor to be different from the ones the base models are optimised for or evaluated. Still, in this context it may seem natural to build an assessor to optimise for the target loss. However, we see that some other proxy losses may be more effective. In regression, looking at the distribution of residuals (Figure 2 for a selection, Figure 15 in the Appendix for all), one explanation may be found in a double penalisation of high residuals (e.g., for outliers). That indicates that for convex loss functions used at the first-order level (base models) we may benefit for concave loss functions at the second level that compensate for the weight in the extremes of the distribution. For classification, the phenomenon could be the reverse (Figure 3 for a selection, Figure 14 in the Appendix for all) we have losses for the model in the interval [0, 1] not really penalising enough, and the logarithmic transformation of the principals can give more relevance to those errors.\nIn this paper, we chose regression problems for this first analysis of proxy losses for assessors because loss functions for regression are well known, generally continuous, and the most common one, the squared error, augments the weight of the extremes. Having explored this question for binary classification and having obtained similar results, this suggests that similar exploration for multiclass classification, and especially for losses in structured or generative tasks, could be done following the methodology in this paper. Similarly, in situations where a metric is composed of several parts, e.g., components in a toxicity metric or precision and recall in the F1 score, it may make more sense to estimate the components (or some monotonic transformations of the components) with separate assessors and then integrate the prediction of the overall metric. Overall, this paper opens a wide range of options for exploring the impact of loss and utility metrics when building assessors."}, {"title": "A. Detailed Exploration of Scoring Rules and Loss Functions and Loss Functions", "content": "In this appendix, we provide a comprehensive analysis of the scoring rules used for classification and the loss functions used for regression in our study. We include graphical representations of these functions and derive their mathematical relationships. These derivations demonstrate monotonic transformations between different loss and scoring functions, allowing predictions to be converted from one function to another. This capability is particularly useful for training assessors, as it allows us to train a assessor on one loss or score function and then transform their output to estimate a different function."}, {"title": "A.1. Proper Scoring Rules for Classification", "content": "In probabilistic classification, we evaluate the quality of predicted probability distributions over classes. Proper scoring rules are functions that encourage truthful reporting of probabilities. We consider the following proper scoring rules: Logarithmic Score ($S_L$), Quadratic (Brier) Score ($S_Q$) and Spherical Score ($S_S$). All three scoring rules are proper, meaning they are minimised when the predicted probabilities match the true probabilities. Figure 10 illustrates the behaviour of the three scoring rules as a function of $r_0$, the predicted probability of the correct class."}, {"title": "A.1.1. DERIVATION OF MONOTONIC TRANSFORMATIONS BETWEEN SCORES", "content": "We derive mathematical relationships between the scoring rules to understand how they are related and to facilitate transformations from one scoring rule to another. Let's start remembering the three proper scoring rules:\n$S_L(r) = ln(r_0)$\n$S_Q(r) = 2r_0 - r \\cdot r$\n$S_S(r) = \\frac{r_0}{||r||}$\nIn binary classification, the predicted probabilities sum to 1, so we can express r in terms of $r_0$ as $r = (r, 1 - r_0)$ or r = $(1 - r_0, r_0)$, depending on which one is the correct class. However, for both cases, the product is the same $r_0^2 + (1 - r_0)^2$, and so is the norm.\nFirst, we expand the quadratic score:\n$S_Q(r) = 2r_0 - r \\cdot r= - r \\cdot r = 2r_0 - [r_0^2 +  [r_0^2 + (1 - r_0)^2] = -2r_0^2 + 4r_0 - 1 = -(2r_0^2 - 4r_0 + 1)$ (15)\nThe same can be done with the spherical score:"}, {"title": "A.2. Loss Functions for Regression", "content": "The performance of regression models is typically evaluated using various loss functions that measure the discrepancy between the predicted values \u0177 and the true values y. In our study, we consider the Signed Simple Error ($L^{+})$, which captures both the magnitude and direction of the error; Signed Squared Error ($L_{\\frac{1}{2}}$), which amplifies the impact of larger errors; and the Signed Logistic Loss ($L^{\\ddagger}$), which adjusts the steepness of the logistic function based on the average absolute error. All are defined as functions of the residual e = \u0177 y. The corresponding unsigned loss functions are obtained by taking the absolute value of the signed losses ($L^{+}$, $L^{+}_{\\frac{1}{2}}$ and $L^{\\ddagger}$):\nFigure 12 illustrates the behaviour of the six loss functions as a function of the residual e. The signed losses ($L^{+}$, $L_{\\frac{1}{2}}$ ,$L^{\\ddagger}$) capture both magnitude and direction, while the unsigned losses ($L^{+}$, $L^{+}_{\\frac{1}{2}}$ , $L^{\\ddagger}$) capture only the magnitude."}, {"title": "A.2.1. DERIVATION OF MONOTONIC TRANSFORMATIONS BETWEEN LOSSES", "content": "We now derive mathematical relationships between the loss functions to understand their interrelationships and to facilitate transformations from one loss function to another. We will start with the signed losses defined in Section 3 in the main text:\n$L(\u0177, y) = \u0177 - y$\n$L_{\\frac{1}{2}}(\u0177, y) = (\u0177 \u2013 y) \u00b7 |\u0177 - y|$\n$L^{\\ddagger}(\u0177, y) = \\frac{2}{1 + e^{-B(y-y)}} - 1, B = \\frac{\\text{In 3}}{\\text{meany |\u0177 - y|}}$\nSimilar to the proper scoring rules, we can obtain (\u0177 - y) (also known as the residual e) in terms of the different losses. Lalready satisfies this, so we can substitute the other losses (similarly to the previous section, we will omit the residual in the following formulae for clarity):\n$L = L \u00b7 |L|$ (27)\n$L^{\\ddagger}= \\frac{2}{1 + e^{-BL}} - 1, B = \\frac{\\text{In 3}}{\\text{meany L}}$ (28)\nFor their unsigned counterparts, we can take their definitions as the absolute value of the signed losses:\n$L+ = |L|$\n$\\frac{1}{2}+ = |L_{\\frac{1}{2}}| = |L_{\\frac{1}{2}} \u00b7 |L_{\\frac{1}{2}}|| = (L_{\\frac{1}{2}})^{2}$\n$L^{\\ddagger} = |L^{\\ddagger}| = \\frac{2}{1 + e^{-BL}}$ (31)"}, {"title": "B. Assessor training methodology details", "content": "This appendix complements the assessor training methodology described in Section 4 complementing the main text with comprehensive details on test result generation and assessor training. For the generation of base model results, where we use five tree-based learning algorithms and varied hyperparameters to create different model configurations. Each configuration is evaluated using a dataset split into training and test sections, yielding predicted and true outputs. An evaluation dataset is then constructed, comprising input features from the original problem and model features with specific loss functions as targets. Importantly, the splitting strategy ensures no data leakage by using instance identifiers to separate training and test sets.\nFigure 13 visually shows this data generation and splitting approach, illustrating how the original problem features and model characteristics form an example for an assessor."}, {"title": "C. Residuals and Principals", "content": "In this appendix we plot the residuals for the regression datasets and the principals for the classification datasets used in our experiments (see Table 1). For each regression dataset, we plot the scatter plots of residuals (errors) \u0177 y across all base models. For each classification dataset, we show histograms illustrating the distribution of principals $r_0$, which are the probabilities assigned by the base classifiers to the correct class.\nFor classification (see Figure 14), the principals are generally skewed towards high probabilities, indicating that the base models perform well and assign high probabilities to the correct class. An exception is the Higgs dataset, where the distribution of principals is more symmetric and resembles a normal distribution. This is the only case where the logarithmic score performs poorly as a proxy for the quadratic score. This may be due to the strong penalisation of low-probability correct classes in the logarithmic score which may not be well suited for this dataset.\nFor regression (see Figure 15), in general, the residuals are centred around zero, indicating that on average the base models provide unbiased estimates. Exceptions, such as Auction Verification have very high residuals, indicating that the base models occasionally make large prediction errors. This can contribute to the \"double punishment\" phenomenon under certain loss functions, such as quadratic loss, penalise both the size of the residual and its square."}, {"title": "D. Score results for all assessor types (regression)", "content": "This appendix provides detailed score and Spearman margin matrices for all the assessor models used in the regression tasks. These results show that the findings discussed in the main text (using XGBoost as the assessor) are consistent across different assessor models. Figure 16 presents the score matrices for each assessor model. Each matrix shows the net score for using a proxy loss to predict a target loss across all datasets. Although the scores vary slightly (there are two groups with similar scores, XGBoost and Bayesian ridge regression vs Linear Regression and Neural Networks), the patterns observed are consistent across all assessor models: using signed losses as proxies for unsigned target losses generally results in poorer performance, often with negative scores. Also, the logistic errors prove to be successful proxies."}, {"title": "E. Underestimation of signed errors", "content": "Here, following the discussion in the main text, we further analyse the phenomenon of underprediction observed when using signed errors as proxy losses to predict unsigned errors (specifically, Figure 5). When assessors are trained using signed losses to predict unsigned target losses, we observe that the predicted losses tend to underestimate the true losses. This underestimation is evident in the scatter plots in Figure 17, where the predictions fall below the diagonal line (ideal prediction) for higher loss values.\nThe underestimation can be attributed to two main factors: mean reversion and loss of magnitude information. The under-estimation of errors in signed losses may be due to two main factors: mean reversion and loss of magnitude information. Signed losses tend to average close to zero because positive and negative errors cancel each other out, causing assessor trained on these losses to predict values around zero and underestimate larger errors. In addition, while the sign indicates the direction of the error, it fails to convey magnitude when converted to an unsigned loss, leading to predicted values being compressed towards zero."}]}