{"title": "Generating Structured Outputs from Language Models: Benchmark and Studies", "authors": ["Saibo Geng", "Hudson Cooper", "Micha\u0142 Moskal", "Samuel Jenkins", "Julian Berman", "Nathan Ranchin", "Robert West", "Eric Horvitz", "Harsha Nori"], "abstract": "Reliably generating structured outputs has become a critical capability for modern language model (LM) applications. Constrained decoding has emerged as the dominant technology across sectors for enforcing structured outputs during generation. Despite its growing adoption, little has been done with the systematic evaluation of the behaviors and performance of constrained decoding. Constrained decoding frameworks have standardized around JSON Schema as a structured data format, with most uses guaranteeing constraint compliance given a schema. However, there is poor understanding of the effectiveness of the methods in practice. We present an evaluation framework to assess constrained decoding approaches across three critical dimensions: efficiency in generating constraint-compliant outputs, coverage of diverse constraint types, and quality of the generated outputs. To facilitate this evaluation, we introduce JSONSchemaBench, a benchmark for constrained decoding comprising 10K real-world JSON schemas that encompass a wide range of constraints with varying complexity. We pair the benchmark with the existing official JSON Schema Test Suite and evaluate six state-of-the-art constrained decoding frameworks, including Guidance, Outlines, Llamacpp, XGrammar, OpenAI, and Gemini. Through extensive experiments, we gain insights into the capabilities and limitations of constrained decoding on structured generation with real-world JSON schemas. Our work provides actionable insights for improving constrained decoding frameworks and structured generation tasks, setting a new standard for evaluating constrained decoding and structured generation. We release JSONSchemaBench at https://github.com/guidance-ai/jsonschemabench.", "sections": [{"title": "1 Introduction", "content": "The rapid advancements in LMs in recent years have significantly broadened their applications, extending beyond natural language tasks to more complex challenges such as web navigation (Yao et al., 2023b), data extraction (Polak & Morgan, 2024), and tool use (Schick et al., 2023). Unlike traditional natural language processing (NLP) tasks where the output is aimed at review by humans, output in these applications is often consumed by machines such as controller and service APIs. The machine-oriented nature of these applications requires LMs to generate structured outputs that strictly adhere to predefined formats and constraints. However, the LM generation process is probabilistic and does not provide guarantees on the output's structure, making it challenging to deploy LMs in applications requiring structured inputs and high reliability.\nThe methodology of constrained decoding, a technique that integrates constraints into the decoding process of LMs, has been developed to address the need to adapt LM generations to the challenge of providing structured output. Constrained decoding intervenes in the decoding process of LMs by masking out invalid tokens based on given constraints and prefix tokens. This intervention guides the LM to sample only from valid tokens, ensuring that the final output perfectly conforms to a predefined structure.\nThe strong demand for structured generation (Liu et al., 2024) has led to the development of various constrained-decoding frameworks2, such as Guidance (Guidance AI, 2023), Outlines (Willard & Louf, 2023), XGrammar (Dong et al., 2024) and the grammar module of Llamacpp (Gerganov & al., 2023) These frameworks provide broad support for different types of constraints, minimal overhead, and compatibility with various LM ecosystems, facilitating the adoption of constrained decoding in real-world applications.\nJSON Schema offers a high level, domain-specific way to define constraints for JSON data, a widely adopted data interchange format. As a result, JSON Schema has emerged"}, {"title": "2 Background and Related Work", "content": "JSON Schema is a meta-language that describes the structure of JSON data. It is capable of expressing a wide variety of constraints, such as the types of JSON object properties, the length of JSON arrays or the pattern that a JSON string must match. The syntax and capabilities of JSON Schema are defined in the JSON Schema specification (Wright et al., 2022), which defines a large number of keywords, each of which may be used or combined with other keywords within a schema to enforce constraints like the ones"}, {"title": "3 The JSONSchemaBench", "content": "Our goal is to design a benchmark that is (1) diverse enough to cover the most common constraint types encountered in real-world applications, (2) large enough to provide a reliable evaluation, and (3) equipped with fair and multidimensional metrics to ensure comprehensive assessments."}, {"title": "3.1 Data Collection", "content": "We start with the 6K JSON schemas collected by Baazizi et al. (2021) from publicly available GitHub repositories, and with the set of schemas from the JSON Schema Test Suite (JSON Schema Org, 2024). We further collect JSON schemas from other"}, {"title": "4 Efficiency", "content": "Na\u00efve implementations of constrained decoding add overhead to the standard LM inference process, including a per-step mask computation and an optional one-time grammar compilation. However, several optimizations can significantly reduce this overhead. For instance, mask computation can run in parallel with the LM's forward pass, and grammar compilation can be performed concurrently with pre-filling computations (Guidance AI, 2023; Dong et al., 2024). Other optimizations such as grammar caching and constraint-based speculative decoding (GuidanceAI, 2024b; Beurer-Kellner et al., 2023; Kurt, 2024a) can further reduce overhead."}, {"title": "Metrics", "content": "We break down the efficiency evaluation into the following components:\n\u2022 Grammar Compilation Time (GCT): The time spent on grammar compilation, if applicable.\n\u2022 Time to First Token (TTFT): Time from the start of generation to the production of the first token.\n\u2022 Time per Output Token (TPOT): Average time to generate each output token after the first."}, {"title": "4.1 Setup", "content": "The efficiency experiment depends on both the size of the model and the tokenizer's vocabulary size. We used Llama-3.1-8B-Instruct with the Llamacpp inference engine as backend for Outlines, Guidance, and Llamacpp. As XGrammar doesn't support Llamacpp as backend, we add an additional experiment with the Hugging Face Transformers inference engine for XGrammar. All experiments are conducted on a single NVIDIA A100-SXM4-80GB GPU with AMD EPYC 7543 (12 cores) CPU. The batch size is set to 1 for all experiments. Additional details about setup are provided in the Appendix E. We also provide a snippet of how we call each engine in the Appendix G."}, {"title": "5 Coverage", "content": "Each constrained decoding framework has limitations when it comes to translating JSON schemas into a set of constraints that can reliably guarantee the validity of LM"}, {"title": "Definition 5.1 (Declared Coverage)", "content": "A schema is considered declared covered if the framework processes the schema without explicitly rejecting it or encountering runtime errors such as exceptions or crashes."}, {"title": "Definition 5.2 (Empirical Coverage)", "content": "A schema is considered empirically covered if our experiments show that the constraints generated by the framework result in LM outputs that are schema-compliant."}, {"title": "Definition 5.3 (True Coverage)", "content": "A schema is considered truly covered if the framework produces constraints that are precisely equivalent to the original JSON Schema definition, i.e., permitting all schema-compliant generations while rejecting all schema-noncompliant generations."}, {"title": "5.1 Setup", "content": "To measure empirical coverage, we conduct all experiments using the Llama-3.2-1B-Instruct model as it is small enough to run efficiently while still producing high-quality outputs. The prompt consists of a simple instruction with two-shot examples (Figure 3), and validation is performed using the jsonschema Python library (Berman (2025)) (using JSON Schema Draft2020-12) with string-format checks enabled. We use greedy decoding with zero-temperature, performing a single generation run, and enforce a 40-second timeout for grammar compilation and an additional 40 seconds for generation. Exceeding"}, {"title": "5.2 Results", "content": "Empirical Coverage Guidance shows the highest empirical coverage on six out of the eight datasets, with Llamacpp taking the lead on the remaining two: the domain-specific Washington Post and notably hard JSON Schema Store. On the other hand, closed-source grammar engines consistently have the lowest coverage; they came in last on all but one dataset. LM-only5 approaches achieve acceptable coverage on easy-to-medium datasets but show significant performance drops on harder datasets, such as Github Hard and JSON Schema Store, as well as domain-specific datasets like Washington Post. We note that while empirical coverage is a reasonable indicator of a framework's real-world performance, it is influenced by factors such as the LM being used and the sampling methods employed.\nCompliance Rate Among open-source engines, guidance consistently demonstrates the highest compliance rate across all datasets, making it the most reliable option for ensuring schema compliance. Outlines has a comparatively lower compliance rate, primarily due to timeouts during generation. Our analysis reveals that JSON Schema features like 'minItems', 'maxItems', 'enum', and 'Array', while supported, often take 40 seconds to 10 minutes for Outlines to process. LM-only exhibits the lowest compliance rate, highlighting its unreliability as a standalone solution. While closed-source implementations have low empirical coverage, they have very high compliance rates, indicating that their providers have taken a more conservative strategy, implementing only a subset of JSON Schema features that they can reliably support."}, {"title": "5.3 JSON Schema Test Suite: Complementary Evaluation", "content": "Originally designed to test the correctness and compliance of JSON Schema validation implementations, the official JSON Schema Test Suite (JSON Schema Org, 2024) is a comprehensive collection of test cases spanning the many features of the JSON Schema specification. We believe that the test suite is an ideal tool for assessing the correctness of grammar engines.\nThe test suite organizes its test cases into 45 categories, each of which corresponds to a feature of JSON Schema, typically a specific keyword such as required or group of tightly related keywords such as if-then-else. A small number of additional categories test broader behaviors, such as infinite-loop-detection. Each test case contains a single schema paired with a collection of JSON instances that are marked as either valid or invalid under that schema. For the purpose of evaluating coverage, we assert that an engine must successfully generate each valid instance and block generation of each invalid instance to \"pass\" a test case. In addition to compilation failures, we define two failure modes that a grammar engine can exhibit:"}, {"title": "Definition 5.4 (Over-constrained)", "content": "A framework is over-constrained if it rejects JSON instances that are valid according to a given JSON Schema. This means the engine is too strict and excludes outputs that should be allowed."}, {"title": "Definition 5.5 (Under-constrained)", "content": "A framework is under-constrained if it allows JSON instances that are invalid according to a given JSON Schema. This means the engine is overly permissive and allows outputs that should be rejected."}, {"title": "5.3.1 Results", "content": "Coverage Analysis For each grammar engine and category in the test suite, we calculate test coverage as the proportion of passing test cases, reported in Figure 6 in Appendix D Additionally, Table 5 aggregates these metrics, counting categories with minimal coverage (> 0%), partial coverage (> 25%), moderate coverage (> 50%), high coverage (> 75%), and full coverage (100%). We indicate the number of categories for which each framework achieves the highest test coverage (either as the single highest or as the sole leader) as well as the number of categories for which each framework is the sole leader."}, {"title": "6 Quality", "content": "In principle, constrained decoding should not affect the quality of the generated output as it only filters out the invalid tokens. However, things become more complicated due to ambiguity of tokenization (Vivien, 2024; GuidanceAI, 2024a; Geng et al., 2024) and the distributional shifts caused by the intervention (Geng et al., 2023; Tam et al., 2024). As a hypothetical toy example, an LM might answer '89,000' instead of the correct '89000' in a GSM8K question. Constrained decoding can block the invalid token ',', enforcing structural compliance but potentially may cause the LM to go out of distribution and generate '890000' instead. Kurt (2024b) argued that the performance decline observed in previous studies (Tam et al., 2024) comes from inadequate prompting, insufficient contextual information, and poorly crafted schemas."}, {"title": "6.1 Setup", "content": "Kurt (2024b); Tam et al. (2024) have introduced a series of tasks to investigate potential quality concerns in constrained decoding, which we leverage and extend in this benchmark. Specifically, we adopt the three reasoning tasks from these studies to evaluate the impact of constrained decoding on task accuracy, as detailed in Table 7. The simple output structure of these tasks was designed to isolate the effects of constrained decoding on reasoning, as outlined by Tam et al. (2024).\nFor our experiments, we use the Llama-3.1-8B-Instruct model to measure task performance. We follow the original setup and prompt specifications from Kurt (2024b), with full details provided in Appendix F."}, {"title": "6.2 Results", "content": "The results in Table 8 show that the constrained decoding, regardless of the framework, achieves higher performance than the unconstrained setting. Among the frameworks evaluated, Guidance consistently delivers the best performance across all tasks, with approximately a 3% improvement over the LM-only approach in every task. We believe this may be attributed to its token-healing implementation (GuidanceAI, 2024a)."}, {"title": "7 Conclusion", "content": "We have proposed a comprehensive evaluation framework for constrained decoding frameworks with JSON schemas, focusing on efficiency, coverage, and output quality. We introduced JSONSchemaBench, a benchmark comprising 10K real-world JSON schemas, to enable robust assessment under realistic conditions. Our evaluation highlights both the advancements and limitations of current state-of-the-art constrained decoding frameworks. We hope that our findings and benchmark will inform future research in structured generation and provide valuable insights to help the community identify the most effective tools and to extend capabilities with constrained decoding."}, {"title": "A JSON Schema Collections Details", "content": "JSONSchemaBench includes a diverse collection of schemas curated from multiple real-world applicationsAttouche et al. (2022), designed to represent a wide range of use cases:"}, {"title": "Sources:", "content": "\u2022 GitHub (Baazizi et al., 2021): Extracted from open-source repositories containing schema definitions, representing practical, widely-used applications. Schemas from GitHub are of various complexities, totaling 6,000 schemas. We split the collection into trivial (fewer than 10 fields), easy (10-30 fields), medium (30-100 fields), hard (100-500 fields), and ultra (more than 500 fields), based on the total number of fields in each JSON schema to reflect increasing complexity and scale.\n\u2022 Snowplow (Analytics, 2022): Sourced from event-based analytics frameworks, showcasing schemas tailored for event-driven data structures.\n\u2022 Kubernetes (Kubernetes, 2022): Schemas defining configurations for container orchestration systems, highlighting schemas with intricate hierarchical structures.\n\u2022 WashingtonPost (Post, 2022): Schemas for The Washington Post's ANS specification.\n\u2022 GlaiveAI2K GlaiveAI (2024): 2,000 schemas extracted from a function-calling dataset. Each schema represents a function signature.\n\u2022 JSON Schema Store (Schema Store Org, 2014): The largest collection of independent JSON schemas in the world."}, {"title": "A.1 Data Processing", "content": "To ensure the quality and reliability of JSONSchemaBench, we applied the following preprocessing steps:\n1. Validation\n\u2022 Verified schemas conform to the JSON Schema specification using the jsonschema library in Python, specifically targeting the Draft2020-12 version. Drop invalid schemas.\n\u2022 Identified additional invalid schemas using validators from Rust and JavaScript libraries.\n2. Cleaning\n\u2022 Deduplicate: Removed duplicate schemas to eliminate redundancy and maintain a diverse dataset. Key ordering within schemas was ignored when determining duplicates.\n\u2022 Empty Schema: Excluded schemas that were lacking meaningful constraints, effectively \"empty.\"\n\u2022 Unresolved References: Removed schemas containing unresolved $ref references to external URLs.\n\u2022 Schema Version Fixes: Corrected mismatched or missing draft versions.\n\u2022 Extraneous Field Removal: Eliminated unrelated fields such as command, config, path, and controls.\n\u2022 Regex Escaping: Fixed escaping issues in regular expressions to ensure validity.\n\u2022 Schema Extraction: Extracted schemas embedded within non-root levels of JSON files."}, {"title": "A.2 Draft versions", "content": null}, {"title": "A.3 Feature Distribution", "content": "We count the appearance of each feature (keyword) in the 10K schemas and show the most frequent features in Figure 2a. We separately plot usage of the format keyword, which is used to specify format of string such as date-time, email, uri. This is worth highlighted because each of these formats can be quite complex to implement on its own The distribution of formats used is shown in Figure 2b."}, {"title": "B Coverage Experiment Details", "content": "The prompting template used for the coverage experiment is shown in Figure 3."}, {"title": "C Theoretical Coverage Details", "content": "A schema is considered theoretically covered if all of its features are supported by the grammar engine."}, {"title": "Definition C.1 (Theoretical Coverage)", "content": "The theoretical coverage, noted as CTheoretical, measures the proportion of JSON schemas that a grammar engine supports based on its implementation. It doesn't involve any model inference or experiments and is solely based on the grammar engine's implementation. CTheoretical is an upper bound of the true coverage, which cannot be empirically measured due to the infinite number of possible generations under the schema constraints.\nOverall, the theoretical coverage provides a good indication of the grammar engine's capability to support a wide range of schema constraints.\nIn our experiment, the theoretical coverage for each framework was determined based on the documentation and resources listed in Table 11."}, {"title": "DJSON Schema Test Suite Experiment Details", "content": "We evaluated each constrained decoding framework's performance on the JSON Schema Test Suite using the following criteria: a framework is considered to pass a test case if it permits generating every valid instance in the test case while preventing the generation of every invalid instance. Some test cases consist exclusively of invalid instances, such as those involving unsatisfiable schemas, i.e., schemas for which no valid instances exist. In these cases, engines raising compile-time errors were allowed to pass.\nCleaning We removed the 'format' category of tests, as the current JSON Schema standard mandates that this keyword be ignored entirely by default. The test suite comes bundled with an 'optional' set of tests, including tests for each officially recognized value of the 'format' keyword. We hope to extend this work to include these optional tests in a follow-up.\nFurthermore, some tests require external resources in the form of JSON schemas available at a remote URL. We dropped these tests from the analysis, as the constrained decoding libraries discussed in the current work do not fetch these resources by default. After filtering out these tests, we are left with 43 of the original 45 test categories.\nImplementation To check whether a given framework accepts or blocks the generation of a particular JSON instance, we tokenize\u00ba JSON-serialized form of the instance and walk the framework's constraints forward one token at a time, essentially simulating the generation process of an LLM attempting to produce the given token sequence:\n\u2022 XGrammar directly expose an interface for updating the token mask after inserting a token and checking validity.\n\u2022 Outlines does not expose a public interface for interacting with the token mask, but outlines-core, which outlines is built on top of, is easily adapted for this purpose.\n\u2022 Similarly, Guidance does not expose a public interface for interacting with the token mask, but llguidance, which guidance is built on top of, is easily adapted for this purpose.\n\u2022 Llamacpp does not expose this interface, but it shares a common grammar-specification language with XGrammar. We use llamacpp to generate GGML BNF and check token-sequence validity using xgrammar's interface.\nThe particular choice of tokenizer is not particularly important, but we use the Llama 3.1 tokenizer for consistency with our other experiments."}, {"title": "E Efficiency Experiment Details", "content": "For efficiency experiments, the results depend on both the size of the model and the tokenizer's vocabulary size. We used Llama-3.1-8B-Instruct (quantized to Q8bit) with"}, {"title": "F Quality Experiment Details", "content": "Prompt and JSON Schema For the task of Shuffle Objects, and GSM8K, we use the same prompt and JSON schema from the dottxt's \"let me speak freely\" rebuttal.\nFor the task of Last Letter, we make a slight modification because the original prompt used was a bad example as pointed out by Kurt (2024b). We also put it into a JSON format to better align with the other tasks."}, {"title": "G Engine calling Snippet", "content": "We provide a snippet of the engine code used in our experiments. The generation method of each engine has two main components: \u201ccompile_grammar\" and \"call_engine\"."}]}