{"title": "MORSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation", "authors": ["Marco Simoni", "Andrea Saracino", "Vinod Puthuvath", "Maurco Conti"], "abstract": "In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the first specialised AI chatbot for cybersecurity. MoRSE aims to provide comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG (Retrieval Augmented Generation) systems designed to retrieve and organize information from multidimensional cybersecurity contexts. MoRSE differs from traditional RAGs by using parallel retrievers that work together to retrieve semantically related information in different formats and structures. Unlike traditional Large Language Models (LLMs) that rely on Parametric Knowledge Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases in response to user queries. Subsequently, MoRSE uses this information to generate accurate answers. In addition, MoRSE benefits from real-time updates to its knowledge bases, enabling continuous knowledge enrichment without retraining. We have evaluated the effectiveness of MORSE against other state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific questions. The experimental evaluation has shown that the improvement in terms of relevance and correctness of the answer is more than 10% compared to known solutions such as GPT-4 and Mixtral 7x8.", "sections": [{"title": "I. INTRODUCTION", "content": "The increasing frequency and sophistication of new cyber threats have made cybersecurity a critical priority across all sectors, with a 15% increase in three years 1 on data breaches only. In recent years, the amount of cybersecurity- related information has exploded, providing important re- sources to protect against these threats by mitigating risk and improving cybersecurity measures. However, this rapid proliferation of information has led to a cluttered and often unstructured data landscape, complicating the task of deriving actionable insights for professionals [1], [2]. In fact, a timely, accurate, and comprehensive understanding of vulnerabilities, exploits, and defense tactics is crucial, as the promptness of such information can significantly impact cybersecurity decisions [3], [4]. Recently, large language models (LLMs) have become important tools for synthesizing huge amounts of information in various fields, including cybersecurity [5]. However, their reliability varies for technical topics where inaccuracies are critical [6], [7], [8], [9]. LLMs can produce hallucinatory responses, meaning that they produce answers that are not true or reliable, especially struggling with the dynamic and evolving nature of cyber threats [10], [11], [12]. This problem is particularly pronounced in code generation tasks, where LLMs often produce non-functioning code for complex queries [13]. Specifically, when the model does not know the correct answer, hallucinations are inevitable (Epistemic [14]). This can happen if the model does not have enough training data or if its capacity is limited. An example of hallucination in cybersecurity is shown in Example 1 (ground truth) and Example 2 (the answer of GPT-4) to the question: What is CVE-2017-5162?. This shows that GPT-4 is not able to provide the correct answer. To overcome these limitations, it is crucial to build continuous learning mechanisms into LLMS that allow them to dynamically update their knowledge base with the latest information [15]. Although retraining can be time-consuming, updating with new information is essential to maintain the relevance of LLM responses [16], [17]. For this reason, companies are quickly discovering the value of Retrieval Augmented Generation chatbots. Although RAG systems have made significant advances on cybersecurity, re- searchers and practitioners often lacks comprehensive models that unify diverse cybersecurity data and dynamic, real-time updated systems that are essential for addressing the evolving cyber threat landscape. In fact, effective analysis of threats, vulnerabilities and exploits requires a thorough understanding of individual entities and their broader interactions."}, {"title": "II. BACKGROUND", "content": "This section provides an overview of the basic concepts necessary for understanding the architecture of MORSE.\nA. Large Language Models\nLarge language models (LLMs) represent a significant ad- vancement in the field of Natural Language Processing (NLP) and are based on the Transformer model [25]. These models are trained on large text datasets and are able to generate coher- ent and contextually relevant texts based on input prompts. The capabilities of LLMs go beyond text generation and include tasks such as language translation, summarizing, answering questions, etc.\nThe introduction of models such as GPT [26] and BERT [27] has demonstrated the potential of LLMs to revolutionize language understanding and generation through unsupervised and bidirectional training [26], [28]. With the development of GPT-3 [29], the scalability of these models reached new heights, illustrating their ability to perform a wide range of NLP tasks without task-specific training.\nDespite their advantages, LLMs face several challenges. Ethi- cal considerations, such as the spread of bias and misinforma- tion, are a major concern [30]. In addition, the environmental impact of training and operating these computationally inten- sive models has raised questions about their sustainability [31]. Efforts to overcome these challenges include research into more efficient training methods and models that are able to understand and generate texts with greater accuracy and less bias [32].\nB. Retrieval Augmented Generation\nRetrieval-Augmented Generation (RAG) combines traditional language models with external databases to improve natural language processing (NLP) tasks [33], [34], [35]. RAG models use a retriever to retrieve relevant information and a genera- tor to generate answers based on the retrieved information. This improves accuracy and relevance, especially for domain- specific queries [36], [16].\nRAG's strengths lie in its ability to update knowledge bases without retraining and customise components for specific tasks such as cybersecurity [33], [34].\nHowever, RAG struggles with latency and scalability issues, especially when processing concurrent queries [33]. Despite these limitations, RAG remains a versatile tool for a range of NLP applications, from chatbots to content creation. Ongo- ing research focuses on optimizing retrieval mechanisms and computational power [36], [16].\nC. Definitions\nWe report in the following a set of definitions which will be used in the rest of the paper:\n\u2022 Retriever is a component that identifies and retrieves relevant information or documents from a knowledge base. This process is essential to provide the necessary context and content that LLM uses to generate accurate and informed answers [35], [37].\n\u2022 Knowledge Base is a repository of information that the retriever accesses to find relevant data or documents. This is fundamental to the system's ability to retrieve contextually relevant content, essential for generating well-informed and accurate answers [38].\n\u2022 Embeddings are numerical representations of text that assign a low dimension to a term. Within this context, embedding vectors of analogous terms exhibit proximity, encapsulating semantic meaning. This facilitates the com- parison between queries and the knowledge base [39].\n\u2022 Context refers to the relevant information or data re- trieved by the system, which surrounds and informs a particular query. The model requires this contextual information to formulate answers that are precise, com- prehensive, and directly linked to the content found in the knowledge base [40].\n\u2022 Prompt refers to the structured input that is created from the retrieved context, which is then fed into the generative model. This prompt guides the model in generating a coherent, contextually relevant response that directly addresses the user's request [41].\n\u2022 Semantic Similarity evaluates how closely the content of a user's query matches the information in the knowledge base, focusing on meaning rather than word-for-word matching. This evaluation guarantees the relevance and accuracy of the retrieved data and supports the generative model in creating appropriate answers. In MoRSE, we use Cosine Similarity [42] to measure the proximity of embedding vectors because it has a high correlation with human judgment [43].\n\u2022 Multi-hop queries are defined as requests for infor- mation that necessitate indirect reasoning over multiple pieces of interconnected data. They typically arise in complex question-answering tasks where a single piece of evidence is insufficient to resolve the query, and the sys- tem must hop across different data points or documents to piece together a response."}, {"title": "III. MORSE ARCHITECTURE", "content": "This section describes the structure of the MoRSE system in detail. It explains the function of the individual components and how they interact to process a request and generate a response. We used the Langchain framework 10 to develop the MoRSE architecture. The Table I contains the key to the symbols used in this explanation to facilitate the understanding of the following discussion. The first two symbols, (\u03b1) and (\u03b2), are embedding models, while the last two, (\u03b3) and (\u03b8), are Transformer models.\nA. MORSE Overview\nAs shown in Figure 1, MORSE consists of two main compo- nents: a graphical user interface (GUI) and the MORSE core. The GUI enables interaction with the user by allowing the input of queries and displaying the answers in a structured way11.\nThe MORSE core consists of three key components, which in turn manage the user query and compose the answer:\n\u2022 Query Handling Module: This module performs the pre-processing of user queries and specializes in the management of multi-hop queries and complex questions, especially in the context of Common Vulnerability and Exposures (CVEs) and Common Weakness Enumerations (CWEs).\n\u2022 Structured RAG: The first of the two RAGs is composed by retrievers that retrieve information from pre-processed, structured data. The pre-processing phase involves con- verting chunks of text from various sources that are part of the knowledge base, such as academic papers and cybersecurity websites, into well-defined structures. These structures are designed to contain generated ques- tions and contextualized entity descriptions that facilitate the precise retrieval of information in response to user queries.\n\u2022 Unstructured RAG: This RAG is used if the structured RAG could not find a suitable answer. It searches for information in unstructured and unprocessed raw text that belongs to its knowledge base. Accessing unstructured data allows the exploration of data in its original form without the limitations imposed by preprocessing, thus providing a wider range of search options in exchange for a higher response time. This type enables the explo-ration of data in its original form without the restrictions imposed by preprocessing.\nThe RAGs will compose the answer to each query and return it to the GUI for structured visualization. In the following we will detail the components of the MORSE Core.\nB. MORSE Core\na) MORSE Core Workflow: Figure 2 shows the first stage of MORSE Core process, starting with the Query Handling model. This module converts the original query x into an opti- mized version x* (see subsection III-C). First, x* is forwarded to the Structured RAG module for processing. The structured RAG path, denoted as S, begins with the Structured Retrievers, focused on high accuracy and fast responses to efficiently process most queries. Their primary function, S(x*), is to identify and retrieve information pertinent to the query. When activated, the structured retrieval process, which is executed via S(x*), assigns a set of potentially relevant documents from a predefined Knowledge Base to the query x*. In particular, D = top-k(S(x*)) represents the selection of the top k documents that S considers most relevant for the query based on a similarity score. If D is not empty (|D| > 0), this means that a relevant context has been found. The workflow then proceeds to use this context and moves on to the next phase, where the retrieved information (D) is wrapped in a Prompt, which is used by LLM to generate a response. If the Structured Retrievers do not yield relevant documents (|D| = 0), the workflow moves to the unstructured path and calls the Unstructured Retrievers, denoted as U. At this stage, E = top-k(U(x*)) represents the set of documents retrieved by U, which are designed to process complex queries that are not readily covered by structured data patterns. After a successful retrieval of relevant information in one of the two ways indicated by |D| > 0 for structured retrieval or |E| > 0 for unstructured retrieval -the Wrapper module integrates the acquired context and generates a prompt for the Large Language Model (LLM). The LLM then performs Answer Generation, creating a detailed response to the user's question.\nb) RAG Architecture: The RAG architecture of the MORSE system, which is used in both the Structured (III-D) and Unstructured RAG (III-E), follows the same underlying logic shown in Figure 3. This architecture is divided into two parts:\n1) The Retrieval part, which consists of Parallel Retrievers used to collect relevant information for the query.\n2) The Generation part, in which the Large Language Model (LLM) uses the context provided in the Prompt to generate responses. After the retrieval phase, the collected information (info 1 to info N) is merged into a Context, which is Wrapped, along with the user query, in a Prompt used by LLM to generate the Answer. The logic of the architecture is formalized in the Algorithm 1.\nC. Query Handling\nThis component improves the intelligence of the MoRSE system by managing complex query types and enriching the context. Below are the specific functions and the composition of this component:\n1) Functionalities:\n\u2022 Multi-Hop Question Handling: Deals with queries in- volving multiple related entities and allows the system to handle and answer complex multi-hop questions. Existing Retrieval Augmented Generation systems struggle with multi-hop queries due to their design limitations and the lack of a dedicated benchmark dataset for this type of query [48], [49].\n\u2022 Context Enriching: Generates additional questions from each identified entity, expanding and enriching the con- text available for generating informed answers.\n\u2022 Solving the CVE-CWE Conundrum: Effectively han- dles queries related to Common Vulnerabilities and Ex- posures (CVE) and Common Weakness Enumerations (CWE), which are challenging for generative models due to their technical complexity [50], [51].\n2) Components:\n\u2022 User Query: Initiates the process when a user submits a query via the graphical user interface.\nAlgorithm 1 RAG with N Parallel Retrievers\nRequire: User query Q\nEnsure: Answer A\n1: Predefined: Retrievers r1, r2, ..., rN\n2: procedure EXECUTERAG(Q)\n3: Initialize context set C \u2190 {}\n4: for i \u2190 1 to N do \u25b7 In parallel for each i-th retriever\n5: Ii \u2190 ri(Q) \u25b7 Retrieve information using i-th retriever\n6: Sort Ii by relevance scores to find the most perti- nent documents\n7: Itop \u2190 topk(Ii) \u25b7 Select the top-k documents to form a new set Itop\n8: if not empty(Itop) then\n9: C \u2190 C \u222a Itop \u25b7 Incorporate the top-k documents into the context C\n10: end if\n11: end for\n12: if C = {} then\n13: return \"No relevant information found.\"\n14: end if\n15: P \u2190 wrap(C, Q) \u25b7 Construct a prompt from the aggregated context C and the User query Q.\n16: A \u2190 LLM(P) \u25b7 Use the Large Language Model to generate an answer.\n17: return A\n18: end procedure\n\u2022 CVE-CWE Keyword Extraction: Extracts keywords related to CVEs and CWEs when a query is received.\n\u2022 Get CVE Description: Retrieves detailed descriptions of CVEs, including information about vulnerabilities, affected software and finders.\n\u2022 Get CWE Description: Retrieves descriptions of CWEs that provide information about the type of software vul- nerabilities, potential impact and mitigation strategies.\n\u2022 Entity Extractor: Utilizes the Haystack framework 12 with the \u03b8 model to identify and extract relevant entities (people or concepts) from user queries, improving the system's ability to handle Multi-Hop queries.\nThe complete workflow is outlined in Algorithm 2, demon- strating the mechanism of Query Handling within the MORSE system.\nD. Structured RAG\nAs illustrated in Figure 4, the Structured RAG module works post-Query Handling by forwarding refined queries to seven Parallel Retrievers, called Structured Retrievers, each of which specializes in specific cybersecurity topics. Given a query, the information contained in the knowledge base of a Retriever is inserted into the Context if its similarity to the query is above a predefined threshold. In order to establish the threshold for each retriever, we conducted an analysis on the scores of top 50 results from a series of test queries. Thresholds were then determined by assessing the distribution of scores 13. In particular, we used the median value of the test distributions as threshold for the MITRE Retriever and the Malware Retriever, as these typically retrieve shorter texts. For Question Retrieval System, CWE Retriever, Metasploit Retriever and Entity Retriever, we chose the third quartile (Q3) of the test distributions as threshold, as they generally retrieve longer texts. The ExploitDB Retriever works without a threshold and uses the TF-IDF algorithm [52]. To mitigate embedding biases [53], we used two different embeddings for the retrievers, (\u03b1) and (\u03b2). The following paragraphs delineate each retriever's functionalities.\na) Mitre Retriever: The knowledge base of this retriever comes from the website of the MITRE Corporation14. It is structured as a graph database containing two primary node categories: Malware and Techniques. Each Malware node in the database contains a name and a description of MITRE. We create Technique nodes, which consist of technique names and descriptions, by collecting and analysing technique-related\nAlgorithm 2 Query Handling Process\nInput: A user query q on a specific cybersecurity topic. Output: A series of refined queries Q' for in-depth analysis. 1: procedure VULNERABILITYEXTRACTOR(q) 2: Extract keywords K related to CVE and CWE from q. 3: Retrieve detailed descriptions D for each keyword in K from relevant databases. 4: Update q by substituting each keyword in K with its corresponding description from D, yielding q'. 5: return q' 6: end procedure 7: procedure ENTITYEXTRACTOR(q') 8: Utilize the Haystack framework with \"dslim/bert-base- NER\" to detect entities E in q'. 9: Start with an empty Queries_List Q and include q'. 10: for each detected entity e \u2208 E do 11: if e refers to a person (PER) then 12: Append a question about e to Q: \"Who is e?\" 13: else if e pertains to an object or concept (OBJ/CON) then 14: Append a question about e to Q: \"What is e?\" 15: end if 16: end for 17: return Q 18: end procedure 19: procedure QUERYHANDLING(q) 20: qvuln \u2190 VULNERABILITYEXTRACTOR(q) 21: Q' \u2190 ENTITYEXTRACTOR(qvuln) 22: return Q' 23: end procedure\nlinks from the MITRE website. This retriever utilizes embed- dings (\u03b1). To ensure accurate matches, the system exclusively evaluates malware with a similarity score exceeding 0.7, corresponding to the Test distribution's median.\nb) Metasploit Retriever: We have developed the Metasploit Retriever so that it can be effectively integrated into the Metasploit Framework. Its knowledge base includes over 4900 cybersecurity elements, including exploits, encoders, payloads, and various modules. To increase the retrieval speed, we only index salient parts of codes such as code descriptions and exploit information. It performs a semantic search using a similarity value of 0.75 (Q3 of the test distribution) with (\u03b1) embedding, along with a keyword search supported by the TF-IDF algorithm [52].\nc) ExploitDB Retriever: The knowledge base of this retriever is made up of exploits from the ExploitDB framework. These codes often lack descriptions, so we use a keyword search with the TF-IDF algorithm [52] to focus only on important data such as CVE identifiers and author names, which are usually at the beginning of the scripts. To increase the retrieval speed, we only index the first 600 characters of each script, as this information is often contained in the first sections of the code.\nd) Question Retrieval System: This system acts as a knowl- edge base containing questions extracted from chunks of the original documents to better select the most important parts of the document and the explanations contained therein. A user query is compared with these questions and if there is a match, the chunk from which the matching question was extracted is retrieved. During preprocessing, documents are divided into 2000-character chunks. Model (\u03b3) generates about seven questions per chunk, which are refined with Mistral- 7B-Instruct-v0.2 [54] for better alignment. The system uses (\u03b2) embeddings and deploys four retrievers in sequence, each selecting the ten most relevant documents. The results are merged, filtered with a similarity threshold of 0.6 (Q3 of the test distribution) and reordered based on the Lost In the Middle Principle [55], by placing key information at the beginning or end of the context. Redundant questions targeting the same document chunk are removed to streamline the context.\ne) Entity Retriever: This retriever contains entities from document chunks together with their descriptions extracted from the context of the chunk. During the pre-processing phase, we segment the documents into 500-word chunks. This segmentation enables the model (\u03b8) to identify and classify relevant entities more precisely. The contextual descriptions for each entity are then created using the mistralai/Mistral-7B- Instruct-v0.2 model and converted by (\u03b2) embeddings into a searchable format that is retrievable with a similarity threshold of 0.5 (Q3 of the test distribution).\nf) Malware Retriever: This retriever contains more than 1000 malware source codes originating from GitHub pages. The Malware Retriever uses a semantic search with (\u03b1) embeddings and a threshold value of 0.7 (median of the test distribution) to match search queries with malware names. In case of matches, all related files are displayed on the graphical interface.\ng) CWE Retriever: The CWE Retriever uses a semantic search with (\u03b1) embeddings to match user queries with CWE descriptions, as described in Section III-C. Operating with a threshold of 0.7 (Q3 of the test distribution), it showcases the 10 most relevant CWEs. When a query closely aligns with a CWE, the retriever presents detailed information, including code examples.\nh) Context Construction: When creating the final context for the prompt, inputs from two main sources (code snippets and contextual information) are organized to ensure visibility and impact during the Generation Phase:\n\u2022 Code Snippets: Following the Lost In the Middle prin- ciple [55], code snippets from Metasploit and ExploitDB are prioritized at the beginning of the prompt so that they are immediately visible.\n\u2022 Contextual Information: To ensure that essential and concise information is presented to the LLM, the content from the queries Mitre, CWE and Entity retrievers is placed at the end of the prompt. The more comprehensive outputs of the Question Retriever, which is equipped with a reordering function that respects the Lost In the Middle principle [55], are placed in the middle.\nE. Unstructured RAG\nThe unstructured RAG, shown in Figure 5, plays a crucial role in the MoRSE system by handling cybersecurity queries that the structured RAG cannot solve. The module utilizes retrievers, called buffers, to store documents in chunks of 2000 characters while maintaining the integrity of the original information. All buffers work as hybrid retrievers that use both semantic search and keyword search with the BM25 algorithm [56]. Unlike other configurations, these retrievers do not have a fixed threshold for semantic search; instead, they are configured to return the top five documents regardless of similarity scores. This decision enables further Context Transformation process to apply semantic thresholds, ensuring the flexibility and comprehensiveness of the retrieval process.\na) Classification of Buffers: Buffers are categorized accord- ing to the type of data they process, and there are four different types of buffers:\n\u2022 Text Buffers: Process content from websites and blogs with five separate buffers, each analyzing data using the (\u03b1) embedding.\n\u2022 Metasploit Buffers: five buffers containing entire codes from the Metasploit framework that uses the (\u03b1) embed- ding for effective processing.\n\u2022 Code Buffers: A single buffer processes code snippets from Exploit DB and also uses (\u03b1) embedding for optimal analysis.\n\u2022 Paper Buffers: Academic papers are managed by three buffers that use the (\u03b2) embedding to better handle the complex language typical of academic content [57]. This choice is based on the higher performance values of embedding, which indicate better retrieval capabilities.\nThe process Context Transformation refines information from buffers through four phases:\n\u2022 Splitting Stage: Document are split into 300-character chunks, improving relevance selection and reducing noise from larger chunks.\n\u2022 Redundant Content Removal: In this phase, \u03b2 embed- dings are used to remove redundant content from the segmented chunks and improve the clarity and uniqueness of the output.\n\u2022 Filtration Stage: Relevant data chunks are selected using (\u03b2) embeddings with a threshold of 0.6 set to the third quartile (Q3) of similarity results from tests with 156 queries to ensure relevance from the knowledge base.\n\u2022 Reordering Phase: Finally, the data is ordered according to the (Lost in the Middle) principle to prioritize the visibility of important information in the response."}, {"title": "IV. EXPERIMENTS AND EVALUATION", "content": "The evaluation of Retrieval Augmented Generation systems and Large Language Models in the field of cybersecurity is particularly challenging due to their dual role in information retrieval and content generation. The lack of standardized benchmarks covering a wide range of real-world operational cyber tasks complicates the evaluation of LLMs for cyberse- curity [23], [58], [59].\nKey evaluation challenges include verifying the accuracy of the retrieved information, the effectiveness of its use by LLM, and the overall quality of the content generated. Traditional methods that focus on language comprehension may not adequately reflect real-world performance [60].\nTo effectively address these challenges, we developed a three- part evaluation strategy for MoRSE and compared its per- formance with other known LLMs and RAG systems in answering cybersecurity questions. MoRSE was compared to competing models such as GPT-4 0125-Preview, MIXTRAL, HACKERGPT, and GEMINI 1.0 Pro. The three different evaluation test suites are:\n\u2022 Using the RAGAS framework [23], we evaluate MoRSE's responses against a ground truth using a set of metrics.\n\u2022 Using a method proposed by Zheng et al. [24], we computed Elo Ratings for MoRSE and competing models by reference-guided pairwise comparison using GPT-4 0125-Preview as a judge. This provides a quantitative measure of relative performance.\nA. First Test Suite: Ground Truth Assessing alignment using the RAGAS framework\nUsing the RAGAS framework [23], we focused on three metrics: answer relevance, answer similarity, and answer correctness. To calculate these metrics, we used GPT-4 0125- Preview as the underlying model for all calculations.\nAnswer Relevance, shown in Equation 1, measures how pertinent the generated answer is to the given prompt. It is calculated by generating related questions from the model's answer and comparing their embeddings to the original ques- tion using cosine similarity:\nAnswer Relevance = $\\frac{1}{N} \\sum_{2=1}^{N} Cos(E_i, E_o),$ (1)\nwhere Ei and Eo are the \u03b2-embeddings of the generated and original questions, respectively, and N equal to 3, is the number of generated questions.\nAnswer Similarity, shown in Equation 2, evaluates semantic congruence between model-generated responses and prede- fined correct answers, calculated as:\nAnswer Similarity = $\\frac{V_{\\text{ground truth}} \\cdot V_{\\text{generated}}}{\\| V_{\\text{ground truth}} \\| \\| V_{\\text{generated}} \\|},$ (2)\nwhere Vground truth and Vgenerated represent vector representa- tions of the ground truth and generated answers, respectively. Answer Correctness, shown in Equation 3 and 4, evaluates the factual accuracy of generated answers against ground truth. It combines semantic similarities and factual correctness:\nAC = wFC FC + wSS SS, (3)\nwhere FC is the factual correctness, quantified using the F1 score that considers True Positives (TP), False Positives (FP), and False Negatives (FN):\nFC = $\\frac{TP}{\\|TP\\| + 0.5 (\\|FP\\| + \\|FN\\|)},$ (4)\nand SS is the semantic similarity between the generated and ground truth answers. wFC and wSS are the weights assigned to FC and SS, respectively 0.75 and 0.25. In order to calculate TP, FP and FN, RAGAS framework uses the following prompt instruction: Extract the following from the given question and ground truth: \"TP\": statements that are present in both the answer and the ground truth, \"FP\": statements present in the answer but not found in the ground truth, \"FN\": relevant statements found in the ground truth but omitted in the answer. Each of these three metrics requires an embedding model to compute distances between sentences and a large language model (LLM) for evaluating answer relevance and correctness. We chose GPT-4 0125-Preview as the LLM and (\u03b2) as the embedding model.\na) Performance Analysis on General and Multi-Hop Ques- tions: Table II shows the results of MORSE and the other models for General Cybersecurity Questions and Multi-Hop Cybersecurity Questions. The metrics for each model are expressed as mean (\u03bc) and standard deviation (\u03c3), which indicate the average performance and variability, respectively. Insights from General Cybersecurity Questions: For the general cybersecurity questions, MoRSE showed superior per- formance in all metrics, with a mean relevance score of 0.90, a similarity score of 0.95, and a correctness score of 0.71, indicating a high degree of agreement between the answers and the query prompts, as well as factual accuracy. In comparison, all other models showed lower consistency and effectiveness, especially in terms of correctness.\nInsights Multi-Hop Cybersecurity Questions: When eval- uating complex multi-hop cybersecurity queries, the MORSE model outperforms its competitors and proves that it is ca- pable of answering complicated questions. The data shows that MORSE scores consistently high on all metrics, with average scores of 0.93 for relevance and similarity and 0.70 for correctness. Other models show significant performance degradation, particularly in correctness, with GPT-4 0125- Preview achieving a mean score of 0.62 and MIXTRAL 0.61, indicating a lower capacity to handle multi-hop questions.\nb) Performance Analysis on CVE Questions: Table III shows how MoRSE and GPT-4 0125 preview approach 300 CVE queries. We chose GPT-4 0125 preview because it performed best as the second model in both general and multi-hop contexts (see Table II). We focus on answer similarity and correctness metrics for scoring answers because they are strictly based on ground truth and answer relevance does not measure factuality. Moreover, we compute Accuracy metric, calculated by checking whether the models correctly identified the vulnerability in the given queries. Regarding Correctness, the MORSE model scored 0.64 because its responses often include explanation of related exploit codes, which are not present in the ground truth that only describes the vulnerability specifics. This extra information, while useful, lowers the correctness score as it deviates from the expected response. GPT-4 0125-Preview lags behind in this domain-specific chal- lenge. MORSE achieved an accuracy of 84%, surpassing the GPT-4 0125-Preview model, which had an accuracy of 34%. Our comparison reveals that MoRSE significantly outperforms GPT-4 0125-Preview in accurately identifying vulnerabilities.\nWe cannot calculate the accuracy metric for general and multi-hop queries because, unlike CVE queries, they lack strictly factual data points, such as a specific vulnerability to identify. For CVEs, accuracy is straightforward: we check if the model identified the correct vulnerability. In contrast, general and multi-hop questions often lack such clear data and require assessment based on multiple aspects depending on the question type.\nB. Retrievers Impact analysis\nTo calculate the impact of each retriever on 600 questions, we applied a systematic methodology. First, we collected all contexts generated for 150 general questions, 150 multi- hop questions and 300 CVE questions. We then analyzed the frequency with which each retriever was able to suc- cessfully retrieve relevant information within these contexts. The frequency of successful retrievals for each retriever was then calculated as a percentage of the total questions in each category. In this way, we were able to quantify the performance and impact of each retriever in both general and multi-hop question scenarios.\nC. Second Test Suite with LLM as Judge: Reference-Guided Pairwise Comparison"}, {"title": "V. RELATED WORK", "content": "We now overview recent developments in Named Entity Recognition (NER)", "Cybersecurity": "Significant progress has been made in the evolving landscape of Named Entity Recognition (NER) for cybersecurity. In particular"}, {"Cybersecurity": "Knowledge graphs (KGs) are revolutionizing cybersecurity", "Malware Entity Extractor": "nd neural networks that improves security analysis through refined query responses. Gao et al. [78] use of a heteroge- neous information network (HIN) and meta-path approach within a graph convolutional network is characterized by its sophisticated threat type identification capabilities validated by real-world data. Sikos et al. [79] discuss how knowledge graphs help in cybersecurity threat intelligence and```json\n{", "title": "MORSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation", "authors": ["Marco Simoni", "Andrea Saracino", "Vinod Puthuvath", "Maurco Conti"], "abstract": "In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the first specialised AI chatbot for cybersecurity. MoRSE aims to provide comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG (Retrieval Augmented Generation) systems designed to retrieve and organize information from multidimensional cybersecurity contexts. MoRSE differs from traditional RAGs by using parallel retrievers that work together to retrieve semantically related information in different formats and structures. Unlike traditional Large Language Models (LLMs) that rely on Parametric Knowledge Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases in response to user queries. Subsequently, MoRSE uses this information to generate accurate answers. In addition, MoRSE benefits from real-time updates to its knowledge bases, enabling continuous knowledge enrichment without retraining. We have evaluated the effectiveness of MORSE against other state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific questions. The experimental evaluation has shown that the improvement in terms of relevance and correctness of the answer is more than 10% compared to known solutions such as GPT-4 and Mixtral 7x8.", "sections": [{"title": "I. INTRODUCTION", "content": "The increasing frequency and sophistication of new cyber threats have made cybersecurity a critical priority across all sectors, with a 15% increase in three years 1 on data breaches only. In recent years, the amount of cybersecurity- related information has exploded, providing important re- sources to protect against these threats by mitigating risk and improving cybersecurity measures. However, this rapid proliferation of information has led to a cluttered and often unstructured data landscape, complicating the task of deriving actionable insights for professionals [1], [2]. In fact, a timely, accurate, and comprehensive understanding of vulnerabilities, exploits, and defense tactics is crucial, as the promptness of such information can significantly impact cybersecurity decisions [3], [4]. Recently, large language models (LLMs) have become important tools for synthesizing huge amounts of information in various fields, including cybersecurity [5]. However, their reliability varies for technical topics where inaccuracies are critical [6], [7], [8], [9]. LLMs can produce hallucinatory responses, meaning that they produce answers that are not true or reliable, especially struggling with the dynamic and evolving nature of cyber threats [10], [11], [12]. This problem is particularly pronounced in code generation tasks, where LLMs often produce non-functioning code for complex queries [13]. Specifically, when the model does not know the correct answer, hallucinations are inevitable (Epistemic [14]). This can happen if the model does not have enough training data or if its capacity is limited. An example of hallucination in cybersecurity is shown in Example 1 (ground truth) and Example 2 (the answer of GPT-4) to the question: What is CVE-2017-5162?. This shows that GPT-4 is not able to provide the correct answer. To overcome these limitations, it is crucial to build continuous learning mechanisms into LLMS that allow them to dynamically update their knowledge base with the latest information [15]. Although retraining can be time-consuming, updating with new information is essential to maintain the relevance of LLM responses [16], [17]. For this reason, companies are quickly discovering the value of Retrieval Augmented Generation chatbots. Although RAG systems have made significant advances on cybersecurity, re- searchers and practitioners often lacks comprehensive models that unify diverse cybersecurity data and dynamic, real-time updated systems that are essential for addressing the evolving cyber threat landscape. In fact, effective analysis of threats, vulnerabilities and exploits requires a thorough understanding of individual entities and their broader interactions."}, {"title": "II. BACKGROUND", "content": "This section provides an overview of the basic concepts necessary for understanding the architecture of MORSE.\nA. Large Language Models\nLarge language models (LLMs) represent a significant ad- vancement in the field of Natural Language Processing (NLP) and are based on the Transformer model [25]. These models are trained on large text datasets and are able to generate coher- ent and contextually relevant texts based on input prompts. The capabilities of LLMs go beyond text generation and include tasks such as language translation, summarizing, answering questions, etc.\nThe introduction of models such as GPT [26] and BERT [27] has demonstrated the potential of LLMs to revolutionize language understanding and generation through unsupervised and bidirectional training [26], [28]. With the development of GPT-3 [29], the scalability of these models reached new heights, illustrating their ability to perform a wide range of NLP tasks without task-specific training.\nDespite their advantages, LLMs face several challenges. Ethi- cal considerations, such as the spread of bias and misinforma- tion, are a major concern [30]. In addition, the environmental impact of training and operating these computationally inten- sive models has raised questions about their sustainability [31]. Efforts to overcome these challenges include research into more efficient training methods and models that are able to understand and generate texts with greater accuracy and less bias [32].\nB. Retrieval Augmented Generation\nRetrieval-Augmented Generation (RAG) combines traditional language models with external databases to improve natural language processing (NLP) tasks [33], [34], [35]. RAG models use a retriever to retrieve relevant information and a genera- tor to generate answers based on the retrieved information. This improves accuracy and relevance, especially for domain- specific queries [36], [16].\nRAG's strengths lie in its ability to update knowledge bases without retraining and customise components for specific tasks such as cybersecurity [33], [34].\nHowever, RAG struggles with latency and scalability issues, especially when processing concurrent queries [33]. Despite these limitations, RAG remains a versatile tool for a range of NLP applications, from chatbots to content creation. Ongo- ing research focuses on optimizing retrieval mechanisms and computational power [36], [16].\nC. Definitions\nWe report in the following a set of definitions which will be used in the rest of the paper:\n\u2022 Retriever is a component that identifies and retrieves relevant information or documents from a knowledge base. This process is essential to provide the necessary context and content that LLM uses to generate accurate and informed answers [35], [37].\n\u2022 Knowledge Base is a repository of information that the retriever accesses to find relevant data or documents. This is fundamental to the system's ability to retrieve contextually relevant content, essential for generating well-informed and accurate answers [38].\n\u2022 Embeddings are numerical representations of text that assign a low dimension to a term. Within this context, embedding vectors of analogous terms exhibit proximity, encapsulating semantic meaning. This facilitates the com- parison between queries and the knowledge base [39].\n\u2022 Context refers to the relevant information or data re- trieved by the system, which surrounds and informs a particular query. The model requires this contextual information to formulate answers that are precise, com- prehensive, and directly linked to the content found in the knowledge base [40].\n\u2022 Prompt refers to the structured input that is created from the retrieved context, which is then fed into the generative model. This prompt guides the model in generating a coherent, contextually relevant response that directly addresses the user's request [41].\n\u2022 Semantic Similarity evaluates how closely the content of a user's query matches the information in the knowledge base, focusing on meaning rather than word-for-word matching. This evaluation guarantees the relevance and accuracy of the retrieved data and supports the generative model in creating appropriate answers. In MoRSE, we use Cosine Similarity [42] to measure the proximity of embedding vectors because it has a high correlation with human judgment [43].\n\u2022 Multi-hop queries are defined as requests for infor- mation that necessitate indirect reasoning over multiple pieces of interconnected data. They typically arise in complex question-answering tasks where a single piece of evidence is insufficient to resolve the query, and the sys- tem must hop across different data points or documents to piece together a response."}, {"title": "III. MORSE ARCHITECTURE", "content": "This section describes the structure of the MORSE system in detail. It explains the function of the individual components and how they interact to process a request and generate a response. We used the Langchain framework 10 to develop the MORSE architecture. The Table I contains the key to the symbols used in this explanation to facilitate the understanding of the following discussion. The first two symbols, (\u03b1) and (\u03b2), are embedding models, while the last two, (\u03b3) and (\u03b8), are Transformer models.\nA. MORSE Overview\nAs shown in Figure 1, MORSE consists of two main compo- nents: a graphical user interface (GUI) and the MORSE core. The GUI enables interaction with the user by allowing the input of queries and displaying the answers in a structured way11.\nThe MORSE core consists of three key components, which in turn manage the user query and compose the answer:\n\u2022 Query Handling Module: This module performs the pre-processing of user queries and specializes in the management of multi-hop queries and complex questions, especially in the context of Common Vulnerability and Exposures (CVEs) and Common Weakness Enumerations (CWEs).\n\u2022 Structured RAG: The first of the two RAGs is composed by retrievers that retrieve information from pre-processed, structured data. The pre-processing phase involves con- verting chunks of text from various sources that are part of the knowledge base, such as academic papers and cybersecurity websites, into well-defined structures. These structures are designed to contain generated ques- tions and contextualized entity descriptions that facilitate the precise retrieval of information in response to user queries.\n\u2022 Unstructured RAG: This RAG is used if the structured RAG could not find a suitable answer. It searches for information in unstructured and unprocessed raw text that belongs to its knowledge base. Accessing unstructured data allows the exploration of data in its original form without the limitations imposed by preprocessing, thus providing a wider range of search options in exchange for a higher response time. This type enables the explo-ration of data in its original form without the restrictions imposed by preprocessing.\nThe RAGs will compose the answer to each query and return it to the GUI for structured visualization. In the following we will detail the components of the MORSE Core.\nB. MORSE Core\na) MORSE Core Workflow: Figure 2 shows the first stage of MORSE Core process, starting with the Query Handling model. This module converts the original query x into an opti- mized version x* (see subsection III-C). First, x* is forwarded to the Structured RAG module for processing. The structured RAG path, denoted as S, begins with the Structured Retrievers, focused on high accuracy and fast responses to efficiently process most queries. Their primary function, S(x*), is to identify and retrieve information pertinent to the query. When activated, the structured retrieval process, which is executed via S(x*), assigns a set of potentially relevant documents from a predefined Knowledge Base to the query x*. In particular, D = top-k(S(x*)) represents the selection of the top k documents that S considers most relevant for the query based on a similarity score. If D is not empty (|D| > 0), this means that a relevant context has been found. The workflow then proceeds to use this context and moves on to the next phase, where the retrieved information (D) is wrapped in a Prompt, which is used by LLM to generate a response. If the Structured Retrievers do not yield relevant documents (|D| = 0), the workflow moves to the unstructured path and calls the Unstructured Retrievers, denoted as U. At this stage, E = top-k(U(x*)) represents the set of documents retrieved by U, which are designed to process complex queries that are not readily covered by structured data patterns. After a successful retrieval of relevant information in one of the two ways indicated by |D| > 0 for structured retrieval or |E| > 0 for unstructured retrieval -the Wrapper module integrates the acquired context and generates a prompt for the Large Language Model (LLM). The LLM then performs Answer Generation, creating a detailed response to the user's question.\nb) RAG Architecture: The RAG architecture of the MORSE system, which is used in both the Structured (III-D) and Unstructured RAG (III-E), follows the same underlying logic shown in Figure 3. This architecture is divided into two parts:\n1) The Retrieval part, which consists of Parallel Retrievers used to collect relevant information for the query.\n2) The Generation part, in which the Large Language Model (LLM) uses the context provided in the Prompt to generate responses. After the retrieval phase, the collected information (info 1 to info N) is merged into a Context, which is Wrapped, along with the user query, in a Prompt used by LLM to generate the Answer. The logic of the architecture is formalized in the Algorithm 1.\nC. Query Handling\nThis component improves the intelligence of the MoRSE system by managing complex query types and enriching the context. Below are the specific functions and the composition of this component:\n\u2022 Multi-Hop Question Handling: Deals with queries in- volving multiple related entities and allows the system to handle and answer complex multi-hop questions. Existing Retrieval Augmented Generation systems struggle with multi-hop queries due to their design limitations and the lack of a dedicated benchmark dataset for this type of query [48], [49].\n\u2022 Context Enriching: Generates additional questions from each identified entity, expanding and enriching the con- text available for generating informed answers.\n\u2022 Solving the CVE-CWE Conundrum: Effectively han- dles queries related to Common Vulnerabilities and Ex- posures (CVE) and Common Weakness Enumerations (CWE), which are challenging for generative models due to their technical complexity [50], [51].\n2) Components:\n\u2022 User Query: Initiates the process when a user submits a query via the graphical user interface.\nAlgorithm 1 RAG with N Parallel Retrievers\nRequire: User query Q\nEnsure: Answer A\n1: Predefined: Retrievers r1, r2, ..., rN\n2: procedure EXECUTERAG(Q)\n3: Initialize context set C \u2190 {}\n4: for i \u2190 1 to N do \u25b7 In parallel for each i-th retriever\n5: Ii \u2190 ri(Q) \u25b7 Retrieve information using i-th retriever\n6: Sort Ii by relevance scores to find the most perti- nent documents\n7: Itop \u2190 topk(Ii) \u25b7 Select the top-k documents to form a new set Itop\n8: if not empty(Itop) then\n9: C \u2190 C \u222a Itop \u25b7 Incorporate the top-k documents into the context C\n10: end if\n11: end for\n12: if C = {} then\n13: return \"No relevant information found.\"\n14: end if\n15: P \u2190 wrap(C, Q) \u25b7 Construct a prompt from the aggregated context C and the User query Q.\n16: A \u2190 LLM(P) \u25b7 Use the Large Language Model to generate an answer.\n17: return A\n18: end procedure\n\u2022 CVE-CWE Keyword Extraction: Extracts keywords related to CVEs and CWEs when a query is received.\n\u2022 Get CVE Description: Retrieves detailed descriptions of CVEs, including information about vulnerabilities, affected software and finders.\n\u2022 Get CWE Description: Retrieves descriptions of CWEs that provide information about the type of software vul- nerabilities, potential impact and mitigation strategies.\n\u2022 Entity Extractor: Utilizes the Haystack framework 12 with the \u03b8 model to identify and extract relevant entities (people or concepts) from user queries, improving the system's ability to handle Multi-Hop queries.\nThe complete workflow is outlined in Algorithm 2, demon- strating the mechanism of Query Handling within the MORSE system.\nD. Structured RAG\nAs illustrated in Figure 4, the Structured RAG module works post-Query Handling by forwarding refined queries to seven Parallel Retrievers, called Structured Retrievers, each of which specializes in specific cybersecurity topics. Given a query, the information contained in the knowledge base of a Retriever is inserted into the Context if its similarity to the query is above a predefined threshold. In order to establish the threshold for each retriever, we conducted an analysis on the scores of top 50 results from a series of test queries. Thresholds were then determined by assessing the distribution of scores 13. In particular, we used the median value of the test distributions as threshold for the MITRE Retriever and the Malware Retriever, as these typically retrieve shorter texts. For Question Retrieval System, CWE Retriever, Metasploit Retriever and Entity Retriever, we chose the third quartile (Q3) of the test distributions as threshold, as they generally retrieve longer texts. The ExploitDB Retriever works without a threshold and uses the TF-IDF algorithm [52]. To mitigate embedding biases [53], we used two different embeddings for the retrievers, (\u03b1) and (\u03b2). The following paragraphs delineate each retriever's functionalities.\na) Mitre Retriever: The knowledge base of this retriever comes from the website of the MITRE Corporation14. It is structured as a graph database containing two primary node categories: Malware and Techniques. Each Malware node in the database contains a name and a description of MITRE. We create Technique nodes, which consist of technique names and descriptions, by collecting and analysing technique-related\nAlgorithm 2 Query Handling Process\nInput: A user query q on a specific cybersecurity topic. Output: A series of refined queries Q' for in-depth analysis. 1: procedure VULNERABILITYEXTRACTOR(q) 2: Extract keywords K related to CVE and CWE from q. 3: Retrieve detailed descriptions D for each keyword in K from relevant databases. 4: Update q by substituting each keyword in K with its corresponding description from D, yielding q'. 5: return q' 6: end procedure 7: procedure ENTITYEXTRACTOR(q') 8: Utilize the Haystack framework with \"dslim/bert-base- NER\" to detect entities E in q'. 9: Start with an empty Queries_List Q and include q'. 10: for each detected entity e \u2208 E do 11: if e refers to a person (PER) then 12: Append a question about e to Q: \"Who is e?\" 13: else if e pertains to an object or concept (OBJ/CON) then 14: Append a question about e to Q: \"What is e?\" 15: end if 16: end for 17: return Q 18: end procedure 19: procedure QUERYHANDLING(q) 20: qvuln \u2190 VULNERABILITYEXTRACTOR(q) 21: Q' \u2190 ENTITYEXTRACTOR(qvuln) 22: return Q' 23: end procedure\nlinks from the MITRE website. This retriever utilizes embed- dings (\u03b1). To ensure accurate matches, the system exclusively evaluates malware with a similarity score exceeding 0.7, corresponding to the Test distribution's median.\nb) Metasploit Retriever: We have developed the Metasploit Retriever so that it can be effectively integrated into the Metasploit Framework. Its knowledge base includes over 4900 cybersecurity elements, including exploits, encoders, payloads, and various modules. To increase the retrieval speed, we only index salient parts of codes such as code descriptions and exploit information. It performs a semantic search using a similarity value of 0.75 (Q3 of the test distribution) with (\u03b1) embedding, along with a keyword search supported by the TF-IDF algorithm [52].\nc) ExploitDB Retriever: The knowledge base of this retriever is made up of exploits from the ExploitDB framework. These codes often lack descriptions, so we use a keyword search with the TF-IDF algorithm [52] to focus only on important data such as CVE identifiers and author names, which are usually at the beginning of the scripts. To increase the retrieval speed, we only index the first 600 characters of each script, as this information is often contained in the first sections of the code.\nd) Question Retrieval System: This system acts as a knowl- edge base containing questions extracted from chunks of the original documents to better select the most important parts of the document and the explanations contained therein. A user query is compared with these questions and if there is a match, the chunk from which the matching question was extracted is retrieved. During preprocessing, documents are divided into 2000-character chunks. Model (\u03b3) generates about seven questions per chunk, which are refined with Mistral- 7B-Instruct-v0.2 [54] for better alignment. The system uses (\u03b2) embeddings and deploys four retrievers in sequence, each selecting the ten most relevant documents. The results are merged, filtered with a similarity threshold of 0.6 (Q3 of the test distribution) and reordered based on the Lost In the Middle Principle [55], by placing key information at the beginning or end of the context. Redundant questions targeting the same document chunk are removed to streamline the context.\ne) Entity Retriever: This retriever contains entities from document chunks together with their descriptions extracted from the context of the chunk. During the pre-processing phase, we segment the documents into 500-word chunks. This segmentation enables the model (\u03b8) to identify and classify relevant entities more precisely. The contextual descriptions for each entity are then created using the mistralai/Mistral-7B- Instruct-v0.2 model and converted by (\u03b2) embeddings into a searchable format that is retrievable with a similarity threshold of 0.5 (Q3 of the test distribution).\nf) Malware Retriever: This retriever contains more than 1000 malware source codes originating from GitHub pages. The Malware Retriever uses a semantic search with (\u03b1) embeddings and a threshold value of 0.7 (median of the test distribution) to match search queries with malware names. In case of matches, all related files are displayed on the graphical interface.\ng) CWE Retriever: The CWE Retriever uses a semantic search with (\u03b1) embeddings to match user queries with CWE descriptions, as described in Section III-C. Operating with a threshold of 0.7 (Q3 of the test distribution), it showcases the 10 most relevant CWEs. When a query closely aligns with a CWE, the retriever presents detailed information, including code examples.\nh) Context Construction: When creating the final context for the prompt, inputs from two main sources (code snippets and contextual information) are organized to ensure visibility and impact during the Generation Phase:\n\u2022 Code Snippets: Following the Lost In the Middle prin- ciple [55], code snippets from Metasploit and ExploitDB are prioritized at the beginning of the prompt so that they are immediately visible.\n\u2022 Contextual Information: To ensure that essential and concise information is presented to the LLM, the content from the queries Mitre, CWE and Entity retrievers is placed at the end of the prompt. The more comprehensive outputs of the Question Retriever, which is equipped with a reordering function that respects the Lost In the Middle principle [55], are placed in the middle.\nE. Unstructured RAG\nThe unstructured RAG, shown in Figure 5, plays a crucial role in the MoRSE system by handling cybersecurity queries that the structured RAG cannot solve. The module utilizes retrievers, called buffers, to store documents in chunks of 2000 characters while maintaining the integrity of the original information. All buffers work as hybrid retrievers that use both semantic search and keyword search with the BM25 algorithm [56]. Unlike other configurations, these retrievers do not have a fixed threshold for semantic search; instead, they are configured to return the top five documents regardless of similarity scores. This decision enables further Context Transformation process to apply semantic thresholds, ensuring the flexibility and comprehensiveness of the retrieval process.\na) Classification of Buffers: Buffers are categorized accord- ing to the type of data they process, and there are four different types of buffers:\n\u2022 Text Buffers: Process content from websites and blogs with five separate buffers, each analyzing data using the (\u03b1) embedding.\n\u2022 Metasploit Buffers: five buffers containing entire codes from the Metasploit framework that uses the (\u03b1) embed- ding for effective processing.\n\u2022 Code Buffers: A single buffer processes code snippets from Exploit DB and also uses (\u03b1) embedding for optimal analysis.\n\u2022 Paper Buffers: Academic papers are managed by three buffers that use the (\u03b2) embedding to better handle the complex language typical of academic content [57]. This choice is based on the higher performance values of embedding, which indicate better retrieval capabilities.\nThe process Context Transformation refines information from buffers through four phases:\n\u2022 Splitting Stage: Document are split into 300-character chunks, improving relevance selection and reducing noise from larger chunks.\n\u2022 Redundant Content Removal: In this phase, \u03b2 embed- dings are used to remove redundant content from the segmented chunks and improve the clarity and uniqueness of the output.\n\u2022 Filtration Stage: Relevant data chunks are selected using (\u03b2) embeddings with a threshold of 0.6 set to the third quartile (Q3) of similarity results from tests with 156 queries to ensure relevance from the knowledge base.\n\u2022 Reordering Phase: Finally, the data is ordered according to the (Lost in the Middle) principle to prioritize the visibility of important information in the response."}, {"title": "IV. EXPERIMENTS AND EVALUATION", "content": "The evaluation of Retrieval Augmented Generation systems and Large Language Models in the field of cybersecurity is particularly challenging due to their dual role in information retrieval and content generation. The lack of standardized benchmarks covering a wide range of real-world operational cyber tasks complicates the evaluation of LLMs for cyberse- curity [23], [58], [59].\nKey evaluation challenges include verifying the accuracy of the retrieved information, the effectiveness of its use by LLM, and the overall quality of the content generated. Traditional methods that focus on language comprehension may not adequately reflect real-world performance [60].\nTo effectively address these challenges, we developed a three- part evaluation strategy for MoRSE and compared its per- formance with other known LLMs and RAG systems in answering cybersecurity questions. MoRSE was compared to competing models such as GPT-4 0125-Preview, MIXTRAL, HACKERGPT, and GEMINI 1.0 Pro. The three different evaluation test suites are:\n\u2022 Using the RAGAS framework [23], we evaluate MoRSE's responses against a ground truth using a set of metrics.\n\u2022 Using a method proposed by Zheng et al. [24], we computed Elo Ratings for MoRSE and competing models by reference-guided pairwise comparison using GPT-4 0125-Preview as a judge. This provides a quantitative measure of relative performance.\nA. First Test Suite: Ground Truth Assessing alignment using the RAGAS framework\nUsing the RAGAS framework [23], we focused on three metrics: answer relevance, answer similarity, and answer correctness. To calculate these metrics, we used GPT-4 0125- Preview as the underlying model for all calculations.\nAnswer Relevance, shown in Equation 1, measures how pertinent the generated answer is to the given prompt. It is calculated by generating related questions from the model's answer and comparing their embeddings to the original ques- tion using cosine similarity:\nAnswer Relevance = $\\frac{1}{N} \\sum_{2=1}^{N} Cos(E_i, E_o),$ (1)\nwhere Ei and Eo are the \u03b2-embeddings of the generated and original questions, respectively, and N equal to 3, is the number of generated questions.\nAnswer Similarity, shown in Equation 2, evaluates semantic congruence between model-generated responses and prede- fined correct answers, calculated as:\nAnswer Similarity = $\\frac{V_{\\text{ground truth}} \\cdot V_{\\text{generated}}}{\\| V_{\\text{ground truth}} \\| \\| V_{\\text{generated}} \\|},$ (2)\nwhere Vground truth and Vgenerated represent vector representa- tions of the ground truth and generated answers, respectively. Answer Correctness, shown in Equation 3 and 4, evaluates the factual accuracy of generated answers against ground truth. It combines semantic similarities and factual correctness:\nAC = wFC FC + wSS SS, (3)\nwhere FC is the factual correctness, quantified using the F1 score that considers True Positives (TP), False Positives (FP), and False Negatives (FN):\nFC = $\\frac{TP}{\\|TP\\| + 0.5 (\\|FP\\| + \\|FN\\|)},$ (4)\nand SS is the semantic similarity between the generated and ground truth answers. wFC and wSS are the weights assigned to FC and SS, respectively 0.75 and 0.25. In order to calculate TP, FP and FN, RAGAS framework uses the following prompt instruction: Extract the following from the given question and ground truth: \"TP\": statements that are present in both the answer and the ground truth, \"FP\": statements present in the answer but not found in the ground truth, \"FN\": relevant statements found in the ground truth but omitted in the answer. Each of these three metrics requires an embedding model to compute distances between sentences and a large language model (LLM) for evaluating answer relevance and correctness. We chose GPT-4 0125-Preview as the LLM and (\u03b2) as the embedding model.\na) Performance Analysis on General and Multi-Hop Ques- tions: Table II shows the results of MORSE and the other models for General Cybersecurity Questions and Multi-Hop Cybersecurity Questions. The metrics for each model are expressed as mean (\u03bc) and standard deviation (\u03c3), which indicate the average performance and variability, respectively. Insights from General Cybersecurity Questions: For the general cybersecurity questions, MoRSE showed superior per- formance in all metrics, with a mean relevance score of 0.90, a similarity score of 0.95, and a correctness score of 0.71, indicating a high degree of agreement between the answers and the query prompts, as well as factual accuracy. In comparison, all other models showed lower consistency and effectiveness, especially in terms of correctness.\nInsights Multi-Hop Cybersecurity Questions: When eval- uating complex multi-hop cybersecurity queries, the MORSE model outperforms its competitors and proves that it is ca- pable of answering complicated questions. The data shows that MORSE scores consistently high on all metrics, with average scores of 0.93 for relevance and similarity and 0.70 for correctness. Other models show significant performance degradation, particularly in correctness, with GPT-4 0125- Preview achieving a mean score of 0.62 and MIXTRAL 0.61, indicating a lower capacity to handle multi-hop questions.\nb) Performance Analysis on CVE Questions: Table III shows how MORSE and GPT-4 0125 preview approach 300 CVE queries. We chose GPT-4 0125 preview because it performed best as the second model in both general and multi-hop contexts (see Table II). We focus on answer similarity and correctness metrics for scoring answers because they are strictly based on ground truth and answer relevance does not measure factuality. Moreover, we compute Accuracy metric, calculated by checking whether the models correctly identified the vulnerability in the given queries. Regarding Correctness, the MORSE model scored 0.64 because its responses often include explanation of related exploit codes, which are not present in the ground truth that only describes the vulnerability specifics. This extra information, while useful, lowers the correctness score as it deviates from the expected response. GPT-4 0125-Preview lags behind in this domain-specific chal- lenge. MORSE achieved an accuracy of 84%, surpassing the GPT-4 0125-Preview model, which had an accuracy of 34%. Our comparison reveals that MoRSE significantly outperforms GPT-4 0125-Preview in accurately identifying vulnerabilities.\nWe cannot calculate the accuracy metric for general and multi-hop queries because, unlike CVE queries, they lack strictly factual data points, such as a specific vulnerability to identify. For CVEs, accuracy is straightforward: we check if the model identified the correct vulnerability. In contrast, general and multi-hop questions often lack such clear data and require assessment based on multiple aspects depending on the question type.\nB. Retrievers Impact analysis\nTo calculate the impact of each retriever on 600 questions, we applied a systematic methodology. First, we collected all contexts generated for 150 general questions, 150 multi- hop questions and 300 CVE questions. We then analyzed the frequency with which each retriever was able to suc- cessfully retrieve relevant information within these contexts. The frequency of successful retrievals for each retriever was then calculated as a percentage of the total questions in each category. In this way, we were able to quantify the performance and impact of each retriever in both general and multi-hop question scenarios.\nC. Second Test Suite with LLM as Judge: Reference-Guided Pairwise Comparison"}, {"title": "V. RELATED WORK", "content": "We now overview recent developments in Named Entity Recognition (NER), Knowledge Graphs (KGs), and Large Language Models (LLMs) that have contributed to more sophisticated, automated, and adaptive cybersecurity systems.\na) Named Entity Recognition in Cybersecurity: Significant progress has been made in the evolving landscape of Named Entity Recognition (NER) for cybersecurity. In particular, the use of BERT and its Whole Word Masking variant with a BiLSTM-CRF framework has shown remarkable improve- ments in entity recognition metrics [63]. Similarly, by fusing rule-based, dictionary-based methods and CRF, the RDF- CRF model has significantly improved entity recognition in the cybersecurity domain [64]. In addition, a hybrid model that combines deep learning with dictionary-based methods has significantly improved precision and recognition in the identification of complex entities [65]. A study by Srivastava et al. [66] highlighted the differential effectiveness of word embeddings such as fastText, GloVe and BERT, with fine- tuned BERT embeddings with a feed forward network achiev- ing an F1 score of 0.974, highlighting the importance of model adaptation to specific domains. In addition, the introduction of the JCLB model, which combines contrastive learning with a Belief Rule Base [67], showed improved accuracy through semantic expansion and optimized BRB parameters. Li et al. developed NEDetector [68], which improves NER by identifying cybersecurity neologisms with 89.11% accuracy, outperforming traditional trending tools in detecting threats on platforms like Twitter. Extractor distills attack behavior from CTI reports into clear, actionable insights and leverages provenance graphs to improve cyber analytics in threat hunting with real-world effectiveness [69]. Koloveas et al. [70] created the inTIME framework, which leverages machine learning to transform web data into actionable CTI, streamlining the intelligence lifecycle with a unified platform for intelligence collection, analysis and sharing. At the same time, a new threat modeling language has been developed by Xiong et al. [71] based on the MITRE Enterprise ATT&CK Matrix that integrates key elements of enterprise security to improve defense strategies through simulations. Husari et al. [72] further refines CTI analysis by automating the extraction of threat actions from unstructured texts, employing NLP and IR for semantic extraction and aligning attack patterns with standards like STIX 2.1, achieving significant precision and recall in its evaluations.\nMORSE goes beyond current NER technologies by providing dynamic entity recognition and response generation. It identi- fies named entities in user queries in real time and uses this information to generate precise, contextualised responses.\nb) Knowledge Graphs for Cybersecurity: Knowledge graphs (KGs) are revolutionizing cybersecurity, from threat intelli- gence to education. Agrawal et al. [73] have shown how KGs from unstructured text enhance cybersecurity learning, with student feedback highlighting improved understanding and engagement. Sewak et al. developed CRUSH [74], which inte- grates Large Language Models (LLMs) such as GPT-3.5/GPT- 4/ChatGPT with Enterprise Knowledge Graphs (EKGs) to create Threat Intelligence Graphs (TIG) that achieve up to 99% recall in identifying malicious scripts. Li et al. devel- oped AttacKG [75], which automates the extraction of attack techniques from CTI reports into structured knowledge graphs, which greatly improves the analysis of attack patterns with high accuracy and supports advanced threat detection efforts. Liu et al. [76] use NLP to convert over 29k cybersecurity reports into 113,543 actionable Cyber Threat Intelligence (CTI) points by highlighting campaign triggers for better classification accuracy. Piplay et al. [77] describes a system for generating Cybersecurity Knowledge Graphs (CKGs) from Af- ter Action Reports (AARs) using a 'Malware Entity Extractor' and"}]}]}