{"title": "Graph Learning for Planning: The Story Thus Far and Open Challenges", "authors": ["Dillon Z. Chen", "Mingyu Hao", "Sylvie Thi\u00e9baux", "Felipe Trevizan"], "abstract": "Graph learning is naturally well suited for use in planning due\nto its ability to exploit relational structures exhibited in plan-ning domains and to take as input planning instances with\narbitrary number of objects. In this paper, we study the us-age of graph learning for planning thus far by studying the\ntheoretical and empirical effects on learning and planning\nperformance of (1) graph representations of planning tasks,\n(2) graph learning architectures, and (3) optimisation formu-lations for learning. Our studies accumulate in the GOOSE\nframework which learns domain knowledge from small plan-ning tasks in order to scale up to much larger planning tasks.\nIn this paper, we also highlight and propose the 5 open chal-lenges in the general Learning for Planning field that we be-lieve need to be addressed for advancing the state-of-the-art.", "sections": [{"title": "1 Introduction", "content": "Learning for Planning (L4P) has gained significant inter-est in recent years due to advancements of machine learn-ing (ML) approaches across various fields, and also because\nplanning is one of the few problems in AI that has been\nunsolved by deep learning and large models [VMH+23,\nVMSK23, VSK24]. An aim of L4P involves designing au-tomated, domain-independent algorithms for learning do-main knowledge from small training problems for scal-ing up planning to problems of arbitrary size [CdIRF+12,\nTTTX18, TTTX20, DML+19, STT20, KS21, SBG22,\nSBG23, MLTK23, CTT24a, CTT24b, WT24, HTT+24,\nCT24a]. Planning tasks can exhibit arbitrary numbers of ob-jects and are represented with relational languages which\nbegs for the use of learning approaches that operate on rela-tional structures such as graphs.\nIn this paper, we recount the story so far for graph learn-ing for planning. This will be done by studying the contri-butions in the literature and of the authors thus far regarding\nthe three main components of graph learning for planning\nas summarised in Fig. 1: (1) graph representations of plan-ning tasks, (2) graph learning architectures, and (3) optimi-sation formulations for learning. On the experimental side,\nwe present the GOOSE\u00b9 learning framework for leveraging\ngraph learning for planning."}, {"title": "2 Learning for Planning", "content": "Here we briefly introduce and describe the general Learning\nfor Planning (L4P) problem statement. The problem setup\nof L4P involves learning knowledge from a set of training\nproblems in either a supervised, unsupervised, or reinforce-ment learning fashion that may be helpful for planning. This\nis in contrast to planning for each individual problem from"}, {"title": "3 Background", "content": "In this section, we provide the technical background and for-malisms of planning used in works described in the paper.\nHowever, they may be skipped upon first read as they are\nnot entirely necessary for a majority of the paper."}, {"title": "3.1 Planning Task", "content": "Let $[n]$ denote the set of integers $\\{1, ..., n\\}$. A planning\ntask can be understood as a state transition model [GB13]\ngiven by a tuple $\\Pi = (S, A, s_0, G)$ where $S$ is a set of states,\n$A$ a set of actions, $s_0 \\in S$ an initial state, and $G \\subseteq S$ a set of\ngoal states. Each action $a \\in A$ is a function $a : S \\rightarrow S \\cup \\{\\perp\\}$ where $a(s) = \\perp$ if $a$ is not applicable in $s$, and $a(s) \\in S$ is\nthe successor state when $a$ is applied to $s$. A solution for a\nplanning task is a plan: a sequence of actions $\\pi = a_1,..., a_n$\nwhere $s_i = a_i(s_{i-1}) \\neq \\perp$ for $i \\in [n]$ and $s_n \\in G$. A\nstate $s$ in a planning task $\\Pi$ induces a new planning task\n$\\Pi' = (S, A, s, G)$. A planning task is solvable if there exists\nat least one plan."}, {"title": "3.2 Planning Representation", "content": "We now describe the numeric planning formalism from\nPDDL2.1 [FL03]. Numeric planning encapsulates classical\nplanning which is a compact, lifted representation of a plan-ning task with the use of predicate logic and relational nu-meric variables. More specifically, a numeric planning task\nis a tuple $\\Pi = (\\mathcal{O}, \\Sigma_p, \\Sigma_f, A, S_0, G)$, where $\\mathcal{O}$ denotes a set\nof objects, $\\Sigma_p/\\Sigma_f$ a set of predicate/function symbols, $A$ a\nset of action schemata, $s_0$ the initial state, and $G$ now the\ngoal condition. Details of the representation of actions in a\nplanning task induced by grounding action schemata from\n$A$ are not required for understanding the paper. We instead\nfocus on the representation of states and the goal condition.\nEach symbol $\\sigma \\in \\Sigma_p \\cup \\Sigma_f$ is associated with an\narity $\\text{arity}(\\sigma) \\in \\mathbb{N}\\cup \\{0\\}$. Predicates and functions take\nthe form $p(x_1,...,x_{n_p})$ and $f(x_1,...,X_{n_f})$ respectively,\nwhere the $x_i$ denotes the $i$th argument. Propositional and nu-meric variables are defined by substituting objects into pred-icate and function variables. A state $s$ is an assignment of\nvalues in $\\{T, \\perp\\}$ (resp. $\\mathbb{R}$) to all possible propositional (resp.\nnumeric) variables in a state. Following the closed world as-sumption, we can equivalently represent a state as a set of\ntrue propositions and numeric assignments.\nA propositional condition is a positive\n(resp. negative) literal $x = \\top$ (resp. $x = \\perp$) where $x$ is\na propositional variable. A numeric condition has the form\n$\\xi \\triangleright 0$ where $\\xi$ is an arithmetic expression over numeric vari-ables and $\\triangleright \\in \\{\\ge, >,=\\}$. The goal condition $G$ is a set of\npropositional and numeric conditions which we denote $G_p$\nand $G_n$, respectively. A state $s$ satisfies the goal condition $G$\nif $s$ satisfies all its conditions.\nGoal Condition"}, {"title": "3.3 Graphs", "content": "We denote a graph with categorical and continuous node fea-tures and edge labels by a tuple $G = (V, E, F_{cat}, F_{con}, L)$.\nWe have that $V$ is a set of nodes, $E$ a set of edges, $F_{cat}:$\n$V \\rightarrow \\Sigma_V$ the categorical node features, $F_{con}: V \\rightarrow$\n$\\mathbb{R}$ are the continuous node features, and $L : E \\rightarrow \\Sigma_E$\nthe edge labels. The neighbourhood of a node $u \\in V$\nin a graph is defined by $N_1(u) = \\{v \\in V | (u, v) \\in E\\}$.\nThe neighbourhood of a node $u \\in V$ in a graph\nwith respect to an edge label is defined by $N_l(u) =$\n$\\{v \\in V | e = (u, v) \\in E \\land L(e) = l\\}$."}, {"title": "4 Graph Representations", "content": "Representations of planning tasks into ML models have a\ngreat impact on both learning and planning performance.\nGraph representations are particularly well-suited due to\ntheir ability to model relational information of planning\ntasks, as well as their ability to model arbitrarily large plan-ning tasks. In this section, we unify the graph representations\nof planning tasks in the learning for planning literature by\ntaxonomising graph definitions. Next, we summarise theo-retical expressivity results concerning such graphs through\nthe lens of their ability to distinguish planning tasks in\nconjunction with message passing graph neural networks\n(MPNNS) [GSR+17]."}, {"title": "4.1 Graph Taxonomy", "content": "The use of predicate logic in planning representations makes\nit natural for several relational or graph representations to\narise from planning tasks. Indeed, early graph representa-tions for planning tasks were geared towards constructing\nalgorithms or heuristic functions for planners, starting with\nthe planning graph which was a vital component of various\nearly planners [BF97, HN01, GS02], to the usage of various\ndifferent graphs and graph algorithms for computing trans-formations and heuristics of planning tasks [Hel04, HD09],\nand also to the use of detecting planning task symme-tries [PZR11, SKH+15, SRWK19].\nOn the learning side, graph representations were heav-ily relied on for representing planning tasks given the un-bounded nature of planning task sizes. ASNets [TTTX18,\nTTTX20] was the seminal work in this field, which made\nuse of MPNNs for learning policies for probabilistic plan-ning tasks, and was recently extended for numeric plan-ning [WT24]. Later MPNN approaches for planning focused\non learning heuristic or value functions [STT20, SBG22,\nSBG23, CTT24a, CTT24b, CT24a], with exceptions being\nlearning policy rules [DML+19], portfolios [MFH+20] and\ndetecting object importance [SCC+21].\nAn underlying component of all such learning works is\nthat planning tasks are represented as some form of graph.\nHowever, there is no such work which compares all such\ndefinitions and provide a high-level view on similar or dif-ferent characteristics of such representations. In our ongoing\nwork, we have identified following classification of many\nexisting graph representations of planning tasks:\n(1) Grounded Graphs \u2013 nodes represent all possible ground\npropositions and actions of planning tasks, and edges\nare defined from precondition and effect relations of\nactions. Example graphs include ASNets [TTTX18,\nTTTX20], STRIPS-HGN [STT20] and the SLG in\nGOOSE [CTT24a].\n(2) Lifted Graphs with Instantiation Relation (IR) \u2013 nodes\nrepresent task objects and only propositions that are true\nin the state or the goal condition, and edges are defined\nby the relations between object instantiations in ground\npropositions. Example graphs include Muninn [SBG22]\nand the ILG in GOOSE [CTT24a].\n(3) Lifted Graphs with Predicate Relation (PR) \u2013 nodes rep-resent only task objects, and edges are defined by the lo-cation of pairs of objects in n-ary predicates for $n \\ge 2$.\nThis can also be viewed as a line graph of lifted graphs\nwith Object Relation (OR), where nodes represent only\ntask atoms, and edges between atoms sharing an object.\nExample graphs include the PLOI graph [SCC+21] and\nthe Object Binary Structure [H\u016024]."}, {"title": "4.2 Graph Hierarchy", "content": "On top of achieving a taxonomy, we would like to under-stand whether there are any theoretical or practical relation-ships between such graphs. Existing [CTT24a, CTT24b] and\nongoing work of authors have attempted to understand such\nrelationships through the lens of expressive power.\nMore specifically, we compare the expressive power of\ngraph representations based on planning tasks they can dis-tinguish when used with MPNNs. Results primarily boot-strap from existing works measuring the distinguishabil-ity of existing general graph learning models by making\nuse of the well-known result that the colour refinement\nor Weisfeiler-Leman (WL) algorithm subsumes MPNN ex-pressivity [MRF+19, XHLJ19], with novelty lying in iden-tifying the effect of graph representations of planning tasks.\nHowever, we note that our results differ from graph learning\nliterature results which focus on expressiveness [MRM20,\nMRKR22, ZSA22, WCW+23, AKG24] of architectures\nrather than representations as the graph representations are\nassumed to be fixed in graph learning datasets. Results are\nsummarised in Fig. 4 and key takeaways are:\n\u2022 grounded representations implicitly encode instantiation\nrelations and thus are more expressive than lifted repre-sentations with IR (green edges)\n\u2022 not encoding planning domain information (predicates\nand schemata) results in lower expressivity across dif-ferent graph classes (red edges)\n\u2022 lifted graphs with PR are incomparable to both lifted\ngraphs with IR, and grounded graphs under a weaker no-tion of expressivity (blue edges)"}, {"title": "5 Graph Learning Models", "content": "Another core component of a graph learning approach for\nplanning is the graph learning model itself. ML techniques\ncan generally be classified into deep learning and classi-cal taxonomies. Deep learning [LBH15] pipelines automati-cally compute parameterised encoder functions which con-vert raw input data into latent features which they deem use-ful for inferring outputs, generally using some form of neu-ral network. In contrast, classical machine learning pipelines\npredefine the feature extractor for converting raw input data\ninto feature vectors which are used by an arbitrary chosen\ndownstream inference model, such as a regression model or\ndecision tree."}, {"title": "Classical ML is Better Suited than Deep Learning for Planning", "content": "The graph learning equivalent of such models include GNNs\nand graph kernels. Due to the popularity of deep learning,\nalmost all recent works in learning for planning since the\nintroduction of ASNets employ some variant of GNN as the\nunderlying learning model. However, it has been shown very\nrecently that classical ML approaches such as linear graph\nkernels significantly outperform GNN approaches for plan-ning [CTT24b], over various metrics and often by several\norders of magnitude.\nThis approach was again motivated by the simple, yet\nwell-known result that the WL algorithm upper bounded the\nexpressivity of MPNNs [MRF+19, XHLJ19], and such an\nalgorithm can be converted into the WL graph kernel for\nextracting features for graphs [SSVL+11]. The WL graph\nkernel has the same polynomial computational complexity\nas the typical MPNN but without the need to perform ma-trix operations which increases its runtime often by a signif-icant constant factor. Furthermore, the features can be com-bined with a simple linear model which lead to explainable\nand very fast models in terms of training and evaluation.\nGiven that planning is a time-sensitive task and graph learn-ing models may be called many times during planning, such\nspeedups from using classical ML approaches over deep\nlearning methods can lead to greater gains. It is also the case\nthat generating training labels for many and large planning\ntasks is expensive such that one should use models with high\ndata efficiency.\nIndeed, Fig. 5 shows how linear models using WL fea-tures exhibit significantly faster training times and fewer pa-rameters than GNN models, and Section 7 later provides re-sults also showcasing better planning performance. We can\nfurther contrast this with the era of scaling with LLMs ex-hibiting parameters in the order of billions and training data\nand times so significant that they exhibit large monetary\ncosts. This story concerning cheap models outperforming\nlarge models still holds true in numeric planning [CT24a],\nwhere one may think neural networks may be more suited to\nreasoning over numbers as function approximators."}, {"title": "6 Optimisation", "content": "Typical machine learning tasks have a well defined problem\nto solve, such as classification or regression, which usually\ngive rise to obvious loss functions to optimise. Conversely,\nthe focus of planning generally involves solving problems\nefficiently, and in some cases optimising over solution qual-ity. Also in contrast to Reinforcement Learning, planning\ndoes not exhibit dense reward signals which can be used to\nguide learning. Thus, there is not obvious method for decid-ing the optimisation problem to solve for training learners\nfor planning."}, {"title": "6.1 Learning Policies", "content": "The seminal deep learning for planning model, AS-Nets [TTTX18, TTTX20], learns policies for solving plan-ning tasks in a RL-style fashion. A benefit of learning poli-cies is that when learned correctly, execution of policies is\nlinear time in the size of the solution. Furthermore, some\nplanning domains only require a 'trick' to be learned in order\nto solve them which can be represented by a neural network\narchitecture. One downside of learning policies is that ex-ecution of such policies have no completeness guarantees,"}, {"title": "6.2 Learning Optimal Heuristic Functions", "content": "Common L4P approaches exploit the fact that most state-of-the-art planners are based on heuristic search, giving rise to\nthe idea of learning reusable heuristic functions\u00b2 in a su-pervised fashion for guiding search. Contrary to policies,\nlearned heuristic functions used with search algorithms such\nas GBFS are complete and will always (eventually) return a\nsolution if one exists.\nHowever, most approaches end up performing the naive\napproach of trying to learn the optimal $h^*$ heuristic for a\ndomain from samples via mean squared error loss. This is\ngenerally not the best idea as theoretical arguments can be\nmade from computational complexity theory, or explicit ex-amples [CTT24a] stating that learning $h^*$ is not possible\nfor some domains, and furthermore, it may not even be\nnecessary to just solve planning tasks. This is because $h^*$\nis derived from optimal solutions which are stronger than\narbitrary solutions for planning tasks. Furthermore, opti-mising cost-to-go as mean squared error (MSE) estimates\ndoes not match up with the original derivation of MSE loss\nas maximising likelihood under Gaussian distribution as-sumptions, which is not the case for planning heuristic val-ues [NAMF24].\nNevertheless, the reason why researchers generally opt to\nlearn $h^*$ is because it is a canonical label for training tasks\nthat can be computed automatically and efficiently. More\nspecifically, it is easier to run an optimal planner on an ar-bitrary task to receive optimal plans and $h^*$ labels, than to\nmanually label tasks with domain-dependent polynomial-time heuristic functions that still guide search efficiently.\nOne such example of heuristics with this property are dead-end avoiding, descending heuristics [SPRH16]. Another rea-son for learning $h^*$ is that in the best case scenario where $h^*$\nis learned correctly, problems can be solved in linear time\nwith respect to solution size."}, {"title": "6.3 Learning Ranking Functions", "content": "Garrett et al. [GKL16] proposed to frame heuristic functions\nnot as learning cost-to-go estimates but as ranking states for\nexpansion in GBFS. An advantage of ranking as a learning\ntask is that it aligns better with the intent of GBFS and fur-thermore allows for a larger hypothesis space of solutions.\nThis idea was extended to neural network architectures in\nindependent works [CEKP23, HTT+24]. However, the latter\nwork identified that the data used in [CEKP23] is quadratic\nin the training plans by encoding all possible ranking pairs,"}, {"title": "7 Experimental Results", "content": "In this section, we provide an outline of experimental results\narising from our work on graph learning for both classical\n(Section 7.1) and numeric (Section 7.2) planning, and sum-marise key takeaways (Section 7.3)."}, {"title": "7.1 Classical Planning", "content": "For classical planning, we use the benchmarks from the\nLearning Track of the 2023 International Planning Compe-tition (IPC23LT) [TAE+24]. The IPC23LT benchmarks em-phasise generalisation across number of objects, where as\ndisplayed in Fig. 7, the testing tasks are often up to an order\nof magnitude larger than the training tasks in the number\nof objects. Furthermore, sizes of planning task state spaces\ngenerally scale in a high polynomial variable with respect to\nthe number of objects. The primary metric we discuss in this\nsection is the coverage of graph learning planners across all\n900 problems (10 domains \u00d7 90 problems) in the IPC23LT\nwithin a fixed 30 minute time and 8GB memory limit for\ntesting, i.e. solving a single planning task. Fig. 8 displays\nthe coverage of various baseline planners and graph learn-ing planner configurations abbreviated as follows:\n\u2022 hFF: GBFS + the $h_{FF}$ heuristic [HN01]\n\u2022 LAMA: a strong satisficing planner which uses multi-ple queues, helpful actions, and lazy heuristic evalua-tion [RW10]\n\u2022 $h_{GNN}^{cost}$: GBFS + a GNN heuristic trained with mean squared error loss on $h^*$ values, detailed in [CTT24b]\n\u2022$h_{WL}^{cost}$: GBFS + a linear WL heuristic trained with Gaus-sian Process Regression, detailed in [CTT24b]\n\u2022$h_{WL}^{rank}$: GBFS + a linear WL heuristic trained with the LP ranking formulation in [CT24a]\n\u2022$h_{WL}^{grid}$: GBFS + a grid search over WL heuristic configura-tions performed on each domain\n\u2022$h_{WL}^{ptfl}$: GBFS + a parallel portfolio over WL heuristic con-figurations"}, {"title": "7.2 Numeric Planning", "content": "In [CT24a], we also performed experiments on numeric en-codings of domains from the IPC23LT, with the exception\nof two domains which have no benefit from numeric en-codings. Fig. 8 displays the coverage of various baseline\nnumeric planners and graph learning planner configurations\nabbreviated as follows:\n\u2022 $h^{mrp}$: GBFS + the $h^{mrp}$ heuristic [SSSG20]\n\u2022 M(3h||3n): state-of-the-art numeric planner with multi-ple queues and novelty heuristics [CT24b]\n\u2022 $h_{GNN}^{cost}$: GBFS + a GNN heuristic trained with mean squared error loss on $h^*$ values\n\u2022 $h_{GNN}^{rank}$: GBFS + a GNN heuristic trained with a differen-tiable ranking loss\n\u2022 $h_{WL}^{cost}$: GBFS + a linear WL heuristic trained with Gaus-sian Process Regression\n\u2022 $h_{WL}^{rank}$: GBFS + a linear WL heuristic trained with a LP ranking formulation"}, {"title": "7.3 Key Takeaways", "content": "Classical ML consistently outperform deep learning for\nsymbolic planning From Fig. 8, we observe that classi-cal ML approaches using learned WL heuristics outperform\ntheir deep learning counterparts over different optimisation\nformulations in terms of coverage. In classical planning,\n$h_{WL}^{rank}$ outperforms $h_{GNN}^{cost}$ by 89 problems, while in numeric\nplanning, $h_{WL}^{rank}$ (resp. $h_{WL}^{cost}$) outperforms $h_{GNN}^{rank}$ (resp. $h_{GNN}^{cost}$)\nby 60 (resp. 119) problems.\nLearned ranking functions consistently outperform\nlearned cost-to-go estimates From Fig. 8, we note that\nlearned ranking heuristics outperform their cost-to-go esti-mate counterparts over graph learning models in terms of\ncoverage. In classical planning, $h_{WL}^{rank}$ outperforms $h_{WL}^{cost}$ by\n18 problems, while in numeric planning, $h_{WL}^{rank}$ (resp. $h_{GNN}^{rank}$)\noutperforms $h_{WL}^{cost}$ (resp. $h_{GNN}^{cost}$) by 66 (resp. 7) problems.\nLearned heuristics with simple search are competitive\nwith strong planners From Fig. 8 for classical planning,\nwe observe that single instantiations of learned WL heuris-tics (502 and 520) with GBFS alone do not outperform the\nstrong LAMA planner (557). However, taking the best con-figuration for each domain (580), or using multiple threads\nfor parallelised portfolios (619), learned heuristics with sim-ple search algorithms can be competitive with planners em-ploying stronger search algorithms. However, for numeric\nplanning, learned heuristics with GBFS alone (445) are a lot\nmore competitive with numeric planners (369)."}, {"title": "8 Open Challenges", "content": "The general L4P field is still in its infancy and poses mul-tiple open challenges for future research. This is reflected\nby the fact that there are still many domains and problems\nin the tested benchmarks that remain unsolved for L4P ap-proaches. We highlight the themes of core open challenges\nthat we believe are crucial for advancing the state-of-the-art."}, {"title": "I Expressivity", "content": "A core challenge that needs to be addressed by L4P ap-proaches is the concept of expressivity: the ability of a\nmodel to represent solutions to planning domains. Propo-sitional planning is PSPACE-complete in general [Byl94]\nbut even many domains that are solvable in polynomial\ntime cannot be solved by learning approaches [SBG22,\nCTT24a]. Recent L4P approaches use some variant of mes-sage passing neural networks which are known to be theoret-ically bounded by two-variable counting logics [MRF+19,\nXHLJ19, BKM+20, Gro21], with some works studying\nthe effect of (approximate) higher-order graph learning ap-proaches [CTT24b, SBG24] and performing distinguishabil-ity tests of models [H\u016024, DSBG24].\nHowever, this direction of research mirrors the graph\nlearning literature of building more expressive mod-els [MRM20, KJM20, ACGL21, BHG+21, MRKR22,\nFCL+22, ZSA22, ZJAS22, WCW+23, BFZB23, AKG24]\nat the cost of runtime complexity, unclear generalisa-tion performance and unclear relevance to downstream\ntasks [MFD+24]. Furthermore, tractable graph learning ar-chitectures alone have yet to be able to achieve expressivity\nfor the basic P-time complexity time which is not directly\nachievable with finite-variable counting logics, and bounded\nnumber of message passing layers.\nPossible directions for research in expressivity involve\nbuilding or making use of models which can handle re-cursion such as inductive logic programming [CDEM22]\nor Generalised Planning [Sri10, CAJ19] approaches, mo-tivated by the fact that P is captured by first-order logic\nwith transitive closure [Var82, Imm82]. Indeed, some earlier\nworks have studied rule-based approaches for learning poli-cies or subgoals for planning [Kha99, GT04, IM19, BG24,\nDSG24], with recent work explicitly studying the use of Dat-alog for directly encoding planning domain solutions and\nsolvability [GRH24, CH\u016024]."}, {"title": "II Generalisation", "content": "Another key challenge in L4P is understanding both theoret-ical and empirical generalisation results for planning. L4P\nis inherently an out-of-distribution task as testing tasks are\narbitrarily large and hence are drawn from a different dis-tribution from bounded-size training tasks. Thus, exploit-ing common generalisation theory tools such as VC dimen-sion [Vap98] and Rademacher complexity [BM01], which\nassume similar training and testing probability distributions,\nfor bounding generalisation theory is not straightforward.\nConversely, planning representations often contain infor-mation encoded as rich relational or logical structures. Re-lated to the concept of expressivity previously, researchers\nhave developed theoretical frameworks to better under-stand the behaviour of planning domains such as nov-elty width [LG12], correlation complexity [SPRH16], the\nriver measure [DH24a], and methods to bound such mea-sures [DH24b]. These tools may provide insights for devel-oping generalisation theory for planning."}, {"title": "III Optimisation", "content": "As discussed in Section 6, there is no clear consensus on\nthe best optimisation criteria for learning for planning as\nthis may depend on the domain, learning architecture and\nalso training data. Although ranking as discussed provides a\nmuch better suited optimisation criteria for learning heuris-tics for planning, it may be the case that learning other forms\nof domain knowledge such as policies, subgoals [DSG24],\nand search effort estimates [OL21, FGT+22] may be easier\nand well-suited for specific domains. In other words, there\nis no single optimisation criterion that is best for all plan-ning domains. Theoretical and empirical results on optimi-sation criteria that are well suited for specific planning do-main characteristics are still unknown."}, {"title": "IV Collecting Data", "content": "Another challenge in L4P is the problem of deciding\nhow and what data to collect for training, mirroring the\nexploration-exploitation tradeoff in RL [SB98]. This also re-lates to the optimisation problem as the choice of optimisa-tion criteria depends on the type of data collected. For exam-ple, optimal plan traces are sufficient for learning cost-to-go\nor ranking heuristic functions, but not optimal policies. This\nis because there may be more than one optimal action to\ntake at each state that cannot be deduced from optimal plans.\nASNets [TTTX18, TTTX20] handles this issue similar to\nthe RL approach of exploring with a partially learned policy\nand exploiting with a teacher planner as proxy for a reward\nfunction. Learning subgoals as policy sketches requires ex-panding entire state spaces [DSG24] which does not scale to\nhigh predicate or asymmetric domains, even with symmetry\ndetection [DSBG24].\nSimilarly to the optimisation problem, theoretical and em-pirical results on how to best collect and how much data to\ncollect for L4P are much appreciated."}, {"title": "V Fair Comparisons", "content": "A final challenge in L4P is the problem of fairly compar-ing different methodologies and approaches, given the addi-tional experimental variables of training data and training\ntime introduced when learning is involved. Thus, this re-sulted in various researchers and groups employing different\nbenchmarks and evaluation strategies. The Learning Track\nof the 2023 International Planning Competition [TAE+24]\nwas a welcome addition for standardising the set of plan-ning domains and testing tasks. However, although training\ntasks are given, the method of generating useful labels is not\nstandardised and instead competitors were given a fixed time\nlimit to both generate labels and train models. This leads to\nan undesirable scheduling task of trading off between data\ngeneration and training time.\nPossible suggestions for future proposed benchmarks or\ngood practices to follow include (1) providing all baselines\nwith the same set of labelled training data generated by a\nfixed time limit that benefits all models, and/or (2) recording\ntraining time and amount of data used for all baselines."}, {"title": "9 Conclusion", "content": "Learning for Planning (L4P) is an increasingly popular re-search field, with planning being one of the few problems in\nAI that has been unsolved by deep learning and large mod-els [VMH+23, VMSK23, VSK24]. This paper summarises\nthe contributions of the authors in L4P, with a specific focus\non using cheap and efficient graph learning approaches for\nplanning. Furthermore, we have identified 5 key challenges\nin L4P that still remain unsolved and offer plenty of oppor-tunities for future research for progressing the field."}]}