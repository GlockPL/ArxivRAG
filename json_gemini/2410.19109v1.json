{"title": "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework", "authors": ["Yifan Wang", "Vera Demberg"], "abstract": "Despite significant advancements in natural language generation, controlling language models to produce texts with desired attributes remains a formidable challenge. In this work, we introduce RSA-Control, a training-free controllable text generation framework grounded in pragmatics. RSA-Control directs the generation process by recursively reasoning between imaginary speakers and listeners, enhancing the likelihood that target attributes are correctly interpreted by listeners amidst distractors. Additionally, we introduce a self-adjustable rationality parameter, which allows for automatic adjustment of control strength based on context. Our experiments, conducted with two task types and two types of language models, demonstrate that RSA-Control achieves strong attribute control while maintaining language fluency and content consistency. Our code is available at https://github.com/Ewanwong/RSA-Control.", "sections": [{"title": "1 Introduction", "content": "Controllable text generation (CTG) focuses on producing natural language texts with specified attributes, such as sentiment and readability. This capability is vital for developing functional and reliable natural language generation (NLG) systems. For instance, dialogue systems must be regulated to consistently generate responses that are low in toxicity and bias (Gehman et al., 2020; Kumar et al., 2023; Sheng et al., 2021). Similarly, summarization systems are expected to be able to create customized summaries for different users by adjusting readability (Ribeiro et al., 2023).\nMany existing studies in CTG rely on fine-tuning pre-trained language models (PLMs) on attribute-specific datasets (Keskar et al., 2019; Gururangan et al., 2020). However, due to the increasing scale of PLMs, fine-tuning them has become resource-intensive. Decoding-based methods that navigate the PLM decoding process using guide modules (Dathathri et al., 2020; Yang and Klein, 2021; Krause et al., 2021; Liu et al., 2021) have achieved strong attribute control and reduced the need to fine-tune PLMs, but still require additional datasets and computational resources for training the guide modules. Besides, introducing external components could potentially hurt coherence during decoding (Xu et al., 2021). As large-scale PLMs become more adept at understanding human instructions (Touvron et al., 2023; Achiam et al., 2023), prompt-based methods have emerged as a lightweight way to adapt PLMs to new domains (Brown et al., 2020; Schick and Sch\u00fctze, 2021). Previous research has explored direct prompting (Mattern et al., 2022) and using auxiliary prompts (Schick et al., 2021; Leong et al., 2023; Yona et al., 2023) for CTG. Nonetheless, due to the black-box nature of PLMs, precise control via prompt-based methods is still challenging and often leads to unexpected outputs (Zhang et al., 2023).\nIn this work, we introduce RSA-Control, a novel CTG method that bridges decoding-based and prompt-based paradigms through the computational pragmatic framework of Rational Speech Acts (RSA) (Frank and Goodman, 2012). The RSA framework elucidates the effective and efficient human communication through a mutual reasoning process: speakers adjust their utterances by reasoning about listeners' perceptions, while listeners, in turn, infer the speakers' intentions. Inspired by RSA's success in modeling conversational behaviors, our approach explicitly models the interactions between speaker and listener modules, enabling a pragmatic speaker to generate utterances that ensure the accurate perception of desired attributes by the listeners. As illustrated in Figure 1, RSA-Control constructs a guide module (pragmatic listener \\(L_1\\)) using PLMs with auxiliary control prompts (literal speaker \\(S_0\\)) to achieve controllable decoding of the pragmatic speaker \\(S_1\\). By replacing fine-tuned discriminator mod-"}, {"title": "2 Related Work", "content": "Fine-tuning Methods Alongside the success of PLMs in generating coherent natural language texts, studies on controlling attributes in generation have also emerged (Zhang et al., 2023). Among various methods, the most straightforward involves adapting models to specific domains. Gururangan et al. (2020) demonstrate that further training on attribute-specific datasets can improve the capacity of PLMs in these areas. Similar approaches have been employed to reduce toxicity (Arora et al., 2022; Wang et al., 2022; Zheng et al., 2023), control language styles (Ficler and Goldberg, 2017; Zhang and Song, 2022), and align PLMs with human preferences (Ziegler et al., 2019; Wei et al., 2022; Ouyang et al., 2022). Nevertheless, these methods are computationally expensive, especially given the ever-larger scale of current PLMs.\nDecoding-based Methods Another line of work, known as decoding-based methods, employs external components to navigate PLM decoding (Yang and Klein, 2021; Meng et al., 2022; Zhang and Wan, 2023; Dekoninck et al., 2024). PPLM (Dathathri et al., 2020) trains attribute classifiers and updates hidden states of PLMs with their gradients to orient the generation towards desired attributes. GeDi (Krause et al., 2021) uses generative classifiers with class conditional language models to guide decoding. Similarly, DExperts (Liu et al., 2021) leverages expert and anti-expert modules to modify model logits. Energy-based models apply multiple modular constraints during decoding to enforce lexical or attribute control (Qin et al., 2022; Mireshghallah et al., 2022). Although decoding-based methods avoid fine-tuning PLMs, they still require training auxiliary modules on attribute-specific datasets. In contrast, our method replaces fine-tuned modules with prompted PLMs, eliminating the need for data collection and model training. Additionally, introducing external components can risk compromising language abilities and encoded knowledge of PLMs (Xu et al., 2021), whereas our approach relies solely on the PLMs themselves.\nPrompt-based Methods The advent of large language models (Brown et al., 2020; Raffel et al., 2020; Achiam et al., 2023) has enabled the adaptation of models to new tasks using only natural language task descriptions (Puri and Catanzaro, 2019; Schick and Sch\u00fctze, 2021). However, directly prompting PLMs to control attributes has shown poor performance in foundation models (Mattern et al., 2022). As a result, various methods have been proposed to extend the prompt-based"}, {"title": "2.1 Controllable Text Generation", "content": "Fine-tuning Methods Alongside the success of PLMs in generating coherent natural language texts, studies on controlling attributes in generation have also emerged (Zhang et al., 2023). Among various methods, the most straightforward involves adapting models to specific domains. Gururangan et al. (2020) demonstrate that further training on attribute-specific datasets can improve the capacity of PLMs in these areas. Similar approaches have been employed to reduce toxicity (Arora et al., 2022; Wang et al., 2022; Zheng et al., 2023), control language styles (Ficler and Goldberg, 2017; Zhang and Song, 2022), and align PLMs with human preferences (Ziegler et al., 2019; Wei et al., 2022; Ouyang et al., 2022). Nevertheless, these methods are computationally expensive, especially given the ever-larger scale of current PLMs.\nDecoding-based Methods Another line of work, known as decoding-based methods, employs external components to navigate PLM decoding (Yang and Klein, 2021; Meng et al., 2022; Zhang and Wan, 2023; Dekoninck et al., 2024). PPLM (Dathathri et al., 2020) trains attribute classifiers and updates hidden states of PLMs with their gradients to orient the generation towards desired attributes. GeDi (Krause et al., 2021) uses generative classifiers with class conditional language models to guide decoding. Similarly, DExperts (Liu et al., 2021) leverages expert and anti-expert modules to modify model logits. Energy-based models apply multiple modular constraints during decoding to enforce lexical or attribute control (Qin et al., 2022; Mireshghallah et al., 2022). Although decoding-based methods avoid fine-tuning PLMs, they still require training auxiliary modules on attribute-specific datasets. In contrast, our method replaces fine-tuned modules with prompted PLMs, eliminating the need for data collection and model training. Additionally, introducing external components can risk compromising language abilities and encoded knowledge of PLMs (Xu et al., 2021), whereas our approach relies solely on the PLMs themselves.\nPrompt-based Methods The advent of large language models (Brown et al., 2020; Raffel et al., 2020; Achiam et al., 2023) has enabled the adaptation of models to new tasks using only natural language task descriptions (Puri and Catanzaro, 2019; Schick and Sch\u00fctze, 2021). However, directly prompting PLMs to control attributes has shown poor performance in foundation models (Mattern et al., 2022). As a result, various methods have been proposed to extend the prompt-based"}, {"title": "2.2 Rational Speech Acts Framework", "content": "The Rational Speech Acts framework is a computational pragmatic model that involves mutual reasoning between speakers and listeners about each other's intentions and interpretations (Frank and Goodman, 2012). This framework has been successfully applied to explain complex pragmatic phenomena in human languages (Lassiter and Goodman, 2013; Kao et al., 2014a,b). Recently, RSA has been adapted to improve informativeness in various NLG tasks (Andreas and Klein, 2016; Cohn-Gordon et al., 2018, 2019; Cohn-Gordon and Goodman, 2019; Shen et al., 2019), and Kim et al. (2020, 2021) exploit RSA to enhance persona and emotion consistency in dialogue systems. Nevertheless, its application to CTG remains underexplored. In this work, we investigate how RSA can improve attribute control in NLG tasks and extend the framework for automatic control strength adjustment by introducing a self-adjustable rationality parameter."}, {"title": "3 Method", "content": "Given input content \\(c\\) and desired attribute \\(a\\), the goal of CTG is to generate a sequence \\(W\\) that is fluent and adheres to \\(c\\) while demonstrating \\(a\\). In practice, \\(W\\) is typically generated incrementally, with the modeling of next token probabilities conditioned on the previously generated tokens. Thus, the task of CTG can be formulated as modeling \\(P(W_n|W_{<n}, c, a)\\) and then sampling an utterance \\(W\\) from the conditional distribution \\(P(W_{1:N}|c, \\alpha) = \\prod_{n=1}^{N} P(w_n|W_{<n}, c, a)\\).\nDepending on the task type, the input content \\(c\\) can vary: in open-ended generation, \\(c\\) is empty and the generation is solely conditioned on \\(a\\) and previously generated tokens \\(W_{<n}\\); in input-output tasks such as summarization, \\(c\\) can include task instructions, input documents and other task-specific components."}, {"title": "3.1 Task Formulation", "content": "Given input content \\(c\\) and desired attribute \\(a\\), the goal of CTG is to generate a sequence \\(W\\) that is fluent and adheres to \\(c\\) while demonstrating \\(a\\). In practice, \\(W\\) is typically generated incrementally, with the modeling of next token probabilities conditioned on the previously generated tokens. Thus, the task of CTG can be formulated as modeling \\(P(W_n|W_{<n}, c, a)\\) and then sampling an utterance \\(W\\) from the conditional distribution \\(P(W_{1:N}|c, \\alpha) = \\prod_{n=1}^{N} P(w_n|W_{<n}, c, a)\\).\nDepending on the task type, the input content \\(c\\) can vary: in open-ended generation, \\(c\\) is empty and the generation is solely conditioned on \\(a\\) and previously generated tokens \\(W_{<n}\\); in input-output tasks such as summarization, \\(c\\) can include task instructions, input documents and other task-specific components."}, {"title": "3.2 RSA-Control", "content": "Standard RSA involves selecting utterances from a finite space, which can limit its flexibility. To address this, we extend the incremental RSA approach from Cohn-Gordon et al. (2019). Specifically, a pragmatic speaker \\(S_1\\) generates the next token that maximizes a utility function \\(U\\):\n\\(P_{S_1}(W_n|W_{<n}, c, a) \\propto exp(U(w_n|W_{<n},c,a))\\) (1)\nWe decompose \\(U\\) into two parts: a content utility function \\(U_c\\) and an attribute utility function \\(U_a\\) which account for different goals. \\(U_c\\) ensures consistency with content \\(c\\), while \\(U_a\\) conveys the desired attribute \\(a\\). Given that PLMs excel at generating coherent texts but struggle with attribute control, we implement \\(U_c\\) with a PLM and define \\(U_a\\) in an RSA manner, i.e., as the log probability that an imaginary pragmatic listener can infer \\(a\\) amidst predefined distractor attributes. Importantly, we assume conditional independence in \\(U_a\\) between content \\(c\\) and attribute \\(a\\) given \\(w_{<n}\\), as the listener is often unaware of \\(c\\) in a conversation. For example, a listener generally does not know which articles a speaker is summarizing. This assumption explicitly integrates a theory of mind ability into our framework, allowing speakers to tailor their utterances based on the listeners' knowledge (De Weerd et al., 2013; Kosinski, 2023). Consequently, \\(U_a\\) is designed to be independent of \\(c\\), and the two utility functions are modeled as follows:\n\\(U_c(W_n|W_{<n}, c) = logP_{LM}(W_n|W_{<n}, c)\\) (2)\n\\(U_a(W_n|W_{<n}, a) = logP_{L_1}(a|w_{<n})\\) (3)\nThe total utility function \\(U\\) is then a weighted sum of the content and attribute utility functions:\n\\(U = U_c + \\alpha U_a\\) (4)"}, {"title": "3.3 Self-Adjustable Rationality", "content": "Most existing CTG methods use the same control strength at each decoding step, leading to either excessive or insufficient constraints and thereby sub-optimal performance. Inspired by the concept of variable rationality in Zarrie\u00df and Schlangen (2019), we argue that introducing context-dependent control strength is essential for balancing attribute control and content consistency. Hence, we propose a more flexible approach called self-adjustable rationality, which achieves automatic adjustment of control strength.\nInstead of utilizing a fixed rationality parameter \\(\\alpha\\) throughout the generation process, we adopt a variable \\(\\alpha\\) which can take different values within the range \\([\\alpha_0, \\alpha_0 + \\alpha_1]\\) at each time step \\(n\\). The value of \\(\\alpha\\) is determined by the extent to which content consistency and attribute control are achieved with the basic rationality \\(\\alpha_0\\) and additional rationality up to \\(\\alpha_1\\) are allowed to be added as needed. Specifically, we compute two ratios, \\(r_c\\) and \\(r_a\\):\n\\(r_c = \\frac{P_{LM}(W_n, \\tilde{a}_n=\\alpha_0|W_{<n}, c)}{P_{LM}(W_n, \\tilde{a}_n=0|W_{<n}, c)}\\) (8)\n\\(r_a = \\frac{P_{L_1}(a|W_n, \\tilde{a}_n=\\alpha_0, W_{<n})}{P_{L_1}(a | w_n, \\tilde{a}_n=0, W_{<n})}\\) (9)\nHere \\(r_c\\) and \\(r_a\\) reflect how well the generated tokens adhere to the input content and how likely \\(L_1\\) can recognize the desired attribute, respectively, by comparing decoding with \\(\\tilde{a}_n = \\alpha_0\\) and \\(\\tilde{a}_n = 0\\) (no control). Since \\(w_n\\) has not yet been generated, we choose the top 5 tokens with the highest probabilities to simulate \\(w_n\\). Then \\(\\tilde{\\alpha}_n\\) is computed as:\n\\(\\tilde{\\alpha}_n = \\alpha_0 + \\frac{r_c}{r_a} \\alpha_1\\) (10)\nEquation 10 indicates that if basic rationality \\(\\alpha_0\\) achieves effective attribute control (high \\(r_a\\)) but compromises content consistency (low \\(r_c\\)), additional rationality should be minimized, and vice versa. By design we have \\(r_c < 1\\) and \\(r_a \\geq 1\\) because controlled decoding is expected to be less"}, {"title": "4 Toxicity Reduction", "content": "PLMs are at risk of learning toxic and offensive content from their training data (Gehman et al., 2020; Kumar et al., 2023), hence it is crucial to mitigate these risks before deploying them. We apply RSA-Control to GPT2 (Radford et al., 2019), a family of foundation models with sizes ranging from 117M to 1.5B parameters, aiming to steer them towards producing safer outputs.\nWe conduct our toxicity reduction experiments on the RealToxicityPrompts (RTP) dataset (Gehman et al., 2020). The RTP dataset comprises 100K prompts from web data, some of which lead to toxic continuations. The examined PLMs perform open-ended generation conditioned on RTP prompts without content constraints, and the toxicity of each continuation is measured by the Perspective API. Specifically, Perspective API predicts a score between 0 and 1 for six attributes: toxicity, severe toxicity, sexually explicit, threat, profanity, and identity attack, indicating the probability that the continuation exhibits each attribute. We use the challenging subset of RTP which contains 1199 strongly toxic prompts."}, {"title": "5 Bias Mitigation", "content": "PLMs are also prone to inheriting stereotypical biases against certain social groups during pre-training (Blodgett et al., 2020; Sheng et al., 2021). To address this issue, we apply RSA-Control to mitigate these biases and assess its effectiveness using the CrowS-Pairs benchmark (Nangia et al., 2020). CrowS-Pairs features pairs of minimally different sentences, with one being stereotypical and the other being anti-stereotypical (e.g., \"My mom spent all day cooking for Thanksgiving\" vs. \"My dad spent all day cooking for Thanksgiving\"). The proportion of instances where a PLM assigns higher probabilities to stereotypical sentences is reported, and a score closer to 50 indicates less bias. Nine types of social biases are covered by"}, {"title": "6 Readability-Controlled Summarization", "content": "We then apply RSA-Control to enhance readability control in instruction-tuned PLMs for news summarization, an input-output task. Generating summaries with desired readability levels ensures that extracted information is accessible to readers with varying literacy proficiency (Goldsack et al., 2022, 2023; Pu et al., 2024). While most studies rely on additional model training to steer summarization (Cao and Wang, 2021; Goyal et al., 2022; Luo et al., 2022; Ribeiro et al., 2023), large-scale PLMs have shown the capability of generating summaries in desired styles following natural language instructions (Pu and Demberg, 2023; Rooein et al., 2023). Thus, we adopt Llama-2-7b-chat (Touvron et al., 2023,"}, {"title": "7 Conclusion", "content": "This work introduces RSA-Control, a pragmatics-grounded lightweight controllable text generation approach which leverages mutual reasoning between speaker and listener modules. With a novel self-adjustable rationality parameter, RSA-Control can automatically adjust control strength based on context. Empirical results across two types of tasks, open-ended generation and input-output tasks, show that our method can effectively guide both foundation models and instruction-tuned PLMs toward desired attributes during generation, while maintaining language fluency and content adherence."}, {"title": "8 Limitations", "content": "Our proposed method has certain limitations that should be acknowledged. Firstly, RSA-Control requires decoding with additional control prompts. Although this process can be run in parallel, it imposes extra demands on GPU memory, restricting its applicability to large-scale PLMs"}, {"title": "9 Ethical Considerations", "content": "RSA-Control offers an effective method for guiding PLMs to generate natural language texts with desired attributes. In this work, we have demonstrated its potential to mitigate toxicity and stereotypical bias in PLMs. However, toxicity and bias are complex and deep-rooted issues, not only within the NLP community but also in the broader world. Therefore, our experiments with human-curated benchmarks and predefined types of toxicity and bias may not fully capture the entire scope of these problems. Furthermore, our proposed method, like any CTG approach, carries the risk of misuse to generate more hateful and biased texts. We hence strongly encourage careful moral considerations before deploying our methods in NLP systems."}, {"title": "10 Acknowledgements", "content": "This work was funded by the DFG project GRK 2853 \"Neuroexplicit Models of Language, Vision, and Action\" (project number 471607914). We are grateful to the anonymous reviewers and area chairs for their exceptionally detailed and helpful feed-back."}, {"title": "A Toxicity Attributes in Perspective API", "content": "Descriptions used to identify and reduce each toxicity attribute can be found in Table 8. Note that non-toxic descriptions are only used for the evaluation of L1. For toxicity reduction, we use 1a from Table 1 as the target prompt."}, {"title": "B Pragmatic Listener Results", "content": "For each attribute in Table 8, we collect 1000 continuations that have the highest and lowest scores from Perspective API. Then these 2000 examples are assigned positive and negative labels based on whether their attribute scores are greater than 0.5. To model L1, we implement So using contrastive control prompts formatted as \"The following sentences contain [BLANK],\" where descriptions of each toxicity type and their antonyms in Appendix A are filled in [BLANK] to create toxic and non-toxic prompts. A sample is predicted to exhibit an attribute by L\u2081 if its likelihood conditioned on the toxic prompt is higher than its likelihood conditioned on the non-toxic prompt. For comparison, we report the performance of a fine-tuned generative classifier implemented using expert and anti-expert modules from DExperts (Liu et al., 2021)."}, {"title": "C Implementation Details", "content": "In the toxicity reduction and bias mitigation experiments, we implement DAPT by fine-tuning GPT2 models of various sizes following the setup from Liu et al. (2021). For GeDi and DExperts, we use checkpoints released in their github repositories and adopt w = 1.0 and a = 1.6 for decoding, respectively, as the hyperparameters in their work yield unreadable generations on RTP with extremely high PPL. For Self-Detoxify and Self-Debias, we adopt the same implementation and hyperparameters as in the original papers.\nIn the readability-controlled summarization task, we use Dynamic Word Unit Prediction released by Cao and Wang (2021). As no checkpoint for Controllable Readability is provided and the training is too computationally expensive, we report results from the original work (Ribeiro et al., 2023)."}, {"title": "D Toxicity Reduction Results for Other Model Sizes", "content": "Toxicity reduction results for GPT2-small, GPT2-medium and GPT2-XL are presented in Table 9, Table 10 and Table 11. The findings are consistent with those reported in the paper: RSA-Control achieves superior detoxification performance compared to other prompt-based baselines."}, {"title": "E Toxicity Reduction and Self-Adjustable Rationality Examples", "content": "We provide more examples of RSA-Control in toxicity reduction experiments in Table 12. In the first two examples, RSA-Control successfully reduces toxicity while the other two fail. In the third example, both Self-Debias and RSA-Control avoid toxic continuations. All three models have very toxic generations in the last example.\nExamples of continuations from RSA-Control with fixed and self-adjustable rationality parameters are given in Table 13. In the self-adjustable rationality examples, numbers following each word denote the value of \\(\\alpha\\) at this step. For words that can be decoded into multiple tokens, the highest \\(\\tilde{a}\\) is reported. In the first two examples, self-adjustable rationality achieves a better balance between reducing toxicity and maintaining fluency. In the third example, it produces less toxic continuations compared to both low and high fixed rationality parameters. However, all three models fail to reduce toxicity in the final example. We observe that \\(\\alpha\\) takes the minimum value at most positions, and it increases when generating nouns or verbs that significantly affect the semantic meaning of a sentence. Additionally, it takes larger values at the beginning of new clauses and sentences to guide the overall direction of the sentence. In the final example, although self-adjustable rationality does not improve over fixed low rationality, it still provides additional control strength when toxic tokens are generated. Therefore, we conclude that self-adjustable rationality can detect when additional rationality is needed and adjust control strength"}, {"title": "F Multiple Reasoning Recursions", "content": "To better understand the effect of additional reasoning turns in RSA, we model a higher-order pragmatic listener \\(L_2\\) based on \\(S_1\\) and then a higher-order pragmatic speaker \\(S_2\\) based on \\(L_2\\) in the toxicity reduction experiment. we fix the rationality parameter by setting \\(a_1 = 0\\) to avoid the influence of changeable rationality parameters.\nThe results in Table 14 reveal that multiple iterations of reasoning lead to outcomes similar to those achieved by increasing the rationality parameter: \\(S_2\\) with a fixed \\(a = 5\\) achieves comparable results to \\(S_1\\) with \\(a = 20\\). Our findings are consistent with experimental results in human communication (Frank, 2016)."}, {"title": "G Incremental vs. Sample-based RSA", "content": "An alternative to incremental RSA described in this work is sample-based RSA, where a PLM initially generates a set of sequences, and then \\(L_1\\) selects the sequence that is most likely to demonstrate the desired attribute. We compare incremental to sample-based RSA on 100 RTP prompts with up to n = 200 samples. Both methods use beam sample with a beam size of 10 and p=0.9 for decoding. Results of using a fine-tuned BERT model for selection (BERT selection) and the oracle's selection of the least toxic samples (oracle) are also included."}, {"title": "H Bias Mitigation Results for Other Model Sizes", "content": "Bias mitigation results for GPT2-small, GPT2-medium and GPT2-XL are presented in Table 15, Table 16, and Table 17. We observe that RSA-Control consistently outperforms vanilla GPT2 and Self-Debias across all model sizes."}, {"title": "I Analyses of Readability-Controlled Summarization", "content": "Factual Consistency To evaluate the impact of RSA-Control on factual consistency in the"}, {"title": "L Application to Other LLMs", "content": "We apply RSA-Control to two other LLMs for the readability-controlled summarization experiments: Qwen2-7B-Instruct (Yang et al., 2024, hereafter referred to as Qwen2) and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023, hereafter referred to as Mistral). As discussed in Section 8, the performance of RSA-Control varies across models due to its reliance on the knowledge encoded in PLMs. For example, when applied to Qwen2, RSA-Control performs worse than the Prompt baseline in formal summarization but shows stronger readability control results in generating readable summaries than other LLMs."}]}