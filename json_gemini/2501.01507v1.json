{"title": "Transfer Learning Analysis of Variational Quantum Circuits", "authors": ["Huan-Hsin Tseng", "Hsin-Yi Lin", "Samuel Yen-Chi Chen", "Shinjae Yoo"], "abstract": "This work analyzes transfer learning of the Variational Quantum Circuit (VQC). Our framework begins with a pretrained VQC configured in one domain and calculates the transition of 1-parameter unitary subgroups required for a new domain. A formalism is established to investigate the adaptability and capability of a VQC under the analysis of loss bounds. Our theory observes knowledge transfer in VQCs and provides a heuristic interpretation for the mechanism. An analytical fine-tuning method is derived to attain the optimal transition for adaptations of similar domains.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum Computing (QC) has demonstrated the ability to address complex computational challenges that are intractable for classical computers. Leveraging unique quantum properties, such as superposition and entanglement, quantum computers exhibit significant advantages over classical systems in solving certain tasks [1].\nAlthough current quantum devices are constrained by imperfections and noise at current stage, a hybrid quantum-classical computing framework has been proposed to harness potential quantum advantages [2]-[4].\nVariational Quantum Algorithms (VQAs) [5] represent a class of hybrid algorithms in which quantum parameters are optimized with classical methods, such as gradient descent. VQAs enable architectures like Quantum Neural Networks (QNNs) to perform a variety of Machine Learning (ML) tasks, including classification [6]\u2013[9], time-series prediction [10], natural language processing [11]\u2013[14], reinforcement learning [15]\u2013[20] and neural network model compression [21]-[25].\nDriven by different computing mechanisms, QC model interpretability [26] and transparency are intriguing topics to be investigated for understanding how quantum systems process information and contribute to optimizations. This direction has attracted much attention lately, primarily due to extensive ML applications and the continuing development of policy and regulation. With the rapid growth of QC, unveiling deeper insights into quantum operations will be essential.\nOn the other hand, Transfer learning [27]-[29] is a well-established technique in classical ML that uses prior knowledge to enhance model performance and reduces resource requirements. Naturally, this empirical approach is introduced in QML to offer similar advantages. An earlier study [30] explored transfer learning in hybrid classical-quantum neural networks, proposing VQCs as fine-tuning layers to replace classical ones in the pretrained network. A major distinction with our approach is that [30] assumes a purely classical pretrained model where classical layers are later swapped with VQCs for fine-tuning, while we consider fine-tuning resumed from a pretrained VQC. Consequently, in [30], quantum gate parameters are initialized from scratch, bearing no memory in regard to previous data. In contrast, we leverage the optimal quantum parameters from pretraining, preserving domain knowledge to digest new data.\nIt is noticed that [31] investigates the transfer learning mechanisms of classical neural networks, illustrating how prior knowledge is transferred to new data domains. However, to analyze VQC, the required techniques and approaches are fundamentally different, which indicates the distinct behaviors and unique characteristics inherent to QML.\nOur contribution is twofold: (1) We derive an analytical solution for adjusting VQC model parameters between similar datasets, which yields global optimal under specific data conditions. This approach is particularly efficient compared to the conventional gradient descent or the parameter shift rule [30]. (2) Our theoretical analysis reveals the underlying transfer learning mechanism of VQCs, providing insights into its physical implications."}, {"title": "II. BACKGROUND", "content": "The Variational Quantum Circuit (VQC) has the design to fit and learn from external signals with tunable parameters in quantum gates. The construction of 1-qubit VQC is succinctly introduced in [8]. VQCs of multiple qubits can be extended via tensor products with the building block below.\nLet the 1-qubit Hilbert space be $\\mathcal{H} = \\mathbb{C}^2$ with a basis $|0\\rangle, |1\\rangle$ such that a spin-$\\frac{1}{2}$ particle wave function $|\\psi\\rangle \\in \\mathcal{H}$ can be decomposed as $|\\psi\\rangle = a |0\\rangle + b|1\\rangle$ for some $a, b \\in \\mathbb{C}$.\nGiven classical data $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$ with input $x^{(i)} \\in \\mathbb{R}^d$ and ground truth $y^{(i)} \\in \\mathbb{R}$ of sample index $i$, where the index may sometimes be neglected for simplicity when a context is clear. Denote all unitary transformations of $\\mathcal{H}$ by $U(\\mathcal{H})$. A VQC is a function $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ of three components $(\\mathcal{V}, \\mathcal{U}, \\mathcal{M})$, $f = \\mathcal{M} \\circ \\mathcal{U} \\circ \\mathcal{V}$, see Fig. 1, where: $\\mathcal{V} : \\mathbb{R}^d \\rightarrow U(\\mathcal{H})$ is called an encoding function converting a classical input $x$ into a quantum state $x \\rightarrow \\mathcal{V}(x)|\\phi_0\\rangle$ from an initial $|\\phi_0\\rangle \\in \\mathcal{H}$. The encoding unitary $\\mathcal{V}(x)$ is typically a choice of circuit sequence $\\{\\mathcal{V}_1, ..., \\mathcal{V}_k\\}$ such that $\\mathcal{V} := \\mathcal{V}_k \\circ ... \\circ \\mathcal{V}_j \\circ ... \\circ \\mathcal{V}_1$ with each $\\mathcal{V}_j : \\mathbb{R}^d \\rightarrow U(\\mathcal{H})$.\nThe variational function $\\mathcal{U} \\in U(\\mathcal{H})$ is the main modeling engine of VQC formed by a sequence of gates $\\{\\mathcal{U}_1, ..., \\mathcal{U}_L\\}$ with tunable parameters $\\theta = \\{\\theta_1, ..., \\theta_L\\}$ such that $\\mathcal{U}(\\theta) = \\mathcal{U}_L(\\theta_L) ... \\circ \\mathcal{U}_\\ell(\\theta_\\ell) \\circ ... \\circ \\mathcal{U}_1(\\theta_1)$. The conventional choice of $\\theta_\\ell \\leftrightarrow \\mathcal{U}_\\ell(\\theta_\\ell)$ is usually a 1-parameter Lie subgroup in $U(\\mathcal{H})$.\nFinally, a measurement $\\mathcal{M} : \\mathcal{H} \\rightarrow \\mathbb{R}$ is a function to collapse a quantum state by the choice of a Hermitian operator $H: \\mathcal{H} \\rightarrow \\mathcal{H}$ such that $\\mathcal{M}(\\psi) := \\langle \\psi| H |\\psi \\rangle$. The property that $\\mathcal{M}(\\psi) \\in \\mathbb{R}$ is due to $H^\\dagger = H$ to mimic the output of a classical neural network. Together, a VQC has the form,\n$x \\rightarrow f(x) = \\langle \\phi_0| \\mathcal{V}^\\dagger(x)\\mathcal{U}^\\dagger(\\theta) H \\mathcal{U}(\\theta)\\mathcal{V}(x) |\\phi_0 \\rangle \\quad (1)$\nwhere $\\mathcal{V}^\\dagger$ is the adjoint operator of $\\mathcal{V} \\in U(\\mathcal{H})$. The exact choice of the $\\mathcal{U}, \\mathcal{V}$ for this study is specified in Sec. III-B."}, {"title": "A. Variational Quantum Circuits", "content": ""}, {"title": "B. An adjoint action approximation", "content": "Recall that the adjoint action defined on a Lie group G: $Ad_g: G \\rightarrow G$ by $Ad_g(h) = ghg^{-1}$ leads to an associated homomorphism $ad : G \\rightarrow GL(\\mathfrak{g})$ by $g \\rightarrow (Ad_g)_{*,e}$, where $\\mathfrak{g}$ is the Lie algebra of G and $(Ad_g)_{*,e} : \\mathfrak{g} \\rightarrow \\mathfrak{g}$ denotes the differential of $Ad_g$ at the identity $e \\in G$. We know that for $A, B \\in \\mathfrak{g}$, the differential $ad_{*,e} : \\mathfrak{g} \\rightarrow gl(\\mathfrak{g})$ at $e$ gives,\n$(ad_{*,e})(A)(B) = \\frac{d^2}{dsdt}(e^{tA} e^{sB} e^{-tA})|_{s,t=0} = [A, B] \\quad (2)$\nwhere $[,]$ is the Lie bracket of $\\mathfrak{g}$. Under a matrix group G, (2) is reduced to\n$\\frac{d}{dt}(e^{tA} B e^{-tA})|_{t=0} = [A, B] = AB - BA$\nwhich tells us,\n$e^{tA} B e^{-tA} = B + t[A, B] + O(t^2) \\quad (3)$\nIn the event of $G = U(\\mathcal{H})$, we can take $A = i\\sigma$ with $\\sigma \\in P$ where $P := \\{\\sigma_1, \\sigma_2, \\sigma_3\\}$ is the collection of Pauli matrices. Then (3) gives,\n$e^{it\\sigma} B e^{-it\\sigma} = B + it[\\sigma, B] + O(t^2) \\quad (4)$\nThis can be viewed as an adjoint action approximation, which allows us to analyze VQC circuit transitions under transfer learning."}, {"title": "III. QUANTUM TRANSFER LEARNING", "content": "A VQC learns to fit one dataset $\\mathcal{D}$ by finding a proper parameter set $\\theta$ minimizing an objective function,\n$L(\\theta; \\mathcal{D}) = \\sum_{i=1}^N || \\langle \\mathcal{H} \\rangle (x^{(i)}; \\theta) - y^{(i)} ||^2 \\quad (5)$\nwhere $\\langle \\mathcal{H} \\rangle (x^{(i)}; \\theta)$ is also used to denote $f(x)$ in (1) to emphasize the dependency on $H, x^{(i)}$, and $\\theta$ under a fixed $\\mathcal{V}(x)$. Note that the optimal parameters in script $v := \\text{arg}\\text{min}_\\theta L(\\theta; \\mathcal{D})$ is to be distinguished from an arbitrary parameter set $\\theta$.\nGiven another domain data $\\widetilde{\\mathcal{D}} = \\{(\\widetilde{x}^{(i)}, \\widetilde{y}^{(i)})\\}_{i=1}^N$ of inputs $\\widetilde{x}^{(i)} \\in \\mathbb{R}^d$ and labels $\\widetilde{y}^{(i)} \\in \\mathbb{R}$ similar to a previous domain (data) $\\mathcal{D}$, we consider the quantum transfer learning by seeking new optimal $\\vartheta$ based on previous optimal $v$ learned in $\\mathcal{D}$, see Fig. 2. Such a process is also called finetune or domain adaptation. In the context of ML, $\\mathcal{D}$ is also called the source domain and $\\widetilde{\\mathcal{D}}$ the target domain with $v$ called the pretrain (model) parameters."}, {"title": "A. VQC Learning in Two Domains", "content": ""}, {"title": "B. VQC Finetune Error Estimates", "content": "In general, it is difficult to analyze the required parameter change $v \\rightarrow \\vartheta$. However, to derive a glimpse of VQC transfer ability, we utilize several techniques for the estimation. First, we observe the target domain loss,\n$L(\\vartheta; \\widetilde{\\mathcal{D}}) = \\sum_{i=1}^N || \\langle \\mathcal{H} \\rangle (\\widetilde{x}^{(i)}; \\vartheta) - \\widetilde{y}^{(i)} ||^2 \\quad (6)$\nwith\n$\\langle \\mathcal{H} \\rangle (\\widetilde{x}^{(i)}; \\vartheta) := \\langle \\phi_0| \\mathcal{V}^\\dagger(\\widetilde{x}^{(i)})\\mathcal{U}^\\dagger(\\vartheta) H \\mathcal{U}(\\vartheta)\\mathcal{V}(\\widetilde{x}^{(i)}) |\\phi_0 \\rangle \\quad (7)$\nFor the convenience of analysis, we assume two domain samples of $\\mathcal{D}$ and $\\widetilde{\\mathcal{D}}$ are rearranged via domain alignment [32], [33] such that $||(x_i, y_i) - (x_i, y_i)|| \\le ||(x_i, y_i) - (x_j, y_j)||$ for all $j \\ne i$. Then we may define $dx_i = \\widetilde{x}^{(i)} - x^{(i)}$, $dy_i = \\widetilde{y}^{(i)} - y^{(i)}$ and expand (6) as,\n$L(\\vartheta; \\widetilde{\\mathcal{D}}) = \\sum_{i=1}^N || (\\langle \\mathcal{H} \\rangle (\\widetilde{x}^{(i)}; \\vartheta) - \\langle \\mathcal{H} \\rangle (x^{(i)}; \\vartheta)) - dy^{(i)} + (\\langle \\mathcal{H} \\rangle (x^{(i)}; \\vartheta) - y^{(i)}) ||^2 \\quad (8)$\nAssume the VQC architecture to learn the two domains is fixed as the following form,\n$\\mathcal{V}(x) := \\mathcal{V}_d(x_d) ... \\mathcal{V}_1(x_1), \\quad (\\mathcal{V}_j(x_j) = e^{i dx_j \\sigma_{k_j}})$\n$\\mathcal{U}(\\vartheta) := \\mathcal{U}_L(\\vartheta_L) ... \\mathcal{U}_1(\\vartheta_1), \\quad (\\mathcal{U}_\\ell(\\vartheta_\\ell) = e^{i d\\theta_\\ell \\sigma_{k_\\ell}}) \\quad (9)$\nwith indices $k_j, k_\\ell \\in \\{1, 2, 3\\}$, $j = 1, ..., d$ and $\\ell = 1, ..., L$, $\\vartheta = (\\vartheta_1, ..., \\vartheta_L)$, and $x = (x_1, ..., x_d)$ denoting a d-dim input with sample index ignored. Note the multiplication order in (9) has the earliest action starting from the right. By (9), we also have,\n$\\mathcal{V}_j (\\widetilde{x}_j) = e^{i dx_j \\sigma_{k_j}} \\cdot \\mathcal{V}_j (x_j) = \\mathcal{V}_j (x_j) \\cdot e^{i dx_j \\sigma_{k_j}}$\n$\\mathcal{U}_\\ell(\\widetilde{\\vartheta}_\\ell) = e^{-i d\\theta_\\ell \\sigma_{k_\\ell}} \\cdot \\mathcal{U}_\\ell(\\vartheta_\\ell) = \\mathcal{U}_\\ell(\\vartheta_\\ell) \\cdot e^{-i d\\theta_\\ell \\sigma_{k_\\ell}} \\quad (10)$\nThe last equalities in (10) stress that the commutativity exists. We expand the first two terms grouped in (8) as follows,\n$\\langle \\mathcal{H} \\rangle (\\widetilde{x}; \\vartheta) - \\langle \\mathcal{H} \\rangle (x; v) = $\n$\\langle \\phi_0| \\mathcal{V}^\\dagger(\\widetilde{x})\\mathcal{U}^\\dagger(\\vartheta) H \\mathcal{U}(\\vartheta)\\mathcal{V}(\\widetilde{x}) |\\phi_0 \\rangle - \\langle \\phi_0| \\mathcal{V}^\\dagger(x)\\mathcal{U}^\\dagger(v) H \\mathcal{U}(v)\\mathcal{V}(x) |\\phi_0 \\rangle$\n$+\\langle \\phi_0| \\mathcal{V}^\\dagger(x)\\mathcal{U}^\\dagger(v) H \\mathcal{U}(v)\\mathcal{V}(x) |\\phi_0 \\rangle - \\langle \\phi_0| \\mathcal{V}^\\dagger(x)\\mathcal{U}^\\dagger(v) H \\mathcal{U}(v)\\mathcal{V}(\\widetilde{x}) |\\phi_0 \\rangle$\n$+\\langle \\phi_0| \\mathcal{V}^\\dagger(x)\\mathcal{U}^\\dagger(v) H \\mathcal{U}(v)\\mathcal{V}(\\widetilde{x}) |\\phi_0 \\rangle - \\langle \\phi_0| \\mathcal{V}^\\dagger(x)\\mathcal{U}^\\dagger(\\vartheta) H \\mathcal{U}(\\vartheta)\\mathcal{V}(\\widetilde{x}) |\\phi_0 \\rangle$\n$+\\langle \\phi_0| \\mathcal{V}^\\dagger(x)\\mathcal{U}^\\dagger(\\vartheta) H \\mathcal{U}(\\vartheta)\\mathcal{V}(\\widetilde{x}) |\\phi_0 \\rangle - \\langle \\mathcal{H} \\rangle (x) \\quad (11)$\nwhere the sample index i is ignored for simplicity, and the four middle auxiliary terms cancel each other. The purpose of the telescope expansion is to observe the penetration of data difference $dx_j$ and parameter transition $d\\theta_\\ell$ via the three pairs in (11). For this, we utilize the Lie group adjoint action approximation given in Sec. II-B. Knowing that (4) can be written as\n$ad_{e^{it\\sigma}}(B) = B + it[\\sigma, B] + O(t^2) \\quad (12)$\n(10) gives us,\n$\\mathcal{V}_j (\\widetilde{x}_j) = \\mathcal{V}_j (dx_j) \\circ \\mathcal{V}_j (x_j) = \\mathcal{V}_j (dx_j) \\mathcal{V}_j (x_j)$\n$= \\mathcal{V}_j (x_j) + \\frac{i}{2} dx_j [\\sigma_{k_j}, \\mathcal{V}_j (x_j)] + O((dx)^2) \\quad (13)$\nwhere function compositions $\\circ$ are automatically implied for the actions between the adjoint actions.\nWith (13), the first pair of RHS (11) yields\n$\\frac{i}{2} dx_1 \\langle \\phi_0| [\\sigma_{k_1}, ad_{\\mathcal{V}_1(x_1)} ad_{\\mathcal{U}(v)} (H)] |\\phi_0 \\rangle$\n$\\frac{i}{2} dx_2 \\langle \\phi_0| ad_{\\mathcal{V}_1(x_1)} [\\sigma_{k_2}, ad_{\\mathcal{V}_2(x_2)} ad_{\\mathcal{U}(v)} (H)] |\\phi_0 \\rangle$\n...\n$\\frac{i}{2} dx_d \\langle \\phi_0| ad_{\\mathcal{V}_1(x_1)} ... ad_{\\mathcal{V}_{d-1}(x_{d-1})} [\\sigma_{k_d}, ad_{\\mathcal{V}_d(x_d)} ad_{\\mathcal{U}(v)} (H)] |\\phi_0 \\rangle + O((dx)^2) \\quad (14)$\nSimilarly, for variational circuits, we have\n$ad_{\\mathcal{U}_\\ell(\\widetilde{\\vartheta}_\\ell)} = ad_{\\mathcal{U}_\\ell(\\vartheta_\\ell)} + d\\vartheta_\\ell [\\sigma_{k_\\ell}, ad_{\\mathcal{U}_\\ell(\\vartheta_\\ell)}] + O((d\\vartheta)^2) \\quad (15)$\nUsing (15) for the second pair in RHS (11) shows us how the VQC parameters react to the new domain change,\n$\\frac{i}{2} d\\vartheta_1 \\langle \\phi_0| ad_{\\mathcal{V}^\\dagger(x)} [\\sigma_{k_1}, ad_{\\mathcal{U}_1(\\vartheta_1)} ... ad_{\\mathcal{V}(x)} (H)] |\\phi_0 \\rangle$\n$\\frac{i}{2} d\\vartheta_2 \\langle \\phi_0| ad_{\\mathcal{V}^\\dagger(x)} ad_{\\mathcal{U}_1(\\vartheta_1)} [\\sigma_{k_2}, ad_{\\mathcal{U}_2(\\vartheta_2)} ... ad_{\\mathcal{V}(x)} (H)] |\\phi_0 \\rangle$\n...\n$+ O((d\\vartheta)^2) \\quad (16)$\nThen it is noticed both (14) and (16) can be simplified as inner products $\\langle r, dx \\rangle_{\\mathbb{R}^d}$ and $\\langle z, d\\vartheta \\rangle_{\\mathbb{R}^L}$, respectively, with two vectors $r := (r_1, ..., r_d) \\in \\mathbb{R}^d$, $z := (z_1, ..., z_L) \\in \\mathbb{R}^L$ defined,\n$r_j := \\frac{i}{2} \\langle \\phi_0| (ad_{\\mathcal{V}_1(x_1)}... ad_{\\mathcal{V}_{j-1}(x_{j-1})} [\\sigma_{k_j}, ad_{\\mathcal{V}_j(x_j)} ad_{\\mathcal{U}(v)} (H)]) |\\phi_0 \\rangle \\quad (17)$\n$z_\\ell := \\frac{i}{2} \\langle \\phi_0| (ad_{\\mathcal{V}^\\dagger(x)} ad_{\\mathcal{U}_1(\\vartheta_1)} ... ad_{\\mathcal{U}_{\\ell-1}(\\vartheta_{\\ell-1})} [\\sigma_{k_\\ell}, ad_{\\mathcal{U}_\\ell(\\vartheta_\\ell)} ad_{\\mathcal{V}(x)} (H)]) |\\phi_0 \\rangle \\quad (18)$\nNote that it is the imaginary number $i$ in front of (17), (18) that makes $r_j, z_\\ell$ all real. The above circuit layer analysis converts the fine-tune loss (6) into the problem of finding $d\\vartheta$ in a linear form,\n$L(\\vartheta; \\widetilde{\\mathcal{D}}) = \\sum_{i=1}^N q_i^2 \\quad (19)$\nwith constant $q_i$ defined as,\n$q_i = dy^{(i)} - \\langle r, dx^{(i)} \\rangle_{\\mathbb{R}^d} + (\\langle \\mathcal{H} \\rangle (x^{(i)}; v) - y^{(i)}) \\quad (20)$\nThis quantity is called the VQC transfer residue that bears a certain meaning in understanding the VQC transfer mechanism."}, {"title": "IV. INTERPRETATIONS", "content": "As (19) and (20) depict how a VQC adjusts to the slight domain mismatch, an optimal finetune can be analytically derived via the Moore-Penrose pseudo inverse with $q := (q_1, ..., q_N)$ defined,\n$d\\vartheta^* = (z^T \\cdot z)^{-1} z^T \\cdot q \\quad (21)$\nThis provides a one-shot parameter shift for VQC to adapt to $\\widetilde{\\mathcal{D}}$ promptly, see Fig. 2. Subsequently, (21) reveals that\n$||q|| \\rightarrow 0, \\quad d\\vartheta^* \\rightarrow 0 \\quad (nothing \\text{ to adapt})$\n$||q|| >> 1, \\quad d\\vartheta^* >> 1 \\quad (much \\text{ to adapt}) \\quad (22)$\nto justify the name VQC transferal residue for quantity $q$.\nWe look further into the composition of $q$, which consists of two pairs,\n$q_i = \\underbrace{dy^{(i)} - \\langle r, dx^{(i)} \\rangle_{\\mathbb{R}^d}}_{\\text{domain mismatch }(\\triangle)} + \\underbrace{(\\langle \\mathcal{H} \\rangle (x^{(i)}; v) - y^{(i)})}_{\\text{pretrain error } (*)} \\quad (20)$\nwhere the first pair ($\\triangle$) describes the mismatch of two domains, and the other pair evaluates the error from the pretrained VQC model. Indeed, (1) a perfect pretrain $f$ with $f(x_i) = y_i$ gives $(*) = 0$, and (2) for two domains perfectly match $\\mathcal{D} = \\widetilde{\\mathcal{D}}$, we have $dx_i = dy_i = 0$ and $(\\triangle) = 0$.\nUnder such perfect circumstances, $q_i = 0$ and there is nothing new for $f$ to learn or adapt in $\\widetilde{\\mathcal{D}}$ by (21). On the other hand, for an ill-pretrained $f$ with large domain difference $||dx||, ||dy||$, $q_i$ becomes large too. This indicates $d\\vartheta^*$ has much to adapt in $\\widetilde{\\mathcal{D}}$. Therefore, the transferal residue characterizes the amount of new knowledge needed to be learned in the new domain, and hence the name."}, {"title": "A. VQC Transferal residue", "content": ""}, {"title": "B. Knowledge transfer in VQC", "content": "As $q$ reflects the amount of new knowledge left to be digested in $\\widetilde{\\mathcal{D}}$, the term $\\langle r, dx^{(i)} \\rangle$ is observed to automatically rise to reduce new label mismatch $dy$, which can be regarded as a self-correction (self-adaptation) to the new domain. Indeed, the self-correction vanishes when $\\mathcal{D} = \\widetilde{\\mathcal{D}}$ or $dx^{(i)} = 0$.\nAs the pretrained VQC $f$ was trained with the old data, the term $r$ in (17) carries previous domain knowledge via old circuits configurations $\\mathcal{U}_1(\\vartheta_1), ... \\mathcal{U}_L(\\vartheta_L)$ to resolve new data mismatch. This formulation clearly indicates how the domain transition is performed to provide an intuition for the VQC transfer learning. Similar interpretations on network-based models can be found in [31]. However, the form of (17) and (18) is particular to the VQC structure. In fact, the above analysis includes four types of domain transitions,\nType 1 ($dx_i = dy_i = 0$): This implies $\\mathcal{D} = \\widetilde{\\mathcal{D}}$ requiring no adaptations, as $q_i = 0$ leads to $d\\vartheta = 0$ by (21).\nType 2 ($dx_i \\ne 0, dy_i = 0$): This corresponds to the regular data augmentation, as there are new inputs $\\widetilde{x}_i \\ne x_i$ giving the same label $y_i$.\nType 3 ($dx_i = 0, dy_i \\ne 0$): This case is to augment mislabelled data since the new input $\\widetilde{x}_i$ is the same as $x_i$, yet labels differ $\\widetilde{y}_i \\ne y_i$.\nType 4 ($dx_i \\ne 0, dy_i \\ne 0$): This is a general case that contains all the types above."}, {"title": "V. EXPERIMENT", "content": "We validate the optimal adaptation solution in Sec. III-B with 1-qubit demonstration. The code for the experiments will be released upon publication.\nA pair of \"two moons\" datasets (with 2000 samples each) are used to conduct transfer learning, one served as the source domain $\\mathcal{D}$; the other as the target domain $\\widetilde{\\mathcal{D}}$, see Fig. 3. A pretrained model $f$ is well-trained (attaining accuracy 81.5%) on $\\mathcal{D}$. However, the accuracy of $f$ drops promptly to 49.8% on $\\widetilde{\\mathcal{D}}$ such that the domain adaptation is necessitated.\nConsequently, our one-shot fine-tune solution (17)~(21) (named Quantum Variational Analysis, QVA for short) is conducted to compare with the conventional Gradient Descent (GD) method. Fig. 4 shows the GD training loss and accuracy over 30 epochs. As QVA requires no training, it is represented by one horizontal line. The results indicate that the QVA directly yields 77.2% accuracy on $\\widetilde{\\mathcal{D}}$ without iterative GD process. While GD continues to improve with more training iterations, our approach provides prompt adaptations competing with the first 17 epochs, offering an efficient alternative."}, {"title": "VI. CONCLUSION", "content": "This study investigates transfer learning in VQC, using algebraic estimations to unveil the underlying mechanism. Our analytical computation yields a direct optimal fine-tune solution under similar domain conditions without the need for iterative gradient descent. The theoretical description additionally provides an intuitive interpretation of how VQC attempts to resolve the domain mismatch and adaptation to new data. This framework also reveals the unique characteristics of transfer learning mechanisms pertaining to VQCs, offering insights into quantum models."}]}