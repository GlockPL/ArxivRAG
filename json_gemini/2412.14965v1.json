{"title": "Movie2Story: A framework for understanding videos and telling stories in the form of novel text", "authors": ["Kangning Li", "Zheyang Jia", "Anyu Ying"], "abstract": "Multimodal video-to-text models have made considerable progress, primarily in generating brief descriptions of video content. However, there is still a deficiency in generating rich long-form text descriptions that integrate both video and audio. In this paper, we introduce a framework called M2S, designed to generate novel-length text by combining audio, video, and character recognition. M2S includes modules for video long-form text description and comprehension, audio-based analysis of emotion, speech rate, and character alignment, and visual-based character recognition alignment. By integrating multimodal information using the large language model GPT40, M2S stands out in the field of multimodal text generation. We demonstrate the effectiveness and accuracy of M2S through comparative experiments and human evaluation. Additionally, the model framework has good scalability and significant potential for future research.", "sections": [{"title": "1 Introduction", "content": "Many popular movies are inaccessible to disabled individuals who can only listen to subtitles, missing critical details such as environment and expressions, leading to a poor viewing experience. Additionally, people often prefer reading books to understand story development, as text is not constrained by movie length and provides greater convenience. Large language models (LLMs) have shown significant success in text understanding and generation, making them suitable for story comprehension and novel writing. Pre-trained multimodal models can map audio, images, and text spaces, enabling text generation when combined with LLMs. This integration can produce novels with detailed environmental and psychological descriptions, and a coherent storyline. We aim to develop a comprehensive video-language model that integrates multimodal information from video and audio to translate films into novels with complete elements. This model will cater to diverse groups, including barrier-free users and those who prefer e-books.\nIn the field of video generated text, multimodal understanding models based on CLIP/BLIP have made good progress (reference A). Some models focus on question and answer understanding for videos, also known as VQA (reference B.), but only provide brief descriptions. However, longer descriptions such as paper C still lack detailed information. Some models choose to use audio to add detailed information, such as paper D, but do not generate sufficiently long and rich text. In contrast, paper E chooses to generate long texts using a hierarchical structure, but lacks the ability to handle other multimodal processing such as audio.\nExisting models can combine video and audio signals to generate text descriptions but lack deeper semantic information such as emotions and detailed environmental context. These models are primarily used for caption generation and question-answer scenarios, limiting their applicability to short film segments and not suitable for creating novels with rich elements.\nIn this article, our method combines the advantages of these papers by combining audio, video, and images to effectively generate novel level long texts with multimodal understanding capabilities. And these texts can be used as datasets for future researchers. Current video-language models focus primarily on generating captions, image descriptions, and question-answer scenes. While Our approach aims to leverage multimodal data, including audio, to extract richer information such as emotions, thereby creating detailed and engaging narratives from movies, referred to as movie2story. This method addresses the needs of disabled individuals and those who prefer reading, enhancing their ability to enjoy and understand movie content through comprehensive and immersive text.\nBy overcoming these challenges, our video- language model will generate detailed and immersive narratives from movies, enhancing accessibility and enriching the experience for all users.\nThis research developed new methodologies for combining audio, video, and textual data to generate comprehensive narratives. By advancing multimodal integration, we will provide significant insights into harmonizing different data types to produce richer, more detailed text. The enhanced model will be particularly relevant to disabled individuals who rely on text or audio descriptions to understand films, improving their movie-watching experience. Additionally, e-book readers who prefer reading will benefit from the comprehensive and immersive stories generated by the model.\nOverall, our experimental results strongly demonstrate the ability of the model to generate text with complete elements and richer semantic information, making it adaptable to various application scenarios. It can be used in educational settings for detailed explanations of visual content or in entertainment to create novelizations of films for broader audiences."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 MMAD: Multi-modal Movie Audio Description", "content": "The MMAD (Multimodal Audio-Visual-Text Description) model combines video, audio, and textual information to achieve concise, real-time descriptions. This model leverages several key components to enhance the richness and accuracy of generated narratives:\nBy integrating these components, the MMAD model provides a comprehensive approach to generating rich, real-time descriptions that enhance accessibility and understanding for all users."}, {"title": "2.2 Distilling Vision-Language Models on Millions of Videos", "content": "This workflow introduces an effective method for transferring image-language models to video-language models to generate large amounts of high-quality video pseudo-labels. The detailed steps are as follows:\nThe generated pseudo-labels provide multi-granularity features, offering more semantic information and background description for our method:"}, {"title": "2.3 VideoChat : Chat-Centric Video Understanding", "content": "VideoChat-Text textualizes videos in stream. VideoChat-Embed encodes videos as embeddings. Both video content can be input in LLMs for multimodal understanding."}, {"title": "2.4 Video-LLaMA An Instruction-tuned Audio-Visual Language Model for Video Understanding", "content": "The ImageBind module, which has been pre-trained, aligns audio, video, and other multimodal information into a single vector space. Instead of training an audio-text dataset, it solely trains a video-text encoder, which indirectly allows the conversion of audio into text."}, {"title": "2.5 Video Storytelling: Textual Summaries for Events", "content": "In this paper, the proposed method consists of three main steps: key frame selection using the Narrator model, multi-modal embedding learning for contextual understanding, and story generation.\nKey Frame Selection using Narrator Model: The Narrator model leverages reinforcement learning to select key frames from long videos. It determines the importance of each frame based on semantic similarity with previous frames and decides on the clip length:\nContext-Aware Multi-Modal Embedding Learning: After selecting key frames, they use a bi-directional RNN (ResBRNN) to learn contextual embeddings that capture the semantic information of video frames and texts:\nThey propose replacing ResBRNN with Transformers to leverage their superior long-range dependency modeling capabilities.\nThis approach can significantly improve the performance and quality of video storytelling, making the generated stories more cohesive and diverse."}, {"title": "3 Preliminary", "content": ""}, {"title": "3.1 Datasets", "content": "InternVid A high-quality, large-scale video-text dataset released in 2024 by the Shanghai AI Laboratory, Nanjing University, and the Chinese Academy of Sciences. InternVid contains over 7 million videos with detailed textual descriptions, covering 16 scenes and approximately 6000 action descriptions, with a total duration of nearly 760,000 hours. The videos and textual descriptions in this dataset are highly matched, making it suitable for video-text semantic matching, video-text retrieval, video-text generation, and other multimodal learning tasks.\nVript Proposed by Shanghai Jiao Tong University, Beihang University, and the Xiaohongshu research team, the Vript dataset includes a meticulously annotated corpus of 12,000 high-resolution videos, providing detailed, dense, script-like subtitles for over 420,000 clips. Each clip's subtitle contains approximately 145 words, which is ten times longer than most video-text datasets. Vript records not only content but also camera operations, including shot types and camera m\nMSR-VTT (Microsoft Research Video to Text) MSR-VTT is a large-scale video description dataset consisting of 10000 video clips divided into 20 categories. Each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. Under standard conditions, the dataset is divided into a training set (6513 segments), a validation set (497 segments), and a testing set (2990 segments).\nActivityNet Captions The ActiveNet Captions dataset is built on ActiveNet v1.3 and contains 20000 unedited YouTube videos with an average duration of 120 seconds and a total of 100000 subtitle annotations. Most videos contain more than three annotated events, each with a corresponding timestamp and manually written sentences. The dataset is divided into a training set (10024 videos), a validation set (4926 videos), and a testing set (5044 videos).\nVideo Story The Video Story dataset was created by Facebook to train Al systems to convert videos into stories. This dataset contains 20000 videos and 123000 descriptive sentences. These videos were selected from social media and have a high level of engagement, covering a wide range of topics and categories. The length of each video ranges from 20 to 180 seconds, with annotated paragraphs describing the object, context, and important details. The fragment contains an average of five sentences, with each sentence taking approximately 18 seconds on average."}, {"title": "3.2 Tools and Models", "content": "M2S maintains a Foundation Models Pool to store various video foundation models, which own the capability to detect and annotate different attributes of tracklets in videos. Here we list several models in it:\nVideo Frame Extraction Tool: FFmpeg, used for extracting key frames from videos.\nAudio Extraction Tool: Whisper model for speech-to-text conversion, noting it generates text without capturing emotions.\nPre-trained Models: Including MMAD (video-language model) and VideoChat for generating detailed descriptions and dialogue content.\nEmotion Analysis Module: Using features like speech intonation for audio-emotion processing, or BERT-like encoder networks for text emotion analysis to extract emotional information.\nCharacter Relationship Network: To enable the model to maintain story coherence based on character relationships.\nContextual Information Linking: Utilizing resampling or contextual information to ensure semantic consistency.\nLong Sequence Processing: Using batch processing and key frame resampling techniques to link information across different segments.\nLarge Language Models: Such as GPT-4, used to expand and refine generated descriptions into a complete novel."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Image Character Recognition", "content": "Our objective is to achieve the matching of real-person names with characters appearing in the video. This necessitates the initial recognition of individuals in the footage, as well as the matching of the same individuals across consecutive video frames. Given that our application scenario involves long videos, we must contend with challenges such as multiple viewpoints of individuals and variations in their appearance, for instance, wearing different attire at various time intervals. These factors present significant challenges in the matching of individuals in extended video footage. Drawing inspiration from the approach proposed in the MMAD paper, which transforms the problem into a person Re-identification (ReID) task, we utilized text feature information to enhance the spatial features of CLIP, thereby further distinguishing individual characteristics and achieving effective person identification and matching.\nConsidering that the appearance and behavior of individuals should remain consistent within shorter video segments, we can also employ person tracking algorithms for matching within these segments. However, given our ultimate application scenario, we still need to match the same individuals across different segments, which inevitably involves dealing with changes in appearance.\nTo address the person matching challenge, we have chosen to utilize Facenet, which can accurately identify faces in individual images and compute similarity for person matching. Since facial features do not change with the scene, this approach offers robustness. Additionally, we propose a text feature-based matching method for short video segments. This involves using VIT to generate descriptions of individuals' appearances, which are then processed by LLM to determine the matching between different descriptions. By combining vector features with textual descriptions, we are able to effectively match individuals across various scenes in long videos.\nFor the global person-name matching, we employ an anchor matching method. We pre-establish a dictionary of key characters in the video, pairing names with 'face features, textual appearance features'. Using these set anchors, we can allocate names to the detected individuals in the video.\nConsidering the angle of pre-obtained person bounding boxes, we propose using the YOLO method to pre-segment the bounding box information of individuals. Unprocessed images may contain multiple individuals, which can interfere with the extraction of facial and textual features. Therefore, we utilize YOLOv8 for the preprocessing and extraction of person bounding boxes. However, YOLO extracts all individuals from the frame image, so we subsequently determine whether a character is a main character by comparing the similarity between anchors and candidates, thereby filtering the information."}, {"title": "4.2 Video", "content": "Our approach employs a segmented video processing strategy, which effectively addresses the challenges of handling long videos while maintaining the continuity of the narrative. Within each video segment, we focus on extracting storyline information and background context as the key components of the video segment. In terms of model selection, we conducted a comprehensive evaluation of various open-source models, including videollava and Internvideo. Following a comparative analysis, we finally adopted for the open-source model Videochat as our baseline model. By carefully designing the prompt format, we ensured that the model could accurately extract the necessary key information, laying the foundation for in-depth analysis of the video content."}, {"title": "4.3 Audio", "content": "Our audio processing pipeline begins with the extraction of audio from video files using Python's audio libraries. The extracted audio is then segmented into smaller, manageable chunks, facilitating efficient processing and analysis. The data is stored in JSON format for downstream text processing, which includes not only audio features and content but also timestamps. Therefore, it can be processed by subsequent LLMs to obtain highly consistent text content."}, {"title": "4.3.1 Speaker Recognition", "content": "Speaker recognition is an important branch of speech technology aimed at identifying or verifying the identity of a speaker based on their speech features. It extracts the unique identity information of the speaker by analyzing the acoustic features of the speech signal, and performs matching and recognition. Speaker recognition technology plays an important role in fields such as security monitoring, intelligent voice assistants, and identity authentication. In this article, we use distinguishing different speakers to achieve audio and video character matching for accurate text description.\n1. Feature Extraction:\nTo capture the acoustic characteristics of each audio segment, we employ the pyannote.audio toolkit, which offers a range of feature extraction techniques. This includes standard methods like Mel-frequency cepstral coefficients (MFCCs) and spectrograms, providing a rich representation of the audio signal. Additionally, pyannote.audio supports on-the-fly data augmentation, enhancing the robustness and generalization of the models.\n2. Speaker Embedding:\nThe core of our speaker recognition system lies in the Speaker Embedding module, powered by pyannote.audio. This module leverages deep learning models to extract unique and discriminative features from each speaker's voice. These speaker embeddings serve as compact representations that capture the essential aspects of a speaker's identity, facilitating accurate recognition and matching.\npyannote.audio provides a unified framework for training sequence labeling models for tasks like Voice Activity Detection, Speaker Change Detection, and Overlapping Speech Detection. By employing recurrent neural networks and attention mechanisms, these models effectively learn to identify and segment different speakers within the audio stream.\nTo further improve speaker recognition performance, pyannote.audio employs metric learning techniques to train speaker embeddings that are optimized for a predefined distance metric, such as cosine similarity. This approach eliminates the need for additional steps like Probabilistic Linear Discriminant Analysis (PLDA), simplifying the clustering process.\n3. End-to-End Training and Pipeline Integration:\npyannote.audio supports end-to-end training, allowing us to directly learn from the raw audio waveform without the need for manual feature extraction. This streamlines the workflow and enables the models to capture more nuanced acoustic information.\nThe pyannote.audio pipeline module integrates these modules into a cohesive framework, optimizing hyperparameters and jointly training the entire speaker segmentation pipeline. This approach, as opposed to independently training and combining individual modules, often leads to improved performance and more accurate speaker segmentation results."}, {"title": "4.3.2 Audio Description", "content": "In the Audio Description module, we leverage the powerful Whisper model to extract specific speech content from the audio segments. Whisper, an open-source speech recognition system developed by OpenAI, demonstrates exceptional performance across a wide range of languages and dialects. It excels in accurately transcribing speech, even in challenging acoustic environments. By employing end-to-end unsupervised training, Whisper efficiently learns from raw audio data, eliminating the need for manual annotations and significantly reducing training costs. Whisper's versatility extends beyond speech recognition, as it can also be utilized for speech translation into multiple languages, enabling cross-lingual communication. Additionally, its ability to search for specific audio content based on keywords or phrases enhances its utility in information retrieval tasks. The modular design of Whisper, along with its availability in various sizes, allows for flexible deployment options based on specific needs and resource constraints. The open-source nature of Whisper further promotes collaboration and innovation within the speech recognition community, fostering the development of new applications and advancements in the field."}, {"title": "4.3.3 Emotion Analysis", "content": "Emotion2Vec is an innovative deep learning based emotion recognition tool that aims to encapsulate the essence of emotional vocabulary in a rich high-dimensional vector space. This method not only captures the subtle relationships between words, but also assigns them the emotional burden of human expression.\nThe journey of Emotion2Vec begins with careful planning of the training dataset, where each word is carefully paired with its emotional essence - whether it's discrete categories of happiness or anger, or subtle levels of intensity and arousal. From the perspective of models such as Word2Vec or GloVe, these words take on a new form, transformed into vectors that preserve their semantic structure while encoding their emotional meanings. As each emotional word gains its unique vector representation, a series of emotional vectors unfold, revealing the closeness of \"happiness\" to \"joy\" and its distance from \"sadness\". This mapping can provide a profound understanding of how emotions are integrated into language structures.\nIn the field of sentiment analysis, Emotion2Vec shines brightly, able to interpret emotions in various forms of data - whether it's written text, spoken language, or visual images. It accurately classifies emotions, estimates their intensity, and identifies the most subtle similarities in emotions. The advantage of Emotion2Vec lies in its ability to grasp subtle semantic differences, express emotions clearly, and easily adapt to new emotional landscapes. Its applications cover the analysis of social media conversations, the classification of emotions in literary works, and the comparison of emotions between different datasets.\nUltimately, Emotion2Vec demonstrated the power of deep learning in the field of emotion recognition, providing emotional storytelling through vector language. The model in this article is derived from the Emotion2Vec related model trained by FunASR."}, {"title": "4.3.4 Word Speed", "content": "3 This part is relatively simple, just calculate the number of words spoken by the speaker in a unit of time to obtain the speaking speed, using text descriptions such as \"fast\" and \"slow\""}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Quantitative Analysis", "content": "Traditional NLP metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015) evaluate generated text by comparing it to reference ground truth descriptions. While effective for assessing surface-level language quality, these metrics fail to capture the richness of knowledge beyond the visual domain, such as audio-related information, character details, or contextual nuances.\nFor instance, while a visual scene may depict a list of ingredients, a narration could highlight only a subset of those ingredients or incorporate additional elements like speaker emotions or environmental sounds, which are critical for generating a more comprehensive story. Since traditional metrics rely solely on visual references, they do not account for these extra layers of information.\nTo address this gap, we propose a set of reference-free metrics: Language Fluency, Key-knowledge Relevance. These metrics evaluate fluency, knowledge integration (including both visual and audio elements), and narrative alignment, providing a more accurate assessment of the generated stories and better reflecting the advantages of our multi-modal framework.\nLanguage Fluency: To assess language fluency, we utilize a keyword-triplet method that evaluates fluency at three levels of granularity: (1) within a single sentence, (2) between different sentences, and (3) the repetition of key-knowledge triplets relative to the overall story. Repetition of triplets within the story is considered a sign of unnatural phrasing and redundancy, which negatively impacts fluency. A higher fluency score is assigned when the text flows smoothly, with minimal repetition and a coherent structure.\nKey-knowledge Relevance: For evaluating the relevance of key knowledge in the generated story, we define a knowledge base that includes both visual and audio information. In contrast to traditional models that focus solely on visual knowledge, our approach incorporates additional audio-related knowledge, such as ASR (Automatic Speech Recognition), environmental sound events, and other audio features. This expanded knowledge base allows us to assess the relevance and diversity of knowledge within the story. Specifically, we use two evaluation criteria: - Information Similarity (InfoSim): Measures the alignment between the knowledge points in the story and the knowledge repository. A higher similarity score indicates that the generated story effectively incorporates relevant knowledge. - Information Diversity (InfoDiverse): Evaluates the breadth of the knowledge used in the story. A higher diversity score indicates that the story incorporates a wide range of knowledge points, avoiding over-reliance on a small subset of information.\nA well-rounded story should demonstrate both high information similarity and diversity, ensuring that relevant visual and audio knowledge are integrated in a meaningful way.\nThese evaluation metrics provide a comprehensive framework for assessing the quality of generated story descriptions, ensuring that the generated narratives meet high standards in terms of fluency, knowledge relevance, and visual coherence."}, {"title": "5.2 Qualitative Analysis", "content": ""}, {"title": "5.2.1 GPT Assistant Metrics", "content": "In this section, we introduce a set of key evaluation metrics designed to assess the quality of generated story descriptions using GPT and prompt engineering. By defining these metrics, we aim to leverage GPT as a rigorous evaluator of novel-like text, assigning scores based on specific story elements. These metrics are particularly useful for assessing narrative quality in terms of various aspects such as environmental details, emotional expression, language proficiency, character portrayal, and storyline coherence.\nThe following metrics are defined:\nEnvironment Description Score: This score evaluates how well the generated text sets the scene and conveys the environment. Key factors include the richness of the setting, sensory detail, and atmosphere. A high score indicates a vivid and immersive portrayal of the environment.\nEmotional Description Score: This metric assesses the depth and authenticity of emotional expression in the story. It measures how well the text captures and conveys the emotional states of characters, as well as the emotional tone of the narrative.\nLanguage Description Score: This score focuses on the clarity, fluency, and grammatical quality of the text. It evaluates how well the story is written, including sentence structure, vocabulary choice, and coherence. A higher score reflects a more polished and readable narrative.\nCharacter Description Score: This metric evaluates how effectively characters are described in terms of appearance, personality, and behavior. It gauges whether the character descriptions are detailed, nuanced, and consistent throughout the narrative.\nStructural and Rationality Coherence Index (SRCI) Score: This metric quantifies the narrative coherence and the logical progression of the storyline. It gauges the effectiveness of plot construction, the lucidity of the story's trajectory, and the seamless incorporation of essential storytelling components such as conflict, resolution, and character progression. The SRCI reflects the story's ability to maintain a consistent and engaging structure that resonates with the audience's expectations of narrative rationality.\"\nStoryline Accuracy with Canonical Outline Reference (SACOR) Score: This score measures the accuracy of the generated story against a predefined canonical storyline. It assesses how closely the generated narrative aligns with the key plot points, character behaviors, and thematic elements of the original story outline. The SACOR score is indicative of the model's fidelity to the source material and its ability to reproduce the intended narrative arc with precision.\"\nFor a more comprehensive evaluation, these scores are tabulated and compared across different story generations. The results can be found in Table 2, which summarizes the performance of our framework on each metric."}, {"title": "Manual Metrics", "content": "Table 2 shows the results of applying our M2S framework for generating detailed story descriptions from several movies. The comparison between our framework and baseline methods demonstrates the clear advantages of our approach.\nMore specifically, our framework excels in producing more comprehensive and accurate story descriptions. By leveraging the Audio module, we incorporate key information such as ASR (Automatic Speech Recognition) output, speaker identity, and speaking rate to further enhance the generated descriptions. Additionally, the combination of the Actor-Matching module, Storyline-Segment module, Audio module, and Vision module enables our framework to generate rich, long-form narrative descriptions that go beyond traditional subtitles, offering a novel way of understanding movies. This integrated approach results in more detailed and accurate descriptions, which could serve as an independent means of movie comprehension, rather than merely being a supplementary subtitle track."}, {"title": "6 Conclusion", "content": ""}, {"title": "A Example Appendix", "content": "This is an appendix."}]}