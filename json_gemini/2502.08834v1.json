{"title": "A REVERSIBLE SOLVER FOR DIFFUSION SDES", "authors": ["Zander W. Blasingame", "Chen Liu"], "abstract": "Diffusion models have quickly become the state-of-the-art for generation tasks across many different data modalities. An important ability of diffusion models is the ability to encode samples from the data distribution back into the sampling prior distribution. This is useful for performing alterations to real data samples along with guided generation via the continuous adjoint equations. We propose an algebraically reversible solver for diffusion SDEs that can exactly invert real data samples into the prior distribution.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models have quickly become the state-of-the-art in many different modalities in generation, e.g., audio (Liu et al., 2023), images (Rombach et al., 2022), video (Blattmann et al., 2023), protein generation (Skreta et al., 2024), &c. The sampling process of diffusion models is done through numerically solving an It\u00f4 Stochastic Differential Equation (SDE) or related Ordinary Differential Equation (ODE) which describes the evolution of a sample drawn for some prior noise distribution to the data distribution.\nInversion of the sampling procedure, i.e., an encoding from the data distribution back to the prior distribution, is invaluable for many downstream applications. E.g., image editing (Hertz et al., 2023; Su et al., 2023; Meng et al., 2022; Nie et al., 2024) and image interpolation (Song et al., 2021a; Blasingame & Liu, 2024a;b) with natural extensions to other data modalities. Existing work on diffusion inversion has focused on the ODE formulation (Wallace et al., 2023; Zhang et al., 2024; Wang et al., 2024); however, recent work (Nie et al., 2024) has shown that the SDE formulation is particularly useful for applications when the latent representation is edited. Motivated by this finding we propose a novel algebraically reversible solver for diffusion SDEs which makes use of the Brownian interval (Kidger et al., 2021) to perform exact inversion with diffusion models without storing the entire Wiener process in memory. To the best of our knowledge this work is the first to propose a technique for exactly inverting diffusion SDEs without storing the sampled noise from every timestep in memory."}, {"title": "2 PRELIMINARIES", "content": "Diffusion models aim to learn a mapping from some simple prior distribution of Gaussian noise p(x) to the data distribution q(x). The namesake for this class of models comes from the forward diffusion process, an It\u00f4 SDE given by:\n$dX_t = f(t)X_t + g(t) dW_t,$\nwhere f, g \u2208 C\u00b9 ([0, T]) form the drift and diffusion coefficients of the SDE, and where {Wt}t\u2208[0,T] is the standard Wiener process on the time interval [0, T]. The reverse-time SDE (Anderson, 1982) of Equation (2.1) is found to be:\n$dX_t = [f(t)X_t + g\u00b2(t)\u2207_x log p_t(X_t)] dt + g(t) dW_t,$\nwhere dt is a negative timestep and {Wt}t\u2208[0,T] is the standard Wiener process in reverse-time. The aim of diffusion models, then, is to learn the score function se(x,t) = \u2207x log pt(x) (Song et al., 2021b) or a closely related quantity, e.g., noise prediction (Song et al., 2021a; Ho et al., 2020) or data prediction (Kingma et al., 2021). Once the score function or another equivalent parameterization is learned, we can use it to sample q(x) by first sampling some x\u0442 ~ p(x) and then numerically"}, {"title": "3 MOTIVATION", "content": "A common task with diffusion models is to encode samples from the data distribution q(x) back into the noise distribution p(x). This can be for tasks such as image editing (Meng et al., 2022; Nie et al., 2024), or for performing the backward pass in solving the continuous adjoint equations (Blasingame & Liu, 2024a; Pan et al., 2024). However, simply solving the diffusion ODE forwards in time\u00b9 can raise some issues due to truncation errors and stability concerns. Furthermore, integrating the diffusion SDE in forwards time requires a bit more care due to the It\u00f4 stochastic integral.\nTruncation errors. Suppose we have some numerical scheme for sampling the diffusion model, e.g., DDIM (Song et al., 2021a), DEIS (Zhang & Chen, 2023), or DPM-Solver (Lu et al., 2022a;b); such that we can sample the solution trajectory {xtn}=0 with timesteps {tn =T>tn-1\u2026 > to = 0} and where tn ~ q(x) and the rest of the trajectory is found via the numerical scheme. Now, suppose we apply the same scheme forwards in time with initial condition to = xto to construct the encoding trajectory {Xn}=1, we have no guarantee that encoding trajectory will equal the sampling trajectory for n > 0, i.e., Xtn = xtn does not necessarily hold for n > 0. This has a few implications: a) a scheme which is used for encoding and then sampling is not guaranteed to have exact inversion and b) the samples generated in the encoding trajectory differ from the samples in the sampling trajectory, causing inaccurate gradients when using the continuous adjoint equations.\nStability. Similarly, there are concerns about the numerical stability of the numerical scheme solved in both directions of time. Consider the test ODE \u00ff(t) = \u03bby(t) with \u5165 < 0 defined in the interval [0, T] with the initial condition yo. An ODE solver with a nontrivial region of convergence (see Harier & Wanner, 2002) will be able to solve this ODE without much trouble, as the magnitude of the errors decreases exponentially since \u5165 is negative. However, the backwards in time solve from y(T) will suffer numerical instability as the errors will grow exponentially. N.B., this problem is simply reversed if the solver has good stability in reverse-time, with the solve in forward-time now suffering. Furthermore, the poor stability in the backward solve is an issue for diffusion guidance techniques which use the continuous adjoint equations.\nAs such we desire a numerical solver for diffusion models which has alignment in truncation errors in both directions of time along with reasonable numerical stability in both directions."}, {"title": "4 REVERSIBLE SOLVERS FOR DIFFUSION SDES", "content": "Now as we alluded earlier, there are some difficulties with solving Equation (2.2) in forward-time as the It\u00f4 integral is adapted to the backward filtration induced by {W+}. It is much easier to instead use the Fisk-Stratonovich symmetric integral (see Kunita, 2019), which has the nice property of symmetry in time. Now as the It\u00f4 integral term in Equation (2.2) is only additive noise, we can freely switch to the Stratonovich integral without consequence, thereby rewriting Equation (2.2) as an integral equation of the form:\n$X_t = \\int_{s}^{t} f(\\tau)X_{\\tau} + g^{2}(\\tau)\\nabla_x log p_{\\tau}(X_{\\tau}) d\\tau + \\int_{s}^{t} g(\\tau) dW_{\\tau}.$\nN.B., the Wiener process {Wt} is not the same process as the one used in Equation (2.1), but we adopt it for a simpler notation, that we ask the reader to keep this in mind. The drift and diffusion coefficients, (f, g), are defined via the schedule (at, ot) in the Variance Preserving (VP) formulation (Song et al., 2021b) with $f(t) = \\frac{dlog a_t}{dt}, g^{2}(t) = \\frac{d\\sigma_t}{dt} - 2\\frac{dlog a_t}{dt}$.\nLet xot(x) := E[XoXt = x] be the data prediction model. Following Lu et al. (2022a) we let \u03bb := logat/ot denote one-half the log-SNR (Signal to Noise Ratio). Since At is a strictly"}, {"title": "4.1 CONSTRUCTION OF THE REVERSIBLE SOLVER", "content": "Now equipped with a simplified form of the diffusion SDE we develop a reversible solver based on approximations of the exponential integral in Equation (4.2). Taking inspiration from the recent work of McCallum & Foster (2024), wherein they design reversible solvers for neural ODEs with a non-trivial region of convergence, we apply their insight of using a coupling parameter to construct an algebraically reversible solver to Equation (4.2). Note, we assume that our data prediction model is trained to zero loss, i.e., 2o|t = Xot\nForward pass. Suppose that we have a single-step solver for the exponential integral term in Equation (4.2) given by \u03a8\u0127 : R \u00d7 Rd \u2192 Rd where h denotes the step size h := |t \u2013 s| and timesteps {tn}=1 which is defined in reverse-time. Let \u03da \u2208 (0, 1) be a coupling parameter that determines the stability of the forward and backward passes and let 2 be an augmented state for algebraic reversibility. For notational simplicity let xn := xt and likewise for other variables. We then define the forward pass as\n$x_{n+1} = (x_n + (1 \u2212 \u03b6)x\u0302_{n+1})e^{-h} + 2a_{n+1}\u03a8_h(t_n, x\u0302_n) + \\sqrt{2\u03c3_{n+1}}e^{-\u03bb_{n+1}} W_{Sn+1,Sn},$\n$x\u0302_{n+1} = x_n - \\frac{\u03c3_n}{\u03c3_{n+1}} e^{-h}x_{n+1} \u2212 2a_n \u03a8_{-h}(t_{n+1}, x_{n+1}) + \\sqrt{2\u03c3_n}e^{-\u03bb_n} W_{Sn+1,Sn}.$\nBackward pass. The backward solve can then be compute algebraically from Equation (4.3) as"}, {"title": "5 INTERPOLATION EXPERIMENT", "content": "Following Song et al. (2021a) we test our method with a small experiment of image interpolation, i.e., given two real images x(a) and x(b) they are inverted to find x(a) and x(b). These representations are then interpolated via spherical linear interpolation (Shoemake, 1985) to obtain an interpolated latent representation. For the reversible solvers we also interpolate 2. For this experiment we used a Latent Diffusion Model (LDM) (Rombach et al., 2022) trained on the CelebA-HQ dataset (Karras et al., 2018) at a 256 \u00d7 256 resolution. Further details can be found in Appendix C.\nIn Figure 1 we plot an example interpolation with three different solvers: a) the standard DDIM inversion which serves as the baseline, b) McCallum and Foster's method applied to DDIM, and c) the reversible solver described in Equations (4.3) and (4.4) with first-order truncated Stratonovich-Taylor expansion. Notice how the DDIM solver struggles to accurately invert the original images; whereas the same solver when used in conjunction with McCallum and Foster's method yields exact inversion. The poor performance of DDIM inversion is unsurprising as we used a low number of discretization steps on purpose to stress test the solvers. The SDE solver with only 20 sampling steps performs better than the ODE-based inversion with less visible distortions while still achieving exact inversion."}, {"title": "6 CONCLUSION", "content": "In this work we propose a novel algebraically reversible solver for diffusion SDEs enabling the exact inversion of samples into the latent space with diffusion SDEs. To the best of our knowledge we are the first to propose a method for exactly inverting diffusion SDEs which does not store the entire realization of the Wiener process in memory. We illustrate the utility of our solver on the experiment of image interpolation. This work has many potential applications in the editing of samples with diffusion SDEs and for eliminating truncation errors in guided generation methods which use the continuous adjoint equations.\nLimitations. As preliminary work we have only explored a proof of concept experiments with image interpolation to illustrate the stability under perturbations and exact inversion of our method. In the future we plan to explore using this solver for guided generation via the continuous adjoint equations and perform a more detailed analysis of the stability."}, {"title": "A RELATED WORKS", "content": "In this section we provide a small discussion on related works.\nReversible solvers. The asynchronous leapfrog method (Zhuang et al., 2021) and the reversible Heun (Kidger et al., 2021) were the standard reversible solvers until recently (cf. McCallum & Foster, 2024), with the former applicable to general neural ODEs and the latter applicable to neural ODEs, CDEs, and SDEs. Recent work by McCallum & Foster (2024) has improved upon these older solvers for neural ODEs by showing it is possible to construct reversible solvers with a non-trivial region of stability.\nExact inversion with diffusion models. The work of Wallace et al. (2023) proposes a reversible solver for diffusion ODEs by solving a dual auxiliary state of the model and then interpolating between the two states. Later work by Zhang et al. (2024) explores a reversible solver by using bidirectional integration approximation scheme as a sort of leapfrog method. More recent work by Wang et al. (2024) has explored the exact inversion of diffusion ODEs via bidirectional linear multi-step methods. However, linear multi-step methods and leapfrog methods often suffer from poor stability (Shampine, 2009).\nInversion with diffusion SDEs. More closely related to our work Wu & De la Torre (2022) propose a method for the exact inversion of diffusion SDEs. Given a particular realization of the Wiener process that admits xt ~ N(atx0 | 07I), then given \u00e6, and noise \u20ac5 ~ N(0, I) we can calculate\n$x_t = \\frac{a_t}{a_s}x_s + \\frac{2\u03c3_t(e^h - 1)}{a_s} x_{\u03c4|s} (x_s) + \u03c3_t\\sqrt{e^{2h} - 1}\u03b5_s.$\nWu & De la Torre (2022) propose to invert this by first calculating for two samples xt and xs the noise es can be calculated by rearranging the previous equation to find\n$\u03b5_s = \\frac{\\frac{x_t}{a_t} - \\frac{a_t}{a_s}x_s - \\frac{2\u03c3_t(e^h - 1)}{a_s}\u03b5_0 (x_s, \u03c4, s)}{\u03c3_t\\sqrt{e^{2h} - 1}}.$\nWith this the sequence {t}\u2081 of added noises can be calculated which can be used to reconstruct the original input from the initial realization of the Wiener process. However, unlike our approach, this process requires storing the entire realization in memory."}, {"title": "B PROOF OF PROPOSITION 4.1", "content": "We restate Proposition 4.1 here:\nProposition B.1 (Exact solution of diffusion SDEs). Given an initial value X5(w) = xs at time s \u2208 [0,T] the exact solution of Equation (4.1) can be expressed as:\n$X_t = \\frac{\u03c3_t}{\u03c3_s}e^{\u03bb_s -\u03bb_t} X_s + \\frac{2\u03b1_t}{\u03c3_s} \\int_s^t e^{2(\u03bb_\u03c4 -\u03bb_t)} a_\u03c4 e^{\u03bb_\u03c4} \u03f5_{\u03bb_\u03c4|\u03bb} (X_{\u03bb\u03c4}) d\u03bb + \\sqrt{2}\u03c3_t e^{\u03bb_t}W_{S_t, S_s},$\nwhere St = (e21 - e21\u0433).\nWe also restate the Dubins-Schwarz representation theorem (Dubins & Schwarz, 1965) (sometimes referred to as the Dambis representation theorem) below:\nTheorem B.2 (Dubins-Schwarz representation theorem). Let M be a continuous local martingale adapted to a filtration {Ft}t\u2208[0,\u221e) vanishing at zero that satisfies (M)\u221e = \u221e almost surely. Define the stopping times {Tt}t\u2208[0,\u221e) by\n$\u03c4_t = inf \\{s \u2208 [0, \u221e) : (M)_s > t\\} = sup \\{s \u2208 [0, \u221e) : (M)_s = t\\}.$\nThen, {Mrt}t\u2208[0,\u221e) is a standard Brownian motion {Bt}t\u2208[0,\u221e) and, for every t \u2208 [0, \u221e),\n$M_t = B_{(M)_t}.$\nNow we state our proof for Proposition 4.1."}, {"title": "Proof.", "content": "First we restate our Stratonovich integral equation from Equation (4.1):\n$X_t = \\int_s^t F(\u03c4)X_\u03c4 + \\int_s^t g\u00b2(\u03c4)\u2207_x log p_\u03c4(X_\u03c4) d\u03c4 + \\int_s^t F(\u03c4)X_\u03c4 + \\int_s^t g(\u03c4) dW_\u03c4,$\nwhere\n$f(t) = \\frac{dlog a_t}{dt},$\n$g\u00b2(t) = \\frac{do_t}{dt} - 2\\frac{dlog a_t}{dt}.$\nWe can express the score function in terms of the data prediction model xo|t(x) := E[X0|Xt = x]:\n$\u2207_x log p_t(x) = -\\frac{1}{\u03c3_t\u00b2} \\frac{a_t}{\u03b1_t} e_0|t(x).$\nUsing Equation (B.5) we can rewrite Equation (4.1) as\n$X_t = \\int_s^t F(\u03c4)X_\u03c4 + \\int_s^t - \\frac{g\u00b2(\u03c4)}{\u03c3_t\u00b2} \\frac{a_t}{\u03b1_t} e_0|\u03c4(X_\u03c4) d\u03c4 + \\int_s^t g(\u03c4) dW_\u03c4.$\nThen we can write the Stratonovich integral equation as\n$X_t = \\int_s^t \u03b1(\u03c4) X_\u03c4 + \\int_s^t b(\u03c4) e_0|\u03c4(X_\u03c4) d\u03c4 + \\int_s^t g(\u03c4) dW_\u03c4,$\nwhere\n$\u03b1(t) = \\frac{dlog a_t}{dt} + \\frac{1}{2} \\frac{g\u00b2(t)}{\u03c3_t\u00b2} \\frac{do_t}{dt},$\n$b(t) = -\\frac{a_t}{\u03b1_t} \\frac{g\u00b2(t)}{\u03c3_t\u00b2} = 2\u03b1_t \\frac{d\u03bb}{dt}.$\nThen we can use the variation-of-parameters formula to find\n$X_t = \u03a6_{\u03b1|s,t} X_t + \\int_s^t \u03a6_{\u03b1(\u03c4,t)} b(\u03c4) e_0|\u03c4(X_\u03c4) d\u03c4 + \\int_s^t \u03a6_{\u03b1(\u03c4,t)} g(\u03c4) e_0|\u03c4(X_\u03c4) dW_\u03c4,$\nwhere Pa(s, t) := exp fa(7) dr is the integrating factor. This technique has been employed by other works to separate the linear and non-linear component of diffusion models (Lu et al., 2022a; Gonzalez et al., 2023; Blasingame & Liu, 2024a). We then simplify a(s, t) such that\n$\u03a6_{\u03b1(s,t)} = exp \\int \\frac{1}{2} \\frac{do_t}{\u03c3_\u03c4\u00b2} \\frac{do_\u03c4}{d\u03c4} d\u03c4 - \\frac{dlog a_\u03c4}{d\u03c4} d\u03c4,$\n$= exp \\int \\frac{1}{2} \\frac{do_t}{\u03c3_\u03c4\u00b2} \\frac{do_\u03c4}{d\u03c4} d\u03c4 - dlog \u03b1_\u03c4,$\n$= exp \\int log \u03c3_t\u00b2 - log \u03c3_\u03c4\u00b2 - (log \u03b1_t - log \u03b1_t),$\n$= exp \\int log \\frac{\u03c3_t\u00b2}{\u03c3_\u03c4\u00b2} - log \\frac{\u03b1_t}{\u03b1_t} .$\nAnother useful form to express the integrating factor in, is in terms of At, which we find:\n$\u03a6_{\u03b1(s,t)} = \\frac{\u03c3_{t\u03b1\u03c4}}{\u03c3_{t\u03b1\u03c4}}.$\n$= \\frac{\u03c3_{t}}{\u03c3_{\u03c4}} exp (log \\frac{\u03c3_{t}}{\u03c3_{\u03c4}} ).$\n$= \\frac{\u03c3_{t}}{\u03c3_{\u03c4}} exp (log \\frac{\u03c3_{t}}{\u03c3_{\u03c4}} - log \\frac{\u03b1_{t}}{\u03b1_{\u03c4}} ).$\n$= \\frac{\u03c3_{t}}{\u03c3_{\u03c4}}.$"}, {"title": "Using this expression for Pa(s, t) we can rewrite Equation (B.10) as:", "content": "$X_t = \\frac{\u03c3_t}{\u03c3_s} X_t + 2\u03c3_t \\int_s^t \\frac{e^{\u03bb_\u03c4-\u03bb_t} \u03b1_\u03c4 e^{\u03bb_\u03c4}}{\u03c3_\u03c4} e_0|\u03c4(X_\u03c4) d\u03c4 + \\sqrt{2}\u03c3_t \\int_s^t \\frac{e^{\u03bb_\u03c4}}{\u03c3_\u03c4} dW.$\nWe first simplify the integral term\n$2\u03c3_t \\int_s^t \\frac{e^{\u03bb_\u03c4-\u03bb_t} \u03b1_\u03c4 e^{\u03bb_\u03c4}}{\u03c3_\u03c4} e_0|\u03c4(X_\u03c4) d\u03c4.$\n$= 2\u03c3_t \\int_s^t \\frac{e^{2\u03bb_\u03c4 -\u03bb_t}}{\u03c3_\u03c4} e_0|\u03c4(X_\u03c4) d\u03c4,$\n$= 2\u03c3_t \\int_s^t \\frac{e^{2\u03bb_\u03c4 -\u03bb_t}}{\u03c3_\u03c4} e_0|\u03c4(X_\u03c4) d\u03c4,$\n$= 2\u03c3_t \\int_s^t \\frac{e^{2\u03bb_\u03c4 -\u03bb_t}}{\u03c3_\u03c4} e_0|\u03c4(X_\u03c4) d\u03c4,$\n$= \\frac{2\u03c3_t}{\u03c3} \\int_s^t e^{2(\u03bb_\u03c4 -\u03bb_t)} e_0|\u03bb (X_\u03bb) d\u03bb.$\nTo simplify the stochastic integral term we first define a continuous martingale Mt via the stochastic integral:\n$M_t := \\int_s^t \\frac{e^{\u03bb_\u03c4}}{\u03c3_\u03c4} dW.$\nWe choose time T as our starting point for the martingale rather than 0 and then integrate in reverse-time, hence the negative sign. We can then express our stochastic integral in Equation (B.13) as\n$\\int_s^t \\frac{e^{\u03bb_\u03c4}}{\u03c3_\u03c4} dW = M_t - M_s.$\nNext we establish a few properties of this martingale. First, Mo = 0 by construction. Second, the quadratic variation of Mt is found to be\n$\\langle M \\rangle = - \\int \\frac{e^{\u03bb_\u03c4}}{\u03c3_\u03c4} d\u03c4 = - \\int_s^t \\frac{e^{2\u03bb}}{\u03c3_\u03c4\u00b2} d\u03c4,$\n$= - \\int \\frac{e^{2\u03bb}}{\u03c3_\u03c4\u00b2} d\u03c4,$\n$= \\int \\frac{e^{2\u03bb}}{\u03c3_\u03c4\u00b2} d\u03c4 = S_t = (e21 - e21\u0433);$\nwhere we let St denote our new time variable. Now we have a deterministic mapping from the original time to our new time via:\n$\\begin{aligned} S_t: [0, T] &\u2192 [0, \u221e), \\ t &\u2192 \u27e8M\u27e9_t. \\end{aligned}$\nNotice that as t \u2192 0 and ot \u2192 0 we have \u3008M\u3009t \u2192 \u221e. Such a martingale can be expressed as time-changed Brownian motion, see Theorem B.2, such that Mt = W(M)\u300f . For notational simplicity we define the Brownian bridge in this time-changed as\n$W_{S_t,S_s}: W_{S_t} - W_{S_s}.$\nThus we have\n$X_t = \\frac{\u03c3_t}{\u03c3_s} e^{\u03bb_s -\u03bb_t} X_t + \\frac{2\u03c3_t}{\u03c3} \\int_s^t e^{2(\u03bb -\u03bb_t)} e_0|\u03bb (X_\u03bb) d\u03bb + \\sqrt{2}\u03c3_t e^{\u03bb_t}W_{S_t,S_s}.$\nthereby finishing the proof."}, {"title": "C EXPERIMENTAL DETAILS", "content": "All experiments were run on a single NVIDIA Telsa V100 32GB. We used the Brownian interval (Kidger et al., 2021) from the torch-sde package. The faces are from the Face Research Lab London (FRLL) dataset (DeBruine & Jones, 2017). We used the pre-trained LDM model from Rombach et al. (2022) which can be found here: https://huggingface.co/CompVis/ldm-celebahq-256."}]}