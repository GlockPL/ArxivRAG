{"title": "Intelligent Task Offloading: Advanced MEC Task Offloading and Resource Management in 5G Networks", "authors": ["Alireza Ebrahimi", "Fatemeh Afghah"], "abstract": "5G technology enhances industries with high-speed, reliable, low-latency communication, revolutionizing mobile broadband and supporting massive IoT connectivity. With the increasing complexity of applications on User Equipment (UE), offloading resource-intensive tasks to robust servers is essential for improving latency and speed. The 3GPP's Multi-access Edge Computing (MEC) framework addresses this challenge by processing tasks closer to the user, highlighting the need for an intelligent controller to optimize task offloading and resource allocation. This paper introduces a novel methodology to efficiently allocate both communication and computational resources among individual UEs. Our approach integrates two critical 5G service imperatives: Ultra-Reliable Low Latency Communication (URLLC) and Massive Machine Type Communication (mMTC), embedding them into the decision-making framework. Central to this approach is the utilization of Proximal Policy Optimization, providing a robust and efficient solution to the challenges posed by the evolving landscape of 5G technology. The proposed model is evaluated in a simulated 5G MEC environment. The model significantly reduces processing time by 4% for URLLC users under strict latency constraints and decreases power consumption by 26% for mMTC users, compared to existing baseline models based on the reported simulation results. These improvements showcases the model's adaptability and superior performance in meeting diverse QoS requirements in 5G networks.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of Virtual Reality/Augmented Reality (VR/AR) and autonomous vehicles is transforming industries and everyday life. By 2028, the VR/AR market is projected to reach 3.67 billion users, generating about USD 58.3 billion, while the autonomous driving sector may achieve a market value of USD 13,632.4 billion with 54 million connected vehicles. These technologies require high data transfer rates, ultra-low latency, and robust security.\nDespite improvements in computational capabilities of user equipment (UE), increasing application complexity poses challenges in meeting strict latency demands for optimal user experiences. Traditional cloud computing introduces latency and congestion, making it unsuitable for these technologies. Consequently, fog and edge computing have emerged as viable alternatives, with edge computing reducing latency by bringing computation closer to end devices [1], [2].\nTo effectively offload tasks to edge servers, a robust communication link is essential, offering broad coverage, low latency, high reliability, and security. Among wireless network technologies, 5G stands out as the most promising solution. Unlike WiFi, which is limited in range, 5G provides extensive coverage and high data rates [3]. The 3rd Generation Partnership Project (3GPP) classifies 5G services into three categories: Enhanced Mobile Broadband (eMBB), Ultra-Reliable Low Latency Communication (URLLC), and Massive Machine Type Communication (mMTC), each supporting a wide range of applications from remote surgery to massive IoT deployments [4]. The synergy between 5G and edge computing addresses the growing demand for low-latency, high-bandwidth applications, facilitating the proliferation of IoT devices, augmented reality, autonomous vehicles, and smart cities [5]-[7].\nThe Multi-access Edge Computing (MEC) framework, introduced in 3GPP technical specifications, enables data processing and storage closer to end users, improving latency, reducing network congestion, and enhancing the overall user experience [8]. In urban environments, where URLLC and mMTC users have varying offloading requests, efficient resource allocation becomes crucial, necessitating a balance between limited communication and computation resources. The Open Radio Access Network (O-RAN), a non-proprietary version of the Radio Access Network (RAN), integrates MEC platforms within the RAN infrastructure, providing the computational resources needed for task offloading. O-RAN's modular design introduces intelligent controllers, such as the Near-Real-Time Intelligent Controller (Near-RT RIC) and the Non-Real-Time Intelligent Controller (Non-RT-RIC), which optimize resource allocation and prioritize critical tasks within the xApp framework of the O-RAN architecture [9].\nHowever, existing research has not fully addressed the consideration of diverse network slices, including URLLC and mMTC users, particularly in the context of task offloading and resource allocation. While some studies focus on the allocation of only communication resources among UEs [10], others that addressed both communication and computation allocation, often overlook the distinct requirements of different network slices [11] which is very important due to intrinsic difference between their tasks. This research gap warrants further exploration.\nIn this study, we develop intelligent models for task offloading as well as allocation of communication and computational resources using Deep-Q Learning and Proximal Policy Optimization (PPO) techniques. Our approach significantly outperforms established baselines, demonstrating its effectiveness and efficiency.\nThe subsequent sections of this paper are structured in"}, {"title": "II. RELATED WORKS", "content": "In this section, we will first review advancements in task offloading on MEC. Next, we will explore scholarly works focused on communication resource allocation in 5G networks. Finally, we will examine studies that integrate both communication and computation resource allocation for task offloading in 5G. This systematic approach will offer a comprehensive understanding of the current research landscape in these areas."}, {"title": "A. Task Offloading in Edge", "content": "In the study presented in [12], the researchers introduce a heuristic algorithm, referred to as the Table Based Task Offloading Algorithm (TBTOA). This algorithm addresses the issue of task offloading in Mobile Edge Computing (MEC), taking into account both dependency and service caching constraints. The authors of [13] delve into the problem of dependency-aware task offloading and service caching within the context of vehicular edge computing. This is a significant application of MEC within the realm of intelligent transportation systems. A similar investigation into MEC task offloading for vehicles is conducted in [14]. The paper [15] explores the balance between delay and energy consumption in the task offloading process within a multi-user MEC scenario. The authors provide insights on the decision-making process regarding whether a task should be offloaded for edge execution. In [16] the authors use edge servers as a processing platform to assist UAVs to offload their signal processing tasks."}, {"title": "B. Resource Block Allocation in 5G", "content": "The study denoted as [10] showcases the use of Long Short-Term Memory (LSTM) networks by researchers for predicting network traffic. This was aimed at network slicing for various 5G services, each with distinct Quality of Service (QoS) levels, with deep Q-learning employed for decision-making. Attention-based deep reinforcement learning (ADRL) technique was proposed by the authors in [17] for the optimal control of dynamic 5G networks. Federated learning and team learning were utilized for resource allocation in the research conducted by [18]."}, {"title": "C. Edge Computing in 5G", "content": "The exploration of URLLC and eMBB slices was the focus of [19]. The authors aimed to decompose base station functions into Virtual Network Functions (VNFs) and utilized federated deep Q-learning for distributed learning, aiming to optimize resource utilization at edge sites while minimizing network reconfiguration errors.\nThe publication [20] addresses concerns regarding execution delay and energy consumption when deciding on offloading requests. They take into account factors such as task data volume, required CPU cycles for data processing, and maximum tolerable task delay, utilizing deep Q-learning to determine which tasks should be offloaded.\nThe authors of [21] proposed a time window for wireless power transfer and task offloading from IoT devices to Multi-access Edge Computing (MEC) servers. They introduced the Deep Reinforcement Learning-based Online Offloading (DROO) framework, employing a deep neural network for scalable decision-making based on past experiences. An adaptive procedure is incorporated to adjust algorithm parameters dynamically.\nIntelligent Ultradense Edge Computing (I-UDEC), introduced in [22], integrates MEC servers and device-to-device communication. The aim was to minimize overall task processing time. The framework determines task processing locations, resource allocation strategies, and identifies services suitable for caching on edge servers to reduce computational time.\nA novel paradigm called Sensing-Assisted Wireless Edge Computing (SAWEC) was introduced in [23] to improve the performance of mobile virtual reality (VR) systems by leveraging knowledge about the physical environment and transmitting only the relevant data for service delivery to the edge server.\nThe authors of [24] presented a novel A2-UAV framework for optimizing task execution in multi-hop UAV networks, considering factors like deep neural network accuracy, image compression, target positions, and UAV energy position. This framework significantly outperforms existing solutions.\nThe authors of [11], the authors explore joint slicing of both communication and computation resources in a Radio Access Network (RAN), employing a two-tier slicing approach. The first tier, computation slicing, determines task execution locations and allocates computation resources using deep Q-learning. The second tier involves communication resource slicing, allocating communication resource blocks also through the application of deep Q-learning."}, {"title": "III. SYSTEM MODEL", "content": "In this paper, we examine an Open Radio Access Network (O-RAN) architecture that incorporates a single Radio Unit (RU). This RU communicates with users via radio communication. The signals received are subsequently transmitted to the Distributed Unit (DU) via the network's fronthaul. The DU is outfitted with a Multi-access Edge Computing (MEC) server, which is responsible for processing offloaded tasks. A real-time RAN Intelligent Controller (RIC) is also housed within the DU, where it operates xApps. Communication between the DU and the Centralized Unit (CU) is facilitated through the midhaul. Lastly, the CU is linked to the core network via the network's backhaul. This comprehensive setup forms the basis of our O-RAN network structure under consideration and is shown in figure 1 [25].\nThe set of services S in the network contains two slice types of URLLC and eMBB (S = {1, 2}). The total number of users connected to the RU is N with N, UEs connected to the RU with s\u2208 S. Each user equipment j of slice s connected to the RU, denoted by uj,s randomly generates a task Tj. Each"}, {"title": "IV. PROPOSED METHOD AND OPTIMIZATION", "content": "In the proposed MEC-equipped system, the volume of offloading task requests from users frequently exceeds the available resources, necessitating an intelligent controller to manage task offloading and resource distribution effectively. The main objective of this optimization is to minimize the total execution time while considering the unique requirements of users belonging to different network slices. For URLLC users, the critical factor is ensuring that processing times do not exceed the predefined latency limits, as any violation would breach the Service Level Agreement (SLA). For mMTC, the focus shifts to minimizing the energy consumption since these tasks are typically handled by battery-powered IoT devices. The intelligent controller must make real-time decisions on which tasks to offload based on the current system load and available resources. This involves prioritizing tasks by slice type and urgency, dynamically allocating computational resources, and adjusting strategies as network conditions change.\nIn a Markov Decision Process (MDP), the state space represents all possible system conditions, defined by current load, available resources, and the task requesting offloading. The action space includes decisions like task offloading, task prioritization by slice type, and resource allocation. Transition probabilities indicate the likelihood of moving between states based on actions, adhering to the Markov property. The reward function quantifies the desirability of state-action pairs, encouraging decisions that minimize execution time.\nReinforcement Learning (RL) helps the controller adapt to changes in a dynamic environment, handling delayed rewards. It manages the trade-off between exploration (testing new strategies) and exploitation (using known methods). The goal is to find an optimal policy that minimizes execution time while meeting network slice requirements, making RL ideal for this scenario. To address this problem, we propose a Proximal Policy Optimization (PPO)-based task offloading and joint resource allocation strategy. PPO is selected due to its ability to handle large and continuous state spaces, which is essential for efficiently managing dynamic resource allocation in a 5G multi-access edge computing environment. The agent learns to assign resources among users with task offloading requests during the training process. The reward function that the agent is trying to maximize is as follows:\n\\begin{equation}\nR = \\sum_{t=1}^{\\infty} \\gamma^t \\sum_{j=1}^{N} w_s r_{j,t}(k^{comp}, k^{comm})\n\\end{equation}\ns.t.:\n\\begin{equation}\n\\sum_{j=1}^{N} K^{comm} \\leq R_{comm}\n\\end{equation}\n\\begin{equation}\n\\sum_{j=1}^{N} K^{comp} \\leq R_{comp}\n\\end{equation}\nwhere \u03b3 is the discount factor, ws is the weight associated with each slice, rj,t(kcomp, kcomm) is the reward from each user after assigning kcomm communication resource and kcomp computation resource. Constraints 7a and 7b make sure that the number of assigned communication and computational resources to the users do not exceed the available resources respectively.\nThe observation for the task Tj from user uj,s is defined as obs = (s, bj, Cj, Tj, Cj, Rcomm, Rcomp, fs, frb), where Rremm and Reem are the remaining communication and computational resources when task Tj arrives. The action is also defines as action = (Kcomm, Kcomp).\nIn case of deciding that a task should be processed locally, the agent would not assign any resource to that task therefore Kcomp = Kcomm = 0. For that reason binary variable xj is defined to show whether that task should be processed locally (xj = 0) or offloaded to the MEC server (xj = 1)."}, {"title": "A. URLLC Reward Function", "content": "As mentioned previously, there are two important criteria for URLLC users while addressing the offloading decision. The first one is to minimize the execution time and the second one enforces not violating the SLA by the latency of processing the task. The following is the reward function for URLLC users:\n\\begin{equation}\nT_{URLLC} = \\frac{2}{1+e^{-d \\times r_{RU}(t_{exe})}} - 1\n\\end{equation}\nwhere,\n\\begin{equation}\nr_{RU}(t_{exe}) = \\alpha \\times (\\frac{T_{local}^{+process}}{t_{exe}}) + \\beta \\times (\\frac{T_{local}^{+process}}{T}), t_{exe} = \\begin{cases} t_{trans} + t_{MEC}^{+process} & \\text{if } x_i = 1 \\\\ t_{local}^{+process} & \\text{if } x_i = 0 \\end{cases}\n\\end{equation}\nwhere $t_{exe}$ is the latency of processing the task. If there are no resources assigned to this task, then the task is processed locally, otherwise the task would be offloaded to MEC server."}, {"title": "B. mMTC Reward Function", "content": "Since most of the devices using this service, are battery powered IoT devices, then in addition to decreasing the latency of processing the data, decreasing the power consumption is also of importance. Therefore, the reward function for this service is as follows:\n\\begin{equation}\nT_{mMTC} = \\frac{2}{1+e^{-d \\times r_{m}(t_{exe}, E_{exe})}} - 1\n\\end{equation}\nwhere,\n\\begin{equation}\nr_{m}(t_{exe}, E_{exe}) = \\alpha \\times (\\frac{t_{local}^{+process}}{t_{exe}}) + \\beta \\times (\\frac{E_{local}^{+process}}{E_{exe}}), \\begin{cases} E_{exe} = E_{MEC}^{+process} & \\text{if } x_i = 1 \\\\ E_{local}^{+process} & \\text{if } x_i = 0 \\end{cases}\n\\end{equation}\nwhere Eexe is the energy consumption of processing the task."}, {"title": "V. SIMULATION RESULTS", "content": "The simulation environment is comprised of a centrally positioned base station, covering a rectangular area of 2000 \u00d7 3000 meters. This base station is equipped with a MEC, a high-performance server featuring a 4-core CPU operating at a frequency of 2 GHz. The server has the capacity to allocate up to 40 units of computational resources to the users. Concurrently, the radio unit, is equipped to supply 80 blocks of communication resources.\nThe number of UEs for each network slice that generate tasks at each time step is modeled as a random variable following a Gaussian distribution. The environment represents an urban setting with a higher density of IoT devices than UEs. Specifically, the number of URLLC UEs have a mean (\u00b5) of 10 and a variance (\u03c3\u00b2) of 2, while the mMTC UEs have a mean of 30 and a variance of 5. These UE are uniformly distributed within the coverage area. The communication channel consists of 80 resource blocks, each with a bandwidth of 4 MHz, which can be dynamically allocated to users based on their requirements. The channel conditions are modeled using a Rayleigh fading model Detailed parameters for the environment are provided in Table II.\nEach UE generates a task that requires b bytes and c CPU cycles, following a uniform distribution within the bounds specified in Table III. The CPU frequency of each UE is contingent upon whether they belong to the URLLC or mMTC slice. Similarly, the power consumption of each UE is dependent on the slice to which they belong. The parameters used for the UEs are outlined in Table III.\nThe PPO agent employs a neural network architecture consisting of four hidden layers with 128, 256, 128, and 64 neurons, respectively. The model was trained for 15,000 episodes to optimize policy performance through iterative updates. The specific hyperparameters employed during the training phase are detailed in Table IV.\nSimulation results averaged over 5,000 experiments, illustrating the variations in performance metrics with varying numbers of UEs, and communication channels for RL agents compared to sequential and fair assignments."}, {"title": "VI. CONCLUSION", "content": "This study presents an end-to-end solution for ultra low latency task offloading at the ORAN-enabled 5G edge, addressing diverse emerging services including autonomous driving and low-power IoT. The reinforcement learning-based solution incorporate the best offloading strategy along with the optimized allocation of the required communication and computing resources to enhance overall performance for each network slice URLLC, mMTC). The decision-making process is powered by Deep Q-Learning and Proximal Policy Optimization. In this methodology, an agent is trained on a diverse set of user equipment, each with varying task bytes, CPU cycles, and requirements that are contingent on the network slice they are associated with. This dynamic and adaptable training process equips the agent with the ability to handle a wide range of scenarios and conditions. The simulation results underscore the effectiveness of Proximal Policy Optimization as a decision-making tool in this context. It provides compelling evidence that our approach can successfully navigate the complexities of resource allocation in a multi-slice network environment, thereby validating the potential of our proposed methodology."}]}