{"title": "Detection Made Easy: Potentials of Large Language Models for Solidity Vulnerabilities", "authors": ["Md Tauseef Alam", "Raju Halder", "Abyayananda Maiti"], "abstract": "The large-scale deployment of Solidity smart contracts on the Ethereum mainnet has increasingly attracted financially-motivated attackers in recent years. A few now-infamous attacks in Ethereum's history includes DAO attack in 2016 (50 million dollars lost), Parity Wallet hack in 2017 (146 million dollars locked), Beautychain's token BEC in 2018 (900 million dollars market value fell to 0), and NFT gaming blockchain breach in 2022 ($600 million in Ether stolen). This paper presents a comprehensive investigation of the use of large language models (LLMs) and their capabilities in detecting OWASP Top Ten vulnerabilities in Solidity. We introduce a novel, class-balanced, structured, and labeled dataset named VulSmart, which we use to benchmark and compare the performance of open-source LLMs such as CodeLlama, Llama2, CodeT5 and Falcon, alongside closed-source models like GPT-3.5 Turbo and GPT-40 Mini. Our proposed SmartVD framework is rigorously tested against these models through extensive automated and manual evaluations, utilizing BLEU and ROUGE metrics to assess the effectiveness of vulnerability detection in smart contracts. We also explore three distinct prompting strategies-zero-shot, few-shot, and chain-of-thought to evaluate the multi-class classification and generative capabilities of the SmartVD framework. Our findings reveal that SmartVD outperforms its open-source counterparts and even exceeds the performance of closed-source base models like GPT-3.5 and GPT-40 Mini. After fine-tuning, the closed-source models, GPT-3.5 Turbo and GPT-40 Mini, achieved remarkable performance with 99% accuracy in detecting vulnerabilities, 94% in identifying their types, and 98% in determining severity. Notably, SmartVD performs best with the 'chain-of-thought' prompting technique, whereas the fine-tuned closed-source models excel with the 'zero-shot' prompting approach.", "sections": [{"title": "I. INTRODUCTION", "content": "The proliferation of blockchain platforms, such as Ethereum, in recent years has driven the widespread adoption of smart contracts across various sectors [1]. However, with this popularity and mass-scale adoption, security vulnerabilities in Solidity smart contracts have surfaced, causing substantial economic losses [2]. These security challenges not only put users' funds at risk but also impede the overall growth of the blockchain ecosystem. As a result, identifying and addressing vulnerabilities in smart contracts becomes increasingly crucial.\nLarge language models (LLMs) have recently demonstrated impressive capabilities in a wide range of language-related tasks [3]. LLMs undergo pre-training with extensive text corpora, utilizing various self-supervised training strategies, including masked language modeling and next sentence prediction [3]. LLMs such as GPT-4 and CodeLlama, which are pre-trained on extensive code corpora, have achieved significant progress in various code-related tasks within a relatively short time. These tasks encompass code completion [4], automated program repair [5], test generation [6], code evolution, and fault localization [7], among others. The success of these models underscores the potential of Large Language Models (LLMs) and opens the door to exploring more advanced techniques. This leads to an interesting question: Can these cutting-edge pre-trained LLMs also be effectively applied to detecting security vulnerabilities in Solidity smart contracts?\nIn this context, several existing works [8], [9], [10], [11], [12], [13], [14] investigated the use of large language models (LLMs), such as GPT models (e.g., GPT-3, GPT-3.5 Turbo), for detecting Solidity vulnerabilities. For instance, [8] evaluated ChatGPT's vulnerability detection capabilities using SmartBugs-curated benchmark codes [15]. However, as the dataset exclusively contains only vulnerable contracts, this potentially led to misleading results due to hallucinations caused by the imbalance in the dataset. Even though, the authors in [12], [13], [14] assessed the performance of LLMs, such as Llama2 and GPT models, on vulnerability detection, they provided only preliminary insights with limited in-depth analysis. While other studies [9], [10], [11] utilized ChatGPT for vulnerability detection and remediation, their primary emphasis was on fixing vulnerable codes.\nThese initial efforts fail to give a clear picture of what impact is made by fine-tuning process needed for correct knowledge enrichment. They also lack to give an idea of open source LLMs and closed source LLMs to proceed in this domain. Importantly, these works often overlook the critical role of prompt engineering techniques in enhancing vulnerability detection. Therefore, there is a pressing need to move beyond these initial efforts and comprehensively evaluate the potential of state-of-the-art LLMs in smart contract vulnerability detection. Additionally, the exploration of open-source LLMs in this domain remains limited, warranting further investigation.\nWhile vulnerability detection is inherently complex with numerous factors that can influence and potentially limit the quality of solutions, this can significantly be improved with the advanced expressiveness and broad knowledge incorporated into state-of-the-art large language models (LLMs). Effective vulnerability detection requires a thorough and sophisticated understanding of different elements within the code, highlighting its importance as a key research area. Investigating this hypothesis is crucial for progressing in the field of"}, {"title": null, "content": "vulnerability detection. Initial results, as presented in [8], [16], indicate that simple prompting techniques with the ChatGPT LLM did not yield significantly better outcomes compared to random prediction in the context of vulnerability detection. This insight underscores the value of specifically fine-tuning large language models (LLMs) for this task, suggesting it as a promising avenue for future research exploration.\nThis paper presents SmartVD, an efficient LLM-driven vulnerability detection framework for Solidity codes, which is developed by fine-tuning the pre-trained LLM, Codelama. To facilitate this fine-tuning, we introduce a carefully curated domain-specific class-balanced dataset, called VulSmart. Our approach involves two key methods to enable a detailed and accurate assessment by the model: a) binary classification, which determines whether vulnerabilities exist within functions, and b) multi-class classification and generation, which identifies specific types of vulnerabilities along with its severity. This evaluative methodology is consistently applied to both traditional deep learning classification techniques and other popular LLMs under review, enabling us to perform a rigorous assessment on the effectiveness of SmartVD in comparison to other pre-trained LLMs. We also attempt to understand how fine-tuning the LLMs on our labeled datasets affects the model's performance in this classification task. Finally, we conduct a manual review and alteration of selected code examples to uncover which features LLMs can and cannot recognize when predicting vulnerabilities."}, {"title": "A. Research Questions", "content": "We formulate the following research questions to meet our aforementioned objectives.\n\u2022\n\u2022\n\u2022 Tapping into the unexplored potential of LLMs in vulnerability detection, our results provide valuable insights and pave the way for future research in this field."}, {"title": "II. RELATED STUDIES", "content": "The following work have been relevant to the following three research areas, namely smart contract vulnerability detection using: a) Formal method tools b) Deep learning based approaches, and c) Large Language Models (LLMs)"}, {"title": "A. Formal Method Tools", "content": "In the work [17] Nikolay Ivanov et al. highlight 133 proposals published in field of smart contract vulnerability out of which 115 solutions are based on formal method techniques. Out of these 115 tools, as highlighted in [18] because of non-availability, non-functionality, non-compatibility and no documentation we only have few popular working tools. Oyente [19] is the pioneer work for smart contract bug detection based on symbolic execution from solidity bytecode. However, it suffers from several false alarms even in case of trivial Smart Contracts (for Reentrancy). Moreover, verification conditions adopted in this tool are neither sound nor complete. Slither [20] is an open source static analysis tool developed by trail of bits. It uses intermediate representation SlithIR (SSA based) for solidity code and predefined rules to match the problematic codes. Since, it uses predefined rules to identify the vulnerability it tends to give several false positives. Krupp et al. develop teEther [21], a symbolic execution based tool to create an end to end exploit generation from smart contracts bytecode. It constructs control flow graph from bytecode to generate path constraints, solved by theorem prover Z3. However, these unreadable bytecodes make detection inefficient and rules difficult to formulate. Tsankov et al. develop Securify [22] at SRI labs (ETH Zurich). It works at bytecode level and derives semantic facts from contract's dependency analysis. Then use this information to check compliance and violation patterns for proving if a property written in domain specific language holds or not. Smartcheck [23] translates solidity source code to XML-based intermediate representation checked against XPath patterns to find vulnerability. However, the intermediate representation restrictions make it difficult to formulate vulnerability rules. Mythril [24] employs symbolic execution to build the Control Flow Graph (CFG) of the contract from the EVM bytecode, and then executes the predefined logic rules to identify vulnerabilities. Manticore [25] uses dynamic symbolic execution technique to detect bug for traditional binaries and Ethereum bytecode. This tool suffers from number of contracts checking failed due to predefined assumptions. Wang et al. proposes SliSE [26] that uses program slicing and symbolic execution technique to detect reentrancy vulnerability among"}, {"title": "B. Deep Learning based methods", "content": "Deep learning-based proposals are not dependent on fixed rules for bug detection and are more extendable than formal method tools. There have been a number of proposals to detect vulnerability in smart contracts using various deep learning techniques. In [27] authors employs long short-term memory (LSTM) to sequentially learn smart contract weakness through ethereum opcodes. Liao et al. in [28] introduce soliaudit tool which uses machine learning technique in combination with fuzz testing of smart contracts to detect vulnerability. In [29] authors use average stochastic gradient descent weight-dropped LSTM (AWD-LSTM), a variant of LSTM to detect vulnerability in smart contract. In [30] the control and data flow information of contract is used by graph neural networks and expert knowledge for detecting vulnerability. The authors in [31] introduces Contractward that uses different machine learning techniques that can be used to detect six types of smart contract vulnerabilities. Liu et al. [32] use graph neural network technique to built AMEVuldetector which uses semantic graph feature from source code and local expert patterns techniques to detect vulnerabilities. In [33] the authors propose a deep learning based framework (DeeSCVHunter) in combination with vulnerability candidate slicing to detect reentrancy and time dependence vulnerability in smart contracts. The authors in [34] propose VSCL framework which uses smart contract bytecode represented as feature vector which is fed to deep neural network to detect vulnerability present. Vulpedia [35] uses structural program features from smart contracts along with detection rules composed of vulnerability signatures to detect vulnerability."}, {"title": "C. Large Language Models", "content": "Chen et al. [8] explore the potential of ChatGPT in detecting smart contract vulnerability types. They compare ChatGPT with other formal method tools and identify the cause of false positives generated by ChatGPT. In [9] the authors evaluate the performance of ChatGPT in fixing vulnerable code. They check for the vulnerability in existing code and then pass the error message along with code to fix it. The authors in [10] propose an approach VulnHunt-GPT using GPT3 to detect smart contract vulnerability by training it on smart contract functions and vulnerability. They also compare their results with the existing formal method tools. In [11] Xia et al. uses the large language model to verify the ERC specification against its solidity source code implementation to better understand its security impact. The authors in [14] leverage the large language model Llama2 to detect the vulnerability present in smart contracts more effectively. They train the LLM on custom dataset to demonstrate its efficacy in terms of accuracy when compared to formal method based tools. In [12] Hu et al. propose GPTLens framework where the LLM plays the dual role of Auditor and Critic. Auditor finds the vulnerability and critic validates it and thus reduces the false positives. In [36] the authors introduce Soley an automated approach using LLMs to detect logic vulnerabilities in smart contracts. In [13] the authors employ static analysis combined with GPT-3.5 Turbo to detect vulnerabilities in smart contract logic. They introduce GPTScan, a tool that utilizes GPT to identify potential vulnerabilities, enhancing accuracy by guiding GPT to intelligently recognize key variables and statements. These identified elements are then validated through static confirmation."}, {"title": "D. Existing datasets", "content": "The existing popular dataset in the field of smart contracts reported in literature are CodeSmell [37] which has smart contract related posts from Ethereum Stack Exchange, as well as 587 real-world smart contracts containing defects. SolidiFi [38] contains 300 real world smart contracts injected with seven different vulnerability types. SmartBugs (curated and wild) [15] consists of 143 annotated vulnerable contracts with 208 tagged vulnerabilities and 47,518 unique contracts collected through Etherscan. Not So Smart Contract [39] consists of 18 real world smart contracts containing bug which lead to substantial economic loss, Smart Contract benchmark Suites [40] consists of 393 real world vulnerable smart contracts compiled from different github repository and sources. ScrawlID [41] consists of 6780 smart contracts address and the vulnerability present. As evident, all these existing datasets are imbalanced and lack localized information regarding the vulnerabilities present within the code."}, {"title": "III. CORPUS DESCRIPTION", "content": "Given the critical importance of smart contract security, accurate data on vulnerabilities is essential to identify real-world contracts that are at risk, along with the specific type of vulnerabilities they contain. Therefore, a comprehensive dataset that includes both smart contract source code and a detailed and localized vulnerability information is crucial before applying it to LLMs. Our study began with an extensive literature review to assess existing datasets related to smart contract vulnerabilities. However, we found that most existing datasets [37], [39], [38], [15], [40], [41] suffer from a significant lack of structured, labeled data, and many are imbalanced as evident from Table I, making them unsuitable for effective training or fine-tuning of LLMs.\nTo the best of our knowledge, no publicly available dataset offers a class balanced collection of smart contract code, along with corresponding vulnerability types and localized information. To address this gap and improve smart contract security, we have curated a novel dataset, VulSmart, which includes the source code of smart contracts, the type of vulnerability, its severity, and the specific vulnerable code segments. In the following sections, we outline the steps taken to develop this dataset."}, {"title": "A. Dataset Collection", "content": "All smart contracts deployed on the Ethereum network can be accessed through Etherscan. However, only a small portion of these contracts have publicly available source codes, as bytecode is the only requirement for deployment on the network, leading many developers to withhold their source code. Additionally, there are numerous identical smart contracts with available source code, reducing the number of unique contracts. This makes the task of collecting smart contract source code particularly challenging.\nWith this in mind, we adopt the principle of re-usability and select the dataset corpus hosted on GitHub by various sources, containing source code from real-world smart contracts that have been featured in peer-reviewed publications. This way, we ensure a broad and diverse representation within the dataset. We collect a total of 9,437 smart contracts source code in this phase. These contracts include both vulnerable and non-vulnerable examples, verified using formal methods tools. Notably, the vulnerability information in existing dataset is often included as comments within the source code, making it challenging to use directly for our objectives. As a result, we proceeded to the next step of data cleaning."}, {"title": "B. Data Cleaning", "content": "We conduct data cleaning by processing the collected data from the previous step. The dataset of 9.4K collected smart contracts consists of both vulnerable and non-vulnerable contracts, with approximately 78% classified as non-vulnerable. To create a class-balanced dataset, we carefully selected 3,000 samples from this pool. The selection is based on appropriate contract size, with incomplete, non-functional, or corrupted contracts excluded through verification using the Remix IDE\u00b9. To eliminate duplicates, we first remove comments and extraneous blank lines from the source code. We then use the hash of the source code to further ensure no duplicate samples remain. Afterward, we proceed to data annotation, classifying each smart contract as either vulnerable or non-vulnerable, ultimately creating a balanced dataset with both types of samples."}, {"title": "C. Data Annotation", "content": "The previous phase results in a corpus of 2,541 unique samples, consisting of smart contract source code along with their respective compiler versions. We adhere to the OWASP Smart Contract Top 10\u00b2 and DASP top 10\u00b3 guidelines to target the most critical vulnerabilities out of the 37 listed in the Smart Contract Weakness Classification (SWC) registry\u2074. For vulnerability severity classification, we follow the Smart Contract Security Verification Standard (SCSVS)\u2075 and utilize SmartCheck [23].\nIn total, we target 13 types of vulnerabilities, including Access Control, Arithmetic Overflow/Underflow, Bad Randomness, Denial of Service, Front Running/Transaction Ordering Dependence, Gasless Send, Reentrancy, Short Addresses, Time Manipulation, tx.origin, Unchecked Low-Level Call, Unsafe Delegate Call, and Unsafe Suicide. These vulnerabilities are classified into three severity levels: High, Medium, and Low.\nWe select four popular state-of-the-art (SOTA) formal method tools\u2014Securify, Slither, Mythril, and Oyente-to detect the presence of these vulnerabilities. A vulnerability is annotated in a smart contract if at least two of the four tools identify the same issue. The annotation process of VulSmart dataset is depicted in Figure 1. We first provide the source code to the Smartbugs framework [15], which contains docker images of the four tools, and monitor the log files generated by each tool. After reports are generated, we label each smart contract sample based on the presence of vulnerabilities (Yes/No), the specific vulnerability type from the 13 under consideration, its severity (High/Medium/Low), the vulnerable function, and the corresponding vulnerable lines.\nThe entire annotation process took approximately one month, completed by two trained undergraduate students under the guidance of a senior PhD scholar specializing in formal methods. To ensure the quality of the annotations, we measure inter-annotator agreement (IAA) using Cohen's Kappa score [42], resulting in a score of 0.76, affirming the high quality and reliability of the annotations."}, {"title": "D. Data validation", "content": "After completing the annotations of the dataset samples, we proceed with manual data validation. For this task, we invite two postgraduate students and one PhD scholar from the computer science department, each with technical expertise in formal methods. Together, they validate 10% of the dataset samples. Once validation was complete, the labeled contracts are added to finalize our VulSmart dataset for further use.\nOur objective is to offer researchers a foundational resource that can be further expanded with additional examples\u2014ideally, encompassing different forms of the same vulnerability."}, {"title": "IV. PROPOSED METHODOLOGY", "content": "Given a smart contract $C$ written in Solidity, the objective is to identify and generate critical information regarding potential vulnerabilities within the code. These tasks are defined as follows:\nDefine a characteristic function $f\u2081:C \u2192 \\{0,1\\}$ such that: \n$f_1(C) = \\{\\begin{array}{ll} 1 & \\text{if } v \\in V, \\\\ & \\text{where } V \\text{ is the set of vulnerabilities} \\\\ 0 & \\text{otherwise} \\end{array}$\nThis function detects the existence of any vulnerability $v$ in the contract $C$.\nGiven that $f_1(C) = 1$, we introduce a mapping $f_2: C \u2192 T$, where $T$ denotes the set of all possible vulnerability class. This mapping identifies the class/type $T \\in T$ of the detected vulnerability.\nDefine a severity function $f\u2083: T \u2192 S$, where $S$ represents the set of all severity levels, structured as a partially ordered set (poset) $(S, \u2264)$. The function $f\u2083(T) = S$ evaluates the severity $S \\in S$ corresponding to the identified vulnerability type $T$.\nThe entire process can be described by a composite function $F: C \u2192 V\u00d7T \u00d7 S$, defined as $F(C) = (f_1(C), f_2(C), f_3(f_2(C)))$. This composite function sequentially applies $f_1(C), f_2(C)$, and $f_3(T)$ to the smart contract code $C$, generating a tuple $(v, T, S)$ that encapsulates information about the presence $(v)$, type $(T)$, and severity $(S)$ of the detected vulnerabilities. The output is a well-defined and structured data that characterizes the vulnerabilities present in the smart contract $C$. Figure 2 illustrates an instance of multi-class classification applied to address the problem defined above. In this instance, the source code of a smart contract is provided to the LLM along with a prompt. The model then outputs the presence of a vulnerability (binary classification), its type (multi-class classification), and the corresponding severity level (multi-class classification)."}, {"title": "B. Selection of LLMs", "content": "We highlight the challenges we encountered during implementing our methodology and selection of the LLMs.\nOut of several LLMs, we identify and fine-tune four optimal code-related models. While these LLMs excel in code comprehension and generation, they lack inherent capabilities for vulnerability detection. The aim was to select models that are easy to use from a technical perspective (i.e. model size, available libraries, inference speed etc). For instance, CodeGeeX [43] does not provide"}, {"title": null, "content": "standard library support for training, which limited its applicability.\nWe meticulously select large language models (LLMs) with configurations aimed at optimizing resource efficiency and minimizing computational overhead. In our selection process, we carefully consider both the context window-the maximum number of tokens the model can process as input and the total number of tokens supported by the LLM. This approach is essential to streamline the development of our SmartVD framework, ensuring that it operates effectively without excessive resource demands.\nExisting pre-trained language models are typically designed for tasks like next token prediction, which differs from our objective of vulnerability classification. Unlike token prediction, our goal is to categorize the entire input sequence rather than anticipate the following tokens. To enhance the performance of LLMs for this specific task, we incorporate vulnerability-related features directly into the smart contract code during training. Despite these efforts, we observe that SolidityT5\u2076 do not perform optimally for classification tasks."}, {"title": "C. Components of SmartVD Framework", "content": "Our SmartVD framework is fine-tuned version of Code Llama [44] and the following are its components:\nThe input layer is composed of smart contract code snippets and the corresponding instructions provided as prompts.\nThe smart contract code, provided as input text, undergoes tokenization using the AutoTokenizer class, where it is divided into a sequence of tokens. $t = AutoTokenizer(C)$, where $C$ is the input smart contract code and t is the resulting sequence of tokens.\nThe tokenized input $t$ is converted into dense vector representations $E$ which are optimized for model processing. $E = Embedding(t)$, where $t$ represents the sequence of tokens and $E$ represents each token with a fixed dimensional embedding, of size $d$.\nThe proposed SmartVD framework employs a Multi-Head Attention (MHA) mechanism within its attention layer. The self-attention process computes the attention scores $A$ over the embeddings using the following equation:\n$A = Softmax(\\frac{QKT}{\\sqrt{dk}})V$\nwhere $Q, K$, and $V$ represent the query, key, and value matrices derived from the embeddings $E$, respectively. Here, $d_k$ denotes the dimensionality of the key vectors. This mechanism enables the framework to effectively capture dependencies and relationships between tokens by computing weighted averages of the value vectors based on their relevance to the query vectors.\nLow-Rank Adaptation (LoRA) is employed within specific layers of the Multi-Head Attention blocks to enhance the model's capability for vulnerability prediction. LORA introduces low-rank matrices that adjust the attention scores without significantly modifying the pre-trained weights. The modified attention scores $A'$ are computed as:\n$A' = A + \\Delta A$\nwhere $\u0394A$ denotes the modifications introduced by LORA. This approach enables efficient adaptation to vulnerability detection tasks with minimal computational overhead and parameter changes.\nResidual connections are incorporated into the outputs of both the self-attention and feed-forward layers, followed by normalization. The normalization process employs RMSNorm, which normalizes activations using the Root Mean Square (RMS) and scales them with learnable parameters.\n$\\overline{a_i} = \\frac{a_i}{RMS(a)}*g_i, \\quad where \\quad RMS(a) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} a_i^2}$\n$a_i$ represents the pre-activation value for the i-th neuron, which is the sum of weighted inputs prior to applying the activation function. The term $g_i$ is a learnable scaling parameter that adjusts the normalized output $\u0101_i$, enabling the network to fine-tune the impact of each neuron's contribution. This technique ensures that the activations are normalized and scaled appropriately, improving the stability and performance of the model.\nIn this component of the SmartVD framework, the system generates text outputs $O$ that deliver prediction regarding vulnerabilities, encompassing both their types and severity.\n$O = OutputLayer(Norm2)$\nwhere Norm2 is the output of the final normalization layer. The generated output includes information on the presence of vulnerabilities (Yes/No), the specific type of vulnerability (selected from 13 possible types), and the severity level (classified as Low, Medium, or High)."}, {"title": "D. Alignment for Vulnerability detection: Fine Tuning of Models", "content": "The framework incorporates LoRA adjustments, which fine-tune the model for specific tasks without substantial modification of the pre-trained weights. These adjustments are encapsulated in the learnable parameters of the attention layers, allowing for task-specific adaptations while preserving the core capabilities of the pre-trained model.\nGPT Models To evaluate the effectiveness of language models in smart contract vulnerability detection beyond open-source LLMs, we fine-tune GPT-3.5 Turbo\u2077 and GPT-40 Mini\u2078 for this specific task. Through prompt engineering, we generate diverse scenarios-zero-shot, few-shot, and step-by-step approaches to guide the models in identifying vulnerabilities within Solidity smart contracts. The dataset is then converted into the JSON format required for GPT-3.5 Turbo and GPT-40 Mini fine-tuning, structuring each entry as a conversation with a system prompt, a user message containing the contract code, and an assistant response providing the expected output. Training and validation datasets are uploaded to OpenAI's servers, where fine-tuning jobs are initiated using the API, specifying the GPT-3.5 Turbo and GPT-40 Mini base models. This process effectively enhances the models' ability to accurately detect and classify vulnerabilities in smart contracts, making them more specialized for this novel task.\nGiven a dataset D = {d\u2081, d\u2082, ..., d\u2099} where each d\u1d62 is a smart contract code snippet, we define the prompt function P such that:\n$P(d_i) = Prompt(d_i) = Input \\ to \\ the \\ model$\nFor LoRA, the scaling factor $\u03b1$ is set to 16, the dropout rate $d$ is 0.2, and the rank $r$ is 4, with no bias $Bias = 0$. The target modules $T$ are specifically adapted during the fine-tuning process. The training uses the optimizer $Optim = PagedAdamW-8bit$, with a learning rate $\\eta = 1 \\times 10^{-3}$ and a weight decay $\u03bb = 0.001$. Mixed precision training is enabled using fp16, and the maximum gradient norm is max_grad_norm = 0.3. The maximum number of training steps is max_steps = 500, with a warm-up ratio warmup_ratio = 0.03 and a cosine learning rate scheduler $S(n,t) = cosine$. The evaluation strategy is based on steps, with logging steps set to log_steps = 25. The total number of steps per epoch is calculated as:\n$Total \\ Steps = \\lceil \\frac{n}{B \\times device \\times G} \\rceil  \\times E$\nwhere $n$ is the total number of samples in the training dataset. The model parameters $\u03b8$ are updated by minimizing the loss function $L$ using the specified optimizer:\n$\u03b8_{new} = \u03b8_{old} \u2013 \u03b7\u2207\u03b8L(D,\u03b8)$\nwhere $D$ represents the training dataset. The process continues until the maximum number of steps max_steps is reached. Following training, the fine-tuned model $M_{fine}$ is merged to optimize deployment. The merged model $M_{merged}$ is defined as:\n$M_{merged} = Merge(M_{base}, M_{fine})$"}, {"title": "V. EXPERIMENTAL RESULTS AND ANALYSIS", "content": "We select six extensively used pre-trained large language models, comprising both code-related and general-purpose LLMs. We opt for two closed-source OpenAI models, GPT-3.5 Turbo and GPT-40 Mini, accessed through the OpenAI API Key. For open-source LLMs, we chose Llama2 (7B), CodeLlama (7B), CodeT5 (770M), and Falcon (7B), all accessible via the Hugging Face API. We fine-tune and execute all experiments involving open-source LLMs on a single-core NVIDIA A100 80GB PCIe server, utilizing its 80 GB GPU. For the fine-tuning and execution of experiments with closed-source LLMs, we leverage the OpenAI API."}, {"title": "B. Qualitative Analysis", "content": "Initially, we select a few smart contract samples from the test dataset and use a standard prompt to query for vulnerabilities, including their types and severity levels. As shown in Figure 4a, we input the Solidity code along with the prompts, but the Codellama base model was unable to generate relevant outputs for the required vulnerability labels. However, after fine-tuning the model, as indicated in Figure 4b, and using the same Solidity code, our SmartVD framework accurately detect vulnerabilities, identify their corresponding classes, and determine the severity level of each vulnerability correctly. Similarly, as shown in Figure 4c, the base Llama2 model produces incoherent results when predicting the type of vulnerability. In contrast, the fine-tune version of Llama2 accurately identifies the vulnerability type, as illustrated in Figure 4d.\nWe select one PhD scholar and two post graduate students working in formal methods field and smart contract vulnerability detection to evaluate the performance of models. They used 10% of the sample and gave a score out of 3 for correct vulnerability present, its class and severity. The average score obtained for our SmartVD is 2.6 out of 3 (i.e. 87% accuracy)\nThe observation here is that pre-trained model hallucinate and gives a large number of false positives in most cases. Whereas, the outputs generated by our SmartVD framework is correct almost 87% of the times in detecting the vulnerability with class and reduces the number of false positives."}, {"title": "C. Quantitative Analysis", "content": "We have utilize four commonly employed evaluation metrics Accuracy, Precision, Recall, and F1-score to assess the performance of the models. Additionally we include BLEU score (Bilingual Evaluation Understudy) [45], ROUGE score (Recall-Oriented Understudy for Gisting Evaluation) [46] and MCC score (Matthew's Correlation Coefficient) [47] to understand the change in improvement of model to solve the underlined problem statement. The BLEU score and ROGUE score are used for evaluating the quality of generated text. It measures the similarity between a machine-generated text and a reference text. The MCC metric help us to compare the same model with different configurations and accordingly rank transformer models.\nThe MCC computes a measurement with true positives (TPs), true negatives (TNs), false positives (FPs), and false negatives (FNs).\nThe MCC can be summarized by the following equation:\n$MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$\nThe MCC value ranges from \u22121 to +1 where +1 means the prediction is 100% accurate, 0 means model produces the random calculations and, -1 means the the predictions are totally false."}, {"title": null, "content": "The base models demonstrate considerable variation in performance across various metrics, indicating their varying underlying capabilities in detecting vulnerabilities. As demonstrated in Table III, GPT-3.5 emerges as the best-performing base model, with 78% accuracy, 82% precision, 78% recall, and a 77% F1-score. This signifies that GPT-3.5 has a strong inherent ability to detect vulnerabilities, without additional training. CodeLlama additionally performs well as a base model, particularly in precision (74%) and accuracy (66%), revealing a relatively strong ability to identify vulnerabilities. Llama2 performs moderately, achieving 56% in accuracy and recall and 43% in precision and F1-score, which suggests that it can detect vulnerabilities but is less reliable than the top-performing models. Falcon matches Llama2's moderate performance, with 48% accuracy and recall, but slightly lower precision (47%) and F1-score (44%). CodeT5 base model is the model with the lowest precision (33%), moderate accuracy (58%), and recall (58%). Its low F1-score (42%) highlights its limited ability to detect vulnerabilities. Overall, there is a significant difference between the leading base models (GPT-3.5 and CodeLlama) and the lagging models (CodeT5, Falcon, and Llama2), highlighting that some models are better suited to vulnerability detection tasks in their pre-trained state. Additionally, deep learning techniques such as LSTM and GRU exhibit lower precision and F1-scores, achieving only 34% and 43%, respectively, compared to several pre-trained LLMs in vulnerability detection tasks. However, we observe improved accuracy, precision, and recall for the vulnerability detection task using BiLSTM. Despite this improvement, BiLSTM performance declines significantly when tasked with identifying the type of vulnerability in a multi-class classification scenario."}, {"title": null, "content": "Fine-tuning significantly enhances the performance of various LLMs in detecting security vulnerabilities in smart contracts as proven by significant improvements in all key metrics, such as Accuracy, Precision, Recall, and F1-score as evident from Table V, VI, and VII. GPT-3.5 and GPT-40 Mini emerge as the finest performers, with near-perfect scores of 99% across all metrics after fine-tuning, confirming their brilliant suitability for vulnerability detection task. CodeLlama also shows strong performance, with its Precision, Recall, and F1-score all rising to 86%, presenting it as a suitable framework for vulnerability detection. On the other hand, CodeT5 remains static, showing no improvement across any metric after fine-tuning, indicating that it is less effective for this particular domain of vulnerability. Llama2 and Falcon show moderate improvements, but their scores remain lower than those of the leading models, indicating a limited ability to accurately detect vulnerabilities even after fine-tuning. Overall, fine-tuning is critical in improving the effectiveness of LLMs for detecting vulnerabilities in smart contracts, with some models benefiting significantly more than others.\nTo further evaluate the learning capabilities of the models, we assess their MCC scores, as presented in Table IV. Among the six LLMs tested, all except CodeT5 show significant improvements in binary classification of vulnerability presence or absence. Llama2 led with the most impressive enhancement, boasting an MCC score increase of approximately 1359.19%. Falcon"}]}