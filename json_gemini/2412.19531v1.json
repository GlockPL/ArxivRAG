{"title": "Is Your Text-to-Image Model Robust to Caption Noise?", "authors": ["Weichen Yu", "Ziyan Yang", "Shanchuan Lin", "Qi Zhao", "Jianyi Wang", "Liangke Gui", "Matt Fredrikson", "Lu Jiang"], "abstract": "In text-to-image (T2I) generation, a prevalent training technique involves utilizing Vision Language Models (VLMs) for image re-captioning. Even though VLMs are known to exhibit hallucination, generating descriptive content that deviates from the visual reality, the ramifications of such caption hallucinations on T2I generation performance remain under-explored. Through our empirical investigation, we first establish a comprehensive dataset comprising VLM-generated captions, and then systematically analyze how caption hallucination influences generation outcomes. Our findings reveal that (1) the disparities in caption quality persistently impact model outputs during fine-tuning. (2) VLMs confidence scores serve as reliable indicators for detecting and characterizing noise-related patterns in the data distribution. (3) even subtle variations in caption fidelity have significant effects on the quality of learned representations.\nThese findings collectively emphasize the profound impact of caption quality on model performance and highlight the need for more sophisticated robust training algorithm in T2I. In response to these observations, we propose a ap-proach leveraging VLM confidence score to mitigate caption noise, thereby enhancing the robustness of T2I models against hallucination in caption.", "sections": [{"title": "1. Introduction", "content": "A text-to-image (T2I) model generates an image based on a natural language description, referred to as a prompt, aim-ing at aligning the output image with the given description. Recent advancements in T2I generation have show-cased impressive capabilities in producing high-fidelity and diverse visual content [37-39, 41]. 1\nEnormous training data, specifically image-text pairs, is essential for training or fine-tuning T2I models. There has"}, {"title": "2. Related Work", "content": "Text-to-Image Generation. Various approaches [14, 17, 18, 38] have been proposed to generate image from natu-ral language. Among those, autoregressive methods [12, 24, 36, 51] and diffusion models [7, 37, 39, 45] have shown promising progress after trained on large-scale in-ternet image-text pairs followed by data filtering steps. To improve text-to-image generative models, previous works mainly focus on the visual side, such as latent diffusion models [37, 39], transformer based diffusion [3, 4, 34], or style generations [40, 44, 56]. The evaluation of these gen-erative models are based on general metrics such as FID score [17] for image generation quality and CLIP score [16] for textual alignment. Our work focuses on the text side of the text-to-image generation. We bring attention to the nu-ances within image captions and their impact on T2I gen-eration and provide more dimensions of evaluation, which help improve many previously overlooked areas such as hal-lucinations in generation, robustness in T2I, and etc.\nCaption Quality in T2I. Recent literature has increas-ingly emphasized the critical role of caption quality in text-to-image (T2I) generation models, leading to a paradigm shift in dataset preparation methodologies [42]. Contem-porary state-of-the-art T2I architectures have adopted so-phisticated vision-language models (VLMs) [1, 10, 29, 49] for image recaptioning to enhance training data quality. Notably, PixArt-Alpha [6] and PixArt-Sigma [7] utilize LLaVA-7B-v1.5 [29] and Share-Captioner [8] for recaption the training dataset respectively, though the former caption model LLaVA1.5 is identified significant hallucination ten-dencies in its outputs. Similarly, [42] employs PaLI [9], while Stable Diffusion 3 [13] leveraged CogVLM [49] for caption generation. While prior studies have addressed hal-lucination detection and mitigation in VLMs [48], these"}, {"title": "3. Robustness to Caption Noise in T2I", "content": "In this section, we first observe that VLMs exhibit patterns in generating hallucinated contents we compute three dis-tinct types of confidence scores using VLM, and then inves-tigate the correlation between these scores and hallucinated content. Building on these insights, as illustrated in Fig. 3, we propose a simple method to use token reweighting to leverage the computed confidence scores to mitigate the ef-fect of hallucinations in T2I model training and improve robustness to caption noise."}, {"title": "3.1. Correlation between Bias in VLM and Caption Noise", "content": "Motivation. We aim to distinguish hallucinated patterns from clean patterns. Given that current Vision-Language Models (VLMs) are typically trained using either a fixed Language Model (LLM) encoder or with minimal modifi-cations to the LLM encoder, there are inherent biases in the text modality. Specifically, we seek to determine whether hallucinated content in VLM outputs is more dependent on the textual input than on the visual input.\nTo explore this hypothesis, we begin by constructing a validation subset. Using GPT-40, we label a sample of 1,000 image-caption pairs, identifying words in the captions that conflict with the content of the images-i.e., halluci-nated elements in the captions. Detailed prompt used for the GPT-40 is provided in the supplementary. Subsequently, we employ the VLM to quantify hallucination scores using three distinct confidence scores, providing insights into the dependence of hallucination on text-based biases.\nGiven an image $I \\in R^{H\\times W}$ and the associated caption $c = [C_0, C_1,\\ldots]$, where $c_k$ denotes the k-th token, we utilize a VLM, denoted by $P$, to compute a confidence score $s_k$ for each individual token $c_k$ in the caption. We calculate the confidence score in the following three ways: Confidence score is calculated only on every text tokens, while the input of the VLM includes both image and caption:\n$s_k^i = P(c_k | I, C_{<k})$\nConfidence score without image, text only:\n$s_k^t = P(c_k | C_{<k})$\nConfidence score difference between with image and without image:\n$s_k^d = P(c_k | C_{<k}) - P(c_k | I, C_{<k})$\nAs shown in Fig. 2, we observe that there's a distribution shift between the noisy tokens and all the tokens. This observation suggests that the hallucinated content in the caption can be captured by the Vision-Language Model's (VLM) confidence scores. Specifically, the confidence scores, $s^d$, indicate the VLM's degree of \"confidence\u201d difference when generating a given token with image and without image."}, {"title": "3.2. An Approach to Alleviate Caption Noise in T2I", "content": "Based on the observation that VLM can be an indicator, out-putting a confidence score for each token in the caption, we leverage this score to downweighting the tokens that have a higher possibility to be noisy.\nIn the T2I model, there are layers with CrossAttention that uses the text to instruct the image generation. Thus, a straightforward method is to assign different weights to dif-ferent tokens. We have also tried some other methods, such as drop out the probably 'noisy' tokens and concatanate other tokens as filtered caption, but the results are not as ideal.\nAs in the Fig. 3, first we compute the confidence score of each token in the caption, and then we map the confidence to the tokens used in the pipeline,"}, {"title": "3.2.1. Tokenizer Mapping", "content": "Since the VLMs we use to calculate the confidence score and the T2I model do not use a unified tokenizer, a tokenizer mapping to align the different tokenizers is needed.\nRecent advancements in vision-language models (VLMs) and large language models (LLMs) have increas-ingly adopted distinct, non-unified tokenization schemes, necessitating robust handling of diverse tokenizers within multi-model frameworks. In response, we introduce a tokenizer mapping strategy designed to align tokenized representations across models.\nSuppose we have two distinct tokenizer, $T^{\\alpha}$ and $T^{\\beta}$, given a caption string $x = [x_0,\\ldots x_n]$, the tokens after the encoding are $c^{\\alpha} = T^{\\alpha}(x) = [c_0, c_1,\\ldots]$, and $c^{\\beta} = T^{\\beta}(x) = [c_0^{\\prime}, c_1^{\\prime},\\ldots]$ respectively. Then we construct a mapping $M_{\\alpha\\rightarrow\\beta}$ from $c^{\\alpha}$ to $c^{\\beta}$ based on their decoding string's overlap. We use $T^{-1}$ to denote decode function. For example, as in Fig. 4, if $T^{-1}(c_1^{\\alpha}) == T^{-1}(c_1^{\\prime})$, then $M_{\\alpha\\beta}(c_1^{\\alpha}) = c_1^{\\beta}$; if $T^{-1}(c_1^{\\alpha}) \\in T^{-1}([c_1^{\\prime},c_2^{\\prime},c_3^{\\prime}])$, then $M_{\\alpha\\beta}(c_1^{\\alpha}) = [c_1^{\\prime}, c_2^{\\prime}, c_3^{\\prime}]$.\nSuch tokenizer mapping enables the construction of a precise transformation function that aligns the output space of one encoding model to the output space of another. This facilitates seamless interoperability between models em-ploying different tokenizers by allowing consistent transla-tion of encoded representations across diverse tokenization"}, {"title": "3.2.2. Attention Map Reweighting", "content": "Observing that higher confidence scores tend to correlate with an increased likelihood of a token being associated with hallucinated content, we introduce an inversely pro-portional weighting coefficient to these scores. Without loss of generality, we employ the following equation as the reweighting mechanism.\n$w_k = \\begin{cases}\n-Softmax(s_k^{\\tau}) & \\text{if } s_k^{\\tau} > \\epsilon\\\\\n1 & \\text{otherwise}\n\\end{cases}$\nwhere $\\epsilon$ denotes the threshold. We use $s_k^{\\tau}$ to denote that the score here is calculated using Eq. (2) and under the model and tokenizer $\\alpha$.\nIn the T2I diffusion framework, usually there are SelfAt-tention layers and CrossAttention layers. In CrossAttention layer, we use the confidence score to reweight the attention matrix.\nHigh precision and low recall. To determine the appropri-ate threshold, it is essential that the threshold enables the retrieval of noisy tokens with high precision. Regarding re-call, as shown in Fig. 2, the hallucinated content exhibits substantial overlap within the lower range of token scores. Therefore, it is sufficient to prioritize high precision without demanding a similarly high recall rate in this span."}, {"title": "4. Experiment", "content": "In this section, we begin with a quantitative analysis of hal-lucination within the T2I training dataset. Then we intro-duce a carefully designed benchmark InstructBench, which aims to evaluate the model's instruction following ability. Then we report our method's metric on these datasets."}, {"title": "4.1. Training Data Construction", "content": "We combine 400K images from Conceputal Captions (CC) [43] and SBU Captions (SBU) [33] to construct our CCSBU dataset for training T2I models. Each image is paired with an visually relevant short description obtained during original data collection."}, {"title": "4.2. Caption Quality Evaluation", "content": "We evaluate the recaption quality using the following mod-els and evaluation metrics.\nCLIP The CLIP-Score is calculated using CLIP model 'openai/clip-vit-base-patch32' and the Long-CLIP-Score uses model 'BeichenZhang/LongCLIP-L' [53] since it can handle longer contexts. Both CLIP-Score and Long-CLIP-Score are averaged over random selected 10000 images.\nGPT-40 We adapt GPT-40 for a more fine-grained evalu-ation. First, we prompt GPT-40 for extracting visible ob-jects from generated detailed captions. For the extracted objects, we further prompt GPT-40 for three times to justify if each object exists in the given image. For each image-caption pair, we count the number of objects as $N$ and calculate the hallucination rate as $\\frac{\\sum_{i=1}^{N} H_i}{N}$, where $H_i$ ="}, {"title": "4.3. InstructBench", "content": "In prior sections, we observed that captions generated by Vision-Language Models (VLMs) frequently exhibit hallu-cinations and inaccuracies, particularly regarding specific attributes such as object color, spatial relationships among"}, {"title": "4.4. Can The Training Process Effectively Reduce Discrepancies Between Various Captions?", "content": "An inquiry arises regarding the impact of varying captions associated with the same image during the fine-tuning pro-cess: Does the influence of discrepancies between these captions amplify or diminish over time? Specifically, if the divergence between the models' outputs decreases throughout fine-tuning, it implies that the differences in captions are negligible and can be effectively disregarded. Conversely, if the models' predictive discrepancies inten-sify during fine-tuning, this suggests that subtle variations in the captions are not inherently reconciled by the fine-tuning process and may significantly affect the models' con-vergence.\nTo explore the aforementioned question, we employed a fixed random seed and fine-tuned the model using datasets with differing captions, resulting in two distinct model"}, {"title": "4.5. Results for Alleviating Caption Noise", "content": "Based on the VLM's confidence score, we propose an ap-proach in Sec. 3.2 to alleviate the caption noise. We evalu-ate the proposed method using both general metric (CLIP-Score and FID) and also on the InstructBench for better evaluating its instruction following ability.\nThe results are presented in Tab. 3 and supplementary Tab.5, visualization examples in Fig. 5. The term base-line (original, Share-Captioner, and LLaVA-v1.6), refers to models fine-tuned using the respective captions listed in Tab. 1. Detailed configurations for these models are pro-vided in the supplementary materials. The ablation 'Rm noisy tokens' refers to directly removing tokens with confi-dence scores exceeding a specific threshold, leaving a sub-set of tokens that are concatenated to form a refined caption. As shown in Tab. 4, we implement several baseline meth-ods for comparison with the proposed approach. The 'Rm Noisy Tokens' baseline involves removing tokens identi-fied as hallucinated and concatenating the remaining tokens to generate captions, with hyperparameter $\\sigma$=30%. The 'Mix-up' baseline employs the mix-up technique [54, 55] a widely used method for noise robustness, by blending the prediction embeddings and caption embeddings. Addition-ally, we perform ablation experiments with various hyper-parameter settings. the $\\sigma$=10% denotes a dynamically cal-culated threshold corresponding to the lowest 10% confi-dence score within a batch, as formulated in Eq. (5). This thresholding strategy eliminates the need for prior assump-tions about caption quality. From these results, we derive several insights."}, {"title": "4.6. Even Minor Hallucination in Caption Affects The Representation Quality", "content": "To better analyze the caption effect on the T2I generation, we can look into the features to analyze their representa-tion quality [25]. Especially, following the work [50, 52] which use linear probing to analyze the middle-layer repre-sentation ability, we adopt the same method in [50] to ana-lyze how the caption quality effect the representation qual-ity. Detailed configuration are in the supplementary. And the results are in Tab. 3 denoted by $Acc_i$ meaning accuracy of linear probing on CIFAR10."}, {"title": "4.7. Implementation details", "content": "We utilize Stable Diffusion v2.1 base as our backbone for all experiments. We train and inference on 8 NVIDIA A100-SXM4-40GB GPUs. Given the memory constraints, input images are resized to a resolution of 3 \u00d7 256 \u00d7 256. We em-ploy a batch size of 192 (split across devices as 24\u00d78) to op-timize GPU utilization while balancing memory efficiency. The learning rate is set at 3e 5, a common choice for fine-tuning in diffusion-based models to ensure stable con-vergence. For sampling, we adopt a classifier-free guidance approach with a guidance scale of 7.5 to enhance the fidelity of generated images relative to the input text prompts. The Euler sampler is utilized for efficient generation, with 20 timesteps per sample during training and 1000 timesteps per sample for testing to enable high-quality output at test time. A squared linear schedule is applied to control the noise level during sampling, which helps in maintaining smooth and gradual progression through the denoising steps. To prevent overfitting, we adopt a drop out rate as 0.1."}, {"title": "5. Limitation and Conclusion", "content": "In this study, we examine the impact of caption noise on text-to-image generation and identify the potential of Vision-Language Model confidence scores as a reliable in-dicator. We subsequently propose a methodology to lever-age these scores through token reweighting to mitigate noise effects. Experiments results demonstrate its effectiveness.\nLimitations. While numerous advanced techniques ex-ist for enhancing noise robustness in understanding tasks, their applicability and efficacy in generative tasks remain relatively underexplored. The approach proposed probably is not the most effective or straightforward method. But this paper and findings highlight the critical importance of addressing caption hallucination in T2I systems."}]}