{"title": "Saliency-Based diversity and fairness Metric and FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and Diversity", "authors": ["Teerath Kumar", "Alessandra Mileob", "Malika Bendechache"], "abstract": "Data augmentation has become a pivotal tool in enhancing the performance of computer vision tasks, with the KeepOriginalAugment method emerging as a standout technique for its intelligent incorporation of salient regions within less prominent areas, enabling augmentation in both regions. Despite its success in image classification, its potential in addressing biases remains unexplored. In this study, we introduce an extension of the KeepOriginalAugment method, termed FaceKeepOriginalAugment, which explores various debiasing aspects-geographical, gender, and stereotypical biases in computer vision models. By maintaining a delicate balance between data diversity and information preservation, our approach empowers models to exploit both diverse salient and non-salient regions, thereby fostering increased diversity and debiasing effects. We investigate multiple strategies for determining the placement of the salient region and swapping perspectives to decide which part undergoes augmentation. Leveraging the Image Similarity Score (ISS), we quantify dataset diversity across a range of datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset. We evaluate the effectiveness of FaceKeepOriginalAugment in mitigating gender bias across CEO, Engineer, Nurse, and School Teacher datasets, utilizing the Image-Image Association Score (IIAS) in convolutional neural networks (CNNs) and vision transformers (ViTs). Our findings shows the efficacy of FaceKeepOriginalAugment in promoting fairness and inclusivity within computer vision models, demonstrated by reduced gender bias and enhanced overall fairness. Additionally, we introduce a novel metric, Saliency-Based Diversity and Fairness Metric, which quantifies both diversity and fairness while handling data imbalance across various datasets.", "sections": [{"title": "1. Introduction", "content": "Deep learning has shown remarkable success across various domains, such as image processing [25, 40, 39, 37, 1, 19], audio analysis [34, 23, 6, 47, 46, 24], and numerous other fields [5, 2, 36, 15, 16, 20, 49, 43, 42, 22, 35, 18]. But bias has been in each domain. Computer vision models and their applications often exhibit various social biases, including gender bias [4, 3], geographical bias [29, 32], and racial bias [4, 13]. For instance, facial recognition systems tend to be less accurate for individuals with darker skin tones and for females [4]. When deploying a model trained on a dataset that doesn't represent the demographics of the patient population, biases can skew results. For instance, if a model trained on mammograms of white patients is used on non-white patients with higher breast density, it may show decreased sensitivity, leading to more missed or delayed diagnoses and potentially worse outcomes for non-White patients [45].\nThe root cause of these biases often lies in the datasets used to train the models, which propagate biases when deployed in real-time applications. Dataset compilation methods typically involve gathering images from the internet, leading to the creation of biased datasets [14]. The auditing of visual datasets for faces has primarily focused on race and gender [29, 4, 13], highlighting the importance of addressing biases in facial regions.\nTo mitigate these biases, several methods have been proposed. Zhang et al. [50] investigated debiasing in image classification tasks by generating adversarial examples to balance training data distribution. Kim et al. [17] introduced Biaswap, a method that debiases deep neural networks without prior knowledge of bias types, utilizing unsupervised sorting and style transfer techniques. Lee et al. [28] proposed DiverseBias, a feature-level data augmentation technique for improving the debiasing of image classification models by synthesizing diverse bias-conflicting samples through disentangled representation learning. Other related approaches including SalfMix [8], KeepAugment [12], and Randaugment [9], have also tackled feature fidelity challenges. However,"}, {"title": "2. Related work", "content": "Efforts have been made to address biases in computer vision models. Several studies have shown gender, racial and geographical biases in computer vision tasks. Buolamwini and Gebru [4] discovered flaws of facial recognition systems, mainly in detecting faces of dark-skinned women, which implies the importance of bias mitigation in face identification. Mitigation of these challenges has led to several proposals including the use of Fairface [13] which is a curated dataset as well as filtering and debiasing internet-based training datasets [3]. Moreover, data augmentation techniques are important for helping alleviate biases by endorsing dataset diversity while maintaining salient information. Information erasing augmentation techniques such as random erasing [53], cutout [11], grid mask [7], hide-and-seek [44] aim at making models learn erased features by removing given information from images. Nevertheless, these approaches may result into overfitting through noisy examples hence not providing enough diversity. However, there also exist information preserving augmentation methods such as SalfMix [8] as well as KeepAugment [12] have problems like overfitting and domain shift between non-salient and salient areas, respectively. Our method FaceKeepOriginalAugment builds on the state-of-the-art method KeepOriginalAugment [21], which solve the limitations of SalfMix and KeepAugment. Through intelligent inclusion of salient regions into non-salient ones with diverse positioning during augmentation, our approach prevents bias and enhances dataset variation without compromising the truthfulness of features. Although Kumar et al.'s previous work [21] demonstrated impressive efficacy in classification and related tasks, this study expands upon their approach by investigating its debiasing potential. In addition to examining the debiasing capabilities of the method, we undertake a thorough analysis of the hyperparameters linked to the data augmentation process in debiasing aspect. By integrating debiasing mechanisms into the existing framework and examining the influence of hyperparameters, our objective is to thoroughly explore the debiasing aspect of FaceKeepOriginalAugment through comprehensive experiments conducted on various datasets.\nRecent research highlights the critical intersection of diversity and fairness in machine learning, addressing the implications of biased datasets on model performance. Dablain et al. [10] emphasize the necessity of balancing class instances while mitigating protected feature bias through their Fair OverSampling (FOS) method, which utilizes SMOTE for class imbalance reduction. Zhao et al. [52] highlighted the importance of establishing clear definitions and rigorous evaluations of the diversity of datasets, focusing on the social constructs inherent in the dataset curation process. Mandal et al. [31] investigated gender bias in multimodal models such as CLIP, employing cosine similarity to measure stereotypical associations in outputs. Their findings reveal the limitations of traditional metrics in capturing the complexity of biases inherent in these models. Similarly, Kuncheva and Whitaker [26] explored diversity measures within classifier ensembles, illustrating the challenges of correlating diversity with ensemble precision. Together, these works underscore the need for innovative metrics that holistically address both diversity and fairness, advancing the discourse on ethical machine learning practices."}, {"title": "3. Methodology", "content": "In this section, we explain our method name FaceKeepOriginalAugment and new matric named Saliency-Based Diversity and Fairness Metric."}, {"title": "3.1. FaceKeepOriginalAugment", "content": "Our primary objective is to enhance diversity by preserving both the original and augmented information within a single image, thereby promoting fairness to encourage the model to learn diverse information. To achieve this, we begin by detecting the salient region in image 1. In our method, we utilize the saliency detection technique proposed by Montabone et al. [33] and Kumar et al. [21], which has demonstrated superior performance compared to other methods [48]. Our choice of this method is inspired by recent research [48], which thoroughly investigated different saliency detection methods for data augmentation. After finding the salient region, we discuss two important questions i) Where to place the salient region? ii) Which part should be augmented?\nBy identifying the important region, we aim to address the issue of redundant salient features present in SalfMix, as well as tackle the domain shift problem encountered in KeepAugment.\nTo determine the placement of the salient region within the non-salient region and achieve diversity, we explore three different strategies:\n\u2022 Where to place the salient region?\nMin-Area: We identify eight regions surrounding the salient region and select the one with the minimum area. The salient region is resized according to the size of that minimum area and placed within it, as shown in output of first row of Fig. 2.\nMax-Area: Conversely, we choose the region with the maximum area among the eight surrounding regions and resize the salient region accordingly, as shown in output of second row of Fig. 2.\nRandom-Area: We adopt a more flexible approach by randomly selecting one of the eight regions. The salient region is resized based on the size of the chosen region and placed within it as shown in output of third row of Fig. 2.\n\u2022 Which part should be augmented?\nAfter identifying the salient region, we propose and investigate three distinct strategies to enhance diversity:\nAugment only salient: We solely apply random augmentation to the salient region and then paste the augmented salient region into the non-salient region of the original image as shown in output of first row of Fig. 3.\nAugment non-salient only: We conduct augmentation on the entire image while preserving the original salient region. The augmented image is then combined with the original salient region, extracted from the unaltered original image as shown in output of second row of Fig. 3.\nAugment both: This strategy involves performing separate augmentations on both the salient region and the entire image. The augmented salient region is integrated with the augmented whole image as shown in output of third row of Fig. 3.\nWe observed that the augment both strategy demonstrates greater diversity across various computer vision tasks, detailed discussion is given in section 4.2.\nIt is important to note that we utilize randAug [9] for augmentation, offering computational efficiency similar to that employed by KeepAugment."}, {"title": "3.2. Saliency-Based Diversity and Fairness Metric", "content": "This work introduces a novel Saliency-Based Diversity and Fairness Metric designed to account for both within-group diversity and inter-group fairness. Within-group diversity measures the variation among samples within a single group, capturing the richness of diversity in each class. Inter-group fairness evaluates the differences between groups, ensuring equitable representation and mitigating bias. The proposed metric integrates both aspects, weighting them appropriately to handle imbalanced datasets while maintaining fairness and diversity across groups. Mandal et al.[29] used cosine similarity, which measures the angle between two vectors, focusing on the direction rather than the magnitude. It works well for comparing the similarity between two feature vectors in terms of orientation, but for data diversity, we generally need to capture more than just the direction.\nWe refer to the feature vectors as $X$. First, we perform saliency detection on the images and then pass these saliency image to the pretrained VGG16 model [41], created by the Visual Geometry Group at the University of Oxford, to obtain the feature vectors. Before calculating the diversity metrics, the feature vectors are normalized to ensure consistency and comparability across groups. The normalization of the feature vectors is performed as follows:\n$X' = \\frac{X}{\\|X\\|}$ (1)\nwhere $X'$ is the normalized feature vector and $\\|X\\|$ is its Euclidean norm. By normalizing the feature vectors, we ensure that the diversity metrics are scale-invariant and comparable across different groups. The normalized feature vectors are then used to calculate Euclidean distances. This ensures that both within-group and inter-group diversity measures are normalized and lie within a comparable range."}, {"title": "3.2.1. Within-Group Diversity", "content": "Let $X'_i$ represent the set of normalized saliency-based features for group $i$ (e.g., Male or Female). The within-group diversity for group $i$ is computed using the Euclidean distance between pairs of feature vectors within that group:\n$D_{within}(X'_i) = \\frac{1}{N_i(N_i - 1)} \\sum_{x_k \\in X'_i} \\sum_{x_l \\in X'_i} dist(X'_{i,k}, X'_{i,l})$ (2)\nwhere $dist(X'_{i,k}, X'_{i,l})$ is the Euclidean distance between two normalized feature vectors $X'_{i,k}$ and $X'_{i,l}$, and $N_i$ is the number of feature vectors in group $i$."}, {"title": "3.2.2. Inter-Group Diversity and Fairness", "content": "The inter-group diversity measures the average pairwise distances between feature vectors from different groups (e.g., between Male and Female groups). This aspect is crucial for ensuring that the metric captures inter-group fairness. Let $X_i$ and $X_j$ represent the feature sets of two different groups. The inter-group diversity is computed as:\n$D_{inter}(X_i, X_j) = \\frac{1}{N_i N_j} \\sum_{x_k \\in X_i} \\sum_{x_l \\in X_j} dist(X'_{i,k}, X'_{j,l})$ (3)\nwhere $N_i$ and $N_j$ are the number of feature vectors in groups $i$ and $j$, respectively. The Euclidean distance between normalized feature vectors ensures that the inter-group diversity reflects the actual distance between different groups."}, {"title": "3.2.3. Combined Metric: Fairness and Diversity", "content": "In scenarios where the dataset is imbalanced (i.e., the number of samples across groups varies significantly), it is important to ensure that larger groups do not dominate the overall diversity metric. To handle this, the metric weights the within-group and inter-group diversity terms by the size of each group, allowing the metric to reflect the actual contributions of both minority and majority groups.\nThe final metric, which combines both diversity within groups and diversity across groups, is given in equation. 4.\n$M_{fairness-diversity} = \\alpha \\cdot \\frac{1}{N} \\sum_{i=1}^{K} N_i \\cdot D_{within}(X_i) + \\beta \\cdot \\frac{1}{N(N-1)} \\sum_{i=1}^{K} \\sum_{j=i+1}^{K} N_i \\cdot N_j \\cdot D_{inter}(X_i, X_j)$ (4)\nwhere: - K is the total number of groups, - $N_i$ is the number of samples in group i, - $N = \\sum_{i=1}^{K} N_i$ is the total number of samples across all groups, - \u03b1 is the weight for within-group diversity (focusing on intra-group diversity), - \u03b2 is the weight for inter-group diversity (focusing on fairness across groups)."}, {"title": "4. Experiments", "content": "To assess data diversity in terms of geographical and stereotypical biases, we employ two variants of the Image Similarity Score (ISS): ISSIntra and ISSCross. ISSIntra quantifies data diversity within a dataset, while ISSCross evaluates diversity across different datasets, as introduced by Mandal et al. [29]. Range of both ISS metrics is 0 to 2. We use data diversity term as more data diversity lead to less bias [29]. We utilize five datasets, namely, Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset, maintaining the same experimental settings as detailed in [29].\nTo measure gender bias in both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), we employ the Image-Image Association Score (IIAS) [31, 30]. IIAS evaluates bias by comparing the similarity between gender attributes in images using cosine similarity and images representing specific concepts, as outlined in [31]. Our experimental setup encompasses four CNN models (VGG16, ResNet152, Inceptionv3, and Xception) and four ViT models (ViT B/16, B/32, L/16, and L/32). We adhere to the training settings outlined in [32], wherein for CNNs, layers are initially frozen and custom layers are trained for 50 epochs. For ViTs, layers are first frozen and trained for 100 epochs, followed by training all layers for 50 epochs with a low learning rate.\nA gender bias study, as detailed in [32], collected images from Google searches using job terms, resulting in two training sets: one with equal gender representation and another with biased representation. The test set remains balanced. The training dataset comprises 7,200 images (900 per category), while the test dataset consists of 1,200 images (300 per category, 150 per gender). Additionally, separate datasets for men and women were utilized for evaluation purposes. We integrate our proposed FaceKeepOriginalAugment approach into the transformations used in the experiments, addressing biases."}, {"title": "4.1. Experimental setup", "content": "To assess data diversity in terms of geographical and stereotypical biases, we employ two variants of the Image Similarity Score (ISS): $ISS_{Intra}$ and $ISS_{Cross}$. $ISS_{Intra}$ quantifies data diversity within a dataset, while $ISS_{Cross}$ evaluates diversity across different datasets, as introduced by Mandal et al. [29]. Range of both ISS metrics is 0 to 2. We use data diversity term as more data diversity lead to less bias [29]. We utilize five datasets, namely, Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset, maintaining the same experimental settings as detailed in [29].\nTo measure gender bias in both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), we employ the Image-Image Association Score (IIAS) [31, 30]. IIAS evaluates bias by comparing the similarity between gender attributes in images using cosine similarity and images representing specific concepts, as outlined in [31]. Our experimental setup encompasses four CNN models (VGG16, ResNet152, Inceptionv3, and Xception) and four ViT models (ViT B/16, B/32, L/16, and L/32). We adhere to the training settings outlined in [32], wherein for CNNs, layers are initially frozen and custom layers are trained for 50 epochs. For ViTs, layers are first frozen and trained for 100 epochs, followed by training all layers for 50 epochs with a low learning rate.\nA gender bias study, as detailed in [32], collected images from Google searches using job terms, resulting in two training sets: one with equal gender representation and another with biased representation. The test set remains balanced. The training dataset comprises 7,200 images (900 per category), while the test dataset consists of 1,200 images (300 per category, 150 per gender). Additionally, separate datasets for men and women were utilized for evaluation purposes. We integrate our proposed FaceKeepOriginalAugment approach into the transformations used in the experiments, addressing biases."}, {"title": "4.2. Hyperparamter", "content": "We conducted an extensive analysis of various hyperparameter combinations for five professional datasets-CEO, Engineer, Nurse, Politician, and School Teacher-using two distinct data diversity metrics: $ISS_{Intra}$ and $ISS_{Cross}$. Specifically, we investigated the combinations of augmentation strategies with area strategies. The $ISS_{Intra}$ and $ISS_{Cross}$ scores for CEO, Engineer, Nurse, Politician, and School Teacher are shown in Table 1. Our analysis revealed that the augmentation both strategy with the random area strategy yielded the optimal scores in both metrics, consistent with findings reported in [21]. The reasons could be, the selection of the random area strategy is attributed to its provision of scaling augmentation, while the augment both strategy enhances diversity, making it particularly effective for debiasing."}, {"title": "4.3. Results", "content": "To assess dataset diversity, we employ the Intra-dataset Image Similarity Score ($ISS_{Intra}$) across various datasets, showcasing substantial improvements compared to the baseline as shown in Table 2. To further anaylse, the Table 3 presents the $ISS_{Intra}$ for various queries across different language-location pairs. The baseline ISS values are compared with ours. The queries include CEO, Engineer, Nurse, Politician, and School Teacher, each with multiple language-location pairs. It's observed that the ISS values for most queries and language-location pairs have improved compared to the baseline values. This improvement indicates the effectiveness of the proposed method in enhancing image similarity in cross-cultural contexts. Notably, certain queries show more significant improvements, such as CEO and School Teacher in Arabic-West Asia & North Africa, Mandarin-East Asia, and Spanish-Latin America language locations as shown in Table 3. Furthermore, we investigate the Inter-dataset Image Similarity Score ($ISS_{Cross}$) and ISSIntra for five occupation datasets. Our approach consistently demonstrates enhanced diversity, with notable exceptions observed in the school teacher dataset, potentially influenced by underlying biases identified by Mandal et al. (2023) [32]. Specifically, our method outperforms the baseline across different datasets, notably achieving significant improvements for occupations such as \"Politician\" and \"Nurse\". While both approaches exhibit comparable performance for \"Politician\" in $ISS_{Cross}$, our method showcases superior results across diverse occupations, emphasizing its effectiveness in promoting dataset diversity as shown in Table 4. Moreover, our approach yields higher mean ISS scores across all queries, highlighting its efficacy in enhancing diversity. Overall, our approach presents promising advancements in dataset diversity assessment as shown in Table 4.\nTable 5 presents the average Image-Image Association Scores (IIAS) for Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) across various classes and masking scenarios. Positive IIAS values indicate biases towards men, while negative values signify biases towards women. Notably, in the biased dataset, biases towards men are observed for the CEO and Engineer classes, as evidenced by positive IIAS values, while biases towards women are observed for the Nurse and School Teacher classes, indicated by negative IIAS values. This nuanced analysis reveals the gender biases inherent in the dataset and underscores the importance of addressing these biases in computer vision models. FaceKeepOriginalAugment approach consistently demonstrates a remarkable reduction in gender bias compared to baseline models. Specifically, our method achieves reductions of approximately 5 to 15 times for CNNs and ViTs in masked scenarios as shown in figure 4, and 4 to 21 times in unmasked scenarios as shown in figure 5 across various occupation classes. These substantial reductions, highlighted in red, underscore the effectiveness of our approach in promoting fairness and inclusivity within computer vision models. Moreover, it's crucial to note the total absolute IIAS values, which reflect the overall magnitude of gender bias within the models. Our approach consistently yields lower total absolute IIAS values compared to baseline results taken from [30], indicating a substantial reduction in the overall magnitude of gender bias. This comprehensive view of bias reduction further reinforces the robustness and efficacy of FaceKeepOriginalAugment in mitigating gender bias within computer vision models. Our findings not only highlight the nuanced gender biases present across different occupation classes but also demonstrate the significant effectiveness of our approach in addressing these biases. By reducing gender bias and promoting fairness within computer vision models, our FaceKeepOriginalAugment approach contributes towards building more bias-free system."}, {"title": "4.4. Saliency-Based Diversity and Fairness Metric Evaluation", "content": "To evaluate the proposed Saliency-Based Diversity and Fairness Metric, we conducted experiments on six datasets commonly used for assessing gender diversity in images. These datasets include the Diverse Dataset [29], FFHQ [14], WIKI [38], IMDB [38], LFW [27], and UTK [51]. For the experiments, we utilized the full set of 81 images from the Diverse Dataset and randomly selected 100 images from each of the remaining five datasets. The images were manually classified into two groups: male and female. To address the inherent class imbalance in these datasets, we created balanced versions using an undersampling data augmentation technique, as illustrated in Fig. 6.\nIn addition to gender-based diversity, we extended our evaluation to measure diversity and fairness across nine geographically distinct language-location datasets [31, 29], which include: West Asia & North Africa, North America, Western Europe, South Asia, South East Asia, East Asia, Eastern Europe, Latin America, and Sub-Saharan Africa. For each of these regions, we manually organized the images into two gender groups: male and female.\nTo further generalize the applicability of the metric beyond gender, we assessed saliency-based diversity and fairness across five professions (CEO, Engineer, Nurse, Politician, and School Teacher) in the same nine language-location combinations as discussed in [31, 29]. Each profession was evaluated across these nine sub-categories to ensure a comprehensive understanding of diversity and fairness in various professional and cultural contexts.\nIn Table 6, the comparison of balanced and imbalanced datasets across the three key metrics within group diversity (Dwithin), between group diversity (Dinter), and the proposed fairness-diversity metric ($M_{fairness-diversity}$) highlights the effect of data augmentation strategies. Overall, FaceKeepOriginalAugment demonstrates a slight improvement in Dwithin and Dinter across most datasets when compared to the baseline. For instance, in the IMDB dataset, FaceKeepOriginalAugment increases Dwithin from 0.80 to 0.81 and maintains Dinter at 0.35 for balanced datasets. Similarly, in the WIKI dataset, FaceKeepOriginalAugment slightly improves Mfairness-diversity from 0.52 to 0.53 in imbalanced conditions, suggesting improved fairness when dealing with underrepresented groups. The results for FFHQ and LFW datasets remain stable with minimal fluctuations, showcasing that the augmentation method preserves diversity and fairness without significant deviations from the baseline metrics. The notable consistency in Dwithin across diverse datasets implies that KeepOriginalAugment maintains intra-group diversity effectively, a strength of the approach. Meanwhile, Mfairness-diversity is relatively stable across both balanced and imbalanced datasets, indicating robustness to bias. FaceKeepOriginalAugment marginally improves fairness while preserving diversity, offering a balanced solution that enhances representation, especially in datasets where class imbalance poses a challenge.\nIn Table 7, we present the diversity and fairness metrics, within group diversity (Dwithin), between-group diversity (Dinter), and the fairness-diversity metric (Mfairness-diversity) for various language-location pair queries both in their baseline state and with the FaceKeepOriginalAugment. The baseline metrics exhibit consistent values across different pairs, with Dwithin ranging from 0.59 to 0.60 and Mfairness-diversity values hovering around 0.53 to 0.55, raising fairness concerns in these datasets. With FaceKeepOriginalAugment, there is a noticeable enhancement in the metrics. For instance, the Dwithin values improve to 0.79 for most pairs, showcasing a substantial increase in intra-group diversity, while Mfairness-diversity also sees an uplift to around 0.57 across the board. Notably, pairs such as Arabic-West Asia & North Africa and English - North America show an increase in Dwithin from 0.59 to 0.79 and Mfairness-diversity from 0.54 to 0.57, respectively. This improvement signifies that the augmentation strategy enhances both diversity and fairness in these pairs, addressing potential biases that might be present in the baseline datasets. The results illustrate that FaceKeepOriginalAugment effectively enriches the diversity while maintaining fairness across gender in various language-location pairs. This highlights the method's strength in fostering a more equitable representation in datasets that may initially show limited diversity. The insights drawn from these metrics suggest that employing such augmentation strategies is crucial in developing models that prioritize fairness and diversity, particularly in linguistically diverse contexts.\nIn Table 8, we present the diversity and fairness metrics within group diversity for various professions both in their baseline state and with FaceKeepOriginalAugment. The baseline results indicate a high level of within-group diversity for each profession, with Dwithin values consistently around 0.82 to 0.83. This suggests that the profession datasets are already fairly diverse in terms of representation. The Mfairness-diversity values for the baseline remain stable at approximately 0.72, indicating a reasonable balance between fairness and diversity across these professions. with FaceKeepOriginalAugment, there is an observable enhancement in the diversity metrics. The Dwithin values increase to between 0.85 and 0.86, reflecting an improvement in intra-group diversity. Similarly, the Mfairness-diversity remains consistent at 0.74 for all professions, demonstrating that the augmentation method maintains fairness while enhancing diversity. Notably, the Dinter values show a slight increase in some cases, such as the CEO and Nurse professions, where the values are maintained around 0.61 to 0.63 after augmentation. This suggests that the method does not negatively impact the between-group diversity while improving within-group diversity. Overall, the results indicate that FaceKeepOriginalAugment effectively increases the diversity of profession datasets while preserving fairness across language location pairs. The enhancements in the metrics highlight the importance of using augmentation strategies to ensure a more equitable representation in datasets used for model training, particularly in contexts where professions may be underrepresented or biased."}, {"title": "5. Conclusion", "content": "We present FaceKeepOriginalAugment, an innovative approach designed to mitigate geographical, gender, and stereotypical biases prevalent in computer vision models. Our method adeptly integrates salient regions within non-salient areas, allowing for flexible augmentation across both regions. By striking a balance between data diversity and preservation, FaceKeepOriginalAugment not only enhances dataset diversity but also effectively reduces biases. We conducted rigorous experiments on a range of diverse datasets, including FFHQ, WIKI, IMDB, LFW, UTK Faces, and a custom Diverse Dataset, employing the Intra-Set Similarity (ISS) metric to assess dataset diversity quantitatively. Additionally, we evaluated the effectiveness of FaceKeepOriginalAugment in addressing gender bias across various professions, such as CEO, Engineer, Nurse, and School Teacher, utilizing the Image-Image Association Score (IIAS) metric for a comprehensive assessment. Our results demonstrate that FaceKeepOriginalAugment significantly reduces gender bias in both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).\nMoreover, we propose a novel saliency-based diversity and fairness metric that allows for a nuanced evaluation of gender, language-location, and profession biases across diverse datasets. By comparing outcomes with and without the application of FaceKeepOriginalAugment, our findings reveal a substantial enhancement in fairness and inclusivity, representing a pivotal advancement in the ongoing efforts to address biases within computer vision applications."}]}