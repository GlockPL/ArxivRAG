{"title": "MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces", "authors": ["Wayne Wu", "Honglin He", "Yiran Wang", "Chenda Duan", "Jack He", "Zhizheng Liu", "Quanyi Li", "Bolei Zhou"], "abstract": "Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while diverse robot dogs and humanoids have recently emerged in the street. Ensuring the generalizability and safety of these forthcoming mobile machines is crucial when navigating through the bustling streets in urban spaces. In this work, we present MetaUrban, a compositional simulation platform for Embodied AI research in urban spaces. MetaUrban can construct an infinite number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for embodied AI research and establish various baselines of Reinforcement Learning and Imitation Learning. Experiments demonstrate that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide more research opportunities and foster safe and trustworthy embodied AI in urban spaces.", "sections": [{"title": "1 Introduction", "content": "Public urban spaces vary widely in type, form, and size, encompassing streetscapes, plazas, and parks. They are crucial spaces for transit and transport, as well as providing opportunities to stage various social events. Since the early 20th century, the study of public urban spaces has long been a cornerstone of urban sociology and planning. For example, William H. Whyte, in his seminal work, \"City - Rediscovering the Center\", revealed that the complexity and vibrant interaction in public spaces profoundly determine humans' social life, underscoring the critical role these environments play in urban safety and vitality.\nRecent development of Robotics and Embodied AI makes the urban space no longer exclusive to humans. Various mobile machines have started emerging. For example, elders and physically disabled people maneuver electronic wheelchairs on the street, while food delivery bots navigate on the sidewalk to accomplish the last-mile food delivery task. Various mobile legged robots like robot dog Spot from Boston Dynamics and humanoid robot Optimus from Tesla are also forthcoming. We can thus imagine a future of public urban spaces that will be shared and co-habitated by humans and mobile machines driven by Embodied AI. Ensuring the generalizability and safety of these mobile machines becomes essential.\nSimulation platforms have played a crucial role in enabling systematic and scalable training of the embodied AI agents and the safety evaluation before real-world deployment. However, most of the existing simulators focus either on indoor household environments or outdoor driving environments. For example, platforms like AI2-THOR, Habitat, and iGibson are designed for household assistant robots in which the environments are mainly apartments or houses with furniture and appliances; platforms like SUMO, CARLA, and MetaDrive are designed for research on autonomous driving and transportation. Yet, simulating urban spaces with diverse layouts and objects, complex dynamics of pedestrians, is much less explored.\nDistinct from the indoor household and driving environments, the urban space has unique characteristics. Let's follow the adventure of a last-mile delivery bot, who aims to deliver a lunch order from a nearby pizzeria to the campus. First, it faces a long-horizon journey across several street blocks at a one-mile distance, with multifarious road hazards, such as fragmented curbs and rugged ground caused by tree roots on sidewalks. Then, it must safely navigate the cluttered street full of obstacles like trash bins, parked scooters, and potted plants. In addition, it needs to handle pedestrians and crowds properly to avoid collisions. It should also take special care around disabled people in wheelchairs. Thus, the layout diversity, object distribution, and dynamic complexity bring unique challenges to the design of simulation environments and the study of the generalizability and safety of Embodied AI agents operating in urban spaces.\nWe present MetaUrban a compositional simulation platform for Embodied AI research in urban spaces. First, we introduce Hierarchical Layout Generation, a procedural generation approach that can generate infinite layouts hierarchically from street blocks to sidewalks, functional zones, and object locations. It can generate scenes at an arbitrary scale with various connections and divisions of street blocks, object locations, and terrains, which are critical for improving the generalizability of trained agents. Then, we design the Scalable Object Retrieval, an automatic pipeline that can obtain an arbitrary number of high-quality objects with real-world distribution. We first compute the object category distribution from broad real-world data to form a description pool. Then, with the sampled descriptions from the pool, we effectively retrieve objects from large-scale 3D asset repositories with a VLM-based open-vocabulary searching schema. Finally, we propose the Cohabitant Populating method to generate complex dynamics in urban spaces. We first tailor recent 3D human and motion datasets to get 1,100 rigged pedestrian models, each with 2,314 movements. Then, to form safety-critical scenarios, we integrate Vulnerable Road Users (VRUs) like bikers, skateboarders, and scooter riders. To broaden the category of mobile machines in urban scenes, we include delivery bots, electric wheelchairs, robot dogs, and humanoid robots. Then, based on path planning algorithms, we can get complex trajectories among hundreds of environmental agents simultaneously with collision and deadlock avoidance. It is critical for enhancing the social conformity and safety of the mobile agents.\nBased on MetaUrban, we construct a large-scale dataset, MetaUrban-12K, that includes 12,800 training scenes and 1,000 test scenes. We further create an unseen test set with 100 manually designed scenes to evaluate trained models' generalizability. Besides, we provide 30,000 steps of high-quality"}, {"title": "2 Related Work", "content": "Many simulation platforms have been developed for Embodied AI research, depending on the target environments \u2013 such as indoor homes and offices, driving freeways and roadways, and crowds in warehouses and squares. We compare representative ones with the proposed MetaUrban simulator.\nIndoor Environments. Platforms for indoor environments are mainly designed for household assistant robots, emphasizing the affordance, realism, and diversity of objects, as well as the inter-activity of environments. VirtualHome pivots towards simulating routine human activities at home. AI2-THOR and its extensions, such as ManipulaTHOR, RoboTHOR, and ProcTHOR, focus on detailed agent-object interactions, dynamic object state changes, and procedural scene generation, alongside robust physics simulations. Habitat offers environments reconstructed from 3D scans of real-world interiors. Its subsequent iterations, Habitat 2.0 and Habitat 3.0, introduce interactable objects and deformable humanoid agents, respectively. iGibson provides photorealistic environments. Its upgrades, Gibson 2.0, and OmniGibson, focus on household tasks with object state changes and a realistic physics simulation of everyday activities, respectively. ThreeDWorld targets real-world physics by integrating high-fidelity simulations of liquids and deformable objects. However, unlike MetaUrban, these simulators are focused on indoor environments with particular tasks like object rearrangement and manipulation.\nDriving Environments. Platforms for driving environments are mainly designed for autonomous vehicle research and development. Simulators like GTA V, Sim4CV, AIRSIM, CARLA, and its extension SUMMIT offer realistic environments that mimic the physical world's detailed visuals, weather conditions, and day-to-night transitions. Other simulators enhance efficiency and extensibility at the expense of visual realism, such as Udacity, DeepDrive, Highway-env, and DriverGym. MetaDrive trades off between visual quality and efficiency, offering a lightweight driving simulator that can support the research of generalizable RL algorithms for vehicles. Although some of the simulators involve traffic participants other than vehicles, such as pedestrians and cyclists, all of them focus on vehicle-centric driving scenarios and neglect environments and things happening in public urban spaces like sidewalks and plazas.\nSocial Navigation Environments. Other than indoor and driving environments, social navigation platforms emphasize the social compatibility of robots. Simulators like Crowd-Nav, Gym-Collision-Avoidance, and Social-Gym 2.0, model scenes and agents in 2D maps, focusing more on the development of path planning algorithms. Other simulators, such as HuNavSim, SEAN 2.0, and SocNavBench, upgrade the environment to 3D space and introduce human pedestrians to support the development of more complex algorithms. However, social navigation platforms focus on crowd navigation, with oversimplified objects and surrounding environmental structures in the scenes. This proposed platform addresses the gap between the existing social naviga- tion platforms and the real-world urban spaces regarding environmental diversity and complexity.\nWe will compare MetaUrban with other simulators below, through the scale, sensor, and feature dimensions. For the scale, MetaUrban can generate infinite scenes with a procedural generation pipeline. It also provides the largest number of humans (1,100) and movements (2,314) among all simulation environments. For the sensor, MetaUrban provides RGBD, semantic, and lidar, while acoustic is our next step to better support multi-model tasks. For the feature, different from other simulators, MetaUrban provides real-world distribution of the object's categories and uses a more sophisticated path plan algorithm to get natural human trajectories. It also provides flexible user interfaces - mouse, keyboard and joystick and racing wheel, which vastly ease the collection of human expert demonstration data. MetaUrban uses PyBullet as its physical engine, which is open-source"}, {"title": "3 MetaUrban Simulator", "content": "MetaUrban is a compositional simulation platform that can generate infinite training and evaluation environments for Embodied AI in urban spaces. MetaUrban uses a structured description script to create urban scenes. Based on the provided information about street blocks, sidewalks, objects, agents, and more, it starts with the street block map, then plans the ground layout by dividing different function zones, then places static objects, and finally populates dynamic agents.\nThis section highlights three key designs in the MetaUrban simulator to support exhibiting three unique characteristics of urban spaces \u2013 diverse layouts, particular object distribution, and complex dynamics. Section 3.1 introduces Hierarchical Layout Generation, which can infinitely generate diverse layouts with different functional zone divisions and object locations that are critical for the generalizability of agents. Section 3.2 introduces Scalable Object Retrieval, which harnesses worldwide urban scene data to obtain real-world object distributions in different places, and then builds large-scale, high-quality static objects set with VLM-enabled open-vocabulary searching. It is useful for training agents specialized for urban scenes. Section 3.3 introduces Cohabitant Populating, in which we leverage the advancements in digital humans to enrich the appearances, movements, and trajectories of pedestrians and vulnerable road users, as well as incorporate other agents to form a vivid cohabiting environment. It is critical for improving the social conformity and safety of the mobile agents."}, {"title": "3.1 Hierarchical Layout Generation", "content": "The diversity of scene layout, i.e., the connection and categories of blocks, the specifications of side- walks and crosswalks, as well as the placement of objects, is crucial for enhancing the generalizability of trained agents maneuvering in public spaces. In the hierarchical layout generation framework, we start by sampling the categories of street blocks and dividing sidewalks and crosswalks and then allocate various objects, with which we can get infinite urban scene layouts with arbitrary sizes and specifications of maps.\nGround plan. We design 5 typical street block categories, i.e., straight, intersection, roundabout, circle, and T-junction. In the simulator, to form a large map with several blocks, we can sample the"}, {"title": "3.2 Scalable Object Retrieval", "content": "Hierarchical layout generation decides the scene's layout and where to place the objects. However, to make the trained agents generalizable when navigating through scenes composed of various objects in the real world, what objects to place is another crucial question. In this section, we propose the Scalable Assets Retrieval pipeline, in which we first get real-world object distributions from web data, and then retrieve objects from 3D asset repositories through an open-vocabulary search schema based on VLMs. This pipeline is flexible and extensible: the retrieved objects can be scaled to arbitrary sizes as we continue to exploit more web data for scene descriptions and include more 3D assets as the candidate objects.\nReal-world object distribution extraction. Urban spaces have unique structures and object dis- tributions, such as the infrastructure built by the urban planning administration and clutters placed by people. Thus, we design a real-world distribution extraction method to get a description pool"}, {"title": "3.3 Cohabitant Populating", "content": "In this section, we will describe how to populate these static urban scenes with varied agents regarding appearances, movements, and trajectories through Cohabitant Populating.\nFollowing BEDLAM and AGORA, we represent humans as parametric human model SMPL-X, in which the 3D human body is controlled by a set of parameters for pose \\(\\theta\\), shape \\(\\beta\\), and facial expression \\(\\phi\\), respectively. Then, built upon SynBody's asset repository, 1,100 3D rigged human models are procedually generated by sampling from 68 garments, 32 hairs, 13 beards, 46 accessories, and 1,038 cloth and skin textures. To form safety-critical scenarios, we also include vulnerable road users like bikers, skateboarders, and scooter riders. For the other agents, we incorporate the 3D assets of COCO Robotics and Starship's delivery robots, Drive Medical's electric wheelchair, Boston Dynamic's robot dog, and Agility Robotics' humanoid robot.\nWe provide two kinds of human movements in the simulator \u2013 daily movements and unique move- ments. Daily movements provide the basic human dynamics in daily life, i.e., idle, walking, and running. Unique movements are the complicated dynamics that appear randomly in public spaces,"}, {"title": "4 MetaUrban-12K Dataset", "content": "Dataset Construction. Based on the MetaUrban simulator, we construct the MetaUrban-12K dataset, including 12,800 interactive urban scenes for training (MetaUrban-train) and 1,000 scenes for testing (MetaUrban-test). For the train and test sets, we sample randomly from the 6 templates (a-f) of sidewalks shown in Figure 3 (right) with the same distributions of objects and dynamics. We further construct an unseen test set (MetaUrban-unseen) with 100 scenes for zero-shot experiments, in which we sample from the unseen template (g) \u2013 Wide Commercial Sidewalk, unseen objects, trajectories of agents with further designers' manual adjustments according to real-world scenes. In addition, to enable the fine-tuning experiments, we construct a training set of 1,000 scenes with the same distribution of MetaUrban-unseen, termed MetaUrban-finetune. 12K scenes can be generated in 12 hours on a local workstation. Notably, our MetaUrban platform can easily extend the scale of urban scenes from a multi-block level to a whole city level. To enable the Offline RL and IL training, we collect expert demonstration data from a well-trained RL agent and human operators, forming 30,000 steps of high-quality demonstration data. The success rate of the demonstration data is 60%, which can be taken as a reference for the experiments of Offline RL and IL.\nStatistics. Scenes in this dataset are connected by one to three street blocks covering average 20,000m\u00b2 areas. There are average 0.03 static objects per m\u00b2 and 10 dynamic agents per street block. The average distance between each two objects is 0.7m. As shown in the distribution of object numbers, there are lots of objects in each scenario with a minimal value of 300. As shown in the distribution of objects' areas, objects in the dataset occupy large areas, which complies with a normal distribution centered at 5,250m\u00b2. As shown in the distribution of episode length, the average episode is 410m and more than 20% of them are more than 800 steps \u2013 90m. From these distributions, we can observe that scenes are significantly challenging in MetaUrban-12K for agents to navigate through, which are crowded and with long horizons."}, {"title": "5 Experiments", "content": "Experimental settings. Tasks. We design two common tasks in urban scenes: Point Navigation (PointNav) and Social Navigation (SocialNav). In PointNav, the agent's goal is to navigate to the target coordinates in static environments without access to a pre-built environment map. In SocialNav, the agent is required to reach a point goal in dynamic environments that contain moving environmental"}, {"title": "5.1 Benchmarks", "content": "We build two benchmarks on the MetaUrban-12K dataset for PointNav and SocialNav tasks. We train 7 typical baselines on the MetaUrban-train dataset and then evaluate them on the MetaUrban-test set. We use the demonstration data provided in MetaUrban-12K for offline RL and IL training. We further make zero-shot evaluations on the MetaUrban-unseen set to demonstrate the generalizability of models trained on the MetaUrban-12K dataset while directly tested on unseen environments.\nTable 1 shows the results in the PointNav and SocialNav benchmarks. From the results, we can draw 4 key observations. 1) The tasks are far from being solved. The highest success rates are only 66% and 36% for PointNav and SocialNav tasks achieved by the baselines, indicating the difficulty of these tasks in the urban environments composed by MetaUrban. Note that these benchmarks are built on a medium level of object and dynamic density; increasing the density will further degrade the performances shown in ablation studies. 2) Models trained on MetaUrban-12K have strong generalizability in unseen environments. With zero-shot testing, models can still achieve 41% and 26% success rates on average for PointNav and SocialNav tasks. These results are strong since the models generalize to not only unseen objects and layouts but also unseen dynamics of environmental agents. It demonstrates that the compositional nature of MetaUrban, supporting the coverage of a large spectrum of complex urban scenes, can successfully empower generalization ability to the trained models. 3) SocialNav is much harder than PointNav due to the dynamics of the mobile environmental agents. On average, the success rate decreases by 15% from PointNav to SocialNav, indicating that dynamic agents, such as pedestrians, vulnerable road users, and other agents common in urban scenes, present significant challenges to the trained agent. 4) Safe RL remarkably improves the safety property at the expense of effectiveness. Among all tasks and settings, the safe RL models achieve the best performance in the Cumulative Cost, indicating that these models are successful at avoiding collision with pedestrians and objects. However, the success rate and SPL or SNS decrease accordingly, indicating future efforts to balance the safety and effectiveness of agents in complex urban scenes."}, {"title": "5.2 Ablation Study", "content": "In this section, we evaluate the generalizability, scaling ability, and effects of the density of static objects and dynamic agents. For unified evaluations, we use PPO for all ablation studies. Except for the results on dynamic density, we use the PointNav task. Observations and hyperparameters remain the same for model training across different evaluations.\nEvaluation of generalizability. To evaluate the generalizable ability of agents trained on data generated by MetaUrban, we compare the success rate of four settings in Figure 7 (a). Setting- 1 and Setting-2 are the results of training on MetaUrban-train while testing on MetaUrban-test and MetaUrban-unseen, respectively. We can observe a performance drop on MetaUrban-unseen. However, the zero-shot results still achieve 49% success rate facing various out-of-distribution scenes, demonstrating the strong generalizability of models trained on large-scale data created by MetaUrban. Setting-3 and Setting-4 are the results of direct training on MetaUrban-finetune, and fine-tuning on MetaUrban-finetune from the pre-trained model on MetaUrban-train. Compared between Setting-2 and Setting-3, we can observe an obvious performance drop, which is caused by an underfitting of the insufficient and complex fine-tuning data. Setting-4 outperforms Setting-3 by a large margin, demonstrating that the model trained on the MetaUrban-12K dataset can provide informative priors as good initializations for quick tuning.\nEvaluation of scaling ability. To evaluate the scaling ability of MetaUrban's compositional archi- tecture, we train models on a different number of generated scenes, from 5 to 1,000. As shown in Figure 7 (b), the performance improves remarkably from 12% to 46%, as we include more scenes for training, demonstrating the strong scaling ability of MetaUrban. MetaUrban's compositional nature has the potential to extend more diverse scenes with a larger element repository in the future, which could further boost the agent's performance.\nEvaluation of static and dynamic density. To evaluate the influence of static object density and dynamic environmental agents, we evaluate the different proportions of them on the PointNav and SocialNav tasks, respectively, from 1% to 100%. Note that we keep the number of training scenes unchanged when sampling different densities. As shown in Figure 7 (c) and (d), with the increasing density of both static objects and dynamic agents, the success rates of both train and test experience dramatic degradations, demonstrating the challenges for embodied agents when facing crowded streets in urban scenes. In our experiments, we observe many interesting failure cases that can indicate promising future directions to improve AI's performance in MetaUrban and, ultimately, in real-world urban scenes."}, {"title": "6 Conclusion", "content": "We propose a new compositional simulator, MetaUrban, to facilitate embodied AI and robotics research in urban scenes. MetaUrban can generate infinite urban environments with complex scene structures and diverse movements of pedestrians and other mobile agents. These environments used as training data can significantly improve the generalizability and safety of the embodied AI underlying different mobile machines from food delivery bots to humanoids. We commit ourselves to developing the open-source simulator and fostering the community effort to turn it into a sustainable community infrastructure."}, {"title": "A MetaUrban Visualization", "content": "A.1 Static Scene Samples\nStreet blocks. We design five typical street block categories \u2013 straight, curve, intersection, T-junction, and roundabout. In the simulator, to form a large map with several blocks, we can sample the category, number, order, lane number, and other related parameters of the blocks. We use the algorithm Block Incremental Generation (BIG) proposed in MetaDrive to generate the target road network defined by users. Figure 8 provides demonstrations of generated street maps composed of different numbers of blocks.\nGround layouts. We construct seven typical templates for sidewalks, more details about the design and the generation process are given in the Section B.1.\nAs shown in Figure 9, different types of sidewalks can be sampled on the same street block; each type has its unique division and specification of functional zones. Figure 10 further shows several block maps with a different type of sidewalks."}, {"title": "A.2 Dynamic Scene Samples", "content": "Dynamic agents such as pedestrians, vulnerable road users like bikers (skateboarders, scooter riders), mobile machines (delivery bots, electric wheelchairs, robot dogs, and humanoid robots), and vehicles will be present in the environment. The density of dynamic agents can be controlled with dynamic density ratio \\(p_d\\). Figure 13 shows ego-view results by randomly sampling viewpoints on block maps. The urban spaces are well populated with different agents."}, {"title": "A.3 Static Asset Samples", "content": "We provide 10,000 high-quality static object assets. The roadside infrastructure is divided into three categories: 1) Standard infrastructure, including poles, trees, and signs, is placed at regular intervals along the road. 2) Non-standard infrastructure, such as buildings, bonsai, and trash bins, is placed randomly within designated zones. 3) Clutter, such as drink cans, bags, and bicycles, is scattered randomly across all functional zones. Figure 14 15 and 16 show examples of these three categories respectively."}, {"title": "A.4 Dynamic Asset Samples", "content": "Human assets. MetaUrban provides 1,100 rigged 3D human models, sampled from 68 garments, 32 hairs, 13 beards, 46 accessories, and 1,038 cloth and skin textures from SynBody dataset. Figure 17 shows randomly sampled humans, which have large variations.\nVulnerable road user assets. MetaUrban provides 5 kinds of vulnerable road users to form safe- critical scenarios. They are bikers, skateboarders, scooter riders, and electric wheelchair users, as shown in the first row of Figure 18. Note that electric wheelchairs, as a human-AI shared control system, can also be seen as mobile machines, not only vulnerable road users.\nMobile machine assets. MetaUrban provides 6 kinds of mobile machines: Starship, Yandex Rover, and COCO Robotics' delivery bots, Boston Dynamic's robot dog, Agility Robotics' humanoid robot, and Drive Medical's electric wheelchair. Figure 19 shows the first 5 assets, while the electric wheelchair, as a cross-category asset (vulnerable road user and mobile machine), is shown in Figure 18.\nVehicle assets. MetaUrban provides 37 kinds of vehicles, covering different body types, sizes, and appearances. Figure 20 shows 10 sampled vehicles."}, {"title": "B MetaUrban Simulator", "content": "B.1 Layout Generation\nThis section gives details about the process we developed to procedurally generate scenes with sidewalks and crosswalks, as well as sample and place static objects on the sidewalk.\nGround plan. As shown in Figure 21 (Top), we define 4 functional zones and 6 geometric zones for sampling the type of sidewalks and choosing the distribution of parameters for each sidewalk component. As shown in Figure 21 (Bottom), we construct 7 typical templates for sidewalks; each type of them has its unique distribution of geometric zones. To match the distribution with the real"}, {"title": "B.2 Object Retrieval", "content": "Distribution extraction. Distinguished from the recent indoor simulation platform, there are no ready-to-use high-quality asset datasets for urban spaces. Urban spaces have their unique data"}, {"title": "B.3 Cohabitant Populating", "content": "Appearances. We include 1,100 3D human models, 5 kinds of vulnerable road users bikers, skateboarders, scooter riders, and electric wheelchair users, and 6 kinds of mobile machines as cohabitants in the MetaUrban simulator. The number of dynamic agents in a scene can be set by the parameters respectively. The environment initialization time and RAM usage are only proportional to the number of individual agents. For example, 100 same agents will take the same initialization time and RAM usage as one. This schema can be used to significantly increase the maximum number of spawned agents for a specific hardware.\nMovements. We include 3 daily movements \u2013 idle, walking, and running, as well as 2,311 unique movements from the BEDLAM dataset. All of the motion sequences are trimmed and checked by designers one by one to ensure their quality. With the same skeletal binding, all of the unique movements can be transferred to all of the 3D human models directly. Thus, we can get 1,100 \u00d7 2,311 numbers of human-motion pairs.\nTrajectories. We harness ORCA and Push and Rotate (P&R) algorithm to get the trajectories of all dynamic agents. First, we build the 0-1 mask that indicates whether the grid is a walkable region or not. Then, we sample the start and ending points for each agent randomly, followed by generating their 2D trajectories by using the model of ORCA and P&R. The trajectory plan process is efficient, running within 5s for 100 agents on a Core i9 CPU processor. Vehicles will also be added in dynamic scenes. All traffic vehicles will follow IDM policies, as MetaDrive does."}, {"title": "B.4 Scene Customization", "content": "MetaUrban supplies various compositional elements, such as street blocks, objects, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. With just a few simple lines of specification, it is easy to create customized urban spaces of interest, such as street corners, plazas, and parks."}, {"title": "B.5 User Interface", "content": "MetaUrban provides user interfaces for two purposes: 1) Demonstration data collection for Offline RL and IL. 2) Object labeling and scene customization. For demonstration data collection, MetaUrban provides interfaces for mouse, keyboard, joystick, and racing wheel. We can easily collect human"}, {"title": "B.6 Simulator Comparison", "content": "We will compare MetaUrban with other simulators below in Table 2, using the scale, sensor, and feature dimensions. For the scale, MetaUrban can generate infinite scenes with a procedural generation pipeline. It provides the largest number of humans (1,100) and movements (2,314) among all simulation environments. For objects, so far, we have provided 10,000. Compared to other simulators, all of the objects from MetaUrban are urban-specific. Also, we provide an interface to extend object data to any size easily with recent advances in 3D content generation (Section B.2). For the sensor, MetaUrban provides RGBD, semantic, and lidar. For the feature, different from other simulators, MetaUrban provides real-world distribution of the object's categories and uses a more sophisticated path plan algorithm to get the natural agent's trajectories. It also provides flexible user interfaces mouse, keyboard, joystick, and racing wheel, which vastly ease the collection of human expert demonstration data. MetaUrban uses PyBullet as its physical engine and Panda3D for rendering."}, {"title": "C Experiment Details", "content": "This section discusses the settings of environments, action spaces, observation spaces, evaluation metrics, training details for methods, as well as the reward and cost in the benchmarks of Point Navigation (PointNav) and Social Navigation (SocialNav), respectively."}, {"title": "C.1 PointNav Experiments", "content": "Environments. For PointNav experiments, there are only static objects besides the ego agent in the environment. To evaluate the trained policy, we split seven types of sidewalks into six types for training and validation with one for test. The one used for the test is the Wide Commercial Sidewalk, in which the frontage zone buffer will be, as well as some unseen objects."}, {"title": "C.2 SocialNav Experiments", "content": "Environments. For SocialNav experiments, most settings are the same as the ones in PointNav. The most important difference is that dynamic agents will also be present in the environment. The trajectories of environmental agents are generated together by using the model of ORCA with P&R. Since vehicles are inherited from MetaDrive, we use the same parameter to control its density, i.e., traffic density 0.05 in our experiments.\nEvaluation metrics. For SocialNav, an episode is considered successful if the agent issues the DONE action, defined as completing 95% of the set route within 1,000 maximum steps. The agent is evaluated using the Success Rate (SR) and Social Navigation Score (SNS) , which is the average of Success weighted by Time Length (STL) and Personal Space Compliance (PSC). SNS measures the agent in terms of safety and efficiency.\nMethods. We benchmark the same methods as in PointNav experiments with the same hyperpa- rameters. However, due to the involvement of lots of dynamic agents, the training speed of SocialNav is about approximately 1/3 of PointNav on online methods. The cost scheme is defined as raising a cost of +1 at each time step if the ego agent crashes with any agents, vehicles, or objects."}, {"title": "D Datasheet", "content": "Motivation\nFor what purpose was the dataset created? The dataset was created to enable agents training on diverse scenes and facilitate Embodied AI research in urban spaces.\nWho created and funded the dataset? This work was created and funded by the MetaUrban team at the University of California, Los Angeles.\nComposition\nWhat do the instances that comprise the dataset represent? Each instance is a JSON file including the configuration of our MetaUrban environment and a specific seed.\nHow many instances are there in total (of each type, if appropriate)? There are 12,800 urban scenes released in the MetaUrban- 12K dataset, along with the code to sample substantially more.\nDoes the dataset contain all possi- ble instances or is it a sample (not necessarily random) of instances from a larger set? We offer 12,800 urban scenes, with the ability to generate more using procedural generation scripts.\nWhat data does each instance con- sist of? Each scene is specified as a JSON file including the configu- ration of our MetaUrban environment and a specific seed.\nIs there a label or target associated with each instance? No.\nIs any information missing from in- dividual instances? No.\nAre relationships between individ- ual instances made explicit (e.g., users' movie ratings, social net- work links)? Each urban scene is created independently, so there are no connections between the scenes.\nAre there recommended data splits? Yes. See Section 4 in the main paper.\nAre there any errors, sources of noise, or redundancies in the dataset? No.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? The dataset is self-contained.\nDoes the dataset contain data that might be considered confidential? No.\nDoes the dataset contain data that, if viewed directly, might be of- fensive, insulting, threatening, or might otherwise cause anxiety? No.\nCollection Process\nHow was the data associated with each instance acquired? Each scene was procedurally generated.\nIf the dataset is a sample from a larger set, what was the sampling strategy? The dataset consists of 12,800 scenes, each by sampling the parameters of its composed elements.\nWho was involved in the data col- lection process? The authors were the sole individuals responsible for creating the dataset.\nOver what timeframe was the data collected? Data was collected in May 2024.\nWere any ethical review processes conducted? No.\nWas any preprocess- ing/cleaning/labeling of the data done? We label each object's location area and pivots to make them spawn in target functional zones and face a natural direction.\nWe use VLMs to automatically label 2D images of cities worldwide, which enables the extraction of real-world cate- gory distribution of objects in urban spaces.\nWas the \"raw\" data saved in addition to the prepro- cessed/cleaned/labeled data? There is no raw data.\nIs the software that was used to pre- process/clean/label the data avail- able? The code related to preprocessing, cleaning, and labeling the data will be made available.\nUses\nHas the dataset been used for any tasks already? Yes. See Section 4 of the main paper.\nWhat (other) tasks could the dataset be used for? The scenes can be used in a wide variety of tasks in embodied AI, computer vision, and urban planning.\nIs there anything about the com- position of the dataset or the way it was collected and prepro- cessed/cleaned/labeled that might impact future uses? No.\nAre there tasks for which the dataset should not be used? Our dataset can be used for both commercial and non- commercial purposes.\nDistribution\nWill the dataset be distributed to third parties outside of the entity on behalf of which the dataset was created? Yes. We plan to make the entirety of the work open-source, including the code used to generate scenes and train agents, the scripts to get the MetaUrban-12K dataset, and the asset repositories.\nHow will the dataset be distributed? The scene files will be distributed with a custom Python package.\nThe code, asset, and repositories will be distributed on GitHub.\nWill the dataset be distributed un- der a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? The scene dataset, 3D asset repository, and code will be released under the Apache 2.0 license.\nHave any third parties imposed IP- based or other restrictions on the data associated with the instances? For 3D human assets, we use Synbody . Its license is CC BY-NC-SA 4."}]}