{"title": "MetaUrban: A Simulation Platform for Embodied AI in Urban Spaces", "authors": ["Wayne Wu", "Honglin He", "Yiran Wang", "Chenda Duan", "Jack He", "Zhizheng Liu", "Quanyi Li", "Bolei Zhou"], "abstract": "Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while diverse robot dogs and humanoids have recently emerged in the street. Ensuring the generalizability and safety of these forthcoming mobile machines is crucial when navigating through the bustling streets in urban spaces. In this work, we present MetaUrban, a compositional simulation platform for Embodied AI research in urban spaces. MetaUrban can construct an infinite number of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents' appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for embodied AI research and establish various baselines of Reinforcement Learning and Imitation Learning. Experiments demonstrate that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide more research opportunities and foster safe and trustworthy embodied AI in urban spaces.", "sections": [{"title": "1 Introduction", "content": "Public urban spaces vary widely in type, form, and size, encompassing streetscapes, plazas, and parks. They are crucial spaces for transit and transport, as well as providing opportunities to stage various social events. Since the early 20th century, the study of public urban spaces has long been a cornerstone of urban sociology and planning. For example, William H. Whyte, in his seminal work, \"City - Rediscovering the Center\", revealed that the complexity and vibrant interaction in public spaces profoundly determine humans' social life, underscoring the critical role these environments play in urban safety and vitality.\nRecent development of Robotics and Embodied AI makes the urban space no longer exclusive to humans. Various mobile machines have started emerging. For example, elders and physically disabled people maneuver electronic wheelchairs on the street, while food delivery bots navigate on the sidewalk to accomplish the last-mile food delivery task. Various mobile legged robots like robot dog Spot from Boston Dynamics and humanoid robot Optimus from Tesla are also forthcoming. We can thus imagine a future of public urban spaces that will be shared and co-habitated by humans and mobile machines driven by Embodied AI. Ensuring the generalizability and safety of these mobile machines becomes essential.\nSimulation platforms have played a crucial role in enabling systematic and scalable training of the embodied AI agents and the safety evaluation before real-world deployment. However, most of the existing simulators focus either on indoor household environments or outdoor driving environments. For example, platforms like AI2-THOR, Habitat, and iGibson are designed for household assistant robots in which the environments are mainly apartments or houses with furniture and appliances; platforms like SUMO, CARLA, and MetaDrive are designed for research on autonomous driving and transportation. Yet, simulating urban spaces with diverse layouts and objects, complex dynamics of pedestrians, is much less explored.\nDistinct from the indoor household and driving environments, the urban space has unique characteristics. Let's follow the adventure of a last-mile delivery bot, who aims to deliver a lunch order from a nearby pizzeria to the campus. First, it faces a long-horizon journey across several street blocks at a one-mile distance, with multifarious road hazards, such as fragmented curbs and rugged ground caused by tree roots on sidewalks. Then, it must safely navigate the cluttered street full of obstacles like trash bins, parked scooters, and potted plants. In addition, it needs to handle pedestrians and crowds properly to avoid collisions. It should also take special care around disabled people in wheelchairs. Thus, the layout diversity, object distribution, and dynamic complexity bring unique challenges to the design of simulation environments and the study of the generalizability and safety of Embodied AI agents operating in urban spaces.\nWe present MetaUrban a compositional simulation platform for Embodied AI research in urban spaces. First, we introduce Hierarchical Layout Generation, a procedural generation approach that can generate infinite layouts hierarchically from street blocks to sidewalks, functional zones, and object locations. It can generate scenes at an arbitrary scale with various connections and divisions of street blocks, object locations, and terrains, which are critical for improving the generalizability of trained agents. Then, we design the Scalable Object Retrieval, an automatic pipeline that can obtain an arbitrary number of high-quality objects with real-world distribution. We first compute the object category distribution from broad real-world data to form a description pool. Then, with the sampled descriptions from the pool, we effectively retrieve objects from large-scale 3D asset repositories with a VLM-based open-vocabulary searching schema. Finally, we propose the Cohabitant Populating method to generate complex dynamics in urban spaces. We first tailor recent 3D human and motion datasets to get 1,100 rigged pedestrian models, each with 2,314 movements. Then, to form safety-critical scenarios, we integrate Vulnerable Road Users (VRUs) like bikers, skateboarders, and scooter riders. To broaden the category of mobile machines in urban scenes, we include delivery bots, electric wheelchairs, robot dogs, and humanoid robots. Then, based on path planning algorithms, we can get complex trajectories among hundreds of environmental agents simultaneously with collision and deadlock avoidance. It is critical for enhancing the social conformity and safety of the mobile agents.\nBased on MetaUrban, we construct a large-scale dataset, MetaUrban-12K, that includes 12,800 training scenes and 1,000 test scenes. We further create an unseen test set with 100 manually designed scenes to evaluate trained models' generalizability. Besides, we provide 30,000 steps of high-quality"}, {"title": "2 Related Work", "content": "Many simulation platforms have been developed for Embodied AI research, depending on the target environments \u2013 such as indoor homes and offices, driving freeways and roadways, and crowds in warehouses and squares. We compare representative ones with the proposed MetaUrban simulator.\nIndoor Environments. Platforms for indoor environments are mainly designed for household assistant robots, emphasizing the affordance, realism, and diversity of objects, as well as the inter-activity of environments. VirtualHome pivots towards simulating routine human activities at home. AI2-THOR and its extensions, such as ManipulaTHOR, RoboTHOR, and ProcTHOR, focus on detailed agent-object interactions, dynamic object state changes, and procedural scene generation, alongside robust physics simulations. Habitat offers environments reconstructed from 3D scans of real-world interiors. Its subsequent iterations, Habitat 2.0 and Habitat 3.0, introduce interactable objects and deformable humanoid agents, respectively. iGibson provides photorealistic environments. Its upgrades, Gibson 2.0, and OmniGibson, focus on household tasks with object state changes and a realistic physics simulation of everyday activities, respectively. ThreeDWorld targets real-world physics by integrating high-fidelity simulations of liquids and deformable objects. However, unlike MetaUrban, these simulators are focused on indoor environments with particular tasks like object rearrangement and manipulation.\nDriving Environments. Platforms for driving environments are mainly designed for autonomous vehicle research and development. Simulators like GTA V, Sim4CV, AIRSIM, CARLA, and its extension SUMMIT offer realistic environments that mimic the physical world's detailed visuals, weather conditions, and day-to-night transitions. Other simulators enhance efficiency and extensibility at the expense of visual realism, such as Udacity, DeepDrive, Highway-env, and DriverGym. MetaDrive trades off between visual quality and efficiency, offering a lightweight driving simulator that can support the research of generalizable RL algorithms for vehicles. Although some of the simulators involve traffic participants other than vehicles, such as pedestrians and cyclists, all of them focus on vehicle-centric driving scenarios and neglect environments and things happening in public urban spaces like sidewalks and plazas.\nSocial Navigation Environments. Other than indoor and driving environments, social navigation platforms emphasize the social compatibility of robots. Simulators like Crowd-Nav, Gym-Collision-Avoidance, and Social-Gym 2.0, model scenes and agents in 2D maps, focusing more on the development of path planning algorithms. Other simulators, such as HuNavSim, SEAN 2.0, and SocNavBench, upgrade the environment to 3D space and introduce human pedestrians to support the development of more complex algorithms. However, social navigation platforms focus on crowd navigation, with oversimplified objects and surrounding environmental structures in the scenes. This proposed platform addresses the gap between the existing social navigation platforms and the real-world urban spaces regarding environmental diversity and complexity.\nWe will compare MetaUrban with other simulators below, through the scale, sensor, and feature dimensions. For the scale, MetaUrban can generate infinite scenes with a procedural generation pipeline. It also provides the largest number of humans (1,100) and movements (2,314) among all simulation environments. For the sensor, MetaUrban provides RGBD, semantic, and lidar, while acoustic is our next step to better support multi-model tasks. For the feature, different from other simulators, MetaUrban provides real-world distribution of the object's categories and uses a more sophisticated path plan algorithm to get natural human trajectories. It also provides flexible user interfaces - mouse, keyboard and joystick and racing wheel, which vastly ease the collection of human expert demonstration data. MetaUrban uses PyBullet as its physical engine, which is open-source"}, {"title": "3 MetaUrban Simulator", "content": "and highly accurate in physics simulation, providing a cost-effective and flexible solution for future developments. MetaUrban uses Panda3D for rendering, which is a lightweight, open-source game engine with seamless Python integration, providing a flexible and accessible development environment. A detailed comparison table is included in the Appendix.\nIn summary, none of the recent simulation platforms have been constructed for urban spaces, and the proposed simulator differs from them significantly in terms of diverse layouts, objects, human dynamics, and different types of mobile agents like delivery robots, electric wheelchairs, robot dogs, humanoid robots, and vehicles, and their intricate interactions. We believe MetaUrban can provide a lot of new research opportunities for Embodied AI in urban settings.\nMetaUrban is a compositional simulation platform that can generate infinite training and evaluation environments for Embodied AI in urban spaces. Figure 2 depicts the procedural generation pipeline. MetaUrban uses a structured description script to create urban scenes. Based on the provided information about street blocks, sidewalks, objects, agents, and more, it starts with the street block map, then plans the ground layout by dividing different function zones, then places static objects, and finally populates dynamic agents.\nThis section highlights three key designs in the MetaUrban simulator to support exhibiting three unique characteristics of urban spaces \u2013 diverse layouts, particular object distribution, and complex dynamics. Section 3.1 introduces Hierarchical Layout Generation, which can infinitely generate diverse layouts with different functional zone divisions and object locations that are critical for the generalizability of agents. Section 3.2 introduces Scalable Object Retrieval, which harnesses worldwide urban scene data to obtain real-world object distributions in different places, and then builds large-scale, high-quality static objects set with VLM-enabled open-vocabulary searching. It is useful for training agents specialized for urban scenes. Section 3.3 introduces Cohabitant Populating, in which we leverage the advancements in digital humans to enrich the appearances, movements, and trajectories of pedestrians and vulnerable road users, as well as incorporate other agents to form a vivid cohabiting environment. It is critical for improving the social conformity and safety of the mobile agents."}, {"title": "3.1 Hierarchical Layout Generation", "content": "The diversity of scene layout, i.e., the connection and categories of blocks, the specifications of sidewalks and crosswalks, as well as the placement of objects, is crucial for enhancing the generalizability of trained agents maneuvering in public spaces. In the hierarchical layout generation framework, we start by sampling the categories of street blocks and dividing sidewalks and crosswalks and then allocate various objects, with which we can get infinite urban scene layouts with arbitrary sizes and specifications of maps.\nGround plan. We design 5 typical street block categories, i.e., straight, intersection, roundabout, circle, and T-junction. In the simulator, to form a large map with several blocks, we can sample the"}, {"title": "3.2 Scalable Object Retrieval", "content": "Hierarchical layout generation decides the scene's layout and where to place the objects. However, to make the trained agents generalizable when navigating through scenes composed of various objects in the real world, what objects to place is another crucial question. In this section, we propose the Scalable Assets Retrieval pipeline, in which we first get real-world object distributions from web data, and then retrieve objects from 3D asset repositories through an open-vocabulary search schema based on VLMs. This pipeline is flexible and extensible: the retrieved objects can be scaled to arbitrary sizes as we continue to exploit more web data for scene descriptions and include more 3D assets as the candidate objects.\nReal-world object distribution extraction. Urban spaces have unique structures and object distributions, such as the infrastructure built by the urban planning administration and clutters placed by people. Thus, we design a real-world distribution extraction method to get a description pool"}, {"title": "3.3 Cohabitant Populating", "content": "In this section, we will describe how to populate these static urban scenes with varied agents regarding appearances, movements, and trajectories through Cohabitant Populating.\nFollowing BEDLAM and AGORA, we represent humans as parametric human model SMPL-X, in which the 3D human body is controlled by a set of parameters for pose 0, shape B, and facial expression 4, respectively. Then, built upon SynBody's asset repository, 1,100 3D rigged human models are procedually generated by sampling from 68 garments, 32 hairs, 13 beards, 46 accessories, and 1,038 cloth and skin textures. To form safety-critical scenarios, we also include vulnerable road users like bikers, skateboarders, and scooter riders. For the other agents, we incorporate the 3D assets of COCO Robotics and Starship's delivery robots, Drive Medical's electric wheelchair, Boston Dynamic's robot dog, and Agility Robotics' humanoid robot.\nWe provide two kinds of human movements in the simulator \u2013 daily movements and unique movements. Daily movements provide the basic human dynamics in daily life, i.e., idle, walking, and running. Unique movements are the complicated dynamics that appear randomly in public spaces,"}, {"title": "4 MetaUrban-12K Dataset", "content": "Dataset Construction. Based on the MetaUrban simulator, we construct the MetaUrban-12K dataset, including 12,800 interactive urban scenes for training (MetaUrban-train) and 1,000 scenes for testing (MetaUrban-test). For the train and test sets, we sample randomly from the 6 templates (a-f) of sidewalks shown in Figure 3 (right) with the same distributions of objects and dynamics. We further construct an unseen test set (MetaUrban-unseen) with 100 scenes for zero-shot experiments, in which we sample from the unseen template (g) \u2013 Wide Commercial Sidewalk, unseen objects, trajectories of agents with further designers' manual adjustments according to real-world scenes. In addition, to enable the fine-tuning experiments, we construct a training set of 1,000 scenes with the same distribution of MetaUrban-unseen, termed MetaUrban-finetune. 12K scenes can be generated in 12 hours on a local workstation. Notably, our MetaUrban platform can easily extend the scale of urban scenes from a multi-block level to a whole city level. To enable the Offline RL and IL training, we collect expert demonstration data from a well-trained RL agent and human operators, forming 30,000 steps of high-quality demonstration data. The success rate of the demonstration data is 60%, which can be taken as a reference for the experiments of Offline RL and IL. The Appendix provides detailed visualizations of the MetaUrban-12K dataset.\nStatistics. Scenes in this dataset are connected by one to three street blocks covering average 20,000m\u00b2 areas. There are average 0.03 static objects per m\u00b2 and 10 dynamic agents per street block. The average distance between each two objects is 0.7m. Figure 6 shows distributions of the number of objects (left), areas of objects occupying (middle), and episode length (right). As shown in the distribution of object numbers, there are lots of objects in each scenario with a minimal value of 300. As shown in the distribution of objects' areas, objects in the dataset occupy large areas, which complies with a normal distribution centered at 5,250m\u00b2. As shown in the distribution of episode length, the average episode is 410m and more than 20% of them are more than 800 steps \u2013 90m. From these distributions, we can observe that scenes are significantly challenging in MetaUrban-12K for agents to navigate through, which are crowded and with long horizons."}, {"title": "5 Experiments", "content": "Experimental settings. Tasks. We design two common tasks in urban scenes: Point Navigation (PointNav) and Social Navigation (SocialNav). In PointNav, the agent's goal is to navigate to the target coordinates in static environments without access to a pre-built environment map. In SocialNav, the agent is required to reach a point goal in dynamic environments that contain moving environmental"}, {"title": "5.1 Benchmarks", "content": "We build two benchmarks on the MetaUrban-12K dataset for PointNav and SocialNav tasks. We train 7 typical baselines on the MetaUrban-train dataset and then evaluate them on the MetaUrban-test set. We use the demonstration data provided in MetaUrban-12K for offline RL and IL training. We further make zero-shot evaluations on the MetaUrban-unseen set to demonstrate the generalizability of models trained on the MetaUrban-12K dataset while directly tested on unseen environments.\nTable 1 shows the results in the PointNav and SocialNav benchmarks. From the results, we can draw 4 key observations. 1) The tasks are far from being solved. The highest success rates are only 66% and 36% for PointNav and SocialNav tasks achieved by the baselines, indicating the difficulty of these tasks in the urban environments composed by MetaUrban. Note that these benchmarks are built on a medium level of object and dynamic density; increasing the density will further degrade the performances shown in ablation studies. 2) Models trained on MetaUrban-12K have strong generalizability in unseen environments. With zero-shot testing, models can still achieve 41% and 26% success rates on average for PointNav and SocialNav tasks. These results are strong since the models generalize to not only unseen objects and layouts but also unseen dynamics of environmental agents. It demonstrates that the compositional nature of MetaUrban, supporting the coverage of a large spectrum of complex urban scenes, can successfully empower generalization ability to the trained models. 3) SocialNav is much harder than PointNav due to the dynamics of the mobile environmental agents. On average, the success rate decreases by 15% from PointNav to SocialNav, indicating that dynamic agents, such as pedestrians, vulnerable road users, and other agents common in urban scenes, present significant challenges to the trained agent. 4) Safe RL remarkably improves the safety property at the expense of effectiveness. Among all tasks and settings, the safe RL models achieve the best performance in the Cumulative Cost, indicating that these models are successful at avoiding collision with pedestrians and objects. However, the success rate and SPL or SNS decrease accordingly, indicating future efforts to balance the safety and effectiveness of agents in complex urban scenes."}, {"title": "5.2 Ablation Study", "content": "In this section, we evaluate the generalizability, scaling ability, and effects of the density of static objects and dynamic agents. For unified evaluations, we use PPO for all ablation studies. Except for the results on dynamic density, we use the PointNav task. Observations and hyperparameters remain the same for model training across different evaluations.\nEvaluation of generalizability. To evaluate the generalizable ability of agents trained on data generated by MetaUrban, we compare the success rate of four settings in Figure 7 (a). Setting-1 and Setting-2 are the results of training on MetaUrban-train while testing on MetaUrban-test and MetaUrban-unseen, respectively. We can observe a performance drop on MetaUrban-unseen. However, the zero-shot results still achieve 49% success rate facing various out-of-distribution scenes, demonstrating the strong generalizability of models trained on large-scale data created by MetaUrban. Setting-3 and Setting-4 are the results of direct training on MetaUrban-finetune, and fine-tuning on MetaUrban-finetune from the pre-trained model on MetaUrban-train. Compared between Setting-2 and Setting-3, we can observe an obvious performance drop, which is caused by an underfitting of the insufficient and complex fine-tuning data. Setting-4 outperforms Setting-3 by a large margin, demonstrating that the model trained on the MetaUrban-12K dataset can provide informative priors as good initializations for quick tuning.\nEvaluation of scaling ability. To evaluate the scaling ability of MetaUrban's compositional archi-tecture, we train models on a different number of generated scenes, from 5 to 1,000. As shown in Figure 7 (b), the performance improves remarkably from 12% to 46%, as we include more scenes for training, demonstrating the strong scaling ability of MetaUrban. MetaUrban's compositional nature has the potential to extend more diverse scenes with a larger element repository in the future, which could further boost the agent's performance.\nEvaluation of static and dynamic density. To evaluate the influence of static object density and dynamic environmental agents, we evaluate the different proportions of them on the PointNav and SocialNav tasks, respectively, from 1% to 100%. Note that we keep the number of training scenes unchanged when sampling different densities. As shown in Figure 7 (c) and (d), with the increasing density of both static objects and dynamic agents, the success rates of both train and test experience dramatic degradations, demonstrating the challenges for embodied agents when facing crowded streets in urban scenes. In our experiments, we observe many interesting failure cases that can indicate promising future directions to improve AI's performance in MetaUrban and, ultimately, in real-world urban scenes. We make a detailed discussion in the Appendix."}, {"title": "6 Conclusion", "content": "We propose a new compositional simulator, MetaUrban, to facilitate embodied AI and robotics research in urban scenes. MetaUrban can generate infinite urban environments with complex scene structures and diverse movements of pedestrians and other mobile agents. These environments used as training data can significantly improve the generalizability and safety of the embodied AI underlying different mobile machines from food delivery bots to humanoids. We commit ourselves to developing the open-source simulator and fostering the community effort to turn it into a sustainable community infrastructure."}]}