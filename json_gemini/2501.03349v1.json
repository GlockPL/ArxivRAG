{"title": "FTA-FTL: A Fine-Tuned Aggregation Federated Transfer Learning Scheme for Lithology Microscopic Image Classification", "authors": ["Keyvan RahimiZadeh", "Ahmad Taheri", "Jan Baumbach", "Esmael Makarian", "Abbas Dehghani", "Bahman Ravaei", "Bahman Javadi", "Amin Beheshti"], "abstract": "Lithology discrimination is a crucial activity in characterizing oil reservoirs, and processing lithology microscopic images is an essential technique for investigating fossils and minerals and geological assessment of shale oil exploration. In this way, Deep Learning (DL) technique is a powerful approach for building robust classifier models. However, there is still a considerable challenge to collect and produce a large dataset. Transfer-learning and data augmentation techniques have emerged as popular approaches to tackle this problem. Furthermore, due to different reasons, especially data privacy, individuals, organizations, and industry companies often are not willing to share their sensitive data and information. Federated Learning (FL) has emerged to train a highly accurate central model across multiple decentralized edge servers without transferring sensitive data, preserving sensitive data, and enhancing security. This study involves two phases; the first phase is to conduct Lithology microscopic image classification on a small dataset using transfer learning. In doing so, various pre-trained DL model architectures are comprehensively compared for the classification task. In the second phase, we formulated the classification task to a Federated Transfer Learning (FTL) scheme and proposed a Fine-Tuned Aggregation strategy for Federated Learning (FTA-FTL). In order to perform a comprehensive experimental study, several metrics such as accuracy, f1 score, precision, specificity, sensitivity (recall), and confusion matrix are taken into account. The results are in excellent agreement and confirm the efficiency of the proposed scheme, and show that the proposed FTA-FTL algorithm is capable enough to achieve approximately the same results obtained by the centralized implementation for Lithology microscopic images classification task.", "sections": [{"title": "1- Introduction", "content": "Reservoir characterization is one of the most crucial tasks in oil and gas studies. In addition, it is an important aspect of determining lithology because it significantly contributes to provide well planning from exploration to enhanced oil recovery (EOR) stag (He et al., 2022). For example, porosity and permeability are critical parameters in a hydrocarbon system and deeply depend on lithology. Therefore, knowing the facies can be advantageous to forecasting the volume of hydrocarbon in place and lessening uncertainty over reservoir characterization (Liu et al., 2022). Additionally, lithology is an integral part of geotechnical and rock physics models and is crucial for drilling and production operations (Johari and Emami Niri, 2021). A lack of correct diagnosis can face petroleum engineers and geologists serious problems such as sand production, collapse, and eruption because each lithology presents specific mechanical and geochemical features that must be recognized separately (Cai et al., 2021). In the EOR and carbon capture and storage (sequestration) (CCS) processes, lithology is one of the primary forces and the parameter that is investigated, firstly. The former is performed to increase production in oil and gas reservoirs in the last half of their life, and the latter is a new technology for storing carbon dioxide (Co2) in the underground reservoirs to reduce carbon footprint in the environment (Tan et al., 2022). A part from petroleum geosciences, in other fields related to earth science such as mining exploration, geodesy, and civil engineering, lithology is a focal factor that must be identified quickly and accurately (Zhang et al., 2017).\nThere are various methods for Lithology discrimination, including direct and indirect approaches. Laboratory-based core analysis is the most common and reliable method for directly identifying different lithologies. In these methods, by using a rotary core catcher, core samples are taken from the side of drilled wells, maintained intact, and transferred to the surface for manual examination, such as thin-sections. A thin layer of taken cores under geological optical microscopes, minerals, and fossils are studied in these sections (Xu et al., 2022).\nMoreover, the lithology of different formations in laboratories is recognized by modern technology facilities such as scanning by electron microscope (SEM), CT scan, and Electron Probe Micro-Analyzer (EPMA) with a focus on minerals of rock and their constituent elements (Izadi et al., 2017). These methods demand a great deal of time and money, so thin sections are more used in laboratories as a cost-effective and reliable method for lithology identification (Xu et al., 2022). Extensive research has long been performed by alternative methods such as geophysical log analysis (well-logging), seismic data, rock physics, numerical modeling, and especially geostatistical methods (Al-Mudhafar, 2017; Johari and Emami Niri, 2021; McCreery and Al-Mudhafar, 2017; Wang et al., 2022). Performing myriad projects by petroleum geologists and engineers, these methods have some issues. For instance, establishing a relationship between lithology and seismic data would be difficult (Ren et al., 2022). Moreover, well-logging operation is challenging or maybe impossible in all situations (e.g., horizontal wells), and well-logging devices may be error-prone in some situations (e.g., well casing) with undesirable effects on the results, (Liu et al., 2022).\nWith extensive improvements in Machine Learning (ML), researchers have shown a great inclination to employ various ML techniques for they are fast, cost-effective, and versatile. Above all, these methods are capable of dealing with big data analysis (Hussain et al., 2022). A brief review of the recent related studies on lithology identification is presented in Table 1. Since these studies did not pay special attention to predicting lithology using images from cores, it is needed to conduct research on lithology types determination based on images captured from cores. Nowadays, many ML techniques are utilized to create robust image classifier models. Among all of them, DL methods have shown a significant performance; however, there are several challenges to training a robust DL-based classifier model, such as computational resource limitations, time constraints, and especially the most crucial limitation is the lack of enough data samples (Olivas et al., 2009). It is useful to employ transfer learning as an efficient and practical approach to surmount the challenges as mentioned earlier. Transfer learning refers to the learning enhancement in a new task by using the knowledge transferred from an already learned related task (Bozinovski, 2020). In addition, due to different reasons, especially data privacy, individuals, organizations, and industry companies often are not willing to share their sensitive data and information. Therefore, it is crucial to employ techniques to preserve sensitive data and enhance security issues. Federated Learning (FL) has emerged as a paradigm to train a global model with decentralized datasets in a federated manner. FL can train a highly accurate central model across multiple decentralized edge servers without transferring sensitive data.\nIn this study, firstly, we apply a central deep learning model architecture using transfer learning to classify/identify different lithologies from thin sections and microscopic studies in a carbonate petroleum reservoir. We perform comprehensive experiments on the state-of-the-art DL model architectures such as DensNet169 (Huang et al., 2017), Xception (Chollet, 2017), ResNet101 (He et al., 2015), VGG19 (Simonyan and Zisserman, 2014), and Inception-ResNet v2 (Szegedy et al., 2016) as the base models and XGBoost Tree (Chen and Guestrin, 2016) and FC model as the classifiers to determine the suitable DL model architecture for the lithologies image classification task. Furthermore, the lithologies microscopic images dataset is developed in the current study. In this way, we apply data augmentation techniques to increase the number of samples and overcome the imbalance challenge of the dataset. Our proposed dataset would be publicly available for further research. In the second phase, to provide"}, {"title": "2- Dataset Generation and Pre-Processing", "content": "In this study, a mini dataset of lithology microscopic images is generated for both multi-class and binary classification task. In the following, the process of producing the mini dataset is described in detail and step by step."}, {"title": "2-1- Lithology Microscopic Images description", "content": "Core studies provide an array of useful information on petrophysical features of the study area. The hydrocarbon formation studied comprises mainly oil and water without gas. The study of thin sections shows that there are three main lithologies, including Argillaceous Limestone, Limestone, and Dolomite, as shown in Fig. 1. Argillaceous Limestone is a type of sedimentary rock that includes a significant amount (but less than 50%) of clay comprising kaolinite, montmorillonite, illite, and chlorite. Limestone is another sedimentary rock that is largely comprised of calcium carbonate (CaCO3), typically in the form of calcite or aragonite. Dolomite which widely recognized as Dolostone, is a sedimentary carbonate rock rich in CaMg (CO3)2. The lithology column effectively shows the intervals"}, {"title": "2-2- Data augmentation", "content": "Generally, the performance of most ML models, DL in particular, depends on the quality, quantity, and relevancy of training data samples to tackle the overfitting challenge and improve the model generalization (Goodfellow et al., 2017). However, labeled data for real-world applications may be limited in practice because collecting data might be time-consuming and costly in many project cases. In these cases, data augmentation techniques have become an important part of successful DL application on image data to enhance the diversity and sufficiency of training data set efficiently (Goodfellow et al., 2017; Yang et al., 2022). Fig. 2 and Table 2, show statistical information and data distribution of our dataset.\nData augmentation enhances the robustness of ML models by employing a wide range of techniques to increase the amount of data by artificially generating new data samples that the model might see in the real world based on the existing samples. It includes making small changes to data or using DL models to generate new data points."}, {"title": "2-3- Train and Test sets", "content": "As augmented datasets can significantly decrease the generalization of an ML model, it is essential to conduct controlled experiments to evaluate the performance of the model trained with such datasets (Goodfellow et al., 2017). Therefore, in order to split a dataset into train and test sets and perform controlled experiments, we select image samples for the test set, manually. In this way, approximately about 20 percent of image samples (440 samples) are selected as the test set, and 80 percent of image samples (1687 samples) are selected for the train set.\nThe dataset* developed in this study will be made publicly available for further research."}, {"title": "3. Preliminaries and Method", "content": "This section describes the proposed FTL-based Lithology microscopic image classifier in detail. First, the concepts of transfer learning and federated learning will be explained. Then, the proposed FTL scheme for Lithology microscopic image classification task is described."}, {"title": "3.1. Transfer Learning", "content": "Human learners would seem to possess inherent ways to transfer knowledge between tasks. Facing new activities, we understand and use relevant knowledge based on prior learning experiences. The more related the new task is to our prior experiences, the more easily we can master it (Olivas et al., 2009). In contrast, isolated tasks are traditionally explained in the widely used ML algorithms. Transfer learning aims to change this by developing strategies for transferring knowledge gained in one or more source tasks and utilizing it to advance learning in a related target task (Zhang et al., 2017).\nDue to the fact that the early layers' features/filters are more generic and the later layers are more original-dataset-specific in Convolutional Neural Network (CNN) models (Yamashita et al., 2018), it is reasonable to map the related task image classification to the early layers of a pre-trained CNN model and conduct transfer learning (Zhang et al., 2017). Subsequently, we use early layers of a pre-trained CNN-based image classifier to build our Lithology image classifier."}, {"title": "3-2- Federated Learning", "content": "Federated learning (FL) is a decentralized learning method that can allow several clients to train a global model without transferring local data off-site (McMahan et al., 2016). In FL, the majority of the training is done by the participants (clients), and a central server utilizes an aggregation approach to update the global model in an iterative process. The privacy-preserving nature of FL has been made it popular for a wide range of applications areas (Nasirigerdeh et al., 2020), for example, in Cross-block Oil-water Layer Identification (Chena et al., 2021), and healthcare data analysis (Chen et al., 2020; Sheller et al., 2018), where data access is restricted due to strict privacy policies.\nFederated averaging (FedAvg) (Collins et al., 2022; McMahan et al., 2016) is a communication-efficient method for FL aiming to obtain an accurate global model with an efficient number of communication rounds between the server and clients. The main goal of FedAvg is to carry out a significant number of local updates in the clients before averaging the local model parameters on the server simply and weightedly. FedAvg can significantly decrease the communication rounds where the client's data has IID characteristics."}, {"title": "3-2- The Golden Section Search Algorithm", "content": "The Golden Section Search (GSS) is an effective method for locating extremum (minimum or maximum) using a sequential narrowing mechanism of the range of values inside that extremum exists (Agrawal and Aware, 2012). The GSS algorithm is more suitable for exploring the optimum of 1-Demintional unimodal functions (Taheri et al., 2021). The primary goal is to find the minimum value of function f(x) within the specific interval [XL X\u028a]. In doing so, in an iterative manner, two points x\u2081 and x2 are chosen in the interval [XL X\u028a] and function f(x) is assessed at these points. Then, to shrink the search interval, as shown in Eq. (1), the point XL will be replaced by X1 if f(x1) is better than f(x2), otherwise, the Xu is replaced by X2.\n$x_L \\leftarrow x_1$, if f(x\u2081) < f(x\u2082)\n$x_U \\leftarrow x_2$, otherwise.  (1)"}, {"title": "3-3- Model Architecture", "content": "The pre-trained Inception-ResNet v.2 is selected as the most suitable model for our purpose. Inception architecture has shown the capability to achieve excellent performance with a considerably low computational cost. Additionally, it has been proven that combining residual connections with the Inception architecture results in more performance improvement. It is because networks with residual Inception architecture outperform Inception networks without using residual connections. InceptionResNet v2 is a powerful hybrid residual Inception architecture network introduced in (Szegedy et al., 2016). We also utilize a dense layers (fully connected) architecture to design the classifier in this study."}, {"title": "3-4- Fine-Tuned Aggregation Federated Transfer Learning (FTA-FTL) Scheme", "content": "In this section, the proposed scheme of Fine-Tuned Aggregation Federated Transfer Learning, namely FTA-FTL, is presented. First, the process of the proposed FTL scheme is described step by step in Alg. 1. Then, our proposed FTA mechanism will be explained in detail, Alg. 2.\nA schematic view of the architecture of the proposed FTL classification model is illustrated in Fig. 5. The process of the FTL scheme includes the server side and client side procedures. In this scheme, as shown in fig.5, the server side procedure acts as a central coordinator and hosted by cloud based infrastructure. According to Alg. 1, in this paradigm, decentralized learning is started by global model weights initialization (liens 1-2). As transfer learning is utilized, only the classifier is needed to be initialized. Then, in an iterative process (lines 3-15), the server communicates with clients to train the global model with distributed image datasets. In this way, a subset of clients is randomly selected to participate in the next round, in line 4. From lines 6 to 10, each participant downloads the full model wall in a parallel fashion if it is the first round. Otherwise, only the classifier wat is required to be downloaded. After that, the client-side procedure is launched on each client to update/train the local model. In doing so, the global model weights are utilized as initial weights for the local model (line c.1). Before uploading the local update to the server, the process of training/optimization is done by using a stochastic gradient-based algorithm (lines c.2 \u2013 c.6). Then, in line c.7, the classifier is disjointed to be sent to the server. On the server, after collecting all local updates from participated clients, an aggregation process is conducted to produce the next generation of the global model using the GSS-based FTA Algorithm Alg. 2. (line 13). Finally, in line 14, the base model and global classifier are connected to generate a full model.\nIn the GSS-based FTA Algorithm (Alg. 2), we employ a One-Dimensional Golden Section Search GSS algorithm (Agrawal and Aware, 2012) to optimize the aggregation process on the server. In doing so, after collecting local updates from participants at each round, the FTA algorithm is launched. According to Alg. 2, the FTA receives the global weights, local updates, and validation set as inputs. Then, in line 1, parameters lower bound X\u2081, upper bound X, and tolerance \u025b are initialized. In line 2, the golden ration o is calculated. Then, in an iterative manner (lines 3-13), the GSS algorithm conducts a search process to find the minimum value of the objective function Eq. 2, with respect to the loss value of the aggregated global model on validation set in the interval [XL,X\u028a]. The aim is to shrink the search interval. The search process is terminated when the interval [XL,X\u028a] is lower than \u025b. After that, in lines 14 and 15, the fine ratio o and the next generation of global weights We are calculated.\n$min_{x \\epsilon \\Re} F_x = L(W_G^{t+1}, D^v)$\nwhere:\n$W_G^{t+1} = W_G^t + \\sigma \\times \\sum_{k=1}^K \\frac{n_k}{N_{Total}} (W_L^{t,k} - W_G^t)$ (2)"}, {"title": "4- Experimental Analysis", "content": "In this section, first, several experiments are conducted to investigate the performance of the state-of-the-art DNN model architectures such as DensNet169, Xception, ResNet10, VGG19, and Inception-ResNet v2, a brief review on the DL model architectures is presented in Table 4, and the results are presented in terms of accuracy, f1 score, precision, specificity, sensitivity (recall), and confusion matrix. Then, the best DNN classifier model is selected according to the results and utilized as the global model in our proposed FTL scheme."}, {"title": "4-1- Performance Metrics", "content": "The following metrics are utilized to analyze the experimental results. Accuracy is the metric that represents the portion of the correct predictions to total predictions, Eq. (3). When both False Negatives (FN) and False Positives (FP) are important, and the dataset is balanced, the Precision metric determines the proportion of positives that are true positives, Eq. (4). Sensitivity (Recall) measures the True Positive (TP) to total actual positives ratio of data Eq. (5). This metric is important when identifying the positives is very important and is useful to be more confident of the predicted positives. Similarly, the negative accuracy is the specificity. It represents True Negative (TN) to the total actual negatives ratio of data, Eq. (6). This metric is used to cover all True Negatives TN. By F1-Score, both precision and recall are considered in this metric. F1-Score is an overall data science measure of a model's accuracy that takes into account precision and recall, Eq. (7). The higher the value of F1-Score, the higher the balance between Recall and Precision metrics. The confusion matrix is a common measurement for classification task evaluation. It can be utilized for both binary and multiclass classification tasks. In Fig. 6, an example of a binary classification confusion matrix is illustrated. As shown in Fig. 6, the confusion matrix summarizes TP, FP, TN, and False FN values.\n$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$ (3)\n$Precision = \\frac{TP}{TP + FP}$ (4)\n$Sensitivity (Recall) = \\frac{TP}{TP + FN}$ (5)\n$Specificity = \\frac{TN}{TN + FP}$ (6)\n$F1-Score = \\frac{2 \\times (Recall \\times Precision)}{(Recall + Precision)}$ (7)"}, {"title": "4-2- Implementation settings", "content": "For implementing all algorithms, the following settings are used. Python 3.7 is used as the implementation programing language, and Tensorflow 2.8 is employed as the ML library and framework. In addition, we utilize the Adam optimization algorithm with a learning rate of 0.001 and 500 epochs to train the dense layer-based classifiers (fully connected layers). We also use the following setting for the XGBoost classifier: maximum depth=3, learning rate=0.3, and the number of estimators=100. To conduct a fair comparison, all models are trained with the same train set consisting of 80% samples, and then they are applied to classify the same images test set (20% of samples)."}, {"title": "4-3- Results and Discussion", "content": "To select the most suitable architecture for lithology microscopic image classification, first, we evaluate various state-of-the-art DNN architecture performances. The results are presented in terms of accuracy, f1 score, precision, specificity, and sensitivity. In adition, we employ the selected DNN architecture to use and setup our FTL scheme."}, {"title": "4-3-1-Multi-class Classification", "content": "According to Table 8, it is clear to be seen that the Inception-ResNet v2 as the base model and a fully connected (FC) classifier with 2 (200,100) Dense layers outperforms other competitors and achieves the best results for evaluation metrics such as accuracy 90.68%, f1 score 90.76%, precision 90.61%, specificity 96.15%. In addition, among all other models, DenseNet169+XGB architecture performs better and takes the second best in terms of accuracy of 88.4%, f1 score of 88.3%, precision of 88.4%, and specificity of 90.6%. However, for the metric of sensitivity obtains the best result by 97.5%."}, {"title": "4-3-2- Binary Classification", "content": "Furthermore, we apply Inception-ResNet v2 architecture to conduct binary classification tasks. In doing so, a binary classification task is done for Argillaceous Limestone, limestone, and Dolomite classes and the experimental results are illustrated in Tables 5, 6, and 7, respectively. According to the presented experimental results, the Inception-ResNet v2 as the base model and a fully connected classifier with two dense layers provides the higher performance in all tasks for all evaluation metrics. Inception-ResNet v2 + FC architecture obtains the best result on the Argillaceous Limestone binary classification with accuracy of 95.45%, f1 score of 94.41%, precision of 95.32%,"}, {"title": "4-3-3- FLT Scheme Results", "content": "To implement the FLT scheme, we use a straightforward and flexible federated deep-leaning simulator, namely, the FedSim* tool. In this way, the following settings are used in our experiments: the number of clients is set to 10, the maximum number of rounds is set to 100, and the optimization settings are the same as those used in the centralized manner. Furthermore, in this study, we utilize the FedAvg algorithm (McMahan et al., 2016) [35] as the base algorithm to develop our proposed FTL scheme for the lithology microscopic image classification task.\nTable 9, presents the results of the proposed FTL scheme for multi-class Lithology image classification. It is clear from the results that the FTA-FTL algorithm is capable enough of achieving an acceptable result in multi-class Lithology image classification tasks with distributed data for both the IID and Non-IID cases. In addition, to conduct experimental analyses, the results of FLT classification are presented by using confusion matrixes, as shown in Fig. 8."}, {"title": "5- Conclusion", "content": "In this study, a novel scheme was introduced for lithology microscopic image classification task. In this way, firstly, the performance of various DL model architectures was investigated in a central manner to determine the most suitable architecture for our purpose. After that, we proposed a Fine-Tuned Aggregation Federated Transfer Learning (FTA-FTL) scheme. In doing so, the classification task was formulated as a FTL scheme and a novel aggregation method named FTA was introduced and applied to enhance the convergence rate and communication costs of the proposed FTL. In addition, a Lithology microscopic images dataset was produced during this study. The results of this study confirmed that the proposed FTA-FTL scheme is able to train a global model with distributed data in the both IID and non-IID forms and obtains the higher accuracy than well-known federated learning algorithm (e.g. FedAvg) for test data. However, it is not expected to achieve a very high accuracy by FL based trained models, our proposed FTA-FTL scheme obtained approximately the same accuracy achieved by a centrally trained model."}]}