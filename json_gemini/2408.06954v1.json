{"title": "Neural Speech and Audio Coding", "authors": ["Minje Kim", "Jan Skoglund"], "abstract": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet-hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or pre-defined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.", "sections": [{"title": "1. Introduction", "content": "Traditional speech and audio coding is a well-established technology, where various model-based approaches have been effective in compressing raw audio signals into compact bitstrings (encoding) and then restoring them to their original signal domain (decoding). These models aim to maintain the original signal's quality, such as speech intelligibility or other perceptual sound qualities, which are often subjectively defined. Hence, developing such models typically involves multiple rounds of listening tests to precisely measure the codec's performance. Although these models are designed by domain experts based on their knowledge and experience, finalizing them still requires tuning their parameters through listening tests and manual adjustments.\nThe main challenge in traditional codec development is that manual tuning of model parameters relies significantly on time-consuming and costly listening tests. Successful models have emerged from extensive manual efforts and decades of research, including psychoacoustic models for bit allocation in MPEG-2 Audio Layer III (a.k.a. MP3) [1, 2], spectral band replication [3], frequency transformation methods such as the modified discrete cosine transform (MDCT) [4], and speech generation models like linear predictive coding (LPC) [5], to name a few. However, standard speech and audio codecs typically consist of multiple heterogeneous coding blocks that interact with one another, complicating the tuning task.\nMeanwhile, data-driven approaches have also been successfully introduced to conventional codecs as their components. For example, unified speech and audio coding (USAC) [6, 7] employs a classification module to detect the transient events from the input signal and then use the appropriate coding module accordingly, i.e., by increasing the temporal resolution of the TF transform in the transient period, and vice versa. Enhanced voice services (EVS), the latest 3GPP standard for voice communication, comes with the voice activity detection (VAD) feature, or more precisely, signal activity detection (SAD) [8]. Due to its discriminative nature, which specifies whether each 20 ms frame contains a meaningful signal, it can be essentially considered a binary classification module, too. Finally, Opus, a royalty-free and open-source audio codec, also utilizes a small neural network-based classifier for VAD and music/speech classification [9]. Training those classification models in a completely data-driven way is not entirely straightforward due to the fact that the ground-truth labels of these sound events, and consequently, the classification performance, should be defined to improve the perceptual quality of the decoded signals. The intricacy hinders the researchers from investigating structural variations of the coding system, compared to a typical supervised learning setup where pre-defined classes manually label training samples.\nRecently, data-driven methods for coding have gained intensive attention. In these neural speech and audio coding (NSAC) systems, a simple approach is to postulate an autoencoding task that re-constructs the input signal as exactly as possible in its output. In the meantime, to achieve the signal compression goal in the autoencoding pipeline, it is typical to decompose the system into two parts: an encoder module that produces a latent representation, followed by a decoder that recovers raw"}, {"title": "2. Potentials and Limitations of NSAC", "content": "Modern speech and audio codecs can be grouped into two categories based on their application. First, codecs are heavily used in communication scenarios where multiple end users are involved. The predominant content, in this case, is spoken language. Hence, codecs are required to process speech in real-time or with a delay low enough for people not to recognize. Digital voice communication applications that use voice over Internet Protocol (VoIP) are representative examples. Since voice communication can happen between mobile (i.e., resource-constrained) devices nowadays, it is also very important for codecs to operate with minimal computational and spatial complexity. In addition, there is an increasing need to handle mixed contents in communication scenarios, such as non-speech signals mixed with users' utterances, necessitating communication codecs to be robust to various contents in adversarial environments. Finally, depending on the bandwidth of the communication channel, this type of codec may be required to operate in low bitrates, e.g., lower than 10 kbps.\nSecond, codecs can be used in a more uni-directional application scenario, such as for media streaming, digital broadcasting, storing music signals, etc. In these uni-directional applications, the user on the decoder side tends to be less sensitive to the delay. Instead, listeners are often more sensitive to the subtle discrepancies and artifacts that the codecs generate, e.g., in music listening. Consequently, these media codecs are designed to provide a high-fidelity reconstruction for various input audio signals, including speech, music, mixed, and multichannel contents.\nAlthough NSAC systems are evolving in the direction that tries to meet the requirements mentioned above, they possess inherent characteristics that come from their data-driven nature. One of the most distinctive factors is the high computational complexity, which ranges from 100G floating-point operations per second (FLOPS) in a large autoregressive model, e.g., a WaveNet decoder [17], to a relatively efficient LPCNet decoder, which still requires 3 GFLOPS in its original version [20]. Meanwhile, one of the standard speech codecs, AMR-WB, can decode with only 7.8 weighted million operations per second (WMOPS) [21]. Likewise, although a direct and rigorous comparison is impossible, neural codecs are multiple orders of magnitudes more complex than traditional codecs. While it is expected that more advanced hardware architecture for neural network inference could make neural codecs more affordable on devices, it is clear that reducing computational complexity"}, {"title": "3. Data-Driven Approaches to Removing Coding Artifacts", "content": "Depending on the characteristics of the underlying model, a codec can produce unique coding artifacts, which can lower the perceptual quality of the decoded signal. In theory, we can postulate a general-purpose signal enhancement system, trained to reduce various types of coding artifacts by mapping decoded signals to their original inputs. However, in practice, learning such a mapping function through a data-driven approach by focusing on a specific type of codec and the signals it processes is more feasible. Let $F(\\cdot)$ represent a legacy codec, which can be decomposed into the encoder $E(\\cdot)$ and decoder $D(\\cdot)$ modules:\n$x \\approx \\hat{x} \\leftarrow F(x) = D \\circ E(x)$.\nWith this framework, we can propose an additional parametric model trained to map the decoded signal $\\hat{x}$ back to the original, unprocessed input $x$,\n$x \\approx \\hat{x} \\leftarrow G(\\hat{x}; \\theta)$,\nwhere the training process, e.g., a variation of gradient descent, updates the model parameter $\\theta$ to the direction that can minimize $L(x||\\hat{x})$, which is a pre-defined metric that measures the difference between the two signals. In other words, with the additional denoising process introduced by $G(\\cdot)$,\nwe hope the quality of the decoded signal improves in terms of the loss function $L(\\cdot)$:\n$L(x||\\hat{x}) < L(x||\\hat{x})$.\nThis pipeline is convenient to implement because the only learnable module, $G(\\cdot; \\theta)$, can be concatenated with any existing codec after it has been trained to enhance $\\hat{x}$ into $x$. It is because the underlying codec, $F(\\cdot)$, is presumed to be already fully configured or frozen from the perspective of the learning algorithm.\nMoreover, this post-processing approach does not increase bitrate. Typically, signals in communication are transmitted in encoded data, i.e., $h \\leftarrow E(x)$, with the decoder located on the receiver side. Therefore, any operations performed after decoding, such as $G(\\hat{x}; \\theta)$, do not impact the bitrate or alter the behavior of the encoder, rendering the post-processing module a bitrate-free signal enhancer.\nThe drawback of this approach is that the codec and enhancement module are segregated. The ability to utilize any existing codec also implies that the codec's inherent characteristics remain unchanged and do not directly benefit from the data-driven approach. Consequently, the enhancement module $G(\\cdot;\\theta)$ may struggle to eliminate codec-specific artifacts, necessitating substantial model capacity and increasing decoding complexity."}, {"title": "3.1 Supervised Signal Enhancement Models for Post-Processing", "content": "In [24], convolutional neural network (CNN) models were introduced to enhance coded speech, serving as an alternative to the $G(\\cdot;\\theta)$. The authors experimented with two distinct versions, one operating directly in the time domain in an end-to-end process, and another leveraging cepstrum features. In comparison to traditional post-filtering methods employed by G.711 [25], their approach demonstrated noticeable improvements across various objective metrics and listening tests. This method closely parallels the supervised DNN-based speech enhancement problem, where the primary objective is to eliminate any undesired artifacts from real-world speech recordings. Specifically applied to coded speech, the model was trained to remove the coding artifacts.\nAs for audio coding, both CNNs and recurrent neural networks (RNN) have been utilized to enhance the MP3-compressed signals [26]. Among them, a long short-term memory (LSTM) network has been effectively used to predict signals in the time and frequency domains defined by the modified discrete cosine transform (MDCT), referred to as T- and F-LSTM, respectively. The TF-LSTM method improved the subjective quality in terms of mean opinion score (MOS), particularly when the post-processing is applied to the more demanding stereophonic signals at 96kbps or on the 64kbps mono signals."}, {"title": "3.2 Generative Models as a Post-Processor to Enhance Coded Speech", "content": "Since the coding artifacts primarily stem from information loss rather than additive noise, supervised learning-based approaches may face challenges in imputing missing values. Therefore, exploring a more generative approach for addressing the codec-specific enhancement issue is justifiable.\nBiswas and Jia proposed a generative adversarial network (GAN) [27] to enhance the performance of the AAC codec [4] at low bitrates, namely deep coded audio enhancer (DCAE) [28]. Utilizing the GAN formulation, the enhancement network functions as the generator, trained to generate realistic examples from random noise $n$ drawn from the standard normal distribution,\n$\\hat{x} \\leftarrow G(\\hat{x},n; \\theta_g), \\quad \\eta \\sim N(0,1)$.\nNote that, the enhancement network $G(\\cdot)$ now acts as a generator, which takes both the decoded audio $\\hat{x}$ to be enhanced and random noise $n$ to add stochasticity to the process.\nFurthermore, the generated examples are examined by a discriminator $C(\\cdot; \\theta_c)$, which acts as a binary classifier trained to differentiate between synthetic and real-world examples. Consequently, the ultimate objective of GAN training is to reach a Nash equilibrium, a state where the discriminator fails to tell the difference between the real and fake examples. To mitigate the AAC codec's artifacts, the original GAN formulation was modified in the following manner:\n$L_G(\\theta_g) = E_{\\eta \\sim N(0,1), x \\sim P_{data}(x)} [(C(x,\\hat{x}; \\theta_c) - 1)^2] + E_{x \\sim P_{data}(x)}||x - \\hat{x}||_1$,\n$L_C(\\theta_c) = E_{x,\\hat{x} \\sim P_{data}(x)} [(C(x,\\hat{x}; \\theta_c) - 1)^2] + E_{\\eta \\sim N(0,1), x \\sim P_{data}(x)} [C(\\hat{x},\\hat{x}; \\theta_c)^2]$.\nEq. (5) defines the loss $L_G$ as a function of the generator parameters $\\theta_g$. Beyond the standard reconstruction loss $||x - \\hat{x}||_1$ term, the first loss term also encourages the generator output $\\hat{x}$ to be classified as 1, the \u201creal\u201d examples category, by deceiving the discriminator. Note that the expectation spans two distributions, $N(0;1)$, responsible for generating the random noise $n$ that introduces stochasticity into the generator (eq. (4)), and the sample distribution of the training set $P_{data}(x)$, whose sample deterministically goes through the coding process and defines the decoded signal $\\hat{x}$, too. In doing so, the fake example $\\hat{x}$ is appended by the decoded signal $\\hat{x}$ to give more context to the classifier, i.e., $C((x,\\hat{x})$. Likewise, aside from the typical reconstruction loss $||x - \\hat{x}||_1$, within the GAN context, the generator's performance is evaluated only by the fake example pairs, $(\\hat{x},\\hat{x})$.\nOn the contrary, the discriminator $C(\\cdot)$, as a classifier, also requires exposure to real example pairs, $(x,\\hat{x})$. In this case, since the examples are real, they must also be assigned label 1. The discriminator loss $L_c(\\theta_c)$ is defined over the discriminator parameters $\\theta_c$, aiming to minimize the disparity between the predicted class of the real example pair $(x, \\hat{x})$ and label 1, while pushing the prediction of the fake example pair $(\\hat{x}, \\hat{x})$ to be near zero. Note that the second term of the discriminator loss $L_c(\\theta_c)$ and the generator loss $L_g(\\theta_g)$ are in conflict, contributing to the instability of GAN training.\nThe GAN-based method significantly enhanced the AAC codec's performance at 24 and 32 kbps, achieving an impressive gain of 10 to 14 points on average in MUSHRA tests for both speech and applause signals. While the baseline AAC codec is designed for general-purpose audio coding, the GAN training was specifically conducted on speech dataset or applaud samples, each treated"}, {"title": "4. Learning to Predict Speech Signals", "content": "The supervised signal enhancement models as well as their GAN variations introduced a sensible quality improvement to the traditional coding pipeline. Although the detached nature of the post-processor and the codec is convenient, the pipeline leaves room for structural innovation.\nWe start with the modeling of speech signals. It has long been known in the literature that a source-filter model can effectively explain the speech generation process [31]. In this model, the source signal produced by the glottal vibration is filtered by the vocal tract to produce the formant effects. Traditional speech codecs have widely used the simple linear predictive coding (LPC) scheme to model the speech signal, by isolating the vocal tract's effects from the spectrum via a simple linear prediction model,\n$x_t \\approx u_t \\leftarrow \\sum_{\\tau=1}^{p} a_\\tau x_{t-\\tau}, \\quad e_t = x_t - u_t$\nwhere $p$ is the order of LPC and $e_t$ stands for the residual between the sample $x_t$ and the prediction $u_t$, which could be a low-energy, quasi-periodic, and impulse-like signal if modeling is successfully done on the periodic component of speech, e.g., voiced areas. Once quantized, the LPC coefficients $a$ account for relatively low bitrates, e.g., 2.4 kbps via multistage vector quantization in AMR-WB, whose total bitrate varies from 6.6 to 23.85 kbps. Therefore, the codec's performance depends on how much coding gain it achieves in compressing the residual signal $e_t$."}, {"title": "4.1 End-to-End Codec for LPC Residual Coding", "content": "A straightforward way to combine neural coding and LPC is to replace the traditional speech coding module that operates in the LPC residual domain with an end-to-end autoencoder. Hence, eq. (1) can be redefined by taking the residual signal $e$ as follows:\n$e \\approx \\hat{e} \\leftarrow F(e) = D \\circ E(e)$.\nIn other words, if the autoencoding performance improves, a better residual reconstruction on the decoder side contributes to better LPC synthesis.\nIn [32], this concept was empirically proved: instead of compressing the raw speech signal directly via a CNN-based autoencoder as in [10], coding in the LPC residual domain can achieve a better perceptual quality at the same bitrate. In addition to the simple concatenation of LPC and"}, {"title": "4.2 LPCNet", "content": "A more sophisticated approach to combining the model-based and data-driven methods is LPCNet. It was originally proposed as a general-purpose vocoder, and then soon developed into a codec by being able to synthesize speech from a very low-bitrate (1.6 kbps) cepstrum-based code [20]."}, {"title": "5. Learning to Predict Speech and Audio Signals in the Feature Space", "content": "A widely used principle in coding, as in LPC-based ones, is to use a predictive model, assuming that the distribution of the residual samples between the original and predicted signals is with lower entropy. Since entropy serves as the lower bound of the bitrate, residual coding is generally beneficial once the model's prediction is good enough.\nMeanwhile, the basic idea behind an end-to-end neural codec, as described in Figure 2, is to convert the raw signal into the feature space where a code vector $h$ can be quantized more effectively. Since quantization happens in the feature space in neural codecs, it is natural to employ residual coding in the feature space rather than in the raw signal domain.\nFirst, the raw samples at the t-th frame $x_t$ is converted into the feature space, where quantization could be conducted if it were not for residual coding. The transformation can be done in a model-based approach, e.g., by using MDCT, but a data-driven method can also learn a custom feature space, e.g., by a CNN encoder $E(\\cdot)$.\nNext, on the sender side, the codec utilizes a predictor module $W^{pred}(\\cdot)$ to predict the current code vector $h_t$ from its preceding feature vectors. In theory, $W^{pred}(\\cdot)$ can be trained using $p$ ground-truth features in the past, i.e., $[h_{t-p}, ..., h_{t-1}]$, but a more robust set up is to use a series of reconstructed features $[\\hat{h}_{t-p},..., \\hat{h}_{t-1}]$ that are the reconstructed feature vectors that the receiver accumulates in its buffer to use as the past examples for prediction. Hence, we write the prediction function,\n$\\hat{h}_t \\leftarrow W^{pred} (\\hat{h}_{t-p},..., \\hat{h}_{t-1})$\nThe reason why the predictor becomes more robust when it was trained to predict from the reconstructed features is that, in this way, the predictor is trained to encompass the potential reconstruction error in its prediction process, which the receiver has to deal with during the test-time inference. Note that the same predictor module is shared between the sender and receiver.\nThe prediction vector $\\hat{h}_t$ is compared against the original feature $h_t$, and then the residual signal $e_t$ is induced. Here, the residual can be computed in a straightforward manner, i.e., $e_t \\leftarrow h_t - \\hat{h}_t$, although a more involved residual learner can be employed as follows:\n$e_t \\leftarrow W^{\\oplus}(h_t, \\hat{h}_t)$.\nAfter quantization, the residual feature vector $e_t$ is sent to the receiver for decoding. At the same time, the dequantized version $\\bar{e}_t$ is harmonized with the prediction $\\hat{h}_t$ back again to form the reconstruction $\\tilde{h}_t$. Once again, the reconstruction can be as simple as a summation, i.e., $\\tilde{h}_t \\leftarrow \\bar{e}_t + \\hat{h}_t$, but it could also employ a neural network module that synthesizes the reconstruction as follows:\n$\\tilde{h}_t \\leftarrow W^{\\circledS}(\\hat{h}_t, \\bar{e}_t)$.\nThe system can choose to utilize $p$ such reconstructed features as a sequential input to the predictor module $W^{pred}(\\hat{h}_{t-p},..., \\hat{h}_{t-1})$, whose prediction $\\hat{h}_t$ is fed back to the pipeline as the input to the residual learner (eq. (14)) and synthesizer (eq. (15)).\nThe receiver operates similarly: (a) it reconstructs $p$ features using the same synthesizer as in eq. (15) using the transmitted residual signal after dequantizing it, $\\bar{e}_t$, as well as the corresponding frame's prediction $\\hat{h}_t$ (b) the reconstructed features $\\hat{h}_t$ are accumulated for predicting the next frame's feature $\\tilde{h}_t$ (c) finally, a decoder function converts the reconstructed feature back to the raw signal $x_t$.\nLikewise, the merit of this approach is that the neural codec can additionally benefit from the temporal context of the signal, which can span longer than learning from the raw signal due to the frame-level prediction that decimates the original temporal resolution. As in other model-based approaches, once the prediction model is successful, the residual signal's energy reduces, which usually leads to a lower entropy."}, {"title": "5.1 Feature Prediction for the LPCNet Codec", "content": "A GRU-based predictive model [36] introduced additional coding gain to the LPCNet codec, which corresponds to the prediction module in eq. (13). It aligns with the general concept shown in Figure 5, except for its own specific configurations. First of all, since it employs a GRU model, which uses its hidden units $y$ to summarize the past information, the prediction model does not need to maintain a buffer of past feature reconstructions. We can rewrite this GRU-based predictor as follows:\n$\\hat{h}_t, Y_t \\leftarrow W^{pred} (h_{t-1}, Y_{t-1})$,\nwhere $y_{t-1}$ essentially summarizes all previous features the GRU model has been exposed to.\nAnother unique setup is that, instead of using a neural network encoder $E(\\cdot)$, it inherits the cepstrum-based code space that LPCNet uses. In addition, the residual signal and the reconstruction are conducted via the simple subtraction and addition operations, i.e.,\n$e_t \\leftarrow h_t - \\hat{h}_t, \\quad \\hat{h}_t \\leftarrow \\hat{h}_t + e_t$.\nFinally, as for the decoder that recovers raw signals from the predicted feature $\\hat{h}_t$, the original LPCNet is directly used as a vocoder. The residual coding scheme introduced additional coding gain, e.g., about 5 points higher at a lower bitrate (1.47 kbps) than LPCNet's (1.6 kbps) in the MUSHRA test."}, {"title": "5.2 TF-Codec", "content": "TF-Codec [37] is equipped with various useful components, such as VQ-VAE, distance Gumbel-Softmax for rate control, and learnable 2D CNN encoder $E(\\cdot)$ and decoders $D(\\cdot)$. Moreover, as a codec that actively predicts features and conducts residual coding, it is noticeable that they tried two different architectures for prediction $W^{pred}(\\cdot)$, a 2D CNN and attention model-based one, respectively. Coupled with the custom encoder's ability to learn a suitable latent space, as well as the more sophisticated residual learner $W^{\\oplus}$ and synthesizer $W^{\\circledS}$ functions, TF-Codec achieves high-quality speech reconstruction at very low bitrates, e.g., 1 kbps."}, {"title": "5.3 MDCTNet", "content": "MDCTNet is another predictive method that works in the MDCT-transformed domain, making it compatible with the existing model-based audio codecs [38]. The overall architecture is more similar to the decoder-only neural codecs, such as the WaveNet speech codec [17] or LPCNet [20], than the above-mentioned predictive codecs, in the sense that prediction is not to compute the residual signal. Instead, the generative MDCTNet operates only on the decoder side to estimate the MDCT coefficients directly.\nOn the encoder side, the input frames are transformed into a series of MDCT coefficient vectors, which are perceptually weighted via a psychoacoustic model and then quantized into the bitstream. The receiver takes the dequantized coefficient vectors $h_t$, but instead of transforming them back to the time domain directly, it only uses them to condition the generative MDCTNet, which consists of three prediction networks. First, a temporal prediction model (a two-layer GRU module) performs per-band temporal prediction out of the past time frames,\n$\\hat{h}_{b,t} \\leftarrow W^{time} (\\hat{h}_{b,t-1};h_t)$,\nwhere $\\hat{h}_{b,t}$ stands for the predicted MDCT coefficient at time frame $t$ and frequency subband $f$. The GRU hidden states are omitted for notational brevity. Note that the model is conditioned with the quantized code vector $h_{bt}$, which improves the prediction accuracy via extra information.\nIn addition, MDCTNet also employs a cross-band prediction module, which is a CNN layer that takes past information from the adjacent subbands, i.e., $b - 1$ and $b + 1$,\n$\\hat{h}_{b,t} \\leftarrow W^{cross} (\\hat{h}_{b-1,t-1}, \\hat{h}_{b+1,t-1};h_t)$.\nFinally, the frequency-domain GRU module predicts the higher subband from the lower ones,\n$\\hat{h}_{b,t} \\leftarrow W^{freq} (\\hat{h}_{b-1,t}; h_t, \\hat{h}_{b,t})$.\nwhere the frequency predictor is conditioned with the temporal prediction $\\hat{h}_{bt}$ achieved so far. The MDCTNet codec also employs various other techniques that make the system more robust,"}, {"title": "6. Psychoacoustic Models for Perceptual Loss Functions", "content": "A fundamental issue with the data-driven approach to coding is that, during training, the quality of the decoded signals can be measured only in an objective way, e.g., using signal-to-noise ratio (SNR), etc. This is due to the nature of the gradient-based optimization algorithm that relies on the differentiation of the loss function with respect to the model parameters. Oftentimes, those objective metrics reflect only a certain aspect of the perceptual quality of the signal, leaving a gap between the perceived quality of the codec's output signal and its loss value, i.e., a signal with a high objective score does not necessarily sound good to human ears. This kind of issue can be more sophisticated if the codec has to maintain subtlety during its processing, such as for music signals with high fidelity."}, {"title": "6.1 Psychoacoustic Calibration of Loss Functions", "content": "Likewise, psychoacoustic models (PAM) have been the crucial component of many traditional audio coding systems. Since PAM's main usage is to make the system's behavior closer to human perception, a natural way to harmonize the concept with a data-driven method is to use it to redefine the training objectives.\nIn [39], two different approaches to psychoacoustic calibration were proposed to improve the loss function's relevance to human perception. In their first proposed loss function, priority weighting, the reconstruction loss is defined as a sum of subband-specific losses, each of which is weighted by the perceptual importance:\n$L_{pw}(t) = \\sum_f w_f (X_f - \\hat{X}_f)^2$,\nwhere $X_f$ and $\\hat{X}_f$ are the magnitude of the Fourier spectrum at subband $f$ and its reconstruction, respectively. The frame index $t$ is omitted from the equation for brevity. Since the loss is a weighted sum of subband-specific reconstruction loss, the weights $w_f$ play a big role. For a given input time domain frame $x$ and its logarithmic PSD $Y$, the perceptual weight vector $w$ is defined by\n$W_f = \\log_{10} (\\frac{10^{0.1 Y_f}}{10^{0.1 M_f}} + 1)$"}, {"title": "7. Conclusion", "content": "In this paper, recent neural speech and audio coding systems were presented as an example of successful harmonization of model-based and data-driven approaches. Various model-based approaches have been proposed and commercialized in the past few decades, which an entirely data-driven approach cannot easily catch up with due to the highly subjective evaluation process of the speech and audio codecs. Carefully designed hybrid systems, which also tend to benefit from a sufficiently large architecture, can be an alternative, introducing sensible coding gain to the already-saturated conventional codecs' performance. The paper first introduced a neural network-based signal enhancer as a post-processor of existing codecs. CMRL and LPCNet were another useful type of hybrid systems, where LPC was harmonized with the neural network-based end-to-end codec and vocoder, respectively. The paper also explored predictive models that work either in the custom feature space or pre-defined transform domain. Finally, we saw that psychoacoustic models can be effectively used in the data-driven training paradigms by improving the perceptual relevance of the"}]}