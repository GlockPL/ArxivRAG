{"title": "Neural Speech and Audio Coding", "authors": ["Minje Kim", "Jan Skoglund"], "abstract": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet-hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or pre-defined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.", "sections": [{"title": "1. Introduction", "content": "Traditional speech and audio coding is a well-established technology, where various model-based approaches have been effective in compressing raw audio signals into compact bitstrings (encoding) and then restoring them to their original signal domain (decoding). These models aim to maintain the original signal's quality, such as speech intelligibility or other perceptual sound qualities, which are often subjectively defined. Hence, developing such models typically involves multiple rounds of listening tests to precisely measure the codec's performance. Although these models are designed by domain experts based on their knowledge and experience, finalizing them still requires tuning their parameters through listening tests and manual adjustments.\nThe main challenge in traditional codec development is that manual tuning of model parameters relies significantly on time-consuming and costly listening tests. Successful models have emerged from extensive manual efforts and decades of research, including psychoacoustic models for bit allocation in MPEG-2 Audio Layer III (a.k.a. MP3) [1, 2], spectral band replication [3], time-\nThis work was supported in part by Electronics and Telecommunications Research Institute (ETRI) grant funded by\nthe Korean government (23ZH1200; \u201cThe research of the basic media contents technologies\").\nAccepted for publication in IEEE Signal Processing Magazine (2024)."}, {"title": "2. Potentials and Limitations of NSAC", "content": "Modern speech and audio codecs can be grouped into two categories based on their application. First, codecs are heavily used in communication scenarios where multiple end users are involved. The predominant content, in this case, is spoken language. Hence, codecs are required to process speech in real-time or with a delay low enough for people not to recognize. Digital voice communication applications that use voice over Internet Protocol (VoIP) are representative examples. Since voice communication can happen between mobile (i.e., resource-constrained) devices nowadays, it is also very important for codecs to operate with minimal computational and spatial complexity. In addition, there is an increasing need to handle mixed contents in communication scenarios, such as non-speech signals mixed with users' utterances, necessitating communication codecs to be robust to various contents in adversarial environments. Finally, depending on the bandwidth of the communication channel, this type of codec may be required to operate in low bitrates, e.g., lower than 10 kbps.\nSecond, codecs can be used in a more uni-directional application scenario, such as for media streaming, digital broadcasting, storing music signals, etc. In these uni-directional applications, the user on the decoder side tends to be less sensitive to the delay. Instead, listeners are often more sensitive to the subtle discrepancies and artifacts that the codecs generate, e.g., in music listening. Consequently, these media codecs are designed to provide a high-fidelity reconstruction for various input audio signals, including speech, music, mixed, and multichannel contents.\nAlthough NSAC systems are evolving in the direction that tries to meet the requirements mentioned above, they possess inherent characteristics that come from their data-driven nature. One of the most distinctive factors is the high computational complexity, which ranges from 100G floating-point operations per second (FLOPS) in a large autoregressive model, e.g., a WaveNet decoder [17], to a relatively efficient LPCNet decoder, which still requires 3 GFLOPS in its original version [20]. Meanwhile, one of the standard speech codecs, AMR-WB, can decode with only 7.8 weighted million operations per second (WMOPS) [21]. Likewise, although a direct and rigorous comparison is impossible, neural codecs are multiple orders of magnitudes more complex than traditional codecs. While it is expected that more advanced hardware architecture for neural network inference could make neural codecs more affordable on devices, it is clear that reducing computational complexity"}, {"title": "3. Data-Driven Approaches to Removing Coding Artifacts", "content": "Depending on the characteristics of the underlying model, a codec can produce unique coding artifacts, which can lower the perceptual quality of the decoded signal. In theory, we can postulate a general-purpose signal enhancement system, trained to reduce various types of coding artifacts by mapping decoded signals to their original inputs. However, in practice, learning such a mapping function through a data-driven approach by focusing on a specific type of codec and the signals it processes is more feasible. Let $F(\u00b7)$ represent a legacy codec, which can be decomposed into the encoder $E(\u00b7)$ and decoder $D(\u00b7)$ modules:\n$x \u2248 \\hat{x} \u2190 F(x) = D\u00b0E(x)$.\nWith this framework, we can propose an additional parametric model trained to map the decoded signal $\\hat{x}$ back to the original, unprocessed input $x$,\n$x \u2248 \\hat{x} \u2190 G(\\hat{x}; \\theta)$,\nwhere the training process, e.g., a variation of gradient descent, updates the model parameter $\\theta$ to the direction that can minimize $L(x||\\hat{x})$, which is a pre-defined metric that measures the difference between the two signals. In other words, with the additional denoising process introduced by $G(\u00b7)$,"}, {"title": "3.1 Supervised Signal Enhancement Models for Post-Processing", "content": "In [24], convolutional neural network (CNN) models were introduced to enhance coded speech, serving as an alternative to the $G(\u00b7;\\theta)$. The authors experimented with two distinct versions, one operating directly in the time domain in an end-to-end process, and another leveraging cepstrum features. In comparison to traditional post-filtering methods employed by G.711 [25], their approach demonstrated noticeable improvements across various objective metrics and listening tests. This method closely parallels the supervised DNN-based speech enhancement problem, where the primary objective is to eliminate any undesired artifacts from real-world speech recordings. Specifically applied to coded speech, the model was trained to remove the coding artifacts.\nAs for audio coding, both CNNs and recurrent neural networks (RNN) have been utilized to enhance the MP3-compressed signals [26]. Among them, a long short-term memory (LSTM) network has been effectively used to predict signals in the time and frequency domains defined by the modified discrete cosine transform (MDCT), referred to as T- and F-LSTM, respectively. The TF-LSTM method improved the subjective quality in terms of mean opinion score (MOS), particularly when the post-processing is applied to the more demanding stereophonic signals at 96kbps or on the 64kbps mono signals."}, {"title": "3.2 Generative Models as a Post-Processor to Enhance Coded Speech", "content": "Since the coding artifacts primarily stem from information loss rather than additive noise, supervised learning-based approaches may face challenges in imputing missing values. Therefore, exploring a"}, {"title": "4. Learning to Predict Speech Signals", "content": "The supervised signal enhancement models as well as their GAN variations introduced a sensible quality improvement to the traditional coding pipeline. Although the detached nature of the post-processor and the codec is convenient, the pipeline leaves room for structural innovation.\nWe start with the modeling of speech signals. It has long been known in the literature that a source-filter model can effectively explain the speech generation process [31]. In this model, the source signal produced by the glottal vibration is filtered by the vocal tract to produce the formant effects. Traditional speech codecs have widely used the simple linear predictive coding (LPC) scheme to model the speech signal, by isolating the vocal tract's effects from the spectrum via a simple linear prediction model,\n$x_t \u2248 u_t\u2190 \\sum_{\\tau=1}^{p} a_{\\tau}x_{t-\\tau}$,\n$e_t=x_t-u_t$,\nwhere $p$ is the order of LPC and $e_t$ stands for the residual between the sample $x_t$ and the prediction $u_t$, which could be a low-energy, quasi-periodic, and impulse-like signal if modeling is successfully done on the periodic component of speech, e.g., voiced areas. Once quantized, the LPC coefficients $a$ account for relatively low bitrates, e.g., 2.4 kbps via multistage vector quantization in AMR-WB, whose total bitrate varies from 6.6 to 23.85 kbps. Therefore, the codec's performance depends on how much coding gain it achieves in compressing the residual signal $e_t$."}, {"title": "4.1 End-to-End Codec for LPC Residual Coding", "content": "A straightforward way to combine neural coding and LPC is to replace the traditional speech coding module that operates in the LPC residual domain with an end-to-end autoencoder. Hence, eq. (1) can be redefined by taking the residual signal $e$ as follows:\n$e \u2248 \\hat{e} \u2190 F(e) = Do E(e)$.\nIn other words, if the autoencoding performance improves, a better residual reconstruction on the decoder side contributes to better LPC synthesis. \nIn [32], this concept was empirically proved: instead of compressing the raw speech signal directly via a CNN-based autoencoder as in [10], coding in the LPC residual domain can achieve a better perceptual quality at the same bitrate. In addition to the simple concatenation of LPC and"}, {"title": "4.2 LPCNet", "content": "A more sophisticated approach to combining the model-based and data-driven methods is LPC-Net. It was originally proposed as a general-purpose vocoder, and then soon developed into a codec by being able to synthesize speech from a very low-bitrate (1.6 kbps) cepstrum-based code [20]."}, {"title": "5. Learning to Predict Speech and Audio Signals in the Feature Space", "content": "A widely used principle in coding, as in LPC-based ones, is to use a predictive model, assuming that the distribution of the residual samples between the original and predicted signals is with lower entropy. Since entropy serves as the lower bound of the bitrate, residual coding is generally beneficial once the model's prediction is good enough.\nMeanwhile, the basic idea behind an end-to-end neural codec, as described in Figure 2, is to convert the raw signal into the feature space where a code vector $h$ can be quantized more effectively. Since quantization happens in the feature space in neural codecs, it is natural to employ residual coding in the feature space rather than in the raw signal domain.\nFigure 5 illustrates the general residual coding concept implemented in the feature space. First, the raw samples at the t-th frame $x_t$ is converted into the feature space, where quantization could be conducted if it were not for residual coding. The transformation can be done in a model-based approach, e.g., by using MDCT, but a data-driven method can also learn a custom feature space, e.g., by a CNN encoder $E(\u00b7)$.\nNext, on the sender side, the codec utilizes a predictor module $W^{pred}(.)$ to predict the cur-"}, {"title": "5.1 Feature Prediction for the LPCNet Codec", "content": "A GRU-based predictive model [36] introduced additional coding gain to the LPCNet codec, which corresponds to the prediction module in eq. (13). It aligns with the general concept shown in Figure 5, except for its own specific configurations. First of all, since it employs a GRU model, which uses its hidden units $y$ to summarize the past information, the prediction model does not need to maintain a buffer of past feature reconstructions. We can rewrite this GRU-based predictor as follows:\n$\\hat{h_t}, Y_t \u2190 W^{pred}(h_{t-1}, Y_{t-1})$,\nwhere $y_{t-1}$ essentially summarizes all previous features the GRU model has been exposed to.\nAnother unique setup is that, instead of using a neural network encoder $E(\u00b7)$, it inherits the cepstrum-based code space that LPCNet uses. In addition, the residual signal and the reconstruction are conducted via the simple subtraction and addition operations, i.e.,\n$e_t \u2190h_t - \\hat{h_t}, \\hat{h_t}\u2190\\hat{h_t} + e_t$.\nFinally, as for the decoder that recovers raw signals from the predicted feature $\\hat{h_t}$, the original LPCNet is directly used as a vocoder. The residual coding scheme introduced additional coding gain, e.g., about 5 points higher at a lower bitrate (1.47 kbps) than LPCNet's (1.6 kbps) in the MUSHRA test."}, {"title": "5.2 TF-Codec", "content": "TF-Codec [37] is equipped with various useful components, such as VQ-VAE, distance Gumbel-Softmax for rate control, and learnable 2D CNN encoder $E(\u00b7)$ and decoders $D(\u00b7)$. Moreover, as a codec that actively predicts features and conducts residual coding, it is noticeable that they tried two different architectures for prediction $W^{pred}(\u00b7)$, a 2D CNN and attention model-based one, respectively. Coupled with the custom encoder's ability to learn a suitable latent space, as well as the more sophisticated residual learner $W^e$ and synthesizer $W^\u2295$ functions, TF-Codec achieves high-quality speech reconstruction at very low bitrates, e.g., 1 kbps."}, {"title": "5.3 MDCTNet", "content": "MDCTNet is another predictive method that works in the MDCT-transformed domain, making it compatible with the existing model-based audio codecs [38]. The overall architecture is more similar to the decoder-only neural codecs, such as the WaveNet speech codec [17] or LPCNet [20], than the above-mentioned predictive codecs, in the sense that prediction is not to compute the residual signal. Instead, the generative MDCTNet operates only on the decoder side to estimate the MDCT coefficients directly.\nFigure 6 illustrates the simplified MDCTNet codec architecture. On the encoder side, the input frames are transformed into a series of MDCT coefficient vectors, which are perceptually weighted via a psychoacoustic model and then quantized into the bitstream. The receiver takes the dequantized coefficient vectors $h_t$, but instead of transforming them back to the time domain directly, it only uses them to condition the generative MDCTNet, which consists of three prediction networks. First, a temporal prediction model (a two-layer GRU module) performs per-band temporal prediction out of the past time frames,\n$\\hat{h_{b,t}} \u2190 W^{time}(h_{b,t-1};h_t)$,\nwhere $h_{b,t}$ stands for the predicted MDCT coefficient at time frame $t$ and frequency subband $f$. The GRU hidden states are omitted for notational brevity. Note that the model is conditioned with the quantized code vector $h_{bt}$, which improves the prediction accuracy via extra information.\nIn addition, MDCTNet also employs a cross-band prediction module, which is a CNN layer that takes past information from the adjacent subbands, i.e., $b - 1$ and $b + 1$,\n$\\hat{h_{b,t}} \u2190 W^{cross}(h_{b-1,t-1}, \\hat{h_{b+1,t-1}};h_t)$.\nFinally, the frequency-domain GRU module predicts the higher subband from the lower ones,\n$\\hat{h_{b,t}} \u2190 W^{freq}(\\hat{h_{b-1,t}}; h_t, \\hat{h_{b,t}})$.\nwhere the frequency predictor is conditioned with the temporal prediction $h_{bt}$ achieved so far.\nThe MDCTNet codec also employs various other techniques that make the system more robust,"}, {"title": "6. Psychoacoustic Models for Perceptual Loss Functions", "content": "A fundamental issue with the data-driven approach to coding is that, during training, the quality of the decoded signals can be measured only in an objective way, e.g., using signal-to-noise ratio (SNR), etc. This is due to the nature of the gradient-based optimization algorithm that relies on the differentiation of the loss function with respect to the model parameters. Oftentimes, those objective metrics reflect only a certain aspect of the perceptual quality of the signal, leaving a gap between the perceived quality of the codec's output signal and its loss value, i.e., a signal with a high objective score does not necessarily sound good to human ears. This kind of issue can be more sophisticated if the codec has to maintain subtlety during its processing, such as for music signals with high fidelity."}, {"title": "6.1 Psychoacoustic Calibration of Loss Functions", "content": "Likewise, psychoacoustic models (PAM) have been the crucial component of many traditional audio coding systems. Since PAM's main usage is to make the system's behavior closer to human perception, a natural way to harmonize the concept with a data-driven method is to use it to redefine the training objectives.\nIn [39], two different approaches to psychoacoustic calibration were proposed to improve the loss function's relevance to human perception. In their first proposed loss function, priority weighting, the reconstruction loss is defined as a sum of subband-specific losses, each of which is weighted by the perceptual importance:\n$L_{pw}(t) = \\sum_{f} w_f (X_f - \\hat{X_f})^2$,\nwhere $X_f$ and $\\hat{X_f}$ are the magnitude of the Fourier spectrum at subband $f$ and its reconstruction, respectively. The frame index $t$ is omitted from the equation for brevity. Since the loss is a weighted sum of subband-specific reconstruction loss, the weights $w_f$ play a big role. For a given input time domain frame $x$ and its logarithmic PSD $Y$, the perceptual weight vector $w$ is defined by\n$w_f = log_{10} (\\frac{10^{0.1Y_f}}{10^{0.1M_f}} + 1)$"}, {"title": "7. Conclusion", "content": "In this paper, recent neural speech and audio coding systems were presented as an example of successful harmonization of model-based and data-driven approaches. Various model-based approaches have been proposed and commercialized in the past few decades, which an entirely data-driven approach cannot easily catch up with due to the highly subjective evaluation process of the speech and audio codecs. Carefully designed hybrid systems, which also tend to benefit from a sufficiently large architecture, can be an alternative, introducing sensible coding gain to the already-saturated conventional codecs' performance. The paper first introduced a neural network-based signal enhancer as a post-processor of existing codecs. CMRL and LPCNet were another useful type of hybrid systems, where LPC was harmonized with the neural network-based end-to-end codec and vocoder, respectively. The paper also explored predictive models that work either in the custom feature space or pre-defined transform domain. Finally, we saw that psychoacoustic models can be effectively used in the data-driven training paradigms by improving the perceptual relevance of the"}]}