{"title": "A Multi-Granularity Supervised Contrastive Framework for Remaining Useful Life Prediction of Aero-engines", "authors": ["Zixuan He", "Ziqian Kong", "Zhengyu Chen", "Yuling Zhan", "Zijun Que", "Zhengguo Xu"], "abstract": "Accurate remaining useful life (RUL) predictions are critical to the safe operation of aero-engines. Currently, the RUL prediction task is mainly a regression paradigm with only mean square error as the loss function and lacks research on feature space structure, the latter of which has shown excellent performance in a large number of studies. This paper develops a multi-granularity supervised contrastive (MGSC) framework from plain intuition that samples with the same RUL label should be aligned in the feature space, and address the problems of too large minibatch size and unbalanced samples in the implementation. The RUL prediction with MGSC is implemented on using the proposed multi-phase training strategy. This paper also demonstrates a simple and scalable basic network structure and validates the proposed MGSC strategy on the CMPASS dataset using a convolutional long short-term memory network as a baseline, which effectively improves the accuracy of RUL prediction.", "sections": [{"title": "I. INTRODUCTION", "content": "Aero-engines gradually experience fatigue and degradation over long periods, leading to a decline in performance and reliability. Therefore, accurate remaining useful life (RUL) predictions are important for timely maintenance and safety improvement.\nThe current RUL prediction approaches can be categorized into two main groups: physical model-based approaches [1] and data-driven approaches [2]. Physical model-based approaches use the understanding and modeling of the physical behavior of a system to predict the RUL of a component or device. However, as the complexity of devices increases, it becomes increasingly difficult to build physical models. With the development of technologies such as sensors and the Internet of Things, it has become easier to acquire and save data, which has led to the growing popularity of data-driven RUL prediction approaches.\nData-driven approaches extract valid features directly from data and use them for RUL prediction without physical knowledge. These can be further categorized into shallow machine learning and deep learning. The former requires manual feature selection and feeding them into predictors such as support vector regressior [3] to infer RUL. On the contrary, the deep learning approaches adopt an end-to-end training strategy, which simultaneously realizes feature extraction and RUL reasoning by optimizing the loss function through stochastic gradient descent, and has great versatility and flexibility. The best-known deep learning approaches are convolutional neural networks (CNN) [4] and long short-term memory neural networks (LSTM) [5]."}, {"title": "II. PROPOSED MGSC FRAMEWORK", "content": "In this paper, the MGSC framework is developed to simultaneously address the conflict between batch size and sample balance as well as the need for feature space structuring in the aero-engine RUL prediction task. It consists of three main parts. The first part is the MGSC strategy to help the encoder network obtain more discriminative and generalizable feature representations with different hierarchical label information. The second part is the multi-phase training strategy to progressively regularize the target feature space, where the embeddings of the same RUL label are aligned. The other part is the base network structure with encoder, projector, and regression layers."}, {"title": "A. Multi-granularity Supervised Contrastive Strategy", "content": "The proposed MGSC strategy incorporates two supervised contrastive forms. For the coarse-grained contrast, the HS serves as the class label, capturing the overall condition of the system. Instead, the fine-grained contrast employs the RUL labels within subsets of each HS. This multi-granularity strategy allows the model to capture roughly high-level semantic information and low-level local variations. Notably, the RULs are real labels reflecting the remaining lifespan of the system, while the HS are pseudo-labels obtained through a segmentation mapping of the RULs, more details are in the III-C2.\nIn the coarse-grained contrast, the HS are used as class labels. This aims to use larger classification scales for supervised contrastive learning, which enables preliminary regularization of embeddings at smaller minibatch sizes compared to directly using RUL labels (units, i.e., the smallest classification scale). For instance, consider N objects, where each object has m samples. In a given object, contains d HS classes, with each HS class containing m/d samples. On the other hand, there are N RUL classes with each RUL class consisting of only one sample. The number of positive samples for both are respectively:\n$N_{HS} = m/d -1+ (N \u2212 1) * m/d$\n$N_{RUL} = 0+N-1$\nwhere usually d \u00ab m. Using HS as a class label produces more positive samples that are more likely to be sampled into a minibatch. Therefore, even a small batch size ensures that there are enough positive samples to participate in supervised contrastive training, avoiding the imbalance of positive-negative samples.\nThe coarse-grained contrastive strategy, as shown in Fig. 2a, drives to keep the embeddings of the same class of HSs close together, while the embeddings of different classes of HSs away from each other in the feature space. Furthermore, the strategy incorporates samples from both inter- and intra-wise engine views, resulting in a more diverse set of samples in the minibatch. This diversity aids in learning more discriminative and generalized representations.\nHowever, relying solely on coarse-grained contrasts based on HS labels results in disordered clusters. To ensure that the samples within each cluster can be aligned according to the RUL, as shown in Fig. 2b, a fine-grained supervised contrastive strategy using RUL labels is designed. In fine-grained contrast, each HS class is processed independently and its sample space is the sample set corresponding to that HS. Since the samples of a single HS class are limited, the minibatch size is completely acceptable and efficient for"}, {"title": "B. Multi-phase Training Strategy", "content": "Each granularity of MGSC has a different design purpose, as well as the respective minibatch size, so the parallel training paradigm is not used. Instead, this paper proposes a multi-phase training strategy to incorporate supervised contrastive learning with different multi-granularity and RUL regression prediction tasks in a sequential manner.\nIn a training epoch, coarse-grained supervised contrastive learning is performed first, followed by fine-grained supervised contrastive learning, and finally regression prediction. For coarse-grained contrast, it can roughly regularize embeddings at a large classification scale. Its loss function is:\n$\\mathcal{L}_{HS} = -\\frac{1}{|P(i)|} \\sum_{p \\in P(i)} log \\frac{exp(z_i \\cdot z_p / \\tau)}{\\sum_{a \\in A(i)} exp(z_i \\cdot z_a / \\tau)}$\nwhere zi = Proj(Enc(xi)). And I is the sample space and i is the index of a sample. Here, P(i) = {p \u2208 A(i) : Yp = Yi} is the set of indices of all positives in the minibatch distinct from i, and |P(i)| is its cardinality. A(i) = I\\{i} is the sample space without i-th sample.\nFor fine-grained contrast, it is able to achieve ordered alignment of embeddings within HS clusters at smaller scales. For the particular class HSk, its loss function is:\n$\\mathcal{L}_{RUL} = -\\frac{1}{|J_k|} \\sum_{j \\in J_k} \\frac{1}{|H_k(j)|} \\sum_{h \\in H_k(j)} log \\frac{exp(z_j \\cdot z_h / \\tau)}{\\sum_{b \\in B_k(j)} exp(z_j \\cdot z_b / \\tau)}$\nwhere Jk is the sample space of HSk and j is the index of a sample. Here, Hk(j) = {h \u2208 Bk(j) : Y = y} is the set of indices of all positives in the minibatch of HSk distinct from j, and |Hk(i)| is its cardinality. Bk(j) = Jk\\{j} is the sample space of HSk without j-th sample.\nFor regression prediction, the loss function is the MSE:\n$\\mathcal{L}_{reg} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{l}_i - l_i)^2$\nwhere n is the batch size, li is the actual RUL, and li is the predicted RUL."}, {"title": "C. Network Structure", "content": "For the proposed MGSC framework, its basic network structure is shown in Fig 3. It includes an encoder to extract the feature embeddings of the samples, a projector to obtain compact embedding representations and a regression layer to predict the RUL. The projector is used only when training, where its projection in the low-dimensional space is used for MGSC training, while the encoder embedding is used directly in regression layers when testing. The overall algorithm is illustrated in Algorithm 1."}, {"title": "III. DATA PREPROCESSING AND MODEL CONSTRUCTION", "content": "To assess the predictive performance of the proposed model, this research conducted experiments utilizing the CMAPSS simulation dataset provided by NASA, including four subsets. Each sub-dataset consists of a training set and a test set, as well as a set of RUL labels corresponding to the test set. The dataset contains 21 sensor sequence data and 3 operating parameters, which simulate the full life cycle failure degradation of the engine under different operating conditions and failure modes, as detailed in TABLE I."}, {"title": "B. Normalization and Sliding Time Windows", "content": "1) Normalization: Since there are large differences in order of magnitude between different sensor data, mapping them to similar scales through normalization ensures that they have the same importance during model training to avoid bias. The raw data is first normalized:\n$x_{norm}^{i,j} = \\frac{2(x^{i,j} - x_{min}^{i,j})}{x_{max}^{i,j} - x_{min}^{i,j}} -1 \\quad \\forall i, j$\nwhere $x^{i,j}$ represents the i-th raw data of the j-th sensor, $x_{norm}^{i,j}$ is the normalized value, and $x_{max}^{i,j}$ and $x_{min}^{i,j}$ are the maximum and minimum values of the j-th sensor, respectively.\n2) Sliding Time Windows: The sliding time window technique is applied to obtain sequence data for the sequence regression task. In this paper, we set the window width W=30 and moving step s=1. There is an overlap between the windows and each window corresponds to a RUL label. By this technique, sensor information over a period of time can be obtained, which helps in feature extraction in the temporal dimension."}, {"title": "C. Supervised Label Setting", "content": "1) RUL Regression Labels: In this paper, the normal operation phase of an aero-engine is defined as the period in which the remaining useful life (RUL) exceeds 125 units. During this phase, the engine experiences minimal performance degradation, and the RUL label is maintained at a constant value of 125. Subsequently, as the engine enters the degradation phase, the RUL label gradually decreases linearly from 125 to 0 with the operational time, as shown in Fig 4.\n2) Coarse-grained HS Labels: For the HS labels, we have defined different HSs based on the RUL values, as shown in Fig 4. HS0 is assigned when the RUL is equal to 125. HS1 is assigned when the RUL is between 100 and 124. HS2 is assigned in the range of 75 to 99. HS3 is assigned in the range of 50 to 74. HS4 is assigned in the range of 25 to 49. Finally, HS5 is assigned when the RUL is between 0 and 24.\n3) Fine-grained RUL Labels: In fine-grained supervised contrastive learning, its sample space is the subclasses divided in the overall sample space using HS labels. Its labels are the RUL labels corresponding to the samples within each HS class."}, {"title": "D. Model Construction", "content": "In this paper, the CNN-LSTM model has been used as a baseline to compare its RUL prediction performance with or without MGSC strategy training. A concrete implementation of the model construction is shown in Fig. 5. The encoder contains three 1D CNN layers for channel feature extraction and utilizes an LSTM module for temporal dimension feature aggregation. The regression layers contain three linear layers, where the output of the last linear layer is mapped to between 0 and 1 utilizing a sigmoid function for normalized RUL prediction. The projector uses two linear layers to project the embedding into a low-dimensional space for MGSC training, where the shallow linear layer uses a Relu activation function for nonlinear transformation [8]."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the predictive performance of the model, two commonly adopted metrics, namely root mean square error (RMSE) and scoring function (Score):\n$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\Delta_i^2}$ \n$Score = \\sum_{i=1}^{n} \\begin{cases}\n    (exp(-\\frac{\\Delta_i}{13})-1), & \\text{if } \\Delta_i \\leq 0, \\\\\n    (exp(\\frac{\\Delta_i}{10})-1), & \\text{if } \\Delta_i > 0.\n\\end{cases}$\nwhere $ \\Delta_i = l_i - \\hat{l}_i$ is the error between the predicted result and the ground truth."}, {"title": "B. Results", "content": "In this paper, the CNN-LSTM model, CNN model and LSTM model have been used as three baselines to compare the RUL prediction performance with or without MGSC strategy training. Notably, the CNN model is a direct replacement of the LSTM in CNN-LSTM with a linear layer and the LSTM model use multi-layer LSTM as encoder. Table II shows that for the CNN-LSTM model and the LSTM model, there are significant improvements in the RUL prediction performance for all four data subsets after using MGSC. For the CNN model, all of them improve except FD004 whose Score metric becomes higher. One of the reasons for the reduced performance may be because FD004 has the most complex operating conditions and failure modes, thus producing a negative contrastive effect. But in general, the proposed MGSC effectively improves the performance of the three baselines.\nFig. 6 shows the RUL prediction results for the four sub-datasets using the CNN-LSTM+MGSC model. The comparison reveals a high similarity between the predicted RUL and the actual RUL, especially in the re FD001 and FD003 datasets. This indicates that the proposed model is able to predict the RUL of each engine better without large bias.\nFig. 7-Fig. 9 shows the encoder feature space visualization results for the FD002-FD004 dataset with or without the MGSC strategy. The t-SNE results show that the encoder of the used MGSC strategy constitutes an ideal manifold structure in the feature space. For samples of different engines with the same or similar RUL labels, their feature embeddings are close to each other and aligned. This is consistent with our intuition that aligning feature embeddings with the same RUL labels in the feature space before the regression layer will aid in RUL prediction. Furthermore, due to the combined effect of the MSE loss function and the fine-grained contrast loss, there is a clear degradation trend in the feature space, which is good for visualizing and explaining the health state of the aero-engine."}, {"title": "V. CONCLUSION", "content": "In this paper, the research is carried out from the perspective of feature space regularization, and an MGSC strategy is proposed for the alignment of sample embeddings under the same RUL labels, which produces an ideal manifold in the feature space. Inputting this manifold into the regression layer greatly improves the prediction accuracy of RUL.\nIn MGSC, coarse-grained contrast with HS as class labels, and fine-grained contrast with RUL labels within each HS class as class labels are established, which achieve the interaction between the high-level overall features and low-level local features. Moreover, the issues of large minibatch size and imbalance of positive and negative samples in supervised contrastive learning are solved by different classification scales.\nThe application of the MGSC strategy to the RUL prediction task of an aero-engine is realized by the proposed multi-phase training framework and network structure.\nIn future work, we will continue to explore multi-granularity contrast learning methods as well as contrast methods that are more applicable to time series. Meanwhile, the MGSC framework will be used for more advanced RUL predictive model to validate the effects."}]}