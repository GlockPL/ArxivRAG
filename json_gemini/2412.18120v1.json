{"title": "Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm", "authors": ["Xiaoyang Hu", "Richard L. Lewis"], "abstract": "Cognitive tasks originally developed for humans are now increasingly used to study language models. While applying these tasks is often straightforward, interpreting their results can be challenging. In particular, when a model underperforms, it's often unclear whether this results from a limitation in the cognitive ability being tested or a failure to understand the task itself. A recent study argued that GPT 3.5's declining performance on 2-back and 3-back tasks reflects a working memory capacity limit similar to humans (Gong et al., 2024). By analyzing a range of open-source language models of varying performance levels on these tasks, we show that the poor performance instead reflects a limitation in task comprehension and task set maintenance. In addition, we push the best performing model to higher n values and experiment with alternative prompting strategies, before analyzing model attentions. Our larger aim is to contribute to the ongoing conversation around refining methodologies for the cognitive evaluation of language models.", "sections": [{"title": "Introduction", "content": "Psychologists rely on behavioral experiments to test hypotheses about cognitive constructs and processes. For these experiments to be valid, participants have to understand exactly what they are being asked to do. To that end, human study protocols often include detailed task instructions, demonstrations, and practice runs. When adapting these experiments for language models, ensuring task comprehension can be more challenging, given that these models are often more hesitant than humans to express uncertainty (Zhou et al., 2024).\n\nA recent study applied the n-back task (Figure 1) to GPT 3.5 and concluded from the model's poor 2-back and 3-back performance that it had a working memory capacity limit of approximately 3, appar-"}, {"title": "Background and Related Work", "content": "There has been a growing body of work running cognitive tasks on pre-trained language models. These efforts often aim to identify whether the models exhibit cognitive constructs or capabilities that are present in humans. Subjects of study include theory of mind (Strachan et al., 2024; Gandhi et al., 2024), analogical reasoning (Hu et al., 2023; Webb et al., 2023), cognitive biases (Binz and Schulz, 2023; Lampinen et al., 2024), and working memory capacity (Gong et al., 2024), among many others.\n\nSuch evaluations are susceptible to both over-claiming and underclaiming. On the one hand, false positives can result from training data contamination (Sainz et al., 2023), potentially compromising the validity of vignette-based assessments where models may produce memorized responses. On the other hand, underestimation of model capabilities can happen when we erroneously assume task comprehension, especially for smaller models (Hu and Frank, 2024). Prior studies have also investigated how well language models adhere to prompt instructions (Webson and Pavlick, 2022) and how their performance compares to that of humans (Webson et al., 2023). In light of other methodological challenges in cognitive evaluations of language models, such as prompt sensitivity and cultural biases, Ivanova 2023 outlines recommendations for best practices.\n\nVirtually any task, from routine text comprehension to complex problem solving, involves the creation of intermediate or partial results. Successful task completion requires that these results be maintained in a way that facilitates later access. In humans, this mechanism is known as working memory, one of the most studied constructs in psychology for over half a century (Miyake and Shah, 1999). This concept can be extended to transformer-based language models designed to process interdependent, serial information. In fact, the transformer architecture, particularly its attention mechanism where key-query matching drives retrieval (Vaswani et al., 2017), bears striking resemblance to cue-based parsing and retrieval models proposed in psycholinguistics (Lewis et al., 2006), making it a promising candidate for modeling human sentence processing.\n\nOne of the most salient and mysterious aspects of human working memory is its severely constrained capacity (Miller, 1956; Cowan, 2012). This capacity limit is often measured using the n-back task (Kirchner, 1958). To the best of our knowledge, Gong et al. 2024 was the first to apply the n-back task to a language model, specifically the GPT 3.5 TURBO variant of ChatGPT. They experimented with different prompting strategies, including those incorporating feedback and reasoning. As n increased from 1 to 3, they observed a sharp decline in model performance and concluded that ChatGPT had a working memory capacity limit of approximately 3."}, {"title": "Methods", "content": ""}, {"title": "Data and Prompts", "content": "We use the same dataset as Gong et al. 2024. For each n-back task, there are 50 trials in total. Each trial consists of a sequence of 24 letters. In exactly 8 random positions within each sequence, the letters are the same as those appearing n position(s) earlier.\n\nAfter each letter prompt, the models are instructed to answer \u201c[ current letter ] and [ letter n back ] are [ different / identical ]\u201d. This is de-"}, {"title": "Models", "content": "We use GPT 3.5 TURBO and open-source instruction-tuned models from the QWEN (Bai et al., 2023), LLAMA (Dubey et al., 2024), and GEMMA (Team et al., 2024) families. Each model is prompted recursively to complete the tasks. For"}, {"title": "Metrics", "content": "The n-back task requires continuously matching the current letter and the letter from n steps back to determine the correct label. However, compared to binary labels, the retrieved letters can tell us more about the models' understanding of the task. And since the correct label is almost always assigned given the correct retrieval, our analyses focus on the retrieval accuracies and the log probabilities of the retrieved letters.\n\nBut how can we be sure that a model has inferred the right task from the instruction? One hypothesis is that, despite being prompted to do the n-back task, the models might be following m-back instructions instead. To investigate this, we adopt counterfactual measures by providing n-back instructions and evaluating the accuracies and log probabilities of retrievals consistent with the m-back task. We also apply variants of these measures, which we will detail in later sections."}, {"title": "Experimental Results", "content": ""}, {"title": "Task Performance", "content": "We begin by comparing retrieval accuracies across models for all three tasks (Figure 2) and categorizing them into three performance tiers (Table 1):\n\n\u2022 T3 models achieve nearly perfect retrieval accuracy on 1-back trials, but their performance drops to around 20% or lower on 2-back and 3-back trials.\n\n\u2022 T2 models achieve nearly perfect retrieval accuracy on 1-back trials and around 50% and 40% on 2-back and 3-back trials, respectively.\n\n\u2022 T1 models achieve 100% retrieval accuracy on 1-back trials and over 80% on 2-back and"}, {"title": "Task Comprehension", "content": "To better understand the source of these performance disparities, we ask: are less successful models able to infer the task from the provided instructions and demonstrations? Moreover, are high-performing models relying more heavily on task cues from the instructions or demonstrations? To address these questions, we:\n\n1. Provide n-back task instructions and/or demonstrations.\n\n2. Present three continuations, each consistent with a different m-back task.\n\n3. Measure the average log probabilities of letters at retrieval positions for each trial.\n\nLet $P_m$ be the average m-back retrieval log probability given n-back instructions only. Let $P_{n,m}$ be the average m-back retrieval log probability given n-back instructions and demonstrations.\n\n1-back. Under 1-back instructions, $P_{1,1} > P_{1,2} > P_{1,3}$ across all models. The same is true when no task demonstrations are provided, with no significant difference between $P_{1,m}$ and $P'_{1m}$ for m = 1, 2, 3, as shown in Figure 3. Overall, this is unsurprising, given the near-perfect performance of all models on the 1-back trials.\n\n2-back. We analyze the representative model from each tier (Figure 4).\n\nT3. Under 2-back instructions, including with demonstrations, the model assigns 1-back continuations to be the most plausible, with both"}, {"title": "Task Set Maintenance", "content": "Each n-back trial presents a sequence of 24 letters. Successful task completion requires consistent adherence to the task instruction as more stimuli are presented. Here, we investigate whether language models show a progressive decline in their ability to produce n-back consistent responses over time. Previously, performance metrics were averaged across time steps for each trial; now, we average across trials for each time step.\n\nSpecifically, at each time step i in the n-back task, we measure the average accuracy of m-back consistent retrievals for each $m \\leq n$, given the model's own responses up to time step i-1. Denote the accuracy as $A_{n,.} (i, m)$.\n\n1-back. For 1-back trials, $A_{1,.} (i, 1)$ stays close to 1 for each model as i increases (not shown), which"}, {"title": "T1 Model Performance as N Increases", "content": "Given that the best model, LLAMA-3.1-70B-INSTRUCT, performs well for 1 through 3-back tasks, we would like to know how its performance might change for larger n's.\n\nFigure 9 shows that the retrieval accuracy gradually declines as n increases, although, even at n = 8,9,10, the model is still able to exactly retrieve the correct letters 75.25%, 66.08% and 57.1% of the times, which translates to task accuracies of 83.33%, 78.25% and 71.92%.\n\nIn addition, we measure $P_{n,m}$ for each n, m \u2208 {1, 2, 3, ..., 10}, as shown in Figure 10. We notice that $max_m P_{n,m} = P_{n,n}$ for 1 < n < 10. Moreover, $P_{n,m}$ tends to decrease symmetrically as m"}, {"title": "Curriculum Learning", "content": "The practice of training models on examples of increasing difficulty is known in machine learning as curriculum learning (Bengio et al., 2009). Here, we repeat the experiments from Section 4.4 with in-context curriculum learning to gradually familiarize the model with the task. Specifically, before prompting LLAMA 3.1 70B INSTRUCT to perform an n-back task, we provide demonstrations that include letter sequences and corresponding correct responses for tasks ranging from 1-back to n-back. As shown in Figure 9, this approach leads to significant improvements in performance for larger n values. The model achieves retrieval accuracies of 79.83%, 80.17% and 71.67% and task accuracies of 90.08%, 90.08% and 84.75%, for n = 8, 9, 10."}, {"title": "Interactive Demo", "content": "We explore an alternative prompting strategy that more closely mirrors human study paradigms. Af-"}, {"title": "Reciting N Most Recent Stimuli", "content": "We experiment with an alternative answer format that encodes task requirements in greater detail. For 2-back trials, models are instructed to answer \"current: [ current letter ], 1 back: [ letter 1 back ], 2 back: [ letter 2 back ]; current letter [ current letter ] and letter 2 back [ letter 2 back ] are [ different / identical ].\" For 3-back trials, they are"}, {"title": "Attention Analysis", "content": "Attentions in transformer-based language models reveal how much each generated token attends to every preceding token. We hypothesize that, for each retrieval, a more performant model should attend more to the source token from n steps back. This is precisely what we observe in the QWEN models. For each (trial, layer, head), we obtain the mean retrieval attention (MRAT) by averaging the attention each retrieval token gives to the correct source token. Indeed, compared to the 14B model, QWEN 2 72B INSTRUCT (T1) shows a higher concentration at the tail end of the MRAT distribution (Figure 12). Figure 13 shows the attention map with the highest MRAT, which almost exactly matches our hypothesized pattern. However, LLAMA models do not exhibit this pattern to the same degree. Attentions in LLAMA 3.1 models are much more diffuse; The maximum MRATS for LLAMA 3.1 8B INSTRUCT and LLAMA 3.1 70B INSTRUCT are 4.86% and 8.52%, respectively."}, {"title": "Conclusion", "content": "In this work, we apply the n-back task, a common working memory test, to a range of language models. We categorize the models into three performance tiers based on retrieval accuracy. Through analyzing a representative model from each tier, we identify three distinct levels of task understanding and task set maintenance capabilities, highlighting their confounding nature. We challenge the best model to perform 1 through 10-back tasks, noticing a gradual decline in performance and the benefit of in-context curriculum learning for larger"}, {"title": "Limitations", "content": ""}, {"title": "Prompt selection", "content": "Despite our careful selection of prompts and experimentation with various prompting strategies, the potential for more effective prompts or techniques to enhance task understanding remains. Future research could investigate systematic approaches to explore a wider range of prompts and their impact on task understanding and task set maintenance."}, {"title": "Mechanistic understanding", "content": "Another limitation is that we do not examine the internal model circuits that may be responsible for inferring and maintaining task sets. However, our experiments with the n-back paradigm provides a good starting point for future research. Causal interventions on"}]}