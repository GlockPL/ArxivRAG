{"title": "Large Language Model-Brained GUI Agents: A Survey", "authors": ["Chaoyun Zhang", "Shilin He", "Jiaxu Qian", "Bowen Li", "Liqun Li", "Si Qin", "Yu Kang", "Minghua Ma", "Guyue Liu", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "abstract": "Graphical User Interfaces (GUIs) have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. Traditionally, automating GUI interactions relied on script-based or rule-based approaches, which, while effective for fixed workflows, lacked the flexibility and adaptability required for dynamic, real-world applications. The advent of Large Language Models (LLMs), particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, task generalization, and visual processing. This has paved the way for a new generation of \"LLM-brained\" GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.\nTo provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address critical research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents. We anticipate that this survey will serve both as a practical cookbook for constructing LLM-powered GUI agents, and as a definitive reference for advancing research in this rapidly evolving domain.", "sections": [{"title": "INTRODUCTION", "content": "Graphical User Interfaces (GUIs) have been a cornerstone\nof human-computer interaction, fundamentally transforming\nhow users navigate and operate within digital systems [1].\nDesigned to make computing more intuitive and accessible,\nGUIs replaced command-line interfaces (CLIs) [2] with visually\ndriven, user-friendly environments. Through the use of icons,\nbuttons, windows, and menus, GUIs empowered a broader\nrange of users to interact with computers using simple actions\nsuch as clicks, typing, and gestures. This shift democratized\naccess to computing, allowing even non-technical users to\neffectively engage with complex systems. However, GUIs\noften sacrifice efficiency for usability, particularly in workflows\nrequiring repetitive or multi-step interactions, where CLIs can\nremain more streamlined [3].\nWhile GUIs revolutionized usability, their design, primar-\nily tailored for human visual interaction, poses significant\nchallenges for automation. The diversity, dynamism, and\nplatform-specific nature of GUI layouts make it difficult to\ndevelop flexible and intelligent automation tools capable of\nadapting to various environments. Early efforts to automate\nGUI interactions predominantly relied on script-based or\nrule-based methods [4], [5]. Although effective for prede-\nfined workflows, these methods were inherently narrow in\nscope, focusing primarily on tasks such as software testing\nand robotic process automation (RPA) [6]. Their rigidity\nrequired frequent manual updates to accommodate new tasks,\nchanges in GUI layouts, or evolving workflows, limiting their\nscalability and versatility. Moreover, these approaches lacked\nthe sophistication needed to support dynamic, human-like\ninteractions, thereby constraining their applicability in complex\nor unpredictable scenarios.\nThe rise of Large Language Models (LLMs)\u00b9 [8], [9],\nespecially those augmented with multimodal capabilities [10],\nhas drastically redefined the possibilities for GUI automation.\nBeginning with models like ChatGPT [11], LLMs have demon-\nstrated extraordinary proficiency in natural language under-\nstanding, code generation, and generalization across diverse"}, {"title": "1.1 Motivation for LLM-Brained GUI agents", "content": "With an LLM serving as its \"brain\u201d, LLM-powered GUI\nautomation introduces a new class of intelligent agents\ncapable of interpreting a user's natural language requests,\nanalyzing GUI screens and their elements, and autonomously\nexecuting appropriate actions. Importantly, these capabilities\nare achieved without reliance on complex, platform-specific\nscripts or predefined workflows. These agents, referred to as\n\"LLM-brained GUI agents\", can be formally defined as:\nIntelligent agents that operate within GUI environ-\nments, leveraging LLMs as their core inference and\ncognitive engine to generate, plan, and execute\nactions in a flexible and adaptive manner.\nThis paradigm represents a transformative leap in GUI\nautomation, fostering dynamic, human-like interactions across\ndiverse platforms. It enables the creation of intelligent, adap-"}, {"title": "1.2 Scope of the Survey", "content": "To address this gap, this paper provides a pioneering, com-\nprehensive survey of LLM-brained GUI agents. We cover\nthe historical evolution of GUI agents, provide a step-by-\nstep guide to building these agents, summarize essential\nand advanced techniques, review notable tools and research\nrelated to frameworks, data and models, showcase represen-\ntative applications, and outline future directions. Specifically,\nthis survey aims to answer the following research questions\n(RQs):\n1) RQ1: What is the historical development trajectory of\nLLM-powered GUI agents? (Section 4)\n2) RQ2: What are the essential components and advanced\ntechnologies that form the foundation of LLM-brained\nGUI agents? (Section 5)\n3) RQ3: What are the principal frameworks for LLM GUI\nagents, and what are their defining characteristics?\n(Section 6)\n4) RQ4: What are the existing datasets, and how can\ncomprehensive datasets be collected to train optimized\nLLMs for GUI agents? (Section 7)\n5) RQ5: How can the collected data be used to train\npurpose-built Large Action Models (LAMs) for GUI\nagents, and what are the current leading models in the\nfield? (Section 8)\n6) RQ6: What metrics and benchmarks are used to eval-\nuate the capability and performance of GUI agents?\n(Section 9)\n7) RQ7: What are the most significant real-world applica-\ntions of LLM-powered GUI agents, and how have they\nbeen adapted for practical use? (Section 10)\n8) RQ8: What are the major challenges, limitations, and\nfuture research directions for developing robust and\nintelligent GUI agents? (Section 11)\nThrough these questions, this survey aims to provide a\ncomprehensive overview of the current state of the field, offer\na guide for building LLM-brained GUI agents, identify key\nresearch gaps, and propose directions for future work. This\nsurvey is one of the pioneers to systematically examine the\ndomain of LLM-brained GUI agents, integrating perspectives\nfrom LLM advancements, GUI automation, and human-\ncomputer interaction."}, {"title": "1.3 Survey Structure", "content": "The survey is organized as follows, with a structural illustration\nprovided in Figure 2. Section 2 reviews related survey\nand review literature on LLM agents and GUI automation.\nSection 3 provides preliminary background on LLMs, LLM\nagents, and GUI automation. Section 2 traces the evolution\nof LLM-powered GUI agents. Section 5 introduces key\ncomponents and advanced technologies within LLM-powered\nGUI agents, serving as a comprehensive guide. Section 6\npresents representative frameworks for LLM-powered GUI\nagents. Section 7 discusses dataset collection and related\ndata-centric research for optimizing LLMs in GUI agent.\nSection 8 covers foundational and optimized models for GUI\nagents. Section 9 outlines evaluation metrics and benchmarks.\nSection 10 explores real-world applications and use cases.\nFinally, Section 11 examines current limitations, challenges,\nand potential future directions, and section 12 conclude this\nsurvey. For clarity, a list of abbreviations is provided in Table 1."}, {"title": "2 RELATED WORK", "content": "The integration of LLMs with GUI agents is an emerging and\nrapidly growing field of research. Several related surveys\nand tutorials provide foundational insights and guidance.\nWe provide a brief review of existing overview articles on\nGUI automation and LLM agents, as these topics closely\nrelate to and inform our research focus. To begin, we provide\nan overview of representative surveys and books on GUI\nautomation, LLM agents, and their integration, as summarized\nin Table 2. These works either directly tackle one or two core\nareas in GUI automation and LLM-driven agents, or provide\nvaluable insights that, while not directly addressing the topic,\ncontribute indirectly to advancing the field."}, {"title": "2.1 Survey on GUI Automation", "content": "GUI automation has a long history and wide applications in\nindustry, especially in GUI testing [25]\u2013[27] and RPA [6] for\ntask automation [40].\nSaid et al., [28] provide an overview of GUI testing for\nmobile applications, covering objectives, approaches, and"}, {"title": "2.2 Surveys on LLM Agents", "content": "The advent of LLMs has significantly enhanced the capabili-\nties of intelligent agents [41], enabling them to tackle complex\ntasks previously out of reach, particularly those involving\nnatural language understanding and code generation [42].\nThis advancement has spurred substantial research into LLM-\nbased agents designed for a wide array of applications [43].\nBoth Xie et al., [44] and Wang et al., [45] offer compre-\nhensive surveys on LLM-powered agents, covering essential\nbackground information, detailed component breakdowns,\ntaxonomies, and various applications. These surveys serve\nas valuable references for a foundational understanding\nof LLM-driven agents, laying the groundwork for further\nexploration into LLM-based GUI agents. Xie et al., [54]\nprovide an extensive overview of multimodal agents, which\ncan process images, videos, and audio in addition to text.\nThis multimodal capability significantly broadens the scope\nbeyond traditional text-based agents [55]. Notably, most GUI\nagents fall under this category, as they rely on image inputs,\nsuch as screenshots, to interpret and interact with graphical\ninterfaces effectively. Multi-agent frameworks are frequently\nemployed in the design of GUI agents to enhance their\ncapabilities and scalability. Surveys by Guo et al., [46] and\nHan et al., [47] provide comprehensive overviews of the\ncurrent landscape, challenges, and future directions in this\narea. Sun et al., [48] provide an overview of recent methods\nthat leverage reinforcement learning to strengthen multi-agent\nLLM systems, opening new pathways for enhancing their\ncapabilities and adaptability. These surveys offer valuable\ninsights and guidance for designing effective multi-agent\nsystems within GUI agent frameworks.\nIn the realm of digital environments, Wu et al., [56]\npresents a survey on LLM agents operating in mobile en-\nvironments, covering key aspects of mobile GUI agents. In\na boarder scope, Wang et al., [57] present a survey on the\nintegration of foundation models with GUI agents. Another\nsurvey by Gao et al., provides an overview of autonomous\nagents operating across various digital platforms [58], high-\nlighting their capabilities, challenges, and applications. All\nthese surveys highlighting emerging trends in this area.\nRegarding individual components within LLM agents,\nseveral surveys provide detailed insights that are especially\nrelevant for GUI agents. Huang et al., [49] examine planning\nmechanisms in LLM agents, which are essential for executing\nlong-term tasks\u2014a frequent requirement in GUI automation."}, {"title": "3 BACKGROUND", "content": "The development of LLM-brained GUI agents is grounded\nin three major advancements: (i) large language models\n(LLMs) [8], which bring advanced capabilities in natural\nlanguage understanding and code generation, forming the\ncore intelligence of these agents; (ii) accompanying agent\narchitectures and tools [45] that extend LLM capabilities,\nbridging the gap between language models and physical envi-\nronments to enable tangible impacts; and (iii) GUI automation\n[60], which has cultivated a robust set of tools, models, and\nmethodologies essential for GUI agent functionality. Each of\nthese components has played a critical role in the emergence\nof LLM-powered GUI agents. In the following subsections, we\nprovide a brief overview of these areas to set the stage for\nour discussion."}, {"title": "3.1 Large Language Models: Foundations and Capabili-ties", "content": "The study of language models has a long and rich history\n[61], beginning with early statistical language models [62] and\nsmaller neural network architectures [63]. Building on these\nfoundational concepts, recent advancements have focused on\ntransformer-based LLMs, such as the Generative Pre-trained\nTransformers (GPTs) [64]. These models are pretrained on\nextensive text corpora and feature significantly larger model"}, {"title": "3.2 LLM Agents: From Language to Action", "content": "Traditional Al agents have often focused on enhancing specific\ncapabilities, such as symbolic reasoning or excelling in\nparticular tasks like Go or Chess. In contrast, the emer-\ngence of LLMs has transformed Al agents by providing\nthem with a natural language interface, enabling human-like\ndecision-making capabilities, and equipping them to perform\na wide variety of tasks and take tangible actions in diverse\nenvironments [12], [45], [91], [92]. In LLM agents, if LLMs\nform the \u201cbrain\u201d of a GUI agent, then its accompanying\ncomponents serve as its \u201ceyes and hands\u201d, enabling the\nLLM to perceive the environment's status and translate its\ntextual output into actionable steps that generate tangible\neffects [44]. These components transform LLMs from passive\ninformation sources into interactive agents that execute tasks\non behalf of users, which redefine the role of LLMs from\npurely text-generative models to systems capable of driving\nactions and achieving specific goals.\nIn the context of GUI agents, the agent typically perceives\nthe GUI status through screenshots and widget trees [93],\nthen performs actions to mimic user operations (e.g., mouse\nclicks, keyboard inputs, touch gestures on phones) within\nthe environment. Since tasks can be long-term, effective\nplanning and task decomposition are often required, posing\nunique challenges. Consequently, an LLM-powered GUI\nagent usually possess multimodal capabilities [54], a robust\nplanning system [49], a memory mechanism to analyze\nhistorical interactions [50], and a specialized toolkit to interact\nwith its environment [25]. We will discuss these tailored\ndesigns for GUI agents in detail in Section 5."}, {"title": "3.3 GUI Automation: Tools, Techniques, and Challenges", "content": "GUI automation has been a critical area of research and\napplication since the early days of GUIs in computing. Initially\ndeveloped to improve software testing efficiency, GUI automa-\ntion focused on simulating user actions, such as clicks, text\ninput, and navigation, across graphical applications to validate\nfunctionality [28]. Early GUI automation tools were designed\nto execute repetitive test cases on static interfaces [26].\nThese approaches streamlined quality assurance processes,\nensuring consistency and reducing manual testing time. As\nthe demand for digital solutions has grown, GUI automation\nhas expanded beyond testing to other applications, including\nRPA [6] and Human-Computer Interaction (HCI) [94]. RPA\nleverages GUI automation to replicate human actions in\nbusiness workflows, automating routine tasks to improve\noperational efficiency. Similarly, HCI research employs GUI\nautomation to simulate user behaviors, enabling usability as-\nsessments and interaction studies. In both cases, automation\nhas significantly enhanced productivity and user experience"}, {"title": "4 EVOLUTION AND PROGRESSION OF LLM-BRAINED GUI AGENTS", "content": "\"Rome wasn't built in a day.\u201d The development of LLM-brained\nGUI agents has been a gradual journey, grounded in decades\nof research and technical progress. Beginning with simple\nGUI testing scripts and rule-based automation frameworks,\nthe field has evolved significantly through the integration\nof machine learning techniques, creating more intelligent\nand adaptive systems. The introduction of LLMs, especially\nmultimodal models, has transformed GUI automation by\nenabling natural language interactions and fundamentally\nreshaping how users interact with software applications.\nAs illustrated in Figure 3, prior to 2023 and the emergence\nof LLMs, work on GUI agents was limited in both scope\nand capability. Since then, the proliferation of LLM-based\napproaches has fostered numerous notable developments\nacross platforms including web, mobile, and desktop environ-\nments. This surge is ongoing and continues to drive innovation\nin the field. This section takes you on a journey tracing the\nevolution of GUI agents, emphasizing key milestones that\nhave brought the field to its present state."}, {"title": "4.1 Early Automation Systems", "content": "In the initial stages of GUI automation, researchers relied\non random-based, rule-based, and script-based strategies.\nWhile foundational, these methods had notable limitations in\nterms of flexibility and adaptability."}, {"title": "4.1.1 Random-Based Automation", "content": "Random-based automation uses random sequences of ac-\ntions within the GUI without relying on specific algorithms or\nstructured models using monkey test [114]. This approach\nwas widely used in GUI testing to uncover potential issues by\nexploring unpredictable input sequences [115]. While effective\nat identifying edge cases and bugs, random-based methods\nwere often inefficient due to a high number of redundant or\nirrelevant trials."}, {"title": "4.1.2 Rule-Based Automation", "content": "Rule-based automation applies predefined rules and logic to\nautomate tasks. In 2001, Memon et al., [116] introduced a\nplanning approach that generated GUI test cases by trans-\nforming initial states to goal states through a series of pre-\ndefined operators. Hellmann et al., [4] (2011) demonstrated\nthe potential of rule-based approaches in exploratory testing,\nenhancing bug detection. In the RPA domain, SmartRPA [117]\n(2020) used rule-based processing to automate routine tasks,\nillustrating the utility of rules for streamlining structured\nprocesses."}, {"title": "4.1.3 Script-Based Automation", "content": "Script-based automation relies on detailed scripts to manage\nGUI interactions. Tools like jRapture [5] (2000) record and re-\nplay Java-based GUI sequences using Java binaries and the\nJVM, enabling consistent execution by precisely reproducing\ninput sequences. Similarly, DART [118] (2003) automated\nthe GUI testing lifecycle, from structural analysis to test\ncase generation and execution, offering a comprehensive\nframework for regression testing."}, {"title": "4.1.4 Tools and Software", "content": "A range of software tools were developed for GUI testing\nand business process automation during this period. Mi-\ncrosoft Power Automate [119] (2019) provides a low-code/no-\ncode environment for creating automated workflows within\nMicrosoft applications. Selenium [120] (2004) supports cross-\nbrowser web testing, while Appium [121] (2012) facilitates mo-\nbile Ul automation. Commercial tools like TestComplete [122]\n(1999), Katalon Studio [123] (2015), and Ranorex [124] (2007)\nallow users to create automated tests with cross-platform\ncapabilities.\nAlthough these early systems were effective for automat-\ning specific, predefined workflows, they lacked flexibility and\nrequired manual scripting or rule-based logic. Nonetheless,\nthey established the foundations of GUI automation, upon\nwhich more intelligent systems were built."}, {"title": "4.2 The Shift Towards Intelligent Agents", "content": "The incorporation of machine learning marked a major shift\ntowards more adaptable and capable GUI agents. Early\nmilestones in this phase included advancements in machine\nlearning, natural language processing, computer vision, and\nreinforcement learning applied to GUI tasks."}, {"title": "4.2.1 Machine Learning and Computer Vision", "content": "RoScript [97] (2020) was a pioneering system that introduced\na non-intrusive robotic testing system for touchscreen ap-\nplications, expanding GUI automation to diverse platforms.\nAppFlow [125] (2018) used machine learning to recognize\ncommon screens and UI components, enabling modular\ntesting for broad categories of applications. Progress in\ncomputer vision also enabled significant advances in GUI\ntesting, with frameworks [104] (2010) automating visual\ninteraction tasks. Humanoid [126] (2019) uses a deep neural\nnetwork model trained on human interaction traces within"}, {"title": "4.2.2 Natural Language Processing", "content": "Natural language processing capabilities introduced a new\ndimension to GUI automation. Systems like RUSS [129]\n(2021) and FLIN [130] (2020) allowed users to control\nGUIs through natural language commands, bridging human\nlanguage and machine actions. Datasets, such as those\nin [131] (2020), further advanced the field by mapping\nnatural language instructions to mobile Ul actions, opening\nup broader applications in GUI control. However, these\napproaches are limited to handling simple natural commands\nand are not equipped to manage long-term tasks."}, {"title": "4.2.3 Reinforcement Learning", "content": "The development of environments like World of Bits (WoB)\n[132] (2017) enabled the training of web-based agents using\nreinforcement learning (RL). Workflow-guided exploration\n[133] (2018) improved RL efficiency and task performance.\nDQT [134] (2024) applied deep reinforcement learning to"}, {"title": "4.3 The Advent of LLM-Brained GUI Agents", "content": "The introduction of LLMs, particularly multimodal models like\nGPT-40 [80] (2023), has radically transformed GUI automation\nby allowing intuitive interactions through natural language.\nUnlike previous approaches that required integration of\nseparate modules, LLMs provide an end-to-end solution for\nGUI automation, offering advanced capabilities in natural\nlanguage understanding, visual recognition, and reasoning.\nLLMs present several unique advantages for GUI agents,\nincluding natural language understanding, multimodal pro-\ncessing, planning, and generalization. These features make\nLLMs and GUI agents a powerful combination. While there\nwere earlier explorations, 2023 marked a pivotal year for\nLLM-powered GUI agents, with significant developments\nacross various platforms such as web, mobile, and desktop\napplications."}, {"title": "4.3.1 Web Domain", "content": "The initial application of LLMs in GUI automation was within\nthe web domain, with early studies establishing benchmark\ndatasets and environments [132], [136]. A key milestone was\nWebAgent [139] (2023), which, alongside WebGUM [140]\n(2023), pioneered real-world web navigation using LLMs.\nThese advancements paved the way for further develop\nments [15], [141], [142], utilizing more specialized LLMs to\nenhance web-based interactions."}, {"title": "4.3.2 Mobile Devices", "content": "The integration of LLMs into mobile devices began with\nAutoDroid [143] (2023), which combined LLMs with domain-\nspecific knowledge for smartphone automation. Additional\ncontributions like MM-Navigator [144] (2023), AppAgent [16]\n(2023), and Mobile-Agent [145] (2023) enabled refined control\nover smartphone applications. Research has continued to\nimprove accuracy for mobile GUI automation through model\nfine-tuning [146], [147] (2024)."}, {"title": "4.3.3 Computer Systems", "content": "For desktop applications, UFO [17] (2024) was one of the\nfirst systems to leverage GPT-4 with visual capabilities to\nfulfill user commands in Windows environments. Cradle [148]\n(2024) extended these capabilities to software applications\nand games, while Wu et al., [149] (2024) provided interaction\nacross diverse desktop applications, including web browsers,\ncode terminals, and multimedia tools."}, {"title": "4.3.4 Industry Models", "content": "In industry, the Claude 3.5 Sonnet model [150] (2024)\nintroduced a \u201ccomputer use\u201d feature capable of interacting\nwith desktop environments through Ul operations [151].\nThis signifies the growing recognition of LLM-powered GUI\nagents as a valuable application in industry, with stakeholders\nincreasingly investing in this technology.\nUndoubtedly, LLMs have introduced new paradigms and\nincreased the intelligence of GUI agents in ways that were\npreviously unattainable. As the field continues to evolve, we\nanticipate a wave of commercialization, leading to transfor-\nmative changes in user interaction with GUI applications."}, {"title": "5 LLM-BRAINED GUI AGENTS: FOUNDATIONSAND DESIGN", "content": "In essence, LLM-brained GUI agents are designed to process\nuser instructions or requests given in natural language,\ninterpret the current state of the GUI through screenshots\nor Ul element trees, and execute actions that simulate\nhuman interaction across various software interfaces [17].\nThese agents harness the sophisticated natural language\nunderstanding, reasoning, and generative capabilities of\nLLMs to accurately comprehend user intent, assess the GUI\ncontext, and autonomously engage with applications across\ndiverse environments, thereby enabling the completion of\ncomplex, multi-step tasks. This integration allows them to\nseamlessly interpret and respond to user requests, bringing\nadaptability and intelligence to GUI automation.\nAs a specialized type of LLM agent, most current GUI\nagents adopt a similar foundational framework, integrating\ncore components such as planning, memory, tool usage,\nand advanced enhancements like multi-agent collaboration,\namong others [45]. However, each component must be\ntailored to meet the specific objectives of GUI agents to en-\nsure adaptability and functionality across various application\nenvironments.\nIn the following sections, we provide an in-depth overview\nof each component, offering a practical guide and tutorial\non building an LLM-powered GUI agent from the ground up.\nThis comprehensive breakdown serves as a cookbook for\ncreating effective and intelligent GUI automation systems that\nleverage the capabilities of LLMs."}, {"title": "5.1 Architecture and Workflow In a Nutshell", "content": "In Figure 4, we present the architecture of an LLM-brained\nGUI agent, showcasing the sequence of operations from\nuser input to task completion. The architecture comprises\nseveral integrated components, each contributing to the\nagent's ability to interpret and execute tasks based on user-\nprovided natural language instructions. Upon receiving a user\nrequest, the agent follows a systematic workflow that includes\nenvironment perception, prompt engineering, model inference,\naction execution, and continuous memory utilization until the\ntask is fully completed.\nIn general, it consists of the following components:\n1) Operating Environment: The environment defines\nthe operational context for the agent, encompassing\nplatforms such as mobile devices, web browsers, and\ndesktop operating systems like Windows. To interact"}, {"title": "5.2 Operating Environment", "content": "The operating environment for LLM-powered GUI agents\nencompasses various platforms, such as mobile, web, and\ndesktop operating systems, where these agents can interact\nwith graphical interfaces. Each platform has distinct charac-\nteristics that impact the way GUI agents perceive, interpret,\nand act within it. Examples of GUIs from each platform are"}, {"title": "5.2.1 Platform", "content": "GUI agents can interact with a wide range of platforms,\nincluding mobile devices, web applications, and computer\noperating systems like Windows. Each platform offers unique\ncapabilities and constraints for GUI automation, requiring\nagents to adapt their perception and interaction strategies\naccordingly.\n1) Mobile Platforms: Mobile devices operate within con-\nstrained screen real estate, rely heavily on touch interac-\ntions [154], and offer varied app architectures (e.g., native\nvs. hybrid apps). Mobile platforms often use accessibility\nframeworks, such as Android's Accessibility API4 [155]\nand iOS's VoiceOver Accessibility Inspector5, to expose\nstructured information about Ul elements. However, GUI\nagents must handle additional complexities in mobile\nenvironments, such as gesture recognition [156], app\nnavigation [157], and platform-specific constraints (e.g.,\nsecurity and privacy permissions) [158], [159].\n2) Web Platforms: Web applications provide a relatively\nstandardized interface, typically accessible through Hy-\npertext Markup Language (HTML) and Document Object\nModel (DOM) structures [160], [161]. GUI agents can\nleverage HTML attributes, such as element ID, class,\nand tag, to identify interactive components. Web environ-\nments also present dynamic content, responsive layouts,\nand asynchronous updates (e.g., AJAX requests) [162],\nrequiring agents to continuously assess the DOM and\nadapt their actions to changing interface elements.\n3) Computer Platforms: Computer OS platforms, such\nas Windows, offer full control over GUI interactions.\nAgents can utilize system-level automation APIs, such as\nWindows UI Automation [30], to obtain comprehensive\nUl element data, including type, label, position, and\nbounding box. These platforms often support a broader\nset of interaction types, mouse, keyboard, and complex\nmulti-window operations. These enable GUI agents to\nexecute intricate workflows. However, these systems also\nrequire sophisticated adaptation for diverse applications,\nranging from simple Uls to complex, multi-layered soft-\nware suites.\nIn summary, the diversity of platforms, spanning mobile, web,\nand desktop environments, enable GUI agents to deliver\nbroad automation capabilities, making them a generalized\nsolution adaptable across a unified framework. However,\neach platform presents unique characteristics and constraints\nat both the system and application levels, necessitating a\ntailored approach for effective integration. By considering\nthese platform-specific features, GUI agents can be optimized\nto address the distinctive requirements of each environment,"}, {"title": "5.2.2 Environment State Perception", "content": "Accurately perceiving the current state of the environment\nis essential for LLM-powered GUI agents, as it directly\ninforms their decision-making and action-planning processes.\nThis perception is enabled by gathering a combination of\nstructured data, such as widget trees, and unstructured data,\nlike screenshots, to capture a complete representation of\nthe interface and its components. In Table 3, we outline key\ntoolkits available for collecting GUI environment data across\nvarious platforms, and below we discuss their roles in detail:\n1) GUI Screenshots: Screenshots provide a visual snap-shot of the application, capturing the entire state of the GUI at a given moment. They offer agents a reference for layout, design, and visual content, which is crucial when structural details about Ul elements are either limited or unavailable. Visual elements like icons, images, and other graphical cues that may hold important context can be analyzed directly from screenshots. Many platforms have built-in tools to capture screenshots (e.g., Windows Snipping Tool7, macOS Screenshot Utility, and Android's MediaProjection API\u00ba), and screenshots can be enhanced with additional annotations, such as Set-of-Mark (SoM) highlights [163] or bounding boxes [164] around key Ul components, to streamline agent decisions. Figure 6 illustrates various screenshots of the VS Code GUI, including a clean version, as well as ones with SoM and bounding boxes that highlight actionable components, helping the agent focus on the most critical areas of the interface.\n2) Widget Trees: Widget trees present a hierarchical view of interface elements, providing structured data about the layout and relationships between components [165]. We show an example of a GUI and its widget tree in Figure 7. By accessing the widget tree, agents can identify attributes such as element type, label, role, and relationships within the interface, all of which are essential for contextual understanding. Tools like Windows UI Automation and macOS's Accessibility API10 provide structured views for desktop applications, while Android's Accessibility API and HTML DOM structures serve mobile and web platforms, respectively. This hierarchical data is indispensable for agents to map out logical interactions and make informed choices based on the Ul structure.\n3) UI Element Properties: Each Ul element in the inter-face contains specific properties, such as control type, label text, position, and bounding box dimensions, that help agents target the appropriate components. These properties are instrumental for agents to make decisions"}, {"title": "5.2.3 Environment Feedback", "content": "Effective feedback mechanisms are essential for GUI agents\nto assess the success of each action and make informed deci-\nsions for subsequent steps. Feedback can take several forms", "Update": "By comparing before-and-afterscreenshots", "170": ".", "Change": "After executing an action, agentscan detect modifications in the widget tree structure,such as the appearance or disappearance of elements,updates to element properties, or hierarchical shifts [171"}]}