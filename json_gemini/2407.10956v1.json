{"title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?", "authors": ["Ruisheng Cao", "Fangyu Lei", "Haoyuan Wu", "Jixuan Chen", "Yeqiao Fu", "Hongcheng Gao", "Xinzhuang Xiong", "Hanchong Zhang", "Yuchen Mao", "Wenjing Hu", "Tianbao Xie", "Hongshen Xu", "Danyang Zhang", "Sida Wang", "Ruoxi Sun", "Pengcheng Yin", "Caiming Xiong", "Ansong Ni", "Qian Liu", "Victor Zhong", "Lu Chen", "Kai Yu", "Tao Yu"], "abstract": "Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.", "sections": [{"title": "1 Introduction", "content": "Data science and engineering pipelines usually rely on professional data software systems such as BigQuery, dbt, and Airbyte to acquire, process, and orchestrate large-scale data. Utilizing these enterprise systems involves writing SQL and Python code, as well as frequent and repetitive graphical user interface (GUI) controls, which can be complex even for experienced data scientists and engineers. With rapid advances in large language models (LLMs) and vision language models (VLMs), LLM/VLM-based autonomous agents have the potential to automate these work-"}, {"title": "2 Executable Computer Environment of Spider2-V", "content": "In this section, we introduce the real-time executable computer environment of Spider2-V, which is built upon virtual machines (VMs) and adapted from OSWORLD [34]."}, {"title": "2.1 Task Definition", "content": "Generally, an autonomous data agent is modeled as a partially observable Markov decision process (POMDP). Given the current observation $o_t \\in O$ which includes a natural language instruction and a screenshot, accessibility tree (a11ytree), or their combination, an agent generates an executable action $a_t \\in A$. This action can be clicking on a certain pixel of the screen (CLICK(560, 200)), or writing code through keyboard (TYPE(\"ls -lh\")). The execution of $a_t$ results in a new state $s_{t+1} \\in S$ (e.g., the updated computer state) and a new partial observation $O_{t+1} \\in O$. The a11ytree is a text-style representation of the desktop environment, which describes the status, position, and text content of each element (e.g., windows, buttons, and input boxes). The interaction loop repeats until an action that marks termination (DONE or FAIL) is generated or the agent reaches the max number of steps. See App. D for more details about the observation space and action space."}, {"title": "2.2 Environment Setup", "content": "To ensure that an agent starts from a consistent initial state, we invoke a series of function calls based on a pre-stored virtual machine (VM) snapshot to reset the environment. These function calls vary among tasks. And we summarize 5 universal categories with their functionalities (see Figure 2), namely: 1) File Transfer: transfer files or project archives (either from local or cloud storage) into the VM; 2) Application Launch: open software on the desktop, e.g., Visual Studio Code and Chromium; 3) Remote API Calls: invoke tool-specific API calls for professional applications, especially those requiring authentic user accounts, to reset and configure cloud workspaces; 4) Script Execution: execute a shell script in VM to set up the initial state, e.g., run a Docker container to start a localhost webserver for Superset; 5) Playwright Automation: run web browser simulation with Playwright, e.g., sign into an account or click a specific button and redirect to the target web page."}, {"title": "2.3 Task-specific Evaluation", "content": "After the interaction terminates, we only have access to the open-ended resulting state of the computer. Thus, to measure whether the goal of each task is accomplished, we write task-specific functions to retrieve the desired result from the open-ended resulting state and return the success flag (0/1). In total, Spider2-V contains 170 initial state configurations and 151 evaluation scripts, respectively. And we classify all evaluation methods into 3 generic categories, also shown in Figure 3:\na) File-based comparison: this method finds and copies the target files from VM to the host, and resorts to file-type based metrics (e.g., .json, csv, etc.) to compare the specified aspect of the generated file with ground truth. Sometimes, the ground truth may be updated over time. In this case, we will fetch the latest labels from the Internet during evaluation.\nb) Information-based validation: this scheme is usually utilized to extract and check desired information from the computer. For example, in Figure 3(b), we want to confirm whether the time schedule of the data transportation is correctly configured in Airbyte. We can invoke Airbyte APIs to retrieve, or Chromium Playwright to locate the target value.\nc) Execution-based verification: to verify whether an expected goal is achieved, we may also need to first execute a complicated Shell script in the final VM. For example, in Figure 3(c), we manually trigger the target Airflow DAG 2 and check the eventual status through running logs."}, {"title": "3 Benchmark Construction", "content": "In this section, we introduce the general annotation pipeline, document warehouse construction, and dataset statistics for Spider2-V. For concrete examples, refer to App. F."}, {"title": "3.1 Annotation Pipeline", "content": "To construct tasks in different categories, we find that official tutorials of enterprise applications serve as an excellent starting point. The 6-step annotation pipeline is illustrated in Figure 4(a), and we elaborate it with a concrete and real example \u201cOrchestrate dbt Core jobs with Airflow and Cosmos\" 3:\n1) Collect tutorials: firstly, we find tutorials from official websites for each professional tool in Figure 5. In total, 10 annotators collected 217 source URLs. Note that these tutorials may utilize other professional software, e.g., MySQL. All involved professional tools are listed in App. B.\n2) Learn tutorials: the annotator selects one tutorial, learns and realizes it in the VM. After that, they can summarize key knowledge points from this tutorial. For example, in Figure 4(b), five key steps in integrating a dbt project into an Airflow task are extracted."}, {"title": "3.2 Document Warehouse", "content": "Even senior data scientists query official documentation of professional applications when completing a complicated data engineering task. To compensate for the deficiencies of the data agents in utilizing enterprise professional software (e.g., unaware of coding specifications or APIs), we build a document warehouse for Spider2-V. Concretely, we recursively crawl the web pages from the root websites of the professional applications in Figure 5. After pre-processing through heuristics (refer to App. C),"}, {"title": "4 Experiments and Analysis", "content": "In this section, we introduce the experiment settings, experimental results, and ablation study to assess the proficiency of current LLM or VLM based agents on Spider2-V benchmark."}, {"title": "4.1 Environment Settings", "content": "Agent baselines The baseline method includes 3 schemes in zero-shot prompt learning: 1) Set-of-Mark (SoM, [36]): following OSWORLD [34] and VisualWebArena [14], we adopt heuristic methods to retrieve coordinates of visible elements from a11ytree (a text-format observation type) and draw indexed bounding box for these elements on the screenshot. We further insert these indexes into the pruned a11ytree to enhance the alignment between screenshot and a11ytree. 2) Execution Feedback (EF, [28]): we append execution feedback messages of those actions which failed to be grounded in the environment due to unexpected errors. The two techniques mentioned above are elaborated in App. D.3.1. 3) Retrieval-Augmented Generation (RAG, [8]): we leverage the task instruction as the query vector, bge-large-en-v1.5 [33] as the embedding model, and LlamaIndex [18] framework as the retrieval to generate document context for each task example. Documents are pre-chunked into segments with maximum length 512 and tokens overlapping size 20. Top 4 segments are selected as additional context in the task prompt (detailed in App. G.3).\nLLMs and VLMS We experiment with state-of-the-art LLMs and VLMs, including open-source representatives such as Mixtral-8x7B [11] and Llama-3-70B [20], and closed-source ones including Qwen-Max [3], Gemini-Pro-1.5 [26], Claude-3-Opus [2] and GPT [1] families (GPT-40 and GPT-4V 5). With respect to the two open-source LLMs and QWen-Max, we utilize pure text-format a11ytree as the observation type on account of their incapability of image processing. For the remaining 4 VLMs which support vision input, we use aligned text and image (that is Set-of-Mark) as the observation type in main experiments. Unless otherwise specified, we set the temperature to 0.5 and top_p to 0.9, the history trajectory window size to 3, the maximum length of a11ytree to 5000 tokens, and the maximum output tokens to 1500 in each turn. Heuristically, we require the agent to complete the tasks within both 15 interaction turns and one hour, which suffices for most tasks 6."}, {"title": "4.2 Main Results", "content": "In Table 3, we compare performances of different LLMs and VLMs. All results above integrate techniques of both execution feedback (EF) and retrieval-augmented generation (RAG) in \u00a7 4.1. Accordingly, we can summarize that:\n1) Existing data agents are far from satisfactory in completing real-world data science and engineering tasks. Even state-of-the-art VLMs (GPT-40 and GPT-4V) perform terribly on Spider2-V, achieving at best 14.0% overall success rate. As for their strongest competitors,"}, {"title": "4.3 Analysis", "content": "In this section, we delve into different factors which influence the eventual success rates, and analyze the underlying logics. The following analyses are based on our agent baseline with VLM GPT-40 unless otherwise specified. Firstly, we split the overall results into different subsets in Table 4.\n1) Tasks with more inherent action steps are more difficult. Each task is associated with one verbose task instruction which gives a step-by-step guidance on how to complete it. We count the number of actions in the verbose instruction and split the entire task set into 3 difficulty levels: < 5 steps (Easy), 5 ~ 15 steps (Medium), and > 15 steps (Hard). Not surprisingly, as the number of intrinsic action steps increases, the average performance decreases significantly. And for those extremely tough tasks, existing VLM-based data agents can hardly accomplish the goal."}, {"title": "5 Related Work", "content": "Benchmarks for data science and engineering In the field of data science and engineering, several recent works propose novel benchmarks to evaluate the capabilities of LLM agents in manipulating Excel spreadsheets [16, 4], common data science libraries (e.g., SQL and pandas) [42, 15, 9, 40], machine learning [10] or software engineering [16] projects. They are usually confined to a single stage within the entire data pipeline, predominantly data processing and analysis, thus overlooking other stages such as data warehousing and orchestration from a broader perspective. Besides, like other coding-related datasets [38, 29, 41], they merely focus on the command line interface, neglecting the fact that enterprise software usually has rich graphical user interfaces (GUIs). And data scientists often combine code programming with intensive GUI operations to fulfill a data workflow. To this end, Spider2-V is proposed as the first-of-its-kind multimodal agent benchmark in the field of data science and engineering, which covers the entire data workflow and integrates visual interfaces.\nBenchmarks for multimodal agents Existing works on GUI interaction mainly encompass web navigation [27, 17, 39, 5, 14], mobile device [43, 44, 24, 25, 30], and computer desktop [34, 32, 7, 13]. One trend of recent advanced benchmarks is to provide an executable simulation environment. Multi-modal agents can explore and interact with this platform through keyboard, mouse, gesture and touch screen actions in a more realistic and complex scenario. However, previous literature mostly focuses on daily life applications (e.g., Web browser and calendar) [35, 23] or workflows of non-specialized business tasks [31]. Few works [6, 34, 31] investigate the capability of multimodal agents to manipulate enterprise-level software. GUIs of professional applications often contain abundant domain-specific terminologies (e.g., \u201cmaterialization\" in Dagster), which requires multimodal agents to understand the specialized knowledge. Spider2-V incorporates 20 professional tools into a real-time computer environment to test the proficiency of agents in data science and engineering. Furthermore, we supplement a large volume of documents for retrieval to compensate for deficiencies of agents in domain knowledge."}, {"title": "6 Conclusion", "content": "In this work, we propose Spider2-V, the first data science and engineering benchmark which integrates enterprise professional applications and supports intensive GUI operations besides code writing across the full data pipeline. It contains 494 tasks, involves 20 professional tools, and provides a real-time executable computer environment. The most advanced VLM (GPT-4V) still performs poorly on Spider2-V (achieving 14.0% success rate), rendering it a very challenging benchmark. Although current multimodal agents are still far from automating data workflows, Spider2-V presents an easily accessible benchmark and lays the foundation for future research."}]}