{"title": "Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?", "authors": ["Ruisheng Cao", "Fangyu Lei", "Haoyuan Wu", "Jixuan Chen", "Yeqiao Fu", "Hongcheng Gao", "Xinzhuang Xiong", "Hanchong Zhang", "Yuchen Mao", "Wenjing Hu", "Tianbao Xie", "Hongshen Xu", "Danyang Zhang", "Sida Wang", "Ruoxi Sun", "Pengcheng Yin", "Caiming Xiong", "Ansong Ni", "Qian Liu", "Victor Zhong", "Lu Chen", "Kai Yu", "Tao Yu"], "abstract": "Data science and engineering workflows often span multiple stages, from ware- housing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code genera- tion, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably auto- mate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.", "sections": [{"title": "Introduction", "content": "Data science and engineering pipelines usually rely on professional data software systems such as BigQuery, dbt, and Airbyte to acquire, process, and orchestrate large-scale data. Utilizing these enterprise systems involves writing SQL and Python code, as well as frequent and repetitive graphical user interface (GUI) controls, which can be complex even for experienced data scien- tists and engineers. With rapid advances in large language models (LLMs) and vision language models (VLMs), LLM/VLM-based autonomous agents have the potential to automate these work- flows "}, {"title": "Executable Computer Environment of Spider2-V", "content": "In this section, we introduce the real-time executable computer environment of Spider2-V, which is built upon virtual machines (VMs) and adapted from OSWORLD [34]."}, {"title": "Task Definition", "content": "Generally, an autonomous data agent is modeled as a partially observable Markov decision pro- cess (POMDP). Given the current observation $o_t \\in O$ which includes a natural language instruction and a screenshot, accessibility tree (a11ytree), or their combination, an agent generates an exe- cutable action $a_t \\in A$. This action can be clicking on a certain pixel of the screen (CLICK(560, 200)), or writing code through keyboard (TYPE(\"ls -lh\")). The execution of $a_t$ results in a new state $s_{t+1} \\in S$ (e.g., the updated computer state) and a new partial observation $O_{t+1} \\in O$. The allytree is a text-style representation of the desktop environment, which describes the status, position, and text content of each element (e.g., windows, buttons, and input boxes). The interaction loop repeats until an action that marks termination (DONE or FAIL) is generated or the agent reaches the max number of steps."}, {"title": "Environment Setup", "content": "To ensure that an agent starts from a consistent initial state, we invoke a series of function calls based on a pre-stored virtual machine (VM) snapshot to reset the environment. These function calls vary among tasks. And we summarize 5 universal categories with their functionalities, namely: 1) File Transfer: transfer files or project archives (either from local or cloud storage) into the VM; 2) Application Launch: open software on the desktop, e.g., Visual Studio Code and Chromium; 3) Remote API Calls: invoke tool-specific API calls for professional applications, especially those requiring authentic user accounts, to reset and configure cloud workspaces; 4) Script Execution: execute a shell script in VM to set up the initial state, e.g., run a Docker container to start a localhost webserver for Superset; 5) Playwright Automation: run web browser simulation with Playwright, e.g., sign into an account or click a specific button and redirect to the target web page."}, {"title": "Task-specific Evaluation", "content": "After the interaction terminates, we only have access to the open-ended resulting state of the computer. Thus, to measure whether the goal of each task is accomplished, we write task-specific functions to retrieve the desired result from the open-ended resulting state and return the success flag (0/1). In total, Spider2-V contains 170 initial state configurations and 151 evaluation scripts, respectively. And we classify all evaluation methods into 3 generic categories:\na) File-based comparison: this method finds and copies the target files from VM to the host, and resorts to file-type based metrics (e.g., .json, csv, etc.) to compare the specified aspect of the generated file with ground truth. Sometimes, the ground truth may be updated over time. In this case, we will fetch the latest labels from the Internet during evaluation.\nb) Information-based validation: this scheme is usually utilized to extract and check desired information from the computer.  We can invoke Airbyte APIs to retrieve, or Chromium Playwright to locate the target value.\nc) Execution-based verification: to verify whether an expected goal is achieved, we may also need to first execute a complicated Shell script in the final VM. For example, we manually trigger the target Airflow DAG 2 and check the eventual status through running logs."}, {"title": "Benchmark Construction", "content": "In this section, we introduce the general annotation pipeline, document warehouse construction, and dataset statistics for Spider2-V."}, {"title": "Annotation Pipeline", "content": "To construct tasks in different categories, we find that official tutorials of enterprise applications serve as an excellent starting point. The 6-step annotation pipeline is illustrated elaborating it with a concrete and real example \u201cOrchestrate dbt Core jobs with Airflow and Cosmos\u201d:\n1) Collect tutorials: firstly, we find tutorials from official websites for each professional tool. In total, 10 annotators collected 217 source URLs. Note that these tutorials may utilize other professional software, e.g., MySQL. All involved professional tools are listed in App. B.\n2) Learn tutorials: the annotator selects one tutorial, learns and realizes it in the VM. After that, they can summarize key knowledge points from this tutorial. "}, {"title": "Write instructions", "content": "since the chosen tutorial is extremely complicated, the annotator can select a few key points to construct the task instruction. we only select key steps to write two versions of instructions, abstract and verbose, indicating different levels of proficiency. Note that, to avoid potential data contamination and make the task more realistic, we ask the annotator to introduce at least two modifications to the raw tutorial. we a) replace the original \"my_simple_dbt_project\" into an open-source dbt project called \"jaffle-shop\" and b) add one extra requirement on the time schedule (10:00 a.m. daily)."}, {"title": "Write environment setup functions", "content": "the next step is to write initialization functions using operations defined in \u00a7 2.2. In the example above, we need to: a) Upload an unfinished Airflow project into the VM. b) Execute a Shell script to launch the web server (via Docker containers) for Airflow under the project folder. c) Open all relevant applications on the desktop to simulate real user scenarios. d) Use Playwright to auto-login to the default Airflow account."}, {"title": "Write task-specific evaluation functions", "content": "In this step, annotators are required to programmati- cally obtain results from the open-ended states of VM and assess whether the task is completed using methods in \u00a7 2.3. In this example, the evaluator contains: a) manually run the target Airflow DAG and verify the final status is \u201csuccess\"; b) using Airflow CLIs to retrieve details of the target Airflow DAG, and compare dbt sub-tasks, status and schedule with ground truth."}, {"title": "Cross-validate on VM", "content": "to ensure correctness, we go through strict cross-validation. Each annotated task is sent to two other annotators to check: a) whether the chosen task reflects a real-world use case; b) whether verbose instruction accurately fulfills the task and its requirements in the abstract instruction; c) whether the environment can be reset to the same state in different trials; d) whether the evaluation is robust when we exactly follow the verbose instruction or modify some inconsequential steps; e) whether the evaluation score is 0 if we deliberately make some mistakes (red-teaming). The task is preserved only if it withstands all these tests.\nOn average, the annotation of one task (including cross-validation) costs roughly 4 hours."}, {"title": "Document Warehouse", "content": "Even senior data scientists query official documentation of professional applications when completing a complicated data engineering task. To compensate for the deficiencies of the data agents in utilizing enterprise professional software (e.g., unaware of coding specifications or APIs), we build a document warehouse for Spider2-V. Concretely, we recursively crawl the web pages from the root websites of the professional applications After pre-processing through heuristics"}, {"title": "Dataset Statistics", "content": "We classify all 494 tasks in Spider2-V into 7 categories and 11 software sub-categories Specifically, most (280 tasks, 56.7%) involve CLI and GUI operations. And 34% examples request registering authentic software accounts. Since each task is associated with a detailed, step-by-step tutorial (verbose instruction), the entire task set can be categorized into three distinct levels based on the number of actions in these instructions. The proportion of easy, medium, and hard tasks is approximately 1:2:1. According to the rightmost distribution most tasks necessitate the coordinated utilization of multiple professional applications, thereby establishing Spider2-V as a particularly challenging benchmark. \nwe compare Spider2-V with other agent benchmarks. Spider2-V incorporates generic computer control commands into the field of data science and engineering and is distinguished by these salient features: 1) a real-time executable environment. Instead of providing static input-output pairs, Spider2-V is equipped with a dynamic computer desktop such that agents can proactively explore it; 2) multiple enterprise software. We integrate 20 professional applications into the benchmark, which include not only tools installed on local hosts but also cloud-based enterprise services; 3) intensive GUI operations. Unlike traditional coding or data science domains, experienced data scientists frequently manipulate the UIs of those professional software to simplify the data workflow (e.g., enabling a specific function on the UI page or visualizing the graph view of data inputs). In summary, Spider2-V focuses on the use of professional enterprise software with visual interface in an interactive computer environment."}, {"title": "Experiments and Analysis", "content": "In this section, we introduce the experiment settings, experimental results, and ablation study to assess the proficiency of current LLM or VLM based agents on Spider2-V benchmark."}, {"title": "Environment Settings", "content": "Agent baselines The baseline method includes 3 schemes in zero-shot prompt learning: 1) Set- of-Mark (SoM, following OSWORLD and VisualWebArena, we adopt heuristic methods to retrieve coordinates of visible elements from a11ytree (a text-format observation type) and draw indexed bounding box for these elements on the screenshot. We further insert these indexes into the pruned a11ytree to enhance the alignment between screenshot and a11ytree. 2) Execution Feedback (EF): we append execution feedback messages of those actions which failed to be grounded in the environment due to unexpected errors. 3) Retrieval-Augmented Generation (RAG, ): we leverage the task instruction as the query vector, bge-large-en-v1.5 as the embedding model, and LlamaIndex framework as the retrieval to generate document context for each task example. Documents are pre-chunked into segments with maximum length 512 and tokens overlapping size 20. Top 4 segments are selected as additional context in the task prompt (detailed in App. G.3).\nLLMs and VLMS We experiment with state-of-the-art LLMs and VLMs, including open-source representatives such as Mixtral-8x7B and Llama-3-70B and closed-source ones including Qwen-Max, Gemini-Pro-1.5, Claude-3-Opus and GPT families (GPT-40 and GPT- 4V). With respect to the two open-source LLMs and QWen-Max, we utilize pure text-format a11ytree as the observation type on account of their incapability of image processing. For the remaining 4 VLMs which support vision input, we use aligned text and image (that is Set-of-Mark) as the observation type in main experiments. Unless otherwise specified, we set the temperature to 0.5 and top_p to 0.9, the history trajectory window size to 3, the maximum length of a11ytree to 5000 tokens, and the maximum output tokens to 1500 in each turn. Heuristically, we require the agent to complete the tasks within both 15 interaction turns and one hour, which suffices for most tasks "}, {"title": "Main Results", "content": "In Table 3, we compare performances of different LLMs and VLMs. All results above integrate techniques of both execution feedback (EF) and retrieval-augmented generation (RAG) in \u00a7 4.1. Accordingly, we can summarize that:\n1) Existing data agents are far from satisfactory in completing real-world data science and engineering tasks. Even state-of-the-art VLMs (GPT-40 and GPT-4V) perform terribly on Spider2-V, achieving at best 14.0% overall success rate.  they attain worse performances, even less than 10% percents. There is still ample room for improvement in future work.\n2) Closed-source models are much more superior than open-source ones. For those open-source LLMs, the success rate is exceedingly low, with some categories approaching zero. On one hand, it can be attributed to the fact that closed-source VLMs are pre-trained and fine-tuned on data of higher quality. On the other hand, closed-source VLMs support inputs with longer contexts and integrate both vision and text modalities (further analyzed in \u00a7 4.3).\n3) Performances of data agents exhibit high variance, especially in categories \u201cdata ingestion\u201d and \"data visualization\u201d. The majority of these two partitions are pure GUI tasks, which means agents mostly interact with the environment through time-dependent GUI operations. However, a minor error in one intermediate step can be amplified, resulting in the entire sequence of actions being wasted. Through error analysis on trajectories, we discover that once agents mispredict the coordinates of the correct button, they will open the wrong window and become trapped in the incorrect area, unable to return.\n4) Across 7 data categories, the partitions \u201cdata warehousing\u201d and \u201ctraditional data processing\u201d are extremely challenging. The reasons for this observation are two-fold: a) data warehousing tasks mostly involve authentic user accounts Compared to other tasks which can be accomplished in a local host, these dynamic real-world scenarios incur extra burden on data agents, such as network connection delay and pop-up windows. Multimodal agents need to deal with these unexpected situations in real-time interaction with the computer. b) As for traditional data processing, the bottleneck is that spreadsheets in Excel contain many cells, and it is particularly difficult for data agents to accurately locate the coordinates of cells. For example, applying the same math formula to the entire column requests multimodal agents to firstly pinpoint the right corner of a specific cell, wait for the mouse to become a cross, press and drag the mouse towards the target cell. This series of actions requires precise and fine-grained GUI controls which are difficult to implement."}, {"title": "Analysis", "content": "In this section, we delve into different factors which influence the eventual success rates, and analyze the underlying logics. The following analyses are based on our agent baseline with VLM GPT-40 unless otherwise specified. Firstly, we split the overall results into different subsets in Table 4.\n1) Tasks with more inherent action steps are more difficult. Each task is associated with one verbose task instruction which gives a step-by-step guidance on how to complete it. We count the number of actions in the verbose instruction and split the entire task set into 3 difficulty levels:\n 5 steps (Easy), 5 ~ 15 steps (Medium), and  15 steps (Hard). Not surprisingly, as the number of intrinsic action steps increases, the average performance decreases significantly. And for those extremely tough tasks, existing VLM-based data agents can hardly accomplish the goal."}, {"title": "Tasks involving authentic user accounts are much more challenging", "content": "One salient feature of Spider2-V is the integration of professional applications that require authentic user accounts. We also split the entire task set accordingly (w/o or w/ account). Notably, data agents struggle to complete tasks involving authentic user accounts (10.6% success rate). These tasks deal with real-world scenarios and incorporate cloud-hosted enterprise services. Compared with Web servers which are launched locally in the VM, the cloud Web UIs 1) generally integrate more comprehensive functionalities or options in their menu panel, and 2) potentially suffer from emergency situation, such as extended network response delay due to bandwidth limitation or server overload. We conjecture these two causes collectively contribute to the inferior performances."}, {"title": "Incorporating GUI operations typically lead to improved performances", "content": "We split the task set by interfaces. If the task can be completed with pure CLIs (e.g., code editor or bash terminal), we classify it as cli. If the task only requires the agent to manipulate the GUI (usually on the Web page), we classify it into gui. For the remaining cases (cli+gui), an agent must write code or scripts, and control the UI screen. We observe that pure gui tasks are much easier than cli tasks. This conclusion can be explained by the following two reasons: 1) GUIs of professional applications are designed to simplify the original coding task. Clicking buttons or typing values on UIs can avoid handling the rigorous and complex coding specification. 2) Both observation types, namely the screenshot and a11ytree, are naturally proposed for GUI tasks. For pure cli tasks, data agents must perform extra actions to locate and switch to the target panel before writing code."}, {"title": "Providing a step-by-step guideline in task instructions results in remarkable performance gains", "content": "The key difference between abstract and verbose instructions is whether a detailed step-by-step guidance is offered. With such stepwise oracle tutorials, data agents do not need to reason and plan, thus dramatically simplifying the original task. And the 4.8 points improvement consolidates this hypothesis. Nevertheless, the low success rate with verbose instructions indicates that current VLMs still yield unsatisfactory results when purely grounding actions in real-world contexts. And significant potential remains for further enhancement."}, {"title": "Regarding action space", "content": "the findings include: 1) Regarding action space, pyautogui code slightly outperforms self-customized JSON dict (12.6% v.s. 10.5%). This can be at- tributed to the advantage that agents can also generate functional Python code like file traversal apart from the limited GUI control operations using the first action space. And it improves the efficiency of action grounding. 2) As for observation types, single screenshot leads to very low performances (4.2%) on account of the agent's failure in pinpointing concrete elements. When inserting a11ytree into the observation which contains precise coordinates, the agent ca- pability of locating target pixels is remarkably promoted. 3) All 3 tricks we integrate into the agent baseline will boost eventual performances. It is interest- ing that if we do not adopt Set-of-Mark that is, enhancing the alignment between two modal- ities of observations), the result of screenshot+a11ytree is even worse than that using pure a11ytree. This emphasizes the significance of modal alignment when handling state observations."}, {"title": "A moderate temperature and longer history window size improve per- formances", "content": "In Figure 7, we inves- tigate the influences of two hyper-parameters on a task subset: 1) The top-ranked performance is achieved with sampling temperature 0.5. 2) With the history window size enlarges, from 0 (no history, only the current observation) to 3, the performance in- creases stably. However, due to constraints on input length and considerations of cost-effectiveness, we are unable to extend the history trajectories any further. This also points out that the interaction efficiency is a serious issue and promising research direction."}, {"title": "Related Work", "content": "Benchmarks for data science and engineering In the field of data science and engineering, several recent works propose novel benchmarks to evaluate the capabilities of LLM agents in manipulating Excel spreadsheets [16, 4], common data science libraries (e.g., SQL and pandas) machine learning [10] or software engineering [16] projects. They are usually confined to a single stage within the entire data pipeline, predominantly data processing and analysis, thus overlooking other stages such as data warehousing and orchestration from a broader perspective. Besides, like other coding-related datasets, they merely focus on the command line interface, neglecting the fact that enterprise software usually has rich graphical user interfaces (GUIs). And data scientists often combine code programming with intensive GUI operations to fulfill a data workflow. To this end, Spider2-V is proposed as the first-of-its-kind multimodal agent benchmark in the field of data science and engineering, which covers the entire data workflow and integrates visual interfaces.\nBenchmarks for multimodal agents Existing works on GUI interaction mainly encompass web navigation mobile device and computer desktop One trend of recent advanced benchmarks is to provide an executable simulation environment. Multi-modal agents can explore and interact with this platform through keyboard, mouse, gesture and touch screen actions in a more realistic and complex scenario. However, previous literature mostly focuses on daily life applications or workflows of non- specialized business tasks Few works investigate the capability of multimodal agents to manipulate enterprise-level software. GUIs of professional applications often contain abundant domain-specific terminologies which requires multimodal agents to understand the specialized knowledge. Spider2-V incorporates 20 professional tools into a real-time computer environment to test the proficiency of agents in data science and engineering. Furthermore, we supplement a large volume of documents for retrieval to compensate for deficiencies of agents in domain knowledge."}, {"title": "Conclusion", "content": "In this work, we propose Spider2-V, the first data science and engineering benchmark which integrates enterprise professional applications and supports intensive GUI operations besides code writing across the full data pipeline. It contains 494 tasks, involves 20 professional tools, and provides a real-time executable computer environment. The most advanced VLM (GPT-4V) still performs poorly on Spider2-V rendering it a very challenging benchmark. Although current multimodal agents are still far from automating data workflows, Spider2-V presents an easily accessible benchmark and lays the foundation for future research."}, {"title": "System Prompt", "content": "The entire system prompt consists of the environment prompt, observation space prompt, action space prompt, and general tips. Different action/observation types have different prompts. In this section, we will introduce each one in turn and present the overall system prompt at last."}, {"title": "Observation Space Prompt", "content": "The four different observation space settings, namely 1) screenshot, 2) allytree, 3) screen- shot+a1lytree, and 4) SoM, each has a different prompt."}, {"title": "Screenshot Setting", "content": "After each action step, you will get an image-style observation, which is the screenshot of the computer screen. And you need to predict the next action on the computer based on this image."}, {"title": "Accessibility Tree Setting", "content": "After each action step, you will get a text-style observation, which is extracted and pruned from the accessibility tree based on AT-SPI library. The accessibility tree describes the elements on the computer desktop, as well as its embedded text content, status and positions. For simplicity, we prune the original tree and only extract useful information into a tabular format for you.\nAnd you will predict the next action of the computer based on the accessibility tree."}, {"title": "Screenshot + Accessibility Tree Setting", "content": "The observation space is a combination of two sources: 1) image-style screenshot of the desktop, and 2) text-style accessibility tree derived from AT-SPI library.\nScreenshot\nAfter each action step, you will get a image-style observation, which is the screenshot of the computer screen. And you need to predict the next action on the computer based on this image. You can use this image to locate the elements on the screen or check the status of the computer, especially whether the previous action is successful or not.\nAccessibility Tree\nThe accessibility tree describes the elements on the computer desktop, as well as its embedded text content, status and positions. For simplicity, we prune the original tree and only extract useful information into a tabular format for you.\nYou can use the accessibility tree to accurately locate positions of useful elements on the screen and check the concrete textual contents of elements.\nBy combining the screenshot and accessibility tree, you should be intelligent to predict the next feasible and meaningful action."}, {"title": "SoM Setting", "content": "The observation space is a combination of two sources: 1) image-style screenshot of the desktop with interact-able elements marked with numerical indexes, and 2) text-style accessibility tree derived from AT-SPI library.\nLabeled Screenshot\nAfter each action step, you will get a image-style observation, which is the screenshot of the computer screen. For ease of locating positions of elements, we extend the original screenshot with index marks. That is, some salient elements which can be interacted with are marked with line boudaries and numeric indexes. You can use this image to locate the elements on the screen or check the status of the computer, especially whether the previous action is successful or not.\nAccessibility Tree\nThe accessibility tree describes the elements on the computer desktop, as well as its embedded text content, status and positions. For simplicity, we prune the original tree and only extract useful information into a tabular format for you.\nBy combining the screenshot and accessibility tree, you should be intelligent to predict the next feasible and meaningful action."}, {"title": "Action Space Prompt", "content": "As for the prompt of action space, we provide two choices: 1) pyautogui code, and 2) JSON dict."}, {"title": "pyautogui Code", "content": "You are required to use `pyautogui to perform the action grounded to the observation. And the action space includes two types:\n1. Python code block using pyautogui wrapped by 3 backticks,\n2. Three pre-defined special actions: [WAIT, FAIL, DONE]\nWhen you think you have to wait for some time, return ```WAIT```;\nWhen you think the task can not be done, return ```FAIL```, don't easily say```FAIL```, try your best to do the task;\nWhen you think the task is done, return ```DONE```.\nThese 3 actions also need to be wrapped by 3 backticks."}, {"title": "JSON Dict", "content": "Firstly, we use json dict to describe the types and parameters for each action we allowed (`required=true` means this argument must be provided). Then, we demonstrate use cases, and precautions."}, {"title": "Overall System Prompt", "content": "You are an intellignet agent who is expert in completing data science/engineering tasks using professional tools on computer. You have deep understanding of computer basics and data science/engineering knowledge. Now, you will interact with a real desktop environment, which is an Ubuntu operating system that has access to the Internet. You should strictly follow the user instruction, communicate with the environment and try your best to complete the given data-related task successfully. Generally, you will communicate with the environment in this interactive and continuous manner:\n1) In each iteration, you should take one action to control the keyboard or mouse in the desktop environment given the actions and observations from a few previous steps;\n2) Then, you will obtain new observations from the environment after the action is grounded (you do not need to worry about the execution, we will perform it for you);\n3) Repeat steps 1) and 2) until you think the work is done."}, {"title": "Task Prompt", "content": "The task instruction for Spider2-V has two forms. The abstract instruction describes the overall goal of a task without a step-by-step solution, thus testing both planning and grounding abilities. The verbose instruction provides a detailed tutorial-like solution to the task, primarily validating the grounding ability."}, {"title": "Example of Task Prompt for Abstract Instructions", "content": "Now, let's start the task!\nYou are asked to complete the following task: I want to build an airflow project connecting to a local postgres database. Could you install docker, astro and postgresql for me. The sudo password is 'password' ('' not included). By the way, configure docker and postgresql to auto-start on boot, and allow me to prevent typing sudo when using docker each time."}, {"title": "Example of Task Prompt for Verbose Instructions", "content": "Here is a step-by-step tutorial from an expert instructing you how to complete it:\nNow we want to upload data from xlang_gcs/google_ads/ in google cloud storage to my dataset google_ads.\nNow you can exactly follow the detailed plan above or proactively tackle the task based on the real-time environment interaction by yourself."}, {"title": "Example of Retrieved Context Augmented Task Prompt", "content": "We also introduce a RAG setting, where we collect and clean the official documents of the professional tools as the retrieval corpus. We select top k (k may depend on the constraint on input length) chunks and insert them into the prompt input. Here are three demonstrations of different formats of the retrieved context."}, {"title": "Pure Text Format", "content": "We also retrieve relevant documentation from the web to help you with the task:"}, {"title": "Markdown Syntax Format", "content": "We also retrieve relevant documentation from the web to help you with the task:"}, {"title": "Simplified HTML Format", "content": "We also retrieve relevant documentation from the web to help you with the task:"}]}