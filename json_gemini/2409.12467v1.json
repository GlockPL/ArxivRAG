{"title": "SurgPLAN++: Universal Surgical Phase Localization Network for Online and Offline Inference", "authors": ["Zhen Chen", "Xingjian Luo", "Jinlin Wu", "Long Bai", "Zhen Lei", "Hongliang Ren", "Sebastien Ourselin", "Hongbin Liu"], "abstract": "Surgical phase recognition is critical for assisting surgeons in understanding surgical videos. Existing studies focused more on online surgical phase recognition, by leveraging preceding frames to predict the current frame. Despite great progress, they formulated the task as a series of frame-wise classification, which resulted in a lack of global context of the entire procedure and incoherent predictions. Moreover, besides online analysis, accurate offline surgical phase recognition is also in significant clinical need for retrospective analysis, and existing online algorithms do not fully analyze the entire video, thereby limiting accuracy in offline analysis. To overcome these challenges and enhance both online and offline inference capabilities, we propose a universal Surgical Phase Localization Network, named SurgPLAN++, with the principle of temporal detection. To ensure a global understanding of the surgical procedure, we devise a phase localization strategy for SurgPLAN++ to predict phase segments across the entire video through phase proposals. For online analysis, to generate high-quality phase proposals, SurgPLAN++ incorporates a data augmentation strategy to extend the streaming video into a pseudo-complete video through mirroring, center-duplication, and down-sampling. For offline analysis, SurgPLAN++ capitalizes on its global phase prediction framework to continuously refine preceding predictions during each online inference step, thereby significantly improving the accuracy of phase recognition. We perform extensive experiments to validate the effectiveness, and our SurgPLAN++ achieves remarkable performance in both online and offline modes, which outperforms state-of-the-art methods.", "sections": [{"title": "I. INTRODUCTION", "content": "The computer-assisted surgery can improve the quality of intervention and facilitate patient healthcare [1]. In particular, accurate surgical phase recognition [2] is significant for developing systems to monitor surgical procedures [3], schedule surgeons [4], promote surgical team coordination [5], and educate junior surgeons [6].\nSurgical phase recognition of surgical videos is challenging and has received great research attention and progress [7]\u2013[10]. These studies predominantly focus on online surgical phase recognition to predict the current frame of video streaming without using future frames. Due to the computational burden, these works sequentially extracted spatial and temporal features of surgical videos to advance surgical phase recognition. In this context, most works adopt 2D convolutional neural networks (CNN) to parse each frame, and then adopt diverse temporal mechanisms to exploit the inherent temporal dynamics of surgical videos, e.g., temporal convolution [7], [8], long short-term memory (LSTM) [9] and transformer [10], generating the phase prediction for the current frame.\nDespite great progress in surgical phase recognition, existing works [7]\u2013[10] still suffer from two major limitations, including the reliance on frame-by-frame classification and the focus on online analysis to the detriment of offline accuracy. First, existing works formulated the task as a series of frame-by-frame classifications and predicted the current frame by leveraging temporal knowledge from preceding frames. This paradigm, akin to a greedy strategy, degrades the task of video analysis to a frame-by-frame image prediction task. As illustrated in Fig. 1 (a), these algorithms are unable to conduct global analysis from the perspective of the entire video, resulting in inconsistent predictions of successive frames. Second, these studies merely considered the online analysis of surgical video streaming. In fact, accurate offline surgical phase recognition is also highly desirable with significant clinical needs for retrospective analysis. As a result, these online algorithms are not designed to fully analyze the entire video and could only regard frame-by-frame predictions as the offline surgical phases, thereby leading to inferior accuracy in the offline analysis scenario. In this way, a universal surgical phase recognition framework is highly demanded to analyze the surgical video with a global perspective and is capable of handling both online and offline analysis effectively.\nTo address these two problems in surgical phase recognition, we propose a universal Surgical Phase Localization Network, named SurgPLAN++, to enhance both online and offline inference capabilities. As depicted in Fig. 1 (b), the SurgPLAN++ is designed with the principle of temporal detection to ensure a global understanding of the surgical procedure. Specifically, our phase localization strategy first generates phase proposals as starting and ending points from the extracted frame features and then identifies surgical phase segments by filtering the high-confidence proposals. For online analysis, to generate high-quality phase proposals, we devise a data augmentation strategy to extend the streaming video into a pseudo-complete video through diverse augmen-"}, {"title": "II. RELATED WORK", "content": "A. Video Features Extraction\nThe extraction of spatiotemporal features is crucial for video recognition. Early works used the 3D CNNs to jointly capture spatiotemporal features, such as C3D [11], P3D [12], and I3D [13]. However, these approaches encountered a significant challenge: the optimization of temporal and spatial dimensions often conflicted, leading to suboptimal performance. To address this issue, subsequent research proposed a novel divide-and-conquer architecture, SlowFast [14]. This approach employed a dual-branch structure: a low-frame-rate branch for capturing spatial information and a high-frame-rate branch for processing temporal information. SlowFast [14] extracts spatiotemporal information simultaneously, avoiding the optimization conflict between temporal and spatial dimensions, thus achieving improved performance in video recognition.\nB. Surgical Phase Recognition\nSurgical phase recognition garnered significant attention in recent years due to its potential to enhance patient safety and streamline surgical workflows. Researchers explored various approaches to automatically identify different phases of surgical procedures. Deep learning models showed promising results in this domain. For instance, PhaseNet [15], MSTCN [16], and TeCNO [7] were proposed for recognizing surgical phases by using 2D CNNs. Other studies, such as TMR [17], SV-RCNet [18] used LSTM to capture temporal dependencies in surgical workflows. Additionally, Transformer-based approaches were explored to improve recognition accuracy, such as Trans-SVNet [10]. Despite these advancements, these methods still face systematical challenges in frame-to-frame classification prediction tasks and do not fully leverage the global information provided by the surgical video."}, {"title": "III. UNIVERSAL SURGICAL PHASE LOCALIZATION NETWORK", "content": "A. Overview of SurgPLAN++\nTo achieve universal online and offline surgical phase recognition, our SurgPLAN++ is proposed with the temporal detection principle, which consists of a spatial temporal encoder and a phase localization network. As illustrated in Fig. 2, the spatial temporal encoder first extracts multi-scale features of each frame, and then the phase localization network generates phase proposals from frame features and predicts the phase segments as the prediction.\nFor the online analysis, SurgPLAN++ utilizes several data augmentations including mirroring, center-duplication, and down-sampling that extend the ongoing video into a pseudo-complete video. For the offline analysis, SurgPLAN++ maintains a dynamic result sequence of phase predictions and updates continuously in each inference step based on the newly proposed segments.\nB. Network Architecture\nSpatial Temporal Encoder. We adopt the spatial temporal encoder [14] for SurgPLAN++. The encoder $\\mathcal{E}$ consists of a slow path and a fast path. The slow path is characterized by a large temporal stride $S_s$, facilitating the focus on static spatial positional information. Meanwhile, the fast path possesses a small stride $S_f$, directing attention toward dynamic motion information. Given the surgical video $V \\in \\mathbb{R}^{T \\times H \\times W \\times 3}$, we generate the slow path features $f_{\\text{slow}} \\in \\mathbb{R}^{\\frac{T}{S_s} \\times C_s}$ and fast path features $f_{\\text{fast}} \\in \\mathbb{R}^{\\frac{T}{S_f} \\times C_f}$ from two distinct 3D temporal convolutional networks $F_{\\text{slow}}$ and $F_{\\text{fast}}$, where $C_s$ and $C_f$ refer to the output feature dimension of the slow and fast path as follows:\n\n$f_{\\text{slow}} = F_{\\text{slow}} (V, S_s)$,\n$f_{\\text{fast}} = F_{\\text{fast}} (V, S_f)$.\n\nThen, to concatenate these two features, we utilize a 3D temporal convolution kernel $K$ to align $f_{\\text{fast}}$ to the same temporal feature length $\\frac{T}{S_s}$ of the slow path [14].\n\n$f_{\\text{fuse}} = [K(f_{\\text{fast}}), f_{\\text{slow}}]$,"}, {"title": "C. Online and Offline Phase Prediction", "content": "As the Phase Localization Network requires complete phase segments to effectively generate phase proposals, SurgPLAN++ has data augmentation techniques including mirroring, center-duplication, and down-sampling that extend the ongoing video to a pseudo-complete video. Meanwhile, SurgPLAN++ can effectively take advantage of the global context information to revise past predictions based on its rectification mechanism.\nOnline Prediction with Data Augmentation. In the surgical video, a symmetrical attribute typically exists in the initial and terminal phases of the video. For instance, the entry of surgical instruments serves as the commencement, while their withdrawal signifies the end. Therefore, we utilize mirroring to reverse the video, allowing the originally incomplete video to be supplemented with segments generated through mirroring, resulting in a complete video that includes distinct features of both the initial and final stages.\nSpecifically, for a video stream $V$ that represents a continuous frames set $\\{x_1, x_2, ..., x_t\\}$, where $x_n \\in \\mathbb{R}^{H \\times W \\times 3}$ refers to the frame at specific time $n$ in the frame sequence $V$, $t$ is the current time point. Mirroring the video stream $V$ means that the processed time frame becomes $\\{x_1, ..., x_{t-1}, x_t, x_{t-1}, ..., x_1\\}$ where $x'$ is identical to the $x_t$. Therefore, we procure a mirrored video sequence $V_p$ centered upon the current temporal juncture, wherein the latter half $V_e = \\{x_{t-1}, x_{t-2}, ..., x_1\\}$ constitutes a retrograde motion of the first half segment $V_s = \\{x_1, x_2, ..., x_{t-1}\\}$.\nAdditionally, if a given surgical phase at the current moment is incomplete and excessively brief, there is a potential for the phase localization network to overlook this phase, leading to imprecise predictions. To mitigate this issue, we utilize a center-duplicating method to duplicate the current moment, thereby ensuring it attains enough attention for the phase localization network. We prolong the duration by duplicating the current video frame $x_t$ and inserting them in the middle of the mirrored time frame, which concurrently preserves the action characteristics more effectively. The time frame becomes $V_d = \\{V_s, x_t, ..., x_t, V_e\\}$.\nAt last, we employ a down-sampling approach when mirroring and center-duplication results in an excessively extended action length. Specifically, slices with a step size $n$ are selected to constrain the action duration. Thus, we get the processed time frame $V_p$.\nBy applying these three methodologies, we standardize the action lengths within a specified range, optimizing the model's detection framework and enhancing its detection capabilities. The prediction for the current frame is the phase of the bounding box that includes the center point.\nOffline Prediction with Rectification Mechanism. In the context of retrospective amendments to the prior phases, due to our persistent maintenance of a dynamic phase prediction sequence $R_{\\text{phase}} = \\{y_1, y_2, ..., y_t\\}$, where $y_n$ is the prediction on time $n$. We can revise the historical results by leveraging the filtered phase proposals before the current time $t$. Phase proposals that include or exceed time $t$ are proposals generated related to the augmented data. Therefore, for those completed phase proposals before time $t$, noted as $\\{Y_m\\}$, we regard those proposals as already gathering enough information to determine phases, we update those phases by replacing the $y_n$ to the phase of those filtered completed segments that contain time $n$ by $y'_n \\rightarrow y_n$, where $y'$ is the phase prediction in set $\\{Y_m\\}$ at the inference step of current moment $t$. By fully utilizing global temporal knowledge, we update the result sequence $R_{\\text{phase}}$ at each time step to form a better offline performance."}, {"title": "D. Optimzation and Inference", "content": "We summarize the training process of our SurgPLAN++ framework in Algorithm 1. We utilize a combination of distinct cross-entropy loss functions and Intersection over Union (IoU) loss to enable the model's multiple heads to perform both temporal proposal bounding box prediction and phase prediction. This dual-task approach facilitates the concurrent optimization of temporal localization and phase classification within the framework of our proposed model architecture. Furthermore, we summarize two different inference modes of our SurgPLAN++ framework in Algorithm 2 and 3. This approach enables seamless utilization of different modes under varying circumstances, as both modes employ the same model and undergo identical training processes."}, {"title": "IV. EXPERIMENT", "content": "A. Dataset and Implementation Details\nCholec80 Dataset. We perform comparisons on the Cholec80 dataset [22] of laparoscopic cholecystectomy procedures, which is the mainstream benchmark for surgical phase recognition. The Cholec80 dataset contains 80 surgical videos with a resolution of 854 \u00d7 480 or 1, 920 \u00d7 1,080 at 25 frame-per-second (FPS). The laparoscopic cholecystectomy procedures are divided into seven surgical phases. We exactly follow the standard splits [10], [22], i.e., the first 40 videos for training and the rest 40 videos for test.\nCataract Dataset. We further conduct our experiment on the public Cataracts [23] dataset and follow the standard split [24] to divide 25 cataract surgery videos for training and the remaining 25 videos for test. These cataract surgery videos are captured with the resolution of 1, 920 \u00d7 1,080 at 30 FPS. The Cataracts dataset contains 19 phase categories, including one background category without clear surgical purposes.\nImplementation Details. We perform the experiments using PyTorch on a single NVIDIA A800 GPU. All videos are resized to 256 \u00d7 256 with 1 FPS after preprocessing. In the training phase of the Phase Localization Network, the learning rate is configured to 0.001, and the Adam optimizer is utilized for the optimization process. For our SurgPLAN++ framework, we transform frame-by-frame labels into segments of surgical phases, and each segment consists of the start time, end time, and phase label. The window sizes for the Max-Pooling of fused features are 1, 2, and 4. The bin size is set as 24 in the Cataract [24] dataset. These parameters are chosen because of the statistical information [25] we collect from the dataset. Since most of the phase lengths are around 0 to 40 seconds, along with the Max-Pooling window size, the bin can cover one complete phase in almost any circumstance. In the inference stage, the threshold is set to 0.15. Concurrently, for data augmentation in the online mode, the number of feature replications is established at 16 which can make the phase length closer to the real phase length in most of the cases. The scaling ratio will be adjusted to ensure the video length will not exceed 512.\nEvaluation Metrics. We adopt four commonly used metrics to comprehensively evaluate the performance of surgical phase recognition, including accuracy (AC), precision (PR), recall (RE), and Jaccard (JA). Higher scores for these metrics indicate better quality of surgical phase recognition. Following the evaluation protocol in previous works [10], we evaluate the selected state-of-the-art methods under the same criteria as the SurgPLAN++ to perform fair comparisons."}, {"title": "V. CONCLUSION", "content": "In this work, we propose a universal SurgPLAN++ framework for both online and offline surgical phase recognition. Different from existing studies that focus on merely online inference and analyze surgical videos as frame-wise classification, our SurgPLAN++ is developed with the principle of temporal detection and predicts phase segments across the entire video through phase proposals. In particular, for online analysis, SurgPLAN++ incorporates a data augmentation strategy to extend the streaming video into a pseudo-complete video. For offline analysis, SurgPLAN++ continuously refines preceding predictions during each online inference step, thereby significantly improving the accuracy of phase recognition. Extensive experiments confirm the superiority of our SurgPLAN++ in both online and offline analysis for surgical videos."}]}