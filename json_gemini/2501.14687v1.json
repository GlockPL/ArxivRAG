{"title": "Decoding Generalization from Memorization\nin Deep Neural Networks", "authors": ["Simran Ketha", "Venkatakrishnan Ramaswamy"], "abstract": "Overparameterized Deep Neural Networks that generalize well have been key to the dra-\nmatic success of Deep Learning in recent years. The reasons for their remarkable ability to\ngeneralize are not well understood yet. It has also been known that deep networks possess\nthe ability to memorize training data, as evidenced by perfect or high training accuracies\non models trained with corrupted data that have class labels shuffled to varying degrees.\nConcomitantly, such models are known to generalize poorly, i.e. they suffer from poor test\naccuracies, due to which it is thought that the act of memorizing substantially degrades\nthe ability to generalize. It has, however, been unclear why the poor generalization that\naccompanies such memorization, comes about. One possibility is that in the process of\ntraining with corrupted data, the layers of the network irretrievably re-organize their repre-\nsentations in a manner that makes generalization difficult. The other possibility is that the\nnetwork retains significant ability to generalize, but the trained network somehow \"chooses\u201d\nto readout in a manner that is detrimental to generalization. Here, we provide evidence for\nthe latter possibility by demonstrating, empirically, that such models possess information\nin their representations for substantially improved generalization, even in the face of memo-\nrization. Furthermore, such generalization abilities can be easily decoded from the internals\nof the trained model, and we build a technique to do so from the outputs of specific layers of\nthe network. We demonstrate results on multiple models trained with a number of standard\ndatasets.", "sections": [{"title": "1 Introduction", "content": "Prior to the advent of Deep Learning, the conventional wisdom for long\u00b9, was that in building a predictive\nmodel, the model should have as few parameters as possible and this number should certainly be less than\nthe number of training samples that one was fitting. The dogma was that, otherwise, the model would\nexactly fit the training points, but invariably generalize poorly to unseen data, i.e. overfit. This intuition\nwas also largely borne out by the models of the day. Modern Deep Learning, however, has gone on to\nshow the opposite, namely that overparameterized models not only don't necessarily overfit, but that they\ncan generalize remarkably well to unseen data. However, over a decade later, we still do not satisfactorily\nunderstand why this is so. Interestingly, it has been shown (Zhang et al., 2017; 2021) that when one\nshuffles class labels of data points from standard training datasets to varying degrees, deep networks can still\nhave high/perfect training accuracy on such corrupted training data; however, this appears to typically be"}, {"title": "1.1 Main Contributions", "content": "\u2022\n\u2022\n\u2022\nFor models trained using standard methods & datasets with training data corrupted by label noise,\nwhile the model has poor test accuracy, we can build a simple classifier with dramatically better\ntest accuracy that uses only the model's hidden layer outputs obtained for the (corrupted) training\nset.\nFor the aforementioned models, if the true training class labels are known post hoc, i.e. after the\nmodel is trained, we can build a simple classifier, with significantly better generalization performance\nthan in (1). This is true, in many cases, even for models where training class labels are shuffled with\nequal probability. This demonstrates that the layers of the network maintain representations in a\nmanner that is amenable to straightforward generalization to a degree not previously recognized.\nOn the other hand, we asked if a model trained on the true training labels similarly retained the\ncapability to memorize easily. Adapting our technique to this setting, we find that in a few cases,\nwe can extract a high degree of memorization. The same classifier sometimes exhibits high test\naccuracy (on the true test labels), which further supports the idea that generalization can co-exist\nwith memorization."}, {"title": "2 Related work", "content": "The idea of probing intermediate layers of Deep Networks isn't new. For example, (Montavon et al., 2011;\nAlain & Bengio, 2018) do so by using kernel PCA & linear classifiers respectively. However, this approach\nhas not been used to investigate memorization. Indeed, (Alain & Bengio, 2018) explicitly avoid examining\nmemorized networks from (Zhang et al., 2017) because they thought such probes would inevitably overfit.\nOur results are therefore especially surprising in this context.\nThere is evidence that DNN's learn simple patterns first, before memorizing (Arpit et al., 2017), & DNNS\nlearn lower frequencies first (Belrose et al., 2024). (Stephenson et al., 2021) study memorized models,\nconcluding that memorization happens in later layers, since rewinding early layer weights to their early\nstopping values recovers some generalization, but rewinding later layer weights doesn't. On the contrary,\nour results suggest that later layers in most models investigated retain significant ability to generalize, & we\ndemonstrate this without modifying the weights of the trained network.\nThere is an important line of theoretical work in deep linear models (Saxe et al., 2013) where the question of\ngeneralization has been studied. In this context, (Lampinen & Ganguli, 2018) offer a theoretical explanation\nfor the phenomenon of memorization in networks trained with noisy labels.\nExperiments towards understanding training dynamics across layers using different Canonical Correlation\nAnalysis have be explored (Raghu et al., 2017) and in various generalized and memorized networks is analyzed\n(Morcos et al., 2018). Centered Kernel Alignment in different random initializations by (Kornblith et al.,\n2019) and network similarity between model trained with same data and different initialization is examined\nby (Wang et al., 2018). Also experiments related to using measures of the representational geometry towards\nunderstanding dynamics of layerwise outputs (Chung et al., 2016; Cohen et al., 2020) and different measures\nsuch as curvature (H\u00e9naff et al., 2019) Dimensionality which express the structures within the representations\n(Sussillo & Abbott, 2009; Farrell et al., 2019; Gao & Ganguli, 2015; Litwin-Kumar et al., 2017; Bakry et al.,\n2015; Cayco-Gajic & Silver, 2019; Yosinski et al., 2014; Stringer et al., 2019; Yosinski et al., 2014) have also\nbeen explored.\nTo deal with label noise, many heuristics have been explored (Khetan et al., 2017; Scott et al., 2013; Reed\net al., 2014; Zhang & Sabuncu, 2018; Malach & Shalev-Shwartz, 2017) & for classification task see (Fr\u00e9nay\net al., 2014; Ren et al., 2018; Menon et al., 2018; Shen & Sanghavi, 2019). For over parameterized models, (Li\net al., 2020) shows that the memorized network weights are far away from the initial random state in order\nfor them to overfit the noisy labels. (Stephenson & Lee, 2021) propose a theoretical model for epochwise\ndouble descent that suggests that for small-sized models, moderate amounts of noise can cause generalization\nerror to dip later on in training."}, {"title": "3 Methodology", "content": "Using the organization of subspaces of class-conditioned training data on layerwise outputs of deep networks,\nwe build a Minimum Angle Subspace Classifier (MASC) with the following steps:\nCreation of subspaces: For a specific layer, we estimate subspaces for each class. The class-conditioned\ntraining data subspaces on layerwise outputs of deep networks are computed using PCA. If the empirical\nmean of the class-conditioned data isn't zero, PCA in effect, will provide us an affine space, i.e. a linear\nspace that doesn't pass via the origin. However, we have determined subspaces which are linear spaces\npassing through the origin - here rather than affine spaces. In order to do so, we add the negative of each\nsample to the dataset so it is guaranteed to have empirical mean be zero, before running PCA. This created\ndataset is sent to the PCA algorithm to calculate PCA components for a certain percentage of variance\nexplained in the dataset. The span of these PCA components is the subspace S. We illustrate the process\nfor a Multi-layer Perceptron (MLP) model in Figure 4 in the Appendix.\nProjection of the data point: Layer output of an incoming data point is projected onto these class-specific\nsubspaces."}, {"title": "Label assignment using minimum angle", "content": "For every data point, the angle between the original data\npoint and projected data point for each class is calculated. The Minimum Angle Subspace Classifier (MASC)\nassigns to the datapoint, the label of the subspace having the minimum angle with the original data point.\nWhile the subspaces are estimated using the training data alone, accuracy of the Minimum Angle Subspace\nClassifier is determined for the training data and the testing data separately. This process is followed for all\nthe layers in the network independently. MASC is using labels of the dataset while creating the class-specific\nsubspaces. For experiments in Section 4, MASC uses corrupted training labels whereas in Section 5, MASC\nuses true training labels to create class-specific subspaces. See Appendix A.7 for MASC algorithm. We have\nused 99% as the percentage of variance explained, unless otherwise mentioned."}, {"title": "3.1 Experimental Setup", "content": "We have used multiple models and datasets, namely Multi-layer Perceptron (MLP) trained on MNIST\n(Deng, 2012) and CIFAR-10 (Krizhevsky, 2009) datasets, Convolutional Neural Networks (CNN) 2 trained\non MNIST, Fashion-MNIST (Xiao et al., 2017), and CIFAR-10 and AlexNet (Krizhevsky et al., 2012) trained\non CIFAR-100 (Krizhevsky, 2009) and Tiny ImageNet (Moustafa, 2017). We have trained these models with\ntraining data having true labels (\u201cgeneralized models\") as well as separately using training data with labels\nshuffled to varing degrees (\u201cmemorized models\") (Zhang et al., 2021).\nFor memorized models, when we say we train it with corruption degree p, we mean that with probability p,\nwe attempt changing the label for a training datapoint. Changing the labels happens uniformly at random.\nNote that this may result in the label remaining the same; therefore the expected fraction of datapoints\nwhose labels changed are $p - p/c$ where c is the number of classes. So, this would mean that for corruption\ndegrees of 20%, 40%, 60%, 80%, 100% the expected percentage of training datapoints with changed labels\nis 18%, 36%, 54%, 72%, 90% respectively, when c = 10. We have run experiments for values of p being 0%\n(generalized model), 20%, 40%, 60%, 80%, 100% (memorized models).\nA summary of the models and datasets with training set size and number of parameters is in Table 1 in the\nAppendix. The average training and testing accuracies of all the models over three runs are shown in Table\n2 and 3 in section A.3. More details of these models, hyperparameters & training are available in Section\nA.2. Following standard practice in probing memorized models (e.g. (Stephenson et al., 2021)), we do not\nuse explicit regularizers such as Dropout or batchnorm, or early stopping, unless otherwise mentioned, as a\nresult of which our baseline test accuracy numbers are often much lower than what is usually found with\nstandard training of these models. All the models are trained to either reach very high training accuracies\n(i.e. 99%-100%) or trained until 500 epochs. Some models did not result in such high accuracies, in which\ncase, results have been shown on the model obtained at epoch 500. We trained 3 instances of each model\nand results displayed are averaged over these instances with the shaded region indicating the range of results\nalso indicated in the plots.\nOnce the model is trained, we apply MASC on each layer of the network with respect to different subspaces.\nFor MLP models, all the MASC experiments were performed for all the layers in the network including on\nthe input (after it is pre-processed). For CNN models and AlexNet models, the experiments were performed\non flatten layer (Flat) and fully connected layers (FC). While we ran the experiments on the input layer for\nCNNs, we did not do so for AlexNet."}, {"title": "3.2 Terminology", "content": "The general terminology used in this work is as follows:\nModel Training Accuracy: The model accuracy on the training set with corrupted labels.\nModel Testing Accuracy: The model accuracy on the testing data set with true labels.\nMinimum Angle Subspace Classifier (MASC) Accuracy on Corrupted Training: Training accu-\nracy of MASC on training data set with corrupted labels was used in determining the subspaces."}, {"title": "4 Enhanced generalization ability in memorized models", "content": "Models trained with corrupted labels have high training accuracy (on corrupted labels) while also having\nlow testing accuracy (Zhang et al., 2021). We ask if we can decode the representations of the hidden layers\nof these memorized models to obtain better generalization.\nTo do so, we build a Minimum Angle Subspace Classifier (MASC) using class-conditioned corrupted training\nsubspaces obtained from the memorized models' hidden layer outputs. MASC is performed layer-wise for\nall the layers of the network independently as described in Section 3. MASC accuracy on corrupted training\ndata, MASC accuracy on original training data, and MASC accuracy on testing data over the layer of MLP\ntrained on MNIST, CNN trained on Fashion-MNIST, AlexNet trained on CIFAR-100 are shown in Figure 1.\nSGD optimizer (Qian, 1999) was used for training MLP models, whereas Adam optimizer (Kingma, 2014)\nwas used for other models.\nImportantly, for every corrupted model we have (with non-zero corruption degree), except those with 100%\ncorruption degree, we find that our Minimum Angle Subspace Classifier (MASC) in at least one layer has\nbetter testing accuracy than the corresponding model itself. In many cases, the MASC testing accuracy is\ndramatically better than that of the model. This is remarkable, because, in addition to the layerwise outputs,\nthe MASC used precisely the same information (including the same corrupted training dataset) that was\navailable to the model itself, and yet is able to extract better generalization. This suggests that the model\nretains significant latent generalization ability, which is not captured in its own test-set performance. In most\nmodels, the same MASC, especially on the later layers, also approaches perfect accuracy on the corrupted\ntraining set, indicating that this improved generalization happens concurrently with memorization of training\ndata points with shuffled labels. Below, we make more specific observations on the performance of the models.\nWith generalized models i.e. those with 0% corruption degree, at the later layers of the network, it is observed\nthat in most of the cases MASC accuracy on training data approaches the models training accuracy. Similarly,\nMASC accuracy on testing data is comparable to or performed better than the models' test accuracy.\nEven for high corruption degrees, we find that the MASC performs well. For example, with 80% corruption\ndegree, which implies that approximately 72% of the training labels have been changed, we observed good\nMASC testing accuracy in many cases. Notably, the MASC test accuracy on the later layers is over 80% on\nMLP-MNIST, in comparison to 34% test accuracy by the model. Similarly, MASC test accuracy on one of\nthe layers is about 75% for CNN-Fashion-MNIST, in contrast to 25% model test accuracy.\nNot only does the MASC have better accuracy than the model on the test data but it also does well on the\ntraining data with the true labels. Although the model has memorized the training data with corrupted\nlabels, outputs from certain layers have the ability to predict the trained true labels. For example, in MLP\nMNIST, for low to moderate degrees of corruption, MASC on the middle layer (FC (512)) has good accuracy\non the true training labels, while also retaining good accuracy on the test set. With 40% corruption degree,\napproximately 36% are changed labels and yet the model has good accuracy on the true training labels in\nat least one layer of the network. e.g. MLP-MNIST has over 90% true training accuracy at layer FC(512),\nCNN-Fashion-MNIST has approximately 85% in Flat (576) layer & AlexNet-CIFAR-100 has approximately\n60% in FC (4096) layer. This means that almost 20% of those labels are predicted correctly even though the\nmodel was trained for 500 epochs or has reached high training accuracy on corrupted labels. In the process\nof doing this, the model does not have any direct information about the true labels and neither does the\nMASC.\nOne way to think about a deep network, is as one that successively transforms input representations in a\nmanner that aids in good prediction performance. Therefore, performance of the MASC on the input is a\ngood baseline measure to assess if subsequent layers have favorable accuracies. Naively, for models trained"}, {"title": "5 Generalization via true training labels with memorized models", "content": "While the previous section demonstrated improved generalization performance by the MASC, we want to\ninvestigate if there exist better subspaces that can offer superior generalization performance. To this end,\nwe consider the setting where the true label identities of the training set are known post training with\ncorrupted labels. Can we extract significantly high training as well as testing performance in this case from\nthe layerwise outputs of the network? To do so, we build MASC using subspaces obtained from training\ndata with true labels. It is a priori unclear if MASCs trained in this manner will have high accuracy. Since\nthe network trained assuming different labels for many of the datapoints, it is conceivable that class-wise\nsubspaces corresponding to true labels lack structure and predictive power. We find, however, that these\npossibilities do not bear out.\nMASC accuracy on original training data and on testing data projected on true training label subspace over\nthe layers of the same networks from Section 4 is shown in Figure 2. For comparison, MASC accuracy on\ncorrupted training data and testing data projected on corrupted training subspace is also shown. We find\nthat, in many cases, accuracies on the true training labels, as well as the test set are dramatically better\nhere than with the experiments where subspaces were determined for the corrupted training data. In fact\nthe MASC test accuracies for the corrupted models (with non-zero corruption degree) are sometimes fairly\nclose to the test accuracy of the uncorrupted model.\nStrikingly, even for models trained with 100% corruption degree, in most cases, the MASC retains significant\naccuracy on the true training labels as well as the test set. This is in spite of the fact that the model itself has\nchance-level test-set accuracy. For example, MASC classifier has 95% test labels accuracy in last FC(2048)\nlayer for MLP-MNIST, 69% test labels accuracy for Flat(576) layer in CNN-Fashion-MNIST, and 4% test\nlabels accuracy for Flat (256) layer in AlexNet-CIFAR-100.\nThe results here are proof of principle that suggest the existence of subspaces which allow one to extract\nsignificantly high generalization performance on models trained with datapoints whose labels are shuffled to\na remarkably high degree. This has two implications. On the one hand, it demonstrates that models trained\nwith very high label noise, surprisingly, retain the latent ability to generalize very well. On the other hand,\nit suggests that development of new techniques to identify favorable subspaces could help markedly boost\ngeneralization performance of models, whose training data is known to have label noise."}, {"title": "6 Inducing Memorization in uncorrupted models", "content": "Conversely, we examined if we could build a MASC classifier on a model trained on true training labels, with\nthe goal of memorizing training data whose labels are corrupted to varying degrees.\nTo do this, we take generalized models, i.e. models trained with uncorrupted training data. We then\nshuffle the labels of the training set to some corruption degree and construct the corresponding class-specific\nsubspaces with respect to the layerwise outputs of the model. We then build a MASC classifier corresponding\nto these subspaces.\nMASC accuracy on original training data and MASC accuracy on testing data over the layer same networks\nfrom Section 4 are shown in Figure 3. Additional results are available in the Appendix in Figures 23, 33, 35\nand 39 and their respective class-wise PCA components are available in Figure 24, 34, 36.\nInterestingly, we find that for uncorrupted model with modest model test accuracies (i.e. AlexNet-CIFAR-\n100), the MASC classifiers described above have high accuracies on the corrupted training set. Conversely,\nin most uncorrupted models with high model test accuracies (i.e. MLP-MNIST and CNN-Fashion-MNIST),\nwe find that these MASC classifiers have more modest accuracies on the training set with corrupted labels.\nOne exception to this, is in Figure 23 of the Appendix, where we have MLP-Adam-MNIST models with high\nmodel test accuracy. Yet, we find that a MASC classifier on the first FC (2048) layer trained with training\nlabels corrupted to 100% corruption degree has over 90% accuracy on training data with corrupted labels.\nAlso, MASC classifiers often have test accuracies that approach or exceed uncorrupted model test accuracies,\neven though they correspond to corrupted subspaces (see e.g. AlexNet on CIFAR-100)."}, {"title": "7 Discussion", "content": "In this work, we investigated the phenomenon of memorized networks not generalizing well, asking why the\nability to generalize is apparently lost during the act of memorizing. We find, surprisingly, that the intrinsic\nability to generalize remains present to a degree not previously recognized, and this ability can be decoded\nfrom the internals of the network by straightforward means.\nAn interesting question is about why this phenomenon even occurs; na\u00efvely one would expect that networks,\non being trained with highly noisy data, discard the ability to generalize in favor of learning noise. Are\nthere specific inductive biases that promote such generalization? And, do such mechanisms also promote\ngeneralization in networks whose training data isn't corrupted significantly by such noise? It would also\nbe instructive to study the dynamics of this form of generalization during training\u00b3. It is known (Arpit\net al., 2017) that the model's test accuracy transiently peaks in the early epochs of training with corrupted\ndata, before dropping while training accuracy of the corrupted training data rises. It is unclear whether this\ntransient rise in model generalization is caused by the subspace organization seen here, and if so, why such\nsubspace organization isn't degraded as much as the model's test error over further epochs of training.\nThe work has a number of implications. On the one-hand, it suggests that the ability to memorize and\ngeneralize may not be antithetical. Indeed, in multiple cases, we are able to construct single MASC classifiers\nthat perform well both on the shuffled training labels as well as on the held-out test data that has true\nlabels. Secondly, theories proposed to explain generalization in deep networks have traditionally argued for\nthe setting where the data distribution is well-behaved, i.e. corresponding to real-world data, but not for\ndata with shuffled labels. We suggest, in light of the present results, that such theories also ought to be\nable to explain why networks retain the ability to generalize even in the face of noisy training data. That\nis, a satisfactory understanding of generalization in deep networks should also cover the settings where the\ntraining data is noisy and its distribution is not well behaved. Thirdly and more pragmatically, techniques\nsuch as the MASC classifier might suggest a way of boosting generalization in trained Deep Networks,\nwhose training data intrinsically contains varying degrees of label noise. While this has been beyond the\nscope of the present paper, possibilities of designing new techniques for learning subspaces that have good\ngeneralization ability could be explored. Indeed, it is possible that significantly better subspaces exist than\nthe ones uncovered here, and it would be interesting to see how much the generalization accuracy can be"}]}