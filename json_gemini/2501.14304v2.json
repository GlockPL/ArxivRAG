{"title": "MASTER: A Multi-Agent System with LLM Specialized MCTS", "authors": ["Bingzheng Gan", "Yufan Zhao", "Tianyi Zhang", "Jing Huang", "Yusu Li", "Shu Xian Teo", "Changwang Zhang", "Wei Shi"], "abstract": "Large Language Models (LLM) are increasingly being explored for problem-solving tasks. However, their strategic planning capability is often viewed with skepticism. Recent studies have incorporated the Monte Carlo Tree Search (MCTS) algorithm to augment the planning capacity of LLM. Despite its potential, MCTS relies on extensive sampling simulations to approximate the true reward distribution, which leads to two primary issues. Firstly, MCTS is effective for tasks like the Game of Go, where simulation results can yield objective rewards (e.g., 1 for a win and 0 for a loss). However, for tasks such as question answering, the result of a simulation is the answer to the question, which cannot yield an objective reward without the ground truth. Secondly, obtaining statistically significant reward estimations typically requires a sample size exceeding 30 simulations, resulting in excessive token usage and time consumption. To address these challenges, we present the Multi-Agent System with Tactical Execution and Reasoning using LLM Specialized MCTS (MASTER), a novel framework that coordinates agent recruitment and communication through LLM specialized MCTS. This system autonomously adjusts the number of agents based on task complexity and ensures focused communication among them. Comprehensive experiments across various tasks demonstrate the effectiveness of the proposed framework. It achieves 76% accuracy on HotpotQA and 80% on WebShop, setting new state-of-the-art performance on these datasets.", "sections": [{"title": "1 Introduction", "content": "LLM represent a significant milestone in artificial intelligence and are increasingly employed in problem-solving tasks (Xi et al., 2023). However, their application to complex problems is often limited due to concerns about their planning capabilities. LLM generate text based on \"next token probability\" conditioned on the context (Vaswani et al., 2017; Huang et al., 2023), but effective reasoning requires rigorous deduction rooted in the first principles of logic, grounded in data and reality (Valmeekam et al., 2023).\nTo enhance the planning capabilities of LLM, recent studies have incorporated the MCTS algorithm (Hao et al., 2023; Zhou et al., 2024; Wang et al., 2024). MCTS uses simulations to evaluate the long-term consequences of actions, backpropagating aggregated rewards up the tree. This allows for backtracking to previous states based on estimated future potential, striking a balance between exploitation and exploration. However, the use of MCTS in existing works presents two significant challenges:\n1. It relies on objective criteria from the external environment to obtain simulation rewards, which are not always available (e.g., in question-answering tasks where the correctness of an answer cannot be determined without ground truth);\n2. It requires numerous simulations to obtain statistically significant rewards, which can be unsustainable due to the time and token costs.\nTo tackle the first challenge, certain approach compares the outcomes of simulations with the ground truth to derive objective rewards (Zhou et al., 2024) which is flawed because revealing the ground truth during problem-solving is inappropriate. In addressing the second challenge, they limit the number of simulations (Hao et al., 2023) or terminate the process once a correct answer is identified (Zhou et al., 2024). Yet, this early termination approach is not feasible if the ground truth remains undisclosed.\nThese issues highlight a critical step of MCTS: simulation. Due to these limitations, MCTS is constrained to a narrow scope of application and is not fully compatible with LLM. Consequently, we propose an adaptation of MCTS tailored to LLM scenarios. Instead of performing limited simulations with uncertain rewards, we eliminate the simulation process, relying on the LLM's self-evaluation capabilities to allocate rewards. Additionally, we propose several methods to enhance the objectivity of rewards: 1). An additional step is introduced for the LLM to provide more context before self-evaluation; 2). The LLM's confidence is incorporated as a weight for the reward to regulate its influence; 3). The backpropagation mechanism is retained, allowing for rewards updates if initially misallocated. While traditional MCTS focuses resources on simulations for an approximate reflection of reality, our approach distributes resources across multiple steps to jointly ensure the accuracy and objectivity of rewards. This is why we named our project MASTER, as it replaces simulation, the core procedure of MCTS, by mastering a series of refined designs.\nAnother contribution of this paper is the introduction of a novel multi-agent system. Current multi-agent systems feature two prominent frameworks: the first allows agents to be independently created and to share ideas freely. While versatile, this open communication can lead to off-topic discussions due to hallucinations (Lin et al., 2024; Hong et al., 2023; Xi et al., 2023; Zhang et al., 2024a), diverting focus from the main task and depleting the token window length for extended conversation history. The second approach involves human-created agents, where communication is predefined. Although more controlled, this approach lacks code reusability (Chu et al., 2023). Additionally, since the procedures are established, it cannot adapt to tasks of varying difficulty. It struggles with unanticipated complex tasks on the one hand and spends unnecessary resources on easy tasks on the other.\nOur system, MASTER, addresses these limitations by employing LLM-specialized MCTS to guide the creation and interaction of agents. In this system, child agents respond to and build upon the outputs of the parent agent, making recruitment and communication more manageable and efficient. The system dynamically scales the number of agents based on task complexity, ensuring flexibility. Although the agents share similar profiles, they assume different roles by taking distinct actions. Unlike the nodes in LATS (Zhou et al., 2024) and RAP (Hao et al., 2023), which represent states on a reasoning tree and are closely tied to specific tasks, our agents are task-agnostic and do not require reconfiguration when the task changes.\nIn summary, our key contributions are:\n1. We propose a novel Multi-Agent System with Tactical Execution and Reasoning using LLM Specialized MCTS (MASTER), a novel multi-agent framework that employs a new agent recruitment process and communication protocol based on the MCTS algorithm. The system autonomously adjusts the number of agents according to task complexity and mitigates distractions and shortage of token window during agent communication.\n2. We introduce a modified version of MCTS tailored to LLM. This adaptation is suitable for tasks where the environment does not provide objective feedback, addressing a limitation of the original MCTS. This revised MCTS is implemented within our MASTER framework.\n3. We conduct comprehensive experiments across diverse tasks, including Question Answering (HotpotQA), Decision Making (WebShop), and Programming (MBPP). It achieves 76% accuracy on HotpotQA and 80% on WebShop, setting new SOTA on these datasets."}, {"title": "2 Related Work", "content": "Many studies have been done to enhance the planning capabilities of LLM. Among these efforts, two primary planning approaches have emerged for agents: Single-Path Planning and Tree-Based Planning. In the context of multi-agent systems, current frameworks predominantly employ either Predefined Frameworks or Open Frameworks depending on their agent communication patterns. We discuss some of the relevant works in this section."}, {"title": "2.1 Planning Processes", "content": ""}, {"title": "2.1.1 Single-Path Planning", "content": "In single-path planning, the LLM follows one trajectory at a time, without branching into multiple possibilities. Early examples include Few-Shot Prompting (Brown et al., 2020), where the LLM is guided by examples of completed tasks, and Chain of Thought approaches (Wei et al., 2022; Kojima et al., 2023; Ning et al., 2024), which require the LLM to reason step-by-step, maintaining a linear trajectory throughout the process. Zhang et al. introduce a structured meta-prompt with placeholders for the LLM to complete (Zhang et al., 2024c) while Suzgun and Kalai provide task-related information to guide the model's path (Suzgun and Kalai, 2024). Single-path planning also benefits from external feedback to refine solutions. ReAct (Yao et al., 2023b) integrates feedback from the environment, and Reflexion (Shinn et al., 2023) supplements this with verbal reasoning based on the received feedback. Chen et al. use outputs and error messages from a code interpreter to assist the LLM in debugging (Chen et al., 2024c), while Qiu et al. leverage outputs from a symbolic interpreter to enhance the LLM's inductive reasoning capabilities (Qiu et al., 2024)."}, {"title": "2.1.2 Tree-Based Planning", "content": "In complex problem-solving, it is often beneficial to explore multiple thought trajectories and backtrack as needed. Tree-based planning organizes these thoughts into a tree structure, on which a search algorithm is applied. For example, BFS/DFS is employed in the Tree of Thoughts (Yao et al., 2023a). RAP (Hao et al., 2023) and LATS (Zhou et al., 2024) utilize MCTS to approximate reality and support the LLM's reasoning processes. However, how much the simulation process contributes to their success remains uncertain due to those two issues mentioned in Introduction section. To address the first challenge, RAP prompts the LLM with \"Is this reasoning step correct?\" and uses the next-word probability of \"yes\" as a reward, thus leveraging the LLM's evaluation capabilities without relying on external criteria. For the second challenge, RAP reduces costs by performing only one simulation. From a mathematical perspective, it is questionable that one simulation can accurately approximate the real reward. Some methods stop simulations once a correct answer is found. However, this early termination mechanism is unavailable if the ground truth is not revealed."}, {"title": "2.2 Multi-Agent Systems", "content": ""}, {"title": "2.2.1 Predefined Framework", "content": "In predefined frameworks, the recruitment and communication of agents are structured in advance. For instance, ChatDev (Qian et al., 2023) and MetaGPT (Hong et al., 2023), both tailored for software development, rely on predefined workflows. Similarly, AutoAgents (Chen et al., 2024a), a framework designed for automatic agent generation, follows a predefined structure. These frameworks have been criticized for their heavy dependence on upfront planning and their lack of flexibility in responding to changing requirements (Pargaonkar, 2023)."}, {"title": "2.2.2 Open Framework", "content": "Conversely, open frameworks offer greater flexibility, allowing agents to interact more dynamically. For example, AgentVerse (Chen et al., 2024b) recruits agents with the freedom to communicate openly, while CAMEL (Li et al., 2023) explores a two-agent system. Additionally, AutoGen (Wu et al., 2023) facilitates next-generation LLM applications by enabling multi-agent conversations, allowing for adaptive and context-aware interactions. As noted by Wei et al., effective multi-agent collaboration in these open frameworks requires autonomous systems to regulate communication, ensuring that agents know when and with whom to interact to avoid the exchange of irrelevant information which is challenging for these frameworks (Wei et al., 2023)."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Preliminaries", "content": "Before introducing our framework, we present MCTS to clarify the motivation behind our work. MCTS (Coulom, 2006) is a widely used planning algorithm and famously employed in AlphaGo (Silver et al., 2016). Taking the Game of Go as an example, the algorithm assists in selecting the best possible action in the current state of the board based on their average rewards. These rewards are obtained through numerous simulations. For instance, consider the current state where action $a_1$ is chosen as the next move. The game then proceeds to completion, with all subsequent actions, whether by our side or the opponent, determined by a policy model rather than a real player. The entire game sequence constitutes one simulation of action $a_1$. If we win, the reward is 1; otherwise, it is 0. Specifically, if we simulate 10 games from the current state with action $a_1$ and win 9 of them, the average reward for $a_1$ would be 0.9. However, due to the vast action space in the Game of Go, it is impractical to simulate every possible action. The Upper Confidence Bound 1 applied to Trees (UCT) identifies actions with higher potential to win, allocating more simulations to these actions rather than distributing simulations equally among all actions. Once an action is decided based on this process and physically executed, leading to a new game state, the same procedure is then applied to select actions from this new state, and the planning continues until the actual end of the Game of Go.\nMCTS typically involves four key procedures:\nSelection: Traverse the current reasoning tree to select the node with the highest UCT for simulation.\nExpansion: Extend the reasoning tree by adding new child nodes from the selected node. Simulation: Continue expanding from a child node until the task is completed. Backpropagation: After each simulation, update the average reward of corresponding node with the newly obtained simulation reward."}, {"title": "3.2 Framework of MASTER", "content": "In our framework, MASTER, the concepts of reasoning tree, selection, expansion, and backpropagation are analogous to those used in MCTS. However, we eliminate the simulation step due to those two issues highlighted in the Introduction section. Instead, our framework incorporates three special mechanisms to derive rewards: providing more context before self-evaluation, incorporating LLM's confidence in our novel UCT formula, and updating rewards in backpropagation. The details of our framework are demonstrated in Figure 1 and further explained in following paragraphs.\nWhen our framework is presented with a task, it initializes a root agent with the problem. This agent then prompts the LLM to generate text that reflects on the current reasoning trace (Thought) and proposes an action to solve the problem (Action). The model uses a temperature of 0.6 to encourage more diverse thoughts and actions in the reasoning tree. The agent then executes this action using tools that interact with the external environment, producing feedback (Observation). The texts, including Thought, Action, and Observation, form the agent's Solution, as illustrated in Figure 1. Subsequently, the agent prompts the LLM to generate a textual output that verifies key facts in the current Solution (Validation). Finally, based on the Solution and Validation, the agent prompts the LLM to evaluate progress toward solving the problem, generating both a score and the LLM's confidence in this score (Assessment). These two values (score and confidence) are then extracted. The model uses a temperature of 0.0 for Validation and Assessment to ensure more stable decisions. All texts (Solution, Validation, and Assessment) form the agent's Context and are stored in its memory (Figure 1). The score serves as the agent's initial reward.\nThe previous paragraph describes the generation of a single agent in our system. After the root agent is generated, child agents are created following the same procedure, with the root agent's Context appended to the prompts of these child agents, as they need to continue solving the problem. As illustrated in Figure 1, two child agents (Agent\u2081 and Agent2) are generated using exactly the same procedure and prompt to explore diverse reasoning paths from the same state (parent agent). The number of child agents is a hyperparameter, Number of Branches, which varies depending on the task. The creation of these child agents is termed Expansion from the parent agent.\nFurther expansion can be carried out from any existing agent using the same procedure. The UCT of each agent is calculated, and the agent with the highest UCT is selected for further expansion. Another hyperparameter, Maximum of Expansion, represents the approximate number of steps required to solve the problem, allowing users to set it based on their understanding of the task. If this limit is reached without finding a satisfactory solution, the solution from the terminal agent with the highest reward is submitted as the final answer.\nDuring the expansion of the reasoning tree, agents that generate a final answer rather than an intermediate step in their Solution, are called Terminal Agents. For instance, in the HotpotQA task, if an agent's Action is 'Finish[]', it is identified as a terminal agent, as this Action indicates a final answer. Similar indicators exist in the Solution of other tasks. During the Evaluation (Figure 1) which applies only to Terminal Agents, the LLM assesses the correctness of the Solution. If the solution is deemed correct, it is submitted as the final answer, concluding the task. If not, Backpropagation is triggered, using the reward from this terminal agent to update the rewards of all agents on the path up to the root agent. Pseudo-code can be found in Appendix A."}, {"title": "3.3 Formula of Modified UCT", "content": "Recent works RAP and LATS directly apply the original UCT formula. To better suit our design, we propose a modified UCT formula.\nThe original UCT formula is derived from Hoeffding's Inequality (Lattimore T, 2020) which can be found in Appendix B. It is typically applied in the following scenario: Given a node representing a state (referred to as noden), there are multiple subsequent actions to choose from (e.g., ai, aj, ak). To determine the Q value of these actions, multiple simulations are conducted, and UCT is employed to decide which action should be simulated. Instead of simply selecting the node with the highest Q value (pure exploitation, the first term in Eqn 1), UCT balances exploitation and exploration by incorporating an exploration term (the second term in Eqn 1) that favors nodes with fewer simulations. The node, representing the state resulting from action ai, is one of the child nodes of nodeh. The UCT formula for node is:\n$UCT = Q_i + \\sqrt{2\\frac{\\ln (N_i)}{n_i}}$\n$Q_i = \\frac{\\sum_{n=1}^{n_i} r_n}{N_i}$\nWhere ni is the number of backpropagations applied to the nodei. rn is the reward of the n-th backpropagation. Qi is the estimation of Q value calculated by Eqn 2. It represents the average reward from simulations. Ni is the total number of simulations by the parent node of the current node which is nodeh. In other words, Ni is the sum of ni, nj and nk in above example although nj and nk are not explicitly shown in the UCT formula. In our system, Qi, as the estimation of Q value, consists of two components: Initial Reward is extracted from Assessment when this agent is generated. Updating Reward is the mean of rewards from backpropagation, similar to Qi in Eqn 2. Inspired by the research on auxiliary information about rewards in the form of control variables (Verma and Hanawal, 2021), we modify the reward estimation in our system as shown in Eqn 3:\n$Q_i = c_0 r_0 + (1 - c_0) \\frac{\\sum_{n=1}^{n_i}r_n}{N_i}$\nWhere ro is the initial reward given by LLM. c\u043e is the confidence of the LLM to this initial reward. rn and ni are the same with Eqn 2.\nThe original MCTS relies heavily on numerous simulations to make this estimation accurate. Consequently, it becomes unreliable when the number of ni is small. On the other hand, our Q value has an extra component (initial reward), as our Q value is the weighted sum of initial reward and updating reward with the confidence as weight. When LLM has high confidence to the initial reward it assigns, the influence of rewards from backpropagation (updating reward) is reduced due to its lower weight (1 - Co). Conversely, when the LLM has low confidence, updating reward is required to dominate the Q value estimation while the weight of updating reward (1 \u2013 co) is higher. Notably, the number of backpropagation ni is automatically adapted to each question rather than being manually set by users. For complex tasks, the model requires more attempts to obtain an acceptable answer, with each failed attempt triggering a backpropagation. For simple question, the model may produce an acceptable result in their first attempt, eliminating the need for backpropagation. This early termination mechanism completes the task while reducing token consumption.\nIn the formula used by RAP and LATS, an exploration constant A replaces the fixed 1/\u221a2 as the weight of the exploration term as following:\n$UCT = Q_i + A\\cdot \\sqrt{\\frac{\\ln (N_i)}{N_i}}$\nIn our approach, we use 1/ (10\u221a2co) as the exploration weight. The significance of this adjustment is twofold: 1). The exploration term reflects the uncertainty associated with this agent. When the LLM has low confidence in the initial reward, the agent's uncertainty is relatively high, necessitating more exploration. In such cases, a higher exploration weight increases the UCT, guiding the algorithm to select this agent for further exploration; 2). The number of backpropagations in our system is significantly lower than in the original MCTS while the slope of the logarithmic function is relatively steep when the variable is small, so the value of the exploration term tends to play a dominant role. Therefore, this exploration weight should be used to control the influence of this term. Moreover, when the minimum confidence of 0.1 is used, the exploration weight equals 1/\u221a2, the same as the weight in Eqn 1.\nIn summary, the revised formula in our system is:\n$UCT = \\begin{cases}\nc_0 r_0 + (1 - c_0) \\cdot \\frac{\\sum_{n=1}^{n_i} r_n}{N_i} + \\frac{1}{10\\sqrt{2c_0}} \\cdot \\sqrt{\\frac{\\ln N_i}{N_i}} & \\text{if } n \\neq 0\\\\\nr_0 & \\text{otherwise}\n\\end{cases}$"}, {"title": "3.4 Strategies of Reward Assignment", "content": "To conclude, the three special mechanisms we implement to ensure the reliability of rewards in our framework are:\n1. Before assigning a reward in the Assessment phase, an additional Validation step is conducted, where the LLM comments on the correctness of the facts in the current solution. These comments are added to the prompt for the Assessment step, guiding the LLM toward a more reliable reward. This design is motivated by the observation that the LLM performs better when it is asked to address one problem at a time. Separately verifying correctness and progress can lead to stable and reliable scoring.\n2. In the Assessment phase, the LLM is asked to provide both a score and its confidence in that score, rather than just the score alone. The confidence value plays two roles in our modified UCT formula, which is detailed in the Formula of Modified UCT subsection. If the LLM has low confidence in the score it provides, the influence of this score is reduced and the likelihood to select this agent for further exploration is increased.\n3. Backpropagation occurs after each simulation in the original MCTS. Although we have removed simulations, backpropagation is retained in our framework. It is triggered whenever a terminal agent produces a Solution that fails Evaluation. A failed Evaluation indicates that the reasoning steps leading to this terminal agent may be flawed, and their Q value should be reduced accordingly. This mechanism allows for the adjustment of rewards if they are inaccurate at the outset."}, {"title": "4 Experiment Setup", "content": "To demonstrate the generalizability of our framework, we conduct experiments across a diverse set of tasks, including question answering (HotpotQA), decision making (WebShop), and coding (MBPP). These datasets are widely recognized benchmarks in their respective domains.\nIn addition to assessing effectiveness and efficiency, we also performed ablation and parameter studies to investigate the contributions of each mechanism in our framework and the impact of hyperparameters."}, {"title": "4.1 Datasets", "content": "HotpotQA: (Yang et al., 2018) tests multi-hop reasoning in LLM, requiring models to parse and reason across multiple paragraphs. We used the Distractor setting, where the task is to answer the question with a mix of relevant and irrelevant paragraphs context. WebShop: (Yao et al., 2022) simulates an e-commerce environment to test decision-making abilities. The task involves navigating a virtual store to find products that best match a given instruction, with success measured by how closely the selected product matches the requirement. MBPP: (Austin et al., 2021)) assesses coding abilities. Each task includes a problem description and test cases for validation. In our system, agents generate and test complete code, with child agents iteratively improving on errors identified by their parent agents. A task is considered solved when the generated code passes all test cases."}, {"title": "4.2 Baselines", "content": "Since we use GPT-4, a highly capable LLM, as the base model, we take GPT-4 itself without any agent, as a baseline. It is prompted with the task description and the same examples as our framework to solve tasks in a single call (Few-shot Chain-of-Thought). Notably, in this setting, GPT-4 cannot solve HotpotQA and WebShop problems because these tasks require multiple interactions with the environment. For HotpotQA, the model must generate search keywords, receive the retrieved context from the environment, and decide whether to search for additional context or answer the question. For WebShop, the model must purchase a target item on a mock website through multiple search and click actions. Expecting GPT-4 to perform these actions without environment feedback in a single call is impractical. When incorporating external environment feedback to enable multi-turn interactions, the setup becomes identical to the experimental conditions of ReAct. In other words, ReAct's performance in Table 1 serves as an indicator of the base model's capacity in these two tasks.\nReAct and Reflexion are well-known methods in the planning domain, and our work integrates some of their ideas. LATS, like our approach, is a tree-based method and demonstrates strong performance. Therefore, we compare against these baselines across all three datasets. MetaGPT and AgentVerse, two representative multi-agent systems, serve as our benchmarks in the multi-agent setting. We evaluate them only on MBPP because MetaGPT is specifically designed for programming tasks, while AgentVerse requires execution tools, and only the tool for programming tasks is available.\nAdditionally, we benchmark our framework against the current state-of-the-art (SOTA) for each dataset: Beam Retrieval for HotpotQA, AgentKit for WebShop, and AgentCoder for MBPP. However, AgentKit is evaluated on two datasets in its original paper (Crafter and WebShop), while its GitHub repository provides code only for Crafter. Moreover, the available code for Crafter cannot be adapted to WebShop due to insufficient implementation details. Consequently, we rely on the original performance claims from their paper. Nevertheless, given the substantial performance gap between our results and theirs, we believe our SOTA claims remain highly plausible."}, {"title": "4.3 Implementation Details", "content": "Given the high cost of GPT-4, we randomly select a sample of 100 questions from each dataset, following the approach used in Reflexion (Shinn et al., 2023) and LATS (Zhou et al., 2024). To ensure fairness, the same random seed is used across all three datasets to select these 100 questions.\nTo mitigate the effect of LLM randomness on accuracy, we repeat each experiment three times on the same samples and report the mean accuracy in Table 1. Our framework effectively controls LLM randomness through the strategies outlined in the Methodology section, where multiple steps support and verify each other."}, {"title": "5 Results and Analysis", "content": ""}, {"title": "5.1 Effectiveness Analysis", "content": "We reproduce all baseline approaches, except for AgentKit due to insufficient information, with GPT-4 as base model on the same 100 questions and record the results. Additionally, we compare these results with those reported in the respective papers. The better result is used as the final value for each baseline in Table 1. This method favors baseline approaches and ensures that our framework demonstrates superior performance across various standards.\nMASTER sets new SOTA performance across multiple tasks: 1). 76.0% Exact Match accuracy on HotpotQA, surpassing Beam Retrieval's 73.3% (Zhang et al., 2024b); 2). 80.0% accuracy on WebShop, exceeding AgentKit's 70.2% (Wu et al., 2024). On MBPP, it closely matches AgentCoder's 91.8% pass@1 accuracy with 91.0% (Huang et al., 2024), showing competitive performance in programming task."}, {"title": "5.2 Efficiency Analysis", "content": "As a tree-based method, token consumption is a concern due to the diverse reasoning trajectories. However, by removing the simulation step and introducing an early termination mechanism, our framework achieves higher efficiency compared to other tree-based methods using MCTS. We benchmark our efficiency against LATS because: 1). LATS is a typical tree-based approach with superior performance compared to similar frameworks; 2). LATS reports the lowest token consumption when compared with ToT (Yao et al., 2023a) and RAP (Hao et al., 2023) in their paper.\nWe measure token consumption for LATS (n = 5, k = 50) and MASTER (number of branches = 2, maximum of expansion = 3) on the same 100 HotpotQA questions, using the same hyperparameter settings as those in the effectiveness analysis experiments. The average cost per question was 185,392 tokens for LATS and 10,937 tokens for MASTER. Our approach uses only about 6% of the tokens compared to LATS while delivering better performance (Table 1).\nLATS (Zhou et al., 2024) reports an average cost of 173,290 tokens per question, lower than our reproduced results. This discrepancy may be due to the fact that their tests were conducted on correctly answered questions, while our tests included some incorrectly answered questions, which tend to consume more tokens as the algorithm continues until reaching the maximum trial limit."}, {"title": "5.3 Ablation Study", "content": ""}, {"title": "5.3.1 UCT Modification", "content": "In MCTS, the UCT formula balances exploration and exploitation. One of our main contributions is adapting this formula to better accommodate LLM. We evaluate the impact of removing components of our modified UCT formula, considering the following cases (Table 2):\n1. Our full modified formula, incorporating the weighted sum of initial and updating rewards, and exploration weight influenced by the LLM's confidence (Eqn 5).\n2. A variant with the weighted sum of rewards and a fixed exploration weight (Eqn 6). This differs from the full formula by not incorporating the LLM's confidence in the exploration term.\n$UCT = c_0\\cdot r_0 + (1 - c_0) \\cdot \\frac{\\sum_{n=1}^{n_i}r_n}{N_i} + \\sqrt{\\frac{\\ln (N_i)}{N_i}}$\n3. A variant using only the weighted sum of rewards (Eqn 7), removing the entire exploration term.\n$UCT = c_0\\cdot r_0 + (1 - c_0) \\cdot \\frac{\\sum_{n=1}^{n_i}r_n}{N_i}$\n4. A variant using only the initial reward for exploitation (Eqn 8), excluding exploration term and updating reward from backpropagation.\n$UCT = r_0$\nWhen using UCT with a fixed exploration weight (case 2), performance declines on two out of three datasets, even worse than the variant without the entire exploration term. As discussed in the Methodology section, the exploration term plays a dominant role and should be modulated by the exploration weight. This result supports that hypothesis, as the system sometimes over-explores without progressing toward task completion when using this formula. Omitting a dynamic exploration weight based on confidence is harmful to the system.\nUsing the weighted sum of initial and updating rewards (case 3) performs better than using the initial reward alone (case 4), likely because the updating reward incorporates additional information from deeper in the reasoning tree.\nThe MBPP results indicate that different UCT variants have no significant effect on this dataset. This outcome likely arises because the Observation, represented by the test case results, is objective. The Observation is appended to the prompt of the Assessment, allowing the LLM to assign rewards with high confidence. When co is 1, all UCT variants reduce to case 4 or very close to it. Therefore, all settings yield identical or nearly identical outcomes."}, {"title": "5.3.2 Agent Design", "content": "The validation step before assessment is another key feature of our framework. We conduct additional ablation studies, removing the validation and assessment steps individually. Since the initial reward is derived from the assessment and the algorithm cannot function without it, we assign random initial rewards when the assessment step is removed (Table 3).\nPerformance drops significantly when the validation or assessment step is removed, even for the MBPP dataset, as validation greatly impacts reward allocation, the core component of the MCTS. Additionally, with random rewards assigned when the assessment step is removed, performance essentially relies on the LLM alone, or worse."}, {"title": "5.4 Parameter Study", "content": "We conduct parameter studies to determine optimal values for two key hyperparameters: Number of Branches (Table 4) and Maximum of Expansion (Table 5), across all three datasets.\nThe Number of Branches has little effect on performance in WebShop and MBPP. However, on HotpotQA, performance drops by nearly 3% when reducing it from 2 to 1. This reduction may hinder the system, as multiple reasoning trajectories help prevent getting stuck in incorrect states. Although WebShop faces similar challenges, the agent can use the dataset in-built action 'prev' to mitigate, though not completely avoid, this problem. To balance performance and cost, we use 2 for all datasets.\nWe select 1, 3, 8 for the Maximum of Expansion here because in our other experiments, 3 is used in HotpotQA and MBPP while 8 is used for WebShop. Normally, questions in HotpotQA and WebShop require multiple steps to solve so their performance drops dramatically when the Maximum of Expansion is lower than the steps needed to solve the problem, as they are forced to stop before getting an answer."}, {"title": "6 Conclusion", "content": "This paper introduces MASTER, a novel multi-agent system framework that leverages a specialized MCTS to enhance the planning capabilities of LLM. Our LLM-optimized MCTS broadens the applicability of MCTS to a wider range of tasks with reduced costs. Besides, we employ this algorithm to guide agent recruitment and communication protocols, thereby introducing an innovative form of multi-agent system. Extensive experiments across various datasets demonstrate MASTER's effectiveness and efficiency over existing frameworks."}, {"title": "7 Limitations", "content": "There are some limitations in our framework. Firstly, it relies heavily on the LLM's ability to provide accurate scores and confidence assessments of the current reasoning state. While GPT-4 performs this task effectively, smaller open-source models may encounter challenges at this step. Additionally, users must configure certain hyperparameters for the system, including the Maximum of Expansion and the Number of Branches. The optimal values for these parameters may vary depending on the specific task."}, {"title": "A Pseudo Code", "content": "The pseudo code of our framework is demonstrated in this section."}, {"title": "B Deduction of UCT Formula", "content": "In the methodology section, we proposed a modification to the UCT formula. Here, we provide the derivation of the original UCT"}]}