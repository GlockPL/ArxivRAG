{"title": "GUI-Bee : Align GUI Action Grounding to Novel Environments via Autonomous Exploration", "authors": ["Yue Fan", "Handong Zhao", "Ruiyi Zhang", "Yu Shen", "Xin Eric Wang", "Gang Wu"], "abstract": "Graphical User Interface (GUI) action grounding\nis a critical step in GUI automation that maps\nlanguage instructions to actionable elements on\nGUI screens. Most recent works of GUI action\ngrounding leverage large GUI datasets to fine-\ntune MLLMs. However, the fine-tuning data\nalways covers limited GUI environments, and\nwe find the performance of the resulting model\ndeteriorates in novel environments. We argue\nthat the GUI grounding models should be further\naligned to the novel environments to reveal their\nfull potential, when the inference is known to\ninvolve novel environments, i.e., environments\nnot used during the previous fine-tuning. To re-\nalize this, we first propose GUI-Bee, an MLLM-\nbased autonomous agent, to collect high-quality,\nenvironment-specific data through exploration\nand then continuously fine-tune GUI grounding\nmodels with the collected data. Our agent lever-\nages a novel Q-value-Incentive In-Context Re-\ninforcement Learning (Q-ICRL) method to op-\ntimize exploration efficiency and data quality.\nAdditionally, we introduce NovelScreenSpot, a\nbenchmark for testing how well the data can help\nalign GUI action grounding models to novel en-\nvironments and demonstrate the effectiveness of\ndata collected by GUI-Bee in the experiments.\nFurthermore, we conduct an ablation study to\nvalidate the Q-ICRL method in enhancing the\nefficiency of GUI-Bee. Project page: https:\n//gui-bee.github.io.", "sections": [{"title": "1. Introduction", "content": "GUI action grounding maps natural language instructions to\nspecific executable elements or locations on a GUI screen. It\nis critical as being a key step adopted by GUI agents to assist\nhumans in complex digital environments, where GUI agents\nusually generate step-by-step instructions and then rely on\nthe action grounding to locate corresponding executable\nGUI elements(Agashe et al., 2024; Zheng et al., 2024a;\nKoh et al., 2024a). As the planning ability of MLLMs\nshines, their relatively weak zero-shot grounding perfor-\nmance becomes a key bottleneck in building strong GUI\nagents (Zheng et al., 2024b), and GUI action grounding\nhas become one of the focus of recent specialized model\ndevelopment efforts (Gou et al., 2024; Cheng et al., 2024;\nLiu et al., 2024; Chen et al., 2024).\nMost advanced GUI models capable of performing GUI\naction grounding tasks are fine-tuned from pre-trained\nMLLMs using substantial amounts of training data within\nthe GUI domain. While the training data for these tasks\nmay inevitably leave certain environments uncovered, prior\nresearch has largely relied on the generalizability of these\nmodels to transfer skills learned from the training data to\nnovel environments. However, in real-world use cases, the\naction grounding task often requires environment-specific\nknowledge that is unique to particular settings and difficult\nto generalize across environments. As illustrated in the in-\nference example at the bottom of Figure 1, the query relates\nto the action outcome of a triple-dot icon in the layer panel.\nGUI action grounding models not trained within the spe-\ncific environment can hardly know that the triple-dot icon\nreveals layer shadow options, making it unlikely for the\nmodel to ground the query correctly. Drawing from daily\nexperience where prior familiarity with an environment\nallows humans to navigate it more effectively-we argue\nthat when the inference environment is known to involve\nsome novel environment, aligning GUI action grounding\nmodels to the novel environment can significantly enhance\ntheir performance.\nIn this work, we propose to align GUI action grounding\nmodels, originating from pre-trained MLLMs to novel GUI\nenvironments that are not trained on during the previous fine-\ntuning process, as shown in the top of Figure 1. This method\nenables GUI developers to strengthen action grounding mod-\nels for their specific novel use cases. The process mainly\nincludes efficiently collecting high-quality data in the infer-"}, {"title": "2. Related Works", "content": null}, {"title": "2.1. GUI Grounding with MLLMS", "content": "As GUI agents evolve from text-based (Sodhi et al., 2024;\nZhou et al., 2023) to multimodal systems (Deng et al.,\n2023; Koh et al., 2024a; Xie et al., 2024), GUI action\ngrounding-linking natural language queries to GUI ele-\nments-has become a key challenge and shifted from being"}, {"title": "2.2. In-Context Learning", "content": "In-context learning (ICL) refers to the method of adapting\nmodels to new tasks by providing context (Brown, 2020;\nChan et al., 2022; Wang et al., 2023). By including exam-\nples directly in prompts, ICL allows large language models\n(LLMs) to generalize to unseen tasks (Garg et al., 2022;\nPan, 2023; Wei et al., 2023). Prior works have explored\napplying ICL to reinforcement learning (RL) either with\nmodel training involved (Laskin et al., 2022; Lee et al.,\n2024; Xu et al., 2022) or by directly leveraging pre-trained\nLLMs (Krishnamurthy et al., 2024; Monea et al., 2024).\nWe propose the Q-value-incentive In-context Reinforcement\nLearning (Q-ICRL), which also utilizes pre-trained LLMs\nbut distinguishes itself by using ICL to predict state-action\nvalues. Our approach combines the adaptability of LLMs\nwith RL's optimization-driven structure, enabling efficient\naction selection in the GUI environment exploration."}, {"title": "3. Aligning GUI Action Grounding Models to\nNovel Environments with GUI-Bee", "content": "In this work, we focus on aligning GUI action ground-\ning models to novel GUI environments that are previously\nnot involved in the model training. It is a solution that\nenables GUI developers to build models tailored to their\nspecific use cases. To realize, we leverage the proposed\nGUI-Bee agent, which autonomously collects data enriched\nwith environment-specific knowledge through exploration\nand data annotation. Using this data, we continuously fine-"}, {"title": "3.1. Autonomous Exploration via GUI-Bee", "content": null}, {"title": "3.1.1. EXPLORATION GOAL", "content": "The goal of the exploration process is to construct an ex-\nploration graph G, where GUI screens I are represented as\nunique nodes and GUI actions a form the edges connecting\nthese nodes, corresponding to screen transitions. During\nexploration, GUI-Bee predicts actions to interact with the\nGUI and captures the screens before and after each action\nto populate the graph. The process begins from a predefined\ninitial GUI screen i\u00b9 at exploration step 1. At each sub-\nsequent step t, t \u2208 [1, tmax], where tmax is the maximum"}, {"title": "3.1.2. CHALLENGES IN THE EXPLORATION", "content": "The exploration process faces several challenges, starting\nwith identifying valid actions within a noisy action space.\nThe set of action candidates Aenv (it), obtained from the en-\nvironment, is defined as $a_t \\in A_{env}(i_t) = \\{a_1, a, ..., a_n\\}$,\nand can originate from sources such as a Document Object\nModel (DOM) tree or a separate module Maction (it), like\nOmniParser (Lu et al., 2024). However, Aenv (it) often in-\ncludes invalid actions targeting non-executable elements,\nrequiring the agent to discern optimal actions that are both\nvalid and meaningful for exploration.\nAnother significant challenge lies in the uncertainty of\nscreen transitions. The outcomes of executing candidate\nactions are unknown beforehand, and transitions are of-\nten irreversible, complicating the decision-making process.\nSuccessful exploration covers as diverse content as possible\nfrom the environment, which demands a balance between\nexploring new states and exploiting known beneficial ac-"}, {"title": "3.1.3. Q-VALUE-INCENTIVE IN-CONTEXT\nREINFORCEMENT LEARNING (Q-ICRL)", "content": "We consider the exploration process as a Markov Decision\nProblem, which is defined by a tuple (S, A, P, r), where\nS denotes the state space, A represents the action space,\nP:S\u00d7A\u00d7S \u2192 {0,1} is the state transition proba-\nbility function, and r: S\u00d7A \u2192 R denotes the reward\nfunction. At each exploration step t \u2208 N, the GUI-Bee\nagent is at St \u2208 S, and takes an action at \u2208 A on the\ncurrent observed screen it which transitions to a new state\nSt+1 \u2208 S with probability P(St+1|St, at), receiving a re-\nward r(St, at). The reward r is binary, where it is positive\nwhen the at leads to a new screen not existing in the explo-\nration graph at the beginning of the current exploration step,\ni.e., it+1 \u2209 Gt-1. Accordingly, the state St is defined as\nthe exploration graph Gt-1 to satisfy the Markov property,\nwhere S\u00b9 contains only the initial screen. For simplicity,\nSt is approximated by a set of natural language descrip-\ntions Dt = {dk | dk = Describe(ak,Gt-1),ak \u2208 Gt-1},\nwhere ak represents the edges in the exploration graph and\nDescribe() uses an MLLM to generate descriptions for the\nactions and screens before and after the action. Further\ndetails on this process are provided in Appendix C.\nWe propose the Q-value-Incentive In-Context Reinforce-\nment Learning (Q-ICRL) method, outlined in Algorithm 1,\nto maximize the accumulated rewards in the exploration.\nQ-ICRL quantifies the future rewards of executing actions\nat at St with a Q-value function Q(St, at). Unlike tra-\nditional Q-learning, Q-ICRL is a training-free algorithm\nas the Q is mainly memory-based, which is detailed in\nthe following subsection. To select an action at at state\nSt, Q-values Q(St, a) are first used as weights to sam-\nple a subset A'env(it) of length H from the action space\nAenv (it). Then, an MLLM with in-context learning is\nemployed to identify the most promising action at from\nA'env (it), guided by the Q-value function. This process in-\nvolves two steps: EXAMPLEACTIONIDENTIFICATION and\nINCONTEXTSCORING, both detailed in the next subsection.\nThis method ensures a balance between exploration and\nexploitation. Once an action at is selected, it is executed in\nthe environment, leading to an updated exploration graph\nGt+1 = Gt \u2295 (at, it+1), where \u2295 denotes adding a new\nnode and edge to the graph. The Q-value function is updated\naccordingly, as described in 3.1.1.\nQ-value Function Given a in Aenv(it), the Q(St, a)\nshould be relatively low if executing it leads to a it+1 that\nis repeated in St, however, as mentioned in 3.1.2, the it+1"}, {"title": "4. NovelScreenSpot Benchmark", "content": "We propose NovelScreenSpot, a benchmark for evaluating\nGUI action grounding models in five diverse web GUI envi-\nronments and their performance improvements after they are\ncontinuously fine-tuned with new data. It includes triplets of\nqueries, screens represented by screenshots and accessibility\ntrees, and ground-truth bounding boxes for the grounding\ntargets. We show the statistics of the NovelScreenSpot in\nTable 1 and examples in Appendix G.\nNovelScreenSpot simulates real-world GUI model deploy-\nment scenarios, where App owners want to evaluate GUI\naction models on their own specific GUI environments, po-\ntentially novel environments that the models are not previ-\nously trained on. Therefore, unlike existing benchmarks that\nemphasize diversity across many environments, such as the\nScreenSpot (Cheng et al., 2024), NovelScreenSpot instead\nprovides a greater data variation within each environment.\nNotably, the NovelScreenSpot includes a large number of\nqueries, around one-third of the total benchmark, focusing"}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Alignment to Novel GUI Environments", "content": "In the first experiment, we leverage the NovelScreenSpot\nbenchmark to evaluate how GUI-Bee collected data help\nalign GUI action grounding models to novel environments.\nWe first select several GUI action grounding models and\nidentify the GUI environments within the NovelScreenSpot\nthat are novel for them. Then we attempt to align the models\nto their corresponding novel environments.\nSetups We employ the GUI-Bee agent to explore the five\nenvironments in NovelScreenSpot, conducting up to 400\nexploration steps per environment with three candidate ac-\ntions sampled at each step. To ensure diverse screen data,\nthe exploration is repeated three times per environment at\nvarying screen resolutions. In this work, we adopt GPT-40\n(OpenAI, 2024) as the multimodal large language model\n(MLLM), though other MLLMs can also be integrated into\nthe framework. The resulting exploration statistics are sum-\nmarized in Table 1. Using the data generated from these\nexplorations, we continuously fine-tune four GUI grounding\nmodels: SeeClick (Cheng et al., 2024), Qwen-GUI (Chen\net al., 2024), and UIX-7B (Liu et al., 2024) to adapt to novel\nenvironments within NovelScreenSpot. Additionally, for\nthe alignment of Qwen-GUI and UIX-7B, we leverage both\nvision-only and Vision+Ally data, as these models were\npreviously fine-tuned to accommodate similar formats. Fur-\nther details on exploration configurations, data formatting,\nand fine-tuning settings are provided in Appendix E."}, {"title": "5.2. Exploration Efficiency Evaluation", "content": "To further evaluate the efficiency of our GUI-Bee agent in\nexploring and generating data within GUI environments,\nwe task it, along with two baseline agents, to explore three\noffline GUI environments: Shopping, Classifieds, and Red-\ndit-from the Visual Web Arena (Koh et al., 2024a). These\nenvironments are reset to identical initial states at the begin-\nning of each exploration, ensuring that all agents start from\nthe same conditions and face equivalent challenges.\nEvaluation and Metrics To evaluate exploration effi-\nciency, we assess the diversity of actions and screens in\nthe exploration graph Gtmax generated by each agent under\nthe same maximum number of exploration steps tmax.\nFirst, we convert each exploration graph into Dtmax+1,\nthe natural language approximated RL state as described\nin Section 3.1.3. GPT-4o is then used to compare pairs\n(Dtmax+1, Dtmax+1) from two agents at a time to deter-\nmine which demonstrates broader coverage. This process,\nreferred to as the Relative Exploration Coverage Ranking,\nprovides an intuitive comparative measure of exploration\nbreadth.\nFurther, we introduce the Depth-fixed DOM Diversity\nCounts (D3C) metric to assess structural variation in the\nscreens within the exploration graph Gt generated by dif-\nferent agents objectively. D3C is defined as the number of\ndistinct page structures in the Gt. Each page structure is\ndetermined by truncating the DOM tree of a screen to a\nfixed depth, retaining only the class attributes of elements.\nBy counting the unique page structures within all the page\nstructures within Gt, we get the D3C value at the explo-\nration step t. With a fixed number of exploration steps, D3C\nprovides a quantitative measure of the agent's efficiency in\nuncovering diverse structural layouts, offering a clear and\nobjective metric for exploration breadth.\nBaselines To compare with our GUI-Bee agent, we intro-\nduce two baseline agents: the In-Context Reinforcement\nLearning (ICRL) method and the random exploration strat-"}, {"title": "6. Conclusion", "content": "This work introduces the GUI-Bee agent, a novel MLLM-\nbased approach for autonomously exploring GUI environ-\nments and collecting high-quality, environment-specific\ndata. We continuously fine-tune GUI action grounding mod-\nels to align to novel environments using this data. Through\nexperiments, we demonstrate significant improvements in\nmodel performance after the alignment, validating the effec-\ntiveness of our approach. Additionally, we propose novel\nmetrics to evaluate GUI-Bee against baselines in generating\ndiverse exploration data, further highlighting the efficiency\nand effectiveness of the Q-ICRL method."}, {"title": "A. GUI Elements Fuzzy Visual Matching", "content": "We develop a GUI Elements Fuzzy Visual Matching module Ffvm, to compare if two GUI elements can be recognized\nas visually the same. Challenges arise from the variations in GUI rendering; for example, web browsers could render the\nsample page with slight element shifts each time. Such variation can make pixel-perfect matching overly sensitive, leading\nto false negatives. Furthermore, dynamic elements on the screen, such as GIFs, can also cause variability unrelated to the\nexecuted action. Our solution is let Ffvm compares e to the corresponding patch e' cropped at the same location in it+1 for\neach GUI element e in it, and output a difference score $p = Max(F_{fvm}(e, e')), e \\in i_t, e' \\in T_{t+1}$. Specifically, a Gaussian\nfilter is first applied to each pair of e and e' to smooth rendering defects. Then e and e' are aligned with varying shifts,\nensuring a minimum overlap of 75%, to compute the maximum normalized pixel-wise difference $F_{fvm}(e, e')$. Finally,\nbased on the Ffvm(e, e'), e and e' are considered identical if Ffvm(e, e') \u2264 0.05.\nThe GUI Elements Fuzzy Visual Matching module is not only used for retrieving in-context examples aeg1 and aeg2 as\nmentioned in Section 3.1.3, it is also crucial for verifying whether the executed action at transitions it to a new screen it+1,\nand whether it+1 is already in the exploration graph, so that the nodes in the graph are unique. it+1 is regarded the same as\nit if p < 0.05. Additionally, for dynamic content, Ffvm compares the same elements across multiple screenshots over time\nfrom the same screen to identify inconsistent regions and excludes them when calculating p."}, {"title": "B. Generating GUI Action Grounding Queries (ut)", "content": "Once a new edge (it, at, it+1) is added during exploration, we send this information to the MLLM to generate ut, a list of\naction grounding queries for the target element et in it. The generation process uses a carefully crafted prompt, designed to\nensure the queries cover both System 1 (focused on current screen content) and System 2 (anticipating interaction outcomes)\ngrounding challenges. The full version of the text prompt for the MLLM is provided in Figure 6. We also show an example\nof the input images in Figure 9 along with the GUI action grounding queries ut in the correspnding output."}, {"title": "C. Approximating State (St) with Natural Language Descriptions Dt", "content": "To simplify the representation of the state St at the t-th exploration step, we approximate it with a list of natural language\ndescriptions Dt, where each description dk corresponds to an action and its resulting state transition. Figure 7 illustrates\nthe input prompt used to generate one such natural language description. The input consists of the current screen it with\nthe action target at visually marked (box 1), along with the resulting screen it+1. Using this input, the MLLM produces a\ntextual description capturing the key details of the transition, including the action at, the visual changes between it and it+1,\nand any notable observations. These natural language descriptions serve as a compact and interpretable representation of the\nexploration history, enabling efficient input to the MLLM during subsequent steps of the Q-ICRL process."}, {"title": "D. Predicting $Q(a_t^*)$ with MLLM through In-Context Learning", "content": "To predict $Q(a_t^*)$ for candidate actions at, in the text prompt, we include the MLLM with the natural language escriptions\nDt approximating current state St, two example actions (aeg1, aeg2) and their Q-values ($Q(a_{eg1}), Q(a_{eg2})$), along with the\nvisual input using the Set-of-Mark method (Yang et al., 2023). Figure 10 shows an example of the full input and output."}, {"title": "E. Experiment Details for Exploration and Fine-tuning", "content": "Exploration Configurations During exploration, the GUI-Bee agent operates with a maximum of T = 400 exploration\nsteps, and at each step, it samples H = 3 candidate actions. The action type ct is restricted to \"click\" or \"scroll\" transitions,\nas these are the most common actions for GUI navigation. For \"scroll\" actions, the target element et is simplified to\nrepresent the \"full page,\" ensuring consistent representation of scroll transitions. To enhance robustness, each environment\nis explored three times using different screen resolutions. This variation ensures the generated data captures diverse screen\nsetups, improving the generalization ability of the fine-tuned models. Each exploration session lasts between 6 to 18 hours,\ndepending on web loading latency. The long loading time is due to the overhead of using the Playwright tool \u00b9 to acquire the\naccessibility tree for each screen, which can be further optimized with some engineering efforts.\nModel Fine-tuning Configurations We fine-tune three GUI grounding models-SeeClick (Cheng et al., 2024), Qwen-\nGUI (Chen et al., 2024), and UIX-7B (Liu et al., 2024)\u2014using the data generated by the GUI-Bee agent. Fine-tuning is\nperformed in two input configurations: vision-only, where the input consists of GUI screenshots only, and Vision+A1ly,\nwhere the input includes both GUI screenshots and the accessibility tree embedded in the text prompt.\nThe accessibility tree (Ally tree) is a structured representation of the GUI that exposes key information about screen\nelements, such as their type, properties, and hierarchical relationships. Typically used for assistive technologies like screen\nreaders, the Ally tree provides textual descriptions and spatial information of the interface components, complementing\nvisual input for models. Including this information in the input prompt allows models to leverage both visual and structural\ncues, improving their grounding accuracy."}, {"title": "F. Data Annotation Details", "content": "We recruited annotators from within our research team, ensuring familiarity with GUI environments. Annotators were\ncompensated fairly for their work to maintain ethical standards. All queries undergo manual review to eliminate ambiguity,\nduplication, and low quality, ensuring alignment with recorded actions."}]}