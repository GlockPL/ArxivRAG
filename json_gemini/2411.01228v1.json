{"title": "The Interaction Layer: An Exploration for Co-Designing User-LLM Interactions in Parental Wellbeing Support Systems", "authors": ["SRUTHI VISWANATHAN", "SERAY IBRAHIM", "RAVI SHANKAR", "REUBEN BINNS", "MAX VAN KLEEK", "PETR SLOVAK"], "abstract": "Parenting brings emotional and physical challenges, from balancing work, childcare, and finances to coping with exhaustion and limited personal time. Yet, one in three parents never seek support. Al systems potentially offer stigma-free, accessible, and affordable solutions. Yet, user adoption often fails due to issues with explainability and reliability. To see if these issues could be solved using a co-design approach, we developed and tested NurtureBot, a wellbeing support assistant for new parents. 32 parents co-designed the system through Asynchronous Remote Communities method, identifying the key challenge as achieving a \"successful chat.\" As part of co-design, parents role-played as NurturBot, rewriting its dialogues to improve user understanding, control, and outcomes.", "sections": [{"title": "1 Introduction", "content": "Parenting, often hailed as the most important job in our world, exacts a significant toll on both mind and body. The perinatal period represents a critical developmental stage for the child, the parent(s), and the formation of attachment relationships [2, 61]. This period, spanning from the onset of pregnancy to the first year postpartum, is marked by profound physical, emotional, and psychological changes, which significantly impact parental wellbeing. In particular, many expectant and new mothers encounter substantial challenges, in mental health, emotional adjustment, and caregiving [98], in addition to the transformation their body goes through. During this period, parents experience heightened stress, anxiety, and feelings of isolation, increasing the need for support in these areas. Yet, we know that one in three parents do not seek help [91]. The literature on perinatal care identifies several key issues, including inadequate access to mental health resources, availability of healthcare professionals, cost to the government, stigma surrounding parental mental health, and challenges in identifying and supporting at-risk mothers [25, 46, 79]. These challenges have been further aggravated by the COVID-19 pandemic [16, 55]. As a result, we see a growing body of research exploring how digital technologies can enhance perinatal care and provide support during this critical period [39, 93].\nNotable Human-Computer Interaction (HCI) research has laid the groundwork for exploring how digital interventions increase wellbeing across aspects such as physical and mental health of a family [74], a child's growth [54], sleep tracking of parents and children [73], and managing family screen-time behaviours [41]. Today, with the availability of Artificial Intelligence (AI) to everyday users, use of general generative applications such a ChatGPT for parenting is on the rise [78]. With responsible design, Large Language Models (LLMs) have the potential to facilitate a range of services, from offering self-guided mental health support [84], real-time emotional support [51] to providing evidence-based information at scale [57]. The current use of LLMs to support parental and child wellbeing is reviewed with high potential and ethical cautions [6, 86]. Their application within this context is fraught with limitations and uncertainties, especially of accuracy, empathy, and usefulness [50].\nGiven these challenges, our research adopts an exploratory approach, probing into a small subset of these broader issues. We focus on understanding how interactions between LLMs and parents might be designed to be perceived as helpful and engaging. Our goal is to explore what constitutes a usable and useful interaction in these contexts and how LLMs can be integrated into wellbeing support in ways that are practically engaging and thus useful for parents. To this end, we conducted a six-part online study involving 78 parents, who trialled iterative improvements of NurtureBot, a technology probe, designed to support parental wellbeing. This study, while not aimed at delivering a finalised solution, served as an exploratory step toward understanding how parents interact with LLM-based agents and identifying necessary features to make these tools genuinely supportive. The following Research Questions (RQs) guide our investigation into the user interaction aspects of engaging with an LLM-based parental wellbeing assistant:"}, {"title": "2 Related Work", "content": "2.1 Supporting Family Wellbeing with Digital Tools\nWithin the field of HCI, there has been a long-term research agenda focusing on the study and development of impactful digital tools that offer a variety of social, physical, and mental wellbeing support to mothers [17, 29, 89]. The design and evaluation of digital tools aimed at promoting maternal wellbeing during the transition to motherhood [70], bump2bump, emphasise the need for holistic approaches in HCI that consider the challenges faced by new mothers. The role of peer support in digital environments is highlighted by the finding that online networks can provide valuable emotional and practical assistance to parents, particularly in situations where traditional face-to-face support may not be available [102]. The use of the Baby Buddy app among first-time mothers in the UK [26], shows that while the app is generally well-received, its effectiveness is influenced by how well it aligns with the mothers' understanding of their needs and the app's usability. This points to the necessity of involving parents in the design process to ensure that digital tools meet their expectations and provide meaningful support.\nThe integration of AI chatbots into parenting and family well-being has opened new avenues for providing support and guidance to parents [32, 100, 103]. Parenting with conversational agents such as Alexa [12], Siri [82] and other voice-activated agents in the home [88] have been critically studied in the past few years. Following the advent of AI chatbots, the integration of LLMs into parenting practices [78], is ushering in an era of unprecedented possibilities and challenges. LLM-based agents such as ChatGPT are being adopted by parents of young children [50, 78], offering new learning opportunities and personalised support, though their impact on parenting remains largely unexplored, marking this as an era of the unknown. The performance and acceptance of ChatGPT's responses in the childcare field [50], is also being studied with caution. Despite technological advancements, several limitations persists such as misinformation [50], lack of diversity [7], threat of data misuse [69], and lack of user adoption [9]. While gamification can be an effective motivational tool [97], it often does not appeal to all users equally, particularly those unfamiliar with or resistant to digital game mechanics. The need for humane interaction with a good user experience has been"}, {"title": "2.2 Interacting with Al for Everyday Users", "content": "As Al permeates daily life, from recommendation systems to digital assistants, the ability of non-expert users to engage meaningfully with Al systems becomes critical [31]. Research highlights the significance of Explainable AI (XAI) in demystifying the opaque processes of machine learning models, thereby enhancing user understanding, an extensively studied dimension of Human-Centred AI (HCAI)[20, 81]. \u03a7AI methods such as Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) have been pivotal in rendering AI decision-making processes more transparent [1, 5, 63, 80]. While XAI tools enable expert users to grasp the rationale behind AI outputs, [21, 40, 65], most non-experts do not engage with XAI [66], highlighting the need for innovative methods to foster AI understanding [66]. Control over Al systems by everyday users is another essential dimension of HCAI, with research underscoring the importance of user agency in human-AI interactions and challenges with it [56]. Interactive interfaces that allow users to refine AI behaviour, such as setting parameters, have been shown to significantly enhance user satisfaction and the alignment of AI outputs with user expectations [4, 44, 49, 94], maintaining user engagement, and ensuring that Al systems serve the intended purposes [19, 90]. Improvement of AI systems through user feedback represents the third crucial dimension of HCAI. Human-in-the-loop approaches, where users contribute to the iterative development and refinement of AI models, have gained traction as a means of optimising AI [4, 30, 33]. Involving end-users in the process, is not only about improving the AI but also about ensuring that the systems evolve in ways that are aligned with user needs and ethical considerations [62]. In summary, HCAI efforts [20] can be categorised into three fundamental objectives: understanding, controlling, and improving AI systems. Borrowing from the cybernetic loop [72], akin to sensing, processing, and reacting; these categories form the basis for a continuous cycle of interaction between humans and algorithms. We plan to employ these three lenses as cues for our co-design of user-LLM interactions, ensuring that the development of AI systems remains aligned with the needs and expectations of everyday users [8, 99].\nThe design of CUIs [22, 28, 58] requires a deep understanding of HCI principles, ensuring that these systems can handle the nuances of human dialogue, such as turn-taking, repair mechanisms, and emotional engagement [23, 52, 58]. As the field progresses, there is a growing emphasis on addressing these challenges to enhance the user experience and make conversational interactions more natural and effective [48, 60]. The emergence of large language models (LLMs) has revolutionised user interactions across various domains, providing unprecedented capabilities in understanding and generating human language. In healthcare, LLMs have been integrated into administrative and clinical tasks, streamlining operations such as patient record management, appointment scheduling, and even aiding in medical diagnostics, thereby enhancing efficiency and reducing the burden on healthcare professionals [18, 24, 37]. One of the most promising applications of LLMs is in the realm of personal wellbeing, where they are employed in the development of conversational agents designed to support mental health, offering personalised interactions that help users manage stress, anxiety, and other mental health conditions [34, 53]. Despite the promising advancements, issues such as hallucinations [101], task-specific limitations [68], and user experience problems persist [96]. Innovative human-centred solutions, are needed to build reliable, interactive and creative agents tailored to unique needs [83]. In the context of"}, {"title": "3 Part I: The Initial Prototype: NurtureBot v1", "content": "3.1 Origins and motivations for the initial prototype\nOur parental wellbeing support prototype, was based on a community wellbeing project called Nurture, initially run by a nonprofit organisation dedicated to children's wellbeing [Org name removed for review]. The original Nurture project was a peer-to-peer mentoring initiative designed to support new mothers during their baby's first year. In this program, experienced volunteer mothers were trained to mentor new moms, providing guidance and encouragement. A digital platform supporting this program offered personalised, week-by-week content delivered via text messages, acting as a conversation catalyst between mentors and mentees. The prompts were designed to initiate meaningful dialogue and help new moms navigate early motherhood.\nThe Nurture project was conducted over a span of three years, involving approximately 60 mentors and 603 parents who participated in one-on-one texting. While the project was successful in providing personalised support and the fostering relationships through these interactions, this model faced scalability challenges. While effective, Nurture project was resource intensive and difficult to expand to a broader audience without significant investment in human resources.\n3.2 Designing the initial prototype\nRecognising the need for a scalable solution that could retain the core support offered by Nurture, we partnered with the organisation to collate resources and findings from Nurture project for developing a new Al tool. In collaboration with the organisation's project team, we wrote the initial prompt for a basic large language model (LLM)-based chatbot using Open Al's GPT-4 API [71], zero-shot prompting, which we named NurtureBot v1.\nNurtureBot v1 was designed to replicate the supportive environment of the original peer-to-peer texting interactions while introducing the scalability and accessibility of AI. The chatbot was refined to deliver three key features: empathetic chatting, wellbeing exercises, and parenting information, all adapted for AI-driven interactions. Additionally, it incorporated \"conversation starters\" thoughtfully crafted by Nurture mentors. Empathetic chatting was intended to provide an active listening that reflect warmth and understanding. Wellbeing exercises were included to offer practical tools for managing stress and mental wellbeing, similar to those shared by mentors. Lastly, parenting information was integrated to ensure that users could access reliable and evidence-based resources tailored to their needs, much like the information shared in the original peer support sessions.\nLeveraging insights from the Nurture project in Part I, we designed NurtureBot v1 and initiated iterative trials in the following Part II by testing the prototype with everyday users."}, {"title": "4 Part II: NurtureBot v1 Testing and Analysis", "content": "In this phase, we address RQ1: \"What are the main problems parents face when interacting with an LLM-based parental wellbeing assistant in support-seeking scenarios?\".\n4.1 Participants and Method\nOnce we had secured ethical approval via our institutional ethics board [removed for review], we recruited 32 parents of children aged 5 or 6 through the Prolific platform [76]. The average age of participants was 41.17 years, with 20 females and 12 males. Five participants (15%) were in the habit of using ChatGPT everyday. Participants self-reported no acute stress or mental or physical health issues during the study period, nor to have they received therapy in the 12 months prior. The study's purpose and methodology were explained to parents through an information sheet, after which informed consent was obtained. They were paid $3 for each day of participation.\nOver the five-day period, participants engaged with NurtureBot v1 for at least five minutes each day, with each day dedicated to simulating a specific stage of early parenthood: pregnancy, the week of childbirth, few weeks postpartum, one month, and six months after birth. The objective was to assess how well the NurtureBot v1 could support them in these hypothetical scenarios. Each day, participants interacted with the chatbot, receiving responses that included links to resources, empathetic messages, and suggestions for wellbeing exercises. At the end of each day, after interacting with NurtureBot hosted on a website via Streamlit [87], participants were asked to complete a survey via Qualtrics [77], providing feedback on their experience with the chatbot by answering the questions listed in Figure 2. On the final day, they also completed the Chatbot Usability Questionnaire (CUQ), a specialised questionnaire designed to access the usability of healthcare chatbots [43].\n4.2 Data Collection and Analysis\nOver the course of the five-day study, 144 conversations and associated feedback were generated, denoting a daily-average of 90% (28.8 out of 32 participants). All data was securely stored on departmental servers, compliant with GDPR [92], and no personal data was collected, consistent with Prolific platform's design.\nThe qualitative analysis of the feedback from the survey (See questions 1 and 2 in Figure 2) was conducted using a thematic analysis approach [15]. Our analysis focused on the instances that captured parent reactions to interacting with NurtureBot. The first author conducted the initial analysis using inductive, open coding, and regularly met with the second author to discuss emerging themes and explore alternative interpretations of participant responses. While participants did share positive feedback on several aspects of the chatbot, highlighting its usefulness and relevance, especially during early stages of parenthood, we do not explore the parents perspectives on the potential of LLM-based parental wellbeing support as this is reported in the literature [6], and to keep our focus on answering RQ1. Hence, we focused on analysing 131 of the 288 (45.48%) pieces of feedback that highlighted negative interactions and suggestions for improvement. This led us to the identification of 28 unique pain points, which were further grouped into seven overarching themes, converged in three subsequent meetings. These themes highlighted the key problems participants faced while interacting with NurtureBot v1 as exemplified below. We provide only a high-level summary here to conserve the reader's focus for the ARC section, which forms the core of the HCI discussion."}, {"title": "4.3 Findings of initial trial: Parent reactions to Nurture Bot v1", "content": "Problem 1: Unable to Chat: Participants expressed frustration with NurtureBot's tendency to cut off their conversation. They were disappointed when the chatbot redirected them to external URLs rather than providing concise information within the chat itself. Even when information or exercises were provided within chat, NurtureBot did not engage further, making it difficult to continue the conversation, thereby rendering the participants unable to experience empathetic chatting, which was a key feature. They found this approach disruptive and felt it hindered the conversational flow. Participants noted:\n\"The only thing I would maybe find better is instead of providing links to articles, maybe include the information from the articles into the messages from NurtureBot. That way you haven't got to click off from NurtureBot and could spend longer speaking.\"\n\"Today I was trying to see if NurtureBot would focus more on chatting and less on giving infomration.\"\nProblem 2: Patronising and Robotic Response Tone: Participants reported that the NurtureBot's responses felt patronising and robotic, often pretending to understand human emotions or pain. For example, a participant stated:\n\"What's the point of platitudes like 'recovering from a c section can be painful' - what does an Al know?? Why would it be helpful to hear that expressed?\"\nAnother participant commented:\n\"I think maybe make it slightly more warm and more human-like rather than 'here's your information.\"\nProblem 3: Lack of Localised Resources: The resources suggested by the chatbot were not tailored to participants' specific country or area, making the information less relevant and useful. One participant highlighted:\n\"When asking for groups in my area, it needs to be more specific, especially for the area I live in as it came up with places that were a bit irrelevant.\"\nProblem 4: Memory and Continuity in Conversations: Participants wanted NurtureBot to remember details about them, such as their name, their baby's name, and their troubles. The lack of continuity made the interactions feel impersonal and disconnected. As one participant mentioned:\n\"I think it works very well the way it is. Obviously I don't know fully how it works but it would be very useful if it remembers everything that you ever tell it so it builds up a profile of the child and parents so you don't have to keep repeating information.\"\nProblem 5: Generic responses: Participants felt that NurtureBot's responses lacked specificity and depth, often resorting to external links without giving user a chance to discuss their situational need and providing direct, tailored answers based on reliable sources. This approach was seen as redundant, as users felt they could achieve similar results using standard search engines. One participant expressed:\n\"Pointless. I wanted something specific, not links. Don't understand why I wouldn't just use Google.\"\nProblem 6: Transactional Nature of Conversations: The chatbot was perceived as overly transactional, asking questions in rapid succession without allowing participants enough time to process the information or engage in a more natural, flowing conversation. One participant described this experience:\n\"It was OK. It still feels quite transactional - NurtureBot wants me to ask questions then I get the answer, and then it asks me the next question. There's no sense of pausing to process what's been said, and asking"}, {"title": "4.3.1 Quantitative results", "content": "Our subjective questions regarding NurtureBot v1's task satisfaction received an average score of 4.29 out of 5, while interactive questions averaged 4.36 for understandability, 3.97 for controllability, and 3.88 for improvability. The standard questionnaire analysis resulted in a mean CUQ score of 85.4 out of 100, which, when compared to the CUQ benchmark of 68 [43] in a one-sample t-test, was extremely statistically significant (t(29) = 7.11, p < 0.0001, mean difference = 17.40, 95% CI [12.40, 22.40], SE = 2.45). These scores and their implications are discussed further in Section 8.3, alongside comparisons with NurtureBot v2 and v3.\nAfter identifying seven key problems with parent-NurtureBot v1 interactions in Part II, our goal in Part III was to co-design by tasking users to prioritise problems and imagine ideal solutions."}, {"title": "5 Part III: ARC Co-Design Study", "content": "To begin addressing RQ2 - \"What key user needs must a parental wellbeing assistant address? How do parents describe their ideal vision of how the assistant should function, particularly regarding their understanding of its workings, control over it, and opportunities to improve the outcomes?\"- the third phase of the study involved Asynchronous Remote Communities (ARC) co-design sessions [64], designed to directly engage participants in the iterative design process. These sessions were facilitated using the Miro platform [67], allowing the same 32 participants who tested NurtureBot v1 (see Subsection 4.1) to return to collaboratively co-design solutions asynchronously, thus accommodating different schedules, total anonymity, and fostering broad participation.\nThe ARC sessions involve two core activities over three sessions (1 week), prioritising, and addressing key issue(s) with NurtureBot v1. The average participation was 29.33 out of 32 (91.63%) participants per day. To help familiarise participants with using Miro, we designed a training activity and instructional tips for using Miro on mobile devices, as well as providing direct messaging support via Prolific."}, {"title": "5.1 ARC Activity One: Diagnosing and Prioritising Pain Points", "content": "The goal of the first activity was to establish which of the expressed difficulties were the biggest priority to address. Using the seven key problems from NurtureBot v1's technology trial (see Section 4.3), we asked participants to prioritise these issues. By disclosing these key issues, the goal was to ensure that all participants could see the broader scope of the negative feedback gathered from others within their group.\n5.1.1 ARC Activity One: Method. We crafted a \"problem statement\" for each issue, synthesising the findings without directly replicating any participant's verbatim feedback (listed as problem statements in Figure 4). This method not only helped verify our interpretation and synthesis of the feedback but also prevented potential bias, ensuring participants did not favour their own comments when voting. To provide sufficient scaffolding for reflection, we organised the activity as follows:"}, {"title": "5.1.2 ARC Activity One: Results", "content": "Based on participants responses, the following options were voted as the top three problems by participants:\n\u2022 Problem 1 - I wanted to have a chat but NurtureBot kept cutting me off by redirecting me or giving me paragraphs of information. emerged as the top challenge with NurtureBot.\n\u2022 Problem 3 - The suggested resources were not localised to my country or area was the second priority.\n\u2022 Problem 4 - I want NurtureBot to remember things! E.g., who I am, my baby's name and our previous conversation and context was ranked third.\nWe next took the top-ranked problem (Problem 1) to begin considering how we might enhance \"successful chat problem\" in Activity 2."}, {"title": "5.2 ARC Activity Two: Co-Designing Solutions", "content": "The second activity focused on solution generation for problem 1. Participants were asked to step into the role of NurtureBot: \"Imagine You Are NurtureBot and Fill in the Blanks.\"\n5.2.1 ARC Activity Two: Method. The activity was structured as follows:\nScenario-Based Problem Solving: As the first step, participants were presented with three specific scenarios corresponding to the three main features of NurtureBot (i) interacting with NurtureBot for the first time and wanting to have an empathetic chat, (ii) seeking wellbeing exercises to manage work stress, and (iii) seeking information to soothe their baby; that exemplified trying to have a successful chat aiming to use all three features of NurtureBot. For each scenario, participants reviewed an example response and were asked to consider how NurtureBot could help users to better understand its offerings, control the conversation, and improve its outcomes.\nCollaborative Dialogue Writing: Next, participants were invited to take on the role of NurtureBot and rewrite it's dialogue for each scenario. We offered cues that encouraged participants to work on \"Understanding, Controlling, and Improving\" NurtureBot, similar to the idea of a cybernetic loop [72] (see Section 2.2). This part of the activity encouraged participants to think critically about how the chatbot could better meet their needs in specific contexts, and which would inform NurtureBot v2\nIterative Feedback and Voting: As the last step, after proposing solutions, participants reviewed and built upon each other's suggestions. We invited participants to vote on the best ideas , ensuring that the most popular and potentially effective solutions were prioritised for implementation.\n5.2.2 ARC Activity Two: Results. ARC Activity Two generated a rich dataset of 189 participant-generated dialogues. Our thematic analysis of these dialogues (as in Section 4.2) generated seven potential features to address user needs. We summarise the seven features below and in Table 1, map examples from participant-generated dialogues onto each feature. These features were distilled into design specifications that directly informing the next iteration.\nFeature 1. Use of Metaphors for Understanding: Parents suggested a range of metaphors to help them better understand and form a meaningful connection with NurtureBot. Rather than perceiving the chatbot as a purely functional AI assistant, many participants preferred to imagine NurtureBot as a trusted companion, often referring to it as a \"friendly advice guru.\" This metaphor conveyed the idea of an accessible, approachable figure offering guidance and support in a non-judgemental manner. Others went further, likening NurtureBot to non-interactive entities such as"}, {"title": "Feature 2. Beginning with Empathy:", "content": "Parents highlighted the importance of empathetic conversation starters, expressing that this initial approach sets the tone for the entire interaction. They wanted NurtureBot to start by first asking users how they were feeling or inquiring about their day, creating a welcoming and supportive tone. Parents expressed that this approach was particularly useful in emotional situations, where a simple, \"How are you today?\" can help, offering comfort and reassurance before moving on to problem-solving.\nMoreover, empathy needed to extend beyond the initial greeting, especially after they disclosed a personal problem or negative feeling. Rather than immediately diving into solutions or resources, NurtureBot's ability to show understanding and validation before proposing options was seen as key to making parents feel heard and supported. Whether engaging in mindful exercises or offering parenting information, the empathetic tone had to persist throughout, without pretending to be human or to understand human pain, thereby reinforcing NurtureBot's supportive role."}, {"title": "Feature 3. Introducing Features Clearly:", "content": "A recurrent goal of parents was to highlight the importance of clearly introducing NurtureBot's core features early in the interaction. While participants recognised that a comprehensive list of features was appropriate for the first onboarding session, they also left clear reminders of NurtureBot's capabilities that should be available throughout subsequent interactions.\nIn practice, parents wanted NurtureBot to weave in these features naturally during conversations, reminding them about available options and how to best utilise them in context. This kept the interaction fluid and allowed parents to feel that they could rely on the chatbot's range of support without needing to recall its functionalities from memory. By continuously reinforcing NurtureBot's capabilities, within the context of the conversation, it became easier for parents to make informed choices during the interaction, thereby maximising the effectiveness of each conversation."}, {"title": "Feature 4. Providing Step-by-Step Guidance:", "content": "Parents wanted NurtureBot to go beyond simply providing information. When guiding them through exercises-whether mindfulness activities, meditations, or stretches-they desired a step-by-step approach that offered a clear, manageable flow of actions. They wanted Nurture to ask when they were \"ready,\" to \"wait for them\" to finish a step before moving on to the next, or even to \"pause\" and \"continue later.\" This guidance needed to be personalised to their situation, transforming NurtureBot into a collaborative tool rather than just a passive source of information.\nBeyond wellbeing exercises, parents also wanted this step-by-step guidance for other interactions, such as when presenting parenting information or helping them manage their schedules. Breaking down long paragraphs of informa-"}, {"title": "Feature 5. Personalising and Deepening Sessions:", "content": "Personalisation was crucial to the parents' experience with NurtureBot. Parents did not just want generic advice pulled from trusted resources. They wanted the chatbot to understand their specific situation before suggesting exercises or offering advice. Parents had rewritten NurtureBot's dialogues to either \"browse a list of topics\u201d or \u201cdive deeper\" into one specific area depending on their immediate needs. They also wanted NurtureBot to \"talk about their situation,\" for some exchanges before going about finding useful information. This deep personalisation allowed for a more meaningful and tailored exchange, where NurtureBot could address the unique concerns of each parent based on the context of their lives."}, {"title": "Feature 6. Offering Clarification and Reassurance:", "content": "At the end of each interaction, parents wanted NurtureBot to provide them with an opportunity to \"clarify\" or \"revisit\" the topic at hand. Whether they had completed an exercise or received parenting advice, participants wanted NurtureBot to ask if they needed further clarifications or wanted to explore additional options. This ongoing dialogue reassured parents that they were in control of the conversation, allowing them to slow down and fully understand the information presented and improve outcomes.\nDuring the course of the conversation, when parents were unsure of what they wanted, participants wanted NurtureBot to \"take the conversation slow,\" offering clarification or suggesting alternative approaches, would ensure that they did not feel rushed or overwhelmed. This reassurance, especially when discussing difficult topics, coupled with the option to fine-tune their experience, would take NurtureBot beyond offering static advice."}, {"title": "Feature 7. Summarising and Offering Follow-Up Options:", "content": "Summarisation played a key role in allowing parents to reflect on their interactions and \"takeaways\" with NurtureBot. At the end of each session, parents desired a brief summary of what had been discussed, along with suggestions for follow-up actions. This summary could be \"saved for future reference,\" allowing parents to revisit important information without needing to restart the conversation. Participants also wanted Nurture to offer to \"bookmark, save, or email\" summaries. Additionally, they wanted NurtureBot to offer suggestions for similar exercises, revisit the session, or offer further support if the previous interaction was not sufficient.\nFinally, participants wanted NurtureBot to link to buttons or hints to click through various follow-up options, helping parents with choices to guide the conversation forward without needing to articulate their next steps. Imagination of this interactive flow of suggested participants wanted to prevent decision fatigue with more seamless and intuitive interactions, reinforcing the effectiveness of NurtureBot as a comprehensive support tool even for new and non-technical users."}, {"title": "6 Part IV: The Interaction Layer: NurtureBot v2", "content": "To continue addressing RQ3 - \"How can we employ co-design methods to craft interactions around Al, enhancing usability and the overall user experience?.\" The development of NurtureBot v2 focused on refining interactions to enhance its user experience. We created an interaction layer central to how the LLM-based chatbot engages with users (see prompt illustrated in Figure 6). The three aspects of NurtureBot v2's interaction layer, namely the core interaction principle of NurtureBot, interaction levels within each feature , and interaction elements"}, {"title": "6.1 Core Interaction Principle", "content": "NurtureBot v2's development was guided by three foundational capabilities we wanted users to have in relation to the chabtot, and vice-versa: understanding, control, and improvement. These core interaction principles  guided all interactions with users. This model, inspired by the concept of the 'cybernetic loop' [72], and supported by the human-centred AI research endeavours discussed in Section 2.2, was designed to ensure that every exchange was meaningful, user-centred, and adaptable to the individual needs of each parent. This principle provided a template for each type of exchange, guiding the chatbot on how to initiate conversations, present options, and offer follow-up support."}, {"title": "6.2 Three Interaction Levels", "content": "1. Understand Level: The chatbot's first task was to understand and acknowledge the user's situation while helping the user grasp how the chatbot could assist them. For instance, it would ask, \"How are you feeling today?\" or \"I can help you with a list of options?\" We employed the metaphors of \"friendly AI support\" and \"AI assistant\" to prevent users from anthropomorphising the chatbot, ensuring they maintained a clear understanding of its role as a helpful and empathetic tool rather than attributing human relationships to it. This phase was essential for setting a supportive tone and ensuring the chatbot could convey its capabilities and tailor its responses to meet the user's needs effectively.\n2. Control Level: Once the user's needs were understood, the chatbot entered the control level, where it provided options to guide the interaction along with its content. This level was designed to give users a sense of agency and control over the conversation. Users could choose to elaborate on their concerns, request a summary, or switch topics. Users would be often provided with \"hints\" such as revise, restart, and elaborate; designed in ARC sessions, in accordance with the context of the chat to enable them to effectively lead NurtureBot towards desired outcome.\n3. Improve Level: After addressing the user's needs, NurtureBot gave a summary and sought feedback to access user satisfactions and improve the interaction by offering additional resources or to revise the conversation (as parents preferred). This phase ensured that the conversation concluded on a constructive note, always asking for user feedback, with the user feeling supported and informed."}, {"title": "6.3 Interaction Elements", "content": "1. Undecided Exchanges When users were undecided, NurtureBot aimed to first understand their hesitation or indecision. With started elements such as, \"What's on your mind today?\" or \"How can I assist you?\" to encourage the user to reflect on their needs. Metaphorical elements like \"assistant\" were also embedded in the chatbot's dialogue to offer a supportive role, and reassurance elements were designed to not overwhelm user with options, reflecting a tone of gentle guidance. For instance, \"I'm here to support you in whatever way works best for you\" to help users feel comfortable in sharing their thoughts or challenges.\n2. Empathetic Chatting: The chatbot was programmed with elements to focus on active listening and providing emotional support. We used dialogue examples to create non-judgemental elements and avoiding any advice. The chatbot's responses were crafted to mirror those of a trusted confidant, providing reassurance and"}, {"title": "2. Empathetic Chatting:", "content": "The chatbot was programmed with elements to focus on active listening and providing emotional support. We used dialogue examples to create non-judgemental elements and avoiding any advice. The chatbot's responses were crafted to mirror those of a trusted confidant, providing reassurance and"}, {"title": "3. Wellbeing Exercises:", "content": "Guiding elements for wellbeing exercises included mindfulness practices and relaxation techniques. The interaction was structured with personalised elements, offering tailored exercises based on user concerns and alternative options. The chatbot also ensured that exercises were broken down into manageable steps, with the user controlling the pace of the session. To embed collaborative elements, the prompts also ensured that NurtureBot could guide users through activities step by step, e.g., \"Would you like to try a simple breathing exercise together? Let me know when you're ready to begin.\" As such, the user could control and personalise the pace and depth of the exercise."}, {"title": "4"}]}