{"title": "Entailment-Driven Privacy Policy Classification with LLMs", "authors": ["Bhanuka Silva", "Dishanika Denipitiyage", "Suranga Seneviratne", "Anirban Mahanti", "Aruna Seneviratne"], "abstract": "While many online services provide privacy policies for end users to read and understand what personal data are being collected, these documents are often lengthy and complicated. As a result, the vast majority of users do not read them at all, leading to data collection under uninformed consent. Several attempts have been made to make privacy policies more user-friendly by summarising them, providing automatic annotations or labels for key sections, or by offering chat interfaces to ask specific questions. With recent advances in Large Language Models (LLMs), there is an opportunity to develop more effective tools to parse privacy policies and help users make informed decisions. In this paper, we propose an entailment-driven LLM-based framework to classify paragraphs of privacy policies into meaningful labels that are easily understood by users. The results demonstrate that our framework outperforms traditional LLM methods, improving the F1 score in average by 11.2%. Additionally, our framework provides inherently explainable and meaningful predictions.", "sections": [{"title": "I. INTRODUCTION", "content": "Many online services and apps we use today collect vast volumes of personal data [1]. Beyond the first-party use of this data for purposes such as personalisation, it is often used for advertising, analytics, and user profiling. Additionally, this data can be shared with, or even sold to, third parties without the direct knowledge of users, posing serious privacy risks [1], [2]. Typically, information regarding such data collection and sharing practices is outlined in the service's privacy policies and providing the users with privacy policies is mandatory in many jurisdictions [3]. However, these policies are usually lengthy, complicated, and written in complex legal jargon. As a result, users frequently agree to data collection practices without thoroughly reading the privacy policies or comprehending the potential risks involved.\nWhile some service providers actively attempt to enhance the readability of privacy policies [4], [5], the vast majority of these documents remain incomprehensible to end-users. Consequently, multiple research efforts have explored the possibility of providing users with more user-friendly representations of complex privacy policies. These approaches range from presenting user-friendly labels [6], to designing chatbots to answer privacy-related questions [7]. However, most of the existing work has leveraged classical Natural Language Processing (NLP) techniques and encoder-only language models, such as variants of the BERT model.\nRecent advancements in Large Language Models (LLMs), such as GPT [8], [9] and LLaMA [10], have established them as the state-of-the-art for a majority of NLP tasks. These models have demonstrated excellent capabilities in areas like text summarisation and understanding. Moreover, the versatility of LLMs has been showcased in various application domains, including medicine [11], finance [12], and others. These developments in LLMs provide a foundation for building novel solutions that can extract useful information from otherwise complex and nearly unreadable privacy policies, and present it to users in a more user-friendly manner.\nIn this paper, we propose a novel entailment-driven LLM-based framework for privacy policy paragraph classification. Traditional LLM approaches are known to suffer from well-known hallucination problems and thus may not always generate the expected result directly (e.g., summarised text, classification label of text). As a result, the onus of determining whether or not the LLM has made up or dropped facts will be on the users. One key idea behind our approach is to bolster LLM-based classification frameworks with an additional \"entailment\" phase to filter out the initial classifications by LLMs in an analogous way to how we would select or drop a particular LLM-generated output. Fig. 1 demonstrates this phenomenon with an example. At stage 1, an explained classifier predicts a class output and a corresponding reason for a given privacy text. Then, we mask the reason from the original text and use an intermediate stage 2 with a blank filler in an attempt to predict the reasoning text again. An entailment verifier receives information from stages 1 and 2 both and decides \u2018entailment', i.e., the class prediction and the original reasoning are acceptable or vice versa. More specifically,\n\u2022\n\u2022\nWe propose an entailment-driven LLM-based framework to classify paragraphs in privacy policies into 12 categories, such as first party collection/use, third party sharing/collection, and user choice/control, that are easier to understand by the users. The key components of our framework include the explained classifier that generates classification thoughts, the blank filler that re-thinks about these original thoughts and the entailment verifier that makes the final decision (analogous to how a human would reason).\nWe evaluate the performance of our proposed method using the OPP-115 dataset and compare our results with existing baselines and zero-shot LLM settings. We find that our method performs better than vanilla LLM-methods; 8.6%, 14.5%, and 10.5% higher than the results of T5, GPT4, and LLaMA2, respectively in terms of macro-average F1 score."}, {"title": "II. RELATED WORK", "content": "A. Empirical Studies on Privacy Policies\nUsers are increasingly concerned about online privacy [13], yet empirical studies consistently show that privacy policy documents have become substantially longer over the past two decades. With median word counts ranging from 1,500 [14] to 2,500 [15], these documents take too long to read [14], [16], resulting in users making little effort to read and understand them [17]. Further, the writing and presentation of these documents often make them inaccessible [7], with the end result often being uninformed consent [18].\nRecent regulatory and compliance attempts, such as the EU GDPR, have aimed to make privacy policies mandatory and more user-friendly. While these efforts have led to positive outcomes like a 4.9% increase in the availability of privacy policies [19], a longitudinal analysis by [20] finds that this has caused privacy policies to become even longer, increasing by around 25% globally. This makes the policies even more challenging to read and therefore, to understand.\nB. Analysing Privacy Policies Using NLP Techniques\nRelying solely on manual analysis of privacy policies or manually crafted rules, as in [21], [22], does not scale. As a result, many researchers have proposed automated methods to provide end-users with meaningful interpretations of privacy documents. Some examples include identifying relevant privacy topics for policy sections [23], [24], summarising sections [25], highlighting subsections where users can make informed choices regarding their personal information (e.g., opt-out choices [26], [27]), developing question-answering systems for policy documents [7], building user-interface tools to identify common data practices (e.g., browser extensions [28]), or detecting privacy policy inconsistencies and non-compliances [20], [29]. The majority of these works have leveraged traditional Natural Language Processing (NLP) techniques together with machine learning methods such as support vector classifiers, logistic regression models, and neural networks, with limited adoption of recent transformer-based language encoder models like BERT [30]. PrivBERT [24] is a domain adapted version of a popular encoder model ROBERTa [31] that performs better than vanilla-encoder models in classification tasks.\nC. Large Language Models (LLMs)\nThe recent surge of Large Language models (LLMs) such as GPT4 [9], LLaMA2 [10] has resulted in generative AI models being adapted with a wide range of tasks, including world knowledge, commonsense, and summarisation. The performance of LLMs could be further enhanced through domain adaptation, as demonstrated by initiatives like BloombergGPT [12] for the financial sector and Code LLaMA [32] for software development. However, such endeavors are resource-intensive, requiring substantial text data and computational power.\nPolicyGPT [33] is a recent attempt to explore zero-shot prompting for privacy policy paragraph classification with LLMs. However, our experiments show that PolicyGPT's zero-shot performance falls short when confronted with multi-class-multi-label classification (refer Sec. IV-E).\nIn this work, leveraging open-source LLMs, we explore how LLMs' unique explainable capabilities can aid in better interpretations of privacy policies. State-of-the-art chain-of-thought (CoT) prompting [34], which maps non-trivial inputs and outputs via intermediate steps, mimics the human thought process by breaking down a complex task into smaller, more interpretable steps. Drawing inspiration from CoT prompting, we investigate how the inclusion of one-step reasoning; i.e. a 'reason' shown in stage 1 of Fig. 1, can improve our classification accuracy and lead to more reliable and explainable outputs."}, {"title": "III. OUR FRAMEWORK", "content": "Our framework starts with explainable privacy policy paragraph classification. This is depicted in the first block of Fig. 2 named as explained classifier. Next, an explanation-masked-out version of the privacy paragraph and previous classification output are given as inputs to the blank filler. It will then generate the most likely token sequence for that 'masked portion' by looking at the broader context of the paragraph. Finally, we check the entailment among these via the last block of our pipeline, entailment verifier. How each module works is explained below in detail.\nA. Explained Classifier\nThe explained classifier's intuition is to generate a sufficient number of class identifications with reasons that are extracted from a privacy policy paragraph. Therefore, this module requires an autoregressive LLM. Formally; an output class label $y_i \\in [C_0, C_1,..C_n]$ will be predicted alongside a subset of original tokenised text, i.e., a reason $t_i \\in T$ during its training and inference stages. Here, $C_0...C_n$ are all the class labels defined in the annotated dataset. T is the tokenised privacy policy paragraph text chunk that the model is fed with. $t_i \\in T$ condition is evaluated using Python Regex based hallucination detector and such filtered $y_i, t_i$ pairs will be forwarded to the next stage of the model. For each paragraph, there could be any number of such pairs establishing a multi-label setting.\nB. Blank Filler\nBlank filler is also an autoregressive LLM that takes paragraph T as input where a reason $t_i$ from the explained classifier is masked out and would try to predict the best text chunk $t_e$ to fill in that masked part of the paragraph. We further provide the explained classifier's output class label $y_i$ as an input indicating the model which kind of text it should try to regenerate. We explore the model's understanding of the paragraph's general landscape here; \"if we mask out the main reason for a particular classification output, can the model look at the rest of the paragraph and then predict what kind of text should be there?\". It is worthwhile to emphasise that we do not expect the outputs $t'_i$ to be word-for-word identical to $t_i$. Instead, we expect $t'_i$ to be similar in meaning to $t_i$.\nC. Entailment Verifier\nThis is an encoder-based language model attached to a neural network classifier head where we feed previously generated outputs $Y_i, t_i,$ and $t'_i$ separated by [SEP] tokens. The output of this module is binary, indicating entailment (output:1) or contradiction (output:0). This module acts as the final filter where we can remove contradictory classifications and reasons from prediction outputs for a given policy paragraph T."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "This section provides an overview of the experimental setup, including a description of the annotated dataset we use and the train-test split we selected. It then introduces the seven baseline models against which we compare our proposed framework, followed by an outline of the evaluation criteria.\nA. Modules of the Framework\nWe used two 8-bit quantised LLaMA2 models [10] with low-rank adaptation (LoRA) [35] for our explained classifier and blank filler modules. Although our framework can accommodate any large language model, we chose LLaMA2 due to its open-source availability and its superior performance in tasks such as commonsense reasoning, world knowledge, and reading comprehension compared to the state-of-the-art at the time of its release.\nFor the entailment verifier, we selected a BERT encoder module with 110 million parameters that demonstrated good results for the Multi-Genre Natural Language Inference (MNLI), which is also an entailment classification task [30]. Again, we emphasise that our framework is flexible enough to incorporate any other encoder model as the entailment verifier. We fine-tune the BERT model based on the inference results from the explained classifier and blank filler (cf. Sec. IV-C).\nB. OPP-115 Dataset\nTo evaluate the performance of our framework, we used the OPP-115 dataset [36], which contains paragraph excerpts of online privacy policies annotated and labelled by legal experts. It is commonly used in comparable work [7], [24], [26], [29]. Each paragraph excerpt in the dataset (extracted from 115 web privacy policies) has a 3-tiered label. The highest level of the labelling tier is called \u201cdata practice\" (e.g., first party collection/use), and there are ten such labels. Each high-level tier subsequently has fine-granular labels according to \"data-attribute\" (e.g., personal information type) and \u201cdata-value\u201d (e.g., contact). Additionally, the paragraph excerpt also has the corresponding parts that led to specific labelling annotated. It is important to note here that one paragraph can have multiple places annotated with different labels.\nC. Training/Testing Pipeline\nInitially, we train the explained classifier and blank filler separately with the training data in supervised fine-tuning setting over five epochs (Phase1 in Fig. 2). Next, we create a labelled dataset for training the entailment verifier, according to the inputs outlined in Sec. III-C and illustrated in Phase2 of Fig. 2. This dataset is created by running the previously fine-tuned explained classifier and blank-filler in inference mode over the training set. During this inference phase, the explained classifier generates incorrect class predictions that did not exist in the training-set. We denote these made up classes as 'contradictions'. Conversely, any accurate class prediction the explained classifier makes is marked as an 'entailment'. This approach removes the necessity of manually augmenting and curating another dataset, just for training the entailment verifier. Further, it enables us to expose the entailment verifier to realistic errors made by the previous modules, thereby facilitating its training to recognise such mistakes. Once all three modules are trained, we deploy them in inference mode, as shown in Phase3 of Fig. 2, to evaluate performance on the held-out test set.\nD. Baselines\nWe compare the performance of our method against seven baselines falling under two broad categories.\n1) Embedding-Based Classification Models: These models have demonstrated effectiveness in text-classification tasks [30], [31]. Typically, embeddings extracted from a language model are processed by a linear classification head to generate the final predictions. For baselines in this category, we employ two generic encoder models, BERT and ROBERTa, then GPT2, which is adapted to the classification task by using the mean embedding representation of the token sequence of a given text input and finally, PrivBERT, which was further pre-trained on privacy policies for domain adaptation.\n2) Language Generation Based Classification Models: The next set of baselines are auto-regressive language generation-based classifiers. In other words, these models generate the most likely classification outputs in the form of natural text. These baselines closely resemble our method as our explained classifier operates in this setting. We evaluate the performance of LLaMA2 in a supervised fine-tuning setting as the vanilla LLM, where the model outputs the class labels in natural text. Next, we adapt the encoder-decoder T5 [37] model by feeding policy text in the training set as inputs and fine-tuning it to generate target text that represents the class labels. Finally, we adopt GPT4 as a baseline model using a prompting structure introduced by [33] to suit the twelve-class, multi-label setting and perform a zero-shot evaluation for our test dataset.\nE. Evaluation Metrics\nAs previously discussed, the challenge addressed by our framework and the relevant baselines involves the assignment of appropriate data practice labels to a given paragraph excerpt from a privacy policy, with a selection available from twelve possible categories (classes). Given that a paragraph may contain multiple categories simultaneously, our method is characterised as a multi-class-multi-label classification problem. We measure the performance of our framework and others using two types of metrics. To measure classification performance, we use the metrics of precision, recall, and F1 score. To measure the quality of explanations of our method, we use two custom metrics; normalised Levenshtein distance and overlap percentage.\n1) Precision, Recall and F1 Score: In multi-class settings, precision (P), recall (R), and F1 Scores are usually reported as micro, macro, and weighted averages. In micro-averaging, the average is calculated globally by counting the total true positives and false positives across all classes, whereas in macro-averaging, the average of class-wise performance is calculated. In other words, in macro-averaging, each class, including the minority classes, contributes equally to the final number. The weighted average is calculated by taking the performance metric of each class, multiplying it by the number of true instances of that class (i.e., the support), and then dividing it by the total number of instances across all classes. This method provides a way to account for the frequency of each class in the dataset when calculating the overall precision."}, {"title": "V. RESULTS", "content": "Next, we present our results, followed by an ablation study to highlight how different components of our overall framework contribute to the final performance. Later, we present the findings on the explainability of predictions.\nA. Performance Comparison\nIn Tab. I, we present class-wise Precision (P), Recall (R), and F1 Scores and their micro, macro, and weighted averages. Although the Table presents results for all metrics, our discussions mainly focus on macro averages because they serve as a representative measure of the overall performance of the models, as discussed in Section IV-E.\nEmbedding Based Classification: As can be seen from Tab. I (a), among the embedding-based classification methods which are not domain fine-tuned (i.e., GPT2 Embeddings, BERT, and ROBERTa), the macro F1 scores are similar with BERT having the lowest score. This is because BERT performs relatively worse in the absolute minority class \u201cDo Not Track\" with only an F1 score of 0.33. The slightly higher performance of ROBERTa over BERT can be attributed to it being pre-trained on a much larger dataset. Also RoBERTa's slightly higher performance over GPT2 embeddings can be attributed to it being an encoder model rather than a decoder model. It is known in the literature that encoder models perform better than decoder models in text classification tasks [39].\nPrivBERT, the privacy policy domain adapted model of RoBERTa, performs well with the highest R value of 0.71 while retaining a high P value of 0.83. Therefore, the resulting F1 score of 0.76 indicates that further pre-training has indeed helped for better privacy policy paragraph classification. However, we should also note that it is only a 5.5% improvement of F1 score compared to RoBERTa. We also observe that it is struggling to recall the class \"practice not covered\". We also note that our PrivBERT results are lower than those reported by the authors [24]; in this paper, we record the best results we could reproduce with our train-test split.\nLanguage Generation Based Classification: As can be seen from Tab. I (b), the macro averaged F1 scores for T5, GPT4, and LlaMA2 are 0.58, 0.55, and 0.57, respectively. Compared to those, our framework has a significantly better performance with a macro-averaged F1 score of 0.63 (i.e., 8.6%, 14.5%, and 10.5% higher than the original results of T5, GPT4, and LLaMA2), indicating the effectiveness of our proposed framework. While our method does not reach the performance levels of embedding-based methods, our method has the advantage of explaining the classification results (cf. Sec. V-C).\nFinally, we highlight that we tried to replicate GPT4 zero-shot results outlined in the recent preprint PolicyGPT [33] using the same dataset. Despite multiple attempts and communications with the authors, we could not reproduce the results presented in that paper. We believe that authors may have had some pre-filtering of data and different evaluation metrics (e.g., considering a classification as successful even if one predicted label is true among multiple annotation labels per paragraph in contrast, our evaluation is more rigorous as explained in Sec. IV-E1), making the exact problem they address different from ours and other comparable baselines, such as PrivBERT.\nB. Ablation Study\nTo provide further evidence on the overall effectiveness of our framework and how contributions from individual components of our framework work together, we conducted an ablation study. That is, we evaluate the performance of our framework by progressively adding modules starting from the explained classifier. \nThe explained classifier could be considered a 'thought generator' where, for each paragraph, it tries to generate multiple 'class output and reason' pairs until the token generation limit is exhausted. In that process, it recalls many of the correct pairs (0.85); however, some of them are not accurate, as indicated by the low precision of 0.38. As soon as we train an entailment verifier to filter out incorrect outputs, our precision improves to 0.61. Nonetheless, it decreases the recall because some correct classifications are filtered, specifically those about which the entailment verifier is not confident. As we employ the full pipeline, including blank filler, we obtain the highest macro-averaged precision of 0.69. Finally, we point out that even without the blank filler, our model's macro-averaged F1 score is higher than zero-shot GPT4 and LLaMA2.\nC. Explainability\nNext, we compare the explainability provided by our method (best-performing language generation-based method) and PrivBERT (best-performing embedding-based method) using the metrics described in Sec. IV-E2. More specifically, we use normalised Levenshtein distance and overlap percentage for our method and LIME-based overlap percentage for PrivBERT. We report the results for our held-out test set.\nFirst, we present Levenshtein distance results in Fig. 4 (a) and (b). Each scatter dot represents a data point in our test set. The x-axis represents the legal annotation's character length, while the y-axis represents the generated reason's character length. Each data point is coloured according to the normalised Levenshtein distance between the two texts. A diagonal data-point with 0 distance indicates a perfect prediction similar to 'what a legal expert would have annotated'.\nWe show the results of our method in Fig. 4 (a). For comparison, in Fig. 4 (b) we present the same result for a random text generation baseline. That is, we run a separate experiment where, for each prediction, we randomly sample a text from the same paragraph and assume it as the reason generated by the model. This random sampling is done according to the annotation length to paragraph length ratio distribution of the training dataset to mimic a realistic sampling process.\nWe observe from the results that the generated reason distribution of our method is more positively correlated with the legal expert annotation distribution, unlike a randomly selected sample (from the same paragraph) distribution. In our case, most of the points are around or in the direction of the diagonal. In contrast, in the random case, there are more samples spread near the x-axis and y-axis, indicating significant differences between the two texts. In the figure, we outline all the samples with little or no overlap with the legal expert annotation despite some having a low normalised Levenshtein distance between them, in red. As can be seen, our method has significantly fewer such points. We further quantitatively analyse overlap percentages later in this section.\nObserving the length of the PrivBERT's LIME-words to legal expert annotation length distribution in Fig. 4 (c), we can visually identify some drawbacks with embedding models. First, LIME can only interpret the explainability of PrivBERT's most confident output; therefore, the number of samples we can analyse is lower. Next, LIME being designed for predictive models, the words it identifies may not necessarily belong to a continuous block, and it can only consider a certain number of perturbations in a selected paragraph. Therefore, the samples are distributed more along the direction of the x-axis with LIME-words' length capped at ~150 characters. As we do not consider normalised Levenshtein distance in this figure, darker shades of blue only represent densely packed sample points. Outlined in red represents the same meaning as with subfigures (a) and (b).\nTo quantitatively analyse the explainability, we present the results for overlap percentages. We observe that with our method, 57.9% of the predictions have at least 50% overlap with legal annotations. However, in contrast to that, nearly 45.9% of randomly selected text had less than 10% overlap with the legally annotated text (these samples with less than 10% overlap are outlined in red colour in all sub-figures in Fig. 4). These results show that our method's explainability pre-dominantly overlaps with legal annotations and quantitatively, there is at least a 10% overlap for more than 90% of data samples with our method.\nWhen we consider LIME-word based overlap percentage for PrivBERT, only around 18.3% of LIME words overlap 50% or more with the legal annotations and even for random sampling, this overlap count was 16.2%. Also, a similar percentage (18.8%) of samples had less than 10% of overlap. From qualitative observations, we further identified that most LIME words concentrate with class-specific words such as \"third\" for \"third party sharing/collection\u201d. This concludes that even when looking at the most confident output of PrivBERT, its quantified explainability is really low compared to our method."}, {"title": "VI. CONCLUSION", "content": "We proposed an entailment-driven LLM-based framework for privacy policy paragraph classification and for providing explanations behind those predictions. Our training pipeline consists of an explained classifier, blank filler, and an entailment verifier that outperformed other language generation-based baselines such as T5, GPT4, and LLaMA2 by ~8%-14%. The key reason for this is that our framework, inspired by one-step Chain of Thoughts (CoTs) reasoning, avoids the commonplace hallucination problem of LLMs by providing a reason for each classification label. Using the blank filler that re-predicts the same reasoning that subsequently undergoes the entailment verification process, our method can filter such hallucinated outcomes effectively. As a result, our model has a macro-average precision increase of 38% compared to GPT4. Though the proposed framework does not achieve the performance levels of embedding-based models such as PrivBERT, it provides explanations behind the label predictions, which is useful in the context of privacy and usability. To this end, we showed that our method generates reasoning texts that are likely to be at least 50% or more overlapping with what a legal expert would have reasoned. Overall, our results show that while LLMs can be useful for providing more user-friendlier means to access privacy policies, they are not that useful in their vanilla form. Rather, it is necessary to have auxiliary steps as we proposed in our framework."}]}