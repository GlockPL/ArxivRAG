{"title": "MemWarp: Discontinuity-Preserving Cardiac\nRegistration with Memorized Anatomical Filters", "authors": ["Hang Zhang", "Xiang Chen", "Renjiu Hu", "Dongdong Liu", "Gaolei Li", "Rongguang Wang"], "abstract": "Many existing learning-based deformable image registration\nmethods impose constraints on deformation fields to ensure they are glob-\nally smooth and continuous. However, this assumption does not hold in\ncardiac image registration, where different anatomical regions exhibit\nasymmetric motions during respiration and movements due to sliding\norgans within the chest. Consequently, such global constraints fail to\naccommodate local discontinuities across organ boundaries, potentially\nresulting in erroneous and unrealistic displacement fields. In this pa-\nper, we address this issue with Mem Warp, a learning framework that\nleverages a memory network to store prototypical information tailored\nto different anatomical regions. Mem Warp is different from earlier ap-\nproaches in two main aspects: firstly, by decoupling feature extraction\nfrom similarity matching in moving and fixed images, it facilitates more\neffective utilization of feature maps; secondly, despite its capability to\npreserve discontinuities, it eliminates the need for segmentation masks\nduring model inference. In experiments on a publicly available cardiac\ndataset, our method achieves considerable improvements in registration\naccuracy and producing realistic deformations, outperforming state-of-\nthe-art methods with a remarkable 7.1% Dice score improvement over\nthe runner-up semi-supervised method. Source code will be available at\nhttps://github.com/tinymilky/Mem-Warp.", "sections": [{"title": "1 Introduction", "content": "Cardiovascular disease, a major cause of death worldwide [22], depends on med-\nical imaging, especially cine-MRI [13], for diagnosis and treatment. Deformable\nimage registration [3], a crucial step for cardiac analysis, has seen improvements\nthrough learning-based neural networks. These models vary from unsupervised\nto semi- and weakly-supervised frameworks. Unsupervised methods are favored"}, {"title": "2 Methodology", "content": ""}, {"title": "2.1 Preliminaries", "content": "Deformable image registration aims to establish voxel-level correspondences be-\ntween a moving image Im and a fixed image If. The spatial relationship is\nrepresented by $\\phi(x) = x + u(x)$, where x is a spatial location within the domain\n$\\Omega\\subset R^{H\\times W\\times D}$, and $u(x)$ denotes the displacement vector at that location.\nIn unsupervised learning, a network $F_\\theta$ is trained to predict the deformation\nfield $\\phi$, with its weights $\\theta$ optimized by minimizing a composite loss function\n$\\mathcal{L}$. This function combines metrics for dissimilarity between the warped mov-\ning image and the fixed image, and the smoothness of the deformation field:\n$\\mathcal{L} = \\mathcal{L}_{sim} (I_f, I_m \\circ \\phi) + \\lambda \\mathcal{L}_{reg}(\\phi)$. Here, $\\lambda$ serves to balance the smoothness con-\nstraint on the deformation field, with methods like the discontinuous regularizer\nproposed by Ng et al. [18] falling under this strategy. Semi-supervised methods,\nincluding our MemWarp, introduce an additional Dice loss $\\mathcal{L}_{dsc}(J_f,J_m\\circ\\phi)$ to\nassess the dissimilarity between the warped moving mask and the fixed mask.\nWeakly-supervised models need mask inputs for the network $F_\\theta$. For instance,\nDDIR [9] requires both moving and fixed masks, while textSCF [8] requires only\nthe fixed mask."}, {"title": "2.2 Laplacian Pyramid Warping Network", "content": "To decouple feature extraction from similarity matching, we develop a Laplacian\npyramid warping network (LapWarp) that leverages residual deformation fields\nacross multiple scales, from coarse to fine. Contrary to previous method LapIRN\n[16,17], which applies image pyramids directly to raw images, LapWarp performs\nwarping on feature maps and allows for interactions at all levels of the pyramid.\nThis ensures stable training within its pyramid framework without requiring the\nwarm starts or multi-stage coarse-to-fine training strategies.\nNetwork Architecture: LapWarp deviates from classic Unet by stacking\nmoving and fixed images across the batch dimension and employing a unique\ndecoder structure. In each decoder level, moving image features are first warped\nusing the previous level's field. A standard decoder layer then extracts features\nfrom both images as a batch, which a flow generator uses at each pyramid level\nto create the residual deformation field by re-stacking features along channels.\nGiven n pyramid levels, we obtain n residual deformation fields, labeled from\n$\\Delta\\phi_n$ to $\\Delta\\phi_1$, and n + 1 total deformation fields, labeled from $\\phi_{n+1}$ to $\\phi_1$, with\nboth sets following the convention that a larger index indicates a coarser level.\nAt level i + 1, the feature maps $\\hat{I}_{m_{i+1}}$ and $\\hat{I}_{f_{i+1}}$ are generated by its decoder\n$d_{i+1}$. These feature maps, stacked along the channel dimension, are processed\nby the flow generator $f_{i+1}$ to produce the residual deformation field $\\Delta\\phi_{i+1} =$\n$f_{i+1}(\\hat{I}_{m_{i+1}}, \\hat{I}_{f_{i+1}})$. This residual field is then combined with the upsampled and\nscaled (by a factor of 2) deformation field $\\phi_{i+2}$ from level i + 2, resulting in\nthe deformation field for level i + 1, given by $\\Phi_{i+1} = \\Delta\\phi_{i+1} + \\Phi_{i+2}$. For the ith\nlevel, the encoder feature maps $I_{m_i}$ and $I_{f_i}$, together with upsampled decoder"}, {"title": "2.3 Discontinuity-Preserving Deformable Registration", "content": "DDIR [9] is the first neural network solution to generate high-quality, discontinuity-\npreserving deformation fields, but it requires segmentation masks for both train-\ning and inference, linking deformation field quality to segmentation accuracy.\nAdditionally, DDIR's use of masks increases computational load by splitting\nimage pairs per anatomical region. MemWarp tackles these challenges by in-\ncorporating a memory network [21] that adaptively learns prototypical feature\nrepresentations for different anatomical regions. Empirical evidence suggests that\nlearning such prototypical features is not feasible when features from moving and\nfixed images are entangled, which led to the development of LapWarp.\nAnatomical Filters: Typically, the flow generator uses convolutional or\nself/cross-attention layers as in transformers, ending with a single convolutional\nfilter of kernel size 1 to produce the deformation field. Our approach replaces\nthis filter with dynamic filters [8, 26], adapting to the voxel context based on\nfixed feature maps. Given x as a location vector within $\\Omega \\subset R^{H_i\\times W_i\\times D_i}$, let\n$\\hat{I}_{m_i}$ and $\\hat{I}_{f_i}$ represent the moving and fixed feature maps from the decoder"}, {"title": "2.4 Loss function & Overall Framework", "content": "The composite loss function for MemWarp is formulated as:\n$\\mathcal{L} = \\mathcal{L}_{sim}(I_f, I_m \\circ \\phi) + \\mathcal{L}_{dsc}(J_f, J_m \\circ \\phi) + \\lambda_1\\mathcal{L}_{reg} + \\mathcal{L}_{rgn},$\nwith $\\mathcal{L}_{reg} = \\Sigma_{i \\in \\pi} ||\\nabla u_i(x)||^2 (u_i(x) = \\phi_i(x) - x)$ and $\\lambda$ adjusting the smooth-\nness regularization strength. The framework of MemWarp aligns with traditional\nregistration frameworks like VoxelMorph but introduces three critical adjust-\nments: 1) Moving and fixed images are combined along the batch dimension;\n2) Flow generators, enhanced by memory networks, supplement a conventional\nUnet, yielding a gradually warped moving image for each decoder level; 3) Deep\nsupervision [14] is employed on the memory-addressed tensors to encourage dis-\ncontinuities across regions."}, {"title": "3 Experiments & Results", "content": "We evaluate MemWarp's effectiveness using the ACDC dataset [5], which in-\ncludes 150 subjects. Each subject is provided with images from both End-diastole\n(ED) and End-systole (ES) phases alongside segmentation masks. For intra-\nsubject registration, images from both ED to ES and ES to ED phases are\nrequired to be aligned, resulting in a total of 300 pairs ([100+50] \u00d7 2). Of these,\n170 pairs are allocated for training, 30 for validation, and the remaining 100 for\ntesting. The distribution is stratified to ensure subjects with various diseases are\nevenly represented across training, validation, and testing phases, with no over-\nlap of subjects between training or validation and testing. All images undergo a\nmin-max normalization to (0,1), are resampled to a voxel size of 1.8 \u00d7 1.8 \u00d7 10\nmm and adjusted to a size of 128 x 128 \u00d7 16."}, {"title": "3.1 Implementation Details & Comparator Methods", "content": "Experiments use Python 3.7 and PyTorch 1.9.0 [19] on a machine equipped\nwith an A100 GPU, and a 16-core CPU with 32GB RAM. Training employs\nthe Adam optimizer with a learning rate of 4e-4, a batch size of 4, and cosine\ndecay, running for 400 epochs. The Mean Square Error (MSE) serves as the\nsimilarity loss, complemented by L2 regularization on the spatial gradients of the\ndeformation field ($\\lambda = 0.01$ in Eq. (4)), following [3, 10], with seven integration\nsteps in the diffeomorphic layer. For MemWarp, a diffeomorphic layer is used\nat all pyramid levels except the first. Other models apply only MSE loss, Dice\nLoss, and regularization as outlined in Eq. (4)'s initial three terms.\nComparator Methods: MemWarp is benchmarked against top learning-\nbased models such as VoxelMorph [3], TransMorph [7], LKU-Net [12], and Slicer\nNetwork [25], as well as DDIR [9] which is recognized for its discontinuity-\npreserving capabilities in cardiac registration. For DDIR, we employ the leading\nmodel nnFormer [27] for segmentation, achieving a Dice score of 90.15% on the\ntest set. Slicer Network is assessed with an added guidance loss per its original\nconfiguration, while MemWarp and the other models are tested under a consis-\ntent experimental framework. We also include traditional methods like ANTs [2]\nand Demons [24]. While MemWarp is model-agnostic, we utilize the backbone\nof LKU-Net in this implementation.\nEvaluation Metrics: Aligned with standard practices [3,7], our evalua-\ntion employs the Dice coefficient and the 95th percentile Hausdorff Distance\n(HD95) for anatomical alignment evaluation. HD95 values are averaged across\nall anatomical structures for individual subjects. Additionally, the standard de-\nviation of the logarithm of the Jacobian determinant (SDlogJ) is utilized to\nevaluate the quality of diffeomorphism."}, {"title": "3.2 Results & Analysis", "content": "Registration Accuracy: Table 1 illustrates that all methods produce smooth\ndisplacement fields with low SDlogJ values; however, increased SDlogJ along-\nside higher Dice scores indicates inherent discontinuities in cardiac alignments."}, {"title": "3.3 Discussions", "content": "Let $I_{m_i}$ and $I_{f_i}$, be the feature maps of moving and fixed images at pyramid level\ni. MemWarp operates under the assumption that the 'brightness' at any given\nlocation $p\\in \\Omega$ in $I_{f_i}$ remains constant compared to moving image [11], which is\nformulated as:\n$\\nabla I_{f_i} (p)^T u(p) = I_{m_i} (p) - I_{f_i} (p),$\nwhere $\\nabla I_{f_i} (p) = \\big[ \\frac{\\partial I_{f_i}}{\\partial x}(p), \\frac{\\partial I_{f_i}}{\\partial y}(p), \\frac{\\partial I_{f_i}}{\\partial z}(p) \\big]^T$. Eq. (5) holds provided that the\nmagnitude of u(x) is less than one voxel. In the MemWarp framework, we employ\nan n-level Laplacian image pyramid to ensure $2^{(n-1)} > d_{max}$, where $d_{max}$ is the\nmaximum possible displacement magnitude. This setup ensures that the coarsest\nlevel meets the conditions of Eq. (5), with each finer level processing a pre-warped\nmoving image, thus maintaining the model's assumption throughout all levels.\nBased on the assumption, we've implemented two major modifications in neu-\nral network architecture to enhance registration performance. First, we decouple\nfeature learning from flow estimation. Unlike traditional registration networks\nthat combine moving and fixed images at the network's input, MemWarp em-\nploys a U-net for feature extraction and adds a simple convolution layer at each\npyramid level to compute flow and performs warping, ensuring each level satisfies\nEq. (5). Second, the smoothness requirement of Eq. (5) aligns well with features\nderived from segmentation networks, as segmentation can be regarded as the\nultimate form of image harmonization [6]. This reinforces that effective segmen-\ntation features are equally beneficial for registration. Consequently, MemWarp"}, {"title": "4 Conclusions", "content": "In conclusion, MemWarp establishes a new benchmark for cardiac registration,\noutperforming existing methods by effectively preserving essential anatomical de-\ntails and reducing artifacts. Its success hinges on two pivotal elements: the decou-\npling of moving and fixed feature maps via LapWarp, and the memory network's\nuse of region loss for maintaining discontinuities across boundaries. MemWarp's\neffectiveness is validated by a significant 7.1% Dice score enhancement over the\nnearest semi-supervised competitors. Moreover, MemWarp uniquely addresses\ndiscontinuities without needing segmentation masks at inference, yet it can still\ngenerate segmentation masks comparable to top segmentation methods."}]}