{"title": "A Random-Key Optimizer for\nCombinatorial Optimization", "authors": ["Antonio A. Chaves", "Mauricio G.C. Resende", "Edilson F. de Arruda", "Ricardo M. A. Silva"], "abstract": "This paper presents the Random-Key Optimizer (RKO), a versatile and efficient\nstochastic local search method tailored for combinatorial optimization problems.\nUsing the random-key concept, RKO encodes solutions as vectors of random\nkeys that are subsequently decoded into feasible solutions via problem-specific\ndecoders. The RKO framework is able to combine a plethora of classic metaheuris-\ntics, each capable of operating independently or in parallel, with solution sharing\nfacilitated through an elite solution pool. This modular approach allows for\nthe adaptation of various metaheuristics, including simulated annealing, iterated\nlocal search, and greedy randomized adaptive search procedures, among oth-\ners. The efficacy of the RKO framework, implemented in C++, is demonstrated\nthrough its application to three NP-hard combinatorial optimization problems:\nthe a-neighborhood p-median problem, the tree of hubs location problem, and\nthe node-capacitated graph partitioning problem. The results highlight the frame-\nwork's ability to produce high-quality solutions across diverse problem domains,\nunderscoring its potential as a robust tool for combinatorial optimization.", "sections": [{"title": "1 Introduction", "content": "An instance of a combinatorial optimization problem is defined by a finite ground\nset $E = {1,...,n}$, a set of feasible solutions $F \\subset 2^E$, and an objective function\n$f: 2^E \\rightarrow R$. In the case of a minimization problem, we seek a global optimal solution\n$S^* \\in F$ such that $f(S^*) \\le f(S), \\forall S \\in F$. The ground set E, the cost function f,\nand the set of feasible solutions F are defined for each specific problem. Similarly,\nin the case of a maximization problem, we seek an optimal solution $S^* \\in F$ such\nthat $f(S^*) \\ge f(S), \\forall S \\in F$. In the traveling salesman problem (TSP) on a graph\n$G = (N, A)$, for example, one seeks the shortest tour of arcs in A that visits each node\nin N exactly once and returns to the first node. The ground set E for the TSP consists\nof the sets of |A| arcs while the set of feasible solutions is made up of all subsets of\narcs in A such that they form a tour of the nodes in N. The cost f of a tour is the\nsum of the lengths of the arcs in the tour.\nA random key x is a real number in the interval [0,1), i.e. $x \\in [0,1)$. A vector X\nof n random keys is a point in the unit hypercube in R"}, {"content": ", $X = (x_1, x_2,...,x_n)$, where\n$x_i \\in [0,1)$. We shall refer to a vector of n random keys simply as random keys. A\nsolution of a combinatorial optimization problem can be encoded with random keys.\nGiven a vector x of random keys, a decoder D takes as input X and outputs a feasible\nsolution S\\in F of the combinatorial optimization problem, i.e., $F = D(x)$.\nThe Random-Key Optimizer (RKO) is a stochastic local search method that\nemploys the random-key concept for solution representation to address combinato-\nrial optimization problems. Since the introduction of the first random-key genetic\nalgorithm by Bean (1994), followed by the biased random-key genetic algorithms\n(BRKGA) of Gon\u00e7alves and Resende (2011a), various metaheuristics have been\nadapted to this framework. Recent adaptations include dual annealing (Schuetz et al.,\n2022), simulated annealing, iterated local search, variable neighborhood search (Man-\ngussi et al., 2023), and the greedy randomized adaptive search procedure (Chaves\net al., 2024).\nThis paper presents a C++ implementation of the RKO framework, which sim-\nplifies user interaction by requiring only the development of a decoder function. The\ncurrent framework incorporates eight classic metaheuristics that can operate indepen-\ndently or in parallel, with the latter approach facilitating solution sharing through\nan elite solution pool. These metaheuristics are problem-independent, relying on the\ndecoder to map between the random-key space and the solution space of the specific\noptimization problem. Additional metaheuristics can be easily added to the framework.\nAs a proof of concept, the RKO was tested on three NP-hard combinatorial opti-\nmization problems: the a-neighborhood p-median problem, the tree of hubs location\nproblem, and the node-capacitated graph partitioning problem.\nThe structure of this paper is as follows. To first illustrate the idea of encoding and\ndecoding with random keys, Section 2 first introduces decoders from successful appli-\ncations. Section 3 introduces the Random-Key Optimizer (RKO) concept. Section 4\ndetails the RKO framework components, including metaheuristics, shaking, blending,\nand local search modules. Section 5 demonstrates the application of RKO to three dis-\ntinct combinatorial optimization problems, each utilizing a different decoder. Finally,\nSection 6 offers concluding remarks."}, {"title": "2 Encoding and decoding with random keys", "content": "The random-key representation confers significant advantages in solving complex com-\nbinatorial optimization problems when coupled with problem-dependent decoders.\nThis approach preserves solution feasibility, simplifies search operators, and enables\nthe development of problem-independent metaheuristics. This paradigm facilitates\nefficient navigation of highly constrained solution spaces by establishing a mapping\nbetween continuous and discrete domains. Furthermore, it stimulates the creation\nof adaptable optimization algorithms applicable across diverse optimization prob-\nlems, allowing for core search mechanisms while adapting problem-specific constraints\nthrough customized decoders.\nIn the next subsections, we illustrate the encoding and decoding processes for a\ndiverse range of application domains, including packing, vehicle routing, and internet\ntraffic engineering."}, {"title": "2.1 Traveling Salesman Problem", "content": "Bean (1994) first proposed random key encoding for problems whose solutions can be\nrepresented as a permutation vector, as is the case for the TSP, an NP-hard problem\n(Karp, 1972). Given a vector of random keys x, the decoder simply sorts the keys of\nthe vector, and the indices of the sorted vector represent a permutation of 1, 2, ..., n.\nConsider a random key vector x = (0.085, 0.277, 0.149, 0.332, 0.148). Sorting the\nvector, we get $\\sigma[x] = (0.085, 0.148, 0.149, 0.277,0.332)$ with corresponding indices\n$\\pi(\\sigma[x]) = (1,5,3, 2, 4)$. Figure 1 shows this tour where we start at node 1, then visit\nnodes 5, 3, 2, and 4, in this order, and finally return to node 1."}, {"title": "2.2 Set Covering Problem", "content": "Given n finite sets $P_1, P_2,..., P_n$, let sets I and J be defined as $I = \\bigcup_{j=1}^{n} P_j =$\n${1,2,...,m}$ and $J = {1, ..., n}$. A subset $J^* \\subseteq J$ is called a cover if $\\bigcup_{j\\in J^*} P_j = I$.\nThe set covering problem is to find a cover of minimum cardinality. Let A be the\nbinary m \u00d7 n matrix such that $A_{i,j} = 1$ if and only if $i \\in P_j$. An integer programming\nformulation for set covering is\n$\\min {e^\\top x : Ax \\ge e_m, x \\in {0,1}^n}$,"}, {"title": "2.3 OSPF routing in intradomain traffic engineering", "content": "Let G = (N, E) be an Internet Protocol (IP) network, where N is its set of router nodes\nand E is its set of links. Given a set of traffic demands between origin-destination (O-\nD) pairs in the network, the Open Shortest Path First (OSPF), an NP-hard problem\n(Giroire et al., 2013), weight setting problem consists in determining weights to be\nassigned to the links so as to optimize a cost function, typically associated with a\nnetwork congestion measure, when traffic is routed on least-weight paths between O-D\npairs. Link weights $W_1, W_2,..., W_{|E|}$ typically are integer-valued in the interval [1, w],\nwhere w = $2^{16}$ \u2013 1.\nSolutions to the OSPF weight setting problem are encoded with a vector X of\nE random keys (Ericsson et al., 2002; Buriol et al., 2005). The decoder first sets\nlink weights as $w_i = [X_i \\times W]$, for $i = 1, 2, ..., |E|$, where [z] is the smallest integer\ngreater than or equal to z. Then, the demand for each O-D pair is routed on a least-\nweight path. For each link e \u2208 E, the flows from each O-D pair on that link are\nsummed up, resulting in the total flow $F_e$ on link e. The link congestion cost $d_e(F_e)$ is"}, {"title": "2.4 Redundant content distribution", "content": "Johnson et al. (2020) consider a situation where we need to distribute data, like video-\non-demand, over a network where link or vertex failures cannot be quickly repaired.\nThese failures could cause costly service interruptions, so we want a robust distribution\nprocess that is resilient to any single vertex or link failure. To achieve this, we need\nto place multiple copies of our data source in the network. However, due to hosting\ncosts, we want to minimize the number of hosting sites rather than placing a copy at\neach service hub.\nThe setwise-disjoint facility location model applies when we do not control routing,\nrelying on the network's shortest-path protocols (like OSPF) instead. Here, to guar-\nantee vertex-disjoint paths to a customer from two facility locations, we must ensure\nthat their shortest path sets intersect only at the customer location. We can examine\nevery pair of facility locations, s and t, and every customer location u. If the short-\nest paths from s to u and the shortest paths from t to u only intersect at u, then s\nand t cover u and the triple (u, s, t) can be saved for possible use in a solution to the\nsetwise-disjoint facility location problem. Model the network as a graph G = (V, E),\nwhere V are the vertices of G and E are its links. Let S CV be the set of nodes where\nhosting facilities can be located and assume users are located on any node belonging\nto the set UCV.\nIn the set cover by pairs problem, we are given a ground set U of elements, a set\nS of cover objects, and a set T of triples (u,s,t), where u \u2208 U and s, t \u2208 S. We seek\na minimum-cardinality cover by pairs subset S* CS for U, where S* covers U if for\neach u \u2208 U, there are s, t \u2208 S* such that (u, s, t) \u2208 T.\nSolutions to the setwise-disjoint facility location problem can be encoded with a\nvector x of S random keys. Decoding is similar to the second decoder for set covering,\nintroduced in Section 2.2. This decoder takes as input the vector of random keys x\nand returns a cover by pairs S* \u2286 S. To describe the decoding procedure, let the cover\nby pairs be represented by a binary vector $Y = (Y_1,...,Y_{|S|})$, where $V_j = 1$ if and\nonly if $j \\in S^*$.\nThe decoder has three phases. In the first phase, for j = 1,..., |S|, the values of\n$V_j$ are initially set according to\n$V_j =\\begin{cases}\n1 & \\text{if } x_j \\ge 0.5 \\\\\n0 & \\text{otherwise}.\n\\end{cases}$\nThe indices implied by the binary vector Y can correspond to either a feasible or\ninfeasible cover S*. If S* is a feasible cover, then the second phase is skipped. If S* is\nnot a valid cover, then the second phase of the decoding procedure builds a valid cover\nwith the greedy algorithm for set cover by pairs of Johnson et al. (2020), starting from\nthe partial cover S* defined by Y. This greedy algorithm proceeds as follows. While\nS* is not a valid cover, select the smallest index j\u2208 S \\ S* for which the inclusion of\njin S* covers a maximum number of yet-uncovered elements of U. If no such element"}, {"title": "2.5 2D orthogonal packing", "content": "In the two-dimensional non-guillotine packing problem, rectangular sheets must be\npacked into a larger rectangular sheet to maximize value. The rectangular sheets can-\nnot overlap or be rotated and must align with the larger sheet's edges. This problem,\nwhich is NP-hard (Garey and Johnson, 1979), is significant both theoretically and\npractically, with applications in industries like textiles, glass, steel, wood, and paper,\nwhere large sheets of material are cut into smaller rectangular pieces.\nGiven a large rectangular sheet of dimension H \u00d7 W and N types of smaller\nrectangular sheets, of dimensions $h_1 \\times w_1, h_2 \\times w_2, ..., h_N \\times w_N$. There are $Q_i$ sheets\nof type i, each having value $V_i$. Let $R_i$ denote the number of sheets of type i that are\npacked into the larger rectangular sheet. We seek a packing that maximizes the total\nvalue $\\sum_{i=1}^{N} R_i \\times V_i$, where $R_i < Q_i$, for $i = 1, 2, . . ., N.$"}, {"title": "2.6 Vehicle routing problem", "content": "The vehicle routing problem (VRP) has many practical applications, e.g., in logistics.\nIn these problems, one or more vehicles depart from a depot and visit a number of\ncustomer nodes and then return to the depot. One common objective is to minimize\nthe cost of delivery, e.g., minimize total distance given a fixed number of vehicles, or\nminimize the number of vehicles, given a maximum distance traveled by each vehicle.\nOften, constraints are imposed on the capacity of vehicles and on when they visit the\ncustomers."}, {"title": "3 Random-Key Optimizer", "content": "Random-Key Optimizer (RKO) is an efficient metaheuristic framework rooted in the\nconcept of the random-key genetic algorithm (RKGA), initially proposed by Bean\n(1994). This approach encodes solutions as vectors of random keys \u2014 real numbers\nuniformly distributed in the interval [0,1) \u2014 enabling a unique representation of com-\nbinatorial optimization problems in a continuous search space. The RKGA evolves\na population of random-key vectors over multiple generations, employing a problem-\nspecific decoder to transform each vector into a feasible solution and evaluate its"}, {"title": "4 Framework", "content": "This section presents the RKO framework developed to find optimal or near-optimal\nsolutions to combinatorial optimization problems. First, we present the components\nof the RKO that are used by the metaheuristics: initial and pool of elite solu-\ntions (Section 4.1), shaking (Section 4.2), blending (Section 4.3), and local search\n(Section 4.4). Then, we present the classical metaheuristics adapted to the random-key\nparadigm (Section 4.5) and an online parameter control method (Section 4.6).\nThe RKO framework, including its source code in C++ language and documen-\ntation, is freely available to researchers and practitioners through our public GitHub\nrepository at https://github.com/antoniochaves19/RKO."}, {"title": "4.1 Initial and pool of elite solutions", "content": "The metaheuristics initialize with solutions represented by n-dimensional vectors $x \\in$\n$[0, 1)^n$, where each random key $x_i$ is randomly generated within the half-open interval\n[0, 1). The quality of each solution is quantified by an objective function value, obtained\nthrough the application of a problem-specific decoder function D to x, denoted as\n$f(D(x))$.\nThe RKO framework maintains a shared pool of elite solutions (pool) accessible\nto all metaheuristics. This pool is initialized with A randomly generated random-key\nvectors. To enhance the quality of the initial pool, each solution undergoes refinement\nvia the Farey Local Search heuristic (detailed in Section 4.4.3). To preserve diversity\nwithin the pool, any clone solutions (i.e., solutions with identical objective function\nvalues) are subjected to a perturbation process using the shaking method (described\nin Section 4.2).\nAny solution generated by a metaheuristic that improves its current best solution\nis considered for inclusion in the elite pool. This inclusion is contingent upon the\nabsence of an existing clone within the pool. Upon acceptance of a new solution, the\npool maintains its size constraint by eliminating the solution with the worst objective\nfunction value, thereby ensuring a continuous improvement in the overall quality of\nthe elite pool."}, {"title": "4.2 Shaking", "content": "To modify a random-key vector X, a perturbation rate \u1e9e is employed. This value\nis randomly generated within a specified interval [\u1e9emin, \u1e9emax], which should be\ndefined according to the specific metaheuristic approach being used. The shaking\nmethod, inspired by the approach proposed by Andrade et al. (2019), applies random\nmodifications to the random-key values by utilizing four distinct neighborhood moves:\n\u2022 Swap: Swap the value of two randomly selected random keys $X_i$ and $X_j$.\n\u2022 Swap Neighbor: Swap the value of a randomly selected random key $X_i$ with its\nneighbor $X_{i+1}$. In case i = n, then $X_n$ is swapped with $X_1$.\n\u2022 Mirror: Change the value of a randomly selected random key $X_i$ with its complement\n$(1-X_i)$.\n\u2022 Random: Assign a newly generated random value within the interval [0,1) to a\nrandomly selected random key $X_i$.\nAlgorithm 1 outlines the shaking method. A shaking rate \u1e9e is initially randomly\ngenerated (line 1). The main loop (lines 2-18) then iterates over the random-key vector\nX, applying perturbations \u03b2\u00d7 n times. During each iteration, one of the four neigh-\nborhood moves is randomly selected and applied. After completing the perturbations,\nthe modified random-key vector x is returned (line 19). This vector must be decoded\nduring the metaheuristics search process."}, {"title": "4.3 Blending", "content": "The blending method creates a new random-key vector by combining two parent solu-\ntions (xa and xb). This process extends the uniform crossover (UX) concept (Davis,\n1991), incorporating additional stochastic elements. For each position in the vector,\na probability p determines whether the corresponding random key from xa or xb is\ninherited. We introduce a factor parameter to modulate the contribution of xb: when\nfactor = 1, the original key is used; when factor = \u22121, its complement (1.0 \u2013 x) is\nemployed. Furthermore, with a small probability \u00b5, the algorithm generates a novel\nrandom value within the [0, 1) interval, injecting additional diversity into the solution\nX. Algorithm 2 presents the pseudocode of the blending method."}, {"title": "4.4 Local Search", "content": "We introduce the local search procedure used in the RKO algorithms, which is car-\nried out by the Random Variable Neighborhood Descent (RVND) algorithm (Penna\net al., 2013). The RVND is an extension of the Variable Neighborhood Descent (VND)\nmethod proposed by Mladenovi\u0107 and Hansen (1997). VND operates by exploring a\nfinite set of neighborhood structures, denoted as $N_k$ for $k = 1,..., k_{max}$, where $N_k (X)$\nrepresents the set of solutions within the k-th neighborhood of a random-key vector"}, {"title": "4.4.1 Swap Local Search", "content": "The Swap LS heuristic involves swapping the positions of two random keys of the\nrandom-key vector. The process begins by generating a vector RK that randomly\norders the indices of the random keys. This ensures that the sequence in which pairs"}, {"title": "4.4.2 Mirror Local Search", "content": "The Mirror LS heuristic modifies the current value of each random key by inverting it.\nThis heuristic utilizes the RK vector to generate a random order of indices. For each\nindex in this sequence, the value $X_{RK_i}$ of the corresponding random key is replaced\nwith its complementary value (1 - $X_{RK_i}$). The first-improvement strategy is applied\nduring this process. The procedure is detailed in Algorithm 5."}, {"title": "4.4.3 Farey Local Search", "content": "The Farey LS heuristic adjusts the value of each random key by randomly selecting\nvalues between consecutive terms of the Farey sequence (Niven et al., 1991). The Farey\nsequence of order \u03b7 includes all completely reduced fractions between 0 and 1 with\ndenominators less than or equal to n, arranged in ascending order. For our application,\nwe use the Farey sequence of order 7:\n$F_7 = \\{\\frac{0}{1}, \\frac{1}{7}, \\frac{1}{6}, \\frac{1}{5}, \\frac{1}{4}, \\frac{2}{7}, \\frac{1}{3}, \\frac{2}{5}, \\frac{3}{7}, \\frac{1}{2}, \\frac{4}{7}, \\frac{3}{5}, \\frac{2}{3}, \\frac{5}{7}, \\frac{3}{4}, \\frac{4}{5}, \\frac{5}{6}, \\frac{6}{7}, \\frac{1}{1} \\}$.\nThis sequence creates 18 intervals that are used to generate new random key values.\nIn each iteration of this heuristic, the random keys are processed in a random order\nas specified by the RK vector and the first-improvement strategy is applied. The\nprocedure is detailed in Algorithm 6."}, {"title": "4.4.4 Nelder-Mead Local Search", "content": "The Nelder-Mead algorithm, originally proposed by Nelder and Mead (1965), is a\nnumerical optimization technique designed to locate the minimum of an objective\nfunction in a multidimensional space. This heuristic method, which relies on function\nvalue comparisons rather than derivatives, is widely employed in nonlinear optimiza-\ntion scenarios where gradient information is unavailable or computationally expensive\nto obtain. The algorithm initializes with a simplex of k + 1 points in a k-dimensional\nspace and iteratively refines the simplex through a series of geometric transforma-\ntions. These transformations include reflection, expansion, contraction (internal and\nexternal), and shrinking, each aimed at improving the worst point of the simplex.\nIn our research, we developed an adapted Nelder-Mead LS heuristic with three solu-\ntions: $x^1$, $x^2$, and $x^3$. The first solution is a current solution of the RVND, while the"}, {"title": "4.5 Metaheuristics", "content": "The versatility of the Random-Key Optimization (RKO) framework extends beyond\nits initial application in genetic algorithms, allowing for integration with a wide array\nof metaheuristics. In this paper, we explore the adaptability of RKO by implement-\ning a comprehensive framework that incorporates eight distinct metaheuristics. This\ndiverse set includes the BRKGA, simulated annealing (SA), greedy randomized adap-\ntive search procedure (GRASP), iterated local search (ILS), variable neighborhood\nsearch (VNS), particle swarm optimization (PSO), genetic algorithm (GA), and large\nneighborhood search (LNS). Each metaheuristic offers an approach to exploring the\nsearch space while benefiting from RKO's random-key representation and decoding\nmechanism features. This integration demonstrates RKO's capacity to adapt to vari-\nous optimization paradigms, providing a robust and flexible framework for addressing\ncombinatorial optimization problems.\nAll metaheuristics start the search process from randomly generated random-key\nvectors. After generating the initial solutions, each metaheuristic follows its search\nparadigm.\nThe RKO framework employs a predefined CPU time limit as its stopping crite-\nrion, ensuring all metaheuristics run with equivalent computational time. We adopted\nthis approach in order to have a fair comparison among diverse optimization meth-\nods when executed on identical hardware architectures. Nevertheless, the framework\nallows alternative stopping criteria, such as a maximum number of objective function\nevaluations or a specified convergence threshold.\nIn the context of RKO, the parameters of each metaheuristic need to be set using\noffline optimization (parameter tuning) or online optimization (parameter control)\nstrategies. Parameter tuning focuses on finding optimal parameter values for a specific\nalgorithm and problem instance and remains fixed throughout optimization. On the\nother hand, parameter control seeks to enhance performance by dynamically adjusting\nthe parameters that control the behavior of the metaheuristics.\nIn this framework, users can choose between parameter tuning and parameter con-\ntrol. In parameter tuning, users conduct an initial set of experiments to make informed\ndecisions, or they can rely on automated tools such as iRace (Birattari et al., 2002),\nparamILS (Hutter et al., 2009), or REVAC (Nannen and Eiben, 2007). This process\ncan be very complex in the RKO due to the number of parameters and problem-\nand instance-dependence. To overcome these difficulties, we develop a reinforcement-\nlearning-inspired parameter control based on the Q-Learning method (Watkins and\nDayan, 1992) (see Section 4.6).\nThe following sections provide a brief overview of the metaheuristics developed in\nthis paper. They outline the key principles and mechanisms underlying each approach\nand highlight the innovative aspects of the random-key solutions."}, {"title": "4.5.1 BRKGA", "content": "The Biased Random-Key Genetic Algorithm (BRKGA) (Gon\u00e7alves and Resende,\n2011a) is an evolutionary metaheuristic that extends the concept of RKGA. The algo-\nrithm maintains a population p of random-key vectors, evolving them over multiple"}, {"title": "4.5.2 GA", "content": "We also implemented a standard Genetic Algorithm (GA) (Holland, 1992; Goldberg,\n1989) to work with the random-key representation. GA is a nature-inspired meta-\nheuristic that emulates the principles of natural selection and genetic evolution to\nsolve optimization problems. In its traditional form, GA operates on a population of\ncandidate solutions, each encoded as a chromosome. The algorithm progresses through\ngenerations, applying genetic operators such as selection, crossover, and mutation\nto evolve the population towards better solutions. Our modified GA performs selec-\ntion using the tournament method, applies crossover by combining random keys from\nselected parent solutions with the blending method, and implements mutation by per-\nturbing individual random keys (see Section 4.3). The algorithm creates an entirely\nnew population of offspring for each generation to replace the current population with\nelitism for population evolution. We apply the RVND heuristic (see Section 4.4) to the\nbest solution of the current population and insert this improved solution into the new\npopulation. The parameters of the GA are the population size (p), and the crossover\n(pc) and mutation (\u00b5) probabilities. Parameter pc represents the likelihood that two\nsolutions will exchange the random keys equally or that the parents will copy to the\nnext population. The parameter \u03bc represents the likelihood that the random key will\nbe randomly altered during the crossover."}, {"title": "4.5.3 SA", "content": "Simulated Annealing (SA) (Kirkpatrick et al., 1983) is a metaheuristic inspired by the\nannealing process in metallurgy, where controlled cooling of materials leads to more\nstable, low-energy states. In optimization contexts, SA navigates the solution space by\niteratively perturbing the current solution and accepting or rejecting the new solution\nbased on a probabilistic criterion. In the case of random-key representation, the shaking\nmethod performs the perturbation process (see Section 4.2). The SA begins with a\nhigh temperature ($T_o$), allowing for frequent acceptance of worse solutions to escape\nlocal optima. As the temperature gradually decreases according to a cooling schedule\ndefined by the parameter a, the algorithm becomes increasingly selective, converging\ntowards high-quality solutions. The Metropolis criterion controls the probability of\naccepting a worse solution based on the difference between the objective function"}, {"title": "4.5.4 GRASP", "content": "Chaves et al. (2024) adapted the Greedy Randomized Adaptive Search Procedure\n(GRASP) (Feo and Resende, 1995) and Continuous-GRASP (Hirsch et al., 2007)\nto solve combinatorial optimization problems using random-key representation. This\nalgorithm has two phases: a constructive phase and a local search phase. The construc-\ntive phase uses a line search strategy inspired by C-GRASP to generate new solutions,\nwhile the local search phase employs the RVND (Section 4.4). Our GRASP iteratively\nimproves solutions by generating random-key vectors, adjusting the grid parameter h,\nand using a simulated annealing acceptance criterion to decide whether to accept new\nsolutions.\nThe constructive phase is an iterative process that perturbs a given solution using\nsemi-greedy moves. Each iteration uses the line search to find the best objective func-\ntion value for each random key that is not fixed, randomly generating new random\nkeys in the sub-intervals defined by h. A Restricted Candidate List (RCL) is then cre-\nated, containing indices of random keys that produced solutions within a range defined\nby a randomly set parameter \u03b3\u2208 [0, 1]. An index is randomly selected from the RCL,\nits corresponding random key is updated with the value found by the line search, and\nthis random key is fixed. This process continues until all random keys have been fixed,\nbalancing randomness and greediness in solution construction. Initially, the parameter\nh is set to hs, and each iteration without improvement of the current solution makes\nthe grid more dense (h = h/2), up to an end grid density (he)."}, {"title": "4.5.5 ILS", "content": "Iterated Local Searcch (ILS) (Louren\u00e7o et al., 2003) is a metaheuristic that alternates\nbetween intensification and diversification to explore the solution space effectively. In\nour implementation, we adapt the ILS framework to operate within the random-key\nrepresentation paradigm. The algorithm begins with an initial solution encoded as a\nvector of random keys. It then enters its main loop, where it iteratively applies local\nsearch to reach a local optimum, followed by a perturbation mechanism to escape\nthis local optimum. We consider two components in this process: the shaking method\n(detailed in Section 4.2) and the RVND method (described in Section 4.4). The shak-\ning method is our perturbation mechanism, introducing controlled randomness to the\ncurrent solution while preserving some of its structure. In each iteration, a param-\neter \u1e9e controls the intensity of the perturbation, generating a random value within\nthe interval [Bmin, \u1e9emax]. The RVND, on the other hand, is utilized within the local\nsearch phase to generate new candidate solutions by combining features of the cur-\nrent solution with other elite pool solutions. By operating on random-key vectors"}, {"title": "4.5.6 VNS", "content": "Variable Neighborhood Search (VNS) (Mladenovi\u0107 and Hansen, 1997) is a metaheuris-\ntic similar to ILS that systematically leverages neighborhood changes to escape local\noptima. In our implementation, VNS begins with an initial solution encoded as a vec-\ntor of random keys and iteratively applies the shaking method (Section 4.2) and the\nRVND method (Section 4.4). The shaking method perturbs the current solution with\na randomly selected intensity, denoted by \u00df, which is defined by the current neigh-\nborhood as k\u00d7 \u1e9emin. If a better solution is found, the search returns to the first\nneighborhood (k = 1); otherwise, it proceeds to the next neighborhood (k = k + 1).\nA maximum neighborhood number ($k_{max}$) is predefined. After the shaking phase,\nthe RVND procedure is applied, systematically exploring multiple heuristics in a\nrandomized order."}, {"title": "4.5.7 PSO", "content": "Similar to BRKGA and GA, Particle Swarm Optimization (PSO) (Kennedy and Eber-\nhart, 1995) is a population-based metaheuristic inspired by the social behavior of\nbird flocking. In PSO, a group of p candidate solutions, known as particles, navigate\nthe search space by adjusting their positions based on their own best-known position\n(Prest) and the swarm's best-known position (Gbest). We adapt PSO by representing\neach particle as a vector of random keys. In each generation, all particles are updated\nby calculating their current velocity V using the following equation:\n$V_i = w \\cdot V_i + C_1 \\cdot r_1\\cdot (Prest - X_i) + C_2\\cdot r_2\\cdot (Gbest - X_i)$,\nwhere $C_1, C_2$, and w are parameters, and $r_1$ and $r_2$ are random numbers uniformly\ndistributed in the real interval [0,1].\nWith these updated velocities, we adjust all random keys j of particle i ($x_i^j$) by\nadding the corresponding velocity $V_i^j$ to the current value $x_i^j$. The positions Prest and\nGbest are then updated with the new population. Additionally, the RVND heuristic\n(Section 4.4) is applied to one randomly selected particle in each generation."}, {}]}