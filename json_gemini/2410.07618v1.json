{"title": "Moyun: A Diffusion-Based Model for Style-Specific Chinese Calligraphy Generation", "authors": ["Kaiyuan Liu", "Jiahao Mei", "Hengyu Zhang", "Yihuai Zhang", "Xingjiao Wu", "Daoguo Dong", "Liang He"], "abstract": "Although Chinese calligraphy generation has achieved style transfer, generating calligraphy by specifying the calligrapher, font, and character style remains challenging. To address this, we propose a new Chinese calligraphy generation model \"Moyun\" which replaces the Unet in the Diffusion model with Vision Mamba and introduces the TripleLabel control mechanism to achieve controllable calligraphy generation. The model was tested on our large-scale dataset \"Mobao\" of over 1.9 million images, and the results demonstrate that \"Moyun\" can effectively control the generation process and produce calligraphy in the specified style. Even for calligraphy the calligrapher has not written, \"Moyun\" can generate calligraphy that matches the style of the calligrapher.", "sections": [{"title": "I. INTRODUCTION", "content": "Chinese calligraphy, with a history spanning over thousands of years, is a cherished cultural treasure of China which rep-resents the artistic of Chinese characters handwriting. Chinese calligraphy is rich in variations. Chinese has tens of thousands of characters, each with a different meaning. Additionally, a single character can be written in various fonts, such as regular script, running script, cursive script, clerical script, seal script, and so on. Moreover, the writing of the same font differs between calligraphers, as shown in Figure 1a. The writing of the same font by the same calligrapher shows consistency, which we refer to as calligraphic style. The rich variations in Chinese calligraphy require an extended period of study for an average person to master. However, Chinese calligraphy is widely used, which has led people to explore the use of Al for generating Chinese calligraphy. Recently, GAN and Diffusion models are applied to Chinese calligraphy generation [1]\u2013[4], yielding impressive outcomes. However, there are still some problems with the current generation models of Chinese calligraphy.\nZiGAN [2] is a seminal work that applies GAN to the field of Chinese calligraphy generation. ZiGAN constructed several small-scale datasets, each with consistent styles, and trained models separately on these datasets. As a result, the trained models gained the ability to transform standard printed characters into calligraphy\u00b9 with the style of the specific"}, {"title": "II. RELATED WORKS", "content": ""}, {"title": "A. Generative Adversarial Networks", "content": "Generative Adversarial Networks (GAN) [11] consists of a generator and a discriminator. The generator tries to produce data similar to real data, while the discriminator's task is to distinguish between the generated data and real data. Research on GAN-based calligraphy generation has been ongoing for a long time, with the earliest work being zi2zi [4], an open-source project based on pix2pix [12]. After that, there were calliGAN [1], ZiGAN [2], and end-to-end [13] models. The most recent work is a style transfer model called CCST-GAN [14]."}, {"title": "B. Diffusion Model", "content": "The Diffusion model is a widely studied image generation model, with its structure initially established in DDPM [5] and DDIM [15], and the introduction of VAE [16] in LDM [17] further improving its application. DiT [18] brought the Trans-former [19] architecture into the Diffusion model. Research on Diffusion-based calligraphy generation began relatively late. The earliest work is Calliffusion [3], which utilized a Unet-based Diffusion model. The most recent work is DP-Font [20], which employs a physical information neural network structure, achieving better control over character shapes."}, {"title": "III. METHOD", "content": "Figure 2 shows the architecture of \"Moyun\". \"Moyun\" is a diffusion model based on Vision Mamba [8], optimized with Mamba2 [9]. Furthermore, we control the generation process by the TripleLabel mechanism."}, {"title": "A. Preliminaries", "content": "Moyun is a diffusion model, so our training and inference follow the diffusion [5], [15] approach. The forward process gradually adds noise to the real data, specifically to our cal-ligraphy image $x_0$. The diffusion process follows the equation as shown in (1) .\n$q(x_t|x_0) = N (x_t; \\bar{\\alpha}_t x_0, (1 - \\bar{\\alpha}_t)I)$(1)\nFurthermore, the noise at step t, $x_t$ can be obtained by equation\n$x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t$(2)\nThe diffusion model trains the reverse denoising process, and the denoising equation is as follows:\n$p_\\theta(x_{t-1} | x_t) = N(\\mu_\\theta(x_t), \\Sigma_\\theta(x_t))$(3)\nOur loss function is given by the following equation:\n$L_{simple} (\\theta) = ||\\epsilon_\\theta(x_t) \u2013 \\epsilon_t||^2$(4)"}, {"title": "B. Block Design", "content": "To improve the model's ability to capture the structure of calligraphy, we replaced the Unet in the Diffusion Model with Vision Mamba [8], thereby introducing the patchify mechanism. Additionally, we used the latest Mamba2 [9] model instead of the first version of Mamba [21] in Vision Mamba. Thus, each patch corresponds to a small part of the calligraphic structure, and Mamba's efficient contextual relational ability effectively matches the relationships between patches, thereby better learning the structure of the calligraphy.\nThe input image is first mapped into the latent space via a VAE [16], followed by patchify processing. For a square image"}, {"title": "C. TripleLabel control", "content": "To accommodate the triple-conditions control requirements of Chinese calligraphy generation, which include calligrapher, font, and character, we designed a TripleLabel control mech-anism. Each calligrapher, font, and character is mapped to a unique class label, represented by a number, and each label is arbitrary. This allows the combination of labels to generate calligraphy that the calligrapher has not written before. In the model, the input label is transformed into the corresponding embedding vector through an embedding table. The three resulting embedding vectors are summed and used to control the generation process via a scale-shift mechanism [18].\nThis control mechanism significantly reduces computational cost compared to introducing a new text encoder. Furthermore, subsequent experiments demonstrated that this method of control is highly effective."}, {"title": "IV. EXPERIMENT", "content": ""}, {"title": "A. Dataset", "content": ""}, {"title": "a) Dataset construct:", "content": "To construct the large-scale an-notated dataset \"Mobao\u201d, we scraped images from web and subject them to binarization processing. The collected callig-raphy images include six fonts: regular script, running script, cursive script, clerical script, seal script, and seal carving. Each calligraphy image is annotated with the calligrapher, font, and character. We organized the data into a hierarchical folder structure of calligrapher-font-character, and each image within these categories was numbered. The complete processing workflow is illustrated in Figure 3."}, {"title": "b) dataset analyse:", "content": "After processing the dataset, we conducted an analysis. We obtained a total of 1,929,393 images, including works from 6 fonts, 2,681 calligraphers and 4,660 characters. Some images without attributed calligraphers were categorized under the \u201canonymous\u201d category.\nTo further evaluate our dataset, we conducted statistical analysis across the three dimensions of calligrapher, font, and character. The results confirm that our dataset is vast: the calligrapher with the most collected works, \u201cWang Xizhi\u201d, has a total of 124,854 images; the font with the largest collection running script, contains 668,040 images; and the character with the most instances, \u201cshu\" (book in chinese), appears in 11,949 images. This provides a rich source of data for the model to learn the structure and style of calligraphy.\nHowever, our dataset has some issues. Nearly half of the calligraphers have fewer than ten collected works, and half of the characters have fewer than one hundred images. This indicates that our dataset exhibits a long-tail distribution."}, {"title": "B. Experiment Setup", "content": ""}, {"title": "a) Experiment Dataset:", "content": "To ensure the accuracy of the experiment, we selected a balanced subset from the complete dataset to serve as the experimental dataset. The specific method is as follows: We chose 40 calligraphers and selected 40 characters that all of them have written. For each callig-rapher, 90% of the characters were used for training, and the remaining characters were used for testing. This ensures that each test character is unseen during training for that particular calligrapher, while other characters by the same calligrapher and the same characters written by other calligraphers are used for training. This approach ensures that both the character shapes and the calligrapher's style are adequately trained. Additionally, due to the varying number of characters for each calligrapher, we randomly selected up to four images for each character. Finally, we chose 12,985 images, including 11,689 images in tranining set and 1,296 images in test set."}, {"title": "b) Model Specifics:", "content": "For the choice of VAE [16], we used the same pre-trained VAE as in LDM [17]. Specifically, the original image of 256 \u00d7 256 \u00d7 3 size is mapped to a latent space of 32 \u00d7 32 \u00d7 4 size. We set the number of iterations N for our model to 4, with a hidden layer depth of 512, and the image segmentation patch size to 8. During training, we used a learning rate of 1e-4 and trained on three A100 GPUs with a global batch size of 768. Ultimately, we selected the model at 288,000 steps (19,199 epochs) for subsequent experiment."}, {"title": "C. Evaluation Metrics", "content": "We evaluated our model from two perspectives: the structure and the style of the generated calligraphy. To assess the basic structural accuracy, we used Tencent's handwritten OCR service\u00b2 to recognize the generated calligraphy. Additionally, we used objective parameters such as IOU (Intersection over Union) and PSNR (Peak Signal-to-Noise Ratio) to measure the similarity between the generated calligraphy and the ground truth. A higher similarity indicates a closer match in style and a more reasonable structure. Lastly, we conducted a human evaluation experiment to verify how well our model captures the calligraphy style."}, {"title": "D. Experiment Results", "content": ""}, {"title": "a) Qualitative Evaluation:", "content": "Tencent's handwritten OCR service was used to recognize the generated calligraphy. The generated calligraphy were provided to the OCR API, which returned the recognized characters. We compared these with the character labels used to guide the generation process. If the recognized character matched the character label, we considered the generated character to have passed the test, indicating good structural integrity.\nWe randomly selected 60 images from each of the five different fonts for evaluation, considering that the recognition accuracy of handwritten OCR varies across different fonts. The results are shown in Table I. The models and datasets of other works have not been open-sourced, so this experiment was not conducted.\nThe results indicate that our model performs well on commonly used fonts like regular script and running script. However, the recognition rates for less commonly used fonts such as cursive script, clerical script and seal script were lower. This is probably due to the significant differences between these fonts and modern simplified Chinese characters, which caused inaccuracies in OCR recognition. We will continue to evaluate structure of the generated calligraphy using other metrics."}, {"title": "b) Quantitative Evaluation:", "content": "We generated the same number of images as the entire test set using the prompts from the test set and evaluated the IOU (Intersection over Union). The results showed that our model significantly outperforms other models in terms of IOU. Using the same test set, we also evaluated the PSNR (Peak Signal-to-Noise Ratio) of the model. The results demonstrated that our model has a better performance in PSNR either. These findings are presented in Table II. This indicates that our model is better at fitting the structural integrity of calligraphy characters and more accurately replicating the style of calligraphy.\nWe designed a human eval-uation experiment by selecting ten generated calligraphy sam-ples and creating ten corresponding questions. Some of these calligraphy was shown in Figure 4. For each question, we provided one generated calligraphy, with four options: one cal-ligraphy from the backbone, and three calligraphy from other calligraphers' works. Both of them are the same character. The question posed was, \"Please select the calligraphy that most closely matches the style of the given one.\" The questionnaire was distributed to 9 participants, and the results shows that 53.3% generated calligraphy was paired with ground truth. This demonstrates that our model is capable of generating calligraphy in the specified style. The models and datasets of other works have not been open-sourced, so they were not included in the questionnaire."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a new calligraphy generation model, \"Moyun\", which could generate calligraphy in a spec-ified style guided by the three labels: calligrapher, font, and character. The core idea was the introduction of Vision Mamba and the development of the TripleLabel control method. Additionally, we collected a large-scale, well-annotated, and properly binarized calligraphy dataset \"Mobao\", which further demonstrated the effectiveness of our work."}]}