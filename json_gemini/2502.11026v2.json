{"title": "Simplify RLHF as Reward-Weighted SFT: A Variational Method", "authors": ["Yuhao Du", "Zhuo Li", "Pengyu Cheng", "Zhihong Chen", "Yuejiao Xie", "Xiang Wan", "Anningzhe Gao"], "abstract": "Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values. However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance. To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called Variational Alignment with Re-weighting (VAR). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) (OpenAI, 2024; Touvron et al., 2023a; Yang et al., 2024) have achieved remarkable success in extensive applications of artificial intelligence (AI), including dialogue generation (Abdullin et al., 2024), coding (Cobbe et al., 2021; Shao et al., 2024), logical reasoning (Suzgun et al., 2022), and AI agents(Wu et al., 2023). Among the diverse LLM training techniques, Reinforcement Learning from Human Feedback (RLHF) plays a core role in ensuring the LLM generation is helpful and harmless. (Ouyang et al., 2022; Rafailov et al., 2024). In particular, RLHF first learns a reward model (RM) from annotated human preferences, then trains LLMs within a reinforcement learning (RL) scheme via Proximal Policy Optimization (Schulman et al., 2017) to optimize the expected rewards from the learned RM (Ouyang et al., 2022). Although recognized as the mainstream solution to LLM alignment (Ouyang et al., 2022; Shao et al., 2024; Touvron et al., 2023b; OpenAI, 2024; Yang et al., 2024), RLHF remains being challenged because of its expensive computational resource consumption (Cheng et al., 2023; Yuan et al., 2023) and complicated implementation (Ouyang et al., 2022; Shao et al., 2024; Yuan et al., 2023) in which multiple models (e.g. the learning policy, the reference, the critic model, and the reward model) are required to cooperate in the online RL training scheme. Moreover, incorporating such a complicated pipeline significantly induces training complexity and instability, leading to the difficulty of training convergence and the high risk of collapse (Song et al., 2023; Go et al., 2023). Towards more stable training than online alignment, several ranking-based offline alternatives are proposed, primarily from the perspective of enlarging the likelihood margin between preferred and rejected response pairs in a contrastive approach. Direct Preference Optimization (DPO) (Rafailov et al., 2024) implicitly maximizes the difference in sampling probabilities between good and bad answers. Ethayarajh et al. (2024) introduces Kahneman-Tversky Optimization (KTO) to directly maximize the utility of generations instead of maximizing the log-likelihood of preferences. Although methods like GPRO (Shao et al., 2024) forego the critic model, instead estimating the baseline from group scores and significantly reducing training resources, its online sampling strategy still challenges the practical implementation and training speed. While effective, these methods usually rely on the collection of preferred / rejected response pairs with high quality, which introduces a substitution data collection consumption. Instead, Advantage Leftover Launch (A-LoL) (Baheti et al.) formulates the reinforcement learning process at the sequence level and derives an advantage-based offline objective that exclusively utilizes preferred responses to achieve human-aligned results. However, it still relies on clipping the importance weights to ensure training stability, which prevents the optimization from reaching the true RLHF optima. Furthermore, approaches like DPO and ALoL could employ negative weights for potential dis-preferred responses, leading to an unstable training process due to the unbounded nature of loss landscape when negative weights are applied (Pal et al., 2024; Yan et al., 2024). In this paper, we address these limitations by proposing a reward-driven variational alignment framework that eliminates the need for clipping and avoids the instability introduced by negative weights. Our approach reformulates RLHF as a variational inference problem over positive measures, ensuring a stable and well-defined optimization landscape. Specifically, starting from the closed-form optimal solution of RLHF, we minimize the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951) between the to-be-learned LLM and its optimal solution. The resulting loss function takes the form of a reward-driven weighted supervised fine-tuning (SFT) loss, where non-negative weights are derived through an exponential reward transformation. Furthermore, we introduce an efficient in-batch normalization technique to approximate the normalization term, enabling scalable and practical implementation. Experimental results demonstrate that our framework outperforms existing methods in both stability and alignment performance, providing a robust solution to the challenges of RLHF."}, {"title": "2. Preliminary", "content": "Reinforcement Learning from Human Feedback (RLHF) is an essential approach to alignment LLMs with human values, especially from the perspectives of helpfulness and harmlessness (Ouyang et al., 2022). RLHF first learns a reward model $r(x, y)$ from a given collection of human preference data $D_p = \\{(x, y_w, y_l)\\}$, where x is a user input prompt, $y_w, y_l$ are the preferred and rejected responses selected by annotators, respectively. To learn a representative RM, following Bradley-Terry (Bradley & Terry, 1952) objective is usually utilized:\n$-E_{(x,y_w,y_l)\\sim D_p}[log \\sigma(r(x, y_w) - r(x, y_l))]$,\nwhere $\\sigma(\\cdot)$ is the Sigmoid function. Intuitively, Equation (1) induces $r(x, y)$ to assign a higher reward score to the preferred response $y_w$ than the rejected response $y_l$ with respect to input x. With a learned RM $r(x, y)$, RLHF optimizes the target LLM policy $\\pi_\\theta(y|x)$ by maximizing the expected reward:\n$E_{x\\sim D, y\\sim \\pi_\\theta(y/x)}[r(x, y)] - \\beta KL[\\pi_\\theta||\\pi_{ref}]$,\nwhere $KL[\\pi_\\theta ||\\pi_{ref}]$ is the KL divergence (Kullback & Leibler, 1951) between the training policy $\\pi_\\theta(y|x)$ with a reference model $\\pi_{ref}(y|x)$ to prevent $\\pi_\\theta(y|x)$ from the degeneration and preserve the generation diversity. $\\beta > 0$ is a hyper-parameter to re-weight the expected reward and the KL regularization term. To solve the RLHF objective in Equation (2), Proximal Policy Optimization (PPO) (Schulman et al., 2017) has been recognized as the mainstream optimization algorithm (Rafailov et al., 2024). However, as mentioned in Section 1, PPO suffers from training instability and high complexity in computation and implementation (Yuan et al., 2023; Cheng et al., 2023). Therefore, many of recent works have been proposed to simplify and improve the original PPO algorithm. Rafailov et al. (2024) theoretically demonstrate that Equation (2) has a closed-form solution:\n$\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{ref}(y|x) exp(\\frac{1}{\\beta}r(x,y))$,\nwhere $Z(x) = E_{y\\sim \\pi_{ref}(x|y)}[exp(\\frac{1}{\\beta}r(x, y))]$ is the denominator that normalizes the conditional distribution. Based on the relation between the optimal policy $\\pi^*(y|x)$ and the RM $r(x, y)$, Rafailov et al. (2024) convert the RM learning objective Equation (1) to an optimal policy learning loss named Direct Preference Optimization (DPO):\n$-E_{D}[log \\sigma(\\beta log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)})]$.\nBaheti et al. adopt the PPO objective into an offline scheme by using importance sampling and converting the expectation of $\\pi_\\theta(y|x)$ to the expectation of $\\pi_{ref}(y|x)$, then propose Advantage-Leftover-Lunch (A-LoL) gradient estimation:\n$-E_{x \\sim D, y\\sim \\pi_{ref}(y/x)}[\\hat{A}^{\\pi_{ref}} \\cdot \\nabla_\\theta log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\bar{\\theta}}(y|x)}]$,\nwhere $\\hat{A}^{\\pi_{ref}}$ is the estimated advantage value (Schulman et al., 2016) with respect to $\\pi_{\\bar{\\theta}}$, also calculated offline. Variational Methods provide a principled framework for approximating unknown probability distributions by leveraging optimization over a family of tractable parameterized distributions (Kingma & Welling, 2022). The fundamental idea of variational methods is to reformulate probabilistic inference as a functional optimization problem. More specifically, the goal is to find a surrogate distribution $q_\\theta(y)$ from a parameterized family $Q = \\{q_\\theta | \\theta \\in \\Theta\\}$, so that $q_\\theta(y)$ can best approximates the target unknown distribution $p(y)$. This is usually achieved by minimizing the KL divergence $KL[q_\\theta(y)||p(y)]$ between $q_\\theta(y)$ and $p(y)$. Mathematically, given the unknown target p(y), variational methods minimize the following KL to find a distribution $q_\\theta(y)$ from a predefined family Q that minimizes the KL divergence:\n$KL(q_\\theta || P) = E_{q_\\theta(y)}[log \\frac{q_\\theta (Y)}{p(Y)}]$"}, {"title": "3. Method", "content": "This objective encourages $q_\\theta(y)$ to concentrate probability mass where p(y) is large. However, directly minimizing Equation (6) is often intractable, as evaluating p(y) requires computing a normalization constant (e.g., a partition function). To bypass this intractability, variational methods maximize the Evidence Lower Bound (ELBO) (Kingma & Welling, 2022), derived by rearranging the log-evidence log p(x):\n$log p(x) = log \\int p(x, y)dy \\\\  \\geq E_{q_\\theta (y)}[log \\frac{p(x, y)}{q_\\theta (y)}] \\triangleq ELBO$\nMaximizing the ELBO is equivalent to minimizing $KL(q_\\theta || p(y|x))$, where $p(y|x) = \\frac{p(x,y)}{p(x)}$. The tightness of the bound depends on how well $q_\\theta (y)$ approximates the true posterior. In our offline policy optimization setting, the target distribution is the optimal policy $\\pi^*(y|x)$ and we seek a parametric policy $\\pi_\\theta(y|x)$ to approximate $\\pi^*$. By minimizing KL($\\pi^* || \\pi_\\theta$), we align $\\pi_\\theta$ with high-reward regions of $\\pi^*$. The ELBO in this context becomes:\n$E_{\\pi_\\theta (y)}[log \\pi_{ref} (y|x) + \\frac{1}{\\beta} r(x,y) - log \\pi_\\theta (y|x)]$.\nVariational methods bridge the gap between tractable optimization and probabilistic inference by learning parametric approximations to complex distributions. In our work, this framework justifies using a learnable policy $\\pi_\\theta(y|x)$ to approximate the optimal policy $\\pi^*$ in a reward-weighted manner, while leveraging offline data to estimate the ELBO efficiently through importance sampling.\n3.1. Motivation Existing preference alignment methods exhibit two fundamental limitations. First, clipping-based approaches like PPO (Schulman et al., 2017) and A-LoL (Baheti et al.) bound the importance ratio $\\pi_\\theta(y|x)/\\pi_{ref}(y|x)$ within the interval $[1-\\epsilon,1+\\epsilon]$. This flattens the reward distinctions between responses with similar values. For instance, when two responses have rewards $R_1 = 100$ and $R_2 = 99$, clipped methods assign nearly identical probabilities (~ 1/2), failing to resolve fine-grained preferences (detailed analysis in Appendix B.2). Second, existing methods that employ negative weights for the to-be-learned policy face intrinsic instability. For example, DPO's treatment of dis-preferred responses and A-LoL's alternative weighted SFT method, where the advantage value $\\hat{A}^{ \\pi_{ref}}$ can be negative, suffer from this issue. When $w(x, y)$ takes negative values for dis-preferred responses, the loss becomes unbounded below. Minimizing the loss corresponds to maximizing $w(x, y) log \\pi_\\theta(y|x)$. For negative weights (w < 0), this reduces to minimizing $log \\pi_\\theta(y|x)$, creating a non-compact optimization landscape. While perfect performance ($log \\pi_\\theta(y|x) \\rightarrow 0$) is theoretically achievable, it is practically unreachable (Gao et al., 2023) (detailed analysis in Appendix B). Our key insight is that reward-driven alignment should operate in the space of positive measures. We therefore propose a variational method that naturally induces non-negative weights through exponential reward transformation:\n$w(x, y) \\propto \\pi_{ref}(y|x) exp(r(x, y)/\\lambda) > 0$.\nThis construction guarantees that the loss landscape has well-defined minima bounded by the reference policy's support. By reformulating RLHF as variational inference over positive measures, we achieve stable optimization without artificial clipping or negative weighting. 3.2. KL Minimization as Variational Inference This closed-form solution motivates our key insight: preference alignment can be reformulated as variational distribution matching. Therefore, we formulate the policy optimization as a variational inference problem. In the variational inference paradigm (Jordan et al., 1999), we approximate a complex target distribution (here $\\pi^*$) by optimizing within a tractable family of distributions (here $\u03c0_\u03b8$). This can be achieved by minimizing the KL divergence between the target and the variational distributions:\n$KL(\\pi^*||\\pi_\\theta) = E_{\\pi^*}[log \\frac{\\pi^*}{\\pi_\\theta(y|x)}]$\n$= H(\\pi^*) -E_{\\pi^*}[log \\pi_\\theta(y|x)]$,\nwhere H(\u00b7) is the entropy function over $\\pi^*$ and is a constant related to $\\pi^*$. For conciseness, we adopt the expectation form of the KL divergence and let $E_{\\pi}$ denote $E_{y\\sim \\pi(y|x)}$. As a result, the approximation of $\\pi^*$ under minimizing KL divergence can be achieved by:\n$min KL(\\pi^*||\\pi_\\theta) = max E_{\\pi^*}[log \\pi_\\theta(y|x)]$. Using importance sampling (Goertzel, 1949; Kahn & Harris, 1951; Kloek & Van Dijk, 1978), which effectively approximates an unknown distribution with a known one, we can rearrange Equation (13) to obtain the following objective by"}, {"title": "3.3. In-Batch Normalization Function Estimation", "content": "Equation (15) implies that the key challenge in effectively approximating $\\pi^*$ through a parameterized model $\u03c0_\u03b8$ lies in the computation of Z(x). However, estimating Z(x) involves summing over all possible outputs y for a given x, which can be computationally expensive, as mentioned in previous work (Rafailov et al., 2024). To avoid directly computing Z(x), some alignment methods adopt policy gradient algorithms (e.g., REINFORCE (Williams, 1992) and PPO (Schulman et al., 2017)) that optimize \u03c0\u03b8 without explicitly normalizing over all outputs. Here, we propose a novel approximation of Z(x) within a mini-batch B, leveraging the insight of \u201ccross-context importance sampling\u201d. Specifically, our method uses pre-collected responses yj associated with inputs xi in the same batch to estimate Z(xi) efficiently, while implicitly suppressing low-reward responses through normalization. Given a batch $B = \\{(x_i, y_i)\\}_{i=1}^{B}$ and a reward model r, we assume each yj can be sampled from a uniform distribution P_{ref}(y) instead of \u03c0_{ref}(y), and thus estimate Z(xi) for each xi by:\n$Z(x_i) = E_{\\pi_{ref}} [exp (\\frac{1}{\\lambda} r(x_i, Y))] = B E_{y \\sim P_{ref}(t)} [\\frac{\\pi_{ref} (y|x_i)}{P_{ref}(y)} exp (\\frac{1}{\\lambda} r(x_i, Y))]$\n$=\\frac{1}{B} \\sum_{j=1}^{B} [\\frac{\\pi_{ref} ((y_j|x_i)}{P_{ref}(y_j)} exp (\\frac{1}{\\lambda} r(x_i, y_j))]  = [\\frac{1}{B} \\sum_{j=1}^{B} [\\pi_{ref} ((y_j|x_i) exp (\\frac{1}{\\lambda} r(x_i, y_j))]$.\n$\\sum_{j=1}^{B} \\pi_{ref}(y_j|x_i) exp (\\frac{1}{\\lambda} r(x_i, Y_j)]$"}, {"title": "3.4. Reward-Driven Weighted SFT", "content": "Combining the above components, we obtain our final objective:\n$L = -E[\\frac{\\pi_{ref}(y|x) exp (r(x, y))}{Z(x)}log \\pi_\\theta(y|x)] = L_{SFT}$"}, {"title": "4. Experiments", "content": "To evaluate the effectiveness of our proposed approach, we conducted experiments under two primary settings: (1) the Helpful and Harmless Assistant Task (HHA) (Bai et al., 2022; Ganguli et al., 2022); and (2) generative benchmarks, including MMLU (Hendrycks et al.), HumanEval (Chen et al., 2021), BigBench-Hard (Srivastava et al., 2023), and GSM8k (Cobbe et al., 2021).\n4.1. HHA Settings Dataset Our primary experiment utilizes the HHA dataset, which consists of user-assistant conversations paired with model-generated responses labeled as \"chosen\" or \"rejected\" based on human preferences. This dataset is divided into four subsets: (1) Harmless-base, containing red-teaming conversations designed to elicit harmful responses; (2) Helpful-base, (3) Helpful-online, and (4) Helpful-rejection, which focus on advice- and assistance-seeking interactions. We evaluate our method using the test sets of these subsets, comprising a total of 8.2K test conversations annotated with human preference labels. For model training\u00b9, we employ the OffsetBias (Park et al., 2024) dataset, a preference dataset similar to HHA. We utilize OffsetBias because our method explicitly relies on reward scores during training. We observed that directly applying the HHA training set and its corresponding reward models often results in inappropriate reward scores, such as instances where the chosen response receives a lower reward score than the rejected response. This issue significantly compromises the effectiveness of our method. In contrast, OffsetBias addresses six common types of biases, including length bias\u2014where models tend to assign higher scores to longer sequences\u2014that can mislead reward models into assigning inaccurate reward scores. By mitigating these biases, OffsetBias provides more robust and reliable reward scores, making it better suited for training our model effectively. For all HHA experiments, we use the full training set of OffsetBias, which consists of 8.5K samples.\n4.2. HHA Results Reward Evaluation Table 1 present the reward scores on the HHA test sets. DPO and VAR denote models trained directly from the Base model (pre-trained only), while SFT+ refers to models first fine-tuned via SFT and then further fine-tuned on the SFT model. From Table 1, we observe that our method outperforms DPO in both Avg. Helpful and Avg. All across all Llama models for the base version, as well as for the SFT+ version, except for Llama3.2-3B, where it shows a marginal decrease of around 0.5% compared to DPO. Additionally, our method achieves comparable results whether trained directly from the base model or the SFT model, particularly for larger LLMs such as Llama3.1-8B and Llama2-13B, whereas DPO struggles to achieve strong results when starting from the base model. Moreover, our method achieves performance comparable to the RLHF objective in a single training step, resembling the simplicity and efficiency of SFT. Table 2 further demonstrates the scalability of our method across different model sizes. Our approach consistently outperforms DPO across all average reward scores on both HHA and OffsetBias, even when starting from the base model. For Qwen2.5-32B, due to limited resources, we employ 4-bit quantization for training. Nevertheless, our method maintains its advantage over DPO."}, {"title": "Winrate Evaluation", "content": "Figure 1 present the win rates evaluated by GPT-40 for answers generated by aligned models compared to the SFT targets (chosen answers in the test set) for the Llama series. The Llama series show results consistent with the reward scores, where our method outperforms DPO across all models except Llama2-13B, which achieves comparable results. These findings further demonstrate that our method can achieve the RLHF objective in a single SFT-like step without the need for resource-intensive reinforcement learning. We provide additional results on Qwen series in Appendix B.3."}, {"title": "4.3. Generative Benchmark", "content": "Following prior settings (Tunstall et al., 2023; Ethayarajh et al., 2024), we utilize UltraFeedback (Cui et al., 2023) as the training dataset. UltraFeedback is a large-scale preference dataset collected from diverse sources, where multiple LLMs generate four distinct responses for each prompt. The dataset comprises 64k prompts, resulting in 256k samples. Additionally, it includes GPT-4-evaluated scores for instruction-following, truthfulness, honesty, and helpfulness. For our experiments, we sampled 10k prompts, selecting the highest average-scored samples for training SFT, OURS, and ALoL, and using the highest-worst score pairs for training DPO. For training, we utilize Llama2-7B and Qwen2.5-7B as the base models. For comparison, we benchmark our method against ALoL and DPO. As for the reward model, we employ OffsetBiasRM for both ALoL and our method, with the same setting in HHA experiments. Table 4 presents the results of different methods on Llama2-7B and Qwen2.5-7B models across four benchmarks: MMLU, GSM8k, HumanEval, and BBH. For Llama2-7B, our method consistently outperforms both DPO and ALoL across most benchmarks, achieving the highest average results. On Qwen2.5-7B, our method also demonstrates strong results, achieving the best performance on multiple benchmarks while maintaining competitive results on others, highlighting the robustness and effectiveness across various settings."}, {"title": "4.4. Ablation Study", "content": "As per Equation (20), we estimate Z(xi) with a mini batch of data, making batch size B crucial for training. We conduct an ablation study using batch sizes 2, 4, and 8 under the settings in Section 4.3, with UltraFeedback as the training set. Table 5 shows the impact of different B values on model performance. The model performs best at B = 8, but the improvement over B = 4 is just 0.14%, suggesting that a larger B can slightly boost performance. Thus, we use B = 8 for all experiments. However, our method is also robust to batch-size changes, which gives satisfactory results even at B = 2, showing stability and suitability for resource-constrained situations."}, {"title": "4.5. Additional Analysis", "content": "We provide case study in Appendix E, from which we can observe our method effectively produce more comprehensive and helpful response with higher quality compared to the baseline methods."}, {"title": "5. Related Work", "content": "Aligning LLMs with human preferences has evolved from studies on RLHF, aiming to achieve human-aligned outcomes (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Lee et al., 2023). The RLHF process typically begins with SFT, followed by further fine-tuning to maximize expected reward scores. This requires the construction of a reward model based on the Maximum Likelihood Estimation (MLE) of the BT model to provide such reward scores. This fine-tuning process is referred to as RLHF, with the PPO algorithm being the most widely applied (Schulman et al., 2017). A series of works focus on self-training, where the workflow involves sampling online data from the model and training it using a two-player min-max game between two policies (Rosset et al., 2024; Swamy et al., 2024; Chen et al., 2024). However, the use of online data in the learning process presents significant challenges, as it requires substantial computational resources and limits training efficiency. To address these challenges, researchers have shifted their focus to offline preference alignment learning algorithms. These methods operate in a single stage and directly optimize a designed loss function to achieve optimal preference alignment based on pairwise datasets (Zhao et al., 2023; Rafailov et al., 2024; Azar et al., 2024; Ethayarajh et al., 2024; Xu et al., 2024). Another approach to alleviate the resource-intensive nature of training is proposed by Remax (Li et al., 2024), which introduces a variance reduction method for LLMs. Most closely related to our work is ALoL (Baheti et al.), which formulates the reinforcement learning process at the sequence level and derives its advantage-based offline objective. Unlike ALoL, which relies on clipping operations to ensure training stability, we formulate our method by directly approaching the optimal solution of RLHF, thereby achieving a more precise solution to the RLHF objective."}, {"title": "6. Conclusion", "content": "In this paper, we proposed a reward-driven variational alignment framework to address the limitations of existing RLHF methods, such as instability from negative weights and suboptimal performance due to clipping. By reformulating RLHF as a variational problem over positive measures, our approach ensures a stable optimization landscape and derives a reward-driven weighted SFT loss through KL divergence minimization. The introduction of an efficient in-batch normalization technique further enables scalable and practical implementation. Experimental results demonstrate improved alignment performance and training stability, offering a robust and effective solution for preference alignment in RLHF."}, {"title": "Impact Statement", "content": "Our work presents a reward-driven variational alignment framework that overcomes key challenges in RLHF, such as instability from negative weights and suboptimal clipping effects. By reformulating RLHF as a variational problem over positive measures, our method offers a stable and efficient optimization framework for aligning language models with human preferences, enhancing their reliability and scalability in applications like conversational AI, content moderation, and personalized recommendations. While our focus is on improving alignment performance and training stability, we recognize the broader societal implications, including risks of bias amplification, misuse of generative capabilities, and ethical concerns around automating human-like decision-making. We emphasize the need for ongoing research to address these challenges and ensure responsible deployment of RLHF advancements."}, {"title": "A. Future Work", "content": "We will explore several promising directions to further enhance our framework. First, we aim to develop an online version of our method, enabling real-time interaction for calculating Z(x) and updating the policy \u03c0\u03b8 dynamically. Second, we plan to conduct extensive experiments across a broader range of tasks, such as multi-turn dialogue and long-form text generation, to validate the generalizability of our approach. Finally, scaling our framework to larger models and testing on more diverse and noisy preference datasets will provide deeper insights into its scalability and robustness."}, {"title": "B. Theoretical Analysis", "content": "B.1. Loss Bound Analysis Lower Bound of Positive Weighted Loss\nTheorem B.1. For any policy \u03c0\u03b8(y|x) satisfying \u03a3\u03c5 \u03c0\u03bf(y|x) = 1 and weights w(x, y) > 0, the weighted SFT loss satisfies:\n$L(0) = -E_{x,y} [w(x, y) log \\pi_\\theta(y|x)] \\geq 0$,\nwith equality if and only if \u03c0\u03bf(y|x) = \u03b4y=y* where \u03b4y=y* is the optimal policy when y* = arg maxy w(x, y). Unboundedness under Negative Weights\nTheorem B.2. If there exists a pair (x0, yo) such that w(x0, Yo) < 0, then the loss L(0) is unbounded below:\n$inf L(0) = -\\infty$.\n\\theta"}, {"title": "B.2. Analysis of Clip Operator and Policy Distinction", "content": "The clip operator:\n$clip(r(\\theta, r_{ref}), 1 - \\epsilon, 1 + \\epsilon)$,\ncommonly used in methods like ALoL to stabilize training, bounds the importance ratio $r(\\theta, r_{ref}) = \\frac{\\pi_\\theta (yi|x)}{\\pi_{ref}(Yi|x)}$ within $[1 - \\epsilon, 1 + \\epsilon]$. While effective in controlling gradient variance, clipping introduces bias by flattening reward distinctions between responses with similar values. For instance, suppose for a given instruction x, we have a set of answers {Y1, Y2, ..., Yn }, and the loss function of R-LoL (R-LoL used here as a simple example; the only difference between A-LoL and R-LoL is replacing r(x, y) in R-LoL to A(x, y) = r(x, y) \u2013 V(x)) with clipping is:\n$L_{R-LoL} = \\sum_{i=1}^{n} (R(x, y_i) \\cdot clip(r(\\theta, r_{ref}), 1 - \\epsilon, 1 + \\epsilon) log \\pi_\\theta(y|x) - \\beta log \\pi_\\theta(y_i|x)) = \\sum_{i=1}^{n} (R(x, y) \\cdot clip(r(\\theta, r_{ref}), 1 - \\epsilon, 1 + \\epsilon) - \\beta) log \\pi_\\theta(y_i|x)$. When e is small, we assume that the parameter 0 in the function r(0, ref) is frozen when we do the update of the policy model \u03c0\u03b8, i.e., the loss function becomes:\n$\\sum_{i=1}^{n} (R(x, y_i) \\cdot clip(r(\\theta_1, r_{ref}), 1 - \\epsilon, 1 + \\epsilon) - \\beta) log \\pi_\\theta(y_i|x)$,\nand we first update 0, then when we do the next iteration we set 01 \u03b8. We denote ni = R(x, yi) \u00b7 clip(r(01, ref), 1 \u2013 \u20ac, 1 + \u20ac) \u2013 \u03b2 and we can see that ni \u2248 R(x, yi) when \u20ac, \u03b2 are small. $\\sum_{i=1}^{n} \\eta_i $\nwe just need to optimize"}, {"title": "D. Consistency Proof of Our Objective with RLHF Optimal Solution", "content": "Proof. Our objective is to minimize:\n$L = -E[\\frac{\\pi_{ref}(y|x) exp (r(x, y))}{Z(x)}log \\pi_\\theta(y|x)]$,\n= F(\u03c0\u03b8 , \u03bb) = -"}, {"title": "E. Case Study", "content": "Table 6 compares responses from SFT, SFT+VAR, SFT+DPO, Base+VAR, and Base+DPO, along with their corresponding reward scores. Notably, the SFT+DPO response is excessively verbose and includes irrelevant details, yet it receives the highest reward score. This highlights a tendency of the reward model to favor longer sequences, even when the content is less helpful or accurate. In contrast, SFT+VAR provides a concise and accurate answer, demonstrating the effectiveness of our method in balancing response quality and length. Additionally, Base+DPO generates a highly structured but overly complex and repetitive response, while Base+VAR delivers a clear and straightforward answer. Importantly, Base+VAR achieves performance comparable to SFT+VAR, showing that our method can effectively align models even when starting from the base (pre-trained only) version, without the need for SFT."}]}