{"title": "EXPLAINREDUCE: Summarising local explanations via proxies", "authors": ["Lauri Sepp\u00e4l\u00e4inen", "Mudong Guo", "Kai Puolam\u00e4ki"], "abstract": "Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small \"proxy set\" of simple models, which can act as a generative global explanation. This reduction procedure, EXPLAINREDUCE, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics.", "sections": [{"title": "1 Introduction", "content": "Explainable artificial intelligence (XAI) aims to elucidate the inner workings of \"closed-box\" machine learning (ML) models: models that are not readily interpretable to humans. As machine learning has found applications in almost all fields, the need for interpretability has likewise led to the use of XAI in medicine [1], manufacturing [2] and atmospheric chemistry [3], among many other domains. In the past two decades, many different XAI methods have been developed to meet the diverse requirements [4]. These methods produce explanations, i.e., distillations of a closed-box model's decision patterns. An ideal XAI method should be model-agnostic - applicable to a wide"}, {"title": null, "content": "range of model types \u2013 and its explanations would be succinct, easily interpretable by the intended user, and stable. Additionally, such explanations would be global, allowing the user to comprehend the entire mechanism of the model. However, producing global explanations is often a challenge. For example, if a model approximates a complex function, describing its behaviour may require describing the complex function itself, thereby defeating the purpose of interpretability.\n A common approach to producing model-agnostic explanations is to relax the requirement for explaining the model globally and instead focus on local behaviour [4]. Assuming a degree of smoothness, approximating the closed-box function in a small neighbourhood is often feasible using simple, interpretable functions, such as sparse linear models, decision trees, or decision rules.\n We argue that these local explanations are inherently unstable. We initially observed this phenomenon with SLISEMAP [5], where we noted that most items could be accurately approximated with several different local models. In the same vein, take two commonly used local explanation methods, LIME [6] and SHAP [7]. Both methods estimate feature importance locally by averaging predictions from the closed-box model for items sampled near the one being explained. Such explanations can be interpreted as linear approximations for the gradient of the closed-box model. It is well known that the loss landscapes of, e.g., deep neural network models, are not smooth. Therefore, it can be conjectured that the predictions from closed-box can be likewise unstable. Indeed, research has shown that local explanations of neural networks can be manipulated due to the geometry of the closed-box function [8]. Furthermore, numerous variants of LIME which aim to increase stability (such as [9, 10]) indicate that instability of LIME is a key issue to address. We hypothesise that this instability is a property of all local explanation methods that use simple models to approximate a complex function. Even in the theoretically ideal case, where a complex model is defined by an exact mathematical formula and the local explanations take the form of gradients, there can be points (e.g., sharp peaks) where the gradient is ill-defined, as in Fig. 1. In practice, especially when working with noisy data and averaging over random samples, the ambiguity is exacerbated; one item may have several nearly equally viable explanations. At the same time, some local explanations may accurately approximate many items. This suggests that if we generate a large set of local explanations, we may be able to find a smaller subset thereof, which could effectively replace the full set without sacrificing accuracy. As an added bonus, while the local explanations may be unstable, this subset of explanations may be more stable, as observed in [3].\n This paper introduces a procedure, coined EXPLAINREDUCE, that can reduce large sets of local explanations to a small subset of so-called proxy models. This small proxy set can be used as a global explanation for the closed-box model, among other possible applications. Fig. 1 provides insight into the process. In the left panel, we show noisy samples (blue dots) from a closed-box function (marked with a solid blue line). The middle panel shows a set of local explanations (as blue and orange dashed lines) produced by the XAI method SLISEMAP [5], with one explanation per data item. The right panel shows a reduction from this large set of local models to two (solid lines), maximising the coverage of items within an error tolerance (shaded area). The procedure offers a trade-off between interpretability, coverage, and fidelity of the"}, {"title": null, "content": "local explanation model by finding a minimal subset of local models that can still approximate the closed-box model for most points with reasonable accuracy.\n We start with an overview of potential applications of local explanation subsets and introduce related research. We then define the problem of reducing a large set of local models to a proxy set as an optimisation problem. We also outline the EXPLAIN REDUCE algorithm and our performance metrics. In the results section, we first show how a small proxy set provides a global explanation for the closed-box model with simulated data and a practical example. Second, we find that a proxy set of modest size attains an adherence to the closed-box model on unseen data comparable to, or even better than, the complete set of explanations. We continue to show that we can find a good proxy set even when starting from a limited set of initial explanations. We then demonstrate how greedy approximation algorithms can efficiently solve the problem of finding the proxy set. The code for the procedure and to recreate each of the experiments presented in this paper can be found at https://github.com/edahelsinki/explainreduce."}, {"title": "2 Applications for reduced local explanation sets", "content": "Global explanations: Producing succinct global explanations for complex closed-box functions remains a challenge. Given a local explanation method, a naive approach would be simply producing a local explanation for each item in a given training set. However, this leaves the user with n local explanations without guaranteeing global interpretability. In practical settings, many of these local explanations are also likely to be similar and thus redundant. Summarising the large set of local explanations with a reduced set of just a few local models would be much more palatable to the user as a global explanation.\n Interpretable replacement for the closed-box model: Assuming that the method used to produce local explanations is faithful, i.e., able to accurately predict the behaviour of the closed-box function locally, the set of all local explanations can replace the closed-box model with little loss to accuracy. However, a large set of (e.g., 500) local models"}, {"title": null, "content": "can hardly be called interpretable. If there is sufficient redundancy in the explanations, selecting a small number of representative models yields a global explanation and provides an interpretable surrogate for the closed-box model.\n Exploratory data analysis and XAI method comparison: Studying how the set of local explanations can be reduced can offer interesting insights about the data and XAI method used. If, for example, different subsets of the data are consistently associated with very similar proxy sets, it implies that the closed-box model can be replaced with a reasonable loss of accuracy by a small set of simple functions. Similarly, comparing the reduced proxy sets with individual explanations from different XAI methods could provide new insights into the explanation generation mechanisms in various settings.\n Outlier detection: Given a novel observation, we can study how well the local models in the reduced set can predict it compared to similar items in the training dataset. If the novel item is better explained by a different model than the one used for other similar items, or if no explanation in the reduced set accurately captures the relationship, the novel item could be considered an outlier."}, {"title": "3 Related work", "content": "Modern machine learning tools have become indispensable in almost all industry and research fields. As these tools find more and more applications, awareness of their limitations has simultaneously spread. Chief among these is the lack of interpretability inherent to many of the most potent ML methods. Understanding why ML models produce a specific prediction instead of something else poses obstacles in their adoption in, e.g., high-trust applications [11]. Such opaque methods are colloquially called black-box or closed-box methods, in contrast with white- or open-box methods, characterised by their interpretability to humans. Examples of closed-box methods include random forests and deep neural networks, whereas, e.g., statistical models like linear regression models are often considered open-box methods. As a result, the field of XAI has grown substantially in the last decade. This section describes several post-hoc local explanation and model aggregation methods to give the reader context for the EXPLAINREDUCE procedure.\n XAI methods generally take a dataset and a closed-box function as inputs and produce an explanation that describes the relationship between the inputs and outputs of the closed-box model. One commonly used approach is to produce post-hoc explanations using local (open-box) surrogate models. In this approach, given a closed-box method f : X \u2192 Y, f(X) = \u0177, a local surrogate model g is another function that replicates the behaviour of the closed-box method in some region of the input space. Mathematically, given a loss measure l and a point in the input space x\u1d62, we might consider models g such that l(f(x), g(x)) \u2264 \u025b \u2200x \u2208 {x \u2208 X | D(x, x\u1d62) < d} as local surrogate models.\n Many methods have been proposed to produce local surrogate models as explanations. Theoretically, the simplest way to produce a local surrogate model would be to calculate the gradient of the closed-box function. This XAI method is often called VANILLAGRAD in the literature. However, in practice, the gradients of machine learning models can be very noisy, as demonstrated by the effectiveness of adversarial"}, {"title": null, "content": "attacks that exploit small perturbations in neural networks [12]. On the other hand, many commonly used machine learning models, such as random forests, do not have well-defined gradients. Hence, more involved approaches are warranted.\n SMOOTHGRAD [13] attempts to solve the gradient noise problem in VANILLAGRAD by averaging over gradients sampled from the vicinity of the point under explanation. Although the original paper only applies SMOOTHGRAD to classification, the method can easily be extended to regression.\n Moving away from directly analysing the gradient, LIME [6] and SHAP [7] are perhaps the most widely known examples of practical local explanation generation methods. In this paper, we focus on the KERNEL-SHAP variant, which combines linear LIME with SHAP. Both LIME and KERNEL-SHAP produce explanations in the form of additive linear models (\u0177 = \u03c6\u1d40x). Given an item x\u1d62, the methods sample novel items x' \u2208 X' in the neighbourhood of the first item, use the closed-box model to predict the labels for the novel items and find the linear model that best fits them [7], \n$$ \u03c6 = arg min_{\u03c6} \\mathbb{E}_{x'\u223c\u03c0_{x\u1d62}} [f(x') \u2013 \u03c6\u1d40x']\u00b2 + \u03a9(\u03c6), $$\\nwhere \u03c0\u2093 represents a distance measure and \u03a9(\u03c6) is a regularisation term. The difference between the methods lies in the choice of distance measure, which defines the notion of neighbourhood for x\u1d62; LIME most often uses either L\u00b2 or cosine distance, whereas KERNEL-SHAP utilizes results in game theory [14].\n In addition to a procedure for generating local explanations, the authors of LIME also propose a method which constructs a global explanation by selecting a set of items whose explanations best capture the global behaviour. They term this procedure submodular pick algorithm. The procedure first generates a local explanation for each item within the dataset. Then, a global feature importance score is calculated as the sum of the square roots of the feature attributions aggregated across all local explanations. If we only pick items according to the presence of the most important features, there is a danger of ending up with many similar explanations. Hence, submodular pick algorithm encourages diversity by framing the problem as a weighted covering problem, balancing feature importance and representativeness such that the user sees a diverse set of explanations.\n SMOOTHGRAD, LIME and SHAP are based on sampling novel items, which, while simple to implement, introduces a unique set of challenges. First, formulating a reliable data generation process or sampling scheme for all possible datasets is difficult, if not impossible [4, 15]. For example, images generated by adding random noise to the pixel values rarely resemble natural images. Second, randomly generating new items might produce items that cannot occur naturally due to, for example, violating the laws of physics. SLISEMAP [5] and its variant, SLIPMAP [16], produce both a low-dimensional embedding for visualization and a local model for all training items without sampling any new points. SLISEMAP finds both the embedding and local models by optimising a loss function consisting of two parts: an embedding term, where items with similar local explanations attract each other while repelling dissimilar ones and a local loss"}, {"title": null, "content": "term for the explanation of each item: \n$$\nmin L\u2081 = \\sum_{i=1}^{n} \\Big[-\\sum_{j=1}^{n} \\frac{exp(-D(z\u1d62, z\u2c7c))}{\\sum_{k=1}^{n} exp(-D(z\u2096, z\u2c7c))} l(g\u1d62(x\u2c7c), y\u2c7c) + \u03a9(g\u1d62) \\Big]\n$$\nwhere D(z\u1d62, z\u2c7c) is the euclidean distance in the embedding, g\u1d62 represents the local model, l is the local loss function, and \u03a9 again denotes regularisation term(s). In SLISEMAP, each item is fitted to its local models; in the SLIPMAP variant, the number of local models is fixed to some p, usually much less than the number of data items n, and the training items are mapped to one of the p local models.\n Another large class of explanations which deserves mention is the so-called case-based or example-based XAI methods, which use representative samples in the training set to explain novel ones [17, 18]. One such method is using prototypes, which present the user with the most similar \"prototype items\" as an explanation, such as showing images of birds with similar plumage as a basis for classification.\n A shared property of post-hoc local surrogate models is the lack of uniqueness; for a given item, many local surrogates may exist with similar performance. The phenomenon is documented for SLISEMAP in [5] and implied for other methods based on the results in [8], as well as for many publications aimed at fixing the inherent instability of LIME [9, 10, 19]. We argue that the existence of such alternative explanations is an inherent feature of using local surrogate models. Intuitively, we can imagine the n-surface of a complex closed-box function and consider local surrogates as planes with dimensionality n - 1. There are many ways to orient a local surrogate on the curved surface of the closed-box function while retaining reasonable local fidelity. Interestingly, the existence of alternative explanations implies that there may be surrogates which perform well for many items in the data distribution. Therefore, we might be able to reduce a large set of local models to a small set of widely applicable surrogates, providing a global explanation of the model's behaviour.\n The method proposed in the previous paragraph falls under model aggregation. The submodular pick algorithm mentioned when discussing LIME is an example of a model aggregation method. Other methods include Global Aggregations of Local Explanations (GALE) [20], GLOCALX [21], and an integer programming-based approach introduced by Li et al. [22]. GALE offers alternative ways to calculate feature importance for the submodular pick algorithm, as the authors argue that the way these importance values are calculated in the original [6] is only applicable to a limited number of scenarios. Furthermore, they show how the choice of the best-performing importance value definition is task-dependent. In GLOCALX [21], the authors propose a method to merge rule-based explanations to find a global explanation. In the programming-based approach ([22]), the authors take a similar approach to the one proposed in this paper. They also attempt to find a representative subset of local models, and formulate model aggregation as an optimisation problem with fidelity and coverage constraints. However, their work has some limitations. First, their method relies on the definition of applicability radii for the local models, i.e., radii within which the explanation holds. Second, the framework only functions in classification tasks. Third, to satisfy the optimisation constraints, the framework requires the inclusion of tens of local models into the aggregation, limiting the interpretability of the"}, {"title": null, "content": "aggregation as a global model. Finally, they only tested their framework with random forest models and two datasets. Because the model aggregation methods described above cannot be directly applied to an arbitrary set of local explanations, in this paper we opt to measure the performance of EXPLAINREDUCE against the full set of local explanations instead."}, {"title": "4 Methods", "content": "In this section, we describe the idea and implementation of EXPLAINREDUCE. Assuming that many items in a dataset can have many alternative explanations of similar performance, a proper subset of explanations can accurately model most of the items in the dataset. We refer to this smaller set as a set of \"proxy models\". Thus, the method combines aspects of local surrogate explanations with prototype items; instead of representative data items, we use representative local surrogate models to summarise global model performance.\n The EXPLAINREDUCE procedure works as follows: after training a closed-box model, we generate a large set of local explanations for the closed-box and then find a covering subset of the local models, which acts as a global explanation. In this section, we first define the problem of finding the subset of local models and then move on to cover the reduction methods and algorithms that generate these proxy sets. We also introduce the quality metrics used to evaluate the performance of reduced sets."}, {"title": "4.1 Problem definition", "content": "A dataset D = {(x\u2081, y\u2081), ..., (x\u2099, y\u2099)} consists n of data items (covariates) x\u1d62 \u2208 X and labels (responses) y\u1d62 \u2208 Y. We use X denote a matrix of n rows such that X\u1d62. = x\u1d62. If we have access to a trained supervised learning algorithm f(x) = \u0177\u1d62, we can instead replace the true labels y\u1d62 with the predictions from the learning algorithm \u0177\u1d62. A local explanation for a data item (x\u1d62, y\u1d62) is a simple model g(x\u1d62) = \u1ef9\u1d62 which locally approximates either the connection between the data items and the labels or the behaviour of the closed-box functions. In the previous section, we gave multiple examples of generating such explanations. Assume we have generated a large set of such local explanations G = {g\u2c7c | j \u2208 [m] = {1, ..., m}} and a mapping from data items to models \u03a6 : X \u2192 [m] using one of these methods. Assume also that these local models can be identified by a set of p real parameters (such as coefficients of linear models), which we will denote with B\u2208 \u211d\u1d50\u00d7\u1d56. We define a loss function l : Y \u00d7 Y \u2192 \u211d>\u2080 and a loss matrix L\u2208 \u211d\u1d50\u00d7\u207f, where individual items are defined as L\u1d62\u2c7c = l(g\u1d62(x\u2c7c), y\u2c7c). A straightforward example of a mapping I would then be chosen from the local models, the one with the lowest loss for each item: \u03a6(x) = arg min\u1d62\u2208[m] l(g\u1d62(x\u2c7c), y\u2c7c).\n Finally, we assume that the large set G contains a reasonable approximation for each item in the dataset D: for all i \u2208 [n] and for a given \u025b \u2208 \u211d>\u2080 there exists j\u2208 [m] such that l(g\u2c7c(x\u1d62), y\u1d62) \u2264 \u025b. This can be achieved by, e.g., learning a local explanation for each item in D.\n We are interested in how many items can be explained by a given set of local surrogates to a satisfactory degree. To measure this, we use coverage C, defined as"}, {"title": null, "content": "the proportion of data items which can be explained sufficiently by at least one local model in a subset S \u2286 [m]. Mathematically, given a loss threshold \u025b \u2208 \u211d>o, the coverage can be calculated as \n$$ C(S, \u03b5) = (1/n) |\\{j \u2208 [n] | min_{i \u2208 S} l(g\u1d62(x\u2c7c), y\u2c7c) \u2264 \u03b5 \\}| . $$\n A c-covering subset of local models S\u2282 [m] is a set for which C(S, \u025b) \u2265 c. \n Next, we define three computational problems to address the task described earlier. Later, we will introduce algorithms to solve each of the problems.\n The first formulation attempts to minimise the number of items for which a satisfactory local explanation is not included in the subset S\ud835\udc50.\n Problem 1. (MAXIMUM COVERAGE) Given k and \u025b \u2208 \u211d>0, find a subset S\ud835\udc50 of cardinality k that maximises coverage, or\n$$S\ud835\udc50 = arg max_{S\u2282 [m] | |S| = k} C(S, \u03b5),$$,\n where we have used [m]\u2096 = {S \u2286 [m] | |S| = k} to denote the subsets of cardinality k.\n The second definition attempts to capture a subset of explanations that can be used as a proxy model with a small average loss.\n Problem 2. (MINIMUM LOSS) Given k, find a subset S\ud835\udc50 with cardinality k such the average loss when picking the lowest loss model from S\ud835\udc50 is minimised, or\n$$S\ud835\udc50 = arg min_{S\u2208 [m]\u2096} (\\frac{1}{n}\\sum_{j=1}^{n} min_{i\u2208S} l(g\u1d62(x\u2c7c), y\u2c7c)). $$\\n The final formulation is a combination of problems 1 and 2.\n Problem 3. (COVERAGE-CONSTRAINED MINIMUM LOSS) Given k, \u025b \u2208 \u211d>0, and minimum coverage c\u2208 (0, 1], find a c-covering S\ud835\udc50 with cardinality k such the average loss when picking the lowest loss model from S\ud835\udc50 is minimised, or\n$$S\ud835\udc50 = arg min_{S\u2208 [m]\u2096,c} (\\frac{1}{n}\\sum_{j=1}^{n} min_{i\u2208S} l(g\u1d62(x\u2c7c), y\u2c7c)),$$\nwhere [m]\u2096,c = {S \u2208 [m]\u2096 | C(S, \u025b) \u2265 c} are the c-coverings of cardinality k."}, {"title": "4.2 General procedure", "content": "The EXPLAINREDUCE algorithm is outlined in Algorithm 1. Given a dataset D, an explanation method Exp that generates a set of m local explanations (a special case being one local explanation for each training data point, in which case m = n) and a reduction algorithm explained later in Sect. 4.3, we first use the explanation method to generate m \u2264 n explanations for m items sampled without replacement from D. If a closed-box function is provided, we replace the original labels in D with predictions from f. We then apply the reduction algorithm to the generated set of local explanations [m] and receive the proxy set S\ud835\udc50\u2286 [m]. Finally, for each sampled data item, we pick the local explanation in the proxy set S\ud835\udc50 with minimal loss, mapping the items and the proxies."}, {"title": "4.3 Reduction algorithms", "content": "In this section, we briefly overview the practical implementations of the reduction algorithms (function reduce in Alg. 1) used to solve the problems outlined in the previous section.\n MAX COVERAGE: Problem 1 is a variant of the NP-complete partial set covering problem, sometimes called MAX k-COVER. The equivalence is obvious if we consider an item (x\u2c7c, y\u2c7c) \u2208 D covered by model g\u1d62 if l(g\u1d62(x\u2c7c), y\u2c7c) \u2264 \u025b. We solve Prob. 1 exactly using integer programming (implemented by the PULP Python library [23]) and approximately using a greedy algorithm. In the greedy approach, given fixed k, we iteratively pick local models g\u2208 G such that the marginal increase in coverage is maximised with each iteration until k models have been chosen. Our problem is submodular, as adding each new model to the subset cannot decrease the coverage. It has been shown that in this case, the greedy algorithm has a guaranteed lower bound to achieve coverage at least 1 \u2013 ((k-1)/k) times the optimal solution [24].\n Notably, this approximation ratio only applies to the original set of data items and their associated local explanations. If we apply the proxy sets to novel data, we should use standard machine learning tools such as a separate validation set to ensure that the model performs appropriately.\n MIN LOSS: Problem 2 is an example of a supermodular minimisation problem. Let f(S) = (\u2211\u2c7c=\u2081\u207f min\u1d62\u2208S l(g\u1d62(x\u2c7c), y\u2c7c)) and A, B : A C B C G be subsets of local models. Additionally, let v be a local model not contained in B. Clearly, the decrease in loss by adding v to the larger set B must be, at most, as great as adding the same model v to A. In other words, \n$$ f(A\u222a \\{v\\}) \u2212 f(A) \u2264 f(B\u222a \\{v\\}) \u2212 f(B) \u2200A C B C G, v \u2209 \u0392, $$\n which is the definition of supermodularity [25].\n Supermodular minimisation problems are known to be NP-hard. Hence, we only use a greedy ascent algorithm to solve Problem 2, as finding an exact solution is"}, {"title": null, "content": "computationally expensive due to the continuous nature of loss. In the worst case, the search for the optimal subset would require (\u1d50) comparisons. Like above, we iteratively pick local models with the best possible decrease in marginal loss until we reach k chosen local models. In the general case, a multiplicative approximation ratio for a supermodular minimisation problem may not exist due to the optimal solution having a value f(S*) = 0, while the greedy algorithm may converge to a solution with non-zero loss. However, in our case, finding a subset of models with exact zero loss for all items is unlikely. In [26], the author derives a curvature-based approximation ratio for a greedy descent (worst out) algorithm. Based on this analysis, authors of [27] derive an approximation ratio for greedy ascent for probabilistic decision trees. Neither of these approaches is directly applicable to our setting, and hence, we cannot give a closed-form approximation ratio. However, as we show in Section 5.4, the empirical approximation ratios for the greedy ascent algorithm are reasonable. Moreover, we show how the greedy approximation performs nearly equally, if not better, on unseen data compared to the exact solution for our datasets.\n CONST MIN LOSS: We solve Problem 3 again with a greedy ascent algorithm, i.e., iteratively picking the best model to include in the subset S\ud835\udc50 until the maximum coverage has been met. The only difference to MIN LOSS algorithm is how the models are scored. If the coverage constraint has been met, we simply follow the procedure in MIN LOSS and select the model with the best possible marginal loss decrease. Otherwise, we divide the marginal loss decrease \u0394l\u1d62 we would get by including model i in the proxy set with the marginal coverage increase of including the same model \u0394c\u1d62. This reduction method can be implemented with a hard constraint by having the algorithm throw an exception if the coverage is not met or with a soft constraint where the resulting proxy set is returned regardless of satisfying the constraint. In this paper, we opt to use the softly constrained variant exclusively. We can see that in the case where the coverage constraint is not met, Problem 3 with this scoring scheme is also a supermodular minimisation problem and equivalent to Problem 2 with a different optimisation objective, namely the coverage-normalised loss. Similar to Problem 2, we cannot give an exact approximation ratio for this algorithm, but the empirical results in Section 5.4 suggest good performance.\n CLUSTERING: Above, we have treated the problem as a set covering problem. We can also approach the problem from the unsupervised clustering perspective: can we cluster the training data to find a proxy set? Given the number of clusters k, we can cluster the training data items X, the local model parameters B, or the training loss matrix L using some clustering algorithm. In this paper, we use K-MEANS with Euclidean distance for X and L, and cosine distance for the local model parameters B. After performing the clustering, we pick the local model closest to the cluster centroid for each cluster to form the proxy set."}, {"title": "4.4 Performance measures", "content": "Fidelity. Fidelity [4] measures the adherence of a surrogate model to the complex closed-box function. Given a closed-box function f, we fidelity is the loss between the"}, {"title": null, "content": "closed-box model prediction \u0177y = f(x) and the surrogate model prediction:\n$$ fidelity = \\frac{1}{n} \\sum_{i=1}^{n} l(g\u1d62(x\u1d62), y\u1d62), $$\nwhere g\u1d62 is the relevant local model either from the full set of local explanations or the proxy model set.\n Instability. Instability [4] (sometimes also referred to as stability, despite lower values denoting better performance) measures how much a slight change in the input changes the explanation. In practice, we model the slight change by measuring the loss of a given local model g\u1d62 associated with item x\u1d62 with its \u03ba nearest neighbours:\n$$ instability = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{\u03ba}\\sum_{j\u2208NN\u03ba(i)}l(g\u1d62(x\u2c7c), y\u2c7c). $$\n This paper uses a fixed number of \u03ba = 5 nearest neighbours when we report instability values (|NN\u2085(i)| = 5)."}, {"title": "5 Experiments", "content": "In this section, we demonstrate the performance of the proxy sets produced via different XAI methods and reduction strategies. We experiment on a variety of different datasets, consisting of both classification and regression tasks. For each of the datasets used, we train a closed-box model and produce the initial set of local explanations using that model. A brief description of the datasets can be found in Table 1, and further details in Appendix A.\n In the experiments, we measure fidelity mostly with test data, i.e., data not used in generating the full set of local explanations. This metric better captures the adherence of the explainer to the closed-box model than calculating the same value on the data used to generate the local explanations.\n The local explanation methods discussed in this paper are generally not generative, that is, there is no obvious mapping from a new covariate x to an explanation model in S\ud835\udc50. Therefore, to use either the full explanation sets or the proxies to produce predictions, we need to generate another mapping between novel items and local models in the explanation set. The simplest of such mappings is to use the distance in the data space: \u03a6new(x) = arg min\u1d62\u2208S\ud835\udc50 ||x\u1d62 \u2013 x||\u209a, i \u2208 (1, ..., n), where ||a||\u209a is the"}, {"title": "5.1 Case studies", "content": "We begin our examination by demonstrating the EXPLAINREDUCE procedure with two case studies to give the reader a better intuition of the procedure and its possible usage."}, {"title": "5.1.1 Synthetic data", "content": "To give a simple example of the procedure, we apply it to a synthetic dataset. The dataset, which is described in detail in Appendix A, is generated by producing k = 4 clusters in the input space and generating a random, different linear model for each cluster. We then generate labels by applying a local model to the data items based on their cluster ID and adding Gaussian noise. The dataset is then randomly split into a training set and a test set, and we train a SMOOTHGRAD explainer on the training data. For reduction, we use a greedy MAX COVERAGE algorithm, where e is defined as the 10th percentile of the loss matrix L.\n In Fig. 2, we show a PCA of the items in the test set on the left, coloured with the ground truth cluster labels, and how the test items are mapped to proxy models on the right. Overall, we can see that most items get mapped to the correct proxy model for that particular cluster. The small impurity in the clusters stems both from the Gaussian noise and the approximative nature of the greedy coverage-maximising algorithm. Furthermore, as Fig. 3 shows, the reduced proxy models (red) correspond well with the ground truth models (blue). The proxy set thus serves well as a generative global explanation for the dataset."}, {"title": "5.1.2 Particle jet classification", "content": "In a previous work [3], we analysed a dataset containing simulated LHC proton-proton collisions using SLISEMAP. These collisions can create either quarks or gluons, which decay into cascades of stable particles called jets. These jets can then be detected, and we can train the classifier to distinguish between jets created by quarks and gluons based on the jet's properties.\n We first trained a random forest model on the data and then applied SLISEMAP to find local explanations. We clustered the SLISEMAP local explanations to 5 clusters and analysed the average models for each cluster, and found them to adhere well to physical theory. When we generate 500 SLISEMAP local explanations on the same dataset and apply the CONST MIN LOSS reduction algorithm with k = 4 proxies, we find the proxy set depicted in Fig. 4. The left panel shows a swarm plot where each item is depicted on a horizontal line based on the random forest-predicted probability of the jet corresponding to a gluon jet and coloured based on which proxy model is associated with the item. The right panel shows the coefficients of the proxy models, which are regularised logistic regression models. We find that the proxies are similar to the cluster mean models shown in the previous publication and show similar adherence to the underlying quantum chromodynamic theory [35]. For example, wider jets (high JETGIRTH and QG_AXIS2) are generally more gluon-like, and therefore these parameters are essential in classifying the jets. Incidentally, proxy 3 (red) is associated with the most quark- and gluon-like jets and has high positive coefficients for both parameters. Similarly, proxy 0 (blue) has negative coefficients for momentum (JETPT) and QG_PTD, which measures the degree to which the total momentum parallels the jet. Both of these features indicate a more quark-like jet."}, {"title": "5.2 Fidelity of proxy sets", "content": "The interpretability of a set of local models as a global explanation is directly related to the size of that set. As explained in a previous section, local explanation mod-els often need to balance interpretability and granularity: a large set of local models"}, {"title": "5.3 Coverage and stability of the proxy sets", "content": "It should be no surprise that methods directly optimising for loss also show the best fidelity results. However, an ideal global explanation method should be able to accurately explain most, if not all, of the items of interest. Additionally, explanations are expected to be locally consistent, meaning that similar data items should generally have similar explanations. In Fig. 7, we show the evolution of training coverage and instability of reduction methods as a function of the proxy set size k with respect to the closed-box predicted labels. The results follow a similar pattern to the previous"}, {"title": "5.4 Performance of greedy algorithms", "content": "In section 4.3, we discussed the performance of greedy approximation algorithms used to solve Problems 1-3. As we have seen in previous sections, the performance of the"}, {"title": "6 Discussion", "content": "Using local surrogate models as explanations is a common approach for model-agnostic XAI methods. In this paper, we have shown that the EXPLAINREDUCE procedure can find a post-hoc global explanation consisting of a small subset of simple models when"}]}