{"title": "Mutual Reinforcement of LLM Dialogue Synthesis and Summarization Capabilities for Few-Shot Dialogue Summarization", "authors": ["Yen-Ju Lu", "Ting-Yao Hu", "Hema Swetha Koppula", "Hadi Pouransari", "Jen-Hao Rick Chang", "Yin Xia", "Xiang Kong", "Qi Zhu", "Simon Wang", "Oncel Tuzel", "Raviteja Vemulapalli"], "abstract": "In this work, we propose Mutual Reinforcing Data Synthesis (MRDS) within LLMs to improve few-shot dialogue summarization task. Unlike prior methods that require external knowledge, we mutually reinforce the LLM's dialogue synthesis and summarization capabilities, allowing them to complement each other during training and enhance overall performances. The dialogue synthesis capability is enhanced by directed preference optimization with preference scoring from summarization capability. The summarization capability is enhanced by the additional high quality dialogue-summary paired data produced by the dialogue synthesis capability. By leveraging the proposed MRDS mechanism, we elicit the internal knowledge of LLM in the format of synthetic data, and use it to augment the few-shot real training dataset. Empirical results demonstrate that our method improves dialogue summarization, achieving a 1.5% increase in ROUGE scores and a 0.3% improvement in BERT scores in few-shot settings. Furthermore, our method attains the highest average scores in human evaluations, surpassing both the pre-trained models and the baselines fine-tuned solely for summarization tasks.", "sections": [{"title": "Introduction", "content": "Dialogue summarization focuses on producing concise and coherent summaries of conversations in various domains such as customer service (Feigenblat et al., 2021; Zhao et al., 2021), medical consultations (Chintagunta et al., 2021; Jain et al., 2022; Zeng et al., 2020), and casual interactions (Chen et al., 2021; Gliwa et al., 2019). While state-of-the-art large language models (LLMs) such as Llama3 (Dubey et al., 2024) have been shown to work well for a wide variety of natural language processing tasks, their dialogue summarization performance is unsatisfactory in many target domains of interest. This could be because, these models, despite being trained on massive datasets, may have seen limited data related to dialogue summarization in specific target domains.\nCollecting a large-scale, real-world dialogue dataset and manually annotating it with concise summaries is not only time-consuming and expensive, but it also raises privacy concerns in many target domains, as conversational data is often sensitive in nature. Consequently, there is a growing interest in developing few-shot learning approaches that improve the dialogue summarization capabilities of pretrained LLMs using limited real dialogue-summary pairs from the target domain.\nExisting works that address data scarcity for dialogue summarization assume access to either additional dialogue-summary pairs from other domains (Li et al., 2023; Park et al., 2024; Yu et al., 2021; Zou et al., 2021; Zhong et al., 2022) or additional unlabeled dialogues from the target domain (Chen and Yang, 2021; He et al., 2024) or teacher models that are larger and more powerful than the target models (Ouyang et al., 2023; Pham et al., 2023). While these approaches have shown some performance improvements, they need access to external resources that may not be available in all scenarios.\nDifferent from these works, we focus on improving the dialogue summarization capability of a pre-trained LLM using limited real dialogue-summary pairs from the target domain without relying on any additional data sources or external models.\nLLMs possess a vast amount of implicit knowledge acquired during pre-training on large-scale text corpora, enabling them to generate contextually relevant text. Motivated by this, we introduce the framework of Mutual Reinforcement via Data Synthesis (MRDS) that harnesses the internal knowledge embedded within these models and their inherent data synthesis capability to address the data scarcity problem in dialogue summarization.\nSpecifically, given a pretrained LLM, our method incorporates dialogue synthesis and summarization into a mutually reinforcing cycle to enhance both capabilities simultaneously (see Fig. 1).\nTo improve the dialogue synthesis capability, we create a dataset of synthetic dialogue preference pairs scored by leveraging the summarization capability of the pretrained LLM, and train a LoRA (Hu et al., 2022) adapter for dialogue synthesis using direct preference optimization (DPO) with the synthetic dialogue preference pairs in addition to supervised finetuning (SFT) with the limited real dialogue-summary pairs. This ensures that the generated dialogues are coherent and closely aligned with their corresponding summaries. To improve the summarization capability, we utilize the trained dialogue synthesis adapter to generate synthetic dialogue-summary pairs, and train a LoRA adapter for summarization by performing SFT with limited real data and generated synthetic data. Effectively, the initial summarization capability of the pretrained LLM helps improve the dialogue synthesis capability which in turn helps further improve the summarization capability.\nMajor contributions:\n\u2022 Mutual reinforcement framework: We introduce a framework that incorporates dialogue synthesis and summarization capabilities of an LLM into a mutually reinforcing cycle, and enhances both capabilities using limited target domain data.\n\u2022 DPO-Enhanced dialogue synthesis: We propose to use direct preference optimization to improve the dialogue synthesis capability of a pretrained LLM, ensuring that the generated dialogues are correctly formatted, coherent, and closely aligned with their summaries.\n\u2022 Experimental validation: We demonstrate consistent improvements over various alternative approaches in terms of BERT and ROUGE scores on two widely-used benchmark datasets, namely SAMSum and Dialog-Sum. The proposed approach also achieves the best average score in human evaluations. We also present several ablation results that demonstrate the effectiveness of individual components of the proposed approach."}, {"title": "Related Work", "content": "Low-resource/Few-shot Dialogue Summarization. Multiple lines of methods have been proposed to address the data sparsity problem of dialogue summarization. Yu et al. (2021) and Zou et al. (2021) employ datasets from multiple source domains to conduct pre-training. He et al. (2024) rely on semi-supervised learning techniques such as pseudo labeling to incorporate the additional dialogue data without summary annotation. Xie et al. (2024) and Zhao et al. (2022) design sophisticated prompt tuning strategies, enabling cross-task knowledge transfer. Recently, several methods have leveraged synthetic data generation using external knowledge or unlabeled datasets. For instance, GENDEX (Park et al., 2024) generates synthetic dialogues by utilizing external knowledge bases, thereby enriching the training data and improving model performance. Similarly, compositional data augmentation proposed by Ouyang et al. (2023) creates new training samples by recombining existing data, enhancing diversity without additional manual annotations. Additionally, Tian et al. (2024) employed a mixture-of-experts framework that integrates external knowledge to enhance summarization capabilities. While these approaches have demonstrated performance gains, they often depend on domain-specific resources, large unlabeled datasets, or external stronger models, which may not be available or practical in few-shot settings. This reliance on external data limits their applicability in scenarios where access to such resources is constrained.\nSynthetic Data from LLMs Many previous works have shown that LLMs are capable of synthesizing high quality training data for machine learning models. One line of methods primarily focus on zero-shot learning scenario (Ye et al., 2022a,b; Gao et al., 2023; Meng et al., 2022; Gupta et al., 2024), where they sample data from LLMs based on task related prompts, and use the synthetic data to train small, task-specific models from scratch. Other works also demonstrate the effectiveness of synthetic data from LLM in different domains such as speech recognition (Su et al., 2024), information extraction (Bonifacio et al., 2022; Josifoski et al., 2023), text-to-SQL (Yang et al., 2024), and dialogue state tracking (Kulkarni et al., 2024; Mehri et al., 2022). Recently, several works also investigate the idea of LLM self-improvement, suggesting that the synthetic data from LLMs can improve their own instruction following abilities. Self-instruct (Wang et al., 2023) samples from an LLM to create a synthetic prompt-response paired dataset, which can be used to finetune the original LLM. Li et al. (2024) introduce instruction backtranslation, which obtains synthetic instruction prompts from back-translating a web scale corpus with the same LLM. Gulcehre et al. (2023), Yuan et al. (2024), and Chen et al. (2024) pay attention to the generation of responses, but utilize them in different manners. Gulcehre et al. (2023) rely on an external scoring function to obtain the reward of synthetic responses. Yuan et al. (2024) propose a self-rewarding framework, using the LLM to score the response generated from itself. Chen et al. (2024) design a self-play mechanism, finetuning the LLM to distinguish the responses generated by the itself and human responses."}, {"title": "Methodology", "content": "Notations Let \\(D_p\\) denote the limited real dataset of dialogue-summary pairs, and \\(S\\) denote the set of summaries in \\(D_p\\).\nGiven a pretrained LLM, we characterize its dialogue synthesis and summarization capabilities using two separate LoRA (Hu et al., 2022) adapters. First, we leverage the summarization capability of the pretrained LLM to score synthetic dialogue preference pairs, and use this data to train the dialogue synthesis adapter using DPO in addition to SFT on real data \\(D_p\\). Then, we leverage the trained dialogue synthesis adapter to generate synthetic dialogue-summary pairs that are used (in addition to real data \\(D_p\\)) to train the summarization adapter. Effectively, the synthesis and summarization capabilities help improve each other without using any external models or additional real data."}, {"title": "Dialogue Synthesizer Training", "content": "The dialogue synthesizer (pretrained LLM with dialogue synthesis adapter) takes a summary s as input and generates a dialogue d. A straightforward way to train this model is to perform SFT with the real dialogue-summary pairs \\(D_p\\). However, when the amount of such training data is limited, the resulting dialogue synthesizer \\(M^{SFT}_{dlg}\\) often generates poor quality dialogues in terms of both dialogue format and content (see Table 8).\nTo encourage the dialogue synthesizer to generate higher quality dialogues, we construct a synthetic dataset of preference pairs \\(\\{s, d_1, d_2 | s \\in S, P(d_1) > P(d_2)\\} \\), where \\(P\\) denotes a dialogue quality scoring function, and train the synthesizer using DPO. Specifically, we generate two types of preference pairs using two different quality scoring functions, one focusing on the dialogue format and the other focusing on the dialogue content (see Fig. 2). Both the preferred and rejected dialogues in these pairs are synthetic dialogues generated using the SFT-trained dialogue synthesizer \\(M^{SFT}_{dlg}\\).\nFormat-based Preference Pairs Since the SFT-trained synthesizer \\(M^{SFT}_{dlg}\\) often generates dialogues with several formatting errors, we develop an iterative dialogue synthesis (IDS) method that generates correctly-formatted dialogues. First, we generate a dialogue conditioned on the input summary and check it for format errors. We discard the portion of the dialogue after the first detected error, concatenate the remaining correctly-formatted partial dialogue to the initial input prompt and give it as input to the dialogue synthesizer to complete the dialogue. This process is repeated until we get a dialogue without formatting errors. Table 11 shows an example run of this IDS process.\nFor a given summary, we generate multiple dialogues with format errors by directly sampling from the SFT-trained synthesizer \\(M^{SFT}_{dlg}\\) and multiple clean dialogues by repeating the above IDS process several times. We use these samples to form preference pairs \\(\\{s, d_1, d_2 | s \\in S, F(d_1) = 1, F(d_2) = 0\\} \\), where \\(F\\) denotes the format check-based binary scoring function (1 for clean dialogues, 0 for those with errors). When trained with these preference pairs, the dialogue synthesizer learns to generate dialogues in correct format. Table 9 shows an example of format-based preference pair.\nContent-based Preference Pairs A dialogue d generated by a well-trained dialogue synthesizer should have high content alignment with the summary s used to generate d. We leverage the summarization capability of the pretrained LLM to measure this alignment. The main motivation is that if we summarize the synthetic dialogue d, the corresponding summary s should have high probability as the summarization output. Based on this, we use the likelihood \\(M_{sum}(s|d)\\) of the summary s conditioned on the dialogue d measured by the pretrained LLM as the alignment score.\nTo encourage the dialogue synthesizer to generate dialogues that have high content alignment with the input summaries, we construct a dataset of preference pairs using the content alignment score \\(M_{sum}(s|d)\\). Specifically, for each summary s, we first generate multiple clean dialogues from the SFT-trained dialogue synthesizer \\(M^{SFT}_{dlg}\\) following the IDS process described above, and then pick the dialogues with the best and least content alignment scores to form preference pairs \\(\\{s, d_1, d_2 | s \\in S, M_{sum}(s|d_1) > M_{sum}(s|d_2) \\}\\). Table 10 shows an example of content-based preference pair.\nInstead of using the pretrained LLM, we also explored training the summarization adapter on the limited real data \\(D_p\\) and using it for alignment scoring to generate DPO training data. However, we did not observe significant improvements in the final summarization results (after training the dialogue synthesizer with DPO, generating synthetic dialogues, and using them to train the final summarization adapter).\nTraining with DPO and SFT In addition to DPO with synthetic preference pairs, we also use SFT with the limited real data \\(D_p\\) to train the dialogue synthesizer. We accumulate gradients from both losses in each optimization step. While DPO with synthetic data encourages the model to generate contextually accurate dialogues in correct format, it does not explicitly encourage the model to generate dialogues that mimic the target distribution represented by real data \\(D_p\\). By combining DPO with SFT on real data, we encourage the model to generate dialogues closer to the target distribution while being contextually accurate and better-formatted."}, {"title": "Summarizer Training", "content": "The summarization model (pretrained LLM with summarization adapter) is trained using SFT with limited real data \\(D_p\\) and additional synthetic dialogue-summary pairs.\nSynthetic Summary Generation To generate synthetic summaries that mimic the distribution represented by the real summary set S, we first extract a (2-3 words) topic for each summary \\(s \\in S\\) using the pretrained LLM. Then, for each topic, we generate multiple new synthetic summaries by using a topic-based summary synthesizer. This synthesizer is obtained by supervised LoRA finetuning of the pretrained LLM using the real summaries in S and the corresponding extracted topics.\nSynthetic Dialogue Generation For each synthetic summary, we generate a dialogue by directly sampling from the dialogue synthesizer that has been trained with both DPO and SFT, as explained in the previous section. Since this improved synthesizer already generates dialogues with high quality, we do not use the costly iterative synthesis approach in this step.\nTraining with Synthetic and Real Data Models trained only on limited real data tend to overfit quickly. While combining real and synthetic data samples in each minibatch could address this issue to some extent, finding the perfect ratio between the two data sources is challenging and highly dependent on the quality of the synthetic data. Moreover, using a fixed ratio of real and synthetic data samples throughout the training may not be optimal. If the minibatch is dominated by synthetic data, then the model may inherit the artifacts present in the synthetic data, and if the minibatch is dominated by real data, the model may start to overfit before taking full advantage of the synthetic data.\nTo address these issues, we follow a two-stage training strategy, where we train only on synthetic data in the first stage and only on real data in the second stage. The synthetic-only first stage allows the model to learn general dialogue summarization skills without the risk of quickly overfitting on limited real data. The real-only second stage allows the model to adapt to the distribution of the real data mitigating the artifacts learned from synthetic data. Following this two-stage approach, we make effective use of both synthetic and real data, resulting in a more accurate summarization model."}, {"title": "Experiments", "content": "Datasets We experiment with two widely-used benchmark datasets, namely SAMSum (Gliwa et al., 2019) and DialogSum (Chen et al., 2021). The SAMSum dataset contains over 16,000 casual conversations mimicking everyday chats among friends and family. The DialogSum dataset includes about 13,460 face-to-face spoken dialogues between friends, colleagues, and between service providers and customers, covering various daily-life topics. These datasets also provide a human-written summary for each dialogue. Since this work focuses on few-shot settings, for each dataset, we experiment with either 100 or 300 dialogue-summary pairs as the few-shot training dataset \\(D_p\\)."}, {"title": "Alternative Methods", "content": "We compare our MRDS method with several dialogue summarization alternative approaches, divided into two categories: methods using the pre-trained Llama3 model without fine-tuning and methods fine-tuned on real or synthetic data."}, {"title": "Pre-trained Methods", "content": "Zero-shot: Zero-shot summarization performance of Llama3.\nICL: Summarization performance of Llama3 using in-context learning with k = 7 examples."}, {"title": "Fine-tuned Methods", "content": "Real only: Fine-tuning with real data only.\nSFT: Two-stage training using synthetic dialogues generated by the SFT dialogue synthesizer.\nSFT + Post-processing: Two-stage training using synthetic dialogues from the SFT dialogue synthesizer, enhanced with Iterative Dialogue Synthesis (IDS) and content alignment filtering."}, {"title": "Implementation Details", "content": "We use Llama3-8B-Instruct (Dubey et al., 2024) as the pretrained base LLM. We use a rank of 16 and an alpha of 32 for all LoRA adapters, and keep the base model parameters frozen while training the LoRA adaptors. Table 6 shows all the prompts used for topic extraction, topic-based summary synthesis, summary-based dialogue synthesis, and dialogue summarization tasks. All presented results are averaged over three runs.\nDialogue Summarization For the baseline model trained exclusively on real data, we optimized the hyperparameters and applied the same settings to all subsequent experiments for consistency. Our training strategy includes a batch size of 10 and a maximum learning rate of \\(2.0 \\times 10^{-4}\\) with a warmup over the first 50 batches. We use the ReduceLROnPlateau scheduler with a patience of 5 and a reduction factor of 0.7. Training is stopped if the loss does not improve for 100 steps. We select the best checkpoint based on the validation loss obtained during the real data training phase.\nIn synthetic data experiments, we employ a two-stage training approach using the same hyperparameters. In the first phase, we train exclusively on synthetic data until the learning rate reduces to \\(2.0 \\times 10^{-5}\\), effectively serving as a pre-training phase. In the second phase, we apply the same training strategy as in the real-only experiments to ensure a fair comparison.\nDialogue Synthesis For the dialogue synthesizer trained with SFT only, we use a learning rate of \\(2.0 \\times 10^{-4}\\) along with the ReduceLROnPlateau scheduler. The batch size and other hyperparameters are the same as those used for dialogue summarization. When training the synthesizer using both SFT and DPO, we start from the SFT checkpoint. In this combined training, we use a batch size of four for DPO and one for SFT, jointly updating the dialogue synthesizer by combining the losses from both objectives. A fixed learning rate of \\(1 \\times 10^{-5}\\) is used during this phase. We validate the synthesizer checkpoints on the official validation set of the dataset, evaluating both format correctness and summarization cross-entropy loss. We select the checkpoint with the lowest summarization CE loss, ensuring at least 85% format correctness. Detailed training hyperparameters are provided in Table 13."}, {"title": "Results", "content": "Dialogue Summarization\nWe conducted experiments on the SAMSum and DialogSum datasets to evaluate the effectiveness of our proposed mutual reinforcing data synthesis (MRDS) method. The results are presented in Table 1, comparing various summarization approaches under 100-shot and 300-shot settings using metrics such as ROUGE-1 (R-1), ROUGE-2 (R-2), ROUGE-L (R-L) (Lin, 2004), and BERTScore (Zhang et al., 2020).\nIn the zero-shot setting, the pre-trained LLM (Zero shot) achieves R-1 scores of 31.3 on SAMSum and 28.2 on DialogSum. In-context learning approach (ICL) improves the R-1 score on SAMSum to 39.5 and DialogSum to 31.4, demonstrating efficiency of ICL in low-resource scenarios.\nWhen fine-tuning with 100 real shots, the real-only method significantly improves performance over zero-shot methods, achieving R-1 scores of 50.9 on SAMSum and 44.0 on DialogSum. Incorporating dialogues from the SFT model maintains similar performance, while post-processing techniques (SFT + Post-processing) further enhance results, increasing R-1 scores to 51.8 on SAMSum and 44.7 on DialogSum, indicating the effectiveness of IDS and content alignment filtering. In the 300-shot setting, all methods benefit from additional training data. The real-only method reaches R-1 scores of 51.1 on SAMSum and 45.2 on DialogSum, with SFT showing incremental gains, and SFT + Post-processing achieving R-1 scores of 52.7 on SAMSum and 46.1 on DialogSum.\nOur proposed MRDS method outperforms all approaches in both 100-shot and 300-shot settings. In the 100-shot setting, MRDS achieves the highest R-1 scores of 52.1 on SAMSum and 45.5 on DialogSum, along with improvements in R-2, R-L, and BERTScore metrics, highlighting its ability to leverage synthesized data effectively. In the 300-shot setting, MRDS continues to deliver the best performance, matching the top R-1 score of 52.7 on SAMSum and setting a new high of 47.0 on DialogSum. The method consistently delivers superior ROUGE and BERTScore values, highlighting its robustness and scalability with increased data. This shows that MRDS not only improves efficiency by eliminating post-processing steps but also significantly boosts summarization performance."}, {"title": "Human Evaluation", "content": "Table 2 presents the results of human evaluations, comparing four groups of summaries: two from the pretrained instructed Llama3 model-zero-shot results (Zero) and in-context learning (ICL)\u2014and two from the fine-tuned Llama3-based summarization model: trained on real data only (Real) and trained with the proposed MRDS approach, which combines real and synthetic summaries and dialogues. Five human evaluators assessed the summaries based on informativeness, faithfulness, fluency, and redundancy, using a scale from zero to two, following the evaluation protocol from (Xie et al., 2024). The average scores across the four metrics are also reported.\nFor the pre-trained models, while the zero-shot model (Zero) achieves the highest score in informativeness (1.95), it scores poorly in redundancy (0.83), indicating a tendency to produce overly long summaries by including too much information undesirable in summarization tasks. The in-context learning model (ICL) shows slight improvements in faithfulness (1.69 vs. 1.50) and redundancy (1.23 vs. 0.83) compared to zero-shot, indicating that it generates more concise and faithful summaries but still inherits some limitations of the pre-trained model. For the fine-tuned models, the Real approach outperforms the pre-trained models in overall average score (1.72 vs. 1.56 for Zero and 1.64 for ICL), demonstrating the benefit of fine-tuning with real data in improving summary quality. However, our proposed MRDS method achieves the highest average score (1.84), outperforming both the real-only and pre-trained models, particularly in faithfulness and redundancy. This suggests that incorporating synthesized data helps the model produce more precise, concise summaries. The results highlight that our MRDS approach significantly enhances summarization quality from the perspective of human evaluators. Example summaries from different approaches are shown in Table 7."}, {"title": "Dialogue Synthesis Efficiency", "content": "We compared the efficiency of the post-processing approach\u2014which includes iterative synthesis and content alignment filtering\u2014with the DPO-based MRDS approach (Table 3). In the 100-shot scenarios, although MRDS still required iterative synthesis due to less consistent format correctness with limited data, it achieved 550 dialogues per hour on SAMSum compared to 63 dialogues per hour with post-processing\u2014an 8.7-fold improvement. In the 300-shot scenarios, MRDS significantly increased throughput, generating 2,900 dialogues per hour on SAMSum versus 37.5 dialogues per hour with post-processing\u2014a 77-fold improvement-by eliminating the need for IDS or content alignment filtering due to enhanced format correctness and summarization alignment."}, {"title": "Model Analysis", "content": "Summaries Effectiveness. To evaluate the impact of different summary types on dialogue synthesis, we experimented with two training strategies: (1) a fixed 1:1 ratio of real to synthesized data and (2) a two-stage training approach, training first on synthesized data and then switching to real data. We tested these strategies using three types of summaries: the same summaries (identical to those in the real data), synthetic summaries generated from our synthesis process in Sec. 3.2, and unseen summaries drawn from additional training data.\nTable 4 presents the results. Under the fixed ratio strategy, using the same summaries did not yield any improvement over the real-only approach, likely due to overfitting to the limited real summaries. Synthetic summaries provided some improvement in the 100-shot setting but introduced artifacts that degraded performance in the 300-shot setting. This suggests that mixing synthetic and real data in a fixed ratio can lead to the model learning undesirable patterns from the synthetic data as the amount of real data increases.\nIn contrast, the two-stage training approach produced better results. Training with synthetic summaries first allowed the model to learn general patterns from the synthesized data before fine-tuning on real data, which improved performance in both the 100-shot and 300-shot settings. Specifically, using synthetic summaries in two-stage training significantly outperformed using the same summaries, mitigating overfitting and enhancing generalization. This highlights the value of synthetic summaries in the more effective two-stage training framework. Additionally, incorporating unseen summaries improved both training strategies, but the two-stage training still provided superior results.\nDialogue Generation and Filtering. To evaluate the benefits of generating and filtering dialogues, we experimented with unseen summaries using the two-stage training strategy. In the first stage, we tested three types of synthetic data: summaries only, summaries with unfiltered dialogues, and approaches involving content alignment filtering or DPO training (MRDS), as shown in Table 5. We found that training with summaries alone improved upon the real-only approach; however, adding dialogues to the training data further enhanced the results. Finally, the models utilizing filtering or DPO outperformed all other methods, demonstrating that including dialogues with filtering and DPO effectively improves the final outcomes."}, {"title": "Conclusion", "content": "We introduce a novel approach that mutually reinforces dialogue synthesis and summarization capabilities of a large language model (LLM) to improve few-shot dialogue summarization without relying on external data. By leveraging the dialogue synthesis capability enhanced by DPO, we synthesize well-formatted, coherent dialogues to augment the few-shot real dataset. Furthermore, the two-stage training strategy effectively incorporated synthesized dialogues without introducing artifacts, improving summarization accuracy. Empirical results demonstrated significant improvements: a 1.5% increase in ROUGE scores, a 0.3% improvement in BERT scores. Human evaluations confirmed that our method outperforms the real-only baseline and, in certain aspects, surpasses human-annotated ground truth summaries. Our approach offers a practical solution for real-world applications with limited data, utilizing the model's inherent capabilities without external resources. Future work could extend this self-alignment framework to other NLP tasks affected by data scarcity and explore its integration with larger or more diverse LLM architectures."}, {"title": "Limitations", "content": "Comparing to the baseline method, training summarization adapters on few shot real data only, our method requires additional computation cost for data synthesis adapter training and sampling. Also, our method makes an assumption that part of the internal knowledge of LLM is useful for the target domain, which might be incorrect for a highly specialized target domain."}, {"title": "Ethics Statement", "content": "Our research introduces Mutual Reinforcing Data Synthesis (MRDS) within LLMs to enhance few-shot dialogue summarization tasks. While advancing natural language processing, we acknowledge ethical considerations associated with our methodology. By leveraging synthetic data generated by LLMs, we reduce reliance on large-scale real-world datasets that may contain sensitive or personally identifiable information (PII). We strive to protect user privacy and adhere to data protection regulations by using publicly available datasets (CC BY-NC-ND 4.0 and CC BY-NC-SA 4.0) and implementing data anonymization techniques.\nOur method utilizes LLMs pre-trained on vast corpora that might contain biases and stereotypes. Although we train our synthesis and summarization models on public datasets with daily conversations, we recognize that biases may persist. We encourage future work to identify and reduce biases in synthetic data generation and in models trained on such data. We are committed to transparency. All experimental details\u2014including data preprocessing, model configurations, and evaluation metrics are thoroughly documented to ensure reproducibility and allow critical assessment by the research community. By openly sharing our methods and findings, we aim to foster collaboration and uphold ethical standards in AI development."}, {"title": "Prompt Templates", "content": "Table 6 shows the prompt templates we use in this work for different purposes."}, {"title": "Examples of Summarization Result", "content": "Table 7 presents an example dialogue along with summaries produced by various methods, including the human-labeled ground truth summary, the output from the zero-shot pre-trained model, summaries generated using in-context learning, and those fine-tuned with real data only as well as with the MRDS approach."}, {"title": "Synthetic Dialogue from SFT Trained Synthesizer", "content": "To show the limited capability of dialogue synthesizer trained on few shot real data, we provide an example of low quality synthetic dialogue in Table 8."}, {"title": "Preference Pair Examples", "content": "Tables 10 and 9 provide examples of content-based and format-based preference pairs, respectively. In Table 10, the summarization model scores the content alignment of the dialogues using \\(M_{sum}(s|d)\\), selecting the best and worst ones as the preferred and rejected dialogues. In Table 9, the preferred dialogues are synthesized using IDS, where \\(F(d) = 1\\), while the rejected dialogues are generated through the SFT synthesizer's one-shot synthesis, where \\(F(d) = 0\\)."}, {"title": "Data Formatting and Anonymization", "content": "To simplify the training of the data synthesizer, we perform anonymization preprocessing on both summaries and dialogues. Specifically, we extract the number of speakers and their names from the dialogues as metadata and replace the names in both dialogues and summaries with uniform identifiers (e.g., \"#1\", \"#2\")."}, {"title": "Data Formatting", "content": "To ensure that the synthetic data adheres to the correct format, we have established a set of formatting rules:\nSpeaker Identity: each sentence in the dialogues must begin with a speaker identifier followed by a colon (e.g., \"#1:\", \"#2:\").\nConsistency of Names: Synthetic dialogues and summaries should not contain incorrect or extra names\u2014for example, \"#4\" in dialogue-summary pairs with fewer than four speakers, or a \"#\" symbol without a number.\nInclusion of Speaker Names: Synthetic summaries must contain at least one anonymized speaker name.\nFor summary synthesis, we discard any summaries that do not comply with these formatting rules. For dialogue synthesis, we apply Iterative Dialogue Synthesis (IDS) to ensure that the synthetic dialogues conform to the correct format."}, {"title": "Iterative Dialogue Synthesis", "content": "Table 11 illustrates an example of iterative dialogue synthesis, showing partial inputs given to the synthesizer and the corresponding synthesized outputs. During the synthesis process, we check for formatting issues as described in Sec. A.5.1. If an error is found, the flawed sentence, along with all subsequent sentences, is discarded, and the correctly formatted segment (e.g., \"#1:\", \"#2:\") is used to re-synthesize the dialogue."}, {"title": "Synthetic Dialogue Recovery", "content": "After data synthesis, we restore the anonymized dialogues and summaries before training the summarization model by replacing placeholders with real names using metadata extracted during the anonymization process. For synthetic summaries that lack original metadata, we seed the synthesis process with metadata from the real data in the few-shot training set, then replace the placeholders accordingly. Alternatively, we can generate random names and speaker numbers to populate the placeholders."}, {"title": "Human Evaluation Details", "content": "We hired five machine learning researchers, and received their consent to report the results from their annotation work. We adopt the human evaluation method proposed in the previous work (Xie et al., 2024). For completeness, we describe the 4 metrics (informativeness, faithfulness, fluency and redundancy) and the corresponding instructions in Table 12."}, {"title": "Informativeness", "content": "Whether the critical information in the dialogue is missed in the summary:\n*0: lots of the critical information in the dialogue is missed;\n*1: a small amount of the critical information in the dialogue is missed;\n*2: no critical information in the dialogue is missed."}, {"title": "Faithfulness", "content": "Whether the information presented in the summary is factually incorrect or unmentioned according to the dialogue:\n*0: lots of the information presented in the summary is factually incorrect or unmentioned;\n*1: a small amount of the information presented in the summary is factually incorrect or unmentioned;\n*2: no information presented in the summary is factually incorrect or unmentioned."}, {"title": "Fluency", "content": "Whether the sentences in the summary are ungrammatical or ill-formed:\n*0: lots of the sentences in the summary are ungrammatical or ill-formed;\n*1: a small amount of the sentences in the summary are ungrammatical or ill-formed;\n*2: no sentence in the summary is ungrammatical or ill-formed."}, {"title": "Redundancy"}]}