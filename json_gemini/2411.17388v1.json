{"title": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?", "authors": ["Haoyu Huang", "Chong Chen", "Conghui He", "Yang Li", "Jiawei Jiang", "Wentao Zhang"], "abstract": "In real-world scenarios, most of the data obtained from information retrieval (IR) system is unstructured. Converting natural language sentences into structured Knowledge Graphs (KGs) remains a critical challenge. The quality of constructed KGs may also impact the performance of some KG-dependent domains like GraphRAG systems and recommendation systems. Recently, Large Language Models (LLMs) have demonstrated impressive capabilities in addressing a wide range of natural language processing tasks. However, there are still challenges when utilizing LLMs to address the task of generating structured KGs. And we have identified three limitations with respect to existing KG construction methods. (1) There is a large amount of information and excessive noise in real-world documents, which could result in extracting messy information. (2)Native LLMs struggle to effectively extract accuracy knowl-edge from some domain-specific documents. (3)Hallucinations phenomenon cannot be overlooked when utilizing LLMs directly as an unsupervised method for constructing KGs.\nIn this paper, we propose GraphJudger, a knowledge graph construction framework to address the aforementioned chal-lenges. We introduce three innovative modules in our method, which are entity-centric iterative text denoising, knowledge aware instruction tuning and graph judgement, respectively. We seek to utilize the capacity of LLMs to function as a graph judger, a capability superior to their role only as a predictor for KG construction problems. Experiments conducted on two general text-graph pair datasets and one domain-specific text-graph pair dataset show superior performances compared to baseline methods. The code of our proposed method is available at https://github.com/hhy-huang/GraphJudger.", "sections": [{"title": "I. INTRODUCTION", "content": "The transition from non-structured text to structured Knowledge Graphs (KGs) is a pivotal step in the evolution of data management and information retrieval systems. The task of knowledge graph construction aims to develop a structured representation of knowledge from various data sources without the need for manual intervention. Knowledge Graphs, which serve as the backbone of numerous data science applications, including GraphRAG systems [1] [2] and recommendation systems [3] [4], are becoming increasingly central to how we process and understand vast amounts of information. Exploring a way to construct high quality Knowledge Graphs (KGs) from unstructured data is crucial because it enables the conversion of raw, disorganized text into a structured, machine-readable format. This transformation allows for easy querying, analysis, and integration of the data into different KG-based downstream applications [5] [6] [7] [8].\nRecently Large Language Models (LLMs) have demonstrated significant generalization capabilities in various zero-shot or few-shot scenarios. And LLMs also have shown excellent performances in various natural language processing (NLP) tasks [9] and knowledge graph related tasks, such as text generation [10], knowledge graph completion [11] and information extraction [12]. Consequently, there are a lot of works utilizing the strong zero-shot generation ability of LLMS (e.g., ChatGPT-3.5\u00b9, PaLM2\u00b2) to construct KGs or semantical graphs from unstructured natural language documents. The incorporation of LLMs can address the issue of generalization in open-domain applications [13]. With its robust zero-shot generation capability, there is no need for us to gather a large volume of annotated data for tasks like named entity recognition (NER), entity extraction or relation extraction. And many studies conduct the In-Context Learning (ICL) approach [14] to guide LLMs how to extract structured triples from natural language documents.\nNevertheless, LLMs may encounter some problems during the KG extraction. Asking the LLM for generation extraction results just once may lead to missing information. To address this issue, a strategy involving iterative prompting is suggested by some work [15] [13] to overcome this limitation. For example, in the GraphRAG system [1], the KG construction strategy involves repeatedly querying the LLM after the initial generation round until the LLM determines that the correct answer has been produced.\nAlthough these recent LLM-based methods have gained some success in the KG construction task, we find that they may still suffer from three challenges.\n(1) Noise Information. Real-world documents are not only voluminous but also rife with noise, which poses a significant challenge for LLMs extracting valuable structured informa-tion. The sheer volume of data can lead to the extraction of excessive and irrelevant information, overshadowing the critical insights that LLMs are meant to uncover [16] [17]. For example, as shown in Fig. 1, the triple <Dr. Johnson, attend conference in, Summer Vacation>is incorrectly constructed for"}, {"title": "(2) Domain-Specific Knowledge", "content": "Native LLMs often struggle with the nuances of domain-specific documents, which require a deep understanding of specialized terminology and context [18] [19]. The lack of consideration for connecting global and local views results in an incomplete extraction of knowledge, as LLMs may not fully grasp the intricate relationships and hierarchies inherent in domain-specific data. For example in Fig. 1, the triple <Protein X, cured, Patient>is inaccurately extracted due to a lack of medical domain-specific knowledge. While there is a related reference marked with blue lines in the document, it is important to clarify that the original text merely indicates a connection between Protein X and improved patient outcomes, rather than asserting that Protein X can cure patients in medical domains."}, {"title": "(3) Hallucinations of LLMs", "content": "When LLMs are used directly as an unsupervised method for constructing KGs, they are prone to generating false or distorted information, which is a phenomenon known as hallucinations [20] [21]. Especially when using the LLM to generate the relations between entities, hallucinations phenomenon could be more severe. This can lead to the incorporation of inaccurate or fabricated facts into the KG, undermining the reliability and integrity of the knowledge base. For instance, as shown in Fig. 1, the triple marked in green <AI Technology, replaces, Chemotherapy>is incorrectly generated without any reference in the original document, even in the entity-related text highlighted with a green line.\nTo this end, we propose a new method called GraphJudger, which utilizes a fine-tuned open-source LLM (e.g., LLaMA2 [22], ChatGLM [23]) as an expert to judge the correctness of the triples generated by another closed-source LLM (e.g., ChatGPT-3.5). In detail, to address the first challenge, we introduce the Entity-Centric Iterative Text Denoising module. In this module, we propose an iterative approach to clean up the original documents by eliminating redundant words and irrelevant information not pertinent to the entities identified by the LLM. To overcome the second challenge, we suggest the module of Knowledge Aware Instruction Tuning. We are inspired by some recent researches that fine-tune LLMs or PLMs (e.g., BERT [24]) to address the task of Knowledge Graph Completion (KGC) [11] [25] [26]. Tuning LLMs for knowledge graph completion tasks can significantly improve their performances in enhancing knowledge graphs. We con-sider that utilizing the fine-tuned LLM for link prediction tasks on the extracted entities may be affected not only by the hallucination phenomenon of LLMs but also by the time-consuming process involving all entities. So here we introduce graph judgement task from the triple classification task, which is one of the KGC tasks. The fine-tuned expert LLM only need to clarify the correctness of triples generated by the closed-source LLM. And by conducting instruction tuning on the graph judgement task, the LLM can enhance its grasp of domain-specific knowledge and general logic principles within structured triples to improve judgment accuracy. To settle the third challenge, the module of Graph Judgement is introduced. To guarantee the comprehensiveness of the triples we extracted, in the first module we will let closed-source LLM generate relations between entities as many as possible, which may exist plenty of wrong items because of LLM hallucinations and lack of domain-specific knowledge. So in this module, we utilize the fine-tuned expert LLM to conduct judgement on these generated triples and filter out the wrong items to finally improve the quality of generated KGs.\nIn summary, the main contributions made in this work are as follows.\n\u2022 Addressing challenges such as information noise, domain knowledge gaps and hallucinations in LLMs represents a critical step towards improving the quality of constructed KGs with real-world documents. To the best of our knowledge, we are the first to leverage both open- and closed-source LLMs to tackle these problems."}, {"title": "II. RELATED WORK", "content": "We first introduce recent LLM-based knowledge graph completion methods, from which we come up with the graph judgement task to enhance knowledge graph constructions in our work. Knowledge Graph Completion (KGC) involves pre-dicting missing information within a knowledge graph, such as head entities, relations, or tail entities and predicting the cor-rectness of triples. The recent incorporation of LLMs allows KGC methods to either encode text or produce facts, enhanc-ing the overall performance of KGC [9]. KG-BERT [25] utilize the encoder-only pre-trained language model (PLM) to encode prior knowledge and contextual information. Next, they predict the likelihood of the triples or masked entities by inputting the encoded representation into a prediction head. SimKGC [27] introduces the negative sampling and contrastive learning methods to improve the representations of triples. KG-S2S [28] propose a general sequence-to-sequence framework to unify the representation of KG facts into natural language text. AutoKG [19] adopts prompt engineering to design tailored prompts for different KGC tasks with closed-source LLMs. KG-LLaMA [11] also views triples in knowledge graphs as text sequences and creates various instruction templates to fine-tune the open-source LLMs for different KGC problems. LLaMA-MLP [26] captures the context-aware hidden states of knowledge triples from LLMs and then they train a MLP to predict the correctness of triples with hidden representations."}, {"title": "B. LLM-based Knowledge Graph Construction", "content": "In this section, we will introduce the recent LLM-based information extraction (IE) and knowledge graph construction methods. Some work [29] [30] have demonstrated that LLMs has remarkable zero and few-shot information extraction abil-ities. They propose that closed-source LLMs like ChatGPT excel at information extraction tasks and simpler IE tasks such as entity typing. However, they encounter difficulties when it comes to more intricate tasks such as relation extraction and event extraction [13]. To address that, Kumar et al. [31] pro-pose a unified approach to construct knowledge graphs from unprocessed text, incorporating a framework that includes two PLMs driven components. They initially fine-tuned a PLM for named entity recognition, enabling it to identify entities within raw text. Subsequently, they introduced a \"2-model BERT\u201d architecture to address the challenge of relation extraction. Grapher [32] presents an end-to-end multi-stage system. The approach initially employs LLMs to generate knowledge graph entities, subsequently utilizing a straightforward relation con-struction head to facilitate the effective creation of knowledge graphs from textual descriptions. GPT-RE [33] introduces the in-context learning method and task-aware representations in demonstration retrieval and aims to enhance the connections between examples and triples. PiVe [15] designs a paradigm that fine-tuning a pre-trained language model (PLM) as the verified model to predict the missing triples. With the iterative verifications, the graph-based generative capability of LLMs can be improved. VicunaNER [34] utilizes the open-source LLM Vicuna to do zero-shot or few-shot named entity recog-nition (NER). Similarly, it also performs re-recognition to identify entities that were not recognized in the previous phase. Carta et al. [13] develops an iterative LLM prompting-based pipeline for generating KGs without requiring predefined sets or external ontologies. iText2KG [35] proposes a zero-shot method to construct consistent KGs from documents with LLMs. It restructures the unprocessed documents using a preset template and identifies distinct entities and connections in a semantic manner. SAC-KG [36] exploit LLMs as skilled automatic constructors for domain KGs and employ a native LLM to predict the correctness of constructed triples."}, {"title": "III. PRELIMINARY AND DEFINITION", "content": "In this section, we will first formulate the problem of the knowledge graph construction and introduce the definitions we may use throughout the paper. Then we detail the definition of the graph judgement task proposed by us."}, {"title": "A. Definition of Knowledge Graph Construction Task", "content": "Here we define the task of knowledge graph construction as a problem of how to extract entities & and relations R from a real world document D, which is also called the text-to-graph generation (T2G) task. And the constructed knowledge graph, which can also be represented as the semantic graph, is defined as G = {(h,r,t)|h \u2208 E,r \u2208 R, t \u2208 E}, where & is the entity set and R is the relation set of the graph G. In other words, each semantic graph G has a corresponding original text D. Our goal is to get a better knowledge graph G from a document D.\nWe also define a set of graphs SG = {G0,G1, \u2026, GN-1} and a set of documents SD = {D0, D1, ..,DN-1}. In our implementation we have a set of graph-text pairs Sp = {P0, P1, .., PN-1}, where P\u2081 = {(Gi, Di)|Gi \u2208 SG,Di E SD}. And N = |Sp| is the number of graph-text pairs."}, {"title": "B. Definition of Graph Judgement Task", "content": "Moreover, we introduce the task of graph judgement to classify each triple in generated graphs is correct or not.\nHere we define the knowledge graph we constructed from a corresponding document is G and S\u0109 represents the set of graphs we constructed. And \u2191 in (1) represents the triples we need to conduct judgements on. Our goal in graph judgement task is to predict the label of each triple in \u00ce, represented as \u0177\u2208 {0,1}|\u2191|.\n$\\GES$\n\\begin{equation}\\tag{1}\\Upsilon = \\bigcup \\{(\\text{h,r,t})\\vert(\\text{h,r,t}) \\in \\hat{G}\\} \\end{equation}"}, {"title": "IV. METHODOLOGY", "content": "In this section, we will first give an overview of our pro-posed method GraphJudger. It includes three key modules, which are Entity-Centric Iterative Text Denoising, Knowl-edge Aware Instruction Tuning and Graph Judgement. Next, we will provide a detailed explanation of each module respectively."}, {"title": "A. Overview", "content": "As shown in Fig. 2, the proposed model GraphJudger consists of three primary modules. In the first module, which is Entity-Centric Iterative Text Denoising, we use a two phrases paradigm to extract the entities and relations sepa-rately following results described in [37]. In the phrase of entity extraction, we introduce an iterative method to generate entities with the denoised document. And in the phrase of relation extraction, we generate relations with the entities and the denoised document in the final round as many as possible. Then, in the module of Knowledge Aware Instruction Tun-ing, we conduct instruction tuning to let large language model (LLM) become an expert of graph judgement by deepening their comprehension of both general logic principles and domain-specific information contained in knowledge graph"}, {"title": "B. Entity-Centric Iterative Text Denoising", "content": "In this module, a two-phrase extraction paradigm is de-signed to extract the entities and relations respectively. Phrase 1 is iterative denoising and entity extraction. In phrase 2, we do the relation extraction and then we obtain the draft knowledge graph. And Fig. 3 is an overview of this module. In both of the two phrases we utilize closed-source LLMs (e.g., GPT-3.5) to do the extraction and denoising, because of their impressive zero-shot generalization capabilities. To better assist native LLMs in denoising documents and extracting entities and relations, we also utilize a few-shot learning method with the designed structured prompts."}, {"title": "1) Iterative denoising and entity extraction", "content": "In phrase 1, we consider that a substantial portion of real-world documents retrieved from information retrieval systems are consist of considerable noise information. And that may influence the quality of relations extracted by LLM [38] [39]. So we design an iterative denoising method to remove messy information in the original text. During each iteration round, entities are also extracted from the document that has been cleaned up in the preceding round.\nSpecifically, in the initial iteration, we extract entities from the original document using LLM. Subsequently, we input these entities and the original document into LLM to generate the denoised document. Then in each iteration after that, we feed into LLM with the document denoised in the previous iteration to extract entities with improved clarity. Similarly, we still need to perform further noise reduction with refined entities in each iteration following the initial one.\nFinally, for each raw document D we will get the extracted entity set & and the denoised document D*."}, {"title": "2) Relation Extraction", "content": "In phrase 2, we aim to create relationships using the denoised document D* and the entity set & obtained in phrase 1 by utilizing LLM to establish as many connections as feasible like shown in (2). And we design the prompt template incorporating the few-shot learning strategy to steer the LLM in generating triples with given documents and entities as shown in Fig. 4. We create numerous relationships between entities to ensure a sufficient number of suitable candidate triples for filtering with LLM judgment in the Graph Judgement module. Next we can construct a draft knowledge graph G* for each original document D, as illustrated in (3), where R* is the draft relation set we generate.\n$\\text{R}^{\\*}=\\text{LLM}(\\hat{E},D^{\\*})$\n$\\text{G}^{\\*}=\\{(\\text{h,r,t})\\vert\\text{h}\\in \\hat{E},\\text{r}\\in \\text{R}^{\\*},\\text{t}\\in \\hat{E}\\}$"}, {"title": "C. Knowledge Aware Instruction Tuning", "content": "In this module, inspired by [11], we propose the method of treating triples in the draft knowledge graph G* as textual sequences and model graph judgement task as a sequence-to-sequence problem. We construct a set of instruction data with the training set of each dataset and fine-tune an open source LLM to achieve the goal of both understanding of general logic in triples and acknowledgement of any domain-specific information present in the dataset. In other words, the general logic in triples usually means the structure of triples in knowledge graph is often analogous to a grammatical subject, predicate, and object or a subject with a relational attribute. LLMs are anticipated to have the ability to identify triples that deviate from this general logical structure. This enables our model to enhance its judgement and uphold the integrity of the knowledge representations it generates. The domain-specific knowledge refers to the necessity of creating knowledge graphs from documents in a new domain in real-world situations [18]. This knowledge is typically not part of the pre-training data of LLMs. By employing our informed instruc-tional tuning, we can incorporate domain-specific knowledge from the document into the fine-tuned LLMs, thus enhancing their performance on the graph judgment task.\nBefore we conduct the instruction tuning on LLM, we need to construct instructions for the graph judgement task from graph match dataset. Because we need to ensure that the LLM not only learns which triples are correct but also understands which triples are incorrect during the inference process, here we employ negative sampling when constructing instruction data. In detail, we first construct the positive triple set T+ from the knowledge graphs in training set as described in (4), where SGtrain is the set of all graphs in the training set.\n$\\text{T}^{+}=\\bigcup_{\\text{G} \\in \\text{SG}\\_{\\text{train}}}\\{(\\text{h,r,t}^{+})\\vert(\\text{h,r,t}^{+})\\in \\text{G}\\}$\nSimilarly, we construct negative triple set T from the knowledge graphs in training set as described in (5), where E represents the entity set of the current graph G. And (h, r,t+) are the positive triples of the current graph G. (h, r, t\u00af) are the negative triples of the current graph G, where t\u00af is a negative entity. In other words, we conduct the negative sampling by replacing the positive tail entity t+ with a randomly selected tail entity t in each positive triple.\n$\\text{T}^{-}=\\bigcup_{\\text{G} \\in \\text{SG}\\_{\\text{train}}}\\{(\\text{h,r,t}^{-})\\vert(\\text{h,r,t}^{+})\\in \\text{G},\\text{t}^{\\text{neg}}\\in \\text{E}\\_{\\text{G}}\\\\backslash\\{\\text{t}^{+}\\}\\}$\nThen we merge the positive triple set T+ and negative triple set T\u00af constructed from knowledge graphs SGtrain and we can obtain all the triples Ttrain we use for constructing instructions.\n$\\text{T}\\_{\\text{train}}=\\text{T}^{+}\\cup \\text{T}^{-}$\nFurthermore, we transfer the triples in Ttrain to natu-ral language sentences to construct the instruction data as Fig. 4 shows. We concatenate the head entity, relation and tail entity to format a sentence, which representing a real fact or a fake fact. And let the LLM to make judgements with these instructions. Mathematically, with the tokenized sentences XTtrain transferred from triples Ttrain and tokenized instruction X1, for a sequence of length L, we compute the probability of generating the target output X as (7) shows, where Xt \u2208 XTtrain. And 0 are the learnable parameters within the open-source LLM to be fine-tuned.\n$\\text{p}(\\text{X}\\_{\\text{o}}\\vert\\text{X}\\_{\\text{t}},\\text{X}\\_{\\text{I}})=\\prod\\limits_{\\text{i}=1}^{\\text{L}}\\text{p}\\_{\\theta}(\\text{x}\\_{\\text{i}}\\vert\\text{x}\\_{\\text{t}},\\text{x}\\_{\\text{I}},<\\text{i},\\text{x}\\_{\\text{o}},<\\text{i})$"}, {"title": "D. Graph Judgement", "content": "The knowledge graphs created in the initial module are preliminary and that is also why we call that draft KGs. In this module, we will evaluate the triples in these draft KGs using our fine-tuned LLM in the second module.\nIn detail, we let LLM do the graph judgement task on the constructed draft KGs like the instruction tuning process. LLM needs to assess the correctness of each triple in draft KGs by considering whether it aligns with both general logical principles and avoids conflicting with domain-specific knowledge. Based on the judgments made by LLM, we filter the triples in draft KGs to obtain high-quality triples, which form the final KGs we seek. Mathematically, we can make judgements to anticipate the label of each triple in each draft KG G*. Here we define the draft KG set as S\u00e7*, and the triples in all draft KGs can be symbolized as (8).\n$\\text{T}^{\\*}=\\bigcup_{\\text{G}^{\\*} \\in \\text{SG}^{\\*}}\\{(\\text{h,r,t})\\vert(\\text{h,r,t}) \\in \\text{G}^{\\*}\\}$\nThen we obtain the predictions of all the triples in T* with the parameters @ learned in the instruction tuning procedure as shown in (9). And Pred(\u00b7) is a function that transforms the outputs of LLM into the predicted results \u0177 \u2208 {0,1}|T*|. Using the prediction results, we can identify and exclude the incorrect triples from T*, resulting in a refined triple set T, as described in (10).\n$\\hat{y}=\\text{Pred}(\\text{p}\\_{\\theta}(\\text{X}\\_{\\text{T}^{\\*}}))$\n$\\hat{T}=\\{(\\text{h,r,t}) \\in T^{\\*}\\vert\\hat{y}(\\text{h,r,t})=1\\}$\nSimilarly, the draft relation set R* can also be filtered by T. And we can represent the refined relation set as R. Lastly, for each draft KG G* \u2208 SG* we can get the refined KG \u0177 that we desire as shown in (12). And \u0177(h,r,t) is the predicted result of a triple (h, r, t). The implementation details of the graph judgment procedure are demonstrated in Algorithm 1.\n$\\hat{R}=\\{r\\vert(\\text{h,r,t}) \\in \\text{G}^{\\*},\\hat{y}(\\text{h,r,t})=1\\}$"}, {"title": "V. EXPERIMENTS", "content": "In this section, we will conduct experiments to address the following key research questions:\n\u2022 RQ1: How well does our proposed method GraphJudger perform on both general knowledge data and domain-specific knowledge data?\n\u2022 RQ2: How do the different key components in our proposed method GraphJudger contribute to its overall performance?\n\u2022 RQ3: How about the generalization capability of Graph-Judger when applied across different datasets?"}, {"title": "A. Experimental Settings", "content": "In our study, we conduct experiments two general datasets and one domain-sepcific dataset. And we also analyse the normalized distributions of the number of triplets and the length of documents in each dataset as shown in Fig. 5, which means we calculate the percentages of knowl-edge graphs with different numbers of triples and different lengths of original documents in each dataset.\n\u2022 REBEL-Sub. REBEL [40] dataset comes from Wikipedia text before the table of contents, as well as Wikidata for the triplets annotation.\n\u2022 GenWiki. GenWiki [41] is an extensive dataset sourced from general Wikipedia, comprising 1.3 million non-parallel texts and graphics that share content.\n\u2022 SCIERC. SCIERC [42] is a scientific domain-specific dataset comprises annotations for scientific entities, their re-lations, and coreference clusters within 500 scientific ab-stracts."}, {"title": "2) Baselines", "content": "In our performance comparison, we consider two methods for comprehensive evaluation. The first model is gpt-3.5-turbo that employs a one-shot in-context learning approach, which is also used in [33]. The second model is PiVe [15] one of the latest methods for the KG construction task."}, {"title": "Large Language Model", "content": "The LLMs we employed in this research are different in different modules. In the Entity-Centric Iterative Text Denoising module, we utilize the closed-source LLM ChatGPT(gpt-3.5-turbo) to extract entities and relations from documents and remove noisy information from the documents with extracted entities. And in the few-shot learning approach, we present the LLM with two examples for each task to help it complete the task successfully. In the Knowledge Aware Instruction Tuning module, the open-source LLM LLaMA2-7B [22] is used as our base model to perform the Supervised Fine-Tuning(SFT). In this way, we can make full use of the great zero-shot or few-shot capabilities of closed-source LLM to generate diverse answers, which are candidate triples in our task. And we can also take advantages of the accessibility to conduct instruction tunings of open-source LLM and make it as a judger to clarify the answers generated by closed-source LLM."}, {"title": "Instruction Tuning", "content": "We utilize LLaMA2-7B as the base model to conduct instruction tuning with LoRA [46]. The instructions are constructed with the same query sentence and the sentences transferred from positive triples and negative triples. And we conduct tuning on autoregression generation tasks, which is a common approach of fine-tuning LLMs [47]. And the expected responses (labels) are either \"Yes, that is true.\" or \"No, that is not true.\". For training detail, we follow the parameter settings in Table. II referring to the tuning process used for triple classification tasks in the KG-LLM [11]."}, {"title": "4) Evaluation Metrics", "content": "We acknowledge that certain con-ventional evaluation techniques are rule-based. These methods assess the resemblance between predictions and ground-truth graphs through strict string matching, potentially overlooking semantic similarities. Therefore, in order to evaluate the qual-ity of the produced graphs against the ground-truth graphs, we utilize one semantic-level and two soft string matching automated evaluation metrics:"}, {"title": "\u2022 G-BERTScore (G-BS)", "content": "Here we use a matching metric that evaluate the degree of similarity between the ground-truth and predicted graphs, which is called G-BERTScore [48]. And it is designed as an extension of the text gen-eration metric BERTScore [49]. In G-BERTScore, each triple within knowledge graphs is treated as a sentence, and subsequently, the similarity score between sentences of triples in the ground-truth and predicted knowledge graphs is computed. And we compute the accuracy and F1 score of each constructed KG against the ground-truth using G-BERTScore, denoted as G-BS-Acc and G-BS-F1, respectively."}, {"title": "\u2022 G-BLEU (G-BL)", "content": "BLEU (Bilingual Evaluation Under-study) [50] is a metric for evaluating the quality of text which has been machine-translated from one natu-ral language to another. Here we use this approach to determine the resemblance between the triple sentences in the ground-truth and predicted KGs, which is called G-BLEU. The formulas are shown in (13), (14), (15), (16), (17). N is the maximum order of n-grams considered in the evaluation and we set N = 4, which is a default number in the Python package. BP is the brevity penalty, which is used to avoid giving too much credit to short translations. And we compute the accuracy and F1 score of each constructed KG against the ground-truth using G-BLEU, denoted as G-BL-Acc and G-BL-F1, respectively.\n$\\text{G}-\\text{BLEU} = \\text{BP} \\times (\\prod\\limits_{\\text{n}=1}^{\\text{N}} \\text{w}\\_{\\text{n}})^{\\frac{1}{N}}$\n$\\text{BP} = \\begin{cases}1 & \\text{if } \\text{X} > \\text{X}\\_{\\text{t}}\\\\e^{(1-\\frac{\\text{X}}{\\text{X}\\_{\\text{t}}} )} & \\text{if } \\text{X} \\le \\text{X}\\_{\\text{t}}\\end{cases}$"}, {"title": "B. Overall Performance Comparison (RQ1)", "content": "We demonstrate the evaluation results of our method Graph-Judger with ChatGPT (gpt-3.5-turbo) and another baseline method PiVe [15] on the test set of two general-knowledge datasets and one domain-specific dataset in Table III. It shows that our model outperforms than the baseline methods in three datasets. And the quality of constructed knowledge graphs consistently improves as more rounds of entity-centric iterative text denoising are performed. And it suggests that GrapJudger excels not only with domain-specific data but also demon-strates superior performance with general data. What is kind of werid is that the performance of PiVe baseline is even worse compared with the gpt-3.5-turbo baseline on our datasets. Actually it is reasonable because extracting knowledge graph triples from the documents with much noise information is a hard task. The study conducted in PiVe utilized datasets with less noise compared to ours in the task of knowledge graph construction. So even though PiVe may improve the performance of recall with the help of its verifier module, the performances of accuracy are still bad. In other words, in our datasets, which closely resemble real-world documents, the verifier module in PiVe may produce incorrect triples, resulting in lower accuracy. On the contrary, we focus on addressing the issue of low accuracy because the impressive zero-shot generation capability of LLMs is already an excellent tool to ensure a good recall performance. The comparison results show that our proposed method outperforms others baselines on three datasets that closely resemble real-world scenarios.\nAdditionally, in our experiments on domain-specific dataset SCIERC, we observe that as iterations increases, the F1 score decreases while accuracy improved. This indicates that closed-source LLMs tend to inaccurately process domain-specific documents, inadvertently removing valuable textual informa-"}, {"title": "C. Module Ablation Study (RQ2)", "content": "We perform an ablation study to explore the specific impacts of various sub-modules within our proposed framework, and the results are reported in Table IV. The findings are outlined below:\nEffect of Entity-Centric Iterative Text Denoising. In our study, we investigate the benefit of introducing entity-centric denoising paradigm using the variant \"w/o ECTD\u201d. In this variant, we do not conduct document denoising with the iterative extracted entities and we directly extract the entities and relations from original documents. Our study results show that our model performs significantly better than the base model that does not include an iterative document denois-ing procedure. It suggests that our entity-centric document denoising module can avoid LLMs extract wrong structured information from irrelevance corpus in the original documents. Furthermore, in order to showcase the noise reduction capa-bility of this module, we visualize the semantic correlation of the triples in a known knowledge graph with the enhanced document and the original document, respectively. In detail, we sample a text-graph pair from the REBEL-Sub dataset. Then we split the original and refined document into the same number of chunks, which we set 20 here. And we use a PLM BERT (bert-base-uncased) to process these chunks and choose the [CLS] embedding as the semantic representation of each chunk. For triples in the corresponding KG, we transform the structured triples into natural language sentences like what we did before. Then, similarly, we get the semantic representations of these triple sentences. And we calculate the cosine similarities between these document chunks and triple sentences, as shown in Fig. 6. Deeper colors in the heat maps suggest a stronger relevance between the specific triple and document chunk. The left part of the figure shows that there are a lot of text chunks that have little relevance to the triples we want. As depicted in Fig. 6, the refined document exhibits a greater relevance to the triples in the ground-truth knowledge graph compared to the original document.\nEffect of Knowledge Aware Instruction Tuning. We also conduct graph judgement on the triples without fine-tuning the open-source LLM. This variant is denoted as \"w/o KAIT\u201d. In detail, we directly use a native LLM to clarify the correctness of each triple we constructed. The result in Table IV and Table III indicates that without knowledge aware instruction tuning, the native LLM has a week ability to do the graph judgement tasks. And with a fine-tuned LLM as the graph judger, the performance can be improved a lot compared with"}, {"title": "D. Generalization Capabilities of GraphJudger (RQ3)", "content": "For better demonstrates the generalization abilities of GraphJuder, we conduct experiment in cross-dataset scenar-ios. We conduct instruction tuning on GenWiki dataset and then evaluate the performance of the fine-tuned LLM with another general-knowledge dataset REBEL-Sub. As shown in Table. V, our methods can still show superior performances than baseline methods, which indicates great capabilities of generalization in different datasets. This is because there are some general logic of triple sentences for LLMs to learn in the graph judgement tasks. Moreover, the findings indicate that the GraphJudger model"}]}