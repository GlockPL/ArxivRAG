{"title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference", "authors": ["Bingzhe Zhao", "Ke Cheng", "Aomufei Yuan", "Yuxuan Tian", "Ruiguang Zhong", "Chengchen Hu", "Tong Yang", "Lian Yu"], "abstract": "KV cache techniques in Transformer models\naim to reduce redundant computations at the ex-\npense of substantially increased memory usage,\nmaking KV cache compression an important\nand popular research topic. Recently, state-\nof-the-art KV cache compression methods im-\nplement imbalanced, per-head allocation algo-\nrithms that dynamically adjust the KV cache\nbudget for each attention head, achieving ex-\ncellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced\ncompression leads to significant load imbal-\nance when deploying multi-GPU inference, as\nsome GPUs become overburdened while others\nremain underutilized. In this paper, we pro-\npose FairKV, a method designed to ensure fair\nmemory usage among attention heads in sys-\ntems employing imbalanced KV cache com-\npression. The core technique of FairKV is\nFair-Copying, which replicates a small sub-\nset of memory-intensive attention heads across\nGPUs using data parallelism to mitigate load\nimbalance. Theoretical analysis provides in-\nsights, while experiments on popular models,\nincluding LLaMA 70b and Mistral 24b model,\ndemonstrate that FairKV increases throughput\nby 1.66x compared to standard tensor paral-\nlelism inference. Our code will be released as\nopen source upon acceptance.", "sections": [{"title": "1 Introduction", "content": "Large-scale Transformer-based models are at the\ncore of modern artificial intelligence. To support\nfast inference, these models rely on a key-value\n(KV) cache that stores key and value embed-\ndings from previously generated tokens, trading\nmemory usage for reduced redundant computation.\nThis cache prevents redundant computations and\nis crucial for efficient sequence generation. How-\never, the huge memory usage of the KV cache\nbegins"}, {"title": "1.1 Background and Motivation", "content": "Large-scale Transformer-based models are at the\ncore of modern artificial intelligence. To support\nfast inference, these models rely on a key-value\n(KV) cache(Vaswani et al., 2017; Dai et al., 2019;\nRae et al., 2019) that stores key and value embed-\ndings from previously generated tokens, trading\nmemory usage for reduced redundant computation.\nThis cache prevents redundant computations and\nis crucial for efficient sequence generation. How-\never, the huge memory usage of the KV cache\nbegins"}, {"title": "1.2 Our solution", "content": "To address the unfair head load problem, we pro-\npose FairKV. Our approach targets the fair mem-\nory usage among attention heads in imbalanced\nKV cache compression systems. FairKV mainly\nemploys two techniques to address this issue: best-\neffort assignment and fair-copying. Best-effort As-\nsignment is responsible for distributing attention\nheads across GPUs to achieve a relatively balanced\nworkload. Fair-Copying involves replicating cer-\ntain attention heads to participate in the assignment,\nthereby reducing the workload of the replicated\nheads.\nTechnique I: Best-effort Assignment. Best-effort\nAssignment works as follows. We first analyze the\nKV cache consumption of each attention head un-\nder imbalanced compression. Based on this analy-\nsis, our allocation algorithm assigns attention heads\nto GPUs such that the aggregate memory and com-\nputational load is balanced across GPUs in a best-\neffort manner. This method requires no additional\noverhead and can be integrated into existing multi-\nGPU inference systems easily, offering a practical\nimprovement in load balancing without modifying\nthe underlying KV cache compression scheme.\nTechnique II: Fair-Copying. While best-effort\nassignment offers a straightforward solution, some\nattention heads remain significantly more memory-\nintensive, leading to persistent bottlenecks. To\nfurther improve load balance, we propose another\ntechnique called Fair-Copying, which utilizes Data\nParallel techniques to enhance the effectiveness of\nthe assignment. In this technique, we set a repli-\ncation budget that allows the algorithm to attempt\nreplicating attention heads. By utilizing data par-\nallelism to reduce the load on the replicated heads,\nthese replicated heads participate in the assignment\nalongside the original ones, thereby expanding the\nsearch space and enabling finer-grained partition-\ning. This replication method minimizes additional\noverhead while substantially reducing GPU idle\ntime and inference latency.\nKey Contributions. This paper makes the follow-\ning key contributions:\n1) To the best of our knowledge, we are the first\nto reveal the unfair head load problem: imbal-\nanced per-head KV cache compression causes\nsignificant load imbalance in multi-GPU in-\nference environments.\n2) To address the unfair head load problem,\nwe propose FairKV, which includes two"}, {"title": "2 Related Work", "content": "Inference efficiency for large language models is\ncritically limited by both memory bandwidth and\ncomputational power. KV cache compression (Ge\net al., 2024; Zhang et al., 2024; Yang et al., 2024),\na widely adopted optimization technique, reduces\nmemory for storing previous key-value states in\nattention layers. It can generally be classified into\ntwo categories: Balanced (Fair) Per-Head Com-\npression and Imbalanced (Unfair) Per-Head Com-\npression.\nBalanced (Fair) Per-Head Compression meth-\nods applies the same strategy to all attention heads.\nFor instance, StreamingLLM (Xiao et al., 2024)\nretains the initial k sink tokens along with the re-\ncent window. H2O (Zhang et al., 2024) further\nprioritizes important cache entries based on accu-\nmulated attention scores, while SnapKV (Li et al.,\n2024) selects entries using attention scores from\nthe observation window. Recently, Pyramid (Yang\net al., 2024; Cai et al., 2024) recognize distinct at-\ntention distribution patterns across different layers.\nHowever, these methods disregard the varying im-\nportance of different heads in actual computations.\nIn contrast, Imbalanced (Unfair) Per-Head Com-\npression algorithms, like Ada-SnapKV (Feng et al.,\n2024) and HeadKV(Fu et al., 2024), dynamically\nadjust the KV cache budget per attention head\nbased on the current layer's computational and\nmemory requirements. Ada-SnapKV determines\nthe budget for each head during inference, with a\nfully dynamic allocation. HeadKV, however, pre-\nallocates a fixed base budget for each head accord-\ning to its importance and then adds a dynamic bud-\nget. This tailored cache allocation offers more flex-\nibility and optimization potential. Table 3 in the ap-\npendix show that Ada-SnapKV outperforms other\nmethods on multiple tasks in the LongBench v1\n(Bai et al., 2024), demonstrating the effectiveness\nof Imbalanced (Unfair) Per-Head Compression ap-\nproach."}, {"title": "3 Preliminary", "content": "We examined per-head KV cache eviction pat-\nterns across multiple datasets using different mod-\nels (LLaMA-3.3-70B-Instruct, Meta-LLaMA-3-\n8B, and Mistral-Small-24B-Instruct) and several\nsubsets from LongBench v1 for our experiments\nand set the KV budget for the attention heads to\n128, 256, 512, and 1024 as our basic experimental\nsetup."}, {"title": "3.1 Observation of KV Cache Selection", "content": "We examined per-head KV cache eviction pat-\nterns across multiple datasets using different mod-\nels (LLaMA-3.3-70B-Instruct, Meta-LLaMA-3-\n8B, and Mistral-Small-24B-Instruct) and several\nsubsets from LongBench v1 for our experiments\nand set the KV budget for the attention heads to\n128, 256, 512, and 1024 as our basic experimental\nsetup.\nThe per-head KV cache compression algorithm\nresults in varying KV cache budgets across differ-\nent attention heads, leading to imbalanced work-\nloads when combined with tensor parallelism. The\nresults, summarized in Table 2, show a clear trend:\nDecreasing GPU Utilization with Larger TP\nSizes: For instance, in LLaMA-3.3-70B-Instruct\nwith KV cache budget 128, GPU utilization drops\nfrom 92.5% (TP=2) to 81.6% (TP=4) and further\nto 64.7% (TP=8). A similar trend is observed\nin Meta-LLaMA-3-8B, where GPU utilization de-\nclines from 92.1% (TP=2) to 84.4% (TP=4) and\n70.8% (TP=8) under the same KV cache budget.\nImpact of KV cache Budget: Lower KV cache\nbudgets generally yield better GPU utilization. For\nexample, in Mistral-Small-24B-Instruct with TP=4,\nGPU utilization is 86.3% at KV=128 but decreases\nto 82.9% at KV=256 and further to 82.1% at\nKV=1024. Similarly, in LLaMA-3.3-70B-Instruct\nwith TP=4, GPU utilization drops from 81.6%\n(KV=128) to 71.2% (KV=1024). This suggests that\nlarger KV cache budgets lead to greater workload\nimbalance among attention heads, which negatively\nimpacts GPU efficiency.\nAdditionally, as shown in Table 1, we use co-\nsine similarity to quantify the difference in KV\ncache allocation between a subset and the remain-\ning datasets. Finally, we averaged the metrics\nacross all subsets to obtain an overall indicator.\nOur analysis indicates that eviction patterns remain\nlargely consistent across datasets, confirming that\nthe statistics for retained KV cache can guide op-\ntimization. Furthermore, the allocation pattern of\nthe KV cache budget is influenced by the specific\nmodel used. Given this dataset-invariant nature,\noptimization strategies can be designed based on\nthese profiles without requiring per-dataset recali-"}, {"title": "3.2 Empirical Performance Analysis", "content": "To optimize the distribution of GPU workload, we\nbuilt an empirical model mapping batch size, re-\ntained KV cache count, and inference latency. Mea-\nsurements were taken across various configurations\non a multi-GPU setup.\nWe conducted experiments on the LongBench v1\ndataset by systematically varying both batch size\nand KV cache budget parameters. Specifically, we\ncontrolled these variables independently to eval-\nuate their individual and combined effects. We\nsimulated the inference scenario of LLaMA 70b\non a single layer and obtained the latency during\ndecoding one token. The experimental results, as il-\nlustrated in Figure 1, demonstrate the performance\ncharacteristics under different configurations.\nFor the batch size latency model (Figure a), we\nobserve that latency (L) increases approximately\nlinearly with batch size (B) across different budget\nconfigurations. The relationship can be expressed\nas \\(L \\approx aB+\u00df\\), where a represents the slope and B\nthe initial offset. The graph shows four distinct bud-\nget levels (128, 256, 512, and 1024), with higher\nbudget values corresponding to steeper slopes in\nthe linear relationship. Similarly, the KV cache la-\ntency model (Figure b) demonstrates a comparable\nlinear pattern, where latency (L) increases propor-\ntionally with the KV cache budget (C). This can be\nrepresented as \\(L \\approx \u04afC + d\\), where y and 8 are the\nslope and offset parameters respectively. The data\npresents five different batch sizes (32, 64, 128, 256,\nand 512), with larger batch sizes showing more"}, {"title": "4 Design of FairKV", "content": "During inference with tensor parallelism, per-head\nKV cache compression algorithms can lead to im-\nbalanced GPU workloads, which in turn reduces\ninference efficiency.To address this issue, we de-\nsigned FairKV, a load-aware static approach that\nuses a search algorithm to reassemble and rearrange\nattention heads across layers, thereby balancing the\nload among GPUs. Moreover, FairKV leverages\nfair-copying mechanism to expand the search space\nvia replication of attention heads and DataParallel\ntechniques, enabling a more fine-grained balancing\nof GPU loads and enhancing GPU utilization dur-\ning inference. Figure 2 visually illustrates the core\nconcept of FairKV.\nThe FairKV algorithm requires predefining the\nmodel and the KV cache budget to be used. It\nthen samples a dataset to analyze the proportion\nof KV cache budget allocated to attention heads\nacross different layers for that model, summarizing\nthe findings into a statistical profile. Based on this\ndata, we first replicate attention heads to expand\nthe search space. Next, we perform a constrained\nsearch to determine the optimal attention head ar-\nrangement. Finally, the model weights are loaded\naccording to this arrangement for inference."}, {"title": "4.1 Overview", "content": "During inference with tensor parallelism, per-head\nKV cache compression algorithms can lead to im-\nbalanced GPU workloads, which in turn reduces\ninference efficiency.To address this issue, we de-\nsigned FairKV, a load-aware static approach that\nuses a search algorithm to reassemble and rearrange\nattention heads across layers, thereby balancing the\nload among GPUs. Moreover, FairKV leverages\nfair-copying mechanism to expand the search space\nvia replication of attention heads and DataParallel\ntechniques, enabling a more fine-grained balancing\nof GPU loads and enhancing GPU utilization dur-\ning inference. Figure 2 visually illustrates the core\nconcept of FairKV.\nThe FairKV algorithm requires predefining the\nmodel and the KV cache budget to be used. It\nthen samples a dataset to analyze the proportion\nof KV cache budget allocated to attention heads\nacross different layers for that model, summarizing\nthe findings into a statistical profile. Based on this\ndata, we first replicate attention heads to expand\nthe search space. Next, we perform a constrained\nsearch to determine the optimal attention head ar-\nrangement. Finally, the model weights are loaded\naccording to this arrangement for inference."}, {"title": "4.2 Search Space", "content": "Fair-copying leverages Data Parallel techniques to\nexpand the search space by replicating some re-\ndundant heads, with the goal of achieving more\neffective search outcomes. Experiments were con-\nducted with selective head replication, and the re-\nsults demonstrated improved latency distribution\nacross GPUs. Allowing for limited redundancy\nprovides an effective way to enhance parallel effi-\nciency without excessive computational overhead."}, {"title": "4.3 Mathematical Model for Hybrid Parallelism in FairKV", "content": "This section presents a comprehensive mathemati-\ncal formulation of the FairKV system for optimiz-\ning multi-GPU inference in large language models\nwith KV cache compression."}, {"title": "4.3.1 System Parameters", "content": "The system parameters are defined to model the\ncomponents involved in the FairKV strategy, for\neach transformer layer \\(l \\in L\\):\n\u2022 \\(H_l = \\{h_1, ..., h_n\\}\\): attention heads\n\u2022 \\(G = \\{g_1, ..., g_m\\}\\): available GPUs\n\u2022 \\(w_i\\): workload for head \\(h_i\\)"}, {"title": "4.3.2 Decision Variables", "content": "The decision variables are crucial for determining\nthe optimal assignment of attention heads to GPUs:\n\\(x_{ij} = \\begin{cases}\nr_{ij} & \\text{if head } h_i \\text{ is assigned to GPU } g_j \\\\\n0 & \\text{otherwise}\n\\end{cases}\\)                                                 (1)"}, {"title": "4.3.3 Parallelism Constraints", "content": "The hybrid parallelism approach in FairKV com-\nbines Tensor Parallelism and Data Parallelism to ad-\ndress the challenges posed by head-wise KV Cache\ncompression. Tensor Parallelism ensures that the\ncomputational workload is distributed across multi-\nple GPUs by partitioning the attention heads, while\nData Parallelism introduces redundancy by allow-\ning selective replication of attention heads across\nGPUs. This hybrid approach aims to balance the\ncomputational load, ensuring that no single GPU\nbecomes a bottleneck due to uneven KV Cache\ncompression rates.\nHead Distribution Each head must have at least\none GPU assignment to ensure that all computa-"}, {"title": "4.3.4 Optimization Objective", "content": "The primary goal of the FairKV strategy is to min-\nimize the maximum processing time across all\nGPUs. This is formulated as:\n\\[\\min \\max_{j \\in G} \\sum_{l \\in L} \\sum_{i \\in H_l} \\frac{x_{ij} w_i}{r_{ij}}\\]                                                        (4)"}, {"title": "4.3.5 System Efficiency", "content": "System efficiency is calculated to evaluate how well\nthe GPUs are utilized. The formula for efficiency\nis:\n\\[E = \\frac{1}{|G|} \\sum_{j \\in G} \\frac{\\sum_{l \\in L} \\sum_{i \\in H_l} \\frac{x_{ij} w_i}{r_{ij}}}{\\max_{k \\in G} \\sum_{l \\in L} \\sum_{i \\in H_l} \\frac{x_{ik} w_i}{r_{ik}}}.\\]                                                       (5)"}, {"title": "4.4 Optimizer Design", "content": "We introduce a recursive backtracking algorithm\nthat systematically explores possible head distribu-\ntions while ensuring workload balance.\nThis approach systematically searches for the\nbest head distribution by iterating over all valid\npartitions. By ensuring balanced allocation, the al-\ngorithm minimizes inference latency across GPUs,\nleading to improved system efficiency."}, {"title": "4.5 Key Innovations", "content": "Workload-aware Redistribution A static yet opti-\nmized head allocation strategy balances GPU work-\nloads efficiently.\nHybrid Parallelism Selective head replication\ncombines tensor and data parallelism.\nLatency-driven Optimization Backtracking min-\nimizes inference latency, improving GPU utiliza-\ntion."}, {"title": "5 Evaluation", "content": "In this section, we primarily conduct a detailed\nevaluation of the FairKV method proposed in the\nprevious sections, assessing whether FairKV can\neffectively improve inference efficiency in hybrid\nparallelism approaches during Per-Head KV cache\nCompression."}, {"title": "5.1 Experimental Setup", "content": "We used Python 3.10.16 as the programming lan-\nguage and implemented FairKV on PyTorch. We\nleveraged AdaKV from KVPress as the per-head\nKV cache compression algorithm and tested our ap-\nproach using Llama models (Touvron et al., 2023)\nof different sizes as well as the Mistral model (Jiang\net al., 2023).\nModel and Hardware Configurations We used\nthe LLaMA-3.3-70B-Instruct model, the Meta-\nLLaMA-3-8B model, and the Mistral-Small-24B-\nInstruct-2501 model as our experimental models.\nOur hardware environment consists of four Nvidia\nA100-80G GPUs and 960GB of CPU memory.\nDataset Configurations We used LongBench v1\n(Bai et al., 2024) as the evaluation dataset. Long-\nBench v1 is a bilingual, multi-task benchmark for\nlong-text understanding, enabling a more rigorous\nevaluation of long-text comprehension.\nBaseline To the best of our knowledge, this study\nis the first to highlight the issue of GPU workload\nimbalance caused by per-head KV cache compres-\nsion algorithms in tensor parallelism. Therefore,"}, {"title": "5.2 Performance Evaluation", "content": "Performance evaluation is a critical component in\nassessing the effectiveness of the FairKV method\nin improving multi-GPU inference efficiency for\nlarge language models. This subsection outlines the\nkey metrics and methodologies used to evaluate the\nperformance of FairKV, ensuring a comprehensive\nunderstanding of its impact on system efficiency."}, {"title": "5.2.1 Throughput Improvement", "content": "We evaluated the performance of FairKV across\ndifferent models. Specifically, we selected a di-\nverse set of models, including LLaMA-3.3-70B-\nInstruct, Meta-LLaMA-3-8B, and Mistral-Small-\n24B-Instruct, and set the tensor parallel size to\neither 4 or 8 with RC fixed at 4. Throughput gains\nrelative to SHA were measured under KV cache\nbudgets of 128, 256, 512, and 1024. As shown in\nFigure 3, FairKV accelerates various models, and\ndue to the inherent characteristics of each model,\nits throughput under SHA conditions differs. As a\nresult, the acceleration effect of FairKV may vary.\nAmong them, FairKV achieves the highest benefit\nof up to 1.66 on the Llama-3.3-70B-Instruct model.\nWe also evaluated the performance of FairKV\nunder different tensor parallel sizes. Firstly, we\nobserved that FairKV provides acceleration ben-\nefits under all tensor parallel size conditions. By\ncomparing subfigures a, b, and c, we can see that\nas the tensor parallel size increases, the accelera-\ntion effect of FairKV significantly improves under\nthe same model and KV cache budget conditions.\nThis suggests that FairKV is more likely to achieve\nbetter acceleration performance when the tensor\nparallel size is larger.\nAs the KV cache budget increases, the accelera-\ntion effect of FairKV shows a slight improvement\nfor Meta-LLaMA-3-8B and Mistral-Small-24B-\nInstruct, while for LLaMA-3.3-70B-Instruct, the\nacceleration effect initially improves significantly\nand then slightly decreases. Overall, across these\nthree models, the acceleration effect of FairKV\ntends to improve as the KV cache budget increases.\nIn general, FairKV demonstrates acceleration\nbenefits across different models, tensor parallel\nsizes, and KV cache budgets. At the same time,\nthe effectiveness of FairKV is influenced by these\nfactors, and the results may vary accordingly."}, {"title": "5.2.2 GPU Utilization Improvement", "content": "We used LLaMA-3.3-70B-Instruct to evaluate the\nimpact of FairKV on GPU utilization. In Figure\n4, we conducted ablation tests among standard\nmodel, FairKV without Fair-copying and FairKV\nwith Fair-copying. The result indicate that both\nFairKV with or without Fair-copying significantly\nimproves GPU utilization compared to standard\nmodel, demonstrating that the FairKV method can\neffectively balance GPU loads. Moreover, FairKV\nwith Fair-copying shows further improvement over\nFairKV without Fair-copying. Then, to measure\nthe impact of the parameter on the FairKV with\nFair-copying group, we set the size of parallel size,\nwhich is also the count of copied heads (CH), to 1,\n2, 3, and 4. We measured the GPU utilization for\nthese groups under KV cache budgets of 128, 256,\n512, and 1024, with the results shown in Figure\n5. The results suggesting that incorporating only a\nsmall number of copied heads via Data Parallel can\nenhance the performance of the FairKV strategy\ngreatly. We also observed a positive correlation\nbetween the performance of FairKV and CH; as\nCH increases, the GPU utilization curve becomes\nless steep, indicating that while increases in CH\nyield significant benefits when CH is small, the\nincremental gains diminish when CH is larger."}, {"title": "6 Conclusion", "content": "In this paper, we propose FairKV, a novel opti-\nmization technique designed to improve inference\nefficiency by dynamically adjusting the allocation\nof attention heads in Tensor Parallelism. FairKV\nleverages the statistics of the retained Key-Value"}, {"title": "7 Limitations and Future works", "content": "Although FairKV provides significant improve-\nments in inference efficiency, there are a few limi-\ntations that need to be addressed. First, FairKV is\ndesigned for a single machine with multiple GPUs\nand does not account for scenarios involving dis-\ntributed systems across multiple machines. Second,\nthe current method primarily focuses on the par-\nallelization of the inference process without con-\nsidering the separation of the prefill and decode\nstages, which is critical in certain real-time applica-\ntions where the inference process is more complex\nand involves sequential decoding. Additionally,\nFairKV relies on the accuracy of the KV cache\nstatistics for optimal head allocation. This means\nthat the effectiveness of KV cache compression de-\npends on the statistical properties of the cache, and\nany deviations from these properties could reduce\nthe method's performance. We will address these\nlimitations in future work."}]}