{"title": "DYNAMIC CODE ORCHESTRATION: HARNESSING THE POWER OF\nLARGE LANGUAGE MODELS FOR ADAPTIVE SCRIPT EXECUTION", "authors": ["Justin Del Vecchio", "Andrew Perrault", "Eliana Furmanek"], "abstract": "Computer programming initially required humans to directly translate their goals into machine code.\nThese goals could have easily been expressed as a written (or human) language directive. Computers,\nhowever, had no capacity to satisfactorily interpret written language. Large language model's provide\nexactly this capability; automatic generation of computer programs or even assembly code from\nwritten language directives. This research examines dynamic code execution of written language\ndirectives within the context of a running application. It implements a text editor whose business logic\nis purely backed by large language model prompts. That is, the program's execution uses prompts and\nwritten language directives to dynamically generate application logic at the point in time it is needed.\nThe research clearly shows how written language directives, backed by a large language model, offer\nradically new programming and operating system paradigms. For example, empowerment of users\nto directly implement requirements via written language directives, thus supplanting the need for a\nteam of programmers, a release schedule and the like. Or, new security mechanisms where static\nexecutables, always a target for reverse engineering or fuzzing, no longer exist. They are replaced by\nephemeral executables that may continually change, be completely removed, and are easily updated.", "sections": [{"title": "1 Introduction", "content": "Imagine a future where software projects are distributed and used as shown in Figure 1. A description of the Figure's\nkey elements follows."}, {"title": "2 Q1: Implement a Dynamic Code Orchestration Application", "content": "We created a dynamically orchestrated Python text editor. The application allows users to open, edit, and save text\ndocuments. The user interface has a drop down menu with items for New, Open, and Save. New creates an entirely\nnew file, Open opens an existing file, and Save saves the file currently open in the editor. The editor itself is shown in\nFigure 2.\nThe skeleton code for the application created the GUI elements for menu items. It added the elements to a layout and\nestablished a command for each menu item. When a user clicked on a menu item, the dynamic code orchestration\nworkflow would initiate. The sequence of events is shown in Figure 3 with a description following. Here, the user has\nclicked Open.\nDynamic Code Orchestration Event Sequence\nStep 1. The controller function is called. It is passed a command that it is to prepare to open a file. It finds the\nrequisite written directive for the desired executable code block. The contents of the directive are discussed in Step 4. It\nthen calls generateCode and passes the directive.\nStep 2. generateCode checks to see if this is the first time dynamic code has been requested. If so, it embeds the\nexisting source code of DynamicTextEditor in terms of the ChatGPT 3.5 model. Think of the embed step as creating\na customized [3] question and answer chat model that is aware of the content of DynamicTextEditor script. Further,"}, {"title": "2.1 User Defined Functionality", "content": "There is a potential problem with Step 6. What if text is already present in the text area? The returned function would\nsimply overwrite the text. The user might want to save the current file before opening a new one. It would be nice to\nwarn the user and allow them to cancel the operation!\nWe edited the directive for openFile while the application ran. The directive was changed with the following text added:\nLLM Open File Directive Enhancement: \"If there is already content in the text area - warn the user!\"\nThe next menu click of Open generated the following code:\nChatGPT augmented the code to add a message box warning of the potential issue. The user now has the option\nto cancel the operation. This new functionally required no editing of the original source code yet integrated with it\nseamlessly. Figure 4 shows what we saw after a simple update to the written directive."}, {"title": "3 Q2: Failure Points of Generated Code", "content": "We performed an experiment to see how well the ChatGPT LLM generated function code for an arbitrary written\ndirective. We utilized the HumanDataEval dataset[2]. It contains 164 algorithm based problems, as well as unit tests,\nand a proper answer to the problem. We judged success if the generated function would pass a set number of unit tests.\nWe wrote code which prompted ChatGPT with a written directive task via its API. We generated 10 different versions of\na single function to satisfy each of the 164 problems. ChatGPT was provided a function header, the problem description,\nand a format for its JSON result. We needed to provide the result format to clearly distinguish the developed code from\nuncommented descriptions of code functionality ChatGPT might add.\nOur test harness wrote the ChatGPT replies to file, imported the code dynamically, and tested it against the unit tests.\nWe implemented a technique that dynamically translated ChatGPT replies into executable code blocks. The approach\nwas used by the text editor detailed in Section 2. The following code listing provides a condensed view of the technique.\nLine 1 submits the written narrative to the ChatGPT LLM as a question. It replies with the source code for the Python\nfunction. Line 2 compiles the code into a code object. Line 4 is the critical step. The exec function is called on the\ncompiled_code code object. Specifically, any functions defined within the compiled_code will be registered in the\nglobal namespace (globals()). This means that these functions will become globally accessible after the exec() call.\nHere's a breakdown of how it works[5]:\n1. The exec() function is used to execute the Python code contained in compiled_code.\n2. When this code is executed, any functions defined within it will be registered in the global symbol table\n(namespace) because globals() is used as the second argument.\n3. Once registered in the global namespace, these functions can be accessed and called from anywhere within the\nprogram.\nLines 5 through 8 test if the code block generated a legitimate, executable function. Line 9 invokes the newly created\nfunction which is now available in the global namespace.\nWe tracked and logged the output and results of generating code block executables for the HumanEval test set. Using\n1400 test files, ChatGPT 3.5 successfully generated code that compiled and ran correctly 61% of the time. This number\nwould likely greatly improve with a switch to the ChatGPT 4 model which we plan to use in the future."}, {"title": "3.1 Challenges for Dynamically Generate Code", "content": "One challenge is getting ChatGPT to respond with a consistent JSON format where it is easy to identify what is source\ncode and what is not. ChatGPT was instructed to explicitly delimit code with three back ticks. However, it did not\nalways correctly do this. 15% of the time we were unable to confidently isolate the code in the response (thus the reason\nwe ran 1400 tests as opposed to 1640). This problem could be overcome through imporved identification and removal\nof uncommented code descriptions added by ChatGPT."}, {"title": "4 Related Work", "content": "Research into pseudo code languages that allow specification of code for real time compilation exist [6][7]. As well, Just\nin Time Compilers (JIT) have used the technique. These approaches have not considered an LLM that can synthesize\nsource code or assembly from a modest set of natural language directives. Natural language instructions to generate\nsource code have also been studied. A survey of approaches from 2021 is found here [8]. These mainly focus on\ncontrolled vocabulary that map to source code constructs.\nResearchers performed a set of experiments on the now deprecated OpenAI platform \u201cCodeX\" [9]. Codex was a\nGPT-3 LLM that was designed specifically for programmers. It was designed to take in prompts, some code, and create\nprograms. It was deprecated upon the release of ChatGPT as ChatGPT was deemed a much better resource."}, {"title": "5 Future Plans", "content": "We will focus on two research areas. Specifically:\nUser Defined Adaptive Scripts - This research will focus on identifying the possibilities and limitations of user\nextended application functionality. We will introduce written directives backed by cascading LLM prompts to extend an\nexisting open source application or library. We will review user requests for enhancements and select a set to implement\nwith dynamic code orchestration, playing the part of empowered user.\nDefensive Coding - We will take an open source project with a Windows distributable. It will have past vulnerabilities\nidentified as MITRE CVEs[10]. We will translate portions of the existing code base into written directives. We will\nthen perform a security analysis of the application to identify vulnerabilities. This will include fuzzing, disassembly,\nand debugging."}]}