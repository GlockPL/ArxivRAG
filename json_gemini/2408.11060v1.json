{"title": "DYNAMIC CODE ORCHESTRATION: HARNESSING THE POWER OF LARGE LANGUAGE MODELS FOR ADAPTIVE SCRIPT EXECUTION", "authors": ["Justin Del Vecchio", "Andrew Perrault", "Eliana Furmanek"], "abstract": "Computer programming initially required humans to directly translate their goals into machine code. These goals could have easily been expressed as a written (or human) language directive. Computers, however, had no capacity to satisfactorily interpret written language. Large language model's provide exactly this capability; automatic generation of computer programs or even assembly code from written language directives. This research examines dynamic code execution of written language directives within the context of a running application. It implements a text editor whose business logic is purely backed by large language model prompts. That is, the program's execution uses prompts and written language directives to dynamically generate application logic at the point in time it is needed. The research clearly shows how written language directives, backed by a large language model, offer radically new programming and operating system paradigms. For example, empowerment of users to directly implement requirements via written language directives, thus supplanting the need for a team of programmers, a release schedule and the like. Or, new security mechanisms where static executables, always a target for reverse engineering or fuzzing, no longer exist. They are replaced by ephemeral executables that may continually change, be completely removed, and are easily updated.", "sections": [{"title": "1 Introduction", "content": "Imagine a future where software projects are distributed and used as shown in Figure 1. A description of the Figure's key elements follows.\nSource Code - Project source code exists in a high level language. However, it serves as a skeleton that enables, application loading and limited functionality. Much of the application's business logic exists as a set of written language directives. An example of such a directive is provided in Section 2.\nInterpretation or Compilation - Users configure the project to use a large language model (LLM) service. This may be installed locally or available via a network. The project skeleton code is interpreted or compiled with hooks to access the LLM when required by the application.\nAdaptive Script Execution - A user runs the application and interacts with it. Written language directives embody large portions of the application's business logic. When this logic is required, the directives are sent to the LLM service and returned as Python code. The code is compiled into an executable code block and registered as function in the global namespace. This function may then be called, or completely regenereated, as often as needed.\nDynamic code orchestration transforms applications into a custom patchwork of ephemeral executable code blocks represented by adaptive written directives. The advantages of this approach include:\n\u2022 User Customization: User's can define custom functionality as a written directive. Applications can be extended by user's themselves. This removes the need for a team of supporting programmers who interpret requirements and develop code.\n\u2022 Reduced Attack Surface: Fuzzing and reverse engineering an executable becomes harder as business logic is encapsulated in written language directives. LLMs can be instructed to return syntactically different but semantically equivalent executable code blocks. This makes it difficult for malware developers to target vulnerabilities as applications are no longer statically defined.\n\u2022 Space Conservation: Code executables become far less bulky. Unused code simply does not exists. The written language directives dictate what composes an executable. The executable code blocks generated by a written directive can periodically be purged to conserve space.\nWe examined the dynamic code orchestration concept via two research questions. We provide the code developed to answer both questions as public GitHub repositories.\n1. Could we develop an application that is dynamically orchestrated? We would need to bake into the application a capability to call an LLM service. Certain user actions would lead to written language directives. The directives would need to be sent to the LLM, executable code blocks generated and added to the global namespace. The functions could then be called by the application. This question would require use of prompt engineering [1] and custom LLM knowledge stores that used our application source code for context when formulating replies.\n2. What is the current veracity of written language directive to code translation? How well do LLMs currently perform at this task? We used the HumanEval set [2] as a benchmark. This benchmark has been tested against ChatGPT 3.5 and 4 models, amongst others. Our goal was to identify what failures (incorrect or uncompilable LLM replies) looked like. How would our dynamic code orchestration pipleline need to adjust for such failures? Additionally, how could the executable code blocks be seamlessly integrated into a running application, in this case the test harness application?"}, {"title": "2 Q1: Implement a Dynamic Code Orchestration Application", "content": "We created a dynamically orchestrated Python text editor. The application allows users to open, edit, and save text documents. The user interface has a drop down menu with items for New, Open, and Save. New creates an entirely new file, Open opens an existing file, and Save saves the file currently open in the editor. The skeleton code for the application created the GUI elements for menu items. It added the elements to a layout and established a command for each menu item. When a user clicked on a menu item, the dynamic code orchestration workflow would initiate. The sequence of events is shown in Figure 3 with a description following. Here, the user has clicked Open.\nDynamic Code Orchestration Event Sequence\nStep 1. The controller function is called. It is passed a command that it is to prepare to open a file. It finds the requisite written directive for the desired executable code block. The contents of the directive are discussed in Step 4. It then calls generateCode and passes the directive.\nStep 2. generateCode checks to see if this is the first time dynamic code has been requested. If so, it embeds the existing source code of DynamicTextEditor in terms of the ChatGPT 3.5 model. Think of the embed step as creating a customized [3] question and answer chat model that is aware of the content of DynamicTextEditor script. Further,\nStep 3. generateCode next creates a prompt which can pose questions to the custom ChatGPT 3.5 model. Following is the text of the prompt used in the application:\nLLM Prompt: \"You are a programmer. You should use the preexisting code in the file DynamicTextEditor.py and create the requested functions so the code operates without error. Pay attention to the imports in DynamicTextEditor and choose code that works within those imports.\"\nStep 4. Next, the written directive for openFile is sent to the LLM's question and answering mechanism. Following is the text of the written directive that instructs on how to open a file:"}, {"title": "2.1 User Defined Functionality", "content": "There is a potential problem with Step 6. What if text is already present in the text area? The returned function would simply overwrite the text. The user might want to save the current file before opening a new one. It would be nice to warn the user and allow them to cancel the operation!\nWe edited the directive for openFile while the application ran. The directive was changed with the following text added:\nLLM Open File Directive Enhancement: \"If there is already content in the text area - warn the user!\"\nThe next menu click of Open generated the following code:\nChatGPT augmented the code to add a message box warning of the potential issue. The user now has the option to cancel the operation. This new functionally required no editing of the original source code yet integrated with it seamlessly. Figure 4 shows what we saw after a simple update to the written directive."}, {"title": "3 Q2: Failure Points of Generated Code", "content": "We performed an experiment to see how well the ChatGPT LLM generated function code for an arbitrary written directive. We utilized the HumanDataEval dataset[2]. It contains 164 algorithm based problems, as well as unit tests, and a proper answer to the problem. We judged success if the generated function would pass a set number of unit tests.\nWe wrote code which prompted ChatGPT with a written directive task via its API. We generated 10 different versions of a single function to satisfy each of the 164 problems. ChatGPT was provided a function header, the problem description, and a format for its JSON result. We needed to provide the result format to clearly distinguish the developed code from uncommented descriptions of code functionality ChatGPT might add.\nOur test harness wrote the ChatGPT replies to file, imported the code dynamically, and tested it against the unit tests. We implemented a technique that dynamically translated ChatGPT replies into executable code blocks. The approach was used by the text editor detailed in Section 2. The following code listing provides a condensed view of the technique.\nLine 1 submits the written narrative to the ChatGPT LLM as a question. It replies with the source code for the Python function. Line 2 compiles the code into a code object. Line 4 is the critical step. The exec function is called on the compiled_code code object. Specifically, any functions defined within the compiled_code will be registered in the global namespace (globals()). This means that these functions will become globally accessible after the exec() call. Here's a breakdown of how it works[5]:\n1. The exec() function is used to execute the Python code contained in compiled_code.\n2. When this code is executed, any functions defined within it will be registered in the global symbol table (namespace) because globals() is used as the second argument.\n3. Once registered in the global namespace, these functions can be accessed and called from anywhere within the program.\nLines 5 through 8 test if the code block generated a legitimate, executable function. Line 9 invokes the newly created function which is now available in the global namespace.\nWe tracked and logged the output and results of generating code block executables for the HumanEval test set. Using 1400 test files, ChatGPT 3.5 successfully generated code that compiled and ran correctly 61% of the time. This number would likely greatly improve with a switch to the ChatGPT 4 model which we plan to use in the future."}, {"title": "3.1 Challenges for Dynamically Generate Code", "content": "One challenge is getting ChatGPT to respond with a consistent JSON format where it is easy to identify what is source code and what is not. ChatGPT was instructed to explicitly delimit code with three back ticks. However, it did not always correctly do this. 15% of the time we were unable to confidently isolate the code in the response (thus the reason we ran 1400 tests as opposed to 1640). This problem could be overcome through imporved identification and removal of uncommented code descriptions added by ChatGPT.\nAnother challenge was getting ChatGPT to not import from external libraries as we could not guarantee they would be available. Even when instructed not to import external libraries it still would. This problem could be overcome through a fine tuned LLM model or by having ! pip install newlibrary commands added for any imported libraries.\nA final challenge we faced was the LLM writing infinite while loops, clearly the greatest problem. Since the HumanEval problems are relatively simple, we implemented a timeout if the function ran pass a time threshold. Part of our future research will include experiments where we specifically instruct the LLM to protect against infinite loops and measure its performance with and without such guidance."}, {"title": "4 Related Work", "content": "Research into pseudo code languages that allow specification of code for real time compilation exist [6][7]. As well, Just in Time Compilers (JIT) have used the technique. These approaches have not considered an LLM that can synthesize source code or assembly from a modest set of natural language directives. Natural language instructions to generate source code have also been studied. A survey of approaches from 2021 is found here [8]. These mainly focus on controlled vocabulary that map to source code constructs.\nResearchers performed a set of experiments on the now deprecated OpenAI platform \u201cCodeX\" [9]. Codex was a GPT-3 LLM that was designed specifically for programmers. It was designed to take in prompts, some code, and create programs. It was deprecated upon the release of ChatGPT as ChatGPT was deemed a much better resource."}, {"title": "5 Future Plans", "content": "We will focus on two research areas. Specifically:\nUser Defined Adaptive Scripts - This research will focus on identifying the possibilities and limitations of user extended application functionality. We will introduce written directives backed by cascading LLM prompts to extend an existing open source application or library. We will review user requests for enhancements and select a set to implement with dynamic code orchestration, playing the part of empowered user.\nDefensive Coding - We will take an open source project with a Windows distributable. It will have past vulnerabilities identified as MITRE CVEs[10]. We will translate portions of the existing code base into written directives. We will then perform a security analysis of the application to identify vulnerabilities. This will include fuzzing, disassembly, and debugging."}]}