{"title": "DiffGrad for Physics-Informed Neural Networks", "authors": ["Jamshaid Ul Rahman", "Nimra"], "abstract": "Physics-Informed Neural Networks (PINNs) are regarded as state-of-the-art tools for addressing highly nonlinear problems based on partial differential equations. Despite their broad range of applications, PINNs encounter several performance challenges, including issues related to efficiency, minimization of computational cost, and enhancement of accuracy. Burgers' equation, a fundamental equation in fluid dynamics that is extensively used in PINNs, provides flexible results with the Adam optimizer that does not account for past gradients. This paper introduces a novel strategy for solving Burgers' equation by incorporating DiffGrad with PINNs, a method that leverages the difference between current and immediately preceding gradients to enhance performance. A comprehensive computational analysis is conducted using optimizers such as Adam, Adamax, RMSprop, and DiffGrad to evaluate and compare their effectiveness. Our approach includes visualizing the solutions over space at various time intervals to demonstrate the accuracy of the network. The results show that DiffGrad not only improves the accuracy of the solution but also reduces training time compared to the other optimizers.", "sections": [{"title": "1. INTRODUCTION", "content": "Deep neural network's invention has revolutionized numerous fields including computer vision [1], game theory, and natural language processing [2]. Through deep learning (DL), the landscape of tasks involving categorization, pattern recognition, and regression has been fundamentally reshaped across a wide array of application domains [3]. Deep neural networks (DNNs) can be effectively used in oscillator simulations. Oscillators, which produce periodic signals, are common in many fields such as electronics, physics, and engineering [4]. Moreover,\nDeep neural networks (DNNs) can be effectively used to simulate the Selkov glycolysis model by learning the non-linear dynamics from synthetic data generated via numerical solutions of the model's differential equations [5]. However, their utilization remains relatively unexplored in the realm of scientific computing. The incorporation of machine learning techniques into scientific computing [6], particularly for dynamic problems, is a relatively recent advancement [7].\nA wide range of dynamic problems can be formulated as differential equations (DEs). Various numerical and analytical approaches such as finite element, finite difference, and finite volume methods have been proposed by researchers to solve DEs [8] While these traditional methods have demonstrated considerable success, they often fall short when dealing with real-world nonlinear problems. Moreover, each traditional method has its own set of strengths, weaknesses, and areas of applicability [9]. Additionally, these methods are problem-specific and rely on structured meshes, limiting their flexibility. Certain Partial Differential Equations(PDEs) are infamously challenging to solve using conventional numerical techniques because of factors like shocks, convective dominance, or large nonlinearities, among others.\nIn the discipline of mathematical physics, the numerical solution of Nonlinear PDEs has always been a hot topic in recent decades. Researchers have been able to take on increasingly challenging and high-dimensional problems thanks to developments in processing capacity and methods [10,11].\nPhysics-Informed Neural Networks (PINNs) represent a state-of-the-art approach in scientific machine learning, specifically engineered to address complex PDEs problems [12-14]. They integrate physical laws into the learning process, enabling the model to achieve accurate and efficient solutions to these challenging equations [15-18]. According to a two-part article, in 2017, PINNs were first introduced as a new category of data-driven solutions [19, 20] which was subsequently merged and published in 2019 [21]. In this work, Raissi et al. [22, 23] present and demonstrate PINN framework for solving nonlinear Partial Differential Equations, such as the Schr\u00f6dinger, Burgers, and Allen-Cahn equations. They introduce PINNs, which are capable of addressing both inverse problems (where model parameters are inferred from observational data) and forward problems (which involve predicting solutions based on governing mathematical models). By integrating physical laws directly into the neural network framework, PINNs enhance the model's generalization capabilities, offering robust and accurate solutions for various\napplications in science and engineering. PINNs base their modeling strategy on a combination of underlying physical laws and empirical data."}, {"title": "2. METHODOLOGY", "content": "The methodology involves creating an approximation using a neural network $u_\\theta (t, x)$ to represent the solution $u(t, x)$ of a nonlinear PDE within a specific domain $D$ and time range $[0, T]$. In this context, $u_\\theta : [0,T] \\times D \\rightarrow R$ represents a function represented by a neural network with parameters $\\theta$. The approach involves a continuous-time formulation for the parabolic PDE, as discussed in (Raissi et al., 2017 (Part I)), where the focus is on the residual of the neural network approximation $u_\\theta(t,x)$ and $r_\\theta(t, x)$.\nThis residual is defined as\n$r_\\theta(t, x) := \\partial_t u_\\theta (t, x) + N[u_\\theta](t,x)$\n(1)\nwhere $N[u_\\theta](t,x)$, represents the nonlinear component of the PDE.\nTo incorporate this PDE residual $r_\\theta$ into a loss function for minimization, Physics-Informed Neural Network(PINN) necessitate additional differentiation techniques to accurately compute differential operators $\\partial_t u_\\theta$ and $N[u_\\theta]$. Consequently, the PINN term shares the same parameters as the original network $u_\\theta(t,x)$ while respecting underlying physics of the nonlinear PDE. The parameters shared between the neural networks $h(t, x)$ and $f (t, x)$ are optimized by minimizing the mean squared error loss. Both type of derivatives can be efficiently computed\nthrough automatic differentiation using modern machine learning libraries such as TensorFlow or PyTorch.\nThe beginning and boundary value problems are now solved using the PINNs technique by minimizing the loss function.\n$\\Phi_\\theta(X) := \\Phi_r^\\theta(X_r) + \\Phi_0^\\theta(X_0) + \\Phi_b^\\theta(X_b)$\n(2)\nhere $X$ stands for both the loss function and the gathering of training data. The following terms are contained in$\\Phi$: The mean squared residual,\n$\\Phi_r^\\theta(X_r):= \\frac{1}{N_r} \\sum_{j=1}^{N_r} |r_\\theta(t^j,x^j)|^2$\n(3)\nAt several collocation points $X_r = \\{(t^j, x^j)\\}_{j=1}^{N_r} \\subset (0, T] \\times D$,where $r_\\theta$ is our physics-informed network and $N_r$ is the total number of collocation points .\nThe mean squared error related to the initial and boundary conditions is determined by averaging the squared differences between the predicted values from the neural network approximation $u_\\theta$ and the actual initial conditions at points $X_0$, and the boundary conditions at points $X_b$. This misfit quantifies the discrepancies in the predictions made by the neural network and the specified initial and boundary conditions\n$\\Phi_0^\\theta(X_0) = \\frac{1}{N_0} \\sum_{j=1}^{N_0} |u_\\theta (t_i, x_i) - u_0 (x_i)|^2$\n(4)\n$\\Phi_b^\\theta(X_b) = \\frac{1}{N_b} \\sum_{j=1}^{N_b} |u_\\theta (t_b, x_b) - u_b (x_b)|^2$\n(5)\nAt several points\n$X_0 := \\{(t_i^0, x_i^0)\\}_{i=1}^{N_0} \\subset \\{0\\} \\times D$\nand\n$X_b := \\{(t_i^b, x_i^b)\\}_{i=1}^{N_b} \\subset (0,T] \\times \\partial D$\nWhere $u_\\theta$ is the neural network's approximation of the solution $u: [0,T] \\times D \\rightarrow R$.\nIt's worth noting that the training dataset X exclusively comprises coordinates representing both time and space.\nThe entire process can be visually represented through a figure to provide a clear illustration of the steps involved as shown in Fig.02."}, {"title": "3. PROBLEM SETUP", "content": "To exemplify the Physics-Informed Neural Networks (PINNs) methodology, we focus on one-dimensional Burgers equation within the spatial domain D = [-1,1]\n$\\partial_t u + u \\partial_x u - (\\frac{0.01}{\\pi}) \\partial_{xx} u = 0, (t,x ) \\in (0,1] \\times (-1,1)$\n(6)\n$u(0, x) = - sin(\\pi x), x \\in [-1,1]$\n$u(t,-1) = u(t, 1) = 0, t \\in (0,1]$\nThe provided equation (6) describes a one-dimensional Burgers' equation incorporating Dirichlet boundary conditions and a source term. It describes the evolution of a velocity field $u(t,x)$ over time t and space w. The equation includes convection, diffusion, and a nonlinear advection term $u\\partial_x u$. The boundary conditions set the velocity to zero at both ends of the domain x = -1 and x = 1 while the initial condition $u(0, x) = - sin(\\pi x)$ defines the initial velocity distribution at t = 0. The source term $(\\frac{0.01}{\\pi}) \\partial_{xx} u$ introduces additional effects on the velocity field. This equation governs the behavior of the velocity field within the specified domain (0,1] \u00d7 (-1,1)over the time interval t \u2208 (0,1]."}, {"title": "4. ALGORITHMIC FRAMEWORK AND ARCHITECTURE", "content": "The mentioned PDE appears in diverse fields including traffic dynamics, fluid mechanics, and gas dynamics. It can be obtained as a simplification of the Navier-Stokes equations.\nThe neural network architecture cosists of the following steps.\ni.\nInitialization of Packages and Problem-Specific Parameters\nThe code utilizes TensorFlow version 2.3.0, along with the NumPy library for scientific computing. TensorFlow is used for machine learning tasks. All the optimizers utilized in the code were built-in optimizers provided by TensorFlow, except for DiffGrad. DiffGrad is a custom optimizer implemented within the code, tailored to specific requirements or experimental purposes.\nii.\nGenerate a set of collocation points\nThe initial time and boundary data points, $X_0$ and $X_b$, along with the collocation points $X_r$, are generated by randomly sampling from a uniform distribution. In our experiments, uniformly distributed data suffice for testing purpose; however, previous work (Raissi et al., 2017 (Part I)) has shown that a space-filling Latin hypercube sampling approach (Stein, 1987) can marginally improve the rate of convergence. This observation is verified by our numerical investigations. However, the code example provided in this paper employ uniform sampling consistently for the sake of simplicity. We choose to use $N_0 = N_b = 50$ and $N_f = 10000$ as our training data sizes.\niii. Establishing Neural Network Configuration\nIn this example, inspired by Raissi et al. (2017, Part I), we employ a Feedforward Neural Network structured as follows:\nEight completely connected layers follow after the input is scaled element-wise to fall inside the range of [-1,1] in the network architecture. Twenty neurons make up each layer, which is driven by a hyperbolic tangent activation function. There is another one output layer that is entirely connected. This setup results in a network with a total of 3021 trainable parameters distributed across its layers:\nThe first hidden layer consists of 60 parameters (2 \u00d7 20 + 20).\nEach of the seven intermediate layers encompasses 420 parameters (20 \u00d7 20 + 20).\nThe output layer comprises 21 parameters (20 \u00d7 1 + 1).\niv.\nEstablish Functions for Calculating Loss and Gradients:\nNext we define a function which evaluates the residual using equation (1) and loss using equation (2) and initial and boundary conditions as given in equation (3) and (4) respectively."}, {"title": "5. RESULTS AND DISCUSSION", "content": "The objective of this study is to analyze the behavior of various optimizers in conjunction with PINNs for solving Burgers' equation. Specifically, we aim to investigate how different optimizers affect the convergence and accuracy of the solution obtained by PINNs.\nWe investigate the performance of various optimization algorithms in minimizing the loss function described above. The optimizers considered in our study include\na) DiffGrad\nb) Adam\nc) RMSprop\nd) Adamax\nEach optimizer aims to update the network weights to minimize the loss function iteratively. We employ the TensorFlow library to implement these optimization algorithms efficiently.\nTo train the PINNs model, we generate collocation points within the domain. These points consist of initial points (x,t), boundary points, and randomly sampled collocation points within the interior of the domain as shown in Fig. 03. Starting with differential gradients (diff grad) as our optimization method, we aim to assess its effectiveness in enhancing the performance of Physics-Informed Neural Networks (PINNs) models. Next we will see the results with Adam.\nThe plot of loss function over epochs is under:\nlearning rate schedule enhances the optimization process, facilitating improved convergence and performance of the PINNs model.\nAfter experimenting with the Adam optimizer, we're now trying out Adamax and RMSprop in Fig. 07 and Fig. 08 respectively. These are different methods for adjusting the learning rate during training, especially useful for complex tasks like solving Burgers' equation with neural networks.\nAdamax is similar to Adam but is particularly effective when dealing with large amounts of data and sparse gradients. It is renowned for its adaptability and robustness in handling large-scale optimization problems. On the other hand, RMSprop adjusts the learning rate individually for each parameter, making it robust and suitable for scenarios where the problem dynamics are constantly changing.\nIn Fig.08, the final segment of our experiment, we examine the performance of RMSprop when training PINNs models for solving Burgers' equation. We aim to assess whether RMSprop exhibits comparable effectiveness to the other optimization methods we have explored. The 3D Visualization of Solution using the Adam Optimizer offers a graphical depiction of the optimized solution obtained from training a model using the Adam optimization algorithm. This visualization provides valuable hints into the structural characteristics and dynamic behavior of the solution in a three-dimensional space. It aids in understanding how the solution evolves over time and across different spatial dimensions, thereby enhancing our comprehension of the underlying physical phenomena described by the Burgers' equation.\nNext, we present a series of 2D figures illustrating the solution of the Burgers equation at various time points:t = 0.25,t = 0.5,t = 0.75, t = 0.9. These figures serve as snapshots of the fluid velocity profile at different stages in the simulation, providing crucial insights into the evolving dynamics of the system. By visualizing the solution at multiple time instances, we gain a comprehensive understanding of how the fluid velocity field evolves over time and space.\nThe Fig.10, Fig.11, Fig12, and Fig.13 showcases the velocity distribution across spatial positions x at the specified time t. The x-axis denotes the spatial domain, while the y-axis represents the corresponding velocity values $u_\\theta$. The solution illustrates how features of the fluid flow, such as\nshocks or discontinuities, propagate through space over time. This propagation can reveal insights into the behavior of the fluid, such as the formation of shock waves or the evolution of turbulence. Time is a crucial factor in experiments like this, where computational resources are often limited, and researchers aim to achieve results within a reasonable timeframe. The computational time required for training neural network models using different optimization algorithms directly impacts the experimental process's efficiency and resource utilization. By analyzing and comparing the computation times of various optimization algorithms, researchers can make informed decisions regarding algorithm selection, resource allocation, and experiment planning."}, {"title": "CONCLUSION", "content": "In conclusion, our study highlights the application of Physics-Informed Neural Networks (PINNs) for solving Burgers' equation, emphasizing the importance of ensuring that the neural network learns the system's behavior while adhering to fundamental physical laws. Through a comprehensive comparative analysis, we evaluate the efficacy of various optimizers, including traditional methods like Adam, Adamax, and RMSprop, as well as the custom-defined DiffGrad optimizer, in minimizing the loss function associated with PINNs for solving Burgers' equation.\nOur findings highlight the critical role of optimizer selection in optimizing the loss landscape of PINNs. Notably, while Adam exhibits lower loss compared to DiffGrad and all other optimizers, DiffGrad is found to be less computationally expensive than Adam. Despite Adam's lower loss, the computational efficiency of DiffGrad makes it an attractive option for training PINNs. Additionally, graphical representations of the solution over spatial coordinates at different time intervals provide valuable insights into the model's performance.\nOverall, our results offer valuable guidance for practitioners seeking to enhance the training performance of PINNs, emphasizing the importance of considering both computational efficiency and user-friendliness when selecting optimizers. This study underscores the significance of optimizer choice in achieving optimal model convergence and efficiency in solving complex physical systems."}]}