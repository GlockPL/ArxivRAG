{"title": "ASR-enhanced Multimodal Representation Learning for Cross-Domain Product Retrieval", "authors": ["Ruixiang Zhao", "Jian Jia", "Yan Li", "Xuehan Bai", "Quan Chen", "Han Li", "Peng Jiang", "Xirong Li"], "abstract": "E-commerce is increasingly multimedia-enriched, with products exhibited in a broad-domain manner as images, short videos, or live stream promotions. A unified and vectorized cross-domain production representation is essential. Due to large intra-product variance and high inter-product similarity in the broad-domain scenario, a visual-only representation is inadequate. While Automatic Speech Recognition (ASR) text derived from the short or live-stream videos is readily accessible, how to de-noise the excessively noisy text for multimodal representation learning is mostly untouched. We propose ASR-enhanced Multimodal Product Representation Learning (AMPere). In order to extract product-specific information from the raw ASR text, AMPere uses an easy-to-implement LLM-based ASR text summarizer. The LLM-summarized text, together with visual data, is then fed into a multi-branch network to generate compact multimodal embeddings. Extensive experiments on a large-scale tri-domain dataset verify the effectiveness of AMPere in obtaining a unified multimodal product representation that clearly improves cross-domain product retrieval.", "sections": [{"title": "I. INTRODUCTION", "content": "The content of E-commerce is becoming increasingly multimedia-enriched. Besides traditional product pages, E-commerce platforms are incorporating more dynamic and interactive media formats, such as short videos and live streams, to present their products more effectively and consequently enhance end-user shopping experience. A specific product can now be exhibited in a broad-domain manner through an image [1], a short descriptive video [2], or a live stream promotion [3], see Fig. 1. As such, a unified and vectorized cross-domain representation of the product is essential for downstream tasks including cross-domain product retrieval (CdPR), user profiling, personalized recommendation, etc. This paper is targeted at multimodal product representation learning (MmPRL) for CdPR.\n\nCurrent efforts in product representation learning are primarily focused on the product-page domain [4]\u2013[11], where a product sample appears as an image-title pair. EI-CLIP [7], for"}, {"title": "II. RELATED WORK", "content": "This work is targeted at learning ASR-enhanced multimodal production representation for cross-domain product retrieval (CdPR). Hence, we first discuss recent progress in CdPR, followed by a brief survey concerning the use of ASR text in a broader context."}, {"title": "A. Cross-domain Product Retrieval", "content": "Compared to single-domain (mostly product-page) product retrieval [5]\u2013[11], [19], [20], there are relatively few studies on CdPR, see Table II. Chen et al. [2] propose QCD for bidirectional retrieval between the product-page (P) and short-video (S) domains. In particular, QCD assumes each visual example, either a product image in the P domain or a short video in the S domain, is associated with a user query obtained based on click-through records. By cross-attention based fusion of the textual feature of the query and the"}, {"title": "III. PROPOSED METHOD", "content": ""}, {"title": "A. Problem Formalization", "content": "We formalize the problem of multimodal product representation learning (MmPRL) as follows. A specific product on an E-commerce platform is typically exhibited as data samples in the following three domains, i.e. product-page (P), short-video (S), and live-stream (L). Let x be a specific sample, with , and be its three domain-specific instantiations. In particular, indicates an image of the given product, is a short video describing the product, whilst \u03b1\u03b9 denotes a longer live-stream video with a network anchor prompting the product. In a multimodal scenario, the image xp is associated with a title tp, while the two videos xs and x are associated with ASR texts ts and ti, respectively. For the ease of consistent description, we re-use x to indicate a given multimodal sample, with its domain identity and the associated text both temporally omitted. The goal of MmPRL is to train a feature extraction network F that encodes the given sample into a d-dimensional embedding e(x). Accordingly, cross-domain product retrieval boils down to calculating cosine similarities between e(xp), e(xs) and e(xi)."}, {"title": "B. COPE in a Nutshell", "content": "To handle visual inputs from the three domains, the COPE network has three branches. Each branch consists of two modules, i.e. a video encoder that converts the corresponding input x into a 512-dimensional video feature v(x), followed by a linear layer to obtain a shorter 128-d embedding e(x) for cross-domain matching.\n\nCOPE adopts the video encoder of X-CLIP [28], which has a novel cross-frame attention mechanism to capture the long-range temporal dependencies across frames. More specifically, given a video x as a sequence of n uniformly sampled frames {f1,..., fn}, a ViT [29] based cross-frame communication"}, {"title": "C. LLM-based ASR Text Summarization", "content": "Given the verbose ASR text obtained from a video of a specific product, say an electric shaver, we aim to extract two key pieces of product-specific information. That is, the name of the product (e.g. Space UFO shaped shaver) and major features of the product (e.g. high quality blade material, fast and clean shaving, ...). To that end, we turn an LLM into an ASR text summarizer with the popular In-Context Learning (ICL) technique [30]. In essence, ICT performs a test-time adaptation of an LLM by instructing the model with a few demonstration examples. See Table I for our demonstrations and our project website2 for source code.\n\nAs our benchmark dataset [3] was gathered from a Chinese E-commerce platform with text in Chinese, a Chinese LLM is needed. In the current work we adopt a 4bits quantized version of the 13B-parameter Baichuan Chat model\u00b3, a leading LLM on the CLiB Chinese LLM benchmark4. This version strikes a good balance between model effectiveness and inference cost. Qualitative results of our LLM-based ASR text summarization are provided in Fig. 3."}, {"title": "D. Adding Summarized ASR to COPE", "content": "Our multimodal extension of COPE is illustrated in Fig. 2. In order to accept the text modality, per branch we add a six-layer Chinese ROBERTa [31] as a text encoder. Given an input text t as a sequence of m tokens {w1,...,Wm}, the text encoder yields m 768-d token-level features plus a CLS-token feature yo.\n\nFor multimodal fusion, the visual and textual features have to be aligned beforehand. We use a linear layer to transform the textual features to a new sequence of 512-d features {0, 1,..., \u0177m}. In a similar manner, we obtain a sequence of n + 1 transformed visual features as {(x), 21,..., n}.\nThe visual and textual feature sequences, concatenated as a (m+n+2)\u00d7512 multimodal tensor, are fed into an array of four Transformer blocks for intra-modality and inter-modality feature interaction and updating. We preserve the video feature and the CLS-token feature updated by the Transformers, denoted as u(x) and yo, respectively. The final multimodal embedding e(x,t) is obtained by summing up (x) and yo followed by a linear layer. More formally, we express the above multimodal fusion process as follows:"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Setup", "content": "Dataset. We adopt ROPE [3], a public dataset released by Kuaishou (Kwai) for large-scale cross-domain product retrieval5. With 3.1M product pages, 5.9M short videos, and 3.5M live streams for over 189K products, ROPE is currently the largest multi-domain multimodal product dataset of its kind. Tab. III shows data statistics of ROPE. We follow the official data split, 98.7% for training and 1.3% for testing.\n\nTasks. Following [3], we evaluate the proposed method for the following six CdPR tasks. That is, P2S, P2L, S2P, S2L, L2P, and L2S, where P, S, and L indicate the product-page, short-video, and live-stream domains, respectively.\n\nEvaluation criteria. Per task, we report standard rank-based metrics: R1, R5 and R10. For overall comparison, we report the mean value of R1 of all tasks (mR1)."}, {"title": "B. Comparison with Baseline Methods", "content": "Baselines. Based on our analysis of current methods for CdPR, c.f. Table II, COPE is a natural baseline to AMPere. Moreover, we compare with CN-CLIP [36]. This Chinese variant of CLIP is a widely accepted baseline for both generic text-to-video retrieval [37] and CdPR [2], [3], [13]. Per video, we obtain its CN-CLIP feature by mean pooling over the 8 frame-level features. The multimodal feature is obtained by averaging the video-level feature and the CN-CLIP feature of the associated ASR text. To check if LLM-summarized text is also beneficial for CN-CLIP, we implement three variants:"}, {"title": "C. Understanding AMPere", "content": "In order to understand how AMPere works and particularly what elements contribute to its superior performance, we conduct a number of ablation studies as follows. For efficiency"}, {"title": "V. CONCLUSIONS AND REMARKS", "content": "We propose AMPere, an ASR-enhanced multimodal product representation learning method, for cross-domain product retrieval. Extensive experiments on the large-scale ROPE dataset allow us to draw conclusions as follows. Due to the extremely noisy nature of the ASR text transcribed from short or live-stream videos, directly feeding the raw ASR text to a multimodal representation learning network is sub-optimal. LLM is a powerful tool for extracting product-specific information from the verbose text. With the LLM-summarized ASR text, AMPere improves the state-of-the-art visual based solution by a large margin. Finally, parameter sharing across different domains is beneficial for learning a better cross-domain multimodal production representation. Our study shows for the first time that incorporating ASR text can largely benefit product retrieval in the E-commerce"}, {"title": "Limitations of this study", "content": "As we have noted, due to the uncontrolled nature of the ASR text, the LLM-based text summarizer yielded no result for the samples. In the current work, we simply use an empty string as a hotfix. How to better handle the modality missing issue warrants further research."}]}