{"title": "ASR-enhanced Multimodal Representation Learning\nfor Cross-Domain Product Retrieval", "authors": ["Ruixiang Zhao", "Jian Jia", "Yan Li", "Xuehan Bai", "Quan Chen", "Han Li", "Peng Jiang", "Xirong Li"], "abstract": "E-commerce is increasingly multimedia-enriched,\nwith products exhibited in a broad-domain manner as images,\nshort videos, or live stream promotions. A unified and vectorized\ncross-domain production representation is essential. Due to large\nintra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inade-\nquate. While Automatic Speech Recognition (ASR) text derived\nfrom the short or live-stream videos is readily accessible, how\nto de-noise the excessively noisy text for multimodal represen-\ntation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In or-\nder to extract product-specific information from the raw ASR\ntext, AMPere uses an easy-to-implement LLM-based ASR text\nsummarizer. The LLM-summarized text, together with visual\ndata, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a\nlarge-scale tri-domain dataset verify the effectiveness of AMPere\nin obtaining a unified multimodal product representation that\nclearly improves cross-domain product retrieval.", "sections": [{"title": "I. INTRODUCTION", "content": "THE content of E-commerce is becoming increasingly\nmultimedia-enriched. Besides traditional product pages,\nE-commerce platforms are incorporating more dynamic and\ninteractive media formats, such as short videos and live\nstreams, to present their products more effectively and con-\nsequently enhance end-user shopping experience. A specific\nproduct can now be exhibited in a broad-domain manner\nthrough an image [1], a short descriptive video [2], or a\nlive stream promotion [3], see Fig. 1. As such, a unified\nand vectorized cross-domain representation of the product is\nessential for downstream tasks including cross-domain product\nretrieval (CdPR), user profiling, personalized recommendation,\netc. This paper is targeted at multimodal product representation\nlearning (MmPRL) for CdPR.\nCurrent efforts in product representation learning are pri-\nmarily focused on the product-page domain [4]\u2013[11], where a\nproduct sample appears as an image-title pair. EI-CLIP [7], for"}, {"title": "II. RELATED WORK", "content": "This work is targeted at learning ASR-enhanced multimodal\nproduction representation for cross-domain product retrieval\n(CdPR). Hence, we first discuss recent progress in CdPR,\nfollowed by a brief survey concerning the use of ASR text\nin a broader context."}, {"title": "A. Cross-domain Product Retrieval", "content": "Compared to single-domain (mostly product-page) product\nretrieval [5]\u2013[11], [19], [20], there are relatively few studies\non CdPR, see Table II. Chen et al. [2] propose QCD for\nbidirectional retrieval between the product-page (P) and short-\nvideo (S) domains. In particular, QCD assumes each visual\nexample, either a product image in the P domain or a short\nvideo in the S domain, is associated with a user query\nobtained based on click-through records. By cross-attention\nbased fusion of the textual feature of the query and the"}, {"title": "III. PROPOSED METHOD", "content": "We formalize the problem of multimodal product represen-\ntation learning (MmPRL) as follows. A specific product on an\nE-commerce platform is typically exhibited as data samples in\nthe following three domains, i.e. product-page (P), short-video\n(S), and live-stream (L). Let x be a specific sample, with Xp, Xs\nand x be its three domain-specific instantiations. In particular,\nXp indicates an image of the given product, xs is a short video\ndescribing the product, whilst \u03b1\u03b9 denotes a longer live-stream\nvideo with a network anchor prompting the product. In a\nmultimodal scenario, the image xp is associated with a title tp,\nwhile the two videos xs and x are associated with ASR texts\nts and ti, respectively. For the ease of consistent description,\nwe re-use x to indicate a given multimodal sample, with\nits domain identity and the associated text both temporally\nomitted. The goal of MmPRL is to train a feature extraction\nnetwork F that encodes the given sample into a d-dimensional\nembedding e(x). Accordingly, cross-domain product retrieval\nboils down to calculating cosine similarities between e(xp),\ne(xs) and e(xi).\nOur method is developed based on the state-of-the-art COPE\n[3]. So we first describe COPE briefly in Sec. III-B, followed\nby our LLM-based text summarizer that handles the noisy\nASR text in Sec. III-C. Our multimodal extension of COPE\nis detailed in Sec. III-D."}, {"title": "B. COPE in a Nutshell", "content": "To handle visual inputs from the three domains, the COPE\nnetwork has three branches. Each branch consists of two\nmodules, i.e. a video encoder that converts the corresponding\ninput x into a 512-dimensional video feature v(x), followed\nby a linear layer to obtain a shorter 128-d embedding e(x) for\ncross-domain matching.\nCOPE adopts the video encoder of X-CLIP [28], which has\na novel cross-frame attention mechanism to capture the long-\nrange temporal dependencies across frames. More specifically,\ngiven a video x as a sequence of n uniformly sampled frames\n{f1,..., fn}, a ViT [29] based cross-frame communication"}, {"title": "C. LLM-based ASR Text Summarization", "content": "Given the verbose ASR text obtained from a video of a\nspecific product, say an electric shaver, we aim to extract two\nkey pieces of product-specific information. That is, the name\nof the product (e.g. Space UFO shaped shaver) and major\nfeatures of the product (e.g. high quality blade material, fast\nand clean shaving, ...). To that end, we turn an LLM into an\nASR text summarizer with the popular In-Context Learning\n(ICL) technique [30]. In essence, ICT performs a test-time\nadaptation of an LLM by instructing the model with a few\ndemonstration examples. See Table I for our demonstrations\nand our project website2 for source code.\nAs our benchmark dataset [3] was gathered from a Chinese\nE-commerce platform with text in Chinese, a Chinese LLM is\nneeded. In the current work we adopt a 4bits quantized version\nof the 13B-parameter Baichuan Chat model\u00b3, a leading LLM\non the CLiB Chinese LLM benchmark4. This version strikes a\ngood balance between model effectiveness and inference cost.\nQualitative results of our LLM-based ASR text summarization\nare provided in Fig. 3."}, {"title": "D. Adding Summarized ASR to COPE", "content": "Our multimodal extension of COPE is illustrated in Fig.\n2. In order to accept the text modality, per branch we add\na six-layer Chinese ROBERTa [31] as a text encoder. Given\nan input text t as a sequence of m tokens {w1,...,Wm}, the\ntext encoder yields m 768-d token-level features {Y1,\u2026\u2026\u2026, Ym}\nplus a CLS-token feature yo.\nFor multimodal fusion, the visual and textual features have\nto be aligned beforehand. We use a linear layer to transform"}, {"title": "IV. EXPERIMENTS", "content": "We adopt ROPE [3], a public dataset released\nby Kuaishou (Kwai) for large-scale cross-domain product\nretrieval5. With 3.1M product pages, 5.9M short videos, and\n3.5M live streams for over 189K products, ROPE is currently\nthe largest multi-domain multimodal product dataset of its\nkind. Tab. III shows data statistics of ROPE. We follow the\nofficial data split, 98.7% for training and 1.3% for testing."}, {"title": "A. Experimental Setup", "content": "Following [3], we evaluate the proposed method for\nthe following six CdPR tasks. That is, P2S, P2L, S2P, S2L,\nL2P, and L2S, where P, S, and L indicate the product-page,\nshort-video, and live-stream domains, respectively.\nPer task, we report standard rank-\nbased metrics: R1, R5 and R10. For overall comparison, we\nreport the mean value of R1 of all tasks (mR1)."}, {"title": "B. Comparison with Baseline Methods", "content": "Based on our analysis of current methods for\nCdPR, c.f. Table II, COPE is a natural baseline to AMPere.\nMoreover, we compare with CN-CLIP [36]. This Chinese\nvariant of CLIP is a widely accepted baseline for both generic\ntext-to-video retrieval [37] and CdPR [2], [3], [13]. Per video,\nwe obtain its CN-CLIP feature by mean pooling over the 8\nframe-level features. The multimodal feature is obtained by\naveraging the video-level feature and the CN-CLIP feature of\nthe associated ASR text. To check if LLM-summarized text is\nalso beneficial for CN-CLIP, we implement three variants:"}, {"title": "C. Understanding AMPere", "content": "In order to understand how AMPere works and particularly\nwhat elements contribute to its superior performance, we\nconduct a number of ablation studies as follows. For efficiency"}, {"title": "V. CONCLUSIONS AND REMARKS", "content": "We propose AMPere, an ASR-enhanced multimodal prod-\nuct representation learning method, for cross-domain product\nretrieval. Extensive experiments on the large-scale ROPE\ndataset allow us to draw conclusions as follows. Due to the\nextremely noisy nature of the ASR text transcribed from\nshort or live-stream videos, directly feeding the raw ASR\ntext to a multimodal representation learning network is sub-\noptimal. LLM is a powerful tool for extracting product-\nspecific information from the verbose text. With the LLM-\nsummarized ASR text, AMPere improves the state-of-the-art\nvisual based solution by a large margin. Finally, parameter\nsharing across different domains is beneficial for learning\na better cross-domain multimodal production representation.\nOur study shows for the first time that incorporating ASR\ntext can largely benefit product retrieval in the E-commerce"}]}