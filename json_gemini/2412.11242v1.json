{"title": "TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs", "authors": ["Lanxiang Hu", "Tajana Rosing", "Hao Zhang"], "abstract": "Specializing large language models (LLMs) for local deployment in domain-specific use cases is necessary for strong performance while meeting latency and privacy constraints. However, conventional task-specific adaptation approaches do not show simultaneous memory saving and inference speedup at deployment time. Practical compression techniques like quantization and pruning require dedicated hardware or kernel support to achieve measured inference speedup. We develop TRIMLLM based on the layer-wise specialization phenomenon we empirically observed and verified on contemporary LLMs. TRIMLLM reduces the depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity in specific domains and achieves inference speedup irrespective of hardware and deep learning frameworks. We evaluated TRIMLLM on LLMs of various sizes for inference; models adapted on medical, legal, and financial datasets all demonstrate 2.1 - 5.7x inference speedup on consumer GPUs and up to 3.1\u00d7 speedup on A100 when compared to state-of-the-art model compression algorithms, with no loss in accuracy at 50~60% model compression ratio.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly prominent, evolving to serve specialized domains such as medicine (Thirunavukarasu et al., 2023), law (Yue et al., 2023), and finance (Wu et al., 2023b). Their deployment in local environments is particularly valuable, addressing latency and privacy concerns, especially where sensitive data are involved. For example, understaffed clinics greatly benefit from medical-specialized LLM assistants deployed locally. However, the substantial memory and computation required for inference present significant barriers to deploying specialized LLMs in such resource-limited scenarios.\nPost-training quantization (PTQ) has emerged as a key technique for adapting LLMs to resource-limited environments, by reducing weight bit precision to 4 or even 3 bits, with minimal degradation in model performance. However, the practical implementations of PTQ methods (Dettmers et al., 2022; Xiao et al., 2023; Frantar et al., 2022; Lin et al., 2023) depend on the availability of efficient kernels and vendor-specific hardware support for quantized computational operations. Unfortunately, such support is not widely accessible. In reality, applying many existing PTQ techniques oftentimes slows down model inference on consumer-level hardware, as shown in Table 1. Similar results are seen with many pruning algorithms (Kwon et al., 2022; Frantar and Alistarh, 2023a; Sun et al., 2023), which fail to translate theoretical speedup into real performance gains when specific hardware or kernel support (e.g., for sparsity) are absent.\nTo address these limitations, this paper explores a new way of compressing LLMs. Recent insights in LLM model editing show that middle layers in LLMs are crucial for domain-specific knowledge (Meng et al., 2022a; Li et al., 2023; Azaria and Mitchell, 2023), with attention modules handling general semantic correlations while MLP layers being more task-specific (Geva et al., 2020). In this study, we delve deeper into the domain-specific relevance of various layers in LLMs. Figure 1 reveals that when fine-tuning on science-common-sense and medical domains, we can remove up to 20 and 16 out of 32 layers respectively in LLaMA-7B without compromising performance.\nBuilding on these findings, we hypothesize layer-wise specialization: the significance of each layer of an LLM, particularly the MLP layer, varies according to the specific knowledge domain; we can fine-tune a more domain-focused LLM by selectively dropping layers unimportant to the targeted domain. This strategy enables us to craft models that are not only more compact but also finely bal-"}, {"title": "2 Related Work", "content": "Task-specific adaptation. A typical workflow for task-specific adaptation is to first fine-tune (Wu et al., 2023a; Yang et al., 2023; Huang et al., 2023b,a) or even pre-train (Wu et al., 2023b; Cui et al., 2023; Shah et al., 2023) LLMs on task-specific datasets before applying any of the following three model compression techniques for reliable performance during inference: quantization, distillation, and pruning. In our case, we adopt layer-dropping to compress the model step-by-step during fine-tuning, i.e., we adapt LLMs to domain-specific tasks by identifying and retaining important layers for the target domain.\nQuantization. Quantization can effectively mitigate memory consumption by reducing the bit-widths of LLMs' weights and activations. Quantization has featured its ability to retain LLM's zero-shot ability with measured memory saving and theoretical speedup. The state-of-the-art quantization algorithms (Dettmers et al., 2022; Xiao"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Preliminaries and Layer-Wise Specialization", "content": "Auto-regressive language models compose of many transformer decoders, where each decoder block is made of one multi-head attention (MHA) layer and one MLP. Many previous studies on model editing (see Section 2) show increasing evidences suggesting that different layers weight differently when it comes to domain-specific inference, that we call layer-wise specialization.\nFormally, consider a pre-trained model f (x; 0), where x \u2208 R is an input sequence with sequence length s and embedding dimension n, \u03b8\u2208 RD is a parameter vector that parameterizes f (\u00b7) with a total parameter size of D.\nConsider layernorm to be part of the MHA and MLP layer along with residual connection with each layer indexed by i \u2208 {1, . . ., N}, where N is the total number of layers in a model. Let the input to each decoder layer DEC\u00bf be y\u00bf-1 at the current generation step, the corresponding output at layer i follows expression in Eq. 1.\ny_i = DEC_i (y_{i-1}) := MLP (MHA (y_{i-1})) \\tag{1}\nAt i = 1, the input has $y_{i-1} = y_0 = (Y_{0,1}, \u2026\u2026\u2026, Y_{0,T})$, where T is the current timestamp and yt is token generated by a previous timestamp t<T.\nLet the feature space for inputs of a downstream task be X and input tokens yo,t \u2208 X, and the feature space for generated output tokens be yn,t \u2208 Y in Equation 2.\nY_N = DEC_N \\circ DEC_{N-1} \\circ \\cdots \\circ DEC_0 (y_0) = f (y_0; 0) \\tag{2}\nOur basic assumption is that for each downstream task, there exists a feature space X, where X can be described as a random variable froma distribution Dx, and Y is a random variable from Dy. Our hypothesis is:\nHypothesis 1 Let the set of all attention layers in Equation 1 be A and the set of all MLP layers be M. For all input sequences xo generated from X, there exists a set of attention and MLP layers Ax CA, Mx C M such that the function com-position of Ux = Ax\u016aMx can be fine-tuned on the joint distribution Dxy for the downstream"}, {"title": "3.2 Fine-Tuning with Progressive Layer Dropping", "content": "In addition to the ordinary fine-tuning procedure for language models, TRIMLLM iteratively picks a layer to drop after one epoch of training and gradually reduces the model depth. This gives TRIMLLM the advantages of reduced memory consumption and inference latency at deployment time.\nOur empirical experiments and recent works (Syed et al., 2023) show drastically changing the model from f (y0; 00) \u2192 Gux (Vo; 0f) by dropping many parameters all at a time generally gives bad results. This function Gux (y0; 0f) maps the generated outputs to a distribution Dyf that's very distinct from Dy and result in bad domain-specific performance. Note that of is the parameter vector and Dy is the output distribution for the full model after fine-tuning. Successive layer dropping, on the other hand, allows domain-specific specialization to be done step by step with\n$\u0192(\u04230;00) \u2192 G\u0438\u0445\u2081 (\u04230; 01) \u2192 GUx2 (\u04230; 02)\u06f0\u06f0\u06f0 \u2192 Gux (yo; 0')$ where 0 is the parameter vector after i epochs. Gux (\u00b7) is the model right after the i-th epoch with the corresponding set of remaining layers being Uxi\nThis observation aligns the intuition that gradually changing the function's parameterization with most important layers retained allows generated outputs to transit more smoothly from $D_y \u2192 D'_y \u2192 D''_y$ such that D'y, is a close approximation of Dy for the full model after fine-tuning. It thereby provides more evidences to verify our hypothesis in Section 3.1 with an additional constraint:\nProposition 1 The functional R : f(\u00b7) \u2192 GUx\u2081 (\u00b7) needs to be decomposed into successive layer-dropping operators {ro,...,rf} such that the parameter vector ''s dimensionality only changes by a small decrement at a time to gradually adapts a downstream task with the most representative parameters.\nDue to the iterative nature of the aforementioned layer dropping algorithm, the time complex-"}, {"title": "3.3 Target Selection Algorithms", "content": "One important aspect of TRIMLLM is choosing the right layer from Ux\u2081 to drop after the i-th epoch and thereby satisfy the successive distribution shift condition (Proposition 1). We introduce two techniques to assign each layer an importance score, where a lower importance score means the layer contribute less to the model's performance on a downstream task.\nSensitivity-based Scoring. The first method is a performance scanning based on a small calibration dataset. Before each time a layer is to be dropped, a small subset of the fine-tuning dataset's validation set is sampled as the calibration dataset. For each layer, its importance score is the reciprocal of the model's performance after dropping the layer. Calibration scanning gives the importance score of any layer i and the expression is presented in Equation 3, where ai \u2208 [0,100] is the accuracy of the model after dropping the i-th layer and 8 is a small positive number such that $\\frac{100}{1+\\delta^2}$ is the maximum importance score when ai = 0.\nS_{i,scan} = \\frac{100}{(1 + \\delta^2) + (1 + \\delta) \\alpha_i} \\tag{3}\nActivation-based Scoring. The second method is to make activation-norm comparison on different layers' activations. Recent studies (Dettmers et al., 2022; Xiao et al., 2023) have shown preserving information carried by activations is critical to model's performance when it comes to model compression techniques.\nIn our work, our goal is to only preserve activations that are meaningful to the knowledge domain of interest. We can drop the rest to trade the model's generality for efficiency and specialization."}, {"title": "3.4 Sparse update as a Regularization", "content": "In TRIMLLM, an important observation is that some less important layers will eventually be dropped regardless whether they have been tuned. Moreover, empirical evidences in Table 3 show fine-tuning all layers could, in effect, perform worse than full fine-tuning.\nThere are two reasons for the possible performance degradation. First, catastrophic forgetting has been a well recognized problem when a language model is trained on downstream data with all parameters are updated (Lee et al., 2022). Second, layer dropping in TRIMLLM is conducted"}, {"title": "4 Experiments", "content": "In this section, we present experiments that provide empirical evidences for our hypothesis as well as the effectiveness of TRIMLLM. The test suite spans a wide range of knowledge domains including common-sense, medical, legal and financial QA benchmarks. All experiments reported in this section are conducted on LLaMA-7B and LLaMA-13B with training performed on NVIDIA V100 32GB servers. Deployment-time inference speeds are tested on NVIDIA A100 40GB, V100 32GB and RTX 3090 GPUs."}, {"title": "4.1 Performance on QA Benchmarks", "content": "To test which of the methods can compress the model to the fullest extent while maintaining more than 90% performance of the full-finetuning baseline, we compare the performance of different sparse update schemes and target selection algorithms. The results are summarized in Table 3. On each QA benchmark, we also compare TRIMLLM and other model compression techniques. The results are presented in Table 2.\nBaselines. We use full fine-tuning (full-FT) as our most basic baseline. We also include a sparse fine-tuning (sparse-FT) baseline that only updates the salient layers identified by calibration scanning with the optimal sparse update ratio (r = 4). While LLM pruning approaches with structured pruning methods can give inference speedup as shown in Table 1, they are generally incapable of reducing memory consumption without hardware support. As a result, we benchmark TRIMLLM with the state-of-the-art LLM quantization techniques: LLM.int8(), GPTQ and AWQ. They are used as stronger baselines that permit both memory saving and potential inference speedup.\nQA benchmarks. We use common-sense QA benchmarks inculuding SciQ (Johannes Welbl, 2017) and PIQA (Bisk et al., 2020) to test LLM's ability of understanding and making basic inference about the physical world the way ordinary humans do. To further assess TRIMLLM's capacity for domain-specific adaptation, we also evaluate its performance on medical, legal, and financial QA datasets: MedMCQA (Pal et al., 2022), LexGLUE-casehold (Chalkidis et al., 2021), and FinanceQA (Bharti, 2023) respectively. For LexGLUE, evaluations are done on the \"law\" subset of MMLU (Hendrycks et al., 2020). For FinanceQA, the dataset includes a combination of"}, {"title": "4.2 Memory Consumption and Latency", "content": "We argue the TRIMLLM has a two-fold advantage. The first one is efficiency and the other is flexibility. On the efficiency side, TRIMLLM has both"}, {"title": "4.3 Applying Other Model Compression Techniques to TRIMLLM", "content": "Our method is orthogonal to all model compression techniques. Applying TRIMLLM alongside with other post-training model compression techniques like quantization can provide further speedup. Results from applying AWQ and SparseGPT with 2:4 structured sparsity to TRIMLLM, and the corresponding compressed models' accuracy, memory consumption and inference latency are reported in"}, {"title": "4.4 Limitations", "content": "While increasing TRIMLLM can extend LLM accessibility to a wider audience in domain-specific use cases, specializing LLMs may raise robustness concern when applying the models to tasks that require knowledge from multiple domains. Striking a balance between accessibility and maintaining the integrity and reliability of language models is essential to ensure their responsible use in various applications."}, {"title": "5 Conclusion", "content": "We propose TRIMLLM, a task-specific adaption and model compression pipeline for contemporary LLMs. TRIMLLM reduces deployment-time memory cost and inference latency by identifying and discarding less significant layers to reduce the specialized model's depth. Unlike baselines, TRIMLLM can obtain both wall-clock inference speedup and memory saving without the need for specialized hardware and efficient computational kernels. We hope that TRIMLLM paves the path for making LLMs accessible to the wider public in personal and professional use cases."}, {"title": "6 Use of AI Assistants", "content": "In adherence to the ACL Publication Ethics Policy, we did not employ AI assistants to generate the initial draft of this paper. We used AI assistants (GPT-40) exclusively at the sentence level to enhance writing quality and correct grammatical errors."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Layer-dropping Patterns", "content": "For each of the downstream task shown in Figure 3, there are a few key observations can be made: (1) LLaMA-7B have different layer dropping patterns on different tasks, (2) there are significantly more MLP layers are dropped than the self-attention ones. The first observation provides more empirical evidences for layer-wise specialization while the second for knowledge localization, which argues domain knowledge is stored in MLPs."}, {"title": "A.2 Ablation: TRIMLLM Robustness on a Similar Domain", "content": "We conduct additional experiments to test TRIMLLM 's robustness. Model fine-tuned with the MedMCQA dataset is validated on another dataset, PubMedQA from the medical knowledge domain. Model fine-tuned using the LexGLUE dataset is tested on the Legalbench benchmark. Results in Table 4 show the model specialized on MedMCQA can perform relatively well on PubMedQA, in comparison with benchmarks from totally different knowledge domains. Same conclusion applies to the model specialized on LexGLUE."}, {"title": "A.3 Ablation: Performance Degradation on Unrelated Domains.", "content": "We test specialized models' performance on other domain-specific tasks and it demonstrates a significant performance degradation. Results of each specialized model's performance on other tasks are provided in Table 5."}, {"title": "A.4 Ablation: Different Sparse Update Ratios", "content": "As we can see in Table 6, results show TRIMLLM performs the worst when all layers are updated with a sparse update ratio r = 1. With a ratio of r = 4, the model can be compressed to a greatest extent with more than 16 decoder layers (out of 32) dropped with nearly no loss in accuracy."}, {"title": "A.5 TRIMLLM Fine-tuning Time Complexity Analysis", "content": "For conventional full FT, assume the average time it takes to train a layer for one epoch can be approximated by some parameter c. In practice, c is a function of positional index (depth of the layer), parameter size, dataset size, sequence length, operator types, hardware types, and other factors for each layer. Specifically, c differs significantly for the MLP and the attention layers. This difference in forward time can be used to assign MLP and attention layers with different weights in addition to the metrics in Equation 3 and Equation 4. In this analysis, we perform order-of-magnitude approximation, assuming c is given as prior knowledge, and leave the opportunities of dynamically estimating c for future work.\nWith this approximation, the time it takes to train N layers for one epoch is T (N) = cN and TFFT = cN \u00d7 n for n epochs. With one layer dropped at a time, let the total number of layers to be dropped be nd, the time can be approximated as:\nT_{FFT,\\Delta=1} (n_d) = T(N) + T(N - 1) + \\ldots + T(N - n_d + 1) = c \\cdot n_d \\cdot \\Big(N - \\frac{n_d-1}{2}\\Big) \\tag{5}\nSimilarly, we can write down approximated time for dropping two layers at a time, and it amounts to:\nT_{FFT,\\Delta=2} (n_d) = T(N) + T(N - 2) + \\ldots + T(N - n_d + 2) = c \\cdot \\frac{n_d}{2} \\cdot \\Big(N - \\frac{n_d}{2} + 1\\Big) \\tag{6}\nOn the other hand, if we apply sparse FT introduced in Section 3.4, empirical results show it reduces training time to ~ 60% at the sparsity ratio of r = 1/4 as evidenced in profiling results from Table 9. The table shows TRIMLLM take around ~ 40% of the time to train in comparison with full FT, and it's now possible to run the proposed fine-tuning scheme that iteratively compress a model without sacrificing too much training overhead.\nHowever, it's arguable that the training overhead is still significant as demonstrated in Figure 4. We further introduce adaptive layer dropping, which is to drop more than one layer per epoch. From table 9, we see the conversion factor for training time from full FT to sparse FT is roughly 0.6. Plug the scaling"}]}