{"title": "Efficient and Scalable Estimation of Tool Representations in Vector Space", "authors": ["Suhong Moon", "Siddharth Jha", "Lutfi Eren Erdogan", "Sehoon Kim", "Woosang Lim", "Kurt Keutzer", "Amir Gholami"], "abstract": "Recent advancements in function calling and tool use have significantly enhanced the capabilities of large language models (LLMs) by enabling them to interact with external information sources and execute complex tasks. However, the limited context window of LLMs presents challenges when a large number of tools are available, necessitating efficient methods to manage prompt length while maintaining accuracy. Existing approaches, such as fine-tuning LLMs or leveraging their reasoning capabilities, either require frequent retraining or incur significant latency overhead. A more efficient solution involves training small models to retrieve the most relevant tools for a given query. However, previous methods rely on tool descriptions, leading to suboptimal performance. To address this, we propose approaches based on a two-stage retrieval technique. In the first stage, candidate tools are retrieved using a fast retriever, incorporating two novel methods: (1) Tool2Vec, which generates usage-driven tool embeddings, and (2) MLC, which frames tool retrieval as a multi-label classification problem. In the second stage, we introduce ToolRefiner, which refines the candidate tools retrieved in the first stage, further enhancing retrieval performance. While this approach requires domain-specific tool retrieval data, we demonstrate that LLMs can generate high-quality datasets. To show this, we create ToolBank, showcasing that LLMs can generate effective tool retrieval data across various domains. With our methods, we achieve improvements of up to 27.28 in Recall@K on the ToolBench dataset and 30.5 in Recall@K on ToolBank. The dataset and code are publicly available.", "sections": [{"title": "1 Introduction", "content": "Recently, function calling and tool use has emerged as a powerful paradigm for using large language models (LLMs) [28, 31, 26, 3]. Rather than relying solely on the model's parametric knowledge, function calling and tool use enable the model to interact with the world [11, 7, 41, 37, 22]. This approach allows the model to perform specific tasks, such as accessing information beyond the LLM's knowledge cut-off date, solving complex math problems, and executing complex planning [36, 33, 20, 5].\nHowever, since function calling requires passing in the tool's description and signature into the model's context window, it is often infeasible to put information about potentially thousands of functions due to context window limitations. Additionally, even when using models with longer context windows, long context inference leads to latency, cost overheads and accuracy challenges, necessitating the need for smaller prompts [21, 16, 11, 22]. Therefore, selectively retrieving tools to present to the model can greatly reduce prompt lengths while preserving accuracy.\nSeveral methods have been proposed to address the issue of the limited context window in LLMs when the number of available tools exceeds the model's capacity. One popular approach is to leverage the reasoning capabilities of LLMs to pre-select the appropriate tools from a large pool [10, 32, 37, 43]. Despite the LLMs' ability to learn and choose tools effectively, this method incurs significant latency overhead, making it less practical in various use cases where real-time responses are critical. An alternative and more efficient solution is dense retrieval of tools [29, 30, 2, 45]. In this approach, each tool's description is converted into an embedding vector using an embedding model, and tools with"}, {"title": "2 Related Work", "content": "Function calling allows LLMs to interact with the world and agentic environments by filling in parameters to API functions and other tools. Typically, function descriptions and signatures are provided in the model's context window. For accurate function calling, models must be able to choose the proper functions for the task and be able to fill in the correct parameters to those functions. Large models such as GPT-4 have demonstrated impressive function calling capabilities [22]. However, smaller models [28, 34], such as 7B and 13B models, have also been developed specifically for function calling tasks. The ToolBench [29] dataset is a popular function calling dataset consisting of real-world APIs that was used to fine-tune a 7B LLAMA model for tool use."}, {"title": "2.2 Tool Retrieval", "content": "As discussed in subsection 2.1, function descriptions and signatures are provided in the model's context window for applications relying on function calling. However, real-world applications often have hundreds or thousands of tools [29]. Providing information about all tools to the model may not be possible due to context length limits. Furthermore, even when using models with longer context windows, providing all the tools in the prompt leads to significant compute and memory overheads [21]. To address this, various tool retrieval methods have been proposed to select and provide only the relevant tools for incoming user queries instead of providing them all.\nA notable approach to enhance tool retrieval performance is leveraging another LLM. AnyTool [10] proposes to use GPT-4 for API retrieval and to further enhance retrieval performance through an iterative self-reflection method."}, {"title": "3 Dataset Generation", "content": "We generate domain-specific tool retrieval datasets with the goals of (1) demonstrating that users can create sufficiently large domain-specific datasets powered by LLMs [23, 6, 11] for small tool retrieval models, and (2) addressing the limitations inherent in existing benchmarks [29, 8, 40, 24, 14, 35], which often lack coherent tool integration and query naturalness.\nParticularly for the second aspect, current benchmarks frequently pair tools without considering their natural co-"}, {"title": "4 Tool Retrieval Approaches", "content": "In this section, we propose a two-stage tool retrieval method, as outlined in Figure 2. The first retrieval stage efficiently prunes the majority of the tool space, while the second refining stage further refines the kept tools to produce the final set of retrieved tools. This is analogous to retrieve-then-rerank pipelines for document retrieval [25].\nThe benefit of this two-stage approach is that the more powerful refining stage can correct any errors from the fast retrieval stage. Additionally, because retrieval is performed first to filter out candidate tools, the refiner stage does not need to operate on the entire toolset, leading to efficient implementation and runtime. For the first stage, we present two methods: (i) usage-driven embedding generation (subsection 4.1) and (ii) multi-label classification (subsection 4.2). For the second stage, we introduce an efficient yet accurate classification model for refining the output of the first stage (subsection 4.3), which can contextualize the inter-tool and tool-query interactions."}, {"title": "4.1 Tool2Vec: Usage-Driven Embedding Generation", "content": "Similar to document retrieval, performing vector search over embeddings can be used to retrieve an initial set of tools in the first stage. Previous tool retrieval methods have relied on tool descriptions to obtain embeddings of each tool [2, 29, 30, 43]. However, this approach may be suboptimal due to the semantic disparity between tool descriptions and user queries. Figure 3 (Left) illustrates how tool descriptions and user queries can be disjoint in the embedding space, making tool retrieval based on embedding similarity challenging. This issue persists even when the descriptions are augmented with additional information, such as tool code, to improve retrieval performance [43, 45, 10].\nTo reduce the distributional gap between query and tool embeddings for retrieval, we propose Tool2Vec, the usage-driven tool embedding generation. Instead of using tool descriptions, we propose to use user queries to obtain tool embeddings. In more detail, if we have multiple user queries that use a specific tool, we use the average embeddings of those user queries as the Tool2Vec embedding that represents the tool. For example, in Figure 4, we have multiple user queries that use the tool `find_email address`, such as \"What is Anna's email address?\" In this case, we use an embedding model (e.g., E5 [38]) to obtain the embedding for each user query, and the average of these embeddings is used as the Tool2Vec embedding for the tool `find_email_address`. Likewise, the Tool2Vec embedding for the tool `find_weather` can be obtained the same way using the associated user queries. As shown in the figure, since the Tool2Vec embeddings of these tools are derived from user queries, they are closer to the incoming user query in the embedding space compared to embeddings derived from tool descriptions.\nTo further justify the benefits of Tool2Vec's usage-driven tool embedding generation, we perform an analysis as illustrated in Figure 3. The left figure is a t-SNE visualization of the embeddings of the user queries, Tool2Vec, and"}, {"title": "4.2 Tool Retrieval as Multi-Label Classification", "content": "In settings where there are enough instructions and associated tool labels, the first stage of our two-stage tool retrieval (Figure 2) can alternatively be formulated as a multi-label classification problem. Furthermore, given the rise of synthetic data generation methods [6, 23, 39, 4], it has become possible to construct such labeled high-quality pairs synthetically with the competent LLMs [1, 27, 17, 19], as demonstrated in section 3.\nOne approach for multi-label classification (MLC) involves training a model that takes the instruction as input and outputs the classification logits for each tool, as illustrated in the left figure of Figure 5. When a user query is provided, such as \u201cWhat is the weather today?\", we assign a label of 1 to all required tools and a label of 0 to unused tools to training the MLC. In this example, the find weather tool receives a label of 1, while other tools receive a label of 0. To achieve this, we fine-tune the pre-trained DeBERTa-V3 base model [13, 9], which features a  $H \\times T$ classification head operating on the output [CLS] token. Here, H represents the dimension of the [CLS] token, and T denotes the total number of tools in the dataset. Surprisingly, this simple MLC alone demonstrates strong tool retrieval capabilities, even surpassing other state-of-the-art methods, as discussed in subsection 5.1.\""}, {"title": "4.3 ToolRefiner", "content": "As the number of tools increases, methods like Tool2Vec and MLC, though effective, may struggle with the vast set of tools. This is because these methods can miss relevant tools due to unnecessary noise between tools. Additionally, Tool2Vec embeddings and MLC cannot capture the tool-query and tool-tool interactions, which limits their effectiveness.\nTo address these challenges, we introduce ToolRefiner for the second stage tool retrieval. ToolRefiner enhances"}, {"title": "5 Experiments", "content": "In this section, we describe experimental results that validate the effectiveness of our proposed methods, Tool2Vec, MLC, and ToolRefiner on various benchmarks including ToolBench [29] and ToolBank."}, {"title": "5.1 ToolBench", "content": "In Table 1, we evaluate our proposed methods in the ToolBench dataset [29], comparing their performance against two established baselines: the ToolBench Retriever [29], and COLT [30]. We observe that our methods constantly outperform the baselines with large margins."}, {"title": "5.1.1 Experimental Details", "content": "For benchmarking, we use the ToolBench dataset, which is the current standard benchmark for multi-tool retrieval. The dataset is divided into three subsets (I1, I2, and I3), and each subset corresponds to different levels in the RapidAPI Hub tool hierarchy. As the subset number increases from I1 to I3, the tools used are sampled from higher levels of the hierarchy. This means that I3 involves more complex or broadly categorized tools compared to I1 and I2.\nFor all methods used in these experiments, pre-trained encoder transformer models are fine-tuned to each subset of the dataset. The COLT retriever, on the other hand, is a fine-tuned version of Contriever [15], another dense retrieval model based on BERT-base."}, {"title": "5.1.2 Result Analysis", "content": "Table 1 presents the performance comparison. The first two rows show the baseline methods: ToolBench retriever [29] and COLT retriever [30]. The last four rows display our methods. The first two rows represent the first-stage fast retrieval methods: Tool2Vec and MLC. The last two rows present the results for the two-stage methods: ToolRefiner combined with Tool2Vec, where the first stage of retrieval is performed by Tool2Vec, and ToolRefiner combined with MLC, where the first stage of retrieval is performed by MLC.\nThe results are summarized in Table 1. We use Recall@K as the primary evaluation metric, with K values of 3, 5, and 7. nDCG results are provided in subsection A.2. The nDCG values in Table A.4 also exhibit a similar trend to the Recall results presented in Table 1. The results for the ToolBench retriever are reproduced using the original codebase, while the Recall values for COLT are taken from [30], as the codebase is unavailable for reproduction.\nMLC and ToolRefiner consistently outperform the baseline methods by significant margins across all ToolBench subsets. Tool2Vec outperforms the ToolBench retriever across all subsets but falls short of the COLT retriever. Comparing the third and fifth rows in Table 1, ToolRefiner achieves up to 3.8 additional Recall@K across all subsets. For MLC, ToolRefiner shows improvements of up to 2.3 Recall@K for subsets 11 and 12."}, {"title": "5.2 ToolBank", "content": "In this section, we benchmark the methods introduced in section 4 with our new dataset, ToolBank. The results are summarized in Table 2. The baseline is a description based retrieval method. For ToolBank, we observe that our methods outperform the baselines, following similar trends to those in subsection 5.1."}, {"title": "5.2.1 Experimental Details", "content": "The baseline used in this experiment is E5-base model [38], fine-tuned with the description of tools. We conduct an extensive hyperparameter search for the baselines to rigorously evaluate our methods. Similar to subsection 5.1, we fine-tune pre-trained encoder model for MLC, Tool2Vec, and ToolRefiner.\nFor all subsets in this data, we split the training set into an 8:2 ratio for training and validation. We conduct hyperparameter tuning using the validation set and report performance on the test set using the best-performing hyperparameters. We evaluate the test set only once across all experiments."}, {"title": "5.2.2 Result Analysis", "content": "In Table 2, the first row is the result with the description-based baseline. Other rows are results with our methods. The first two rows represent the first-stage fast retrieval methods: Tool2Vec and MLC. The last two rows present the"}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Ablation Studies", "content": "We conduct several ablation studies. First, we study providing ToolRefiner embeddings of tool descriptions, rather than Tool2Vec embeddings. As shown in Table A.1, we find that providing Tool2Vec embeddings significantly improves the performance of ToolRefiner. Additionally, we examine the effect on ToolRefiner performance when varying the number of retrieved tools in the first stage as provided in Table A.2. We observe that the ToolRefiner performance improves at some point but decreases after some point. Furthermore, we analyze the quality of retrieval with Tool2Vec compared to description-based retrieval across a variety of embedding models. We do not perform any fine-tuning of the embedding models and use both open and closed source models of various sizes. As shown in Table A.3, Tool2Vec leads to significant improvement in retrieval quality compared to description-based retrieval across all models. For more details about above studies, please refer to subsection A.1."}, {"title": "6.2 Analysis on How ToolRefiner Improves Performance", "content": "We analyze how ToolRefiner improves the performance compared to first-stage retrieval. We find ToolRefiner excels in handling complex queries and maintains consistent performance across tools, while Tool2Vec struggles more with simpler queries and shows higher error rates on certain tools. We provide more details in Appendix B."}, {"title": "7 Conclusions", "content": "Although function calling allows LLM agents to perform a wider range of tasks beyond their inherent capabilities, as tasks become increasingly complex, the set of tools required can grow vast. This expansion can lead to context window limitations and system overheads caused by long prompts, which can also degrade performance. For this reason, we propose an efficient two-stage tool retrieval system that combines a fast first-stage tool retriever, Tool2Vec and MLC, with a powerful second-stage tool refiner, ToolRefiner. Additionally, domain-specific dataset generation is critical for building specialized tool retrieval applications. Current LLMs, however, can generate high-quality tool retrieval data. To demonstrate this, we create a new tool retrieval dataset, ToolBank with LLMs. Our retrieval strategy achieves over 25% higher Recall than ToolBench's description-based retriever and outperforms description-based retrieval by up to 30% on ToolBank. We look forward to future work building upon our framework, including dataset generation and tool retrieval methods, to streamline tool-augmented LLMs for complex, large-scale tasks."}, {"title": "A Additional Results", "content": "This section details the ablation studies. First, we investigate the effectiveness for ToolRefiner performance of Tool2Vec embeddings compared to tool description embeddings and find that Tool2Vec consistently outperforms tool description embeddings. Then, we explore the impact of the number of initially retrieved candidate tools on overall retriever performance. We observe that increasing the number of candidate tools consistently enhances performance up to a certain point, after which the improvement plateaus and the retrieval metrics degrade. Finally, we compare Tool2Vec-based retrieval and description-based retrieval for various embedding models. We find that Tool2Vec-based retrieval outperforms description-based retrieval consistently over all embedding models."}, {"title": "A.1.1 Tool2Vec vs. Tool Description Embeddings", "content": "In this ablation study, we demonstrate the effectiveness of using Tool2Vec embeddings to the tool description embeddings for ToolRefiner. Specifically, we train two ToolRefiner models, one with Tool2Vec embeddings and one with the tool description embeddings, on ToolBench I3 dataset. We retrieve top-64 tools first by cosine similarites between Tool2Vec embeddings and query embeddings. We observe that ToolRefiner trained with Tool2Vec embeddings outperforms ToolRefiner trained with tool description embeddings across most retrieval settings. Our results are shown in Table A.1."}, {"title": "A.1.2 Analysis on the Impact of the Number of Candidate Tools", "content": "In this set of experiments, we explore the impact of the number of candidate tools on the overall retrieval performance of ToolRefiner. Specifically, for each query, we retrieve the top-N tools either from the output of MLC or from cosine-"}, {"title": "A.1.3 Various Embedding Models", "content": "The experiments performed in section 5 rely on E5-base as an embedding model. To demonstrate the effectiveness of Tool2Vec compared to description-based retrieval, we show the results with other embedding models in Table A.3. Tool2Vec consistently outperforms description-based retrieval across model families and sizes."}, {"title": "A.2 nDCG Results", "content": "We evaluate nDCG@K, where K is 3, 5, and 7 on the ToolBench dataset and compare our method to the baselines. The results are summarized in Table A.4. Our method consistently outperform both the retriever introduced in the ToolBench paper, as well as COLT."}, {"title": "B Detailed Discussion", "content": "In this section, we conduct a series of analyses to investigate why the ToolRefiner combined with Tool2Vec performs better than Tool2Vec across all ToolBank datasets. For simplicity, we will refer to ToolRefiner combined with Tool2Vec as simply ToolRefiner in this section. Particularly, we aim to pinpoint the specific tools that both methods struggle with, quantify the mistakes, and asses how query complexity affects tool retrieval performance. Our results show that ToolRefiner is better able to handle complex queries and maintain consistent performance across a diverse set of tools, while Tool2Vec struggles with simpler queries and makes errors on certain tools more frequently.\nIn our initial analysis, we aimed to identify the tools that ToolRefiner and Tool2Vec most frequently failed to retrieve. Specifically, the model fails to retrieve a tool when the tool is one of the ground truth tools but isn't retrieved. We then divided the number of these failures by the total occurrences of each tool in the dataset to calculate the percentage failure rate for each tool. For all experiments, we retrieved the top-5 tools, which is the maximum number of tools any data point in ToolBank needs to retrieve.\nIn Figure B.1, we show the distribution statistics of the percentage failure rates of all tools in ToolBank subsets. We observe that ToolRefiner demonstrates a more uniform failure rate distribution, with a relatively low mean and standard deviation. It rarely makes more than five errors per tool and averages 2.07 mistakes across all datasets. This suggests that ToolRefiner has a robust understanding of a broad range of tools, managing to maintain relatively low failure rates consistently.\nOn the other hand, Tool2Vec exhibits significant variability in its performance. Certain tools are prone to high failure rates, with some reaching up to 50 mistakes, while others have no errors at all. From Figure B.1, we observe that Tool2Vec's failure rate distribution is highly skewed, meaning that there are some tools that are responsible for a majority of Tool2Vec's errors. Furthermore, on average, Tool2Vec makes 7.86 mistakes per tool, indicating a less consistent performance across the board. This variability might be due to Tool2Vec's handling of tool embeddings, where it fails to adequately differentiate between tools with similar functionalities. In contrast, ToolRefiner effectively separates embeddings of tools with overlapping or similar use cases, which can be closely clustered in the Tool2Vec space. By diversifying these embeddings, ToolRefiner reduces the likelihood of confusion and errors, particularly in complex query scenarios.\nIn our further analysis in Figure B.1, we focus on the length of the queries where the methods fail and discover that ToolRefiner generally makes errors on longer queries, averaging nearly 20 tokens more than those where Tool2Vec failed. This finding implies that while ToolRefiner is equipped to handle more complex and lengthy queries, Tool2Vec tends to struggle with simpler, shorter queries. The ability of ToolRefiner to process longer and potentially more complex queries underscores its enhanced capability to manage intricate or verbose user requests effectively.\nThe disparities in percentage failure rate distribution and the correlation with query length suggest that ToolRefiner's superior performance can primarily be attributed to its refined handling of challenging queries and its robustness across a diverse set of tools."}, {"title": "C Details on Dataset Generation", "content": ""}, {"title": "C.1 Prompt for Data Generation", "content": ""}, {"title": "C.1.1 Query Generation", "content": "The following prompt is used to generate user query for ToolBank.\nYou are an expert in utilizing a library of functions and generating\ndiverse scenarios where a set of selected functions are applied to\nsolve real-world problems. You will be provided with a set of\nfunctions and their descriptions, and will be tasked with selecting\na subset of these functions to craft detailed scenarios. You will\ngenerate clear and detailed user instructions, list the names of\nthe relevant functions, and explain how these functions can be\napplied to complete the task. These tasks should demonstrate a wide\nrange of functionalities and real-life applications to ensure variety\nand utility.\nGuidelines:\nThe instructions must be clear and comprehensive, allowing users\nto understand how to apply the functions without ambiguity. However,\nthe instructions shouldn't be robotic and shouldn't sound like\n'step-by-step' instructions. For example, instead of writing\n''Calculate the non-negative square root of an array element-wise,\nthen round the resulting array to the nearest even value, and return\nthe indices that would sort the array along a specified axis.'' which\nbreaks down each step mechanically, you MUST instead write a more\nnatural and fluid instruction like \u2018\u2018Sort the array along a specified\naxis after calculating the non-negative square root of each element"}, {"title": "C.1.2 Query Polish", "content": "The following prompt is used to polish user query for ToolBank.\nYou are an expert at refining user instructions to make them more coherent\nand less robotic. You will be given a user instruction and will be tasked\nto refine the user instruction if it:\nSounds too robotic or step-by-step like saying 'Do this, do that, and\nthen do this'. In other words, the instructions shouldn't break down each\nstep mechanically but be more fluid. For example, instead of writing\n\"Analyze the lyrics of the song 'XYZ', generate a playlist based on\nthe emotions and themes found, and create a Spotify playlist with\nthe recommended songs.\" you would write \"Create a Spotify playlist based\non the emotions and themes found in the lyrics of the song 'XYZ'.\nHas conditional statements like 'if this, then do that' or 'when this\nhappens, do that'. It should be more direct and non-conditional.\nIf none of the above applies to an instruction, you should mark it as good,\nand provide a reasoning for why it is good. Here example outputs of a JSON\nobject representing a refined user instruction:\n``{in_context_examples}''"}, {"title": "C.2 Tool Selection Criteria", "content": "For tool collection, we crawl each library's official API reference and retrieve detailed information about function descriptions, arguments, and example code snippets. For NumPy, we exclude the numpy.ctypeslib, numpy.dtypes, numpy.emath, numpy.rec, and numpy.version modules since they don't provide rich functions or are outdated. For Pandas, we only use the public sub-packages and exclude the pandas.core, pandas.compat, and pandas.util modules. For Boto3, we include functions for five popular AWS services: EC2, RDS, IAM, S3, and SNS."}, {"title": "C.3 Parameters and LLMs to Generate Dataset", "content": "For dataset generation, we used T = 10 and M = 2 \u2013 5, which means that at each iteration, we sample 10 tools from the tool pool and let the language model choose 2-5 tools to generate instructions. For both the Instruction Generation and Instruction Polish stages, we use Llama-3-70B-Instruct [1]."}, {"title": "C.4 Dataset Statistics", "content": "We collect 511 NumPy tools, 1655 Pandas tools, and 1002 Boto3 tools and curate about 20,000 NumPy queries, 70,000 Pandas queries, and 73,000 AWS queries. There are 19530 tool combinations in our NumPy dataset, 69550 in Pandas dataset, and 70816 in AWS dataset. This means that almost all of our data represents distinct usage scenarios and queries."}, {"title": "C.5 Qualitative Analysis", "content": ""}, {"title": "C.5.1 Comparing ToolBank and ToolBench", "content": "Continuing the discussion on section 3, we present a qualitative comparison between ToolBank and ToolBench. We observe that the format of queries in ToolBench often follows the pattern \"Do this, do this, and do this,\" which results from randomly sampling multiple tools from RapidAPI Hub. This format is somewhat artificial compared to how real human users give queries to LLMs for certain tasks. Additionally, some queries directly or indirectly mention the required APIs, simplifying tool retrieval. In contrast, the queries sampled from ToolBank are more natural, closely resembling how real human users are likely to ask LLMs to perform tasks. Furthermore, the tools required for each user query task in ToolBank are more coherent and related to each other, ensuring better alignment with user needs. For further detail, please refer to Appendix C."}, {"title": "C.5.2 Comparing Polished and Unpolished Queries", "content": "We provide a qualitative analysis of ToolBank and investigate the effect of the Query Polish step. We randomly sample three queries from ToolBank and present them before and after the polishing step in Figure C.2. The left column in Figure C.2 shows the queries before applying the Query Polish step, while the right column shows the queries after polishing. Before applying Query Polish, the queries exhibit a rigid and instructional style, similar to the ToolBench"}, {"title": "C.5.3 Qualitative Analysis on ToolBank", "content": "In this section, we compare NumpyBank, PandasBank, and AWSBank, which are subsets of ToolBank, and provide insights into why the tool retrieval performance on PandasBank is worse than on the other subsets. This performance degradation can largely be attributed to the similarity between operations in PandasBank. Specifically, PandasBank contains various data types like time series, periods, intervals, and indexes. For these data types, there are some common set of operations that apply to all of them such as .equals, .argmin, or .all. This results in instructions that are very close to each other in meaning but use different data types. Hence, the model gets confused about which data type to operate on. For example, the first part of \"Load a delimited data file with specific columns and data types, counting the total number of entries, for the next fiscal quarter starting from the first business day of the year based on a given timestamp.\" query requires a call to `pandas.read_csv` tool which returns a `pandas.DataFrame` object and a subsequent call to `pandas.DataFrame.size` tool to count the total number of entries. However, ToolRefiner + Tool2Vec model makes the mistake of calling `pandas.Series.size` after calling `pandas.read_csv`. This doesn't become an issue with NumpyBank and AWSBank since NumpyBank tools operate almost entirely on the array type and AWSBank contains only the 5 most popular AWS services, creating a clear distinction between tools."}]}