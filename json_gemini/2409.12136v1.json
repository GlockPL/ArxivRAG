{"title": "GRIN: GRadient-INformed MoE", "authors": ["Liyuan Liu", "Yelong Shen", "Young Jin Kim", "Shuohang Wang", "Chen Liang", "Hao Cheng", "Xiaodong Liu", "Masahiro Tanaka", "Xiaoxia Wu", "Wenxiang Hu", "Vishrav Chaudhary", "Zeqi Lin", "Chenruidong Zhang", "Jilong Xue", "Hany Awadalla", "Jianfeng Gao", "Weizhu Chen"], "abstract": "Mixture-of-Experts (MoE) models scale more effectively than dense models due to sparse computation through expert routing, selectively activating only a small subset of expert modules. However, sparse computation challenges traditional training practices, as discrete expert routing hinders standard backpropagation and thus gradient-based optimization, which are the cornerstone of deep learning. To better pursue the scaling power of MoE, we introduce GRIN (GRadient-INformed MoE training), which incorporates sparse gradient estimation for expert routing and configures model parallelism to avoid token dropping. Applying GRIN to autoregressive language modeling, we develop a top-2 16\u00d73.8B MoE model. Our model, with only 6.6B activated parameters, outperforms a 7B dense model and matches the performance of a 14B dense model trained on the same data. Extensive evaluations across diverse tasks demonstrate the potential of GRIN to significantly enhance MoE efficacy, achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.", "sections": [{"title": "1 Introduction", "content": "The success of large-scale pre-training highlights the importance of model scalability (OpenAI, 2023; Touvron et al., 2023). Mixture-of-Experts (MoE) models have emerged as a promising approach, selectively activating only a small subset of modules on specific inputs through an expert routing process, thus improving the model scalability by orders of magnitude (Lepikhin et al., 2021; Fedus et al., 2022; Zoph et al., 2022).\nHowever, the sparse activation mechanism of MoE presents several challenges to model training. For example, while the discrete routing function produces non-differentiable outputs, backpropagation, the cornerstone of deep learning, is exclusively compatible with differentiable functions (Rosenblatt, 1957; Bengio et al., 2013). Consequently, backpropagation cannot be directly applied for gradient computation of expert routing.\nTo fully leverage the scaling potential of MoE, we study gradient estimation for expert routing and configure model parallelism to avoid token dropping in this work. Extending Liu et al. (2023a,b), we propose SparseMixer-v2 to estimate gradient for expert routing, which differs from conventional practices that use the gating gradient as a proxy for the routing gradient. Additionally, we propose a scalable MoE training recipe that uses pipeline"}, {"title": "2 Model Architecture", "content": "Similar to existing state-of-the-art MoE models, GRIN MOE is based on a transformer architecture (Vaswani et al., 2017) with a stack of transformer blocks.\nTransformer. The Transformer network is constructed by stacking Transformer blocks, each consisting of an attention layer and a feedforward layer. Residual connections and layer normalization are applied to all sub-layers in a Pre-LN manner.\nMixture of Experts. Different from conventional Transformer models, we construct the feedforward layer as a Mixture-of-Experts layer, employing a router network to sparsely activate selected networks for each input.\nThe idea of MoE is originally discussed in Jacobs et al. (1991) and Jordan & Jacobs (1994), which integrates separate networks together and uses each to handle a separate subset of training cases. Recently, many attempts have been made to leverage MoE for scaling large language models (Shazeer et al., 2017; Lepikhin et al., 2021; Lewis et al., 2021; Kim et al., 2021; Lepikhin et al., 2021; Fedus et al., 2022; Zoph et al., 2022).\nFor each MoE layer, the model picks from a set of $n$ expert parameters $\\{W_0, ..., W_{n-1}\\}$, the output of one MoE module for inference is\n$\\sum_{i=0}^{n-1} Gating(z)_i \\cdot TopK(z)_i \\cdot Expert(x, w_i)$,\nwhere $z = Router(x, r)$, $r$ is the router parameters, $Gating(\\cdot)$ is a gating function (usually softmax), and $Expert(\\cdot)$ is a FNN. In our study, we define use a linear network as the router, i.e., $Router(x, r) = x \\cdot r^T$ As to $TopK(z)$, it is the TopK function, i.e., $TopK(z)_i := 1$ if $z_i$ is among the TopK coordinates of $z$ and $TopK(z)_i := 0$ otherwise.\nDuring model training, different MoE algorithms may produce different outputs, as we will discuss in detail in Section 3."}, {"title": "3 GRIN MOE", "content": "In this section, we discuss in detail the two key techniques used in GRIN MOE:\nWe propose SparseMixer-v2 to estimate the gradient related to expert routing, while the conventional MoE training treats expert gating as a proxy for the gradient estimation.\nWe scale MoE training with neither expert parallelism nor token dropping, while the conventional MoE training employs expert parallelism and deploys token dropping."}, {"title": "3.1 Gradient Estimation for Expert Routing", "content": "Expert routing produces discrete expert assignment, allowing the network to be sparsely activated and thus bringing the great scaling potential. At the same time, such a routing process is not differentiable, thus making it infeasible to directly apply the vanilla backpropagation and obtain reliable gradient estimation.\nConventional MoE Training. Conventional MoE training uses the router outputs to compute gating results, treating the gating gradients as a proxy of the router gradient. Particularly, as in Equation 1, the output of the MoE module is: $\\sum_{i=0}^{n-1} Gating_i(z) \\cdot TopK(z)_i \\cdot Expert(x, w_i)$, where $z = Router(x,r)$. Conventional MoE training views $TopK(z)_i$ as constants and only back-propagates through $Gating(z)$ to compute router weight gradients. In this way, it treats the gating gradient as a proxy of the router gradient, i.e., $\\nabla_{conventional}r := Gating(z) \\cdot \\frac{\\partial Gating(z)}{\\partial r}$\nSparseMixer-v2. Inspired by recent advances in Straight-Through gradient estimators (Bengio et al., 2013; Liu et al., 2023a), we proposed the SparseMixer-v2 method, an extension of SparseMixer (Liu et al., 2023b), to obtain scalable and reliable gradient estimations in this study.\nWe briefly introduce the SparseMixer-v2 method below, and leave a detailed description to Appendix A. We first replace the $TopK()$ function as random sampling of discrete variables in model training. Then, following Liu et al. (2023a) and Liu et al. (2023b), we apply Heun's third order method to approximate the expert routing gradient and construct a modified back-propagation to give a mathematically sound gradient estimation for expert routing.\nEffectiveness of SparseMixer-v2. In Liu et al. (2023b), the effectiveness of SparseMixer is demonstrated on the neural machine translation task and the ELECTRA language model training. However, it has not been applied to autoregressive language model training at a large scale. In the development of GRIN MOE, we conducted controlled experiments that showd promising results for SparseMixer-v2. The result motivates us to apply this algorithm to training GRIN MOE."}, {"title": "3.2 Implementation and Scaling", "content": "Comparing to conventional models that activate all parameters for all inputs, MoE models have more parameters for the same FLOPs due to their structured sparse computation, significantly impacting computational efficiency. Conventional MoE training distributes different expert networks across devices (i.e., expert parallelism) and employs strategies like token dropping to facilitate the training process.\nAs our first step towards pursuing the scalability brought by MoE, we focus on MoE training with a relative small number of experts (i.e., top2 routing over 16 experts). Leveraging recent engineering advances, we avoid expert parallelism and eliminate the need for capacity factor or token dropping. In the end, we are able to achieve over 80% relative training efficiency improvement, compared to a dense model with the same active parameters, for GRIN MOE training.\nMoE Implementation. For MoE computation without expert parallelism, we find the Megablocks (Gale et al., 2023) package to be very helpful. Particularly, we find its grouped_GEMM kernels and wrappers outperform its sparse version, offering substantial performance improvement. In addition, we rely on data parallelism, pipeline parallelism, and activation checkpointing in the training of GRIN MOE, which lead to the best throughput for our 16\u00d73.8B model.\nTraining Throughput Comparisons of Dense and MoE Models. To showcase the benefits of MoE training, we compare its training throughput to that of a conventional dense model. Hardware details for these studies are in Appendix B. It is important to note that, the throughput of the dense model is measured under the same parallelism setting as that of the MoE model, and the comparison here is to study the GPU kernel efficiency of densely activated networks (i.e., Dense) and sparsely activated networks (i.e., MoE).\nAs summarized in Table 3, we compare MoE models of two different sizes to their corresponding dense models with the same number of parameters, measuring their training throughput using the identical hardware. Despite having over six times as many parameters as the dense model, MoE models achieve more than 80% relative throughput in this experiment, confirming the significant computational scaling potential of models with GRIN MOE method.\nAdditionally, our observations indicate that MoE models do not experience more severe or different throughput degradation compared to dense models when scaling up model size. Both dense and MoE models show similar slowdown patterns in our experiments. For instance,"}, {"title": "Scaling Study and Tensor Parallelism.", "content": "In this section, we discuss whether it is computationally feasible to train MoE models with a larger number of experts without employing expert parallelism.\nFirst, by relying solely on pipeline parallelism, the maximum number of experts can be extended from 16 to 32 by further partitioning different layers across GPUs. However, increasing the number of experts beyond this would result in too many parameters for a single layer, making it difficult to support without partitioning one layer across multiple GPUs.\nTo address this challenge, conventional MoE training relies on expert parallelism to further partition the model, which introduces the side effects of capacity factor and token dropping. In our study, we explore the use of tensor parallelism instead of expert parallelism (Narayanan et al., 2021). Similar to expert parallelism, which has two all-to-all communication overheads in both forward and backward computations, tensor parallelism has two all-reduce communication overheads in forward and backward computations. Although all-reduce operations have higher latency than all-to-all operations, we can mitigate these overheads by overlapping communication with computation during backward computation.\nAs in Figure 3, the maximum number of supported experts is extended to 52 (with 132B total parameters) by combining pipeline parallelism and tensor parallelism (i.e., 3D parallelism). It is worth noting that, since our throughput study hardware setting has only 64 GPUs, we can partition the model into at most 64 stages. With 272 H100 GPUs, the limit of this parallelism can be further extended to 200+ experts.\nWhile this demonstrates the feasibility of scaling MoE training without expert parallelism, we observe that using more complex parallelism typically leads to reduced computational throughput. Correspondingly, one important direction for our future work is to perform MoE training with more experts in a more sparse manner."}, {"title": "3.3 Global Load Balance Loss Adaptations", "content": "As previously discussed, our training framework is designed to scale MoE training using tensor parallelism, pipeline parallelism, and data parallelism, but not expert parallelism. Correspondingly, there is no need to employ token dropping or capacity factor, allowing the distribution of activated experts to deviate from a uniform distribution.\nTherefore, we adapt the load balance loss to regulate the global expert load balance instead of the local one. Particularly, the popular load balancing loss is defined as :\n$\\alpha \\cdot \\eta \\cdot \\sum_{i=1}^{n} f_i \\cdot E[softmax(z)_i]$,\nwhere $\\alpha$ is a hyper parameter, n is the number of experts, and $f_i$ is the fraction of tokens dispatched to expert i (Fedus et al., 2022). Conventionally, $f_i$ is computed at different GPUs locally and thus the load balance loss will regulate the local expert load balance and alleviate token dropping. In our study, we modified the load balance auxiliary loss by computing $f_i$ globally (i.e., all-reduced within the data-parallel process group), regulating the expert load to be balanced globally. Although this adjustment incurs additional communication overheads, similar to tensor parallelism, these communications can be performed in parallel with computations in an asynchronized way, thus largely reducing the additional latency."}, {"title": "4 Experiment", "content": "4.1 Training Setting\nPre-training. GRIN MOE is pre-trained on 4T tokens as a Causal Language Model. The same training dataset has been used to train Phi-3 dense models (Abdin et al., 2024).\nPost-training. Post-training consists of two stages: Supervised Fine-Tuning (SFT) based on the causal language modeling objective, followed by Direct Preference Optimization (DPO; Rafailov et al., 2024). The model is trained with 24B tokens in SFT, using high-quality data across diverse categories, e.g., math, coding and conversation (Abdin et al., 2024). The DPO dataset contains 1.4B tokens, including safety and identity preference data that is used to align the model output with Microsoft's Responsible AI principles (Haider et al., 2024). We further adopt regularization techniques, such as adding random noise to the input embedding (Jain et al., 2024) and applying dropout in expert layers (Fedus et al., 2022), to improve the model's generalization performance. It is worth mentioning that another version of mid-training and post-training have been conducted with an emphasize on long context and multilingual ability, which has been released as Phi-3.5-MoE (Abdin et al., 2024)."}, {"title": "4.2 Evaluation of GRIN MOE", "content": "Table 2 summarizes the performance of GRIN MOE on popular benchmarks. Benchmarks and baseline methods are elaborated in Appendix B.\nSince both Phi-3 and GRIN MOE models are trained on the same datasets, the effectiveness of our MoE training recipe is easily demonstrated. We can see that GRIN MOE with 6.6B activated parameters performs significantly better than 7B dense model and similar to the 14B dense model. Compared to the 14B dense model, GRIN MOE performs better on math, coding, and MMLU tasks.\nComparing GRIN MOE to Phi-3.5-MoE, which has been developed with a different focus (i.e., multilingual capabilities and long context handling), we find that these two models have distinct strengths. We observed that GRIN MOE excels in math and reasoning tasks, while Phi-3.5-MoE demonstrates superior performance in question-answering (QA). Despite their different strengths, both models yield similar average scores across various benchmarks, which is expected given that both are configured as 16x3.8B MoEs and both are trained with sparse backpropagation. Further comparisons are available in Section4.3.\nOur evaluation also shows that GRIN MOE is significantly better than many open-sourced models with a similar number of active parameters, such as Mixtral 8\u00d77B (12.9b activated parameters), Mistral 7B, Gemma 7B, Llama3 8B. And GRIN MOE is better than Mixtral 8\u00d722B on most of the tasks. Nevertheless, GRIN MOE's performance still falls short of Llama3 70B and GPT-40. This gap is expected, given the substantially larger computational and data resources utilized in training these two latter models."}, {"title": "4.3 Math Ability Case Study", "content": "Phi-3 data, the training data of GRIN MOE, contains a massive amount of synthetic data, greatly boosting model performance on benchmarks. Despite its effectiveness, it left doubts on the performance of GRIN MOE on real-world tasks. Correspondingly, we conduct case studies on math questions of the newly released GAOKAO exam (i.e., Nationwide Unified Examination for Admissions to General Universities and Colleges), which is the annual national undergraduate admission exam in China. Known for its rigorous security protocols, this exam serves as an ideal \"truly held-out\" test bed for assessing AI models' ability to answer math questions. Note that the training of GRIN MOE concludes on June 3, Pacific Standard Time, and the 2024 GAOKAO starts on June 7, China Standard Time.\nExam Score. To assess the capability of various models in answering math questions, we used translated questions as the input, scored their responses manually, and visualized the result in Figure 4 (detailed scoring results and the response of GRIN MOE can be find in Appendix C). GRIN MoE scored 46 out of 73 points on these questions, outperforming Llama3 70B by 11 points, and is only 6 and 5 points away from the Gemini Ultra-1.0 and Claude3 Opus, respectively.\nThese results demonstrate GRIN MOE 's strong capacity for mathematical reasoning. As the chance of data leakage in this case study is slim, the result suggests that the capacity of GRIN MOE is likely attributed to the generative distillation approach (Hsieh et al., 2023; Mukherjee et al., 2023), instead of memorization.\nGRIN MOE Responses Discussion. Analyzing the response of GRIN MOE to these questions, we have some interesting observations:"}, {"title": "4.4 Limitations and Weakness", "content": "Since the Phi-3 data, the training corpus of GRIN MOE, is constructed with an emphasize on reasoning and coding abilities, we observe the model to yield a suboptimal performance on natural language tasks. We use the 2024-07-25 release of the LiveBench for model evaluation (White et al., 2024) and summarize the performance of GRIN MOE in Table 4, which also shows the performance of 6 other models that have similar average scores.\nComparing to baselines having similar average score on this benchmark, GRIN MOE achieves better scores on the reasoning, coding, and mathematics. The result is consistent with our case study in Section 4.3. Meanwhile, we observe that GRIN MOE achieves an exceptionally low average score (i.e., 16.9) on natural language tasks. We suspect that this is due to the limitation of the training corpus, since other models trained on the same corpus exhibit similar problems."}, {"title": "5 Analyses", "content": "As described in Section 3, we have tailored the training recipe for GRIN MOE, featuring SparseMixer-v2 and load balance loss adaptation. However, due to resource constraints, we were unable to set up a controlled environment to individually study the impact of each variable at the scale of GRIN MOE. Therefore, we conducted a semi-controlled comparison to quantify the effect of the training recipe."}, {"title": "5.1 Semi-controlled Setting", "content": "We compare the following two training recipes (the Main recipe and the Control recipe):\nMain recipe is the one used for GRIN MOE training, as described in Section 3.\nControl recipe resembles conventional MoE training recipes and is used for comparison. It differs from the main recipe in that the former replaces SparseMixer-v2 used in the main recipe with GShard, exchanges global load balance loss for local load balance loss, and modifies several hyper-parameters.\nWe then trained MoE models using the two recipes on a 4T-token corpus, and compared them with the Phi-3 7B and 14B models trained on a super-set of the 4T-token corpus on downstream tasks without post-training.\nNote that comparing to the controlled experiment in Figure 2, the control recipe does not adapts the global load balance loss as in Section 3.3, while the GShard baseline in Figure 2 adapts the global load balance loss adaptation."}, {"title": "5.2 Downstream Performance", "content": "The results are presented in Table 5. The model trained using the control recipe matches the performance of the 7B dense model. The main recipe is more effective, resulting in a model whose performance matches that of the 14B dense model. We attribute the effectiveness to the use of SparseMixer-v2 and the adaptive loss modifications."}, {"title": "5.3 Routing Analyses", "content": "We analyze the routing distributions of models trained with the main and control recipes. We count how many times each expert (in each layer) is selected by different hidden states as routing distribution.\nMoE routing on pretraining data. To analyze the routing distribution on the pretraining dataset, we randomly select 2 million tokens from the pretraining dataset and visualize the expert loading distribution on them in Figure 5. First, we see that all layers for both recipes have reasonably balanced expert loading. Note that the maximum value in Figure 5 is 0.09, which is about 1.44 times of the perfectly balanced expert loading (0.0625). Then, relatively, we observe that the model trained with the main recipe is less balanced than the control recipe.\nMoE routing on different MMLU tasks. Next, we empirically verify whether different experts contain domain specific information. We first compare the routing distribution among the MMLU 57 tasks. For each task, we sample 24 prompts with 5-shot. The routing distribution for each task is a vector with 16 (experts per layer) \u00d7 32 (layers) dimensions (total number of experts). We then compute cosine similarity between the routing distribution of different tasks and visualize the similarities as a heatmap in Figure 6 (a) and 6 (b), where we group these 57 tasks into 4 categories based on their meta data. Note that the meta data we used here is provided in Hendrycks et al. (2021),\nThe MoE trained with our main recipe is shown in Figure 6 (a), we can see the STEM category has a clear boundary to social_science and humanities. Additionally, it is quite reasonable for the two outliers in social_science and humanities, (i.e., econometrics and formal_logic), to have a higher similarity to the STEM category. This indicates that the routing distribution can vary significantly among different tasks. We also visualize the model trained with the control recipe (as in Figure 6 (b)), in which the routing distributions are similar across different tasks."}, {"title": "A case study of MoE routing.", "content": "This analysis uses the MoE model trained with the main recipe. As shown in Figure 7, the routing distributions are different in different layers. The bottom (shallow) layer has the most balanced expert distribution. In the middle layer, the experts 10 and 15 are selected more other than the other experts. The final layer (deep) comes to be more balanced than the middle layer. These findings reveal that MoE routing distributions are related to such information as context, word, position, etc.\nOur study seems to verify our hypothesis that expert networks in GRIN MoE have developed highly-specialized and heterogeneous expertise. As pointed out in Wei et al. (2024), such experts are likely to improve models' capacity."}, {"title": "6 Conclusion", "content": "In this paper, we describe in detail a new MoE model, known as GRIN MOE, and the model training techniques (i.e., sparse backpropgationa and model parallelism configuration) used to train the model. Compared to dense models trained on the same pretraining corpus, GRIN MoE demonstrates a remarkable scaling potential of MoE. We also provided a summary of our observations and insights gained during GRIN MOE 's development, aiming to deepen our understanding of MoE training. Through controlled and semi-controlled experiments we have demonstrated how gradient estimation methods and model parallelism strategies, along with corresponding auxiliary adaptations, significantly improves model training.\nLooking ahead, many important open questions remain. For example, the training and inference of MoE models present challenges to both algorithms and engineering implementations. Also, since softmax is originally designed to approximate the argmax operation, it presents new challenges to approximate topk as sampling. We plan to further explore solutions to these challenges, with a focus on enhancing sparsity and developing efficient computing and scaling methods to advance state-of-the-art MoE modeling."}]}