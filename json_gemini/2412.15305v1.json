{"title": "Tree-of-Code: A Tree-Structured Exploring Framework for End-to-End Code Generation and Execution in Complex Task Handling", "authors": ["Ziyi Ni", "Yifan Li", "Ning Yang", "Dou Shen", "Pin Lv", "Daxiang Dong"], "abstract": "Solving complex reasoning tasks is a key real-world application of agents. Thanks to the pre-training of Large Language Models (LLMs) on code data, recent approaches like CodeAct successfully use code as LLM agents' action, achieving good results. However, CodeAct greedily generates the next action's code block by relying on fragmented thoughts, resulting in inconsistency and instability. Moreover, Code-Act lacks action-related ground-truth (GT), making its supervision signals and termination conditions questionable in multi-turn interactions. To address these issues, we first introduce a simple yet effective end-to-end code generation paradigm, CodeProgram, which leverages code's systematic logic to align with global reasoning and enable cohesive problem-solving. Then, we propose Tree-of-Code (ToC), which self-grows CodeProgram nodes based on the executable nature of the code and enables self-supervision in a GT-free scenario. Experimental results on two datasets using ten popular zero-shot LLMs show ToC remarkably boosts accuracy by nearly 20% over CodeAct with less than 1/4 turns. Several LLMs even perform better on one-turn CodeProgram than on multi-turn CodeAct. To further investigate the trade-off between efficacy and efficiency, we test different ToC tree sizes and exploration mechanisms. We also highlight the potential of ToC's end-to-end data generation for supervised and reinforced fine-tuning.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) significantly improve agents' ability to leverage external tools. (Chen et al., 2023b; Hong et al., 2023; Paul, 2024). Effectively and efficiently handling complex real-world problems (Blount and Clarke, 1994), especially those requiring multiple tools and calls (Li et al., 2023b; Wang et al., 2024), has become a key focus across industry and academia. Currently, the widely used paradigm, ReAct, (Yao et al., 2022),\ncombines reasoning with action strategies, allowing for actions to be performed incrementally and adjusted based on environmental feedback.\nThe application of code generation techniques to complex task planning and execution has garnered significant attention (Holt et al., 2024; Wen et al., 2024b; Xu et al., 2024), particularly with the emergence of CodeAct (Wang et al., 2024) approaches. CodeAct moves the interaction unit from ReAct's individual tool calls to generating code blocks with local reasoning, while uniquely using code logic and libraries. Rather than JSON (Qin et al., 2023) or text (Park et al., 2023), it treats code as action, utilizing LLM's pre-trained coding skills for efficient handling of complex tasks.\nHowever, each turn of CodeAct is based on individual actions rather than the entire program. It can not autonomously reason and generate complete code in one turn. Instead, it follows a step-by-step generation and interaction process. This is akin to the brain's control of motor actions, processing tasks iteratively and incrementally, with the basal ganglia supporting step-granularity execution and the cerebellum refining and sequencing for the complete complex motor task (Baladron et al., 2023).\nOn the one hand, the fragmented and stalled thinking in code can hinder a thorough understanding of the logical chains embedded within it, potentially leading to redundant code (Wang et al., 2023; Guo et al., 2024). Repeatedly integrating all prior thoughts causes context overload due to long histories, making it more prone to accumulating significant model hallucinations (Ji et al., 2023).\nOn the other hand, solving complex problems can have multiple solutions (Mialon et al., 2023), such as calling tools in different sequences. LLMs also tend to explore randomly, resulting in varied solutions, making it difficult to set a standard answer for each step. Long action sequences over multiple turns only provide sparse rewards (Xu et al., 2023). When using these trajectories as train-"}, {"title": "Design Motivation", "content": "In industry, complex tasks requiring multiple tools and function calls, are typically driven by open-ended user queries. This creates two key challenges: (1) For zero-shot queries, it is challenging to pre-obtain task-level ground-truth (GT). Manual annotation or assigning different rewards to responses is required for subsequent SFT (Chung et al., 2024) or reinforced fine-tuning (ReFT) (Luong et al., 2024). Moreover, without GT, the termination criteria become unclear. (2) Multi-turn interactions lack a fixed trajectory, making it difficult to define the process supervised signals (Luo et al., 2024). Current methods often rely on a 'judge' model to evaluate whether the user's needs from the task query are met at each step (Chen et al., 2024; Li et al., 2024a). However, each evaluation demands strong analytical and reasoning skills from the model, making it costly and time-consuming. Existing methods deliberately avoid these challenges by assuming GT is known, matching task-level GT with action-related outcomes at each step. The process stops if they match, or continues until the step limit is reached. The tool agent in Figure 1 illustrates this.\nWe aim to explore whether it is possible to develop a method that can self-provide supervision and termination criteria at each step while approximating GT in a GT-free scenario.\nUnlike cerebellum-controlled step-by-step motor tasks, we learn from the DPC to treat each turn as a complete task. By iteratively approaching the feasible region, we collect a batch of feasible solu-"}, {"title": "CodeProgram", "content": "A block of code in CodeAct's actions corresponds to the eyes or mouths of the smiley face in Figure 1, when faces represent complex tasks. We propose CodeProgram, which can draw the complete face in one turn. Figure 2 illustrates how it works. Specifically, code serves as a bridge, aligning with natural language reasoning and connecting it to execution outcomes in the environment. By decoupling the reasoning process from code execution, we achieve flexibility while ensuring consistency."}, {"title": "Code as Reasoning", "content": "Code generation plays a crucial role in the concept of \"code-as-reasoning,\" where the process of writing code itself reflects a reasoning process.\nGlobal reasoning is required to guide complete code generation in a single end-to-end process. This enables the seamless integration of various reasoning methods for large language models (LLMs), such as prompt engineering (Chen et al., 2023a), Chain-of-Thoughts (CoT) (Wei et al., 2022), Tree-of Thoughts (ToT) (Yao et al., 2024a), in-context learning (Kojima et al., 2022), self-reflection (Zhang et al., 2024), and System2 reasoning (Frankish, 2010; OpenAI, 2024b; Yao et al., 2024b). Furthermore, longer chains of thought have consistently been shown to improve task performance (Wei et al., 2022; Zelikman et al., 2024).\nBuilding on this foundation of global reasoning, we write the root prompt based on previous\nwork (Wang et al., 2024) to guide the generation of step-by-step CoT thoughts and the corresponding complete code. LLMs are prompted to first analyze and break down the problem, generate reasoning-based thoughts for solving it, and then produce the complete code that reflects and executes that reasoning. The thoughts and codes are enclosed using the \"<thought>-</thought>\" and \"<execute>-</execute>\" tags, respectively. The root prompt is shown in Appendix A."}, {"title": "Two Helper Tools", "content": "CodeProgram struggles when LLMs must rely on tool outputs to determine the next steps. For example, we can only provide the final summary answer based on all tool outputs; in web browsing tasks, the next action is determined only after the page content is viewed. To maintain end-to-end flow, we introduce two additional functions that call the LLM into our code: a general res_handler, which defines a prompt to generate results that meet the prompt requirements for final summarization, and a specific next_action for web tasks, which decides the next action from a given set of possible browsing actions based on the page content, visited URLs, and task query. Their tool descriptions and functions are shown in Appendix B. They help better understand the semantic relationships between tools, ensuring a smooth and cohesive sequence of tool calls during code generation."}, {"title": "Execution Outcome can be Label", "content": "The code solution is task-level, and its execution outcome is a self-provided annotation that can be directly used as labels. Specifically, it includes supervision signals to select the \"successfully executed\" samples for SFT, and various rich comments (such as specific results or error messages) that can be quantified as rewards for ReFT, as long as we repeat the CodeProgram in different settings multiple times to receive feedback. In this way, the code acts as a verifier. This idea inspires us to build a single-layer multi-node Tree-of-Code (ToC). Thanks to the task-level granularity, the code's execution outcomes align with the task query and the thought-code output, facilitating the generation of valuable training data, as the thoughts, code, and execution-based labels are tightly synchronized.\nIn summary, CodeProgram is an annotation-free, end-to-end generation approach well-suited for producing large-scale training data, increasing efficiency, and improving task-solving performance."}, {"title": "Tree-of-Code Method", "content": "Following the design motivation in Figure 1, we need to collect all successfully executed solutions and identify the one closest to the GT. CodeProgram introduced in Section 3 has allowed us to achieve two key goals: (1) to directly reflect on and refine the task-level code, and (2) to use its execution results as terminal criteria. We now propose an execution-based, self-growing, and self-filtering tree, with CodeProgram as its nodes."}, {"title": "Overview of Tree-of-Code", "content": "We represent the ToC framework as $T = (N, S)$, where $N$ denotes a set of nodes (N), and $S$ represents the stems (unidirectional arrows in Figure 3), modeling the reflection reasoning process of LLMs when expanding the nodes. The overview of ToC and how it works is illustrated in Figure 3. Let $L$ denote the max depth, $I$ the layer index, $M$ the expanded layer's max width, $m$ the node index, $l \\in \\{1, ..., L\\}$, $m \\in \\{1, ..., M\\}$. We use $T$ for the thoughts of the $N$, $C$ for code, and $E$ for its execution result. The next-layer $N$ is denoted as:\n$N_{(l+1)-m} = S_{t\\rightarrow(l+1)} (f, \\sum_{j=0}^{l} (T_{j-m} + C_{j-m} + E_{j-m}))$\nwhere $f$ represents the basic information of the task, such as the user's query, and all tool descriptions. The sum $\\sum_{j=0}^{l}$ indicates that each reflection\nreasoning process for generating the next node relies on the thoughts, code, and execution results from all ancestor nodes in the history. The node index is fixed for simplicity in the formula."}, {"title": "Tree Expansion", "content": "We initialize from a root node and recursively expand the tree. The expansion process follows: (1) The breadth-first search (BFS) strategy is applied, with each parent node branching into M child nodes. (2) Whether the node continues to grow depends solely on the evaluation of its own execution state (success or failure). For each $N_l$,\n$\\begin{cases}\nstop \\& collect, \\text{ if } E_l \\neq None \\text{ or error},\\\\grow \\& N_{(l+1)},\\text{ otherwise}.\n\\end{cases}$\n(3) Expansion continues until all child nodes stop or the maximum depth (L) of 3 is reached.\nExecution-based Reflection. We can not guarantee that the end-to-end code solution will be correct on the first attempt. Treating task-level execution errors as continuation signals, we propose execution-based reflection, which enables LLMs to self-reflect, identify errors, refine thoughts, and improve code through prompting, significantly en-hancing problem-solving. The prompt for reflection is shown in Appendix A.2.1.\nAs long as execution fails, self-reflection continues iteratively, generating new nodes. This al-lows the branch to grow as a data sample, with"}, {"title": "Final Result Generator", "content": "Once valid outputs from successfully executed nodes are collected, the same LLM makes the final decision by performing a majority vote and summarization to determine the most likely answer - this corresponds to the green node in Figure 3. Other valid responses (red nodes) are discarded."}, {"title": "Experiment and Analysis", "content": "Datasets. Following CodeAct, our evaluation is based on M3ToolEval\u00b9 (M3) (Wang et al., 2024)\nand the test set of API-Bank\u00b2 (Li et al., 2023b). M3"}, {"title": "One-turn vs. Multi-turn", "content": "CodeProgram enables global planning and complete solutions in a single turn by leveraging code's ability to handle long logic chains, aligning with global reasoning in language, and defining clear tool inputs and outputs. With a significant advantage in the number of turn, Table 1 and Figure 6 demonstrate that the performance of some models even surpasses the multi-turn CodeAct, particularly for the Claude series models. We grow CodeProgram into a single-layer, three-node (1-3) Tree-of-Code (ToC). The prompt for each node is randomly sampled from our prompt pool. Compared to CodeProgram, the simple 1-3 tree-structured ToC with random prompts significantly boosts performance. The average performance of 1-3 ToC already surpasses CodeAct, highlighting the power of prompt randomness. Upon reviewing the generated code, we observe that LLMs often produce modular code for each step or add comments before modules, even when not explicitly required."}, {"title": "ToC vs. CodeAct and ReAct", "content": "We primarily compare the ToC framework, which is comprised of CodeProgram nodes, with the CodeAct and ReAct framework, which are comprised of steps, using the M3 and the level-3 datasets. For ToC, we randomly sample the LLM and prompt from the LLM list and prompt pool, respectively, at each node exploration. For CodeAct and ReAct, we report the average results across all models used in this paper. Table 2 shows that ToC consistently achieves superior performance and demonstrates a significant advantage with fewer interaction steps, highlighting its efficiency in managing complex tool-use scenarios."}, {"title": "Analysis and Ablation Studies", "content": "Varying tree sizes. We test the performance of the top model, claude-3-5-sonnet, on different tree sizes to evaluate the trade-off between efficacy"}, {"title": "Discussion", "content": "Why we do not try a search tree? We initially explored using evolutionary algorithms to merge nodes from different branches for the next generation, aiming to reduce the search space. However, after extensive testing, we found this approach impractical and hard to implement. We analyzed error cases from ToC and found that a single branch can easily fall into specific errors. Since different branches follow distinct reasoning paths, they struggle to learn from each other. Even with reflection and random exploration, LLMs are prone to early-stage errors that disrupt subsequent reasoning (An et al., 2023; Bao et al., 2024; Tong et al., 2024).\nThis issue mirrors the human cognitive challenge of breaking out of a \"bistable state\" \u2014 like the famous \"duck-rabbit illusion\" or Rubin's vase (Hancock, 2013). In such states, a person may generate multiple parallel thoughts, but once one is chosen, it is difficult to switch to another without external intervention (Andreev et al., 2020). This highlights the need for introducing random exploration and multiple nodes at each growth step, as these strategies help overcome cognitive bottlenecks."}, {"title": "Related Work", "content": "LLM Code Generation. Chain of Codes framework (Li et al., 2023a) expands the range of reasoning tasks that LLMs can solve by \"thinking in code.\" Similarly, CodePlan (Wen et al., 2024a) utilizes pseudocode to guide structured reasoning. Additionally, the Structured Chain-of-Thought Prompting (SCoT) technique (Esfahani et al., 2024) has highlighted the potential of structured prompts in this domain. Recent works combining code with agents have primarily focused on task completion within programming-related domains, such as software development (Qian et al., 2024; Wang et al., 2023), programming assistance (Islam et al., 2024; Wen et al., 2024c), and scientific problems (Chen et al., 2022; Gao et al., 2023; Hong et al., 2024). Few methods (Wang et al., 2024) treat code as a scalable language format to call multiple tools for solving complex real-world tasks.\nRecently, we found a new work, CodeTree (Li et al., 2024b), which uses a tree structure to explore the search space of code generation tasks. Unlike our approach, it focuses on multi-agent searching rather than an end-to-end self-growing tree. Additionally, it was released three months later than the initial submission of our work."}, {"title": "Conclusion", "content": "In this paper, we introduced the Tree-of-Code (TOC) method, which combines execution-based reflection with end-to-end thought-code generation for complex task handling. With efficient model integration and prompt exploration, ToC significantly outperformed the baselines on two complex task datasets, boosting both efficiency and task-solving capabilities. The ToC framework opens up exciting possibilities for advancing human-like global cognition, inspiring further exploration of tree structures in end-to-end code generation, particularly for complex multi-tool tasks, data generation, and reinforced fine-tuning."}, {"title": "Limitations", "content": "Additional engineering effort\nWe efficiently transform multi-turn interactions into a single-turn complete program comprising a series of tool calls. However, this approach demands more detailed usage instructions for the tools. For actions with limited semantic information (e.g., webpage clicking and scrolling), an artificially constrained action exploration space is also necessary. Consequently, additional engineering efforts are required to fully implement our end-to-end code generation approach.\nLimited reasoning scope for Program\nWe emphasize that our method operates at the granularity of code \"program\" rather than \"action\". However, it is limited in fully open-ended scenarios requiring step-by-step exploration, such as a robot navigating an unfamiliar environment, or in handling tasks with extremely long sequences beyond the capabilities of current reasoning methods, like generating an entire paper. In such cases, it cannot provide a complete final solution. For larger and more complex system programs in the future, our method may serve as a \"subprogram\" within the overall solution, similar to a single agent's role in multi-agent systems.\nOpportunities for Reflection Refinement\nWhile our framework provides a solid foundation inspired by human problem-solving, it uses a basic reflection mechanism, relying on execution feed-back alone. Whether tracking full execution history or selectively summarizing with LLMs offers better performance remains an open question. Future research could explore enhanced search strategies or adaptive pruning methods to handle more complex real-world tasks.\nVast Potential in Prompt Pool Design\nWe enhanced the diversity of strategies and the robustness of results in our Tree-of-Code by designing a prompt pool composed of multiple prompts. The introduction of multiple reasoning paths guided by diverse prompts represents a significant innovation. However, our current approach relies primarily on simple prompt evolution and manual adjustments. Future work should focus on more in-depth and systematic research into constructing prompt pools."}, {"title": "Prompt", "content": "A.1 Root Prompt\nYou are a helpful assistant assigned with the task of problem-solving.\nTo achieve this, you will be using an interactive coding environment equipped with a variety of tool\nfunctions to assist you throughout the process.\n\nAt each turn, you should first provide your step-by-step thinking for solving the task, for example: <\nthought I need to print \"hello world!\"</thought >.\nAfter that, you can Interact with a Python programming environment and receive the corresponding output.\nYour code should be enclosed using \"<execute >\" tag, for example: <execute> print(\"Hello World!\") </\nexecute >.\n\nYou can use the following functions:\n{toolset_descs}\n.\nEnsure the code matches the fn_signature and input-output formats for proper execution.\n\nHere's the chat history for your reference:\n{chat_history}\n\n\nHistory End : \nUser's Query:\n{query}\nYour Thought And Code : \n\nA.2 Additional Prompt\nA.2.1 Reflection Prompt\nBased on the provided chat history, reflect on the code and its execution. Identify potential issues or\nareas for optimization and provide specific suggestions to refine and improve the code. Consider\nedge cases, efficiency, and clarity in your reflections.\n\nA.2.2 The Prompt for Prompt Evolution\nIn order to guide the diversity of results and enhance the performance through ensemble methods, we need\nto increase the diversity of prompts. We diversify the current prompt while maintaining consistency\nin core content, aiming for orthogonal expressions or prompts that lead to different directions and\ndivergent thinking.\n\nA.2.3 The Prompt Sample from Prompt Pool for API-Bank\nNote:\nThe outputs produced by the tool will be formatted like a JSON dictionary.\nFor example, 'result = {{'api_name': 'QueryMeeting', 'input': {{'user_name': 'John'}}, 'output': {{'\nmeetings': [{{'meeting_id': 1, 'meeting_name': 'Meeting with the client', 'meeting_time':\n'2021-01-01 10:00:00', 'meeting_location': 'Room 1', 'meeting_attendees': ['John', 'Mary', 'Peter\n']}}, {{'meeting_id': 2, 'meeting_name': 'Meeting about the new project', 'meeting_time':\n'2021-01-02 10:00:00', 'meeting_location': 'Room 2', 'meeting_attendees': ['John', 'Mary', 'Peter\n']}}]}}, 'exception': None }}'\nEnsure that the code strictly adheres to the function descriptions and the input-output format provided.\nNavigate through the 'output' key correctly to retrieve results.\nIf you encounter any unfamiliar formats, first print the structure to ensure proper handling in the\nfuture.\nConsistently focus on the user's request and attempt to produce the complete solution without needing\nmultiple steps."}, {"title": "Helper tools", "content": "B.1 ResHandler\nB.1.1 ResHandler Tool Description\nres_handler():\nname=\"res_handler\",\ndescription='Define a prompt to generate erate results that meet the prompt requirements. Note that you\nneed to define the requirements for the generated results in the prompt. input: prompt (str):\nThe input prompt for the large language model, defining the task requirements for the generated\nresults. Common tasks include summarization, stylistic writing, translation, question answering,\netc. output: completion (str): The inference result generated by the large model, typically a\nsummary, writing output, translation result, or answer that meets the requirements.',\nfunction=res_handler,\nfn_signature = 'res_handler(prompt: str) -> str')"}, {"title": "ResHandler Tool Function", "content": "from some_model_API import Ilm_playground\ndef res_handler(prompt):\nresult_str = \"\"\nresult = 1lm_playground (prompt[:20000], stream=False)\nfor item in result:\nresult_str += item\nreturn result_str"}, {"title": "NextAction for Web Task", "content": "B.2.1 NextAction Tool Description\nfrom typing import Tuple\nnext_action():\nname=\"next_action\",\ndescription = 'Examine the results of the view function to determine if it can answer the user's\noriginal question, and decide what to do next. Return the next action and the viewed whole page\ncontent. The next possible actions include click_url(URL), go_to_previous_page() and end(), which\nrepresent clicking a link, and go_to_previous_page() means you should go to previous page to\nfind answer, and end() means you have found the answer page, respectively. If next action is end\n(), it means that relevant information to user query is found, you should summarize string\nresult based on res_handler click_url(URL), go_to_previous_page() can be directly called, and\nURL should be Clickable url. Note that query mery should be user's original question and can not be\nrewritten.',\nfunction=next_action,\nfn_signature=\"next_action (query: str, current_page_content: str, visited_urls: List[str]) -> Tuple [\nstr, str])\"\nB.2.2 NextAction Tool Description\nfrom some_model_API import Ilm_playground\ndef next_action (query=\"\", current_page_content=\"\", visited_urls = []):\nvisited_urls = [x.replace('\\'', '').replace('\"', '') for x in visited_urls]\nvisited_urls = list(set(visited_urls))\nwhole_page_content = current_page_content\nwhile True:\nscroll_down_page = scroll_down ()\nif scroll_down_page == \"[Reached the bottom of the page.]\\n\":\nbreak\nelse:\nwhole_page_content += scroll_down_page\ndef extract_clickable_paths (text: str) -> list [str]:\nimport re\npattern = r\"Clickable '([^']*)'\"\nmatches = re.findall (pattern, text)\nreturn matches\nall_urls = extract_clickable_paths (whole_page_content)\nnot_visited = []\nhighlight_urls = []\nfor v in all_urls:\nif v in visited_urls:\nelse:\nhighlight_urls.append(v)\nnot_visited.append(v)\nif len(highlight_urls) == 0:\njson_str_format = \"<thought>your thought of your decision </thought >\\n<action>click_url(\nspecific_url) or end() or not_found () </action >\"\nprompt = f\"You are viewing page contents, the content is: \\n{whole_page_content}\\n You should\nmake decision on the next step. given user query {query}, you have the following options,\nplease follow the output format. \\n1. end(): it means current user query can be answered by\ncurrent page content. \\n2. click_url (URL): it means current user query should be checked by\nclicking one of the urls shown on the current page content for more details. specify the"}, {"title": null, "content": "detailed url into URL field.\\nPlease visit any Clickable urls as many as possible that has\nnot been visited. \\n3. not_found(): it means that current page does not contain answer for\ncurrent query and all Clickable URLS have been clicked. \\nYour output format: {\njson_str_format}\\n\\nYour Output:\\n\"\nvisited_url_str = ', '.join(['\\'' + x + '\\'' for x in highlight_urls])\njson_str_format = \"<thought>your thought of your decision </thought >\\n<action>click_url(\nspecific_url) or end() or not_found () </action >\"\nprompt f\"You are viewing page contents, the content is: \\n{whole_page_content}\\n You should\nmake decision on the next step. given user query {query}, you have the following options,\nplease follow the output format. \\n1. end(): it means current user query can be answered by\ncurrent page content. \\n2. click_url(URL): it means current user query should be checked by\nclicking one of the urls shown on the current page content for more details specify the\ndetailed url into URL field.\\n3. not_found(): it means that current page does not contain\nanswer for current query and all Clickable URLS have been clicked. \\nRemember that you have\nvisited the url list [{ visited_url_str}]. You are not allowed to visit the urls you have\nvisited. Please visit any Clickable urls as many as possible that has not been visited.\\n\nYour output format: {json_str_format}\\n\\nYour Output:\\n\"\nresult_str = \"\"\nresult = Ilm_playground (prompt[:20000])\nfor item in result:\nresult_str += item\nif not \"Clickable\" in whole_page_content and not \"end()\" in result_str:\nreturn (\"go_to_previous_page()\", whole_page_content)\nif not \"end()\" in result_str and len (not_visited) == 0:\nreturn (\"go_to_previous_page()\", whole_page_content)\nif \"click_url\" in result_str:\nimport re\npattern = r\"click_url\\('.*'\\)\"\nmatch research (pattern, result_str)\nif match:\nelse:\nreturn (match.group(), whole_page_content)\npattern = r\"click_url\\(.*\\)\"\nmatch research (pattern, result_str)\nif match:\nreturn (match.group(), whole_page_content)\nelif \"end()\" in result_str:\nreturn (\"end()\", whole_page_content)\nelif \"not_found()\" in result_str:\nreturn (\"go_to_previous_page()\", whole_page_content)\nreturn (\"end()\", whole_page_content)"}]}