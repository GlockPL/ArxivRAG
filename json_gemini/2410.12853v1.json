{"title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks", "authors": ["Mahmood Hegazy"], "abstract": "Large language models (LLMs) excel in natural language generation but often confidently produce incorrect responses, especially in tasks like mathematical reasoning. Chain-of-thought prompting, self-verification, and multi-agent debate are among the strategies proposed to improve the reasoning and factual accuracy of LLMs. Building on Du et al.'s multi-agent debate framework, we find that multi-agent debate helps at any model scale, and that diversity of thought elicits stronger reasoning in debating LLMs. Across various model sizes, performance on mathematical reasoning tasks benefits most when diverse trained models are used. Remarkably, after 4 rounds of debate, a diverse set of medium-capacity models (Gemini-Pro, Mixtral 7B\u00d78, and PaLM 2-M) outperforms GPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3 instances of Gemini-Pro are used, performance only reaches 82%. Finally, this diverse set of medium-capacity models sets a new state-of-the-art performance on the ASDiv benchmark (94%). These results underscore the idea that the future of AI is agentic, with diverse cooperating agents yielding emergent capabilities beyond even the most powerful individual models.", "sections": [{"title": "1 Introduction", "content": "In the dynamic realm of artificial intelligence, enhancing the reasoning abilities and factual accuracy of large language models (LLMs) stands as a pivotal challenge. Central to this pursuit is the exploration of innovative methodologies that address existing shortcomings and chart new pathways for advancement. Our research aims to fortify the foundations of LLMs through the lens of multi-agent debate.\nThe key motivation behind this project is solving the issue of \"hallucination\" within language models, where plausible yet erroneous information is generated, undermining their reliability and trustworthiness. Inspired by the collaborative nature of human intellectual discourse, the methodology of multi-agent debate emerges as a promising solution to this problem. By harnessing the collective insights of multiple AI agents engaged in structured debate, we seek to not only mitigate hallucinations but also elevate the precision and reliability of LLM responses.\nA glance at the landscape of current research reveals a series of endeavors aimed at fortifying the reasoning capabilities of LLMs. While recent iterations of LLMs, such as GPT-4, represent significant strides forward, concerns persist regarding their reasoning capabilities. In response to these limitations, Du et al. [2023] advocates the transformative potential of multi-agent debate. Furthermore, advancements in agentic approaches such as MetaGPT, as proposed by Hong et al. [2023], and Agentverse, as proposed by Chen et al. [2023], offer diverse perspectives, delving into collaborative problem-solving and simulation of human behavior, thus broadening the horizons of LLM research."}, {"title": "2 Related work", "content": "The landscape of large language models (LLMs) has witnessed remarkable advancements in recent years, exemplified by innovations such as GPT-4 (OpenAI [2023]), Llama (Touvron et al. [2023]), and PaLM (Chowdhery et al. [2023]). However, despite these breakthroughs, a critical examination reveals significant concerns regarding the reasoning capabilities of LLMs, as highlighted by Arkoudas [2023]. This recognition has motivated a series of research efforts aimed at enhancing the reasoning and problem-solving abilities of LLMs through various methodologies.\nApproaches like chain-of-thought prompting, self-verification, and multi-agent debate, as introduced by Wei et al. [2022], Weng et al. [2023] and Du et al. [2023] respectively, have been proposed to address this challenge. The multi-agent debate approach, inspired by Minsky [1988]'s \"Society of Mind\" theory, posits that diverse agents approaching a problem with different methods, purposes, knowledge representations, and result-production techniques can enhance factual accuracy through debate and communication. Similarly, Hong et al. [2023] introduced MetaGPT, a meta-programming framework designed to tackle logic inconsistencies and hallucination by incorporating Standardized Operating Procedures (SOPs) and structured communication within LLM-based multi-agent systems.\nIn parallel, efforts have been directed towards enhancing the generative capabilities of LLMs to simulate believable human behavior, as studied by Park et al. [2023]; Wang et al. [2023]; Yao et al. [2022]. These endeavors, while successful in creating realistic simulations, have also prompted further exploration into refining retrieval modules and real-time interactivity to mitigate instances of hallucination.\nFurthermore, frameworks such as Agentverse, as proposed by Chen et al. [2023], prioritize collab-orative problem-solving among autonomous agents, emulating human group dynamics to achieve superior performance across diverse tasks. This emphasis on collaborative reasoning sets a precedent"}, {"title": "3 Methodology", "content": "Our multi-agent debate framework for enhancing the mathematical reasoning capabilities of LLMs broadened the scope of the framework, as introduced by Du et al. [2023], to ensure it was compatible with diverse models architectures. As illustrated in Figure 3, it consists of the following key components:\n1. Question Encoding: The mathematical question or problem is provided as input to the system. This question serves as the starting point for the debate among the participating models.\n2. Debating Models: Three diverse language models - Model 1, Model 2, and Model 3 - are employed as the debating agents. These models can be chosen to have different architectures. We utilized this architecture to run experiments with and without diverse models.\n3. Debate Rounds: The debating models engage in multiple rounds of debate, where each model generates a response to the question based on its own reasoning capabilities. In each round, the models take turns providing their responses.\n4. Response Summarization: After each round, the responses from all three debating models are passed through a fourth model - Model 4 (Response Summarizer). This model's role is to analyze and summarize the key arguments, reasoning steps, and conclusions presented by the debating models. The summarized response captures the most salient and convincing points from the debate round. Our model of choice for response summarization was Gemini-Pro for all experiments.\n5. Iterative Refinement: The summarized response from Model 4 is then fed back as input to the debating models for the next round of debate. This iterative process allows the models to build upon each other's reasoning, identify and correct errors, and refine their arguments based on the collective insights generated in the previous rounds.\n6. Final Summary: After a predefined number of debate rounds (n), the final summarized response from the summarizer model (Model 4) is considered as the output of the multi-agent debate framework. This final summary represents the consolidated reasoning and conclusion arrived at through the iterative debate process. Here we can extract what the mode of the answers of the 3 models was."}, {"title": "3.1 Datasets", "content": "To empirically evaluate the effectiveness of our multi-agent debate framework, we employ a diverse set of benchmarks that assess various aspects of language understanding and mathematical reasoning.\n\u2022 GSM-8K (Cobbe et al. [2021]): This benchmark comprises 8.5K linguistically diverse grade school math word problems, making it ideal for evaluating multi-step mathematical reasoning.\n\u2022 Academia Sinica Diverse MWP Dataset (ASDiv) (Miao et al. [2021]): ASDiv features 2,306 diverse math word problems covering various language patterns and problem types encountered in elementary school. It includes annotations for problem type and grade level.\n\u2022 MATH (Enderton [2001]): This historically challenging dataset provides 12,500 competition mathematics problems, each accompanied by step-by-step solutions. It facilitates the teaching of answer derivations and explanations.\nThrough extensive experiments, we analyze the impact of model diversity, debate round count, and model size on reasoning performance. The results provide insights into the optimal configuration of the multi-agent debate framework for achieving superior mathematical reasoning capabilities compared to individual models.\nBy leveraging these diverse datasets, we aim to comprehensively assess the effectiveness of our approach in enhancing the reasoning capabilities of LLMs across a wide range of problem types, difficulty levels, and language patterns. This rigorous evaluation enables us to draw meaningful conclusions about the potential of multi-agent debate in advancing the state-of-the-art in language understanding and mathematical reasoning."}, {"title": "4 Experiments", "content": "To validate the effectiveness of our multi-agent debate framework, we conducted a series of ex-periments using diverse and homogeneous sets of language models with varying capacities. These experiments were performed on multiple mathematical reasoning benchmarks, as outlined in sec-"}, {"title": "4.1 Effect of model capacity on performance", "content": "While previous work by Du et al. [2023] explored the effectiveness of multi-agent debate by varying the number of agents and the number of debate rounds, they did not investigate the effect of model scale on the framework's performance. Considering the findings of Wei et al. [2022], which demon-strated that chain-of-thought (COT) reasoning is an emergent ability that arises with increased model size, we recognized the importance of conducting a similar experiment for multi-agent debate.\nTo address this gap in the literature, we performed a scaling experiment on the GSM-8K dataset to examine the relationship between model capacity and the performance of the multi-agent debate framework. We evaluated the framework's performance with and without COT reasoning across a range of model sizes, from small-scale models to large-scale ones. The results of our experiment, as presented in Figure 5, revealed a surprising finding. Contrary to the expectation that multi-agent debate performance would emerge as a direct consequence of increasing model size, we observed similar performance gains across all model scales. This result suggests that the effectiveness of multi-agent debate in enhancing reasoning capabilities is not solely dependent on the model's capacity.\nOur findings have significant implications for the development and deployment of multi-agent debate systems. They indicate that even smaller-scale models can benefit from the collaborative reasoning process facilitated by the debate framework, without the need for resource-intensive large-scale models. This insight opens up new possibilities for implementing multi-agent debate in resource-constrained environments and facilitates the widespread adoption of this approach.\nFurthermore, our experiment highlights the importance of considering factors beyond model size when designing and optimizing multi-agent debate systems. It suggests that the effectiveness of the debate process may be influenced by other aspects, such as the diversity of the participating models, the structure of the debate, and the quality of the response summarization."}, {"title": "4.2 Diversity of thought", "content": "Building upon the previous experiment, we aimed to investigate the impact of model diversity on the performance of the multi-agent debate framework. To introduce diversity, we replicated the study using three models of similar capacity but featuring diverse model families."}, {"title": "5 Conclusion", "content": "In this paper, we have presented a comprehensive investigation into the effectiveness of multi-agent debate in enhancing the reasoning capabilities and factual accuracy of large language models (LLMs). By building upon the foundational work of Du et al. [2023], we have developed an advanced framework that leverages the power of diverse models and iterative refinement to push the boundaries of what is possible in collaborative reasoning and problem-solving.\nOur experiments on a range of challenging benchmarks, including GSM-8K, ASDiv, and MATH, have consistently demonstrated the remarkable performance gains achieved by the multi-agent debate framework. The diversity of thought inherent in the framework, brought about by the inclusion of"}, {"title": "A Appendix / supplemental material", "content": "This appendix presents a deeper dive into the debates, providing supplementary analysis and visual representations to enhance understanding."}, {"title": "A.1 Supplementary analysis.Teacher - Student behaviour appearing in framework with diverse capacities", "content": "Given the previous result of diversity of thought, we sought to better understand what is truly happening in the mutliagent debate framework. We tested the framework while allowing one of the 3 models to be of higher capacity to observe if the smaller models can be taught at faster pace. The first experiment was done by using 2 small models (Gemma 7B and Qwen1.5 7B) and a third of higher capacity; Mixtral 7Bx8. The final outcomes are presented in Figure 9a, where the effectiveness of the multiagent debate framework is showcased. Remarkably, without any rounds of debate and simply by adopting the summary answer of the three models, we achieve the best performance compared to the individual models at 69% accuracy. However, surprisingly, the framework's performance does degrade to 66% as the models debate. Looking at the performance of individual models in a vacuum we can start to understand why. The 2 models that start at sub-30% accuracy, Gemma7B and Qwen1.5 7B, mark some impressive performance gains, with Gemma7B in particular punching above it's weight and getting more than half the questions correctly by the 3rd round. However, Mixtral, which notably was performing very well in this benchmark as expected et al. [2024], decreased it's performance by more than 20% as soon as debate started but started to regain performance on the 2nd round. One hypothesis for the observed behavior suggests that the superior model, Mixtral, functions akin to a teacher for the two lesser models. These models learn and adapt throughout the debate process. Meanwhile, the \"teacher\" model's performance diminishes as it becomes influenced by the lesser models.\nTo confirm this hypothesis, a similar experiment was made where Gemma7B was made to be the \"teacher\" as it was coupled with smaller inferior models; Gemma2B and TinyLlama (1.1B parameter Llama-based architecture model introduced by Zhang et al. [2024]). The final outcomes are presented in Figure 9b. Here we observe similar patterns to what we observed in Figure 9a and confirm our hypothesis. While the, now \"teacher\", Gemma7B decreases its performance as it is influenced by the 2 less capable models, the other models increase their performance as debate continues. With this finding we can adjust our previous theory to be that for a debate to be effective, the crucial requirement is that the participating models must have diverse architectures of similar sizes."}]}