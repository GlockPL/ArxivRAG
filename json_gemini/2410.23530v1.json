{"title": "THERE AND BACK AGAIN:\nON THE RELATION BETWEEN NOISES, IMAGES, AND\nTHEIR INVERSIONS IN DIFFUSION MODELS", "authors": ["\u0141ukasz Staniszewski", "\u0141ukasz Kuci\u0144ski", "Kamil Deja"], "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) achieve state-of-the-art performance in synthesizing new images from random noise, but they lack meaningful latent space that encodes data into features. Recent DDPM-based editing techniques try to mitigate this issue by inverting images back to their approximated staring noise. In this work, we study the relation between the initial Gaussian noise, the samples generated from it, and their corresponding latent encodings obtained through the inversion procedure. First, we interpret their spatial distance relations to show the inaccuracy of the DDIM inversion technique by localizing latent representations manifold between the initial noise and generated samples. Then, we demonstrate the peculiar relation between initial Gaussian noise and its corresponding generations during diffusion training, showing that the high-level features of generated images stabilize rapidly, keeping the spatial distance relationship between noises and generations consistent throughout the training.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion-based probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015), have surpassed state- of-the-art solutions in many generative domains including image (Dhariwal & Nichol, 2021), speech (Popov et al., 2021)), video (Ho et al., 2022), and music (Liu et al., 2021) synthesis. Nevertheless, one of the significant drawbacks that distinguishes diffusion-based approaches from other generative models like Variational Autoencoders (Kingma & Welling, 2014), Flows (Kingma & Dhariwal, 2018), or Generative Adversarial Networks (Goodfellow et al., 2014) is the lack of implicit latent space that encodes training data into low-dimensional, interpretable representations. Several works try to mitigate this issue, treating as latent features internal data representations extracted from the denoising UNet model (Kwon et al., 2022), combining diffusion models with ad- ditional external models (Preechakul et al., 2021), or by seeking structure in the starting noise used for generations (Song et al., 2020). The last example, introduced by (Song et al., 2020) with Denoising Diffusion Implicit Models (DDIM), led to the proliferation of methods grouped under the name of inversion techniques (Garibi et al., 2024; Mokady et al., 2023; Huberman-Spiegelglas et al., 2024; Meiri et al., 2023; Hong et al., 2024). The main idea behind those approaches is to use a diffusion model to predict the noise that can be added to the original or generated image. Applying this procedure several times allows for tracing back the backward diffusion process and approximating the initial noise that results in the starting image. However, due to approximation error and biases induced by a trained denoising model, there are inconsistencies between initial Gaussian noise and the reversed so-called latent representation."}, {"title": null, "content": "While recent works try to improve the approximation and mitigate discrepancies between noise and latent, in this work, we propose to closely study the DDIM inversion procedure and highlight the relation between Gaussian noise, generated samples, and their inverted latent encodings. First, we emphasize the main differences between initial Gaussian noise and latent codes. We locate the latent space between the initial Gaussian noise and generated samples, showing that DDIM inversion does not properly turn the image into noise. We show how this relation changes with time, highlighting the importance of early training steps in the formation of sample-to-latent mappings. Second, we move to the analysis of the relation between noise and samples. We show that it is possible to correctly assign initial Gaussian noise to the generated sample with Euclidean distance. We further deepen this analysis to a non-stationary setup, showing that the mapping between noises and generations emerges at the very beginning of the diffusion model training. The main contribution of this work can be summarized as follows:\n\u2022 We show that reverse DDIM produces latent representations that are not standard multivariate Gaussian, creating a gap between the diffusion models' theory and practice.\n\u2022 We study how this relation changes with training and show that improving the generative capabilities of the model does not improve the accuracy of reverse DDIM.\n\u2022 We show that the relation between images and noises is defined by the simple L2 distance at the early stage of the training."}, {"title": "2 BACKGROUND", "content": "The training of diffusion models consists of forward and backward diffusion processes, where in the context of Denoising Diffusion Probabilistic Models (DDPMs), the former one with training image xo and {t}=1 being some variance schedule, can be expressed as\n$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_t,$\nwith $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$, and $\\epsilon_t \\sim N(0, I)$.\nIn the backward process, the noise is gradually removed starting from a random noise $x_T \\sim N(0, I)$,\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t, c)) + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\epsilon_\\theta(x_t, t, c) + \\sigma_t z_t,$\nwith $\\sigma_t = \\eta \\beta_t (1 - \\bar{\\alpha}_{t-1})/(1 - \\bar{\\alpha}_t)$ being a variance schedule, $\\epsilon_\\theta(x_t,t, c)$ being the output of a network trained to remove noise, and $z_t \\sim N(0, I)$.\nWhile for \u03b7 = 1, a non-deterministic DDPM model is used, setting \u03b7 = 0 removes the random component from the equation 2, making it a Denoised Diffusion Implicit Model (DDIM), characterized by a deterministic mapping from noise space xr to image space xo. By removing the stochasticity of the DDPM sampling, we can additionally reverse the direction of the backward diffusion process deterministically and encode images back to the original noise space. The DDIM inversion is obtained by rewriting equation 2 as:\n$x_{t-1} = \\frac{\\sqrt{\\bar{\\alpha}_t}}{\\sqrt{\\bar{\\alpha}_{t-1}}} x_t + (\\sqrt{1 - \\bar{\\alpha}_t} - \\frac{\\sqrt{\\bar{\\alpha}_t} - \\sqrt{\\bar{\\alpha}_t \\bar{\\alpha}_{t-1}}}{\\sqrt{\\bar{\\alpha}_{t-1}}}) \\cdot \\epsilon_\\theta(x_t, t, c)$\nHowever, due to circular dependency on $\\epsilon_\\theta(x_t, t, c)$, DDIM inversion approximates this equation by assuming linear trajectory with direction to xr in t-th step being same as in (t \u2013 1)-th step, i.e., $\\epsilon_0(x_t, t, c) \\approx \\epsilon_0(x_{t-1}, t, c)$.\nWhile such approximation is often sufficient to obtain good reconstructions of images, it introduces the error that depends on the difference $(x_t - x_{t-1})$, which can be detrimental for models that leverage a few diffusion steps or use the classifier-free guidance Ho & Salimans (2021); Mokady et al. (2023). In this work, we empirically study the consequences of this approximation error."}, {"title": null, "content": "We will study the relations between the following three objects:\n\u2022 Gaussian noise variable, xT, used to generate an image through a diffusion process.\n\u2022 Image sample, x\u00ba, the outcome of the generation process produced by the Diffusion Model, i.e., the result of going through denoising stages, starting from t T and ending on t = 0.\n\u2022 Latent variable, \u00c2T, the result of starting from x\u00ba and applying T steps of a reversed DDIM generation process, see equation 4."}, {"title": "3 RELATED WORK", "content": "Diffusion models inversion techniques Thanks to the reversible sampling procedure, Denoising Diffusion Implicit Models are often employed in tasks such as inpainting (Zhang et al., 2023), im- age (Su et al., 2022; Kim et al., 2022; Hertz et al., 2022), video (Ceylan et al., 2023) or speech (Deja et al., 2023a) edition. However, the baseline DDIM approach is based on the assumption that the prediction of the noise removed from the image in the t-th backward diffusion step closely approx- imates the noise of the (t \u2013 1)-th step. This assumption is not always true, and recent works try to mitigate the discrepancies of such approximation. In particular, Renoise (Garibi et al., 2024) itera- tively improves the prediction of added noise using the predictor-corrector technique. This allows us to closely estimate the original Gaussian noise for a given image, even with fewer diffusion steps. Several works aim to reverse the diffusion process in text-to-image models. In such a case, the prompt selection significantly influences the final latent. To mitigate this issue, Null-text inversion method (Mokady et al., 2023) extends the DDIM inversion with optimized pivotal noise vectors and additional Null-text optimization technique, where unconditional textual embeddings employed by classifier free-guidance (Ho & Salimans, 2021) are optimized in order to reduce the reconstruc- tion error. Another work (Huberman-Spiegelglas et al., 2024) proposes an alternative DDPM noise space, where noise maps do not have a normal distribution, yet it enables better image editing ca- pabilities and perfect reconstructions. On the other hand, a regularized Newton-Raphson inversion method (Meiri et al., 2023) formulates the inversion process as solving an implicit equation using numerical techniques, achieving faster and higher-quality reconstructions with prompt-aware adjust- ments, enabling improvements in image interpolation, and boosting models' diversity. Furthermore, exact inversion methods for DPM-solvers (Hong et al., 2024) mitigate the errors introduced by classifier-free guidance by leveraging gradients, boosting both the image and noise reconstruction."}, {"title": null, "content": "Latent space in diffusion models One of the significant drawbacks of diffusion-based generative models compared to approaches such as VAEs, Flows, or GANs is the lack of meaningful latent space that encodes features of the generated samples. Several approaches try to mitigate this issue. Kwon et al. (2022) show that so-called h-features, which are the activations located inside the U- Net model used as a diffusion decoder, can be used as meaningful representations providing space for semantically coherent image manipulation. This idea is further extended by Park et al. (2023), where the authors show that we can calculate the pullback metric that directly associates h-features with the original image space. Several works show that we can directly benefit from such features in downstream tasks such as image segmentation (Baranchuk et al., 2021; Tumanyan et al., 2023; Rosnati et al., 2023), image correspondence (Luo et al., 2024) or classification (Deja et al., 2023b)."}, {"title": null, "content": "Noise-to-image mapping in diffusion models Contrary to latent variable models such as VAEs, the forward diffusion process that maps images to the Gaussian noise is a parameter-free process that can be understood as hierarchical VAE with pre-defined unlearnable encoder Kingma et al. (2021). However, several works show interesting properties resulting from the training objective of DDPMS and score-based models. Kadkhodaie et al. (2024) show that due to inductive biases of denoising models, different DDPMs trained on similar datasets converge to almost identical solutions. This idea is further explored by Zhang et al. (2024), where the authors show that even models with different architectures converge to the same score function and, hence, the same noise-to-generations mapping. This mapping itself is further analyzed by Khrulkov & Oseledets (2022), where the authors show that the encoder map coincides with the optimal transport map for common distributions. In this work, we extend this analysis further by empirically validating that examples are generated from the closest random noise, even for more complex distributions."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EXPERIMENTAL DETAILS", "content": "In our experiments, we employ three unconditional diffusion models: two pixel-space Denoising Diffusion Probabilistic Models (Dhariwal & Nichol, 2021) trained on the CIFAR-10 (32 \u00d7 32 resolution) and ImageNet (64 \u00d7 64 image resolution) datasets, as well as a Latent Diffusion Model (LDM) (Rombach et al., 2021) trained on the CelebA dataset. Sampling from these models is performed with the DDIM (Song et al., 2020) sampler, using T = 100 diffusion steps for both sampling and inverting to latent encodings (for more details on impact of T see Appendix A.2). For experiments involving latent encoding localization (see: Table 1, Figure 2, Figure 3), we use N = 2048 image generations. For investigating the noise-to-sample mapping process (see: Table 2), metrics are averaged over N = 1000 samples."}, {"title": null, "content": "For fine-tuning the diffusion models, we follow the setup from Nichol & Dhariwal (2021) and train two unconditional pixel-space diffusion models on the ImageNet and CIFAR-10 datasets. The CIFAR-10 model undergoes training for 700K steps, while the ImageNet model is trained for 1.5M steps. Both DDPM models are trained with a cosine schedule leveraging 4K diffusion steps, with a batch size of 128. In fine-tuning experiments, metrics are computed over N = 2048 samples and averaged across generations from three models trained with three different random seeds."}, {"title": "4.2 NOISE \u2260 LATENT", "content": "Numerous methods employ reversed DDIM in applications such as image editing or inpainting. In this case, the underlying assumption is that by encoding the image back with a denoising decoder, we can obtain the original noise that can be used to reconstruct the original image. However, as noticed by recent works Garibi et al. (2024); Parmar et al. (2023); Hong et al. (2024), the latents created with DDIM inversion (\u00c2T) often do not follow standard multivariate Gaussian distribution, creating a gap between the promise of diffusion models' theory and practice. We study the implications of this fact and show its far-reaching consequences."}, {"title": null, "content": "We visualize this phenomenon in Figure 1 for three models, showing the latent or its difference from the generated image. For simpler ones trained with smaller datasets, such as CIFAR-10, we can observe clear structures of original images in the inverted latents, as presented in Figure 1 (C). However, even for more complex datasets, we can highlight the inversion error by plotting the image difference between the latent and the noise, as presented for DDPM trained on ImageNet and LDM trained on CelebA (Figure 1 (A, B)). We can observe that the highest inversion error can be observed in the areas of large monotonic surfaces, where more high-frequency noise needs to be added."}, {"title": "4.3 WHAT IS THE LOCATION OF THE LATENT VARIABLE?", "content": "Given the results of the previous analysis, we pose the next question: what is the location of the latent variables calculated with the reverse DDIM procedure? We show that the DDIM-generated latent (T) is located along the trajectory xt between the Gaussian noise (xT) and the generated sample (x0) and discuss how this relation changes with different characteristics of diffusion models. In the first experiment, shown in Figure 2, we calculate the angles between noises, samples, and latents. We average the angles across samples from three different models and plot the triangles in 2D space. We can observe that in each case, the angle located at the image's vertex, \u2220x\u00ba, is acute but never zero. On the other hand, the angle located at the vertex representing latents, \u2220\u00c2T, is always obtuse. This leads to the conclusion that due to the imperfect approximation of the reverse DDIM procedure, latents are located along the trajectory xt of the generated image."}, {"title": null, "content": "To approximate the exact location of the latents more closely, we analyze the distance between different steps of the backward diffusion process and the Noise-Latent side of the triangle, i.e., the interval xT\u00c2T, see Figure 3. Each pixel, with coordinates (t, \u5165), is colored according to the L2 distance between the intermediate step of trajectory xt and the corresponding interpolation step, i.e. ||(1 \u2013 \u5165)x + \u00c2Tx \u2013 Xt ||2. Figure 3 shows that while moving from the random Gaussian noise (xT) towards the final sample (x\u00ba), the intermediate steps xt are getting closer to the latent (xT), which is visually represented by the light-colored area in the upper right corner of the plot. This also demonstrates that from some time onwards, say to, the trajectory xt closest point to the interval xT\u00c2T is the right endpoint \u00c2T."}, {"title": null, "content": "This has non-trivial consequences in the light of latent encodings T featuring a lot of structure (see Section 4.2), in the form of an unprecedented amount of structure between xt, x, x unac- counted by theory. Failing to realize this implication could lead to incorrect reasoning and spurious discoveries. The same trend can be observed across all three evaluated models. Finally, we show that the situation is persistent across the training process; see Figure 4. Hypothet- ically, the value of all three triangle angles shown in Figure 2 could fluctuate during the training process. However, both the angle adjacent to the noise /xT and the distance between the latent and noise quickly converge to a certain value that remains constant through the rest of the training. This observation brings two main conclusions: (1) The relation between noises, latents, and samples is defined at the early stage of the training, and (2) The inverse DDIM method does not benefit from the prolonged training time of the diffusion model."}, {"title": "4.4 NOISE-TO-SAMPLE MAPPING", "content": "As indicated by previous works (Kadkhodaie et al., 2023; Zhang et al., 2024), diffusion models converge to the same mapping between the random Gaussian noise (xT) and the generated images (x0) independently on the random seed, parts of the training dataset, or even the model architecture. In this work, we investigate this phenomenon further and study the nature of mapping between noises and samples and how it changes during training."}, {"title": null, "content": "To that end, we first generate 1K samples from random Gaussian noises and try to predict which noise (xT) was used to generate which sample (x\u00ba) and vice versa. As presented in Table 2, we show that we can accurately assign the noises to the corresponding images (x\u00ba \u2192 xT) according to the smallest L2 distance criterion. This is especially true for the higher number of diffusion timesteps, where for all models, we achieve over 99% accuracy. The situation changes when trying to assign the generated image to the corre- sponding noise (xT \u2192 x\u00ba). We can observe high accuracy with a low number of generation timesteps (T = 10), but the results deteriorate quickly with the increase of this parameter. The rea- son for this is that greater values of T allow the generation of a broader range of images, including the ones with large plain areas of low pixel variance. Such generations turn out to be close to the majority of initial Gaussian noises. We provide examples of such generations in the Appendix A.1. Notably, we do not observe such behavior in the LDM model, where final generations are well nor- malized through the KL-divergence applied to the latent space of the LDM's autoencoder."}, {"title": null, "content": "To further analyze the nature of this property, we show how those metrics change when sampling generations from intermediate checkpoints of the diffusion models' training; see Figure 5. We can observe that the distance between noises and latents accurately defines the assignment of initial noises given the generated samples (x\u00ba \u2192 xT) from the beginning of the training till the end. At the same time, the accurate reverse assignment (xT \u2192 x\u00ba) can only be observed at the beginning of the training when the fine-tuned model is not yet capable of generating properly formed images. Moreover, in Figure 6, we measure what is the coverage of the k-nearest generations for the same noise during training according to the generation after full training of the diffusion model. Also, in this case, such a spatial relationship between the noise xT and the images x\u00ba is established at the beginning of the fine-tuning and does not change much later."}, {"title": null, "content": "We measure how the relation between noises and samples changes when training the model. Thus, for each training step n \u2208 {1... 700K} for CIFAR-10 and {1 . . . 1.5M} for ImageNet we generate 2048 samples {Xn}2018 2048 from the same random noise xved Xfixed N(0, I), and compare them with generations obtained for the fully trained model. We present the visualization of this comparison in Figure 7 using CKA, DINO, SSIM, and SVCCA as image-alignment metrics. We notice that image features rapidly converge to the level that persists until the end of the training. This means that prolonged learning does not significantly alter how the data is assigned to the Gaussian noise after the early stage of the training. It is especially visible when considering the SVCCA metric, which measures the average correlation of top-10 correlated data features between two sets of samples. We can observe that this quantity is high and stable through training, showing that generating the most important image concepts from a given noise will not be affected by a longer learning process. For visual comparison, we plot the generations sampled from the model trained with different numbers of training steps in Figure 7 (right). In Appendix A.3, we show that the same situation with mapping between noise and image at the beginning of the fine-tuning process occurs for the DDPM trained on the CIFAR-10 dataset, along with examples containing more image generations from both ImageNet and CIFAR-10 models."}, {"title": "5 CONCLUSIONS", "content": "In this work, we empirically study the relation between initial Gaussian noise, generated samples, and their latent representations calculated with the inverse DDIM technique. First, we show that the error in the approximation of the previous noise in DDIM leads to representations located next to the generation trajectory between starting samples and their true noises. Moreover, prolonged diffusion training does not affect this property, as the accuracy of DDIM inversion does not improve in time. Then, studying the relation between the generated samples and Gaussian noise, we show that we can accurately assign the initial noise of the given generation with a simple L2 distance. We also demonstrate that this behavior emerges at the very beginning of the diffusion models training. Finally, we show that low-frequency features of the images generated using the DDIM sampler for a specific input Gaussian noise converge to a certain level at the beginning of the training, and only the high-frequency features of the images are improved further. Our experiments lead to the conclusion that the initial part of the diffusion model's training is responsible for building the relation between the initial Gaussian noise, final generations, and their inverted representations."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 MISTAKES IN NOISE TO SAMPLE MAPPING", "content": "In Figures 8 and 9, we show, in (A), examples of generated images that are not correctly assigned\nto their corresponding noise (xT \u2192 x\u00ba). We can observe that such imperfection comes from a\nfew repetitive images that tend to attract most of the noises (C). As shown in (B), those images are\ncharacterized by a low variance of the pixels they consist of."}, {"title": "A.2 IMPACT OF T ON SPATIAL RELATIONS OF NOISE, IMAGE AND LATENT", "content": "We measure the influence of the number of diffusion steps T on the spatial relation between the\nGaussian Noise xT, image generations x\u00ba from the DDIM sampling, and their latent encodings T,\nbeing an output of the inversion process. We determine the vectors going from each vertex to the\nother vertices: xx, TXT, xx, XTXT, XTx, xx, xx and convert cosine similarities between\nthese vectors to degrees. Also, we calculate the 12-norm as a distance metric between all three\nobjects.\nThe impact of the number of diffusion steps T on the metrics has been presented for all three models\nin Table 3, Table 4 and Table 5, consecutively for DDPM CIFAR-10, DDPM ImageNet and LDM\nCelebA. We generate 2048 images from the same noise A larger number of T leads to a smaller\nimpact of an error in the DDIM inversion approximation on the \u0176T position, which can be seen in\nthe form of a decreasing noise to latent distance ||xT \u2013 XT||2 and a smaller \u2220x\u00ba. Moreover, no\nmatter the value of T, the latent angle \u2220\u00c2T is always obtuse and ||x\u00b0 \u2013 \u00c2T||2 < ||x\u00b0 \u2212 xT||2,\nmeaning the latent encoding T is always spatially between an image x\u00ba and Gaussian noise xT.\n=In this work, we chose T = 100 as a proper balance between capturing the spatial properties\ncharacteristic of diffusion models and achieving the fast sampling efficiency that Denoising\nDiffusion Implicit Models (DDIMs) are known for."}, {"title": "A.3 DETAILS ON IMAGE ALIGNMENT OVER TRAINING TIME", "content": "In Figure 10, we visualize how the diffusion model learns the low-frequency features of the image\nalready at the beginning of the fine-tuning when comparing generations from the next training steps\nagainst the generations after finished fine-tuning using four image-alignment metrics: CKA, DINO,\nSSIM, and SVCCA for the DDPM model trained on the CIFAR-10 dataset."}]}