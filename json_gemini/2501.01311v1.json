{"title": "Multi-Head Explainer: A General Framework to Improve Explainability in CNNs and Transformers", "authors": ["Bohang Sun", "Pietro Li\u00f2"], "abstract": "In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and modular framework that enhances both the explainability and accuracy of Convolutional Neural Networks (CNNs) and Transformer-based models. MHEX consists of three core components: an Attention Gate that dynamically highlights task-relevant features, Deep Supervision that guides early layers to capture fine-grained details pertinent to the target class, and an Equivalent Matrix that unifies refined local and global representations to generate comprehensive saliency maps. Our approach demonstrates superior compatibility, enabling effortless integration into existing residual networks like ResNet and Transformer architectures such as BERT with minimal modifications. Extensive experiments on benchmark datasets in medical imaging and text classification show that MHEX not only improves classification accuracy but also produces highly interpretable and detailed saliency scores.", "sections": [{"title": "1. Introduction", "content": "Deep learning models have achieved remarkable success across various fields, but their complex and opaque nature often hinders interpretability and trust, particularly in critical applications.\nIn the realm of computer vision, medical imaging stands out as a field where the need for explainable models is particularly pronounced. Medical professionals rely on precise and interpretable model predictions to make informed decisions, as inaccuracies or lack of transparency can have serious consequences for patient care (Shamshad et al., 2023). However, standard explainability methods such as Grad-CAM (Selvaraju et al., 2017) and SHAP (Lundberg, 2017) often fall short in this context. These methods frequently fail to capture the rich texture details and subtle features inherent in medical images, leading to incomplete or misleading interpretations that are insufficient for clinical use.\nSimilarly, in Transformer-based architectures, which have become the backbone of many state-of-the-art NLP models (Vaswani, 2017), significant challenges persist regarding explainability. Recent studies have highlighted issues such as over-smoothing (Dovonon et al., 2024), where attention mechanisms tend to produce uniform attention distributions across layers, diluting the interpretative power of attention scores. Additionally, research by Jain and Wallace (Jain & Wallace, 2019) has demonstrated that attention weights in Transformers do not necessarily provide meaningful explanations for model predictions.\nTo address these challenges, we introduce Multi-Head Explainer (MHEX)\u00b9, a general and modular framework designed to enhance the explainability and accuracy of both Convolutional Neural Networks (CNNs) and Transformer-based models. MHEX serves as a versatile \"scaffold\" that can be seamlessly integrated into various network architectures, including residual networks like ResNet (He et al., 2015) and Transformer models such as BERT (Devlin et al., 2018). After the model is fine-tuned, the MHEX prediction heads modules can be removed, ensuring minimal impact on the original model's architecture and performance. Our primary contributions are threefold:\n1. MHEX Framework: We present the Multi-Head Explainer framework, detailing its components and demonstrating its application in enhancing both residual networks and Transformer architectures. This framework significantly improves the generation of comprehensive and interpretable saliency scores across different model types."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Explainability in CNNs", "content": "Convolutional Neural Networks (CNNs) have been extensively utilized across various domains. Over time, CNN architectures have evolved to incorporate sophisticated mechanisms that enhance their representational power and adaptability. For instance, attention-based CNNs such as Squeeze-and-Excitation Networks (SENet) (Hu et al., 2018), Convolutional Block Attention Module (CBAM) (Woo et al., 2018), and Bottleneck Attention Module (BAM) (Park, 2018) dynamically emphasize important features, thereby improving both performance and interpretability. Additionally, spherical CNNs (Cohen et al., 2018) have been developed to process 360-degree spherical data, maintaining rotation and translation invariance on spherical surfaces, which is crucial for applications like omnidirectional vision and geospatial analysis. Recent hybrid architectures, such as ConvNeXt (Liu et al., 2022) and MobileViT (Mehta & Rastegari, 2021), integrate Transformer within CNN frameworks to leverage both local feature extraction and global contextual understanding.\nDespite the advancements in CNN architectures, the interpretability of these models remains a critical concern. Numerous saliency-based methods have been proposed to address this issue, aiming to highlight important regions in input images that contribute to model predictions. Techniques like Grad-CAM (Selvaraju et al., 2017), Grad-CAM++ (Chattopadhay et al., 2018), Layer-CAM (Jiang et al., 2021), Score-CAM (Wang et al., 2020), Eigen-CAM (Muhammad & Yeasin, 2020), SHAP (Lundberg, 2017), Integrated Gradients (Sundararajan et al., 2017), and Guided Backpropagation (Springenberg et al., 2014) have been widely adopted to visualize model decision-making processes. However, these methods often struggle to capture the rich texture details and subtle features inherent in medical images, leading to incomplete or misleading interpretations."}, {"title": "2.2. Transformer Explainability", "content": "Transformer-based architectures have revolutionized natural language processing (NLP) and have made significant inroads into computer vision tasks (Vaswani, 2017; Devlin et al., 2018). Despite their success, the interpretability of Transformers remains a challenging issue. Early efforts focused on analyzing attention weights to understand model focus (Clark et al., 2019), but subsequent studies revealed that these weights often exhibit intricate and repetitive patterns that do not necessarily correlate with meaningful explanations (Kovaleva et al., 2019).\nRecent advancements have sought to develop more sophisticated explainability techniques tailored for Transformers. For instance, Qiang et al. (2022) introduced AttCat, which leverages attentive class activation tokens by integrating encoded features, gradients, and self-attention weights to provide more granular and faithful explanations of Transformer predictions. Similarly, ViT-CX (Xie et al., 2023) focuses on causal explanations in Vision Transformers, enhancing the interpretability of models in computer vision by identifying causal relationships between input features and model outputs. Attention Flow (Abnar & Zuidema, 2020) proposed methods such as attention rollout and attention flow to model information flow in Transformers using a directed acyclic graph (DAG), offering more accurate and reliable quantifications of self-attention mechanisms compared to raw attention weights."}, {"title": "3. Method", "content": "In this section, we present the Multi-Head Explainer (MHEX) framework, its core components, and its integration into both CNNs and Transformer-based models, along with neuron analysis techniques and evaluation metrics for saliency maps."}, {"title": "3.1. Overall Framework Introduction", "content": "Multi-Head Explainer (MHEX) is a modular framework designed to enhance the explainability and accuracy of deep learning models, including CNNs and Transformers. Its design allows seamless integration into various architectures, improving model interpretability across tasks. The MHEX framework comprises three components:\n\u2022 Attention Gate: Dynamically emphasizes task-relevant features.\n\u2022 Deep Supervision: Guides early layers to capture fine-grained features pertinent to the target class.\n\u2022 Equivalent Matrix: Unifies refined local and global representations to generate comprehensive saliency scores.\nBy integrating these components, MHEX enhances the model's ability to produce detailed and interpretable saliency scores, thereby bridging the gap between model performance and interpretability."}, {"title": "3.2. MHEX Core Components", "content": ""}, {"title": "3.2.1. ATTENTION GATE", "content": "The Attention Gate (Schlemper et al., 2019) prioritizes task-relevant features by generating channel-wise weights based on local and global information. It computes:\n$g = \\sigma(W_1\\cdot GAP(x + x_{global})),$ where $x_{global}$ is a global feature map, GAP is Global Average Pooling, and $\\sigma$ is the sigmoid activation function. The combined input $x + x_{global}$ captures both local and global features, ensuring the network focuses on critical semantic regions. The weights g reweight the input features as:\n$x_{att} g \\circ x.$"}, {"title": "3.2.2. DEEP SUPERVISION", "content": "Deep Supervision (Lee et al., 2014; Li et al., 2022) aligns feature learning with classification objectives by optimizing the equivalent matrix $W_{equiv}$. It minimizes a task-specific loss function, guiding feature transformations to capture both local details and global semantics:\n$\\mathcal{L}_{pred} = \\mathcal{L}(W_2W_1ReLU(x + x_{global})),$ where $W_1$ and $W_2$ are weight matrices, $x$ represents the feature map, and $x_{global}$ is a global context feature map. The ReLU activation ensures that only positive feature contributions are considered.\nDeep supervision refines shallow-layer representations, enabling the model to capture finer details. During testing, only the final layer is used for predictions, improving computational efficiency as the intermediate deep-supervision components are removed."}, {"title": "3.2.3. EQUIVALENT MATRIX & SALIENCY SCORE", "content": "The Equivalent Matrix $W_{equiv} = W_2W_1$ models interactions between local and global features. The Attention Gate ensures fine-grained channel selection, while Deep Supervision aligns feature representations globally. Together, they enable $W_{equiv}$ to capture high-resolution texture details and task-relevant semantics crucial for various applications, including medical imaging and natural language processing.\nIn Convolutional Neural Networks (CNNs), the Equivalent Matrix facilitates the generation of Class Activation Maps (CAMs), which highlight the regions of the input image that are most influential for a specific class prediction. For each layer l, the CAM is computed as:\n$CAM^{(l)}(x, y) = \\sum_{k=1}^{C} \\tilde{w}_{k}f_{k}^{(l)}(x, y),$\nwhere $\\tilde{w}_{k}$ are the adjusted weights derived using Non-Negativity and Salience Sharpness (Section 3.4).\n\u2022 $f_{k}^{(l)}(x, y)$ are spatial activations of channel k at layer l\nThe final CAM is obtained by aggregating CAMs across all layers with a weighting factor $a^l$:\n$CAM = \\sum a^lCAM^{(l)},$\nwhere $a^l$ is typically set to 0.9 to balance the contributions from shallow-layer fine details and deep-layer global semantics.\nIn Transformer-based architectures, such as BERT, saliency is represented through saliency scores rather than saliency maps due to the sequential nature of the data. The saliency score for each token is computed by aggregating contributions from multiple layers. To address the over-smoothing problem, we set the number of layers L to 3, as activations tend to become too smooth after the third layer in certain datasets, such as AG News. This empirical choice prevents over-smoothing from diminishing the quality of saliency explanations. The process involves the following steps:\n$S^{(l, c)}(j) = \\sum_{k=1}^{D} w_{equiv, c, k} \\cdot A^{(l)}(j, k),$"}, {"title": "3.3. Integration", "content": "In this section, we describe how MHEX is integrated into residual networks, such as ResNet, and Transformer models, like BERT, to enhance feature representation and interpretability while maintaining the original architecture.\nAs illustrated in Figure 2, MHEX is deployed between the residual blocks of ResNet.\nSimilarly, for Transformer models such as BERT, we insert MHEX between the attention layers and feed-forward layers, as shown in Figure 3. This integration allows us to extract detailed saliency scores, enhancing the model's interpretability. Specifically, after the attention mechanism, we apply Layer Normalization to stabilize the activations:\n$x_{att} = LayerNorm(x_{att}),$\nwhere $x_{att}$ refers to the output of the attention mechanism, as previously defined in Section 3.2.1."}, {"title": "3.4. Neuron Analysis", "content": "In the early layers of the model, saliency scores often exhibit uncertainty due to limited semantic information, leading the model to mistakenly highlight background or noise as important activations. To mitigate this, we impose a non-negativity constraint by applying the ReLU activation function, which projects feature representations onto a non-negative subspace. This constraint effectively removes irrelevant features and reduces entropy (see Appendix A.1), thereby facilitating more accurate neuron analysis and enhancing the interpretability of the equivalent matrix $W_{equiv}$.\nAt a local level, focusing on individual neurons, we leverage the non-negativity constraint alongside a parameter a (Appendix A.2) to decompose the weights in $W_{equiv}$ into positive and negative components. By controlling the influence of negative contributions through $\\alpha$, we ensure that saliency scores emphasize features that positively contribute to class predictions.\nGlobally, considering all neurons collectively, we introduce salience sharpness (SS) (see Appendix A.3), which quantifies the specificity of each feature's contribution to class predictions across the entire network. By analyzing the distribution of salience scores in both positive and negative domains, SS highlights the most relevant features for the target class, ensuring that the saliency maps accurately reflect the integrated contributions of all neurons."}, {"title": "3.5. Quantitative Study", "content": "To quantitatively evaluate the quality of saliency scores generated by MHEX, we utilize several metrics that assess the impact of salient regions on model predictions."}, {"title": "3.5.1. AVERAGE DROP (AVG DROP)", "content": "The Average Drop (AVG Drop) metric measures the decrease in the model's confidence after the removal of salient regions, reflecting their importance in decision-making (Chattopadhay et al., 2018). It is computed as:\n$AVG \\text{ } Drop = 1 - \\frac{1}{N} \\sum_{i=1}^{N} max \\left(0, \\frac{P_{i}^{mask} - P_{i}^{orig}}{P_{i}^{orig}}\\right)^2,$ where $P_{i}^{orig}$ and $P_{i}^{mask}$ are the model's prediction confidences on the original and masked inputs for the i-th sample, respectively."}, {"title": "3.5.2. SOFT AVERAGE DROP (SAD)", "content": "Soft Average Drop (SAD) improves upon AVG Drop by employing a soft replacement strategy that preserves the overall structure of the input, focusing the evaluation on saliency map quality. Due to the typically homogeneous image distribution in medical imaging, and to avoid the bias introduced by \"hard\" removal and confounders caused by model generalization issues, we propose this approach:\n$I_{soft}(x, y) = I(x, y) \\cdot (1 - CAM(x, y)) + \\mu \\cdot CAM(x, y),$, where $I(x, y)$ is the original input intensity at location (x, y), and $\\mu$ is the mean intensity of the input."}, {"title": "3.5.3. EFFECTIVE AVERAGE DROP (EAD)", "content": "We introduce the Effective Average Drop (EAD), which incorporates the saliency map's area into the evaluation using an area-based weighting function f(x) (Appendix B.1). This function penalizes saliency maps that are excessively sparse or overly distributed, promoting compact and informative explanations. The EAD metric is computed as:\n$EAD = \\frac{1}{N} \\sum (Drop_i \\cdot f(x_i)),$, where Dropi is the confidence drop for the i-th sample, and $x_i$ is the proportion of the saliency map's area for that sample."}, {"title": "3.6. Collaboration Analysis", "content": "In the MHEX framework, the Attention Gate (AG) and Deep Supervision (DS) modules have distinct optimization objectives. To investigate their collaboration, we analyze the direction of gradients during training.\nSpecifically, we compute the cosine similarity between the gradients of the Equivalent Matrix $W_{equiv}$ with respect to $W_1$, contributed by the Attention Gate ($\\nabla_{W_1}^{AG}$) and Deep Supervision ($\\nabla_{W_1}^{DS}$). These gradients are defined as:\n$\\bullet \\nabla_{W_1}^{AG} = \\frac{\\partial L_{DS}^{(l+1)}}{\\partial W_1}$, where $L_{DS}^{(l+1)}$ is the loss from the Deep Supervision module in the next layer (l + 1).\n$\\bullet \\nabla_{W_1}^{DS} = \\frac{\\partial L_{DS}^{(l)}}{\\partial W_1}$, where $L_{DS}^{(l)}$ is the loss from the current layer (l).\nThe cosine similarity between gradients is calculated as:\n$Cosine\\text{ }Similarity = \\frac{\\nabla_{W_1}^{AG} \\cdot \\nabla_{W_1}^{DS}}{|\\nabla_{W_1}^{AG}||||\\nabla_{W_1}^{DS}|| + \\epsilon},$ where $\\epsilon$ is a small constant to prevent division by zero.\nIn the Experiments section, we apply this metric to assess the collaboration strength between AG and DS across multiple layers and evaluate its impact on the quality of the saliency scores."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset", "content": "We evaluate our proposed method on three datasets: ImageNet1k, MedMNIST, and AG News. These datasets provide diverse challenges across general-purpose and biomedical image classification tasks.\nImageNet1k is a large-scale dataset widely used for benchmarking deep learning models (Deng et al., 2009). It contains 1,000 object categories with approximately 1.28 million training images and 50,000 validation images.\nMedMNIST is a collection of biomedical image datasets designed for lightweight classification tasks (Yang et al., 2021; 2023). It includes both 2D and 3D datasets across various data modalities and task types.\nAG News\u00b2 is a widely recognized benchmark for text classification tasks, comprising 120,000 training samples and 7,600 test samples across four distinct classes: World, Sports, Business, and Sci/Tech. The dataset consists of news articles, providing a diverse range of topics and writing styles.\nDetails of training and fine-tuning configurations, can be found in Appendix D."}, {"title": "4.2. Results for CNNS", "content": "Table 1 summarizes the performance of MHEX-Net compared to baseline ResNet-18 models. All baseline metrics are derived from the official MedMNIST and PyTorch benchmarks for ResNet-18 (Yang et al., 2023). MHEX-Net consistently outperforms the baseline in most tasks, achieving higher accuracy with minimal additional parameters. Specifically, for ResNet-18, the added parameters amount to 1920 \u00d7 nclasses +0.69M."}, {"title": "4.3. Saliency Score Analysis", "content": "To assess the interpretability and localization capabilities of the MHEX-Net framework, we compared saliency scores generated by MHEX-Net, Grad-CAM, SHAP (with blur masking using 20k \u00d7 256 or 50k \u00d7 256 evaluations), and Layer-CAM across five representative tissue types.\nImage Classification: MHEX-Net consistently demonstrated superior coverage, closely resembling semantic segmentation, while other methods exhibited characteristic strengths and limitations, such as focusing on localized high-level features or producing fragmented scores. The results are illustrated in Figure 4 (colorectal adenocarcinoma epithelium) and Figure 5 (smooth muscle tissue).\nFor Text Classification tasks on the AG News dataset, MHEX-Net effectively captured important tokens relevant to class predictions. Table 2 presents saliency scores for selected test samples from the Business, Sci/Tech, and Sports categories, illustrating how each method assigns importance to different tokens. MHEX-Net consistently reveals rich and detailed insights in this task.\nAdditional results are available in Appendix C for further reference."}, {"title": "4.4. Neuron Analysis", "content": "In this study, we investigate the impact of two critical hyperparameters, $\\alpha$ and SS (Salience Sharpness), on the quality and interpretability of the saliency maps generated by our MHEX-Net framework.\nThe hyperparameter $\\alpha$ plays a crucial role in enhancing the clarity of early-layer interpretations by filtering out noisy channels. By tuning $\\alpha$, we effectively exclude channels with negative contributions, resulting in sharper and more interpretable saliency maps. As illustrated in Figure 6, decreasing $\\alpha$ significantly reduces early-layer uncertainties. For instance, when $\\alpha = 0$, only channels with positive contributions are included in the calculation of the saliency map.\nWith $\\alpha$ fixed at 0, we further explore the impact of SS, another hyperparameter designed to filter uncertain channels by prioritizing those that exhibit strong connections to specific class neurons. Higher SS values correspond to sharper and more focused saliency maps, as demonstrated in Figure 6. Increasing SS enhances the concentration of saliency in regions highly relevant to the class predictions, thereby improving the interpretability of the model's decision-making process.\nBased on our observations, we recommend setting $SS = \\frac{N_{class}}{1 + \\epsilon}$, where $\\epsilon \\in [0.1, 0.3]$. This configuration effectively balances the sharpness and coverage of saliency maps across different classes. Additionally, for different datasets, $\\alpha$ values in the range [0,0.5] generally yield favorable results."}, {"title": "4.5. Quantitative Study", "content": "We evaluate the performance of our MHEX-Net and MHEX-BERT models in comparison to Grad-CAM, Layer-CAM, and SHAP using the metrics Average Drop (AVG Drop), Soft Average Drop (SAD), and Effective Average Drop (EAD) on the PathMNIST, BloodMNIST, and AG News datasets.\nThe results are summarized in Table 3. Bold values indicate the best performance in each category.\nIn the PathMNIST dataset, which contains abundant fine-grained features, Grad-CAM and Layer-CAM often produce suboptimal results, struggling to capture intricate patterns. MHEX-Net effectively captures these details, resulting in superior performance across all three metrics.\nFor the BloodMNIST dataset, the situation differs. Since BloodMNIST images feature distinct cell bodies without pervasive fine details, explanation methods like Grad-CAM and Layer-CAM can effectively highlight the main structures, yielding reasonable results. This is showed in their comparable SAD and AVG Drop metrics. However, when considering the EAD metric, which accounts for the effectiveness of the explanation relative to the area of the saliency map, MHEX outperforms the other methods significantly.\nIn the AG News dataset, we compared MHEX-BERT against SHAP using the AVG Drop metric. To evaluate the impact of saliency methods on the BERT model's performance, we selected the top 10% of saliency tokens for perturbation using the [MASK] token across the first 512 samples of the AG News dataset. This threshold balances computational efficiency, addressing the high cost associated with SHAP.\nThe results showed that SHAP achieved a higher AVG Drop, indicating that it is more effective in identifying critical tokens that influence the model's predictions. This suggests that while MHEX-BERT provides detailed saliency scores, SHAP's theoretically grounded methodology performs better in pinpointing the most impactful tokens.\nOne possible explanation is that SHAP's scores are derived from drop-oriented methods based on Shapley values, which inherently consider each token's contribution across all possible feature subsets. In contrast, MHEX-BERT'S saliency scores are generated intrinsically from the model's internal mechanisms without relying on feature perturbations, thereby avoiding the biases and limitations of drop-oriented approaches.\nAdditionally, we found that using the [MASK] token for perturbation with both MHEX-BERT and SHAP did not significantly affect the model's performance based on the AVG Drop metric. This contrasts with image tasks, where perturbing salient features typically leads to greater performance degradation. A possible explanation is that BERT has learned to effectively predict the [MASK] token during pretraining, enhancing its robustness to such perturbations."}, {"title": "4.6. Collaboration Analysis & Saliency Quality", "content": "We analyze the interrelationships among collaboration strength (cosine similarity) across the last three MHEX-Blocks, model confidence $P_{orig}$, and saliency map quality (SAD) on the PathMinist validation set, quantified using Pearson correlation coefficients (details in Appendix A.4). Figure 7 illustrates these relationships.\nAs the triangle shows, collaboration strength positively correlates with SAD, indicating that stronger collaboration improves the quality of saliency maps by aligning the highlighted regions with decision-critical features.\nAt the same time, collaboration strength exhibits a negative correlation with model confidence ($P_{orig}$). One possible explanation is that the observed relationship arises due to an underlying causal link between SAD and $P_{orig}$: when the model has higher confidence in a sample, it often produces a lower SAD. This statistical association might make it appear that collaboration strength reduces confidence.\nBased on this, we further evaluate saliency map quality at a finer spatial scale by dividing the input image into a 7 \u00d7 7 grid and computing collaboration strength for each spatial block. This allows us to assess the localized quality of saliency maps. Examples are shown in Figure 8, where the collaboration metric serves as an internal measure of the model's confidence in aligning saliency maps with decision-critical features."}, {"title": "5. Conclusion", "content": "In this work, we introduced the Multi-Head Explainer (MHEX), a comprehensive framework designed to enhance the explainability of both Convolutional Neural Networks (CNNs) and Transformer-based models. Our key contributions are summarized below:\n\u2022 MHEX Framework: Developed a versatile explanation framework compatible with CNNs and Transformers, with the potential for deployment in large language models (LLM) and AI agents, effectively addressing the limitations of existing methods in capturing fine-grained details and mitigating over-smoothing issues.\n\u2022 Neuron Analysis: Presented a novel neuron analysis technique that enables more precise interpretations by examining individual neuron contributions, thereby enhancing the accuracy and reliability of saliency maps.\n\u2022 Collaboration Analysis: Investigated the cooperative interactions between model components through gradient similarity measures, providing a new approach to assess the confidence and robustness of saliency scores.\nExtensive experiments on medical imaging and text classification datasets demonstrate that MHEX enhances both classification performance and saliency map clarity compared to existing methods.\nFor those who wish to explore the implementation of MHEX, we kindly refer you to the guidelines in Appendix E."}, {"title": "A. Mathematical Derivations", "content": ""}, {"title": "A.1. Entropy Reduction", "content": "To quantify the impact of non-negativity, we analyze entropy reduction through ReLU activation. Assume the output features T follow a standard normal distribution N(0, 1), with differential entropy:\n$\\mathcal{H}(T) = \\int_{-\\infty}^{\\infty} p(t) \\log p(t) dt = - \\frac{1}{2} \\log(2 \\pi e).$\nAfter applying ReLU, the distribution becomes:\n$p_{T_{\\text{ReLU}}}(t) = \\begin{cases}0.5 & \\text{if } t = 0, \\\\frac{e^{-t^{2} / 2}}{\\sqrt{2 \\pi}} & \\text{if } t > 0.\\end{cases}$ The total entropy $\\mathcal{H}(T_{\\text{ReLU}})$ consists of discrete and continuous components:\n$\\mathcal{H}(T_{\\text{ReLU}}) = \\mathcal{H}_{\\text{discrete}} + \\mathcal{H}_{\\text{continuous}},$ where:\n$\\mathcal{H}_{\\text{discrete}} = \\frac{1}{2} \\ln 2, \\quad \\mathcal{H}_{\\text{continuous}} = - \\int_{0}^{\\infty} \\frac{e^{-t^{2} / 2}}{\\sqrt{2 \\pi}} \\ln \\left(\\frac{e^{-t^{2} / 2}}{\\sqrt{2 \\pi}}\\right) d t = \\frac{1}{2} \\ln (2 \\pi e) - \\ln 2.$\nThe overall entropy after ReLU activation is:\n$\\mathcal{H}(T_{\\text{ReLU}}) = \\frac{1}{2} \\ln (2 \\pi e) - \\frac{1}{2} \\ln 2.$"}, {"title": "A.2. Non-Negativity and Confidence Saliency Maps", "content": "Building on the entropy reduction, we leverage the non-negativity constraint to generate \"confidence saliency maps.\" ReLU's non-negativity ensures that a neuron's contributions can be split into positive and negative components. For class i, let $w^i \\in \\mathbb{R}^C$ denote the weights connecting feature channels to the class. These are decomposed as:\n$w_{\\text{positive}}^i = \\max(w^i, 0), \\quad w_{\\text{negative}}^i = \\min(w^i, 0).$\nTo control the influence of negative contributions, we introduce a parameter $\\alpha \\in [0, 1]$:\n$w_{\\text{adjusted}} = w_{\\text{positive}}^i + \\alpha \\cdot w_{\\text{negative}}^i.$\nThe Class Activation Map (CAM) for class i is then computed as:\n$CAM^i(x, y) = \\sum_{j=1}^{C} w_{\\text{adjusted}}[i] \\cdot f_j(x, y),$ where $f_j(x, y)$ represents the activation of feature channel j at spatial location (x, y)."}, {"title": "A.3. Salience Sharpness", "content": "To further enhance interpretability, we analyze the \"specificity\" of each feature's contribution to class predictions. In fully connected layers, a feature can contribute to the activations of neurons across multiple classes. To quantify this, we define salience sharpness (SS) for positive and negative contributions:\n$SS_{\\text{positive}}(i) = \\frac{w_{\\text{positive}}^i[i]}{\\sum_{k=1}^{n} |w_{\\text{positive}}^i[i]| + \\epsilon},$\n$SS_{\\text{negative}}(i) = \\frac{w_{\\text{negative}}^i[i]}{\\sum_{k=1}^{n} |w_{\\text{negative}}^i[i]| + \\epsilon}.$\nHere, n is the total number of classes, and $\\epsilon > 0$ is a small positive constant (e.g., $10^{-8}$) to avoid division by zero.\nThe salience sharpness is used to adjust the weights, emphasizing class-specific contributions:\n$w_{\\text{final}}[i] = \\begin{cases}w_{\\text{positive}}[i] & |SS_{\\text{positive}}(i) > s s| \\\\+ w_{\\text{negative}}[i] & |SS_{\\text{negative}}^*(i) > s s|.\\end{cases}$"}, {"title": "A.4. Pearson Correlation Coefficient and p-value", "content": "The Pearson correlation coefficient (r) measures the linear relationship between two variables X and Y. It ranges from -1 to 1:\n$r = \\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_i - \\overline{X})^2 \\sum_{i=1}^{n}(Y_i - \\overline{Y})^2}},$ where X and Y are individual observations, $\\overline{X}$ and $\\overline{Y}$ are the sample means, and n is the number of data points. An r-value close to \u00b11 indicates a strong linear relationship, while r\u2248 0 suggests little or no linear correlation.\nTo assess the statistical significance of r, we test the null hypothesis that X and Y are linearly uncorrelated (r = 0). Under this assumption, the following t-statistic follows a t-distribution with n - 2 degrees of freedom:\n$t = \\frac{r \\sqrt{n-2}}{\\sqrt{1 - r^2}}.$\nThe p-value is derived from the t-distribution: - A small p-value (e.g., p < 0.05) indicates that the observed correlation is unlikely due to random chance, providing evidence against the null hypothesis. - A large p-value suggests insufficient evidence to reject the null hypothesis.\nWhen interpreting r and p: A high absolute value of r (e.g., |r| > 0.5) implies a stronger linear correlation. - A small p-value (e.g., p < 0.05) denotes statistical significance, meaning the correlation is not easily attributed to random variation. - If p is not small, we cannot confidently conclude a significant linear relationship.\nTaken together, r and p provide insight into both the strength and the statistical reliability of the linear relationship between two variables."}, {"title": "B. Additional Quantitative Study Details", "content": ""}, {"title": "B.1. Effective Average Drop (EAD) Weighting Function", "content": "To define the area-based weighting function f(x), we consider the function:\n$f(x) = 5 \\cdot \\frac{x}{1 + kx^{2n}},$ where x \u2208 [0,1] is the proportion of the saliency map's area, and a, k, n are parameters controlling the shape of the function.\nWe set the optimal area E = 0.25, and design f(x) such that:\n$f(E) = 1, \\quad f'(E) = 0.$\nSolving these conditions, we obtain one of the solutions:\n$f(x) = 5 \\cdot \\frac{x}{1 + 256 x^5}.$"}, {"title": "B.2. Insertion and Deletion Curves", "content": "For each sample, the saliency score $S_i$ is used to rank pixels by their importance. The model's confidence is measured as pixels are progressively inserted or deleted in increments.\nThe average value at each step i is computed as:\n$Avg \\text{ } Curve_i = \\frac{1}{N} \\sum_{j=1}^{N} y_j (f(I_j, S^{(i)})),$ where:\n\u2022 $I_j$: Input image for the j-th sample.\n\u2022 $y_j$: True label for the j-th sample.\n\u2022 $S^{(i)}$: The modified image at step i, with pixels inserted or deleted based on the saliency score.\n\u2022 f: The function representing the insertion or deletion operation.\nBy plotting the average confidence over all samples at each step, we obtain the insertion and deletion curves. The Area Under the Curve (AUC) is then computed to quantify the overall performance."}, {"title": "B.3. Results of Insertion and Deletion Curves", "content": "We employ the Insertion and Deletion curves to evaluate the contribution of saliency scores to model predictions (Petsiuk, 2018). The insertion curve measures the model's confidence recovery as salient regions are progressively added to a baseline image, while the deletion curve measures the confidence degradation as salient regions are progressively removed.\nInsertion and deletion curves assess the impact of saliency scores on model confidence. The insertion curve tracks confidence recovery as salient regions are added to a blank image"}]}