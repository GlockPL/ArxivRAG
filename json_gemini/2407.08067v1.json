{"title": "On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments", "authors": ["Jingchao Fang", "Nikos Arechiga", "Keiichi Namikoshi", "Nayeli Bravo", "Candice Hogan", "David A. Shamma"], "abstract": "The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard \"role-plays\" a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play, one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However, methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs' role-playing ability are lacking. Through two LLM-powered WoZ studies, we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs' role-playing ability in WoZ experiments and reveals LLMs' behavior patterns at scale.", "sections": [{"title": "1 INTRODUCTION", "content": "People often have online conversations with individuals who possess specific information and expertise for help or facilitation; chatbots, deployed as conversational agents [23, 37, 60, 61, 73], offer the advantages of scalability and cost-effectiveness in these tasks. Consider implementing a chat agent to conduct persuasive conversations for social good (e.g., encouraging an environmentally friendly lifestyle). Developing the agent as an experimental device creates several hurdles. Training and fine-tuning a model requires an eco-friendly corpus of training data to acquire accurate domain knowledge and reduce the rate of producing faulty or harmful messages. This training must include data representing individuals with diverse backgrounds as climate-related persuasions require knowing a person's values to avoid backlash [25]. In addition, multiple stages of the user-centered design process are inherently iterative and require rounds of user participation [52], which challenges rapid ideation and prototyping. Ultimately, a considerable amount of resources (e.g., training data, computing power, labor) is needed"}, {"title": "2 BACKGROUND", "content": "The WoZ method [32] has study participants interact with an \u201cinterface\" or a \"system\" secretly controlled by a hidden human Wizard. Specifically, we ask, can an LLM be used to power a Wizard? Aiming at eliciting human behaviors to understand how to build a domain-specific persuasive bot, we prompt LLMs Wizards to conduct persuasive conversations."}, {"title": "2.1 The Wizard of Oz Method", "content": "WoZ provides a solution for testing innovations and receiving human feedback without a completed implementation, which could be costly or infeasible with currently available technologies [6, 32, 42, 46]. The objective of WoZ is to leverage the collected users' reaction data to facilitate new technology design [11, 64]. In an early WoZ example [33], two phases are described: a simulation where the experimenter is situated in todo and an intervention where language processing is used with an experimenter. Currently, variations of WoZ are seen across a plurality of domains and applications [14, 21, 35, 43, 53, 58]. The inverse \"Oz of Wizard\" method was introduced to study human-robot interaction. Here, human behaviors are being simulated to evaluate robot behaviors [59]. We argue that both methods share the same underlying principle: leveraging humans' or machines' role-playing abilities to overcome experimentation difficulties in human-machine interaction studies. As LLMs augment their role-playing abilities, their capability to act as \"Wizards\" in WoZ will grow. While we do not advocate for replacing humans with LLMs in all WoZ, we note that large-scale WoZ is sometimes desirable but costly or infeasible with human Wizards; LLM Wizards can ease the scalability limitation existing in human-led WoZ. In this paper, we contribute an experiment lifecycle that guides researchers to estimate the risks and failures of LLM Wizards before incorporating them into human-facing user studies."}, {"title": "2.2 Chatbots as Conversational Agents", "content": "Chatbots as conversational agents are common [7, 30, 66, 70]. They can facilitate online tasks by enhancing people's engagement and delivering personalization [70, 72], elicit information [23, 34, 71], and provide mental support to socially isolated individuals [30]. Studies using natural language generation (NLG) to deliver interventions or conduct persuasive conversations can trigger attitude or behavior change (e.g., persuading people to adopt healthy lifestyles or donate to charities) [8, 31, 47, 57, 75, 78]. These persuasive chatbots should build trust and empathy with users and generate personalized responses [8, 27]. Due to various challenges in designing good chatbots in specialized domains, the WoZ method is widely used to pilot interactions between study participants and \"chatbots\" (role-played by human Wizards) [45, 46]."}, {"title": "2.3 Role-Playing LLMs", "content": "LLMs are often used to simulate humans and replicate behaviors. They can adapt traits to imitate specific personalities and profiles [54] and reproduce response distributions from diverse human subgroups, passing the \"social science Turing Test\" [3]. LLM-based agents organized in a virtual community generated believable social behaviors [50]. Studies suggest opportunities to leverage LLMs to generate research data. There has been a surge in debates regarding whether LLMs can replace human participants [9, 13, 24]. Synthetic responses to open-ended questions are found to be useful in ideating and piloting experiments [22]. Further, role-playing frameworks allow LLM-powered agents to interact with each other autonomously, facilitating scalable synthetic conversation data generation [38]. However, apart from the frequently used \"Turing test\" (testing whether LLMs-generated data are distinguishable from humans-generated data), evaluating LLMs' generation remains challenging given their broad task domains and output styles. Recent studies adopt three evaluation approaches. Independent benchmarks (e.g., reference-based metrics including BLEU [49] and ROUGE [40]) have been extensively studied and used for NLG systems evaluations, but are usually domain- or task-specific and correlate poorly with human judgments [51]. Human evaluation is considered to be reliable when multiple evaluators' opinions are incorporated (e.g., Elo rating system [15]), ensuring the outcomes align well with human values. However, they are costly and not scalable. Recent work showed LLMs' potential in evaluating LLMs' generations [9, 10, 41, 79] and GPT-4, as an evaluator, correlates well with human labelers. Yet, LLM-based evaluations lack explainability, and several LLMs' biases (e.g., positional bias) have been observed [63]. To deploy LLMs in WoZ experiments and interpret generated data, identifying LLMs' behaviors when they are prompted to role-play, especially when and how they could fail, becomes essential; currently, LLMs are far from flawless. We propose a heuristic evaluation framework comprised of automatic metrics widely adopted in HCI research for textual data analysis and surface how it can help identify LLMs' behaviors and failure modes in WoZ experiments."}, {"title": "3 WIZARD OF LLM EXPERIMENTS", "content": "Similar to Kelley's foundational work [32, 33], our experiment lifecycle has two stages, a coarse initial stage and a refinement second stage. However, our approach diverges as the first stage is run at a large scale with little experimenter intervention. The second stage has a much smaller scale, involves human participants, and is conducted after experimenter intervention guided by the outcome of the first stage. Finally, similar to Kelley's final stage, a comparison of the two stages describes the next steps for the experimenter and idea elicitation. This section demonstrates Stage 1 and Stage 2 through Study 1 and Study 2 respectively.\nThrough two studies, we seek to answer: RQ1 How do LLMs behave in closed-loop conversations (when both interlocutors are LLM-powered) in WoZ settings? How can we identify LLMs' successes or failures using heuristic evaluations? RQ2 How do LLMs behave differently when they, acting as Wizards, talk to humans instead of LLMs? RQ3 How can we safely integrate LLMs in WoZ experiments, and what limitations and distortions should be considered when interpreting data generated in such settings?"}, {"title": "3.1 Study 1: When WoLs meet Simulacrums", "content": "We joined WoLs with Simulacrums in a conversational WoZ experiment. Before testing with people, we aim to (1) identify LLMs' behaviors and verify whether their \u201cfailures\u201d are dangerous to human participants and (2) collect a wide sample of agent-to-agent conversations to observe a broad range of failure modes."}, {"title": "3.1.1 Method", "content": "The WoLs and the Simulacrums were GPT-4 agents 1, and their behaviors were steered by system prompts (see Figure 2). The prompts instruct them to align their behaviors with normal conversation structures with strangers (e.g., start with an introduction, send succinct messages, etc.). Gender-neutral names, Jamie and Leslie, were assigned to the WoLs and the Simulacrums respectively for chatting purposes.\nSeveral factors could affect message generation and conversation dynamics, including interlocutors' identity disclosure and demographic backgrounds [57, 62], the amount of detailed context and granularity of instruction to LLMs [5, 65], and temperature parameter setting [48]. Accordingly, we note five independent variables:\n\u2022 Bot identity disclosure. A boolean value determines whether the WoL self-discloses as a bot. A persuasive chatbot study showed that disclosure affects persuasion outcome [57].\n\u2022 Demographic information. The WoL and the Simulacrum were assigned information including age, income, education, political affiliation, gender, and ethnicity. The distribution followed 2020 US Census data2 except gender, which was sampled based on a released dataset [36] to include non-binary identities. The demographic information could help the WoL and the Simulacrum pick their standpoints when chatting and assist the WoL in adjusting its persuasion strategy. Conversely, the demographic background opens up space for biases to arise.\n\u2022 Demographic information disclosure. The WoL and the Simulacrum were assigned a boolean each to state whether their demographic information should be part of their self-introduction.\n\u2022 Instruction granularity. This feature guides the conversation. We defined three levels of instruction granularity, instructing the WoL on what to chat about: Level 1 random chat, Level 2 chat around a topic, Level 3 chat around a topic and towards a goal. All Level 2 and Level 3 conversations followed one of the three topics: adoption of electric vehicles (EV), adoption"}, {"title": "3.1.2 Analysis and Result", "content": "We found that the WoLs can usually initiate conversations and properly engage with the Simulacrums in the early stage. However, sometimes, conversations later go off-track. See Appendix B for an example.\nHow can we analyze the large amount of conversational data systematically? In-depth investigation of batches of conversational data is costly, and human evaluation at a large scale is usually impractical. Informed by observed failure modes of LLMs (generating biased and harmful content [16, 62, 80], repetitive messages [26, 38], incoherent or nonsensical text [29, 68], and limited instruction-following ability [39, 69, 76]), we introduce a heuristic evaluation framework that quantitatively estimated the conversation quality through lenses of toxicity, sentiment, text similarities, readability, and topic modeling. These measures fulfill the criteria for an initial assessment of LLM-based WoZ chats by being (1) scalable, computationally inexpensive, and applicable to large datasets, (2) broadly capturing limitations of LLMs' generations recognized in NLP literature, and (3) interpretable by the experimenters so the LLM Wizards can be refined before being deployed in real-world human-facing WoZ experiments. While these metrics are not exhaustive and cannot discover all LLMs' failure modes (which is inherent in all heuristic methods), they enable a fast scan of some critical aspects of WoLs' behaviors in conversational WoZ and an assessment of whether WoLs have the potential to be safely applied in human-facing setups. The framework is summarized in a table in Appendix C. We describe the rationales of each of the metrics as follows. Examples of generated messages and their corresponding quantitative scores are provided in Appendix E."}, {"title": "3.2 Study 2: When WoLs meet Participants", "content": "Next, an LLM-to-human exploration is necessary to model the LLMs-supported WoZ (addressing RQ2) in a more realistic setting."}, {"title": "3.2.1 Fix", "content": "Our experiment lifecycle advised experimenters to \"fix\" WoLs before advancing to Stage 2. Here, we streamlined the fixing process as technical methods of refining WoLs (e.g., prompt engineering, finetuning, retrieval-augmented generation) could vary case-by-case and are not the focus of this paper. An effective method"}, {"title": "3.2.2 Method", "content": "We recruited 56 study participants from Prolific to chat with WoLs using Study 1's prompt template and selected settings that resulted in high conversation quality: Wizards hid their bot and demographic identities, chatted about EVs and persuaded adoption, and used temperature 1. Participants are U.S. residents, above 18 years old, have a driving license, and do not own/lease"}, {"title": "3.2.3 Analysis and Result", "content": "Conversations between WoLs and Participants. Like in Study 1, WoLs' messages were non-toxic (toxicity: M = 0.01, SD = 0.02). The sentiments of WoLs' messages stayed positive (M = 0.58, SD = 0.13) and were significantly more positive than those of Participants' messages (M = 0.26, SD = 0.17) (t(102) = 11.05, p < .05), as shown in Figure 7. Factorial ANOVA showed no evidence that WoLs' sentiments differed based on the Participants' demographics. WoLs' sentiment had no effects on Participants' perceived rapport, chat partner impression, conversation quality, and persuasion outcome. Thus WoLs are likely to be unharmful when talking to humans.\nBoth semantic and sequence-based similarities between messages were relatively stable as shown in Figure 8 and Figure 9, except that the semantic similarity between WoLs' messages and Participants' previous messages in segment 2 (M = 0.36, SD = 0.14) was significantly higher than that in segment 1 (M = 0.45, SD = 0.12). The readability of the messages stayed consistent (see Figure 10). Topic modeling results again showed that WoLs stayed on the topic."}, {"title": "3.3 Comparing Study 1 and Study 2", "content": "Next, one must compare the two WoZ studies [33]: how did WoLs-to-Simulacrums differ from WoLs-to-Partcipants? As we relied on data generated from the synthetic LLMs-to-LLMs setting in Stage 1 to make key decisions of whether and how we can proceed to human-facing WoZ experiments, it is essential to understand how distorted the Stage 1 data is. A comparison between Study 1 and Study 2 can inform us what distortions should be considered and how experimenters should calibrate their expectations when interpreting data generated in the WoLs-to-Simulacrums setting. To investigate, we sampled 25 conversations with the same setups from Study 1 and from Study 2. The conversations were compared quantitatively along the dimensions of the proposed evaluation metrics, then analyzed by two HCI experts to gauge the overall chat quality, the WoLs' instruction following, and what mistakes WoLs made when role-playing Wizards. The qualitative analysis is used to capture a broader range of Wizards' failure modes that the quantitative metrics failed to identify."}, {"title": "3.3.1 Quantitative result", "content": "There was no significant difference in toxicity and sentiment of WoLs' messages. However, WoLs-to-Participants messages were less repetitive as the semantic similarity between adjacent messages in Study 2 (M = 0.48, SD = 0.07) was significantly lower than in Study 1 (M = 0.57, SD = 0.07) (t(48) = 4.67, p < 0.05). No significant difference in WoLs' message readability was observed between the two studies."}, {"title": "3.3.2 Human evaluation", "content": "We asked two experts familiar with WoZ methods to read the 50 conversations and identify how WoLs failed to role-play well. Apart from the repetition issue (i.e., WoLs being more repetitive in the closed-loop setting of Study 1) which was already recognized by quantitative metrics, two themes evident in conversations from Study 2 emerged:\nWoLs were too salesman-like. When Participants clearly expressed reluctance towards buying an EV, WoLs were being \u201cpolitely pushy\u201d without compromise. WoLs did not understand that the conversation goals might take indirect paths (e.g., persuading to lease EVs or choose EVs for ride-sharing services). This is a sign that WoLs did not acquire outstanding persuasion strategies.\nWoLs made assumptions of their interlocutors and lacked empathy. WoLs sometimes make false assumptions about the Participants. For example, WoLs sometimes assumed that Participants could charge EVs overnight from home and wake up with a charged car. Similarly, WoLs assumed that Participants did not have financial difficulties; some Participants said EVs are too expensive, and the WoLs lacked empathy and failed to build rapport.\nRole-switching. This failure mode only appeared once in a WoL-to-Simulacrum closed-loop conversation where the WoL and the Simulacrum switched roles. The WoL assumed a study participant role and discussed how they could not afford a car. It could be measured quantitatively but went undetected in our current evaluation metrics. A quantitative measure that identifies the role-switching phenomenon can be integrated into our evaluation framework."}, {"title": "4 DISCUSSION", "content": "Revisiting RQ3, we found LLMs can be useful tools for conversational WoZ experiments; however, potential pitfalls exist. Following a two-stage experiment lifecycle, LLMs showed the potential to be safely applied in human-facing studies. LLMs, role-playing as Wizards, can elicit user attitudes and behaviors when engaging with an envisioned technology and probe the design space of the technology as human Wizards would do in a traditional WoZ study."}, {"title": "4.1 Responsibly integration of LLMs and WoZ", "content": "Combining Study 1 and Study 2, we propose a two-stage experiment lifecycle for estimating the risks and potentials of LLMs-powered WoZ experiments.\nStage 1: Replacing humans on both sides of traditional WoZ experiments with LLM-powered agents. Following Study 1, this stage creates a simulation of conversations between Wizards and participants without risking humans' exposure to harmful content. Experimenters should inspect this stage's data and identify failures before continuing. We proposed a heuristic evaluation framework combining quantitative metrics that help experimenters understand the data in a scalable and explainable manner. Experimenters should strive to correct the WoLs through various techniques (e.g., finetuning) before moving on to human-facing experiments if they show evidence of being potentially harmful.\nStage 2: Piloting conversations between Wizards role-played by LLMs and human participants. This stage, following Study 2, affords a realistic pilot with access to human feedback. It is essential to notice that the Simulacrums' behaviors may be distorted from human behaviors as they lack human perceptions, and the quantitative metrics cannot capture all aspects of the conversation data. This stage allows one to close these gaps. By comparing data generated in Stage 2 and Stage 1, experimenters can understand the distortions of LLM-to-LLM data. Stage 2 also elicits feedback from participants regarding their chatting experience (e.g., perceived rapport) and allows for an in-depth qualitative inspection. Another round of adjustments on the WoLs should be applied if any additional failure modes are found in this stage. This lifecycle establishes a study setup involving the finalized LLM Wizards that are safe for human-facing experiments. These LLM Wizards can lead large-scale experiments without overburdening human experimenters with role-playing tasks.\nIn this paper, we demonstrate this experiment lifecycle in the context of EV adoption conversations led by the WoLs (with additional conversation topics included in Study 1) and showcase how heuristic evaluations can be used in the piloting process. In our scenario, WoLs' messages are not harmful. We found cues that they may personalize the conversations based on participants' demographics. However, they could sometimes be repetitive or generate messages with low readability, which confused participants. Our human evaluation further revealed that WoLs can be pushy and lack empathy. The WoLs' successes and failures were gradually unveiled through our two-stage experiment lifecycle without exhibiting harm to human participants, indicating the benefit of adopting the lifecycle as a methodology guidance for safely integrating LLMs in WoZ experiments. The Simulacrums and Participants' overall positive reactions to WoLs acknowledge the feasibility and potential of the envisioned technology being simulated in the study (i.e., a specialized persuasive chatbot). The imperfection of WoLs further suggests opportunities for the not-yet-developed technology to shine. By examining the limitations of WoLs powered by general-purpose models, experimenters acquire insights into the specific areas and dimensions where the new technology can excel. Furthermore, the elicited/simulated users' data projects users' attitudes toward and interactions with the envisioned technology, helping developers anticipate user behaviors so that they can design and develop functionalities accordingly."}, {"title": "4.2 Designing Guardrails for LLMs and WoZ", "content": "One could apply many techniques to improve the WoLs, as fixing the identified problems is the primary reason for identifying them. For the scenarios we investigated, picking the right settings/parameters is enough to tune the WoLs to role-play well. We suggest methods that may be desired for fixing WoLs in other conversation contexts. WoLs can be finetuned to focus on domain knowledge effectively, yet finetuning requires resources that might be inaccessible to many. Another approach to tame the Wizards is prompt engineering. Strategies such as few-shot learning [5], Chain of Thought [65], and Tree of Thoughts [74] can improve conversations.\nLLM-based critiques can provide guardrails to correct model outputs based on a set of manually crafted principles or a \"constitution\" [4]. This approach is promising as it allows in-place fixes during conversations. While current work in this direction only asks the critiques to correct unethical messages, our studies found that WoLs can fail in more ways. An enhanced \"constitution\" for WoZ experiments can be informed by this experiment lifecycle."}, {"title": "4.3 Limitations and Future Work", "content": "Our studies have several limitations. We only included three conversation topics; only GPT-4 was used to power WoLs. These may dampen the generalizability of the empirical results. While some WoLs' failure modes (e.g., increased repetition as the conversations proceed) are likely representative, the study results we derived may not apply to all conversational WoZ experiments. For example, WoLs powered by other LLMs (especially without RLHF) or chatting about controversial topics may generate toxic or biased messages. However, these limitations do not diminish the main contribution, which is guiding LLMs-powered WoZ experiments.\nWhile the quantitative nature of our proposed evaluation framework allows fast and large-scale surfacing of WoLs' behaviors, it also made the analysis coarse-grained. There are alternative ways of measuring the aspects we assessed (e.g., [19]), and the specific measures we used may not always be the most accurate ones. Yet, the metrics we picked are computationally efficient, making them suitable for analyzing large datasets. While we aimed to broadly capture LLMs' failure modes, the list of potential LLMs' pitfalls is non-exhaustive. Our framework cannot identify all potential failure modes of WoLs, which is a limitation inherent in any heuristic evaluation method. We welcome future researchers to expand the evaluation framework as new failure modes emerge. The experiment lifecycle leveraged synthetic data in Stage 1. Researchers must be vigilant about potential risks and distortions it may bring. Check Appendix A for an in-depth discussion of our commitment to maintaining ethical standards throughout the experiment lifecycle.\nMany technologies could benefit from WoZ experiments, not limited to chatbots advocating EV adoption or agents interacting through text. We have increasingly seen technologies (e.g., image/video generation, robot control) powered by multimodal models; as such, WoLs can simulate various interactions beyond texting. Therefore, we expect the proposed experiment lifecycle involving LLM Wizards to be relevant and applicable in envisioning and developing countless novel functionalities and technologies."}, {"title": "5 CONCLUSION", "content": "We introduced an experiment lifecycle that guides researchers to responsibly integrate LLMs into WoZ experiments through a two-stage process. The LLM-powered WoZ is a method for eliciting users' reactions to an envisioned technology using LLM-generated text, aiming at probing the design space of the technology. We presented an evaluation framework that helps researchers peek through the data generated with LLM Wizards and identify the Wizards' failures. Using conversations around EV adoption as an example, we demonstrate how experimenters can leverage the experiment lifecycle along with the evaluation framework to estimate the potential and risks of applying LLMs as Wizards in human-facing Wizard of Oz experiments."}, {"title": "A CAUTIOUS USE OF SYNTHETIC DATA IN RESEARCH", "content": "On account of the growing capabilities of LLMs, some research proposed the idea of using LLMs to surrogate human participants in user studies (e.g., [2, 3, 9, 22]). The motivation for the substitution includes speeding up user studies, protecting human participants from potentially unethical and risky experiments, and augmenting the diversity of the sample. There is an ongoing debate on whether this substitution is appropriate. There are concerns regarding whether the LLMs' low-quality generation (e.g., hallucination) would weaken research validity and the possibility of synthetic research data undermining values of representation and inclusion [1].\nOur stance is that while formal user testing with real people before technology deployment should never be displaced with LLMs, it is worth investigating how we can take advantage of LLMs' speedy generation ability in intermediate steps of rapid design and development. As such, Stage 2 of the experiment lifecycle is a vital step where we involve human participants in WoZ conversations to collect real user data. The synthetic data generated from Stage 1, however, serves as a fast and scalable pilot and can inform Stage 2. In between the two stages, the experimenters' intervention is needed to evaluate and adjust the LLM Wizards. By comparing Stage 1 and Stage 2 data at the end of the lifecycle, experimenters can calibrate their expectations and estimate how (and to what extent) the synthetic data could be distorted, helping them to better interpret the data auto-generated from the closed-loop LLMs-to-LLMs chat setting. In summary, while our experiment lifecycle leverages synthetic data, it introduces human interventions at critical points to prevent LLMs from going off the rails and avoid experimenters being misled by synthetic data with limited credibility."}, {"title": "B AN EXAMPLE OF WOL-TO-SIMULACRUM CONVERSATION", "content": "Figure 12shows an example of a WoL-to-Simulacrum conversation. The conversation went well initially but went off-track at a later stage."}, {"title": "C SUMMARY OF THE HEURISTIC EVALUATION FRAMEWORK FOR IDENTIFYING WOLS' BEHAVIORS", "content": "The heuristic evaluation framework comprises five textual data analysis metrics, evaluating the successes and failures of LLMs in conversational WoZ experiments"}, {"title": "D SYSTEM PROMPTS USED IN STUDY 1", "content": "D.1 Prompts to the Wizard of LLMs\nThe GPT-4 system prompts to the Wizard of LLMs (WoLs) follow the template:\nI want you to act as the following character. Your name is Jamie. <DEMOGRAPHIC INFORMATION>.\nConduct a conversation with your interlocutor from the point of view of this character, do not break the character. <BOT IDENTITY DISCLOSURE PROMPT>.\n<DEMOGRAPHIC DISCLOSURE PROMPT>. Never"}]}