{"title": "On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments", "authors": ["Jingchao Fang", "Nikos Arechiga", "Keiichi Namikoshi", "Nayeli Bravo", "Candice Hogan", "David A. Shamma"], "abstract": "The Wizard of Oz (WoZ) method is a widely adopted research approach where a human Wizard \"role-plays\" a not readily available technology and interacts with participants to elicit user behaviors and probe the design space. With the growing ability for modern large language models (LLMs) to role-play, one can apply LLMs as Wizards in WoZ experiments with better scalability and lower cost than the traditional approach. However, methodological guidance on responsibly applying LLMs in WoZ experiments and a systematic evaluation of LLMs' role-playing ability are lacking. Through two LLM-powered WoZ studies, we take the first step towards identifying an experiment lifecycle for researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings that involve Wizards role-played by LLMs. We also contribute a heuristic-based evaluation framework that allows the estimation of LLMs' role-playing ability in WoZ experiments and reveals LLMs' behavior patterns at scale.", "sections": [{"title": "1 INTRODUCTION", "content": "People often have online conversations with individuals who possess specific information and expertise for help or facilitation; chatbots, deployed as conversational agents [23, 37, 60, 61, 73], offer the advantages of scalability and cost-effectiveness in these tasks. Consider implementing a chat agent to conduct persuasive conversations for social good (e.g., encouraging an environmentally friendly lifestyle). Developing the agent as an experimental device creates several hurdles. Training and fine-tuning a model requires an eco-friendly corpus of training data to acquire accurate domain knowledge and reduce the rate of producing faulty or harmful messages. This training must include data representing individuals with diverse backgrounds as climate-related persuasions require knowing a person's values to avoid backlash [25]. In addition, multiple stages of the user-centered design process are inherently iterative and require rounds of user participation [52], which challenges rapid ideation and prototyping. Ultimately, a considerable amount of resources (e.g., training data, computing power, labor) is needed before the chat agent is polished for early user testing. Thus, gauging target users' attitudes and interactions with the agent before putting in much development effort is usually desired.\nThe Wizard of Oz (WoZ) method [21, 32, 35] and its Oz of Wizard variant [59] could be helpful for this purpose. Both methods are designed to overcome experimentation obstacles by simulating automatic systems or humans when testing ideas with them is expensive or infeasible. In our persuasive chatbot example, we can set up a WoZ experiment where a human Wizard (experimenter) role-plays the to-be-developed technology and talks to participants. The experiment can elicit data revealing users' behaviors and attitudes when interacting with an envisioned technology before it is implemented, reducing the cost of design and development iterations. Yet, scaling up WoZ is challenging due to the required human labor for role-playing.\nClosely parallel with the \"role-playing\" in the WoZ method, recent studies propose leveraging large language models (LLMs) to \"role-play\" and simulate human-to-human or human-to-agent chats and generate synthetic data with low cost [38, 55]. The advancement of LLMs points to the potential of harnessing LLMs' speedy generation ability to role-play Wizards and scale up WoZ experiments. An overarching question that needs to be addressed is whether we can reliably use LLMs to elicit data that can be translated into design and development insights as a human Wizard would do in traditional WoZ.\nIn this paper, we take the first step towards exploring the feasibility of applying LLMs in conversational WoZ experiments. We present an experiment lifecycle (Figure 1) for safely piloting and integrating LLMs into WoZ experiments where GPT-4 empowered agents, instead of humans, role-play as Wizards at scale. The goal of the LLM Wizards is the elicitation of users' reactions to an envisioned technology being simulated in WoZ experiments (e.g., a specialized chatbot conducting persuasive conversations for social good), which provides design and development insights, rather than becoming the envisioned technology itself. The lifecycle is demonstrated via two studies, where GPT-4 agents act as \"Wizards\" (named as Wizard of LLMs, or WoLs) in WoZ experiments to talk to Simulacrums (also GPT-4 agents) and Participants (humans). This WoZ process generates insights guiding the development of new tools by: (1) collecting data that unveils how users engage with the to-be-invented tools on a large scale, and (2) understanding design spaces and opportunities for improvement for the envisioned tools, based on observed limitations LLM Wizards.\nFollowing traditional experimentation models (e.g., original WoZ methodology [33], many are called / refine / few are called framework [18]), the experiment lifecycle starts with a coarse, cheap, and large-scale WoLs-to-Simulacrums setting (Stage 1). While LLMs' role-playing bears promise, their role-playing ability in conversational WoZ experiments has not been formally evaluated, making the appropriateness of incorporating WoLs directly into human-facing experiments questionable. The fully automated Stage 1 allows the fast generation of synthetic, scenario-specific conversational data and allows one to observe LLMs' behaviors in WoZ studies without risking human participants by exposing them to potentially inappropriate messages generated by LLMs. Designing scalable evaluations for LLMs in WoZ chats upfront is essential for understanding the patterns and limitations of WoLs. Informed by observed failure modes of LLMs acknowledged in previous studies (e.g., producing biased and toxic text [16, 62, 80] and noncompliance with instructions [39, 69, 76]), we quantitatively estimated the WoZ conversation quality through lenses of toxicity, sentiment, text similarities, readability, and topic modeling. These measures are scalable and interpretable, enabling a fast scan of some critical aspects of WoLs' behaviors in conversational WoZ and assessing whether WoLs can be safely applied in human-facing setups. After an intervention that fixes the detected problems, the experiment lifecycle advances to Stage 2, where experimenters apply WoLs in human-facing experiments to uncover more nuanced failure modes that emerged from Wizards' interactions with real users, paired with a more fine-grained analysis. Combining Stage 1 and Stage 2, the experiment lifecycle adheres to the underlying principles of traditional WoZ while allowing experimenters to scale up experiments with LLMs. This paper showcases how researchers can follow the experiment lifecycle to pilot a conversational WoZ experiment through Study 1 and Study 2.\nIn addition to the experiment lifecycle, this paper offers two contributions: (1) Propose a heuristic evaluation framework for LLM-generated synthetic conversational data. Show how automatic"}, {"title": "2 BACKGROUND", "content": "The WoZ method [32] has study participants interact with an \u201cinterface\" or a \"system\" secretly controlled by a hidden human Wizard. Specifically, we ask, can an LLM be used to power a Wizard? Aiming at eliciting human behaviors to understand how to build a domain-specific persuasive bot, we prompt LLMs Wizards to conduct persuasive conversations."}, {"title": "2.1 The Wizard of Oz Method", "content": "WoZ provides a solution for testing innovations and receiving human feedback without a completed implementation, which could be costly or infeasible with currently available technologies [6, 32, 42, 46]. The objective of WoZ is to leverage the collected users' reaction data to facilitate new technology design [11, 64]. In an early WoZ example [33], two phases are described: a simulation where the experimenter is situated in todo and an intervention where language processing is used with an experimenter. Currently, variations of WoZ are seen across a plurality of domains and applications [14, 21, 35, 43, 53, 58]. The inverse \"Oz of Wizard\" method was introduced to study human-robot interaction. Here, human behaviors are being simulated to evaluate robot behaviors [59].\nWe argue that both methods share the same underlying principle: leveraging humans' or machines' role-playing abilities to overcome experimentation difficulties in human-machine interaction studies. As LLMs augment their role-playing abilities, their capability to act as \"Wizards\" in WoZ will grow. While we do not advocate for replacing humans with LLMs in all WoZ, we note that large-scale WoZ is sometimes desirable but costly or infeasible with human Wizards; LLM Wizards can ease the scalability limitation existing in human-led WoZ. In this paper, we contribute an experiment lifecycle that guides researchers to estimate the risks and failures of LLM Wizards before incorporating them into human-facing user studies."}, {"title": "2.2 Chatbots as Conversational Agents", "content": "Chatbots as conversational agents are common [7, 30, 66, 70]. They can facilitate online tasks by enhancing people's engagement and delivering personalization [70, 72], elicit information [23, 34, 71], and provide mental support to socially isolated individuals [30]. Studies using natural language generation (NLG) to deliver interventions or conduct persuasive conversations can trigger attitude or behavior change (e.g., persuading people to adopt healthy lifestyles or donate to charities) [8, 31, 47, 57, 75, 78]. These persuasive chatbots should build trust and empathy with users and generate personalized responses [8, 27]. Due to various challenges in designing good chatbots in specialized domains, the WoZ method is widely used to pilot interactions between study participants and \"chatbots\" (role-played by human Wizards) [45, 46]."}, {"title": "2.3 Role-Playing LLMs", "content": "LLMs are often used to simulate humans and replicate behaviors. They can adapt traits to imitate specific personalities and profiles [54] and reproduce response distributions from diverse human subgroups, passing the \"social science Turing Test\" [3]. LLM-based agents organized in a virtual community generated believable social behaviors [50]. Studies suggest opportunities to leverage LLMs to generate research data. There has been a surge in debates regarding whether LLMs can replace human participants [9, 13, 24]. Synthetic responses to open-ended questions are found to be useful in ideating and piloting experiments [22]. Further, role-playing frameworks allow LLM-powered agents to interact with each other autonomously, facilitating scalable synthetic conversation data generation [38].\nHowever, apart from the frequently used \"Turing test\" (testing whether LLMs-generated data are distinguishable from humans-generated data), evaluating LLMs' generation remains challenging given their broad task domains and output styles. Recent studies adopt three evaluation approaches. Independent benchmarks (e.g., reference-based metrics including BLEU [49] and ROUGE [40]) have been extensively studied and used for NLG systems evaluations, but are usually domain- or task-specific and correlate poorly with human judgments [51]. Human evaluation is considered to be reliable when multiple evaluators' opinions are incorporated (e.g., Elo rating system [15]), ensuring the outcomes align well with human values. However, they are costly and not scalable. Recent work showed LLMs' potential in evaluating LLMs' generations [9, 10, 41, 79] and GPT-4, as an evaluator, correlates well with human labelers. Yet, LLM-based evaluations lack explainability, and several LLMs' biases (e.g., positional bias) have been observed [63]. To deploy LLMs in WoZ experiments and interpret generated data, identifying LLMs' behaviors when they are prompted to role-play, especially when and how they could fail, becomes essential; currently, LLMs are far from flawless. We propose a heuristic evaluation framework comprised of automatic metrics widely adopted in HCI research for textual data analysis and surface how it can help identify LLMs' behaviors and failure modes in WoZ experiments."}, {"title": "3 WIZARD OF LLM EXPERIMENTS", "content": "Similar to Kelley's foundational work [32, 33], our experiment lifecycle has two stages, a coarse initial stage and a refinement second stage. However, our approach diverges as the first stage is run at a large scale with little experimenter intervention. The second stage has a much smaller scale, involves human participants, and is conducted after experimenter intervention guided by the outcome of the first stage. Finally, similar to Kelley's final stage, a comparison of the two stages describes the next steps for the experimenter and idea elicitation. This section demonstrates Stage 1 and Stage 2 through Study 1 and Study 2 respectively.\nThrough two studies, we seek to answer: RQ1 How do LLMs behave in closed-loop conversations (when both interlocutors are LLM-powered) in WoZ settings? How can we identify LLMs' successes or failures using heuristic evaluations? RQ2 How do LLMs behave differently when they, acting as Wizards, talk to humans instead of LLMs? RQ3 How can we safely integrate LLMs in WoZ experiments, and what limitations and distortions should be considered when interpreting data generated in such settings?"}, {"title": "3.1 Study 1: When WoLs meet Simulacrums", "content": "We joined WoLs with Simulacrums in a conversational WoZ experiment. Before testing with people, we aim to (1) identify LLMs' behaviors and verify whether their \u201cfailures\u201d are dangerous to human participants and (2) collect a wide sample of agent-to-agent conversations to observe a broad range of failure modes.\n3.1.1 Method. The WoLs and the Simulacrums were GPT-4 agents 1, and their behaviors were steered by system prompts (see Figure 2). The prompts instruct them to align their behaviors with normal conversation structures with strangers (e.g., start with an introduction, send succinct messages, etc.). Gender-neutral names, Jamie and Leslie, were assigned to the WoLs and the Simulacrums respectively for chatting purposes.\nSeveral factors could affect message generation and conversation dynamics, including interlocutors' identity disclosure and demographic backgrounds [57, 62], the amount of detailed context and granularity of instruction to LLMs [5, 65], and temperature parameter setting [48]. Accordingly, we note five independent variables:\n\u2022 Bot identity disclosure. A boolean value determines whether the WoL self-discloses as a bot. A persuasive chatbot study showed that disclosure affects persuasion outcome [57].\n\u2022 Demographic information. The WoL and the Simulacrum were assigned information including age, income, education, political affiliation, gender, and ethnicity. The distribution followed 2020 US Census data\u00b2 except gender, which was sampled based on a released dataset [36] to include non-binary identities. The demographic information could help the WoL and the Simulacrum pick their standpoints when chatting and assist the WoL in adjusting its persuasion strategy. Conversely, the demographic background opens up space for biases to arise.\n\u2022 Demographic information disclosure. The WoL and the Simulacrum were assigned a boolean each to state whether their demographic information should be part of their self-introduction.\n\u2022 Instruction granularity. This feature guides the conversation. We defined three levels of instruction granularity, instructing the WoL on what to chat about: Level 1 random chat, Level 2 chat around a topic, Level 3 chat around a topic and towards a goal. All Level 2 and Level 3 conversations followed one of the three topics: adoption of electric vehicles (EV), adoption of green household electrification, and donating to a charity, while the conversation goals (Level 3 only) are to persuade the interlocutors to adopt/donate. The embedded TOPIC PROMPT and GOAL PROMPT follow the instruction granularity. For example, when instruction granularity is set to Level 1, the TOPIC PROMPT fed to the WoL is \"You will initiate a random chat with your interlocutor\" while the GOAL PROMPT is left empty.\n\u2022 Temperature. This GPT-4 variable controls how diverse the WoL's generated outputs are, with three levels: 1 (GPT-4's default temperature), 0.5 (more stable), and 1.5 (more diverse outputs). The temperature of the Simulacrum stayed at the default value.\nWe generated 131 WoLs and Simulacrums conversations; each conversation includes 12 turns (i.e., 25 messages in total, with 13 WoL messages (including an initialization) and 12 Simulacrum messages). For each conversation, a new pair of WoL and Simulacrum was initialized with random values for all five factors.3.\nClosed-loop chatting between LLMs is an under-explored scenario. Can the WoLs lead meaningful conversations? Will the Simulacrums follow? Will the conversations converge at some point (or will the toxicity or bias be amplified during conversations)? We analyze these LLMs-generated dialogues to answer RQ1.\n3.1.2 Analysis and Result. We found that the WoLs can usually initiate conversations and properly engage with the Simulacrums in the early stage. However, sometimes, conversations later go off-track. See Appendix B for an example.\nHow can we analyze the large amount of conversational data systematically? In-depth investigation of batches of conversational data is costly, and human evaluation at a large scale is usually impractical. Informed by observed failure modes of LLMs (generating biased and harmful content [16, 62, 80], repetitive messages [26, 38], incoherent or nonsensical text [29, 68], and limited instruction-following ability [39, 69, 76]), we introduce a heuristic evaluation framework that quantitatively estimated the conversation quality through lenses of toxicity, sentiment, text similarities, readability, and topic modeling. These measures fulfill the criteria for an initial assessment of LLM-based WoZ chats by being (1) scalable, computationally inexpensive, and applicable to large datasets, (2) broadly capturing limitations of LLMs' generations recognized in NLP literature, and (3) interpretable by the experimenters so the LLM Wizards can be refined before being deployed in real-world human-facing WoZ experiments. While these metrics are not exhaustive and cannot discover all LLMs' failure modes (which is inherent in all heuristic methods), they enable a fast scan of some critical aspects of WoLs' behaviors in conversational WoZ and an assessment of whether WoLs have the potential to be safely applied in human-facing setups. The framework is summarized in a table in Appendix C. We describe the rationales of each of the metrics as follows. Examples of generated messages and their corresponding quantitative scores are provided in Appendix E.\nToxicity. Toxicity is the most important consideration when we gauge the potential of applying LLMs in real-world human-facing WoZ experiments. Our Simulacrums had profiles with diverse combinations of demographics, which made a good estimation of how"}, {"title": "3.2 Study 2: When WoLs meet Participants", "content": "Next, an LLM-to-human exploration is necessary to model the LLMs-supported WoZ (addressing RQ2) in a more realistic setting.\n3.2.1 Fix. Our experiment lifecycle advised experimenters to \"fix\" WoLs before advancing to Stage 2. Here, we streamlined the fixing process as technical methods of refining WoLs (e.g., prompt engineering, finetuning, retrieval-augmented generation) could vary case-by-case and are not the focus of this paper. An effective method"}, {"title": "4 DISCUSSION", "content": "Revisiting RQ3, we found LLMs can be useful tools for conversational WoZ experiments; however, potential pitfalls exist. Following a two-stage experiment lifecycle, LLMs showed the potential to be safely applied in human-facing studies. LLMs, role-playing as Wizards, can elicit user attitudes and behaviors when engaging with an envisioned technology and probe the design space of the technology as human Wizards would do in a traditional WoZ study."}, {"title": "4.1 Responsibly integration of LLMs and WoZ", "content": "Combining Study 1 and Study 2, we propose a two-stage experiment lifecycle (Figure 1) for estimating the risks and potentials of LLMs-powered WoZ experiments.\nStage 1: Replacing humans on both sides of traditional WoZ experiments with LLM-powered agents. Following Study 1, this stage creates a simulation of conversations between Wizards and participants without risking humans' exposure to harmful content. Experimenters should inspect this stage's data and identify failures before continuing. We proposed a heuristic evaluation framework combining quantitative metrics that help experimenters understand the data in a scalable and explainable manner. Experimenters should strive to correct the WoLs through various techniques (e.g., finetuning) before moving on to human-facing experiments if they show evidence of being potentially harmful.\nStage 2: Piloting conversations between Wizards role-played by LLMs and human participants. This stage, following Study 2, affords a realistic pilot with access to human feedback. It is essential to notice that the Simulacrums' behaviors may be distorted from human behaviors as they lack human perceptions, and the quantitative metrics cannot capture all aspects of the conversation data. This stage allows one to close these gaps. By comparing data generated in Stage 2 and Stage 1, experimenters can understand the distortions of LLM-to-LLM data. Stage 2 also elicits feedback from participants regarding their chatting experience (e.g., perceived rapport) and allows for an in-depth qualitative inspection. Another round of adjustments on the WoLs should be applied if any additional failure modes are found in this stage. This lifecycle establishes a study setup involving the finalized LLM Wizards that are safe for human-facing experiments. These LLM Wizards can lead large-scale experiments without overburdening human experimenters with role-playing tasks.\nIn this paper, we demonstrate this experiment lifecycle in the context of EV adoption conversations led by the WoLs (with additional conversation topics included in Study 1) and showcase how heuristic evaluations can be used in the piloting process. In our scenario, WoLs' messages are not harmful. We found cues that they may personalize the conversations based on participants' demographics. However, they could sometimes be repetitive or generate"}, {"title": "4.2 Designing Guardrails for LLMs and WoZ", "content": "One could apply many techniques to improve the WoLs, as fixing the identified problems is the primary reason for identifying them. For the scenarios we investigated, picking the right settings/parameters is enough to tune the WoLs to role-play well. We suggest methods that may be desired for fixing WoLs in other conversation contexts. WoLs can be finetuned to focus on domain knowledge effectively, yet finetuning requires resources that might be inaccessible to many. Another approach to tame the Wizards is prompt engineering. Strategies such as few-shot learning [5], Chain of Thought [65], and Tree of Thoughts [74] can improve conversations.\nLLM-based critiques can provide guardrails to correct model outputs based on a set of manually crafted principles or a \"constitution\" [4]. This approach is promising as it allows in-place fixes during conversations. While current work in this direction only asks the critiques to correct unethical messages, our studies found that WoLs can fail in more ways. An enhanced \"constitution\" for WoZ experiments can be informed by this experiment lifecycle."}, {"title": "4.3 Limitations and Future Work", "content": "Our studies have several limitations. We only included three conversation topics; only GPT-4 was used to power WoLs. These may dampen the generalizability of the empirical results. While some WoLs' failure modes (e.g., increased repetition as the conversations proceed) are likely representative, the study results we derived may not apply to all conversational WoZ experiments. For example, WoLs powered by other LLMs (especially without RLHF) or chatting about controversial topics may generate toxic or biased messages. However, these limitations do not diminish the main contribution, which is guiding LLMs-powered WoZ experiments.\nWhile the quantitative nature of our proposed evaluation framework allows fast and large-scale surfacing of WoLs' behaviors, it also made the analysis coarse-grained. There are alternative ways"}, {"title": "5 CONCLUSION", "content": "We introduced an experiment lifecycle that guides researchers to responsibly integrate LLMs into WoZ experiments through a two-stage process. The LLM-powered WoZ is a method for eliciting users' reactions to an envisioned technology using LLM-generated text, aiming at probing the design space of the technology. We presented an evaluation framework that helps researchers peek through the data generated with LLM Wizards and identify the Wizards' failures. Using conversations around EV adoption as an example, we demonstrate how experimenters can leverage the experiment lifecycle along with the evaluation framework to estimate the potential and risks of applying LLMs as Wizards in human-facing Wizard of Oz experiments."}, {"title": "A CAUTIOUS USE OF SYNTHETIC DATA IN RESEARCH", "content": "On account of the growing capabilities of LLMs, some research proposed the idea of using LLMs to surrogate human participants in user studies (e.g., [2, 3, 9, 22]). The motivation for the substitution includes speeding up user studies, protecting human participants from potentially unethical and risky experiments, and augmenting the diversity of the sample. There is an ongoing debate on whether this substitution is appropriate. There are concerns regarding whether the LLMs' low-quality generation (e.g., hallucination) would weaken research validity and the possibility of synthetic research data undermining values of representation and inclusion [1].\nOur stance is that while formal user testing with real people before technology deployment should never be displaced with LLMs, it is worth investigating how we can take advantage of LLMs' speedy generation ability in intermediate steps of rapid design and development. As such, Stage 2 of the experiment lifecycle is a vital step where we involve human participants in WoZ conversations to collect real user data. The synthetic data generated from Stage 1, however, serves as a fast and scalable pilot and can inform Stage 2. In between the two stages, the experimenters' intervention is needed to evaluate and adjust the LLM Wizards. By comparing Stage 1 and Stage 2 data at the end of the lifecycle, experimenters can calibrate their expectations and estimate how (and to what extent) the synthetic data could be distorted, helping them to better interpret the data auto-generated from the closed-loop LLMs-to-LLMs chat setting. In summary, while our experiment lifecycle leverages synthetic data, it introduces human interventions at critical points to prevent LLMs from going off the rails and avoid experimenters being misled by synthetic data with limited credibility."}, {"title": "B AN EXAMPLE OF WOL-TO-SIMULACRUM CONVERSATION", "content": "Figure 12shows an example of a WoL-to-Simulacrum conversation. The conversation went well initially but went off-track at a later stage."}, {"title": "C SUMMARY OF THE HEURISTIC EVALUATION FRAMEWORK FOR IDENTIFYING WOLS' BEHAVIORS", "content": "The heuristic evaluation framework comprises five textual data analysis metrics, evaluating the successes and failures of LLMs in conversational WoZ experiments (Table 1)."}, {"title": "D SYSTEM PROMPTS USED IN STUDY 1", "content": "D.1 Prompts to the Wizard of LLMs\nThe GPT-4 system prompts to the Wizard of LLMs (WoLs) follow the template:\nI want you to act as the following character. Your name is Jamie. <DEMOGRAPHIC INFORMATION>. Conduct a conversation with your interlocutor from the point of view of this character, do not break the character. <BOT IDENTITY DISCLOSURE PROMPT>.\n<DEMOGRAPHIC DISCLOSURE PROMPT>. Never flip roles with your interlocutor. Your message in each conversation turn should be succinct. Each message sent by you should include less than 80 words. <TOPIC PROMPT>. <GOAL PROMPT>. Do not be repetitive.\nThe contents enclosed with angle brackets are decided based on the independent variables. Specifically:\n\u2022 DEMOGRAPHIC INFORMATION is presented in the format of:\nage (with one of the following options): * 18 to 24 years * 24 to 55 years * 45 to 54 years * 55 to 64 years * 65 to 74 years * 85 years or older\nincome (with one of the following options):* Under $15,000 per year * $15,000 to 24,999 per year * $25,000 to 34,999 per year * $35,000 to 49,999 per year * $50,000 to 74,999 per year * $75,000 to 99,999 per year * $100,000 to 149,999 per year * $150,000 to 199,999 per year * over $200,000 per year\neducation (with one of the following options): * Some high school * High school diploma * Some college * College degree * Postgraduate degree\npolitics (with one of the following options): * Democratic party * Republican party * Libertarian party * Socialist party * Green party\ngender (with one of the following options): * Woman * Man * Transgender * Non-conforming\nethnicity (with one of the following options): * White * Latino or hispanic * Black or African American * Asian American * Other\n\u2022 BOT IDENTITY DISCLOSURE PROMPT is presented as:\nLet your interlocutor know you are a chatbot at the beginning of the conversation. Let them know your name in the format of \"I'm [NAME], a chatbot.\" (When bot identity disclosure is True.)\nPretend to be a human during the conversation. (When bot identity disclosure is False.)\n\u2022 DEMOGRAPHIC DISCLOSURE PROMPT is presented as:\nYour first message should have a self-introduction that includes all demographic information you are given. (When demographic information disclosure is True.)\nHowever, you should hide your demographic information, except your name, from your interlocutor. (When demographic information disclosure is False.9)\n\u2022 TOPIC PROMPT is presented as:\nYou will initiate a random chat with your interlocutor. Keep your interlocutor engaged with the chat. (For Level 1 instruction granularity.)\nYou will have a conversation about <TOPIC> with your interlocutor. Keep your interlocutor engaged with the chat. (For both Level 2 and Level 3 instruction granularity.)\n\u2022 GOAL PROMPT is presented as:\nAn empty string. (For both Level 1 and Level 2 instruction granularity.)\nYour conversation goal is to persuade your interlocutor to <GOAL>. (For Level 3 instruction granularity.)\n\u2022 The TOPIC and GOAL fields in the topic prompt and the goal prompt are filled with one of the following topic-goal pair. (Note that the GOAL field is omitted when the instruction granularity is set to Level 1 or Level 2):\nTOPIC: attitude towards electric vehicles; GOAL: adopt an electric vehicle."}]}