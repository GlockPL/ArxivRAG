{"title": "LEMOLE: LLM-ENHANCED MIXTURE OF LINEAR EXPERTS FOR TIME SERIES FORECASTING", "authors": ["Lingzheng Zhang", "Lifeng Shen", "Yimin Zheng", "Shiyuan Piao", "Ziyue Li", "Fugee Tsung"], "abstract": "Recent research has shown that large language models (LLMs) can be effectively\nused for real-world time series forecasting due to their strong natural language\nunderstanding capabilities. However, aligning time series into semantic spaces\nof LLMs comes with high computational costs and inference complexity, partic-\nularly for long-range time series generation. Building on recent advancements\nin using linear models for time series, this paper introduces an LLM-enhanced\nmixture of linear experts for precise and efficient time series forecasting. This\napproach involves developing a mixture of linear experts with multiple lookback\nlengths and a new multimodal fusion mechanism. The use of a mixture of linear\nexperts is efficient due to its simplicity, while the multimodal fusion mechanism\nadaptively combines multiple linear experts based on the learned features of the\ntext modality from pre-trained large language models. In experiments, we rethink\nthe need to align time series to LLMs by existing time-series large language mod-\nels and further discuss their efficiency and effectiveness in time series forecasting.\nOur experimental results show that the proposed LeMoLE model presents lower\nprediction errors and higher computational efficiency than existing LLM models.", "sections": [{"title": "1 INTRODUCTION", "content": "Long-term time series forecasting (LTSF) is a significant challenge in machine learning due to its\nwide range of applications. It has been important in various domains such as weather modeling\n(Ma et al., 2023; Lin et al., 2022), traffic flow management (Lv et al., 2014), and financial analysis\n(Abu-Mostafa & Atiya, 1996). Traditional statistical models like ARIMA (Box & Pierce, 1970)\nand exponential smoothing (Gardner Jr, 1985) have served as the foundation for forecasting tasks\nfor decades. However, these models often struggle to handle the complexities arising from real-\nworld applications, such as non-linearity, high dimensionality, and intricate temporal dynamics. In\nrecent years, deep learning models have emerged as a breakthrough in forecasting, revolutionizing\naccuracy and efficiency. These models can remarkably capture complex temporal patterns and in-\nteractions within the data. By leveraging the power of deep learning, they excel in forecasting tasks\nby effectively learning from large-scale datasets.\nIt is intriguing to note that while deep models (e.g., transformer-based models) have gained pop-\nularity and achieved significant success in various fields like computer vision, natural language\nprocessing, and time series research, they usually come at the cost of extensive computational bur-\ndens. Recent empirical studies have revealed scenarios where simpler and more computationally ef-\nficient linear-based models outperform complex deep learning models. Models like DLinear (Zeng\net al., 2023) and RLinear (Li et al., 2023) have demonstrated superior performance. Linear mod-\nels have proven effective in time series forecasting due to their capacity to capture and leverage\nthe linear relationships inherent in many time series datasets. By exploiting these linear relation-\nships, linear-based models can provide competitive predictions while maintaining computational"}, {"title": "2 RELATED WORK", "content": "efficiency. While linear-based models have demonstrated strengths in certain time series forecasting\nscenarios, it is important to acknowledge their limitations:\ni) Non-linear patterns: Real-world time series data often exhibit non-linear patterns resulting from\ncomplex underlying mechanisms, such as variable interactions or abrupt regime shifts. Linear mod-\nels may struggle to capture and model these non-linear relationships effectively (Chen et al., 2023;\nNi et al., 2024; Lin et al., 2024).\nii) Long-range dependencies: Linear models might face difficulties handling long-term dependen-\ncies within time series data. As the dependency structure becomes more intricate and extends over\nlonger periods, the effectiveness of linear models diminishes (Nie et al., 2023a; Liu et al., 2024c).\nTherefore, the challenge of developing a powerful prediction model that retains the high efficiency\nof linear models remains an open question.\nA mixture of linear experts is a promising solution to build such a model. Intuitively, multiple linear\nexperts can convert the original nonlinear time series prediction into several component prediction\nproblems. For example, some experts focus on trends, while others handle seasonals, or some deal\nwith short-term patterns while others learn long-term patterns. For example in (Ni et al., 2024),\nMixture-of-Linear-Experts (MoLE) is proposed to train multiple linear-centric models (i.e., experts)\nto collaboratively predict the time series. Additionally, a router model, which accepts a timestamp\nembedding of the input sequence as input, learns to weigh these experts adaptively. This allows that\ndifferent experts specialize in different periods of the time series.\nIn addition, incorporating multimodal knowledge into\npredictive models is also a promising solution. Recently,\nthere has been a significant surge of interest in multi-\nmodal time series forecasting. For example, TimeLLM\n(Jin et al., 2024) aims to align the modalities of time se-\nries data and natural language such that the capabilities of\npretrained large language model (LLM) from natural lan-\nguage process (NLP) can be activated to model time se-\nries dynamics. In practice, the alignment of multimodal-\nity in time series forecasting can be easily achieved by\nfine-tuning the input and output layers. In this way, both\ntime series and non-time series data (such as text data)\ncan be jointly inputted to LLM for multimodal time series forecasting. Although such alignment-\nbased LLMs have shown improvement in time series forecasting tasks, compared to linear models,\nthey are not very effective and suffer from slow inference speed (Liu et al., 2024b) as they have to\nuse large language model as time series predictor."}, {"title": "2.1 LINEAR MODELS AND LINEAR ENSEMBLE MODELS", "content": "While transformer-based models (Zhou et al., 2022; Nie et al., 2023a; Wu et al., 2021) have been\nsuccessful in Long-Term Time Series Forecasting (LTSF), (Zeng et al., 2023) questioned their uni-\nversal superiority and suggested simpler architectural approaches like DLinear and NLinear. DLin-\near (Zeng et al., 2023) decomposes time series into trend and season branches and uses linear models\nfor forecasting. Subsequent research by (Li et al., 2023) further confirmed the potential of linear-\ncentric models like RLinear and RMLP, which outperformed PatchTST (Nie et al., 2023a) in specific\nbenchmarks. Based on linear-based models and research focusing on the frequency domain, FITS\n(Xu et al., 2024) operates within the complex frequency domain. Although linear models are effi-\ncient, they are still limited in high-nonlinear time series (Chen et al., 2023; Ni et al., 2024). Related\nensemble linear models, such as TimeMixer (Wang et al., 2024) mixing the decomposed season and\ntrend components of time series from multiple resolutions. Then, multiple predictors are utilized\nto project the resolution features for the final prediction. Based on a mixture of experts, MoLE\n(Ni et al., 2024) applies multiple linear experts for forecasting, which is based on a router module\nto adaptively reweigh experts' outputs for the final generation. The proposed LeMoLE is different\nfrom them due to its multimodal fusion mechanism."}, {"title": "2.2 LLM-BASED MULTIMODAL FORECASTING", "content": "Pre-trained foundation models, such as large language models (LLMs), have driven rapid progress\nin natural language processing (NLP) (Radford et al., 2019; Brown, 2020; Touvron et al., 2023)\nand multimodal modeling (Caffagni et al., 2024; Hu et al., 2024). Several works have tried to\ntransfer LLMs' capabilities of other modalities to advance time series forecasting. However, the\nmain challenges lie in discussing the relationships between the two modalities, time series and text.\nSome previous works claim that aligning them is important and useful for multimodal forecasting.\nLLM4TS (Chang et al., 2023) use a two-stage fine-tuning process on the LLM, first supervised\npre-training on time series, then task-specific fine-tuning. Zhou et al. (2024) leverages pre-trained\nlanguage models without altering the self-attention and feedforward layers of the residual blocks. It\nis fine-tuned and evaluated on various time series analysis tasks to transfer knowledge from natural\nlanguage pre-training. Jin et al. (2024) reprograms the input time series with text prototypes before\nfeeding it into the frozen LLM to align the two modalities. Conversely, AutoTimes (Liu et al.,\n2024b) states the aligning is overlooked, resulting in insufficient utilization of the LLM potentials.\nIt presents token-wise prompting that utilizes corresponding timestamps and then concatenates the\ntime and prompt features as the multimodal input.\nAlthough these LLM-based time series methods have improved, their main limitation is their effi-\nciency compared with lightweight models like linear-based models. In this work, we rethink the\nuse of large language models for time series and strive to develop a more efficient and effective\nLLM-enhanced prediction model."}, {"title": "3 LEMOLE: LLM-ENHANCED MIXTURE OF LINEAR EXPERTS", "content": "Problem formulation. Given a lookback window $X_{1:T} \\in \\mathbb{R}^{T\\times C}$ (T is the length of history obser-\nvations and C is the number of variables), a task of time series forecasting aims to train a model F\nto predict its future values in a forecast window $X_{T+1:T+H}$ (H is the forecast length). Ideally, an\noptimal model $F^*$ builds a mapping between the lookback window and the forecast window:\n$X_{T+1:T+H} = F^*(X_{1:T}).$\n(1)\nHere, X denotes the estimation output of the forecast window. Figure 2 illustrates the proposed\nLeMoLE. Note that rather than simply combining multiple linear experts with the same lookback\nlengths as in (Ni et al., 2024), we set different lookback lengths for our linear experts. This allows\ndifferent experts to focus on various short-term and long-term temporal patterns. This section will\nformally elaborate on each component in the proposed model."}, {"title": "3.1 MIXTURE OF LINEAR EXPERTS", "content": "Linear models have demonstrated effectiveness in time series forecasting (Zeng et al., 2023). How-\never, due to their inherent simplicity, they are still limited to complex non-periodic changes in time\nseries patterns (Ni et al., 2024). In the proposed LeMoLE, we introduce a mixture of linear experts\nwith different lookback lengths to model both short-term and long-term temporal patterns.\nMathematically, let the number of experts be M. Given a time series window $X_{1:T}$, we generate its\nM views for M experts respectively. For the mth expert (m = 1, 2, . . ., M), we have the input as\n$X_{T-w_m:T}$. Here, $w_m$ is the window length for the mth expert (we assume $w_1 \\geq w_2 \\geq \\cdots \\geq w_M$).\nThen we can obtain the prediction of the mth expert by\n$Y^{(m)} = W_mX_{T-w_m:T} + b_m,$\n(3)\nwhere m = 1,...,M, $W_m \\in \\mathbb{R}^{H\\times w_m}$ and $b_m \\in \\mathbb{R}^{H\\times C}$ are trainable expert-specific parameters.\nBased on Equation (3), we can obtain M prediction output from M linear experts, denoted by\n$\\left\\{Y^{(1)},Y^{(2)},...,Y^{(M)}\\right\\}$. All of these outputs are with the same sizes of H \u00d7 C."}, {"title": "3.2 LLM-ENHANCED CONDITIONING MODULE", "content": "Prompting serves as a straightforward yet effective approach to task-specific activation of LLMs.\nTo leverage abundant multimodal knowledge to help time series forecasting, it is essential to design\nappropriate text prompts and the corresponding conditioning module to activate our multi-expert\nprediction network.\nStatic prompt. Figure 5 (left) in Appendix B shows a static prompt example we used on the ETTh\ndataset. It is about the data source description. Specifically, it includes what, where, and how the\ndata was collected. Also, it contains the meanings of variables in the multivariate time series. This\ninformation helps understand and assess the reliability and relevance of the particular prediction\ntasks. We assume the static prompt $P_S$ contains the $L_S$ length of texts (including punctuation\nmarks). To facilitate the LLM ability of text understanding, the LLM encoder denoted as LLM(\u00b7)\nis utilized to obtain the text representation vector $Z_S \\in \\mathbb{R}^{L_S \\times d_{llm}}$, i.e.\n$Z_S = LLM(P_S),$\n(4)\nwhere $d_{llm}$ is the dimension of the LLM encoder LLM token embeddings.\nDynamic prompt. Distinct from the static prompt, the timestamps in the datasets indicate when\nthe observations were recorded. We follow AutoTimes (Liu et al., 2024b) to use the timestamps\nas related dynamic text data and design our dynamic prompt as in Figure 5 (right) in Appendix B.\nWe aggregate textual covariates $T_{T-w_1},..., T_T$ to generate the dynamic prompt as $P_D\\in \\mathbb{R}^{L_D\\times 1}$.\nFormally, it is given by $P_D = Prompt([T_{T-w_1}, T_{T-W_1+1},..., T_T])$, where $w_1$ is the maximum\nlookback length in all experts. Then by LLM, the dynamic prompt is encoded into representations\n$Z_D \\in \\mathbb{R}^{L_D\\times d_{llm}}$ by\n$Z_D = LLM(P_D).$\n(5)"}, {"title": "3.3 CONDITIONING MODULE", "content": "After obtaining the representations $Z_S \\in \\mathbb{R}^{L_S\\times d_{llm}}$ and $Z_D \\in \\mathbb{R}^{L_D\\times d_{llm}}$ from the static prompt and\ndynamic prompt respectively, we can use them as conditions to activate our multi-expert prediction\nnetwork. Specifically, we first introduce two conditioning modules to fuse $Z_S$ and $Z_D$ respectively\nand then use light-weight CNN blocks to summarize all branches to get the final prediction.\nThe proposed conditioning module is based on the popular conditioning layer, FiLM (Perez et al.,\n2018). First, we use a CNN to map the multi-linear experts' outputs $\\left\\{Y^{(1)}, Y^{(2)}, . . ., Y^{(M)} \\right\\}$ into a\ntensor Y of H \u00d7 C, say $Y = CNN([Y^{(1)}; Y^{(2)}; . . . ; Y^{(M)}])$. Then, we fuse the static representation\n$Z_S \\in \\mathbb{R}^{L_S\\times d_{llm}}$ with Y by\n$Y'_S = \\gamma_S \\odot Y + \\beta_S,$\n(6)\nwhere $\\gamma_S = Linear_{S,1}Linear_{S,1}(Z_S)$, $\\beta_S = Linear_{S,2}Linear_{S,2}(Z_S)$. Here, $Linear^t$\nis the linear mapping to change the time dimension from $L_S$ to H. $Linear^c$ changes the channel\ndimension from $d_{llm}$ to C. Finally, we have $\\gamma_S \\in \\mathbb{R}^{H\\times C}$, $\\beta_S \\in \\mathbb{R}^{H\\times C}$, and the fused output\n$Y'_S \\in \\mathbb{R}^{H\\times C}$.\nSimilarly, when using dynamic representation $Z_D$ as condition, we have\n$Y'_D = \\gamma_D \\odot Y + \\beta_D,$\n(7)\nwhere $\\gamma_D = Linear_{D,1}Linear_{D,1}(Z_D)$, $\\beta_D = Linear_{D,2}Linear_{D,2}(Z_D)$. Here, we\nobtain output $Y'_D \\in \\mathbb{R}^{H\\times C}$. Finally, we get the final prediction $\\hat{Y}$ by\n$\\hat{Y} = CNN_{final} ([Y; Y'_S; Y'_D]).$\n(8)\nGiven the final prediction $\\hat{Y}$, we can minimize the distance (e.g., mean square errors) between the\nground truths $X_{T+1:T+H}$ and predictions $\\hat{Y}$ to train the whole network in an end-to-end way\n$L = ||X_{T+1:T+H} \u2013 \\hat{Y}||_2.$\n(9)"}, {"title": "4 EXPERIMENT", "content": "To verify the proposed LeMoLE model's effectiveness and efficiency, we conducted extensive ex-\nperiments to address the following research questions. In Appendix F, we further provided the\nvisualization results about using the proposed LeMoLE on real-world time series.\nRQ1: How does LeMoLE perform on long-range prediction and few-shot learning scenarios?\nRQ2: Is multimodal knowledge, specifically text features, always useful on various datasets?\nRQ3: What about using linear experts in the frequency domain?\nRQ4: What are the effects of the hyperparameter sensitivity?\nRQ5: Is LeMOLE computationally efficient compared to existing LLM-based time series models?"}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Datasets. We conider four commonly-used\nreal-world datasets (Jin et al., 2024; Wu\net al., 2023): ETTh1, ETTm1, Electricity\n(ECL), and Traffic datasets. As in (Liu\net al., 2022b), we use the Augmented Dick-\nFuller (ADF) test statistic (Elliott et al., 1996)\nto evaluate if they are non-stationary. The null\nhypothesis is that the time series is not station-\nary (has some time-dependent structure) and\ncan be represented by a unit root. Extension to frequency domain. In the proposed LeMoLE, we introduce linear experts with vary-ing lookback window lengths to enhance ensemble diversity. In this section, drawing inspirationfrom a recent frequency-based linear model known as FITS (Xu et al., 2024) (Frequency Interpola-tion Time Series Analysis Baseline), we propose an extension of LeMoLE called LeMoLE-F, whereeach linear expert is implemented using FITS. Consequently, we can rename the original LeMoLEin the time domain as LeMoLE-T. The setup of lookback window lengths of LeMoLE-F is the sameas that in LeMoLE-T. In LeMoLE-F, each linear expert takes the input as a frequency domain pro-jection of a specific lookback window. This projection is achieved by applying a real FFT (FastFourier Transform). Subsequently, a single complex-valued linear layer is used to interpolate thefrequencies. To revert the interpolated frequency back to the time domain and obtain the output ofthe linear expert, zero padding and an inverse real FFT are applied."}, {"title": "4.2 MAIN RESULTS (RQ1)", "content": "Long-range forecasting. In this section, we consider long-range prediction tasks on four real-world\ndatasets: Electricity, Traffic, ETTh1, and ETTm1. As shown in Table 1, the proposed\nmodel achieves the best average performance in the long-range prediction tasks. Specifically, the\nproposed models consistently outperform the linear ensemble model MoLE and TimeMixer with an\naverage improvement of 23.17% and 20.70% respectively in terms of MSE, which demonstrates the\neffectiveness of using multimodal knowledge. As a large language model is used for text information\nextraction, the proposed mixture of linear experts allows for better modeling of nonlinear parts in\nreal-world time series. By comparing the LLM-based time series model GPT4TS and AutoTimes,\nwe also have average improvements of 11.76% and 29.85% in terms of MSE. This demonstrates\nthe effectiveness of the proposed multimodal fusion strategies and multiple linear expert ensembles.\nDirectly aligning language models for time series may degrade the forecasting performance due\nto the essential differences between the time series structure and the natural language syntactic\nstructure (Tan et al., 2024). Due to the lack of space, MAE results are reported in Appendix E.\nFew-shot forecasting refers to the scenario of making predictions with limited data, which is partic-\nularly difficult for data-driven deep learning methods. Recently, LLM time series models Jin et al.\n(2024); Zhou et al. (2024) have shown impressive few-shot learning capabilities. In this section,\nwe will evaluate whether the proposed multimodal time series fusion mechanism outperforms those\nLLM-alignment methods in forecasting tasks. We will follow the setups in (Zhou et al., 2024; Jin\net al., 2024) for fair comparisons, and we will assess scenarios with limited training data (i.e., using\nonly 10% of the training data, while keeping the test data the same for the long-range forecast-\ning task)."}, {"title": "4.3 COMPONENT ANALYSIS (RQ2)", "content": "This section explores the impact of the static and dynamic prompts in LeMoLE. We analyze the\neffects of removing each prompt individually, as well as both prompts, on long-range forecasting\nand few-shot forecasting tasks. Through this experiment, we aim to provide a detailed discussion on\nwhether and which text prompts improve prediction performance.\nThe results in Table 4 summarize the analysis of the components. It is evident that the prediction\nperformance declines when either or both components are removed from the proposed LeMoLE.\nThis shows that introducing the text modality using the proposed multimodality fusion strategy is\neffective. Interestingly, we observed that in the non-stationary ETT datasets, the proposed LeMoLE\nbenefits more from the dynamic prompt. On the other hand, for ECL, which is relatively easy due to\nits significant periodicity, the dynamic prompt is less important than the static prompt. This could\nbe explained by the fact that the dynamic prompt introduces more local temporal information suit-\nable for capturing non-stationary temporal behaviors. When a forecasting task exhibits significant\nperiodic behaviors, the static prompt with global information contributes relatively more."}, {"title": "4.4 MIXUP OF LINEAR EXPERTS IN TIME OR FREQUENCY DOMAIN (RQ3)", "content": "This section compares the proposed LeMoLE-T with its frequency extension LeMoLE-F introduced\nin Section 3. In Figure 3, the MSE prediction errors are reported with varying horizon H's. As\ncan be seen, the mixture of time experts proves to be a better choice than that of the frequency\nexperts in the proposed LeMoLE framework. This is mainly because LeMoLE-T contains experts\nwith different historical lookback lengths, allowing for good short- and long-term pattern modeling.\nOn the other hand, LeMoLE-F is based on the linear frequency model FITS (Xu et al., 2024),\nwhich emphasizes modeling low-frequency components and tends to generate smooth trends while\noverlooking detailed local variations."}, {"title": "4.5 EFFECTS OF THE NUMBER OF EXPERTS (RQ4)", "content": "In this experiment, we analyze the effects of the number of experts in the proposed LeMoLE. In\nFigure 4, we observed that when the prediction task is relative stationary and with significant pe-\nriodic, say Electricity and Traffic, the number of experts for mixture is relatively small.\nFor example, the best number of experts on Electricity and Traffic are 1 and 3, respec-\ntively. However, more experts are expected for more challenging datasets ETTh1 and ETTm1 that\nare highly nonlinear and non-stationary."}, {"title": "4.6 EFFICIENCY (RQ5)", "content": "Table 5 shows the number of trainable param-\neters and training/inference speed. The exist-\ning alignment-based LLM models suffer from\nslower training and inference speeds due to the\nimmensity of LLMs. While AutoTimes has a\nfaster inference speed compared to TimeLLM\ndue to its patching-based inference strategy, its\nautoregressive decoding process still necessi-\ntates multiple forward processes of LLM. The\ninference efficiency of LeMoLE over exist-\ning LLM time series models is due to: i) In\nLeMoLE, time series are modeled using a com-\nbination of linear experts instead of aligning a\nlarge language model with time series. This re-\nsults in lower computational costs. ii) Addition-\nally, the multimodal fusion module is imple-\nmented using lightweight CNNs, avoiding the\nintroduction of additional self-attention layers,\nwhich have quadratic complexity with the length of the time series for time series-text alignment."}, {"title": "5 CONCLUSION", "content": "This study introduces LeMoLE, a multimodal mixture of linear experts, for time series forecasting.\nBy harnessing the powerful capabilities of a pre-trained large language model, LeMoLE allows for a\nflexible ensemble of multiple linear experts by integrating static and dynamic text knowledge corre-\nlated to time series data. By comparing existing LLM time series models aligning text and time series"}, {"title": "G HYPERPARAMETER SENSITIVITY", "content": "Table 9 presents the results of our comparison tests between the choices of the number and type of\nexperts. Here, we observe under the same number of experts, the temporal linear expert is better\nthan the frequency expert in the average results.\nOur analysis shows that increasing the number of experts, in the LeMoLE and LeMoLE-F mod-\nels affects their performance, varying depending on the dataset as shown in Table 9. In the\nElectricity dataset, LeMoLE improves up to three experts, but additional experts add com-\nplexity without accuracy gains. In contrast, the Traffic dataset shows consistent improvements up\nto three experts. For the ETTh1 and ETTm1 datasets, they are nonstationary and present highly\nnonlinear behaviors, suggesting these datasets require more experts for time series modeling. The\nfrequency-based LeMoLE-F model benefits specific configurations but needs careful tuning for op-\ntimal results."}]}