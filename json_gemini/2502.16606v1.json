{"title": "Reasoning about Affordances: Causal and Compositional Reasoning in LLMs", "authors": ["Magnus F. Gjerde", "Vanessa Cheung", "David Lagnado"], "abstract": "With the rapid progress of Large Language Models (LLMs), it becomes increasingly important to understand their abilities and limitations. In two experiments, we investigate the causal and compositional reasoning abilities of LLMs and humans in the domain of object affordances, an area traditionally linked to embodied cognition. The tasks - designed from scratch to avoid data contamination \u2013 require decision-makers to select unconventional objects to replace a typical tool for a particular purpose, such as using a table tennis racket to dig a hole. In Experiment 1, we evaluated GPT-3.5 and GPT-40, finding that GPT-4o, when given chain-of-thought prompting, performed on par with human participants, while GPT-3.5 lagged significantly. In Experiment 2, we introduced two new conditions, Distractor (more object choices, increasing difficulty) and Image (object options presented visually), and evaluated Claude 3 Sonnet and Claude 3.5 Sonnet in addition to the GPT models. The Distractor condition significantly impaired performance across humans and models, although GPT-40 and Claude 3.5 still performed well above chance. Surprisingly, the Image condition had little impact on humans or GPT-40, but significantly lowered Claude 3.5's accuracy. Qualitative analysis showed that GPT-40 and Claude 3.5 have a stronger ability than their predecessors to identify and flexibly apply causally relevant object properties. The improvement from GPT-3.5 and Claude 3 to GPT-40 and Claude 3.5 suggests that models are increasingly capable of causal and compositional reasoning in some domains, although further mechanistic research is necessary to understand how LLMs reason.", "sections": [{"title": "1 Introduction", "content": "For a long time scholars have debated whether our conceptual system is grounded in embodiment and experience (Lakoff & Johnson, 1982; Harnad, 1990). The remarkable progress of large language models (LLMs; Zheng et al., 2024; Lu et al., 2024; Zhong et al., 2023) in a variety of tasks, despite these models being trained on text and static images without direct, embodied experience in the real world, has rekindled this debate (Harnad, 2024; Pavlick, 2023). This debate is unlikely to be resolved soon, but an intriguing way to explore the issues it raises is to test LLMs in domains that are closely connected to embodiment and interactive experience with the real world. Are LLMs able to perform well on tasks that have been thought to depend on embodied experience (Glenberg & Robertson, 2000)? One such approach is to test LLMs in the domain of object affordances and their capacity to reason about objects and their functional properties.\nRecent studies suggest that LLMs have developed increasing sensitivity to object affordances (Jones et al., 2022; Yiu et al., 2023), which may suggest that they are capable of solving such tasks without embodiment. However, as LLMs are trained on vast datasets containing extensive knowledge about objects and their uses, they might simply be retrieving memorised patterns in the training data when solving object affordance tasks. This ties in with a central debate about LLMs' reasoning abilities: do they reason in robust ways that generalise, or do they mainly memorise (Geirhos et al., 2020; Krakauer & Mitchell, 2023; McCoy et al., 2023)? To test LLMs' ability to reason and generalise about object affordances, not just memorise typical object uses, we need novel tasks that prevent LLMs from relying on training data patterns."}, {"title": "2 Background and Related Work", "content": "Causal reasoning and generalisation There have been several studies examining the causal reasoning abilities of LLMs, some positive (Lampinen et al., 2023; Thagard, 2024) and some negative (Joshi et al., 2024; Bao et al., 2024; Binz & Schulz, 2023). K\u0131c\u0131man et al. (2023) examined multiple varieties of causal reasoning in LLMs, including causal discovery, effect inference, and causal attribution. Results showed that fairly recent LLMs, such as GPT-4, perform remarkably well across the tasks compared to earlier LLMs, even approaching human baseline performance in one test of counterfactual reasoning. Jin et al. (2023) examined LLMs' formal causal reasoning based on Pearl's causal framework (Pearl & Mackenzie, 2018). GPT-4, the best performing model, achieved an accuracy of 64.28%, roughly 14% above chance. On the interventional tasks, GPT-4 scored 81.87%, suggesting that causal reasoning can, to some degree, be learnt passively (see also Lampinen et al., 2023).\nHowever, as K\u0131c\u0131man et al. (2023) emphasise, it is unclear how much of the LLMs' accuracy reflects general reasoning and how much is a result of retrieval of memorised patterns. Larger models tend to perform better, and this may be due to them generalising better than smaller models. Generalisation concerns a cognitive system's ability to deal with novelty. It is not a discrete either-or ability, but a spectrum (Chollet, 2019, p. 9-12). Testing LLMs' ability to generalise is typically done in one of three main ways: by introducing novel examples of tasks with familiar structures (e.g., Zhang et al., 2024), by designing tasks with entirely new structures (e.g., Wu et al., 2023), or by creating tasks that challenge LLMs to navigate a tension between associative retrieval and rule-based reasoning (e.g., Yiu et al., 2023). Associative retrieval involves recognizing and recalling previously encountered patterns, whereas rule-based reasoning requires understanding broad invariant principles.\nWu et al. (2024), for example, created a dual test set composed of standard and structurally modified tasks. Results showed that GPT-4, GPT-3.5, Claude, and PaLM all performed significantly worse in the non-standard tasks compared to the standard tasks (Wu et al., 2024). However, the performance decrease is substantially less steep for GPT-4 than for the other models. This suggests that the state-of-the-art model (at that time) may be capable of some general reasoning. Other studies have found a similar pattern, where the latest models are more capable of solving novel tasks than smaller models (for an example in analogical reasoning, see Webb et al., 2023; 2024, for one in maths, see Zhang et al., 2024).\nAffordances and tool innovation Sensitivity to affordances (i.e., action-possibilities available to an agent in an environment; Gibson, 2014) is closely connected to causal knowledge, as knowing what is and is not afforded means being sensitive to objects' causal properties. To know what can be done with various kitchen utensils requires knowledge about the effects of sharpness, shape, solidity, weight, and so on.\nIn a study comparing humans and a statistical natural language processing (NLP) model, Glenberg and Robertson (2000) showed that humans were sensitive to the difference between afforded and non-afforded sentences, while the NLP model, Latent Semantic Analysis, was not. The study involved asking participants and the NLP model to judge sentences such as \"He used his shirt/glasses/towel to dry his feet.\" More recently, Jones et al. (2022) tested language models on Glenberg and Robertson's tasks. They found that GPT-3 was sensitive \u2013 although not as sensitive as humans \u2013 to the difference between afforded and non-afforded words. While GPT-3 was trained exclusively on text, it was still to some degree able to identify that drying your feet with glasses is more surprising than doing it with a shirt. The two other models tested, BERT and ROBERTa, were not sensitive to this difference.\nIn a study which forms the inspiration for our experiments, Yiu et al. (2023) examined LLMs' ability to solve a tool innovation task. In their experiment, humans and LLMs were presented with a goal typically achieved with a specific tool (e.g., drawing a circle without a compass) and three alternative object options: 1) a superficially similar but functionally inappropriate object (ruler), 2) a superficially different but functionally suitable object (round-bottomed teapot), and 3) an irrelevant object (stove). In the innovation task, participants had to select the alternative object that would best accomplish the given goal. In the imitation task, they were asked to select which object would \"go best\" (i.e., which object is most associated) with the typical tool.\nUnlike selecting an associated object, Yiu et al. (2023) argued that discovering new applications for everyday objects requires going beyond statistical co-occurrence patterns (\u201cimitating"}, {"title": "3 Present studies", "content": "Our two experiments investigate LLMs' causal and compositional reasoning abilities in the domain of object affordances. In both experiments, we present human participants and LLMs with a task comprised of 20 tool innovation questions. In each task, the decision-maker must accomplish a given goal in the absence of a typical tool used to accomplish this goal. The question is whether the decision-maker can select an appropriate object to accomplish the goal, even though it is not typically used for that purpose. In our revision of Yiu et al.'s (2023) paradigm, we increase the difficulty of the task by including another associated but inappropriate object, for a total of four options. This makes random answers less likely to succeed.\nIn Experiment 1, we test OpenAI's GPT-3.5-turbo and GPT-40 models. We vary their temperature settings (which determines how variable or deterministic the model's output is), allowing us to examine the robustness of the models' reasoning skills. We also test the models with a chain-of-thought (CoT) prompt, as CoT prompting has been shown to improve LLM performance in several domains (Kojima et al., 2022; Wei et al., 2022). In Experiment 2, we test two further LLMs in addition to the GPT models, namely Anthropic's Claude 3 Sonnet and Claude 3.5 Sonnet models, and we add two new experimental conditions: Distractor and Image. The Distractor condition adds five further object options to the existing four options, making the task harder. The Image condition uses images instead of words to present the four object options. We also examine the effect of CoT prompting in Experiment 2.\nFinally, one limitation of past research is that it tends to focus on reporting quantitative performance scores, without going into qualitative detail about how LLMs responded to queries. This makes it difficult to evaluate what different AI models got right and wrong. Therefore, in both experiments, we conduct qualitative analyses of the answers generated by the models to better characterise their performance differences."}, {"title": "4 Experiment 1", "content": "4.1 Method\nParticipants This study was preregistered on AsPredicted.org (#184165). This study received ethical approval from the UCL Psychology Ethics Committee under code EP/2018/005. We recruited 100 participants from the United Kingdom using the online platform Prolific. The age range was 19-81 years (M = 42.3, SD = 12.7), with 50 male and 50 female participants. We paid participants \u00a38.62/h for the 6-minute study. All participants passed the attention checks, resulting in no exclusions.\nMaterials The experiment consisted of 20 tool innovation tasks presented to both human participants and LLMs (see Appendix A). Each task required achieving a goal without a typical tool used for that purpose. Participants chose from four options: two objects associated with but unsuitable for the task (\"associated objects\"), one unrelated but suitable object (\"afforded object\"), and one irrelevant object (\"distractor object\"). Associated objects were chosen on the basis of use in similar contexts (e.g., a plate is associated with a glass). An example of a task is presented below:\n4.2 Results\nAccuracy comparisons The mean accuracy of humans and the GPTs with different temperatures and prompts is shown in Figure 3 below. To test whether there were significant differences between the aggregated means of GPT-3.5, GPT-40, and humans, we performed a one-way ANOVA with Model Type (human, GPT-3.5-turbo, GPT-40) as the independent variable and accuracy as the dependent variable. It showed a statistically significant effect of Model Type on accuracy, F(2, 267) = 668.3, p < .001. Follow-up pairwise comparisons, shown in Table 1, revealed significant differences between all three pairs (p < .001). Both models demonstrated significantly lower accuracy compared to humans, with GPT-3.5 showing the largest difference.\nWhen comparing the accuracy of humans with various configurations of GPT-40, we found significant differences between humans and GPT-4o_t0, GPT-4o_t1, and GPT-4o_t2 (all p < .001; see Table 2). However, there was no significant difference between human performance and GPT-40 with CoT prompting (p = 1.00). Indeed, the mean accuracy of GPT-4o_cot was 85%, only 0.8% lower than the mean accuracy of human responses. This shows that chain-of-thought prompting enables GPT-40 to perform significantly better and reach a similar performance level as humans on this task. As can be seen in Figure 3, GPT-40_cot also performed significantly better than all temperature variations of GPT-40 with default prompting.\nEffect of prompt and temperature As CoT prompts were given to models at temperature 1, we statistically compared their CoT performance to that with a normal prompt at temperature 1. We found that CoT prompting had a significant effect for both GPT-3.5 and GPT-4o (t(33) = 3.13, p = .04 and t(33) = 6.57, p < .001). The effects of the different temperature settings are visible in Figure 3. While GPT-3.5's\nHuman problem-solving strategies When asked how they approached the tasks, 65% of human participants reported using visualization, 23% reported reasoning about object properties, and 12% relied on intuition. Two additional available options\u2014\"I don't know or remember how I solved the tasks\" and \"Other\" (with space for specification)\u2014were not selected by any participants. This suggests that the three categories selected by participants captured their perceived problem-solving strategies.\nQualitative analysis of GPT responses Here, we explore how GPT-3.5 and GPT-4o's responses to the chain-of-thought prompts differ. Ten responses per model were generated for each of the 20 questions. Figure 4 shows the models' accuracy on each question (see Appendix A for an overview of all the 20 questions). GPT-40 generated an average of 261 words per response, while GPT-3.5-turbo generated 163 words on average. Due to this dataset containing almost 85,000 words, we only present and analyse cases where responses differ in notable ways.\nQuestion 3 - hammering a nail This task required replacing a hammer for hammering a nail. The afforded object was a saucepan, while the two associated objects were a screwdriver and a saw. GPT-3.5 scored 30% on this task and GPT-40 scored 80%. Below are examples from GPT-3.5 and GPT-40's evaluations of the saucepan and the screwdriver for the task:\nOverall, GPT-3.5 demonstrated some sensitivity to the task requirements and object properties, but often lacked precision and occasionally provided contradictory responses. It did not consistently consider the\nimportance of a flat, sturdy surface. GPT-40, on the other hand, was precise and consistently emphasised the correct causal properties required for the task.\nQuestion 4 - transporting water This task was about replacing a glass for transporting water. The afforded object was a swimming cap and the two associated objects were a plate and a fork. GPT-3.5 scored 10% on this task and GPT-40 scored 100%. Below are examples of their evaluations of the swimming cap and the plate:\nHere, GPT-3.5 consistently noted that the plate is not a good option due to the risk of spilling water. However, it did not appreciate that the property which enables swimming caps to keep water out is the same property that can, if the cap is held upside-down, keep water inside. In other words, GPT-3.5 recognized this relevant property, but was not able to flexibly apply it to a new use case. GPT-40, on the other hand, consistently considered and correctly applied the relevant causal properties of the objects.\nQuestion 5 - cutting a cucumber into cubes Question 5 represents an interesting case, where both GPT-3.5 and GPT-40 did very poorly, but humans scored 80%. The task was to replace a kitchen knife for cutting a cucumber into cubes. The afforded object was a ruler and the two associated objects were a peeler and a whisk. Both GPT models argued that a ruler is not sharp enough, and instead they tended to opt for the peeler. They recognized that the peeler is not ideal, but still consistently viewed it as the best option. This task is arguably difficult for the following reason: it requires a fine-grained understanding that while rulers are not sharp enough to cut in general, they may be thin and hard enough to cut certain soft foods into cubes. This understanding may be difficult to acquire without embodied interaction with such objects.\nQuestion 7 \u2013 sweeping the floor This task involved replacing a broom for sweeping the floor. The afforded object was a wig and the two associated objects were laundry detergent and a bucket. GPT-3.5 scored 0% on this task and GPT-40 scored 100%. Below are examples of their evaluations of the wig and the bucket:\nUnlike GPT-40, GPT-3.5 was completely unable to appreciate the functional analogy between hair and a broom's bristles. Instead, GPT-3.5 emphasised that a wig is not a cleaning tool. GPT-40 solved the task well and successfully identified the causally relevant property of the wig."}, {"title": "4.3 Discussion", "content": "As expected, there was a significant difference between the performance of GPT-3.5 and humans. Regarding the difference between GPT-4o and humans, the evidence was mixed. With normal prompting, GPT-40 performed significantly below humans. This aligns with the findings of Yiu et al. (2023). With chain-of-thought prompting, on the other hand, which Yiu et al. did not test, GPT-4o achieved a mean accuracy nearly identical to that of humans. Moreover, the models' results at different temperatures revealed that GPT-40's reasoning was more robust and less likely to degrade when the temperature setting was increased. The large performance differences between GPT-3.5 and GPT-40 are consistent with the existing literature, which has found that larger models tend to outperform smaller, earlier models (Zheng et al., 2023; Chang & Bergen, 2023).\nFinally, the qualitative review of GPT-3.5 and GPT-4o's text output revealed some interesting patterns. Both models generally identified the causally relevant properties required to solve the tasks. GPT-3.5, however, was often unable to recognize and apply these properties in the unrelated but afforded objects. Rather than put the afforded object to a new and unusual use, it tended to focus on its usual function and area of use. Success required disregarding common use cases and flexibly applying familiar properties in new ways; in other words, to generalise beyond the familiar patterns. Moreover, GPT-3.5's factual statements were at times incorrect and contradictory. In comparison, GPT-40 showed a more precise appreciation of the causally relevant object properties and a consistent ability to flexibly apply the afforded objects appropriately. It was also evident in GPT-40's responses that its factual and causal knowledge was richer, more detailed, and less prone to error.\nThis finding aligns with previous research that has found that GPT-4 performs better than earlier models on tasks that are designed to prevent memorisation (Zhang et al., 2024; Wu et al., 2023). Unlike previous research, however, this study has offered a qualitative evaluation of the ways in which GPT-40 generalises better than the earlier GPT-3.5.\nWhen asked about their problem-solving strategy, the majority of human participants reported using mental simulation, with the rest reporting reasoning about properties or using intuition. This aligns with existing scholarship on the use of visualization and simulation in human causal reasoning (Gerstenberg, 2024; Johnson-Laird, 2010; Lagnado, 2021; Sloman & Lagnado, 2015). At the same time, it raises interesting new research questions. If humans report relying mostly but not exclusively on visualisation or simulation, is there any variability in how different LLMs solve these tasks? Future behavioural research should aim to develop experiments that can help us dissociate different cognitive problem-solving strategies in humans and LLMs. One benchmark, where solving tasks often requires visualising scenes, finds that LLMs still perform well below humans (Philip & Hemang, 2024), suggesting that LLMs struggle with visualisation (see also Rahmanzadehgervi et al., 2024)."}, {"title": "5 Experiment 2", "content": "The second experiment was designed as a replication and expansion of the first experiment. It had the same structure as Experiment 1, but with some modifications. It was preregistered on AsPredicted.org (#188326).\nImage and Distractor conditions In addition to the original text-based survey with four object options per task (hereafter called the Standard condition), we introduced two new conditions. First, a Distractor condition where the decision-maker was faced with nine options in total. This consisted of four associated but incorrect options, four irrelevant options, and one afforded option (see Appendix A for an overview). Second, we added an Image condition where the four original object options were shown as images instead of text. This allowed us to test (1) whether using images instead of text to show the options affects the accuracy of humans and LLMs, and (2) whether having more distractor options would affect the accuracy of humans and LLMs.\nOur hypothesis was that the image format would not negatively impact human performance, but that it would negatively impact the accuracy of LLMs. We believed that the LLMs, being mainly text-based, would perform less well in non-text formats. If a model has similar accuracy in the standard and image\n5.1 Method\nParticipants and new LLMs For the new experiment, we recruited a representative sample of 300 participants from the United Kingdom using Prolific, which is based on UK 2021/2022 census data and stratified across age, sex, ethnicity, and political affiliation.\u00b9 The age range of participants was 18-87 years (M= 47.0, SD = 16.2), with 141 male and 155 female participants. We paid participants \u00a38.75/hr for the 7-minute study. We excluded four participants: one participant because their data did not come through due to technical errors, two participants who failed the attention checks, and one participant for spending less than one second answering the majority of the tasks. This left us with a sample of 296 participants, with 101 in the Standard condition, 98 in the Image condition, and 97 in the Distractor condition. In addition to testing OpenAI's GPT-3.5 and GPT-40, we also tested Anthropic's Claude 3 Sonnet and Claude 3.5 Sonnet (hereafter referred to as Claude 3 and Claude 3.5).\nPrompts In Experiment 1, we used a standard prompt that asked the LLMs to \"only specify the chosen object,\" and a CoT prompt. The standard prompt restricted models to only generate the chosen option and not to elaborate on their reasoning, unlike the CoT prompt. This left open the possibility that the positive effect of CoT prompting found in Experiment 1 was to some degree due to the fact that the CoT prompt enabled longer answers, rather than the structure of the CoT prompt itself.\nIn Experiment 2, we changed the standard prompt to a more open-ended format, asking: \"Which one of these would you use to accomplish the task?\" This resulted in the models giving longer answers. The CoT prompt remained largely the same, asking models to \u201cEvaluate each option separately before specifying your choice.\" Rather than testing the models at different temperatures, we set the temperature of all models to 0. This is because Experiment 1 showed that model performance at temperature of 0 was higher and because it makes responses less variable and our results more reproducible. We only collected CoT data in the Standard and Image conditions, as collecting CoT data in the Distractor condition would entail the models separately evaluating 9 options, yielding excessively long responses.\n5.2 Results\nHumans compared to models The overall mean accuracies of humans and the two most powerful LLMs, GPT-40 and Claude 3.5, are shown in Figure 6 below (see also Table 3). Consistent with the results from Experiment 1, in the Standard condition, we found that only the performance of GPT-40 with CoT prompting did not differ significantly from that of humans (t(212) = 0.95, p = 1.00). All other models performed significantly worse than humans (see Appendix B, Table B1). In the Distractor condition, humans performed significantly better than both GPT-40 (t(152) = 2.52, p = .03) and Claude 3.5 (t(152) = 4.28, p < .001). In the Image condition, the performance of GPT-40, with normal and CoT prompting, did not differ significantly from that of humans (t(152) = 0.70, p = 1.00 and t(152) = 0.82, p = 1.00 respectively), while Claude 3.5's performance, with normal and CoT prompting, was significantly lower (t(152) = 13.44, p < .001 and t(152) = 9.08, p < .001 respectively).\nEffect of Distractor condition Compared to the four-option Standard (text) condition, the performance of humans and all LLMs declined significantly in the nine-option Distractor condition (see Appendix B, Table B2). Notably, Claude 3's performance decreased by 18.7%, while the other models' performance declined\nby around 10 and 11% and humans by 9.1%. This significant decline across the board contradicts our expectation that only the earlier models would show this trend.\nEffect of Image condition The effect of the Image condition was mixed. Compared to the Standard text condition, performance did not decline significantly for humans and GPT-40 in the Image condition (humans: t(293) = 1.68, p = .19, GPT-4o: t(70) = -1.34, p = .74). For Claude 3.5, however, performance fell significantly by 28% (t(70) = 12.53, p < .001). CoT prompting did not affect GPT-40 in the Image condition (p = 1.00), while Claude 3.5's performance was significantly strengthened (p < .001, for test statistics, see Appendix B, Table B3).\nThe effect of CoT prompting Overall, CoT prompting had a positive effect on performance, increasing the LLMs' scores by 5.35% on average. Only in one out of six cases, for GPT-40 in the image condition, did CoT prompting not improve performance. Statistically, CoT prompting had a significant positive effect in three cases (see Appendix B, Table B3). This suggests that CoT prompting is still effective to some degree, but that the effect is not universal and not always strong. The smaller positive effect of CoT prompting in Experiment 2 may be due to the change in the standard prompt discussed earlier.\nQualitative analysis of Claude's answers Here, like we did for the GPT models in Experiment 1, we offer a brief qualitative analysis of some of the answers of Claude 3 and Claude 3.5, both with CoT prompting. By examining some illustrative examples, we hope to give the reader a sense of the ways in which the models succeed and fail at the tasks.\nQuestion 7-sweeping the floor This task involved replacing a broom for sweeping the floor. The afforded object was a wig and the two associated objects were laundry detergent and a bucket. Claude 3 scored 0% on this task and GPT-40 scored 73%. Below are examples of their evaluations of the wig and the bucket:\nSimilarly to the difference between GPT-3.5 and GPT-40, Claude 3.5 showed a stronger appreciation than Claude 3 that the wig is somewhat analogous to a broom's bristles. Claude 3 showed some sensitivity to the analogy, but opted generally for the bucket and chose the irrelevant object, a vase, three times.\nQuestion 9 - taking a cake out of the oven This task required replacing over mittens to take a hot cake out of the oven. The afforded object was a beach towel, while the two associated objects were a saucepan lid and a chef's hat. Claude 3 scored 7% on this task and Claude 3.5 scored 100%. Below are examples from Claude 3 and Claude 3.5's evaluations of the beach towel and the saucepan lid:\nWhile Claude 3 recognized that a beach towel could provide some insulation against heat, it only mentioned the possibility of folding the towel once, and it only opted for the beach towel in 1 out of 15 answers. Instead, it went for the associated saucepan lid 14 times and usually claimed that the lid would provide a good \"grip\" on the cake. Claude 3.5, however, always opted for the beach towel and suggested folding the towel in 14 out of 15 answers.\n5.3 Discussion\nIn the Standard condition, the performance of the LLMs relative to humans was similar to the results in the first experiment. Humans scored highest, with GPT-40 close behind and GPT-3.5 far behind. The Claude models also followed this pattern, with Claude 3.5 scoring slightly (but significantly) worse than humans, while Claude 3, Anthropic's earlier model, scored similarly to GPT-3.5, OpenAI's earlier model.\nThe effect of using images to symbolize the object options instead of words was mixed. Humans and GPT-40's performance were not significantly affected. This suggests that the sort of reasoning or computation that multimodal LLMs perform to solve the text-condition tasks is not restricted to that modality alone. LLMs can take text and image inputs simultaneously and arrive at the same or similar answers whether it is presented with the word \u201capple\u201d or an image of an apple. However, it is difficult to say whether these results indicate that the model is able to translate the image input into a text format which it then uses to solve the tasks or if the model reasons using a shared and amodal representational format. There is evidence suggesting that the representations of language and vision transformer models converge to some extent Moreover, a mechanistic study of extractable features in the Claude 3 Sonnet model found multimodal features, shown by the fact that the same circuits in the model would activate for words and images of the same concept, such as the Golden Gate Bridge This suggests that these LLMs have a shared embedding space for both text and images. Claude 3.5's weak performance in the image condition compared to the text condition suggests that LLMs may differ in their ability to integrate text and image inputs on reasoning tasks.\nThe effect of including five additional distractor options was significant across the board, for humans and LLMs alike. This contradicted our expectation that only the earlier LLMs would be significantly affected. The performance decline of humans, GPT-4o, and Claude 3.5 was quite similar. This similar sensitivity to the increase in task difficulty suggests that whatever the reasoning process GPT-40 and Claude 3.5 use to solve these types of tasks is somewhat robust. The performance in the distractor condition, even if lower than in the standard condition, might provide somewhat stronger evidence for robust reasoning, given that random guessing would only produce a score of about 11%.\nThe qualitative review of Claude 3 and Claude 3.5's text output showed similar patterns to those seen for GPT-3.5 and GPT-40. Claude 3.5's factual and causal knowledge is more detailed, precise, and less error-prone than that of Claude 3. Claude 3.5 was also able to disregard associated and typical options to a much greater degree than Claude 3, suggesting that Claude 3.5 can generalise to a greater extent than its predecessor."}, {"title": "6 General Discussion", "content": "Our experiments examined the ability of humans and LLMs to deal with unconventional tool use tasks. The experiments were designed to address key issues in the debates about LLMs' abilities. Inspired by Yiu et al.'s (2023) tool innovation task design, we tested LLMs' abilities to reason compositionally and flexibly apply causal knowledge. To better probe the robustness and generality of the LLMs' reasoning, we tested the models in a variety of conditions (different temperatures, prompting techniques, modalities, and levels of difficulty). The task itself involves a variety of objects and situations, from cutting and throwing to baking and gardening. Overall, we found strong progress in LLM responses, whereby GPT-40 and Claude 3.5 performed much better than the earlier GPT-3.5 and Claude 3 models. Notably, GPT-40 performed almost at a human level in the Standard and Image conditions, with Claude 3.5 not far behind in the Standard condition.\nOur findings raise many unanswered \u2013 and currently unanswerable \u2013 questions for the research community. OpenAI and Anthropic, for commercial and safety reasons, do not publish the technical details of their models. This means that we can only speculate about the technical changes behind the noted progress. Given recent trends, it is plausible that scaling has played a major part. Scaling the size of a"}]}