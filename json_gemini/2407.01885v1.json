{"title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "authors": ["Chuanpeng Yang", "Wang Lu", "Yao Zhu", "Yidong Wang", "Qian Chen", "Chenlong Gao", "Bingjie Yan", "Yiqiang Chen"], "abstract": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs pose considerable challenges for practical deployment, particularly in environments with limited resources. The endeavor to compress language models while maintaining their accuracy has become a focal point of research. Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance. This paper presents a thorough survey from three aspects: method, evaluation, and application, exploring knowledge distillation techniques tailored specifically for LLMs. Specifically, we divide the methods into white-box KD and black-box KD to better illustrate their differences. Furthermore, we also explored the evaluation tasks and distillation effects between different distillation methods, and proposed directions for future research. Through in-depth understanding of the latest advancements and practical applications, this survey provides valuable resources for researchers, paving the way for sustained progress in this field.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of Large Language Models (LLMs) [2, 17, 130, 146, 166] has significantly improved text generation quality in various generative tasks, becoming a pivotal and widely discussed topic in the field of artificial intelligence. These models, compared to their predecessors, show superior generalization to unseen data. Moreover, they exhibit capabilities that smaller models lack, such as multi-step reasoning [47, 69, 83] and instruction-following [103, 144, 154]. The success of LLMs is often attributed to increased training data and a larger number of model parameters (e.g., GPT-3 with 175 billion parameters [12]). However, the expansion in parameter size brings significant drawbacks, particularly in terms of high inference costs and substantial memory requirements, making practical deployment challenging. For example, GPT-3 requires around 350GB of model storage (float16) and at least 5 A100 GPUs with 80GB of memory each for inference, contributing significantly to carbon emissions. To mitigate these challenges, model compression [30, 40] has emerged as a viable solution. Model compression aims to transform large, resource-heavy models into more compact versions that are suitable for storage on constrained mobile devices. This process may involve optimizing for reduced latency to achieve faster execution or balancing between minimal latency and model performance. Thus, a key goal in applying these high-capacity models in real-world scenarios is to compress them, reducing the number of parameters while preserving maximum performance.\nAs the necessity to reduce computational resource demands becomes increasingly crucial, Knowl-edge Distillation (KD) [43] emerges as a promising technique. KD is a machine learning method focused on compressing and speeding up models by transferring knowledge from a large, complex model to a smaller, more efficient one. This technique is frequently employed to condense the knowledge stored in large deep neural network models into smaller counterparts, thus reducing computational resource requirements and improving inference speed without substantial perfor-mance sacrifices. Fundamentally, knowledge distillation leverages the extensive knowledge acquired by a large model on a substantial dataset to guide the training of a smaller model. This knowledge typically includes the output probability distribution, intermediate layer representations, and loss function of the large model. During training, the smaller models aim not only to match the original data labels but also to mimic the behavior of the larger models. For advanced models like GPT-4 [2], which are accessible only through APIs, the generated instructions and explanations can aid in the training of student models [54]."}, {"title": "2 OVERVIEW OF KNOWLEDGE DISTILLATION", "content": "In this section, we summarized the optimization objectives of each knowledge distillation algorithm."}, {"title": "2.1 Logits-based KD", "content": "As implied by its name, logic-based KD [43] is a distillation paradigm that employs logic within teacher models for knowledge transfer. We can formulate the general knowledge distillation loss function as follows:\n$L_{logits} = KL (p^t||p^s) = \\sum_{j=1}^{C} p^t_j log \\left(\\frac{p^t_j}{p^s_j}\\right),$ \n$P p = \\frac{exp (z^t/t)}{\\sum_{j=1}^{C} exp (z^t/\u03c4)} \\text{ and } P s = \\frac{exp (z^s/t)}{\\sum_{i=1}^{C} exp (z^s/\u03c4)}$\nwhere $z^s, z^t \\in R^C$ denote the logits output of the student and teacher network, respectively. t is\na temperature parameter that adjusts the smoothness of the logits. C represents the number of\nclasses. The Kullback-Leibler divergence (KLD) [43] loss can also be replaced with other functions,\nsuch as Reverse Kullback-Leibler (RKL) [20, 53, 65, 96] distillation, Jenson-Shannon (JS) [129]\ndistillation, etc."}, {"title": "2.2 Hint-based KD", "content": "Given the restricted ability of students to extract knowledge in logit-based knowledge distillation,\nresearchers strive to more precisely replicate the behavior of teachers. Consequently, intermediate\nfeature-based knowledge distillation [46, 122] was introduced. This technique involves matching\nthe outputs of the intermediate layers between student and teacher models. This approach requires\nstudents to understand both the results and the processes leading to those results. The general\nform of the feature-based knowledge distillation loss function is outlined below:\n$L_{hint} = H (F_s, F_t) = ||F_t \u2013 \u00a2 (F\u00b0)||^2,$\nwhere $F_s, F_t \\in R^{H\u00d7W\u00d7C}$ denote the intermediate features of the student and teacher networks,\nrespectively. The function f is used to ensure that the student features match the dimensions of\nthe teacher features. The metric function is represented by H, and as an example, we use mean\nsquare error."}, {"title": "2.3 In-Context Learning", "content": "ICL [12, 52] utilizes a natural language prompt composed of task descriptions or several task\nexamples as demonstrations. Formally, let $D_K = {f (x_1, y_1), . . ., f (x_k, y_k ) }$ represent a set of k\nexamples, where $f (x_k, y_k )$ is a function that converts the k-th task example into a natural language\nprompt. Given the task description I, the demonstration set Dk, and a new input query $x_{k+1}$, the\npredicted output \u0177k+1 generated by the LLM can be described by the following formula:\nLLM(I, f ($x_1$, $y_1$), . . ., f ($x_k$, $y_k$ ), f( $x_{k+1}$, )) \u2192 \u0177k+1,\nwhere the answer yk+1 is left blank for the LLM to predict. The student model is used to predict the\nresults generated by the LLM."}, {"title": "2.4 Chain-of-Thought", "content": "CoT [47, 69, 83, 136] integrates intermediate reasoning steps into prompts, rather than relying\nsolely on simple input-output pairs as done in ICL.\nLLM(I, f ($x_1$, $r_1$, $y_1$), . . ., f ($x_k$, $r_k$, $y_k$ ), f( $x_{k+1}$, )) \u2192 \u00cek+1, \u0177k+1\u00b7\nwhere rk represents the rationale provided by the user that explains why the answer to xk is yk. At\nthis point, the student model not only needs to predict the labels of the teacher model, but also\nneeds to emulate the reasons generated by the teacher."}, {"title": "2.5 Instruction Following", "content": "By fine-tuning on a structured multitask dataset that utilizes natural language descriptions, LLMs\nexhibit proficiency on unseen tasks that are similarly expressed in instructional formats [98, 112,\n149]. Through instruction tuning, LLMs can follow task guidelines for new assignments without\nneeding explicit examples, thus improving their generalization abilities. The process of distilling\ninstruction-following skills involves generating task-specific instructions with the LLM and then\nfine-tuning the student model using this instruction dataset."}, {"title": "3 KNOWLEDGE DISTILLATION IN LARGE LANGUAGE MODELS", "content": "The Transformer architecture is highly scalable, allowing for the creation of extremely large models\nwith billions or even trillions of parameters. It underpins many of the most prominent large-scale\nmodels in NLP, CV, and multimodal domains. For example, notable large language models like\nthe GPT series [2, 12], LLaMA [130], and Qwen [7] are based on its decoder-only configuration.\nBefore 2023, research on Transformer-based NLP distillation [111, 126] mainly centered around the\nBERT architecture. However, with the rise of pre-trained large language models [2, 98], there has\nbeen increasing interest in distilling Transformers with billion-scale parameters and in developing\nmore efficient distillation methods for scenarios with limited data and high computational costs\n[44, 47]. The existing distillation algorithms are mainly divided into two categories: white-box KD\nand black-box KD."}, {"title": "3.1 White-box Knowledge Distillation", "content": "White-box distillation depends on methods that require access to the teacher model's internal\ndata during training, utilizing the accessible internal information of the teacher model. In the\nfollowing discussion, we explore two distinct types of white-box knowledge distillation. Firstly,\nlogits-based methods, introduced by Hinton et al.[43], transfer knowledge at the logits level, where\nthe knowledge is conveyed using the teacher model's logits. Given the limited knowledge acquired\nby students in logits-based knowledge distillation, researchers aim to more accurately replicate the\nteacher's behavior. To this end, Romero et al.[109] propose hint-based knowledge distillation, which\ninvolves aligning the feature outputs of intermediate layers between the student and teacher models.\nThis approach requires the student to understand not only the final results but also the processes\nleading to those results. In the following section, we analyze in detail the characteristics of each\nmethod from the perspective of evaluation tasks (as shown in Table 1). Furthermore, we evaluate\nthe strengths and weaknesses of the two types of distillation algorithms based on robustness,\nproviding certain guidance in the applicable scenarios of the algorithms."}, {"title": "3.1.1 Logits-based KD", "content": "The distillation of Bidirectional Long Short-Term Memory Networks (BiL-STM) [126] marks the earliest attempt to apply knowledge distillation to BERT [60]. The distillation objective is to minimize the mean squared error loss between the logits of the student network and those of the teacher. This approach has been tested on three tasks: sentence classification and sentence matching. Experimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed. Similarly, DistillBERT [111] initializes a shallower student model using the teacher's parameters and minimizes the difference in soft target probabilities between the teacher and student, a technique known as word-level knowledge distillation. It introduced a triple loss that combines language modeling, distillation, and cosine distance loss to leverage the inductive bias learned by the pre-trained model. DistilBERT achieved performance equivalent to or exceeding the ELMo baseline in nine tasks. Compared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%."}, {"title": "3.1.2 Hint-based KD", "content": "The feature-based knowledge distillation methods [46, 122] extract knowledge from the embedding space, transformer layers, and prediction layers, allowing the student model to learn various aspects of the teacher model comprehensively. For instance, Sun et al.[122] proposed a patient knowledge distillation (PKD) method aimed at compressing a large-scale teacher model into an equally effective lightweight student model. They proposed two distinct distillation strategies: 1) PKD-Last: The student model learns from the last k layers of the teacher model, based on the assumption that the top layers contain the most informative knowledge. 2) PKD-Skip: The student learns from every k-layer of the teacher, suggesting that the lower layers also contain essential information that should be gradually transferred during distillation. Experiments conducted on seven datasets across four tasks\u2014sentiment classification, paraphrase similarity matching, natural language inference, and machine reading comprehension-showed that the PKD method outperformed standard knowledge distillation methods. It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while"}, {"title": "3.2 Robustness Evaluation of White-box KD", "content": "There are various evaluation standards for existing white-box KD algorithms, most of which utilize BERT as the teacher model. However, the effectiveness of these distillation algorithms in the context of LLMs remains unclear. Building on the work presented in [139], we conducted a unified evaluation of these algorithms from a robustness perspective, specifically focusing on adversarial robustness and out-of-distribution (OOD) robustness. Both types of robustness pertain to performance under input disturbances, which is particularly critical for safety-sensitive applications. Adversarial robustness examines the stability of models against adversarial and imperceptible disturbances, while OOD robustness assesses performance on unseen data that differs from the training data distribution. To evaluate adversarial robustness, we employed the AdvGLUE [138] and ANLI [95] benchmarks, using Attack Success Rate (ASR) as the metric. For OOD robustness, we used the Flipkart [135] review and DDXPlus [128] medical diagnostic datasets, with F1-score (F1) as the indicator. Inspired by the work on MINILLM [38], we utilized the Dolly \u00b9 dataset for distillation, fine-tuning both student and teacher models. We evaluated five distillation algorithms and four models concurrently to assess their robustness.\nThe evaluation results are shown in Tables 2-4. Firstly, we observed that MINILLM demonstrated superior overall distillation performance in GPT-2. Notably, for the 340M-sized GPT-2, it achieved state-of-the-art results on both adversarial and out-of-distribution datasets when compared to the other four distillation algorithms. Furthermore, MINILLM outperformed the other algorithms on the Flipkart and DDXPlus datasets for GPT-2 of any size, highlighting its exceptional generaliza-tion capability to out-of-distribution data. Secondly, for the OPT model, we discovered that the most straightforward KD algorithm, which employs the teacher distribution as supervision for each token step to fine-tune the student model, achieved the best overall performance. Likewise, MINILLM outperformed other distillation algorithms and even exceeded the performance of teacher"}, {"title": "3.3 Discussion on White-box KD", "content": "Logits-based KD methods typically focus on aligning the output distributions between the teacher and student models. In contrast, hint-based KD methods can convey richer information by aligning the intermediate layers, leading to better results. However, implementing layer-to-layer knowledge distillation necessitates careful design of the layer mappings between the teacher and student models and requires a deep understanding of the model architecture. Both logits-based and hint-based KD methods demand substantial GPU memory during the distillation process. Even though the teacher network doesn't need backpropagation, the activation of intermediate features during forward propagation consumes a significant amount of GPU memory. Therefore, exploring ways to reduce training costs and shorten training times is crucial."}, {"title": "3.4 Black-box Knowledge Distillation", "content": "The two previously discussed distillation techniques rely on access to the internal data of the teacher model, categorizing them as white-box distillation methods, which require internal data during training. However, many modern large-scale closed-source models do not provide access to internal data, limiting us to using only model predictions. Distillation where knowledge is transferred solely through the teacher model's predictions is known as black-box knowledge distillation. Researchers have found that when model parameters are sufficiently large, the models exhibit remarkable versatility, enabling them to handle complex tasks. Many black-box distillation methods take advantage of this capability, typically utilizing three techniques: In-Context Learning, Chain-of-Thought, and Instruction Following. In this section, we further categorize black-box KD methods based on the use of emergent capabilities."}, {"title": "3.4.1 In-Context Learning", "content": "ICL was initially introduced in GPT-3 [12], where it employs a natural language prompt that includes both task descriptions and multiple task examples as demonstrations.\nThe process begins with the task description, followed by selecting specific instances from the task\ndataset to serve as examples. These instances are then formatted into natural language prompts using\na predefined template and arranged in a particular order. Finally, the test samples are incorporated\ninto the input of the LLM to produce the output.\nExpanding on this concept, Huang et al. [52] propose In-Context Learning Distillation, which\naims to enhance the few-shot learning capabilities of multitask models by effectively extracting\nand transferring knowledge through context learning and language modeling objectives. This\napproach introduces two paradigms for few-shot learning: Meta In-context Tuning and Multitask\nIn-context Tuning. In Meta-ICT [22, 88], the language model undergoes meta-training across a\nbroad spectrum of tasks using in-context learning objectives. Subsequently, it adapts to unseen\ntarget tasks through in-context learning. However, the efficacy of in-context learning heavily\nrelies on the knowledge accumulated during pretraining [108], potentially limiting its ability\nto fully leverage the input-label correspondence provided in the training data [89]. To address\nthis limitation, an alternative few-shot learning paradigm called Multitask In-Context Tuning is\nproposed. While Meta-ICT enables the student model to adapt to new tasks via context learning\nand teacher guidance, Multitask-ICT treats all target tasks as training tasks and utilizes examples\ndirectly from these tasks for in-context learning distillation. These two paradigms for few-shot\nlearning involve a trade-off between performance and computational efficiency. Results across tasks\nsuch as classification, natural language inference, and question answering indicate that Multitask-\nICT achieves a reduction in model size by 93% while retaining 91.4% of the teacher's performance.\nTherefore, Multitask-ICT proves to be more effective, albeit with higher computational costs. LLM-R\n[140] utilizes a pre-trained frozen LLM to retrieve high-quality contextual examples, which are then\nranked to generate training data. Subsequently, it constructs a reward model using a cross-encoder\nto capture ranking preferences. Finally, knowledge distillation is applied to train a dense retriever\nbased on dual encoders. Our comprehensive evaluation of LLM-R across diverse tasks consistently\ndemonstrates superior performance compared to several robust baselines. Furthermore, our model\nexhibits scalability across different task sizes and LLM architectures. Detailed analysis indicates\nthat our approach enhances context learning performance by an average of 7.8%, with consistent\nimprovements observed across various sizes of LLMs."}, {"title": "3.4.2 Chain-of-Thought", "content": "Chain-of-Thought (CoT) [47, 69, 83, 136] represents an advanced prompt-ing strategy aimed at enhancing LLMs' ability to tackle complex reasoning tasks. Unlike the input-output pair approach used in ICL for prompt formulation, CoT integrates intermediate infer-ence steps that incorporate final outputs into the prompts. Typically, CoT distillation [16, 19, 44, 56, 59, 68, 69, 116, 142, 147, 170] involves leveraging large-scale models to construct enriched datasets focused on reasoning tasks, which are then utilized for fine-tuning student models. Thus, the primary focus is on generating high-quality rationales for training and ensuring effective utilization of these rationales by students [47, 59, 69, 116, 142]."}, {"title": "3.4.3 Instruction Following", "content": "The instruction following capability aims to enhance the language model's ability to perform new tasks without heavy reliance on limited examples. Through fine-tuning across various tasks specified by instructions, the language model demonstrates its pro-ficiency in accurately executing tasks described in previously unseen instructions. However, in black-box distillation, knowledge transfer relies solely on datasets, making the availability of a suffi-ciently large dataset crucial. Therefore, collaborative efforts in these approaches [54, 103, 144, 154] involve creating a comprehensive dataset comprising instructions, inputs, and outputs. This dataset enables the student model to acquire extensive knowledge from the teacher model.\nSpecifically, Wang et al.[144] propose self-instruction, a semi-automatic process that utilizes indicator signals from the model itself to refine the language model's instructions. The process begins with a constrained seed set of manually crafted tasks, such as the 175 tasks used in our study, to guide the overall generation process. Initially, the prompt model uses this initial set of instructions to generate a broader array of task descriptions. Furthermore, for newly generated sets of instructions, the framework creates input-output instances that can be used for supervised instruction tuning in the future. Finally, various heuristic methods are employed to automatically filter out low-quality or duplicate instructions before incorporating the remaining valid tasks into the task pool. This iterative process can be repeated multiple times until a significant number of tasks are obtained. This method has influenced subsequent research, leading to adjustments in the 13B open-source models like Alpaca [127], Vicuna [24], and GPT4All [6] following this paradigm.\nExpanding on these ideas, Peng et al.[103] explore the use of GPT-4 to generate instruction-following data for fine-tuning LLMs. They curated a dataset of 52,000 instruction-following examples in both English and Chinese, along with feedback datasets generated by GPT-4. Using these datasets, they fine-tuned two student models, LLaMA-GPT4 and LLaMA-GPT4-CN. Additionally, they developed a feedback model to evaluate the quality of model responses. Wu et al.[154] meticulously compiled a dataset comprising 2.58 million instructions, ensuring coverage of diverse topics. These instructions were used as input to generate responses using GPT-3.5 Turbo. They fine-tuned a range of models under the LaMini-LM, including both encoder-decoder and decoder-only architectures. Evaluation of the LaMini-LM models' performance involved applying automatic metrics across 15 benchmarks, alongside manual assessment. Results illustrate that the proposed LaMini-LM model achieves comparable performance to competitive baselines despite being only one-tenth the size.\nHowever, existing methodologies have predominantly concentrated on one-way knowledge distillation, where student model responses align with those of teacher models to generate instruc-tions without incorporating a \"feedback\" mechanism. To address this limitation, Jiang et al.[54] introduce an innovative adversarial distillation framework consisting of three stages: imitation, discrimination, and generation. Leveraging the adaptable nature of LLMs, this framework incen-tivizes teacher models to identify \"challenging\" instructions and generate new instructions for student models, thereby enhancing the effectiveness of knowledge transfer. This approach achieves open-generation capability comparable to ChatGPT using only 70,000 training samples, surpassing traditional state-of-the-art instruction adjustment models (such as Vicuna-13B) by 55.4% and 16.7% on the zero-shot inference BBH and AGIEval tasks, respectively. In efforts to provide task-specific guidance, Chen et al.[18] propose a fine-tuning dataset for code generation instructions and develop a multi-round personalized distillation approach. This approach enables student models to first attempt solving tasks independently, followed by adaptive refinements provided by the teacher to enhance their performance through executive feedback. Unlike traditional knowledge transfer"}, {"title": "3.5 Robustness Evaluation of Black-box KD", "content": "Inspired by the work in [47], we conducted a unified evaluation of the step-by-step distillation algorithm based on CoT from a robustness perspective. Due to the closed-source nature of the PaLM 540B model, we adhered to the experimental setup in [47] and used the generated CoT"}, {"title": "3.6 Discussion on Black-box KD", "content": "The black-box based KD method is typically used by LLMs to generate explanations or instruction pairs to fine-tune the student model. In this approach, only the teacher model generates data, and only the student model is involved in training, making it memory-efficient. However, most current methods rely on closed-source teacher models, and generating additional data can be costly. Additionally, many methods do not have open-source data generation techniques or involve closed-source generated data, posing challenges for the fair evaluation of these black-box based distillation algorithms."}, {"title": "3.7 Others", "content": "As large language models have advanced significantly, their inherent limitation lies in their inability to comprehend visual information, as they are primarily designed for processing discrete texts. Consequently, researchers are increasingly exploring ways to transfer the capabilities of language models into multimodal domains, where text and image data are integrated to enable a wider range of tasks [32, 36, 158]. Extracting knowledge from pre-trained multimodal models to enhance the performance and generalization of compact multimodal language models has become a focal point of interest in this field."}, {"title": "3.7.1 Muiti-Modal Large Language Models", "content": "Knowledge distillation for multimodal large models is still in its nascent stages, focusing primarily on refining instruction-following capabilities. Li et al.[70] have pioneered a novel framework featuring two stages for distilling knowledge in multimodal large models. The initial stage involves multimodal pre-training to align multimodal"}, {"title": "4 APPLICATIONS", "content": "In this section, we briefly explore the applications of LLM distillation in various critical domains such as healthcare, education, and law."}, {"title": "4.1 Healthcare", "content": "Healthcare represents a critical domain deeply intertwined with human well-being. Since the inception of ChatGPT, numerous endeavors have endeavored to harness the prowess of ChatGPT and other LLMs in the realm of medicine. For example, Zhang et al.[162] introduced HuatuoGPT, a specialized LLM designed for medical consultations. By distilling data from ChatGPT and integrat-ing real-world insights from physicians through supervised fine-tuning, HuatuoGPT incorporates a reward model aimed at synergizing the strengths derived from both datasets. Empirical results demonstrate that HuatuoGPT achieves state-of-the-art performance in medical consultations, out-performing GPT-3.5turbo across various metrics evaluated on GPT-4, including manual assessments and medical benchmark datasets. Li et al.[72] highlight the scarcity of LLMs specifically tailored to medical domains. Using LLaMA as a developmental and evaluative platform, they explored two enhancement strategies: model fine-tuning and knowledge integration to augment the efficacy of LLMs as medical chatbots. Fine-tuning the dialogue model on a dataset comprising 100K patient physiological dialogues sourced from online medical consultation platforms, their experiments"}, {"title": "4.2 Education", "content": "Education represents another critical domain where LLMs show significant promise. Current re-search demonstrates that LLMs can achieve proficiency comparable to students in standardized exams across various mathematical disciplines such as physics and computer science [2]. Xie et al.[156] introduced DARWIN, a framework aimed at enhancing natural sciences by accelerating and enriching the automation of discovery processes. This approach incorporates the Scientific Instruc-tion Generation (SIG) model, which integrates structured and unstructured scientific knowledge from public datasets and literature. By eliminating the need for manual extraction or domain-specific knowledge graphs, DARWIN achieves state-of-the-art performance across diverse scientific tasks. Luo et al.[81] proposed WizardMath, which utilizes the Reinforcement Learning from Evol-Instruct Feedback (RLEIF) technique to enhance the mathematical reasoning capabilities of LLaMA-2 [130]. This method employs math-specific Evol-Instruct to generate diverse mathematical instruction data, subsequently training the Instruction Reward Model (IRM) and the Process Supervised Reward Model (PRM) [160]. The IRM evaluates the quality of evolutionary instructions, while the PRM receives feedback at each step of the solution process. Through extensive experimentation on two mathematical reasoning benchmarks, GSM8k [27] and MATH [42], WizardMath significantly outperforms other open-source LLMs. Furthermore, Deng et al.[29] introduced K2, a LLM tailored for geoscience, and established the GeoBench, the first geoscience benchmark, to evaluate LLMs within this domain."}, {"title": "4.3 Law", "content": "Law, a domain rich in professional expertise, has recently adopted LLMs to address various legal tasks, such as legal document analysis [9] and legal document generation [25]. Huang et al.[50] integrated legal expertise into the continuous training phase of LLaMA by employing carefully designed supervised fine-tuning tasks. These tasks aimed to impart professional skills to the model while mitigating the issue of model-generated illusions. To enhance training, they introduced a retrieval module that extracts relevant legal articles before the model generates responses. Similarly, Cui et al.[28] integrated legal-specific data into LLaMA, resulting in the creation of ChatLaw. Concerned with the accuracy of reference retrieval from legal datasets, they developed a hybrid approach combining vector database retrieval and keyword-based retrieval. This approach addresses hallucination concerns and improves accuracy by implementing a self-attention mechanism. This mechanism enhances the ability of large models to correct errors within reference data, thereby improving coherence and augmenting problem-solving proficiency in legal contexts."}, {"title": "5 CHALLENGES AND FUTURE DIRECTIONS", "content": ""}, {"title": "5.1 Unified Evaluation Benchmark", "content": "The existing benchmark for evaluating knowledge distillation primarily falls into four categories: 1) General Language Understanding Evaluation (GLUE) Benchmark [137]: This benchmark consists of nine sentence-level classification tasks, including language acceptability [148], sentiment analysis [118], text similarity [15], entailment detection [31], and natural language inference [107]. It"}, {"title": "5.2 Advanced Algorithms", "content": "Current methodologies primarily aim to equip student models with specific capabilities. For example, symbolic knowledge distillation [152] leverages LLMs to gather and filter data, extracting high-quality commonsense maps for training commonsense models. Similarly, DISCO [23] employs LLMs to acquire counterfactual data, which is then filtered using a large teacher Natural Language Inference model to improve students' proficiency in natural language reasoning tasks. As open-source LLMs continue to evolve, exploring white-box distillation algorithms for LLMs could prove to be an effective approach for integrating multiple capabilities. Furthermore, the current development pace of MLLMs distillation lags behind that of LLMs. Thus, investigating more advanced MLLMs distillation algorithms could facilitate the integration of multiple modalities more effectively."}, {"title": "5.3 Interpretability", "content": "Stanton et al.[120] explore the interpretability of knowledge distillation and introduce the concept of matching degree to enhance its reliability. Their study reveals several significant insights: 1) The relationship between student models' generalization performance and matching degree is not uniformly consistent. Excluding self-distillation, models with the best generalization performance do not always exhibit the highest fidelity. 2) There is a notable correlation between student models\u2019 fidelity and the calibration of the distillation process. Although the most faithful student model may not always achieve the highest accuracy, it consistently shows superior calibration. 3) Optimization during the knowledge distillation process is challenging, resulting in lower fidelity. Similarly, in the era of large language models, knowledge distillation faces comparable difficulties. For example, current methods struggle to elucidate how CoT-distillation imparts CoT capability to student language models or to determine the required amount of data for fine-tuning instructions. Therefore, integrating interpretability into the process is crucial for advancing LLM knowledge distillation. This integration not only aids in evaluating model distillation but also enhances the reliability and predictability of models in production"}, {"title": "6 CONCLUSION", "content": "In this survey, we systematically investigate the knowledge distillation algorithms from three perspectives: methods, evaluation, and application. Compared to smaller models, distillation in"}]}