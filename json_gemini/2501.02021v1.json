{"title": "Weakly Supervised Learning on Large Graphs", "authors": ["Aditya Prakash"], "abstract": "Graph classification plays a pivotal role in various domains, including pathology, where images can be represented as graphs.In this domain, images can be represented as graphs, where nodes might represent individual nuclei, and edges capture the spatial or functional relationships between them. Often, the overall label of the graph, such as a cancer type or disease state, is determined by patterns within smaller, localized regions of the image. This work introduces a weakly-supervised graph classification framework leveraging two subgraph extraction techniques: (1) Sliding-window approach (2) BFS-based approach. Subgraphs are processed using a Graph Attention Network (GAT), which employs attention mechanisms to identify the most informative subgraphs for classification. Weak supervision is achieved by propagating graph-level labels to subgraphs, eliminating the need for detailed subgraph annotations.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph classification is a critical task in fields like cheminformatics, social networks, and medical diagnostics. In pathology, graphs represent images with nodes corresponding to cellular structures and edges capturing spatial relationships. Disease-relevant patterns are often localized in specific subregions of these graphs, making subgraph-based approaches essential. Traditional graph classification methods require detailed annotations, which are expensive and time-consuming to obtain. To address this, we propose a weakly-supervised graph classification framework that uses graph-level labels to learn subgraph-level patterns.\nOur approach introduces two subgraph extraction techniques:\n\u2022 BFS-based extraction: Captures connected and meaningful subgraphs using Breadth-First Search.\n\u2022 Sliding-window extraction: Iteratively selects localized subgraphs using a fixed window size.\nThese subgraphs are processed using a Graph Attention Network (GAT), which identifies and aggregates the most informative subgraphs using attention scores. The proposed method is evaluated on the D&D and MSRC-21 datasets, achieving competitive accuracy and providing interpretable insights."}, {"title": "II. RELATED WORK", "content": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations on graph-structured data. They extend the capabilities of traditional neural networks to non-Euclidean domains by leveraging the relationships encoded in graph edges. Among the most prominent architectures is the Graph Convolutional Network (GCN) [1], which generalizes the convolution operation to graphs. GCNs update node features by aggregating information from neighboring nodes using a propagation rule, enabling the learning of rich graph-level representations. However, GCNs assume equal importance for all neighbors, which may not always align with the underlying graph structure.\nTo address this limitation, Graph Attention Networks (GATs) [2] introduce an attention mechanism that learns the relative importance of edges. By assigning attention weights to edges, GATs enable the model to prioritize more relevant neighbors during feature aggregation. This mechanism has proven particularly effective in capturing localized patterns in tasks like node classification and graph classification.\nIn the context of medical image analysis, particularly in pathology, several studies have employed GNNs for graph-based classification. For example, the work by [3] explores the use of GCNs for histopathology image classification by transforming image patches into graphs, where nodes represent image features and edges capture spatial relationships between them. Their method demonstrated the effectiveness of GCNS in learning from histopathology images and provided valuable visualizations of the learned graph structures. This approach aligns with our goal of applying graph-based models to pathology images, where localized, disease-specific regions need to be identified for accurate diagnosis.\nBuilding on these foundational works, our approach integrates GNN-based models with two novel subgraph extraction techniques\u2014BFS-based and sliding-window methods\u2014to address weakly-supervised graph classification. These methods enable the identification of relevant subgraphs from pathology graphs, while the attention mechanism in GATs ensures the model focuses on the most informative subgraphs, achieving good performance without requiring detailed subgraph-level annotations."}, {"title": "III. METHODOLOGY", "content": "In this section, we describe the approach for weakly-supervised graph classification, focusing on two subgraph extraction methods\u2014BFS-based and sliding-window-based. We then detail how these subgraphs are processed using a Graph Attention Network (GAT), and how weak supervision is applied through attention-based subgraph selection."}, {"title": "A. Subgraph Extraction Techniques", "content": "Effective subgraph extraction is essential for isolating meaningful regions of a graph that contribute to graph-level classification. The two subgraph extraction techniques used in this work are:\n1) BFS-based Subgraph Extraction: The BFS-based subgraph extraction method captures subgraphs starting from random nodes and expanding using breadth-first search (BFS) until a depth limit is reached.\n\u2022 Start from a randomly selected node in the graph.\n\u2022 Traverse the graph using BFS up to a specified depth limit (e.g., 11).\n\u2022 Collect all the nodes and edges encountered during the traversal.\n\u2022 If the subgraph has fewer than a specified minimum number of nodes or edges, discard it.\n\u2022 Retain the subgraphs that meet the minimum criteria for further processing.\n2) Sliding-Window Subgraph Extraction: The sliding-window method extracts subgraphs by moving a fixed-size window across the nodes of the graph, creating overlapping subgraphs. This method is particularly useful for capturing localized graph regions and allows for greater flexibility in exploring different parts of the graph.\n\u2022 Define a window size and step size.\n\u2022 For each position of the window, create a subgraph by selecting the nodes within the window.\n\u2022 Filter the edges to include only those that connect nodes within the window.\n\u2022 Retain the node features and graph-level label for each subgraph."}, {"title": "B. Model Architecture", "content": "Once the subgraphs are extracted, they are processed using a Graph Attention Network (GAT), a model designed to focus on the most relevant parts of a graph by assigning different attention weights to its edges and nodes. The GAT model applies attention mechanisms that allow it to prioritize important neighbors during feature aggregation, which is crucial for tasks like graph classification.\n1) Graph Attention Mechanism: In a traditional Graph Convolutional Network (GCN), the node features are aggregated by averaging the features of neighboring nodes. However, this approach assumes equal importance for all neighbors. In contrast, GATs use an attention mechanism to dynamically assign different importance to each neighbor, allowing the model to focus on the most relevant nodes during aggregation. The key idea behind the GAT is to compute an attention score $a_{ij}$ for each pair of nodes $i$ and $j$ in the graph, which indicates how much importance node $j$ should have when aggregating node $i$'s features. The attention scores are computed using the following steps:\n\u2022 Attention Coefficients\nFor each edge in the graph, the attention score $a_{ij}$ is computed as:\n$a_{ij} = \\frac{\\exp (\\text{LeakyReLU}(a^T [Wh_i || Wh_j]))}{\\sum_{k\\in N(i)} \\exp (\\text{LeakyReLU}(a^T [Wh_i || Wh_k]))}$\nwhere: - $h_i$ and $h_j$ are the feature vectors of nodes $i$ and $j$, respectively. W is a learnable weight matrix that projects the node features to a higher-dimensional space. a is a learnable attention vector. || denotes concatenation. The denominator is the normalization term to ensure that the attention scores sum to 1 over all neighbors of node $i$.\n\u2022 Feature Aggregation\nOnce the attention scores are computed, the node features are aggregated using a weighted sum of the neighbors' features, where the attention scores serve as the weights:\n$h'_i = \\sigma(\\sum_{j\\in N(i)} a_{ij} Wh_j)$\nwhere: - $h'_i$ is the updated feature for node $i$. - $N(i)$ denotes the neighbors of node $i$. - $\\sigma$ is a non-linear activation function, typically ReLU or LeakyReLU.\n2) Multi-Head Attention: In practice, we use multi-head attention to stabilize learning and allow the model to jointly attend to information from different subspaces. The multi-head attention mechanism applies multiple attention heads independently, aggregates their outputs, and then concatenates them.\nFor K attention heads, the output of the multi-head attention for node i is computed as:\n$h'_i = ||_{k=1}^{K} \\sigma(\\sum_{j \\in N(i)} a^k_{ij} W h_j)$"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In this section, we describe the experimental setup, including the datasets used for evaluation, as well as the training and evaluation processes.\nFor the evaluation of our method, we use two widely used datasets in the graph classification domain: the D&D dataset and the MSRC-21 dataset."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "The report introduces a weakly-supervised graph classification framework utilizing BFS-based and sliding-window subgraph extraction techniques. The evaluation on D&D and MSRC-21 datasets demonstrates competitive accuracy, particularly in datasets with higher node counts. The Graph Attention Network effectively identifies and prioritizes informative subgraphs, addressing the challenge of weak supervision. However, limitations persist, including instability in smaller datasets like MSRC and reliance on pre-defined hyperparameters.\nAdditionally, the current approach lacks qualitative insights into the extracted subgraphs, making it challenging to assess their relevance or interpretability. Applying this method to pathology image graphs presents an exciting direction for exploration. This would not only help assess the practical utility of the proposed approach but also provide insights into its ability to identify biologically or clinically meaningful subgraph patterns, thereby bridging the gap between model predictions and domain-specific knowledge."}]}