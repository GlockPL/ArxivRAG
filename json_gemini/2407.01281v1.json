{"title": "Bridging Smoothness and Approximation: Theoretical Insights into Over-Smoothing in Graph Neural Networks", "authors": ["Guangrui Yang", "Jianfei Li", "Ming Li", "Han Feng", "Ding-Xuan Zhou"], "abstract": "In this paper, we explore the approximation theory of functions defined on graphs. Our study builds upon the approximation results derived from the K-functional. We establish a theoretical framework to assess the lower bounds of approximation for target functions using Graph Convolutional Networks (GCNs) and examine the over-smoothing phenomenon commonly observed in these networks. Initially, we introduce the concept of a K-functional on graphs, establishing its equivalence to the modulus of smoothness. We then analyze a typical type of GCN to demonstrate how the high-frequency energy of the output decays, an indicator of over-smoothing. This analysis provides theoretical insights into the nature of over-smoothing within GCNs. Furthermore, we establish a lower bound for the approximation of target functions by GCNs, which is governed by the modulus of smoothness of these functions. This finding offers a new perspective on the approximation capabilities of GCNs. In our numerical experiments, we analyze several widely applied GCNs and observe the phenomenon of energy decay. These observations corroborate our theoretical results on exponential decay order.", "sections": [{"title": "1. Introduction", "content": "Graph Convolutional Networks (GCNs) have emerged as a powerful category of deep learning models that extend the prowess of traditional neural network architectures to data that is structured as graphs [1, 2]. Such data encapsulates relationships and interdependencies between entities, which are crucial in domains like social networks, biological networks, recommendation systems, and more. The ability of GCNs to leverage the connectivity and features of graph data has led to significant advancements and state-of-the-art results in various tasks [3-7].\nHowever, unlike other deep learning models, graph neural networks did not, until recently, benefit significantly from increasing depth. As GCNs deepen, a common issue they encounter is known as over-smoothing [8-13], where the node representations become indistinguishable and converge to a subspace that lacks the discriminative power necessary for downstream tasks. This phenomenon fundamentally limits the ability of GCNs to capture higher-order dependencies with graphs, and has attracted intense research attention [14\u201317]. Li et al. [18] demonstrates that the graph convolution in the GCN model is essentially a specific type of Laplacian smoothing. This mechanism underpins the effectiveness of GCNs; however, it also introduces the risk of over-smoothing when numerous convolutional layers are employed. Chen et al. [14] investigated the problem of over-smoothing using Dirichlet energy, noting that for deep GCN models, the Dirichlet energy of the embeddings tends to approach zero, which leads to a reduction in their ability to distinguish between different features. Wu et al. [19] prove that the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. For more detailed information on the formal definition of over-smoothing, associated metrics, and existing strategies for reducing it, we refer the readers to the recent comprehensive survey by Rusch et al. [8].\nThe work presented in this paper addresses the theoretical underpinnings of the over-smoothing phenomenon through the lens of approximation theory on graph signals. The approximation theory for signals on graphs has arisen in some literature. Building on Schr\u00f6dinger's semigroup of operators, I. Z. Pesenson et al. [20] established a sparse approximation theory on graphs."}, {"title": "2. Notations and Preliminaries", "content": "This section recalls some necessary notations and concepts for subsequent sections."}, {"title": "2.1. Graph Signal Processing", "content": "Let $\\mathcal{G} = {\\mathcal{V}, A}$ be an undirected, weighted, and connected graph, where $\\mathcal{V} = {v_1, v_2, ..., v_\\mathcal{N}}$ denotes the vertices of the graph, and $A = (A_{ij}) \\epsilon \\mathbb{R}^{\\mathcal{N}\\times\\mathcal{N}}$ is the adjacency matrix. Here, $A_{ij} > 0$ if there is an edge connecting vertex $v_i$ and $v_j$, and $A_{ij} = 0$ otherwise. The graph Laplacian of $\\mathcal{G}$ is defined as $L = D - A$, where $D = diag(d_1, d_2, ..., d_{\\mathcal{N}})$ is the diagonal degree matrix with $d_i = \\sum_{j=1}^{\\mathcal{N}} A_{ij}$ for $i = 1, 2, ..., \\mathcal{N}$.\nA graph signal defined on $\\mathcal{G}$ is a function $f : \\mathcal{V} \\rightarrow \\mathbb{C}$, where $f(v_i)$ denotes the signal value at node $v_i$. In this paper, we only consider graphs of finite size $\\mathcal{N}$, so the signal $f$ is also equivalent to a vector $f \\in \\mathbb{C}^\\mathcal{N}$. For convenience, we use $f(i)$ to represent $f(v_i)$ in the rest of this paper. Since $L$ is real and symmetric, it can be orthogonally diagonalized as $L = U\\Lambda U^*$, where $U = (u_1, u_2, ..., u_{\\mathcal{N}}) \\in \\mathbb{C}^{\\mathcal{N}\\times\\mathcal{N}}$, $\\Lambda = diag(\\lambda_1, \\lambda_2, ..., \\lambda_{\\mathcal{N}}) \\in \\mathbb{R}^{\\mathcal{N}\\times\\mathcal{N}}$ with $0 = \\lambda_1 \\le \\lambda_2 \\le \\ldots \\le \\lambda_{\\mathcal{N}} = \\lambda_{max}$, and $U^*$ denotes the conjugate transpose of $U$.\nMatrix $U$ (called the graph Fourier basis) is often used to define the Graph Fourier Transform (GFT) [28, 29]. Specifically, for any $f \\in \\mathbb{C}^\\mathcal{N}$, its GFT is defined as $\\hat{f} = U^*f$, and the inverse graph Fourier transform is $f = U\\hat{f}$. In graph signal processing, the pair {$\\lambda_j, u_j$} is often referred to as the frequency and frequency component of graph $\\mathcal{G}$. Based on the GFT, the function $f = U\\hat{f} = \\sum_{j=1}^{\\mathcal{N}} \\hat{f}(j)u_j$ is decomposed into the sum of $\\mathcal{N}$ components {$\\hat{f}(j)u_j\\}_{j=1}^{\\mathcal{N}}$, where $\\hat{f}(j)$ is called the spectrum of $f$ corresponding to the frequency $\\lambda_j$.\nWe denote by $PW_n(\\mathcal{G})$ the signal space in which each function $f$ satisfies that $\\hat{f}(j) = 0$ for all $j > n$, that is,\n$$PW_n(\\mathcal{G}) = span (u_1, u_2, ..., u_n).$$The space $PW_n(\\mathcal{G})$ is the well-known Paley-Wiener space, and its orthogonal projection operator is denoted by $P_n$, i.e., $P_n = U_nU^*$, where $U_n = (u_1, u_2, ..., u_n) \\in \\mathbb{C}^{\\mathcal{N}\\times n}$.\nFor $n = 1, 2, . . ., \\mathcal{N}$, the best approximation error (with 2 -norm) of $f$ by the functions in $PW_n(\\mathcal{G})$ is\n$$E_n(f) := \\min_{g \\in PW_n(\\mathcal{G})} ||f - g||_2$$"}, {"title": "2.2. Modulus of Smoothness and K-functional on Graphs", "content": "We now prepare to define the modulus of smoothness on graphs. For $s \\in \\mathbb{R}$,\n$$T_s := e^{is\\sqrt{L}} = U diag (e^{is\\sqrt{\\lambda_1}}, e^{is\\sqrt{\\lambda_2}},..., e^{is\\sqrt{\\lambda_{\\mathcal{N}}}}) U^*$$\ndefines a family of graph translation operators. In [27], using the definition of graph translation operator, the r-order (where r is a nonnegative integer) modulus of smoothness (with 2-norm) is defined as\n$$\\omega_r(f, t) := sup_{|s| \\le t} ||\\triangle_s^r f||_2,$$\nwhere $\\triangle_s^r : \\mathbb{C}^\\mathcal{N} \\rightarrow \\mathbb{C}^\\mathcal{N}$ is the difference operator of r-order, defined by\n$$\\triangle_s^r f := (T_s - I)^r f, f \\in \\mathbb{C}^\\mathcal{N},$$\nwith I denoting the identity matrix. Note that in [27], the modulus of smoothness is defined using a general p-norm, but in this paper, we focus only on the modulus of smoothness with the 2-norm. According to the results in [27], $\\omega_r(f, t)$ has the following four properties:"}, {"title": "3. Approximation theory for K-functional on the graph", "content": "In this section, we will establish a strong equivalence relation between the K-functional and the modulus of smoothness for graph signals as demonstrated in Theorem 1. Prior to this, two lemmas are introduced, which play a crucial role in supporting the proof of Theorem 1."}, {"title": "4. A lower bound of approximation capability of GCNs", "content": "In this section, we investigate the approximation capability of Graph Convolutional Networks (GCNs) by using the estimation presented in Theorem 1. Furthermore, we will provide a theoretical insight about over-smoothing phenomenon.\nThe GCNs under our consideration are structured as follows equipped with a graph filter H:\n$$F^{(k)} = \\sigma(HF^{(k-1)}W^{(k-1)}),$$\nfor $k = 1, 2, . . ., K$, where $\\sigma(x) = max{x, 0}$ is the ReLU activation function, $F^{(k)} \\in \\mathbb{R}^{N\\times m_k}$ is the output of the $k$-th layer (with the input feature $F^{(0)} \\in \\mathbb{R}^{N\\times m_0}$) and $W^{(k-1)} \\in \\mathbb{R}^{m_{k-1}\\times m_k}$ is the learnable weights of the $k$-th layer.\nThis architecture is widely adopted in GCNs. Particularly, in many cases, the filter H is chosen to be a polynomial of the graph Laplacian L or the normalized Laplacian, as mentioned in [30] and [18]. These filters, H, possess a low-frequency eigenvector that comprises nonnegative elements and does not oscillate around zero. With such filters, GCNs typically achieve good performance at limited depths but tend to suffer from the over-smoothing problem. In the following theorem, it is demonstrated that this occurs when low-frequency components are present and high-frequency eigenvalues are less than one.\nTheorem 3. Suppose $F^{(0)} \\in \\mathbb{R}^{N\\times m_0}$ is a graph input signal with N nodes and $m_0$ channels, and {$F^{(k)}$} is generated by a GCN $F^{(k+1)} = \\sigma (HF^{(k)}W^{(k)})$ with a filter H and weights $W^{(k)} \\in \\mathbb{R}^{m_k\\times m_{k+1}}$ for $k = 1,..., K. Let {(h_i, \\mu_i)}_{i=1}^N form an orthonormal eigenvector-eigenvalue pairs of H. If H has a low frequency eigenvector, denoted by $h_1$, and $||W^{(k)}||_F < 1$ for all $k = 1, . . ., K$, then the high frequency parts of $F^{(K)}$ can be bounded by\n$$\\sum_{j=1}^{m_K} \\sum_{i=2}^{N} |(f_j^{(K)}, h_i)|^2 \\le |\\mu_{high}|^{2K} ||F^{(0)}||_F,$$"}, {"title": "5. Experiments", "content": "In this section, we conduct experiments to verify our theoretical results.\nIn the experiments, we use the stochastic block model (SBM) with two classes characterised by $y \\in {\\pm1}^N$ to generate the graph structure, where edges (i, j) are added independently with probability $p \\in (0, 1]$ if $y_i = y_j$, and with probability $q \\in [0, p)$ if $y_i \\neq y_j$. Then, we obtain a random binary adjacency matrix $A \\in \\mathbb{R}^{N\\times N}$. Furthermore, the node features $F^{(0)}$ are sampled from a Gaussian mixture model (GMM), i.e.,\n$$F^{(0)} = y\\mu + \\epsilon \\in \\mathbb{R}^{N\\times N},$$\nwhere $\\mu \\in \\mathbb{R}^N$ and $\\epsilon = (\\epsilon_{ij}) \\in \\mathbb{R}^{N\\times N}$ with $\\epsilon_{ij} \\stackrel{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)$ and $\\sigma \\in (0, +\\infty)$. Unless otherwise stated, we set $p = 0.8, q = 0.3, \\sigma = 10$, and $\\mu$ is generated randomly.\nFor simplicity, the experiments are conducted on feed-forward GCNs to verify our theoretical findings (i.e., Theorem 3) for over-smoothing problems. Therefore, in the experiments, we randomly select the weight matrix $W^{(k)}$ of (5) based on the normal distribution $\\mathcal{N}(0, 1)$ and then normalize it to satisfy that $||W^{(k)}||_F = 10$. Then, for each layer k, we evaluate the high frequency energy of $F^{(k)}$ according to the filter H:\n$$E_h (F^{(k)}) := \\sum_{j=1}^{m_k} \\sum_{i=2}^{N} |(f_j^{(k)}, h_i)|^2,$$"}, {"title": "5.1. Over-smoothing problems in classical GCNs", "content": "First, we test the effect of three different filters on the over-smoothing problem in classical GCNs, namely: $H_{gen}$, $H_{sym}$, and $H_{rw}$ with $\\alpha = 0.75$.\nTo this end, we conduct the experiments over 1000 independent trials on"}, {"title": "5.2. Decay rate with respect to |Phigh", "content": "According to Theorem 3, we have that\n$$E_h(F^{(K)}) \\le |\\mu_{high}|^{2K} ||F^{(0)}||_F,$$\nthat is, the decay rate of the upper bound of $E_h$ is dependent on $|\\mu_{high}|$. To show this, we next conduct the experiments on the following four filters $H_j$, $j = 1, . . ., 4$:"}, {"title": "5.3. Exploration to alleviate over-smoothing problems with skip-connections?", "content": "In the existing works, a large number of results have shown that the skip-connection architecture helps to alleviate the over-smoothing problem of the network. In this part, based on our Theorem 3, we conduct experiment to evaluate the ability of the following three skip-connection GCNs to alleviate the over-smoothing problem, namely: ResGCN, APPNP, and GCNII, which are defined iteratively as follows:\n\\begin{align*}F_{ResGCN}^{(k+1)} & = \\sigma \\bigg(H_{gen} F_{ResGCN}^{(k)}W^{(k)} + F_{ResGCN}^{(k)} \\bigg), \\\\F_{APPNP}^{(k+1)} & = (1 - \\alpha_k)H_{gen} F_{APPNP}^{(k)} + \\alpha_k F^{(0)}W^{(k)}, \\\\F_{GCNII}^{(k+1)} & = \\sigma \\bigg( \\bigg((1 - \\alpha_k)H_{gen} F_{GCNII}^{(k)} + \\alpha_k F^{(0)} \\bigg) (\\beta_k W^{(k)} + (1 - \\beta_k) I )\\bigg).\\end{align*}"}, {"title": "6. Conclusion and Discussion", "content": "This study has explored the approximation theory of functions on graphs, leveraging the K-functional to enhance our understanding of Graph Convolutional Networks (GCNs). Our findings contribute to the theoretical underpinnings of GCNs, particularly in terms of their abilities and limitations in approximating functions and managing the over-smoothing phenomenon.\nThe insights from this study highlight the need to maintain a balance between low and high-frequency information during the forward pass of data through GCNs. Such balance is crucial for mitigating over-smoothing and enhancing the approximation capabilities of these networks. Potential strategies derived from our findings to address these challenges include:\n1. Incorporating Residual Connections: Adding residual connections between the input layer and intermediate layers to GCNs can help preserve rich information, allowing the network to maintain access to high-frequency details that might otherwise diminish through successive layers.\n2. Enhancing the Filter Channels: Introducing additional or adaptive filter channels can provide finer control over how frequency components are processed, enabling the network to selectively emphasize or de-emphasize certain frequencies based on the task requirements.\n3. Separating Different Frequencies for Processing: Implementing mechanisms to process different frequency components separately instead of only employing low-pass filters can prevent the loss of crucial information, particularly the high-frequency signals, thus reducing the risk of over-smoothing."}]}