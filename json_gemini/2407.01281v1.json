{"title": "Bridging Smoothness and Approximation: Theoretical Insights into Over-Smoothing in Graph Neural Networks", "authors": ["Guangrui Yang", "Jianfei Li", "Ming Li", "Han Feng", "Ding-Xuan Zhou"], "abstract": "In this paper, we explore the approximation theory of functions defined on graphs. Our study builds upon the approximation results derived from the K- functional. We establish a theoretical framework to assess the lower bounds of approximation for target functions using Graph Convolutional Networks (GCNs) and examine the over-smoothing phenomenon commonly observed in these networks. Initially, we introduce the concept of a K-functional on graphs, establishing its equivalence to the modulus of smoothness. We then analyze a typical type of GCN to demonstrate how the high-frequency energy of the output decays, an indicator of over-smoothing. This analysis provides theoretical insights into the nature of over-smoothing within GCNs. Furthermore, we establish a lower bound for the approximation of target functions by GCNs, which is governed by the modulus of smoothness of these func- tions. This finding offers a new perspective on the approximation capabilities of GCNs. In our numerical experiments, we analyze several widely applied GCNs and observe the phenomenon of energy decay. These observations corroborate our theoretical results on exponential decay order.", "sections": [{"title": "1. Introduction", "content": "Graph Convolutional Networks (GCNs) have emerged as a powerful cat- egory of deep learning models that extend the prowess of traditional neural network architectures to data that is structured as graphs [1, 2]. Such data encapsulates relationships and interdependencies between entities, which are crucial in domains like social networks, biological networks, recommendation systems, and more. The ability of GCNs to leverage the connectivity and fea- tures of graph data has led to significant advancements and state-of-the-art results in various tasks [3-7].\nHowever, unlike other deep learning models, graph neural networks did not, until recently, benefit significantly from increasing depth. As GCNs deepen, a common issue they encounter is known as over-smoothing [8-13], where the node representations become indistinguishable and converge to a subspace that lacks the discriminative power necessary for downstream tasks. This phenomenon fundamentally limits the ability of GCNs to cap- ture higher-order dependencies with graphs, and has attracted intense re- search attention [14\u201317]. Li et al. [18] demonstrates that the graph convolu- tion in the GCN model is essentially a specific type of Laplacian smoothing. This mechanism underpins the effectiveness of GCNs; however, it also in- troduces the risk of over-smoothing when numerous convolutional layers are employed. Chen et al. [14] investigated the problem of over-smoothing using Dirichlet energy, noting that for deep GCN models, the Dirichlet energy of the embeddings tends to approach zero, which leads to a reduction in their ability to distinguish between different features. Wu et al. [19] prove that the graph attention mechanism cannot prevent oversmoothing and loses ex- pressive power exponentially. For more detailed information on the formal definition of over-smoothing, associated metrics, and existing strategies for reducing it, we refer the readers to the recent comprehensive survey by Rusch et al. [8].\nThe work presented in this paper addresses the theoretical underpinnings of the over-smoothing phenomenon through the lens of approximation theory on graph signals. The approximation theory for signals on graphs has arisen in some literature. Building on Schr\u00f6dinger's semigroup of operators, I. Z. Pesenson et al. [20] established a sparse approximation theory on graphs."}, {"title": "2. Notations and Preliminaries", "content": "This section recalls some necessary notations and concepts for subsequent sections.\n2.1. Graph Signal Processing\nLet G = {V, A} be an undirected, weighted, and connected graph, where V = {v\u2081, v\u2082, ..., v\u1d65} denotes the vertices of the graph, and A = (A\u1d62\u2c7c) \u2208 \u211d\u1d3a\u00d7\u1d3a is the adjacency matrix. Here, A\u1d62\u2c7c > 0 if there is an edge connecting vertex v\u1d62 and v\u2c7c, and A\u1d62\u2c7c = 0 otherwise. The graph Laplacian of G is defined as L = D \u2013 A, where D = diag(d\u2081, d\u2082, ..., d\u1d65) is the diagonal degree matrix with \\(d_i = \\sum_{j=1}^N A_{ij}\\) for i = 1,2,..., N.\nA graph signal defined on G is a function f : V \u2192 \u2102, where f(v\u1d62) denotes the signal value at node v\u1d62. In this paper, we only consider graphs of finite size N, so the signal f is also equivalent to a vector f \u2208 \u2102\u1d3a. For convenience, we use f(i) to represent f(v\u1d62) in the rest of this paper. Since L is real and symmetric, it can be orthogonally diagonalized as L = U\u039bU*, where U = (u\u2081, u\u2082, . . ., u\u2099) \u2208 \u2102\u1d3a\u00d7\u1d3a, \u039b = diag(\u03bb\u2081, \u03bb\u2082, ..., \u03bb\u2099) \u2208 \u211d\u1d3a\u00d7\u1d3a with 0 = \u03bb\u2081 \u2264 \u03bb\u2082 \u2264 \u2026 \u2264 \u03bb\u1d65 = \u03bb\u2098\u2090\u2093, and U* denotes the conjugate transpose of U. Matrix U (called the graph Fourier basis) is often used to define the Graph Fourier Transform (GFT) [28, 29]. Specifically, for any f \u2208 \u2102\u1d3a, its"}, {"title": "2.2. Modulus of Smoothness and K-functional on Graphs", "content": "We now prepare to define the modulus of smoothness on graphs. For s \u2208 \u211d,\n$\\T_s := e^{is\\sqrt{L}} = U \\text{diag} (e^{is\\sqrt{\\lambda_1}}, e^{is\\sqrt{\\lambda_2}},..., e^{is\\sqrt{\\lambda_N}}) U^*$\n defines a family of graph translation operators. In [27], using the definition of graph translation operator, the r-order (where r is a nonnegative integer) modulus of smoothness (with 2-norm) is defined as\n$\\omega_r(f, t) := \\sup_{|s| \\leq t} ||\\triangle_s^r f||_2$,\nwhere \u25b3\u02e2 : \u2102\u1d3a \u2192 \u2102\u1d3a is the difference operator of r-order, defined by\n$\\triangle_s^r f := (T_s \u2013 I)^r f, \\quad f \\in \\mathbb{C}^N$,\nwith I denoting the identity matrix. Note that in [27], the modulus of smoothness is defined using a general p-norm, but in this paper, we focus only on the modulus of smoothness with the 2-norm. According to the re- sults in [27], \\(\\omega_r(f, t)\\) has the following four properties:"}, {"title": "3. Approximation theory for K-functional on the graph", "content": "In this section, we will establish a strong equivalence relation between the K-functional and the modulus of smoothness for graph signals as demon- strated in Theorem 1. Prior to this, two lemmas are introduced, which play a crucial role in supporting the proof of Theorem 1."}, {"title": "4. A lower bound of approximation capability of GCNs", "content": "In this section, we investigate the approximation capability of Graph Con- volutional Networks (GCNs) by using the estimation presented in Theorem 1. Furthermore, we will provide a theoretical insight about over-smoothing phe- nomenon.\nThe GCNs under our consideration are structured as follows equipped with a graph filter H:\n$\\F^{(k)} = \\sigma(H F^{(k-1)}W^{(k-1)})$,\nfor k = 1, 2, . . ., K, where \u03c3(x) = max{x, 0} is the ReLU activation function, F\u207d\u1d4f\u207e \u2208 \u211d\u1d3a\u00d7\u1d50\u1d4f is the output of the k-th layer (with the input feature F\u207d\u2070\u207e \u2208 \u211d\u1d3a\u00d7\u1d50\u2080) and W\u207d\u1d4f\u207b\u00b9\u207e \u2208 \u211d\u1d50\u1d4f\u207b\u00b9\u00d7\u1d50\u1d4f is the learnable weights of the k-th layer.\nThis architecture is widely adopted in GCNs. Particularly, in many cases, the filter H is chosen to be a polynomial of the graph Laplacian L or the normalized Laplacian, as mentioned in [30] and [18]. These filters, H, possess a low-frequency eigenvector that comprises nonnegative elements and does not oscillate around zero. With such filters, GCNs typically achieve good performance at limited depths but tend to suffer from the over-smoothing problem. In the following theorem, it is demonstrated that this occurs when low-frequency components are present and high-frequency eigenvalues are less than one."}, {"title": "5. Experiments", "content": "In this section, we conduct experiments to verify our theoretical results. In the experiments, we use the stochastic block model (SBM) with two classes characterised by y \u2208 {\u00b11}\u1d3a to generate the graph structure, where edges (i, j) are added independently with probability p \u2208 (0, 1] if y\u1d62 = y\u2c7c, and with probability q \u2208 [0, p) if y\u1d62 \u2260 y\u2c7c. Then, we obtain a random binary adjacency matrix A \u2208 \u211d\u1d3a\u00d7\u1d3a. Furthermore, the node features F\u207d\u2070\u207e are sampled from a Gaussian mixture model (GMM), i.e.,\n$\\F^{(0)} = y\\mu + \\epsilon \\in \\mathbb{R}^{N\\times N}$,\nwhere \u03bc \u2208 \u211d\u1d3a and \u03b5 = (\u03b5\u1d62\u2c7c) \u2208 \u211d\u1d3a\u00d7\u1d3a with \u03b5\u1d62\u2c7c ~ N(0, \u03c3\u00b2) and \u03c3 \u2208 (0, +\u221e). Unless otherwise stated, we set p = 0.8, q = 0.3, \u03c3 = 10, and \u03bc is generated randomly.\nFor simplicity, the experiments are conducted on feed-forward GCNs to verify our theoretical findings (i.e., Theorem 3) for over-smoothing problems. Therefore, in the experiments, we randomly select the weight matrix W\u207d\u1d4f\u207e of (5) based on the normal distribution N(0, 1) and then normalize it to satisfy that ||W\u207d\u1d4f\u207e||_F = 10. Then, for each layer k, we evaluate the high frequency energy of F\u207d\u1d4f\u207e according to the filter H:\n$E_h (F^{(k)}) := \\sum_{j=1}^{m_k}\\sum_{i=2}^N |\\langle f_j^{(k)}, h_i \\rangle|^2$,\nwhere h\u1d62 is the eigenvector of filter H and we set m\u2096 = N for all k.\n5.1. Over-smoothing problems in classical GCNs\nFirst, we test the effect of three different filters on the over-smoothing problem in classical GCNs, namely: Hgen, Hsym, and Hrw with \u03b1 = 0.75. To this end, we conduct the experiments over 1000 independent trials on"}, {"title": "5.2. Decay rate with respect to |Phigh", "content": "According to Theorem 3, we have that\n$E_h (F^{(K)}) \\leq |\\mu_{high}|^{2K} ||F^{(0)}||_F^2$,\nthat is, the decay rate of the upper bound of E\u2095 is dependent on |\u03bc\u2095\u1d62g\u2095|. To show this, we next conduct the experiments on the following four filters H\u2c7c, j = 1, . . ., 4:\nH\u2c7c = [h\u2081, ..., h\u1d65]  [h\u2081, ..., h\u1d65]\u1d40,"}, {"title": "5.3. Exploration to alleviate over-smoothing problems with skip-connections?", "content": "In the existing works, a large number of results have shown that the skip- connection architecture helps to alleviate the over-smoothing problem of the network. In this part, based on our Theorem 3, we conduct experiment to evaluate the ability of the following three skip-connection GCNs to alleviate the over-smoothing problem, namely: ResGCN, APPNP, and GCNII, which are defined iteratively as follows:\n$F^{(k+1)}_{ResGCN} = \\sigma\\Big(H_{gen} F^{(k)}_{ResGCN} W^{(k)} + F^{(k)}_{ResGCN}\\Big)$,\n$F^{(k+1)}_{APPNP} = (1 - \\alpha_k) H_{gen} F^{(k)}_{APPNP} + \\alpha_k F^{(0)}W^{(k)}$,\n$F^{(k+1)}_{GCNII} = \\sigma\\Big(((1 - \\alpha_k) H_{gen} F^{(k)}_{GCNII} + \\alpha_k F^{(0)}) \\Big( \\beta_k W^{(k)} + (1 - \\beta_k) I \\Big) \\Big)$,\nlimited ability of ResGCN to alleviate the over-smoothing problem in deep networks.\nCombining the experimental results in this part and the structure of the three networks, we can conclude that there may exist two effective av- enues to alleviate the over-smoothing problem in GCNs, namely: 1) remov- ing/replacing ReLU activation function, and 2) retaining some major high frequency of the input features."}, {"title": "6. Conclusion and Discussion", "content": "This study has explored the approximation theory of functions on graphs, leveraging the K-functional to enhance our understanding of Graph Convo- lutional Networks (GCNs). Our findings contribute to the theoretical under- pinnings of GCNs, particularly in terms of their abilities and limitations in approximating functions and managing the over-smoothing phenomenon.\nThe insights from this study highlight the need to maintain a balance between low and high-frequency information during the forward pass of data through GCNs. Such balance is crucial for mitigating over-smoothing and enhancing the approximation capabilities of these networks. Potential strate- gies derived from our findings to address these challenges include:\n1. Incorporating Residual Connections: Adding residual connections be- tween the input layer and intermediate layers to GCNs can help pre- serve rich information, allowing the network to maintain access to high- frequency details that might otherwise diminish through successive lay- ers.\n2. Enhancing the Filter Channels: Introducing additional or adaptive fil- ter channels can provide finer control over how frequency components are processed, enabling the network to selectively emphasize or de-emphasize certain frequencies based on the task requirements.\n3. Separating Different Frequencies for Processing: Implementing mecha- nisms to process different frequency components separately instead of only employing low-pass filters can prevent the loss of crucial informa- tion, particularly the high-frequency signals, thus reducing the risk of over-smoothing."}]}