{"title": "Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks", "authors": ["Florian Gr\u00f6tschla", "Luca Str\u00e4ssle", "Luca A. Lanzend\u00f6rfer", "Roger Wattenhofer"], "abstract": "Music recommender systems frequently utilize network-based models to capture relationships between music pieces, artists, and users. Although these relationships provide valuable insights for predictions, new music pieces or artists often face the cold-start problem due to insufficient initial information. To address this, one can extract content-based information directly from the music to enhance collaborative-filtering-based methods. While previous approaches have relied on hand-crafted audio features for this purpose, we explore the use of contrastively pretrained neural audio embedding models, which offer a richer and more nuanced representation of music. Our experiments demonstrate that neural embeddings, particularly those generated with the Contrastive Language-Audio Pretraining (CLAP) model, present a promising approach to enhancing music recommendation tasks within graph-based frameworks.", "sections": [{"title": "1. Introduction", "content": "Music and artist recommendations have become a cornerstone of streaming services, profoundly influencing how users discover and engage with music. Algorithmically generated playlists, tailored to individual tastes, are integral to the listening experience, enabling users to find music that suits their mood and environment, as well as discover new artists. For artists, inclusion in these playlists can significantly boost their listener base, while exclusion poses challenges for discovery. Music recommendation systems can be broadly categorized into collaborative filtering-based approaches [1] and content-based approaches [2]. Collaborative filtering leverages relational data, capturing relationships between artists or tracks from manually curated similarities, tags, and user listening behavior. Content-based approaches utilize descriptive data to encapsulate the essence of an artist's music, representing attributes like melody, harmony, and rhythm. Hybrid recommender systems [3, 4] combine both types of data to enhance recommendation quality. In recent years, contrastive learning approaches have gained traction for their effectiveness in representing various types of data [5, 6]. One such model, Contrastive Language-Audio Pretraining (CLAP) [7], maps text and audio into a joint multi-modal space, offering a novel method for representing music. Our work explores the utility of CLAP representations as descriptive data in music recommendation systems."}, {"title": "2. Related Work", "content": "Artist Similarity with Graph Neural Networks. Graph Neural Networks (GNNs) [9] extend deep learning techniques to graph-structured data, addressing the limitations of traditional neural networks that require structured inputs. GNNs operate on graphs defined by nodes and edges, leveraging message passing to aggregate and update node information based on their neighbors. This approach has shown success in tasks such as node classification, edge prediction, and graph classification [10]. GNNs lend themselves to music recommender tasks as they can encode the structural, relational information together with additional features [11, 12]. The study by Korzeniowski et al. [8] introduces the OLGA dataset, which includes artist relations from AllMusic\u00b9 and audio features from AcousticBrainz [13]. Their GNN architecture combines graph convolution layers with fully connected layers and was trained with a triplet loss. Performance evaluations on an artist similarity task demonstrated that incorporating graph layers and meaningful artist features significantly improved prediction accuracy over using deep neural networks alone.\nNeural Embeddings for Recommender Tasks. Various methods have been explored for music similarity detection. Previous approaches used a graph autoencoder to learn latent representations in an artist graph [14], or leveraging a Siamese DCNN model for feature extraction and genre classification [15]. Oramas et al. [16] use CNNs to extract music information, which, in contrast to our work, can not benefit from contrastive pertaining. Furthermore, hybrid recommendation systems using GNNs have been applied in other domains, such as predicting anime recommendations by combining user-anime interaction graphs with BERT embeddings [17].\nContrastive Language-Audio Pretraining (CLAP) [7] learns the (dis)similarity between audio and text through contrastive learning, mapping both modalities into a joint multimodal space. Through the contrastive learning approach, even the audio embeddings alone maintain semantic information, making it suitable for tasks such as music recommendation and artist similarity."}, {"title": "3. Neural Audio Embeddings for Artist Relationships", "content": "We investigate an established artist similarity task similar to the OLGA dataset to evaluate the effectiveness of neural audio embeddings over classical audio features in music recommendation tasks. This dataset comprises a large graph of artists, and the performance of our model is assessed based on its ability to predict new relationships between previously unseen artists,"}, {"title": "3.1. Experimental Setup", "content": "Our setup is inspired by the approach of Korzeniowski et al. [8] on OLGA, where artists are represented as connected nodes based on their relationships described in AllMusic. Following the same methodology, we create an updated version of the original dataset. This allows us to ensure that the song for which we extract features from AcousticBrainz is consistent with the song for which we create CLAP embeddings. We start with the same set of artists and collect additional information during preprocessing, specifically the categorical features for moods and themes of an artist, which we use during evaluation. Low-level music features for songs were retrieved from AcousticBrainz, and CLAP embeddings were computed using the LAION CLAP model from tracks on YouTube. In contrast to the original OLGA dataset, we only use one song per artist and do not aggregate the features over multiple songs. Due to constantly changing information on AllMusic, some artists without connections to other artists or missing matches on MusicBrainz or AcousticBrainz had to be dropped. Overall, this reduced the total number of artists from 17,673 in the original to 16,864 in our version. We reuse the split allocation of the OLGA dataset, which is possible since every artist in our dataset is present in the OLGA dataset as well. This resulted in 13,489 artists in the training, 1,679 artists in the validation, and 1,696 artists in the test split. We utilize the same loss functions and GNN backbone as proposed by Korzeniowski et al. [8], but with a uniform sampling based on triplets instead of distance-weighted sampling. More specifically, we employed the triplet loss, finding that using both endpoints as anchors performed better than randomly selecting one endpoint. Euclidean distance was used for the loss, and the Normalized Discounted Cumulative Gain (NDCG) serves for the evaluation. For the graph neural network layers, we experimented with SAGE [18], GATEDGCN [19], and GIN [20], with SAGE demonstrating the best performance.\nWe vary two primary aspects in our experiments: the number of graph layers and the node features. The number of graph layers ranges from zero to four and is varied to assess the contribution that the graph topology can make to the task. With zero graph layers, the architecture only utilizes an MLP to make predictions and does not consider the graph topology, thus serving as a baseline for models that use GNN layers. As the number of graph layers increases, nodes can aggregate information from a larger neighborhood, enhancing the model's capacity to learn from the graph structure. For node features, we use random features as a baseline and experimented with AcousticBrainz features, CLAP features, and Moods-Themes features. We also test combinations of these non-random features."}, {"title": "3.2. Results", "content": "Figure 1a compares the performance of models using random features, AcousticBrainz fea-tures, Moods-Themes features, and CLAP features. The baseline model, which does not utilize"}, {"title": "4. Conclusion", "content": "In this work, we explored the use of CLAP embeddings as descriptive data for music recom-mendation systems. Our experiments focused on a graph-based artist-relationship prediction task, comparing the effectiveness of various feature representations, including AcousticBrainz, CLAP, and a combination of both. Our results indicate that models incorporating CLAP em-beddings significantly outperform those using traditional features, particularly as the number of graph convolutional layers increases. This highlights the potential of CLAP embeddings to capture rich and relevant information about music, thereby enhancing the performance of music recommendation systems."}]}