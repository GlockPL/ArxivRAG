{"title": "Time Step Generating: A Universal Synthesized Deepfake Image Detector", "authors": ["Ziyue Zeng", "Haoyuan Liu", "Dingjie Peng", "Luoxu Jin", "Hiroshi Watanabe"], "abstract": "Currently, high-fidelity text-to-image models are developed in an accelerating pace. Among them, Diffusion Models have led to a remarkable improvement in the quality of image generation, making it vary challenging to distinguish between real and synthesized images. It simultaneously raises serious concerns regarding privacy and security. Some methods are proposed to distinguish the diffusion model generated images through reconstructing. However, the inversion and denoising processes are time-consuming and heavily reliant on the pre-trained generative model. Consequently, if the pre-trained generative model meet the problem of out-of-domain, the detection performance declines. To address this issue, we propose a universal synthetic image detector Time Step Generating (TSG), which does not rely on pre-trained models' reconstructing ability, specific datasets, or sampling algorithms. Our method utilizes a pre-trained diffusion model's network as a feature extractor to capture fine-grained details, focusing on the subtle differences between real and synthetic images. By controlling the time step t of the network input, we can effectively extract these distinguishing detail features. Then, those features can be passed through a classifier (i.e. Resnet), which efficiently detects whether an image is synthetic or real. We test the proposed TSG on the large-scale GenImage benchmark and it achieves significant improvements in both accuracy and generalizability. The code and dataset are available at:", "sections": [{"title": "1. Introduction", "content": "Recently, diffusion models have achieved state-of-the-art performance in the field of image generation. The Denoising Diffusion Probabilistic Models (DDPMs) [1] have introduced a new method for high-quality image generation and have been widely researched. Improvements to diffusion models have focused on multiple aspects, such as accelerating sampling [2-4], innovate the backbone network [5, 6], improved model framework [7-9] and optimizing training strategies [10, 11]. Diffusion models have also been investigated for various downstream tasks, including video generation [12], controllable image synthesis [13, 14], and image editing [15, 16]. The proliferation of diffusion model-based technologies in everyday life has raised significant concerns [17] regarding privacy, the dissemination of misleading information, and copyright infringement. Therefore, it is imperative to develop a method to detecting generated images to ensure the integrity of a trustworthy social environment.\nThe diffusion model differs substantially from previous"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Image Generation", "content": "Generative Adversarial Networks [21] (GANs) and Variational Autoencoders [22] (VAEs) have been pioneers in the field of image generation. However, they are limited by the quality of generated images and the stability of training. Diffusion models represent a newer approach to image generation, particularly notable for their stable training and quality in creating detailed and diverse images. These models work by defining a reverse process that progressively refines images from pure noise to detailed data, guided by probabilistic modeling. This approach captures complex data distributions and avoids some common challenges seen in GANs, like mode collapse. Denoising Diffusion Probabilistic Models [1] (DDPM) are based on a Markov chain of steps that iteratively add noise to data and then learn to reverse this noise. The forward process involves gradually corrupting the data with Gaussian noise until it becomes unrecognizable. In contrast, the reverse process learns to reconstruct the image from noise, resulting in high-quality generation. This method is particularly effective for generating diverse and high-resolution images by modeling the gradual improvement of details over a series of steps. Moreover, recent work such as Stable Diffusion [9], PixArt-a [23] have reached state-of-the-art status in text-to-image generation."}, {"title": "2.2. Generated Image Detection", "content": "In the past few years, research on detecting generated images has primarily centered on images produced by GAN-based generation methods [24, 25]. Detection approaches for GAN-generated images have largely relied on feature detection, which attempt to identify subtle artifacts and inconsistencies specific to GANs. These methods leverage Convolutional Neural Networks (CNNs) to analyze and classify visual features, enabling the identification of synthetic images by learning unique patterns that distinguish GAN-generated content from real images. Some works focus on detecting fake faces [26-28], while others focus on general models [29, 30]. However, with the rise of newer models like diffusion-based image generation, these CNN-based feature detection methods have shown limitations. To address these challenges, recent research has explored innovative detection approaches tailored to the unique characteristics of diffusion models. Wu et al. [31] introduced a method that leverages a CLIP-based model for detection [32], utilizing the feature representations of CLIP to better distinguish between real and diffusion-generated images. Wang et al. [18] proposed the Diffusion Reconstruction Error (DIRE) method which exploits the error in real image reconstruction for image detection. Cazenavette et al. [33] further develop synthetic image detection by utilizing inversion feature maps for classification. Luo et al. [19] perform an effective detection method compared to the DIRE method, while Tan et al. [34] propose a gradient-based detection approach. Inspired by this previous work, we propose the Time Step Generating (TSG), which improves both accuracy and speed."}, {"title": "3. Methods", "content": "We first provide background information on Score-Based Diffusion Model and DDPM in Section 3.1. Then, we introduce our proposed TSG feature extraction method in Section 3.2."}, {"title": "3.1. Preliminaries", "content": "Score-Based Diffusion Model constructs a diffusion process {xt}=0 indexed by a continuous time horizon [0, T], which gradually transform a data distribution qo (x0) into a noise distribution q\u0442(\u0445\u0442) using a stochastic differential equation (SDE). The forward process can be formulated as:\ndx = f(x)dt + g(t)dw,\nwhere w is standard Wiener process. ft(\u00b7) and g(t) denote the drift coefficient and diffusion coefficient, respectively. The reverse process relies exclusively on the time-dependent gradient field (or score) of the perturbed data distribution as:\ndx = [ft(x) - g(t)2\u2207x log qt(x)] dt + g(t)w,\nwhere w and dt denote the standard Wiener process in the reverse-time direction and an infinitesimal negative time step, respectively. Score-based models estimate this gradient field x\u2081 log qt(xt) by training a neural network se(xt, t), with score matching losses as the objective:\nJDSM (0) :=\nEgo (20)qt(Pe|20) [|| Vrt log qt(xt|xo) - So(xt, t)||2] .\nThe score-based model se(xt,t) can be incorporated into (2) once the it has finished training. Subsequently, samples are generated by numerically solving this reverse process SDE, which retraces the forward diffusion in (1) in reverse time, ultimately producing an approximate data sample.\nDenoising Diffusion Probabilistic Model is intuitively composed of a forward process and a reverse process. In the diffusion forward process, let xo be the original image selected from the dataset, random Gaussian noise is sampled and gradually add to 20, denoted as:\nq(xt Xt-1) = N(xt; \u221aAt Xt-1, (1 -\n Vat-1\nAt\nat-1\n-I)),\nwhere xt denotes the images in the process of adding noise, t and at are two pre-defined sequence of hyperparameters. An important corollary is that we can directly add noise in a single step to obtain a noisy image at any given time step:\nq(x \u2013 t|xo) = N(xt; \u221aatxo, (1 \u2013 at)I).\nThe reverse process is also defined as a series of Markov processes following normal distribution:\nPo(Xt-1|Xt) = N(xt\u22121; \u03bc\u04e9 (xt, t), \u03a3o(xt, t)),\nwhere pe can be indirectly predicted by a neural network \u20ac\u03b8. In the training process, e take xt and time step t as input to predict the added noise e, which is defined as:\nLo(xo,t) = ||\u20ac \u2013 eo(\u221a\u0101txo + \u221a1 \u2013 \u0101te, t)||2."}, {"title": "3.2. Time Step Generating", "content": "We first assume a reverse denoising process of a DDPM, the xt near the completion of the reverse process already contains numerous details and is close to samples of images generated by the generative model. TSG feeds the original images, along with a timestamp close to the end of the generation process, into the U-Net within a pre-trained diffusion model. From the DDPM perspective, setting t close to 0 in the reverse process refines details for the given image. Therefore, the noise predicted by the neural network will naturally contain the detailed information of the input image. On the other hand, Figure 2 illustrates our TSG from the perspective of Score-Based Diffusion Model, where the"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets and Pretrained Models", "content": "We use the GenImage [20] dataset to test the detection accuracy of our proposed method and analyze changes brought by different time step t. In the GenImage dataset, we select the images from 5 different generative models: BigGAN [36], VQDM [37], SD V1.5 [9], ADM [7] and Wukong [38], where the last four datasets are generated by diffusion models. Each subset of generated models contains approximately 330k images in total, and divided into training and validation sets. To facilitate comparison with previous work, we follow the dataset's original settings to divide the training and validation sets.\nIn all experiments, we adopt the class-unconditional ImageNet diffusion model at resolution 256 \u00d7 256 which released with the paper [7]."}, {"title": "4.2. Implementation Details", "content": "Our code is basically modified based on DIRE. During the feature extraction process, images are resized to 256 \u00d7 256 and input to the U-Net. Then, during the classifier training phase, we crop the images with a size of 224 \u00d7 224 and use ResNet-50 [35] as the base network. To study the generalization of our method, we selected images generated by five different generation methods as subsets and trained a classifier on each subset for subsequent classification experiments. The results and other experiments are presented below."}, {"title": "4.3. Generalization on Generative Methods", "content": "To evaluate the generalization performance of our model, we select the LaRE2 model trained on the same datasets and consider it as the baseline method. Specifically, we train a classifier on each of the five selected subsets and then use each classifier to detect the other four different subsets. We conduct experiments using the validation set as the test set while keeping the dataset unchanged. We selected the time step t 0 and t = 50, as they are relatively close to the end of the reverse process. The accuracies of different methods"}, {"title": "4.4. Influence of Time Step t", "content": "In TSG generation, we can control the time step t input to the neural network. This subsection discusses the impact of different time steps on feature extraction.\nFrom Figure 4, we can compare the impact of different time steps t on detection performance. Compared to t = 0, as t increases, the amount of detailed information in the feature maps decreases. This results in the information content in the TSG images being insufficient, leading to poorer classification performance in distinguishing between real and fake images. Thus, the detection accuracy at t = 50 slightly declines but remains, on average, higher than that of LaRE2."}, {"title": "4.5. Performance Comparison with State of the Art", "content": "Table 1 shows the accuracy comparisons on the selected dataset. From this, we can observe that our method significantly outperforms in accuracy, whether tested on diffusion model-based datasets or on BigGAN generated dataset. The average accuracy shows an improvement of nearly 20 percentage compared to baseline LaRE2."}, {"title": "4.6. Robustness Against Compression", "content": "In studies focused on datasets for fake detection, there has been specific analysis [40] of how image size and quality impact classification results. We are particularly interested in whether the new features introduced by JPEG's lossy compression lead the classifier to learn some unintended features. Therefore, based on the quality of JPEG images, we selected images with a compression rate greater than 96 from the GenImage's subsets Glide [41], SD V1.4, and Midjourney to create three new unbiased datasets. The specific composition of the dataset can be seen in Table 3."}, {"title": "4.7. Grad-CAM Visualizations", "content": "To further analyze how the classifier determines whether an image is generated based on features extracted by TSG, we use the Grad-CAM [42] to visualize the ResNet-50 prediction process through heatmaps.\nThe highlights in the heatmap represent contributions to the classification result. Figure 5 give the examples. From the whole-image perspective, the highlighted areas are extensive, indicating that the classifier does not make judgments based on just one or a few details. By comparing the heatmap distributions of the same image at t = 0 and t = 50, we can see that the areas contributing the most to the classification results are different. Moreover, at t = 50,"}, {"title": "4.8. Training on the Mixed dataset", "content": "Our goal in the field of fake detection is to develop a fully generalizable detector that can accurately handle images generated by any type of generator. However, from Section 4.3, We observe a decrease in cross-validation accuracy for the BigGAN and diffusion series models. Therefore, we aim to enable the classifier to correctly classify images generated by two different types of generative models through TSG, while still using ResNet-50. We combined the training sets of the ADM, SD V1.5, and BigGAN subsets and trained a classifier using TSG(t=0). The great performance of this new classifier is shown in Table 4. In which, Diff.-based is the average accuracy mentioned in Section 4.5 and the Un-bias is the average accuracy of three Un-bias subsets Glide, SD V1.4, and Midjourney."}, {"title": "5. Conclusion", "content": "In this work, we innovatively addressed the problem of generated image detection by using the U-Net neural network from the pretrained diffusion model as a feature extractor. This approach significantly improved detection accuracy while also accelerating the detection process. This method is more accurate than previous reconstruction-based detection models. In our experiments, we performed cross-validation, robustness testing, and heatmap analysis, all of which demonstrated the effectiveness and efficiency of our method. The proposed TSG is 19% better than the previous method LaRE2 and the time required was only one-tenth of that for DIRE."}]}