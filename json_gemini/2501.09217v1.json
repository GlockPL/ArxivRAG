{"title": "Adaptive Law-Based Transformation (ALT): A Lightweight Feature Representation for Time Series Classification", "authors": ["Marcell T. Kurbucz", "Bal\u00e1zs Haj\u00f3s", "Bal\u00e1zs P. Halmos", "Vince \u00c1. Moln\u00e1r", "Antal Jakov\u00e1ca"], "abstract": "Time series classification (TSC) is fundamental in numerous domains, including finance, healthcare, and environ- mental monitoring. However, traditional TSC methods often struggle with the inherent complexity and variability of time series data. Building on our previous work with the linear law-based transformation (LLT)\u2014which improved classification accuracy by transforming the feature space based on key data patterns\u2014we introduce adaptive law- based transformation (ALT). ALT enhances LLT by incorporating variable-length shifted time windows, enabling it to capture distinguishing patterns of various lengths and thereby handle complex time series more effectively. By mapping features into a linearly separable space, ALT provides a fast, robust, and transparent solution that achieves state-of-the-art performance with only a few hyperparameters.", "sections": [{"title": "1. Introduction", "content": "Time series classification (TSC) is essential in various domains such as finance, healthcare, and environmen- tal monitoring, where the goal is to categorize temporal data into predefined classes [12, 13]. Traditional TSC approaches often rely on feature extraction methods de- signed to capture the temporal dynamics and structural patterns inherent in time series data [1, 4, 14]. How- ever, these methods may struggle with the complexities and variability of time series data.\nOur previous work introduced the linear law-based transformation (LLT) method, which performs uni- and multivariate TSC tasks by transforming the feature space based on identified governing patterns in the data [20]. LLT uses time-delay embedding and spectral decomposi- tion to extract linear laws from training data and applies these laws to transform test data, resulting in improved classification accuracy with low computational cost.\nIn this paper, we build upon the LLT method by intro- ducing an enhanced approach called adaptive law-based transformation (ALT) that utilizes variable-length shifted time windows. Unlike LLT, which operates on fixed- length windows, ALT explores patterns of varying lengths and shifts, making it more effective in capturing distin- guishable patterns within time series data. This flexibility allows the method to identify local patterns of different scales, enhancing its ability to classify complex time se- ries.\nSimilar to LLT, our method aims to transform features into a linearly separable feature space, offering a fast, robust, and transparent solution that achieves state-of- the-art performance. By reducing the need for extensive hyperparameter tuning and incorporating variable-length patterns, ALT simplifies the modeling process and en- hances interpretability, setting it apart from mainstream neural networks and other deep learning techniques.\nWe evaluated ALT on eleven benchmark time se- ries datasets, demonstrating its effectiveness compared to existing TSC techniques, including the original LLT method. The results show that the proposed approach not only achieves higher accuracy but also offers advantages in speed and transparency.\nThe remainder of this paper is organized as fol- lows: Section 2 reviews related work, including the LLT method. Section 3 describes the datasets used and details our proposed method. Section 4 presents and discusses the experimental results. Finally, Section 5 concludes the paper and suggests directions for future research."}, {"title": "2. Related Work", "content": "Time series classification (TSC) methods can generally be grouped into three main categories: feature-based, distance-based, and deep learning-based approaches. Each category offers distinct advantages and faces spe- cific challenges.\nFeature-based approaches extract meaningful represen- tations from time series data before applying classification algorithms. These representations may capture statistical descriptors [14], spectral transformations such as the dis- crete Fourier transform (DFT) or discrete wavelet trans- form (DWT) [1], or model-based features derived from techniques like autoregressive integrated moving average (ARIMA) [4]. Shapelet-based methods, which identify short, discriminative subsequences (shapelets) within the data [29], can be considered a subset of feature-based methods [18]. Shapelet-based approaches focus on local features that are highly interpretable and often effective for capturing localized variations, though they may strug- gle with multi-scale patterns and can be computationally intensive for long time series. Feature-based represen- tations are typically classified using conventional meth- ods such as logistic regression, random forests, or support vector machines (SVM).\nDistance-based methods measure the similarity or dis- similarity between entire time series without explicitly transforming them into feature vectors. A well-known ex- ample is dynamic time warping (DTW) [26], which is ro- bust to local temporal distortions and useful for aligning time series. However, these methods can become compu- tationally expensive as the dataset size grows, and they lack an interpretable intermediate representation of the data.\nDeep learning-based methods automatically learn hi- erarchical feature representations directly from raw time series. Convolutional neural networks (CNNs) are adept at identifying local temporal correlations, while recur- rent neural networks (RNNs) excel at capturing sequential patterns, including long-term dependencies [13, 19, 30]. While deep learning methods often achieve strong em- pirical performance, they typically require large labeled datasets, involve extensive hyperparameter tuning, and may lack transparency in their learned representations.\nThe linear law-based transformation (LLT) [20] in- tegrates elements of feature-based and distance-based methods. By using time-delay embedding and spec- tral decomposition, LLT extracts governing patterns from training data and applies these patterns to unseen in- stances, transforming the feature space to improve clas- sification accuracy. Despite its low computational cost, LLT relies on fixed-length windows, which can limit its ability to capture patterns of variable lengths.\nBuilding on LLT, this work introduces the adaptive law- based transformation (ALT). ALT incorporates variable- length shifted time windows to capture local patterns across multiple temporal scales while maintaining in- terpretability and computational efficiency. This adap- tive design enables ALT to effectively handle complex time series, bridging the gaps between diverse TSC ap- proaches."}, {"title": "3. Data and Methodology", "content": "This study utilizes eleven real-world datasets sourced from the UCR Time Series Classification Archive [8, 9].\nA general TSC task can be formalized as follows. The input data is represented as $x_{i}^{j}$, where $t \\in 1,2,...,h$\n denotes the observation times, $i \\in 1,2,..., \\tau$ identifies\nthe instances, and $j \\in 1,2,..., m$ indexes the different\ninput series belonging to a given instance. The output\n$y^{i} \\in 1, 2, ..., c$ identifies the class of instance $i$. The task\nis to predict the classes from the input data. To address\nthis task, we use the following algorithm:\n[A1] Data Splitting. Divide the instances into learning\n(Lr), training (Tr), and test (Te) subsets using ran-\ndom selection stratified by class representation.\n[A2] Sequence Extraction. (For each Lr, j, and (r, l, k)):\nExtract r-length sequences using shifted time win-\ndows (shifted by k), and take out $2l - 1$ points\nevenly. The triplets (r,l, k) are pre-defined param-\neters, where $r \\leq h$, and $(2l \u2013 2) \\vert (r\u2212 1)$. For a given\nLr, j, and (r, l, k), $\\lfloor \\frac{h-r}{k}+1 \\rfloor$ sequences are generated.\n[A3] Shapelet Vectors. (For each sequence): Per- form l-dimensional time-delay embedding [27]\n(S)-where 2l \u2013 1 denotes the length of the given\nsequence, and S is a symmetric matrix. Perform\nspectral decomposition of S. The eigenvector for\nthe smallest absolute eigenvalue ($\\epsilon \\in \\mathbb{R^{+}} \\cup \\{0\\}$) is\ncalled the shapelet vector, and $Sv \\approx 0$.\n[A4] Shapelet Matrices. (For each j and (r, l, k)): Use shapelet vectors related to the same j and (r, l, k) pairs as the column vectors of the shapelet matrix P.\n[A5] Transformation. (For each Tr, j, and (r, l, k)): Let $o = \\lfloor \\frac{h-sl+1}{k} \\rfloor$ . Embed the instance into\nan o \u00d7 1 matrix (A) as follows:\n$A =\n\\begin{pmatrix}\nX_{1}^{i,j} & X_{k+1}^{i,j} &  ... & X_{(o-1)k+1}^{i,j} \\\nX_{s+1}^{i,j} & X_{k+s+1}^{i,j} & ... & X_{(o-1)k+s+1}^{i,j} \\\n... & ... & ... & ...\\\nX_{(l-1)s+1}^{i,j} & X_{k+(l-1)s+1}^{i,j} & ... & X_{(o-1)k+(l-1)s+1}^{i,j}\n\\end{pmatrix} \\quad (1)$\nRight-multiply this matrix with the P shapelet ma- trix related to the same pair of j and (r, l, k), that is, O = AP. Shapelets from each class in P \u201ccompete\u201d to transform the A matrix close to null vectors.\n[A6] Feature Generation. (For each transformed ma- trix): Square the values of the resulting O and par- tition it by the class from which the shapelets origi- nate. Different methods are used to extract features from the resulting partitions. For example, identify a specific percentile in all the rows, then calculate different statistical indicators from the percentiles. Alternatively, calculate a statistical indicator from all the values in the partitions. After this step, the original m signals of an instance are represented in an m X dimensional feature space, where n is the number of extraction methods used, and g is the number of (r, l, k) triplets used.\n[B1] Classifier Tuning and Evaluation. Utilize new features to tune advanced classifiers (e.g., K-nearest neighbors) via Bayesian hyperparameter optimiza- tion and cross-validation. Evaluate classifiers' ac-"}, {"title": "4. Results and discussion", "content": "The classification outcomes obtained with ALT are sum- marized.\nAs  shows, ALT consistently achieves high validation and test accuracies across all eleven datasets, including perfect scores (100%) on BasicMotions, Cof- fee, and GunPoint4. Transformations typically complete within a practical time frame; however, for larger datasets (e.g., FordB), the transformation step can be more time- consuming. This overhead arises primarily from shapelet vector generation and spectral decomposition steps. Once the transformed features are computed, classification (via KNN or SVM) is relatively fast.\ndetails the hyperparameter and feature- extraction settings employed for each experiment, includ- ing the ratio of data used for shapelet generation versus classifier training. Notably, only a small subset of the data is typically required for learning shapelets, highlighting ALT's efficiency in deriving class-relevant patterns.\nFor additional context, compares ALT's ac- curacy to that of a neural network benchmark using an op- timizable feed-forward architecture (MLP) implemented in MATLAB on the raw time-series data. On most datasets, regardless of their length, ALT outperforms or closely matches the neural network solution despite hav- ing far fewer hyperparameters and a shorter optimiza- tion process. Furthermore, the benchmark compilation  demonstrates that ALT is highly competitive against a wide range of state-of-the-art approaches, in- cluding shapelet-based methods and advanced neural and kernel techniques.\nAcross tasks, ALT's ability to capture subsequence pat- terns of varying lengths proves advantageous, particularly for datasets with subtle class-distinguishing events (e.g., Epilepsy, GunPoint2). This adaptability is reflected in consistent improvements over baseline neural methods, which often struggle with more complex sensor signals (e.g., FordA, FordB). Although certain tasks (e.g., Coffee, GunPoint4) are relatively straightforward for most algo- rithms, ALT maintains robust reliability while retaining interpretability by design."}, {"title": "5. Conclusion and Future Works", "content": "In this paper, we introduced ALT, a novel method for time series classification that generalizes our previous LLT ap- proach. By incorporating variable-length shifted win- dows, it captures local subsequence patterns of differ- ent scales and embeds them in a linearly separable fea- ture space. Extensive experiments across eleven diverse datasets confirm ALT's capacity to deliver competitive or state-of-the-art results, as evidenced by Tables . A.1, and A.3.\nIn future work, we plan to integrate data-driven mecha- nisms for automatically tuning (r, l, k), thus further reduc- ing manual hyperparameter exploration. Additionally, we aim to investigate shapelet pruning techniques (see step [C] in Figure 1) to lower computational overhead, mak- ing ALT scalable to very large time series with minimal performance loss. The method's interpretability could also be enriched by qualitative visualization of extracted shapelet vectors, potentially illuminating latent domain structures. Finally, exploring ALT's capabilities in spe- cialized domains like multi-channel EEG monitoring or IoT anomaly detection may reveal further performance gains and highlight the role of domain-specific knowledge in shaping the transformation pipeline."}]}