{"title": "Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning", "authors": ["Yuqi Pang", "Bowen Yang", "Haoqin Tu", "Yun Cao", "Zeyu Zhang"], "abstract": "Although Large Language Models (LLMs) excel in reasoning and generation for language tasks, they are not specifically designed for multimodal challenges. Training Multi-modal Large Language Models (MLLMs), however, is resource-intensive and constrained by various training limitations. In this paper, we propose the Modular-based Visual Contrastive Decoding (MVCD) framework to move this obstacle. Our framework leverages LLMs' In-Context Learning (ICL) capability and the proposed visual contrastive-example decoding (CED), specifically tailored for this framework, without requiring any additional training. By converting visual signals into text and focusing on contrastive output distributions during decoding, we can highlight the new information introduced by contextual examples, explore their connections, and avoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual perception to make it see and reason over the input visuals. To demonstrate MVCD's effectiveness, we conduct experiments with four LLMs across five question answering datasets. Our results not only show consistent improvement in model accuracy but well explain the effective components inside our decoding strategy. Our code will be available at https://github.com/Pbhgit/MVCD.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) [1, 2] have made significant advances in Natural Language Processing (NLP), demonstrating extraordinary capabilities such as instruction following [3], In-Context Learning (ICL) [4], and Chain-of-Thought (CoT) [5] reasoning by scaling up both data and model size. The introduction of vision language models has further enhanced these capabilities by enabling reasoning over visual content in diverse vision language tasks, such as visual question answering (VQA) [6, 7], visual dialogue [8]. Yet, what is the best representation, in efficiency and performance, for transferring LLMs to Multimodal Large Language Models (MLLMs) remains an open question. Transforming LLMs to MLLMs typically requires combining visual and textual signals using extra training parameters and the corresponding data [9, 10, 11, 12, 7]. This paradigm not only demands significant computational resources and extensive data, but also requires model retraining whenever a new modality is introduced.\nExisting efficient multimodal methods focus on integrating probability-based visual control to the decoding stage [13, 14] or simply bridging visual and language modules using symbolic representations [15, 16]. These methods acquire multimodal knowledge without task-specific training, thereby minimizing the computational cost of multimodal reasoning using LLM. Although efficient MLLM methods reduce resource require-ments through lightweight designs, they often underperform on complex tasks like multimodal reasoning and generation. These models often prioritize knowledge from a single modality during multimodal reasoning, neglecting the need to explore connections from different modalities. As a result, they fail to fully capture patterns within these data pairs, leading to suboptimal performance in related tasks and exposing potential issues while deploying these models [17, 18].\nTo overcome those shortcomings, we equip LLMs with visual reasoning capability and propose a plug-and-play framework, MVCD. As shown in Fig. 1, adding a visual perception module to the LLM to extract text features (e.g., tags, attributes, and captions) enables task handling without retraining. To achieve multimodal reasoning, we are aware of the need to capture connections between different modal data pairs to guide response. Specifically, we first propose the use of the text-only ICL format to better instruct MLLMs for reasoning tasks. Based on traditional contrastive decoding [19, 20], we proposed contrastive-example decoding (CED). By strategically selecting appropriate contextual examples, CED compares output proba-bility distributions with and without these examples, filtering low-probability tokens under adaptive constraints to optimize reasoning and address the limitations of prior knowledge. We apply MVCD to five question answering datasets, showing that CED enhances LLMs' visual perception and robustness across different shot settings while preserving their original strengths. In summary, our main contributions are: (1) We explore multimodal reasoning with LLMs and propose the MVCD framework, which integrates a visual perception module"}, {"title": "II. METHODOLOGY", "content": "A. Problem Definition\nFor visual perception tasks, we formally define it as: given a visual-text dataset, such as image-text pairs S = {(I\u00b9,T\u00b9), (I\u00b9, T\u00b9),\u2026\u2026,(In,Tn)} (or video-text pairs S = {(V1, T1), (V1, T1),\u2026\u2026, (Vn,Tn)}), and given an image I (or a video V), a language model 8 (the weights of the LLM), and input text T = {t1,t2,\u2026, tj\u22121}, where ti is the i-th token, our goal is to generate the corresponding output text. As shown in Fig. 1, the visual perception module converts images or videos to text, such as tags, attributes, and captions. For the current image i, at decoding step j, the probability output of the original LLMs based on all textual information of image i is p(tj). After prepending contextual examples information to the text, the output probability of t; becomes p(tj). It is important to note that for the prediction of tj, the token in the contrastive probability output space should be dynamically selected based on the decoding strategy of the language model.\nB. Visual Module\na) Image Modularization: Inspired by the LENS [15] feature extraction method, we match each image with the corresponding tags, attributes, and captions. Tag/Attribute Module \u2013 We use CLIP [21] (i.e., OpenCLIP-H/14 and CLIP-L/14) as our default visual encoder to obtain N tags for each image and, with task-specific prompts from [22], obtain N attributes. Image Caption Module \u2013 We adopt BLIP [23] image caption model to generate N captions for each image using random top-k sampling [24], where k represents the desired number of captions.\nb) Video Modularization: Tag/Attribute Module \u2013 To obtain high-quality visual-language representations, we employ the recently released LanguageBind [25] to extract features and obtain N tags/attributes for each video. Video Caption Module - We select Video-LLaVA [26] as the video language model for captioning, adjusting the top k = 50 and temperature parameters to generate N diverse captions.\nC. Text Information Fusion\nTo enhance multimodal information expression, we obtain descriptive textual features for every visual input (i.e., image or video) by integrating tags, attributes, and captions, providing complete example fusion as the foundation for the subsequent task. To fully leverage the potential of LLMs, we adopt ICL and introduce a visual-incorporated ICL method. This approach aims to enhance the way LLMs utilize visual information during the decoding process. We select k suitable data pairs (I1,T1),\u2026\u2026(Ik,Tk) from the dataset S for the current image I, where k \u2208 {1,2,\u2026\u2026,n}. In all experiments, contextual examples were selected from S based on the question type (matching the current question), rather than randomly. Using the visual perception module described in section II-B, N tags, attributes, and captions are extracted for each image and concatenated as \"Tags: Top-N tags, Attributes: Top-N attributes, Captions: Top-N captions\". These are combined to form descriptive features. Let F = (Tags, Attributes, Captions) and Fi = (Tags\u00b2, Attributes, Captions). The former rep-"}, {"title": "D. Contrastive-Example Decoding \u2013 CED", "content": "When LLM predicts the j-th token tj, the original decoding concatenates F and T into (F,T<j) to predict the output probability under task-specific instructions. If we consider the hidden information contained in the contextual examples, it is prefixed to the above information with k contextual examples to form the final input: ((E1, E2,\u2026\u2026\u2026, Ek), F,T<j). However, in cases where contextual examples contain knowledge that is out-of-distribution with respect to the language model 0, which may cause the model to struggle with effectively focusing on the associations between examples and to overly rely on the prior knowledge encoded in 0.\nTo address this issue, we propose a decoding strategy, contrastive-example decoding (CED). Prior knowledge is extracted from the model's original distribution, with the prior knowledge modeling the original output probability distribution as p(tj) \u2261 po(tj|F,T<j). Next, we use contextual examples to adjust the model's original output probability distribution to obtain the adjusted probability distribution: p(t) po(tj (E1, E2,..., Ek), F,T<;), focusing more on the knowledge from the examples and their relationships. Essentially, outputs that become much more likely when contextual examples are included are preferred. Our goal is to enhance the adjusted output while minimizing reliance on incorrect elements of the original output:\nlogp(tj) - logp(tj), (1)\nHowever, the original output probability can still inherently capture much simple contextual information (e.g., grammar and common sense) and maintain strong relevance to the context. Thus, penalizing all behaviors from the original output distribution indiscriminately would penalize these simple aspects that are correct (i.e., False negative), and conversely reward implausible tokens (i.e., False positive). To address this, we introduce adaptive constraints that use the optimized output probability to limit the contrastive objective when the original output probability is high:"}, {"title": "III. EXPERIMENTS", "content": "a) Implementation Details: In (2), a is a hyperparameter in [0, 1] that truncates the next token distribution of p. Larger a entails more aggressive truncation, retaining only high-probability tokens, whereas smaller a allows the generation of lower-probability tokens. We set a = 0.1 throughout the paper. We set the default number of tags, attributes, and captions for each image or video to N = 5. These experiments were conducted on 4 NVIDIA A40 (48GB) GPUs.\nb) Models and Baselines: We examined our framework (with CED) using four LLMs: LLaMA2-7B-Chat, LLaMA2-13B-Chat, LLaMA3-8B-Instruct, and Qwen2-7B, and compared them with two baselines: 1) original decoding (LENS), where different shot information can be added to the input based on the original LENS; 2) Dola [20] contrasts the probability differences between projections onto the vocabulary space at earlier and later layers, without using any visual input.\nc) Datasets: We evaluated our approach using five visual question answering datasets, including three image-question tasks (i.e., VQAv2 [30], OKVQA [31], MME [32]) and two video-question tasks (i.e., MSRVTT-QA [33], MSVD-QA).\nd) Comparison with MLLMs: We separately select MLLMs for image and video question answering tasks to comprehensively evaluate the differences and advantages of our approach. We conduct a quantitative assessment of video question-answering capabilities on two datasets and evaluate our approach for image understanding on three image question-answering benchmarks. Our results are based on the Qwen2-7B-Instruct model under three-shot and four-shot settings."}, {"title": "IV. RESULTS AND ANALYSIS", "content": "A. Results Analyze\nTable II demonstrates that MVCD achieves competitive performance in both image and video question answering tasks, surpassing MLLMs in several key metrics. Notably, in video question-answering, this advantage can be attributed to the effective integration of temporal contextual information and the efficient application of a contrastive example decoding strategy.\nIn Table I, we observe a notable performance boost by adding contextual examples when comparing LENS and Dola. Specifically, LENS shows up to 9% improvement when moving from zero to one example. CED, designed for our framework, outperforms Dola and LENS in the experiments, further validating its effectiveness within the MVCD framework. Thus, we focus on analyzing performance of MVCD (with CED) and draw the following conclusions:\n(i) MVCD enhances the visual perception capabilities of LLMs: In Table I, compared with LENS and Dola, our MVCD shows overall performance improvements across the five datasets, as illustrated in Fig. 2(a), with performance enhancements observed on VQAv2. As the number of examples increases, MVCD's performance boost is more significant than Dola's. Importantly, the original reasoning capabilities of LLMs are not weakened by the addition of the visual perception mod-ule; instead, they gain enhanced visual perception abilities. We speculate that the greater the performance gap between MVCD and LENS, the stronger its ability to learn associations between examples and contrastively enhance contextual knowledge.\n(ii) More contextual examples do not necessarily lead to better performance: In Table I, as the number of shots increases, the performance gap between MVCD and LENS reaches a saturation point and exhibits irregular fluctuations. For instance, Fig. 2(b) shows that for LLaMA2-7B-Chat in the three-shot setting on VQAv2, the performance gap reaches saturation and then fluctuates. Too much contextual example information may negatively affect MVCD's performance.\n(iii) Stonger LLMs indicate better multimodal reasoning under the MVCD schema: In Fig. 2, LLaMA3-8B-Instruct and Qwen2-7B outperform LLaMA2-7B-Chat and LLaMA2-13B-Chat at the saturation point of the performance gap. This suggests that stronger LLMs perform better in multimodal reasoning tasks."}, {"title": "B. Ablation Studies", "content": "In Fig. 3, we present model results with varied number of ICL examples. Our method demonstrates better robustness than baselines since our performance is further boosted with increased number of examples. Table III demonstrates that selecting examples based on question type is more effective than random selection, and example selection is crucial for achieving high performance in multimodal reasoning. In Table IV, removing either the attribute or tag module results in a performance drop. In the baseline, the tag module is more valuable than the attribute module, as it provides useful visual information within a certain threshold. Conversely, the attribute module is more effective than the tag module for contrastively learning contextual examples knowledge. Therefore, the combination of tags, attributes, and captions is crucial for achieving high performance in this task."}, {"title": "V. CONCLUSION", "content": "This paper introduces the MVCD framework with contrastive-example decoding (CED). Without task-specific training, it converts visual signals into text through a visual module. By using LLMs' in-context learning (ICL) to compare output probabilities with and without cross-modal examples, MVCD enhances multimodal reasoning and consistently performs well across five major datasets.\nThe proposed framework exhibits distinct advantages. First, it fully leverages the strengths of multimodal large language models such as CLIP, LanguageBind, BLIP, and Video-LLaVA utilizing their pretrained outputs to achieve exceptional visual content understanding without requiring retraining. Second, these multimodal large language models can be seamlessly replaced with smaller models tailored to similar tasks, such as image recognition models or video captioning models. However, this paper does not include comparative experiments with different pretrained models, which will be explored in future research."}]}