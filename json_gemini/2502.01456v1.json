{"title": "PROCESS REINFORCEMENT THROUGH IMPLICIT REWARDS", "authors": ["Ganqu Cui", "Lifan Yuan", "Zefan Wang", "Hanbin Wang", "Wendi Li", "Bingxiang He", "Yuchen Fan", "Tianyu Yu", "Qixin Xu", "Weize Chen", "Jiarui Yuan", "Huayu Chen", "Kaiyan Zhang", "Xingtai Lv", "Shuo Wang", "Yuan Yao", "Xu Han", "Hao Peng", "Yu Cheng", "Zhiyuan Liu", "Maosong Sun", "Bowen Zhou", "Ning Ding"], "abstract": "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.", "sections": [{"title": "INTRODUCTION", "content": "Dense process rewards, which provide feedback at each intermediate step rather than only the whole trajectory, have proven effective in inference-time scaling of large language models (LLMs) on challenging reasoning tasks (Uesato et al., 2022; Lightman et al., 2023; Wang et al., 2023; Yuan et al., 2024b). On the training side, they also present superiorities in the reinforcement learning (RL) of LLMs, particularly in improving training efficiency (Sutton & Barto, 2018) and credit assignment (Leike et al., 2018) compared with sparse outcome rewards. However, successful applications of dense rewards in RL for LLMs are limited (Setlur et al., 2024), as current industry-leading models primarily depend on verifiable outcome rewards and have not yet demonstrated meaningful progress with dense rewards (DeepSeek-AI et al., 2025; Team et al., 2025).\nWe identify the central challenge as how to acquire and utilize high-quality dense rewards at scale, which enables online process reward model (PRM) update efficiently. The reason is that, optimizing towards a static reward model eventually leads to overoptimization or reward hacking (Gao et al., 2022) due to distribution shift. Ideally, this can be solved by improving the reward model online (Leike et al., 2018). However, acquiring dense process labels for training is prohibitively more expensive. Existing methods either need to build complicated human annotation pipelines (Lightman et al., 2023) or rely on estimation-based methods, which require about 10\u00d7 more rollouts for each step than sampling only the response-level trajectories (Wang et al., 2023; Kazemnejad et al., 2024). Neither of them is scalable in online RL. Moreover, to the best of our knowledge, it remains underexplored how to incorporate dense rewards into RL for LLMs.\nIn this work, we propose Process Reinforcement through Implicit Rewards (PRIME), a scalable framework for enhancing reasoning capabilities via efficient reinforcement learning with dense token-level rewards. At its core, the framework employs recently proposed implicit process reward modeling (Yuan et al., 2024b) to train dense reward models with only outcome-level labels. This enables PRIME to perform online learning of reward signals using only outcome labels on policy rollouts, thereby fundamentally mitigating reward hacking while maintaining the same computational cost as traditional outcome reward models (ORMs). Besides scalability, PRIME also (1) serves as a general method to fuse token-level dense rewards and sparse outcome rewards by calculating their returns separately before summing together, which is compatible with diverse RL algorithms (Williams, 1992; Kool et al., 2019; Shao et al., 2024; Ahmadian et al., 2024; Schulman et al., 2017); (2) eliminates the dedicated reward modeling stage, which is required by existing works, by simply initializing from the SFT model or even the base model (\u00a7 5.6). In summary, starting from one single language model, the PRIME framework can efficiently accomplish the generation of dense rewards, the initialization and updating of reward models, as well as the reinforcement learning (RL) training of the policy model.\nIn experiments, we train Qwen2.5-Math-7B-Base (Yang et al., 2024b) with PRIME after a lightweight SFT warmup stage. Compared to RL using outcome rewards only, PRIME achieves a 2.5x sample efficiency gain and a 6.9% performance improvements on challenging math problems. As shown in Figure 1, through PRIME, we successfully achieve substantial improvement on key mathematical reasoning benchmarks over the SFT model, leading to 16.7% improvement on average, and over 20% on AMC&AIME competitions. Our final model Eurus-2-7B-PRIME surpassed Qwen2.5-Math-7B-Instruct on five key mathematical benchmarks. Notably, this is achieved with only 10% of the data used by Qwen-Math, as in Table 1."}, {"title": "2 REINFORCEMENT LEARNING FOR LLMS AND THE CHALLENGES OF INCOPORATING DENSE REWARDS", "content": "Reinforcement Learning (RL) aims to learn an optimal policy \u03c0\u03b8 that maximizes the expected cumulative discounted reward, namely return, when interacting with an environment. In the context of autoregressive language modeling, state at step t is the concatenation of prompt x and current response y<t, and the action is the t-th token or step Yt.\n2.1 RL PRELIMINARIES FOR LLMS\nPolicy Gradient. Policy gradient is a fundamental algorithm that directly optimizes this objective. Central to this approach is the advantage function At, which quantifies how much better an action is compared to alternatives in a given state:\n$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_{\\theta}} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (Y_t | y_{<t}) A_t$ (1)\nwhere (x, y) represents a pair of input and output. x is omitted for brevity. In practice, the advantage function is implemented as cumulative discounted rewards subtracting a baseline:\n$A_t = \\sum_{s=t}^{T} \\gamma^{s-t} r(y_s) - b$ (2)\n\u03b3\u2208 [0, 1] is a discount factor that optionally decays future rewards, and r(ys) is the reward provided by the environment at time step s with x and y<s being omitted in conditions. Eq. 2 is the general formula of the Monte-Carlo (MC) advantage estimate, which indicates that, the high-quality and dense reward at each step is crucial for RL. Different choices of b include, e.g. directly using values Williams (1992), group average of rewards (Shao et al., 2024), and leave-one-out average of rewards Ahmadian et al. (2024); Kool et al. (2019).\nValue Models. Though the MC estimate is unbiased, it suffers from high variance because of the reliance on all future actions and rewards, which can be random and noisy. Value models, which predict expected accumulated rewards starting from a state, are adopted to help reduce the variance in advantage estimation, such as Generalized Advantage Estimation (GAE; Schulman et al., 2016):\n$A^{GAE}_{t,(\\gamma, \\lambda)} = \\sum_{0}^{\\infty} (\\gamma \\lambda)^s \\delta_{t+s}$, where $\\delta_t = r(y_t) + \\gamma V(y_{<t+1}) - V(y_{<t})$ is the temporal difference (TD) error (Sutton, 1988), V is a value model, and \u03bb controls the bias-variance tradeoff in advantage estimation. PPO (Schulman et al., 2017) is a representative of such actor-critic algorithms that explicitly train a value model along with the policy.\nReward Sparsity. Although dense rewards can be naturally integrated into the advantage function through Eq. 2, unfortunately, only outcome reward models (ORMs) are available in most practices of LLMs, i.e., only the final token bears a meaningful reward while intermediate tokens receive no rewards (Rafailov et al., 2023; Shao et al., 2024; DeepSeek-AI et al., 2025). In this bandit setting, r(yt) = 0 for t < T while r(yT) can be non-zero, and Eq. 2 becomes A = r(yT) \u2212 b. This formulation, while simpler, can suffer from reward sparsity issues as the policy receives feedback only at the end of the entire generation. This may (1) encourage spurious solutions with incorrect processes but correct answers, (2) largely reduce sample efficiency in training, and (3) encounter the credit assignment problem (Sutton & Barto, 2018). These drawbacks could be further amplified on complicated tasks, which require more thinking and execution steps, urging the need of dense rewards (Uesato et al., 2022; Lightman et al., 2023). Some may consider employing a value model to mitigate the problem, as it predicts values at every step t. However, previous work showed that value models may not be able to solve the reward sparsity issue effectively due to training challenges, despite the additional computation overhead (Shao et al., 2024; Ahmadian et al., 2024). We will also empirically validate this claim in \u00a75.5."}, {"title": "2.2 KEY CHALLENGES IN SCALABLE DENSE REWARDS", "content": "The way to mitigate the reward sparsity problem is to adopt dense reward models, namely PRMs, which score model responses over each token or step. However, it is usually infeasible in practice to incorporate dense rewards into online RL because of three critical challenges in implementation.\nC1. Process rewards are hard to define. It is difficult to collect step-level labels since reasoning steps do not naturally occur in sequences. Although tokens are easily distinguishable, annotating labels for each token is too costly. Moreover, defining the absolute correctness of intermediate processes as dense rewards can be ambiguous, as some incorrect steps can also positively contribute to the final answer by pruning searching branches (OpenAI, 2024; DeepSeek-AI et al., 2025).\nC2. PRM online updates are not scalable. It is crucial to prevent reward overoptimization or reward hacking, which requires the reward model or value model to be updated online along with the policy model (Schulman et al., 2017; Gao et al., 2022). However, training PRMs often requires extensive nuanced step-level annotation, which is infeasible in online RL training. Therefore, this brings about considerable scalability and generalization concerns in dense rewards for RL.\nC3. Explicit reward modeling brings extra cost. Training reward models requires extensive annotation and broad data coverage to ensure a good balance between adaptability to the policy distribution and generalization to distribution shifts. Hence, the explicit training stage introduces a very costly data collection and an additional training overhead, especially for PRMs which typically require stepwise labels.\nNotably, a concurrent work shares similar conclusions and thus is impeded from incorporating PRMS into their large-scale RL training (DeepSeek-AI et al., 2025)."}, {"title": "3 PRIME", "content": "To address the above challenges, we propose PRIME, a scalable online RL method with dense rewards. The key insight of PRIME is to apply implicit process rewards, which are derivable from the Implicit PRM that is trained with only outcome labels (Yuan et al., 2024b). This property enables us to update the PRMs online to avoid reward hacking. We then design a flexible framework to incorporate implicit process rewards with outcome rewards into any kind of MC advantage estimate. PRIME is illustrated in Figure 2 and Algorithm 1. Next, we will detail the implicit process rewards (\u00a73.1) and how we leverage them to calculate advantages (\u00a73.2), and introduce other techniques we used (\u00a73.3).\n3.1 ENABLING SCALABLE REWARD UPDATE WITH IMPLICIT REWARD MODELING\nWe consider dense rewards from the Implicit PRM because of the scalability. In short, Implicit PRM enables training an ORM with outcome labels only while repurposing it as a PRM at inference. The training stage is the same as standard ORM pipelines, with the only difference being representing the reward as $\\text{r}(y) := \\beta \\log \\frac{\\pi_{\\phi} (y)}{\\pi_{ref} (y)}$, where \u03c0 is the RM and \u03c0ref is the reference model, both of which are causal LMs. At inference, the process rewards are obtained by:\n$r(y_t) := \\beta \\log \\frac{\\pi_{\\phi}(Y_t | y_{<t})}{\\pi_{ref} (Y_t | y_{<t})}$ (3)\nIn PRIME, upon rollouts being generated and graded by the (ground truth) outcome verifier, we update the Implicit PRM online with on-policy rollouts and outcome supervision and then calculate token-level dense rewards to estimate advantages, which solves C1 and C2 mentioned in \u00a72.2 respectively: (1) To prevent overoptimization and reward hacking, it is crucial to update reward models online. However, updating previous PRMs (Lightman et al., 2023) requires annotating step labels on the latest policy rollouts, which is neither efficient nor scalable during online RL. In contrast, the Implicit PRM only demands outcome labels to train due to its special reward representation, and thus it can be easily updated with policy rollouts and outcome labels or rewards, both of which have already been collected to update the policy model. (2) Unlike common PRMs that produce only step-level rewards, the Implicit PRM provides more fine-grained token-level rewards at no additional cost. This addresses the ambiguity in identifying steps in LLM responses while not introducing extra overhead, making it easy to combine with any RL algorithms for advantage estimation."}, {"title": "3.2 ADVANTAGE ESTIMATION AND POLICY UPDATE", "content": "Estimating advantages using Monte Carlo estimator with a leave-one-out baseline. After obtaining token-level dense rewards, we calculate advantages based on either MC estimators or GAE. To determine the advantage function in PRIME, we compare GAE with several MC estimators, including REINFORCE (Williams, 1992), RLOO (Ahmadian et al., 2024), and GRPO (Shao et al., 2024). Experimental details and results can be found in \u00a75.4.\nWe find that MC estimators, despite being simpler, are strong enough to produce stable results. Therefore, we choose MC estimate as our advantage function and despite PRIME being compatible with any baseline estimation approaches, we instantiate it with a leave-one-out baseline from K samples (Ahmadian et al., 2024) in this paper, as it performs better in the experiments:\n$A_i^T = r(y_T^i) - \\frac{1}{K-1} \\sum_{j\\neq i}^{K} r(y_T^j)$ (4)\nwhere r(yiT) denotes the reward of i-th response at final step T, K is the number of samples for one prompt. The leave-one-out (LOO) baseline helps reduce variances.\nMore specifically, we use an Implicit PRM \u03c0\u03c6 and an outcome verifier or reward model ro. We calculate the return of implicit process rewards and outcome rewards separately if both are available, since directly mixing their values may lead to numerical instability (Shao et al., 2024). For implicit process rewards, we perform a three-step process to calculate return: (1) Use the averaged implicit process rewards to calculate the leave-one-out baseline; (2) Normalize the process reward at step t by subtracting the baseline; (3) Calculate the discounted return for each response. For outcome rewards, we directly adopt LOO without any modification. Finally, the advantage is set to the combination of both returns:\n$A_t^T = \\sum_{s=t}^T \\gamma^{s-t} [\\gamma_{\\phi}(y^i_s) - \\frac{1}{K-1} \\sum_{j\\neq i} \\gamma_{\\phi}(y^j_s)] + r_o (y^i_T) - \\frac{1}{K-1} \\sum_{j\\neq i} r_o (y^j_T)$ (5)\nUpdating policy with PPO clip surrogate loss. We adopt PPO clip surrogate loss for more stable policy updates:\n$\\mathcal{L}_{CLIP}(\\theta) = \\mathbb{E}_t [\\min(\\frac{\\pi_{\\theta}(Y_t | y_{<t})}{\\pi_{\\theta^{old}} (Y_t | y_{<t})} A_t, clip(\\frac{\\pi_{\\theta}(Y_t | y_{<t})}{\\pi_{\\theta^{old}} (Y_t | y_{<t})}, 1-\\epsilon, 1+\\epsilon) A_t)]$ (6)\nwhere \u03f5 is a clipping parameter. The loss prevents the updated policy from deviating too far from the original distribution, which is the prerequisite of importance sampling. The legitimacy of importance sampling then enables the reuse of rollouts sampled in previous steps, thus improving sampling efficiency."}, {"title": "3.3 OTHER TECHNIQUES", "content": "Initializing PRM with SFT/base model. In practice, we find that the starting policy model itself serves as a decent initialization of PRM, bypassing the PRM training stage. This solves C3 in \u00a72.2 and even outperforms a dedicatedly trained PRM, as shown in \u00a7 5.1.\nOnline Prompt Filtering. As we sample multiple trajectories for each prompt, we introduce online prompt filtering which filters prompts within a certain accuracy range. This (1) preserves only the prompts within a certain median-level difficulty range (Yang et al., 2024b) and (2) balances data distribution for the Implicit PRM online training.\nWe present the ablation study results in Figure 3 using RLOO with outcome rewards only, from which we can see that the online prompt filter largely lowers the variance of RL training.\nHow PRIME addresses challenges in \u00a72.2. In summary, as illustrated in Figure 2 and Algorithm 1, PRIME adopts implicit process rewards for efficient PRM online update (C2), then integrates token-level dense rewards with outcome rewards in MC advantage estimate (C1). The PRMs are directly initialized from SFT or base models, which foregoes explicit reward modeling (C3)."}, {"title": "4 EXPERIMENTS", "content": "4.1 IMITATION WARMUP\nWe focus on mathematical and coding problems in this paper. For models, we start with Qwen2.5-Math-7B-Base (Yang et al., 2024b) for its great mathematical capabilities. We first performed supervised finetuning for RL preparation.\nData Construction. To construct the SFT dataset, we collect reasoning instructions from several open-source datasets. For completion, we employed LLaMA-3.1-70B-Instruct (Meta, 2024) to answer the instructions, with a system prompt requesting the model to perform action-centric chain-of-thought. We finally obtained 230K SFT data, the detailed sources and statistics can be found in \u00a7 A.\nSFT Results. After finetuning, the performance of our SFT model is reported in Figure 1. Compared to baselines, Eurus-2-7B-SFT lags Qwen2.5-Math-7B-Instruct on all mathematics benchmarks."}, {"title": "4.2 RL SETTINGS", "content": "Rule-based Outcome Verifier. Consistent with recent research that adopts exact match with ground truth as unhackable rewards (Gao et al., 2024; Lambert et al., 2024; DeepSeek-AI et al., 2025), we define the rule-based ground truth outcome verifiers (OV) for math and coding as follows:\n$r_{math}^{(T)}(y) = \\begin{cases} 1, \\text{matched} \\\\ 0, \\text{otherwise} \\end{cases}$ $r_{code}^{(T)}(y) = \\frac{\\sum # passes}{# test cases}$\nHyperparameters. We use veRL (Sheng et al., 2024) to conduct experiments. By default, we initialize the Implicit PRM with SFT model and retain the SFT model for reference logprobs. For hyperparameters, we use a constant 5 \u00d7 10\u22127 learning rate together with AdamW optimizer for policy model, and use a 10\u22126 learning rate for PRMs. Both policy and PRMs use a batch size of 256 and micro batchsize of 8. The rollout stage collects 256 prompts and samples 4 responses for each prompt. We set \u03b2 = 0.05 for PRM training. We set KL coefficient to 0 in all experiments.\nEvaluation Benchmarks. We evaluate on 7 reasoning benchmarks, focusing on competition-level mathematics and programming tasks, including AIME 2024 (Li et al., 2024), AMC (Li et al., 2024), MATH-500 (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024), LeetCode (Guo et al., 2024), and LiveCodeBench (v2) (Jain et al., 2024).\n4.3 MAIN RESULTS\nAs shown in Figure 1 and Table 2, Eurus-2-7B-PRIME achieves substantial improvements on key reasoning benchmarks over the SFT version of the model, leading to 15.1% improvement on average, and over 20% on AMC and AIME competitions. Besides, Eurus-2-7B-PRIME achieves 26.7% pass@1 on AIME 2024, surpassing GPT-40, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct, demonstrating its excellent reasoning ability."}, {"title": "4.4 DENSE REWARDS V.S. SPARSE REWARDS", "content": "We first validate the effect of dense rewards compared to RLOO with outcome rewards only. We train this model for 240 steps. For PRIME, we use the same setting and train the model for 592 steps. We plot the training rewards measured by the outcome verifier and test accuracy in Figure 4. Compared with sparse reward, PRIME takes 40% of the training steps to achieve the same training rewards as RLOO and improves the final rewards by 6.9%, with lower variances. On downstream tasks, PRIME also consistently outperforms OV only setup. Detailed results are listed in Table 2."}, {"title": "5 ANALYSIS", "content": "5.1 DESIGN CHOICES FOR THE IMPLICIT PRM\nThe Implicit PRM is the key component of PRIME, and its design choices greatly affect RL. In this section, we explore two major factors: (1) the initialization model and (2) the update mechanism.\nSFT model initializes a good PRM. Conventionally, we need to collect data to train RMs and PRMs, and then we can use them in RL. However, the Implicit PRM is a language model, so we can initialize it from any language model with the same tokenizer as the policy model. To investigate whether it is still necessary to train a PRM in advance, we conduct experiments with different PRM initialization strategies: with the SFT model itself and with a specially trained PRM. For the later one, we train EurusPRM from Eurus-2-7B-SFT with additional 500K data generated by Llama3.1 and Qwen2.5 series (data details in \u00a7 B.5).\nWe report the experiment results in Figure 5. Surprisingly, directly using Eurus-2-7B-SFT to initialize the PRM greatly outperforms EurusPRM which was trained on more samples. We conjecture that initializing policy model and PRM from the same model largely alleviates the distribution shift issue, as the PRM is only trained on the online rollouts from the policy model.\nOnline PRM update is essential. To verify the effect of online PRM update, we pair the correct and wrong samples and calculate the PRM prediction accuracy using $\\frac{r_{\\phi}(y)}{r(y)}$. We report the PRM classification accuracy in Figure 6. The figure clearly shows that, online update mitigates overoptimization and reward"}, {"title": "5.2 REFERENCE MODEL CHOICE IS FLEXIBLE", "content": "We implement two variants of our algorithms to explore the effect of reference model of implicit PRM, one using the initial SFT model as the reference model (SFT ref) while the other using the running policy's old logprobs as reference (policy ref), as shown in Figure 7a. The policy ref simply adopts the old logprob of the policy model as \u03c0ref, while the SFT ref remains the initial SFT model for an additional \u03c0ref calculation. We compare their performance in this section.\nFrom the training rewards in Figure 8, we find the two strategies are close and have pros and cons in different aspects: The Q value calculated by implicit PRM is the expectation under the distribution of the reference model. So the updating policy could natrually serve as the reference. On the other hand, KL divergence calculation is only allowed when the initial SFT model is retained."}, {"title": "5.3 SINGLE-FORWARD V.S. DOUBLE-FORWARD", "content": "Since our implicit PRM is concurrently updated in training, for each rollout stage, we can update the PRM before the policy model and use the updated PRM to re-calculate the process rewards, which"}, {"title": "5.4 PRIME WITH OTHER RL ALGORITHMS", "content": "As we stated before, PRIME is equally applicable to other RL algorithms beyond RLOO. In this section, we implement PRIME with REINFORCE (Williams, 1992), GRPO (Shao et al., 2024), and PPO (Schulman et al., 2017). Similarly to RLOO, we only modify the advantage estimation functions and leave the clip surrogate loss unchanged.\nFirst of all, We compare different REINFORCE-like advantage estimators including REINFORCE, GRPO, and RLOO, toggling the existence of implicit process reward. To make different algorithms compatible with the compound of outcome verifier reward and process reward, we accordingly make adaptions similar to Eq. 5. For GRPO, we have\n$A_t^i = \\frac{r_o (y) - mean(r_o (y_i))}{\\text{std}(r_o (y_i))}$ $\\sum_{i=1}^{y} \\frac{\\gamma(y_{t})}{\\text{std} (y_{t})}$ (7)\nFor REINFORCE, we have\n$A_t^i = r_o  + \\sum y  $(8)\nFrom Figure 10 and Table 3, We show that PRIME boosts these algorithms on both efficiency and performance as it does with RLOO. PRIME contributes consistently regardless of the policy update method, making it a generic algorithm. It indicates that PRIME is a general plug-in for almost any RL algorithm for LLM., which largely extends the use cases of PRIME.\nMoreover, the PPO variant of PRIME provides no performance gain, demonstrating that the additional computation cost from the critic model is redundant. This makes it possible to compensate for the expense of the process reward model by using REINFORCE-like algorithms with simpler advantage estimators. Finally, we choose the best-performing RLOO as the advantage estimator in our algorithm."}, {"title": "5.5 VALUE OR REWARD, HOW TO USE THE IMPLICIT PRM?", "content": "Besides using process rewards to estimate returns, we can also employ the Implicit PRM to predict values for advantage estimation in Eq. 2. Therefore, we compare four variants of MC estimate to determine the best way to incorporate dense supervision. Recall that the Implicit PRM has the form $\\pi_{\\phi}(y_i | y_{<i})\\log \\frac{\\pi_{\\phi}(y_i | y_{<i})}{\\pi_{ref} (y_i | y_{<i})}$ with the process reward being $\\text{r}(y_t) = V(y_{<t+1}) - V(y_{<t})$, and we assume a ground-truth outcome verifier r_o(y) = 1, then we represent the variants as follows:\n(1) REINFORCE: At = ro(y).\n(2) On top of (1), using a linear-head value model V to calculate the baseline: At = ro(y)-V(y<t). This is the original PPO in Figure 10 as we set \u03b3 = 1 and \u03bb = 1.\n(3) On top of (1), using values from the Implicit PRM to serve as the baseline: At = ro(y) - Vr(y). This is equivalent to PPO with its value model being replaced by values from the Implicit PRM when \u03b3 = 1 and \u03bb = 1.\n(4) On top of (1), using process rewards from the Implicit PRM to calculate the return: $A_t = r_o(y) + \\sum \\gamma  $. This is the REINFORCE w/ PRIME in Figure 10.\nFigure 11 reports the results. Comparing PPO and REINFORCE, we find that an additional value model does not benefit policy performance. Notably, using rewards from the Implicit PRM to calculate returns, which is the default setting in PRIME, greatly outperforms all three baselines, regardless of where the values come from. This indicates that PRMs work better than value models in RL for LLMs."}, {"title": "5.6 \"ZERO\" EXPERIMENTS", "content": "DeepSeek-AI et al. (2025) proposed DeepSeek-R1-Zero, which is directly trained from a base model with reinforcement learning. To further investigate the \"Zero\" setting, we also perform RL from"}, {"title": "6 RELATED WORK", "content": "RL for LLM Reasoning. In the context of LLMs, reinforcement learning has been widely used for aligning human preferences (Christiano et al., 2017; Ouyang et al., 2022; Cui et al., 2024), but the open-source community mostly adopt the data-driven imitation learning methods (Yuan et al., 2024a; Yue et al., 2024; Wei et al., 2024; Liu et al., 2024) to enhance the reasoning capabities of LLMs. Over the past few months, the paradigm gradually shifted. OpenAI o1 (Jaech et al., 2024) first showed the tremendous potential of large-sacle RL for reasoning LLMs, and recent works have verified the scaling effect of the simple RL recipe with merely outcome rewards (DeepSeek-AI et al., 2025; Team"}, {"title": "7 CONCLUSION", "content": "As the fuel of LLMs, data, will be depleted in the near future, we are entering a new era of search and exploration, which is exemplified by reinforcement learning (Sutton, 2019). This work develops PRIME, which produces and leverages dense rewards in online RL for LLM reasoning. Throughout the experiments, we validate that PRIME (1) greatly benefits sample efficiency and policy performance, (2) is easy to use with minimum cost, and (3) is a general method that works with broad RL algorithms together."}]}