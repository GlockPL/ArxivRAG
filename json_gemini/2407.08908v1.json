{"title": "Are They the Same Picture? Adapting Concept Bottleneck Models for Human-AI Collaboration in Image Retrieval", "authors": ["Vaibhav Balloli", "Sara Beery", "Elizabeth Bondi-Kelly"], "abstract": "Image retrieval plays a pivotal role in applications from wildlife conservation to healthcare, for find- ing individual animals or relevant images to aid diagnosis. Although deep learning techniques for image retrieval have advanced significantly, their imperfect real-world performance often necessi- tates including human expertise. Human-in-the- loop approaches typically rely on humans com- pleting the task independently and then combining their opinions with an AI model in various ways, as these models offer very little interpretability or correctability. To allow humans to intervene in the AI model instead, thereby saving human time and effort, we adapt the Concept Bottleneck Model (CBM) and propose CHAIR. CHAIR (a) enables humans to correct intermediate concepts, which helps improve embeddings generated, and (b) al- lows for flexible levels of intervention that accom- modate varying levels of human expertise for bet- ter retrieval. To show the efficacy of CHAIR, we demonstrate that our method performs better than similar models on image retrieval metrics without any external intervention. Furthermore, we also showcase how human intervention helps further improve retrieval performance, thereby achieving human-AI complementarity.", "sections": [{"title": "1 Introduction", "content": "Recent advances in AI have shown great promise in impor- tant, often high-risk domains like healthcare [Peiffer-Smadja et al., 2020; Rajpurkar et al., 2020], wildlife conservation [Kulits et al., 2021; Beery et al., 2019], and reducing mis- information [Mendes et al., 2022]. However, these advances are imperfect, and can lead to harm when deployed. For ex- ample, [Beede et al., 2020] reports errors when deploying AI models to detect diabetic retinopathy due to challenging real- world factors like lighting, leading to potential human harms. Researchers have proposed human-AI collaboration as a promising approach to mitigate the shortcomings of AI mod- els in these domains [De-Arteaga et al., 2020]. For example, prior work in health AI has sought to achieve better accuracy via decision-support tools for clinicians [Peiffer-Smadja et al., 2020] that assist humans in decision-making. Human-in- the-loop participatory systems have also been proposed to ac- curately and robustly categorize wildlife images [Kulits et al., 2021; Miao et al., 2021; Bondi et al., 2022] and to aid fact- checkers [Nguyen et al., 2018; Mendes et al., 2022]. Concept Bottleneck Models (CBMs) are another recent and promising method to facilitate collaboration [Koh et al., 2020]. They allow humans to interact with AI models by viewing and ma- nipulating intermediate, high-level concepts (e.g., whether a bird has a blue wing), which are then used for prediction.\nWhile recent works using CBMs have significantly im- proved performance in classification [Zarlenga et al., 2023], we note that there exist many non-classification application areas that also require human-AI collaboration. For example, ElephantBook [Kulits et al., 2021] is a state-of-the-art ele- phant re-identification platform \u2013 meant to determine which individual elephant is depicted in each image \u2013 that adopts a semi-automated human-in-the-loop approach. This approach retrieves and presents to a user the most likely matches to individuals in a known population database for each new ele- phant sighting. They combine a neural network-based visual similarity ranking with a similarity ranking based on domain expert input via a weighted average (see Figure 1a). However, the weighted average approach requires hyperparameter tun- ing (a) depending on each user's expertise. Enabling humans to leverage concepts that they already use for retrieval to in- stead interact with the neural network in the embedding space has the potential to significantly reduce the amount of tuning required for each user while still including human inputs to improve team performance, as illustrated in Figure 1b).\nIn this work, we aim to adapt CBMs to facilitate close human-AI collaboration at deployment time for image re- trieval tasks, such as ElephantBook. We specifically aim to answer the following research questions: (RQ1) How do representations generated by CBMs compare to corre- sponding traditional models? (RQ2) How can we aug- ment CBMs to enable human intervention in image re- trieval and classification? (RQ3) How can we train these models to incorporate varying levels of expertise? To answer these questions, we analyze how CBMs com- pare with traditional models for image retrieval, introduce CHAIR (CBM-Enabled Human-AI Collaboration for Image Retrieval), a novel CBM architecture that allows humans to intervene and encode concepts to improve"}, {"title": "2 Background and Motivation", "content": "Concept Bottleneck Models: The key promise of CBMs\nis two-fold: CBMs a) predict high-level intermediate con- cepts, which are then used to predict the final class label, thus improving interpretability, and b) enable humans to in- tervene and correct these intermediate concepts to improve classification performance, thus providing intervenability. This is achieved in two steps: i) Concept Bottleneck, which predicts the high-level, understandable concepts, and ii) Classifier, which predicts the final class based on the predicted concepts (see Figure 2). Human-AI collabo- ration is made possible here by allowing humans to correct the model by modifying these intermediate concepts. This is shown to increase the overall classification performance on various tasks [Koh et al., 2020].\nImage retrieval: Image retrieval is a key component in vi- sual tasks like image re-identification [Wang et al., 2020], wildlife conservation [Kulits et al., 2021], remote sens- ing [Liu et al., 2020], and visual recommendation systems [Shankar et al., 2017]. Image retrieval requires systems to fetch the most relevant images from a database given a query image, where relevance is defined depending on the appli- cation. Traditionally, applications using deep learning tech- niques to perform image retrieval leverage (latent) embed- dings generated by neural network trained for classification. For a given query image, a query embedding is generated us- ing the same neural network to find the top-k nearest em- beddings using some distance function [Wan et al., 2014], typically cosine distance. We refer to the images and embed- dings that are searched over as gallery images and embed- dings, respectively. Following previous literature, we utilize the Recall@k metric and RecallAccuracy@k metrics to measure the performance of an image retrieval technique. For a given image and label pair (xi, Yi), let the top-k re- trieved images be y', \u2208 R. Then, the Recall@k and\nRecallAccuracy@k are defined as follows:\n$Recall@k = \\frac{\\sum_{i=1}^{N}|| Y_{i} \\cap y'_{i}||}{N}$\n(1)\n$RecallAccuracy@k = \\frac{\\sum_{i=1}^{N}|| Y_{i} \\cap y'_{i}||}{N*k}$\n(2)\nThe Recall@k metric evaluates if there exists at least one accurate image in the top-k images that were retrieved, whereas the RecallAccuracy@k evaluates the number of accurate images retrieved in the top-K images.\nMotivation of our work: Current methods in image re- trieval provide little room for human-AI collaboration. Plat- forms like ElephantBook, a practical, state-of-the-art com- puter vision system, still require a human-in-the-loop to achieve reliable performance, thus highlighting the impor- tance of human-AI collaboration. Our contributions in this paper leverage CBMs to enable human-AI collaboration in image retrieval tasks. Previous research in CBMs until now focus largely on improving performance by proposing dif- ferent architectures [Espinosa Zarlenga et al., 2022; Kim et al., 2023; Marconato et al., 2022], modify loss functions [Sheth and Ebrahimi Kahou, 2024], mitigate leakage [Havasi et al., 2022], improve intervenability [Zarlenga et al., 2023; Marcinkevi\u010ds et al., 2024] or circumvent requirements of la- bels [Oikarinen et al., 2023]. In this work, our novelty lies in expanding intervenability capabilities of CBMs to tasks like image retrieval, which require capturing corrected concepts into the embedding. All the previously mentioned works are orthogonal to the aim of this paper and are complementary in improving the performance of CBMs."}, {"title": "3 Do CBMs Already Work for Image Retrieval?", "content": "This section aims to establish the difficulties faced in\nadopting single-agent (only AI or human) approaches and\nshow how human-AI teams, specifically through CBMs, can\npotentially resolve some of these issues."}, {"title": "Neural networks - irremediable by humans", "content": "The key\ndrawback of using neural networks is that when these models\nmake mistakes or inaccurate inferences, their architecture and\ndesign allow little room for intermediate human oversight, es-\npecially for tasks like image retrieval where the inaccuracies\nare not obvious until the results are inspected."}, {"title": "Expert coding - costly and high barrier of entry", "content": "Hand-\ncrafted features and coding of retrieval systems require ad-\nvanced domain knowledge and experience, which raises the\nbarrier of entry for inexperienced users to utilize such sys-\ntems. Furthermore, these codes often require entire feature\nsets to achieve perfect retrieval. Any ambiguity in identifying\nthese codes, for example, due to ambiguity in or occlusion of\nconcept-relevant parts of the image, is marked as Wildcards\nin such systems. Since these codes typically lie on a spec-\ntrum of easily identifiable to years of expertise needed, we\ncan leverage neural networks to alleviate some of these diffi-\nculties while having a variable level of human oversight and\nhelp reduce wildcard entries with a collaborative human-AI\nretrieval system."}, {"title": "Are CBMs the solution? Potentially", "content": "Our contributions\nstem from the observation that high-level concepts are anal-\nogous to the codes developed by domain experts to help hu-\nmans retrieve relevant images. However, adopting these con-\ncepts (in the case of CBMs) or expert codes (in the case of hu-\nmans) to enable collaborative embedding generation for im-\nage retrieval is not supported by existing CBM architectures\n(see Table 1)."}, {"title": "4 Our Proposed Architecture: CHAIR", "content": "We have established the importance of enabling human-AI\ncollaboration for tasks beyond classification. Keeping in\nmind (RQ2), which asks \"How can we augment CBMS\nto enable human interventions on the representations\"\nand (RQ3) that asks \"How can we enable different levels\nof expertise for intervention\", we now present CHAIR, a\ntwo-stage CBM-like architecture that helps address these\nquestions. This section is organized into two parts: (a)\nArchitecture and (b) Training.\nArchitecture: Figure 4 illustrates our proposed CHAIR ar-\nchitecture. Let ( indicate the encoder that generates embed-\ndings z from the input image x. f(x) denotes the concept\nhead that generates the concept vector c from the input image\nx and (c) denotes the classification head that outputs the fi-\nnal class label y. The Concept Bottleneck comprises of the\nencoder and the concept head, that is $(((x)). The final class\nprediction in a CBM is as follows:\n$y_{CBM} = ((((x))$\n(3)\nWe propose a simple two-stage modification to the vanilla\nCBM architecture that enables integrating human inter-\nventions to improve retrieval. Specifically, as illustrated in\nFigure 4, our architecture introduces a Fusion Head, whose"}, {"title": "5 Results", "content": "5.1\nDatasets and Evaluation\nWe conduct experiments on two real-world datasets (simi-\nlar to [Espinosa Zarlenga et al., 2022]), the Caltech-UCSD-\nBirds-200-2011 (CUB) dataset [Wah et al., 2011] and Large-\nscale CelebFaces Attributes dataset (CelebA) [Liu et al.,\n2015] to demonstrate the effectiveness of our proposed archi-\ntecture. Specifically, we utilize the CUB dataset to measure\nperformance in both classification and image retrieval tasks,\nwhile the CelebA dataset is used for classification only. The\nCUB dataset comprises n = 11,788 bird images belonging to\na total of 200 possible species. There are 112 binary concepts\nassociated with each image. We follow the same experimen-\ntal setup for the classification task as detailed in [Koh et al.,\n2020], where the dataset is divided into training, validation,\nand testing. Furthermore, we follow the data split established\nin the literature for image retrieval. The training data here\nconsists of bird images from the first 100 classes. The images"}, {"title": "5.2 Retrieval Performance", "content": "Figure 5 shows how CHAIR performs against a standard\nResNet-18 model and CBM. A typical CBM with the same\nunderlying ResNet-18 architecture performs slightly worse\nthan the standard ResNet model, which aligns with the behav-\nior noted by [Koh et al., 2020] in classification. Our model\nprovides 15-20 % Recall@k improvement for k = [1, 5, 10],\nindicating that our model has learned better-unedited embed-\ndings as opposed to the embeddings from the standard and\nCBM models. Furthermore, we randomly select p% of con-\ncepts for every image in the gallery and each image dur-\ning query to investigate how intervention improves retrieval\nand evaluate the Recall@k performance, as shown in Figure\n6. We observe that as the number of interventions (correc-\ntions) increases, the retrieval performance improves, eventu-\nally reaching near-perfect Recall@k. A key observation that\nemerges from this plot is that when all the predicted concepts\nare set to their correct values, the method achieves nearly per-\nfect recall when k = 10.\nIn the real world, it is possible that the labelers construct-\ning the gallery embeddings could be less experienced than"}, {"title": "5.3 Importance of Stage 2", "content": "Recall that Stage 2 was introduced to help improve perfor-\nmance under partial intervention. To measure the impact of\nStage 2, we calculate the recall values of both the training\nmodes (Seq and Joint, as defined in Section 4) perform\nwith and without Stage-2 training for each subset of inter-\nvention in Figure 9. This figure clearly shows the benefit ac-\nquired when Stage 2 training is employed, especially in the\n0-80% range, where we obtain 5%-25% improvement in re-\ncall depending on the intervention. Note that the Seq. per-\nformance here without Stage 2 implies the classification head\nis trained on predicted concept activation values without any\nintervention."}, {"title": "5.4 Quality of Edited Representations", "content": "In order to visualize the representations at each intervention\nlevel, we use t-Distributed Stochastic Neighbor Embedding\n(t-SNE), a dimensionality reduction technique that helps vi-\nsualize high-dimensional data, which in our case are the em-\nbeddings that are used for retrieval. We extract embeddings\nfor each image present in our test data (all the classes in the\ntest data are unseen during training) and apply t-SNE to re-\nduce them to 2 dimensions. We then plot these reduced em-\nbeddings, as shown in Figure 7 for different levels of inter-"}, {"title": "5.5 Classification Performance", "content": "As referred to in (RQ2), our model must be able to match\nthe performance of CBMs in classification and allow inter-\nvenable retrieval so that practitioners do not have a trade-\noff when selecting the architectures. Figure 10 compares the\nclassification performance of both sequential (Seq) and joint\ntraining modes under perfect concept intervention against the\nbest-performing classifying CBM on both the CUB and the\nCelebA datasets. It is evident that our proposed CHAIR\nmodel outperforms the standard best-performing CBM in\nthese real-world datasets. Furthermore, this plot shows\nhow Stage 2 helps improves classification performance, thus\nshowing its importance beyond improving retrieval under par-\ntial intervention."}, {"title": "6 Conclusion and Future Work", "content": "In this work, we first establish that CBMs, a model that allows\nfor human-AI collaboration, underperform when compared\nto standard neural networks on retrieval tasks. We propose\nCHAIR, a modification of both CBM architecture and training\nstrategy, which can a) incorporate human input in the form\nof concept correction for image retrieval, b) allow varying\nlevels of human input and expertise, and c) significantly im-\nprove retrieval performance while maintaining similar classi-\nfication performance to vanilla CBMs. Furthermore, we show\nthat the quality of the embeddings generated with some cor-\nrections performs better than the alternatives through t-SNE\nplots and improved retrieval performance. Finally, it is evi-\ndent that while both the Seq and Joint training modes perform\nequally well with a high level of intervention, we recommend\nchoosing the joint training mode for image retrieval-related\ntasks. Our work enables the expansion of CBMs to domains\nthat move beyond classification, further improving human-AI\ncollaboration in impactful applications such as wildlife pop-\nulation monitoring.\nWe envision future work in this area along multiple fron-\ntiers: a) incorporating probabilistic concepts, label predic-\ntion, and embeddings to capture the uncertainty of the pre-\ndiction process better [Kim et al., 2023; Li et al., 2021], b)\nlearning when and how to defer to humans that are inter-\nfacing with CBMs to achieve the best possible complemen-\ntary performance [Bondi et al., 2022; Mozannar and Sontag,\n2020], and c) conducting human studies to understand ef-\nfective strategies of presenting CBM information to achieve\ncomplementarity."}, {"title": "Ethical Statement", "content": "Our contributions and future work involve including and\ncomplementing human efforts. Hence, we strongly advo-\ncate rigorously testing with all stakeholders before deploying\nCHAIR-like models."}]}