{"title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMS", "authors": ["Yifei Zhang", "Hao Zhu", "Aiwei Liu", "Han Yu", "Piotr Koniusz", "Irwin King"], "abstract": "Fine-tuning Large Language Models (LLMs) has become a crucial technique\nfor adapting pre-trained models to downstream tasks. However, the enormous\nsize of LLMs poses significant challenges in terms of computational complexity\nand resource requirements. Low-Rank Adaptation (LoRA) has emerged as a\npromising solution. However, there exists a gap between the practical performance\nof low-rank adaptations and its theoretical optimum. In this work, we propose\neXtreme Gradient Boosting LoRA (XGBLORA), a novel framework that bridges\nthis gap by leveraging the power of ensemble learning. Inspired by gradient\nboosting, XGBLORA iteratively learns and merges a sequence of LoRA adaptations\nto refine model predictions. It achieves better performance than the standard LoRA,\nwhile enjoying the computational efficiency of rank-1 adaptations. We provide\ntheoretical analysis to show the convergence and optimality of our approach, and\nconduct extensive experiments on a range of natural language processing tasks.\nThe results demonstrate that XGBLORA consistently outperforms standard LORA\nand achieves performance comparable to full fine-tuning with significantly fewer\ntrainable parameters. This work advances parameter-efficient fine-tuning for LLMs,\nand offers a promising solution for adapting LLMs to downstream tasks while\noptimizing performance and efficiency.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved re-\nmarkable success in various natural language process-\ning tasks, enabling breakthroughs in language under-\nstanding, generation, and reasoning [7, 35, 4]. Simi-\nlar to Self-Supervised Learning methods in other do-\nmains [58, 42, 57, 31, 56, 59], LLMs are typically pre-\ntrained on vast amounts of unlabeled text data, and then\nfine-tuned on specific downstream tasks to adapt their\nknowledge to the target domain [48, 38, 49]. However,\nthe enormous size of LLMs, often reaching billions of\nparameters, poses significant challenges in terms of com-\nputational complexity and resource requirements during\nfine-tuning [24, 4].\nTo address these challenges, a promising direction called\nparameter-efficient fine-tuning (PEFT) [16, 50, 18]\nadapts LLMs to downstream tasks while minimizing\nthe number of trainable parameters, thereby reducing"}, {"title": "2 Related Works", "content": "Fine-tuning LLMs has become a prevailing approach for adapting these models to specific down-\nstream tasks [17, 8, 19, 9]. The process involves training the model on a task-specific dataset, usually\nwith a smaller learning rate compared to pre-training, to adapt its parameters to the target task. Fine-\ntuning has been successfully applied to a wide range of natural language processing tasks, including\ntext classification, question answering, and natural language inference [4, 25, 34, 45]. However,\nfine-tuning LLMs faces several challenges. A major challenge is the computational complexity\nand memory requirements associated with updating billions of model parameters, which can be\nprohibitively expensive and time-consuming [43, 3, 36, 53, 40, 1, 45, 46, 6, 2, 23]. Additionally, the\nlimited availability of labeled data for specific tasks poses challenges in terms of sample efficiency\nand generalization ability [54]. To address these challenges, various approaches have been proposed\nto improve the efficiency and effectiveness of LLM fine-tuning. One notable technique is LORA [18]\nwhich freezes the pre-trained model's weights and introduces a low-rank matrix to adapt the model to\nnew tasks. LoRA reduces the number of trainable parameters and reduces the computational burden\nof LLM fine-tuning. Recently, [51] provided theoretical results that characterize the expressive power\nof LORA for Fully Connected Neural Networks (FCNN) and Transformer Networks (TFN), which\nidentify the necessary rank of LoRA for adapting a frozen model to exactly match a target model.\nFor Transformer networks, any model can be adapted to a target model of the same size with LORA\nadapters of rank_r=embedding_size/2.\nGradient Boosting [11, 12] is a powerful ensemble learning technique that combines multiple\nweak learners to create a strong learner. The theoretical foundations of gradient boosting have been\nextensively studied, providing insights into its convergence properties, generalization ability, and\nrobustness to overfitting. A seminal work on gradient boosting theory by Zhang et al. [55] proved\nthat gradient boosting achieves the optimal convergence rate for a broad class of loss functions, high-\nlighting its theoretical optimality. Koltchinskii et al. [26] further investigated theoretical properties\nof gradient boosting from the perspective of empirical risk minimization. They derived bounds on\nthe generalization error of gradient boosting and showed that the technique is resilient to overfitting\nwhen the base learners are weak and the step size is appropriately chosen. They include insights into\nits convergence behavior, generalization ability, and robustness, which are relevant to the theoretical\nanalysis of our proposed XGBLORA framework."}, {"title": "3 Gradient Boosting Low-Rank Adaption", "content": ""}, {"title": "3.1 Preliminaries", "content": "Before delving into the details of XGBLORA, we first introduce key concepts and techniques that form\nthe foundation of our approach. Below, we provide an overview of gradient boosting and LoRA,\nhighlighting their principles, advantages, and relevance to LLM fine-tuning.\nN\nGradient Boosting (GB) [11, 12] combines multiple weak learners to create a strong learner. The\nkey idea is to iteratively train a sequence of models, each of which corrects mistakes of the preceding\nmodel. At each iteration, the model is trained to minimize the residual error between the current\npredictions and the target outputs. Formally, let $D = \\{(x_i, Y_i)\\}_{i=1}^N$ be a dataset of N examples,\nwhere $x \\in \\mathbb{R}^d$ is the input feature vector and $y_i \\in \\mathbb{R}$ is the corresponding target output. The goal of\ngradient boosting is to learn a function F(x) that maps the input features to the target outputs. The\nfunction F(x) is expressed as a sum of M weak learners $f_m(x)$:\n$F(x) = \\sum_{m=1}^{M} f_m(x)$.\n(1)\nThe weak learners $f_m (x)$ are typically simple models, such as decision trees or linear models, that\nare trained to minimize the residual error. At each iteration m, the residual error $r_{im}$ for the i-th\nexample is computed as:\n$r_{im} = y_i - F_{m-1}(x_i)$,\n(2)"}, {"title": "3.2 When Gradient Boosting Meets LoRA", "content": "Below we present the eXtreme Gradient Boosting LoRA method for Transformers, which combines\nthe principles of gradient boosting with the parameter-efficient adaptation technique of LoRA. It"}, {"title": "3.3 Understanding Ensemble of Weak Learners.", "content": "The crucial principle of Gradient Boosting is building a strong ensemble model with weak learn-\ners. For instance, a very successful gradient boosting method, i.e., Gradient Boost Decision Tree\n(GBDT) [5] artificially limits the tree boosters to a very shallow depth (usually only 1 split) to ensure\nthat each booster is only slightly better than the random decision. Thus, boosting algorithms are\nhighly resilient against noisy data and overfitting [12]. Since the individual booster is too simple to"}, {"title": "3.4 Computational Costs", "content": "Table 1 compares the cost of XGBLORA and LoRA. XGBLORA's computational cost is upper-bounded\nby the LoRA's cost, as XGBLORA selects fewer LoRA layers (random selection) and uses lower ranks\nfor training (rank-1 updates). The computational costs incurred by these two approaches are equal\nonly when XGBLORA selects ALL layers and uses the same rank as LoRA."}, {"title": "3.5 Theoretical Analysis of Gradient Boosting LORA", "content": "In this subsection, we present a theoretical analysis of the eXtreme Gradient Boosting LORA\n(XGBLORA) framework for Transformer-based language models. Our analysis aims to provide conver-\ngence guarantees and approximation error bounds for the proposed method. We begin by introducing\nnecessary definitions and then present key lemmas and theorems.\nNotation. Consider a neural network with L layers: $f(x) = f_L \\circ f_{L-1} \\circ \\cdot \\circ f_2 \\circ f_1(x)$, where\n$f_1(x) = W_1x$ is an embedding layer. Moreover, $f_i(x) = \\sigma(W_ix)$ for i = 2, . . ., L \u2212 1, where\n\u03c3 is a Lipschitz continuous activation function. $f(x) = \\phi(W_Lx)$, where \u03c6 is a convex function.\n$W_i \\in \\mathbb{R}^{d_i \\times d_{i-1}}$ are weight matrices."}, {"title": "4 Experimental Evaluation", "content": "Experiment Settings. We set the rank of XGBLORA to r = 1 and rank of LoRA to r = 8 as default.\nThe number of sampled layers for XGBLORA is L\u209b = 8. To ensure a fair comparison, we initially\nfine-tuned models with XGBLoRA following the LoRA configuration, e.g., weight initialization,\nlearning rate, etc. [18], and maintained the same training steps K for both XGBLORA and LoRA\nwhen fine-tuning on the same datasets. Since K is fixed, the number of iterations T for gradient\nboosting is calculated as $T = \\frac{K}{\u03ba}$. The training steps for each booster is set to \u043a = 8 to maintain\nminimal prediction power. We conduct experiment on three tasks including the GLUE benchmark,\ncommonsense reasoning, and MMLU. The codebases for baselines implementation and evaluation\nare sourced from their official GitHub repositories/library (i.e., Commonsense Reasoning, GLUE,\nand MMLU are from Hu et al. [20], Si et al. [41], Zheng et al. [60], respectively).\nGLUE Benchmark. In GLUE experiments, we employed one small scales of transformer, RoBERTa-\nbase [30], as the base model. We used the General Language Understanding Evaluation (GLUE) [37]\nbenchmark as our dataset, which comprises two single-sentence classification tasks, three similarity\nand paraphrase tasks, and four natural language inference tasks. Details of the GLUE dataset are\nprovided in Table 7 (Appendix). There are two prominent series of extension-based methods within\nparameter-efficient tuning. The first series, the Adapter derivatives, comprises methods such as\nthose introduced by Houlsby et al. [16, 17], and introduced by [33, 50], which incorporate small-\nscale neural modules, or adapters, into existing architectures. The second series, known as LoRA\nderivatives, includes developments such as LoRA [18], AdaLoRA [52], TriLoRA [10], FLORA [13],\nDORA [29], and DyLoRA [47], AdaLoRA [52], Delta-LoRA [61], MeLoRA [39], and ReLORA [28]."}, {"title": "4.1 Investigating the Weak Learner of Gradient Boosting", "content": "Number of Weak Learners (Iteration). The number of weak learners in XGBLORA is equal to the\nnumber of the iterations in gradient boosting. Since the total train step K is fixed per dataset. the"}, {"title": "5 Conclusions", "content": "We have proposed XGBLORA for fine-tuning LLMs in a parameter-efficient manner by posing fine-\ntuning as a gradient boosting where LoRA matrices are used as weak learners to be iteratively\ncombined to form a strong ensemble model. We provide theoretical analysis establishing the\nconvergence and approximation error of XGBLORA, highlighting the interplay between the LORA\nrank, expressiveness, and the number of boosting iterations. Extensive experiments demonstrate the\neffectiveness of XGBLORA, which consistently outperforms the standard LoRA while maintaining\nparameter/computational efficiency. Broader Impact & Limitations are in Appendices D & E."}, {"title": "A Detailed Proofs for XGBLORA Lemmas", "content": "Lemma 4 (XGBLORA Gradient Approximation) The XGBLORA update approximates the full gradi-\nent update with error:\n$\\|| \\nabla_WL(W^{(t)} + A^{(t)}B^{(t)T}) - A^{(t)} B^{(t)T ||_F \\leq \\frac{C_1}{\\sqrt{r}}+\\frac{C_2}{\\sqrt{M}}}$\nwhere r is the LoRA rank, M is the number of minibatches, and C\u2081, C\u2082 are constants.\nProof 1 1) Let $G = \\nabla_W L(W^{(t)} + A^{(t)}B^{(t)T})$ be the true gradient.\n2) The XGBLORA update $A^{(t)}B^{(t)T}$ can be seen as an approximation of G.\n3) Let $G_r$ be the best rank-r approximation of G. By the Eckart-Young-Mirsky theorem:\n$\\||G - G_r||_F \\leq \\frac{||G||_*}{\\sqrt{r}} \\triangleq \\frac{C_1}{\\sqrt{r}}$\nwhere $||.||_*$ is the nuclear norm and $C_1$ is a constant depending on the properties of L.\n4) The XGBLORA update $A^{(t)}B^{(t)T}$ is computed using M minibatches. Let $G_j$ be the gradient estimate\nfrom the j-th minibatch. Then:\n$A^{(t)}B^{(t)T} \\approx \\frac{1}{M} \\sum_{j=1}^{M} G_j$\n5) By the law of large numbers and assuming bounded variance of gradient estimates:\n$\\frac{1}{M} \\sum_{j=1}^{M} G_j - G ||_F \\leq \\frac{C_2}{\\sqrt{M}}$\nwhere $C_2$ is a constant related to the gradient variance.\n6) Combining these bounds using the triangle inequality:\n$\\||G \u2013 A^{(t)}B^{(t)T} ||_F \\leq ||G \u2013 G_r ||_F + ||G_r - A^{(t)}B^{(t)T}||_F \\leq \\frac{C_1}{\\sqrt{r}} + \\frac{C_2}{\\sqrt{M}}$\nThis completes the proof.\nLemma 5 (Accumulated Update Bound) For the XGBLORA update process:\n$\\||A^{(t)} ||_F \\leq \\eta_m M G \\text{ and } ||B^{(t)} ||_F \\leq \\eta_m M G$\nwhere G is an upper bound on $\\||\\nabla_W L(W^{(t)} + A^{(t)}B^{(t)T})||_F$.\nProof 2 1) Recall the update rule for $A^{(t)}$:\n$A^{(t)} = A^{(t)} - \\eta_m \\nabla_W L(W^{(t)} + A^{(t)}B^{(t)T})B^{(t)}$\n2) Taking the Frobenius norm and applying the triangle inequality:\n$\\||A^{(t)} ||_F \\leq || A^{(t-1)} ||_F + \\eta_m ||\\nabla_W L(W^{(t)} + A^{(t)}B^{(t)T}) ||_F ||B^{(t)} ||_F$\n3) Using the gradient bound $\\||\\nabla_W L(W^{(t)} + A^{(t)}B^{(t)T}) ||_F < G$:\n$\\||A^{(t)} ||_F \\leq || A ^{(t-1)} ||_F + \\eta_m G ||B^{(t)} ||_F$\n4) Applying this inequality recursively for all M minibatches, and noting that $A^{(t)}$ is initialized to 0:\n$\\||A^{(t)} ||_F \\leq \\eta_m M G ||B^{(t)} ||_F$"}, {"title": "5) Similarly for B(t), we can derive:", "content": "$\\||B^{(t)} ||_F \\leq \\eta_m M G ||A^{(t)} ||_F$\n6) Combining these inequalities:\n$\\||A^{(t)} ||_F \\leq \\eta_m M G \\text{ and } ||B^{(t)} ||_F \\leq \\eta_m M G$\nThis completes the proof.\nLemma 6 (Gradient Lipschitz Continuity) For any two weight matrices W\u2081 and W\u2082:\n$\\||\\nabla_W L(W_1) \u2013 \\nabla_W L(W_2)||_F \\leq L\u2019||W_1-W_2||_F$\nwhere L is the Lipschitz constant of the gradient.\nProof 3 1) This lemma is a standard assumption in optimization theory, often referred to as the\nsmoothness condition.\n2) It can be derived from the assumption that the Hessian of L is bounded:\n$\\||\\nabla^2 L(W)||_2 \\leq L\u2019 \\forall W$\nwhere $||.||_2$ denotes the spectral norm.\n3) By the mean value theorem, there exists a $W_t = tW_1 + (1 \u2212 t)W_2$ for some t \u2208 [0,1] such that:\n$\\nabla_W L(W_1) \u2013 \\nabla_W L(W_2) = \\nabla^2 L(W+)(W_1 \u2013 W_2)$\n4) Taking the Frobenius norm of both sides:\n$\\||\\nabla_W L(W_1) \u2013 \\nabla_W L(W_2)||_F = ||\\nabla^2 L(W+)(W_1 \u2013 W_2)||_F$\n5) Using the property that $||AB||_F \\leq ||A||_2||B||_F$:\n$\\||\\nabla^2 L(W+)(W_1 \u2013 W_2)||_F \\leq ||\\nabla^2 L(W_t)||_2 ||W_1 - W_2||_F$\n6) Applying the bound on the Hessian:\n$\\||\\nabla^2 L(W_t)||_2 ||W_1 - W_2||_F < L' ||W_1 - W_2||_F$\nThis completes the proof."}, {"title": "B Detailed Proof of XGBLORA Convergence Theorem", "content": "Theorem 3 (XGBLORA Convergence) Under the XGBLORA update process, assuming \u03b2-smoothness\nand \u00b5-strong convexity of L, after T iterations:\n$E[L(W^{(T)})] - L^* < \\frac{C_3}{\\sqrt{TM}} + \\frac{C_4}{\\sqrt{T}} + e(r)$\nwhere C\u2083 and C\u2084 are constants depending on \u03b2, \u03bc, G, \u03b7m, L, and $e(r) = \\frac{C_5}{r}$ for some constant C\u2085.\nProof 4 1) Let $W^{(t+1)} = W^{(t)} + A^{(t)}B^{(t)T}$ be the update at iteration t.\n2) By the \u03b2-smoothness of L:\n$L(W^{(t+1)}) \\leq L(W^{(t)}) + (\\nabla L(W^{(t)}), A^{(t)}B^{(t)T}) + \\frac{\\beta}{2} ||A^{(t)}B^{(t)T} ||^2_F$\n$< L(W^{(t)}) + (\\nabla L(W^{(t)}), A^{(t)}B^{(t)T}) + \\frac{\\beta}{2} ||A^{(t)} ||_F^2 ||B^{(t)} ||_F^2$\n3) Using the XGBLORA Gradient Approximation Lemma:\n$A^{(t)}B^{(t)T} = \\nabla_{W^{(t)}} L(W^{(t)} + A^{(t)}B^{(t)T}) + E^{(t)}$"}, {"title": "where $|| E^{(t)} ||_F < \\frac{C_1}{\\sqrt{r}} +\\frac{C_2}{\\sqrt{M}}$", "content": "4) Substituting this into the inequality from step 2:\n$L(W^{(t+1)}) \\leq L(W^{(t)}) + (\\nabla L(W^{(t)}), \\nabla_{W^{(t)}} L(W^{(t)} + A^{(t)}B^{(t)T}) + E^{(t)})\n+ \\frac{\\beta}{2} ||A^{(t)} ||_F^2 ||B^{(t)} ||_F^2$\n5) Using the Gradient Lipschitz Continuity Lemma:\n$\\||\\nabla L(W^{(t)}) \u2013 \\nabla_{W^{(t)}} L(W^{(t)} + A^{(t)}B^{(t)T}) ||_F < L' ||A^{(t)}B^{(t)T} ||_F$\n6) Applying Cauchy-Schwarz inequality and the bound from step 5:\n$L(W^{(t+1)}) < L(W^{(t)}) - || \\nabla_{W^{(t)}} L(W^{(t)} + A^{(t)}B^{(t)T})||\n+ L' ||A^{(t)}B^{(t)T} ||_F \\text{ + } ||\\nabla L(W^{(t)}||_F ||E^{(t)}||_F + \\frac{\\beta}{2} ||A^{(t)} ||_F^2 ||B^{(t)} ||_F^2$\n7) Using the Accumulated Update Bound Lemma and the gradient bound G:\n$L(W^{(t+1)}) \\leq L(W^{(t)}) \u2013 (1 \u2013 L\u2019\\eta_m^2 M^2 G^2 \u2013 \\frac{\\beta}{2} \\eta_m^2 M^2 G^2) || \\nabla_{W^{(t)}} L(W^{(t)} + A^{(t)}B^{(t)T}) ||_F\n- G (\\frac{C_1}{\\sqrt{r}} + \\frac{C_2}{\\sqrt{M}})$"}, {"title": "8) By \u00b5-strong convexity of L:", "content": "$\\||\\nabla_{W^{(t)}} L(W^{(t)} + A^{(t)}B^{(t)T}) || \\geq 2 \\mu (L(W^{(t)} + A^{(t)}B^{(t)T}) \u2013 L^*)$\n9) Substituting this into the inequality from step 7:\n$L(W^{(t+1)}) \u2013 L^* \\leq (1 \u2013 2 \\mu \\alpha) (L(W^{(t)}) \u2013 L^*) + G (\\frac{C_1}{\\sqrt{r}} + \\frac{C_2}{\\sqrt{M}})$\nwhere $\u03b1 = 1 \u2013 L\u2019\\eta_m^2 M^2 G^2 \u2013 \\frac{\\beta}{2} \\eta_m^2 M^2 G^2$.\n10) Taking expectation and applying this inequality recursively for T iterations:\n$E[L(W^{(T)}) \u2013 L^*] \\leq (1 \u2013 2 \\mu \u03b1)^T (L(W^{(0)}) \u2013 L^*) + \\frac{G (\\frac{C_1}{\\sqrt{r}} + \\frac{C_2}{\\sqrt{M}})}{2 \\mu \u03b1}$"}, {"title": "11) Using the inequality $(1 \u2212 x)^T < exp(\u2212xT ) \u2264 for x \u2208 (0, 1):", "content": "$E[L(W^{(T)}) \u2013 L^*] < \\frac{C_3}{\\sqrt{TM}} + \\frac{C_4}{\\sqrt{T}} + \\frac{C_5}{r}$"}, {"title": "Proof 5 1) Let W* be the weights that exactly represent f* in the original function class.", "content": "2) Define fopt as the best function that can be represented by XGBLORA updates:\n$f_{opt} = \\text{arg min}_{f \\in F_{XGBLORA}} E_{x \\sim D} [(f(x) \u2013 f^*(x))^2]$\nwhere $F_{XGBLORA}$ is the class of functions representable by XGBLORA updates.\n3) We can decompose the error as:\n$E_{x \\sim D}[(f_1(x) \u2013 f^*(x))^2] \\leq 2E_{x \\sim D}[(f_T(x) \u2013 f_{opt}(x))^2] + 2E_{x \\sim D}[(f_{opt}(x) \u2212 f^*(x))^2]$\n$\\triangleq 2E_1 + 2E_2$\n4) For E\u2081, we can use the Convergence Theorem (Theorem 1):\n$E_1 \\leq K_1(\\frac{1}{\\sqrt{TM}} + \\frac{1}{\\sqrt{T}})$\nwhere K\u2081 is a constant related to C\u2083 and C\u2084 from Theorem 1.\n5) For E\u2082, we need to analyze how well XGBLORA updates can approximate W*. Let $\u0394W = W^* - W^{(0)}$.\n6) We can approximate \u25b3W with a sequence of low-rank updates:\n$\\Delta W \\approx \\sum_{t=1}^{T} A^{(t)} (B^{(t)})^T$\n7) By the properties of low-rank matrix approximation:\n$\\||\\Delta W - \\sum_{t=1}^{T} A^{(t)} (B^{(t)})^T || \\leq \\frac{||\\Delta W||_*}{\\sqrt{rT}}$\nwhere $|| . ||_*$ denotes the nuclear norm.\n8) Assuming the network function is Lipschitz continuous with respect to its weights with Lipschitz\nconstant $L_f$:\n$E_2 \\leq L_f^2 ||\\Delta W - \\sum_{t=1}^{T} A^{(t)} (B^{(t)})^T ||_F \\leq \\frac{L_f^2 ||\\Delta W||_*}{\\sqrt{rT}}$\n9) Combining the bounds for E\u2081 and E\u2082:\n$E_{x \\sim D}[(f_T(x) \u2013 f^*(x))^2] \\leq 2K_1(\\frac{1}{\\sqrt{TM}} + \\frac{1}{\\sqrt{T}}) + \\frac{2L_f^2 ||\\Delta W||_*}{\\sqrt{rT}}$\n$\\leq C_6(\\frac{1}{\\sqrt{TM}} + \\frac{1}{\\sqrt{T}}) + \\frac{1}{\\sqrt{rT}}$\nwhere $C_6 = max(2K_1, 2L_f^2 ||\\Delta W||_*)$.\nThis completes the proof."}, {"title": "D Broader Impact", "content": "The proposed XGBLORA framework has the potential to bring about significant positive societal\nimpacts by democratizing access to state-of-the-art language technologies. By enabling efficient and\neffective fine-tuning of large language models, XGBLORA can empower researchers and practitioners\nwith limited computational resources to leverage the power of pre-trained models for a wide range\nof downstream tasks. This can foster innovation and accelerate progress in various domains, such\nas healthcare, education, and social sciences, where natural language understanding and generation\ncan be applied to improve decision-making, personalize learning experiences, and analyze large-\nscale social data. However, it is crucial to acknowledge and mitigate potential negative societal"}, {"title": "E Limitations", "content": "One limitation of our current approach is that our theoretical analysis is based on linear models, which\nmay influence the generalizability of our findings to more complex, non-linear systems. Additionally,\nthe assumptions made in our theoretical framework may not hold in certain real-world scenarios,\npotentially limiting the applicability of our method in such cases. Future work will focus on extending\nour theory to encompass more generalized forms, allowing for a broader range of applications and\nimproved robustness to model misspecification."}]}