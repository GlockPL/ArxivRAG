{"title": "CG-MER: A Card Game-based Multimodal dataset for Emotion\nRecognition", "authors": ["Nessrine Farhat", "Amine Bohi", "Leila Ben Letaifa", "Rim Slama"], "abstract": "The field of affective computing has seen significant advancements in exploring the relationship between emotions\nand emerging technologies. This paper presents a novel and valuable contribution to this field with the introduction of a\ncomprehensive French multimodal dataset designed specifically for emotion recognition. The dataset encompasses three\nprimary modalities: facial expressions, speech, and gestures, providing a holistic perspective on emotions. Moreover, the\ndataset has the potential to incorporate additional modalities, such as Natural Language Processing (NLP) to expand the\nscope of emotion recognition research. The dataset was curated through engaging participants in card game sessions,\nwhere they were prompted to express a range of emotions while responding to diverse questions. The study included 10\nsessions with 20 participants (9 females and 11 males). The dataset serves as a valuable resource for furthering research\nin emotion recognition and provides an avenue for exploring the intricate connections between human emotions and\ndigital technologies.", "sections": [{"title": "1. INTRODUCTION", "content": "Emotions profoundly impact our daily experiences and decision-making processes. Recognizing and understanding\nemotions is crucial for improving interpersonal interactions and has diverse applications in fields such as human-\ncomputer interaction [1], mental healthcare [2], market research [3], and education [4]. These applications demonstrate\nthe broad impact of emotion recognition in advancing various communities, including artificial intelligence, computer\nvision, speech processing, human sciences, and psychology.\n\nThe development and evaluation of emotion recognition models rely on the availability of diverse and high-quality\nunimodal datasets. In the realm of facial emotion recognition, widely used datasets include AffectNet [5] and Fer2013\n[6]. For speech emotion recognition, datasets such as RAVDESS [7] and EmoDB [8] have been utilized. Moreover, we\nidentified the ChaLearn Gesture Challenge (CGC) [9] dataset for gesture-based emotion recognition. These unimodal\ndatasets have significantly contributed to advancing emotion recognition models within their respective modalities.\nHowever, it is important to acknowledge that unimodal datasets may not cover the complete spectrum of emotions\nexpressed through multiple modalities [10]. To overcome this limitation, using multimodal datasets has become crucial,\nas they provide a comprehensive understanding of emotions. These datasets combine audio, visual, physiological signals,\nand other modalities, offering a holistic representation and allowing for a more in-depth exploration of emotional cues.\n\nIn this paper, we introduce CG-MER, a Card Game-based Multimodal dataset for Emotion Recognition, constructed\nduring sessions involving an emotional card game. Our primary contribution lies in the introduction of a new benchmark\naccompanied by multimodal annotations. Furthermore, we present an original protocol centered on the emotion game,\nwherein participants engage in spontaneous and convivial conversations, expressing various emotions while responding\nto questions that vary in intensity.\n\nThis paper is organized into three main sections. Section 2 delves into the related work, where we explore human\nemotion recognition, covering both monomodal and multimodal approaches. In Section 3, we present our CG-MER\ndataset, providing comprehensive details on its setup, experimental procedure, data description, and annotation process.\nFinally, Section 4 concludes the paper, summarizing our findings and discussing the perspectives."}, {"title": "2. RELATED WORK", "content": "In this section, we will provide a concise overview of recent studies concerning the identification of human emotions.\nOur primary focus will be on the utilization of multimodal datasets, which have played a vital role in evaluating the\neffectiveness of deep learning models in the field of emotion recognition.\n\n2.1. Human emotion recognition\n\nEmotion recognition is a prominent research area with potential impacts on understanding human behavior and\nenhancing human-computer interactions. It involves multiple modalities such as facial expressions, speech, gestures, and\nphysiological signals. In the domain of facial emotion recognition, extensive surveys have been conducted, providing\ncomprehensive overviews of techniques and advancements. Notably, Zhang et al. [11] conducted an in-depth survey that\nexplored various facial expression recognition techniques. Speech emotion recognition has also been extensively\ninvestigated, with surveys like the one conducted by Abbaschian et al. [12] delving into feature extraction, classification\ntechniques, and relevant databases, offering valuable insights into the domain. Furthermore, Khalil et al. [13] provided\nan overview of deep learning techniques and their applications in speech-based emotion recognition. Gesture-based\nemotion recognition has also received considerable attention, and surveys, such as the one conducted by Noroozi et al.\n[14], have provided valuable insights into this modality. With the advent of multimodal approaches, surveys focusing on\nfusion techniques and challenges have emerged as well. For instance, Koromilas et al. [15] reviewed the state-of-the-art\nmultimodal speech emotion recognition methodologies, with a particular emphasis on integrating audio, text, and visual\ninformation, another survey [16] has presented the basic research in the field and the recent advances into the emotion\nrecognition from facial, voice, and physiological signals, where the different modalities are treated independently. These\nsurveys contribute significant insights into the advancements and potential of multimodal emotion recognition,\nhighlighting the benefits of leveraging multiple modalities to enhance accuracy and robustness in emotion recognition\nmodels.\n\n2.2. Emotion recognition datasets\n\nThis subsection provides an overview of well-known multimodal datasets that have been widely utilized for\nmultimodal emotion recognition in the literature.\n\nIn [17], the authors introduced the K-EmoCon dataset, which encompasses videos, vocal audio, and biosignals\nrecorded during debates involving 32 individuals. This dataset covers 5 emotional classes. Another notable dataset,\nAMIGOS [18], focuses on group affect, personality, and mood. It comprises facial expressions, audio recordings, and\nphysiological responses collected from 40 participants across 7 emotional classes. LUMED [19], a multimodal dataset\ncapturing simultaneous audiovisual data from 13 participants. The dataset includes selected web-based video clips and\ncovers 3 basic emotional classes. The MELD dataset was presented in [20], featuring dialogue excerpts from the sitcom\nFriends along with textual, audio, and video recordings. This dataset involves 7 participants and spans 7 emotional\nclasses. IEMOCAP [21] introduced by Busso et al., offers audiovisual and motion data from dyadic sessions involving\n10 actors with 5 emotional classes. Additionally, the EMOFBVP dataset, outlined in [22], captures the expressions of 10\nactors through facial and vocal expressions, body gestures, and physiological signals, covering 23 different emotional\nclasses. In addition, the MSP-IMPROV dataset, presented by Caridakis et al. [23], that serves to study non-verbal\nbehavior across different cultures specifically, German, Greek, and Italian. The dataset comprises 51 participants who\nexpress emotions through gestures and speech while reading Velten sentences. Recently, a large audio-visual dataset\ndenoted Empathic [24] has been developed. This dataset involves 250 speakers from three European countries namely\nSpain, France and Norway.\n\nAlthough the reviewed datasets have contributed significantly to the field of emotion recognition, they also exhibit\ncertain limitations that warrant further improvements. Some datasets may lack enough participants to train deep learning\nmodels effectively, while others might not contain all the necessary modalities with annotated data for robust model\ntraining. Additionally, limited classes in some datasets might not cover the full spectrum of emotional states.\nFurthermore, the absence of a suitable French dataset with adequate simplicity and favorable conditions for usage is also\na concern."}, {"title": "3. THE PROPOSED DATASET: CG-MER", "content": "The challenge with existing emotion datasets lies in their limited applicability to real-world contexts, due to induced\nemotions in controlled environments. Unlike typical datasets that use specific stimuli, CG-MER dataset employs an\nemotional card game to explore genuine emotions between participants. Its goals are to investigate correlations between\nemotional stimuli and expressions, understand emotions in social contexts, and capture dynamic emotional experiences.\nThe dataset adheres to ethical standards approved by GDPR (General Data Protection Regulation) in France, with\nparticipants providing written consent after reviewing detailed information about data collection, privacy, and data\ndeletion options.\n\n3.1. Data collection setup\n\nThe dataset comprises 10 sessions involving 20 participants, including 9 females and 11 males. The age range of the\nparticipants spans from 20 to 43 years old, ensuring a diverse representation within the dataset. To introduce the study\nand emphasize the importance of constructing a multimodal dataset for emotion recognition, we prepared an engaging\nanimation campaign. This campaign served as an introduction, presenting the research topic and emphasizing the\nsignificance of developing a multimodal dataset for emotion recognition. To facilitate participant involvement, a\nstructured protocol was implemented. Participants were asked to complete a form, allowing them to choose their\npreferred partner and select their desired participation time slot. A pre-arranged schedule facilitated the organization and\ncoordination of the participant's involvement in the study. All data collection sessions took place in a dedicated room\nwith the same temperature and illumination conditions. To ensure optimal data capture, two participants were seated\nacross a table, maintaining a comfortable distance between them to facilitate communication. To capture facial\nexpressions and movements, two Kinect cameras V1 were positioned at the center of the table, facing each participant.\n\nTo configure the cameras, we installed KinectSDK-v1.8, which included the Kinect driver and runtime for the\nWindows Software Development Kit (SDK). This SDK enabled developers to create applications that support face,\ngesture, and voice recognition utilizing the Kinect sensor technology. Additionally, we utilized the Kinect for Windows\nDeveloper Toolkit, which provided valuable resources such as source code samples and tools, streamlining the\ndevelopment of Kinect for Windows applications. During the data collection sessions, we utilized OBS Studio (Version:\n29.1.1)\u00b9 to capture both color and depth videos for each participant, allowing us to capture facial expressions and\ngestures accurately. For audio capture, we employed the Audio Capture Row-Console application available in the Kinect\ndeveloper kit for each participant.\n\n3.2. Data collection procedure\n\nThe data collection procedure consisted of three steps to ensure a systematic and organized process. Two\nexperimenters administered each session to configure the cameras and software, to explain the game to participants, and\nto manage the record during the session.\n\nStep 1: Consent and data usage agreement\n\nUpon arrival, each participant was provided with a consent form outlining the purpose of the study and the use of\ntheir data, including facial expressions, voice, and gestures, for research purposes. They were given ample time to review\nthe form and provide their written consent for participating in the data collection process.\n\nStep 2: Game explanation and annotation instructions\n\nFollowing the consent process, the participants were briefed about the card game and its rules. They were given a\ndetailed explanation of how the game would unfold and were encouraged to ask any questions for clarification.\nAdditionally, they were introduced to the annotation form, which contained sections for self-annotation and partner\nannotation for the seven predefined emotions (happiness, anger, neutral, fear, sadness, surprise, and disgust). The\nparticipants were guided on how to effectively use the annotation form to record their own emotional experiences and\nobserve their partner's emotions during the card game. Furthermore, the participants were directed to select two cards\nfrom each of the three categories (red, yellow, and green), comprising 30 different questions, with 10 questions in each\ncard category. Each category was associated with a specific color\nand score. The green cards, indicated by a score of +1, represented questions with lower intensity and less personal\nnature. These cards contained simpler and non-personal questions. The yellow cards, marked with a score of +3, had a\nmoderate intensity level and included questions that were slightly more personal. Lastly, the red cards, carrying a score\nof +5, had the highest intensity and featured questions of a more personal nature. With their selected cards in hand,\nencompassing a variety of colors and corresponding question types, the participants were prepared to commence the\ngame.\n\nStep 3: Gameplay and emotion annotation\n\nDuring this step, the participants actively engaged in the card game, creating a convivial and spontaneous\natmosphere. Unlike a mechanized question-and-answer process, participants had the opportunity to ask additional\nquestions to gain further insights into the given situation. After responding to each question, both participants were\nencouraged to annotate their own emotional states and provide observations regarding their partner's emotions using the\nprovided annotation form. This interactive and dynamic process enabled the comprehensive collection of data on self-\nperceived emotions as well as the perceived emotions of their partner throughout the gameplay.\n\nBy following these three steps, we ensured a structured and standardized procedure for data collection during the\ncard game sessions. This approach facilitated the systematic capture of multimodal data and enriched our dataset for\nfurther analysis and research on emotion recognition.\n\n3.3. Dataset description\n\nThe CG-MER dataset comprises a comprehensive collection of multimodal data obtained from 10 distinct sessions\ninvolving an emotional card game, resulting in a total duration of approximately 10 hours of dyadic interactions. Each\nsession was recorded in the form of two audiovisual recordings, saved as mp4 files. These recordings encompassed the\nRGB video and depth video of each participant, enabling a detailed analysis of facial expressions and body movements.\nTo ensure focused analysis, the videos were carefully cropped using Adobe Premium software to extract the RGB and\ndepth videos as separate entities. Furthermore, continuous annotations of emotions were collected from three distinct\nperspectives of the subject, the partner, and the external observers. This multimodal approach allowed for a\ncomprehensive understanding of the emotional dynamics at play during card game interactions. In addition to the visual\ndata, audio tracks capturing the participants' speeches were recorded, then saved as individual WAV files. This audio\ncomponent provides valuable insights into the verbal expressions and vocal cues associated with different emotional\nstates. To provide an overview of the dataset's contents and data collection outcomes, the key\ndetails, including the number of sessions, duration, and the various data modalities captured.\n\n3.4. Data annotation\n\nPrior studies have demonstrated that incorporating multiple sources for emotion annotations, as evidenced by [27,\n28], can significantly enhance the accuracy of emotion recognition systems. The use of multiple perspectives, including\nthose of the subject, interacting partner, and external observers, is essential for establishing a comprehensive ground\ntruth in emotion annotations. With the CG-MER dataset, we aim to address these complexities by embracing all three\navailable perspectives in the annotation of emotions within the context of social interactions.\n\nSelf and Partner Annotation: During the interactive gameplay sessions of the emotional card game, participants\nwere involved in self and partner annotation. After responding to each question, both participants were given\napproximately 1 minute to annotate their own emotional experiences and those of their partner. They utilized a\npredefined table containing the 7 basic emotions, allowing them to express their feelings and observations in real-time.\nThe self-annotation provided insights into the subject's immediate emotional responses, while the partner annotation\ncaptured the perceived emotions experienced by the interacting partner during the dyadic interactions.\n\nExternal Observers Annotation: Three external raters were recruited for this purpose. Using \"label-studio\", the\nopen-source data labeling tool. For the external observers' annotation, we adopted a data-driven approach that relies on\nboth the self and partner annotations provided during the game sessions. These annotations served as the basis for\nexternal observers to assess and annotate the participants' emotions.\n\nBy leveraging the combination of self and partner annotations along with the impartial observations of external\nraters, we aimed to capture a more holistic and robust representation of the participants' emotional experiences.\npresents examples of an external observer's annotation for some sessions, while illustrates the emotion\ndistribution across participants in selected sessions.\n\n3.5. Final dataset\n\nThe final dataset encompasses a rich collection of multimodal data from 10 distinct sessions of emotional card\ngames. These sessions capture genuine and spontaneous interactions between participants, offering valuable insights into\nemotional expressions and perception during social interactions. The CG-MER dataset is organized into ten folders, each\ncorresponding to a specific session of the emotional card game. Within each session folder, there are ten individual files,\neach representing a participant's data. For each participant, the dataset includes the following data files:\n\nRGB Video (mp4): A video file capturing the participant's facial expressions and gestures during the game session.\n\nDepth Video (mp4): A video file providing in-depth information of the participant's body movements.\n\nAudio (wav): An audio file capturing the participant's speech and vocal expressions.\n\nSkeleton Video (mp4): A video file visualizing the skeletal movements of the participant during the game session.\n\nExternal Observers' Annotations (json, csv): Two separate files in JSON and CSV formats containing the annotations\nprovided by external observers who rated the participants' emotional states during the interactions.\n\nThe dataset's multimodal nature enables researchers to delve into various aspects of emotion recognition,\nincorporating facial expressions, vocal cues, and gestures to gain a comprehensive understanding of emotional dynamics\nduring social interactions."}, {"title": "4. CONCLUSION", "content": "In conclusion, the CG-MER dataset presents a valuable resource for emotion recognition research among French-\nspeaking individuals. While not publicly available at this time, its potential impact on affective computing is promising,\nfostering interdisciplinary research and advancing our understanding of human emotions and social interactions in\ndiverse cultural contexts. In future work, a primary focus of our research will be to investigate the acquisition of\nadditional data from individuals with neurodegenerative diseases to augment the second version of the CG-MER dataset.\nBy collecting data in this specific context, we aim to gain deeper insights into the emotional responses and social\ninteractions of individuals affected by neurodegenerative disorders."}]}