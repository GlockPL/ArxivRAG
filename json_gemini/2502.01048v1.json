{"title": "Sparks of Explainability Recent Advancements in Explaining Large Vision Models", "authors": ["Thomas Fel"], "abstract": "This doctoral thesis aims to advance the state of the art and the development of tools in the field of explainability in computer vision. It specifically focuses on creating a set of tools designed to enhance our understanding of the features utilized by deep neural networks currently employed in vision tasks. Explainability represents a key area for improving interactions between humans and artificial intelligence systems, as well as from a scientific standpoint to decipher a new type of intelligence: artificial intelligence. More concretely, the complexity and lack of transparency of these models pose a major obstacle to their adoption in critical systems and raise crucial questions, potentially capable of leading to significant advances in our understanding of intelligence, provided their mechanisms can be deciphered. Through this manuscript, we explore several explainability methods, each contributing to the understanding and improvement of the explainability of vision models while acknowledging their respective limitations. We begin this thesis with a detailed analysis of attribution methods, also known as saliency maps or heat maps. These techniques reveal where the model focuses its attention to make a decision, through the use of heatmaps. The first paper proposes a metric inspired by algorithmic stability that is based on these attributions to assess the quality of explanations provided by the models, thus identifying those offering the best explanations. We then introduce a new attribution method inspired by the field of Global Sensitivity Analysis based on Sobol indices. This black-box approach, supported by a solid theoretical foundation, allows for halving the computation time compared to the state of the art through the use of quasi-Monte Carlo sequences. We continue by presenting the first attribution method with formal guarantees, EVA, which relies on verified perturbation analysis. Next, we examine the second hypothesis, according to which current attribution methods are insufficient because they reveal only where the model focuses its attention, without specifying what it perceives. We adopt an explainability approach based on concepts, moving from focusing on the \"where\" to understanding the \"what\" perceived by the model. This transition is materialized by the CRAFT method, which automates the extraction of concepts used by a model and then assesses the importance of each extracted concept. We thoroughly analyze the components of current concept extraction methods and demonstrate that they include two phases: an extraction phase and an importance estimation phase. We then unify the different approaches in the literature by showing that the extraction phase can be conceptualized as a dictionary learning problem, and that the importance estimation phase implicitly uses attribution methods. After establishing this unifying framework, we introduce MACO, a feature visualization method that we apply to concepts, allowing the visualization of extracted concepts. We conclude by integrating these different methods into an interactive demonstration, offering exploration and understanding of the most important concepts for the 1000 ImageNet classes of a ResNet model. The thesis concludes with a thorough reflection on the developed methods, the progress made, and the challenges encountered, opening perspectives on future research directions in AI explainability. We emphasize the importance of continuing the search for synergies between the different methods studied, as well as promising avenues for fully leveraging the potential of explainable AI.", "sections": [{"title": "General Introduction", "content": "Some mysteries are meant to remain unsolved; Deep Learning is not one of them. At the entrance of this document, before exploring the subject of Deep Learning\u2014where the silence of our understanding contrasts sharply with the powerful noise of its achievements\u2014let us take a moment to reflect on the origins of \u0391\u0399.\nThe concept of creating thinking machines emerged during the Dartmouth Workshop in 1956 McCarthy et al. [1956]\u00b9, a seminal event that marked the inception of Artificial Intelligence (AI) as a formal research discipline. This gathering laid the foundational stones for exploring the potential of developing machines endowed with intelligent capabilities. Since then, AI has traversed through various evolutionary stages, characterized by alternating waves of enthusiasm spurred by significant breakthroughs. Alan Turing's prediction regarding the ascent of artificial intelligence appears to have been prophetic, as evidenced by the successive emergence of Machine Learning and subsequently Deep Learning, two branches of statistical learning.\nApproximately a decade ago, AI witnessed a transformation with the advent of Deep Learning (DL) LeCun et al. [2015]; Serre [2019b]. Deep Learning methodologies, rooted in deep neural networks, have catalyzed revolutionary advancements across diverse domains by showcasing exceptional aptitude in discerning complex patterns and behaviors from large datasets. The surge in Deep Learning's prominence can be attributed to several factors, including the exponential increase of data, advancements in hardware and software for machine learning, and pivotal breakthroughs in research methodologies.\nA pivotal moment in the adoption of Deep Learning occurred in 2012, when\n\u00b9Interestingly, Lloyd Shapley, who we will encounter later in this manuscript, was invited and is already mentioned in the workshop proposal."}, {"title": "Deep Learning Background", "content": "In this section, we will briefly revisit the framework of this study, namely deep neural networks. To do so, we will briefly review the statistical learning framework in which we operate, and then we will explore the different components of neural network architectures for vision tasks. For a more comprehensive background, we encourage the reader to refer to Goodfellow et al. [2016]."}, {"title": "Statistical Learning", "content": "Deep learning methodologies are firmly rooted in the principles of statistical learning theory Vapnik [1999]. Consider measurable spaces $X \\subset \\mathbb{R}^d$ and $Y \\in \\mathbb{R}$, representing the input and output spaces respectively\u00b2. In supervised learning, we are presented with a dataset of labeled instances:\n$\\mathcal{D} = \\{ (x_1, y_1), ..., (x_n, y_n) \\} \\in (X \\times Y)^n$.\nOur objective in this context is to ascertain the best approximation $f$, the stochastic relationship between input $x \\in X$ and its corresponding label $y \\in Y$, which is\n\u00b2All topological spaces are equipped with their Borel \u03c3-algebra"}, {"title": "Neural Networks", "content": "In this section, we revisit the core components of deep learning: neural networks, emphasizing their parameterization and the pivotal role of convolution operations, particularly for image data. Neural networks, parameterized by weights and biases collectively denoted as $\\theta$, are foundational to deep learning's success in various domains.\nDefinition 1.1.4 (Neuron). A neuron is a function $\\eta : \\mathbb{R}^d \\rightarrow \\mathbb{R}$, parameterized by"}, {"title": "Explainability Landscape", "content": "The objective of this section is twofold: firstly, to underscore the imperative of explainability within machine learning, and secondly, to delineate a concise overview of the diverse methodologies underpinning explainability \u2013 to say it otherwise, to \"flag\" the existing sub-fields. We aim to acquaint the reader with pivotal terms and explainability methods discussed throughout this manuscript. To do so, we propose a taxonomy categorizing explainability methods into three dimensions: methods that explain individual predictions, those studying the model internal mechanics, and those interpreting the data's influence. This classification, albeit simplistic, facilitates a structured introduction to the landscape of explainability."}, {"title": "Motivation", "content": "Tracing the origins of explainability in AI is akin to exploring the very essence of science, as the pursuit of explanations, particularly within the realm of scientific thought, has historically been a foundational pillar, as highlighted by Hospers [1946], suggesting that the impetus for explanation is deeply rooted in the fabric of scientific discourse itself. A closer intellectual lineage to modern explainability could be found back over half a century, finding ground in the domain of mathematical logic Hempel and Oppenheim [1948]. However, it was not until the advent of deep learning, that the modern conceptualization of explainability as it is addressed within this manuscript \u2013 emerged. Unsurprisingly, it is deep learning that has been the catalyst for the establishment of this burgeoning research field, and it is important to delineate the goal of XAI as well as its expected outcomes prior to examining the existing body of work.\nIt would typically be prudent to start with a definition; however, the quest for a formal definition of explainability is unlikely to be straightforward. Lombrozo [2006] notes that explanations serve as the currency of our belief systems, a medium through which we exchange and interrogate our understanding of the world. This discourse raises fundamental questions about the nature of explanations and the criteria that distinguish more effective explanations from their less compelling counterparts. The academic community has variously characterized explanations as embodying a deductive-nomological essence Hempel and Oppenheim [1948], akin to logical proofs, or as mechanisms that provide a deeper understanding of underlying processes, as proposed by Bechtel and Abrahamsen [2005]. Keil [2006] proposed a broader conceptualization, advocating for an understanding of explanations as embodying an implicit explanatory comprehension4.\nGiven the rich literature on this subject, attempting to distill a singular definition that encompasses the entire spectrum of use-cases and motivations within the field would be a Sisyphean task and would take us too far. Therefore, we propose to adopts a pragmatic approach to defining explainable AI (XAI), not through abstract\n\u2074Interestingly, one could interpret the essence of this article from an informational perspective on explainability as an addition of information given a common body of knowledge."}, {"title": "Explaining Predictions", "content": "The development of methods to explain model predictions has been a critical aspect of research, originating with the introduction of attribution methods Zeiler et al. [2011]. These approaches aim to clarify the rationale behind a model's decision, whether it is the classification of an instance, the detection of an object within an image, or the prediction of a value in regression tasks. Attribution methods, which produce a heatmap to represent the importance of each input variable (see Figure 1.2), are among the most widely used due to their straightforward"}, {"title": "Explaining the Model", "content": "Explaining a model involves uncovering the internal mechanics that drive its predictions. This can be approached through various methodologies, each aiming to make the model's operations or internal states more transparent.\nConcept-based Explainability. Recent developments in explainability have underscored the need to go beyond attribution methods Doshi-Velez and Kim [2017]. A flagship of these methods is concept-based explainability Kim et al. [2018], which involves identifying human-understandable concepts within a model. Briefly, this method compares two datasets, one containing the concept of interest and a 'random' dataset used for one-class detection with a linear model. The orthogonal to the decision boundary is called a concept vector, see Figure 1.3. Further methods have"}, {"title": "Explaining through Data", "content": "Understanding model behavior extends to examining the influence of training data on the model's learning and predictions. Influence functions Cook and Weisberg [1980] are a key tool in this domain, enabling the estimation of how the model's parameters or predictions would change if a particular data point were removed\u2075\n\u2075The actual formulation is expressing the difference in the parameter space for an infinitesimal perturbation."}, {"title": "Application: FRSign Dataset", "content": "During this thesis, we propose the exploration and application of our work on the FRSign dataset Harb et al. [2020], a recent railway dataset, as a case study to monitor the progression of work and the development of new tools. The FRSign dataset is introduced and presented, as well as models trained on it, and will be referenced in subsequent chapters, specifically in Chapter Chapter 2 and Chapter Chapter 4."}, {"title": "Introduction to the FRSign Dataset", "content": "The FRSign dataset is a recent open-source dataset, released in Harb et al. [2020], with the aim of pushing advancements in autonomous transportation, particularly in the less-explored area of rail systems. Despite the prevalent focus on datasets tailored for autonomous driving applications in recent years, alternative modes of transportation, such as railways, have not received comparable attention. FRSign tries to address this gap by providing a meticulously collected dataset from various locations across France, focusing exclusively on railway infrastructure. This vision- based dataset is primarily geared towards enhancing the detection and recognition capabilities concerning railway traffic lights, and thus adapted as an application for our work on Explainable AI for vision.\nThe dataset labelisation benefits from detailed, hand-labeled annotations, encompassing over 100,000 images. Each image is labeled over six distinct types of French railway traffic lights, complete with metadata such as acquisition date, time, sensor parameters, and bounding boxes. This dataset was developed collaboratively by IRT SystemX as a part of the TAS (Safe Autonomous Land Transport) project, in conjunction with industry leaders in railway traffic SNCF."}, {"title": "Detailed Dataset Statistics", "content": "The FRSign dataset is voluminous, with a total size of 310GB, comprising 393 sequences that depict the journey of trains from one station to another. Each sequence, captured in video format, can be decomposed into individual images, leading to an average of 469 images per sequence when sampled at one image per second. The distribution of images per sequence exhibits a high degree of variability, with a standard deviation of 469, following an exponential distribution pattern, see Figure 1.5. This distribution features a predominance of sequences with a relatively small image count (ranging from 1 to 10) to sequences with the highest image count, reaching up to 5200 images. It is noteworthy that a significant proportion of sequences contain approximately 1000 images.\nThe sequence composition of the dataset underscores the importance of considering the non-independent and identically distributed (i.i.d.) nature of the images when training machine learning models. To address this, we propose a tailored train/test split strategy. But first, we describe briefly the type of data.\nData Types and Structure At its core, the FRSign dataset comprises tuples of images and labels, with the images representing cropped segments of railway signaling lights at varying resolutions. These cropped images, derived from original footage with a uniform resolution of 1980 \u00d7 1080, vary in size due to the differing dimensions of the region of interest across sequences. This variability introduces a unique challenge in maintaining consistency across the dataset. Each cropped image is associated with one of 6 distinct classes, encompassing various states of railway signals (e.g., red light, yellow-red light). An illustrative sample of these cropped images is provided in 1.6.\nStrategic Train/Test Split Given the sequential nature of the images within the FRSign dataset, employing a traditional random train/test split could potentially introduce significant biases and negatively impact the performance of models"}, {"title": "Models", "content": "We have trained several models that we will use to showcase our explainability methods and track the progression in this manuscript. The primary objective with these models is not necessarily to achieve the highest performance, but rather to observe and compare the benefits of XAI tools. We will now describe the set of models trained on this dataset. For each of these models, a mild form of data augmentation was applied, consisting of left-right flipping, stochastic noise addition with a probability of 0.5 from a Gaussian distribution:\n$\\delta(u) = \\begin{cases}\n\\mathcal{N}(0, I\\sigma^2) \\text{ if } u > 0.5, \\\\\n0 \\qquad \\text{otherwise}.\n\\end{cases}$\nfor $u \\sim \\mathcal{U}([0, 1])$ and $\\sigma = 0.1$. Additionally, we implemented slight cropping augmentation that crops from 0.8 to 1.0 of the original image size and contrast augmentation $c \\sim \\mathcal{U}([0.8, 1.2])$. Overall, we focused on three vision models for our experiments, described as follows:\n\u2022 VGG-16. The first model trained is a modified version of the classic VGG- 16, which accepts input images of size 128 \u00d7 128. This variant of VGG-16 includes Batch Normalization added after each convolutional layer and before ReLU activation functions. The architecture head is a global average pooling layer, omitting the original dual dense layers due to their significant memory consumption without a corresponding increase in performance. The model is trained with a dropout rate of 0.7 and a weight decay of 1e \u2013 5, using AdamW optimizer and cosine annealing scheduling with a warmup over 500 iterations (half an epoch) across a total of 60 epochs. The learning rate varies from a maximum of le-\u00b3 to a minimum of le\u00af5. This model achieved an accuracy of 85% in testing.\n\u2022 ResNet-50. The second model is a ResNet50, configured to accept input images of size 224 \u00d7 224. The architecture adheres to the original ResNet design, the head being a global average pooling layer followed by a linear layer at the top. This model was trained without dropout but with a weight decay of le - 5, using the AdamW optimizer and a cosine annealing schedule"}, {"title": "Outline & Contributions", "content": "This doctoral dissertation is organized to further the field of explainability for computer vision. It delves into a variety of specific methodologies across several chapters, outlined as follows:\nChapter 1: Introduction. This initial section provides a concise introduction to deep learning and explainability, establishing the foundational knowledge necessary for the remainder of this document.\nChapter 2: Attribution Methods. The second chapter is dedicated to attribution methods. It begins by illustrating the feasibility of identifying models that offer superior explanations through the lens of algorithmic stability of its attributions maps. Subsequently, a state-of-the-art black-box attribution method based on Sobol indices and Quasi-Monte Carlo sampling is introduced, which reduces computational costs by a factor of two compared to its predecessors. The discussion progresses to the development of an attribution method, EVA, that offers strong formal guarantees using perturbation verification analysis. The practical applicability of these methods, particularly in real-world scenarios and from a human perspective, is subsequently evaluated. It is found that while attribution methods prove to be sufficient and highly useful for straightforward scenarios, their utility vanish when faced with more intricate situations. The chapter concludes by proposing two hypotheses to address these limitations: (1) the need for models that better align with human reasoning, and (2) the necessity to go beyond current attribution methods. These hypotheses are then investigated in Chapter 3 and Chapter 4, respectively.\nChapter 3: Model Alignment. This chapter explores how explainable AI (XAI) can be employed to align current vision models with human cognition through novel training routines. It highlights a trend towards decreasing alignment between models and human understanding and demonstrates how the proposed routine can counter this trend, even improving accuracy. The chapter concludes by noting an intriguing link between model robustness and explainability, exemplified by 1-Lipschitz models."}, {"title": "Related publications", "content": "This dissertation integrates and builds upon a series of peer-reviewed publications, open-source projects, and contributions to the wider research community. Below, we categorize these works based on their relevance to the core chapters of this thesis and additional contributions that extend beyond the thesis scope."}, {"title": "Foundational Contributions", "content": "This section outlines the peer-reviewed publications that form the backbone of the thesis, organized by the relevant chapters.\nAttributions (Chapter 2)\n\u2022 Thomas Fel, David Vigouroux, Remi Cadene, Thomas Serre (2022). \u201cHow Good is your Explanation? Algorithmic Stability Measures to Assess the Quality of Explanations for Deep Neural Networks\u201d. In: Proceedings of the Winter Conference on Computer Vision (WACV)\n\u2022 Thomas Fel\u2020, Remi Cadene\u2020, Mathieu Chalvidal, Matthieu Cord, David Vigouroux, Thomas Serre, (2021). \u201cLook at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis\u201d. In: Advances in Neural Information Processing Systems (NeurIPS)\n\u2022 Thomas Fel\u2020, Melanie Ducoffe\u2020, David Vigouroux\u2020, Remi Cadene, Mikael Capelle, Claire Nicodeme, Thomas Serre, (2023). \u201cDon't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis\u201d. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n\u2022 Julien Colin\u2020, Thomas Fel\u2020, Remi Cad\u00e8ne, Thomas Serre, (2021). \u201cWhat I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods\u201d. In: Advances in Neural Information Processing Systems (NeurIPS)\n\u2020The symbol denotes equal contributions."}, {"title": "Open Source Contributions", "content": "Throughout my PhD, I have actively contributed to the open-source community, leading to the development and maintenance of several projects, notably Xplique which now count 500+ stars on GitHub and implement more than 50 articles in Explainability and lead to the following publication:\n\u2022 Thomas Fel\u2020, Lucas Hervier\u2020, David Vigouroux, Antonin Poche, Justin Plakoo, Remi Cadene, Mathieu Chalvidal, Julien Colin, Thibaut Boissin, Louis Bethune, Agustin Picard, Claire Nicodeme, Laurent Gardes, Gregory Flandin, Thomas Serre, (2022). \u201cXplique: A Deep Learning Explainability Toolbox\u201d. In: Workshop on Explainable Artificial Intelligence for Computer Vision (CVPR W.)\nin total, I open sourced and mainteaned 5 projects, all of them available on GitHub:\n\u2022 Xplique:, an open source Explainability toolbox implementing more than 50 papers of explainability, with a proper documentation, tutorials and notebooks. Available at https://github.com/deel-ai/xplique or available through pip install xplique.\n\u2022 CRAFT: an open source repo to reproduce our work on Concept based explainability (see Chapter 4), in Tensorflow and Pytorch, with tutoriels. Available at https://github.com/deel-ai/Craft or available through pip install craft-xai."}, {"title": "Extended Contributions", "content": "In addition to my direct thesis work, I have also contributed to several projects that, while not the main focus of this dissertation, address related challenges in the field.\n\u2022 Thomas Felt, Louis Bethune\u2020, Andrew Kyle Lampinen, Thomas Serre, Katherine Hermann, (2024). \u201cUnderstanding Visual Feature Reliance through the Lens of Complexity\u201d. In: Advances in Neural Information Processing Systems (NeurIPS)\n\u2022 Katherine L. Hermann, Hossein Mobahi, Thomas Fel, Michael C. Mozer, (2024). \u201cOn the Foundations of Shortcut Learning\u201d. In: Proceedings of the International Conference on Learning Representations (ICLR)\n\u2022 Paul Novello, Thomas Fel, David Vigouroux, (2022). \u201cMaking Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure\u201d. In: Advances in Neural Information Processing Systems (NeurIPS)\n\u2022 Victor Boutin, Thomas Fel, Lakshya Singhal, Rishav Mukherji, Akash Nagaraj, Julien Colin, Thomas Serre, (2023). \u201cDiffusion Models as Artists: Are we Closing the Gap between Humans and Machines?\u201d. In: Proceedings of the International Conference on Machine Learning (ICML)\n\u2022 Drew Linsley, Ivan F Rodriguez, Thomas Fel, Michael Arcaro, Saloni Sharma, Margaret Livingstone, Thomas Serre, (2023). \u201cPerformance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex\u201d. In: Advances in Neural Information Processing Systems (NeurIPS).\n\u2022 Sabine Muzellec, Thomas Fel, Victor Boutin, Leo Andeol, Rufin VanRullen, Thomas Serre, (2024). \u201cSaliency strikes back: How filtering out high frequencies improves white-box explanations\u201d. In: Proceedings of the International Conference on Machine Learning (ICML)\n\u2022 Agustin Martin Picard, Lucas Hervier, Thomas Fel, David Vigouroux, (2023). \u201cInfluenci\u00e6: A library for tracing the influence back to the data-points\u201d. In: Proceedings of World Conference on eXplainable Artificial Intelligence (xAI)."}, {"title": "Attributions Methods", "content": null}, {"title": "Overview", "content": "As mentioned in Chapter 1, attribution methods (see Definition 1.2.1) aim to explain a specific prediction of a model. That is, for a topological input space $X \\subset \\mathbb{R}^d$ and $Y \\subset \\mathbb{R}$ an output space, we study a predictor\u00b9 $f : X \\rightarrow \\mathbb{R}$, which is a measurable\u00b2 function map any image $x \\in X$ to a prediction $f(x) \\in Y$. The goal of attribution methods is to explain which variables of \u00e6 are most important in explaining the decision f(x). We will later see that the crux of the matter boils down to defining what we mean by importance. Formally, an attribution method $\\Phi : \\mathcal{F} \\times X \\rightarrow \\mathbb{R}^{|x|}$ is a functional that given a predictor and an input return a real for each variable in the input\u2014we note that in this definition, the score is not necessarily bounded. The higher the score, the more important the variable is considered; the lower the score, the more dispensable the variable may be. In this section, we aim not to exhaustively cover all attribution methods, but to highlight the most popular ones for vision models. We'll begin by examining gradient-based methods, followed by those utilizing internal states, and conclude with black box methods relying solely on forward calls to the models. Subsequently, we will recall the most common automatic metrics used for evaluating attributions explanations.\n\u00b9For brevity, we intentionally omit the parameters $f (x; \u03b8)$ of the predictor.\n\u00b2All topological spaces are still equipped with their Borel \u03c3-algebra.\n\u00b3In the original paper, the authors propose to apply the $l_\\infty$-norm over the channel in case of RGB images."}, {"title": "Gradient-based methods.", "content": "When exploring the importance of variables in a system or model, one of the primary approaches is through local sensitivity methods. These methods offer quantitative techniques for evaluating the impact of infinitesimal changes around the nominal value of an input. By studying how outputs vary with small shifts in inputs, these methods focus on partial derivatives concerning each input parameter. In essence, they allow us to understand how sensitive a system is to infinitesimal alterations in its initial conditions or parameters. In practice, those methods use the auto-differentiation framework and thus assume derivability, $f$ need to be at least of class $C^1$ \u2013 which is not strictly true for ReLU networks Bertoin et al. [2021].\nSaliency. It turns out that one of the first attribution methods for deep neural network, Saliency \u2013 introduced in Simonyan et al. [2013b] \u2013 is a local sensitivity method and is using absolute value of gradient as importance measure. Formally, Saliency (Sa) defined as:\n$\\Phi_{Sa}(f,x) = \\left| \\nabla_x f(x) \\right|$\nIn essence, as $f(\u00b7)$ often represent the logit value for a specific class, indicating which pixels in a small neighborhood need modification to most significantly impact the class score, whether positively or negatively\u00b3.\nGradient-Input. Another close variant is the Gradient-Input (GI) method proposed by Shrikumar et al. [2017]. This method involves element-wise multiplication of the input with the gradient of the target score. Formally:\n$\\Phi_{GI}(f, x) = x \\odot \\nabla_x f(x)."}, {"title": "Internal methods.", "content": "Gradient-based methods rely on the assumption of differentiability, but may not fully exploit the architectural components of models. In contrast, the upcoming methods we will discuss leverage these structural components to provide more faithful explanations."}, {"title": "Black-box methods.", "content": "The final section explores black-box methods, which exclusively rely on model forward passes and manipulate input perturbations to infer variable importance. These methods, known for their causal influence on the model, often offer straightforward interpretability and mitigate confidence issues inherent in gradient- based approaches Adebayo et al. [2018]; Ghorbani et al. [2017]; Sixt et al. [2020]. However, they typically demand extensive computational resources and exhibit poor scalability. These challenges form the core focus of the work presented in the thesis, Section 2.3, which proposes a novel, efficient, and theoretically sound black-box method based on Sobol indices.\nOne simple way to study model sensitivity through image perturbations is the One-At-a-Time (OAT) method, where each input variable is sequentially modified while keeping others at nominal values. This process observes the resulting effect on the output. OAT typically involves shifting one input variable while keeping others at a nominal value. The nominal value often represents the target image, while perturbations can span an entire space or adhere to a specific baseline state.\nFor instance, the Occlusion method involves setting each variable $x_i$ of the input to a baseline state $x_0$ and measuring the score difference to determine variable importance:\n$\\Phi_i^{oc}(f,x) = f(x) - f(x[x_i=x_0])$"}, {"title": "Metrics", "content": "The initial metrics proposed in the field of explainable AI were based on the concept of plausibility, aligning with the terminology introduced by Jacovi et al. [2021]. These metrics aim to measure the extent to which an attribution-based explanation correlates with a \"ground truth\" explanation, i.e., an ideal representation that precisely indicates the model's rationale. For instance, to explain a model's recognition of a cat, an ideal explanation might be a segmentation map highlighting the cat or the hottest point on a heatmap situated directly on the cat.\nAmong such approaches is the framework proposed by Fong and Vedaldi [2017b], which, along with its associated library, simplifies the measure of plausibility. Other notable mentions include Poerner et al. [2018]; Lundberg and Lee [2017], which provides a benchmark for evaluating explanations against ground truth annotations.\nHowever, these plausibility metrics have been critiqued for a significant limitation: a high plausibility score does not necessarily affirm the explanation method's effectiveness, but rather the quality of the explanation itself. To accurately evaluate explanation methods, the criteria should reflect how well an explanation reveals the true basis of the model's decision-making process, regardless of whether the model's decisions are correct or desirable. For example, if an explanation method uncovers that the model is using grass to identify a cat, it demonstrates the method's accuracy in capturing the model's focus but might be penalized by plausibility metrics. To address this, various fidelity metrics have been developed.\nAs we have described, plausibility metrics introduce a significant issue related to confirmation bias: the fact that an explanation appears coherent and plausible does not necessarily mean it accurately reflects the underlying model processes Adebayo et al. [2018]. A seminal study by Adebayo et al. [2018] demonstrated that certain explainability methods provided similar explanations for both a randomized model and a trained model. This phenomenon, illustrated in Figure 2.2, is problematic as some methods resembles contour detection rather than a meaningful explanation, validating a critical concern: an explanation's coherence does not guarantee its relevance to the prediction's evidential basis.\nThis issue has led to the development of fidelity metrics that we will now describe. They should ensure that the attribution accurately transcribe what is happening within the model, regardless of whether the outcome is aesthetically pleasing or seems plausible.\nDeletion. Petsiuk et al. [2018] The first metric is Deletion, it consists in measuring the drop in the score when the important variables are set to a baseline state. Intuitively, a sharper drop indicates that the explanation method has well-identified the important variables for the decision. The operation is repeated on the whole image until all the pixels are at a baseline state. Formally, at step k, with u the k-most important variables according to an attribution method, the Deletion(k) score is given by:\n$Deletion^{(k)} = f(x[x_i=x_0,i\\in u])$"}, {"title": "Algorithmic Stability to find model with better explanations", "content": "In this section, we aim to develop a new metric for explainability to identify better models. As we've seen in Section 2.1, many attributions methods have been proposed to explain how deep neural networks make decisions, but there hasn't been much effort to ensure that the explanations they provide are objectively relevant. While several desirable properties for trustworthy explanations have been identified, it's been challenging to come up with objective measures for them. Here, we propose two new measures to assess explanations, borrowed from the field of algorithmic stability: mean generalizability (MeGe) and relative consistency (ReCo).\nWe'll begin by briefly reviewing related work on metrics, then we'll introduce our methods and the two metrics. Afterward, we conduct extensive experiments using various network architectures, common explainability methods, and several image datasets to showcase the advantages of these measures. We'll demonstrate that they pass sanity checks, allowing us to move on to the experimental phase where we'll show (1) that current fidelity measures are not sufficient to guarantee algorithmically stable and trustworthy explanations, (2) that our metrics can be use to select the best attribution method for a given model, and finally (3) that our metrics can be use to identify models with better explanations.\nFigure 2.4: The trustworthiness of a predictor's explanations hinges on its algorithmic stability. This concept implies that when an image \u00e6 is excluded from the training dataset D, a separate predictor trained using the same algorithm A (but without x) should yield a comparable explanation for that image. This stability indicates that the explanations are drawn from multiple points, making them more general. For instance, when classifying images as \"pandas,\" the explanations should consistently highlight the dark areas around their eyes, with or without x in the dataset."}, {"title": "Background.", "content": "In this section, we focus on evaluating explanations provided by explainability methods, which give insight into how a given neural network architecture reaches"}]}