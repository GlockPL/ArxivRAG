{"title": "Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification", "authors": ["Ziyu Liu", "Azadeh Alavi", "Minyi Li", "Xiang Zhang"], "abstract": "Self-supervised contrastive learning has become a key technique in deep learning, particularly in time series analysis, due to its ability to learn meaningful representations without explicit supervision. Augmentation is a critical component in contrastive learning, where different augmentations can dramatically impact performance, sometimes influencing accuracy by over 30%. However, the selection of augmentations is predominantly empirical which can be suboptimal, or grid searching that is time-consuming. In this paper, we establish a principled framework for selecting augmentations based on dataset characteristics such as trend and seasonality. Specifically, we construct 12 synthetic datasets incorporating trend, seasonality, and integration weights. We then evaluate the effectiveness of 8 different augmentations across these synthetic datasets, thereby inducing generalizable associations between time series characteristics and augmentation efficiency. Additionally, we evaluated the induced associations across 6 real-world datasets encompassing domains such as activity recognition, disease diagnosis, traffic monitoring, electricity usage, mechanical fault prognosis, and finance. These real-world datasets are diverse, covering a range from 1 to 12 channels, 2 to 10 classes, sequence lengths of 14 to 1280, and data frequencies from 250 Hz to daily intervals. The experimental results show that our proposed trend-seasonality-based augmentation recommendation algorithm can accurately identify the effective augmentations for a given time series dataset, achieving an average Recall@3 of 0.667, outperforming baselines. Our work provides guidance for studies employing contrastive learning in time series analysis, with wide-ranging applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Self-supervised contrastive learning has emerged as a significant topic in deep learning, gaining attention across diverse fields, especially in time series analysis [1]\u2013[4]. Contrastive learning algorithms focus on learning meaningful data rep- resentations by differentiating between similar (positive) and dissimilar (negative) data pairs, thus enabling feature extrac- tion without explicit supervision [5], [6]. This technique is especially relevant in time series analysis because time series data often contain complex temporal dependencies and pat- terns that are difficult to capture with traditional methods [7], [8]. Moreover, it's generally expensive to gain gold-standard labels for time series data, especially in healthcare, traffic, manufacturing, and environmental monitoring.\nHowever, the success of the contrastive learning paradigm significantly depends on the quality and variety of augmenta-"}, {"title": "B. Selection of augmentations in contrastive learning", "content": "Differ from the fields of images and languages where intuitive guidelines can guide the creation of suitable aug- mented samples based on well-understood principles [13], the process of manually selecting augmentations for time series data faces significant obstacles. This challenge arises from the complex and often imperceptible nature of the temporal structures in time series data, which makes it difficult to apply straightforward rules or human intuition effectively [14].\nThe prevalent approach to selecting augmentations in time series analysis typically involves manual strategies, encom- passing both direct and indirect empirical methods. Direct empirical selection entails conducting initial experiments to assess the effectiveness of widely used augmentations (Sec- tion III-B), a process that, despite its thoroughness, proves to be both time-consuming and resource-intensive. On the other hand, the indirect method relies on existing literature, adopting augmentations that are frequently utilized [15]. However, the efficacy of such augmentations can vary significantly across different time series datasets, often rendering literature-based approaches less effective.\nIn this study, we benchmark the effectiveness of augmenta- tions in time series datasets, offering guidance for applying augmentations to diverse datasets. A critical challenge we address is ensuring the applicability of our findings across a broad range of datasets. Our approach creates 12 synthetic datasets, leveraging signal decomposition to quantitatively evaluate the impact of eight augmentations. This process en- ables us to establish a bridge between the patterns of trend and seasonality within a dataset and the effectiveness of specific augmentations. We further validate our findings by applying these insights to six real-world datasets, thereby offering a more generalized and practical framework for augmentation selection in time series analysis."}, {"title": "III. CONTRASTIVE LEARNING AND PRELIMINARIES", "content": null}, {"title": "A. Self-Supervised contrastive learning framework", "content": "Self-supervised contrastive learning learns effective repre- sentations by discriminating similarities and differences be- tween samples. It aims to map similar samples closely while pushing dissimilar ones apart, based on augmented views [16]. Contrastive learning is crucial for various reasons. First, con- trastive learning eliminates the need to label data, relieving the burden of extensive annotation efforts, especially in some expertise-demand areas like healthcare [17], [18]. Additionally, the approach of leveraging the intrinsic structure of data offers powerful and semantically meaningful embeddings that can remain stable and general across different tasks.\nThe framework of self-supervised contrastive learning (Ap- pendix Figure 1) typically includes the pre-training, fine- tuning, and testing stages. In the pre-training stage, a self- supervised encoder takes the positive pairs and negative pairs and learns to project the input samples into embeddings. Based on the learned embeddings, we calculate and minimize the contrastive loss, making positive pairs close to each other and negative pairs far away from each other in the embedding"}, {"title": "B. Augmentations in time series", "content": "In self-supervised contrastive learning for time series, the augmentations can be seen as transformations that slightly alter the original sample to create a contrastive pair (along with the original sample) during the phase of pertaining.\nSuppose a time series sample $x = \\{x_1,x_2,\\ldots,x_{N-1},x_N\\}$ contains $K$ timestamps while there is an observation at each timestamp. This work focuses on univariate time series (i.e., $x_N$ is a scalar), but our experimental design and conclusions can easily extend to multivariate time series (i.e., $x_n$ is a vec- tor). Although there is a wide range of time series augmenta- tions were proposed in the last three years since the emergence of self-supervised contrastive learning framework [5], some are borrowed bluntly from image processing (like rotation) or have limited usage (such as R-peak masking is limited to ECG signals, channel-wise neighboring only applies to multivariate time series). Therefore, in this work, we investigate 8 types of augmentations that are most commonly used in time series studies: jittering, scaling, flipping, permutation, resizing, time masking, and frequency masking. For better intuition, we visualize the augmentations in Appendix Figure 2 (adopted from [9]). The 8 augmentations we investigate in this work are elaborated in Appendix Section XI."}, {"title": "C. Signal decomposition", "content": "Signal decomposition is a fundamental technique in time series analysis, where the goal is to dissect a time series into"}, {"title": "D. Seasonal and Trend decomposition using Loess (STL)", "content": "STL is the most popular and effective method for decom- posing a time series [21]. It employs Loess, a local regression technique, to extract the trend and seasonality components, allowing STL to handle any type of seasonality pattern, not just fixed periodicity. STL's flexibility comes from its non- parametric nature, meaning it does not assume a specific statistical model for the time series data. This makes STL particularly useful for complex data with varying patterns of trend and seasonality.\nIn practice, STL decomposition iteratively fits loess smoother to the observed time series to separate the data into trend, seasonal, and residual components. This iterative approach ensures that the decomposition accurately reflects the underlying patterns of the time series, even in the presence of noise and outliers."}, {"title": "IV. DATASETS", "content": "In this section, we introduce the process of building syn- thetic datasets to derive generalizable guidelines for augmen- tation selection. Inspired by STL decomposition, we construct 12 synthetic datasets by integrating trend, seasonal, and resid- ual components. Additionally, we outline 6 real-world datasets to validate the guidelines learned from the synthetic datasets."}, {"title": "A. Synthetic datasets generation", "content": "In principle, an arbitrary time series signal can be decom- posed into the sum of a trend, a seasonality, and a residual component, as shown in Eq. 1. The difference between the variety of time series datasets is the different functions of trend, seasonality, and residual parts, along with the weights when they integrate together.\nTo increase the generalization of our results, therefore, we establish a series of synthetic datasets to cover as broad as"}, {"title": "Dataset Generation", "content": "Considering 2 trends and 2 seasonali- ties, we generate 4 groups of datasets, naming Dataset groups A, B, C, and D, respectively. For each dataset, we consider three situations:\n*   The trend is dominant (w\u2081 = 0.9, W2 = 0.1).\n*   The trend and seasonality are even (W\u2081 = w2 = 0.5).\n*   The seasonality is dominant (W\u2081 = 0.1, W2 = 0.9).\nFor each synthetic dataset group, we build 3 datasets corre- sponding to the three situations above, denoted by a suffix '1', '2', or '3'. Thus, we have 12 synthetic datasets in total, named A1, A2, and D3. For example, dataset Al is composed of linear trend and trigonometric seasonality, with w\u2081 = 0.9. We provide the workflow in Figure 3. See Table I for more details."}, {"title": "V. AUGMENTATION BENCHMARKING METHODS", "content": "In this section, we introduce how to compare the effective- ness of augmentations in contrastive learning frameworks. We start by identifying two concepts in augmentation: sing-view and double-view augmentations, as shown in Figure 4."}, {"title": "A. Single-view vs. single-view augmentation", "content": "We first evaluate the effectiveness of single-view augmenta- tions. For each of the 8 commonly-used time series augment- ations introduced in Section III-B, we conduct 5 independent training (including pre-training and fine-tuning) in a single"}, {"title": "B. Single-view vs. Double-view augmentation", "content": "SimCLR and the follow-up studies use double-viewed aug- mentation in visual representation learning. However, in time series, single-view and double-view augmentations are both widely used and proven empirically helpful. Here we provide a comprehensive comparison between single-view and double- view augmentation.\nIn each dataset, we select the three most useful augmen- tations based on the results from single-view augmentation benchmarking (Section V-A), then conduct double-view aug- mentations. There are 6 situations in total (select 2 from 3 augmentations with replacement). Note, the top 3 augmenta- tions and consequenced 6 situations are dataset-specific."}, {"title": "VI. TREND-SEASONALITY-BASED RECOMMENDATION", "content": "The overarching aim of this work is to offer guidance on augmentation selection for any given time series dataset, which we refer to as the query dataset. We introduce an innovative trend-seasonality-based method to recommend the most effective augmentations tailored to specific trend and seasonality patterns. Additionally, we propose a popularity- based recommendation as a proxy for augmentation selection from literature and include a random recommendation as a baseline for comparison.\nThis trend-seasonality-based recommendation is the key novelty in this work. It personalizes recommendations to an arbitrary dataset based on the dataset properties. The key idea is to find a 'synthetic twin dataset' for the query dataset, and then recommend the augmentations that are effective in the synthetic twin dataset. The recommendation is conducted following the steps below, as shown in Figure 5."}, {"title": "VII. RESULTS ON CLASSIFICATION", "content": "In this section, we first analyze the results from 12 synthetic datasets by dataset group (Section VII-A), then report the results in terms of trend and seasonality (Section VII-B), fol- lowed by the results from 6 real-world datasets (Section VII-C) for single-view versus single-view augmentation benchmark- ing. Additionally, we present a comparison of single-view and double-view augmentations.\nWe evaluate the performance of time series classification tasks with widely used metrics including accuracy, precision, recall, and F1 score. We mainly report F1 score in the text as it is a comprehensive metric balancing precision and recall. For a dataset, if two augmentations yield F1 scores that are closely comparable within a specified margin, we consider them to be equivalently effective. We define this margin as 0.01 times the performance of no-pretraining."}, {"title": "A. Results of synthetic datasets by dataset groups", "content": "We present the results for the 12 synthetic datasets by categorical groups represented by the prefixes A, B, C, and D. The datasets with the same prefix exhibit consistent trends and seasonality components, although the weights w\u2081 and W2 differ. Due to space constraints in the main manuscript, we provide a detailed discussion exclusively only on the top three ranked augmentations. A comprehensive table containing re- sults on all augmentations can be found in Table IV."}, {"title": "Dataset group A.", "content": "*   We observed that 6, 6, and 5 augmentations respectively outperform the no-pretraining baseline on datasets A1, A2, and A3.\n*   Across all sub-datasets within Group A, resizing consis- tently emerges as the top-1 ranked augmentation, under- scoring its effectiveness in synthetic datasets character- ized by linear trends and trigonometric seasonalities. In addition to resizing, time masking and jittering also prove to be highly effective, securing the second and third ranks.\n*   The best F1 scores are 0.9715, 0.9184, and 0.6085, corresponding to increased margins of 13.05%, 9.31%, and 5.04%, respectively. We observe a decreasing trend in the margin as the seasonality component becomes less dominant (with a lower weight of season), indicating that the effectiveness of resizing is primarily driven by the compounded trigonometric seasonality."}, {"title": "Dataset group B.", "content": "*   In datasets B1 and B2, 8 and 5 augmentations show supe- rior performance compared to the no-pretraining baseline, respectively. However, in dataset B3, no augmentation over the baseline, only permutation and scaling show results close to (but lower than) the no-pretrain scenario.\n*   While Time masking and jittering share the same rank as the top augmentation for datasets B1 and B2, these augmentations did not demonstrate improvement over the no-pretraining baseline in dataset B3. This observation highlights that time masking and jittering augmenta- tions are particularly useful when the trigonometric seasonality predominates in dataset group B.\n*   The best F1 scores are 0.9025, 0.769, and 0.3314, achiev- ing margins of 26.04%, 32.27%, and 0.24%, on datasets B1, B2, and B3, respectively."}, {"title": "Dataset group C.", "content": "*   We observe 5, 6, and 5 augmentations outperform no- pretraining on datasets C1, C2, and C3, respectively.\n*   Resizing is the top augmentation for datasets C1 and C2, while for dataset C3 it occupies the third position. This indicates the effectiveness of resizing weakens within group C as the weight of trigonometric seasonality de- creases and the weight of the non-linear trend increases. This confirms the observation we get from Dataset group A.\n*   For dataset C1, permutation and time-wise neighboring occupy the second and third positions, respectively. In dataset C2, jittering and time masking take the second and third ranks. However, for dataset C3, both time masking and jittering rank the top augmentation.\n*   The highest F1 scores obtained in three datasets are 0.8915, 0.9411, and 0.6217, resulting in increases of 25.55%, 13.13%, and 8.08% respectively compared to each no-pretraining baseline. This observation is con- sistent with the tendency indicating that the effec- tiveness of resizing is associated with the weight of trigonometric seasonality."}, {"title": "Dataset group D.", "content": "*   For datasets D1 and D2, 6 augmentations outperform the no-pretrain baseline, whereas the result of dataset D3 shows no augmentation achieving better performance.\n*   Jittering and time masking emerged as the most effective augmentations, which rank first and second in dataset D1"}, {"title": "B. Results of synthetic datasets by trends and seasonalities", "content": "In this part, we present the findings regarding the trends and seasonalities in the synthetic datasets and their relation to the top augmentations. We first report seasonality then trend.\nSeasonality component. Based on the experimental results from dataset groups with the same seasonality component,"}, {"title": "D. Results of single- vs. double-view augmentations", "content": "In this section, we compare the single-view augmentation method with double-view augmentations. Our goal is to as- sess whether a smaller distance within a positive pair (i.e., augmented view versus original sample) or a larger distance (i.e., augmented view versus another augmented view) is more effective for contrastive learning in time series classification."}, {"title": "Experimental setting", "content": "In practice, for each dataset, we first identify the three best augmentations, as shown in Figure 6 and"}, {"title": "Results", "content": "We report the comparison results in Tables VI-IX. We describe how to interpret the table by taking the first row in Table VI for dataset Al as an example: the no-pretraining (i.e., without any augmentation) F1 is 0.841; the best single- view augmentation boosts the F1 to 0.9715, claiming the increased margin of 13.05%. We use the (Resizing, Resizing) combination for double-view augmentation (both x' and x\" are generated by resizing but with different parameters), the double-view archives the F1 of 0.918, claiming 7.70% margin over the no-pretraining baseline. So we know the single- view augmentation (0.9715) works better than the (Resizing, Resizing) double-view augmentation (0.918).\nIn dataset group A, the results of all datasets show that single-view augmentation can achieve a higher increased margin than double-view augmentation. Both Al and A2 show some improvement with double-view augmentations over no-pretraining baseline, but pairing the top one augmenta- tion with itself does not consistently yield better performance compared to when it is paired with the second or third best augmentation method. For Dataset A3, only the (time masking, jittering) combination results in a slight improvement over the no-pretraining baseline, while other pairs present worse"}, {"title": "VIII. RESULTS ON AUGMENTATION RECOMMENDATION", "content": null}, {"title": "A. Evaluation metrics for Augmentation recommendation", "content": "We evaluate the effectiveness of recommended augmenta- tions by Recall@K which is borrowed from recommendation system studies [23]. Recall@K measures the proportion of relevant items found in the top-K recommendations provided"}, {"title": "B. Recommendation Results", "content": "We compare three recommendation methods: our trend-seasonality-based, popularity-based, and random recommen- dations, as detailed in Section VI. Each method generates a list, RK, of the recommended top K augmentations.\nOn the six real-world datasets, we identify the truly top K augmentations, denoted as Tk, based on the results in Sec- tion VII-C. We then calculate Recall@K using the formula in Eq 12. We report the results in Table X. We provide a concrete example below for better interpretation. Recall@3 = 0.667 =\n2/3 means that: 2 out of 3 recommended augmentations fall within the true 3 best augmentations."}, {"title": "IX. DISCUSSION", "content": "In this work, we constructed 12 synthetic datasets based on signal decomposition rules, gained insights into the effective- ness of augmentations for time series analysis, and validated the conclusions on 6 diverse real-world datasets. We also introduced a trend-seasonality-based method to recommend the most effective augmentation for any given time series dataset. Next, we discuss the limitations of this study and outline several important directions for future research."}, {"title": "More patterns of trends and seasonalities", "content": "In this study, we investigated two synthetic trends and two synthetic season-alities, resulting in four combinations of trends and seasons. However, in real-world scenarios, this may not suffice to cover the high diversity of time series data types. It's important to investigate trend and seasonality patterns to increase robust- ness in terms of data complexity and variability. We propose three specific directions for improvement:\n*   Investigating more complex trend and seasonality compo- nents to cover the diverse patterns in real-world datasets.\n*   Incorporating compound seasonality components by com- bining two or more functions of seasonalities (such as Morlet + Cosine), mirroring the complexity of seasonal patterns encountered in practical scenarios.\n*   Select highly representative real-world datasets as anchor datasets for specific types of time series, akin to the concept of synthetic datasets used in this study. For instance, a large-scale high-quality ECG dataset could serve as the anchor dataset for other small-scale ECG data even though this dataset is monitoring a different disease with the anchor."}, {"title": "X. CONCLUSION", "content": "Through comprehensive analysis of 12 originally estab- lished synthetic datasets and the evaluation of 8 common aug- mentations, we have identified critical associations that guide the selection process. Our trend-seasonality-based recom- mendation system precisely tailors augmentation suggestions to specific dataset characteristics, consistently outperform- ing popularity-based and random recommendations. These findings not only enhance our understanding of contrastive learning dynamics but also pave the way for more effective implementations in practical settings across various industries."}, {"title": "ACKNOWLEDGEMENT", "content": "Z. L. is supported by the Australian Government Research Training Program (RTP) Scholarship at RMIT, Australia. This work is partially supported by the National Science Foundation under Grant No. 2245894. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funders."}]}