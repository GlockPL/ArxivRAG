{"title": "Guidelines for Augmentation Selection in Contrastive Learning for Time Series Classification", "authors": ["Ziyu Liu", "Azadeh Alavi", "Minyi Li", "Xiang Zhang"], "abstract": "Self-supervised contrastive learning has become a key technique in deep learning, particularly in time series analysis, due to its ability to learn meaningful representations without explicit supervision. Augmentation is a critical component in contrastive learning, where different augmentations can dramatically impact performance, sometimes influencing accuracy by over 30%. However, the selection of augmentations is predominantly empirical which can be suboptimal, or grid searching that is time-consuming. In this paper, we establish a principled framework for selecting augmentations based on dataset characteristics such as trend and seasonality. Specifically, we construct 12 synthetic datasets incorporating trend, seasonality, and integration weights. We then evaluate the effectiveness of 8 different augmentations across these synthetic datasets, thereby inducing generalizable associations between time series characteristics and augmentation efficiency. Additionally, we evaluated the induced associations across 6 real-world datasets encompassing domains such as activity recognition, disease diagnosis, traffic monitoring, electricity usage, mechanical fault prognosis, and finance. These real-world datasets are diverse, covering a range from 1 to 12 channels, 2 to 10 classes, sequence lengths of 14 to 1280, and data frequencies from 250 Hz to daily intervals. The experimental results show that our proposed trend-seasonality-based augmentation recommendation algorithm can accurately identify the effective augmentations for a given time series dataset, achieving an average Recall@3 of 0.667, outperforming baselines. Our work provides guidance for studies employing contrastive learning in time series analysis, with wide-ranging applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Self-supervised contrastive learning has emerged as a signif-icant topic in deep learning, gaining attention across diverse fields, especially in time series analysis [1]\u2013[4]. Contrastive learning algorithms focus on learning meaningful data rep-resentations by differentiating between similar (positive) and dissimilar (negative) data pairs, thus enabling feature extrac-tion without explicit supervision [5], [6]. This technique is especially relevant in time series analysis because time series data often contain complex temporal dependencies and pat-terns that are difficult to capture with traditional methods [7], [8]. Moreover, it's generally expensive to gain gold-standard labels for time series data, especially in healthcare, traffic, manufacturing, and environmental monitoring.\nHowever, the success of the contrastive learning paradigm significantly depends on the quality and variety of augmenta-"}, {"title": "B. Selection of augmentations in contrastive learning", "content": "Differ from the fields of images and languages where intuitive guidelines can guide the creation of suitable aug-mented samples based on well-understood principles [13], the process of manually selecting augmentations for time series data faces significant obstacles. This challenge arises from the complex and often imperceptible nature of the temporal structures in time series data, which makes it difficult to apply straightforward rules or human intuition effectively [14].\nThe prevalent approach to selecting augmentations in time series analysis typically involves manual strategies, encom-passing both direct and indirect empirical methods. Direct empirical selection entails conducting initial experiments to assess the effectiveness of widely used augmentations (Sec-tion III-B), a process that, despite its thoroughness, proves to be both time-consuming and resource-intensive. On the other hand, the indirect method relies on existing literature, adopting augmentations that are frequently utilized [15]. However, the efficacy of such augmentations can vary significantly across different time series datasets, often rendering literature-based approaches less effective.\nIn this study, we benchmark the effectiveness of augmenta-tions in time series datasets, offering guidance for applying augmentations to diverse datasets. A critical challenge we address is ensuring the applicability of our findings across a broad range of datasets. Our approach creates 12 synthetic datasets, leveraging signal decomposition to quantitatively evaluate the impact of eight augmentations. This process en-ables us to establish a bridge between the patterns of trend and seasonality within a dataset and the effectiveness of specific augmentations. We further validate our findings by applying these insights to six real-world datasets, thereby offering a more generalized and practical framework for augmentation selection in time series analysis."}, {"title": "III. CONTRASTIVE LEARNING AND PRELIMINARIES", "content": "Self-supervised contrastive learning learns effective repre-sentations by discriminating similarities and differences be-tween samples. It aims to map similar samples closely while pushing dissimilar ones apart, based on augmented views [16]. Contrastive learning is crucial for various reasons. First, con-trastive learning eliminates the need to label data, relieving the burden of extensive annotation efforts, especially in some expertise-demand areas like healthcare [17], [18]. Additionally, the approach of leveraging the intrinsic structure of data offers powerful and semantically meaningful embeddings that can remain stable and general across different tasks.\nThe framework of self-supervised contrastive learning (Ap-pendix Figure 1) typically includes the pre-training, fine-tuning, and testing stages. In the pre-training stage, a self-supervised encoder takes the positive pairs and negative pairs and learns to project the input samples into embeddings. Based on the learned embeddings, we calculate and minimize the contrastive loss, making positive pairs close to each other and negative pairs far away from each other in the embedding"}, {"title": "B. Augmentations in time series", "content": "In self-supervised contrastive learning for time series, the augmentations can be seen as transformations that slightly alter the original sample to create a contrastive pair (along with the original sample) during the phase of pertaining.\nSuppose a time series sample x = {x1,x2,\u2026\u2026,XN\u22121,XN} contains K timestamps while there is an observation at each timestamp. This work focuses on univariate time series (i.e., xN is a scalar), but our experimental design and conclusions can easily extend to multivariate time series (i.e., xn is a vec-tor). Although there is a wide range of time series augmenta-tions were proposed in the last three years since the emergence of self-supervised contrastive learning framework [5], some are borrowed bluntly from image processing (like rotation) or have limited usage (such as R-peak masking is limited to ECG signals, channel-wise neighboring only applies to multivariate time series). Therefore, in this work, we investigate 8 types of augmentations that are most commonly used in time series studies: jittering, scaling, flipping, permutation, resizing, time masking, and frequency masking."}, {"title": "C. Signal decomposition", "content": "Signal decomposition is a fundamental technique in time series analysis, where the goal is to dissect a time series into several interpretable components, typically including trend, seasonality, and residual component [20]. In math, a time series sample x can be decomposed into:\n$$x(t) = w_1T(t) + w_2S(t) + w_3R(t)$$\nwhere t \u2208 {1,2,\u2026\u2026, K} denotes the timestamp. The trend T, seasonality S, and residual component R are functions of timestamp. The w1, w2, and w3 are coefficients to adjust the scale of each component in the whole signal.\nThe trend component reflects the underlying long-term progression of the dataset, showing how the data evolves over time, irrespective of cyclical or irregular patterns. Seasonality shows the pattern that repeats over a known, fixed period, such as daily, weekly, monthly, or quarterly seasonality. Identifying seasonality helps in understanding regular variations in the time series. The residual component, also called noise in some studies, encompasses the random variation in the time series. These irregularities and fluctuations cannot be attributed to the trend or seasonality components.\nThe decomposition allows us to understand the underlying structure of the time series signal, facilitating better represen-tation learning and downstream classification."}, {"title": "D. Seasonal and Trend decomposition using Loess (STL)", "content": "STL is the most popular and effective method for decom-posing a time series [21]. It employs Loess, a local regression technique, to extract the trend and seasonality components, allowing STL to handle any type of seasonality pattern, not just fixed periodicity. STL's flexibility comes from its non-parametric nature, meaning it does not assume a specific statistical model for the time series data. This makes STL particularly useful for complex data with varying patterns of trend and seasonality.\nIn practice, STL decomposition iteratively fits loess smoother to the observed time series to separate the data into trend, seasonal, and residual components. This iterative approach ensures that the decomposition accurately reflects the underlying patterns of the time series, even in the presence of noise and outliers."}, {"title": "IV. DATASETS", "content": "In this section, we introduce the process of building syn-thetic datasets to derive generalizable guidelines for augmen-tation selection. Inspired by STL decomposition, we construct 12 synthetic datasets by integrating trend, seasonal, and resid-ual components. Additionally, we outline 6 real-world datasets to validate the guidelines learned from the synthetic datasets."}, {"title": "A. Synthetic datasets generation", "content": "In principle, an arbitrary time series signal can be decom-posed into the sum of a trend, a seasonality, and a residual component, as shown in Eq. 1. The difference between the variety of time series datasets is the different functions of trend, seasonality, and residual parts, along with the weights when they integrate together.\nTo increase the generalization of our results, therefore, we establish a series of synthetic datasets to cover as broad as time series patterns. To generate synthetic datasets, we need to consider 3 factors:\n1) How to select the functions of trend T(t), seasonality S(t), and residual component R(t)?\n2) How to select the weights w for each function when integrating them into the constructed time series sample?\n3) How to generate different classes in each synthetic dataset as we are working on classification tasks?\nNext, we answer the questions.\nTrends. We investigate a linear trend T1(t) and a non-linear trend T2(t) as two representatives in time series data (Section IX):\n$$T_1(t) = at, T_2(t) = t^2$$\nwhere a is a coefficient to adjust the details of the trend.\nSeasonalities. Any time series signal can be regarded as the combination of a series of trigonometric functions, according to the Fourier series theorem [22]. This theorem states that any periodic signal can be represented as a sum of sines and cosines (trigonometric functions) with different frequencies, amplitudes, and phases. Thus, we employ a typical trigono-metric function as S1(t):\n$$S_1(t) = sin(\\lambda t - \\varphi) + (1 - \\beta)cos(\\lambda t - \\varphi)$$\nwhich is the weighted average of sine and cosine functions with phrase shift, where \u03b2, \u03bb, and \u03c6 are parameters for amplitude, frequency, and phrase, respectively.\nMoreover, the Morlet Wavelet is a popular function for time-frequency analysis, particularly effective for analyzing non-stationary signals. We define S2(t) as:\n$$S_2(t) = \\pi^{-0.25}\\beta^{0.5} te^{-t^2/2}$$\nThis is a common form of simplified Morlet wavelet, which combines a complex exponential (sine wave) with a Gaussian window. In Eq 4, \u03b2 = 2\u03c0fo where fo denotes the central frequency.\nFor the text below, we omit the (t) in trend and seasonality functions for simplicity.\nResiduals components. To simplify matters, we use a standard Normal Gaussian distribution N(0, 1) as the residual component and keep it consistent across all synthetic datasets."}, {"title": "Dataset Generation", "content": "Considering 2 trends and 2 seasonali-ties, we generate 4 groups of datasets, naming Dataset groups A, B, C, and D, respectively. For each dataset, we consider three situations:\n*   The trend is dominant (w1 = 0.9, w2 = 0.1).\n*   The trend and seasonality are even (w1 = w2 = 0.5).\n*   The seasonality is dominant (w1 = 0.1, w2 = 0.9).\nFor each synthetic dataset group, we build 3 datasets corre-sponding to the three situations above, denoted by a suffix '1', '2', or '3'. Thus, we have 12 synthetic datasets in total, named A1, A2, and D3. For example, dataset A1 is composed of linear trend and trigonometric seasonality, with w1 = 0.9. We provide the workflow in Figure 3. See Table I for more details.\nIn this work, we focus on trend and seasonality, so keep w3 = 0.3 constant across all synthetic datasets. We get 0.3 from preliminary studies: a larger residual component will overwhelm the synthetic dataset, making it too noisy; otherwise, a too small w3 will make the datasets too simple, and all augmentations can achieve great performance, leading to less discriminative in benchmarking.\na) Generate classes in each dataset: Within each syn-thetic datasets, we use different coefficients in the trend and seasonality functions to simulate different classes, forming a 6-class classification task. In detail, we have two options for the a in trend T (no matter T1 or T2): a = 0.2 or a = -0.2. For Seasonality, the coefficient in seasonality has a larger effect on signals, for better generalization, we choose three options for the coefficient \u03b2 in S1: \u03b2 = 0.1, \u03b2 = 0.5, or \u03b2 = 0.9. For S2, we use \u03b2 = 4, \u03b2 = 5, or \u03b2 = 6.\nBy considering two options in a and three options in \u03b2, we have 6 classes in each of 12 datasets.\nb) Generate samples in each class: Within each class of each dataset, we need a large amount of samples to train deep learning models. In other words, we need intra-class sample variations. In principle, this variation can be synthesized by changing residual component, or changing the trends and sea-sonalities. While changing the residual component, the instinct properties of samples within a class are still obvious, making the classification performance very high, losing discriminative. Thus, we add perturbations to the trend and seasonality to formulate the sample variations."}, {"title": "B. Real-world datasets", "content": "Based on the results from 12 synthetic datasets, we can draw comprehensive conclusions about the effectiveness of various augmentations across a broad range of time series datasets. These datasets include those with linear or non-linear trends, those with trigonometric or Morlet seasonalities, and those that combine trends and seasonalities with varying weights.\nTo evaluate the correctness of the conclusions summarized on synthetic datasets, we evaluate the conclusions on 6 real-world datasets. The 6 datasets cover different classes (from 2 classes to 10 classes), from single channel to 12 channels, from short series (24) to long series (1280), and from diverse scenarios including human activity recognition, heart disease diagnosis, mechanical faulty detection, traffic pedestrian moni-toring, electronic demand, and financial market prediction. For datasets with unbalanced classes, we employ upsampling by randomly duplicating instances of the minority class until it matches the number of instances in the majority class, thereby balancing the sample amount in different classes in the training set."}, {"title": "V. AUGMENTATION BENCHMARKING METHODS", "content": "In this section, we introduce how to compare the effective-ness of augmentations in contrastive learning frameworks. We start by identifying two concepts in augmentation: sing-view and double-view augmentations, as shown in Figure 4.\nDef 1. Single-view augmentation. For an unlabeled time series sample \u00e6, generated an augmented view \u00e6'. Learns similar embeddings between the augmented view and the original view: the positive pair is (x, x').\nDef 2. Double-view augmentation. For an unlabeled time series sample \u00e6, generated two augmented views \u00e6' and \u00e6\". Learns similar embeddings between two augmented views: the positive pair is (x', x\"). Note the two views could be generated with the same augmentation method (e.g., both using jittering but with different sampling/initialization)."}, {"title": "A. Single-view vs. single-view augmentation", "content": "We first evaluate the effectiveness of single-view augmenta-tions. For each of the 8 commonly-used time series augmen-tations introduced in Section III-B, we conduct 5 independent training (including pre-training and fine-tuning) in a single augmentation setting and report the average performance. Note that in this work, we do not discuss combinational augmentations, such as jittering + flipping."}, {"title": "B. Single-view vs. Double-view augmentation", "content": "SimCLR and the follow-up studies use double-viewed aug-mentation in visual representation learning. However, in time series, single-view and double-view augmentations are both widely used and proven empirically helpful. Here we provide a comprehensive comparison between single-view and double-view augmentation.\nIn each dataset, we select the three most useful augmen-tations based on the results from single-view augmentation benchmarking (Section V-A), then conduct double-view aug-mentations. There are 6 situations in total (select 2 from 3 augmentations with replacement). Note, the top 3 augmenta-tions and consequenced 6 situations are dataset-specific."}, {"title": "VI. TREND-SEASONALITY-BASED RECOMMENDATION", "content": "The overarching aim of this work is to offer guidance on augmentation selection for any given time series dataset, which we refer to as the query dataset. We introduce an innovative trend-seasonality-based method to recommend the most effective augmentations tailored to specific trend and seasonality patterns. Additionally, we propose a popularity-based recommendation as a proxy for augmentation selection from literature and include a random recommendation as a baseline for comparison.\nThis trend-seasonality-based recommendation is the key novelty in this work. It personalizes recommendations to an arbitrary dataset based on the dataset properties. The key idea is to find a 'synthetic twin dataset' for the query dataset, and then recommend the augmentations that are effective in the synthetic twin dataset. The recommendation is conducted following the steps below, as shown in Figure 5.\nStep 1: Decompose the query dataset with STL, producing trend Tq and Sq. The STL is sensitive to period size or the length of each seasonality. We address this issue in two manners: 1) if we can observe clearly seasonality (such as the heartbeat seasonality in ECG) or there exist meaningful seasonality (such as the weekly or bi-weekly cycle in finical time series), we will use the observed meaning seasonality to determine the period; 2) otherwise, we will try several periods separately based on the sampling frequency of the signal, then take the average of similarities.\nStep 2: Calculate the similarity between Tq and T1 and T2, respectively:\n$$Sim^T_1 = d(T_q, T_1)$$\n$$Sim^T_2 = d(T_q, T_2)$$\nwhere d is a similarity function (cosine similarity in this work).\nLikewise, we calculate the similarities in terms of season-ality, between Sq and S1 and S2, respectively:\n$$Sim^S_1 = d(S_q, S_1)$$\n$$Sim^S_2 = d(S_q, S_2)$$\nWhen calculating trend similarity, we constrain the T1 and T2 to have the same length as Tq. When calculating the seasonality similarity, we use a single period instead of the whole signal. For the period used in STL to decompose the query dataset, we adjust the length of S1 and S2 to align with one period of Sq.\nThe calculated similarities for all real-world datasets are in Table III. Based on the similarity calculation, there would be three potential situations:\n1) If Tq is closer to T1, we will make recommendation considering T1.\n2) If Tq is closer to T2, we will make recommendation considering T2.\n3) If Tq is in the middle between T1 and T2, we will not consider the influence of trend (i.e., neither T1 nor T2).\nThe same rules apply to seasonality.\nStep 3: To determine the third case in Step 2, we further define the divergence score DST that is used to describe the preference of Tq on T1 and T2. We calculate DST through\n$$DST = \\frac{2 Max(Sim^T_1, Sim^T_2) - Min(Sim^T_1, Sim^T_2)}{Sim^T_1 + Sim^T_2}$$\nThe numerator denotes the differences between Sim1 and Sim2. The denominator denotes the average between Sim1 and Sim2 (along with the coefficient of 2), serving as a normalization scaler. By design, DST \u2208 [0,2], the smaller DST indicates the query trend is more neutral between T1 and T2. Based on empirical results, we set a threshold of DST as 0.05: we don't consider the effect of the trend (only consider seasonality) if DST < 0.05, because in this case, the query trend is at the middle between Trend 1 and Trend 2.\nStep 4: Determine dataset group. Based on the calculation in Steps 2-3, we identify which trend and seasonality the query dataset is closer to. For example, if the query dataset is close to Trend 1 and Seasonality 2, the query dataset is close to the synthetic dataset group B.\nStep 5: Calculate the combination weights w1 and w2. After identifying the trend and seasonality the query dataset is close to, we calculate the power of trend and seasonality, respectively:\n$$P_T = \\frac{1}{N} \\sum_{i<N}T^2_i$$\n$$P_S = \\frac{1}{N} \\sum_{i<N}S^2_i$$\nwhere N denotes the length of the time series sample and i denotes the timestamp.\nDependent on the relative values of PT and PS, we assign one of the three cases:\n*   If PT/PS < 5/9, we set w1 = 0.1, w2 = 0.9;\n*   If 5/9 \u2264 PT/PS \u2265 5, we set w1 = 0.5, w2 = 0.5;\n*   If PT/PS \u2265 5, we set w1 = 0.9, w2 = 0.1.\nStep 6: Based on the weights w1 and w2, we further narrow down the synthetic twin dataset from dataset group to a specific synthetic dataset. For example, narrowing down from group B to dataset B2 if we got w1 = 0.5, w2 = 0.5. The query dataset and the synthetic twin dataset are similar in terms of the trend, seasonality, and dominant component (taking w1 and w2 as proxy). Thus, we assume the effective augmentations in the synthetic twin dataset should also be effective in the query dataset. Therefore, the augmentation recommendation is simply recommending the top augmentations from the synthetic twin dataset (which we know from the benchmarking experiments).\nTo validate the effectiveness of our recommendation system, we compare it with two baselines: a random recommendation and a popularity-based recommendation. The latter is also developed by us and is calculated using benchmarking results from 12 synthetic datasets, it serves as a substitute for augmentation selection practices commonly found in the literature."}, {"title": "VII. RESULTS ON CLASSIFICATION", "content": "In this section, we first analyze the results from 12 synthetic datasets by dataset group (Section VII-A), then report the results in terms of trend and seasonality (Section VII-B), fol-lowed by the results from 6 real-world datasets (Section VII-C) for single-view versus single-view augmentation benchmark-ing. Additionally, we present a comparison of single-view and double-view augmentations.\nWe evaluate the performance of time series classification tasks with widely used metrics including accuracy, precision, recall, and F1 score. We mainly report F1 score in the text as it is a comprehensive metric balancing precision and recall. For a dataset, if two augmentations yield F1 scores that are closely comparable within a specified margin, we consider them to be equivalently effective. We define this margin as 0.01 times the performance of no-pretraining."}, {"title": "A. Results of synthetic datasets by dataset groups", "content": "We present the results for the 12 synthetic datasets by categorical groups represented by the prefixes A, B, C, and D. The datasets with the same prefix exhibit consistent trends and seasonality components, although the weights w1 and w2 differ. Due to space constraints in the main manuscript, we provide a detailed discussion exclusively only on the top three ranked augmentations. A comprehensive table containing re-sults on all augmentations can be found in Table IV.\nDataset group A.\n*   We observed that 6, 6, and 5 augmentations respectively outperform the no-pretraining baseline on datasets A1, A2, and A3.\n*   Across all sub-datasets within Group A, resizing consis-tently emerges as the top-1 ranked augmentation, under-scoring its effectiveness in synthetic datasets character-ized by linear trends and trigonometric seasonalities. In addition to resizing, time masking and jittering also prove to be highly effective, securing the second and third ranks.\n*   The best F1 scores are 0.9715, 0.9184, and 0.6085, corresponding to increased margins of 13.05%, 9.31%, and 5.40%, respectively. We observe a decreasing trend in the margin as the seasonality component becomes less dominant (with a lower weight of season), indicating that the effectiveness of resizing is primarily driven by the compounded trigonometric seasonality.\nDataset group B.\n*   In datasets B1 and B2, 8 and 5 augmentations show supe-rior performance compared to the no-pretraining baseline, respectively. However, in dataset B3, no augmentation over the baseline, only permutation and scaling show results close to (but lower than) the no-pretrain scenario.\n*   While Time masking and jittering share the same rank as the top augmentation for datasets B1 and B2, these augmentations did not demonstrate improvement over the no-pretraining baseline in dataset B3. This observation highlights that time masking and jittering augmenta-tions are particularly useful when the trigonometric seasonality predominates in dataset group B.\n*   The best F1 scores are 0.9025, 0.769, and 0.3314, achiev-ing margins of 26.04%, 32.27%, and 0.24%, on datasets B1, B2, and B3, respectively.\nDataset group C.\n*   We observe 5, 6, and 5 augmentations outperform no-pretraining on datasets C1, C2, and C3, respectively.\n*   Resizing is the top augmentation for datasets C1 and C2, while for dataset C3 it occupies the third position. This indicates the effectiveness of resizing weakens within group C as the weight of trigonometric seasonality de-creases and the weight of the non-linear trend increases. This confirms the observation we get from Dataset group A.\n*   For dataset C1, permutation and time-wise neighboring occupy the second and third positions, respectively. In dataset C2, jittering and time masking take the second and third ranks. However, for dataset C3, both time masking and jittering rank the top augmentation.\n*   The highest F1 scores obtained in three datasets are 0.8915, 0.9411, and 0.6217, resulting in increases of 25.55%, 13.13%, and 8.08% respectively compared to each no-pretraining baseline. This observation is con-sistent with the tendency indicating that the effec-tiveness of resizing is associated with the weight of trigonometric seasonality.\nDataset group D.\n*   For datasets D1 and D2, 6 augmentations outperform the no-pretrain baseline, whereas the result of dataset D3 shows no augmentation achieving better performance.\n*   Jittering and time masking emerged as the most effective augmentations, which rank first and second in dataset D1 and tied for first in dataset D2, but not in D3. This demon-strates that jittering and time masking augmentations are only effective when the seasonality component plays a predominant or equal role to the trend compo-nent. We reach the same conclusion in Group B, even though it uses trigonometric seasonality, while here Morlet seasonality is employed. In terms of the third-ranked augmentation, frequency masking is for dataset D1, while flipping is for dataset D2.\n*   The best F1 scores are 0.6989, 0.8051, and 0.3585 corresponding to increased margins of 18.27%, 39.71%, and -0.23%, respectively."}, {"title": "B. Results of synthetic datasets by trends and seasonalities", "content": "In this part, we present the findings regarding the trends and seasonalities in the synthetic datasets and their relation to the top augmentations. We first report seasonality then trend.\nSeasonality component. Based on the experimental results from dataset groups with the same seasonality component, we can conclude that in these time series synthetic datasets, seasonalities hold greater importance compared to trends. In other words, seasonality factors play a dominant role in determining the top augmentations and their performance.\nMore detailed observations are shown below:\n*   Both dataset Groups A and C highlight resizing as the top augmentation, emphasizing the critical role of the compound trigonometric seasonality shared by these groups, despite differing in their trend components (linear versus non-linear).\n*   For Groups A and C, increased margins consistently decrease as the influence of compound trigonometric seasonality diminishes from A1 to A2 to A3, with similar observations noted for Group C. These findings correlate with the progressively reduced impact of the compound trigonometric seasonality.\n*   In Groups B and D, which utilize Morlet seasonality, the top three augmentations for sub-datasets with suffix 1 are consistently time masking, jittering, and frequency masking. For sub-datasets with suffix 2, despite differ-"}, {"title": "C. Results of real-world datasets", "content": "Next, we report the experimental results of single augmen-tation benchmarking on real-world datasets. The summarized detailed results can be found in Table V.\nHAR. Time masking and jittering share the top augmentations, with frequency masking occupying the third-highest rank. The increased margin of top-1 augmentation compared to the no-pretraining baseline is 3.36%.\nPTB. Five augmentations have similar performance, with the best augmentation achieving an increase of 1.30% compared to the baseline performance. This dataset is not sensitive to augmentations.\nFD. Resizing, permutation, and time masking rank the top three augmentations, respectively. The resizing achieved an in-creased margin of 21.64%, which is the highest margin across"}, {"title": "D. Results of single- vs. double-view augmentations", "content": "In this section, we compare the single-view augmentation method with double-view augmentations. Our goal is to as-sess whether a smaller distance within a positive pair (i.e., augmented view versus original sample) or a larger distance (i.e., augmented view versus another augmented view) is more effective for contrastive learning in time series classification.\nExperimental setting. In practice, for each dataset, we first identify the three best augmentations, as shown in Figure 6 and Table IV. Let's call them Top-1, Top-2, and Top-3, respec-tively. For single-view augmentation, the best performance is achieved by Top-1. For double-view augmentation, the results are symmetrical for the two views, leading to six possible combinations: (Top-1, Top-1), (Top-1, Top-2), (Top-1, Top-3), (Top-2, Top-2), (Top-2, Top-3), and (Top-3, Top-3). Given that Top-1 outperforms Top-2 and Top-3, we reasonably assume that the (Top-1, Top-1) combination is more effective than (Top-2, Top-2) and (Top-3, Top-3), allowing us to disregard the latter scenarios. In summary, for double-view augmentations, we conduct four combinations: (Top-1, Top-1), (Top-1, Top-2), (Top-1, Top-3), and (Top-2, Top-3).\nResults. We report the comparison results in Tables VI-IX. We describe how to interpret the table by taking the first row in Table VI for dataset A1 as an example: the no-pretraining (i.e., without any augmentation) F1 is 0.841; the best single-view augmentation boosts the F1 to 0.9715, claiming the increased margin of 13.05%. We use the (Resizing, Resizing) combination for double-view augmentation (both x' and x\" are generated by resizing but with different parameters), the double-view archives the F1 of 0.918, claiming 7.70% margin over the no-pretraining baseline. So we know the single-view augmentation (0.9715) works better than the (Resizing, Resizing) double-view augmentation (0.918).\nIn dataset group A, the results of all datasets show that single-view augmentation can achieve a higher increased margin than double-view augmentation. Both A1 and A2 show some improvement with double-view augmentations over no-pretraining baseline, but pairing the top one augmenta-tion with itself does not consistently yield better performance compared to when it is paired with the second or third best augmentation method. For Dataset A3, only the (time masking, jittering) combination results in a slight improvement over the no-pretraining baseline, while other pairs present worse performance. This indicates that double-view augmentations can sometimes lead to poorer results than using each augmentation individually.\nThe results for Dataset Group B follow a similar pattern as observed in Groups A1 and A2 for B1 and B2. For Dataset B3, permutation is the only augmentation that outperforms the no-pretraining baseline, leading us to focus solely on the double-view of (permutation, permutation). This double-view augmentation obtains better performance (0.3391) than its single-view counterpart (0.3314). Although the performance of B3, at approximately 0.33, is not particularly high (likely due to the less discriminative nature of the linear trend across classes), it still surpasses that of random classification, which stands at 0.167 for a 6-class classification.\nIn Dataset C1, the double-view combination of (resizing, time-neighboring) performs equally to the single-view of resiz-ing, with both achieving a margin of 25.55%. For C2, single-view augmentation outperforms all double-view combinations. In the case of C3, the combination of (time-masking, time-masking) shows better performance than the top-ranked single-view. However, in all the datasets, the single-view and the best double-view augmentations perform similarly.\nDataset group D continues to follow the same pattern observed in dataset groups A and B, where single-view aug-mentation outperforms double-view augmentations.\nIn summary, in most cases, single-view provides better performance, while top-to-top dual-view enhancement fails to provide additional performance gains, not even surpassing single-view augmentation in most instances."}, {"title": "VIII. RESULTS ON AUGMENTATION RECOMMENDATION", "content": "Evaluation metrics for Augmentation recommendation\nWe evaluate the effectiveness of recommended augmenta-tions by Recall@K which is borrowed from recommendation system studies [23", "that": "how many among the recommended 3 best augmentations are truly in the 3 best augmentations for the query dataset.\n$$Recall@K = \\frac{|R_K \\cap T_K|}{K}$$\nwhere RK denotes the set of recommended"}]}