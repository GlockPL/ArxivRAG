{"title": "Using Large Language Models for education managements in Vietnamese with low resources", "authors": ["Duc Do Minh", "Vinh Nguyen Van", "Thang Dam Cong"], "abstract": "Large language models (LLMs), such as GPT-4, Gemini 1.5, Claude 3.5 Sonnet, and Llama3, have demonstrated significant advancements in various NLP tasks since the release of ChatGPT in 2022. Despite their success, fine-tuning and deploying LLMs remain computationally expensive, especially in resource-constrained environments. In this paper, we proposed Viet-EduFrame, a framework specifically designed to apply LLMs to educational management tasks in Vietnamese institutions. Our key contribution includes the development of a tailored dataset, derived from student education documents at Hanoi VNU, which addresses the unique challenges faced by educational systems with limited resources. Through extensive experiments, we show that our approach outperforms existing methods in terms of accuracy and efficiency, offering a promising solution for improving educational management in under-resourced environments. While our framework leverages synthetic data to supplement real-world examples, we discuss potential limitations regarding broader applicability and robustness in future implementations.", "sections": [{"title": "Introduction", "content": "Most current tasks in Natural Language Processing (NLP) are dominated by large language models (LLMs) such as GPT4 and Gemini 1.5, which have set new benchmarks for performance. These models excel in a wide range of applications, demonstrating superior capabilities in understanding and generating human language.\n\nIn recent years, artificial intelligence (AI) and machine learning (ML) for education have received a great deal of interest and have been applied in various educational scenarios (Chen et al., 2020), (Xia et al., 2022), (Latif et al., 2023), (Denny et al., 2023), (Li et al., 2024). Educational data mining methods have been widely adopted in different aspects such as cognitive diagnosis (Batool et al., 2022), knowledge tracking (Koedinger et al., 2015), and specifically question answering (Lende and Raghuwanshi, 2016), (Thiruvanantharajah et al., 2021), (Bhowmick et al., 2023).\n\nLarge language models (LLMs) have emerged as a powerful paradigm across different areas (Chen et al., 2023b), (Fan et al., 2023), (Jin et al., 2024), (Zeng et al., 2023), and have achieved state-of-the-art performances in multiple educational scenarios (Kasneci et al., 2023), (Li et al., 2023), (Yan et al., 2023). Existing work has found that LLMs can achieve student-level performance on standardized tests in a variety of subjects, including mathematics, physics, and computer science, on both multiple-choice and free-response problems. A recent study (Susnjak, 2022) reveals that ChatGPT is capable of generating logically consistent answers across disciplines, balancing both depth and breadth. Another quantitative analysis (Malinka et al., 2023) shows that students using ChatGPT (by keeping or refining the results from LLMs as their own answers) perform better than average students in some courses in the field of computer security.\n\nDespite the global advancements, there remains a significant gap in the application of these technologies within the context of Vietnamese education, particularly in educational management. My research is among the first in Vietnam to explore these applications broadly in education and specifically in educational management. Due to the limitations of resources and data within Vietnamese institutions, this area has not yet received adequate attention. This scarcity of local studies and resources has driven us to undertake this research, aiming to bridge the gap and contribute to the growing body of knowledge in this critical field.\n\nIn this study, our main contributions can be summarized as follows:\n\n\u2022 Framework Proposal: We propose a simple yet highly effective framework for applying large language models (LLMs) to educational management tasks. This framework is designed to be easily implementable and adaptable within the constraints of Vietnamese educational institutions. To the best of our knowledge, this first study focuses applying LLMs for education in Vietnamese.\n\n\u2022 New Dataset: We introduce a new dataset specifically tailored for educational management in Vietnam. This dataset addresses the unique challenges and characteristics of the Vietnamese educational context, providing a valuable resource for future research and development.\n\n\u2022 Model Development with Limited Resources: We successfully develop and deploy a model using the limited computational resources available at our institution. This demonstrates the feasibility of implementing advanced AI solutions in resource-constrained environments and provides a blueprint for similar institutions."}, {"title": "Related work", "content": "Providing students with timely learning support has been widely recognized as crucial in improving student engagement and learning efficiency during their independent studies (Dewhurst et al., 2000). Due to the limitation of prior algorithms in generating fixed-form responses, many of the existing study-assisting approaches face poor generalization challenges while being implemented in real-world scenarios (K\u00f6nig et al., 2022). Fortunately, the appearance of LLMs brings revolutionary changes to this field. Using finetuned LLMs (Ouyang et al., 2022) to generate human-like responses, recent studies in LLM-based educational support have demonstrated promising results.\n\nContributing to the large-scale parameter size of LLMs and the enormous sized and diverse web corpus used during the pre-training phase, LLMs have been proven to be a powerful question zero-shot solver to questions spread from a wide spread of subjects, including math (Wu et al., 2023c) (Yuan et al., 2023), law (Bommarito and Katz, 2022) (Cui et al., 2023), medicine (Li'evin et al., 2022) (Thirunavukarasu et al., 2023), finance (Wu et al., 2023b) (Yang et al., 2023), programming (Kazemitabaar et al., 2023) (avelka et al., 2023), language understands(Zhang et al., 2023). In addition, to further improve LLM's problem-solving performance while facing complicated questions, various studies have been actively proposed. For example, (Wei et al., 2022) proposes the Chain-of-Thought (CoT) prompting method, which guides LLMs to solve a challenging problem by decomposing it into simpler sequential steps. Other works exploit the strong in-context learning ability of LLMs and propose advanced few-shot demonstration-selection algorithms to improve LLM's problem-solving performance to general questions. (Chen et al., 2022) and (Gao et al., 2022b) leverage external programming tools to avoid calculation errors introduced during the textual problem-solving process of raw LLMs. (Wu et al., 2023a) regard chat-optimized LLMs as powerful agents and design a multi-agent conversation to solve those complicated questions through a collaborative process."}, {"title": "Education toolkit", "content": "Utilizing a chatbot powered by a Large Language Model (LLM) as an educational tool presents numerous benefits and opportunities. LLM chatbots can tailor their responses to meet the unique needs of each learner, offering personalized feedback and assistance. This ability to customize can cater to various learning styles, speeds, and preferences. They are available 24/7, making learning accessible at any time and from any place, which is especially advantageous for learners in different time zones or with diverse schedules. The interactive features of chatbots can make learning more engaging and enjoyable. They can mimic conversations, set up interactive learning scenarios, and give immediate feedback, which can be more effective than passive learning approaches. Chatbots can manage thousands of inquiries at once, providing a scalable solution for educational institutions to support a large number of students without needing more teaching staff. They can also automate repetitive teaching tasks, such as grading quizzes or offering basic feedback, enabling educators to concentrate on more complex and creative teaching duties. Notable examples of such chatbots include ChatGPT, Bing Chat, Google Bard, Perplexity, and Pi Pi.ai."}, {"title": "Textbook question answering", "content": "Textbook Question Answering (TQA) is a task that requires a system to comprehensively understand the multi-modal information from the textbook curriculum, spreading across text documents, images, and diagrams. The major challenge of textbook question answering is to comprehend the multi-modal domain-specific contexts as well as the questions, and then identify the key information to the questions.\n\nDatasets (Kembhavi et al., 2017) presented the TQA dataset, designed to assess a system that integrates multi-modal contexts and a wide range of scientific topics. Comparable datasets, such as AI2D (Kembhavi et al., 2016), DVQA (Kafle et al., 2018), and VLQA (Sampat et al., 2020), have been developed to facilitate research in multi-modal reasoning within the scientific domain. Nonetheless, these datasets lack annotated explanations for answers in the form of supporting facts. SCIENCEQA (Lu et al., 2022) is a comprehensive textbook question-answering dataset that includes annotated lectures and explanations. This dataset is derived from elementary and high school science curricula, covering a variety of science topics such as natural science, social science, and language science. Recently, the TheoremQA dataset has been released, which includes textbook questions at the university level (Chen et al., 2023a). Beyond the scientific domain, there are datasets focused on the medical field. MEDQA (Jin et al., 2020) and MedMCQA (Pal et al., 2022) are two medical question-answering datasets that encompass a broad range of healthcare topics, derived from both real-world scenarios and simulated exams.\n\nMethods. From a technical perspective, textbook question answering is inherently similar to visual question answering (VQA) (Dosovitskiy et al., 2020), (Gao et al., 2018), (Gao et al., 2022a). Traditional VQA approaches use RNNs to encode the question and CNNs to encode the image (Agrawal et al., 2015), (Malinowski et al., 2015). The multi-modal information is then fused to understand the questions. Additionally, other methods that utilize spatial attention (Lu et al., 2016), (Noh and Han, 2016), (Xu et al., 2015), (Yang et al., 2015), compositional strategies (Andreas et al., 2016), and bilinear pooling schemes (Fukui et al., 2016), (Liu et al., 2022) have been proposed to enhance VQA performance.\n\nWhile VQA and textbook question answering share significant similarities, textbook question answering requires domain-specific knowledge for the accompanying context and innovative integration of diagrams and tables. To address this gap, (Ram et al., 2021) proposed a pre-training schema tailored for question answering. Specifically, their method improves performance in textbook question answering by masking recurring span selections and selecting the correct span in the passage, even when only a hundred examples are available in specific domains. An adversarial training framework is also adapted for domain generalization (Lee et al., 2019), enabling question-answering models to learn domain-invariant features. (Xu et al., 2022) introduced a novel Pre-trained Machine Reader as an enhancement of pre-trained Masked Language Models (MLMs), which addresses the discrepancy between model pre-training and downstream fine-tuning for specific domain MLMs. To comprehend diagrams and tables, graph-based parsing methods have been developed to extract concepts from diagrams (Kembhavi et al., 2016) by converting a diagram into a diagram parse graph. Optical Character Recognition (OCR) is employed to identify chart-specific answers from the charts, which are then aligned with the questions (Poco and Heer, 2017), (Kafle et al., 2018).\n\nOur research is different from previous works in some significant ways:\n\n\u2022 First, we have developed a simple yet effective framework for the textbook question-answering problem. This framework has proven to be both efficient and robust, delivering high performance within a short development cycle.\n\n\u2022 Second, leveraging this framework, we have created a dedicated dataset specifically tailored for the training management process at the Vietnam National University of Hanoi. This dataset is instrumental in enhancing the quality and effectiveness of training management, marking a substantial contribution to the educational resources available for Vietnamese institutions."}, {"title": "Dataset", "content": "The use of data in the field of educational management presents several significant challenges, particularly when developing a question-answering system for the Vietnamese language. These challenges include:\n\n\u2022 Institutional Variability: Each educational institution must comply with the regulations set forth by the Ministry of Education. However, beyond these mandatory guidelines, institutions often have additional rules and policies specific to their own organization or the larger entity they are affiliated with. This variability can lead to inconsistencies in data structure, terminology, and reporting practices, complicating the task of creating a unified dataset.\n\n\u2022 Data Standardization: Due to the diverse regulatory requirements and internal policies across different institutions, standardizing data becomes a complex process. Ensuring consistency and compatibility of data from various sources is essential for effective analysis and model training but is difficult to achieve given the heterogeneity of the data.\n\n\u2022 Data Availability and Quality: As one of the first studies addressing the question-answering problem in the Vietnamese language within the educational management domain, there is a scarcity of readily available datasets. Existing datasets in other languages or educational contexts may not be directly applicable due to linguistic and contextual differences. Therefore, sourcing high-quality data externally is challenging, necessitating the creation of a new dataset from scratch.\n\n\u2022 Data Collection and Annotation: Building a new dataset requires significant effort in data collection and annotation. This process involves gathering data from various educational institutions, ensuring its accuracy and relevance, and annotating it to create a structured dataset suitable for training machine learning models. The annotation process, in particular, is time-consuming and demands a deep understanding of the educational domain.\n\nAddressing these challenges is crucial for the success of our research. By acknowledging and systematically tackling these issues, we aim to build a robust and reliable dataset that will facilitate the development of effective AI solutions for educational management in Vietnam."}, {"title": "Building data", "content": "In this subsection, I describe the process of constructing a dataset from the \"Regulations on Student Affairs of Vietnam National University\"(VNU) to train a model for question-answering tasks. By using prompts, we generate data points that each consist of a \"context,\"\"question,\"and \"answer.\"This structured approach ensures comprehensive coverage of the regulations and facilitates the creation of a robust dataset for training. The process consists of five critical steps: data preprocessing, data analysis and prompt design, data generation using prompts and LLMs, and data quality evaluation.\n\nThe first step data preprocessing involves preparing the raw data for subsequent analysis and prompt generation. This includes:\n\n\u2022 Data Cleaning: Removing any irrelevant information, and duplicates, and ensuring consistency in formatting.\n\n\u2022 Text Segmentation: Breaking down the regulations into manageable sections that can be used as context for generating questions and answers.\n\n\u2022 Whitespace and Extraneous Character Removal: Removing unnecessary spaces and characters to ensure clean text.\n\n\u2022 Spell Checking: Correcting any spelling errors in the text.\n\n\u2022 Math Formula Conversion: Converting mathematical formulas into KATEX format for consistent representation.\n\nAfter preprocessing the data, the next step is to analyze the content and design effective prompts. This involves:\n\n\u2022 Content Analysis: Identifying key themes, rules, and guidelines within the regulations.\n\n\u2022 Prompt Crafting: Developing specific prompts that will be used to generate questions and answers. Each prompt focuses on different aspects of the regulations, ensuring comprehensive coverage.\n\n\u2022 Using technique prompting Chain of Thought, Self-Consistency Chain of Thought, and Tree of Thought: Employing advanced prompting techniques to enhance the generation process.\n\nThe next steps are data generation using prompts and LLMs, and data quality evaluation. To generate the desired dataset, we utilized prompts that were meticulously designed in the previous phase. These prompts were fed into large language models (LLMs) such as GPT-3,5 turbo, which then generated a comprehensive set of synthetic data. The generation process was systematic and aimed to produce data that closely aligned with our research objectives and covered the necessary range of scenarios.\n\nThe quality of the generated data was evaluated using both automated metrics and human assessment. Specifically, we employed ROUGE and BLEU scores to quantify the relevance and coherence of the generated text. These metrics provided an objective measure of how well the generated data matched the expected output in terms of n-gram overlap and sequence similarity.\n\nIn addition to automated metrics, human evaluators conducted a qualitative review of the generated data. These domain experts assessed the data for relevance, coherence, and diversity, ensuring that the synthetic data met the high standards required for our study. This dual approach of combining quantitative scores with qualitative human judgment ensured a robust evaluation of the generated dataset, confirming its suitability for subsequent analyses and experiments."}, {"title": "Methodology", "content": "In this section, we detail the methodology employed to address the question-answering problem within the domain of university educational management in 2. Our approach encompasses several key stages: leveraging a Large Language Model (LLM) for initial data pre-labeling, human labeling for data refinement, training the model, evaluating its performance, and conducting a thorough analysis of the results. Each step in this pipeline is meticulously designed to ensure accuracy and effectiveness, tailored to the specific needs and constraints of the educational context in Vietnam.\n\nWe will systematically describe each stage of our methodology as follows:\n\n\u2022 Large Language Model (LLM):An overview of the LLM utilized in our study, highlighting its features and advantages in handling natural language processing tasks.\n\n\u2022 Pre-labeling: A description of the pre-labeling process using the LLM to provide initial annotations for the dataset, which sets a foundation for further refinement.\n\n\u2022 Human Labeling: An explanation of the human labeling process, emphasizing its role in ensuring high-quality data by correcting and improving the initial LLM-generated labels.\n\n\u2022 Training: Details on the training phase, including the algorithms and techniques applied to build a robust question-answering model.\n\n\u2022 Evaluation: Presentation of the evaluation methods and criteria used to assess the model's performance, ensuring it meets the desired standards of accuracy and reliability.\n\n\u2022 Analysis: A comprehensive analysis of the results obtained from the evaluation, providing insights into the model's strengths and areas for improvement."}, {"title": "Pre labeling and human labeling", "content": "With two steps using LLMs pre-labeling v\u00e0 human labeling, I illustrated in section 3 building data."}, {"title": "Training and context adaptation", "content": "In this subsection, we describe the training process and context adaptation techniques employed to enhance the question-answering capabilities of our model, particularly tailored to university educational management."}, {"title": "Training", "content": "Vistral (Vo, 2024) is a deep learning model that uses many transformer decoder layers to generate coherent and natural language text. The model was pre-trained on a large corpus of text data using an unsupervised learning approach, which enabled it to learn the statistical patterns and structures of natural language. Vistral has been widely used for various NLP tasks such as language translation, question-answering, text summarization, and even creative writing. As of now April 2024, the Vistral model is the highest-scoring public model on the VMLU leaderboard. Vistral model is an innovative Large Language Model designed expressly for the Vietnamese language.\n\n\u2022 Rolling Buffer Cache\n\n\u2022 Sliding-Window Attention\n\n\u2022 Pre-fill and Chunking\n\nSliding Window Attention utilizes the multiple layers of a transformer to access information beyond a defined window size $W$. In this method, the hidden state at position $i$ in layer $k$, denoted as $h_i$, attends to all hidden states in the preceding layer within the range from $i \u2013 W$ to $i$. This process allows $h_i$ to recursively access tokens from the input layer at a distance of up to $W \u00d7 k$ tokens.\n\nRolling Buffer Cache. By having a fixed attention span, we can manage our cache size with a rolling buffer cache. This cache has a set size of $W$, and the keys and values for timestep $i$ are saved in the position $i \\mod W$ of the cache. Consequently, when position $i$ exceeds $W$, the older values in the cache are overwritten, preventing the cache size from growing indefinitely.\n\nPre-fill and Chunking. When generating a sequence, tokens must be predicted one at a time, as each token depends on the previous ones. However, since the prompt is known beforehand, we can pre-fill the (k, v) cache with the prompt. If the prompt is very large, it can be divided into smaller chunks, and the cache can be pre-filled with these chunks. The window size can be used as the chunk size. For each chunk, it is necessary to compute the attention over both the cache and the chunk.\n\nModel Bloom BLOOM is a powerful autoregressive Large Language Model (LLM) designed to extend text from a given prompt, utilizing extensive computational resources on massive text datasets. This capability allows it to produce fluent text in 46 different languages and 13 programming languages, making it almost indistinguishable from human-written content. Additionally, BLOOM can be directed to undertake text-related tasks it wasn't specifically trained for by framing them as text-generation problems.\n\nModeling Details Several key innovations were incorporated into the BLOOM model to enhance its performance and stability:\n\nALiBi Positional Embeddings: The model employs ALiBi (Attention Linear Bias) positional embeddings instead of traditional positional embeddings. ALiBi attenuates attention scores based on the distance between keys and queries, which results in smoother training dynamics and improved performance.\n\nEmbedding LayerNorm: An additional layer normalization step is applied immediately after the embedding layer. This modification was implemented to improve training stability, especially considering the use of bfloat16 precision in the final training phase, which offers more stability than float16.\n\nLow rank Adaptation\n\nFor a given pretrained weight matrix $W_o \\epsilon R^{d\\times k}$, LoRA introduces two trainable weight matrices, $W_{up} \\epsilon R^{d\\times r}$ and $W_{down} \\epsilon R^{r \\times k}$ where the rank $r < min(d, k)$, operating in parallel to $W_o$. Let represent the input. Under normal conditions, the output through $W_o$ is $h_{out} = W_o h_{in}$. Instead, LORA modifies this output by introducing an incremental update $\\Delta W$ that encapsulates task-specific knowledge:\n\n$h_{out} = W_o h_{in} + \\frac{\\alpha}{r} W_{up} W_{down} h_{in} = W_o h_{in} + \\frac{\\alpha \\Delta W h_{in}}{r}$       (1)\n\nwhere $\\alpha$ denotes a scaling factor. At the onset of training, $W_{down}$ is initialized using a random Gaussian distribution, while $W_{up}$ is initialized to zero, ensuring that $\\Delta W$ initially holds a value of zero. LoRA is straightforward to implement and has been evaluated on models with up to 175 billion parameters. In this research, I use this method for the model Bloom and Vistral-7B. Once fine-tuning is complete, LoRA's adaptive weights seamlessly integrate with the pre-trained backbone weights. This integration ensures that LoRA maintains the model's efficiency, adding no extra burden during inference. The number of parameters training is reduced $dk/(d+ k)/r$ times."}, {"title": "Context Adaptation", "content": "Context adaptation is crucial for activating the model's question-answering capabilities. We enhance the training data by incorporating detailed instructions and contextual cues that guide the model in understanding and generating accurate responses to educational queries.\n\nBy adding specific instructions, we provide the model with explicit examples of how to approach different types of questions within the educational domain. These instructions act as triggers, enabling the model to apply its learned knowledge effectively and respond accurately to complex queries.\n\nOur training and context adaptation approach ensures that the models are not only finely tuned to our dataset but also contextually aware, enhancing their ability to provide precise and relevant answers in the context of university educational management. The combination of dual-model training and LORA, along with detailed context adaptation, significantly boosts the model's performance and usability in real-world applications."}, {"title": "Evaluate", "content": "Exact Match (EM): For each question-answer pair, if the characters of the MRC system's predicted answer exactly match the characters of (one of) the gold standard answer(s), EM = 1, otherwise EM = 0. EM is a stringent all-or-nothing metric, with a score of 0 for being off by a single character. When evaluating against a negative question, if the system predicts any textual span as an answer, it automatically obtains a zero score for that question.\n\nF1-score: F1-score is a popular metric for natural language processing and is also used in machine reading comprehension. F1-score is estimated over the individual tokens in the predicted answer against those in the gold standard answers. The F1-score is based on the number of matched tokens between the predicted and gold standard answers.\nPrecision = $\\frac{the \\ number \\ of \\ matched \\ tokens}{the \\ total \\ tokens \\ in \\ the \\ predicted \\ answer}$        (2)\nRecall = $\\frac{the \\ number \\ of \\ matched \\ tokens}{the \\ total \\ tokens \\ in \\ the \\ gold \\ standard \\ answer}$              (3)\nF1-score = $\\frac{2 \\times Precision \\times Recall}{Precision + Recall}$                                     (4)"}, {"title": "Result and Experiment", "content": "In this subsection, we present a comprehensive statistical analysis of our dataset, which includes an in-depth survey of the lengths and averages of contexts, questions, and answers. Understanding these metrics is crucial for evaluating the overall quality and characteristics of the data used in our experiments."}, {"title": "Data review", "content": "In our study, we categorize the dataset into five distinct levels of question-answering data quality:\n\nVery Good, Good, Medium, Bad, and Very Bad.\n\nThese levels are comprehensively described in Table 3"}, {"title": "Result of model", "content": "In this section, we present the performance of the Bloom and Vistral models. The results are evaluated using the training and validation loss metrics, as well as a comparison of the exact match (Exact) and F1 scores.\n\nI implement hyperparameters with full fine-tuning model in table 5 and hyperparameter using LORA for tuning model in table 5."}, {"title": "Analysis and discussion", "content": "\u2022 Bloom model + LORA vs. Bloom model: The Bloom model with LoRA shows a slight decrease in Exact and F1-score compared to the Bloom model without LoRA. The Exact score drops from 34.23 to 33.89, and the F1 score decreases from 73.16 to 72.36. This suggests that LoRA might slightly affect the performance of the Bloom model in terms of these metrics.\n\n\u2022 Vistral + LORA vs. Vistral: The Vistral model with LORA also exhibits a minor reduction in performance compared to the Vistral model without LoRA. The Exact score drops from 43.72 to 43.23, and the F1 score decreases from 81.57 to 81.24. This indicates that the inclusion of LoRA may have a small impact on the Vistral model's performance.\n\n\u2022 Bloom model vs. Vistral: Comparing the two models, Vistral consistently outperforms Bloom in both Exact and F1-score, both with and without LoRA. This demonstrates that the Vistral model is more effective in capturing and processing the information needed for higher precision and overall accuracy."}, {"title": "Resource Utilization", "content": "\u2022 Training Time: The training time per epoch is significantly lower for models using LoRA. The Bloom model with LoRA takes 1.5 hours per epoch, whereas without LoRA, it takes 5 hours. Similarly, the Vistral model with LoRA takes 6 hours per epoch, compared to 14 hours without LoRA. This reduction in training time highlights the efficiency of the LoRA method in speeding up the training process.\n\n\u2022 GPU RAM Usage: Models with LoRA also require less GPU RAM. The Bloom model with LoRA uses 16 GB, while the original Bloom model uses 29 GB. The Vistral model with LoRA uses 32 GB, compared to 61.2 GB for the Vistral model without LoRA. This reduction in memory usage indicates that LoRA helps in optimizing resource utilization during training."}, {"title": "Real-world inference", "content": "Example in table 8 there are four reasons explain why this is a good answer:\n\n\u2022 Comprehensive and Detailed: The answer includes the main responsibilities of the lecturers such as imparting scientific ambition, a passion for learning, scientific research, scientific thinking, and creative ability. These elements are clearly stated in the context provided.\n\n\u2022 Clear and Understandable: The answer is articulated clearly and understandably, making it easy for the reader to grasp the responsibilities of the lecturers.\n\n\u2022 Contextual Connection: The answer is closely linked to the context provided, ensuring that the information presented is accurate and relevant.\n\n\u2022 Repetition for Emphasis: Repeating the key points at the end of the answer emphasizes the lecturers' responsibilities and highlights the main aspects that need to be noted.\n\nIn table 9 illustrates the bad answer. This poor answer does not provide a comprehensive response based on the provided context. It lacks details and does not address the specific aspects mentioned in the context, such as training programs, study time, organization and management of training, scientific research, rights and obligations of lecturers, advisors, students, examinations, assessments, and graduation recognition."}, {"title": "Conclusion and Limitations", "content": "In this paper, we present a simple and effective framework for applying large language models (LLMs) to educational domain. We conduct the experiments with fine-tuning methods on resource-constrained environments to optimally leverage existing GPU capabilities and hardware. Our results demonstrated that using LLMs models for vietnamese improved performance by over 10 points compared to previous model. This significant improvement highlights the effectiveness of our approach in maximizing the potential of limited computational resources."}, {"title": "Limitations", "content": "In this study and in the realm of natural language processing, particularly in the application of question-answering (QA) systems for educational management in Vietnamese, several limitations of current models and data quality have been identified. These limitations are crucial to understand for the continued development and improvement of such systems.\n\n1. Reasoning Capabilities of the Model\n\n\u2022 Logical Reasoning: The models may produce answers that lack coherent logical structure or fail to follow a clear line of reasoning, especially for complex or multi-step problems.\n\n\u2022 Contextual Understanding: While models can understand the context to a certain extent, they often miss subtle nuances and deeper connections within the provided context, leading to less accurate or irrelevant responses.\n\n2. Contextual Errors and Ambiguity\n\n\u2022 Error in Capturing Context: Models sometimes fail to capture the full context of a question, particularly when the context is lengthy or contains intricate details.\n\n\u2022 Ambiguity in Responses: Due to the models' probabilistic nature, they can produce responses that are ambiguous or vague, which can be particularly problematic in educational management where precision is crucial.\n\n3. Lack of Specialized Knowledge\n\n\u2022 Handling Specific Regulations: The models might not fully grasp the specific regulations and guidelines unique to different educational institutions or contexts, leading to incorrect or incomplete answers.\n\n\u2022 Domain-Specific Expertise: The absence of deep domain expertise means that the models might misinterpret or overlook critical aspects of educational management tasks."}]}