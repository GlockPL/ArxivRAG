{"title": "Towards a Formal Theory of the Need for Competence via Computational Intrinsic Motivation", "authors": ["Erik M. Lintunen", "Nadia M. Ady", "Sebastian Deterding", "Christian Guckelsberger"], "abstract": "Computational models offer powerful tools for formalising psychological theories, making them both testable and applicable in digital contexts. However, they remain little used in the study of motivation within psychology. We focus on the \"need for competence\", postulated as a key basic human need within Self-Determination Theory (SDT)-arguably the most influential psychological framework for studying intrinsic motivation (IM). The need for competence is treated as a single construct across SDT texts. Yet, recent research has identified multiple, ambiguously defined facets of competence in SDT. We propose that these inconsistencies may be alleviated by drawing on computational models from the field of artificial intelligence, specifically from the domain of reinforcement learning (RL). By aligning the aforementioned facets of competence-effectance, skill use, task performance, and capacity growth-with existing RL formalisms, we provide a foundation for advancing competence-related theory in SDT and motivational psychology more broadly. The formalisms reveal underlying preconditions that SDT fails to make explicit, demonstrating how computational models can improve our understanding of IM. Additionally, our work can support a cycle of theory development by inspiring new computational models formalising aspects of the theory, which can then be tested empirically to refine the theory. While our research lays a promising foundation, empirical studies of these models in both humans and machines are needed, inviting collaboration across disciplines.", "sections": [{"title": "Introduction", "content": "Self-Determination Theory (SDT) (Deci and Ryan, 1985) is one of the most influential psychological theories of motivation, including intrinsic motivation (IM). It has also inspired artificial intelligence (AI) research on computational IM (e.g., Oudeyer and Kaplan, 2007). Computational IM equips artificial agents with abilities for open-ended development, task-agnostic learning, and dealing with the potential sparsity of rewards (Colas et al., 2022, p. 1161). Notably, SDT, like many psychological theories, is expressed in \u201csoft\u201d verbal propositions. In light of the current theory crisis (Oberauer and Lewandowsky, 2019), verbal theory has drawn increasing critique: theories that are not formally specified may fail to produce hypotheses that can be rigorously tested. While many psychology researchers have advocated for computational modelling as a paradigm for theory development (e.g., Marsella et al., 2010; Oberauer and Lewandowsky, 2019; Robinaugh et al., 2021), formal specification in the form of precisely defined computational models remains largely absent in SDT. Judging by the most recent integrative volume of the theory, the Oxford Handbook of Self-Determination Theory (Ryan, 2023), methodological advances chiefly focus on new psychometric approaches and neuroscientific evidence.\nHere, we make the case that computational models of IM can benefit the development of SDT. More specifically, Deterding et al. (2024) have shown that the need for competence, one of the basic psychological needs underpinning IM in SDT, is poorly specified, with multiple definitions that lack clear operationalisation. They derive four distinct facets of competence by means of a detailed analysis of SDT texts spanning many decades (1985\u20132023). We focus on matching computational formalisms to each of these facets."}, {"title": "Background: Computational IM & RL", "content": "We begin by justifying why RL is a suitable domain from which to draw computational models of competence. Additionally, to support understanding of the models we discuss, we introduce some key concepts from intrinsically motivated RL-most notably, goals and skills and their relationship to the reward function-and recommend further reading.\nThe idea that IM might be driven by biological reward systems (Barto et al., 2004, p. 113, in reference to Kakade and Dayan, 2002; Dayan and Balleine, 2002) has driven the choice of methods centrally used in computational IM: RL (Baldassarre, 2022, p. 252). Due to their usefulness, intrinsically motivated RL methods have been studied extensively in the computational RL literature at large (see reviews by Colas et al., 2022; Aubret et al., 2023; Lidayan et al., 2024). An assumption underlying much work in RL is the reward hypothesis: \"all of what we mean by goals and purposes can well be thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)\" (Sutton and Barto, 2018, p. 53; cf., Silver et al., 2021, p. 4). This hypothesis informs the formalisms used for competence-based intrinsic motivation (CB-IM), which tend to rely on the assumption that every goal can be sufficiently defined by a reward function. Intrinsically motivated RL is characterised by the use of task-agnostic intrinsic reward functions that collate or compare agent-internal variables independently of their semantics (Oudeyer and Kaplan, 2008, p. 3; cf., Berlyne, 1965, p. 246 and Oudeyer and Kaplan, 2007).\nMany of our examples operate within the framework of goal-conditioned reinforcement learning (GCRL) (see review by Liu et al., 2022), which extends the standard definition of a reward function to be conditioned on goals. A goal, or a goal-defining variable, is a parameter to the reward function (cf., Colas et al., 2022, p. 1165; Aubret et al., 2023, p. 6). Then, a skill is a policy given a goal, optimising for the return according to the reward conditioned on that goal. Examples of goal-defining variables include indices (e.g., Eysenbach et al., 2019) or elements drawn from a learned distribution (e.g., Nair et al., 2018). Their role is simply to indicate which reward function the agent is aiming to maximise.\nGoals can be viewed as \"a set of constraints that the agent seeks to respect\" (Colas et al., 2022, p. 1165, emphasis in original). While the most immediate intuition of a goal is often as a desired state for the agent to reach (e.g., Kaelbling, 1993, p. 1; Schaul et al., 2015, p. 2), the formalism allows for a more general set of constraints on behaviour (Lintunen et al., 2024b, pp. 22-23). In effect, any behaviour that can be defined by attempting to maximise some reward function on the environment can be formulated as a goal-skill pairing."}, {"title": "Aligning Computational Models With SDT", "content": "Based on a review of classic (Oudeyer and Kaplan, 2007) and state-of-the-art (Colas et al., 2022; Aubret et al., 2023) computational IM, we next turn our focus to matching computational formalisms to each of the competence facets. Our matching is non-exhaustive, and instead we focus on exemplifying how each facet could be formalised."}, {"title": "Effectance (C1)", "content": "Let us consider the case where competence in SDT refers to effectance, i.e., motivation by observing that one's own actions cause change in the environment. To recognise effectance, an agent needs a mechanism to distinguish whether a change in the environment is caused by the agent itself or some external factor. An example of a computational model fitting effectance is Rewarding Impact-Driven Exploration (RIDE) (Raileanu and Rockt\u00e4schel, 2020), as part of which the agent is rewarded for the amount of change it causes in its state observation: the \"impact\" of the agent's actions, measured between two consecutive observations. Specifically, the impact-driven reward is defined as:\n$R_t(s_t, s_{t+1}) := \\frac{||\\phi(s_{t+1}) - \\phi(s_t)||_2}{\\sqrt{N(s_{t+1})}}$ (1)\nwhere $\\phi(s_t)$ and $\\phi(s_{t+1})$ are the learned representations of two consecutive states and $N(s_{t+1})$ represents or approximates the number (count) of times the latter state has been visited (Raileanu and Rockt\u00e4schel, 2020, pp. 4-5).\nIn RIDE, the agent computes its reward using the learned representation of the observation as opposed to the raw sensory space (e.g., pixels). Inspired by Pathak et al. (2017), the parameters of the model $\\phi$ are optimised so that an agent learns the features it can control\u2014that is, it learns to ignore aspects of the environment it cannot affect. Decomposing the reward function, the agent is rewarded for a combination of changes in those state features that it is able to consistently have some effect on (Eq. 1, numerator) and the novelty of the state into which its action leads (Eq. 1, denominator). The effect of the denominator is that the reward obtainable from particular states diminishes the more the agent observes them, even if the agent's action has the same effective outcome as previously observed. In recent SDT texts, effectance motivation does not require novelty, but the original usage of the term required observations to be novel (White, 1959, p. 322). By having the reward given by RIDE wane the more a particular situation is perceived, RIDE offers a simple formalisation of this original novelty aspect of effectance motivation."}, {"title": "Skill use (C2)", "content": "We next consider the case in which competence refers to skill use or the \"exercise of one's capacities\u201d (Ryan and Deci, 2017, p. 86). Following Deterding et al. (2024, p. 6), we cover the specific understanding of \u201cexercise\u201d as implementing or bringing something to bear (Merriam-Webster.com, 2025); an alternative interpretation as engaging a skill repeatedly in order to grow it will be treated under Capacity growth (C4). As per Table 1, competence as skill-use motivation can be understood in two different ways: observing an opportunity (an appropriate situation) to use a skill (C2.a) and that one uses a particular capacity in the course of one's action (C2.b).\nAn opportunity (an appropriate situation) to use a skill (C2.a) Broadly, there are two sets of computational approaches used to model an agent learning about opportunities to use particular skills: those in which the agent ignores the current state of the world and those in which it does not. In both cases, hierarchy is used: an agent learns a distribution over its set of policies (i.e., a high-level policy) to determine which skill to use. In the former case, an agent follows the same policy regardless of its state observation in this case, an \"appropriate\u201d opportunity can be thought of as a learning opportunity, determined not by observing the environment, but from information about how the learning is proceeding. In the latter case, identifying an opportunity to use a particular skill can also draw on observations of the environment.\nSpecifically, the first case of C2.a, in which an agent is learning about opportunities to use a skill while ignoring the current state of the world, is well exemplified by computational approaches using Learning Progress (LP), which approximates some derivative of performance over time (initially proposed by Oudeyer et al., 2007, p. 7; see also related earlier work by Schmidhuber, 1991, p. 1461). In systems employing LP, it is typically used as a measure to support the selection of intermediate-difficulty goals with respect to the changing skills of an agent over time. LP, as an intrinsic reward for goal selection, has been well studied across both machine and human reinforcement learners (e.g., Colas et al., 2019; Ten et al., 2021; Molinaro et al., 2024), and could thus offer good starting points for formalising C2.a.\nWhile a reward function prioritises the choice of skills, an agent also needs a mechanism to select a skill from that prioritisation. One commonly used mechanism is Thompson sampling (Thompson, 1933), also known as probability matching, where an agent draws a", "evidence\\\" (e.g., a change in LP values) and its \"belief\" (i.e., skill-selection policy) is updated. Thompson sampling has been shown to explain aspects of human exploration in certain tasks (Gershman, 2018), making it a candidate for a psychologically plausible formalism of human skill selection.\nAn example of a system formalising the second case of C2.a, in which an agent learns about opportunities to use particular skills while accounting for the current state of the world, is Variational Intrinsic Control (VIC). In VIC, the appropriate skill for a given starting state is decided using a separate learned model of how empowering the skill has been in the past. We mean empowering in the sense of its definition in the AI literature,\u00b9 as the combined ability of an agent to control its environment and to sense this control (cf., Salge et al., 2014, p. 2; Gregor et al., 2016, p. 2). The decision over skills is conditional on the state of the world, in that the policy for choosing skills is updated based on how empowering the skill turned out to be for that given situation. The distribution over skills is updated during skill use, such that more empowering skills become more likely to be chosen (Gregor et al., 2016, p. 4), using the reward (Gregor et al., 2016, p. 2)": "n$R_t(s_0, s_t, g) := log q_{\\phi}(g | s_0, s_t) \u2013 log p_{\\theta}(g | s_0)$. (2)\n(\u03b1)\n(\u03b2)\n(a) The discriminator, $q_{\\phi}$, is an arbitrary variational distribution parametrised by $\\phi$ (Barber and Agakov, 2003, p. 2). Given the first ($s_0$) and last ($s_t$) observations induced by the skill corresponding to the goal being pursued, $q_{\\phi}$ defines a probability distribution over goals. Successfully discriminating goals in the observation space requires the agent to observe distinct regions of the state space. If a goal is not discriminable based on observations, two or more skills are producing overlapping behaviours (and therefore, the skills lack diversity); conversely, if a goal is discriminable, then the corresponding skill is inducing trajectories unique to that skill. Thus, this reward term is maximised when there is no uncertainty in the prediction.\n(\u03b2) The probability model, $p_{\\theta}(g | s_0)$, parametrised by $\\theta$, represents the goal-selection policy. As part of the reward, the purpose of this term is to reinforce high entropy: the agent should keep pursuing a wide range of goals, for each starting state, $s_0$, as the reward is higher when the goal had a low probability of being selected.\nGiven a starting state, $s_0$, a goal, g, is selected according to the distribution defined by $q_{\\phi}$, which the agent pursues until the termination state $s_t$. Then, $p_{\\theta}$ is reinforced based on how empowering the corresponding skill was (estimated using Eq. 2). The final state becomes the new starting state, $s_0 \\leftarrow s_t$, and the process is repeated. This can motivate the"}, {"title": "Task performance (C3)", "content": "Next, we discuss computational models for two possible ways of understanding task performance: observing that one performs well at an intended task (C3.a) and performing to an extent that requires a certain skill or skill level (C3.b).\nObserving that one performs well at an intended task (C3.a) Given an agent has set itself a goal (task), if an agent has a mechanism to estimate its progress towards a chosen goal, it can use that estimate as a proxy for its own performance on the task. An example system of this kind is Reinforcement Learning With Imagined Goals (RIG) (Nair et al., 2018). RIG encodes the agent's observations into a lower-dimensional latent space from which the agent samples goals to pursue. Defining the reward as the negative distance between the goal and the current state in latent space encourages the agent to move its observations closer to the goal:\n$R_t (s_{t+1}, g) := - ||\\phi(s_{t+1}) - \\phi(g) ||_A$, (4)\nwhere $\\phi(s_{t+1})$ is the learned state representation of $s_{t+1}$, $\\phi(g)$ denotes the goal drawn from the latent space, and A is a matrix that can give more importance to some dimensions of the latent space over others (Nair et al., 2018, p. 5).\nIn contrast to DIAYN, in which the agent learns to master a fixed number of skills, in RIG, the agent selects from an infinite number of goals (the space is continuous). This results in a sort of smoothness of the space that the agent can benefit from: if one goal is sampled near another that the agent has already learned to achieve, it may be able to generalise and use some parts of an existing policy to more efficiently learn to achieve the new goal. In the RIG system, goals take on the intuitive meaning as desired states for the agent to reach (discussed in Background: Computational IM & RL), and \"performing well\" at an intended task is operationalised as minimising the semantic distance between the agent's observations and its self-generated goals. Over time, RIG agents learn to generate goals similar to the observations they make of their environment, while simultaneously learning the skills necessary to consistently perform well (reach the goals).\nSpecifically to an extent that requires a certain skill or skill level (C3.b) While RIG captures the idea of motivation by performance on a self-selected task (C3.a), it does not take into account the level of skill needed to perform well on the task (C3.b). For this, we can look to other system designs inspired by the idea of preferring to focus on goals or skills at an appropriate level of difficulty, given what the agent already knows. To identify skills of appropriate difficulty, such systems can make use of Automatic Curriculum Learning (ACL) (for a review see Portelas et al., 2021), enabling an agent to self-organise their goal selection. This can significantly reduce the number of training examples required for an agent to learn to reach its goals (Florensa et al., 2018, p. 1).\nFor example, in the Continual Universal Reinforcement learning with Intrinsically motivated substitutionS (CURIOUS) system by Colas et al. (2019), instead of considering task performance as distance from the goal, the system considers its capability to achieve its goals. Specifically, the agent is motivated to pursue goals in certain parts of the sensory space in which it believes to be increasingly or decreasingly successful at achieving its goals, estimated by measuring how successful it has been in the recent past-that is, its LP. The underpinning logic of the system is that such goals will be of optimal difficulty, neither too easy nor too difficult.\nIn the CURIOUS experiments, an agent manipulates cubes by learning to control a robotic arm (Colas et al., 2019, Supplementary p. 3). Each observation includes information about the gripper and the objects of interest (e.g., their position and rotation), and each goal is a desired observation. The high-dimensional observation space makes learning challenging. Furthermore, some parts of the sensory space are irrelevant for achieving certain goals-for example, if the goal is to ma-"}, {"title": "Capacity growth (C4)", "content": "Competence, understood as capacity growth, can be interpreted in two different ways: observing gain in the strength (C4.a) and/or range (C4.b) of one's skills. While we saw that a reward reflecting a derivative of some measure of performance (e.g., LP) can direct the agent towards goals requiring a certain level of skill (C3.b), the same derivative directly encourages capacity growth or improvement (C4.a). While these two facets appear to have different meanings, from a computational perspective, they can be closely aligned.\nAn example of a system that explores both C4.a and C4.b-both repeatedly engaging in skills and extending the agent's skill repertoire-is Intrinsically Motivated Reinforcement Learning (IMRL) described by Barto et al. (2004) and Singh et al. (2004). IMRL extends the agent's repertoire of skills (C4.b): each time a \"salient\" event is observed, the agent adds a new skill associated with the salient event. After this first observation, the agent learns how to return to observe the salient event again from any other situation via experience over time-this is the new skill associated with the salient event. Strictly speaking, the skill repertoire expands as a matter of course, not because the agent is intrinsically rewarded for expanding it.\nThe intrinsic reward used in the IMRL system (Singh et al., 2004, p. 4) can be defined as:\n$R_t (s_t, s_{t+1}, g_{s_{t+1}}) := \\begin{cases}\n1-p_{g_{s_{t+1}}}(s_{t+1} | s_t) & \\text{if } s_{t+1} \\text{ is salient} \\\\\n0 & \\text{otherwise}\n\\end{cases}$ (6)\nwhere $p_{g_{s_{t+1}}}$ is what Precup and Sutton (1997) call a multi-time model for skill $g_{s_{t+1}}$. $p_{g_{s_{t+1}}}$ is not quite a probability estimate for the transition from $s_t$ to $s_{t+1}$ given the agent is following skill $g_{s_{t+1}}$, but that is close to the correct intuition.\nThe agent only receives a non-zero intrinsic reward when it reaches a salient state. The intrinsic reward here can be thought of as modelling how surprising it is that the agent reached $s_{t+1}$ immediately from state $s_t$-which it has just done by following the skill $g_{s_{t+1}}$. If the agent expected, without uncertainty, to transition directly from $s_t$ to $s_{t+1}$, then $p_{g_{s_{t+1}}}$ is 1 and the reward is 0 (the transition is very unsurprising). If the agent did not expect it could transition from $s_t$ to $s_{t+1}$ by following skill $g_{s_{t+1}}$, then $p_{g_{s_{t+1}}}$ is 0 and the reward is 1 (maximally surprising). If the agent is uncertain, $p_{g_{s_{t+1}}}$ will"}, {"title": "Advancing the Theory of Competence in SDT", "content": "Why should human motivation researchers care about the computational models we have identified? By identifying distinct ways the different facets of the need for competence in SDT (Table 1) can be formalised, we better understand the ways the theory is underspecified. For instance, LP-based methods (discussed in C2, C3, C4) can exemplify skill use as an opportunity to use a particular skill (C2.a), task performance that requires a certain skill or skill level (C3.b), and capacity growth in the sense of observing gains in the strength of one's skills (C4.a). Given that SDT does not acknowledge distinct facets, finding such a formalism that fits multiple facets supports the theory. On the other hand, some facets are formally distinct. For example, it is unclear how effectance (C1), being motivated by extending one's repertoire of skills (C4.b), or being motivated by observing that one performs well at an intended task (C3.a) relate to each other and other facets. Different interpretations of the theory might lead to drastically different operationalisations of the need for competence. Our research can alleviate this problem by uncovering ways the theory conflates formally distinct concepts, thus prompting review of the theory.\nAnother reason human motivation researchers should care about this work is that the computational formalisms reveal underlying preconditions that SDT fails to make explicit, such as that a competence-motivated agent is able to recognise when actions cause effects (C1) and differentiate between distinct skills (C2, C3, C4). Similarly, concepts from intrinsically motivated RL, such as novelty, diversity, and progress towards goals, underlie competence-driven learning in computational contexts, yet are often overlooked in SDT. Our research shines light on mechanisms used to computationally fulfil such preconditions, and these could be investigated for further insights. Additionally, our work can support a cycle of theory development by inspiring new computational models formalising aspects of the theory, which can then be tested empirically to refine the theory."}, {"title": "Future Work and Conclusion", "content": "For computational IM to be effective in informing theory on motivation, more empirical work is needed in the comparison and evaluation of computational IM. While some work exists comparing behaviour induced by different intrinsic reward functions (e.g., Santucci et al., 2012; Biehl et al., 2018; Linke et al., 2020), as does work studying variables such as the choice of representation (e.g., Burda et al., 2019), more research is needed on the properties of different models, such as how they behave in varying environments and bodies. Showing that RL formalisms can be aligned with SDT uncovers a wealth of resources for motivation research, such as existing implementations of models, simulators, and evaluation metrics. These resources can enable large-scale simulation studies to explore psychologically plausible computational IMs and how they may explain human data, thus contributing to formal competence-related theory. However, we need work to decide how to effectively compare artificial and biological agents (a project partially undertaken by researchers in representational alignment; e.g., Sucholutsky et al., 2024).\nFurther research is also required to understand the extent to which computational IM research intersects with intrinsically motivated learning as understood in psychology. Early work on computational IM emerged at the intersection of developmental psychology and robotics, attempting to model intrinsically motivated human behaviours, but a large portion of state-of-the-art intrinsically motivated RL is not necessarily aligned with relevant goals of understanding the human mind or developing safe (human-aligned) AI systems.\nAs most formalisms we identified relate to the pursuit of goals, we need to better understand how humans represent and generate goals (Colas et al., 2024, p. 1). The topic has not been well studied for various reasons, including computational intractability (Byers et al., 2024, p. 1). Excitingly, goals have recently emerged as a multidisciplinary area of research, cutting across AI, cognitive science, and developmental psychology (e.g., Colas et al., 2022; Molinaro and Collins, 2023; Chu et al., 2024; Davidson et al., 2024). Recent work by Molinaro and Collins (2023, p. 1150) suggests that goals affect many agent dynamics, such as representing the environment, choosing relevant actions, and evaluating rewards.\nIn summary, we build on recent research arguing how the need for competence is at present poorly specified in SDT. We do so by identifying computational formalisms to inspire empirical work to resolve this issue. We show that the four distinct facets of competence-effectance, skill use, task performance, and capacity growth-can be aligned with existing formalisms from intrinsically motivated RL, highlighting the potential of computational models to provide deeper insights into competence-motivated behaviours. While this is a promising first step, further research is required to empirically study these computational models, inviting collaboration from motivation researchers across disciplines."}]}