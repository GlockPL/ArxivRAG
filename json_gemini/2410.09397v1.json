{"title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes", "authors": ["Xiaoyu Li", "Yingyu Liang", "Zhenmei Shi", "Zhao Song", "Yufa Zhou"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in processing long-context information. However, the quadratic complexity of attention computation with respect to sequence length poses significant computational challenges, and I/O aware algorithms have been proposed. This paper presents a comprehensive analysis of the I/O complexity for attention mechanisms, focusing on backward passes by categorizing into small and large cache scenarios. Using the red-blue pebble game framework, we establish tight bounds on I/O complexity across all cache sizes. We confirm that the de facto standard I/O aware algorithm FlashAttention is optimal for both forward and backward passes for the large cache size scenario. For small cache sizes, we provide an algorithm that improves over existing methods and achieves the tight bounds. Additionally, we extend our analysis to sparse attention, a main-stream speeding-up approach, deriving fine-grained lower bounds for both forward and backward passes and both small and large caches. Our findings complete the theoretical foundation for I/O complexity in attention mechanisms, offering insights for designing efficient algorithms of LLM training and inference.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs), such as GPT-4 [AAA+23], Claude [Ant24], Llama [LT24], and more recently o1 [Ope24] from OpenAI, have demonstrated immense potential to enhance various aspects of our daily lives, including conversational AI [LCT+24], AI agents [XCG+23, CYL+24], search AI [Ope24], AI assistants [KHC+24, FJL+24], and many others. One of the most emergent abilities of LLMs is dealing with long-context information, which is crucial for processing materials such as academic papers, official reports, and legal documents. LLMs have proven adept at tackling long-context tasks, such as zero-shot summarization [CAM24, ZJV+24] and maintaining very long-term conversations [XGW+22, MLT+24]. OpenAI's o1 model [Ope24] serves as a significant advancement in this area. It leverages Chain-of-Thought (CoT) reasoning [WWS+22, KGR+22] and employs Retrieval Augmented Generation (RAG) [LPP+20, GXG+23] to exhibit PhD-level abilities, where both techniques require long context inputs for generation. This proficiency underscores the necessity for developing long-context modeling capabilities within LLMs.\nLLMs are primarily based on the Transformer architecture [VSP+17], whose core component is the self-attention mechanism. However, the quadratic complexity of attention computation with respect to sequence length dominates the computational FLOPs during long-context training and inference. To address this issue, FlashAttention [DFE+22, Dao23, SBZ+24] accelerates attention computation and has become the de facto standard in the industry of LLM training and inference deployment. The success of FlashAttention lies in its I/O awareness [AV88], accounting for reads and writes to different levels of fast cache (e.g., GPU on-chip SRAM) and slow memory (e.g., GPU high-bandwidth memory) within the hardware hierarchy. Leveraging modern hardware design in GPUs, e.g., NVIDIA A100 and H100, efficiently allows FlashAttention to be integrated as a go-to method for LLM training and inference.\nFor the I/O complexity of exact attention\u00b9 forward computation, the theoretical analysis of FlashAttention in [DFE+22] only provides upper and lower bounds when the cache size $M \\in [d, nd]$. Their bounds are only tight in the range of $M = \\Theta(nd)$, where $n$ is the input sequence length and $d$ is the hidden dimension. By fine-grained analysis, a recent work [SY24] provides matching upper and lower I/O complexity bounds of the attention forward passes for any cache size $M$. For the I/O complexity of attention backward passes, existing work only provides an upper bound for FlashAttention for the cache size $M \\in [d, nd]$ [DFE+22], without known lower bounds. Thus, the tight bounds for the I/O complexity of attention backward passes are lacking. This raises a natural question:\nWhat is the optimal I/O complexity of attention backward computations for any cache size?\nIn this paper, we address this question and provide matching upper and lower I/O complexity bounds for backward passes of exact attention computation for all cache sizes, completing the picture of I/O complexity for the attention mechanism."}, {"title": "1.1 Our Contributions", "content": "In this work, we analyze the I/O complexity in the same setting as the existing work of FlashAt-tention [DFE+22] and [SY24]. We consider a two-level memory hierarchy consisting of a small but fast layer called the cache and a large but slower layer referred to as memory. The I/O complexity quantifies the data transfer between these two layers, which can be formally defined as a red-blue"}, {"title": "2 Related Work", "content": "Attention Computation Acceleration. The quadratic time complexity of attention compu-tation with respect to the length of the input sequence [VSP+17] poses significant computational challenges, especially for long sequences. Consequently, accelerating attention computation has become a crucial research area. From a theoretical standpoint, numerous works focus on approx-imating the attention matrix to accelerate computation [HJK+24, AS23, AS24a, LSS+24, AS24b, LSSZ24b]. Experimental approaches involve modifying model architectures and optimizing im-plementations to accelerate inference. Methods such as Mamba [GD23, DG24], Linearizing Trans-formers [ZBKR24, MVK+24], Hopfield Models [HYW+23, WHL+24, HLSL24, XHH+24, WHHL24, HCL+24, HCW+24] and PolySketchFormer [ZHDK23, KMZ23] aim to improve model performance and inference speed. System-level optimizations, such as FlashAttention [DFE+22, Dao23, SBZ+24] and block-wise parallel decoding [SSU18], address bottlenecks in attention mechanisms and enhance inference speed through efficient implementation strategies. Collectively, these advancements con-tribute to making attention mechanisms more scalable and efficient, facilitating the deployment of large-scale language models. [SMN+24] accelerates inference by compressing the input text.\nLearning with Bounded Memory and I/O Complexity. A common memory model in computational systems is the two-level memory hierarchy. In this model, there are two layers of memory: a small but fast layer called the cache, and a large but slower layer called the memory. The I/O (input/output) complexity of an algorithm measures its efficiency based on the number of data transfer operations it performs between the cache and the memory. The early work of [HK81] formulated the I/O complexity mathematically using the language of graph theory. Learn-ing with bounded memory has been studied in various fields in machine learning such as online learning [SWXZ22, PR23, PZ23], convex optimization [MSSV22, CP23], active learning [HKLM21], attention computation [ALSY23], and continual learning [CPP22, EZW+22].\nSparse Attention. Over the past few years, there has been extensive research on sparse Trans-former/Attention models with weights pruning and inputs pruning, aimed at accelerating compu-tation and training [YGG+19, SGBJ19, BPC20, TBY+20, GXD+23, SVV+23, SCY+24, LLSS24, DSY24, CLS+24]. In practice, the attention matrix is sparse, significantly reducing computational costs. Theoretical studies, such as [YCB+20], have demonstrated that sparse transformers are expressive enough and can achieve universal approximation properties."}, {"title": "3 Preliminary", "content": "In this work, we consider using a standard algorithm for matrix multiplication, which means that for any two matrices $A \\in \\mathbb{R}^{n_1 \\times d}, B \\in \\mathbb{R}^{d \\times n_2}$, each entry of $AB$ is computed by $(AB)_{i,j} = \\sum_{k=1}^{d} A_{i,k}B_{k,j}$ for $i \\in [n_1], j \\in [n_2]$. Note that this setting is also used in FlashAttetnion [DFE+22] and [SY24]. Then, we introduce some key concepts needed for this paper."}, {"title": "3.1 Key Concept of Attention", "content": "Before formally stating our results, we begin by precisely defining the problems we study. We define the following computation of the general Softmax attention forward layer.\nDefinition 3.1 (Attention forward computation). Let $n$ be the input length and $d$ be the head dimension. Let $A_1, A_2, A_3 \\in \\mathbb{R}^{n \\times d}$ be the inputs of previous layer. Given query, key and value"}, {"title": "3.2 Summation Tree", "content": "In this subsection, we need to introduce the computational graph of the attention backward gradi-ent, which is the key concept in our I/O complexity analysis.\nIn the computational graph shown in Fig. 2, we can first compute $A_1X$ and then compute $(A_1X)A$, or first compute $XA_2$ and then compute $A_1(XA_2)$. In either case, we perform two matrix multiplications: one between an $n \\times d$ matrix and a $d \\times d$ matrix, and the other between an $n \\times d$ matrix and a $d \\times n$ matrix. Without loss of generality for illustration, we consider the first case. To compute $A_1X$, we need to calculate the products $\\{(A_1)_{i,k}X_{k,j}\\}$ for all $i \\in [n], k \\in [d], j \\in [d]$. Each entry $(A_1X)_{i,j}$ is then obtained by summing these products over $k$: $(A_1X)_{i,j} = \\Sigma_{k=1}^d (A_1)_{i,k}X_{k,j}$. In the computational graph, this summation is represented by a summation tree that connects the product nodes $(A_1)_{i,k}X_{k,j}$ to the sum node $(A_1X)_{i,j}$. We define the product nodes $(A_1)_{i,k}X_{k,j}$, the nodes corresponding to the sums $(A_1X)_{i,j}$, and all intermediate nodes in the summation trees as level-1 nodes. Similarly, we define level-2 nodes as these nodes in the summation trees involved in computing $(A_1X)A_2$. We give an example of the summation tree with $d = 2$ in Fig. 3."}, {"title": "3.3 I/O Complexity", "content": "There are various ways to define the two-level memory hierarchy and the I/O complexity. We state the definition in [HK81], which formulates the two-level memory hierarchy as a red-blue pebble game played on a computational graph. Very recently, [SY24] proved that the I/O complexity of forward computation of FlashAttention is optimal by analyzing the red-blue pebble game on an attention forward computational graph."}, {"title": "4 Main Results", "content": "In Theorem 1.1, we provide matching upper and lower bounds for the I/O complexity of attention gradient computation in the backward passes. In detail, Theorem 1.1 states that the I/O complexity of the attention gradient computation is $\\Theta(\\min {\\frac{n^2d^2 +nd^3}{M}, \\frac{n^2d+nd^2}{\\sqrt{M}} })$, which splits the cache size into two cases: (1) small cache $M = o(d^2)$; (2) large cache $M = \\Omega(d^2)$. At the cross point $M = d^2$, we have $\\frac{n^2d^2 +nd^3}{M} = \\frac{n^2d+nd^2}{\\sqrt{M}} = n^2 + nd$. An intuitive figure of the asymptotic I/O complexity is shown in Fig. 1.\nHere we discuss two implications of Theorem 1.1. First, through the fine-grained analysis, our result identifies a critical point at $M = d^2$, where the I/O complexity changes its behavior. For $M = o(d^2)$, we establish better upper and lower bounds compared to existing results, demonstrating that FlashAttention is not optimal in this regime. Second, when $M = \\Omega(d^2)$, Theorem 1.1 provides a tighter lower bound than existing work using red-blue pebble game (Definition 3.4), offering insights of algorithm design.\nSecond, by combining the results of [SY24] with our findings, we provide a more general and tighter I/O complexity characterization of FlashAttention 1/2 [DFE+22, Dao23]. In the large"}, {"title": "4.1 Large Cache", "content": "The large cache scenario is more interesting and practical. We now prove an upper bound below.\nTheorem 4.1 (Large cache upper bound, informal version of Theorem D.5). Suppose $n$ is the input length, $d$ is the head dimension, and $M = \\Omega(d^2)$ is the cache size. There is an algorithm (see Algorithm 9) outputs a d \u00d7 d matrix $g = \\frac{dL(X)}{dX}$ (Definition 3.2) with I/O complexity $O(\\frac{n^2d^2 +nd^3}{M})$.\nWe then demonstrate that this upper bound is tight by providing a matching lower bound for the I/O complexity of the attention backward passes. To achieve this, we employ the framework developed in [HK81], which shows that executing an algorithm on a machine with a two-level memory hierarchy can be modeled by a red-blue pebble game (Definition 3.4) on a directed acyclic graph. We present the large cache lower bound below, which shows as long as the cache size $M = \\Omega(d^2)$, the I/O complexity is at least $\\Theta(\\frac{n^2d^2 +nd^3}{M})$.\nTheorem 4.2 (Large cache lower bound, informal version of Theorem E.9). Suppose $n$ is the input length and $d$ is the head dimension. Suppose the cache size $M = \\Omega(d^2)$. Then the I/O complexity of attention gradient computation using standard matrix multiplication is always $\\Omega(\\frac{n^2d^2 +nd^3}{M})$."}, {"title": "4.2 Small Cache", "content": "In the small cache case, we provide an upper bound below. Notice that this is better than the I/O complexity of FlashAttention which is $O(\\frac{n^2d^2 +nd^3}{\\sqrt{M}}) > O(\\frac{n^2d+nd^2}{\\sqrt{M}})$ when $M = o(d^2)$.\nTheorem 4.3 (Small cache upper bound, informal version of Theorem C.12). Suppose $n$ is the input length, $d$ is the head dimension, and $M = o(d^2)$ is the cache size. There is an algorithm (see Algorithm 6) outputs a d \u00d7 d matrix $g = \\frac{dL(X)}{dX}$ (Definition 3.2) with I/O complexity $O(\\frac{n^2d+nd^2}{\\sqrt{M}})$, time complexity $O(n^2d + nd^2)$, and space complexity $O(n^2 + d^2)$.\nFurthermore, we show that attention gradient computation can be reduced to matrix multipli-cation, establishing a matching lower bound.\nTheorem 4.4 (Small cache lower bound, informal version of Theorem E.10). Suppose $n$ is the input length and $d$ is the head dimension. Suppose the cache size $M = o(d^2)$. Then the I/O complexity of attention gradient computation using standard matrix multiplication is always $\\Omega(\\frac{n^2d+nd^2}{\\sqrt{M}})$."}, {"title": "4.3 Lower Bound of Sparse Attention Forward and Backward Passes", "content": "Sparse attention is a generalization of standard attention and has been popular in practical appli-cations. We refer readers to Section 2 for more discussion. To state our results, we first introduce some notations. For any matrix A, we use nnz(A) to denote the number of non-zero entries in the matrix A. We assume that sparse matrices are stored by listing only their non-zero entries"}, {"title": "5 Technical Overview", "content": "Upper Bound of Small Cache. In Section C, we present algorithms for the backward passes of attention in the small cache case, where $M = o(d^2)$. We observe that when $M = o(d^2)$, we have $\\frac{n^2d^2 +nd^3}{M} > \\frac{n^2d+nd^2}{\\sqrt{M}} > n^2 + nd$. Then we can exploit this to design a better algorithm with I/O complexity better than $\\frac{n^2d^2 +nd^3}{M}$, by reading/writing the $n \\times n$ attention matrix and other $n \\times d$ intermediate matrices from/to memory. In detail, our small cache algorithm (Algorithm 6) follows the computational graph in Figure 2 and is divided into four phases. In Phase 1 (Algorithm 2), we compute the attention matrix $f$ (Definition B.5) and write it to memory. In Phase 2 (Algorithm 3), we compute $q$ (Definition B.8), incorporating the information from the upstream gradient $dO$. Phase 3 (Algorithm 4) computes the gradient component matrix $p$ (Definition B.9). Finally, in Phase 4 (Algorithm 5), we compute the final gradient $g = A_1pA_2$ (Definition 3.2). At a high level, our algorithm splits the input and output matrices into blocks of size $\\sqrt{M} \\times \\sqrt{M}$. On the other hand, FlashAttention divides the $n \\times d$ input matrices into multiple $k \\times d$ matrices, where $k < n$. Compared to our upper bound, we can see that FlashAttention is not optimal in this case. Following the computational graph in Figure 2, we perform the backward passes of attention using each $\\sqrt{M} \\times \\sqrt{M}$ block as basic elements in standard matrix multiplication. Compared to forward passes, the computational graph of backward passes is more complicated and requires more fine-grained analysis, e.g., the four phases mentioned above. Through a detailed analysis of Algorithm 6, we establish Theorem 4.3.\nUpper Bound of Large Cache. In Section D, we present algorithms for attention backward in the large cache case, where $M = \\Omega(d^2)$. Similar to FlashAttention, the $n \\times n$ attention matrix $f$ (Definition B.5) cannot be directly loaded into cache, even though it has been computed and can be stored in memory. The overall algorithm (Algorithm 9) consists of two phases. In Phase 1 (Algorithm 7), we compute $S = A_1X$ and $h = A_3Y$, and these two matrices are then passed to Phase 2. In Phase 2 (Algorithm 8), the inputs are matrices $A_1, A_2, S, h, O, dO \\in \\mathbb{R}^{n \\times d}$ (Definitions 3.1, B.6, B.7, and B.8), and vector $l \\in \\mathbb{R}^{n}$ (Definition B.4). We vertically divide the inputs into row block matrices of size $B \\times d$ or $B_c \\times d$, where $B_r = \\min{\\{\\lceil M/4d \\rceil, d\\}}$ and $B_c = \\lceil M/4d \\rceil$. Using these row block matrices as computation units, we follow the computational graph (Fig. 2) and FlashAttention's procedure. After accounting for the reads and writes of the overall algorithm (Algorithm 9), we prove Theorem 4.1. Furthermore, when the cache size is as large as $\\Theta(nd)$, the"}, {"title": "6 Conclusion", "content": "This work provided a comprehensive analysis of the I/O complexity for attention mechanisms, focusing on backward passes. We established tight bounds on I/O complexity for both small and large caches. Our results confirm that FlashAttention is optimal for both forward and backward on large cache sizes. For small cache sizes, we provided improved upper and lower bounds compared to existing methods. Additionally, we derived lower bounds for sparse attention for both forward and backward and across cache sizes. Our findings complete the theoretical foundation for I/O complexity in attention mechanisms, offering insights for efficient LLM training and inference. We leave exploring practical implementations leveraging these theoretical insights and investigating I/O complexity for other emerging attention variants as our future work."}, {"title": "A More Related Work", "content": "Large Language Models. The exceptional success of generative large language models (LLMs), such as GPT-4 [AAA+23], Claude 3 [Ant24], Gemini 1.5 [RST+24], Llama 3.1 [LT24], Mistral Nemo [JSM+23], Phi 3.5 [AJA+24], is fundamentally attributed to the transformer architecture introduced by [VSP+17] and all support at least 128k input token length. The transformer ar-chitecture and its self-attention mechanism have become indispensable in leading natural lan-guage processing (NLP) models [CWW+24], demonstrating remarkable capabilities across a di-verse array of applications, including language translation [HWL21], sentiment analysis [UAS+20], language modeling [MMS+19], the integration of differential privacy [SAMB24, LSSZ24a], and multi-modal tasks [ZHJL24, LSSZ24b, WMS+24]. Transformers' emergent compositional abili-ties [DLS+24, XSL24] and proficiency in in-context learning [OEN+22, MLH+22, SWXL24] have led some to consider them as early indicators of Artificial General Intelligence (AGI) [BCE+23]. As such, the transformer architecture continues to play a pivotal role in advancing the field of AI.\nMore about Attention Computation Acceleration. The quadratic time complexity of at-tention computation with respect to the length of the input sequence [VSP+17] poses significant computational challenges, especially for long sequences. Consequently, accelerating attention com-putation has become a crucial research area, with approaches broadly divided into two categories: (1) theoretical optimization of computational complexity [AS23, AS24a], and (2) experimental im-provements to model performance [DFE+22, Dao23, SBZ+24, GZL+23, FTH+24].\nFrom a theoretical standpoint, numerous works focus on approximating the attention matrix to accelerate computation. For example, [AS23, AS24a] utilize polynomial kernel approximation tech-niques [AA22] to speed up both training and inference of a single attention layer, achieving almost linear time complexity, and extend this approach to multi-layer transformer [LSS+24] and tensor attention [AS24b, LSSZ24b]. Other theoretical contributions include the conv-basis method intro-duced by [LLS+24a] and a near-linear time algorithm proposed by [HJK+24] under the assumptions of uniform softmax column norms and sparsity.\nExperimental approaches involve modifying model architectures and optimizing implementa-tions to accelerate inference. Methods such as Mamba [GD23, DG24], Linearizing Transform-ers [ZBKR24, MVK+24], PolySketchFormer [ZHDK23, KMZ23], and various implementations of the Hopfield Model [HCW+24, HCL+24, WHHL24, XHH+24, HLSL24, WHL+24, HYW+23] aim to improve model performance and inference speed. Additionally, specific techniques like weight prun-ing [LLS+24b, LLSS24] have been developed to accelerate LLM generation. Some other techniques are introduced for efficient adaptation, such as LORA [HSW+22, ZL24, HSK+24] and prefix turning [LL21, LSSY24]. System-level optimizations, such as Flash Attention [DFE+22, Dao23, SBZ+24] and block-wise parallel decoding [SSU18], address bottlenecks in attention mechanisms and enhance"}, {"title": "B Preliminary", "content": "In Section B.1, we define some basic notation we will use. In Section B.2, we introduce the memory hierarchy we consider. In Section B.3, we state important facts related to fast matrix multiplication. In Section B.4, we define several intermediate functions which will arise in our algorithms."}, {"title": "B.1 Notations", "content": "For any positive integer $n$, we define $[n] := \\{1,2,...,n\\}$. For two same length vector $x$ and $y$, we use $\\langle x, y\\rangle$ to denote the inner product between $x$ and $y$, i.e., $\\langle x,y\\rangle = \\sum_{i=1}^{n} x_iy_i$. We use $\\circ$ to denote the Hadamard product i.e. the $(i, j)$-entry of $A\\circ B$ is $A_{i,j}B_{i,j}$. We use $x \\circ y$ to denote vector that $i$-th entry is $x_iy_i$. Let $\\mathbf{1}_n$ denote the length-$n$ all ones vector. It is not hard to see that $\\langle x \\circ y, \\mathbf{1}_n \\rangle = \\langle x, y \\rangle$. For a vector $x$, we use $x^{\\mathsf{T}}$ to denote the transpose of $x$. For a matrix $A$, we use $A^{\\mathsf{T}}$ to denote the transpose of matrix $A$. For a matrix $A$, we use $\\exp(A)$ to denote the matrix that $(i, j)$-th coordinate is $\\exp(A_{i,j})$.\nGiven a matrix $A \\in \\mathbb{R}^{n \\times m}$, we index an individual entry as $A[i,j]$. The $i$-th row is denoted $A[i]$ while the $j$-th column is denoted $A[*,j]$. $A[i_1 : i_2, j_1 : j_2]$ denotes a block of $A$ consisting of entries $(i, j)$ where $i \\in [i_1, i_2]$ and $j \\in [j_1, j_2]$. Given a block size $B$, the block $A[(i - 1) \\cdot B + 1 : i \\cdot B, (j - 1) \\cdot B + 1 : j \\cdot B]$ is denoted $A^{(B)}[i, j]$."}, {"title": "B.2 Memory Hierarchy", "content": "In this study, we consider a two-level memory hierarchy composed of a small but fast layer called the cache and a large, slower layer referred to as the memory. We assume that the memory has unlimited capacity, while the cache is constrained by a finite size $M$. Moreover, all computations are performed exclusively within the cache."}, {"title": "B.3 Matrix Multiplication", "content": "We define matrix multiplication notation and state some well-known facts here.\nDefinition B.1. Let $n_1, n_2, n_3$, denote any three positive integers. We use $T_{\\text{mat}}(n_1, n_2,n_3)$ to denote the time of multiplying an $n_1 \\times n_2$ matrix with another $n_2 \\times n_3$.\nThen, we introduce a well-known fact.\nFact B.2. Let $n_1, n_2, n_3$, denote any three positive integers. $T_{\\text{mat}}(n_1, n_2, n_3) = O(T_{\\text{mat}}(n_1, n_3, n_2)) = O(T_{\\text{mat}} (n_2, n_1, n_3)) = O(T_{\\text{mat}}(n_2, n_3, n_1)) = O(T_{\\text{mat}}(n_3, n_1, n_2)) = O(T_{\\text{mat}} (n_3, n_2, n_1))$."}, {"title": "B.4 Definitions of Intermediate Variables", "content": "We start by some definitions about $X \\in \\mathbb{R}^{d \\times d}$.\nDefinition B.3 (Definition 3.4 in [AS24a]). Let $A_1, A_2 \\in \\mathbb{R}^{n \\times d}$ be two matrices. Let $X \\in \\mathbb{R}^{d \\times d}$. Let us define function $A(X)$ to be:\n$A(X) := \\exp(A_1XA_2)$.\nDefinition B.4 (Definition 3.5 in [AS24a]). For $A(X) \\in \\mathbb{R}^{n \\times n}$ defined in Definition B.3, we define the softmax normalizing vector $l(X) \\in \\mathbb{R}^n$ to be\n$l(X) := \\frac{A(X) \\mathbf{1}_n}{\\langle A(X) \\mathbf{1}_n, \\mathbf{1}_n \\rangle}.$\nDefinition B.5 (Definition 3.6 in [AS24a]). Suppose that $l(X) \\in \\mathbb{R}^n$ is defined as in Definition B.4. Let $A(X) \\in \\mathbb{R}^{n \\times n}$ be defined as in Definition B.3. For a fixed $j_0 \\in [n]$, let us consider $f(X)_{*, j_0}$\n$f(X)_{*, j_0} := \\frac{l(X)^{-1}}{ \\langle l(X)^{-1} \\rangle_{j_0} } A(X)_{*, j_0}.$\nLet $f(X) \\in \\mathbb{R}^{n \\times n}$ denote the matrix where $j_0$-th row is $(f(X)_{*, j_0})^{\\mathsf{T}}$.\nFurthermore, the matrix form of $f(X)$ is\n$f(X) = \\text{diag}(l(X))A(X)$\nWe then define $h(Y)$ related to $Y \\in \\mathbb{R}^{d \\times d}$."}, {"title": "Definition B.6 (Definition 3.7 in [AS24a]).", "content": "For $A_3 \\in \\mathbb{R}^{n \\times d}$ and $Y \\in \\mathbb{R}^{d \\times d}$, we define $h(Y) \\in \\mathbb{R}^{n \\times d}$ as:\n$h(Y) := A_3 \\cdot Y.$\nLet us define the forward output matrix $O$.\nDefinition B.7. Let $f(X), h(Y)$ be defined in Definition B.5 and B.6. We define the output of attention as:\n$O := f(X) \\cdot h(Y)$\nwhere $O \\in \\mathbb{R}^{n \\times d}$ is the output matrix of attention forward computation.\nNow, we define $q$, which incorporates the information from upstream gradient.\nDefinition B.8 (Definition C.10 in [LSS+24]). Let $dO \\in \\mathbb{R}^{n \\times d}$ be the upstream gradient, the matrix resulting from the application of the chain rule. Define $h(Y) \\in \\mathbb{R}^{n \\times d}$ as in Definition B.6. We define $q(Y) \\in \\mathbb{R}^{n \\times n}$ as\n$q(Y) := dO \\cdot h(Y)^{\\mathsf{T}}.$\nThen we use $q(Y)_{j_0}$ to denote the $j_0$-th row of $q(Y) \\in \\mathbb{R}^{n \\times n}$.\nFinally, we define the gradient component matrix $p$.\nDefinition B.9 (Definition C.5 in [AS24a]). For every index $j_0 \\in [n]$, we define $p(X)_{j_0} \\in \\mathbb{R}^{n}$ as\n$p(X)_{j_0} := (\\text{diag}(f(X)_{*, j_0}) - f(X)_{*, j_0} f(X)_{*, j_0})q(Y)_{j_0}.$\nWe define $p(X) \\in \\mathbb{R}^{n \\times n}$ in the sense that $p(X)_{j_0}$ is the $j_0$-th row of $p(X)$. Additionally, $p(X)$ has matrix form as\n$p(X) = f(X) \\circ q(Y) - \\text{diag}((f(X) \\circ q(Y)) \\cdot \\mathbf{1}_n) f(X)$\n$\\ \\ \\ = f(X) \\circ q(Y) - \\text{diag}((O \\circ dO) \\cdot \\mathbf{1}_n) f(X)$\nwhere $f(X), O$ are defined in Definition B.5 and B.7, and $q(Y), dO$ are defined in Definition B.8."}, {"title": "CI/O Complexity Upper Bound for Small Cache", "content": "In this section, we prove the I/O complexity upper bound (Theorem C.12) for small cache case $M = o(d^2)$. Specifically, in Section C.1, we introduce an algorithm of attention gradient computation without cache to guide our algorithm design. Section C.2 presents algorithms and analyses for attention gradient computation in the small cache setting. Finally, Section C.3 provides the upper bound theorem for the small cache case."}, {"title": "C.1 Algorithm for Attention Backward Without Cache", "content": "Using results from [AS24a", "AS24a": "."}]}