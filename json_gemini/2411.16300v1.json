{"title": "BayLing 2: A Multilingual Large Language Model with Efficient Language Alignment", "authors": ["Shaolei Zhang", "Kehao Zhang", "Qingkai Fang", "Shoutao Guo", "Yan Zhou", "XiaoDong Liu", "Yang Feng"], "abstract": "Large language models (LLMs), with their powerful generative capabilities and vast knowledge, empower various tasks in everyday life. However, these abilities are primarily concentrated in high-resource languages, leaving low-resource languages with weaker generative capabilities and relatively limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore crucial for serving over 100 linguistic communities worldwide. An intuitive approach to enhance the multilingual capabilities would be to construct instruction data for various languages, but constructing instruction data for over 100 languages is prohibitively costly. In this paper, we introduce BayLing 2, which efficiently transfers generative capabilities and knowledge from high-resource languages to low-resource languages through language alignment. To achieve this, we constructed a dataset of 3.2 million instructions, comprising high-resource language instructions (Chinese and English) and cross-lingual instructions for 100+ languages and performed instruction tuning based on the dataset to facilitate the capability transfer between languages. Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B, and BayLing-3-8B, and conducted a comprehensive evaluation of BayLing. For multilingual translation across 100+ languages, BayLing shows superior performance compared to open-source models of similar scale. For multilingual knowledge and understanding benchmarks, BayLing achieves significant improvements across over 20 low-resource languages, demonstrating its capability of effective knowledge transfer from high-resource to low-resource languages. Furthermore, results on English benchmarks indicate that BayLing maintains high performance in high-resource languages while enhancing the performance in low-resource languages. Demo2, homepage\u00b3, code and models of BayLing are available.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of natural language processing (NLP) has witnessed a significant surge in the development and utilization of large language models (LLMs) [OpenAI, 2022, 2023]. Equipped with rich knowledge, strong generative capabilities, and diverse instruction-following abilities, LLMs empower various specific tasks such as translation, summarization, chat and question answering, seamlessly integratd into everyday life."}, {"title": "2 Related Work", "content": "Multilingual LLMs, with their capability to handle and produce content in multiple languages simultaneously, hold promise for serving diverse linguistic communities. Foundational models, such as Llama [Touvron et al., 2023], GPT-3 [Brown et al., 2020], PaLM [Chowdhery et al., 2022], OPT [Zhang et al., 2022] and GLM [Du et al., 2022], are pretrained on corpora sourced from the web and books, which often encompass multiple languages. However, the distribution of languages in these corpora is notably imbalanced. Specifically, a few high-resource languages dominate a significant portion of the corpus, while a vast number of low-resource languages occupy only a small fraction [Touvron et al., 2023]. This leads to performance variations across different languages [Ojo and Ogueji, 2023, Nguyen et al., 2023a]. Moreover, subsequent supervised fine-tuning on English-centric instruction data exacerbates the issue of language imbalance [Lai et al., 2023], rendering LLMs lower interactive capability with low-resource languages.\nCurrent approaches mainly fall into two categories: continual pretraining and supervised fine-tuning. With continual pretraining, some works focus on continuously pretraining foundational models using multilingual corpora to enhance their multilingual capabilities [Nguyen et al., 2023b, Lai et al., 2023, Ke et al., 2023, Gupta et al., 2023]. These approaches effectively supplements LLMs with multilingual knowledge and generation abilities. However, continual pretraining often relies on large amounts of multilingual data, and thereby the costs associated with data collection and training are significant [Nguyen et al., 2023b, Liu et al., 2024]. Moreover, there is a risk of catastrophic forgetting with continual pretraining, which may compromise the performance of the foundational model on high-resource languages [Li et al., 2024]. Additionally, since the pretraining corpora of foundational models are often close-sourced, it is challenging to maintain the same distribution between the continual pretraining data and the pretraining data, which may lead to conflicting knowledge and potential hallucinations.\nFor supervised fine-tuning, existing methods attempt to manually annotate multilingual instructions to activate LLMs' ability for multilingual interaction [Eisenschlos et al., 2020, Alabi et al., 2022, Lai et al., 2023, Wang et al., 2024, Shaham et al., 2024]. This approach often relies on manually annotation and overlooks leveraging the capabilities of foundational models in high-resource languages as well as the generalization ability of LLMs. To address this, BayLing 2 attempts to enhance the multilingual capabilities of LLMs in a more efficient manner. The instruction dataset of BayLing 2 comprises instructions in both high-resource languages and cross-lingual instructions. The instructions in high-resource languages are designed to activate LLMs' instruction-following capability, while cross-lingual instructions aim to facilitate multilingual alignment of LLMs, thereby transferring"}, {"title": "3 BayLing 2", "content": "We introduce BayLing 2, a LLM equipped with enhanced multilingual capabilities through efficient language alignment. Building upon open-source foundational models, BayLing 2 endeavors to explore an efficient and cost-effective approach to enhance the multilingual capabilities, thereby addressing the demands for multilingual interaction."}, {"title": "3.1 Multilingual Alignments with Cross-lingual Tasks", "content": "During the pre-training stage, the distribution of languages in the corpus is highly imbalanced. For instance, English, being a high-resource language, accounts for over 90% of the corpus, while low-resource languages such as Sinhalese, Marathi, and Macedonian collectively comprise less than 1% of the corpus. Naturally, foundational models trained on such language-imbalanced corpus exhibit superior performance on English compared to low-resource languages. Previous studies have often noted that due to the generalization capability, LLMs also demonstrate a certain advantage on those languages within the same language family as English. Naturally, aligning low-resource languages with high-resource languages already mastered by LLMs allows us to transfer the knowledge and generation capabilities of LLMs from high-resource languages to other languages efficiently, thereby enhancing the multilingual capabilities of LLMs.\nWe employ cross-lingual tasks to align low-resource languages with high-resource languages, thereby achieving multilingual alignment. Fortunately, translation tasks naturally serve as well-defined cross-lingual tasks, demanding outputs that maintain consistent meanings with inputs while differing in language. More importantly, translation tasks boast abundant high-quality parallel corpus across diverse domains, thus laying the groundwork for the efficient achievement of language alignment."}, {"title": "3.2 Training", "content": "Using Llama-2-7B-Chat, Llama-2-13B-Chat and Llama-3-8B-Instruct as foundational models, We fine-tune BayLing-2-7B, BayLing-2-13B and BayLing-3-8B respectively on the instruction dataset proposed in Section 3.1. We fine-tune BayLing 2 on 8 NVIDIA A800 80G GPUs for 3 epochs, using a global batch size of 128, learning rate of 2e-5 and weight decay of 0.0. Note that we apply learning rate of 2e-6 for BayLing-3-8B. We employ DeepSpeed [Rasley et al., 2020] and Gradient Checkpointing [Chen et al., 2016] techniques to optimize memory consumption. The training loss curve of BayLing-3-8B is depicted in Figure 5."}, {"title": "4 Evaluation", "content": "In this section, we comprehensively evaluate the performance of BayLing-2-7B, BayLing-2-13B and BayLing-3-8B on multilingual tasks and general tasks respectively."}, {"title": "4.1 Multilingual Capability", "content": "BayLing's multilingual capabilities are primarily manifested in two aspects: multilingual translation and multilingual interaction. Multilingual translation aims to accomplish translation between different languages, which can be utilized to assess the language alignment within LLMs as well as the comprehension and generation capabilities across different languages. Multilingual interaction involves multitask language understanding using multiple languages, which can be employed to evaluate the multilingual knowledge and reasoning abilities of LLMs."}, {"title": "4.1.1 Multilingual Translation", "content": "We employ multilingual translation to assess the multilingual alignment within LLMs, which entails producing outputs that retain the same meaning but in different languages. We conduct evaluation on the Flores-101 and WMT22 benchmarks. For metrics, BLEU (sacrebleu) [Post, 2018] and COMET [Rei et al., 2022] are used to assess the quality of LLMs' translation. BLEU score measures the statistical similarity based on n-gram accuracy, COMET score measures the semantic similarity using cross-lingual pre-trained models, which is currently regarded as the most human-aligned evaluation metric for translation tasks.\nFlores-101 Flores-101 benchmark encompasses 101 languages from around the world, and the sentences is sourced from various domains, including news, travel guides and books. Due to the rarity of some low-resource languages, LLMs may suffer from off-target issues. To address this, we adopt a 1-shot setting (i.e., randomly selecting an example from the dev set) to help LLMs follow the target language through in-context learning. We compare BayLing models with their corresponding foundational models, and the results are shown in Table 1.\nThe results in Table 1 indicate that BayLing achieves better performance in most translation directions between 100 languages and Chinese/English. Specifically, compared to the foundation LLMs Llama-1-7B and Llama-2-7B-Chat, which have relatively weak multilingual capabilities, BayLing effectively scales their language understanding and generation capabilities to over 100 languages, leading to significantly improved translation performance. Furthermore, Figures 6(a), 6(b), 7(a) and 7(b) illustrate the specific BLEU score improvements achieved by BayLing across 100 languages. BayLing consistently delivers the highest translation quality for most languages, particularly in translation directions to low-resource languages. This demonstrates BayLing's potential to enhance LLM in serving such low-source linguistic communities."}, {"title": "WMT22", "content": "WMT22 benchmark6 encompass is used to evaluate high-resource multilingual translation performance, including translation directions of Chinese\u21d4English, German\u21d4English, Czech\u21d4English, Japanese\u21d4English, Russian\u21d4English, and Ukrainian\u21d4English. We compared BayLing with the best closed-sourced and open-sourced models, including GPT-47 [OpenAI, 2023], GPT-3.5-turbo [OpenAI, 2022], Google Translate, Llama[Touvron et al., 2023] and Vicuna [Chiang et al., 2023].\nThe translation results on WMT22 are shown in Table 2, where the results illustrate the superior multilingual translation capabilities of BayLing models. Among the open-sourced models, BayLing achieves the highest overall translation performance, coming remarkably close to the performance levels of closed-sourced models like GPT-4 and GPT-3.5-turbo. This exceptional performance can be attributed to BayLing's improved language alignment, which enables it to produce more accurate and reliable translations across different languages. In particular, for the Zh\u21d2En translation, BayLing-3-8B achieve a COMET score of 79.75 on Zh\u21d2En and 85.75 on En\u21d2Zh, which is very close to the performance of Google Translate.\nImproving Mulitlingual Generation Capabilities We have observed that foundational mod-els often exhibit off-target issues when generating low-resource languages. In contrast, BayLing demonstrates significantly enhanced multilingual generation capabilities, consistently improving translation performance from English to other languages. This indicates that BayLing can activate the multilingual generation abilities of LLMs solely through cross-lingual translation data, without the need for extensive multilingual instruction data. This finding is crucial for efficiently enhancing the multilingual capabilities of LLMs, as it is nearly impossible to collect instruction data covering more than 100 languages while multilingual translation data is relatively abundant and easier to"}, {"title": "4.1.2 Multilingual Multi-task Evaluation", "content": "We assessed the multilingual performance of BayLing using several benchmarks. All evaluations were conducted through the Language Model Evaluation Harness10 [Gao et al., 2023], an open-source, unified framework designed to assess LLMs across a wide variety of evaluation tasks. Each result was obtained in a zero-shot setting. The models Llama-2-7B, Llama-2-7B-Chat, Llama-3-8B-Instruct, Vicuna-7B and Mistral-7B served as baselines for comparison. The multilingual benchmarks are discribed as follows.\nBelebele [Bandarkar et al., 2023] Belebele is a multiple-choice machine reading comprehension benchmark, which evaluates mono- and multi-lingual models across different resource levels with rigorously checked questions. Each question has four multiple-choice answers and is linked to a short passage from the FLORES-200 dataset.\nMultilingual HellaSwag [Dac Lai et al., 2023] Multilingual HellaSwag is a multilingual adaptation of HellaSwag, a benchmark dataset designed to assess commonsense inference. Despite its questions being straightforward for humans, state-of-the-art models struggle with it, highlighting the challenges in AI comprehension.\nXNLI [Conneau et al., 2018] XNLI is an evaluation dataset created by extending the MultiNLI corpus to multiple languages, including low-resource ones like Swahili and Urdu. It serves as a standardized benchmark for assessing cross-lingual sentence understanding, aiming to foster research in this area.\nMultilingual ARC [Dac Lai et al., 2023] The Multilingual ARC, a multilingual extension of ARC [Clark et al., 2018], encompasses science examination queries, stratified into a Challenge Set comprising intricate questions and an Easy Set. All queries adhere to a multiple-choice structure.\nFigure 8(a), 8(b), 8(c), 8(d) provide detailed illustrations of the experimental outcomes on the Belebele, Multilingual HellaSwag, XNLI, Multilingual ARC benchmarks across several low-resource languages. The BayLing-2-7B and BayLing-3-8B models demonstrate notable performance benefits. Among these, BayLing-3-8B consistently delivers the best results across most of the low-resource languages evaluated. Meanwhile, BayLing-2-7B outperforms other 7B models in most of these languages. Remarkably, BayLing-2-7B even surpasses the Llama-3-8B-Instruct model in the Swati language subset on Belebele benchmark (belebele_ssw_Latn).\nNote that BayLing's training data does not include instruction data for these low-resource languages but only cross-lingual instructions between these low-resource languages and Chinese/English. Therefore, the performance improvements observed in these low-resource languages demonstrate that BayLing effectively transfers knowledge and understanding capability from high-resource languages to low-resource ones through language alignment. Overall, BayLing's approach of leveraging language alignment for capability transfer offers an efficient solution to enhance LLM's performance in low-resource languages."}, {"title": "4.2 General Capability", "content": "Furthermore, we evaluated the general capability of BayLing on the following benchmarks employing the same settings as described in section 4.1.2.\nCMMLU [Li et al., 2023] CMMLU serves as a specialized evaluation benchmark tailored to assess the knowledge and reasoning capacities of LLMs in within the context of Chinese language and culture. Encompassing a wide range of subjects, CMMLU includes 67 topics, which vary from basic to advanced professional levels.\nC-Eval [Huang et al., 2023] C-Eval is an exhaustive Chinese evaluation suite designed for foundation models. It features a total of 13,948 multiple-choice questions, covering 52 distinct disciplines across four levels of difficulty.\nArabic EXAMS [Hardalov et al., 2020] The Arabic EXAMS comprise the Arabic segment of EXAMS, a resource dedicated to multilingual high school examination questions. This section includes five subjects: Islamic Studies, Biology, Physics, General Science, and Social Studies.\nANLI [Nie et al., 2020] Adversarial NLI (ANLI) is a dataset assembled through an iterative adversarial procedure involving both human and model participation. It is structured into three rounds, each escalating in difficulty and complexity. Additionally, each question-answer pair in the dataset is supplemented with explanations provided by the annotators.\nCB [De Marneffe et al., 2019, Wang et al., 2019] CB (CommitmentBank) is a corpus featuring texts with embedded clauses evaluated for the author's commitment to their truth. This corpus is used in a three-class textual entailment task. Examples are organized with a premise and a corresponding hypothesis extracted from the embedded clause.\nGLUE [Wang et al., 2018] The GLUE benchmark is a benchmark for evaluating natural language understanding systems. It consists of nine language understanding tasks and a diagnostic dataset for assessing model performance across linguistic phenomena.\nACLUE [Zhang and Li, 2023] ACLUE is a benchmark designed to evaluate large language models' comprehension of ancient Chinese, featuring 15 tasks across multiple domains. The questions, covering historical periods from the Xia to the Ming dynasty, are presented in a multiple-choice format.\nGSM8K [Cobbe et al., 2021] GSM8K is a benchmark used to evaluate the math capability of LLMs as described in section 4.1.2.\nFigure 9 illustrates the performance of BayLing on the specified benchmarks. BayLing-2-7B and BayLing-3-8B demonstrate exceptional performance across several benchmarks. Notably, BayLing-3-8B outperforms all other models, achieving a score of 0.8214 on the CommitmentBank Benchmark. BayLing-2-7B attains the highest performance on GLUE and GSM8K benchmarks. Despite not being specifically trained for it, BayLing-2-7B and BayLing-3-8B still deliver comparable performances on other benchmarks when compared to other models. Overall, BayLing enhances the multilingual capabilities of LLMs, especially in low-resource languages, without significantly impacting the per-"}, {"title": "4.3 Effect of Language Alignment", "content": "To validate the effect of language alignment brought by cross-lingual instructions, we conducted an ablation study on cross-lingual instructions. Specifically, we removed all cross-lingual instructions from the training data, denoting this variant as BayLing-3-8B (w/o cross-lingual instructions).\nImproving Performance of Low-resource Languages Figure 10 compares the performance of BayLing-3-8B and BayLing-3-8B (w/o cross-lingual instructions) on the multilingual benchmark Belebele. The results show that cross-lingual instructions significantly enhance LLM performance in low-resource languages. This indicates that cross-lingual instructions successfully help LLM achieve language alignment, thereby transferring knowledge and comprehension from high-resource languages to low-resource ones. When removing all cross-lingual instructions, the performance of LLMs in low-resource languages is adversely affected due to catastrophic forgetting. Therefore, involving cross-lingual instructions in supervised fine-tuning is both efficient and crucial for improving the multilingual capabilities of LLMs.\nAvoiding Inter-language Conflicts Figure 11 compares BayLing-3-8B and BayLing-3-8B (w/o cross-lingual instructions) on the Chinese/English benchmark. When removing cross-lingual instructions, we observed a significant performance decline in Chinese benchmark, indicating LLM will suffer from conflicts between Chinese and English instructions. The presence of numerous cross-lingual instructions between Chinese and English largely prevents these conflicts. Therefore, to simultaneously enhance LLM performance across multiple languages, introducing cross-lingual instructions is an effective way to avoid inter-language conflicts."}, {"title": "5 Conclusion", "content": "In this study, we develop BayLing 2, which enhances LLM's multilingual capabilities through language alignment. Adhering to an efficiency-focused approach, BayLing 2 transfers knowledge and generative abilities from high-resource languages to low-resource languages within LLM via language alignment. Comprehensive evaluation results demonstrate that BayLing 2 achieves outstanding translation performance across over 100 languages, possesses superior multilingual knowledge and understanding capability, and maintains robust proficiency in high-resource languages of Chinese and English."}]}