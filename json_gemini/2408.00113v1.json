{"title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models", "authors": ["Adam Karvonen", "Benjamin Wright", "Can Rager", "Rico Angell", "Jannik Brinkmann", "Logan Smith", "Claudio Mayrink Verdun", "David Bau", "Samuel Marks"], "abstract": "What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features that we expect good SAEs to recover. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on chess and Othello transcripts. These settings carry natural collections of interpretable features-for example, \u201cthere is a knight on F3\"-which we leverage into supervised metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, $p$-annealing, which improves performance on prior unsupervised metrics as well as our new metrics.", "sections": [{"title": "1. Introduction", "content": "Mechanistic interpretability aims to reverse engineer neural networks into human-understandable components. What, however, should these components be? Recent work has applied Sparse Autoencoders (SAEs) (Bricken et al., 2023; Cunningham et al., 2023), a scalable unsupervised learning method inspired by sparse dictionary learning to find a disentangled representation of language model (LM) internals. However, measuring progress in training SAEs is challenging because we do not know what a gold-standard dictionary would look like, as it is difficult to anticipate which ground-truth features underlie model cognition. Prior work has either attempted to measure SAE quality in toy synthetic settings (Sharkey et al., 2023) or relied on various proxies such as sparsity, fidelity of the reconstruction, and LM-assisted autointerpretability (Bills et al., 2023).\nIn this work, we explore a setting that lies between toy synthetic data (where all ground-truth features are known; cf. Elhage et al. (2022)) and natural language: LMs trained on board game transcripts. This setting allows us to formally specify natural categories of interpretable features, e.g., \"there is a knight on e3\" or \"the bishop on f5 is pinned.\" We leverage this to introduce two novel metrics for how much of a model's knowledge an SAEs has captured:\n\u2022 Board reconstruction. Can we reconstruct the state of the game board by interpreting each feature as a classifier for some board configuration?\n\u2022 Coverage. Out of a catalog of researcher-specified candidate features, how many of these candidate features actually appear in the SAE?\nThese metrics carry the limitation that they are sensitive to researcher preconceptions. Nevertheless, we show that they provide a useful new signal of SAE quality.\nAdditionally, we introduce $p$-annealing, a novel technique for training SAEs. When training an SAE with $p$-annealing, we use an $L_p$-norm-based sparsity penalty with $p$ ranging from $p = 1$ at the beginning of training (corresponding to a convex minimization problem) to some $p < 1$ (a non-convex objective) by the end of training. We demonstrate that $p$-annealing improves over prior methods, giving performance on par with the more compute-intensive Gated SAEs from Rajamanoharan et al. (2024), as measured both by prior metrics and our new metrics.\nOverall, our main contributions are as follows:\n1. We train and open-source over 500 SAEs trained on chess and Othello models each.\n2. We introduce two new metrics for measuring the quality of SAEs.\nCode, models, and data are available at https://github.com/adamkarvonen/SAE_BoardGameEval"}, {"title": "2. Background", "content": null}, {"title": "2.1. Language models for Othello and chess", "content": "In this work, we make use of LMs trained to autoregressively predict transcripts of chess and Othello games. We emphasize that these transcripts only give lists of moves in a standard notation and do not directly expose the board state. Based on behavioral evidence (the high accuracy of the LMs for predicting legal moves) and prior studies of LM representations (Li et al., 2023a; Nanda et al., 2023; Karvonen, 2024) we infer that the LMs internally model the board state, making them a good testbed setting for studying LM representations.\nOthello. Othello is a two-player strategy board game played on an 8x8 grid, with players using black and white discs. Players take turns placing discs on the board, capturing their opponent's discs by bracketing them between their own, causing the captured discs to turn their color. The goal is to have more discs turned to display your color at the end of the game. The game ends if every square on the board is covered or either player cannot make a move.\nIn our experiments, we use an 8-layer GPT model with 8 attention heads and a $n = 512$ dimensional hidden space, as provided by Li et al. (2023a). This model had no prior knowledge of the game or its rules and was trained from scratch on 20 million game transcripts, where each token in the corpus represents a tile on which players place their discs. The game transcripts were synthetically generated by uniformly sampling from the Othello game tree. Thus, the data distribution captures valid move sequences rather than strategic depth. For this model, Li et al. (2023a) demonstrated the emergence of a world model\u2014an internal representation of the correct board state allowing it to predict the next move-that can be extracted from the model activations using a non-linear probe. Nanda et al. (2023) extended this finding, showing that a similar internal representation could be extracted using linear probes, supporting the linear representation hypothesis (Mikolov et al., 2013).\nChess. Othello makes a natural testbed for studying emergent internal representations since the game tree is far too large to memorize. However, the rules and state are not particularly complex. Therefore, we also consider a language model trained on chess game transcripts with identical architecture, provided by Karvonen (2024). The model again had no prior knowledge of chess and was trained from scratch on 16 million human games from the Lichess chess game database (Lichess, 2024). The input to the model is a string in the Portable Game Notation (PGN) format (e.g., \"1. e4"}, {"title": "2.2. Sparse autoencoders", "content": "Given a dataset $D$ of vectors $x \\in \\mathbb{R}^d$, a sparse autoencoder (SAE) is trained to produce an approximation\n$x \\approx \\sum f_i(x)d_i + b$ \nHere, the feature vectors $d_i \\in \\mathbb{R}^d$ are unit vectors, the feature activations $f_i(x) \\geq 0$ are a sparse set of coefficients, and $b \\in \\mathbb{R}^d$ is a bias term. Concretely, an SAE is a neural network with an encoder-decoder architecture, where the encoder maps $x$ to the vector $f = [f_1(x)...f_{d_{SAE}}(x)]$ of feature activations, and the decoder maps $f$ to an approximate reconstruction of $x$.\nIn this paper, we train SAEs on datasets consisting of activations extracted from the residual stream after the sixth layer for both the chess and Othello models. At these layers, linear probes trained with logistic regression were accurate for classifying a variety of properties of the game board (Karvonen, 2024; Nanda et al., 2023). For training SAEs, we employ a variety of SAE architectures and training algorithms, as detailed in Section 4."}, {"title": "3. Measuring autoencoder quality for chess and Othello models", "content": "Many of the features learned by our SAEs reflect uninteresting, surface-level properties of the input, such as the presence of certain tokens. However, upon inspection, we additionally find many SAE features which seem to reflect a latent model of the board state, e.g., features that reflect the presence of certain pieces on particular squares, squares that are legal to play on, and strategy-relevant properties like the presence of a pin in chess (Figure 1).\nFortunately, in the setting of board games, we can formally specify certain classes of these interesting features, allowing us to more rigorously detect them and use them to understand our SAEs. In Section 3.1, we specify certain classes of interesting game board properties. Then, in Section 3.2, we leverage these classes into two metrics of SAE quality."}, {"title": "3.1. Board state properties in chess and Othello models", "content": "We formalize a board state property (BSP) to be a function $g: {game board} \\rightarrow {0,1}$. In this work, we will consider the following interpretable classes of BSPs:"}, {"title": "3.2. Measuring SAE quality with board state properties", "content": "In this section, we introduce two metrics of SAE quality: coverage and board reconstruction.\nCoverage. Given a collection $G$ of BSPs, our coverage metric quantifies the extent to which an SAE has identified features that coincide with the BSPs in $G$. In more detail, suppose that $f_i$ is an SAE feature and $t \\in [0, 1]$ is a threshold, we define the function\n$\\phi_{f_i,t}(x) = I [f_i(x) > t \\cdot f_{i}^{max}]$\nwhere $f_{i}^{max}$ is (an empirical estimate of) $max_{x \\sim D} f_i(x)$, the maximum value that $f_i$ takes over the dataset $D$ of activations extracted from our model, and $I$ is the indicator function. We interpret $\\phi_{f_i,t}$ as a binary classifier; intuitively, it corresponds to binarizing the activations of $f_i$ into \"on\" vs. \"off\" at some fraction $t$ of the maximum value of $f_i$ on $D$. Given some BSP $g \\in G$, let $F_1(\\phi_{f_i,t}; g) \\in [0,1]$ denote the F1-score for $\\phi_{f_i,t}$ classifying $g$. Then we define the coverage of an SAE with features ${f_i}$ relative to a set of BSPs $G$ to be\n$Cov({f_i}, G) := \\frac{1}{|G|} \\sum_{g \\in G} max_t max_{f_i} F_1(\\phi_{f_i,t}; g)$.\nIn other words, we take, for each $g \\in G$, the $F_1$-score of the feature that best serves as a classifier for $g$, and then take the mean of these maximal $F_1$-scores. An SAE receives a coverage score of 1 if, for each BSP $g \\in G$, it has some feature that is a perfect classifier for $g$. Since Cov depends on the choice of threshold $t$, we sweep over $t \\in {0, 0.1, 0.2, ..., 0.9}$ and take the best coverage score; typically this best $t$ is in {0, 0.1, 0.2}.\nBoard reconstruction. Again, let $G$ be a set of BSPs. Intuitively, the idea of our board reconstruction metric is that, for a sufficiently good SAE, there should be a simple, human-interpretable way to recover the state of the board from the profile of feature activations ${f_i(x)}$ on an activation $x \\in \\mathbb{R}^d$. Here, the activation $x$ was extracted after the post-MLP residual connection in layer 6.\nWe will base our board reconstruction metric around the following human-interpretable way of recovering a board state from a feature activation profile; we emphasize that different ways of recovering boards from feature activations"}, {"title": "4. Training methodologies for SAES", "content": "In our experiments, we investigate four methods for training SAEs, as explained in this section. These are given by two autoencoder architectures and two training methodologies-one with $p$-annealing and one without $p$-annealing-for each architecture. Our SAEs are available at https://huggingface.co/adamkarvonen/chess_saes/tree/main (chess) and https://huggingface.co/adamkarvonen/othello_saes/tree/main (Othello)."}, {"title": "4.1. Standard SAES", "content": "Let $n$ be the dimension of the model's residual stream activations that are input to the autoencoder, $m$ the autoencoder hidden dimension, and $s$ the dataset size. Our baseline \"standard\" SAE architecture, as introduced in Bricken et al. (2023) is defined by encoder weights $W_e \\in \\mathbb{R}^{m \\times n}$, decoder weights $W_d \\in \\mathbb{R}^{n \\times m}$ with columns constrained to have a $L_2$-norm of 1, and biases $b_e \\in \\mathbb{R}^m, b_d \\in \\mathbb{R}^n$. Given an input $x \\in \\mathbb{R}$, the SAE computes\n$f(x) = ReLU(W_e(x \u2013 b_d) + b_e)$\n$\\hat{x} = W_d f(x) + b_d$\nwhere $f(x)$ is the vector of feature activations, and $\\hat{x}$ is the reconstruction.\nFor a standard SAE, our baseline training method is as implemented in the open-source dictionary-learning"}, {"title": "4.2. Gated SAES", "content": "The $L_1$ penalty used in the original training method encourages feature activations to be smaller than they would be for optimal reconstruction (Wright & Sharkey, 2024). To address this, Rajamanoharan et al. (2024) introduced a modification to the original SAE architecture that separates the selection of dictionary elements to use in a reconstruction and estimating the coefficients of these dictionary elements. This results in the following gated architecture:\n$\\pi_{gate} (x) := W_{gate} (x - b_d) + b_{gate}$\n$f(x) := I [\\pi_{gate} (x) > 0] ReLU(W_{mag} (x - b_d) + b_{mag})$\n$\\hat{x}(f(x)) = W_d f(x) + b_d$\nwhere $I[\\cdot > 0]$ is the Heaviside step function and $\\odot$ denotes elementwise multiplication. Then, the loss function uses $\\hat{x}_{frozen}$, a frozen copy of the decoder:\n$L_{gated} := E_{x \\sim D_{train}} [||x \u2212 \\hat{x}(f(x))||^2_2 + \\lambda||ReLU(\\pi_{gate} (x))||_1 + ||x \u2212 \\hat{x}_{frozen} (ReLU(\\pi_{gate} (x))||^2_2]$"}, {"title": "4.3. p-Annealing", "content": "Fundamentally, an $L_1$ penalty has been used to induce sparsity in SAE features because it serves as a convex relaxation of the true sparsity measure, the $L_0$-norm. The $L_1$-norm is the convex hull of the $L_0$-norm, making it a tractable alternative for promoting sparsity (Wright & Ma, 2022). However, the proxy loss function is not the same as directly optimizing for sparsity, leading to issues such as feature shrinkage (Wright & Sharkey, 2024) and potentially less sparse learned features. Unfortunately, the $L_0$-norm is non-differentiable and directly minimizing it is an NP-hard problem (Natarajan, 1995; Davis et al., 1997), rendering it impractical for training.\nIn this work, we propose the use of nonconvex $L_p$-minimization, with $p < 1$, as an alternative to the standard $L_1$ minimization in sparse autoencoders (SAEs). This approach has been successfully employed in compressive sensing and sparse recovery to achieve even sparser representations (Chartrand, 2007; Wen et al., 2015; Yang et al., 2018; Wang et al., 2011). To perform this optimization, we introduce a method called $p$-annealing for training SAEs, based on the compressive sensing technique called"}, {"title": "5. Results", "content": "In this section, we explore the performance of SAEs applied to language models trained on Othello and chess. Consistent with Nanda et al. (2023), we find that interpretable SAE features typically track properties relative to the player whose turn it is (e.g. \"my king is pinned\" rather than \"the white king is pinned\"). To side-step subtleties arising from this, we only extract our activations from the token immediately preceding white's move. Specifically, we consider SAEs trained on the residual stream activations after the sixth"}, {"title": "6. Limitations", "content": "The proposed metrics for board reconstruction and coverage provide a more objective evaluation of SAE quality than previous subjective methods. Nevertheless, these metrics exhibit several limitations. Primarily, their applicability is confined to the chess and Othello domains, raising concerns about their generalizability to other domains or different models. Additionally, the set of BSPs that underpin these metrics is determined by researchers based on their domain knowledge. This approach may not encompass all pertinent features or strategic concepts, thus potentially overlooking essential aspects of model evaluation. Developing comparable objective metrics for other domains, such as natural language processing, remains a significant challenge. Moreover, our current focus is on evaluating the quality of SAEs in terms of their ability to capture internal representations of the model. However, this does not directly address how these learned features could be utilized for downstream interpretability tasks."}, {"title": "7. Related work", "content": "Sparse dictionary learning. Since the nineties, dictionary learning (Elad, 2010; Dumitrescu & Irofti, 2018), sparse regression (Foucart & Rauhut, 2013), and later, sparse autoencoders (Ng et al., 2011) have been extensively studied"}, {"title": "D. Model Internal Board State Representation", "content": null}, {"title": "D.1. Othello Models", "content": "Previous research of Othello-playing language models found that the model learned a nonlinear model of the board state (Li et al., 2023a). Further investigation found a closely related linear representation of the board when probing for \"my color\" vs. \"opponent's color\u201d rather than white vs. black (Nanda et al., 2023). Based on these findings, when measuring the state of the board in Othello, we represent squares as (mine, yours) rather than (white, black)."}, {"title": "D.2. Chess Models", "content": "Similar to Othello models, prior studies of chess-playing language models found the same property, where linear probes were only successful on the objective of the (mine, yours) representation and were unsuccessful on the (white, black) representation (Karvonen, 2024). They measured board state at the location of every period in the Portable Game Notation (PGN) string, which indicates that it is white's turn to move and maintain the (mine, yours) objective. Some characters in the PGN string contain little board state information as measured by linear probes, and there is not a clear ground truth board state part way through a move (e.g., the \"f\u201d in \u201cNf3\u201d). We follow these findings and measure the board state at every period in the PGN string.\nWhen measuring chess piece locations, we do not measure pieces on their initial starting location, as this correlates with position in the PGN string. An SAE trained on residual stream activations after the first layer of the chess model (which contains very little board state information as measured by linear probes) obtains a board reconstruction $F_1$-score of 0.01 in this setting. If we also measure pieces on their initial starting location, the layer 1 SAE's $F_1$-score increases to 0.52, as the board can be mostly reconstructed in early game positions purely from the token's location in the PGN string. Masking the initial board state and blank squares decreases the $F_1$-score of the linear probe from 0.99 to 0.98."}]}