{"title": "Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models", "authors": ["Adam Karvonen", "Benjamin Wright", "Can Rager", "Rico Angell", "Jannik Brinkmann", "Logan Smith", "Claudio Mayrink Verdun", "David Bau", "Samuel Marks"], "abstract": "What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features that we expect good SAEs to recover. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on chess and Othello transcripts. These settings carry natural collections of interpretable features-for example, \u201cthere is a knight on F3\"-which we leverage into supervised metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, p-annealing, which improves performance on prior unsupervised metrics as well as our new metrics.\"", "sections": [{"title": "1. Introduction", "content": "Mechanistic interpretability aims to reverse engineer neural networks into human-understandable components. What, however, should these components be? Recent work has applied Sparse Autoencoders (SAEs) (Bricken et al., 2023; Cunningham et al., 2023), a scalable unsupervised learning method inspired by sparse dictionary learning to find a disentangled representation of language model (LM) internals. However, measuring progress in training SAEs is challenging because we do not know what a gold-standard dictionary would look like, as it is difficult to anticipate which ground-truth features underlie model cognition. Prior work has either attempted to measure SAE quality in toy synthetic settings (Sharkey et al., 2023) or relied on various proxies such as sparsity, fidelity of the reconstruction, and LM-assisted autointerpretability (Bills et al., 2023).\nIn this work, we explore a setting that lies between toy synthetic data (where all ground-truth features are known; cf. Elhage et al. (2022)) and natural language: LMs trained on board game transcripts. This setting allows us to formally specify natural categories of interpretable features, e.g., \"there is a knight on e3\" or \"the bishop on f5 is pinned.\" We leverage this to introduce two novel metrics for how much of a model's knowledge an SAEs has captured:\n\u2022 Board reconstruction. Can we reconstruct the state of the game board by interpreting each feature as a classifier for some board configuration?\n\u2022 Coverage. Out of a catalog of researcher-specified candidate features, how many of these candidate features actually appear in the SAE?\nThese metrics carry the limitation that they are sensitive to researcher preconceptions. Nevertheless, we show that they provide a useful new signal of SAE quality.\nAdditionally, we introduce p-annealing, a novel technique for training SAEs. When training an SAE with p-annealing, we use an Lp-norm-based sparsity penalty with p ranging from p = 1 at the beginning of training (corresponding to a convex minimization problem) to some p < 1 (a non-convex objective) by the end of training. We demonstrate that p-annealing improves over prior methods, giving performance on par with the more compute-intensive Gated SAEs from Rajamanoharan et al. (2024), as measured both by prior metrics and our new metrics.\nOverall, our main contributions are as follows:\n1. We train and open-source over 500 SAEs trained on chess and Othello models each.\n2. We introduce two new metrics for measuring the quality of SAEs.\n3. We introduce p-annealing, a novel technique for training SAEs that improves on prior techniques."}, {"title": "2. Background", "content": "2.1. Language models for Othello and chess\nIn this work, we make use of LMs trained to autoregressively predict transcripts of chess and Othello games. We emphasize that these transcripts only give lists of moves in a standard notation and do not directly expose the board state. Based on behavioral evidence (the high accuracy of the LMs for predicting legal moves) and prior studies of LM representations (Li et al., 2023a; Nanda et al., 2023; Karvonen, 2024) we infer that the LMs internally model the board state, making them a good testbed setting for studying LM representations.\nOthello. Othello is a two-player strategy board game played on an 8x8 grid, with players using black and white discs. Players take turns placing discs on the board, capturing their opponent's discs by bracketing them between their own, causing the captured discs to turn their color. The goal is to have more discs turned to display your color at the end of the game. The game ends if every square on the board is covered or either player cannot make a move.\nIn our experiments, we use an 8-layer GPT model with 8 attention heads and a n = 512 dimensional hidden space, as provided by Li et al. (2023a). This model had no prior knowledge of the game or its rules and was trained from scratch on 20 million game transcripts, where each token in the corpus represents a tile on which players place their discs. The game transcripts were synthetically generated by uniformly sampling from the Othello game tree. Thus, the data distribution captures valid move sequences rather than strategic depth. For this model, Li et al. (2023a) demonstrated the emergence of a world model\u2014an internal representation of the correct board state allowing it to predict the next move-that can be extracted from the model activations using a non-linear probe. Nanda et al. (2023) extended this finding, showing that a similar internal representation could be extracted using linear probes, supporting the linear representation hypothesis (Mikolov et al., 2013).\nChess. Othello makes a natural testbed for studying emergent internal representations since the game tree is far too large to memorize. However, the rules and state are not particularly complex. Therefore, we also consider a language model trained on chess game transcripts with identical architecture, provided by Karvonen (2024). The model again had no prior knowledge of chess and was trained from scratch on 16 million human games from the Lichess chess game database (Lichess, 2024). The input to the model is a string in the Portable Game Notation (PGN) format (e.g., \"1. e4\n2. e5 2.Nf3 ...\"). The model predicts a legal move in 99.8% of cases and, similar to Othello, it has an internal representation of the board state that can be extracted from the internal activations using a linear probe (Karvonen, 2024).\n2.2. Sparse autoencoders\nGiven a dataset D of vectors x \u2208 \u211d\u1d48, a sparse autoencoder (SAE) is trained to produce an approximation\n$x \\approx \\sum_i f_i(x)d_i + b$\n(1)\nas a sparse linear combination of features. Here, the feature vectors di \u2208 \u211d\u1d48 are unit vectors, the feature activations fi(x) \u2265 0 are a sparse set of coefficients, and b\u2208 \u211d\u1d48 is a bias term. Concretely, an SAE is a neural network with an encoder-decoder architecture, where the encoder maps x to the vector f = [f\u2081(x) ... f_{dSAE}(x)] of feature activations, and the decoder maps f to an approximate reconstruction of x.\nIn this paper, we train SAEs on datasets consisting of activations extracted from the residual stream after the sixth layer for both the chess and Othello models. At these layers, linear probes trained with logistic regression were accurate for classifying a variety of properties of the game board (Karvonen, 2024; Nanda et al., 2023). For training SAEs, we employ a variety of SAE architectures and training algorithms, as detailed in Section 4."}, {"title": "3. Measuring autoencoder quality for chess and Othello models", "content": "Many of the features learned by our SAEs reflect uninteresting, surface-level properties of the input, such as the presence of certain tokens. However, upon inspection, we additionally find many SAE features which seem to reflect a latent model of the board state, e.g., features that reflect the presence of certain pieces on particular squares, squares that are legal to play on, and strategy-relevant properties like the presence of a pin in chess (Figure 1).\nFortunately, in the setting of board games, we can formally specify certain classes of these interesting features, allowing us to more rigorously detect them and use them to understand our SAEs. In Section 3.1, we specify certain classes of interesting game board properties. Then, in Section 3.2, we leverage these classes into two metrics of SAE quality.\n3.1. Board state properties in chess and Othello models\nWe formalize a board state property (BSP) to be a function g: {game board} \u2192 {0,1}. In this work, we will consider the following interpretable classes of BSPs:\n\u2022  contains BSPs which classify the presence of a piece at a specific board square, where the board consists of 8 \u00d7 8 squares in both games. For chess, we consider the full board for the twelve distinct piece types (white king, white queen, ..., black king), giving a total of 8 \u00d7 8 \u00d7 12 BSPs. For Othello, we consider the full board for the two distinct piece types (black and white), yielding 8 \u00d7 8 \u00d7 2 BSPs.\n\u2022  consists of BSPs relevant for predicting legal moves and playing strategically in chess, such as a pin detector. They were selected by the authors based on domain knowledge and prior interpretability work on the chess model AlphaZero (McGrath et al., 2022). We provide a full list of strategy BSPs in Table 3 in the Appendix. Because our Othello model was trained to play random legal moves, we do not consider strategy BSPs for Othello.\n3.2. Measuring SAE quality with board state properties\nIn this section, we introduce two metrics of SAE quality: coverage and board reconstruction.\nCoverage. Given a collection G of BSPs, our coverage metric quantifies the extent to which an SAE has identified features that coincide with the BSPs in G. In more detail, suppose that fi is an SAE feature and t \u2208 [0, 1] is a threshold, we define the function\n$\\phi_{f_i,t}(x) = \\mathbb{I} [f_i(x) > t \\cdot f_i^{\\text{max}}]$\n(2)\nwhere  is (an empirical estimate of) , the maximum value that fi takes over the dataset D of activations extracted from our model, and \ud835\udd40 is the indicator function. We interpret  as a binary classifier; intuitively, it corresponds to binarizing the activations of fi into \"on\" vs. \"off\" at some fraction t of the maximum value of fi on D. Given some BSP g \u2208 G, let F\u2081(, g) \u2208 [0,1] denote the F\u2081-score for  classifying g. Then we define the coverage of an SAE with features {fi} relative to a set of BSPs G to be\n$\\text{Cov}(\\{f_i\\}, \\mathcal{G}) := \\frac{1}{|\\mathcal{G}|} \\sum_{g \\in \\mathcal{G}} \\max_t \\max_{f_i} F_1(\\phi_{f_i,t}; g).$\n(3)\nIn other words, we take, for each g\u2208 G, the F\u2081-score of the feature that best serves as a classifier for g, and then take the mean of these maximal F\u2081-scores. An SAE receives a coverage score of 1 if, for each BSP g \u2208 G, it has some feature that is a perfect classifier for g. Since Cov depends on the choice of threshold t, we sweep over t\u2208 {0, 0.1, 0.2, ..., 0.9} and take the best coverage score; typically this best t is in {0, 0.1, 0.2}.\nBoard reconstruction. Again, let G be a set of BSPs. Intuitively, the idea of our board reconstruction metric is that, for a sufficiently good SAE, there should be a simple, human-interpretable way to recover the state of the board from the profile of feature activations {fi(x)} on an activation x \u2208 \u211d\u1d48. Here, the activation x was extracted after the post-MLP residual connection in layer 6.\nWe will base our board reconstruction metric around the following human-interpretable way of recovering a board state from a feature activation profile; we emphasize that different ways of recovering boards from feature activations may lead to qualitatively different results. This recovery rule is based on the assumption that interpretable SAE features tend to be high precision for some subset of BSPs, in line with Templeton et al. (2024). For example, features that classify common configurations of pieces are high precision (but not necessarily high recall) for multiple BSPs. Thus, using a batch of \u201ctrain\u201d data Dtrain we identify, for each SAE feature fi, all of the BSPs g \u2208 G for which  is a high precision (of at least 0.95) classifier. Then for each g \u2208 G our prediction rule is\n$P_g(\\{f_i(x)\\}) = \\begin{cases}\n    1, & \\text{if } \\phi_{f_i,t}(x) = 1 \\text{ for any } f_i \\text{ which}\\\\ &\\text{is high precision for } g \\text{ on } \\mathcal{D}_{\\text{train}}\\\\\n    0, & \\text{otherwise}.\n\\end{cases}$\n(4)\nLet F\u2081(P({fi(x)}); b) denote the F\u2081-score for a given board state b, where P({fi(x)}) = {Pg({fi(x)})}geg represents the full predicted board (containing predictions for all 64 squares) obtained from the SAE activations. Then, the average F\u2081-score over all board states in the test dataset Dtest can be calculated as:\n$\\text{Rec}(\\{x\\}, \\mathcal{D}_{\\text{test}}) = \\frac{1}{|\\mathcal{D}_{\\text{test}}|} \\sum_{x \\in \\mathcal{D}_{\\text{test}}} \\max_t F_1(P(\\{f_i(x)\\}); b),$\n(5)"}, {"title": "4. Training methodologies for SAEs", "content": "In our experiments, we investigate four methods for training SAEs, as explained in this section. These are given by two autoencoder architectures and two training methodologies-one with p-annealing and one without p-annealing-for each architecture. Our SAEs are available at https://huggingface.co/ adamkarvonen/chess_saes/tree/main (chess) and https://huggingface.co/adamkarvonen/ othello_saes/tree/main (Othello).\n4.1. Standard SAEs\nLet n be the dimension of the model's residual stream activations that are input to the autoencoder, m the autoencoder hidden dimension, and s the dataset size. Our baseline \"standard\" SAE architecture, as introduced in Bricken et al. (2023) is defined by encoder weights We \u2208 \u211d\u1d50\u00d7\u207f, decoder weights Wa \u2208 \u211d\u207f\u00d7\u1d50 with columns constrained to have a L2-norm of 1, and biases be \u2208 \u211d\u1d50, ba \u2208 \u211d\u207f. Given an input x \u2208 \u211d, the SAE computes\nf(x) = ReLU(We(x \u2013 bd) + be)\n(6)\nx = Wa f(x) + bd\n(7)\nwhere f(x) is the vector of feature activations, and x is the reconstruction.\nFor a standard SAE, our baseline training method is as implemented in the open-source dictionary-learning repository (Marks & Mueller, 2024), optimizing the loss\n$\\mathcal{L}_{\\text{standard}} = \\mathbb{E}_{x \\sim \\mathcal{D}_{\\text{train}}} [||x - \\hat{x}||_2 + \\lambda ||f(x)||_1].$\n(8)\nfor some hyperparameter \u03bb > 0 controlling sparsity.\n4.2. Gated SAEs\nThe L\u2081 penalty used in the original training method encourages feature activations to be smaller than they would be for optimal reconstruction (Wright & Sharkey, 2024). To address this, Rajamanoharan et al. (2024) introduced a modification to the original SAE architecture that separates the selection of dictionary elements to use in a reconstruction and estimating the coefficients of these dictionary elements. This results in the following gated architecture:\n$g_{\\text{gate}} (x) := W_{\\text{gate}} (x - b_d) + b_{\\text{gate}}$\n$f(x) := \\mathbb{I} [g_{\\text{gate}} (x) > 0] \\text{ReLU}(W_{\\text{mag}} (x - b_d) + b_{\\text{mag}})$\\\n$\\hat{x}(f(x)) = W_a f(x) + b_d$\nwhere \ud835\udd40[\u00b7 > 0] is the Heaviside step function and  denotes elementwise multiplication. Then, the loss function uses , a frozen copy of the decoder:\n$\\mathcal{L}_{\\text{gated}} := \\mathbb{E}_{x \\sim \\mathcal{D}_{\\text{train}}} [||x - \\hat{x}(\\text{f(x)})|| + \\lambda||\\text{ReLU}(g_{\\text{gate}} (x))||_1 + ||x - \\hat{x}_{\\text{frozen}} (\\text{ReLU}(g_{\\text{gate}} (x))||_2]$\n(9)\n4.3. p-Annealing\nFundamentally, an L\u2081 penalty has been used to induce sparsity in SAE features because it serves as a convex relaxation of the true sparsity measure, the L\u2080-norm. The L\u2081-norm is the convex hull of the L\u2080-norm, making it a tractable alternative for promoting sparsity (Wright & Ma, 2022). However, the proxy loss function is not the same as directly optimizing for sparsity, leading to issues such as feature shrinkage (Wright & Sharkey, 2024) and potentially less sparse learned features. Unfortunately, the L\u2080-norm is non-differentiable and directly minimizing it is an NP-hard problem (Natarajan, 1995; Davis et al., 1997), rendering it impractical for training.\nIn this work, we propose the use of nonconvex L\u209a-minimization, with p < 1, as an alternative to the standard L\u2081 minimization in sparse autoencoders (SAEs). This approach has been successfully employed in compressive sensing and sparse recovery to achieve even sparser representations (Chartrand, 2007; Wen et al., 2015; Yang et al., 2018; Wang et al., 2011). To perform this optimization, we introduce a method called p-annealing for training SAEs, based on the compressive sensing technique called p-continuation (Zheng et al., 2017). The key idea is to start with convex L\u2081-minimization through setting p = 1 and progressively decrease the value of p during training, resulting in closer approximations of the true sparsity measure, L\u2080, as p approaches 0. We define the sparsity penalty for each batch x as a function of the current training step s:\n$\\mathcal{L}_{\\text{sparse}}(x, s) = \\lambda_s ||f(x)||_{p_s} = \\lambda_s \\sum_i f_i(x)^{p_s}$\n(10)\nIn other words, the sparsity penalty will be a scaled  norm of the SAE feature activations, with p decreasing over time. At p = 1, the  norm is equal to the  norm, and as p\u2192 0, the  norm limits to the  norm, as .\nThe purpose of annealing p from 1 \u2192 0 instead of starting from a fixed, low value for p is that the lower the p, the more concave (non-convex) the  norm is, increasing the likelihood of the training process getting stuck in local optima, which we have observed in initial experiments. Therefore, we aim to first arrive at a region of an optimum using the easier-to-train  penalty and then gradually shift the loss function. This manifests as keeping p = 1 for a certain number of steps and then starting decreasing p linearly down to Pend > 0 at the end of training. We set Pend = 0.2.\nCoefficient Annealing. Changing the value of p changes the scale of the  norm. Without also adapting the coefficient \u03bb, the strength of the sparsity penalty would vary too wildly across training. Empirically, we found that keeping a constant \u03bb would lead to far too weak of a sparsity penalty for the larger p's at the start of training, making the process worse than simply training with a constant p from the beginning.\nConsequently, we aim to adapt the coefficient \u03bb such that the strength of the sparsity penalty is not changed significantly due to p updates. Formally, the update step is:\n$\\lambda_{s+1} \\leftarrow \\lambda_{s} \\frac{\\sum_{j=s-q+1}^{s} \\sum_i f_i(x_j)^{p_s}}{\\sum_{j=s-q+1}^{s} \\sum_i f_i(x_j)^{p_{s+1}}}$\n(11)\nWe keep a queue of the most recent q batches of feature activations mid-training and use them to calibrate the \u03bb updates. Therefore, the strength of the sparsity penalty is kept locally constant."}, {"title": "5. Results", "content": "In this section, we explore the performance of SAEs applied to language models trained on Othello and chess. Consistent with Nanda et al. (2023), we find that interpretable SAE features typically track properties relative to the player whose turn it is (e.g. \"my king is pinned\" rather than \"the white king is pinned\"). To side-step subtleties arising from this, we only extract our activations from the token immediately preceding white's move. Specifically, we consider SAEs trained on the residual stream activations after the sixth layer using the four methods from Section 4 (see Table 2 for additional hyperparameters). In addition to our metrics introduced above, we also make use of unsupervised metrics previously appearing in the literature (Bricken et al., 2023; Cunningham et al., 2023; Rajamanoharan et al., 2024):\n\u2022 Lo measures the average number of active SAE active features (i.e., positive activation) on a given input.\n\u2022 Loss recovered measures the change in model performance when replacing activations with the corresponding SAE reconstruction during a forward pass. This metric is quantified as (H* - Ho)/(Horig - Ho), where Horig is the cross-entropy loss of the board game model for next-token prediction, H\u2217 is the cross-entropy loss after substituting the model activation x with its SAE reconstruction during the forward pass, and Ho is the cross-entropy loss when zero-ablating x.\nOur key takeaways are as follows.\nSAE features can accurately reconstruct game boards. In general, we find that SAE features are effective at capturing board state information in both Othello and chess (see Table 1, Figure 2d and 4d). In contrast, SAEs trained on a model with random weights perform very poorly according to our metrics, showing that SAE performance is driven by identifying structure in the models' learned representation of game boards. Nonetheless, SAEs do not match the performance of linear probes in terms of reconstructing the board state. This performance gap suggests that SAEs do not capture all of the information encoded in the model's internal representations.\nStandard SAEs trained with p-annealing perform on par with Gated SAEs. We find that standard SAEs trained using p-annealing consistently perform better than those trained with a constant L\u2081 penalty (Equation 8), as measured by existing proxy metrics and in terms of improvement in coverage (see Figure 2a and 4a). In fact, standard SAEs trained using p-annealing show a coverage score that is comparable to Gated SAEs trained without p-annealing. However, we find cases where our coverage metric disagrees with existing metrics. In Figure 2, for example, Gated SAEs perform achieve a higher loss recovered score than Standard SAEs trained using p-annealing. We emphasize that the training and inference of Gated SAEs is more computationally expensive, requiring 50% more compute per forward pass compared to Standard SAEs (Rajamanoharan et al., 2024).\nCoverage and board reconstruction reveal differences in SAE quality not captured by unsupervised metrics. Our metrics reveal improvements in SAE performance that traditional proxy metrics fail to capture. For example, we trained SAEs with hidden dimensions 4096 and 8192 (expansion factors of 8 and 16, respectively). We expect the SAEs with 8192 hidden dimensions to perform better since they have greater capacity. However, we observe that they perform equally well according to prior unsupervised metrics (see Figures 2 a, c and 4 a, c). In contrast, our metrics reveal that SAEs with larger hidden dimensions are better. For the Standard architecture, this is reflected by the parallel lines (of purple diamonds) in Figures 2 b, d and 4 b, d. Thus, our metrics are able to capture improvements from larger expansion factors. In addition, we find that the performance of p-annealing closely resembles that of Gated SAEs when evaluated using standard proxy metrics; it demonstrates clear improvements under our proposed metrics.\nCoverage and board reconstruction are consistent with existing metrics. Figures 2, 4, and 3 demonstrate that both coverage and board reconstruction metrics are optimal in the elbow region of the Pareto frontier. This region, where SAEs reconstruct internal activations efficiently with minimal features, also yielded the most coherent interpretations during our manual inspections. This provides precise, empirical validation to the common wisdom that SAEs in this region of the Pareto frontier are the best."}, {"title": "6. Limitations", "content": "The proposed metrics for board reconstruction and coverage provide a more objective evaluation of SAE quality than previous subjective methods. Nevertheless, these metrics exhibit several limitations. Primarily, their applicability is confined to the chess and Othello domains, raising concerns about their generalizability to other domains or different models. Additionally, the set of BSPs that underpin these metrics is determined by researchers based on their domain knowledge. This approach may not encompass all pertinent features or strategic concepts, thus potentially overlooking essential aspects of model evaluation. Developing comparable objective metrics for other domains, such as natural language processing, remains a significant challenge. Moreover, our current focus is on evaluating the quality of SAEs in terms of their ability to capture internal representations of the model. However, this does not directly address how these learned features could be utilized for downstream interpretability tasks."}, {"title": "7. Related work", "content": "Sparse dictionary learning. Since the nineties, dictionary learning (Elad, 2010; Dumitrescu & Irofti, 2018), sparse regression (Foucart & Rauhut, 2013), and later, sparse autoencoders (Ng et al., 2011) have been extensively studied in the machine learning and signal processing literature. The seminal work of Olshausen & Field (1996) introduced the concept of sparse coding in neuroscience (see also (Olshausen & Field, 2004), building upon the earlier concept of sparse representations (Donoho, 1992) and matching pursuit (Mallat & Zhang, 1993). Subsequently, a series of works established the theoretical and algorithmic foundations of sparse dictionary learning (Elad & Bruckstein, 2002; Hoyer, 2002; Eggert & Korner, 2004; Aharon et al., 2006; Yang et al., 2009; Jung et al., 2014; Tillmann, 2014; Arora et al., 2015; Bao et al., 2015; Blasiok & Nelson, 2016; Chalk et al., 2018). Notably, Gregor & LeCun (2010) introduced LISTA, an unrolled version of ISTA (Daubechies et al., 2004) that learns the dictionary instead of having it fixed.\nIn parallel, autoencoders were introduced in machine learning to automatically learn data features and perform dimensionality reduction (Hinton & Zemel, 1993; Li et al., 2023b). Inspired by sparse dictionary learning, sparse autoencoders (Ng et al., 2011; Coates et al., 2011; Coates & Ng, 2011; Makhzani & Frey, 2013; Li et al., 2016) were proposed as an unsupervised learning model to build deep sparse hierarchical models of data, assuming a certain degree of sparsity in the hidden layer activations. Later, Luo et al. (2017) generalized sparse autoencoders (SAEs) to convolutional SAEs. Although the theory for SAEs is less developed than that of dictionary learning with a fixed dictionary, some progress has been made in quantifying whether autoencoders can, indeed, do sparse coding, e.g., Arpit et al. (2016); Rangamani et al. (2018); Nguyen et al. (2019).\nFeature disentanglement using sparse autoencoders. The individual computational units of neural networks are often polysemantic, i.e., they respond to multiple seemingly unrelated inputs (Arora et al., 2018). Elhage et al. (2022) investigated this phenomenon and suggested that neural networks represent features in linear superposition, which allows them to represent more features than they have dimensions. Thus, in an internal representation of dimension n, a model can encode m > n concepts as linear directions (Park et al., 2023), such that only a sparse subset of concepts are active across all inputs \u2013 a concept deeply related to the coherence of vectors (Foucart & Rauhut, 2013) and to frame theory in general (Christensen et al., 2003). To identify these concepts, Sharkey et al. (2023) used SAEs to perform dictionary learning on a one-layer transformer,"}, {"title": "8. Conclusion", "content": "Most SAE research has relied on proxy metrics such as loss recovered and Lo, or subjective manual evaluation of interpretability by examining top activations. However, proxy measures only serve as an estimate of interpretability, monosemantic nature, and comprehensiveness of the learned features, while manual evaluations depend on the researcher's domain knowledge and tend to be inconsistent.\nOur work provides a new, more objective paradigm for evaluating the quality of an SAE methodology; coverage serves as a quantifiable measure of monosemanticity and quality of feature extraction, while board reconstruction serves as a quantifiable measure of the extent to which an SAE is exhaustively representing the information contained within the language model. Therefore, the optimal SAE methodology can be judged by whether it yields both high coverage and high board reconstruction.\nFinally, we propose the p-annealing method, a modification to the SAE training paradigm that can be combined with other SAE methodologies and results in an improvement in both coverage and board reconstruction."}, {"title": "Author Contributions", "content": "A.K. built and maintained our infrastructure for working with board-game models. A.K., S.M., C.R., J.B., and L.S. designed the proposed metrics. B.W. performed initial experiments demonstrating the benefits of training SAEs with p < 1. B.W., C.M.V., and S.M. then proposed p-annealing, with B.W. leading the implementation and developing coefficient annealing. The basic framework for our dictionary learning work was built and maintained by S.M. and C.R. The training algorithms studied were implemented by S.M., C.R., B.W., R.A., and J.B. R.A. trained the SAEs used in our experiments. A.K., C.R., and J.B. selected and implemented the BSPs. A.K. and J.B. trained the linear probes. Many of the authors (including L.S., J.B., R.A.) did experiments applying traditional dictionary learning methods and exploring both toy problems and natural language settings, which helped build valuable intuition. The manuscript was primarily drafted by A.K., B.W., C.R., R.A., J.B., C.M.V., and S.M., with extensive feedback and editing from all authors. D.B. suggested the original project idea."}, {"title": "A. Sparse Autoencoder Training Parameters", "content": "We used a single NVIDIA A100 GPU for training SAEs and experiments. It takes much less than 24 hours to train a single SAE on 300 million tokens. Given a trained SAE, our evaluation requires less than 5 minutes of computing time."}, {"title": "B. List of Board State Properties", "content": "Table 3 summarizes the high-level board state properties considered in . The selection of concepts was inspired by McGrath et al. (2022). The column indicated by # denotes the number of individual BSPs per concept. A single BSP per concept indicates we match this condition globally for any corresponding piece."}, {"title": "C. Performance of Linear Probes and SAEs on Board State Properties", "content": "In Figure 3, we present a mean coverage score over strategy board state properties GBSP. Properties within GBSP vary significantly in complexity. For example, queen detection can be inferred directly from the move history, while fork detection requires an accurate representation of the board state. Table 4 shows that linear probe F1-score is below 0.95 for 6 out of 15 properties in GBSP. This suggests that chess-GPT (Karvonen, 2024) does not represent these properties linearly. Additional experiments are required to determine whether the representation is present at all.\nFor the board state case, reconstruction is significantly higher than coverage. This is because there are many SAE features that are high precision classifiers for a configuration of squares, such as \"white pawn on e4, white knight of f3\". In cases where coverage is higher than reconstruction (such as for can_check), it is because there are not many features that are over 95% precision for \u201cthere is a check move available\u201d from which we can recover if there is an available check move. Coverage is significantly higher because there is at least one feature that has an F\u2081-score of 0.54 for can_check, which may not have a precision greater than 95%."}, {"title": "D. Model Internal Board State Representation", "content": "D.1. Othello Models\nPrevious research of Othello-playing language models found that the model learned a nonlinear model of the board state (Li et al., 2023a). Further investigation found a closely related linear representation of the board when probing for \"my color\" vs. \"opponent's color\u201d rather than white vs. black (Nanda et al., 2023). Based on these findings, when measuring the state of the board in Othello, we represent squares as (mine, yours) rather than (white, black).\nD.2. Chess Models\nSimilar to Othello models, prior studies of chess-playing language models found the same property, where linear probes were only successful on the objective of the (mine, yours) representation and were unsuccessful on the (white, black) representation (Karvonen, 2024). They measured board state at the location of every period in the Portable Game Notation (PGN) string, which indicates that it is white's turn to move and maintain the (mine, yours) objective. Some characters in the PGN string contain little board state information as measured by linear probes, and there is not a clear ground truth board state part way through a move (e.g., the \"f\u201d in \u201cNf"}]}