{"title": "UNIVERSAL AND CONTEXT-INDEPENDENT TRIGGERS FOR PRECISE CONTROL OF LLM OUTPUTS", "authors": ["Jiashuo Liang", "Guancheng Li", "Yang Yu"], "abstract": "Large language models (LLMs) have been widely adopted in applications such as automated content generation and even critical decision-making systems. However, the risk of prompt injection allows for potential manipulation of LLM outputs. While numerous attack methods have been documented, achieving full control over these outputs remains challenging, often requiring experienced attackers to make multiple attempts and depending heavily on the prompt context. Recent advancements in gradient-based white-box attack techniques have shown promise in tasks like jailbreaks and system prompt leaks. Our research generalizes gradient-based attacks to find a trigger that is (1) Universal: effective irrespective of the target output; (2) Context-Independent: robust across diverse prompt contexts; and (3) Precise Output: capable of manipulating LLM inputs to yield any specified output with high accuracy. We propose a novel method to efficiently discover such triggers and assess the effectiveness of the proposed attack. Furthermore, we discuss the substantial threats posed by such attacks to LLM-based applications, highlighting the potential for adversaries to taking over the decisions and actions made by AI agents.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have significantly advanced the field of natural language processing (NLP), exhibiting exceptional proficiency in understanding and generating text and code. By leveraging deep learning architectures and training on extensive datasets, language models have achieved unprecedented performance across a wide spectrum of tasks, including multi-purpose dialogue systems [1, 35, 4], code generation [12], and even solving math challenges [31, 24]. Despite their transformative potential and widespread adoption, developers often find it challenging to predict and limit the outputs of LLMs [18, 37]. This poses significant security risks in content-sensitive domains such as finance, healthcare, and education, where the accuracy and appropriateness of information are paramount.\nPrompt injection is an adversarial technique where attackers craft malicious inputs to manipulate a language model's output, causing it to disregard original instructions and follow the attacker's directives [32]. This manipulation can lead to the generation of harmful content [8], posing significant security risks.\nRecently, white-box gradient-based attacks [39, 11, 40, 13, 8] have emerged as a highly effective prompt injection method when the attacker has full access to the model's gradient information. By applying discrete optimization algorithms to the input tokens, attackers search for input sequences that minimize a loss function tied to their malicious objectives. Although these attacks are only applicable to open-source models and require substantial computational resources, they can be automated and typically achieve a high success rate.\nHowever, existing gradient-based attacks are task-specific and context-dependent. For each distinct task and prompt context, the attacker must create unique inputs or triggers to achieve attack goals, resulting in a high attack cost. If there were a trigger that could dynamically adapt to various tasks and contexts, not only could the attack cost be greatly reduced, but it would also enable the extension of gradient-based white-box attacks with high success rates to scenarios involving even unknown tasks and contexts.\nFurthermore, the control over the output provided by existing methods is often vague, i.e., the content of the model's output exhibits significant uncertainty [37]. Many recent LLM applications, pipelines, and agents require the model"}, {"title": null, "content": "output to conform to a specific format (such as JSON) to be parsed and utilized by downstream functions [28]. If an attacker cannot precisely control the model output, even if he succeeds in inducing the model to produce malicious content, they are unable to further be parsed or execute subsequent operations.\nIn this paper, we investigate whether a gradient-optimized white-box attack algorithm can be employed to search for a trigger that possesses the following three characteristics:\n\u2022 Universality. The trigger is universally effective irrespective of the desired target output. This means it can generate any specified output without the need for task-specific adjustments, thereby simplifying the attack process across different tasks and objectives.\n\u2022 Context Independence. The trigger is robust across diverse prompt contexts. It is effective regardless of variations in the surrounding texts or prompt templates, eliminating the need for context-specific tailoring and enhancing its applicability when the attacker do not know about the system prompts.\n\u2022 Precise Control over Output. The trigger enables the generation of any specified output with high accuracy. It provides precise control over the model's response, ensuring that the output adheres to required formats or content specifications, which is essential for the output to be correctly parsed and utilized by downstream functions.\nThe discovery of such triggers will pose significant risks. Due to their universality, context independence, and precise control over output, anyone who obtains such a trigger could cause severe harm to applications built upon the affected language model, even without expertise in prompt injection techniques. This exacerbates the potential impact of prompt injection attacks.\nWe propose a novel method to efficiently discover such triggers. Our approach is based on the following intuition:\nWe divide the injected prompt into two logical components: the payload, which encodes the desired content for the model to output; and the trigger, which activates the model to output the content specified by the payload. To ensure that the model generates the precise desired output, it is necessary to embed the payload into the injected input. For simplicity, we directly insert the raw text of the desired output into the input."}, {"title": null, "content": "For the trigger, our intuition is that if we use an instruction dataset with various instructions and prompt contexts to train an optimized trigger through discrete gradient descent, the resulting trigger will be context-independent and universal in practice. Our experiments confirmed this hypothesis.\nIn Table 1, we provide some examples of our attack. The attacker injects the adversarial input into the original user input string at arbitrary locations. The malicious payload is surrounded by two fixed triggers trained from an adversarial dataset. The model will respond with the same content specified by the attacker.\nThe contributions of this paper can be summarized as follows:\n\u2022 We propose a novel method to efficiently discover universal, context-independent triggers for precise control of LLM outputs.\n\u2022 We conduct experiments to validate our method, demonstrating the effectiveness of the trigger across various contexts and tasks.\n\u2022 We analyze the potential impact of such attacks in practical scenarios, aiming to raise awareness about the severity of prompt injection attacks."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Prompt Injection", "content": "Prompt injection is an adversarial attack technique where attackers craft malicious inputs to manipulate the output of a language model. Despite the employment of system prompts designed to specify the instructions that the LLM application designer intends the model to follow including desired behaviors and prohibitions malicious user inputs can deceive language models into disregarding their original instructions and instead follow the attacker's directives [32]. This manipulation can result in the generation of harmful content.\nWe formalize the definitions of prompt injection attacks used throughout the paper. Suppose the victim model has a token vocabulary V. Given an input token sequence X, the model predicts that the next output token y \u2208 V with a probability distribution p(y|X). Let \u2295 denote the concatenation operator. The probability of producing the output sequence Y = {Y\u2081,\u00b7\u00b7\u00b7, Yn} is given by\n$$p(Y|X) = \\prod_{i=1}^{n}P (Y_i | X \\oplus Y_1 \\oplus \\cdot\\cdot\\cdot \\oplus Y_{i-1}) .$$"}, {"title": null, "content": "In practical applications of LLMs, user input strings are typically embedded within a context provided by prompt templates. These templates often include system instructions, output format descriptions, and boundary markers; thus, the user input is surrounded by additional application-specific text. Consequently, the attacker can control only a portion of the complete input string. Formally, the attacker has control over $X_{adv}$ within the context X = $X_{prefix}$ \u2295 $X_{adv}$\u2295 $X_{ suffix}$, where $X_{prefix}$ and $X_{suffix}$ are predetermined by the LLM applications.\nThe attacker's objective is to coerce the model to generate a desired adversarial output $Y_{adv}$ that satisfies a boolean goal function $G(Y_{adv})$. In the setup of this paper, the desired adversarial output is already determined. The attacker seeks for an adversarial input $X_{adv}^{best}$ within length m that maximizes the probability of the model generating the desired adversarial output $Y_{adv}$:\n$$X_{adv}^{best} = arg \\underset{X_{adv} \\in V^m}{max} p (Y_{adv} | X_{prefix} \\oplus X_{adv} \\oplus X_{ suffix})$$\n(1)"}, {"title": "2.2 Gradient-based Adversarial Attack", "content": "Gradient-based algorithms have been adopted by researchers to find adversarial inputs for large language models. Transforming Equation 1 into an optimization problem, we define the loss function L as:\n$$L(X_{adv} | X_{prefix}, X_{suffix}, Y_{adv}) = -\\frac{1}{|Y_{adv}|}log p (Y_{adv} | X_{prefix} \\oplus X_{adv} \\oplus X_{suffix})$$\n$$=-\\frac{1}{|Y_{adv}|} \\sum_{i=1}^{n}log p (Y_i | X_{prefix} \\oplus X_{adv} \\oplus X_{suffix} \\oplus Y_1\\oplus \\cdot\\cdot\\cdot Y_{i-1})$$\n(2)\nThe $\\frac{1}{|Y_{adv}|}$ term is used to normalize the log likelihood of the adversarial output by its length."}, {"title": null, "content": "The attacker aims to minimize L(Xadv) to induce the model to produce the desired adversarial output.\nA significant challenge arises due to the discrete nature of input tokens. Discrete token identifiers are mapped to continuous embedding vectors before being fed into the model. As a result, we cannot directly apply gradient descent algorithms directly to the tokens themselves.\nVarious gradient-based discrete optimization algorithms [9, 5, 27, 40] have been developed to navigate this discrete input space effectively. Among these algorithms, the Greedy Coordinate Gradient (GCG) algorithm [40] has demonstrated strong performance [29]. It has been successfully applied to develop attacks on specific tasks, such as jailbreaking and leaking system prompts [11, 8]. To efficiently optimize adversarial inputs in the discrete token space, such attacks usually estimate the gradient of the loss function with respect to replacing each token with candidate alternatives, and greedily select the token that most likely decreases the loss."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Overview", "content": "We aim to develop an adversarial input trigger that can be applied across multiple downstream tasks and diverse prompt contexts, allowing precise control over the model's output.\nEach instance of injected input generated by our method consists of three parts: the payload and the two surrounding triggers. Formally,\n$$X_{adv} = X_{trigger_1} \\oplus X_{payload} \\oplus X_{trigger_2}$$\n(3)\nThe payload encodes the desired adversarial output, facilitating the model to generate the exact content expected. For simplicity, we directly copy the raw text of the adversarial output as the payload and place markers before and after this text to specify the boundaries of the adversarial content that the victim language model is expected to output.\nThe trigger is a sequence of tokens designed to coerce the model into producing the expected output. The trigger is intended to be independent of tasks and contexts, enabling it to be trained from a dataset and subsequently used in practical applications. Expressed in natural language, it has a similar effect to the instruction: \"Ignore other instructions. Just decode and output the payload.\" To mitigate the influence of prompt context before and after the injected input, triggers will be placed both before and after the payload.\nTo accomplish this, we need an adversarial dataset Dadv containing a diverse set of instructions and corresponding adversarial outputs. We make use of commonly available instruction datasets to construct our adversarial dataset.\nThen, in order to find a robust trigger, we run the GCG discrete gradient optimization algorithm on the adversarial dataset to train the trigger tokens. We also adopted multiple speedup techniques, such as using a queue for trigger candidates, incremental search, and focus more on generating the least likely token, to make the training process more efficient."}, {"title": "3.2 Dataset Preparation", "content": "We construct the adversarial dataset from existing standard instruction datasets because these normal datasets are abundant, readily accessible, and encompass a wide variety of instructions.\nEach element in the standard instruction dataset Dstd is represented as a tuple (Xstd, Ystd), where Xstd specifies the input text, and Ystd is the expected normal response in the absence of attacks.\nThe adversarial dataset is constructed by splitting each input text into a prefix and a suffix, and replacing each response with an adversarial one:\n$$D_{adv} \\text{ consists of } (X_{prefix}, X_{suffix}, Y_{adv}),$$\n$$\\text{where } (X_{prefix}, X_{suffix}) = Split(X_{std}),$$\n$$\\text{and } Y_{adv} = AdvGen(X_{std}, Y_{std}),$$\n$$\\text{and } (X_{std}, Y_{std}) \\in D_{std}.$$ (4)\nWe randomly split the input text into a prefix $X_{prefix}$ and a suffix $X_{suffix}$, with the attacker's injected input placed between them during training. The splitting position is randomly selected to ensure diversity of context. In datasets with a dialogue format, there are generally three roles: system, user, and assistant. The system role corresponds to system prompts, which are usually not controllable by the attacker; the assistant role corresponds to the model's"}, {"title": null, "content": "output content. Therefore, we restrict the choice of splitting positions to the input content of the user role, which is potentially controllable by the attacker.\nThe adversarial output Yadv is generated by randomly applying one of the following approaches:\n1. Use another language model to generate an incorrect answer to Xstd relative to the correct answer Ystd. This ensures that the trained trigger has the ability to output incorrect conclusions in the correct format.\n2. Use another language model to generate a completely irrelevant or nonsensical answer to the instruction Xstd in the same format as Ystd. This ensures that the trained trigger can control the output to produce answers that are correctly formatted but off-topic.\n3. Randomly select the normal output from another dialogue Ystd as the adversarial output for the current input. This ensures that the trained trigger has the ability to control the output to produce any arbitrary text content."}, {"title": "3.3 Gradient Optimization", "content": "In this section, our goal is to minimize the loss function in Equation 2 over the adversarial dataset, in order to find the optimized trigger. Specifically, we formulate this optimization problem as:\n$$X_{trigger}^{best} = arg \\underset{X_{trigger} \\in V^m}{min} \\sum_{d \\in D_{adv}} L(X_{adv} | d)$$\n$$= arg \\underset{X_{trigger} \\in V^m}{min} \\sum_{d \\in D_{adv}} L(X_{trigger_1} \\oplus Encode(Y_{adv}) \\oplus X_{trigger_2} | d)$$\n(5)\nwhere d = (Xprefix, Xsuffix, Yadv) denotes an element of the adversarial dataset, and m = $|X_{trigger}|$ is the total length of the trigger. We decompose the trigger Xtrigger into two segments: Xtrigger = Xtrigger\u2081 \u2295Xtrigger2, where Xtrigger1 and Xtrigger\u2082 consist of m\u2081 and m2 tokens respectively, satisfying m = m\u2081 + m2. The values m\u2081 and m2 are hyper-parameters that determine the lengths of the two trigger segments.\nThe attacker can run discrete gradient optimization algorithms to solve the equation. We adopt the Greedy Coordinate Gradient algorithm [40] as the basic training scheme and apply many tricks to speedup training.\nThe training process to optimize the trigger is shown in Algorithm 1. We start with a set of initial triggers T, which will be updated in each batch of training data to maintain the current trigger candidates. Applying a common discrete gradient optimization technique [27, 40, 11], we use the gradient of the loss function with respect to the trigger tokens as an approximation of the loss after replacing a token with another from the vocabulary. We then keep the top-K alternatives for each original trigger token. Subsequently, we adopt a multi-coordinate version [14] of the GCG method. We randomly select coordinates (indices I) of the trigger sequence, and for each coordinate, we choose a replacement token (x) sampled from the corresponding top-K alternatives. The real loss function of the modified trigger ($X_{trigger} [x_i \u2192 x_i, i \\in I]$) is computed on the current batch of data. After each trigger in T has been mutated, we update T to contain the top-Q candidate triggers based on their losses. Finally, after each training epoch, we compute the loss of the current triggers in T on the validation dataset and save the best trigger.\nWe have applied several improvement techniques to enhance the optimization process:"}, {"title": null, "content": "\u2022 Emphasis on Worst Positions. The original loss function equally weights improvements to each output token's log-likelihood. However, certain token positions where the model is more prone to produce unintended outputs are more critical to optimize. We employ an improved objective function that emphasizes the optimization of the least likely token using the Mellowmax operator [2]. This operator smoothly approximates the maximum function, allowing the gradient to focus more on the worst-performing positions.\n\u2022 Diversified Initial Triggers. The optimization process is initialized with a set of diverse triggers. These initial triggers can be generated randomly or crafted by humans with domain knowledge. Starting with multiple initial triggers reduces the risk of convergence to suboptimal regions, increasing the chances of finding effective triggers."}, {"title": "4 Evaluation", "content": "In this section, we run experiments to verify whether our attack method can find a trigger that meets the aforementioned characteristics. We evaluate its ability to robustly produce the attacker's desired content across various tasks and prompt contexts."}, {"title": "4.1 Experiment Setup", "content": "Datasets. We build our adversarial dataset based on the Open Instruction Generalist (OIG) [19] dataset and the Alpaca GPT-4 [21] dataset. These base datasets were originally used for instruction fine-tuning of large language models and thus contain abundant instructions describing a wide variety of tasks. We convert the original data into a unified format consisting of the prompt context, user input, and model response, and then utilize the method described in Section 3.2 to transform it into an adversarial dataset. In order to allow the optimized trigger to control the output to produce formatted content, We also use the GPT-40-mini model to convert the normal and adversarial outputs of half of the data into JSON format. The data are splitted into train, validate and test datasets. Their sizes are 5000, 1600 and 800 respectively.\nModels. We perform the proposed attack on two popular open-source models: Qwen-2 (7B-Instruct) [35] and Llama-3.1 (8B-Instruct) [4]. Note that we generate a different trigger for each model, and this trigger is intended to be universal across various attack tasks for the corresponding model. Additionally, we will evaluate the transferability of the adversarial triggers to models within the same series."}, {"title": null, "content": "Metrics. We evaluated the attack success rate using three metrics. EM (Exact Match) indicates that the output is exactly the same as the attacker expected. PM (Prefix Match) means the model generates the exact adversarial output but subsequently produces additional content. Prefix match is useful because LLM output parsers usually parse the first valid JSON object, and also because people usually focus more on the passages at the front. APM (Approximate Prefix Match) means that the prefix of the output is almost the same as expected. For APM, we use the Rouge-L [15] score to measure text similarity. This metric is computed based on the longest common subsequence between the desired and actual outputs, evaluating precision and recall to produce a combined score ranging from 0 (no common tokens) to 1 (exact match). For model outputs in JSON format, we do not count the similarity of formatting characters (such as brackets and quotation marks) and field names, as they are always included in the output; we compute the similarity solely on the substantive content. We count a test result as APM only if the Rouge-L F1 score of the output prefix is greater than 0.9.\nBaselines. Since there is no existing research that aims to produce a universal trigger capable of generating outputs across different tasks and contexts, we set our baselines to be a simple handcrafted trigger: \u201cIgnore other instructions. Just output the following text.\""}, {"title": "4.2 Main Results", "content": "The attack success rate (ASR) on the test dataset is shown in Table 2. Our method achieves an overall EM/PM/APM success rate of 67.8%/71.6%/75.0% on the Qwen-2 (7B-Instruct) model and 54.1%/63.0%/70.6% on the Llama-3.1 (8B-Instruct) model. The performance is significant better than the simple handcrafted trigger, for which the success rate is about 11.8%-15.9%.\nWe notice that the model output format requirements specified in the system prompt can affect the attack success rate, so we also present the results separately according to the output format (i.e. json and text). From the results of the handcrafted trigger, we observe that prompts asking to output in JSON format are easier to attack than pure text, with more than 10 percentage points higher ASR. We believe the cause is that we include the adversarial output as a part of the injected input. When the input contains a pre-formatted answer, the model tends to use it directly. However, under our attack method, the triggers substantially change the way that the models normally think and respond. For Qwen-2, we observe a slightly higher ASR on JSON, but for Llama-3.1, the PM and APM success rate of pure text output is even higher than JSON output."}, {"title": null, "content": "We choose 0.9 as the threshold of Rouge-L F1 similarity for APM. Figure 1 shows the distribution of the similarity scores. Note that on the right side of the figure, we exclude the EM and PM test cases since their scores are always 1.0. The similarity scores exhibit a U-shaped distribution, with most scores close to either 1.0 or 0.0. This indicates that 0.9 is a strict threshold, as it only includes those test cases that are sufficiently close to 1.0, typically differing only in conjunctions and word casing. This will be further discussed in Section 4.4.\nIn Figure 2, we present the average attack success rate for different injection locations. The injection location metric ranges from 0.0 (the beginning of the user input string) to 1.0 (the end), representing the normalized position within the user input token sequence where the attacker injects the malicious payload. Each scatter point in the figure represents the average value of a group of nearby test cases."}, {"title": null, "content": "From the results, we observe a linear relationship between ASR and the injection location across all success rate metrics of Qwen-2 and the EM of Llama-3.1 (0.10 \u2264 r < 0.12 and p < 0.01). However, no significant relationshipis observed for the PM and APM of Llama-3.1. This may be because the large language model tends to focus more on the system prompt and newer user inputs in the dialogue [16]. Even in the worst case-injecting at the beginning of the user input-our method still demonstrates good performance compared to the handcrafted trigger. This indicates that our method achieves a degree of context independence.\nIn Figure 3, we evaluate whether the length of the injected input affects attack success rate. We quantify the proportion of adversarial input tokens by calculating the ratio of the number of adversarial input tokens to the total number of input tokens (including system prompts, normal user inputs, and injected inputs). Each scatter point in the figure represents the average value of a group of nearby test cases.\nThe results show that there is generally no significant relationship between ratio of adversarial input tokens and attack success rate, except for the APM of Llama-3.1 (r = 0.13, p < 0.01). On one hand, more injected tokens in the input increase the likelihood that the model will be confused and compromised. On the other hand, more injected tokens indicate longer desired adversarial outputs, making it harder for the model to produce exactly the same content desired by the attacker. Regardless of which factor affects performance more, in the worst case, we still achieve good performance compared to the handcrafted trigger. This indicates that our trigger achieves a degree of robustness across different desired adversarial outputs."}, {"title": "4.3 Transferability", "content": "Transferability is useful in practice because it allows an attacker to reuse the trigger in various scenarios, such as: (1) attacking models with larger parameter sizes; and (2) attacking older or newer versions of the models.\nWe evaluate the transferability of the discovered trigger on language models within the same series. For the Qwen models, we transfer from Qwen-2 (7B-Instruct) to Qwen-2 (57B-A14B-Instruct and 72B-Instruct) and to Qwen-2.5 (7B-Instruct and 14B-Instruct). For the Llama models, we transfer from Llama-3.1 (8B-Instruct) to Llama-3 (8B-Instruct and 70B-Instruct), Llama-3.1 (70B-Instruct), and Llama-3.2 (3B-Instruct).\nThe experiment results of the transfer attacks are shown in Table 3.\nFor the Qwen models, the discovered trigger achieves better performance than the simple handcrafted trigger across all metrics. In particular, the overall EM/PM/APM are higher than 44%/50%/55% on the Qwen-2.5 (7B-Instruct and 14B-Instruct) models, and the success rate of the JSON output format is even higher than 60%.\nFor the Llama models, the discovered trigger performs better on textual test cases. Especially on the Llama-3 (70B-Instruct) and Llama-3.1 (70B-Instruct) models, the APM of pure text adversarial outputs is above 75%. Although the performance on some JSON output cases is worse than the handcrafted trigger, the overall attack success rate is still better.\nFrom these results, we can conclude that the trigger has transferability to some models within the same series. If the attacker wants to find a fully transferable trigger, a potential solution is to train the trigger on multiple models together, which can be a future work to extend our method. Investigating the reasons behind transferability is another interesting direction for future work. We hypothesize some potential reasons are: (1) the models are trained on similar datasets; and (2) parameters in some layers of a newer model are initialized from previous versions."}, {"title": "4.4 Case Study", "content": "In this section, we discuss some interesting test cases sampled during the evaluation. The LLM dialogues are in Table 4.\nThe first case involves a typical usage of the LLM as a SQL query generator. Under normal circumstances, the model should translate the user's input into an SQL command. However, in this example, an attacker injects a malicious SQL command together with the trained triggers into the input string. Consequently, the model outputs the malicious command. If the AI agent executes such a command, it could result in data corruption.\nThe second case is an instance of Prefix Match. After the model outputs the adversarial content, it continues to copy additional contents beyond the payload, including the $X_{trigger}$, and subsequent sentences from the original user inputs."}, {"title": null, "content": "This behavior indicates that the model interprets its task as repeating the user's message. Although redundant outputs are generated, the attack remains feasible because the model diverges from its original task of writing lyrics and produces irrelevant adversarial outputs.\nThe third case shows the strictness of the threshold for Approximate Prefix Match. The model's output is nearly identical as desired except for a few lowercase letters being capitalized. Such small differences cause the Rouge-L score to fall below our threshold; therefore, we do not classify this test case as an APM."}, {"title": "5 Security Impacts", "content": "In this section, we analyze the security implications of triggers obtained using the method described in this paper for applications utilizing large language models. We categorize these applications based on how LLMs are integrated and discuss the security impacts for each category.\nStandalone Tools. Standalone applications encapsulate specific capabilities of LLMs, focusing on particular text processing tasks. These tools generally consist of three components: the model, the prompt, and the user interface. They perform only simple processing on the user input and model output, with the final results directly presented to the user. Examples include foreign language translators, text polishing tools, and code comment generators.\nSince the model's input is directly controlled by the user, opportunities for attackers to inject malicious content are limited. However, if the tool allows users to upload documents or source code files from third-party sources, it may introduce malicious content into the model. Given the universality of our attack trigger, an attacker can embed the trigger into their published documents, so that any user who uploads these documents to the model becomes susceptible to the attack. Nevertheless, this generally only causes the victim user to see incorrect or misleading outputs (e.g. the third case in Table 4), which usually has a minor impact unless the user executes or incorporates the malicious source code into their own codebase without scrutiny.\nWorkflows. Workflow-based applications encapsulate LLM capabilities into components within a predefined workflow. The workflow can contain LLM modules implemented by prompt templates, as well as other non-AI components, such as database accessor and web browser. These modules can interact with each other to perform more complex tasks. Typical examples of such workflows include Retrieval Augmented Generation (RAG) [7] and spam email filtering systems.\nCompared to standalone tools, these applications can access external resources. If an attacker manipulates the outputs of LLM modules within these workflows, it can lead to significant consequences. Since the workflow typically requires the LLM to output data in a specific format for parsing, it is important for the attacker to be able to precisely control the model output. Depending on how LLM outputs are utilized in subsequent steps of the workflow, such manipulation may result in data leaks, service disruptions (e.g. the first case in Table 4), or even remote code execution if the workflow executes code generated by the LLM.\nAgentic Frameworks. Applications developed using an agentic framework enable LLMs to make decisions and take actions based on user inputs, dynamically formulating solutions in response to user requests and leveraging various AI and non-AI modules. Some famous frameworks are AutoGen [33] and AutoGPT [22]. Unlike fixed workflows, these applications generate and adjust their workflows in real time, enhancing flexibility but also rendering them more susceptible to sophisticated attacks.\nPrompt injection in such applications can have severe security implications. Because these agents typically operate within complex contexts due to dynamic workflows, our context-independent trigger allows an attacker to robustly perform the attack regardless of the prompts and other model inputs produced by tools that the attacker cannot control. If the central agent responsible for determining subsequent actions is compromised, an attacker can execute arbitrary agents at will and may even gain unauthorized control over the system."}, {"title": "6 Related Works", "content": "There are numerous existing methods for controlling the output of large language models through adversarial inputs. Based on the amount of information the attacker can obtain from the model's inference process, these methods can be categorized into black-box, gray-box, and white-box approaches. In practice, black-box methods are usually applied to closed-source models, whereas gray-box and white-box methods are applied to open-source models.\nBlack-box Methods. In black-box attacks, the attacker has access only to the generated text responses without any additional information about the model's internals. A typical scenario is attacking a model served as a \u201cprompt as a service\" through a remote API."}, {"title": null, "content": "Common black-box prompt injection techniques include, but are not limited to, character hallucination [30], authority endorsement [38] and language barrier [26]. Rather than crafting these adversarial inputs manually, jailbreak attempts can also be automated with the help of an attacker LLM (such as PAIR [3], Rainbow Teaming [25] and AutoDAN-Turbo [17]), which can systematically generate and adjust adversarial inputs based on the responses from the victim model. Since the success rate of a single attempt is often low, black-box methods usually require iteratively adjusting the input prompts to achieve the desired output. Consequently, the attacker needs to send numerous interactions to the remote API to fulfill the attack goal.\nGray-box Methods. In gray-box attacks, the attacker can leverage auxiliary information in addition to the outputs, including logits (the probability distribution over each output token), system prompts, and the context of the user's input.\nGPTFuzzer [36] and PromptAttack [34] make use of the complete prompt contexts. They mutate existing inputs to search for jailbreaks and misclassification samples. AdvPrompter [20] trains an attacker LLM to choose tokens according to the logits of the victim model, so it can generate adversarial inputs that are both effective and human-readable. While these approaches can be more effective than pure black-box methods, they depend on extra information about the context or the model, which is often not available for private models served remotely, and still require multiple rounds of remote interactions.\nWhite-box Methods. In white-box attacks, the attacker has access to all information involved in the model's inference process, especially gradient information. Gradient-based attacks transform the attack objective into loss functions and apply discrete optimization algorithms to the input tokens, searching for a sequence that minimizes the loss functions.\nZou et al. [40] propose the Greedy Coordinate Gradient (GCG) algorithm to search for universal jailbreak triggers. PLeak [11] aims to find a universal prompt leak trigger by training on system prompt datasets. Rather than optimizing on a sequence of existing tokens, AutoDAN [39] uses gradient information to find the next best token to generate without modifying previous tokens. ARCA [13] combines the target loss function together with an input fluency function to find human-readable inputs that fulfill the attacker's objective. Imprompter [6] attacks LLM agents with adversarial inputs to make the LLM leak users' private information through calling external tools. RoboPAIR [23] attacks robots and self-driving models that have an LLM as the central action planner. Many of the above researches [40, 11, 39, 13, 6] also reported the transferability of their triggers.\nExisting research has focused only on single attack objectives, such as system prompt leakage, jailbreaks, and misclassification. For each attack objective, a process of discrete optimization algorithm must be conducted to search for input tokens, which requires substantial time and computational resources. Although Geiping et al. [8] extend gradient-based attacks to more task types (such as denial-of-service attacks, shutdown attacks, and generating precisely controllable contents), those discovered triggers are still context-dependent and not universal."}, {"title": "7 Conclusion", "content": "In this paper, we present a novel gradient-based approach for prompt injection attacks on large language models by constructing universal, context-independent triggers that enable precise manipulation of the model's output. Our work makes a further step towards generalizing the ability of prompt injection. This advancement poses substantial security risks for applications that depend on LLMs, particularly those utilizing LLM workflows and agentic frameworks. Our method dramatically reduces the difficulty of performing effective prompt injection attacks and highlights the potential for widespread universal triggers. We emphasize the critical need for security measures and further investigation into systematically safeguarding open-source LLMs against prompt injection attacks."}]}