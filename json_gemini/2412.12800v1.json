{"title": "Breaking the Programming Language Barrier: Multilingual Prompting to Empower Non-Native English Learners", "authors": ["James Prather", "Brent N. Reeves", "Paul Denny", "Juho Leinonen", "Stephen MacNeil", "Andrew Luxton-Reilly", "Jo\u00e3o Orvalho", "Amin Alipour", "Ali Alfageeh", "Thezyrie Amarouche", "Bailey Kimmel", "Jared Wright", "Musa Blake", "Gweneth Barbre"], "abstract": "Non-native English speakers (NNES) face multiple barriers to learning programming. These barriers can be obvious, such as the fact that programming language syntax and instruction are often in English, or more subtle, such as being afraid to ask for help in a classroom full of native English speakers. However, these barriers are frustrating because many NNES students know more about programming than they can articulate in English. Advances in generative AI (GenAI) have the potential to break down these barriers because state of the art models can support interactions in multiple languages. Moreover, recent work has shown that GenAI can be highly accurate at code generation and explanation. In this paper, we provide the first exploration of NNES students prompting in their native languages (Arabic, Chinese, and Portuguese) to generate code to solve programming problems. Our results show that students are able to successfully use their native language to solve programming problems, but not without some difficulty specifying programming terminology and concepts. We discuss the challenges they faced, the implications for practice in the short term, and how this might transform computing education globally in the long term.", "sections": [{"title": "1 INTRODUCTION", "content": "Non-native English speakers (NNES) face significant challenges when learning programming through English language instruction. They tend to set higher academic goals for themselves and spend more time studying compared to their native English-speaking peers [23]. The stance that one must know English to be a programmer is quite entrenched in the broader community\u00b9, but this view is often critiqued [24, 46]. The impacts of learning English-adjacent programming languages through English language instruction are not well understood. Although NNES sense of belonging does not appear to be impacted by learning programming in English, other underlying factors can lead to a less inclusive environment. On the one hand, these students often express higher self-doubt and embarrassment by asking for help [3, 47]. On the other hand, students enjoy learning programming in their native language and report that it positively impacts their experience [52]. However, instruction in native language does not seem to impact learning outcomes for NNES students [3, 59]. While one might think that English language instruction would vary across the globe being sensitive to local contexts, a recent study showed that it was remarkably monolithic across three different continents [5].\nIn 2019, Becker wrote that native natural language programming would have obvious advantages for NNES but remained a far-off fantasy [4]. The advent of generative AI (GenAI) since then has radically shifted our perception of the role that natural language might play in programming [14, 49]. Initial work on generating programming exercises using GenAI found that it could create coherent and customizable programming assignments [55]. Follow-up work showed that students in Finland preferred customized programming assignments, albeit in English [42]. Recent work has demonstrated the capabilities of ChatGPT-3.5 to generate programming problems not only in English but also in Tamil, Spanish, and Vietnamese [31]. Other types of programming problems, such as the classic \"Explain in Plain English\" (EiPE), are also being automated by GenAI that can be used to grade them at scale [15, 33]. These advancements offer a unique opportunity to explore expressing programming concepts in a learner's native language.\nIn this paper, we present the first exploration of NNES prompting GenAI to solve programming problems using their native language. We do this by utilizing Prompt Problems with prompts in a learner's native language. Prompt Problems are a type of programming exercise designed to teach programming concepts via GenAI prompting. Students receive a problem visually and must write a prompt that can generate code to solve the problem. If the generated code does not pass the suite of test cases, then they must edit their prompt again, iterating until it successfully passes. Initial work has shown that students find Prompt Problems engaging and encourage metacognitive reflection [13]. Here, we combine several threads of recent work to see if students can solve these problems in their native languages (Chinese, Portuguese, Arabic) and how it impacts their learning experience.\nTherefore, our research questions are:\n(1) RQ1: How successful are students at solving Prompt Problems in their non-English native languages?\n(2) RQ2: How does solving Prompt Problems in non-English native language impact user experience?\n\u00b9 Reinforced by documents such as PEP 8 - a style guide for Python - with guidance\nsuch as Python coders from non-English speaking countries: please write your comments\nin English."}, {"title": "2 RELATED WORK", "content": "Since the popularization of large language models (LLMs) with the release of ChatGPT in November 2022, researchers in computing education have explored both the opportunities and challenges presented by these models."}, {"title": "2.1 LLMs in Computing Education", "content": "One of the earliest identified opportunities of LLMs for computing education is the automatic generation of programming exercises. Sarsa et al. studied the capabilities of Codex in generating novel programming exercises focusing on specific themes (such as 'cooking' or 'basketball') and concepts (such as 'for-loops') [55]. Their findings suggested that LLMs can create novel exercises that are often good enough to provide to students as-is. Follow-up work has found that while LLMs are good at generating high-quality exercises [11, 42], the thematic contextualization of created content is often shallow [42].\nIn addition to programming exercises, LLMs have been used to create multiple-choice questions (MCQs) for computing courses [1, 17, 62]. Tran et al. found promising results when using GPT-4 to generate multiple-choice questions that were isomorphic to provided examples [62]. In similar work, Doughty et al. examined GPT-4's performance for generating MCQs that would align with course learning objectives [17]. Their results suggest that GPT-4 was able to generate very high-quality MCQs that were evaluated to be of similar quality as those generated by human educators.\nResearchers have also explored using LLMs to help students understand code. Recent work by Bernstein et al. had students create personally meaningful analogies for recursion using ChatGPT, a notoriously difficult threshold concept [6]. The analogies created by students with the help of ChatGPT were more diverse compared to generic analogies that LLMs would generate on their own. Additionally, the participants reported that these analogies helped them understand recursion.\nLLMs have also been used to explain both program code [32, 37, 44, 55] and programming error messages [38, 54, 61, 65]. Sarsa et al. found that Codex was able to explain program code in natural language correctly for approximately two thirds of the lines of code in the examples they provided it [55]. MacNeil et al. reported that students found LLM-generated code explanations useful and helpful for their learning [44]. Leinonen et al. found that code explanations generated by LLMs were rated as being more accurate and easier to understand compared to student-generated explanations [37]. For programming error messages, while Codex was only sometimes helpful in enhancing them [38], follow-up work using GPT-3.5 and GPT-4 has found improved results. Taylor et al. found that GPT-3.5 explained errors correctly in up to 90% of cases [61], and Wang et al. found that students who were provided enhanced programming error messages created by GPT-4 repeated errors less frequently and resolved errors with fewer attempts [65]. However, Santos et al. found that the time to fix errors was not improved by providing GPT-4 enhanced error messages [54].\nLLMs have also been explored for providing direct assistance to students, either through responses to help requests [25] or by feedback on program code [34, 35]. Hellas et al. explored how well LLMs"}, {"title": "2.1.1 Opportunities", "content": "Since the popularization of large language models (LLMs) with the release of ChatGPT in November 2022, researchers in computing education have explored both the opportunities and challenges presented by these models."}, {"title": "2.1.2 Challenges", "content": "Despite the many opportunities that LLMs provide, there are challenges too. The earliest computing education work that utilized LLMs found that they can solve most introductory programming (CS1) exercises [19], and follow-up work with more recent models has demonstrated rapidly improving performance [49]. Their problem solving is not limited to programming exercises, as LLMs have been found to solve Parsons problems too [53], even if provided as images [27], as well as computer graphics questions requiring both visual perception and geometric reasoning skills [18]. LLMs can also solve multiple-choice questions related to programming [56]. This has raised fears that students might use LLM support for academic misconduct, or over-rely on LLM support even when their intention is not to cheat. While some work has looked into automatically detecting LLM-generated solutions [26, 48], there are no sure-fire methods to conclusively classify code as LLM-generated.\nPerhaps more concerning is that students could inadvertently be negatively affected by LLM use. Prather et al. replicated an earlier study [50] looking into metacognitive difficulties that novice programmers face while they're programming, but this time giving students access to LLM tools such as GitHub Copilot and ChatGPT [51]. They found that not only did students still face the same metacognitive difficulties, but some of these were exacerbated by the LLMs and new difficulties were also introduced. While the best students were able to accelerate with the help of GenAI tools, students who were already struggling faced more difficulties, which could widen the gap between the best and the worst performing students. This result is similar to help-seeking, where Hou et al. found that students who are capable at using models get the most benefits [28]. These issues highlight the need for novel pedagogies that help students to learn to successfully leverage GenAI models in their work."}, {"title": "2.2 Emerging LLM-Powered Pedagogies", "content": "Teaching students to write natural language prompts is an emerging area in computing education, and can be applied equally well to support both code writing and code comprehension tasks. Moreover, given the language translation capabilities of LLMs, pedagogies based on this idea could be applicable in any spoken language, potentially improving the accessibility of programming education for diverse student populations."}, {"title": "2.2.1 Prompt Problems", "content": "When exploring the performance of LLMs for solving typical CS1 problems, Denny et al. observed that making certain refinements to the prompts led to greater accuracy in the generated code [12]. They argued that learning how to craft effective prompts is essential for novice programmers, but did not directly propose teaching strategies for developing this skill. In subsequent work, Denny et al. introduced the idea of 'Prompt Problems' as a novel exercise for helping students learn how to solve computational tasks through natural language prompts [13]. In a Prompt Problem, a student would be shown a computational task, typically presented visually without any textual description, and they would write a natural language prompt for an LLM to generate code to solve that task. Prompt Problems allow shifting the focus from syntax mastery, often emphasized early in programming education, towards higher-level problem-solving. The authors presented and evaluated a tool called Promptly which would execute the generated code against predefined test cases to assess both the effectiveness of the prompt and the student's understanding of the problem. Feedback on the activity from students in both a CS1 and CS2 course suggested that they appreciated that it engaged their computational thinking skills and introduced them to new programming constructs. However, it also highlighted the need for further research into how these exercises could be best integrated into classroom practice and whether they could meaningfully improve learning outcomes. In addition, all prior research involving Prompt Problems has been in the context of English-language prompting."}, {"title": "2.2.2 Explain in Plain Language (EiPL)", "content": "While Prompt Problems can be viewed as an alternative approach to solving code writing tasks, by evaluating code generated from a student's prompt against a test suite, an analogous approach can be used to provide feedback on code comprehension tasks. There is a considerable body of research in the computing education literature on 'Explain in Plain English' (EiPE) questions, which require students to articulate the purpose of a code fragment in natural language. Early work exploring this type of question by Whalley et al. identified a connection between students' programming skills and their ability to describe code accurately in simple terms [66]. Over the following decade, further research validated this approach across various contexts [40, 43, 64]. In 2014, Corney et al. highlighted that EiPE tasks enhance students' ability to reason about code, which can, in turn, improve their coding skills [9], however scaling these exercises has been challenging due to the subjective nature of grading free-text responses.\nApplying LLMs to the task of grading EiPE responses has shown great promise [15]. Smith et al. proposed 'Code Generation Based Grading' (CGBG) [58], a method conceptually similar to the idea for generating feedback on Prompt Problems, by using LLMs to generate code based on a student's EiPE response. The generated code is then tested against predefined test cases, offering both objective grading and actionable feedback for students. Their study demonstrated that CGBG aligns well with human grading while providing a scalable solution for large classes, making it a promising approach for automated assessments. Smith et al. further explored the connection between prompt writing and code comprehension [57]. Their research showed that students benefit from the dual challenge of writing prompts and understanding generated code. By performing both tasks, students not only develop their code comprehension skills but also gain proficiency in prompt engineering. Similar recent work has shown that LLM-grading of EiPE questions is engaging for students, but importantly also found that relational responses, where students integrated code elements into high-level summaries, were the most successful in generating correct code"}, {"title": "2.2.3 Multilingual Support in Programming Education", "content": "Non-native English-speaking students often face significant challenges in programming education [3, 22, 23]. For example, Becker highlighted how the predominance of English in programming languages, documentation, and error messages adds a significant cognitive load to non-native speakers, making it harder for them to fully participate in programming courses [4]. This study revealed that non-native speakers often encounter difficulties interpreting keywords and comments, increasing their cognitive load and limiting their ability to focus on problem-solving. Although instructors have been shown to adjust their speech patterns and vocabulary to meet the needs of diverse student groups [5], modern LLMs have suddenly opened the door to providing significant multilingual support in computing education. Recent work by Jordan et al. explored the use of LLMs to generate programming exercises in Spanish, Vietnamese, and Tamil [31]. Although the model they used at the time of their research is no longer state of the art, support for generating resources in Spanish and Vietnamese was generally good. Despite some limitations, which will likely lessen over time as model capabilities improve, the authors see great potential in using LLMs as a pathway for creating culturally relevant programming resources tailored to non-native speakers. This is a particularly important result given that when students see their identities reflected in learning activities, they experience positive academic and social outcomes, making computer science more relevant and accessible to them [30].\nThere has recently been some early exploration of multilingual support for prompting-based activities in computing education. Smith et al. explored the use of EiPE questions in Indic languages such as Hindi, Tamil, and Marathi, revealing both opportunities and challenges in supporting multilingual learners [29]. While students appreciated the ability to engage with programming tasks in their native languages, many still preferred English for technical precision and familiarity. This suggests that while multilingual support can lower accessibility barriers, students' existing preferences and technical contexts must also be considered when designing programming exercises.\nThe relationship between programming and natural languages has been further explored by Veldthuis and Hermans who applied natural language vocabulary acquisition models to programming education [63]. Their study restructured introductory programming lessons to include strategies typically used in second language learning, such as scaffolding vocabulary - the gradual introduction of new concepts in phases, with opportunities for practice and verification. Using the Hedy programming environment, students were progressively exposed to increasing complexity, initially permitting errors in syntax to mirror natural language learning processes. The researchers found that these strategies not only enhanced students' understanding of programming concepts but also improved engagement and motivation. This approach aligns with the broader goal of our present study, which investigates how LLMs can support multilingual Prompt Problems by offering adaptive, language-aware programming exercises that accommodate students' linguistic backgrounds and lower cognitive barriers to learning."}, {"title": "3 METHODOLOGY", "content": "We explored the use of Prompt Problems in three distinct environments: a university with English as the language of instruction, but where NNES students completed tasks using Chinese; a Portugese university where students completed tasks using Portugese; and, a Middle-Eastern university in which students used Arabic. In each institution, the prompts used by students were collected along with a post-survey."}, {"title": "3.1 Data Collection", "content": "The first data collection was conducted in February 2024 at Polytechnic University of Coimbra in Portugal with 27 students whose native language was Portuguese. Students were in a CS2 course learning Python and instructed in Portuguese. The second data collection was conducted in May 2024 at the University of Auckland, New Zealand, in a post-graduate course with 19 students using Python. The language of instruction was English, but most of the students enrolled in the course were non-native English speakers whose primary language was Chinese. The third data collection was also in May 2024 at Umm Al-Qura University in Saudi Arabia with 34 students. These students were undergraduates learning in Java and the language of instruction was Arabic. In each context, the instructor explained the concept of Prompt Problems beforehand and it was demonstrated to them. Students were then invited to participate by completing the Prompt Problems shown in Table 1 by prompting in their native (i.e. non-English) language. After the activity, a short post-survey was administered to capture their experience solving Prompt Problems in their native language."}, {"title": "3.2 Analysis", "content": "We performed a quantitative analysis of the prompt submission data (RQ1) and a qualitative analysis of the survey data (RQ2)."}, {"title": "3.2.1 Quantitative Analysis", "content": "The initial dataset included 1,771 total prompts by students in all three groups. Because we were only interested in the prompts by NNES students, we cleaned the data by removing all prompts by students who only prompted in English. While it's possible that a NNES student decided to only prompt in English, our research questions are directed at the experience of students prompting in their native language. Therefore, all of the 1679 remaining prompts contained at least some non-English words. We then calculated completion data for each problem for each language group.\nAfter visually examining the text of the remaining prompts, two researchers created six categories of prompts based on the amount of English, native language, syntax, or code in the prompt. Four other researchers then categorized every prompt into one of those categories. If there was concern about which category to place a prompt into, the four researchers categorizing discussed the prompt"}, {"title": "3.2.2 Qualitative Analysis", "content": "The data was coded by a member of the research team following a reflexive thematic analysis as outlined by Braun and Clarke [7]. This approach highlights the researcher's active role in the development of themes. This inductive approach is particularly suited to exploratory research with smaller, diverse samples. Given the relatively small sample size and the heterogeneity between the three participant groups, an inductive strategy facilitated the generation of themes grounded in the data, rather than being constrained by preconceived theoretical frameworks.\nThe primary goal of the analysis was therefore to identify patterns and trends that can contextualize our quantitative findings and to inform new hypotheses. Unlike in positivist methods, where sample size is predetermined based on statistical power, thematic analysis does not require a set number of participants. Instead, our process was guided by thematic saturation, which is a point at which new responses cease to produce new insights [21]. In our analysis, thematic saturation was considered with regards to each participant linguistic group. Each new sample provided additional insights, but within groups, saturation was often observed within the first 15 responses. Responses consistently aligned with the primary themes that emerged early in the coding process for each sample, and despite minor variations in individual responses, no new dominant themes were identified during the latter stages of"}, {"title": "4 RESULTS", "content": "The data presented in Table 2 show that a majority of students were able to complete the exercises using at least some of their native language. As noted in previous work on Prompt Problems, the sequential nature of the tasks means there are naturally fewer students who attempt subsequent problems [13].\nThe data from the categorization (see Table 3) show that most students attempted to prompt in their native language (category \"N\"). The next highest categories were \u201cM\u201d (Mixed language) and \"E\" (almost all English except for one native language word or phrase). Given that students were able to solve the Prompt Problems (see Table 2) and that most prompted in their native language almost exclusively, these data show that students were able to successfully solve the Prompt Problems in their native language. However, there is a clear divide between correct solutions in Chinese and Portuguese compared to Arabic. For category \"N\", Chinese had a 15% success rate with that prompting strategy and Portuguese had"}, {"title": "4.1 Quantitative Data", "content": "The data presented in Table 2 show that a majority of students were able to complete the exercises using at least some of their native language. As noted in previous work on Prompt Problems, the sequential nature of the tasks means there are naturally fewer students who attempt subsequent problems [13].\nThe data from the categorization (see Table 3) show that most students attempted to prompt in their native language (category \u201cN\u201d). The next highest categories were \u201cM\u201d (Mixed language) and \"E\" (almost all English except for one native language word or phrase). Given that students were able to solve the Prompt Problems (see Table 2) and that most prompted in their native language almost exclusively, these data show that students were able to successfully solve the Prompt Problems in their native language. However, there is a clear divide between correct solutions in Chinese and Portuguese compared to Arabic. For category \u201cN\u201d, Chinese had a 15% success rate with that prompting strategy and Portuguese had"}, {"title": "4.2 Qualitative Data", "content": "Across the survey responses, three primary themes were identified regarding the use of native versus foreign language. First, participants shared that models generally performed poorly when interacting in their native language. This poor performance appeared to be more pronounced for the Arabic-speaking sample, a finding which can be seen in the quantitative data above. Second, participants highlighted trade-offs between using their native language and communicating in English, often finding English leads to better results, despite being less expressive for them. Finally, some participants mentioned that while their native spoken language was not English, they were more accustomed to coding in English, suggesting it felt more natural to use English in the context of programming. In the following subsections, participants are labeled by language: PA = Participant Arabic, PC = Participant Chinese, PP = Participant Portuguese."}, {"title": "4.2.1 Poor Support for Some Languages", "content": "A recurring theme across all participant samples was the inadequate support LLMs provide for some languages compared to others. This observation is consistent with prior research in natural language processing, which refers to these languages as 'low-resource'-languages that have less data available in the original model's training set [45]. For example, PA-1 explained how ChatGPT can face challenges when understanding the Arabic language:\n\"There are challenges with ChatGPT's understanding of\nthe Arabic language.\" (PA-1)\nAnecdotally, this theme was much more prevalent in responses in the Arabic-speaking sample than in the Chinese-speaking sample. This again relates to the concept of 'low-resource' languages as there is currently more Chinese text included in training sets for large language models.\nOne participant speculated that this might stem from code often being written in English, which creates a gap when using other languages to interact with code or to explain technical concepts.\n\"In English, it's easy to write what you want because\nthe programming is in English, while in Arabic, it was\na bit difficult to convey the information to ChatGPT\"\n(PA-4)\nDue to this poor performance for native languages, multiple participants expressed an explicit preference for using English rather than their native language when interacting with language models.\n\"It was somewhat bad, but I think it would be much\nbetter if it were in English.\" (PA-14)\nInterestingly, some participants implied that using their second language, English, had an unexpected benefit. They described how writing prompts in English required them to be more deliberate and considerate about the language they used, which forced them to slow down and organize their thoughts more carefully.\n\"I'm so familiar with native language, so the advantage\nis I can write the command more smoothly, while this\nmakes my command omit some points, which makes\nthe model hard to understand. [To] use English, I need\nto organize my language and always consider about the\ngrammar, so it might be a little bit slow, but accurate.\u201d\n(PC-16)\nThis observation suggests that slowing down and planning may offer metacognitive benefits, as planning is an important metacognitive strategy. However, more research is needed to systematically investigate this potential effect.\nDespite the poor performance claimed by many participants, a small minority of participants expressed a preference for their native language. For example PA-24 explained that Arabic was easier but required more effort for them to explain the requirements to the model:\n\"The Arabic language is easier, but it requires a lot of\nexplanation.\" (PA-24)"}, {"title": "4.2.2 Trade-offs between Expressivity and Model Performance", "content": "Participants across the samples frequently shared about the trade-offs they face being more expressive in their native language and achieving better model performance when using English. Many participants used the word 'expressive' to describe how much easier it was to communicate their intent and goals in their native language. For example, PC-12 explained:\n\"Native language is more easy for me to express my\nthoughts. But more difficult for ChatGPT to understand\nme.\" (PC-12)\nAnd this experience was also shared by PA-4 in the Arabic-speaking sample:\n\"The advantages are that Arabic is my language, so I\nwill have strong expression in it. However, the disad-\nvantage I faced is that the program did not understand\nthe explanation well.\" (PA-4)\nThis trade-off was not unique to one language. For instance, PA-33 and others shared similar experiences about how language models did not appear to understand some of the words from their native language:\n\"Since it is my native language, it was much easier than\nEnglish. [However,] it didn't understand some Arabic\nwords.\" (PA-33)\nAnother participant noted the balance between providing more detailed descriptions in their native language and achieving greater accuracy in English:"}, {"title": "4.2.3 Writing Native English Code", "content": "While the participants are not native English speakers, many of them described how they had a lot of experience writing code in English and that trying to think about writing code in their own language presented a unique challenge. For example, PC-6 highlights how variable names did not translate well from Chinese to English:\n\u201cI think the advantage is using my native language, I\ncan describe the situation better and the disadvantage\nmay be the variable name is English and it can't be\ntranslated accurately.\u201d (PC-6)\nSimilarly, PC-9 claimed that \"Certain words don't exist\", which suggests that even when writing in their native language, participants had to incorporate English terminology. PC-13 echoed this sentiment, explaining that naming conventions in code still had to conform to English standards, regardless of the language used for other parts of the input:\n\"It behaves quite well in both English and Mandarin.\nbut it have to make me explicitly name the function\nname so that it can pass.", "problem-solving": "n\"I am really used to solve programming problems in eng-\nlish, answers, documentation, tutorials, youtube videos\nare mostly in English, trying to write them in my own\nlanguage felt like I was translating my own thoughts.\nHowever writing prompts helps me have a look at the\noverall objective of a task and try to be precise which\ncan be frustrating at first, but I guess it is useful in the\nlong run."}, {"title": "5 DISCUSSION", "content": "In this work, we investigated students' success when using their non-English native languages to solve Prompt Problems (RQ1) and examined how using native-language prompts influenced their experience (RQ2). While it is now widely known that ChatGPT and other LLMs are highly effective at solving programming problems [49] - particularly at the introductory level [19, 20] \u2013 their effectiveness with non-English prompts remains underexplored. As GenAI tools become increasingly integrated into computing education [14], understanding how they perform for non-English speakers is essential to fostering an inclusive learning environment.\nOur findings indicate varying levels of success across language groups: students using Portuguese and Chinese achieved relatively high success rates, whereas Arabic speakers faced greater challenges in generating correct solutions. In terms of perceptions, although many students felt more able to express themselves using their native language, they often thought that using English was better for solving the Prompt Problems citing better performance and closer alignment with programming constructs. Several possible explanations may account for the challenges faced by students in this study."}, {"title": "5.1 Challenges of Non-English Prompting", "content": "Firstly, although state-of-the-art LLMs have been shown to have multilingual capabilities, they are most capable in English due to the simple fact that the vast majority of the data they have been trained on is in English [41]. In fact, although information on how proprietary models work internally is difficult to find, it appears that most generative AI models will first translate a non-English prompt into English before attempting to answer it [60]. As a result of this English-centric bias, it is not surprising that models may be less effective at determining the intent of non-English prompts [16, 67]. Our results align with prior research that shows LLMs perform better with high-resource languages where more training data is available. Specifically, Arabic language data makes up less"}, {"title": "5.2 Implications for Practice", "content": "Despite the challenges we observed and have hypothesized, our findings also highlight the potential for using the language translation capabilities of LLMs to make Prompt Problems \u2013 and other kinds of new pedagogical assessments using GenAI more broadly and globally accessible. This enables students to interact with programming concepts in their native languages, lowering barriers to entry and providing immediate access to programming activities and problem-solving tasks without being constrained by their English language abilities.\nOur work contributes to the growing body of literature on broadening access to programming education for students with limited English proficiency. Kumar's Refute questions, for example, offer an alternative to Explain in Plain English (EiPE) questions by requiring students to identify incorrect logic rather than having to articulate explanations in English [2, 36]. Recent work by Smith et al. demonstrates that Code Generation Based Grading can allow students to answer code comprehension questions in multiple languages, including with high correctness rates across several Indic languages [29]. Similarly, we have observed that Prompt Problems offer students from diverse linguistic backgrounds a way to engage meaningfully with programming concepts in their preferred languages, promoting a more inclusive learning environment. While"}, {"title": "5.3 Limitations", "content": "This study has several limitations that could be addressed in future work. First, the three groups of learners differed in their educational backgrounds: the students prompting in Chinese were postgraduates, while those prompting in Portuguese and Arabic were undergraduates. This variation may have influenced the results, as postgraduate students are typically more experienced with programming concepts. Second, the programming languages of instruction were not the same across the groups. Students prompting in Arabic generated code in Java, while those prompting in Chinese and Portuguese generated code in Python. While these differences in programming language could influence performance, we believe the findings remain valid, as current AI models are proficient at generating correct code in multiple popular languages, including Java and Python. Moreover, this reflects an interesting strength of the study, as the results suggest that non-English prompting can work across different programming languages.\nThird, the study focused on three languages - Arabic, Chinese, and Portuguese \u2013 which limits the generalizability of our findings to other languages. Variations in the linguistic structure and the representation of languages in the training data of AI models could lead to different outcomes for students whose native languages were not included in this study. Further research is needed to explore the effectiveness of native-language prompting in a broader range of languages, including those classified as low-resource languages.\nFinally, the sample sizes for each language group were relatively small, and additional factors, such as individual language fluency, prior programming experience, and familiarity with AI tools, could influence students' success. These factors should be further investigated in future work."}, {"title": "6 CONCLUSION", "content": "English has long been the primary language of programming instruction, with the keywords and syntax of most popular programming languages being English-centric. This has created a widely acknowledged barrier for non-native English speakers who must navigate both linguistic and technical challenges in computing courses. With the rise of generative AI (GenAI), and in particular the language translation capabilities of modern large language models, there is potential to lower this barrier by allowing students to approach problem-solving in their native languages. To investigate this potential, we introduced learners fluent in Chinese, Arabic, and Portuguese to Prompt Problems, a new kind of task in which programming exercises are solved by crafting natural language prompts instead of writing code directly. We found that students working in Portuguese and Chinese were generally able to achieve high success rates on these tasks, often expressing that native-language prompting allowed for a more natural articulation of their intent. In contrast, Arabic-speaking students appeared to face greater challenges, with lower accuracy and higher model misinterpretations. This finding may reflect the limited quantity of training data in Arabic that modern GenAI models have been trained on. Across all language groups, we observed a balance between expressivity and model performance - students often found it easier to formulate ideas in their native language, but English often yielded more accurate responses, especially for programming terms embedded in the English-language syntax. While we expect GenAI capabilities to continue to improve over time, especially for currently under-represented languages, our findings illustrate the transformative potential of existing multilingual GenAI tools. They can serve as valuable aids in democratizing programming education, making it more accessible and engaging for diverse learners globally."}]}