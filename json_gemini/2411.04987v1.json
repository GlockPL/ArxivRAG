{"title": "Few-Shot Task Learning through\nInverse Generative Modeling", "authors": ["Aviv Netanyahu", "Yilun Du", "Antonia Bronars", "Jyothish Pari", "Joshua Tenenbaum", "Tianmin Shu", "Pulkit Agrawal"], "abstract": "Learning the intents of an agent, defined by its goals or motion style, is often\nextremely challenging from just a few examples. We refer to this problem as\ntask concept learning and present our approach, Few-Shot Task Learning through\nInverse Generative Modeling (FTL-IGM), which learns new task concepts by lever-\naging invertible neural generative models. The core idea is to pretrain a generative\nmodel on a set of basic concepts and their demonstrations. Then, given a few\ndemonstrations of a new concept (such as a new goal or a new action), our method\nlearns the underlying concepts through backpropagation without updating the\nmodel weights, thanks to the invertibility of the generative model. We evaluate our\nmethod in five domains \u2013 object rearrangement, goal-oriented navigation, motion\ncaption of human actions, autonomous driving, and real-world table-top manip-\nulation. Our experimental results demonstrate that via the pretrained generative\nmodel, we successfully learn novel concepts and generate agent plans or motion\ncorresponding to these concepts in (1) unseen environments and (2) in composition\nwith training concepts.", "sections": [{"title": "1 Introduction", "content": "The ability to learn concepts about a novel task, such as the goal and motion plans, from a few\ndemonstrations is a crucial building block for intelligent agents it allows an agent to learn to\nperform new tasks from other agents (including humans) from little data. Humans, even from a young\nage, can learn various new tasks from little data and generalize what they learned to perform these\ntasks in new situations [1].\nIn machine learning and robotics, this class of problems is referred to as Few-Shot Learning [2].\nDespite being a widely studied problem, it remains unclear how we can enable machine learning\nmodels to learn concepts of a novel task from only a few demonstrations and generalize the concepts\nto new situations, just like humans do. Common approaches learn policies either directly, which often\nsuffer from covariate shift [3], or via rewards [4-6], which are largely limited to previously seen\nbehavior [7]. In a different vein, other work has relied on pretraining on task families and assumes\nthat task learning corresponds to learning similar tasks to ones already seen in the task family [8, 9].\nInspired by the success of generative modeling in few-shot visual concept learning [10-12], where\nconcepts are latent representations, in this work, we investigate whether and how few-shot task\nconcept learning can benefit from generative modeling as well. Learning concepts from sequential\ndemonstrations rather than images is by nature more challenging due to sequential data often not"}, {"title": "2 Related Work", "content": "Learning from few demonstrations. Our problem setting is closely related to learning from few\ndemonstrations. There has been much work on learning to generate agent behavior given few\ndemonstrations. There are several common approaches to this problem. First, behavior cloning (BC)\nis a supervised learning method to learn a policy from demonstrations that predicts actions from\nstates. Similar to our framework, goal-conditioned BC can predict states from task representations\nand states [18]. Finetuning these models to learn new behaviors requires labeled demonstrations of\nthe new task. We assume unlabeled demonstrations. BC often suffers from covariate shift [3] and fails\nto generate the demonstrated behavior in novel scenarios. This can be mitigated by assuming access\nto a human in the loop [19]. Second, the inverse reinforcement learning (IRL) framework learns a\npolicy that maximizes the return of an explicitly [5, 6, 20] or implicitly [21] learned reward function.\nThese works learn a reward for a single task or for a set of tasks (e.g., goal-conditioned IRL [22, 23]\nand multi-task IRL [24]). While IRL is more data efficient than BC, it is computationally costly\ndue to learning a policy every iteration via an inner reinforcement learning (RL) loop. Additionally,\nit requires access to taking actions in the environment during training and when faced with a new\ntask, we have to retrain the reward again. A third approach is inverse planning [25-27], which can\nrobustly infer concepts such as goals and beliefs even in unseen scenarios. However, it assumes access\nto a planner, knowledge about environment dynamics, and the task/goal space. Finally, in-context\nlearning approaches [8, 28, 29] learn actions in a supervised manner by representing the task with\ndemonstrations. This allows few-shot generalization without further training.\nIn contrast, we do not learn an action-generating policy directly or via a reward function. For concept\nlearning, we do not assume having access to any given planner, world model, actions, rewards, or\nprior over the task space. Instead, we learn concepts (task representations) from demonstrations via a\npretrained generative model that takes a concept as input and directly produces state sequences. We\nthen input the learned concept into the generative model to produce behavior similar yet diverse to\nthe demonstrated one. We further demonstrate how to use these state sequences with a planner to\ntake actions and achieve the desired behavior. The idea of concept learning via generative models has\nbeen explored for computer vision applications [11, 12]. We build on this work and show how to\nextend it to learn agent task concepts. Our work also differs from prior works on learning trajectory\nrepresentations [30-34]. These works focus on learning plans over trajectory embeddings, whereas\nwe learn a task representation from demonstrations on which we condition to generate behavior.\nGenerative Models in Decision Making. There has been work on generative modeling for decision-\nmaking, including generative models for single-agent behaviors, such as implicit BC [35], Diffuser\n[36, 17], Diffusion Policy [37], Decision Transformer [38-40], and for multi-agent motion prediction\nsuch as Jiang et al. [41]. The success of diffusion policy in predicting sequences of future actions\nhas led to 3D extensions [42], and combined with ongoing robotic data collection efforts [43] and\nadvanced vision and language models, has led to vision-language-action generative models [44-46].\nIn this work, we utilize a conditional generative model for the inverse problem, i.e., learning concepts\nfrom demonstrations.\nComposable representations. There has been work on obtaining composable data representations.\nB-VAE [47] learns unsupervised disentangled representations for images. MONet [48] and IODINE\n[49] decompose visual scenes via segmentation masks and COMET [50] and [12] via energy functions.\nThere is also work on composing representations to generate data with composed concepts. Generative\nmodels can be composed together to generate visual concepts [14, 51-55] and robotic skills [17]. The\ngenerative process can also be altered to generate compositions of visual [56-59] and molecular [60]\nconcepts. We aim to obtain task concepts and generate them in composition with other task concepts."}, {"title": "3 Formulation", "content": "Inspired by recent success in large generative models, we propose a generative formulation for learning\nspecific behavior given a small set of demonstrations, which we term Few-Shot Task Learning through"}, {"title": "4 Few-Shot Concept Learning Based on FTL-IGM", "content": "We adapt a few-shot concept learning method to task concepts based on the FTL-IGM framework.\nDuring training we learn a generative model $G_\\theta$ from training ${(\\tau_i, c_i)}_i$ pairs. We then freeze $G_\\theta$,\nand given demonstrations of a new task ${\\{\\tau\\}_i}$, optimize a concept $\\tilde{c}$ to produce the new behavior via\n$G_\\theta$. We then generate a diverse set of behaviors via $G_\\theta$, either for the learned concept $\\tilde{c}$ conditioned\non new initial states or for compositions of $\\tilde{c}$ with other concepts."}, {"title": "4.1 Training a diffusion model to generate behavior", "content": "A diffusion model is a generative model that given a forward noise adding process $q(x_t|x_{t-1}) :=$\n$\\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_t I)$ starting from data $x_0$ according to a variance schedule $\\beta_1, ..., \\beta_T$, learns\nthe reverse process $p_\\theta(x_{t-1}|X_t) := \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$. Ho et al. [61] simplify the training\nobjective to estimate noise $\\mathbb{E}_{t~U\\{1,T\\}, x_0, \\epsilon~\\mathcal{N}(0,1)}[||\\epsilon - \\epsilon_\\theta(x_t,t)||_2]$ where $x_t$ is produced by adding\nnoise $\\epsilon$ to data $x_0$ by the forward noising process at diffusion step t, $q(x_t|x_0) := \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, (1 -$\n$\\bar{\\alpha}_t)I)$ where $\\alpha_t := \\Pi_{s=1}^t (1 - \\beta_s)$. Dhariwal and Nichol [62] enable conditioned generation\nby guiding the reverse noising process with classifier gradients. The noise prediction becomes\n$\\epsilon = \\epsilon_\\theta(x_t, t) - w\\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{x_t} log\\ p_\\phi(y|x_t, t)$ where classifier $p_\\phi(y|x_t, t)$ is trained on noisy images,\nand w is the guidance scale. Ho and Salimans [63] introduce classifier-free guidance that achieves\nthe same objective without the need for training a separate classifier. This is done by learning a\nconditional and unconditional model by removing the conditioning information with dropout during\ntraining. The noise prediction is then $\\epsilon = \\epsilon_\\theta(x_t, t) + w(\\epsilon_\\theta(x_t, y, t) - \\epsilon_\\theta(x_t, t))$. Ramesh et al. [64]\nand Nichol et al. [65] demonstrate how this idea can be used to generate images conditioned on a\nclass. Diffusion models have recently shown success as generative models for decision making"}, {"title": "4.2 Few-shot concept learning", "content": "Gal et al. [11] use a frozen generative model to learn visual concept representations from few\nimages depicting the concept by optimizing the model's input $v^* = arg\\ min\\ \\mathbb{E}_{x_0,v,\\epsilon~\\mathcal{N} (0,1),t} [||\\epsilon -$\n$\\epsilon_\\theta(x_t, C_0(v), t)||_2]$, where $c_0$ and $e_\\theta$ are fixed. Liu et al. [12] extend this and learn visual concept\ncompositions with a pretrained diffusion model in an unsupervised manner. Namely, from a set of\nimages that depict various concepts, for each image $x^i$ they learn a set of weights $w_i$ and a shared\nset of visual concepts for all images $c_k$, $\\epsilon = \\epsilon(x, t) + \\sum_{k=1}^{K} w_i(\\epsilon(x^i, c_k, t) - e(x, t))$. We extend\nthese formulations to inferring multiple concepts, whose composition describes a single task concept,\nfrom few demonstrations of a task.\nGiven a trained diffusion model $e_\\theta$ and demonstrations of a new concept ${\\{\\tau\\}_i}$ from $D_{new}$, we learn\nconcepts ${\\{\\tilde{c}_1, ..., \\tilde{c}_K\\}}$ for $K \\ge 1$ and their weights ${\\{w_1, ..., w_K\\}}$ that best describe the demonstrations.\nStarting from uniformly sampled concept embeddings $\\tilde{c}_k \\sim U([0, 1]^n)$, we freeze $e_\\theta$, and optimize\n$\\tilde{c}_k$ and $w_k$:\n$\\mathbb{E}_{\\epsilon~\\mathcal{N}(0,1)} [||\\epsilon - (\\epsilon_\\theta(x_t(\\tau), C_0, s_0, t) + \\sum_{k=1}^{K} w_k (\\epsilon_\\theta(x_t(\\tau), \\tilde{c}_k, s_0, t) - \\epsilon_\\theta(x_t(\\tau), C_0, s_0, t)))||_2].$\nWe find that this compositional approach enables us to effectively represent and learn new demonstra-\ntions, even when demonstrations are substantially different than those seen in training tasks."}, {"title": "4.3 Generating the learned concept", "content": "After learning concepts $\\tilde{c}_k$, whose composition describes the new task $\\tilde{c}$, we evaluate the behavior\nit generates by initializing $x_T(\\tau) \\sim \\mathcal{N}(0, \\alpha I)$, and compute $x_t \\sim \\mathcal{N}(\\mu_{t-1}, \\alpha\\Sigma_{t-1})$ iteratively as a\nfunction of the estimated denoising function $\\epsilon(\\epsilon_\\theta)$, where $\\mu$ and $\\Sigma$ are the mean and variance that\ndefine the reverse process, and $\\alpha \\in [0, 1)$ is a scaling factor that leads to lower temperature samples,\nuntil generating $x_0 = \\tau$ representing the trajectory of the agent. The denoising function is constructed\nby fixed or learned weights as defined in Eq. 2 and by any number of concepts $> 1$. The applications\nof the generation procedure can be summarized as:\nLearned concept and demonstrated initial states. We apply our learned concept to a set of\ndemonstrated initial states. In domains where the initial state and concept jointly determine optimal\nbehavior, the generated trajectory corresponds to optimal actions to execute (e.g. goal-oriented\nnavigation). In contrast to other domains where the initial state is irrelevant for a task due to the\nrandomness in sampling $x_T(\\tau)$ (e.g. motion capture), generated trajectories correspond to diverse\nplausible behaviors exhibiting the learned concept.\nLearned concept and novel initial states. We further apply the generation procedure from our\nlearned concept on novel initial states, to generate trajectories of new behaviors exhibiting our\nconditioned concept. Prior methods may suffer from covariate shift in this setting [21]. We empirically\nshow that our method is less prone to this problem.\nLearned concept composed with other concepts. Finally, we modify the generation procedure of\nour newly learned concept to generate trajectories that simultaneously exhibit other concepts. To\ngenerate a trajectory with an added another concept, we add another term to the sum in Eq. 2 where the\nlearned concept is composed with a training concept $c_k$ and its weight $\\omega_k$: $\\omega_k(\\epsilon_\\theta(x_t(\\tau), c_k, s_0, t) -$\n$\\epsilon_\\theta(x_t(\\tau), C_0, s_0,t))$. This modified generation procedure constructs trajectories which exhibits\nbehavior that has a composition of the learned concept and the other specified concepts [14]."}, {"title": "5 Experiments", "content": "We demonstrate results in four domains where concept representations are T5 [66] embeddings of task\ndescriptions in natural language for training, and empty string embeddings for the dummy condition.\nDuring few-shot concept learning, we are provided with three to five demonstrations of a composition\nof training concepts or of a novel concept that is not an explicit composition of training tasks in\nnatural language symbolic space. We ask a model to learn the concept from these demonstrations."}, {"title": "5.1 Task-Concepts", "content": "Learning concepts describing goals that are spatial relations between objects. Object rearrange-\nment is a common task in robotics [67-69] and embodied artificial intelligence (AI) [70, 71], serving\nas a foundation for a broader range of tasks such as housekeeping and manufacturing. Here, we use a\n2D object rearrangement domain to evaluate the ability of our method to learn task specification con-\ncepts. Given a concept representing a relation between objects, we generate a single state describing\nthat relation. The concept in a training example describes the relation (either 'right of' or 'above')\nbetween only one pair of objects (out of three objects) in the environment. Then, a model must\nlearn compositions of these pairwise relations and new concepts such as 'diagonal' and 'circle' (see\nFigure 4). The results in Figures 5 and 6 demonstrate that our method learns unseen compositions\nof training concepts and new concepts. They further demonstrate how our method composes new\nconcepts with learned concepts. For additional qualitative results, please refer to Appendix A.\nWhile successful in most cases, there are also a few failure examples. The accuracy for the new\n'circle' concept is low (0.44) compared to the mean over task types in Figure 6 Object Rearrangement\nNew Concept (0.82\u00b10.09). This is most likely due to this concept lying far out of the training\ndistribution. The task 'square right of circle / triangle above circle' has low accuracy for 2 concepts\n(0.32) compared to the mean in Table 2 Object Rearrangement Training Composition (0.75 \u00b1 0.11).\nThis may arise from the combined concept-weight optimization process as there is no explicit\nregularization on weights, they may converge to 0 or diverge. In Figure 12, we show that concept\ncomponents may or may not capture new concept relations.\nLearning concepts describing goals based on attributes of target objects. We test our method in a\ngoal-oriented navigation domain adapted from the AGENT dataset [72], where an agent navigates\nto one of two potential targets. Conditioned on a concept representing the attributes of the desired"}, {"title": "5.2 Baselines", "content": "Goal conditioned behavior cloning. We compare our method with goal-conditioned behavior\ncloning (BC), which, given a condition and a state, outputs the next state in a sequence. It is\ntrained on our paired pretraining dataset and learns concepts by optimizing the input condition to\nmaximize the likelihood of new concept demonstration transitions. We test its ability to compose\nnew learned concepts with training concepts naively by adding conditions that are then inputted into\nthe model. We observe that even though goal-conditioned BC has access to the pretrained dataset and\nconditions, and while it may learn new concepts, it suffers from covariate shift on new initial states\nand lacks the ability to generalize to novel compositions of the learned concept with training concepts\n(Figures 6, 9, 13). To achieve these generalization capacities, we need a model that can process\ninterpolated (initial states) and composed (concepts) conditions, such as our generative model.\nLearning the concept space with a VAE. We compare our method with VAE [76] that does not\nutilize the concepts in the paired pretraining data but rather learns the concept space by reconstructing\npretraining data trajectories through their encoded representation z. Trajectories are generated via a\ndecoder conditioned on an initial state and z with added noise. z is obtained by encoding a trajectory\nfor training evaluation and by fixing the decoder and optimizing z to generate a given demonstration\nwhen learning a new concept. We find that the VAE model learns a latent space that captures training\nand new concepts but does not enable generalization to new initial states (Figures 6, 9, 13). This\nhighlights the importance of concept representations in the pretrained data."}, {"title": "5.3 Learning two concepts yields higher accuracy than one concept", "content": "When learning weights together with concepts, we check the effect of the number of learned concepts\nand weights. We report results in Table 2 for Object Rearrangement, AGENT, and Driving, and find\nthat, on average, learning two concepts improves concept learning. We demonstrate qualitatively for\nMoCap that learning two conditions is preferable. In 'jumping jacks', we observe that the motion\nlacks raising and lowering both arms and in \u2018breaststroke', it lacks complete arm and upper torso\nmovement. Results are best viewed on our website."}, {"title": "5.4 How are learned new concepts related to training concepts?", "content": "New concepts that are compositions of training concepts. We analyze what the learned two\nconcepts in Object Rearrangement and AGENT learn for novel concept compositions (e.g., 'red\nbowl'). For each concept (e.g., \u2018red' and 'bowl'), we generate two sets of 50 samples from the learned\ncomponents. Table 3 shows accuracy for these sets over the concepts. In some cases (most notably the\n'line' concept in Object Rearrangement, \u2018circle right of triangle and triangle right of square'), each"}, {"title": "6 Discussion and Limitations", "content": "In this work, we formulate the problem of new task concept learning as Few-Shot Task Learning\nthrough Inverse Generative Modeling (FTL-IGM). We adapt a method for concept learning based\non this new formulation and evaluate task concept learning against baselines in four domains. Our\nextensive experimental results show that, unlike the baselines, FTL-IGM successfully learns novel\nconcepts from a few examples and generalizes the learned concepts to unfamiliar scenarios. It also\ncomposes learned concepts to form unseen behavior thanks to the compositionality of the generative\nmodel. These results demonstrate the efficacy, sample efficiency, and generalizability of FTL-IGM.\nHowever, our work has several limitations. First, while our framework is general for any parameterized\ngenerative model, our implementation with a diffusion model incurs high inference time. We note\nthat there is still space for improvement in the MoCap generation quality and in the compatibility\nrate of demonstrations generated by composing learned and training concepts. In addition, we\nassume that learned concepts lie within the landscape of training concepts to learn them from a few\ndemonstrations without retraining the model. We have approached the question of what new concepts\ncan be represented by compositions of concepts in this landscape empirically, leaving a theoretical\nanalysis as future work. We are hopeful that with the continued progress in the field of generative AI,\nmore powerful pretrained models will become available. Combined with our framework, this will\nunlock a stronger ability to learn and generalize various task concepts in complex domains."}, {"title": "A Additional Results", "content": "Object Rearrangement. We display additional states generated by our model for various new\nconcepts. We generate learned concepts that are compositions of training concepts (Figure 14), new\nconcepts that are not explicit compositions of training concepts (Figure 15), and a new concept\ncomposed with a training concept (Figure 16). We analyze the learned concepts qualitatively\n(Figure 12) and quantitatively (Table 3).\nGoal-Oriented Navigation. We provide accuracy for open loop evaluation in Figure 13. We further\nanalyze the learned concepts quantitatively (Table 3)."}, {"title": "B Data Generation and Evaluation", "content": "B.1 Object Rearrangement\nTraining. The training dataset of ~ 11k samples consists of concepts 'A right of B' and 'A above\nB', where A and B are one of three objects: circle, triangle, or square. Altogether, there are 12\npossible concepts (two relations and three objects where order is important). In the data generation\nprocess, A at center position $(x_A, y_A) \\in [0,5]^2$ with radius $r_A \\in [0.3, 1]$ and angle $\\theta_A \\in [0, 2\\pi]$ is\nconsidered 'right of' B at center position $(x_B, y_B)$ with radius $r_B$ if $x_A > x_B$ and $|y_A - y_B| \\le r_A$.\nSimilarly, A is considered \u2018above' B if $y_A > y_B$ and $|x_A-x_B|< r_A$. We further verify that training\nobjects do not overlap.\nNew Tasks. The new scenarios include\n\\begin{itemize}\n    \\item Five novel compositions of training concepts (training composition in Figure 6). 'triangle\n    right of square /\\ triangle above square', 'square right of triangle /\\ circle above triangle',\n\\end{itemize}"}, {"title": "B.2 Goal-Oriented Navigation", "content": "Training. To collect demonstrations, we follow the data generation process in the AGENT bench-\nmark environment [72], which provides a planner for navigation given the desired target. In the\nprovided environment, the agent's initial position $a_0 = (0,0.102, -3.806)$, color (yellow) and\nshape (cone) are fixed. There are two objects: the target and a distractor. Each object has a color\n$o_1, o_2 \\in {red, yellow, purple, green}$, a shape $o_3, o_4 \\in {cone, sphere, bowl, cube}$ and a position\n$o_5 \\in [0,1.66] \\times \\{0.102\\} \\times [-4.355, -3.257], o_6 \\in [-1.66,0] \\times \\{0.102\\} \\times [-4.355, -3.257]$."}, {"title": "B.3 MoCap", "content": "Training. All human actions in the CMU Graphics Lab Motion Capture Database are included in\nthe training set except three new scenarios. We further discard videos with less than 128 frames. The\ntraining set includes 2210 demonstrations."}, {"title": "B.4 Autonomous Driving", "content": "Training. We use the following scenarios from the HighwayEnv driving simulation environment\n[75]. The training scenarios include four driving scenarios: \u2018highway', \u2018exit', \u2018merge', and \u2018intersec-\ntion'. In 'highway', the objective is driving on a highway at high speed on the rightmost lanes while\navoiding collisions. The highway has four lanes and 50 vehicles. The initial lane and position of all\nvehicles are sampled, as well as non-controlled vehicle speeds. An episode ends if the controlled\nvehicle crashes or a time limit is reached. In \u2018exit' the objective is to take a highway exit while driving\non a four-lane highway with an exit lane and 20 vehicles. The controlled vehicle is rewarded for\nexiting at high speed and driving on the rightmost lanes while avoiding collisions. The initial position\nof all vehicles is sampled. An episode ends if the controlled vehicle crashes or a time limit is reached.\nIn 'merge', the objective is driving on a highway with three lanes and a merging lane and three\nvehicles, one of them merging. The controlled vehicle is rewarded for driving at high speed while\navoiding collisions and allowing another vehicle to merge into the highway. The lane, position, and\nspeed are sampled for non-controlled vehicles. An episode ends if the controlled vehicle passes the\nmerging lane or crashes. In \u2018intersection', the objective is making a left turn at an intersection with\nfour two-way roads and 10 vehicles. The controlled vehicle is rewarded for crossing the intersection\nat high speed while staying on the road and avoiding collisions. The controlled vehicle's position is\nsampled, as well as the lane, position, and speed of other vehicles. An episode ends if the controlled\nvehicle completes crossing the intersection or crashes. Expert demonstrations were collected using a\ndeterministic tree search planner provided in [83]."}, {"title": "B.5 Table-Top Manipulation", "content": "Training. We collect 214 expert demonstrations with a Franka Research 3 robot via teleop with\na Spacemouse for four table-top manipulation tasks: 'pick green circle and place on book' (29\ndemonstrations), 'pick green circle and place on elevated white surface' (30), 'push green circle to\norange triangle' (124), and 'push green circle to orange triangle around purple bowl' (31). Examples\nof these tasks are best viewed on our website.\nNew task. The new scenario includes pushing the green circle to the orange triangle on a book. We\nprovide ten demonstrations of this task.\nState space. The initial state space we condition on includes an overhead RGB image of the scene\n(Figure 10) and the robot's end effector pose, a 7-tuple of its 3D position and quaternion, and gripper\nstate in R. We verify that most states are fully observable. The demonstration state space includes\nobservations of the end effector pose and gripper state in R8 for horizons $H_i, i \\in [N]$. We sample\ntrajectories to generate subtrajectories of length 32, which we train on."}, {"title": "C Implementation Details", "content": "Diffusion model. We represent the noise model $e_\\theta$ with an MLP for the object rearrangement\ndomain and with a temporal U-Net for the AGENT, MoCap and Driving domains as implemented\nin Ajay et al. [17]. For Manipulation, we use a temporal U-Net where images are processed"}]}