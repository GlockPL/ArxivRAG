{"title": "Research on Autonomous Robots Navigation based on Reinforcement Learning", "authors": ["Zixiang Wang", "Yining Wang", "Zhuoyue Wang", "Hao Yan", "Zhengjia Xu", "Zhizhong Wu"], "abstract": "Reinforcement learning continuously optimizes decision-making based on real-time feedback reward signals through continuous interaction with the environment, demonstrating strong adaptive and self-learning capabilities. In recent years, it has become one of the key methods to achieve autonomous navigation of robots. In this work, an autonomous robot navigation method based on reinforcement learning is introduced. We use the Deep Q Network (DQN) and Proximal Policy Optimization (PPO) models to optimize the path planning and decision-making process through the continuous interaction between the robot and the environment, and the reward signals with real-time feedback. By combining the Q-value function with the deep neural network, deep Q network can handle high-dimensional state space, so as to realize path planning in complex environments. Proximal policy optimization is a strategy gradient-based method, which enables robots to explore and utilize environmental information more efficiently by optimizing policy functions. These methods not only improve the robot's navigation ability in the unknown environment, but also enhance its adaptive and self-learning capabilities. Through multiple training and simulation experiments, we have verified the effectiveness and robustness of these models in various complex scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Mobile robots can make autonomous decisions and perform tasks according to changes in the environment, and their autonomous navigation capabilities are one of the key technologies to achieve industrial automation, improve production efficiency and reduce labor costs. In a complex and changeable production environment, path planning is the core technology for robots to achieve autonomous navigation. An excellent path planning algorithm can effectively reduce transportation costs, improve the efficiency of transportation operation management, and ensure the efficient operation of intelligent logistics systems, thereby further improving the efficiency and competitiveness of industrial production [1]. At present, most of the use scenarios of mobile robots are customized scenarios, such as independent planning and construction or targeted transformation of factories, which are relatively simple and helpful for effective path planning of robots.\nPath planning is a key part of robot navigation and is of great significance. In the early days, path planning mainly used pre-programmed paths, that is, a certain number of guide lines or magnetic guide markers were pre-installed on the ground of the factory or warehouse, and mobile robots automatically navigated according to these guide lines or markers. This type of path planning is known as guideway path planning [2]. Since the number and position of the guide lines or markers are fixed, this method makes path planning less difficult and ensures that the mobile robot does not deviate from the predetermined path while driving.\nAlthough leader line path planning is simple and easy, there are some drawbacks. First of all, pre-cabling requires a certain amount of cost and time, and at the same time, there are certain restrictions on the layout of the factory or warehouse. Second, guide wires or markers may need to be rerouted due to changes in the layout of the factory or warehouse, increasing maintenance costs [3]. Finally, the application scenarios of guide line path planning are relatively limited, and it is difficult to adapt to the path planning needs of different scenarios and complex environments.\nIn recent years, reinforcement learning has shown great potential in the field of autonomous robotics, especially for decision-making and control tasks in unknown environments. By interacting with its surroundings, the robot can learn the"}, {"title": "II. RELATED WORK", "content": "In recent years, scholars have made many innovations and improvements based on such methods. Wu et al. [9] proposed a hybrid algorithm combining a beetle antenna search algorithm and an artificial potential field algorithm for real-time path planning. Experiments show that this method can not only generate better paths, but also have significant advantages in planning time. Kashyap et al. [10] experimented in a dynamic terrain consisting of multiple NAO robots and some static obstacles using a combination of dynamic window method (DWA) and teaching-based optimization technique (TLBO). The results show that the technology shows robustness and effectiveness in the path planning and obstacle avoidance process of single and multiple humanoid robots to cope with static and dynamic terrain.\nMolinos et al. [11] proposed the dynamic obstacle window method and the dynamic obstacle tree window method, which ensure the normal operation of the robot in a dynamic environment by incorporating improvements such as robot speed to evaluate the stability of the planned path. In order to solve the problem that the traditional DWA algorithm only considers the obstacles on the trajectory, Saranrittichai et al. [12] proposed a regional dynamic window method (RDW), which modifies the objective function and considers the obstacles near the trajectory at the same time. The experimental results show that the robot can drive more safely when encountering near-orbit obstacles. Randhavane et al. [13] proposed a new pedestrian feature model to identify pedestrians based on the trajectory information of pedestrians in the navigation environment, so as to promote robot perception and navigation and avoid collisions with pedestrians. Experiments have shown that the robot is capable of performing socially aware navigation among dozens of pedestrians.\nThe existing methods are mainly improved on the basis of the traditional global path planning algorithm and local path planning algorithm, and the path planning performance of the algorithm is improved by increasing the path smoothness and reducing the path length. The research of scholars has effectively promoted the progress of robot path planning. However, path planning is not always possible to design in advance, as global environmental information is not always available a priori. Traditional algorithms often rely on map information to calculate the cost function of path planning, and when the environment changes dynamically, the route needs to be re-planned, resulting in low efficiency."}, {"title": "III. METHODOLOGIES", "content": "Autonomous robot navigation is a complex task that requires robots to autonomously plan paths and avoid obstacles in unknown or dynamic environments. Reinforcement learning has shown great potential in this field as a trial-and-error learning method that can continuously optimize decision-making through interaction with the environment. This section will introduce specific model methods in autonomous robot navigation, including deep Q network and proximal policy optimization models. We summarize the main parameters in Table 1.\n\nA. Deep Q network\nDeep Q networks are the result of the combination of Q-learning and deep neural networks, which are used to solve problems in high-dimensional state spaces. The basic principle of Q-learning is to learn a Q function that represents the expected cumulative reward for performing action a in state s, which is expressed as Equation 1.\n$Q(s, a) = E[\\sum_{t=0}^{T} \\gamma^t r_t | S_t = s, a_t = a]$ #(1)\nWhere y is the discount factor, and $r_t$ is the reward for time step t In deep Q networks, the Q function is approximated by a neural network with a parameter \u03b8. Use empirical playback and target network mechanisms to stabilize the training process. The experience replay stores the state, action, reward, and next state (s, a, r, s') of the interaction in the experience pool, from which samples are randomly selected for training to reduce the correlation between samples. The target network introduces a target Q network with the parameter \u03b8', and the parameters of e are copied to O' at fixed steps to keep the target value stable. The loss function is Equation 2.\n$L(\\theta) = E_{(s,a,r,s') \\sim D} \\cdot [(r + \\gamma max_{a'}Q(s', a'; \\theta') \u2013 Q(s, a; \\theta))^2]$#(2)\nB. Proximal policy optimization\nProximal policy optimization is a reinforcement learning method based on policy gradient, which aims to improve the stability and sample efficiency of the policy gradient method. The strategy gradient approach maximizes the expected reward by directly optimizing the $I_{\\theta}(a|s)$ of the policy function, which is expressed as Equation 3.\n$J(\\theta) = E_{\\tau \\sim \\pi_{\\theta}}[\\sum_{t=0}^{T} \\gamma r_t]$ #(3)\nWhere T represents the state-action sequence and J(0) means expected rewards. Proximal policy optimization avoids instability caused by drastic updates by limiting the step size of each policy update. The core of proximal policy optimization is to introduce clipping methods to constrain the change of old and new strategies, which is manifested in the following Equation 4.\n$L^{CLIP} (\\theta) = E_t[min(\\frac{\\Pi_{\\theta}(a_t|s_t)}{\\Pi_{oold}(a_t|s_t)}, clip(\\frac{\\Pi_{\\theta}(a_t|s_t)}{\\Pi_{oold}(a_t|s_t)}, 1 - \\epsilon, 1 + \\epsilon) \\widehat{A}_t)]$#(4)\nWhere $A_t$ is the advantage function and e is the clip threshold to ensure that the policy change does not exceed a certain range. The loss function after clipping is $L^{CLIP} (\\theta)$. Through the above methods, robots can achieve efficient and stable autonomous navigation in dynamic and complex environments, effectively improving the performance of industrial automation and intelligent logistics systems.\nProximal policy optimization is a reinforcement learning method based on policy gradient, which aims to improve the stability and sample efficiency of the policy gradient method. Proximal policy optimization maximizes the expected reward by directly optimizing the policy function and limits the step size on each policy update to avoid the instability caused by drastic updates. The core of this is to introduce the editing method to constrain the changes of the old and new strategies, ensure that the policy changes are within a reasonable range, so as to achieve a more stable training process."}, {"title": "IV. PREPARE YOUR PAPER BEFORE STYLING", "content": "A. Experimental setups\nIn this experiment, we evaluated the performance of different reinforcement learning algorithms in robot navigation tasks. The robot needs to navigate a 10\u00d710 grid world environment and find the best path to reach the target location. In order to compare the effectiveness of different algorithms, we made the following settings: the environment settings included a grid size of 10\u00d710, the starting position in the lower left corner, and the target position in the upper right corner, and a random number of obstacles, the position remained the same in each experiment. Following Figure 1 shows the simulation environment.\nB. Experimental analysis\nWe compare the performance of different reinforcement learning algorithms by plotting robot navigation paths, and in order to visually compare the performance of different algorithms in robot navigation tasks, we plot the navigation paths generated by each algorithm. Following Figure 2 shows the different navigation paths for different methods.\nThe number of collisions is an important indicator to evaluate the performance of autonomous robot navigation algorithms, which directly reflects the safety and obstacle avoidance ability of robots in the navigation process. By conducting multiple independent experiments in a 10\u00d710 grid environment, the number of collisions of each algorithm in 100"}, {"title": "V. Conclusions", "content": "In conclusion, our research on autonomous robots navigation based on reinforcement learning demonstrates the effectiveness and potential of advanced RL algorithms in improving robotic navigation performance. Our proposed method in a 10\u00d710 grid world environment, we highlighted key metrics like collision counts and path smoothness. Our findings indicate that our method consistently outperforms others, showcasing superior safety, efficiency, and adaptability. Enhanced path smoothness\nnot only reduces energy consumption and wear but also contributes to longer operational life and reduced maintenance costs. These results underscore the importance of reinforcement learning in developing robust and efficient autonomous navigation systems, paving the way for future advancements in industrial automation and intelligent logistics."}]}