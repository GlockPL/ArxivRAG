{"title": "Conditional Latent Space Molecular Scaffold Optimization\nfor Accelerated Molecular Design", "authors": ["Onur Boyar", "Hiroyuki Hanada", "Ichiro Takeuchi"], "abstract": "The rapid discovery of new chemical compounds is essential for advancing global\nhealth and developing treatments. While generative models show promise in cre-\nating novel molecules, challenges remain in ensuring the real-world applicability of\nthese molecules and finding such molecules efficiently. To address this, we introduce\nConditional Latent Space Molecular Scaffold Optimization (CLaSMO), which com-\nbines a Conditional Variational Autoencoder (CVAE) with Latent Space Bayesian\nOptimization (LSBO) to modify molecules strategically while maintaining similarity\nto the original input. Our LSBO setting improves the sample-efficiency of our opti-\nmization, and our modification approach helps us to obtain molecules with higher\nchances of real-world applicability. CLaSMO explores substructures of molecules in\na sample-efficient manner by performing BO in the latent space of a CVAE condi-\ntioned on the atomic environment of the molecule to be optimized. Our experiments\ndemonstrate that CLaSMO efficiently enhances target properties with minimal sub-\nstructure modifications, achieving state-of-the-art results with a smaller model and\ndataset compared to existing methods. We also provide an open-source web ap-\nplication that enables chemical experts to apply CLaSMO in a Human-in-the-Loop\nsetting.", "sections": [{"title": "Introduction", "content": "The accelerated discovery of chemical compounds represents a crucial challenge with the potential\nto revolutionize global health, offering new ways to combat diseases and viruses. The ability to\nefficiently discover and develop new chemical compounds could lead to groundbreaking treatments\nand therapies, addressing some of the most pressing health issues of our time. As the importance\nof this field grows, so too does the research focused on finding effective solutions. Over the past few\nyears, artificial intelligence (AI) has emerged as a powerful tool in this endeavor. The combination\nof increased computational power and advancements in generative modeling has brought us closer\nthan ever to achieving significant breakthroughs in accelerated discovery."}, {"title": "Related Works", "content": "Molecular design strategies can broadly be divided into two categories: from-scratch-generation\nof molecules and modification-based approaches. Both categories have made significant strides in\nrecent years, yet they also face unique challenges, particularly regarding real-world applicability\nand sample efficiency.\nFrom-scratch-generation approaches focus on creating entirely new molecules by conducting a search\nto optimize the target property. A seminal work by G\u00f3mez-Bombarelli et al. (2018) introduced\nlatent space optimization-based methodology, using a VAE to generate novel compounds by navi-\ngating the latent space of molecular representations. LSBO builds on this by efficiently reducing the\nnumber of expensive black-box evaluations required for molecular optimization, enabling the dis-\ncovery of compounds with desirable properties in a continuous latent space. Since then, numerous"}, {"title": "Preliminaries and Problem Setup", "content": "In this section, we provide preliminary knowledge on CVAES, LSBO, and scaffolds. We then discuss\nthe challenges of property optimization and scaffold modifications."}, {"title": "Conditional Variational Autoencoders (CVAEs)", "content": "A VAE Kingma & Welling (2014) consists of an encoder $f_{enc} : X \\rightarrow Z$ and a decoder $f_{dec} : Z \\rightarrow X$,\nwhere X represents the input space and Z the latent space. CVAEs extend the framework of\nVAEs by incorporating additional condition vector c into the latent variable model, facilitating the\ncontrolled generation of new instances. In the CVAE architecture, the encoder $q(z|x,c)$ maps\nan input \u00e6 and a condition c to a latent representation z. Simultaneously, the decoder $p_{\\theta}(x|z, c)$\nreconstructs using both zand c. The training of CVAEs is formulated as the minimization of\nthe conditional variational lower bound:\n$L(\\theta, \\phi; x, c) = -E_{q_{\\phi}(z|x,c)} [log p_{\\theta} (x|z, c)] + KL (q_{\\phi}(z|x, c) || p(z)),$"}, {"title": "Latent Space Bayesian Optimization (LSBO)", "content": "In BO, we start with numerous unlabeled instances ${x_i}_{i\\in[U]}$ and a smaller set of labeled instances\n$(x_i, y_i)_{i\\in[L]}$, where an input x \u2208 X represents a chemical compound, and a label yi \u2208 Y \u2286 R\nindicates its properties such as docking scores. BO seeks to optimize a costly black-box function\n$f^{BB} : X \\rightarrow Y$, which corresponds to obtaining the physical properties of chemical compounds\nthrough experiments or time-consuming simulations in the context of molecular design problems.\nThe goal is to maximize $f^{BB}$ with minimal evaluations, using typically a Gaussian Process (GP)\nsurrogate, trained using the labeled instances L, to predict the function over X. BO uses the\nsurrogate to select an input x that may yield values surpassing the current maximum $max_{i\\inL} y_i$.\nHowever, building a GP surrogate in high-dimensional spaces like chemical compounds is challeng-\ning. LSBO tackles this by employing a VAE/CVAE trained on the unlabelled instances U to reduce\ndimensionality, encoding instance in X to lower dimensional latent space Z. This simplifies surro-\ngate modeling and optimization because Z has lower-dimension than X. During LSBO iterations,\nthe acquisition function applied to the GP's predictions selects new points in Z to evaluate. The\nchosen latent variable $z_{ij}$ is decoded into a new input $x_i = f^{dec}(z_{iv})$. This new instance is evaluated\nby $f^{BB}$, and the results update L and refine the GP model. This cycle repeats until optimal results\nare achieved or resources are exhausted. In contexts like molecular design, LSBO aims to discover\nchemical compounds with optimal properties by efficiently navigating the reduced latent space."}, {"title": "Scaffolds", "content": "Scaffolds Bemis & Murcko (1996) are the stable core structures within molecules that serve as\nthe framework for chemical modifications in drug design. Scaffolds retain the essential biological\nactivity of the molecule. They play a crucial role in molecular design by providing a foundation for\nchemical modifications aimed at optimizing properties like QED. Researchers often use scaffolds to\nsystematically explore chemical variations Schreiber (2000); Welsch et al. (2010), which can lead\nto the discovery of new compounds with improved properties.\nIn this study, we follow the scaffold extraction method from Bemis & Murcko (1996), where non-\nessential components like side chains are removed, leaving the core structure. This extracted scaffold"}, {"title": "Problem Definition", "content": "We denote the scaffold, which is the base of the modification, as S, and the modified molecule\nas S'. Our goal is to efficiently find the modification that maximizes the molecular property P\nwhich is evaluated by $f^{BB}(S')$, while keeping the difference between S and S' small. Directly\noptimizing over all possible S' to find the best modification that maximizes $f^{BB}(S')$ is impractical,\nas it involves high complexity and costly evaluations of $f^{BB}$.\nThe primary challenge in optimizing molecular scaffolds lies in i) determining the optimal bonding\npoint on the base scaffold S, and ii) selecting the appropriate substructures added to the bonding\npoint to ensure meaningful improvements in the desired property P. A molecular scaffold S,\ncomposed of several atoms $P1,P2,...,Pk$, may have atoms with the remaining capacity to form\nadditional chemical bonds. These atoms serve as potential candidates for bonding with newly\ngenerated substructures. Therefore, the task involves not only selecting the right substructure\nbut also identifying the most suitable bonding point pi to optimize scaffold properties. This adds\ncomplexity, as the need for precise modifications must be balanced with the challenges of high-\ndimensional search spaces and evaluation costs. Consequently, a more efficient approach is required\nto explore scaffold modifications effectively while minimizing the number of evaluations.\nTo address this, the problem can be reframed as an optimization task in a reduced latent space Z,\nobtained through a CVAE. In this space, each point z \u2208 Z corresponds to a potential substructure\nthat can be integrated into the scaffold. By encoding the molecular substructures into this lower-\ndimensional space Z \u2208 Rd, the search becomes more tractable. The objective is to find the optimal\nlatent representation z* conditioned on the optimal bonding point p that, together, maximize the\ndesired property P when the generated substructure s' \u2190 $f^{dec}(z)$ is added to the scaffold. Let us\ndenote this modification as S' \u2190 $g(S\\oplus s', pi)$, where the function g() adds substructure s to the\nscaffold S at pi. The optimization problem is then formulated as:\n$z*, p = arg \\underset{z\\in Z, p_i\\in B(S)}{max} f^{BB} (S') = arg \\underset{z\\in Z,p_i\\in B(S)}{max} f^{BB}g(S\\oplus f^{dec}(z), P_i),$"}, {"title": "Controlling Molecular Similarity", "content": "Current modification-based methods often fail to account for how changes impact molecular sim-\nilarity between the original scaffold S and the updated scaffold S', or any structure in general.\nAdding substructures typically increases molecular weight, which can hinder real-world applicabil-\nity, especially when exceeding 500 Daltons, as indicated by Lipinski's Rule of Five Lipinski et al.\n(2001). Higher molecular weight compounds are more difficult to synthesize, making them less\nsuitable for molecular design. Additionally, it is sometimes necessary to ensure that modifications\nresult in only minor adjustments to avoid drastic changes.\nThus, a key challenge is to guide the optimization process by considering molecular similarity,\nensuring that the modified molecules remain structurally close to the original scaffold. Such a"}, {"title": "Proposed Method", "content": "Our proposed CLaSMO framework comprises two key components: the CVAE and the LSBO\nalgorithm. However, an essential first step in our approach is the data preparation required to\ntrain the CVAE. In this section, we will begin by outlining the data preparation process, followed\nby an explanation of the CVAE and the CLaSMO methodology."}, {"title": "Data Preparation", "content": "Our proposed method requires a uniquely tailored dataset because no existing dataset in the lit-\nerature fully meets the specific needs of our approach. To create this dataset, we developed a\nBRICS (Breaking Retrosynthetically Interesting Chemical Substructures) Degen et al. (2008) based\napproach. BRICS is an algorithm designed to decompose organic molecules into smaller, synthet-\nically feasible substructures by identifying breaking points within a molecule's structure based on\nchemical retrosynthetic rules. These breaking points, also known as division points, are the connec-\ntions between subgraphs within the molecule that BRICS identifies. The algorithm systematically\nbreaks down a molecule M into k substructures, Fig. 3 demonstrates this procedure.\nThe division points found by BRICS are of particular importance because they serve as both the\npoints where the molecule is split into substructures and the potential bonding sites where these\nsubstructures can be reattached. Therefore, they provide us a valuable information about these\nsubstructures in terms of what kind of bonds they can form. This dual role makes the division points\na crucial piece of information for our generative model. By preserving the properties and features of\natoms at these division points, we capture the chemical context around the atom that dictates how\nsubstructures can bond with other parts of a molecule. The atomic features we consider are atom"}, {"title": "Substructure CVAE", "content": "Our CVAE consists of an encoder, parameterized by $f_{enc}$, and a decoder, parameterized by $f_{dec}$.\nThe encoder takes the substructure s and the associated condition vector c, and maps this input\ninto a latent space, producing a latent representation z. The decoder then takes a point z from this\nlatent space, along with the condition vector c, and generates a substructure s' that is conditioned\non the given atomic properties. The loss function of the proposed CVAE is defined as:\n$L(\\theta, \\phi; s, c) = ||s \u2013 E_{q_{\\phi}(z|s,c)} [P_{\\theta}(s|z, c)] ||\u00b2 + \\beta\\cdot D_{KL}(q_{\\phi}(z|s, c)||p(z)),$"}, {"title": "Condition Vector Embeddings", "content": "In Section 4.1, we detailed our data preparation process and the extraction of six atomic environ-\nmental features, resulting in a 6-dimensional condition vector. As we will explain in Section 5.1,\nthe relatively simple nature of the substructures used in CVAE training allows for a much lower\nlatent dimension than the actual conditional vector dimension of 6. To align with this simplicity\nand ensure efficient representation, we employed an Autoencoder model to generate d-dimensional\nembeddings of the atomic features. These embeddings preserve the key characteristics within the\natomic environments and learn the relationship among distinct atomic features while reducing\ndimensionality, which helps lower computational complexity and improves the model's ability to\ngeneralize. After its training, the encoder of the Autoencoder, $f_{condemd}$, is used to provide the final\ncondition vector c to be inputted to CVAE by encoding the six-dimensional atomic environmental\nfeature vector. This process is visualized in Fig. 3C."}, {"title": "CLaSMO", "content": "In CLaSMO, we perform a targeted search in the latent space Z \u2208 Rd of the CVAE, where decodings\nfrom each point z represent a potential substructure to be added to the scaffold. The challenge is\nto modify the scaffold S by selecting a substructure and integrating it at an appropriate bonding\npoint pi, optimizing the desired molecular property P. Therefore, the search space \u03a9 we consider\nin our optimization tasks is defined as the Cartesian product of the latent space and the set of\npossible bonding points on the scaffold:\n\u03a9 = Rd \u00d7 {P1, P2, ..., Pn}.\nWithin this search space, the goal is to modify the scaffold S to maximize the BB function $f^{BB}(S')$.\nHowever, although our main goal is optimizing the target property, we also aim to keep the similarity\nbetween S and S' at a certain level. Therefore, in CLaSMO, we apply a similarity constraint using\nthe Dice Similarity Dice (1945) metric to compare the input scaffold S with the modified molecule\nS' before evaluating the BB function. In order to measure the similarity between S and S', we use\nMorgan Fingerprints (Morgan, 1965; Rogers & Hahn, 2010). The Morgan fingerprint of a molecule\nis represented as a binary vector, where each bit indicates the presence or absence of a specific\nsubstructure within the molecule, making them effective and popular for comparing molecular\nsimilarities. Using these, the similarity between S and S' is measured by computing the Dice\nSimilarity of their Morgan fingerprint vectors\u00b9, defined as:\nDICE(S, S') =$\\frac{2|M_S \\cap M'_S|}{|M_S|+|M'_S|}$\nwhere Ms and M's are the Morgan fingerprint vectors of S and S', respectively. Dice Similarity,\nranging from 0 (no overlap) to 1 (identical), helps us track structural changes during optimization.\nThe final optimization objective, incorporating both the search for optimal substructures and the\nsimilarity constraint, is given by:\n$z*, p = arg \\underset{(z,p_i)\\in\\Omega}{max} f^{BB} (S')$\nsubject to: DICE(S, S') \u2265 \u03c4,"}, {"title": "Kernel Design of CLaSMO", "content": "The problem we try to solve requires simultaneous optimization over continuous latent vectors and\ndiscrete bonding points. To handle this complexity, we use a GP model with a covariance function k\nthat accommodates the mixed input space of continuous and discrete variables. We define separate\nkernels for these inputs:\n$k_{cont} (z, z') = exp (-\\frac{1}{2l^2}||z - z'||^2), k_{cat} (p_i, p_j) = exp (-\\frac{\\delta_{p_i,p_j}}{l}).$\nwhere $k_{cont} (z, z')$ is an RBF kernel, and $k_{cat} (p_i, p_j)$ measures the similarity between atoms in the\nmolecule that are ready for additional bond via Kronecker delta function, measuring equality of\natom positions, and l is the lengthscale parameter. The combined kernel used in CLaSMO is then\nexpressed as:\n$k_{CLaSMO} = k((z, pi), (z',pj)) = k_{cont}(z, z') \u00b7 k_{cat} (Pi, Pj).$"}, {"title": "Experiments", "content": "In this section, we first provide details about the training of our CVAE model. Next, we discuss the\nresults from two different experimental settings, which are QED property optimization and docking\nscore property optimization."}, {"title": "CVAE Training", "content": "For our data preparation, we used the QM9 dataset Ruddigkeit et al. (2012); Ramakrishnan et al.\n(2014), which contains 130,000 small molecules. QM9 was chosen for its simplicity and suitability\nfor our task, as it consists of smaller molecules compared to other well-known datasets like ZINC\nIrwin & Shoichet (2005); Irwin et al. (2020). Using our data preparation strategy, we extracted\n18,706 unique substructure pairs along with their atomic environment features to train our CVAE.\nMost of the substructures obtained from QM9 through our method are under 100 Daltons, making\nthem ideal for training a model focused on generating small substructures to modify the input\nscaffold. Since our goal is to optimize the scaffold while maintaining a high degree of similarity\nin the modifications, using such a dataset allows CVAE to learn to generate small substructures,\nwhich is crucial for achieving our goals. Of the 18,706 instances, 80% were used for model training,\nwith the remaining data allocated for testing and validation. We represented the molecules using\nSELFIES Krenn et al. (2020), a string-based molecular representation, in the form of one-hot\nencoding matrices. Our CVAE architecture consists of three fully connected layers in the encoder\nand three GRU layers in the decoder."}, {"title": "CLaSMO", "content": "Algorithm 1 CLaSMO\n1: Input: GP training data D, Trained CVAE, Trained Autoencoder encoder $f_{condEmd}$ for condi-\ntion embeddings, Input molecules M, Optimization budget per molecule K, Similarity thresh-\nold T, Penalization terms \u03bb1, \u03bb2\n2: Fit GP using D\n3: for i = 1 to M do\n4: Pick molecule mi, obtain its scaffold Si \u2190 Scaffold(mi),\n5: Evaluate target property value y \u2190 fBB (mi)\n6: for j = 1 to K do\n7: Identify available atoms in the scaffolds for bonding, pj \u2208 B(S)\n8: Find z*, p = arg max(zi,pi)\u2208n fBB (S')\n9: Create condition vector c* for atom p in molecule Si\n10: Obtain condition embeddings c \u2190 $f_{condemd}(c*)$\n11: Generate substructure s* \u2190 $f_{dec}([z*, c])$\n12: Add substructure s* to molecule Si at region p to obtain S\n13: if Si \u2260 S then\n14: if DICE(Si, S) > T then\n15: Evaluate new property: y' = fBB (S)\n16: Compute improvement y'\u2081 = (y \u2013 y')\n17: if y' > 0 then\n18: Update Si S\n19: end if\n20: else\n21: Set y to penalization term \u51651\n22: end if\n23: else\n24: Set y to penalization term \u51652\n25: end if\n26: Update D\u2190 D\u222a {[z*, p*], y'\u2081}\n27: Update GP with D\n28: end for\n29: end for\nGiven the simplicity of the dataset, we determined that a 2-dimensional latent space was sufficient\nto achieve over 99% reconstruction accuracy on the test set. This lower-dimensional space also\nenhances the performance of BO, which typically excels in smaller-dimensional spaces. Additionally,\nto maintain this simplicity, as explained in Section 4.2.1, we generated a 2-dimensional embedding\nfor the condition vectors using an Autoencoder model, which is trained with fully connected layers\nin both the encoder and decoder, achieved 93% reconstruction accuracy for the condition vectors.\nPrior to CVAE training, we obtained the condition vector embeddings via the encoder of this VAE\nmodel, and used them during CVAE training. As a result, the CVAE was trained with both a\n2-dimensional latent space for the substructures and 2-dimensional condition vectors, effectively\nbalancing simplicity and optimization performance."}, {"title": "QED Optimization", "content": "In this section, we present the results of our QED optimization experiments, where we used RDKit\nLandrum (2010) to calculate QED values. The QED metric, defined between 0 and 1, inherently\nlimits the range of potential improvements, making optimization within this closed range particu-\nlarly challenging. To further complicate this task, we imposed similarity constraints to ensure the\noptimized molecules remain close to their original scaffolds, by running the CLaSMO algorithm\nwith values t\u2208 [0, 0.25,0.5,0.6]. For each threshold, CLaSMO was run for 100 iterations on 100\ninput scaffolds, where their whole molecules are sampled randomly from the ZINC250K dataset\nG\u00f3mez-Bombarelli et al. (2018). The search domain for each latent dimension in Z is set to [-6, 6].\nThe GP model is trained using 100 training instances. Such an experimental setting is designed\nto demonstrate the optimization capabilities of CLaSMO at changing similarity thresholds, within\nthe limited range of QED optimization tasks."}, {"title": "Model Benchmark", "content": "Table 2 shows the top QED values achieved by various models, highlighting CLaSMO's perfor-\nmance. Our method reaches a top QED score of 0.948, matching state-of-the-art results from\nPGFS Gottipati et al. (2020) and GP-MOLFORMER Ross et al. (2024). Among these models,\nScaffold-GGM, DrugHIVE, and PGFS are substructure-addition-based methodologies, while the\nothers generate molecules from scratch. A key distinction is that CLaSMO's CVAE is trained only\non substructures, not complete molecules, using a dataset of only 18,706 instances. The maximum\nQED value of the substructure among these instances is 0.53, with average QED value overall 0.4273.\nIn contrast, competitor models are trained on significantly larger datasets and full molecules, often\nencountering molecules with high QED values during training. These results underscore CLaSMO's\nefficiency and lower computational cost, demonstrating the advantages of using carefully curated\ndatasets and tailored methodologies for the target task."}, {"title": "Docking Simulation Score Optimization", "content": "In this section, we present our docking simulation score optimization results using the KAT1 protein\nas the target. KAT1 is an ion channel protein, and compounds with favorable docking scores are\nmore likely to bind effectively and influence its function, which is crucial for developing therapeutic\nagents targeting ion channel-related conditions\u2074. By optimizing docking scores, we aim to identify\ncompounds with the potential to modulate ion channel function and develop novel therapeutic\nstrategies. Unlike the relatively simple calculation of QED values, optimizing docking scores for\nKAT1 requires computationally intensive simulations. For these calculations, we used Schr\u00f6dinger\nSoftware Schr\u00f6dinger (2023), known for providing reliable and accurate results, with each simulation\ntaking anywhere from a few minutes to several hours. Given the high computational cost of\nthese simulations, CLaSMO's sample-efficient approach is particularly valuable, enabling effective\noptimization while minimizing the number of expensive evaluations.\nSimilar to the QED optimization setting, the search domain for each latent dimension in Z was set\nto [-6,6]. To address the high cost of obtaining labeled data for docking scores, we leveraged the\nsame training set used in the QED experiments, treating them as low-fidelity instances for training\nthe GP model. This approach allows the surrogate model to learn the relationships between atoms\nand latent vectors, even though the labels originate from QED rather than docking scores. By\nreusing this shared data, we enhance the GP model's capacity to generalize and effectively guide\nthe optimization process, reducing the dependency on extensive docking score evaluations. Given\nthe significant time required for each docking score calculation, we limited the experiments to 10\ncompounds from the ZINC250K dataset and ran CLaSMO for 100 iterations per compound using\ntwo Dice similarity thresholds, \u03c4\u2208 [0.25, 0.50]. This setup ensures that despite the computational\nexpense, we can still obtain comprehensive and meaningful results for docking score optimization,\nand observe if CLaSMO is capable of optimizing docking scores under similarity constraints.\nWe present the distribution of docking scores obtained from both CLaSMO experiments in Fig. 6.\nThe results clearly demonstrate that CLaSMO, at both similarity thresholds, achieves significant\nimprovements in docking scores compared to the initial scaffolds. Figure 7 provides detailed met-\nrics, including the maximum and average improvements across the models, as well as the initial\nbest values. Notably, CLaSMO with a similarity threshold of t = 0.25 achieved improvements of\nup to 96.3% over the initial scaffold docking score, while CLaSMO with \u0442 = 0.50 achieved im-\nprovements of up to 75.1%, both indicating strong optimization performance despite the limited\nnumber of LSBO iterations, showcasing CLasMO's sample-efficiency. In Fig. 8, we provide example"}, {"title": "Ablation Study", "content": "We conducted an ablation study to evaluate the impact of incorporating atomic environment fea-\ntures as condition vectors in the generative process. In the training of the baseline VAE model,\nonly substructures of the molecules were used, without conditioning on atomic properties. This\nsetup allowed us to isolate the effect of conditioning on scaffold optimization.\nWe repeated the QED optimization experiments using the VAE model. The QED optimization\nresults, shown in Table 3, demonstrate that incorporating atomic environment features in the CVAE\nsignificantly improves the generation of substructures. The conditioning mechanism enhances the\nmodel's ability to generate substructures that bond more effectively with the scaffold, leading\nto higher success rates in optimizing molecular properties. While the VAE without conditioning\nstill gains from the LSBO framework's efficient optimization, incorporating atomic properties as\nconditions significantly enhances the quality of the generated molecules."}, {"title": "Interactive Optimization", "content": "In CLaSMO, the modification region of the input scaffold is typically selected during the automated\noptimization process. However, the framework also supports an interactive mode, where a chemical\nexpert manually selects the region of the molecule to modify. In this mode, the expert identifies the\nspecific atom or region for modification, rather than relying on CLaSMO's automated selection.\nOnce the region is chosen, the rest of the process remains unchanged-CLaSMO continues to"}, {"title": "Conclusion", "content": "In this paper, we introduced CLaSMO, a novel framework that combines CVAE and LSBO for\nscaffold-based molecular optimization. Our approach efficiently explores latent spaces to optimize\nmolecular properties, such as QED and docking scores, while maintaining structural similarity\nwith the input scaffold to improve the chances of real-world viability of optimized molecules. By\nconditioning substructure generation on the atomic environment of the target region in the in-\nput molecule, CLaSMO generates chemically meaningful modifications. The experimental results\ndemonstrate that CLaSMO achieves state-of-the-art performance, even with limited training data,\na significantly smaller model compared to existing methods, and a limited number of optimization\niterations, showcasing its sample-efficiency and lower computational cost. Furthermore, CLaSMO's\nability to control structural divergence through similarity constraints ensures robust performance\nacross different optimization tasks. Although this paper focuses on scaffold-based modifications,\nCLaSMO is fully compatible with whole molecules, requiring no changes to its methodology. Ad-\nditionally, we have open-sourced a web application to allow chemical experts to use CLaSMO in a\nHuman-in-the-Loop setting, further extending its practical applicability. Overall, CLaSMO exem-\nplifies the power of combining scaffold-based strategies with LSBO, offering a highly effective tool\nfor targeted drug discovery and broader molecular design challenges."}, {"title": "Broader Impact Statement", "content": "The work presented in this paper has the potential to accelerate the discovery of new chemical\ncompounds, which can positively impact various industries, particularly pharmaceuticals and ma-\nterials science. By improving the efficiency of molecular optimization, CLaSMO could contribute\nto the development of more effective drugs, especially in regions facing significant public health\nchallenges, such as the need for rapid vaccine development. Moreover, CLaSMO's focus on real-\nworld applicability increases the chances that the compounds discovered are not just theoretical but\ncan be realistically produced, which is critical for translating scientific innovation into real-world\nsolutions. Moreover, CLaSMO's ability to work in a Human-in-the-Loop setting enables domain ex-\nperts to directly contribute to the optimization process, enhancing collaboration between artificial\nintelligence and human expertise."}, {"title": "Appendix", "content": ""}, {"title": "Definitions of Atomic Features", "content": "In Table 4, we provide the definitions of the six atomic features we utilized in our data preparation\nsetting."}, {"title": "QED Value Distribution of Input Substructures", "content": "Figure 9 shows the distribution of QED values for the substructures used to train the CVAE model,\ngenerated through our data preparation process."}, {"title": "Model and Hyperparameter Selection", "content": "In chemical VAE models, it has been established that setting the KL divergence weight \u03b2 <\n1 can improve generative performance Yan et al. (2020), and our findings are consistent with\nthis. We experimented with a range of \u1e9e values from 1 to 1-7, selecting models based on their"}, {"title": "Human-in-the-Loop via Web-Application", "content": "In Fig. 10, we showcase a sequence of screenshots from our web application, demonstrating the\nprocess of molecule optimization. First, the user inputs a SMILES Weininger (1988) string of the\nchemical compound into the designated text field. Once the input is provided, the application auto-\nmatically computes the molecule's QED value and generates a visual representation. Subsequently,\nthe user selects a region of interest by drawing a rectangle around the area they wish to modify.\nUpon confirming the selection, the CLaSMO optimization process is initiated, targeting improve-\nments in the selected molecular region. Upon completion, the optimized molecule is displayed,\nand the process can be continued by using the resulting molecule as input for further iterations.\nBy incorporating user input in the region selection, we create a Human-in-the-Loop optimization\nworkflow."}]}