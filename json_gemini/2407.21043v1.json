{"title": "CP-Prompt: Composition-Based Cross-modal Prompting for Domain-Incremental Continual Learning", "authors": ["Yu Feng", "Zhen Tian", "Yifan Zhu", "Zongfu Han", "Haoran Luo", "Guangwei Zhang", "Meina Song"], "abstract": "The key challenge of cross-modal domain-incremental learning (DIL) is to enable the learning model to continuously learn from novel data with different feature distributions under the same task without forgetting old ones. However, existing top-performing methods still cause high forgetting rates, by lacking intra-domain knowledge extraction and inter-domain common prompting strategy. In this paper, we propose a simple yet effective framework, CP-Prompt, by training limited parameters to instruct a pre-trained model to learn new domains and avoid forgetting existing feature distributions. CP-Prompt captures intra-domain knowledge by compositionally inserting personalized prompts on multi-head self-attention layers and then learns the inter-domain knowledge with a common prompting strategy. CP-Prompt shows superiority compared with state-of-the-art baselines among three widely evaluated DIL tasks. The source code is available at https://github.com/dannis97500/CP_Prompt.", "sections": [{"title": "1 INTRODUCTION", "content": "Cross-modal models have garnered significant attention due to their capability to process and integrate diverse types of data. However, these models often encounter the challenge of different domains data feature distributions in practical applications. Domain Incremental Learning (DIL) [42] is a special incremental learning task, where the learning model is trained on a sequence of domains over time, with each domain or task presenting new and potentially information, e.g. distributional shift [1]. Under this setting, the tasks in each domain remain the same and the testing sample does not know which domain it belongs to. A vivid example is shown in Figure 1, where the learned model was firstly trained with quickdraw-style pictures, and then tested to classify the same category under different styles, such as infographics, painting, and clipart. The key success of DIL algorithm is to adapt and learn from sequential domains without forgetting the knowledge it has acquired from previous ones.\nA key challenge for domain incremental learning is how to deal with the phenomenon of catastrophic forgetting [31, 32]. When learning new domains in sequence, the model may forget previous knowledge, leading to poor performance on old domains. To alleviate this issue, previous work [14, 35, 38] utilizes a buffer containing exemplars from previous tasks to facilitate learning new tasks. Then remarkable progress has recently been made in DIL tasks using prompt learning methods. Such as building prompt pool [46], adding different classification tokens [9], employing prompt on multi-modal pre-trained models [44].\nDespite this, two challenges still remain: (1) How to make the trade-off between common and personalized knowledge within DIL process? Previous studies have shown that extracting common patterns between domains and enhancing personalized knowledge with each domain are both helpful in DIL. However, from the other side, how to balance inter-domain and intra-domain feature learning is still unaddressed. (2) How to depict the impact of domain context on embedding tokens? For the transformer which is widely adopted by DIL models, the effectiveness comes from routing information of lists of complex tokens to acquire the correlation by self-attention. However, this structure is difficult to learn information outside the fix-sized space of transformer [11]. Thus, additional domain context information should be guided into the transformer encoding process.\nTo this end, in this paper, we present a prompt learning framework, namely CP-Prompt (Common & Personalized), to instruct a pre-trained model to learn on incremental data domains with different data styles. As Figure 1 depicts, CP-Prompt adopts a twin-prompt strategy. The shared common prompts, embedding within shallow part of the model, are employed to learn knowledge of new domains sequentially and then frozen. Common prompts embedding of models can preserve knowledge among domains. The personalized prompts, called Prefix-One, embedded within the self-attention layers of the pre-trained model, contribute to model inference with"}, {"title": "2 RELATED WORK", "content": "Domain Incremental Learning. DIL refers to a type of continuous learning scenario where the feature distribution of the same task changes across different domains [42]. In other words, the data in each domain is used to accomplish the same task but differs from each other significantly [20, 33]. The goal of DIL is to enable the model to learn about newly added domains without re-training from scratch while maintaining its generalization in the original domains. Traditionally employed methods typically include architecture-, regularization-, and replay-based approaches. Architecture-based methods create independent components for each task or focus on task-specific subnetworks to avoid interference between network parameters to alleviate forgetting, such as XdG [30], DEN [48], PAE [17], and CPG [18]. Regularization-based approaches [21] [34]constrain or penalize significant model changes to keep memory on the previous domain with regularized losses such as distillation loss [25], and parameter update loss [50]. Replay-based methods mitigate catastrophic forgetting by preserving a small subset of the previous domain and replaying them when training new tasks, such as ER [39], DGR [2], iCaRL [38], BI-R [41], and A-GEM [6].\nPrompt Learning. Prompt learning originated from manually designing templates as extra instructions to pre-trained models for efficient adaptation to downstream tasks. Compared with human-defined fixed ones, treating prompts as learnable parameters significantly enhances the efficiency and effectiveness of the model instruction [19, 29, 52]. In this setting, prompt learning only needs a tiny set of parameters for training instead of tuning the entire pre-trained model, benefiting much time- and cost-sensitive scenarios such as incremental learning and transfer learning [12, 13]. This parameter-efficient tuning method is primarily classified into three types, including addition-, specification, and reparameterization-based approaches. Addition-based methods introduce extra trainable neural modules not present in the original model, such as Prompt Tuning [22], Prefix Tuning [24], Adapter Tuning [15], and P-Tuning"}, {"title": "3 PRELIMINARY", "content": "Prompt Learning on Pre-trained Models. Pre-trained models follow a paradigm that trains its parameters via massive self-supervised labeled data for general ability and fine-tunes them with few labeled data for downstream tasks. Prompt learning provides a tiny-parameter-sized embedding to guide a model to generate better responses, thereby significantly reducing the resource burden of model training and tuning. Taking the visual-text pre-trained model CLIP as an example, it comprises a visual encoder and a text one. In the image encoder, an image \\(x \\in \\mathbb{R}^{H\\times W\\times C}\\) in encoded as a sequence of vectors \\(x_{emb} \\in \\mathbb{R}^{E_1\\times D}\\) by the visual encoder, where H, W represents the resolution of the original image, C is the number of channels, \\(E_1\\) is the feature size after convolution, and D is the embedding dimension. To perform prompt tuning on CLIP, we can inject tiny-sized parameters into a pre-trained model and only train them to adapt to downstream tasks. To formalize, the vector of image samples \\(x_{emb}\\) is concatenated with soft prompts \\(P\\in \\mathbb{R}^{L\\times D}\\), \n\\[x_p = [P, X_{emb}] \\in \\mathbb{R}^{(E_1+L)\\times D},\\]\nwhere L is the prompts length. Discovering the best prompts involves picking specific tokens, which can be achieved through either manual exploration or non-gradient-based search techniques [40].\nThe vector \\(x_p\\) is then encoded by transformer layers, resulting in a high-dimensional projection \\(x_h \\in \\mathbb{R}^{H_1\\times D}\\), where \\(H_1\\) is the number of image features in high dimensional space. Similarly, in the text encoder, we encode words through vocabulary and positional embedding \\(t_{emb} \\in \\mathbb{R}^{E_T\\times D}\\), which after transformer encoding also yields \\(t_h \\in \\mathbb{R}^{H_T\\times D}\\), where \\(E_T\\) is the feature size, and \\(H_T\\) is the"}, {"title": "4 THE CP-PROMPT FRAMEWORK", "content": "4.1 Overall Structure\nThe overall pipeline of the proposed CP-Prompt is presented in Figure 2. In CP-Prompt, we propose a twin-prompting strategy. The underlying assumption of this design is that the learning model should be guided by inter-domain shared prompts to enhance the generalization of common knowledge for the overall task. Simultaneously, personalized prompts within the domain guide the model to capture personalized knowledge, improving accuracy for specificities. Specifically, personalized prompts are embedded into key and value vectors in different transformer layers for guiding the model to learn latent semantics with different granularities. During the inference, a simple K-Means algorithm is utilized to select appropriate common and personalized prompts to guide the pre-trained model to encode new image tokens for classification.\n4.2 Common Prompts\nAs shown in Figure 2, we design a continually tuned common prompting strategy for guiding the learning model to extract shared knowledge across each domain. The common prompts are tiny-sized parameters that are tuned by loss gradient calculated by prediction on each domain data sample. At this moment, the entire parameters of the pre-trained model are frozen so that the generation variation of the model would only be affected by inputs and prompts."}, {"title": "4.3 Personalized Prompts", "content": "In addition to the complementary formation of common prompts, personalized prompts are embedded across the transformer's attention layers in the form of parameters, capturing semantics at different granularities.\nPrefix-One Prompting. The embeddings \\(x\\), then undergo projection transformation by the transformer layers of the pre-trained model. Therefore, inserting personalized prompts \\(P_{P,img} \\in \\mathbb{R}^{L_{PI}\\times D}\\) into MSA layers can help to instruct the attention-capturing domain-individual semantics and knowledge, where \\(L_{PI}\\) denotes the length of the personalized prompts.\n\\[\\begin{aligned}f_{\\text{pre-one}}(P_P, img, h^{(l)}) & = \\text{Softmax}\\left(\\frac{q_k^{(l)^T k_i^{(l)}}}{\\sqrt{d_{q,k}}}\\right) \\\\q_k^{(l)} & = h_m^{(l)} W_q^{(l)}, \\ k_i^{(l)} & = [h_m^{(l)}; P_{img}]W_k^{(l)}, \\ v_i^{(l)} & = [h_m^{(l)}; P_{img}]W_v^{(l)} ,\\end{aligned}\\]\nwhere l = 0, 1, ... R (R indicates the number of transformer layers), \\(h_m^{(0)} = x\\), and \\(h_q^{(l)}, h_k^{(l)}, h_v^{(l)}\\) are calculated by:\n\\[\\begin{aligned}h_q^{(l)} & \\in \\mathbb{R}^{M \\times D} = h_m^{(l)}W_q^{(l)},  \\\\h_k^{(l)} & \\in \\mathbb{R}^{(M+L_{PI}) \\times D} = [h_m^{(l)}; P_{img}]W_k^{(l)},  \\\\h_v^{(l)} & \\in \\mathbb{R}^{(M+L_{PI}) \\times D} = [h_m^{(l)}; P_{img}]W_v^{(l)} ,\\end{aligned}\\]\nwhere \\(P_{P,img}^{(l)}\\) represents the image personalized prompts parameters for the l-th layer. \\(h_q^{(l)}, h_k^{(l)}, h_v^{(l)}\\) are outputs to the l-th MSA layer in the image transformer. \\(W_q^{(l)}, W_k^{(l)}, W_v^{(l)}\\) represent the corresponding model parameters. Since the prompts on the image side are embedded in the MSA layer, for a pre-trained model with R layers of the transformer architecture, prompts can be embedded in multiple layers of MSA to better learn domain-specific knowledge.\nGeneralizing to text encoder. The Prefix-One for text-based encoder is similar to the image one. Taking our adopted CLIP architecture as an example, the complete label set \\(Y = \\{y_j\\}_{j=1}^U\\) is transformed to texts. The encoded label text set \\(Y_{emb} = \\{y_{emb}\\}^{U\\times D}\\) is concatenated with the text personalized prompts \\(P_{P,tex} \\in \\mathbb{R}^{L_{PT}\\times D}\\), where \\(L_{PT}\\) is the length of the personalized prompt for text. The high-dimensional projection after feature extraction by the text transformer is \\(t \\in \\mathbb{R}^{U\\times D}\\), and L_Y represents the feature count after encoding the label set using the vocabulary and adding positional vectors.\nThe composition of \\(P_{P,tex}\\) is similar to the shared prompts on the image side. The encoded label \\(y_{emb}\\) is concatenated with the class token TexCLS and prompts \\(P_{P,tex}\\), to derive \\(e_s\\):\n\\[e_s \\in \\mathbb{R}^{(1+U+L_{PT})\\times D} = [\\text{TexCLS}; P_{P,tex}; Y_{emb}].\\]"}, {"title": "4.4 Overall Objective for CP-Prompt", "content": "Finally, the logits \\(z_s \\in \\mathbb{R}^{U}\\), are computed by matrix multiplication of the high-dimensional projections from the image and text sides:\n\\[z_s = h_i^{(t)^T} \\cdot h_m^{(t)}.\\]\nIt should be noted that, after continual training on s-th domain, \\(P_C\\) is not only used as common prompts for the current domain, but also the initialization for the next one. However, the deep domain-specific knowledge-oriented personalized prompts \\(P_{P,img}\\) are isolated and optimized in different domains.\nDuring the inference stage, we adopt a simple yet effective unsu-pervised clustering, K-Means, as domain selector to assign model extracted features with K domain centroids \\(F = \\{f_j\\}_{j=1}^S\\) as feature pool. Given a new arriving sample \\(x_{new}\\), the domain selector selects the most relevant personalized prompts for inference by measuring the distance between \\(x_{new}\\) and F. Following the multimodal pre-training setting [44], the model inferences prediction \\(z_s^{'}\\) is derived by freezing prompts parameters and untuned pre-trained model:\n\\[z_s^{'} = f_{\\text{Pre-One}}(P_C, P_{P.img}) \\times g_{M_{\\text{pre}}}(P_{P.tex}).\\]\nThe goal of the model is to optimize the following loss function by tuning the tiny-sized prompts parameters \\(P_C, P_{P,img}, P_{P,tex}\n\\[\\mathcal{L} = \\frac{1}{2n} \\sum_{i=1}^n y \\log (z_s^{'}) + (1 - y) \\log (1 - z_s^{'}).\\]\nThe overall training process is formally described in Algorithm 1."}, {"title": "4.5 Model Analysis", "content": "We conduct model analysis to demonstrate the rationality behind the simple design of CP-Prompt. We will demonstrate the relationship with Dual-Prompt, a class-incremental learning model that utilizes a combination of double prompts. We extracted Dual Prompt core modules and transformed them for use in domain incremental tasks. First we will explain the difference between General-Prompt and Common-Prompt, and then demonstrate the difference between Expert-Prompt and Personalized-Prompt.\nCommon prompts vs. General prompts. In CP-Prompt, we design the common prompt as: \\(x_p = [P_C, x_{emb}] \\in \\mathbb{R}^{(E_1+L)\\times D}\\), where \\(x_p\\) is the feature after initial encoding and L is the prompts length. The"}, {"title": "5 EXPERIMENT", "content": "5.1 Experiment Setup\nDataset and Model Setting. To evaluate the effectiveness of CP-Prompt, we test three widely used DIL benchmarks, including CDDB-Hard [23], CORe50 [36], and DomainNet [28]. For a fair performance comparison, we adopt the same dataset and experiment setting with the previous studies [44]. A detailed description of datasets and settings is available in supplementary materials.\nBaselines. We compare the proposed CP-Prompt with state-of-the-art DIL methods, including replay-based methods including iCaRL [38], LUCIR [14], LRCIL [35], distillation-based method"}, {"title": "6 CONCLUSION", "content": "In this paper, we propose CP-Prompt, which introduces common and personalized prompts into the cross-modal domain-incremental learning task. CP-Prompt integrates prompts into pre-trained models based on the transformer architecture, learning common prompts in the shallow layers and personalized prompts in the deep layers"}]}