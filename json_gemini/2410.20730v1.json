{"title": "Bi-level User Modeling for Deep Recommenders", "authors": ["Yejing Wang", "Dong Xu", "Xiangyu Zhao", "Zhiren Mao", "Peng Xiang", "Ling Yan", "Yao Hu", "Zijian Zhang", "Xuetao Wei", "Qidong Liu"], "abstract": "Deep Recommender Systems (DRS) are essential for navigating the extensive data across various platforms in today's digital landscape. Current DRS models often treat all features equally and implement complex structures to enhance the capture of feature interactions. However, they may fail to recognize crucial user patterns due to not fully utilizing user-specific features for user modeling. Moreover, prevailing user modeling techniques concentrate exclusively on either the group or individual level, overlooking the potential insights from the unaddressed one. This oversight can miss shared group preferences or learn group patterns that conflict with individual preferences. To overcome these limitations, we introduce GPRec, a novel bi-level user modeling approach that substantially improves DRS. GPRec explicitly categorizes users into groups in a learnable manner and aligns them with corresponding group embeddings. We design the dual group embedding space to offer a diverse perspective on group preferences by contrasting positive and negative patterns. On the individual level, GPRec identifies personal preferences from ID-like features and refines the obtained individual representations to be independent of group ones, thereby providing a robust complement to the group-level modeling. We also present various strategies for the flexible integration of GPRec into various DRS models. Rigorous testing of GPRec on three public datasets has demonstrated significant improvements in recommendation quality. Additional experiments further explore crucial components of GPRec, its parameter sensitivity, and the group diversity. The implementation code is readily available online to facilitate future research and practical deployment: https://github.com/Applied-Machine-Learning-Lab/GPRec.", "sections": [{"title": "I. INTRODUCTION", "content": "With the advent of the information explosion, Deep Recommender Systems (DRS) have emerged as indispensable tools for sifting through data overload [1], [2]. These systems are widely implemented across various online services [3]\u2013[5], including shopping websites, content streaming platforms and social media networks, where they enhance the user experience by delivering personalized recommendations [6], [7]. Existing DRS models can be formally represented by $F(x)$ [8], where $x$ encompasses features derived from users and items, i.e., $x = [x_u, x_v]$, with $F$ denoting the model architecture.\nTo improve the prediction capacity of DRS, considerable efforts have been devoted to designing advanced model architectures (F) that better capture predictive feature interactions [9]\u2013[12]. Typical frameworks include linear structures [13], [14], factorization machines [15]\u2013[17], attention mechanism-based frameworks [18]\u2013[20]. Researchers have also developed advanced structures like cross networks for better feature crossing [21]\u2013[23] and gating mechanisms [24] for better interaction fusion [22], [25]. Nevertheless, these approaches often employ a uniform treatment across each feature field\u2014whether user or item features\u2014thus neglecting the crucial necessity of prioritizing user features. Such prioritization is vital for uncovering valuable user patterns that are specifically tailored to the personalization needs of both individuals and groups. This lack of emphasis on user features can result in suboptimal performance [8], [26], [27].\nConsequently, integrating user modeling on user features ($x_u$) into DRS\u2014a relatively underexplored area\u2014holds promise for improving prediction ability. Current user modeling methods are generally categorized into two levels:\n\u2022 Individual Modeling: These approaches usually create specialized structures or parameters tailored for each user [8], [26], formally denoted as $F_u(x)$.\n\u2022 Group Modeling: This level first divides users into groups according to user features, denoted as $g(x_u)$ (ranging from 1 to G for G groups). Then, different architectures and parameters are assigned to each group as $F_{g(x_u)}(x)$. Compared with the individual level, group modeling generates coarse-grained structures, offering a more generalized but less personalized solution.\nHowever, existing approaches tend to concentrate exclusively"}, {"title": "II. NOTATIONS", "content": "As the understanding of DRS structures can vary significantly, this section elaborates on the specific DRS structure referenced in this paper, as well as commonly used notations and definitions.\nWe structure the DRS into three modules: feature input, representation learning, and prediction. Specifically, an input sample $x$ typically includes item features $x_v$ and user features $x_u$, formatted as one-hot vectors. We split user features into personal features ($x_p$) and other features ($x_o$) based on prior knowledge\u00b9. Thus, this paper represents $x$ as $x = [x_p, x_o, x_v]$. The feature input module converts these one-hot vectors into low-dimensional dense vectors:\n$E = [E_p, E_o, E_v] = V[x_p, x_o, x_v]$  (1)\nHere, V represents the embedding table for all features. Equation (1) maps the one-hot features x to their corresponding embeddings in V, and returns the dense matrix E.\nFollowing the feature input is the representation learning, whose structure is determined by the chosen backbone model, including options like the cross network [21], product network [23], and other advanced architectures [22], [25], [33]. This module transforms E into hidden states $r_b$, which denote the representation learned from the backbone. Finally, $r_b$ is input into the prediction module, typically a Multi-Layer Perceptron (MLP), to generate the output $\\hat{y}_b$."}, {"title": "III. FRAMEWORK", "content": "This section provides an overview of GPRec, and elaborates on the bi-level user modeling, i.e., user group modeling and individual preference learning. Additionally, we discuss the model construction strategies to emphasize how GPRec can be conveniently combined with DRS models, and conclude with the optimization method.\nA. Overview\nFigure 1 presents the GPRec framework with three components: Figure 1 (a) depicts the overall data flow, Figure 1 (b) focuses on user group modeling, and Figure 1 (c) elaborates on the model construction strategies. In this figure, the gray balls represent the embeddings of item features ($E_v$), the green balls represent the embeddings of other user features ($E_o$), and the red balls denote the embeddings of user personal features ($E_p$). The data flow of the backbone model is indicated by yellow arrows. Additionally, green and red arrows highlight the GPRec's data flows associated with group modeling and individual preference learning.\nThe overall process of GPRec is visualized in Figure 1 (a). We can see that the workflow of the backbone remains unaffected, indicating the modular design of GPRec. It processes all feature embeddings $E = [E_p, E_o, E_v]$ to produce the hidden state $r_b$. GPRec's role involves user group modeling (Section III-B) and individual preference learning (Section III-C). The group modeling uses all user features $E_u = [E_p, E_o]$ to generate the group representation $r_G$, accompanied by two auxiliary losses $L_G$, $L_{Con}^G$. The individual modeling focuses on capturing the user's unique preferences $r_P$ from personal embeddings $E_p$ to enhance personalization, where an orthogonal loss is introduced, denoted as $L_o$. Ultimately, the base representation $r_b$, together with group and individual representations $r_G$, $r_P$, are integrated into the prediction module to generate the final prediction $\\hat{y}$."}, {"title": "B. User Group Modeling", "content": "This section details the user group modeling, including the group division method, the dual group embedding space, and the processes of obtaining user group representations $r_G$ and two auxiliary losses.\nThe structure is visualized in Figure 1 (b) for easier comprehension, which is exemplified by four user groups. GPRec first divides users into groups based on user embeddings ($E_u$). The division is represented by binary masks (M). The corresponding group representation ($r_G$) is then obtained from the dual group embedding space ($E_G$). Additionally, a contrasting group representation ($r_{Con}^G$) is produced, utilizing the inverse of the group division result (1 \u2013 M). Two auxiliary losses ($L_G$ and $L_{Con}$) are computed during the group modeling. In the depicted example, the user is classified into Group#2 and Group#3 (denoted in orange) while not included in Group#1 or Group#4 (denoted in purple). Consequently, the user group representation $r_G$ includes positive embeddings for Group#2 and Group#3 and negative embeddings for Group#1 and Group#4. And the contrasting representation $r_{Con}^G$ is the reverse, i.e., including positive embeddings for Group#1 and Group#4 and negative embeddings for Group#2 and Group#3.\nGroup Division. A straightforward method for group division is organizing groups based on explicit user attributes such as 'Gender' and 'Age' [28], [29], [31]. However, this attribute-centric approach has notable limitations due to its rigidity. It relies heavily on prior knowledge to select division criteria and binds users to fixed, unmodifiable groups post model construction. Another method used in existing work involves calculating cosine similarities between user embeddings and group embeddings. This method first computes the similarities between user embeddings and each group embedding. It then employs a Softmax operation over these results to determine users' connections to each group. Finally, it calculates a weighted sum of the group embeddings to represent the user group representations [27]. Nevertheless, this approach is limited to linear relationships and risks overfitting group embeddings to user embeddings, which can reduce diversity. In response, we propose a learnable classifier, $g$, for group division, which can incorporate various deep learning structures. This classifier is designed to capture the non-linear dynamics within user embeddings $E_u$, enabling a more nuanced and flexible division of user groups. Furthermore, $g$ helps to decouple the learning of group embeddings from user embeddings, thereby enhancing the diversity and effectiveness of groups. This group division process is formulated as:\n$S = [s_1^+, ..., s_G^+; s_1^-, ..., s_G^-]$; $s_i^+, ..., s_i^- = g(E_u;\\theta_g)$  (2)\nwhere $S \\in \\mathbb{R}^{2\\times G}$ denotes the array of classification scores for G groups. And $s_i^+$ reflects the classification score for assigning a user to group $i$, $s_i^-$ is the score for excluding a user from group $i$. $\\theta_g$ denotes the learnable parameters of $g$.\nTo achieve a more definitive group division for diverse group representation, we apply the Gumbel-Softmax technique [34], [35]. Specifically, this technique converts the smooth scores in S into binary-like masks M, approximating values close to 0 or 1 [36]. This transformation is crucial because using smooth scores to fuse positive and negative group embeddings may lead to the blending of contradictory group patterns, potentially reducing their distinctiveness and impairing the efficiency. ($m_i^+, m_i^-$) in M are generated as:\n$m_i^+ = \\frac{\\exp((\\log s_i^+ + g_n^+) / \\tau)}{\\exp((\\log s_i^+ + g_n^+) / \\tau) + \\exp((\\log s_i^- + g_n^-) / \\tau)}$  (3)\nwhere $g_n^i = - \\log(-\\log(u^i))$,\n$u^i \\sim Uniform(0, 1)$, $\\forall j \\in [0, 1]$\n$m_i^- = 1 - m_i^+$  (4)"}, {"title": "C. Individual Preference Learning", "content": "This section is dedicated to illustrating the other level of user modeling, individual preference learning.\nSolely concentrating on user group patterns and neglecting individual preferences could be detrimental to users with heterogeneous tastes, thereby constraining the capacity of DRS to improve recommendation quality. To mitigate this limitation, we discern individual preferences.\nSpecifically, general user attributes like 'Gender', 'Age', and 'Occupation' are widespread amongst many users, but features like 'UserID' are unique. We hypothesize that these unique features harbor a deeper level of individual preference. This understanding guides us to prioritize id-like features as personal features, aiming to delve into the rich individual preference. We extract individual representations $r_P$ from personal features $E_p$ by an MLP $f_2$ with parameter $\\theta_2$:\n$r_P = f_2(E_p; \\theta_2)$  (12)\nHowever, there is a potential overlap between the group representations $r_G$ and individual representations $r_P$. For instance, a user identified as a sports fan within a group might exhibit similar traits in their personal features, leading to redundancy. This overlap could dilute the representation of other individual preferences. For example, although the user in the same example might also engage with music content, his individual preference could be overshadowed by his interest in sports. Given that sports preference is already captured in group representations, diluting this preference in individual preference could enhance the visibility of his interest in music, thereby improving the diversity of recommendations. To address this, we incorporate an orthogonal loss in GPRec, designed to minimize overlaps between group and individual representations, thus accentuating the unique aspects of individual preferences. The orthogonal loss can be expressed as:\n$L_O = \\frac{r_G\\cdot r_P}{\\|r_G\\|_2 \\|r_P\\|_2}$  (13)\nwhere $\\| \\|_2$ indicates $L_2$-norm. Equation (13) computes the cosine similarity between $r_G$ and $r_P$, which is frequently utilized for decorrelation purposes [37], [38]."}, {"title": "D. Model Construction", "content": "This section presents three model construction strategies combining our GPRec with DRS models. The strategies detailed in this section are illustrated in Figure 1 (c).\nBi-level user modeling representations ($r_G$ and $r_P$) are readily combinable with existing DRS frameworks. Given the heterogeneous structures of DRS in real-world applications, a one-size-fits-all prediction model for GPRec is not universally effective. Therefore, we demonstrate three model construction strategies as demonstrative examples: the input strategy, the dynamic parameter strategy, and the ensemble strategy. It is important to note that GPRec's design is inherently adaptable and can be integrated with any backbone, allowing additional strategies to be employed as needed.\nInput Strategy. The input strategy is conceptually straightforward. This approach inputs the additional representations provided by GPRec to the prediction module ($f_y$) as:\n$\\hat{y} = f_y(r_b, r_G, r_P; \\theta_y)$  (14)\nwhere $r_b$ signifies the base representation from the backbone. And $\\theta_y$ denotes the trainable parameter of $f_y$."}, {"title": "E. Optimization", "content": "By integrating GPRec with backbone DRS, we can determine the final prediction $\\hat{y}$ as Equation (14), Equation (16), or Equation (18). Then, the primary optimization objective is:\n$L_{major} = L(\\hat{y}, Y)$  (19)\nThe loss function L utilized in Equation (11), Equation (8), and Equation (19) is consistent with the specific recommendation task. For instance, for click-through rate prediction tasks, L is defined as the binary cross-entropy loss (BCE), mathematically expressed as $L(\\hat{y}, y) = y \\cdot \\log \\hat{y} + (1 - y) \\cdot \\log (1 \u2013 \\hat{y})$. The overall optimization objective is formulated as:\n$L = L_{major} + \\lambda_1 L_G - \\lambda_2 L_{Con}^G + \\lambda_3 L_O$  (20)\n$L_{major}$ is the main loss from Equation (19). $L_G$, $L_{Con}^G$, and $L_O$ are auxiliary losses from previous sections. And $\u03bb_1$, $\u03bb_2$, $\u03bb_3$ are hyper-parameters set to balance the training objectives. Finally, GPRec is an end-to-end system that enables the direct application of the gradient descent strategy."}, {"title": "IV. EXPERIMENT", "content": "In this section, we provide detailed descriptions of our experimental results to address the following research questions:\n\u2022 RQ1: How does GPRec perform compared with baselines?\n\u2022 RQ2: Is GPRec compatible with various DRS backbones?\n\u2022 RQ3: What is the impact of the core components?\n\u2022 RQ4: How do hyper-parameters affect the effectiveness of the proposed GPRec?\n\u2022 RQ5: Can GPRec successfully learn contrasting and diverse group patterns as expected?\nA. Experiment Settings\nThis section provides details of our experimental setup, including datasets, evaluation metrics, baselines, and implementation details.\n1) Dataset: We validate the effectiveness of GPRec on three public datasets:\n\u2022 ML1M\u00b2, short for MovieLens 1M, a benchmark dataset commonly used in DRS research. It comprises 5 user features, with 'userid' serving as the only user personal feature.\n\u2022 TenRec\u00b3, a large-scale dataset recently released by Tencent [41] collected from their browsers. We utilize the sampled version designed for the CTR prediction task as described in the original study. This dataset contains 3 user features, with 'userid' as the unique personal feature.\n\u2022 KuaiRand\u2074, collected from KuaiShou. This dataset contains 89 features, where 30 are user features. We adopt the pure version as advised by the literature [42].\nThe detailed statistics are presented in Table I, which includes the number of users, items, interactions, user features, and the total number of features. For all datasets, we randomly allocate 80% for training, 10% for validation, and the remaining 10% for testing.\n2) Evaluation Metric: We conduct the Click-Through Rate (CTR) prediction task on all datasets and assess the overall performance using the AUC score (Area Under the ROC Curve) and LogLoss (logarithmic loss). A larger AUC score or a lower LogLoss value at the 0.001 level can indicate a significant improvement [39]. Additionally, we employ the single-sided t-test [43] to validate the superiority of the best-performing methods over the second-best.\n3) Baselines: We compare GPRec with three sorts of methods: (1) Feature interaction modeling: MLP [13], DeepFM [39], DCN [21], GDCN [22], FinalMLP [33], and DESTINE [19]; (2) Group modeling: MMOE [28], PLE [29], STAR [31], and DGPM [27]; (3) Individual modeling: APG [8] and PEPNet [26].\n4) Implementation Details: For the baseline implementation, we utilized three public GitHub libraries567. The embedding size, both for DRS in Equation (1) and for GPRec's group embeddings in Equation (5), is set to 16. The dimensions of the MLPs used for prediction purposes, including f1 for group embedding learning (Section III-B), as well as fy, fb, fG, fp for model construction (Section III-D), are configured as [64, 32, 16, 1]. The MLP FG, serving as the group classifier in Equation (2), has dimensions of [64, 32, 2] for each group, with the total number of groups (G) set at 60. For Equation (20), the parameters are \u5165\u2081 = 1.0, X2 = 0.001, \u03bb3 = 0.0001, and T = 0.5 as per Equation (3).\nB. Overall Performance (RQ1&RQ2)\nIn this section, we conduct a comprehensive evaluation of GPRec on three public datasets, comparing its performance against baseline models and examining its compatibility with various DRS backbones. To ensure the reliability of our findings, each value is the average of 5 replicated tests with different seeds. For our GPRec, we present only the results of the best-performing model construction.\nWe display the GPRec with GDCN and MLP backbones in Table II, demonstrating the optimal performance and enabling a fair comparison with MLP-based baselines, respectively. To test the compatibility of GPRec, we also provide results with other backbones and compare these against the most similar baseline DGPM in Table III. The results table marks the highest-performing method in bold, and the runner-up method is underlined. We conclude that:\n\u2022 From the upper half of Table II, which presents the results for 'Feature Interaction' methods, we observe that advanced architectures (GDCN, FinalMLP, and DESTINE) significantly outperform classical models (MLP, DeepFM, DCN). This underscores the effectiveness of developing complex architectures to enhance recommendation quality. When integrated with the GDCN backbone, GPRec outperforms all baselines. Meanwhile, GPRec with the MLP backbone also demonstrates competitive results, as depicted in the last column of the lower half of Table II, surpassing most baselines except for GDCN. These findings highlight the significant improvements achieved by GPRec through integrating bi-level user modeling.\n\u2022 The lower half of Table II includes the results of 'User Modeling' methods, all of which primarily comprise MLP components. It is observed that all baselines generally outperform the MLP (the first column in the upper half), validating the effectiveness of considering either user group patterns (MMOE, PLE, STAR, DGPM) or individual patterns (APG, PEPNet). However, methods that require predefined criteria for user division (MMOE, PLE, STAR) fail to enhance performance on KuaiRand. This underperformance can be attributed to inappropriate division criteria. Due to the lack of the 'Gender' attribute (used for user division in ML1M and TenRec), we use 'User Active Degree' as an alternative in KuaiRand, which may not be as effective. In contrast, the group modeling method with flexible group division (DGPM) demonstrates stability across datasets. This underscores the advantages of implementing flexible group division over rigid predefined groups. Additionally, user individual modeling methods prove to be very effective, highlighting the importance of recognizing individual patterns. With the integration of both group and individual patterns, our GPRec (MLP) achieves optimal performance among all MLP-based methods.\n\u2022 In Table III, where 'Base' indicates the results for backbone models without any additional structures, we observe that GPRec consistently enhances the backbone model and outperforms DGPM across all datasets and backbones. Notably, DGPM fails to integrate with FinalMLP, indicated with '-' in the table, whose prediction layer is not suitable for dynamic parameter integration\u2014the sole construction strategy offered by DGPM. These results underscore the modular and adaptive design of GPRec, which allows it to seamlessly integrate with a variety of backbone architectures, thereby consistently enhancing their performance.\nTo sum up, the bi-level user modeling of GPRec contributes significantly to enhancing recommendation quality across various datasets and backbones. The plug-in design and flexible construction strategies enable it to seamlessly integrate with any backbones. Consequently, the proposed GPRec can serve as a convenient and effective solution for most DRS to improve their performance."}, {"title": "C. Ablation Study (RQ3)", "content": "In this section, we execute experiments to ascertain the significance of key components within GPRec. We introduce five variants to discern the contributions of dual group embedding ($E_G$), individual preference ($r_P$), and three auxiliary losses in Equation (20) ($L_G$, $L_{Con}^G$, $L_O$):\n\u2022 GPRec-1: This variant replaces dual embeddings with a single embedding for each group, correspondingly altering the group division method (Equation (2)-Equation(6)). Specifically, the dual embedding for each group is replaced by a single embedding, and Softmax is employed on S to classify users, similar to the approach used in the baseline DGPM [27]. This modification is intended to evaluate the effectiveness of the dual group embedding ($E_G$), alongside the group division method proposed.\n\u2022 GPRec-2: To assess the impact of individual representations $r_P$ in Section III-C, this version omits $r_P$ entirely, including its involvement in $L_O$ from Equation (20) and its use in predicting $\\hat{y}$ in Section III-D (e.g., Equation (16)).\n\u2022 GPRec-3, GPRec-4, and GPRec-5: These three variants respectively remove $L_G$, $L_{Con}^G$, $L_O$ in Equation (20) to validate the effectiveness of each auxiliary loss. Each removal is designed to validate the effectiveness of each auxiliary loss in enhancing the overall model performance, enabling a focused analysis of the contribution of each component to GPRec.\nThe performance of these variants was tested on ML1M and KuaiRand with MLP backbone. The results are illustrated in Figure 2, which displays the ratio of performance (AUC, LogLoss) between the variants and the original GPRec. We can observe that:\n\u2022 GPRec-1's underperformance highlights the critical role of the dual group embedding and corresponding group division method in learning effective group representations.\n\u2022 The variant GPRec-2, which omits $r_P$, records the high LogLoss on both datasets, indicating poor performance. This outcome strongly supports our assertion regarding the crucial role of individual preferences in enhancing DRS.\n\u2022 The underperformance of GPRec-3, GPRec-4, and GPRec-5 compared to the original GPRec demonstrates that the omission of any auxiliary loss can significantly degrade model performance. Notably, GPRec-3 and GPRec-4 exhibit the lowest performance on ML1M and KuaiRand, respectively. This underscores the importance of $L_G$ and $L_{Con}^G$ in learning representative group embeddings, as expounded in Section III-B. Furthermore, as anticipated in Section III-C, incorporating an orthogonal loss slightly improves individual modeling and benefits recommendations.\nIn summary, this experiment supports the rationale and the effectiveness of essential components in GPRec, including dual group embeddings, individual preference modeling, and three auxiliary losses."}, {"title": "D. Hyper-Parameter Study (RQ4)", "content": "In this section, we explore the influence of crucial parameters on GPRec. Our focus is primarily on the number of groups G for $E_G$ and the temperature parameter \\tau from Equation (3). We test the AUC score on the ML1M dataset with the MLP backbone. Parameters are altered one at a time, with G varying among [3, 7, 20, 40, 60, 80, 100, 1000] and \\tau tested at values [0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 1]. The results are presented in Figure 3. Notably, the AUC for the base MLP model, which stands at 0.8081, is omitted in Figure 3 due to its comparatively low value. Our observations include:\n\u2022 From Figure 3 (a), the AUC score initially improves with increasing G. However, beyond a certain threshold, further increments in G cease to enhance AUC. The reason is that GPRec initially benefits from more fine-grained group division, and an excessive number of groups can lead to undertraining of group embeddings.\n\u2022 Figure 3 (b) demonstrates a similar trend with \\tau. A highly definitive group division (small \\tau) can confine users to static choices during the training, making the optimization stuck at the suboptimal state. While overly flexible division (large \\tau) blurs positive and negative group patterns, impairing the learning of contrastive and predictive group embeddings.\n\u2022 The optimal configuration is G = 60, \\tau = 0.5. This setting was selected for overall performance evaluation in Section IV-B. Remarkably, GPRec consistently surpasses the base model in AUC across all parameter settings, demonstrating its stability."}, {"title": "E. Model Visualization (RQ5)", "content": "In this section, we examine group embedding similarities of GPRec to evaluate whether our expectations are fulfilled. Figure 4 displays the group embedding similarities obtained by GPRec on ML1M with MLP backbone (G = 60). For clarity, we visually represent the cosine similarities between five randomly selected groups. The results indicate that GPRec successfully learns contrasting group embeddings, evidenced by the notably low similarities between negative and positive embedding pairs (the diagonal of the top right and bottom left quadrants, e.g., 0.03 for p2 and n2). Besides, the heatmap predominantly shows light green, with a maximum similarity of 0.6 between n3 and n4 (two negative group embeddings), suggesting a robust representation capability of group embeddings in GPRec. In comparison, group embeddings in DGPM [27] exhibit an average similarity of 0.9. This finding aligns with our expectations in Section III-B, where we apply Gumbel-Softmax and design dual embedding space to obtain diverse representations."}, {"title": "V. RELATED WORK", "content": "The proposed framework aims to improve DRS through bi-level user modeling. Consequently, this section discusses the related work from two perspectives: DRS architectures and user modeling methods.\nDRS Architectures The architecture of DRS significantly impacts the performance, making it a popular area of focus in both industry and academia. Basic DRS architectures include linear structures, feature factorization machines (FM) [15], and attention mechanisms [44]. Other structures found effective for feature crossing or fusing interactions include cross networks [21], product networks [23], and gating mechanisms [24]. Combining these components has led to the development of numerous popular architectures, such as MLP [13], DeepFM [39], and Wide & Deep (WD) [40]. More advanced DRS architectures have been developed in recent years based on these foundational components [18], [19], [22], [25], [33]. For example, GDCN [22] designs the gating mechanism for crossed features based on DCN [21], significantly improving the recommendation quality and interpretability. FinalMLP [33] invises a dual stream network with bi-linear fusion as the prediction layer for explicit feature interaction modeling. While DESTINE [19] proposes the disentangled attention to model the importance of the single feature and interaction pairs, achieving the superior performance. However, these methods focus on enhancing the general feature interaction modeling ability for all features without sufficiently distinguishing valuable user features to improve user modeling.\nIn this paper, we propose a novel bi-level user modeling framework designed to enhance various DRS backbones. We present results from combining this framework with basic MLP, attention-based methods (DESTINE [19]), cross networks (DCN [21]), gating methods (GDCN [22]), and the latest structure, FinalMLP [33].\nUser Modeling. User modeling methods can be categorized based on the modeling level: individual modeling and group modeling. In individual modeling, APG [8] and PPNet [26] typically generate personalized parameters based on user features to capture fine-grained user patterns and integrate these parameters for embeddings or prediction MLP. Conversely, group modeling aims to identify shared patterns within user groups, usually including the group division and pattern modeling. A straightforward approach involves categorizing users based on specific attributes and applying multi-task and multi-domain frameworks [28], [29], [31], [45] to model the group pattern. Recently, DGPM [27] provides a flexible group division method and feeds the group parameters to the prediction MLP for group modeling. It assigns group embeddings and prediction parameters to each group, which are updated by a Memory Network [46]. Users are first categorized into groups based on their similarities to group embeddings. Then, corresponding prediction parameters are feed to the prediction MLP for group-wise recommendations.\nWhile existing user modeling approaches typically concentrate on a single level, either group or individual, GPRec introduces a novel approach by integrating bi-level modeling. Our approach has demonstrated superior performance in recommendation tasks."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduced GPRec, a cutting-edge approach to user modeling, which incorporates both group and individual levels and significantly improves deep recommender systems. GPRec innovatively utilizes a learnable classifier to categorize users based on user features and employs dual group embeddings to represent their group patterns. It then captures individual preferences from user personal features. These elements are seamlessly integrated into various recommendation models using flexible construction strategies for practical applications. Our comprehensive experiments have demonstrated GPRec's superiority."}]}