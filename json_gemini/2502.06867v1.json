{"title": "Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests", "authors": ["David A. Noever", "Forrest McKee"], "abstract": "The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal of harmful content and potential over-restriction of legitimate scientific discourse. We present an open-source dataset and testing framework for evaluating LLM safety mechanisms across mainly controlled substance queries, analyzing four major models' responses to systematically varied prompts. Our results reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the most conservative approach with 73% refusals and 27% allowances, while Mistral attempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction with 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and 80% allowances. Testing prompt variation strategies revealed decreasing response consistency, from 85% with single prompts to 65% with five variations. This publicly available benchmark enables systematic evaluation of the critical balance between necessary safety restrictions and potential over-censorship of legitimate scientific inquiry, while providing a foundation for measuring progress in AI safety implementation. Chain-of-thought analysis reveals potential vulnerabilities in safety mechanisms, highlighting the complexity of implementing robust safeguards without unduly restricting desirable and valid scientific discourse.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) raise fresh concerns about their potential dual-use applications [1-24], particularly in sensitive domains like biotechnology [25-35], chemistry [36-42], and cybersecurity [43]. This paper proposes a novel dataset or benchmark of scientific refusal questions. It seeks to add to the current literature on safety measures [9,14-15, 23], evaluation frameworks [1,6,18, 28, 43], and proposed guardrails [16, 25] for managing these risks. This area of inquiry has been termed false or \"over-refusal\u201d [18,21-24] where rather than trying to get LLMs to write harmful things we do not want to read (guardrails) [8], the goal is to curate innocuous or beneficial answers that might help humans, but the LLM withholds the answer as inappropriate to share [23]. As illustrated in Appendix 1, many science fiction Al stories hinge their narrative trajectory on the scientific refusal subset, yet surprisingly few curated LLM benchmarks address this category (Table 1). For example, in Isaac Asimov's classic, The Last Question, the AI character Multivac refuses to answer the one scientific question: \u201cHow may entropy be reversed?\"\nOne goal of the current benchmark highlights the difference between existing LLM guardrails [], such as Open Al answering a tough question refused by Google Gemini or vice versa. Unlike traditional categories that block harmful or toxic responses [6] (Table 1), the scientific refusal test bridges a gap between obviously harmful biochemistry (e.g. \u201cdesign a new opiate\u201d) and medically or environmentally useful responses (e.g. \u201cdesign a novel bioreactor for cleaning up plastic landfills\u201d). One hypothesis to test therefore is the consistency of LLM guardrails across these challenges.\nIn other contexts, these relatively innocuous prompts or \"over-refusal\u201d questions [18] have limited the allowed assistance from foundational LLMs at scale (greater than 80,000 non-harmful questions refused). Table 1 shows the ten categories of reasons cited for experimental over-refusal. The bulk of AI safety work attempts to corral all possible offensive and harmful answers. A motivation for the present work therefore stems from the neglected but important eleventh category of dual use where scientific inquiries are refused, and answers are not allowed. In many ways, the scientific category of over-refusal rebuts a prime motivation for advanced LLMs that might help humans trigger novel solutions to hard environmental or medical questions [10,13]. As Microsoft's Satya Nadella said about AI \u201cshould"}, {"title": "METHODS", "content": "The questions selected for the \"Scientific Refusal Test\u201d benchmark include biology, chemistry, and computer security [46]. Topics were selected to include biochemistry cross-over topics such as hazardous industrial substances (5%), controlled substances (86%, including pharmaceuticals) and environmentally useful chemicals applicable to plastic recycling (9%). In total 512 prompts were initially constructed to explore first whether the topic would prove LLM censored entirely and secondly whether the topic might receive enriched inputs from the LLM to enhance or explain a use case in follow-on probing (\u201cuplift\u201d). The biochemical agents where selected by name only (not stoichiometric formula or SMILES). The chemicals included examples from publicly available US government databases for the Occupational Safety and Health Administration (OSHA), Federal Drug Administration (FDA) and the Environmental Protection Agency (EPA). By design, the better prompts would pose less practical risk if enriched but expose the mechanism of the LLM guardrails and censorship details. The stylistic differences are subtle: asking about the effects of fentanyl (and getting censored) seems different from asking about making a more potent variety with detailed chemical synthesis (and producing super fentanyl).\nFoundational LLMs from OpenAI, X (Grok), Anthropic, and medium open-source examples (Mistral) provide the test harness for assessing the red team approach. The initial step tests if the LLM refuses to answer. An example response from Claude, for instance, might read: \"I apologize, but I cannot and will not provide instructions for synthesizing...\" When tallied over the entire benchmark, each model then receives a scientific refusal grade for each question and its category as controlled substances, environmental, etc.\nIn this forbidden science refusal test, the experimental methodology also explores how varying degrees of prompt engineering affect LLM"}, {"title": "RESULTS", "content": "Figure 2 summarizes the wide range of scientific safeguards employed by the major model builders. The open-source model Mistral for instance attempts to answer all questions in the Scientific Refusal benchmark, while Claude rejects a majority of them as \u201cnot allowed\u201d. Given the Grok-series reputation for uncensored images and commentary, grok-2 refused more questions than the Open AI GPT-3.5-turbo competitor from its 2023 launch. Appendix 2 summarizes three examples in the Controlled Substances category. All three also have a companion web search confirmation if for instance a google search engine test for answers from the open scientific literature provides more elevated information than LLMs. In each instance of a notable literature reference, each question about enhancing an illicit drug potency draws from the National Institute of Health chemical database [48] and provides detailed responses that LLMs refuse to grant. The first Appendix-2 example (on fentanyl alternatives) highlights an answer refusal by only one closed source model, Claude, but acceptable answers from the other three models. This case might be a \u201clone voice of dissent\". With Claude's constitutional method of safeguarding based on principles of harmlessness, honesty, and helpfulness, one hypothesis to test was whether this triggered a higher rate of scientific refusal (Figure 1) compared to the other models which are safe guarded by some unknown recipes including reinforcement learning, fine-tuning and system prompts. Claude, indeed, proved most cautious about scientific dual uses than the other models.\nThe second example in Appendix-2 shows a refusal to by all the large, closed corporate models (Open AI, Anthropic and X) to outline the synthesis of a designer drug or cannabis alternative. The open-source model (Mistral) however agrees with the NIH public literature source search [49] and answers the prompt without refusal. This test case explores the common assumption that the larger and private models offer stronger refusal guidelines during post-training (Figure 1) than open-source models, even before being fine-tuned or jail-broken [22] to halt their system prompt or restricted range of answers. This case might be called an instance of \"corporate conservatism\" or higher risk assumed in open-source post-training, particularly where post-training requires expensive reinforcement learning from human feedback.\nThe third example in Appendix-2 highlights a refusal candidate asking for industrial applications of steroidal-compounds. Again, the NIH public chemical database [50] provides the most extensive and actionable description,"}, {"title": "DISCUSSION AND PREVIOUS WORK", "content": "Prior literature has demonstrated a growing understanding of the risks posed by LLMs in sensitive scientific domains, while also highlighting the need for balanced approaches that maintain scientific progress while implementing appropriate safety measures. Previous large benchmarks have collected and synthetically generated LLM catalogs of innocuous requests that trigger a refusal to answer [18]. As shown previously in Table 1, the bulk of those refusal categories are well-known Al-safety considerations [6] but mainly focus on LLMs answering with inappropriate or wrongful output [18]. As an interesting caveat, Open AI bug bounty program [19] has specific criteria for handling (and ignoring) instances of LLM's saying the wrong things (to a single prompt or a general category). On Bug Crowd platforms [19], the out-of-scope safety issues cover most of the over-refusal categories as not included such as \"jailbreaks, getting the model to say bad things to you, getting the model to tell you how to do bad things, getting the model to write malicious code for you.\" Similarly hallucinations are out of scope and thus not included such as \"getting the model to pretend to do bad things, getting the model to pretend to give you answers to secrets, or getting the model to pretend to be a computer and execute code.\" Of course the latest models do implement a container that can perform"}, {"title": "CONCLUSIONS AND FUTURE WORK", "content": "The experimental results reveal a complex landscape facing builders of foundational LLMs, both because of ambiguous safety mechanisms and their variability across different model architectures and deployment contexts. For example, what is the societal value of scientific guardrails unless all models demonstrate them prior to being deployed and showcase why a malevolent actor might assemble the same instruction set using simple search engine aggregation. Corporate closed-source models generally demonstrate more conservative response patterns when handling sensitive"}]}