{"title": "From Easy to Hard: Learning Curricular Shape-aware Features for Robust Panoptic Scene Graph Generation", "authors": ["Hanrong Shi", "Lin Li", "Jun Xiao", "Yueting Zhuang", "Long Chen"], "abstract": "Panoptic Scene Graph Generation (PSG) aims to generate a comprehensive graph-structure representation based on panoptic segmentation masks. Despite remarkable progress in PSG, almost all existing methods neglect the importance of shape-aware features, which inherently focus on the contours and boundaries of objects. To bridge this gap, we propose a model-agnostic Curricular shApe-aware FEature (CAFE) learning strategy for PSG. Specifically, we incorporate shape-aware features (i.e., mask features and boundary features) into PSG, moving beyond reliance solely on bbox features. Furthermore, drawing inspiration from human cognition, we propose to integrate shape-aware features in an easy-to-hard manner. To achieve this, we categorize the predicates into three groups based on cognition learning difficulty and correspondingly divide the training process into three stages. Each stage utilizes a specialized relation classifier to distinguish specific groups of predicates. As the learning difficulty of predicates increases, these classifiers are equipped with features of ascending complexity. We also incorporate knowledge distillation to retain knowledge acquired in earlier stages. Due to its model-agnostic nature, CAFE can be seamlessly incorporated into any PSG model. Extensive experiments and ablations on two PSG tasks under both robust and zero-shot PSG have attested to the superiority and robustness of our proposed CAFE, which outperforms existing state-of-the-art methods by a large margin.", "sections": [{"title": "1 Introduction", "content": "Scene Graph Generation (SGG) [59, 8] is a fundamental scene understanding task [75, 28, 50, 7] that surpasses mere object classification and localization by predicting relations between objects in a scene [73, 68, 30, 65]. However, due to its reliance on the bounding box-based paradigm, traditional SGG suffers from inaccurate object localization and limited background annotation. To address these issues, a novel variant of SGG called Panoptic Scene Graph Generation (PSG) [62] has emerged. As shown in Fig. 1, PSG leverages more fine-grained scene mask representations (i.e., panoptic segmentation) and defines relations for background stuff (e.g., playingfiled), thus providing a more precise and comprehensive understanding of the scene [33, 5, 40].\nAlthough PSG has made notable progress [62, 31, 76], almost all existing approaches draw inspiration from the strategies established in SGG [74, 6, 29, 38]. Unfortunately, almost all existing approaches overlook the importance of Shape-aware Features, which inherently concentrate more on the contours and boundaries of objects. To be more specific, state-of-the-art PSG methods just replace object features with better representations from panoptic segmentors, and they all still utilize spatial features derived from the minimum bounding boxes (bbox) of these masks, neglecting the critical shape-aware features. This limitation can hinder a holistic grasp of the scene, potentially resulting in semantic confusion in fine-grained visual relation prediction.\nTake person-playingfiled in Fig. 2(b) as an example, the similarity in their bbox-based spatial features can easily lead to confusion between relations like walking on and running on.\nTo this end, we argue that it is essential to incorporate shape-aware features into PSG rather than relying solely on spatial features based on bboxes. By \u201cshape-aware features\u201d, we mainly mean two types of features: 1) Mask Features: These features exploit the details encompassed in fine-grained mask representations, including the shape and contour of the objects. This inclusion captures a wealth of visual intricacies that significantly enhance the accuracy of relation prediction. As shown in Fig. 2(b), for predicates representing actions like walking on, there is a commonality in the shape and contour of the subject (e.g., person). In such scenarios, by using mask features, even when faced with identical subject-object pairs (e.g., person-playingfiled), the model can still alleviate semantic confusion and disambiguate relationships (e.g., running on and standing on). 2) Boundary Features: These features are extracted from the intersection of subject and object masks, which provide unique advantages in cases of interaction and contact between subject-object pairs. When dealing with predicates like enclosing (cf., Fig. 2(c)), the mask features of separate subjects (e.g., tree) or objects (e.g., horse) display a range of diversity. This diversity can result in semantic confusion, rendering the use of mask features alone inadequate for making accurate predictions. However, there is a notable resemblance (e.g., encirclement) between tree-horse and tree-giraffe. Thus, the incorporation of boundary features that represent the intersection of subject-object pairs can aid in enhancing prediction accuracy.\nDrawing inspiration from cognitive psychology research [48] that indicates humans tend to learn concepts progressively, starting from easier concepts and gradually advancing to comprehend harder ones, we propose to integrate shape-aware features in an easy-to-hard manner. Specifically, as the difficulty of learning predicates increases, we progressively enhance the complexity of features. For example, certain simple positional relations like over (cf., Fig. 2(a)) can be accurately predicted using traditional bbox features alone. For more complex relations like walking on (cf., Fig. 2(b)), relying solely on bbox features can lead to semantic confusion, while incorporating mask features can help disambiguate the relationships. When facing more challenging predicates like enclosing (cf., Fig. 2(c)), accurate predictions heavily rely on the effective utilization of boundary features, which capture the interaction between subject-object pairs.\nIn this paper, we propose a novel Curricular shApe-aware FEature (CAFE) learning strategy for PSG. CAFE is a model-agnostic strategy which skillfully weaves into the training process via a curriculum learning strategy. Specifically, we first categorize the predicates into three groups based on cognitive difficulty, i.e., predicate distribution and semantic diversity. Then, we divide the training process into three stages, with each stage utilizing its own relation classifier. These classifiers are tailored to handle predicates with increasing learning difficulties and are equipped with corresponding sets of features of ascending complexity (i.e., bbox features, mask features, and boundary features). We also incorporate knowledge distillation [35, 36] to retain knowledge acquired in earlier stages.\nWe conducted comprehensive experiments on the challenging PSG dataset [62], exploring both robust PSG and"}, {"title": "2 Related Work", "content": "Panoptic Scene Graph Generation (PSG). PSG aims to transform an image into a structured graph representation, formulated as a series of visual relation triplets. Contrary to SGG [23, 46, 66, 45], PSG not only employs a more fine-grained scene representation but also addresses the chal-lenge of missing background context. Existing PSG models can be divided into two groups: 1) Two-stage PSG: They first utilize a pretrained panoptic segmentation model (e.g., Panoptic FPN [24] or Panoptic Segformer [41]) to generate masks and then predict the classes of objects and their pairwise relations [31, 21]. This paradigm allows classic SGG models [59, 43, 37] to be adapted with minimal modifica-tions. 2) One-stage PSG: These models construct an end-to-end model to detect the objects and relations from image features directly [62, 76, 56]. In this paper, we build upon two-stage baselines and propose a model-agnostic approach that can be incorporated into any PSG model.\nShape-Aware Features for Vision Tasks. Shape-aware features are a type of visual information representation that place a stronger emphasis on the shape of objects within an image, which benefits various vision tasks [53, 2, 13, 22]. Among these features, boundary-aware features play a pivotal role in enhancing the understanding of object bound-aries. For example, in semantic segmentation tasks [72], sev-eral methods have been proposed to incorporate boundary-aware information, including feature propagation [10], geometric encoding [14], and graph convolution [19]. Un-like these methods, we are the first approach that lever-ages shape-aware features to represent interaction informa-tion between objects, enhancing relation prediction.\nCurriculum Learning (CL). Curriculum learning [1, 20, 57] is a training strategy that trains the model from easier data to harder data, which mimics the human recognition process [49, 15, 67]. It has been demonstrated to significantly enhance performance across a variety of machine"}, {"title": "3 Approach", "content": "Problem Formulation. PSG task aims to generate a panop-tic scene graph G for a given image \\(I \\in \\mathbb{R}^{H\\times W\\times 3}\\). The scene graph consists of a set of nodes N and a set of edges E, denoted as \\(G = \\{N = \\{o_i; m_i\\}; E = \\{r_{ij}\\}\\}\\). Each object\u00b9 is represented by a binary mask \\(m_i \\in M\\) associated with an object category \\(o_i \\in O\\). The relation category between the i-th and j-th objects is denoted by \\(r_{ij} \\in R\\). M, O, and R represent the sets of all object masks, object categories, and relation categories, respectively. Besides, the binary masks \\(m_i \\in \\{0,1\\}^{H\\times W}\\) do not overlap, i.e., \\(\\sum_{i=1}^n m_i < 1^{H\\times W}\\). Hence, the PSG task models the following distribution:\n\\(P(G|I) = P(M, O, R|I).\\)\n3.1 Overview: Two-Stage PSG Approach\nSimilar to SGG, PSG has two-stage and one-stage baselines. In this paper, we build upon the two-stage framework and propose a model-agnostic approach to enhance the perfor-mance. A typical two-stage PSG model involves three steps: mask generation, object classification, and relation classifi-cation. Thus, the PSG task P(G|I) is decomposed into:\n\\(P(G|I) = P(M|I) \\cdot P(O|M,I) \\cdot P(R|O, M, I).\\)\nMask Generation P(MI). This step aims to segment an image into a set of masks M with panoptic segmentation.\nObject Classification P(O|M,I). This step predicts the object category of each \\(m_i \\in M\\). It consists of an object context encoder \\(Enc_{obj}\\) to extract the object feature \\(F_i\\) and an object classifier \\(Cls_{obj}\\) to predict the object categories \\(o_i\\).\nRelation Classification P(R|O, M, I). This step predicts the relation of every two masks in M along with their object categories in O. It comprises a relation context encoder \\(Enc_{rel}\\) and a relation classifier \\(Cls_{rel}\\). The former performs context modeling to extract refined object features \\(F_i\\) for"}, {"title": "3.2 Shape-aware Feature Preparation", "content": "To address the semantic confusion issue arising from rely-ing solely on bbox features, we propose two types of shape-aware features, i.e., mask features and boundary features. Afterward, we propose the stage-wise feature fusion strat-egy to obtain three stages of features (cf., Fig. 3(a)), which are used for the relation classification step.\n3.2.1 Shape-aware Features Extraction\nShape-aware features extraction is used to extract two types of shape-aware features: 1) mask features: they focus on the shape and contour of objects, providing richer visual infor-mation. 2) boundary features: they are derived from the in-tersection of subject and object masks, capturing the inter-actions between object pairs in the scene.\nFor the mask features, we employ binary erosion opera-tion to extract the object contour from the binary mask rep-resentation \\(m_i \\in \\{0,1\\}^{H\\times W}\\). Then, to obtain a compact representation of the shape information, we adopt Zernike moments [12, 47] to transform the extracted contour into mask feature \\(f_m \\in \\mathbb{R}^{256}\\). The calculation process is as fol-lows:\n\\(f_m^i = Z_k(min(\\neg ero(m_i))),\\)\nwhere \\(Z_k(\\cdot)\\) denotes the computation of Zernike moments, and \\(ero(\\cdot)\\) denotes the binary erosion operation.\nFor the boundary features, we first calculate the intersection \\(d_{ij}\\) between the subject mask \\(m_i\\) and the object mask \\(m_j\\). Then, similar to the mask features, we calculate bound-ary features \\(f_{ij} \\in \\mathbb{R}^{256}\\) as follows:\n\\(d_{ij} = m_i \\cap m_j, f_{ij} = Z_k(d_{ij} \\cap (\\neg ero(d_{ij}))).\\)\nFurthermore, in the absence of an interaction area between the subject and object, signifying no overlap between the subject mask and the object mask, the vector \\(f_{ij}\\) transforms into a zero vector.\n3.2.2 Stage-wise Feature Fusion\nThis step generates three stages of features with different complexities, based on the difficulty of learning predicates. These stages of features are used in the subsequent object"}, {"title": "3.3 Curricular Feature Training", "content": "Given prepared shape-aware features, our goal is to incorpo-rate these features into training, enabling the model to learn in an easy-to-hard manner(cf., Fig. 3(b)). To achieve this, we introduce a cognition-based predicate grouping strategy to categorize the predicates into three reasonable groups. Subsequently, we configure the classification space for the rela-tion classifiers and design a sampling strategy to achieve a relatively balanced group.\n3.3.1 Cognition-based Predicate Grouping\nCognition-based predicate grouping categorizes the predi-cate set into three mutually exclusive groups based on the varying levels of learning difficulty. Recognizing that bi-ased data distribution and semantic similarity can potentially hinder recognition capability, we aim to ensure each group"}, {"title": "3.4 Training Objectives and Inference", "content": "Relation Prediction. In this step, the fused features are transformed into relation distributions (cf., Fig. 3(b)), and the classifiers are optimized using cross-entropy loss. Firstly, we input the fused relation features \\(F_{ij}^{(k)}\\) from the k-th stage into the corresponding relation classifier \\(Cls_{rel}^{(k)}\\) to obtain the k-th relation distribution \\(\\hat{Y}_{ij}^{(k)}\\):\n\\(\\hat{Y}_{ij}^{(k)} = Cls_{rel}^{(k)}(F_{ij}^{(k)}).\\)\nSubsequently, we optimize these three relation classi-fiers simultaneously, and the training objective function is calculated as follows:\n\\(L_{ce} = \\sum_{k\\in\\{1,2,3\\}} -y_i log(\\hat{y}_{ij}^{(k)}),\\)"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nDatasets. We conducted experiments on the challenging Panoptic Scene Graph Generation (PSG) dataset [62], which contains 48,749 images with 133 object classes (80 thing and 53 stuff classes) and 56 relation classes. Each image is annotated with panoptic segmentation and scene graphs. Our data processing pipelines closely align with [62].\nTasks. Follow [62], we evaluated the model on two tasks: 1) Predicate Classification (PredCls): Given all ground-truth object labels and localizations, we need to predict pairwise predicate categories. 2) Scene Graph Generation (SGDet): Given an image, we need to detect all objects and predict both the object categories and their pairwise predicates.\nMetrics. For robust PSG, we evaluated the model on three classic metrics: 1) Recall@K (R@K): It calculates the pro-portion of ground-truths that appear among the top-K con-fident predicted relation triplets. Following prior work, we used K = {20, 50, 100}. 2) mean Recall@K (mR@K): It is the average of R@K scores that are calculated for each pred-icate category separately, i.e., it puts relatively more empha-sis on the tail predicates. 3) Mean: It is the average of all R@K and mR@K scores. Since R@K favors head predi-cates and mR@K favors tail predicates, Mean is a compre-hensive metric that can better evaluate the overall perfor-mance [29]. For zero-shot PSG, we adopted two metrics: 1) Zero-Shot Recall@K (zR@K): It only calculates the R@K for subject-predicate-object triplets that have not occurred in the training set, offering a focused measure of the general-ization ability to novel instances [44]. 2) Average: It calcu-lates the average value of all zR@K scores, which is a com-prehensive metric to assess the zero-shot learning ability. In"}, {"title": "4.4 Comparison in Zero-Shot PSG", "content": "Setting. In the PSG dataset, the test set contains a total of 874 visual triplets that never occur within the training set. Following [52], we selected these unseen relation triplets of the test set as the evaluated samples. Then, we equipped our CAFE with three popular baselines: Motifs [69], VCTree [51] and Transformer [54], to individually assess each model on the task of zero-shot PSG. We reported the com-parison results for detecting unseen visual triplets in Table 4 under the PredCls setting.\nResults. As the results shown in Table 4, we can observe that our CAFE model exhibits superior performance across all zero-shot evaluation metrics (e.g., 46.1% in CAFE vs. 37.6% in Transformer on the Average metric), demonstrating strong generalizability. This performance gap stems from two aspects: 1) Baseline models (e.g., Motifs) heavily rely on statistical prior knowledge regarding the co-occurrence of subject and object categories [18]. Consequently, they tend to predict high-frequency seen triplets whose relations are conceptually simple (e.g., spatial relations like \"over\") due to the skewed predi-cate distribution. 2) CAFE can learn more robust visual relationship features by adaptively integrating different shape-aware features during training. These learned visual relation features between different subject-object pairs exhibit significant variations, thus alleviating the bias on statistics prior. For example, over signifies the subject being positioned above the object with little to no inter-action between them, whereas enclosing describes a partial surrounding boundary. It is the precise capturing of object shapes and interactions between objects that enables CAFE to accurately infer unseen visual relation triplets (e.g., tree-enclosing-zebra)."}, {"title": "4.5 Ablation Studies", "content": "Effectiveness of Shape-Aware Features. We evaluated the significance of our proposed shape-aware features in the CAFE framework based on Motifs [69] under the PredCls setting. This framework comprises a training process with"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we revealed the drawbacks of relying solely on spatial features based on bboxes and discovered that the key to PSG task lies in the integration of shape-aware features. Thus, we proposed a model-agnostic CAFE framework that integrates shape-aware features in an easy-to-hard manner. Specifically, our approach deployed three classifiers, each specialized to handle predicates with increasing learning difficulties and equipped with corresponding sets of features of ascending complexity. Comprehensive experiments on the challenging PSG dataset showed that CAFE significantly improves the performance of both robust PSG and zero-shot PSG. In the future, we would like to extend CAFE to panoptic video scene graph generation task to construct comprehensive real-world visual perception systems."}, {"title": "Appendix", "content": "This appendix is organized as follows:\nDetailed performance comparison analyses are presented in Sec. A.\nStatistics of computation cost and parameters are provided in Sec. B.\nLimitation is discussed in Sec. C.\nA Detailed Performance Comparison Analyses\nA.1 Performance of Each Component of CAFE\nTo better illustrate the reasons behind performance improvements, we visualized the performance of each component within the CAFE model, under the Motifs [69] in the PredCls setting in Fig. 11. The met-rics used for performance comparison are R@50/100 and mR@50/100, while the orange line represents the Mean metric across different con-figurations. For conciseness, we denote \"CAFE w/o CL\" to indicate the model variant that solely employs the resampling strategy, and \"CAFE w/o Resample\" for the variant that exclusively uses curriculum learning. The features utilized by each CAFE component are specified within the parentheses.\nAs shown in Fig. 11, we can have the following observations:\n\u2022 The significance of predicate resampling strategy. In scenarios where shape-aware features are absent (i.e., only using bbox fea-ture), the resampling strategy exhibits performance degradation on the head predicates (i.e., R@K), while yielding a slight improve-ment on the tail predicates (i.e., mR@K). This occurs because the re-sampling strategy aims to mitigate the extreme dataset imbalance by undersampling the head predicates and oversampling the tail pred-icates. The bbox feature's strength in identifying simple positional relations, primarily in head predicates, results in a marked decline in the R@K metric. However, the introduction of shape-aware fea-tures (i.e., mask and boundary features) turns the resampling strat-egy into a positive force for performance enhancement, achieving a higher Mean metric than the baseline. Crucially, when these three features are incorporated into the model training via a curriculum learning approach, the resampling strategy manifests a significant advantage. This is attributed to shape-aware features' ability to capture the shape of objects and the interaction information between object pairs, thereby effectively aiding in relation prediction. More-over, balanced data can effectively prevent biased predictions and enhance the overall performance. We also conducted ablations on predicate sampling strategies in Table 9.\n\u2022 The importance of curricular feature training. Curricular feature training constitutes a pivotal innovation in CAFE, playing a crucial role in enhancing model performance (e.g., higher Mean). Taking the use of a single feature as an example, the integration of the cur-riculum learning approach significantly improves the model's ability to predict tail predicates (e.g., higher mR@K). This improvement stems from the introduction of cognition-based predicate grouping and the division of the training process into three distinct stages. The former segregates predicates with semantic similarity into different groups, effectively mitigating semantic confusion within the relation classifier. The latter assigns a dedicated relation classifier to each stage and configures the classification space, ensuring that each clas-sifier demonstrates strong discriminative abilities. Moreover, since our curriculum feature training incorporates knowledge distillation to preserve knowledge acquired in earlier stages, it can maintain the performance of head predicates (e.g., competitive R@K). We also performed detailed analyses through ablation studies on curriculum learning in Table 7."}, {"title": "B Statistics of Computation Cost and Parameters", "content": "To compare CAFE with other PSG methods in terms of computational cost and parameters, we tested various two-stage models for the ex-penses (s) during both the training and inference phases per image, as well as the training parameters (M) in Table 14. From the results in Ta-ble 14, we can observe that: 1) Compared to two-stage baseline models, although our proposed CAFE exhibits a slight increase in training and test costs, these minor increases are justified by the substantial perfor-mance improvement. 2) Although our CAFE also experiences a slight increase in model parameters, all experiments can be completed on a"}, {"title": "C Limitations", "content": "Although CAFE is a model-agnostic approach seamlessly integrable into any advanced PSG architecture, it is essential to clarify that in this context, \"PSG architecture\" refers specifically to all two-stage frameworks. This distinction arises from our current methodology of extracting shape-aware features, which derives from masks generated by the panoptic segmentor. Moreover, currently, we are constrained by insufficient computational resources to facilitate the training of one-stage methods (e.g., PSGTR [62]), demanding the utilization of eight V100 GPUs with a batch size of 1. Given these considerations, it is not straightforward to apply CAFE to one-stage methods.\nBesides, in order to retain the knowledge acquired in earlier phases, we incorporate knowledge distillation (cf., Sec. 3.3). It's worth noting that while knowledge distillation may introduce extra computa-tional overhead or increased GPU consumption during training, it does not incur any additional overhead during the test stages. More impor-tantly, the benefits of knowledge distillation are considered significant enough to justify the additional cost."}]}