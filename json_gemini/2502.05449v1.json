{"title": "Iterative Deepening Sampling for Large Language Models", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "abstract": "The recent release of OpenAI's ol models and other similar frameworks showcasing test-time scaling laws has demonstrated their exceptional capability to tackle complex reasoning tasks. Inspired by this, subsequent research has revealed that such test-time scaling laws hinge on the model's ability to search both within a single response (intra-response) and across multiple responses (inter-response) during training. Crucially, beyond selecting a single optimal response, the model must also develop robust self-correction capabilities within its own outputs. However, training models to achieve effective self-evaluation and self-correction remains a significant challenge, heavily dependent on the quality of self-reflection data. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving, which can subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how manually triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.", "sections": [{"title": "1. Introduction", "content": "Since ChatGPT, large language models (LLMs) have been a rapidly evolving domain that tries to solve problems beyond traditional language tasks like summarization or question answering (Chen et al., 2023; Yao et al., 2023; Chen et al., 2024b;d). Significantly, the newly released OpenAI O1 has demonstrated its strong capability in complex problem-solving through its detailed reasoning steps before outputting the final answer (Jaech et al., 2024). Since then, many researchers have studied how to replicate success from an open-source perspective and how to train models that are even better at efficiently solving problems that still remain unsolvable by the current LLMs (Huang et al., 2024; Zeng et al., 2024; DeepSeek-AI et al., 2025).\n\nThe remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of OpenAI's ol series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inference-time performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities. Moreover, this paradigm not only enhances inference-time efficiency but also facilitates the generation of high-quality synthetic data, which can be leveraged to train the next generation of models and evaluate their performance (Guan et al., 2025).\n\nA key insight in this line of research is that high-quality data capable of eliciting self-reflection in LLMs is crucial for enhancing their reasoning capabilities. Notably, o1-like models demonstrate improved performance when longer reasoning chains are utilized (Huang et al., 2024; DeepSeek-AI et al., 2025). One straightforward yet effective approach to collecting such data involves manually inserting prompts such as \"Wait! Maybe I made some mistakes! I need to rethink from scratch.\" These interventions have been shown to successfully trigger self-reflection, thereby improving the model's reasoning depth and accuracy. However, as this remains an emerging area, many design choices in implementing self-reflection mechanisms lack systematic analy-"}, {"title": "2. Related Works", "content": "Researchers have been exploring two primary directions to efficiently improve response quality under a frozen pre-trained LLM. One of the most commonly known technique is to increase the number of reasoning steps to encourage the LLMs to think more before the final output is provided (Wei et al., 2022). Following this direction, people have been using search to improve the performance of the inference of LLMs, and the recently released ol models are one of the products that lead on this (Jaech et al., 2024). Following the categorization in Zeng et al. (2024), we define search as the process of LLMs making multiple attempts to solve the problem based on internal or external guidance, and the search strategy can be mainly categorized into tree-search and sequential revisions.\n\nFrom the perspective of tree search, researchers try to generate a few responses at the same time and use external guidance to choose the one that is the best, i.e., tree search is searching inter-responses. Based on this classification, the simplest method in this direction is the Best-of-N (BON) sampling (Cobbe et al., 2021; Brown et al., 2024). However, because of its efficiency compared to other actual search algorithms that involve a value estimation, there are many work that focuses on this setting to make it more efficient and effective. For example, Sun et al. (2024) proposed to use speculative rejection in BoN to reject bad responses through early score. Chen et al. (2024e) proposed to use extremely high temperature on the first token to greatly improve the Best-of-N performance on math and coding tasks. Besides BoN sampling, actual search that happens on the tree is also very popular. Inspired by the success of AlphaGo, many researchers believe Monte Carlo Tree Search can be a very efficient search algorithm to be combined with machine learning, and it is a recently hot topic in test-time computation as it has a more accurate value estimation and naturally balances exploration and exploitations (Zeng et al., 2024). Research for MCTS can be further classified into three classes based on how a node is defined, namely token-level actions (Zhang et al., 2023; Liu et al., 2023), step-level actions (Hao et al., 2023; Chen et al., 2024a; Zhou et al., 2023), and solution-level actions (Zhang et al., 2024a;b). On the other hand, beam search, as a very classic sampling algorithm, has been shown to be also efficient, and even better than MCTS in test-time scaling (Chen et al., 2024b). Similar to the different design in MCTS, Xie et al. (2024) proposed to let the LLM themselves be the value function and conduct beam search on a step level. TreeBoN(Qiu et al., 2024) proposed to use a similar step-level beam search data in a DPO training. Snell et al. (2024) proposed a k-step roll-out to estimate the partial sequence in beam-search. In this paper, we adopt the simplest tree-search setting, Best-of-N (BON) sampling, due to the absence of a highly accurate process reward model (PRM).\n\nThe sequential revisions in the search strategy are more about self-evaluation and self-correction that happen sequentially in a single response, i.e, searching inner-responses. This direction is previously known as self-correction, and have also been known as iterations based on reflections."}, {"title": "3. Preliminaries", "content": "3.1. Best-of-N Sampling\n\nFor reasoning-intensive tasks such as mathematical problem-solving and coding, Best-of-N sampling is one of the most widely used strategies for data generation. This approach involves sampling N outputs from the same model using predefined sampling parameters-typically with a higher temperature than single-sample settings-followed by a selection process to determine the best response. The selection criteria depend on the intended use of the samples. During training, responses are typically evaluated using rule-based checkers for mathematical problems or online judges for coding tasks to identify correct answers within the sampled set. At test time, a reward model is often employed to score the generated responses, with the highest-scoring output selected as the final answer. This methodology effectively balances exploration and optimization, making it a fundamental component in enhancing the performance of LLMs on reasoning tasks.\n\nThe BoN sampling is a simple yet effective method that can be fully parallelized to enhance performance. Increasing N guarantees improved results during training when a ground-truth checker is available and generally leads to better performance at inference time, provided that the reward model is sufficiently accurate. However, a key limitation"}, {"title": "3.2. Majority Voting", "content": "Similar to BoN sampling, majority voting, also known as self-consistency, provides an alternative approach for aggregating N different responses (Wang et al., 2023). As the name suggests, this method involves generating N responses, counting the frequency of each unique answer, and selecting the most frequently occurring response as the final output. This approach leverages the inherent redundancy in multiple generations to improve robustness and reliability, making it particularly useful for tasks requiring high confidence in correctness.\n\nVanilla majority voting offers the advantage of aggregating responses efficiently without relying on a reward model. Additionally, it can be extended to a weighted version, where weights are assigned based on PRM scores or confidence estimations of the generated answers (Wang et al., 2023). While majority voting benefits from not requiring a highly accurate reward model, it faces challenges in identifying equivalent answers in complex reasoning tasks. For instance, in mathematical problems, expressions such as 3 and \u221a3 are equivalent but must be recognized as such to ensure correct vote counting. A common solution in mathematical domains involves using symbolic-based checkers to compare answer pairs and identify equivalences. However, this process can be computationally expensive, particularly when N responses exhibit high diversity, requiring up to O(N2) comparisons. In some cases, this comparison overhead can be as time-consuming as the GPU inference itself, posing a significant bottleneck in large-scale reasoning tasks."}, {"title": "4. Iterative Deepening Sampling", "content": "In this paper, we explore a setting where tree-search-based strategies are combined with sequential revision. Given a fixed computational budget, an important challenge is determining how much additional budget should be allocated to refining a given prefix x0 that the model has already sampled. Efficient budget allocation is crucial, as any saved resources can be redirected to increasing the number of N in Best-of-N sampling or deepening tree search, ultimately improving overall performance. Understanding this trade-off is key to optimizing both search efficiency and model output quality.\n\nMore specifically, in the setting where sequential revision"}, {"title": "Theorem 4.1.", "content": "Suppose the final answer obtained through ID-Sampling needs a budget of L. Then the total number of budget used is no more than \u03b3+1L.\n\nProof Sketch Note that the budget for each iteration of generation is a geometric sequence with a common ratio of y, the theorem is a direct conclusion by using a summarization of the geometric sequence.\n\nIt is important to note that Iterative Deepening (ID) sampling does not impose any assumptions or constraints on the model's inherent self-correction or self-evaluation capabilities. In some cases, the model may naturally generate a response that already includes a trigger sentence before an explicit manual insertion. We observe that the built-in reasoning capabilities of recent state-of-the-art models, such as OpenAI-01 (Jaech et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025), significantly impact the effectiveness of ID-sampling. To better understand these effects, we conduct a comprehensive study using DeepSeek-R1, which we present in the experimental section.\n\nIn this paper, we instantiate the proposed algorithm in a setting where N responses are sampled in parallel and later aggregated-a common framework used in Best-of-N sampling and majority voting. Since response generation is independent and typically performed in parallel, the computational cost primarily depends on the total number of generated responses N and their respective lengths. To manage"}, {"title": "5. Experiments", "content": "5.1. Experiment Setup\n\nIn our experiments, we evaluate Iterative Deepening (ID) sampling on a set of mathematical problems. Given the strong problem-solving capabilities of modern models, which inherently possess self-evaluation and self-correction mechanisms, we focus on more challenging benchmarks and omit relatively easier datasets such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). Instead, we directly assess performance on competition-level and Olympiad-level benchmarks, including MATH-500 (Lightman et al., 2023) and AIME-24. Specifically, MATH-500 is a curated subset of 500 problems from the original MATH dataset, designed to evaluate process reward models trained for mathematical reasoning. Meanwhile, AIME (American Invitational Mathematics Examination) is a highly competitive exam aimed at identifying top-performing high school students in the U.S. The AIME-24 dataset comprises 30 problems sourced from the 2024 AIME I and II exams, providing a rigorous benchmark for assessing reasoning capabilities under competition-level constraints. For both datasets, answers are evaluated using symbolic-based checkers to ensure that all mathematically equivalent responses are recognized as correct.\n\nFor our experiments, we primarily use LLAMA-3.1-1B-Instruct (Dubey et al., 2024) and Phi-4 (Abdin et al., 2024) before transitioning to the latest DeepSeek-R1-Distill-Qwen-7B (DeepSeek-AI et al., 2025). This allows us to investigate how the strong built-in self-correction capabilities of recently released R1 models influence the performance of"}, {"title": "5.2. Results", "content": "Before evaluating the pass rate, we first analyze the runtime overhead of ID-sampling. We observe that ID-sampling requires approximately 1.6\u20131.9\u00d7 the runtime of a baseline approach without the trigger sentence. Given that this additional computational cost could instead be allocated to generating more responses and selecting the best one, we present our results in terms of an equivalent N. Specifically, if the original results correspond to Bo8, we report them as equivalent N = 16, as ID-sampling consistently completes within twice the runtime of the original method.\n\n5.2.1. \u039c\u0391TH-500\n\nThe MATH-500 dataset is a curated subset of the MATH dataset, selected to retain its more challenging problems. Originally used by OpenAI to train process reward models"}, {"title": "Ablation Study", "content": "Given the significant performance gains of ID-sampling on AIME, we conducted an ablation study to analyze the impact of the scaling factor y on ID-sampling. We present the pass rate results in Figures 5 and 6, and report the relative inference time for each setting in Table 2.\n\nOur find that adjusting y substantially affects both performance and computational cost. The worst-performing setting was y = 2.5, which failed to provide a stable and significant improvement over vanilla sampling. The best performance was achieved at y = 1.2, where self-evaluation and self-correction were triggered frequently. We also tested smaller values \u03b3 = 0.8, 1.0), but these settings were computationally infeasible, failing to complete within 10x the runtime of vanilla sampling, and were thus omitted.\n\nInterestingly, we observed a non-convex relationship between y and performance, with the second-best setting being \u03b3 = 2.0. This non-convex behavior complicates hyperparameter selection. From our perspective, although increasing the number of responses does not provide a clear scaling advantage in the current setting, runtime efficiency remains a critical factor. Notably, selecting y = 1.2 nearly doubles the runtime compared to y = 2.0. Given this trade-off, we adopt y = 2.0 as the default setting, as it balances inference speed and output quality."}, {"title": "6. Discussions", "content": "Due to computational constraints and the sudden release of the DeepSeek-R1 series, which introduced new phenomena that deserve investigation, our study primarily focuses on ID-sampling without integrating it into tree-search algorithms. However, we emphasize that our proposed framework can be directly extended to methods such as beam search and Monte Carlo Tree Search (MCTS). In these cases, the budget in Algorithm 1 can be directly interpreted as the computational budget in MCTS or the number of iterations in beam search. One challenge in this extension is that self-correction mechanisms reduce the reliance on early model outputs being correct, distinguishing the need for a new generation of PRMs from previously released ones. However, future researchers who develop or have access to more accurate PRMs that explicitly model the self-correction process can leverage our ID-sampling framework to enhance the reasoning capabilities of fixed models.\n\nBeyond combining ID-sampling with tree-search algorithms, several promising research directions could further enhance performance. However, these avenues remain largely unexplored due to the substantial computational resources required. First of all, in our study, we employ a fixed trigger sentence-\"Wait! Maybe I made some mistakes! I need to rethink from scratch.\" -as introduced by Zhao et al. (2024). However, recent findings from DeepSeek-R1 suggest that self-evaluation and self-correction can emerge spontaneously without explicit manual prompting, implying that trigger sentences can be highly flexible. Moreover, DeepSeek-R1 models demonstrate a tendency to self-correct locally, often requiring only minimal prompts such as a single word (\"Wait!\")-followed by context-specific revisions. This suggests a significant opportunity: modifying the trigger sentence could substantially impact both the computational efficiency and the pass rate performance of ID-sampling. In our preliminary experiments, we tested a minimal trigger sentence\u2014\u201cWait!\u201d\u2014but found that it was insufficient to consistently induce self-correction, often introducing noise rather than meaningful refinements. As reasoning models like o1 continue to evolve, future research could provide deeper insights into optimizing trigger sentences-both in form and function-to maximize the effectiveness of self-reflection mechanisms. A second promising research direction is exploring alternative frequencies for inserting trigger sentences beyond the current geometric sequence approach. While ID-sampling provides a key advantage-offering a theoretical guarantee of bounded additional computational cost compared to one-shot generation-it remains possible that alternative designs, even without strong theoretical guarantees, could yield superior empirical performance. However, the design space for such strategies is vast, making exhaustive exploration infeasible. We believe that our proposed algorithm establishes"}, {"title": "7. Conclusions", "content": "In this paper, we introduce Iterative Deepening (ID) Sampling, a simple yet effective sampling algorithm designed to adaptively allocate computational budget for sequential revisions in large language models (LLMs). We demonstrate that ID-sampling effectively enhances inference-time performance by improving the pass rate on challenging mathematical reasoning tasks across various models. Our results indicate that while current models exhibit strong self-correction capabilities, they still have significant room for improvement in determining when to trigger self-correction themselves without human intervention."}]}