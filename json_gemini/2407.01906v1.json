{"title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models", "authors": ["Zihan Wang", "Deli Chen", "Damai Dai", "Runxin Xu", "Zhuoshu Li", "Y. Wu"], "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large Language Models (LLMs) with constrained resources. Although there have been various PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture LLMs is still underexplored. In this work, we study the PEFT method for LLMs with the Mixture-of-Experts (MoE) architecture and the contents of this work are mainly threefold: (1) We investigate the dispersion degree of the activated experts in customized tasks, and found that the routing distribution for a specific task tends to be highly concentrated, while the distribution of activated experts varies significantly across different tasks. (2) We propose Expert-Specialized Fine-Tuning, or ESFT, which tunes the experts most relevant to downstream tasks while freezing the other experts and modules; experimental results demonstrate that our method not only improves the tuning efficiency, but also matches or even surpasses the performance of full-parameter fine-tuning. (3) We further analyze the impact of the MoE architecture on expert-specialized fine-tuning. We find that MoE models with finer-grained experts are more advantageous in selecting the combination of experts that are most relevant to downstream tasks, thereby enhancing both the training efficiency and effectiveness.", "sections": [{"title": "1 Introduction", "content": "As the parameter scale of large language models (LLMs) continues to increase (Meta, 2024; Mistral, 2024a; DeepSeek, 2024; Qwen, 2024), parameter-efficient fine-tuning (PEFT) methods (Han et al., 2024) are becoming increasingly important in adapting pre-trained LLMs to downstream customization tasks. However, existing works on PEFT like low-rank adaptation (LoRA) and P-Tuning (Hu et al., 2021; Liu et al., 2021) have primarily focused on dense-architecture LLMs, with"}, {"title": "2 Related Work", "content": "research on sparse-architecture LLMs still being markedly insufficient.\nIn this work, we focus on exploring PEFT techniques within the Mixture-of-Experts (MoE) LLMs (Mistral, 2024b; Databricks, 2024), as introduced in \u00a73.1. Unlike dense models where all tasks are handled by the same parameters, in the MoE architecture, different tasks are processed by distinct activated experts (Lepikhin et al., 2021; Fedus et al., 2021). Observations indicate that task specialization in expert systems is the key to the MoE LLM performance (Dai et al., 2024). We further illustrate such specialization in \u00a73.2 that experts activated by the same task's data are concentrated, while those for different tasks vary significantly, suggesting MoE models use specialized expert combinations to handle different tasks.\nMotivated by this, we propose Expert-Specialized Fine-Tuning (ESFT), as illustrated in \u00a73.3. ESFT only tunes the experts with the highest affinity to the task, while freezing the parameters of other experts and modules.\nThe primary advantages of ESFT lie in two aspects: (1) Maintaining Expert Specialization: ESFT prevents the decrement of specialization in full-parameter fine-tuning, where experts not adept at the task also update their parameters. Experimental results in \u00a75.1 show that ESFT can achieve aligned or even superior performance in downstream tasks compared to full-parameter fine-tuning, and better maintains performance in general tasks. (2) Saving Computation Resources: ESFT only trains the parameters of the selected experts, which effectively reduces the storage of up to 90% and training time up to 30% compared to full-parameter fine-tuning, as shown in \u00a75.2.\nBesides, we delve deeper into the working mechanism of the ESFT method. We analyze the expert selection process in \u00a76.1 and demonstrate how ESFT leverages specialized experts effectively, as selecting 5-15% experts can achieve promising per-"}, {"title": "2.1 Parameter-efficient fine-tuning for dense architectural LLMs", "content": "The goal of parameter-efficient fine-tuning (Han et al., 2024) is to efficiently customize LLMS for downstream tasks, while existing studies primarily focus on dense architectural LLMs. PEFT methods for dense models can generally be categorized into three approaches: (1) Adding new parameters: methods of this kind fix the existing model parameters and fine-tune the model on a small number of newly added parameters. Adapter (Houlsby et al., 2019; Pfeiffer et al., 2020; He et al., 2021; Wang et al., 2022) and Soft Prompt (Li and Liang, 2021; Liu et al., 2021; Zhang et al., 2023b; Lester et al., 2021) are two typical representatives of this category of methods. (2) Selecting existing parameters: methods of this type fine-tune a limited part of existing parameters, while keeping the majority of the other parameters fixed. Based on whether the trainable parameter space is continuous, these methods can generally be divided into structured training (Guo et al., 2020; Gheini et al., 2021; He et al., 2023; Vucetic et al., 2022) and unstructured training (Liao et al., 2023; Ansell et al., 2021; Sung et al., 2021; Xu et al., 2021). (3) Applying low-rank adaptation: LoRA (Hu et al., 2021; Fomenko et al., 2024) is a widely-used PEFT method, which decomposes the origin weight matrices into low-rank components. Subsequent works (Zhang et al., 2023a; Ding et al., 2023; Lin et al., 2024; Liu et al., 2023) have introduced numerous improvements to the original LORA method. However, the study of PEFT in sparse models is still scarce. In this work, we select and tune part of the experts based on their downstream task affinity, which is a unique selection dimension exclusive to the sparse MoE architecture."}, {"title": "2.2 Coarse- and Fine-grained MoE LLMs", "content": "Compared to dense LLMs (e.g., LLaMA series, Meta, 2023b,a), MoE LLMs (e.g., Mixtral series, Mistral, 2024a,b) can increase model size while saving training and inference costs. Based on the granularity of experts, existing large MoE models can generally be divided into two categories: coarse- and fine-grained expert LLMs. Most existing MoE LLMs (Lepikhin et al., 2021; Fedus et al., 2021; Roller et al., 2021; Dai et al., 2022; Shen et al., 2024) have coarse-grained experts where the number of experts is very limited. For example, 2 out of 8 experts are activated for Mixtral MoE series (Mistral, 2024a,b) and Grok-V1 (\u03a7\u0391\u0399, 2024). As a result, a single expert has to learn complicated patterns from different domain tasks simultaneously. To address this issue, DeepSeek MoE (Dai et al., 2024) has introduced fine-grained expert segmentation. In the DeepSeek-V2 (DeepSeek, 2024), there are as many as 162 experts, with 8 active experts (8 out of 66 experts are activated for the DeepSeek-V2-Lite). The fine-grained division of experts ensures a high degree of specialization among the experts. Moreover, the specialized expert system enables the selection of experts that are most relevant to the task for efficient tuning."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Preliminaries: Mixture-of-Experts for Transformers", "content": "Mixture-of-Experts (MoE) for Transformers replace Feed-Forward Networks (FFNs) with MoE layers. Each MoE layer consists of multiple experts structurally identical to a FFN. Tokens are assigned to and processed by a subset of the most relevant experts based on their affinity scores, ensuring computational efficiency in MoE layers. The output hidden state $h_t^l$ of the t-th token in the l-th MoE layer is computed as:\n$h_t^l = \\sum_{i=1}^{N} (g_{i,t} \\cdot FFN_i^l(u_t^{l-1})) + u_t^{l-1},$ (1)\n$g_{i,t} = \\begin{cases} s_{i,t}, \\quad s_{i,t} \\in TopK(\\lbrace s_{j,t}|1 \\leq j \\leq N \\rbrace, K),\n0, \\quad otherwise, \\end{cases}$ (2)\n$s_{i,t} = Softmax_i (u_t^{l-1}e_i^l),$ (3)\nwhere N denotes the total number of experts, $FFN_i^l(\\cdot)$ is the i-th expert FFN, $g_{i,t}$ denotes the gate"}, {"title": "3.2 Probing Task-Specific Expert Specialization in MoE Models", "content": "value for the i-th expert, $s_{i,t}$ denotes the token-to-expert affinity, $TopK(\\cdot, K)$ denotes the set comprising K highest affinity scores among those calculated for the t-th token and all N experts, and $e_i^l$ is the centroid of the i-th expert in the l-th layer.\nRecently, DeepSeekMoE (Dai et al., 2024) proposes enhancements to the MoE architecture through several techniques, including (1) Fine-grained segmentation, segmenting each expert into multiple smaller ones and keeping the same fraction of experts to process each token, allowing specialization in different knowledge types while maintaining the same computational cost. (2) Shared expert isolation, leveraging shared experts that process all tokens to capture common knowledge, reducing parameter redundancy and enhancing efficiency. The output of an MoE layer in DeepSeekMoE is:\n$h_t^l = FFN_S^l(u_t^{l-1}) + \\sum_{i=1}^{N} (g_{i,t}FFN_i^l(u_t^{l-1})) + u_t^{l-1},$\n$g_{i,t} = \\begin{cases} s_{i,t}, \\quad s_{i,t} \\in TopK(\\lbrace s_{j,t}|1 \\leq j \\leq N \\rbrace,K-K_s),\n0, \\quad otherwise, \\end{cases}$ (5)\nwhere $K_s$ is the number of shared experts, $FFN_S^l$ and $FFN_i^l$ denote the shared and non-shared experts, respectively. Each expert is segmented into m ones, with N and K also multiplied by m times compared to the coarse-grained architecture.\nDespite the significant success of MoE LLMs, a clear understanding of the underlying mechanism remains elusive. We conduct probing experiments to understand how non-shared experts are utilized"}, {"title": "3.3 Expert-Specialized Fine-tuning (ESFT)", "content": "across various tasks. These tasks, as detailed in \u00a74.1, include general domains like math and code, as well as specialized domains like intent recognition, summarization, legal judgment prediction, and translation. These experiments reveal the expert specialization in MoE models in two aspects:\nExpert Routing is Concentrated in the Same Task We investigate the distribution of normalized gate values, i.e., the sum of all expert-token gate values for each expert, divided by the total across all experts. Figure 2 displays this distribution, where the experts are sorted by their normalized values from high to low. The figure shows that a small subset of experts handles the majority of gate values, indicating the model's and concentrated expert allocation for a specific task.\nActive Experts Vary Significantly across Tasks We investigate the joint distribution of experts across tasks. Figure 3 shows a heatmap of the shared Top-6 experts for two independent data samples per task averaged across layers. This indicates the degree of overlap of experts used within the same task or between different tasks. Off-diagonal values are near 0, and diagonal values are near 6, indicating that the same task uses similar experts, while different tasks use different sets.\nThe highly specialized expert system suggests that different experts can be optimized for specific tasks. Inspired by this, we propose Expert-Specialized Fine-Tuning (ESFT) for MoE LLM customization, which selectively fine-tunes the most relevant experts for downstream tasks to enhance computational efficiency and maintain expert specialization."}, {"title": "3 Methods", "content": ""}, {"title": "Data Sampling", "content": "Data Sampling We randomly sample a subset $D_s = \\lbrace (x_i, y_i) \\rbrace_{i=1}^{N_s}$ from the training data $D = \\lbrace (x_i, y_i) \\rbrace_{i=1}^N$ for expert selection, where $x_i$ and $y_i$ denote the input and label, respectively. Empirically, we find that a subset of 32 concatenated samples, each with a fixed length of L = 4096, is robust enough to select the most relevant experts for a task. We detail this claim in Appendix C."}, {"title": "Expert Relevance Score", "content": "Expert Relevance Score We propose two methods to calculate the relevance of an expert to a task based on its affinity to the sample tokens, defined as average gate score and token selection ratio, respectively. Both methods assess each expert's relevance to downstream tasks and can be chosen based on task-specific experimental performance.\nAverage Gate Score (ESFT-Gate) This score"}, {"title": "Expert Selection and Fine-tuning", "content": "calculates the average affinity of expert $e_i$ to all tokens in the sampled data. It is defined as:\n$g_i = \\frac{1}{N_s} \\sum_{j=1}^{N_s} \\frac{1}{L_j} \\sum_{k=1}^{L_j} g_{i,k}$ (6)\nwhere $L_j$ is the length of the input sequence $x_j$ in the sampled data $D_s$.\nToken Selection Ratio (ESFT-Token) This score calculates the ratio of tokens for which expert $e_i$ is selected. It is defined as:\n$r_i = \\frac{1}{N_s} \\sum_{j=1}^{N_s} \\frac{1}{L_j} \\sum_{k=1}^{L_i} \\frac{1(g_{i,k} > 0)}{K}$ (7)\nwhere $1(g_{i,k} > 0)$ is an indicator that equals 1 if the gate score $g_{i,k}$ is positive, and 0 otherwise. K is the number of experts selected per token.\nFor each MoE layer l, we select a subset of experts to be fine-tuned based on their relevance scores. We define a threshold $p \\in (0, 1]$ as a hyperparameter controlling the proportion of total relevance scores to be included in the selected subset. For each layer l, we select a set of top-scored experts $E_l$ whose cumulative relevance score exceeds the threshold $p$, satisfying:\n$\\sum_{i \\in E_l} R_i \\geq p,$ (8)\nwhere $R_i^l$ is the relevance score (either $r_i^l$ or $g_i^l$) of expert i in layer l. During training and inference, tokens can be assigned to any expert. However, only the selected experts $E_l$ in each layer can be updated; other experts and modules remain frozen."}, {"title": "4 Experiment Setup", "content": "We evaluate our ESFT method on two common LLM customization scenarios: (1) improving the model's specific ability in a domain where the model may already have decent performance; (2) adapting the model to a possibly narrow but unfamiliar specialized task."}, {"title": "4.1.1 Tasks for Model Enhancement", "content": "We choose two domain-specific tasks, i.e., Math and Code, to evaluate how our method can enhance the model's existing abilities. The two domains are widely concerned in current LLM research and"}, {"title": "4.1 Main Evaluation", "content": ""}, {"title": "4.1.2 Tasks for Model Adaptation", "content": "We select four specialized tasks to evaluate how our method can facilitate language models to adapt to an unfamiliar downstream task, covering a diverse range of abilities that most models can excel at after training but not without training: (1) Text-to-JSON Intent Recognition in the BDCI-21 Smart HCI NLU Challenge\u00b9, which requires converting text instructions into JSON format for home appliances. (2) Text Summarization in the BDCI-21 Summarization Challenge2, which summarizes customer service call transcripts. (3) Legal judgment Prediction in the the BDCI-21 Law Event Prediction Challenge\u00b3, where the \"case description\" and \"judgment\" are repurposed as a legal judgment prediction task. (4) Low-resource Translation in the ChrEn dataset (Zhang et al., 2020), translating the minority Cherokee to English. Examples of the tasks are shown in Appendix A.\nTo measure model performance, for the text-to-JSON task, we calculate the exact match between model output and reference answer; for other tasks, we employ GPT-4 to score model output between 0 and 10 given reference answer4."}, {"title": "4.2 General Ability Evaluation", "content": "We select a broad range of benchmarks to evaluate the extent to which the models' general abilities are preserved after training on new tasks. These benchmarks include MMLU (Hendrycks et al., 2021b), TriviaQA (Joshi et al., 2017), HellaSwag (Zellers et al., 2019), ARC-Challenge (Clark et al., 2018),\",\n      \""}, {"title": "4.3 Backbone Model and Training Settings", "content": "IFEval (Zhou et al., 2023), CEval (Huang et al., 2023), and CLUEWSC (Xu et al., 2020), covering comprehensive model ability evaluations across various domains including natural language understanding, question answering, instruction following, and common sense reasoning.\nWe use the backbone architecture of DeepSeek-V2-Lite (DeepSeek, 2024) for all experiments. The model includes a fine-grained set of 66 experts for each transformer layer. This makes it uniquely suitable at the time of this study for our method, which benefits from expert specialization. We train the model on a carefully curated alignment dataset that excludes math and code data and take the resulting checkpoint as our vanilla model for subsequent experiments. This alignment phase can activate model ability across various domains while keeping Math/Code ability as elementary to better verify the performance gains of our method in these two fields.\nWe adopt two baselines: Full-Parameter Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA, Hu et al., 2021). For LoRA, we add low-rank matrices to all parameters for training except token embeddings and the language modeling head. We maintain a 1:1 ratio for task-specific data and alignment data for all methods, which we find is highly effective in preserving general abilities obtained from the alignment phase for FFT and LoRA. However, for our ESFT method, not adopting this data mixing strategy may even better maintain general ability. We detail this in Appendix F. All experiments are done on the HFAI clusters with 2 nodes of 8x Nvidia A100 PCIe GPUs.\nFor hyperparameter settings, all methods use a batch size of 32 and a sequence length of 4096 for training. For every task, we set the maximum steps of training to 500, and evaluate the model every 100 steps. The learning rates are set to 3e-5, 1e-4, and le-5 for FFT, LoRA, and ESFT, respectively, based on a hyperparameter search in {1e-5, 3e-5, 1e-4, 3e-4}. The LoRA rank is set to 8 and scaling is set to 2, following Hu et al. (2021). The threshold p is set to 0.1 for ESFT-Gate and 0.2 for ESFT-Token, respectively. \u00a76.2 shows how we determine the threshold for ESFT."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Benchmark Performance Results", "content": "The results in Table 1 and Table 2 demonstrate several conclusions. All methods can improve model performance in customization tasks compared to the vanilla model, while they may cause a performance decrease in general tasks. Generally, the performance increase is higher in model adaptation tasks than in model enhancement tasks.\nFor customization ability evaluation, ESFT surpasses LoRA significantly and is competitive with FFT. As shown in Table 1, ESFT-Token and ESFT-Gate achieve near-best results in model enhancement tasks like Math, and ESFT-Gate achieves the best performance in the Humaneval task. ESFT also excels in model adaptation tasks, with ESFT-Gate achieving near-best performance in 3 tasks out of 4. Notably, ESFT-Gate's average of 50.2 is competitive compared to FFT's 51.0, slightly better than ESFT-Token's 49.4, and significantly surpasses LoRA's 44.9. This demonstrates that finding task-relevant experts can efficiently adapt the model for efficient customization.\nFor general ability evaluation, ESFT consistently outperforms FFT and LoRA by showing less performance degradation. As illustrated in Table 2, ESFT-token performs better than ESFT-gate, with average scores of 61.5 and 60.6, respectively. The results demonstrate a wide range of retention"}, {"title": "5.2 Computational Efficiency Results", "content": "in tasks such as TriviaQA and IFEval, surpassing FFT's 58.8 and LoRA's 59.1. Both methods retain performance better than LoRA and FFT, highlighting their effectiveness in maintaining general task performance7. Analyses in \u00a76.3 indicate that such degradation on general tasks for FFT and LoRA may result from training shared parameters.\nThe results in Figure 6 demonstrates that ESFT exhibits several advantages in terms of training time and storage space requirements:\nTraining Time The average training time for ESFT-Token and ESFT-Gate is 19.8 minutes and 20.9 minutes, respectively. The FFT method takes significantly longer at 28.5 minutes. Although LORA achieves a shorter training time of 16.5 minutes, our methods are relatively close.\nStorage Space The average storage space of parameters trained is 2.57 GB for ESFT-Token and 3.20 GB for ESFT-Gate, while FFT demands a substantial 28.6 GB. Although LoRA requires less storage, ESFT performs significantly better than LORA in downstream task performance."}, {"title": "6 Analysis", "content": "In this section, we investigate the expert selection process of ESFT in \u00a76.1, and demonstrate the performance of ESFT and LoRA under different computational constraints in \u00a76.2. We analyze the effects of training shared and non-shared parameters in \u00a76.3, and conduct ablation studies in \u00a76.4 to verify the importance of our expert relevance scores and model structure of fine-grained experts."}, {"title": "6.1 ESFT Leverages Specialized Experts Effectively", "content": "We analyze the number of experts ESFT trains across tasks and layers to understand its expert selection process. Results are shown in Figure 4. From the results, we have several observations:\n(1) The average number of experts used per task across layers ranges from 2 to 15 out of 66, indicating ESFT can have 75%-95% fewer trainable parameters than FFT. (2) ESFT-Token generally employs fewer experts while better maintaining general performance, comparable to ESFT-Gate in tasks like Math, Intent, and Law. (3) The number of experts varies by task, with more specialized tasks like Math and Translation using fewer experts; our method's performances for these tasks exceed LoRA to the greatest extent, indicating that our method is especially suitable for more specialized tasks. (4) For most tasks, few experts are chosen in the middle layers, indicating that expert distribution is more concentrated in these layers."}, {"title": "6.2 ESFT Leverages Training Resources Efficiently", "content": "Both ESFT and LoRA have a training efficiency hyperparameter (p for ESFT and rank for LoRA). Increasing its value would raise computational resource usage and potentially improve performance. To understand how ESFT and LoRA perform under different efficiency settings, we evaluate benchmark performance on the Math task. We set rank <"}, {"title": "6.3 Selectively Training Non-Shared Parameters is the Key to ESFT", "content": "In our proposed ESFT method, we only fine-tune a subset of non-shared experts. This section provides detailed discussions of several variants of our method that may also train shared parameters. The variables are based on:\nWhether all non-shared experts or a task-relevant subset of them (we use the Token Selection Ratio and set p=0.2) are trained.\nWhether shared experts are trained.\nWhether other parameters, including gates, attention layers, and embeddings, are trained.\nThe results are shown in Table 3. We report average trainable parameters across all tasks, performance of specialized and general abilities, and their average. Detailed numbers for all benchmarks are shown in Appendix D. From the results, we can draw several conclusions:\nSpecialized performance increases as trainable parameters increase. The rank of trainable parameters from 450M to 15.7B highly aligns with the rank of specialized ability from 47.4 to 51.0. This suggests that increasing trainable parameters is effective in enhancing specialized performance.\nGeneral performance decreases as trainable shared parameters increase. Whether relevant"}, {"title": "6.4 Analysis of Key Modules in ESFT", "content": "In this section, we analyze and demonstrate that the effectiveness of our method lies in two modules: (1) our proposed expert relevance score functions and (2) the fine-grained expert segmentation of the MoE model architecture.\nExpert Relevance Score Function In this work, we propose Average Gate Score and Token Selection Ratio as expert relevance score functions to filter relevant experts for different tasks. To demonstrate their effectiveness, we replace the experts obtained from these functions with random experts while keeping the number of activated experts per layer the same. Results in Table 4 show that replacing relevant experts with random ones significantly decreases task performance, demonstrating the effectiveness of our proposed relevance scores.\nFine-Grained Expert Segmentation of the MoE Model We use the fine-grained segmented DeepSeek-V2 MoE model as our backbone. To prove the effectiveness of such fine-grained expert segmentation, we use greedy search (as detailed in Appendix B) to group experts, simulating coarse-grained expert segmentation. Experts in the same group share the same gate for each token, initialized by the average of the original gates' vector. We conduct experiments in the Math domain as an example. Results in Figure 7 show that as the group size increases, the performance of our method decreases more severely than FFT, while the training cost (i.e., the average number of experts used) becomes larger. These findings show that our method, as well as even more effective LLM customization, highly relies on a fine-grained segmented LLM architecture that has more specialized experts."}, {"title": "7 Conclusion", "content": "In this work, we study parameter-efficient fine-tuning methods for sparse large language models"}, {"title": "Limitations", "content": "with the Mixture of Experts (MoE) architecture. We first observe that tasks from different domains are handled by distinct combinations of experts. We then propose selecting the most relevant experts for downstream tasks using two metrics: average gate score and token selection ratio. Experimental results show that our method significantly reduces training costs while matching or surpassing full parameter fine-tuning results. Further analysis confirms that our method enhances the specialization of the expert system within the MoE architecture.\nFirstly, due to the limitation of the availability of other fine-grained MoE models, our method was only tested on the DeepSeek-V2-Lite MoE model. The conclusions drawn from this model require further validation when applied to other contexts. Besides, due to the lack of parameter-wise and structurally aligned MoE models with different expert granularities, we used a simulation approach by binding several groups of experts to compare coarse-grained and fine-grained MoE methods."}, {"title": "Appendix", "content": ""}, {"title": "A Examples for Specialized Tasks", "content": "Table 5 presents task examples as prompts and corresponding reference responses for each specialized task, including intent recognition, text summarization, legal judgment prediction, and low-resource translation."}, {"title": "B Strategy for Grouping Experts", "content": "To group experts together and simulate coarse-grained mixture-of-experts transformer models, we calculate expert similarity and group the experts by maximizing in-group similarities using a greedy search algorithm.\nWe sample data from the alignment dataset, containing 32 samples each with a sequence length of 4096, to calculate the similarity between experts. We initialize a co-occurrence matrix for all expert pairs as a zero matrix. For each pair of experts that occur simultaneously in a token's Top-6 expert choices, we increment their score by 1 in the matrix. After iterating through the dataset, we calculate the similarity between each pair of experts i and expert j using the cosine similarity between the vectors of row i and row j in the matrix.\nTo obtain an expert grouping strategy through greedy search, we calculate the average intra-group similarity (the average pairwise similarity of all experts within the group) for all possible K-expert groups (where K is the group size, either 2 or 4) from the 64 non-shared experts out of the 66 experts in each layer. We then select the K-expert group with the highest score. For the remaining unselected experts, we repeat this process until all experts are selected and grouped."}, {"title": "C Analysis of Expert Affinity Sample Size", "content": "To evaluate the amount of data needed to identify the most relevant experts for a task, we independently sample two sets of data from the training set for each of the six tasks and calculate the shared Top-6 experts between the two sets. The results are shown in Figure 8. As the sample size reaches $2^{17}$ (i.e., 32 samples with a sequence length of 4096), all tasks exhibit a high number of shared experts between the two samples. This indicates that the sample size is sufficiently large to select the top-relevant experts for the tasks."}, {"title": "D Detailed Results for Ablations on Training Shared Parameters", "content": "We present two tables that summarize the performance of various methods with different configurations for training shared or non-shared parameters. Table 6 shows results on general tasks, and Table 7 focuses on specialized tasks. The results indicate that training only task-relevant non-shared experts consistently maintains the best general task performance. Additionally, training task-relevant non-shared experts and all shared parameters yields the best specialized task performance, short of full-parameter fine-tuning."}, {"title": "E Qualitative Examples of the Expert Choices", "content": "We present qualitative examples of the amount that routed experts are trainable among all tokens for each task in Figure 9. Each subfigure demonstrates examples drawn from a task. Deeper tokens indicate more trainable experts across all 26 layers (top-6 experts per layer). The parameter p is set to 0.2 for the token selection ratio. Results show that our method, even handling only about 20% of expert choices, covers a wide range of key task-relevant words.\nFor example, in the Intent recognition task, the deepest tokens are \u201c\u610f\u56fe\u201d (Intent); in the legal judgment task, the deepest tokens include \u201c\u5a5a\u540e\u201d (Post-marriage), \u201c\u8981\u6c42\u201d(request), \u201c\u539f\u544a\u201d (plaintiff) and \u201c\u88ab\u544a\u201d (defendant); in the Math task, the deepest tokens are mainly numerical tokens such as \"3\", \"5\", \"6\u201d and \u201c7\u201d; in the Code task, the deep-"}, {"title": "F The Impact of Mixing Alignment Data for Training", "content": "We adopt a 1:1 ratio for downstream task data and alignment data for all methods during training to better maintain general task performance. This manual ratio is kept constant to avoid the significant additional costs associated with fine-tuning the ratio for each task.\nIn this section, we present performance comparisons across various methods and tasks to reveal the impact of mixing alignment data during training.\nTable 9 presents the performance on downstream specialized tasks, and Table 10 shows the performance on general tasks.\nThe results indicate that FFT and LORA benefit from the inclusion of alignment data, leading to"}, {"title": "G Evaluation Instructions for Specialized Tasks", "content": "improved performance in general tasks while only slightly decreasing performance in downstream tasks. Conversely, our ESFT method does not exhibit the same advantage. Specifically, mixing alignment data does not result in performance increases in either general or downstream tasks. The findings suggest that ESFT is inherently capable of adapting to downstream tasks without significant performance degradation in general tasks, even without added alignment data. This highlights the robustness and adaptability of ESFT in diverse task settings.\nTable 11 presents the detailed criteria to evaluate specialized tasks including text summarization, legal judgment prediction, and low-resource translation. Each task includes specific instructions on"}]}