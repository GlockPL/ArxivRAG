{"title": "Optimal Control Operator Perspective and a Neural Adaptive Spectral Method", "authors": ["Mingquan Feng", "Zhijie Chen", "Yixin Huang", "Yizhou Liu", "Junchi Yan"], "abstract": "Optimal control problems (OCPs) involve finding a control function for a dynamical system such that a cost functional is optimized. It is central to physical systems in both academia and industry. In this paper, we propose a novel instance-solution control operator perspective, which solves OCPs in a one-shot manner without direct dependence on the explicit expression of dynamics or iterative optimization processes. The control operator is implemented by a new neural operator architecture named Neural Adaptive Spectral Method (NASM), a generalization of classical spectral methods. We theoretically validate the perspective and architecture by presenting the approximation error bounds of NASM for the control operator. Experiments on synthetic environments and a real-world dataset verify the effectiveness and efficiency of our approach, including substantial speedup in running time, and high-quality in- and out-of-distribution generalization.", "sections": [{"title": "1 Introduction", "content": "Although control theory has been rooted in a model-based design and solving paradigm, the demands of model reusability and the opacity of complex dynamical systems call for a rapprochement of modern control theory, machine learning, and optimization. Recent years have witnessed the emerging trends of control theories with successful applications to engineering and scientific research, such as robotics (Krimsky and Collins 2020), aerospace technology (He et al. 2019), and economics and management (Lapin, Zhang, and Lapin 2019) etc.\nWe consider the well-established formulation of optimal control (Kirk 2004) in finite time horizon T = [to, tf]. Denote X and U as two vector-valued function sets, representing state functions and control functions respectively. Functions in X (resp. U) are defined over T and have their outputs in Rdx (resp. Rdu). State functions x \u2208 X and control functions u \u2208 U are governed by a differential equation. The optimal control problem (OCP) is targeted at finding a control function that minimizes the cost functional f (Kirk 2004; Lewis, Vrabie, and Syrmos 2012):\n\\begin{align}\n\\min _{u \\in U} \\quad & f(x, u) = \\int_{t_0}^{t_f} p(x(t), u(t)) dt + h(x(t_f))  \\\\ \ns.t. \\quad & \\dot{x}(t) = d(x(t), u(t)), \\\\ \n& x(t_0) = x_0,\n\\end{align}\nwhere d is the dynamics of differential equations; p evaluates the cost alongside the dynamics and h evaluates the cost at the termination state x(tf); and x0 is the initial state. We restrict our discussion to differential equation-governed optimal control problems, leaving the control problems in stochastic networks (Dai and Gluzman 2022), inventory management (Abdolazimi et al. 2021), etc. out of the scope of this paper. The analytic solution of Eq. 1 is usually unavailable, especially for complex dynamical systems. Thus, there has been a wealth of research towards accurate, efficient, and scalable numerical OCP solvers (Rao 2009) and neural network-based solvers (Kiumarsi et al. 2017). However, both classic and modern OCP solvers are facing challenges, especially in the big data era, as briefly discussed below.\n1) Opacity of Dynamical Systems. Existing works (B\u00f6hme and Frank 2017a; Effati and Pakdaman 2013; Jin et al. 2020) assume the dynamical systems a priori and exploit their explicit forms to ease the optimization. However, real-world dynamical systems can be unknown and hard to model. It raises serious challenges in data collection and system identification (Ghosh, Birrell, and De Angelis 2021), where special care is required for error reduction.\n2) Model Reusability. Model reusability is conceptually measured by the capability of utilizing historical data when facing an unprecedented problem instance. Since solving an individual instance of Eq. 1 from scratch is expensive, a reusable model that can be well adapted to new problems is welcomed for practical usage.\n3) Running Paradigm. Numerical optimal control solvers traditionally use iterative methods before picking the control solution, thus introducing a multiplicative term regarding the iteration in the running time complexity. This sheds light on the high cost of solving a single OC problem.\n4) Control Solution Continuity. Control functions are"}, {"title": "2 Methodology", "content": "In this section, we will present the instance-solution control operator perspective for solving OCPs. The input of operator G is an OCP instance i, and the output is the optimal control function u*, i.e. G : I \u2192 U. Then we propose the Neural Control Operator (NCO), a class of end-to-end OCP neural solvers that learn the underlying infinite-dimensional operator G via a neural operator. A novel neural operator architecture, the Neural Adaptive Spectral Method (NASM), is implemented for NCO method."}, {"title": "Instance-Solution Control Operator Perspective of OCP Solvers", "content": "This section presents a high-level instance-solution control operator perspective of OCPs. In this perspective, an OCP solver is an operator that maps problem instances (defined by cost functionals, dynamics, and initial conditions) into their solutions, i.e. the optimal controls. Denote the problem instance by i = (f,d, xinit) \u2208 I, with its optimal control solution u* \u2208 U. The operator is defined as G: I \u2192 U, a mapping from cost functional to optimal control.\nIn practice, the non-linear operator G can hardly have a closed-form expression. Therefore, various numerical"}, {"title": "Neural Adaptive Spectral Method", "content": "From the operator's perspective, constructing an OCP solver is equivalent to an operator-learning problem. We propose to learn the operator by neural network in a data-driven man-ner. Now we elaborate on a novel neural operator architecture named Neural Adaptive Spectral Method (NASM), inspired by the spectral method.\nThe spectral method assumes that the solution is the linear combination of p basis functions:\n\\begin{equation}\nN_{spec(i)}(t) = \\sum_{j=1}^p c_j(i) b_j(t),\n\\end{equation}\nwhere the basis functions {bj}p are chosen as orthogonal polynomials e.g. Fourier series (trigonometric polynomial) and Chebyshev polynomials. The coefficients {cj}p are derived from instance i by a numerical algorithm. The Spectral Neural Operator (SNO) (Fanaskov and Oseledets 2022) obtains the coefficients by a network.\nOur NASM model is based on a similar idea but with more flexible and adaptive components than SNO. The first difference is that summation \u2211 is extended to aggregation \u2295. The aggregation is defined as any mapping RP \u2192 R and can be implemented by either summation or another network. Secondly, the coefficient {cj}p is obtained from Coefficient Net, which is dependent on not only instance i but time index t as well, inspired by time-frequency analysis (Cohen 1995). It is a generalization and refinement of Nspec(Eq. 3), for the case when the characteristics of G are non-static. For example, Nspec with Fourier basis can only capture globally periodic functions, while the NNASM with the same basis can model both periodic and non-periodic functions. Lastly, the basis functions {bj}p are adaptive instead of fixed. The adaptiveness is realized by parameterizing basis functions by \u03b8, which is inferred from t, i also by the neural network.\n\\begin{equation}\nN_{NASM(i)}(t) = \\sum_{j=1}^p c_j(t, i) b_j(t; \\theta(t, i)).\n\\end{equation}\nAs a concrete example of adaptive basis functions, the adaptive Fourier series is obtained from the original basis by scaling and shifting, according to the parameter \u03b8. We limit the absolute value of elements of \u03b8 to avoid overlapping the adaptive range of basis functions. Following this principle, one can design adaptive versions for other basis function sets.\n\\begin{equation}\n\\{b_j(t;\\theta)\\} = \\{1, \\sin(\\pi[(1 + \\theta_1)t + \\theta_2]),\n\\cos(\\pi[(1 + \\theta_3)t + \\theta_4]), \\cdots\\}, |\\theta_k| \\leq 0.5, \\forall k.\n\\end{equation}"}, {"title": "3 Experiments", "content": "We give experiments on synthetic and real-world environments to evaluate our instance-solution operator framework."}, {"title": "Synthetic Control Systems", "content": "Control Systems and Data Generation We evaluate NASM on five representative optimal control systems by following the same protocol of (Jin et al. 2020), as summarized in Table 8. We postpone the rest systems to Appendix E, and only describe the details of the Quadrotor control here:\n\\begin{align}\n& \\dot{p} = v,  \\\\ \n& m\\dot{v} = \\begin{bmatrix} 0 \\\\ 0 \\\\ mg \\end{bmatrix} + R^T(q) \\begin{bmatrix} 0 \\\\ 0 \\\\ u_1 \\end{bmatrix}, \\\\ \n& \\dot{q} = \\Omega(\\omega) q, \\\\ \n& J \\dot{\\omega} = T_u - \\omega \\times J \\omega.\n\\end{align}\nThis system describes the dynamics of a helicopter with four rotors. The state x = [p,v,\u03c9T]T \u2208 R9 consists of parts: position p, velocity v, and angular velocity \u03c9. The control u \u2208 R4 is the thrusts of the four rotating propellers"}, {"title": "Extended Experiment Result on Quadrotor 1) More Variables.", "content": "In the previous experiments, only the cost func-"}, {"title": "Real-world Dataset of Planar Pushing", "content": "We present how to learn OC of a robot arm for pushing objects of varying shapes on various planar surfaces. We use the Pushing dataset (Yu et al. 2016), a challenging dataset consisting of noisy, real-world data produced by an ABB IRB 120 industrial robotic arm (Fig. 4, right part).\nThe robot arm is controlled to push objects starting from various contact positions and 9 angles (initial state), along different trajectories (target state functions), with 11 object shapes and 4 surface materials (dynamics). The control function is represented by the force exerted on to object. Left of Fig. 4 gives an overview of input variables.\nWe apply NASM to learn a mapping from a pushing OCP instance (represented by variables above) to the optimal control function. The input now is no longer the parameters of cost functional f only, but parameters and representations of (f, d, xinit). The encoder is realized by different techniques for different inputs, such as Savitzky-Golay smoothing (Savitzky and Golay 1964) and down-sampling for target trajectories, mean and standard value extraction for friction map, and CNN for shape images.\nWe extract training data from ID, validation, and test data from both ID/OOD. The ID/OOD is distinguished by different initial contact positions of the arm and object. The accuracy metric MAPE is now defined as ||\u00fb \u2013 u*||/||u*||."}, {"title": "4 Conclusion and Outlook", "content": "We have proposed an instance-solution operator perspective of OCPs, where the operator directly maps cost functionals to OC functions. We then present a neural operator NASM, with a theoretic guarantee on its approximation capability. Experiments show outstanding generalization ability and efficiency, in both ID and OOD settings. We envision the proposed model will be beneficial in solving numerous high-dimensional problems in the learning and control fields.\nWe currently do not specify the forms of problem instances (e.g. ODE/PDE-constrained OCP) or investigate sophisticated models for specific problems, which calls for careful designs and exploitation of the problem structures."}, {"title": "A Algorithms", "content": "The data generation algorithm is listed in Alg. 1."}, {"title": "B Related Works", "content": "OCP Solvers\nTraditional numerical solvers are well developed over the decades, which are learning-free and often involve tedious optimization iterations to find an optimal solution.\nDirect methods (B\u00f6hme and Frank 2017a) reformulate OCP as finite-dimensional nonlinear programming (NLP) (Bazaraa, Sherali, and Shetty 2013), and solve the problem by NLP algorithms, e.g. sequential quadratic programming (Boggs and Tolle 1995) and interior-point method (Mehrotra 1992). The reformulation essentially constructs surrogate models, where the state and control function (infinite dimension) is replaced by polynomial or piece-wise constant functions. The dynamics constraint is discretized into equality constraints. The direct methods optimize the surrogate models, thus the solution is not guaranteed to be optimal for the origin problem. Likewise, typical direct neural solvers (Chen, Shi, and Zhang 2018; Wang, Bhouri, and Perdikaris 2021; Hwang et al. 2022), termed as Two-Phase models, consist of two phases: 1) approximating the dynamics by a neural network (surrogate model); 2) solving the NLP via gradient descent, by differentiating through the network. The advantage of Two-Phase models against traditional direct methods is computational efficiency, especially in high-dimensional cases. However, the two-phase method is sensitive to distribution shift (see Fig. 1).\nIndirect methods (B\u00f6hme and Frank 2017b) are based on Pontryagin's Maximum Principle (PMP) (Pontryagin 1987). By PMP, indirect methods convert OCP (Eq. 1) into a boundary-value problem (BVP) (Lasota 1968), which is then solved by numerical methods such as shooting method (Bock and Plitt 1984), collocation method (Xiu and Hesthaven 2005), adjoint-based gradient descend (Effati and Pakdaman 2013; Jin et al. 2020). These numerical methods are sensitive to the initial guesses of the solution. Some indirect methods-based neural solvers approximate the finite-dimensional mapping from state x*(t) \u2208 Rdx to control u*(t) \u2208 Rdu (Cheng et al. 2020), or to co-state \u03bb*(t) \u2208 Rdx (Xie et al. 2018). The full trajectory of the control function is obtained by repeatedly applying the mapping and getting feedback from the system, and such sequential nature is the efficiency bottleneck. Another work (D'ambrosio et al. 2021) proposes to solve the BVP via a PINN, thus its trained network works only for one specific OCP instance.\nDynamic programming (DP) is an alternative, based on Bellman's principle of optimality (Bellman and Kalaba 1960). It offers a rule to divide a high-dimensional optimization problem with a long time horizon into smaller, easier-to-solve auxiliary optimization problems. Typical methods are Hamilton-Jacobi-Bellman (HJB) equation (Al-Tamimi, Lewis, and Abu-Khalaf 2008), differential dynamical programming (DDP) (Tassa, Mansard, and Todorov 2014), which assumes quadratic dynamics and value function, and iterative linear quadratic regulator (iLQR) (Li and Todorov 2004), which assumes linear dynamics and quadratic value function. Similar to dynamic programming, model predictive control (MPC) synthesizes the approximate control function via the repeated solution of finite overlapping horizons (Hewing et al. 2020). The main drawback of DP is the curse of dimensionality on the number and complexity of the auxiliary problem. MPC alleviates this problem at the expense of optimality. Yet fast implementation of MPC is still under exploration and remains open (Nubert et al. 2020).\nDifferential Equation Neural solvers\nA variety of networks have been developed to solve DE, including Physics-informed neural networks (PINNs) (Raissi, Perdikaris, and Karniadakis 2019), neural operators (Lu et al. 2021), hybrid models (Mathiesen, Yang, and Hu 2022), and frequency domain models (Li et al. 2020). We will briefly introduce the first two models for their close relevance to our work."}, {"title": "C Approximation Error Theorems", "content": "We give the estimation for the approximation error of the proposed model. The theoretic result guarantees that there exists a neural network instance of NASM architecture approximating the operator G to arbitrary error tolerance. Furthermore, the size and depth of such a network are upper-bounded. The error bound will be compared with the DON error bound derived in (Lanthaler, Mishra, and Karniadakis 2022). The technical line of our analysis is inspired by (Lanthaler, Mishra, and Karniadakis 2022) which provides error estimation for DON's three components: encoder, approximation, and reconstructor.\nDefinition and Decomposition of Errors\nFor fair and systematic comparison, we also decompose the architecture of NASM into three components. The encoder aims to convert the infinite-dimensional input to a finite vector, as introduced in the main text. The approximator of NASM is the coefficient network that produces the (approximated) coefficients. The reconstructor is the basis functions and the aggregation, that reconstructs the optimal control via the coefficients. In this way, the approximation error of NASM can also be decomposed into three parts: 1) encoder error, 2) approximator error, and 3) reconstructor error.\nFor ease of analysis, the error of the encoder is temporally not considered and is assumed to be zero. The reason is that the design of the encoder is more related to the problem setting instead of the neural operator itself. In our experiment, all neural operators use the same encoder, thus the error incurred by the encoder should be almost the same. Therefore, assuming the zero encoder error is a safe simplification. In our assumption, for a given encoder E, there exists an inverse mapping E\u22121, such that E\u22121 \u25e6 E = Id. The non-zero encoder error is analyzed in Appx D.\nFor a reconstructor R, its error ER is estimated by the mismatch between R and its approximate inverse mapping, projector P, weighted by push-forward measure G#\u00b5(u) := \u00b5(G\u22121(u)).\n\\begin{equation}\nE_R := \\int_{\\mathcal{U}} ||R \\circ P(u) - u||_{L^1(G_{\\#} \\mu)} d(G_{\\#} \\mu)(u) \\leqno{(6)}\n\\end{equation}\n\\begin{equation}\n P := \\underset{P}{argmin} \\ E_R , s.t. P \\circ R = Id.\n\\end{equation}\nIntuitively, such reconstructor error quantifies the information loss induced by R. An ideal R without any information loss should be invertible, i.e. its optimal inverse P is exactly R\u22121, thus we have ER = 0."}, {"title": "D Mathematical Preliminaries and Proofs", "content": "Preliminaries on Sobolev Space\nFor d \u2265 1, \u03a9 an open subset of Rd, p \u2208 [1; +\u221e] and s \u2208 N, the Sobolev space Ws,p (Rd) is defined by\n\\begin{equation}\nW^{s,p}(\\Omega) = \\{f \\in L^{p}(\\Omega) : \\forall |\\alpha| \\leq s, \\partial^{\\alpha} f \\in L^{p}(\\Omega)\\}\n\\end{equation}\nwhere \u03b1 = (\u03b11,...,\u03b1d), |\u03b1| = \u03b11 + ... + \u03b1d, and the derivatives \u2202\u03b1 f = \u2202x\u03b111... \u2202x\u03b1dd f are considered in the weak sense.\nWs,p(\u03a9) is a Banach space if its norm is defined as (Sobolev norm):\n\\begin{equation}\n||f||_{W^{s,p}} = \\sum_{|\\alpha|\\leq s} ||\\partial^{\\alpha} f||_{L^p}\n\\end{equation}\nIn the special case p = 2, Ws,2(\u03a9) is denoted by H\u02da s(\u03a9), that is, a Hilbert space for the inner product\n\\begin{equation}\n<f, g>_{\\Omega} = \\sum_{|\\alpha|\\leq s} <\\partial^{\\alpha}f, \\partial^{\\alpha}g>_{L^2(\\Omega)} = \\sum_{|\\alpha|\\leq s} \\int_{\\Omega} \\partial^{\\alpha}f \\partial^{\\alpha}g d\\mu\n\\end{equation}\nError estimation of MLP and CNN\nThe fundamental neural modules of NASM are MLP and CNN, thus the error estimation of NASM entails the error estimation of those two modules. In this section, we cite and extend their error analysis from previous work.\nMLP Firstly, the error bound of an MLP approximating a mapping in vector spaces is well-studied in the deep learning theory. One of the existing works (G\u00fchring, Kutyniok, and Petersen 2020) derives the estimation based on the Sobolev regularity of the mapping. We cite the main result as the following lemma (notation modified for consistency):\nLemma 6 (Approximation Error of one-dimensional MLP (G\u00fchring, Kutyniok, and Petersen 2020)). Let m \u2208 N,s \u2208 N\u22652,1 \u2264 q \u2264 \u221e, M > 0, and 0 \u2264 \u03b7 \u2264 1, then there exists a constant C = C(m,s,q, M, \u03b7), with the following properties: For any function f with m-dimensional input and one-dimensional output in subsets of the Sobolev space Ws,q:\n\\begin{equation}\n||f||_{W^{s,q}} \\leq M,\n\\end{equation}\nand for any \u0454 \u2208 (0,1/2),there exists a ReLU MLP N such that:\n\\begin{equation}\n||N - f||_{W^{\\eta,q}} \\leq \\epsilon,\n\\end{equation}\nand:\n\\begin{equation}\nsize(N) \\leq C\\epsilon^{-m/(s-\\eta)} log(\\epsilon^{-s/(s-\\eta)}),\n\\end{equation}\n\\begin{equation}\ndepth(N) \\leq Clog(\\epsilon^{-s/(s-\\eta)}).\n\\end{equation}\nHowever, such error bounds of one-dimensional MLP can not be directly applied to an MLP with p-dimensional output. A p-dimensional MLP can not be represented by simply stacking p independent one-dimensional output networks {Nj}1\u2264j\u2264p and concatenating the outputs. The key difference lies in the parameter sharing of hidden layers. To fill the gap and estimate the error of \u00df, we design a special structure of MLP without parameter sharing, as explained below.\nGiven p independent one-dimensional output networks {Nj : Rm \u2192 R1}1\u2264j\u2264p, denote the weight matrix of the i-th layer of the Nj as Wij. The weight matrix of i-th layer of the p-dimensional MLP \u00df, denoted as W, can be constructed as:\nNN"}, {"title": "E Environment Settings", "content": "Pendulum\n\\begin{equation}\n\\min_u \\int_{t_0}^{t_f} c_a (x(t) - x_{goal})^2 + c_u u^2(t) dt\n\\end{equation}"}, {"title": "H Extended Experiments on OCP with Analytical Solutions", "content": "Brachistochrone\nThe Brachistochrone is the curve along which a massive point without initial speed must move frictionlessly in a uniform gravitational field g so that its travel time is the shortest among all curves y = u(x) connecting two fixed points (x1,y1), (x2,y2). Formally, it can be formulated as the following OCP:\n\\begin{equation}\nmin\n_ {u\\in C^{1} ([x_1,x_2])}\\ T = \\frac{1}{\\sqrt{2g}} \\int_{x1}^{x2} \\sqrt{\\frac{1+\\nu'(x)^2}{y_1 - \\nu(x)}} dx \\leqno{(17a)}\n\\end{equation}\ns.t.\n\\begin{equation}\n\\nu(x_1) = y_1,\\leqno{(17b)}\n\\end{equation}\n\\begin{equation}\n\\nu(x_2) = y_2.\\leqno{(17c)}\n\\end{equation}\nAnd the optimal solution is defined by a parametric equation:\n\\begin{equation}\nx(\\theta) = x_1 + k(\\theta - sin \\theta),\\\n\\end{equation}\n\\begin{equation}\n \\nu(\\theta) = y_1 - k(1 - cos \\theta),\n\\end{equation}\n\\begin{equation}\n \\theta \\in [0, \\Theta],\n\\end{equation}\nwhere the k, \u0398 are determined by the boundary condition x(\u0398) = x2, \u03bd(\u0398) = y2.\nWe want to examine the performance of the Instance-Solution operator using the analytical optimal solution. The x-coordinate of endpoints is fixed as x1 = 0, x2 = 2, discretized uniformly into 100 intervals. Then the OCP instance is uniquely determined by (y1, y2), which is defined as the input of neural operators. The in-distribution(ID) is yid ~ U(2, 3), yid ~ U(1, 2) and out-of-distribution (OOD) is yout ~ U(2.9, 3.8), yout ~ U(1.9, 2.8). The training data is still generated by the direct method(DM), not the analytical solution. The performance metric MAPE denotes the distance with the analytical solution.\nThe results are displayed in Table 15. The NASM backbone achieves the best ID MAPE and the second-best OOD MAPE, with the shortest running time. All neural operators have performance close to baseline solver DM, demonstrating the effectiveness of our Instance-Solution operator framework. It is interesting that the FNO backbone has a better OOD MAPE than DM, although it is trained by DM data. This result may indicate that the Instance-Solution operator can learn a more accurate solution from less accurate supervision data."}, {"title": "Zermelo's Navigation Problem", "content": "Zermelo's navigation problem searches for the optimal trajectory and the associated guidance of a boat traveling between two given points with minimal time. Suppose the speed of the boat relative to the water is a constant V. And the \u03b2(t) is the heading angle of the boat's axis relative to the horizontal axis, which is defined as the control function. Suppose u, v are the speed of currents along x, y directions. Then the OCP is formalized as:\n\\begin{equation}\nmin_{\\beta \\in C^{1} ([0,T])} T \\leqno{(18a)}\n\\end{equation}\ns.t.\n\\begin{equation}\nx'(t) = V cos \\beta(t) + u(t, x(t), y(t)),\\leqno{(18b)}\n\\end{equation}\n\\begin{equation}\ny'(t) = V sin \\beta(t) + v(t, x(t), y(t)),\\leqno{(18c)}\n\\end{equation}\n\\begin{equation}\nx(0) = x_1, y(0) = y_1,\\leqno{(18d)}\n\\end{equation}\n\\begin{equation}\nx(T) = x_2, y(T) = y_2.\\leqno{(18e)}\n\\end{equation}\nThe analytical solution derived from PMP is an ODE, named Zermelo's navigation formula:\n\\begin{equation}\n\\frac{d\\beta}{dt} = \\frac{\\partial u}{\\partial x} sin^2 \\beta + \\frac{\\partial u}{\\partial y} sin \\beta cos \\beta - (\\frac{\\partial u}{\\partial x} - \\frac{\\partial u}{\\partial y}) cos^2 \\beta \\leqno{(19)}\n\\end{equation}\nIn our experiment, we simplify u, v as linear function u(x,y) = Ax + By, v(x,y) = Cx + Dy, where A, B, C, D are parameters. And we fix the initial point x1 = 0, y1 = 0. Then an OCP instance is uniquely defined by a tuple (x2,y2, V, A, B, C, D), which is defined as the input of our Instance-Solution operator. For the ID and OOD, we first define a base tuple (1, 1, 2, 0, 0, 0, 0), then add with noises \u03b5in \u2208 U(0, 1) and \u03b5out \u2208 U(1, 2) for ID and OOD respectively. Again, all operators are trained with direct method(DM) solution data, and MAPE evaluates the distance toward the analytical solution.\nThe results are displayed in Table 16. The NASM backbone has the best performance in time, ID MAPE and OOD MAPE. Most neural operators have close ID performance to DM, supporting the feasibility of the Instance-Solution operator framework."}, {"title": "I Future Direction: Enforcing the Constraints", "content": "In the current setting of Instance-Solution operators, boundary conditions can not be exactly enforced, since the operators are set to be purely data-driven and physics-agnostic. Instead, the operators approximately satisfy the boundary conditions by learning from the reference solution. Such a physics-agnostic setting is designed (and reasonable) for scenarios where explicit expressions of dynamics and boundary conditions are unavailable (e.g. the Pusher experiments in Section 3.2). However, our Instance-Solution operator framework may also be applied to a physics-informed setting, by changing the network backbone to the Physics-informed neural operator (Wang, Wang, and Perdikaris 2021).\nAnd there are more important constraints in practice, such as the input constraints (e.g. maximum thrusts of quadrotor) and state constraints (e.g. quadrotor with obstacles. In our paper, they are formulated in the form of function space. Take the state x for example, we define x \u2208 X \u2282 L(T; Rd), where X is its function space, and L(T; Rd) is the full Lebesgue space. Then X = L(T; Rdx) represents the unconstrained states, and X \u2286 L(T; Rd) denotes constrained states. The constrained state and control can be naturally incorporated into the PMP (Bettiol and Bourdin 2021), thus they can be modeled in our Instance-Solution framework. Again, these constraints are satisfied approximately in our model.\nInspired by the recent works that use SMT solver (Gao, Kong, and Clarke 2013) to guarantee Lyapunov stability for unknown systems (Chang, Roohi, and Gao 2019; Zhou et al. 2022), we propose a possible approach to provably satisfy the OCP constraints. Given a trained neural operator, the domain of OCP instances, and constraints, we can easily establish a first-order logic formula over real numbers, and then check the formula via the SMT solver. The solver will either prove that the neural operator satisfies all constraints for all instances within the domain or generate some counterexamples that further enhance the training dataset. If it is the latter case, the operator will be re-trained on the enhanced dataset, until the SMT solver proves that it satisfies all constraints."}]}