{"title": "From Principles to Practice: A Deep Dive into AI Ethics and Regulations", "authors": ["Nan Sun", "Yuantian Miao", "Hao Jiang", "Ming Ding", "Jun Zhang"], "abstract": "In the rapidly evolving domain of Artificial Intelligence (AI), the complex interaction between innovation and regulation has become an emerging focus of our society. Despite tremendous advancements in AI's capabilities to excel in specific tasks and contribute to diverse sectors, establishing a high degree of trust in AI-generated outputs and decisions necessitates meticulous caution and continuous oversight. A broad spectrum of stakeholders, including governmental bodies, private sector corporations, academic institutions, and individuals, have launched significant initiatives. These efforts include developing ethical guidelines for AI and engaging in vibrant discussions on AI ethics, both among AI practitioners and within the broader society. This article thoroughly analyzes the ground-breaking AI regulatory framework proposed by the European Union. It delves into the fundamental ethical principles of safety, transparency, non-discrimination, traceability, and environmental sustainability for AI developments and deployments. Considering the technical efforts and strategies undertaken by academics and industry to uphold these principles, we explore the synergies and conflicts among the five ethical principles. Through this lens, work presents a forward-looking perspective on the future of AI regulations, advocating for a harmonized approach that safeguards societal values while encouraging technological advancement.", "sections": [{"title": "1 Introduction", "content": "In today's world, where Artificial Intelligence (AI) is swiftly reshaping numerous facets of our daily life, the demand for robust and efficient AI regulations has become more pronounced. Acknowledging this need, on October 30, 2023, U.S. President Joe Biden enacted an Executive Order, signifying a historical move towards tackling the complex issues raised by AI technologies [1]. This action highlights an increasing recognition of the profound influence of AI on aspects such as safety and security, privacy, fairness and civil rights, as well as innovation and competition, calling for a forward-thinking and preemptive strategy in regulatory measures regarding AI advancements.\nThe evolution of AI regulations represents a contemporary and dynamic narrative that has emerged significantly over recent decades. Initial apprehensions about ethics and data privacy have evolved into more defined guidelines and potential legislative frameworks, though the journey is far from complete. The chronicle of AI regulation is a modern and swiftly developing area, mirroring the rapid advancements in AI technology. The genesis of this regulatory trajectory can be traced back to the mid-20th century, marked notably by the introduction of the Fair Credit Reporting Act in 1970 in the United States [2]. The legislation safeguards data gathered by consumer reporting agencies, including credit bureaus, medical data firms, and tenant screening services. The Act prohibits sharing information in a consumer report with individuals or entities that do not have a designated purpose as outlined in the Act. This act represented an early significant step in the formalization of data protection. Another significant achievement is the General Data Protection Regulation (GDPR) [3], which is a thorough data privacy and security legislation enacted by the European Union (EU) in 2016. It stands as the most stringent data privacy and security law globally. While it was formulated and ratified by the EU, it places responsibilities on organizations worldwide as long as they interact with or gather data from individuals in the EU. The GDPR introduces a set of novel data privacy rights to grant individuals greater authority over the information they provide to organizations.\nAI regulation goes beyond the data component to encompass AI systems' design, development, deployment, and use. It is concerned not only with the data these systems process but also their decisions, the biases they may perpetuate, and the ethical implications of their interaction with humans and other systems. AI regulation aims to address the societal and ethical challenges posed by increasingly autonomous technologies. Regulation of AI varies from country to country, reflecting different approaches, priorities, and concerns. For example, in August 2023, the Australian Human Rights Commission (i.e., Commission) submitted a discussion document titled \"Promoting Responsible AI\" to the Department of Industry, Science, and Resources in Australia [4]. The Commission expresses particular concern about emerging issues, including privacy breaches, algorithmic discrimination, biases in automation, and the spread of misinformation and disinformation. Furthermore, the intricate interaction between AI, neurotechnology, the metaverse, and extended reality technologies adds complexity to this rapidly evolving field. The Commission has advised the government to initiate an assessment of regulatory deficiencies to ascertain the relevance of existing legislation to AI. In cases where gaps are identified, the associated legislation should be examined and updated to effectively address AI-related challenges.\nIn Europe, the EU AI Act, the first comprehensive regulation on AI, adopts a risk-based approach emphasizing safety, transparency, and environmental sustainability. It is expected to be fully implemented by 2026. In October 2023, the White House issued an Executive Order to ensure safe, secure, and trustworthy AI development in the United States, prioritizing standards for AI safety and civil rights. By April 2024, the U.S. introduced the American Privacy Rights Act, focusing on comprehensive data privacy. Concurrently, Australia responded to AI safety discussions by proposing a voluntary AI safety standard emphasising a risk-based framework for AI deployment. These initiatives reflect a growing international commitment to responsibly managing AI's societal impacts.\nIn the initial stages of research, Scherer [5] initiated a discussion on the feasibility and challenges of government AI regulation, suggesting paths for effective regulation. Concurrently, other researchers delved into AI's regulatory and ethical aspects in specific domains. For example, Pesapane et al. [6] explored these issues in medical services, while Reddy et al. [7] proposed a governance model for ethical and regulatory concerns in AI healthcare applications. Focusing on particular aspects of AI regulation, W\u00e4schle [8] conducted a systematic literature review on highly automated driving, emphasizing safety assessment methods for AI systems. Other reviews have concentrated on different aspects: reviews [9, 10] on AI transparency; works [11\u201313] on bias and discrimination in AI; and reviews [14\u201317] on AI's sustainability.\nAI and algorithm-driven decision-making are increasingly impacting our daily lives, being extensively used in critical sectors, such as healthcare, business, government, education, and justice. This shift towards an algorithmic society comes with many benefits and risks, as these systems can sometimes cause harm to users and society. Ensuring the safety, reliability, and trustworthiness of these systems is crucial. Trustworthy AI systems are those that are reliable, ethical, and transparent, enabling users to have confidence in the decisions made by these systems. Recent literature reviews on trustworthy AI [18\u201320], including work [20] focused on the elements of fairness, explainability, accountability, reliability, and acceptance to mitigate AI risks and enhance user and societal trust. On the other hand, Li et al. [18] provided specific recommendations for AI practitioners, covering the entire AI system lifecycle, from data collection, model development, and system deployment to ongoing monitoring and governance.\nIn April 2021, the European Commission proposed the initial AI regulatory framework for the European Union [21]. This framework classifies AI systems into various risk levels, with corresponding levels of regulation. The primary objective of this Act is to guarantee the safety, transparency, traceability, non-discrimination, and environmental sustainability of AI systems used in the EU. The oversight of AI systems should rely on human decision-making (i.e., autonomy) rather than full automation to avoid adverse consequences. Members of the European Parliament endorsed the regulation, which was reached in negotiations with member states in December 2023, with 523 votes in favour, 46 against and 49 abstentions [22].\nTo the best of our knowledge, there lacks a comprehensive survey that fully addresses the five fundamental principles laid out in the AI Act [21], i.e., safety, transparency, traceability, non-discrimination, and environmental sustainability, as they apply to AI models and systems. An analytical paper is needed to explore how regulations might balance the push for innovation with the need to mitigate risks such as unforeseen consequences or misuse of AI technology. In addition, a detailed review of existing AI regulations would enlighten policymakers, stakeholders, and the public and support more informed decisions in AI governance. Motivated by these considerations, we thoroughly examine AI regulation, exploring the interactions and tensions among its essential aspects. We also provide structured, actionable recommendations for AI professionals in academia and industry. The following is a summary of the main contributions of this paper.\n\u2022 We present and analyze the stipulations detailed in the AI Act, concentrating on the key areas covered by AI regulations, including safety, transparency, non-discrimination, traceability, and environmental sustainability within the realm of AI. These discussions are specifically rooted in the regulatory necessities and the foundational concepts introduced in the AI Act, providing a comprehensive overview of its scope and implications.\n\u2022 We discuss the synergies and conflicts among components of AI regulation through an analysis of current technical efforts focused on crafting AI systems that are safe, transparent, traceable, environmentally sustainable, and free from bias, respectively. This exploration aids in understanding how these elements interact and sometimes clash, guiding the development and deployment of AI that adheres to high ethical and regulatory standards."}, {"title": "2 Preliminaries and definitions", "content": "This section establishes the foundational concepts and definitions for this survey, beginning with a rationale for AI regulation to underscore its importance. We then introduce key concepts within the AI Act, emphasizing its focus on human-centric considerations."}, {"title": "2.1 The need for AI regulation", "content": "The journey of AI regulation is an evolving narrative, having gained significant momentum in recent years. Initially, the focus was primarily on ethical considerations and data privacy concerns. These foundational issues have gradually paved the way for more defined guidelines and, in some instances, the drafting of legislative proposals. However, despite these strides, there is still a considerable amount of progress to be made in this area. Before delving deeper into this topic, it's essential to outline the core reasons necessitating AI regulations, which range from safeguarding individual privacy and ensuring ethical AI use to mitigating potential societal risks and establishing accountability in AI development and application."}, {"title": "2.1.1 Ethical and societal reasons", "content": "The unbiasedness of the AI system depends on its training data and design decisions. Without proper regulations, these systems risk perpetuating existing societal biases, leading to unfair or discriminatory outcomes. This is particularly concerning in areas such as employment, lending, and criminal justice, where biased AI could reinforce existing inequalities. Regulations are necessary to ensure that AI systems are developed and deployed in a manner that is equitable and fair to all segments of society [23].\nThe increasing deployment of AI in critical sectors like healthcare, finance, and law enforcement raises important questions about accountability [24]. When an AI system makes a decision that negatively impacts individuals or communities, it is essential to have clear guidelines determining who is responsible \u2013 the developers, the users, or the AI itself. Regulations in this area would help clarify lines of responsibility and ensure that affected parties have recourse in the event of harm or injustice.\nAI's reliance on vast datasets, often comprising personal and sensitive information, heightens concerns about data privacy and security [25]. The potential for misuse or unauthorized access to this data poses significant risks to individuals' privacy rights. Regulations are needed to establish strict guidelines on data collection, use, storage, and sharing. These include ensuring that individuals are informed and consented to how their data is used and implementing robust security measures to protect data from breaches and leaks."}, {"title": "2.1.2 Technical reasons", "content": "In fields such as autonomous driving or medical diagnostics, where AI systems make decisions that can have life-or-death consequences, the importance of risk management cannot be overstated [27]. Regulations are critical in ensuring that these systems are designed, tested, and operated under stringent safety standards. These involve setting benchmarks for performance, establishing protocols for failure detection and response, and ensuring systems are resilient to various types of risks, including cyber threats, system malfunctions, and unexpected environmental conditions. Regulatory frameworks can guide the development and deployment of these systems to prevent accidents and mitigate potential harms.\nFrom a technical standpoint, AI regulation is essential for promoting interoperability and scalability. Ensuring AI systems can effectively communicate and work together across different platforms and applications is crucial for maximizing their utility and efficiency. Regulations could establish standards that enhance this interoperability. Additionally, scalability is vital for AI systems to handle increasing loads smoothly. Regulations could help set guidelines that ensure AI systems are designed to scale efficiently without disproportionate increases in resource demands.\nFurthermore, large AI models typically require substantial computational power, translating into significant energy consumption and consequent environmental impacts [28]. Regulations could play a critical role in pushing for developing more energy-efficient AI technologies, potentially reducing both the environmental footprint and the costs associated with high energy consumption."}, {"title": "2.1.3 Global and National Security Reasons", "content": "AI's capabilities in processing vast amounts of data make it a powerful tool for surveillance, espionage, and cyber warfare. This raises concerns about the protection of national interests and security [29]. A regulatory framework is necessary to govern the use of AI in these domains, ensuring that it does not compromise national security or infringe on citizens' rights and freedoms. Regulations can help define permissible uses of AI in surveillance and intelligence, establish safeguards against unauthorized data access, and protect against cyber threats that utilize AI technologies. This regulatory oversight is pivotal in maintaining the delicate balance between the use of AI for national security and the preservation of democratic values and individual rights.\nThe prospect of AI being used in autonomous weapons systems is a growing concern [30, 31]. These technologies can potentially revolutionize warfare, raising ethical and strategic questions about their deployment. Regulations are crucial in preventing the harmful use of AI in military applications, ensuring that such systems are developed and used in compliance with international humanitarian laws and ethical standards.\nBy establishing clear boundaries and oversight mechanisms, regulations can prevent an arms race in lethal autonomous weapons and ensure that AI is used to enhance, rather than undermine global security and peace."}, {"title": "2.1.4 Economic Reasons", "content": "AI technology has the potential to disrupt market dynamics significantly. Without proper regulatory oversight, there is a risk that large corporations with substantial resources could monopolize the AI sector. Such dominance could hinder innovation, as smaller companies and startups may find it challenging to compete on an unequal playing field. Regulations can foster a competitive market by ensuring fair access to AI technologies and preventing monopolistic practices. This encourages a diverse ecosystem of AI developers and service providers, fostering innovation and ensuring that the benefits of AI are not confined to a few dominant players.\nAs AI becomes more integrated into consumer products and services, it is vital to protect consumers from faulty or misleading AI applications. Regulations are crucial in setting standards for AI quality, reliability, and truthfulness in advertising [32]. This includes ensuring that AI-driven products meet certain performance benchmarks and that claims made about AI capabilities are accurate and not misleading. By doing so, regulations safeguard consumers from subpar or potentially harmful AI-driven products and services, fostering trust and confidence in the market."}, {"title": "2.2 Key concepts in the AI Act", "content": "The AI Act aims to guarantee that AI systems utilized in the EU uphold safety, transparency, traceability, non-discrimination, and environmental sustainability. It places a strong focus on human oversight to avert detrimental consequences. In this context, we explain these five crucial elements as outlined in the AI Act."}, {"title": "2.2.1 Safety", "content": "AI safety, a multifaceted and interdisciplinary domain, focuses on preventing adverse consequences that might arise from AI systems [33]. This field tackles not only ethical considerations, ensuring AI systems align with moral values and contribute positively, but also addresses technical challenges [34]. These challenges include monitoring and maintaining the reliability of AI systems to avert risks.\nIn the EU AI Act, safety refers primarily to the regulation and mitigation of risks associated with using AI systems, ensuring that these systems do not pose unacceptable risks to health, safety, and fundamental rights [35]. Consider the machinery industry as an example. High-risk AI systems here might involve AI-driven robots in manufacturing plants, such as an autonomous robotic arm assembling heavy machinery parts or an AI system managing critical manufacturing processes. The malfunction of these systems can cause severe production setbacks, expensive damages, or even endanger human workers [36]. To mitigate these risks, it's crucial to equip AI systems with robust safety features like fail-safes or emergency stops. They must undergo comprehensive testing and validation across different operational scenarios. Moreover, resilience against cyber threats or manipulation is vital to prevent potentially disastrous outcomes."}, {"title": "2.2.2 Transparency", "content": "In the context of the EU AI Act, transparency refers primarily to obligations imposed on AI system providers to ensure that the operation and output of AI systems are clear and understandable by those who deploy or interact with them. Transparency measures are designed to ensure that AI systems are used in a way that is understandable and respects the rights and freedoms of all individuals involved. This supports the overall goal of human-centric AI development and deployment within the EU.\nThe concept of transparency in AI regulation encompasses the level of openness and comprehensibility in how AI systems are developed, implemented, and managed [29]. For instance, AI systems identified as high-risk must accompany these systems with detailed documentation and instructions that disclose the system's capabilities, characteristics, and limitations, allowing users to understand and correctly apply the outputs generated by these systems [21].\nBeyond these operational aspects, transparency in AI also significantly intersects with the concepts of explainability[37]. This facet of transparency is about demystifying the internal workings of AI systems, providing clarity on how specific decisions are reached. It's a move towards making AI systems less of a \"black box\" and more of an open book, where users and stakeholders can understand and rationalize the logic behind AI-driven decisions. This level of transparency is not just a regulatory requirement but a foundation for building trust between users and AI technologies, ensuring that users can interact with these systems confidently and effectively, knowing the rationale behind their outputs and actions [38]."}, {"title": "2.2.3 Non-discrimination", "content": "The principle of non-discrimination in the realm of AI is centered on reducing the likelihood of biased decision-making by algorithms [39]. This involves careful attention to the design and the integrity of data sets used in creating AI systems, combined with commitments to rigorous testing, effective risk management, thorough documentation, and consistent human oversight throughout the entire lifecycle of these AI systems.\nIn the context of AI regulations, non-discrimination is a guiding principle that ensures AI technologies are developed and utilized to prevent biased and unfair outcomes or decisions, particularly those that might unjustly target or disadvantage different groups of people[21]. This aspect of AI regulation is crucial given the risk that AI systems might unintentionally sustain, magnify, or even introduce new forms of biases or discriminatory practices [40]. Such regulations aim to encourage the creation and application of AI technologies in a manner that upholds fairness and ethical standards, thus preventing the reinforcement of existing social disparities. For instance, as outlined in OpenAI's GPT-4 technical report, content considered harmful encompasses elements like \"hate speech, discriminatory language, incitements to violence, or content that is then used to either spread false narratives or to exploit an individual\" [41]. In response to this, GPT-4 was developed using a technique known as model-refusal. This approach, grounded in reinforcement learning, rewards the model during its training phase for actively declining to produce such harmful content."}, {"title": "2.2.4 Environmental sustainability", "content": "In AI regulation, the concept of environmental sustainability emphasizes the creation, implementation, and utilization of AI systems in ways that mitigate adverse environmental effects. This approach recognizes the ecological impact of AI technology, focusing on issues such as energy use, efficient resource management, and promoting ecological sustainability [15]. This includes safeguarding the environment and enhancing its quality, which also pertains to human health and safety [21].\nCrucial aspects of this environmental focus in AI regulation include promoting energy efficiency, particularly in AI systems with high computational demands like large data centers, which are significant electricity consumers [28]. Additionally, it encompasses the pursuit of sustainable development, efforts to decrease the carbon footprint of AI technologies, and the advancement of \u2018Green AI' [14]. This latter term refers to designing AI models that are not just high-performing but also optimized for minimal consumption of computational resources and energy. Moreover, it involves adopting eco-friendly practices, such as minimizing electronic waste and encouraging the recycling of AI hardware components."}, {"title": "2.2.5 Traceability", "content": "The principle of traceability within the framework of AI regulation encompasses the systematic capability to monitor, record, and chronicle the decision-making processes employed by AI systems during their entire operational lifespan [21]. This principle is crucial for ensuring that AI operations are transparent and accountable. For instance, an AI system should possess robust logging features that provide a traceability level suitable for its intended use, as per the guidelines outlined in the AI Act. Such traceability measures are vital for verifying the AI system's performance and compliance throughout its lifecycle [42].\nAdditionally, traceability involves closely tracking the various iterations of AI models, encompassing all modifications and updates that occur over time. This tracking is not just a record-keeping exercise but a critical tool for understanding how an AI model evolves [43]. It enables stakeholders to comprehend the changes made to the model, assess their impacts, and, if necessary, revert to prior versions when newer updates do not perform as expected or introduce unforeseen issues."}, {"title": "2.3 Human-centered AI Act", "content": "The AI Act emphasizes putting people first. Hence, we discuss integrating human values into the design, development, and deployment of ethical AI systems in this subsection. We map various stakeholders' interests and concerns to the ethical considerations across each stage of the AI lifecycle. This comprehensive alignment ensures that AI systems are developed and deployed in a manner that addresses ethical aspects thoroughly, with the overarching goal of benefiting individuals and society at large.\nWe categorize stakeholders into four main groups: enterprises, researchers and developers, regulators and policymakers, and users and customers. According to recent research, these stakeholders have distinct interests and responsibilities regarding ethical AI development and deployment. Enterprises are responding to both internal and external pressures to ensure their AI technologies are ethically deployed and do not cause harm. For instance, Microsoft conducted a Human Rights Impact Assessment (HRIA) for AI, and Google has established AI principles that reference human rights [44, 45]. These efforts aim to mitigate adverse impacts, ensure corporate social responsibility, and maintain public trust. Regulators and policymakers are responsible for aligning AI with human rights principles throughout each stage of the AI lifecycle. Researchers and developers, who are deeply involved in each lifecycle stage, ensure technical performance and efficiency while promoting responsible AI behavior. They also contribute to the discourse on AI and human rights by examining the social impacts of AI, developing ethical frameworks, and advocating for policies that prioritize human dignity [45]. Lastly, end-users and customers focus on the risks and harms associated with AI. They advocate for AI systems that are free from biases, transparent in their operations and decision-making processes, particularly concerning conflicts of interest, and safe and traceable, including maintaining data integrity and security [46].\nThe AI Act influences people by creating a comprehensive framework that addresses various aspects of AI development and deployment, all with the ultimate goal of protecting human safety, rights, and well-being while promoting beneficial AI technologies. It enhances safety by reducing risks of harm from AI applications, supports environmental sustainability to benefit both the environment and human well-being, ensures traceability for accountability, and increases transparency to foster trust, fairness, and informed decision-making.\nEthical concerns in the data preparation stage include ensuring data safety, obtaining informed consent, and addressing biases in data collection [48]. It is crucial to collect representative data that accurately reflects the diversity of the population to avoid discrimination and ensure fairness. This process must also ensure the data owners' safety and make the data collection process transparent and traceable for data providers. During model training, it is essential to build a robust AI against various threats to ensure its safety, while the ethical use of computational resources should be considered to minimize environmental impacts. Transparency and traceability in the deployment process of AI models, including integrating other components within the AI system, are crucial to ensuring user safety. During the inference stage, it is important to evaluate the model's fairness, environmental sustainability, and safety in real-world scenarios [48], and accordingly can opt to implement bias mitigation, cost-effective, and threats mitigation techniques."}, {"title": "3 How to design regulation-compliant systems: the synergies and conflicts", "content": "This section investigates how the five key principles of AI regulation interact, exploring both their capacity to work together and the conflicts that exist between them. By analyzing these dynamics, this section aims to provide a thorough understanding of the techniques that can help AI systems adhere to these principles. This understanding will serve as a guide for developing AI systems that are both ethically sound and legally compliant."}, {"title": "3.1 Safety", "content": "3.  1.   1 Overview\nEstablishing well-defined safety protocols for the development and implementation of AI in an ethical manner is crucial. Initially, we present a comprehensive overview of safety considerations in AI systems. This forms the foundation for a more detailed exploration of how these safety considerations interact and potentially conflict with the other four key principles outlined in the AI Act: traceability, transparency, environmental sustainability, and non-discrimination. As depicted in Figure 4, we identify potential attacks at each stage of the AI development pipeline. These attacks stem from the typical Tactics, Techniques, and Procedures (TTPs) employed by hackers. Following this, we outline defensive objectives for the establishment of secure and reliable AI systems.\n-Common TTPs: Tactics, Techniques, and Procedures (TTPs) are integral concepts in cybersecurity, utilized to categorize and understand attacker behaviors. They serve as tools for testing and assessing the robustness of an organization's threat detection capabilities. Breaking down these concepts: (1) Tactics refer to the overarching goals or 'why' behind an attack, explaining the motives for executing certain actions; (2) Techniques detail the \u2018how' aspect, outlining the methods adversaries employ to achieve their tactical objectives; (3) Procedures are the specific, detailed methods used to execute these techniques.\nThere's a concerted effort within the AI security community to compile and comprehend the TTPs that adversaries might deploy against AI systems. A notable contribution in this domain comes from MITRE [49], renowned for their MITRE ATT&CK TTP framework. They have extended their expertise to the AI field by publishing a comprehensive set of TTPs tailored for AI systems [50]. This work by MITRE is instrumental in aiding organizations to better understand and prepare for potential threats against their AI infrastructures, reflecting the growing importance of specialized TTPs in the evolving landscape of AI security.\n-Possible attacks for each stage in AI pipeline: Leveraging the knowledge in threat intelligence and AI system development, we have identified specific TTPs, leading to potential attacks that are most relevant and realistic for real-world adversaries. These potential attacks encompass a range of strategies, such as prompt attacks, extracting training data, backdoor attacks, adversarial attacks, data poisoning, exfiltration, and ethical hacking. The pipeline shown in Figure 4 encompasses stages such as data collection and preparation, model creation and assessment, and the deployment and updating of models. This structured approach aids in comprehending specific vulnerabilities and developing appropriate countermeasures for each phase of AI system development, taking into account the safety considerations emphasized in the AI Act. We've organized possible attacks in a structured table, correlating each attack type with its respective phase in the AI development pipeline. In terms of their mechanisms and objectives, several of these attacks can overlap. For example, training-time attacks could encompass methods like data poisoning and backdoor attacks. Adversarial examples can be a form of both inference-time attacks and exfiltration if used to extract model behavior or data. Training data extraction and privacy attacks may involve unauthorized access to sensitive data.\n-Defensive objectives: To enhance the safety of AI systems, we summarize four strategic objectives to defend against threat actors and attacks, inspired by considering the TTPs used by adversaries targeting various stages of the AI pipeline. The first objective involves evaluating the impact of attacks on both users and products, and determining methods to bolster cyber resilience against such incursions. The second objective focuses on examining the AI system's robustness, particularly in terms of detecting and thwarting potential attacks, as well as understanding how these attacks might circumvent existing safeguards. The third objective is to enhance the system's ability to detect threats. Finally, the fourth objective is to cultivate awareness among stakeholders. This is aimed at assisting AI system developers in recognizing risks and potential attacks at each stage of the AI pipeline, and in advocating for a risk-driven, well-informed approach to investing in security measures for AI systems for organizations."}, {"title": "3.1.2 Techniques related to safety and their relations to other aspects of the AI Act", "content": "Below", "Act": "transparency", "training": "Adversarial training", "37": "and further developed by Madry et al. [66", "67": ".", "68": [69], "robustness": "Certified adversarial robustness is crucial for enhancing AI system safety", "70": [71], "53": ".", "scenarios[53": ".", "sanitization": "Training data sanitization plays a crucial role in enhancing the safety of AI systems by mitigating the impact of poisoned samples. These methods are rooted in the understanding that poisoned samples often exhibit distinct characteristics compared to regular training samples. Data sanitization techniques are purposefully designed to cleanse the training dataset of these malicious samples before machine learning training takes place. By actively excluding poisoned samples from the training sample set", "72": ".", "73": ".", "74": "and clustering methods [75", "76": "further refine the dataset by identifying and removing anomalies", "attempts[77": "."}, {"training": "Robust training enhances the transparency and safety of AI systems by integrating techniques like ensemble model voting", "78": [79], "80": [81], "82": ".", "unlearning": "Machine unlearning represents a novel and distinctive approach aimed at mitigating privacy concerns in machine learning by allowing users to request the removal of their data from trained models", "forms": "exact unlearning", "100": [101], "102": [103], "104": ".", "privacy": "By enhancing privacy protections for sensitive data used in AI training, testing, and deployment, Differential Privacy (DP) contributes to the safety of AI systems [84", "86": ".", "110": "which has seen enhancements in recent times, including DP-FTRL [88", "89": ".", "87": "have derived tight bounds on the success of membership inference attacks, further strengthening the efficacy of DP in protecting against such attacks. However, it's important to note that DP does not provide complete protection against all types of attacks. Specifically, it does not offer guarantees against model extraction attacks, which aim to recover the underlying model itself rather than individual data points. DP is primarily designed to safeguard the privacy of the training data, not the model parameters"}]}