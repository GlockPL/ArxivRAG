{"title": "The Solution for Temporal Action Localisation Task of Perception Test Challenge\n2024", "authors": ["Yinan Han", "Qingyuan Jiang", "Hongming Mei", "Yang Yang", "Jinhui Tang"], "abstract": "This report presents our method for Temporal Action\nLocalisation (TAL), which focuses on identifying and clas-\nsifying actions within specific time intervals throughout a\nvideo sequence. We employ a data augmentation technique\nby expanding the training dataset using overlapping labels\nfrom the Something-SomethingV2 dataset, enhancing the\nmodel's ability to generalize across various action classes.\nFor feature extraction, we utilize state-of-the-art models, in-\ncluding UMT, VideoMAEv2 for video features, and BEATS\nand CAV-MAE for audio features. Our approach involves\ntraining both multimodal (video and audio) and unimodal\n(video only) models, followed by combining their predic-\ntions using the Weighted Box Fusion (WBF) method. This\nfusion strategy ensures robust action localisation. our over-\nall approach achieves a score of 0.5498, securing first place\nin the competition.", "sections": [{"title": "1. Introduction", "content": "In recent years, deep learning techniques have gained\nsignificant attention across numerous research fields [1-4].\nThe Perception Test Temporal Action Localisation Chal-\nlenge 2024 is part of the broader Perception Test Challenge,\naiming to comprehensively evaluate the perception and rea-\nsoning capabilities of multimodal video models [5]. Un-\nlike traditional benchmarks that focus primarily on compu-\ntational performance, this challenge emphasizes perceptual\ntasks by leveraging real-world video data that are intention-\nally designed, filmed, and annotated. The goal is to assess a\nwide range of skills, reasoning types, and modalities within\nmultimodal perception models.\nIn the Temporal Action Localis ation (TAL) task, the\nobjective is to develop a method that can accurately lo-\ncalize and classify actions occurring within untrimmed\nvideos according to a predefined set of classes. Each\naction is represented by start and end timestamps along\nwith its corresponding class label, as illustrated in Fig-\nure1. This task is critical for various applications, including\nvideo surveillance, content analysis, and human-computer\ninteraction. The dataset provided for this challenge is de-\nrived from the Perception Test, comprising high-resolution\nvideos (up to 35 seconds long, 30fps, and a maximum res-\nolution of 1080p). Each video contains multiple action seg-\nment annotations. To facilitate experimentation, both video\nand audio features are provided, along with detailed anno-\ntations for the training and validation phases.\nTo tackle this challenge, we experimented with various\nmodels such as UMT [6], Internal Video [7], and Video-\nMAE [8]. Ultimately, we selected UMT [6] and Video-\nMAEv2 [9] as our video feature extraction models due\nto their superior ability to capture temporal dynamics in\nvideo sequences. For audio feature extraction, we employed\nBEATS [10] and CAV-MAE [11], which effectively cap-\nture essential audio patterns and integrate audio-visual in-\nformation. Additionally, we augmented our training dataset\nby incorporating overlapping labels from the Something-\nSomethingV2 dataset a large-scale, crowd-sourced video\ndataset focusing on basic actions and interactions with ev-\neryday objects. By integrating these overlapping action\nclasses, we enriched our dataset and enhanced the model's\ngeneralization capabilities across different scenarios. Fi-\nnally, we fused the predictions from our models using the\nWeighted Box Fusion (WBF) [12] method, which combines\nthe strengths of each model's outputs. Through this com-\nprehensive approach, we achieved first place in the compe-\ntition."}, {"title": "2. Method", "content": "In this section, we describe the methodology employed\nto address the Temporal Action Localisation (TAL) task.\nOur approach leverages state-of-the-art models for both"}, {"title": "2.1. Dataset", "content": "One of the main challenges in TAL tasks is the availabil-\nity of sufficient training data. To enhance the generaliza-\ntion of our models, we augmented the competition dataset\nby incorporating overlapping labels from the Something-\nSomethingV2 dataset. Something-SomethingV2 is a large-\nscale, crowd-sourced video dataset focusing on basic inter-\nactions with everyday objects. We identified classes that\noverlap with the competition dataset and extracted corre-\nsponding samples to supplement our training set. This aug-\nmentation expanded the diversity of the training data and\nhelped our models better capture temporal dependencies in\nvarious actions."}, {"title": "2.2. Video Feature Extraction", "content": "UMT. Unified Multimodal Transformers (UMT) [6] is\ndesigned to address the dual tasks of video moment retrieval\nand highlight detection by leveraging multimodal learning\nand model flexibility. Unlike traditional methods that only\nconsider visual data, UMT [6] incorporates video, audio,\nand textual information, making it capable of handling dif-\nferent combinations and reliability levels of these modali-\nties. For example, when textual data is unavailable or unre-\nliable, UMT [6] can still perform effectively by focusing on\nthe remaining modalities. This flexibility allows UMT [6]\nto adapt to natural variations in the data without requiring\nmultiple specialized models. UMT [6] has demonstrated its\neffectiveness on several benchmark datasets, outperforming\nstate-of-the-art approaches in both video moment retrieval\nand highlight detection tasks.\nVideoMAEv2. VideoMAEv2 [9] is an enhanced version\nof VideoMAE [8], designed to efficiently pre-train large-\nscale video models using a dual masking strategy. This\napproach involves applying separate masks to both the en-\ncoder and the decoder, reducing computational costs while\nmaintaining strong performance. The encoder processes a\nsmall subset of visible tokens, while the decoder operates\non a combination of latent features and additional masked\ntokens. The final loss is computed based on the reconstruc-\ntion of masked pixels:\nl= $\\frac{1}{(1-p_e)(1-p_d)N} \\sum ||t_i -I_i||_1$ (1)\nwhere $M_e$ and $M_d$ are the masking maps for the encoder\nand decoder, respectively, and I represents the recon-\nstructed pixels. This dual masking approach improves train-\ning efficiency, enabling VideoMAEv2 [9] to scale to billion-\nparameter models.\nAdditionally, VideoMAEv2 [9] employs a progressive\ntraining pipeline. It starts with unsupervised pre-training on\na large-scale, unlabeled hybrid dataset, followed by super-\nvised post-pre-training on a labeled dataset to incorporate\nsemantic knowledge, and finally fine-tuning on specific tar-\nget tasks. This design allows VideoMAEv2 [9] to achieve\nstate-of-the-art results in video action recognition and tem-\nporal action detection, pushing the performance limits of\nlarge video transformers."}, {"title": "2.3. Audio Feature Extraction", "content": "BEATS. BEATs [10] is an iterative self-supervised learn-\ning framework for audio. It introduces an acoustic tok-\nenizer that generates discrete, semantic-rich labels for gen-\neral audio pre-training. Through iterative training, the tok-\nenizer and the audio model improve each other. BEATS [10]\nutilizes a mask and label prediction strategy, enabling it\nto capture high-level semantic information in audio. The\nmodel achieves state-of-the-art performance across various\naudio classification benchmarks, including AudioSet-2M\nand ESC-50, surpassing previous models in accuracy and\nefficiency.\nCAV-MAE. CAV-MAE [11] extends the traditional\nMasked Autoencoder (MAE) framework to a multimodal\nsetting, integrating both contrastive learning and masked\ndata modeling. By jointly encoding audio and visual in-\nformation, CAV-MAE [11] learns coordinated audio-visual\nrepresentations. Contrastive learning is applied between the\ntwo modalities, while masked data modeling is used for re-\nconstructing the missing portions of the inputs. The total\nloss is computed as:\n$L_{CAV-MAE} = L_r + \\lambda_c . L_c$,\n(2)\nwhere $L_r$ is the reconstruction loss and $L_c$ is the contrastive\nloss. This combination allows CAV-MAE to outperform\nprevious models on both audio-visual classification and re-\ntrieval tasks, achieving state-of-the-art results on datasets\nlike VGGSound and AudioSet."}, {"title": "2.4. Prediction Fusion using WBF", "content": "We trained two versions of the model:\nMultimodal Model: This model utilizes both video and\naudio features to predict the start and end times of actions,"}, {"title": "3. Experiments", "content": "Datasets. We evaluated our method on the Percep-\ntion Test Temporal Action Localisation dataset, which con-\nsists of high-resolution videos with multiple annotated ac-\ntion segments. To augment the training data, we incor-\nporated overlapping action classes from the Something-\nSomethingV2 dataset, enriching the diversity and quantity\nof training samples.\nMetric. The evaluation metric is mean Average Preci-\nsion (mAP). It measures the average precision across all\nclasses and IoU thresholds. The mAP is calculated by eval-\nuating the precision and recall at various IoU thresholds be-\ntween the predicted action segments and the ground truth\nannotations.\nComparative Experiments. We compared our method\nwith several state-of-the-art models:\nTable 1 shows that our proposed method outperforms ex-\nisting models across all IoU thresholds.\nAblation Studies. To assess the contribution of each\ncomponent, we conducted ablation studies on Table 2. The"}, {"title": "4. Conclusion", "content": "This report presents a comprehensive evaluation of tem-\nporal action localisation (TAL) methods, highlighting the\nselection and application of advanced models such as\nUMT [6] and VideoMAEv2 [9] as the core video feature\nextraction algorithms, and BEATS [10], CAV-MAE [11] for\naudio feature extraction.. By leveraging diverse datasets,\nincluding the Perception Test dataset augmented with over-\nlapping labels from Something-SomethingV2, and employ-\ning advanced techniques such as the incorporation of both\nmultimodal and unimodal inputs and prediction fusion us-\ning WBF [12], the method demonstrated significant im-\nprovements in action localisation performance. Despite the\nlimited impact of certain methods, the overall approach ef-\nfectively addressed the challenges of temporal action local-\nisation, leading to a top ranking in the final test phase."}]}