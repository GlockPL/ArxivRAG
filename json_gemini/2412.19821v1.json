{"title": "Nanoscaling Floating-Point (NxFP): NanoMantissa, Adaptive Microexponents, and Code Recycling for Direct-Cast Compression of Large Language Models", "authors": ["Yun-Chen Lo", "Gu-Yeon Wei", "David Brooks"], "abstract": "As cutting-edge large language models (LLMs)\ncontinue to transform various industries, their\nfast-growing model size and sequence length\nhave led to memory traffic and capacity chal-\nlenges. Recently, AMD, Arm, Intel, Meta,\nMicrosoft, NVIDIA, and Qualcomm have pro-\nposed a Microscaling standard (Mx), which aug-\nments block floating-point with microexponents\nto achieve promising perplexity-to-footprint trade-\noffs. However, the Microscaling suffers from sig-\nnificant perplexity degradation on modern LLMs\nwith less than six bits.\nThis paper profiles modern LLMs and identifies\nthree main challenges of low-bit Microscaling\nformat, i.e., inaccurate tracking of outliers, va-\ncant quantization levels, and wasted binary code.\nIn response, Nanoscaling (NxFP) proposes three\ntechniques, i.e., NanoMantissa, Adaptive Microex-\nponent, and Code Recycling to enable better accu-\nracy and smaller memory footprint than state-of-\nthe-art MxFP. Experimental results on direct-cast\ninference across various modern LLMs demon-\nstrate that our proposed methods outperform state-of-the-art MxFP by up to 0.64 in perplexity and\nby up to 30% in accuracy on MMLU benchmarks.\nFurthermore, NxFP reduces memory footprint by\nup to 16% while achieving comparable perplexity\nas MxFP.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have successfully enabled\nmore and more ground-breaking applications, e.g., general\nassistant (Touvron et al., 2023a; Brown et al., 2020), au-\ntomatic code generation (Li et al., 2023), and humanoid\nrobots (Macdonald et al., 2024). Following the promising\nneural scaling law (Hoffmann et al., 2022; Kaplan et al.,\n2020), next-generation LLMs continue to scale up in pa-\nrameter counts, training computation, training data, and"}, {"title": "2. Understanding Microscaling (Mx) Standard", "content": "Open Compute Project (OCP) has defined four stan-\ndards in the Microscaling family, i.e., MxINT8, MxFP8\n(E5M2/E4M3), MxFP6 (E3M2/E2M3), and MxFP4\n(E2M1) (Rouhani et al., 2023a), where the suffix denotes\nthe element format and the bitwidth. In addition, MxFP8\nand MxFP6 have defined multiple optimized configurations\nto optimize for different scenarios. For example, MxFP8\nwith 4-bit exponent and 3-bit mantissa (E4M3) prioritizes\nprecision and is more suitable for inference.\nThe high-level ideas for Microscaling formats are summa-\nrized as follows. First, the MxINT8 is fundamentally BFP\nwhich replaces sign magnitude with two's complement. Sec-\nond, MxFP augments conventional BFP with microexpo-\nnents. Hence, MxFP incorporates two exponent fields per\nvector: one shared exponent and multiple microexponents.\nThe shared exponent is responsible for vector-wise scaling,\nand the microexponents serve as element-wise scaling.\nFigure 2 visualizes quantizing a FP16 vector to a MxFP4\nvector (E2M1). First, given an FP16 vector, we obtain the\nmantissa binary and align each mantissa with the most sig-\nnificant exponent value. Second, we configure the shared\nexponent as the largest exponent value in the vector (vertical\narrow). Third, the microexponents serve as exponent off-\nsets to track each element's leading one (horizontal arrow).\nFinally, the mantissa slices the trailing bit (blue rectangle)\nwhile assuming the leading one exists (grey rectangle). In\nsum, the MxFP format balances vector-wise dynamic range,\nelement-wise dynamic range, and element-wise precision."}, {"title": "3. Challenges of Low-bit MxFP", "content": "There are three challenges of low-bit MxFP, i.e., inaccurate\ntracking of largest values, vacant quantization level, and\nwasted code. To understand these challenges, we profile\nthe distribution of weight values after its being scaled by\n$E_{shared}$ for each 32-wide weight vector on five modern"}, {"title": "4. Nanoscaling Floating-Point (NxFP)", "content": "We present three enabling techniques of the NxFP format,\ni.e., NanoMantissa, Adaptive Microexponent, and Code\nRecycling. We will provide a comprehensive overview of\nthese techniques, offering a high-level understanding, and\nthen delve into specific examples to illustrate them."}, {"title": "4.1. NanoMantissa (NM)", "content": "To help low-bit MxFP accurately track the largest value in\neach block, we propose incorporating a 2-bit mantissa field\nin the shared scaling factor. Conceptually, the 2-bit shared\nNanoMantissa helps low-bit MxFP formats boost their pre-\ncision and track the full-precision values more accurately."}, {"title": "4.2. Adaptive Microexponent (AM)", "content": "To mitigate the challenge of vacant quantization level, we\npropose Adaptive Microexponent, which only allocates mi-\ncroexponent fields for vectors with scattered distribution. In\nother word, we successfully mitigate the vacant quantization\nlevel issue by choosing a suitable format for each block.\nFigure 5(a) presents that different blocks have distinct dis-\ntributions, where the first block is more clustered in val-\nues than the second block. We quantize each block using\nMxFP4 and BFP4 to showcase that vacant quantization level\nissue appeals when there is a mismatch between the low-\nbit format and the per-block distribution. When quantizes\nvalue-clustered block 1 (B1), BFP4 outperforms MxFP4 be-\ncause the latter suffer from vacant quantization level around\n5 (as denoted by the red cross in MxFP4). When quantizing\nvalue-scattered block 2 (B2), MxFP4 outperforms BFP4\nbecause BFP4 suffers from a more significant quantization\nerror around 0 (as denoted by the red cross in BFP4)."}, {"title": "4.3. Code Recycling (CR)", "content": "We propose code recycling to mitigate the wasted code issue\nin sign-magnitude format. Our approach involves remap-\nping the binary code representing -0 in sign-magnitude num-\nbers to a useful value, which improves the quantization\nerror.\nThe blue arrows in Figure 6 present the process of sweep-\ning for different remapped values. A good remapped value\nshould simultaneously improve the perplexity and have low\nimplementation overheads. We set the remapped value to be\n\u00d7 $V_{smallest}$ based on our empirical profiling of three rep-\nresentative LLMs (Llama2, Llama3, and Llama3.1). During\nthe dequantization, we can right-shift the smallest number\nby one bit to obtain the remapped value during decoding."}, {"title": "5. Quantization Algorithm", "content": "Similar to MxFP standard, NxFP format is not limited to\njust one quantization algorithm. For example, we present\nin below an MSE-based quantization algorithms, which\nmaximizes the accuracy.\nMSE-based quantization algorithm: Algorithm 1 presents\nthe process of quantizing a full-precision vector into a low-\nbit Nanoscaling vector. First, we find the largest absolute\nvalue $V_{max}$ from the full-precision vector $V_{fp}$. Second, we\nfind the largest exponent $E_{max}$. Third, we normalize the\nlargest absolute value and round to get a 2-bit NanoMan-\ntissa for scaling. Fourth, we scale the original vector and\nquantize it into MxFP and BFP formats. Then, we evaluate\nthe quantization error (MSE) to determine which format is\nbetter and set the format indicator bit fmt. The above quan-\ntization is repeated with NanoMantissa set to zero to get the\nMSE-optimized binary. Please note that the quantization\nfunction contains code recycling, which maps -0 to be half\nof the smallest quantization level."}, {"title": "6. On-the-fly Nanoscaling Dequantization on\nOff-the-shelf Hardware", "content": "On-the-fly dequantization is an intriguing technique for de-\nploying Microscaling format on off-the-shelf AI accelera-\ntors. This deployment method reduces memory footprint\nwhile maintaining great hardware compatibility. Specifi-\ncally, we allow data to be quantized into Mx format for\nstorage in off-chip memory and then decompressed for ex-\necution on FP16/BF16 cores. For instance, Qualcomm's\nAI-100 accelerator supports MxFP6 on-the-fly dequantiza-\ntion to achieve approximately 2\u00d7 speedup over FP16 (Vaid-"}, {"title": "7. Experiments", "content": "7.1. Settings\nQuantization. This work focuses on direct-cast inference,\nwhich directly reflects the efficiency of different number\nsystems. Please note that our proposed number system is\northogonal and can be integrated with many post-training\nquantization techniques. Further, direct-cast quantization\nminimizes the deployment friction and avoids the risk of\noverfitting to the calibration set.\nWe present the results of 1) quantizing only the weights,\nand 2) quantizing the weights and KV cache because they\ndominate the memory footprint (Hooper et al., 2024). We\nused a block size of 32 unless explicitly specified, where\n32 is officially defined in Microscaling standard (Rouhani\net al., 2023a).\nModels. We benchmark our method on numerous main-\nstream large language models, including Llama3 (8B) (Meta,\n2024), Llama3.1 (8B) (Dubey et al., 2024), Phi3 (4B) (Ab-\ndin et al., 2024), Gemma2 (Riviere et al., 2024), Llama2\n(7B) (Touvron et al., 2023b), Llama2 (13B) (Touvron et al.,\n2023b), and Mistral (7B) (Jiang et al., 2023). These rep-\nresentative open-source models are trained using different"}, {"title": "7.2. Quantization Error Analysis", "content": "Figure 8 shows the quantization error (MSE) improvement\nof NxFP4 over MxFP4 using the direct-cast quantization,\nwhere we can isolate the benefits of different techniques. In\nbrief, NxFP4 reduces the quantization error by up to 45%.\nThe quantization error improvement is similar across all\nmodels.\nWe further isolate the contribution of each technique below.\nFirst, NanoMantissa helps reduce the quantization error by\nup to 26%. Then, the adaptive microexponent further re-\nduces the quantization error by 14%. Finally, code recycling\nhelps reduce the quantization error by 4.7%."}, {"title": "7.3. Perplexity Degradation Analysis", "content": "Table 1 analyzes the perplexity degradation of weight-only\nquantization on modern LLMs. We mark the best perplexity\nwith bold. We compare MxFP against BFP and MxFP,\nwhere NxFP consistently achieves the best perplexity under\n4~6 bits. We can observe that three proposed techniques\nsuccessfully improves the perplexity, especially at ultra-low\nbits (i.e., 4-bit).\nSince the trend is similar across different models, let us\nutilize Llama3 as the main illustrative example. The orig-\ninal FP16 perplexity is 6.14. Under 6-bit weight, MxFP\n(6.18) and BFP (6.21) degrade the perplexity by 0.4 and\n0.7, respectively. On the other hand, NxFP (6.17) achieves\nthe smallest perplexity degradation (0.03). Similarly, BFP5\n(6.38) and MxFP5 (6.31) degrade perplexity by 0.24 and\n0.17, respectively. NxFP5 (6.25), again, achieves the small-\nest perplexity degradation of 0.11 Finally, using 4-bit weight,\nthe perplexity degradation of BFP, MxFP, and Nanoscaling\nare 0.93, 0.81, and 0.43, respectively."}, {"title": "7.4. Perplexity-to-Footprint Trade-offs", "content": "Figure 9 presents the perplexity-to-footprint trade-offs of\nNanoscaling (NxFP), Microscaling (MxFP), and BFP on\nfive modern LLMs. NxFP consistently achieves the best\nperplexity-to-footprint trade-off. We highlight the design\npoints with \u22640.1 perplexity degradation compared to FP16\nin green. Since we perform experiments on weight-only\nquantization and quantizing the weight and KV cache, the\nx-axis showcases the total footprint of both weights and KV\ncache."}, {"title": "7.5. Accuracy Degradation Analysis on MMLU\nReasoning", "content": "Figure 10 summarizes the accuracy degradation of various\nLLMs (Llama3, Llama3.1, Gemma2, Llama2, and Mistral)\non MMLU-SocialScience.\nThere are three takeaways. First, NxFP significantly miti-\ngates the accuracy degradation on low bitwidths, i.e., 4-bit\nand 3-bit. Specifically, NxFP improves the accuracy by up\nto 30.2% on MMLU-SocialScience compared to MxFP and\nBFP. Second, the smaller language models (e.g., Gemma2-\n2B) are generally less quantizable than larger language mod-\nels (Llama2-13B). Third, Mistral is surprisingly the most\nquantizable model in the 7B category."}, {"title": "7.7. Different Block Size", "content": "Figure 12 compares the perplexity-to-footprint trade-offs\nunder different block sizes (BS) on Llama3 (8B). The first\nobservation is that NxFP4 consistently outperforms MxFP4\nand BFP4 regardless of the block size. The second observa-\ntion is that MxFP4 outperforms BFP4 when the block size\nis large, which is because the microexponents can ensure\nenough element-wise dynamic range when the values are\nmuch more scattered."}, {"title": "7.6. Sweeping the Remapped Value of Code Recycling", "content": "We sweep the perplexity of different remapped value on\nMxFP and BFP. Although sweeping through all remapped\nvalue is possible, we consider the implementation overheads\nand only sweeps through the middle-points in between the\noriginal quantization levels.\nFigure 11(a) shows the perplexity of different remapped\nvalues on code-recycled MxFP4 for Llama3-8B. The dotted\nline shows the baseline perplexity of MxFP4. We show that\nhalf of the smallest quantization number and the middle-\npoint between the 1st and 2nd-largest-quantization-value im-\nproves the perplexity the most. Figure 11(b) also shows the\nperplexity of different remapped values on code-recycled\nBFP4. The dotted line shows the baseline perplexity of\nBFP4. We show that half of the smallest quantization num-\nber improves the perplexity the most."}, {"title": "8. Conclusions", "content": "This paper profiles modern LLMs and identifies three chal-\nlenges of low-bit Microscaling standards. In response, we\npropose a Nanoscaling (NxFP) format, which contains three\ntechniques, i.e., NanoMantissa, Adaptive Microexponent,\nand Code Recycling.\nExperimental results across various modern LLMs demon-\nstrate that our proposed methods outperform state-of-the-art\nMxFP by up to 0.64 in perplexity, by up to 30% in MMLU\naccuracy, and 16% smaller memory footprint."}, {"title": "9. Impact Statement", "content": "This paper presents work whose goal is to advance the field\nof Efficient Machine Learning. There are many positive\nsocietal consequences of our work, which can make AI\nmore accessible."}]}