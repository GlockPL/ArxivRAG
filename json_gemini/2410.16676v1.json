{"title": "Improving Causal Reasoning in Large Language Models: A Survey", "authors": ["Siheng Xiong", "Delin Chen", "Qingyang Wu", "Longxuan Yu", "Qingzhen Liu", "Dawei Li", "Zhikai Chen", "Xiaoze Liu", "Liangming Pan"], "abstract": "Causal reasoning (CR) is a crucial aspect\nof intelligence, essential for problem-solving,\ndecision-making, and understanding the world.\nWhile large language models (LLMs) can gen-\nerate rationales for their outputs, their ability to\nreliably perform causal reasoning remains un-\ncertain, often falling short in tasks requiring a\ndeeper understanding of causality. In this survey,\nwe provide a comprehensive review of research\naimed at enhancing LLMs for causal reasoning.\nWe categorize existing methods based on the\nrole of LLMs: either as reasoning engines or as\nhelpers providing knowledge or data to tradi-\ntional CR methods, followed by a detailed dis-\ncussion of the methodologies in each category.\nWe then evaluate the performance of LLMS\non various causal reasoning tasks, providing\nkey findings and in-depth analysis. Finally, we\nprovide insights from current studies and high-\nlight promising directions for future research.\nWe aim for this work to serve as a comprehen-\nsive resource, fostering further advancements\nin causal reasoning with LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (OpenAI et al.,\n2023; Dubey et al., 2024) have achieved signifi-\ncant success across various domains. Causal rea-\nsoning (CR), which includes causal discovery and\ninference, is a fundamental aspect of human in-\ntelligence, facilitating effective problem-solving,\ndecision-making (Cavenaghi et al., 2024), and un-\nderstanding of the world (Yao et al., 2021), with\napplications in healthcare (Mahmooda et al., 2014;\nProsperi et al., 2020), economics (Celli, 2022; Ve-\nmuri, 2015), biology (Ramsey and Andrews, 2018),\namong other fields. While LLMs can often gen-\nerate rationales for their outputs (Huang et al.,\n2022), it remains controversial whether they gen-\nuinely understand causal relationships (K\u0131c\u0131man\net al., 2023a). For example, Liu et al. (2024a) found\nthat LLMs struggle to integrate pre-trained causal\nknowledge in statistical reasoning tasks.\nEnhancing the causal reasoning capabilities of\nLLMs differs from improving their general reason-\ning skills. CR focuses on understanding causal rela-\ntionships, while general reasoning primarily relies\non identifying correlations (Bao et al., 2024). Un-\nlike general reasoning, which often involves sim-\npler, more static relationships, CR requires deeper\nanalysis and managing uncertainty to determine\nhow events are causally linked (Pearl, 2009). Ad-\nditionally, CR must account for confounding vari-\nables (Greenland et al., 1999), where an observed\ncorrelation between two variables is due to the\ninfluence of a third variable. Counterfactual rea-\nsoning (Pearl, 2009), often necessary for causality\nverification, is also challenging, as it involves rea-\nsoning about hypothetical scenarios that did not\nactually occur.\nExisting research on enhancing the CR capabili-\nties of LLMs can be broadly categorized based on\ntheir role: either as the engine directly performing\ncausal inferences or as the helper providing ex-\ntra information to traditional CR methods (Figure"}, {"title": "2 Preliminaries", "content": "CR in the context of machine learning (ML) in-\nvolves understanding and modeling cause-and-\neffect relationships within data, which goes beyond\nthe traditional ML focus on correlations. Tradi-\ntional ML methods often identify patterns or cor-\nrelations, limiting their applicability in understand-\ning the mechanisms behind complex systems (Pe-\nters et al., 2017). Causal ML, on the other hand,\naddresses this gap by focusing on cause-and-effect\nrelationships, which is crucial for making informed\ndecisions, predicting the effects of actions, and\nunderstanding complex systems (Prosperi et al.,\n2020; Fern\u00e1ndez-Lor\u00eda and Provost, 2022; Lage-\nmann et al., 2023; Berrevoets et al., 2023).\nThe CR framework can be categorized into three\nlevels: association, intervention, and counterfactual\n(Pearl et al., 2000; Pearl and Mackenzie, 2018),\nwhich form \"the ladder of causality\" (Table 1). As-\nsociation is formalized in statistical terms through\nconditional probability between variables. Inter-"}, {"title": "3 Towards Causal Reasoning in Large\nLanguage Models", "content": "We separate the roles of language models in CR\ninto two categories. First, LLMs can serve as\ncausal reasoning engines, employing methods\nsuch as fine-tuning, prompt engineering, external\ntool integration, and alternative approaches to di-\nrectly draw causal inferences from input prompts.\nSecond, LLMs can act as helpers to traditional\nmethods by extracting causal information and gen-\nerating causality data to enhance causal analysis\nacross various domains."}, {"title": "3.1 Serving as Causal Reasoning Engines", "content": ""}, {"title": "3.1.1 Injecting Causal Knowledge through\nFine-Tuning", "content": "Fine-tuning language models has been proven to\nbe a simple and effective technique for transferring\nlearned knowledge from a pre-trained model to\nnew downstream tasks. Compared to training from\nscratch, it can significantly reduce training time,\nand improve performance on new tasks (Lester"}, {"title": "3.1.2 Embedding Causal Knowledge within\nContexts", "content": "Prompt-based methods have become crucial in un-\nlocking the internal reasoning capabilities of LLMs\nacross various tasks. Compared with general rea-\nsoning, CR tasks, particularly multi-step ones, pose\ngreater challenges due to factors such as the com-\nplexity of understanding cause-and-effect relation-\nships, the need for reasoning about interventions\nand counterfactuals, and the presence of confound-\ning variables (Gandhi et al., 2023; Tan et al., 2023;\nBan et al., 2023; Abdali et al., 2023).\nTo address these challenges, various innovative\nprompting strategies have been proposed (Liu et al.,\n2023b; Bagheri et al., 2024). Building on Chain-\nof-Thought (CoT) prompting (Wei et al., 2023),\ncausalCoT (Li et al., 2023c) enables LLMs to\nreason step-by-step through causal relationships,\nsignificantly improving causal inference tasks in-\nvolving multiple causes and effects. Recent ad-\nvancements, such as Causal Coherence Contex-\ntual Learning (causalCCL) (Jin et al., 2023a),\nand Causal Prompt Tuning (CPT) (Zhang et al.,\n2023), further enhance CR by embedding prompts\nwith complex logic, incorporating causal structures,\nand helping models distinguish between correla-\ntion and causation. Multi-modal Causal Reason-\ning (MCR) (Wang et al., 2023c) extends CR to\nmulti-modal settings, integrating visual and textual\ninformation for a comprehensive analysis. Coun-\nterfactual Prompting for Causal Reasoning (CPCR)\n(Chen et al., 2023c) utilizes counterfactual thinking\nto guide models in exploring alternative scenarios,\nthereby improving their understanding of causal\nmechanisms.\nThese advanced prompting techniques collec-\ntively enhance the ability of LLMs to understand\ncausal relationships, thereby improving precision\nin CR tasks and enabling models to manage long-\nterm dependencies more effectively (Lu et al.,\n2022). Furthermore, they reduce biases from train-\ning data, enrich the model's knowledge with exter-\nnal information and commonsense, and do so with-\nout substantial increases in computational costs\n(Yao et al., 2022; Jin et al., 2023a)."}, {"title": "3.1.3 External Causal Tool Integration", "content": "The motivation for integrating external tools into\nLLMs is that effective causal reasoning requires\nmore than just internal model representations. By\naccessing structured knowledge bases and perform-\ning specialized computations, external tools en-\nhance LLMs' capacity for complex causal reason-\ning, complementing their inherent capabilities. The\nability to use these tools can be acquired through\nmethods such as fine-tuning, in-context learning,\nand prompt manipulation. This integration estab-\nlishes a crucial link between causal reasoning and\nlanguage understanding, enabling LLMs to handle\ncomplex causal tasks more effectively.\nFor methods that incorporate tools in prompts,\nPLAN (Lu et al., 2023) introduced a causal frame-\nwork for procedural planning that leverages exter-\nnal knowledge bases, such as ConceptNet (Speer\net al., 2017). It semantically parses tasks into an\nentity set to retrieve relevant sub-graphs for im-\nproved planning. Additionally, Pawlowski et al.\n(2023) compared context augmentation and tool\naugmentation methods: context augmentation uses\nlanguage models for supplementary operations be-\nyond the causal expert system, whereas tool aug-\nmentation incorporates basic Python tools to ma-\nnipulate outputs from the expert system. Their ex-\nperiments showed that context-augmented LLMs\nwere more prone to errors compared to tool-\naugmented ones. For methods involving super-\nvised fine-tuning, dialogue agents such as those by\nShuster et al. (2022); Komeili (2021) are trained to\neffectively use search engines."}, {"title": "3.1.4 Alternative Approaches", "content": "Recent studies have explored various alternative\napproaches to enhance the causal reasoning ca-\npabilities of LLMs, including iterative improve-\nment protocols, multi-agent systems, and rationale-\nbased evaluation. For example, ALLURE (Hasan-\nbeig et al., 2023) uses a systematic auditing and it-\nerative improvement protocol with in-context learn-\ning examples to refine LLM evaluation. ECHO\n(Xie et al., 2023) introduces a visio-linguistic\ndataset based on real-world scenarios to evaluate\nevent causality through human-centric reasoning.\nAgentic frameworks have also been employed\nfor causal reasoning in LLMs. CausalGPT (Tang\net al., 2023) employs a multi-agent system compris-\ning reasoners and evaluators, with reasoners gen-\nerating answers and evaluators assessing factual\naccuracy and causal consistency. Du et al. (2023)\npresents a debate framework where multiple in-\nstances of LLMs debate a causality-related query\nto reach a consensus. Selection and inference-\nbased techniques are another approach. Creswell\net al. (2022) alternate between selection and infer-\nence steps using pre-trained LLMs to generate in-\nterpretable causal reasoning steps, while Creswell\nand Shanahan (2022) chain reasoning steps to pro-\nduce valid reasoning traces. Rationale-based eval-\nuation methods are also receiving increased atten-\ntion. Atanasova et al. (2023) use rationale-based\nmethods to conduct tests for evaluating the faith-\nfulness of natural language explanations. Abdali\net al. (2023) generate a prior model to address do-\nmain knowledge gaps, serving as a heuristic for as-\nsessing the informativeness of user feedback. Addi-\ntionally, Yue et al. (2023) utilize a cascaded model\nincorporating CoT and Program-of-Thought (PoT)\napproaches, along with answer sampling and con-\nsistency checking, to enhance causal reasoning."}, {"title": "3.2 Enhancing Traditional CR Methods", "content": "Beyond serving as causal reasoning engines, LLMs\nalso act as helpers in causal analysis across do-\nmains (Figure 4), fulfilling several supportive roles:\n(1) Causal Information Extraction: LLMs can ex-\ntract causal variables, event sequences, and meta-"}, {"title": "4 Evaluating Casual Reasoning in Large\nLanguage Models", "content": ""}, {"title": "4.1 Experimental Setup", "content": "We first categorize the end tasks into three groups:\ncausal discovery, causal inference, and additional\ncausal tasks. For each category, we evaluate recent\nLLMs using pass@1 accuracy with strategies such\nas zero-shot, few-shot, direct I/O prompting, and\nChain-of-Thought (CoT) reasoning. Specifically,\nwe use COPA (Roemmele et al., 2011), NPDS"}, {"title": "4.2 Main Results", "content": "Overall performance is shown in Table 2. We sum-\nmarize the key observations as follows:\nLLMs significantly underperform humans in\nCR Tasks. Our results reveal a notable perfor-\nmance gap between LLMs and humans across\nvarious causal reasoning tasks, as illustrated in\nthe Radar chart (Figure 8). Despite recent ad-\nvancements, LLMs continue to face challenges in\nfully grasping and applying causal reasoning, of-"}, {"title": "4.3 Analysis and Discussion", "content": "We further analyze the impact of various factors on\naccuracy below.\nZero-shot or few-shot under CoT prompting.\nFigure 5 shows that both zero-shot and few-shot\nmodels benefit from CoT prompting, with few-shot\nCoT achieving the highest overall performance.\nWhile few-shot consistently outperforms zero-shot\nacross all tasks, the models struggle the most with\ninference tasks, where accuracy remains lower than\nin discovery and additional tasks. These findings\nhighlight the effectiveness of combining example-\nbased learning with structured reasoning but also\nunderscore the need for further improvements, es-\npecially in inference tasks, to enhance model per-\nformance across complex causal scenarios.\nModel size vs. Causal reasoning performance.\nFigure 6 demonstrates that causal reasoning per-\nformance improves consistently with increasing\nmodel size across all evaluated models. Both\nLLaMA3 and LLaMA3.1 achieve significant gains,"}, {"title": "5 Findings and Reflections", "content": "LLMs demonstrate shallow causal reasoning\nskills. Evaluation metrics often emphasize task-\nspecific accuracy, leaving open questions about the\ndepth of the model's causal reasoning (Ze\u010devi\u0107\net al., 2023). For instance, Hong et al. (2023) in-\nvestigated how LLMs process event B in a script-\nbased story, which causally depends on a prior"}, {"title": "6 Future Directions", "content": "Deep understanding of existing theories. Inte-\ngrating causal reasoning capabilities into LLMs\npresents unique challenges (Chan et al., 2024), es-\npecially when requiring adherence to established\ntheories. These theories, such as the potential out-\ncomes framework, graph-based causal inference\nmethods, and structural equation modeling, often\nrely on foundational assumptions, including the\nstable unit treatment value assumption, ignorabil-\nity/unconfoundedness, and positivity (Liu et al.,\n2024b). Considering these assumptions necessi-\ntates innovative strategies, such as incorporating\nspecific post-training data and implementing filter-"}, {"title": "7 Conclusion", "content": "LLMs equipped with these advanced technologies\ndemonstrate strong potential in causal reasoning,\nbut further research is needed to fully realize and re-\nfine their capabilities. Addressing challenges such\nas managing intricate causal structures is essen-\ntial for continued development. Enhancing inter-\npretability and transparency through causal reason-\ning is crucial for building trust and helping users\nrely on model inferences. Exploring innovative\nmethods will be key to overcoming current limi-\ntations and unlocking the full potential of LLMs"}]}