{"title": "UNOBSERVED OBJECT DETECTION\nUSING GENERATIVE MODELS", "authors": ["Subhransu S. Bhattacharjee", "Dylan Campbell", "Rahul Shome"], "abstract": "Can we detect an object that is not visible in an image? This study introduces the novel task of 2D\nand 3D unobserved object detection for predicting the location of objects that are occluded or lie\noutside the image frame. We adapt several state-of-the-art pre-trained generative models to solve this\ntask, including 2D and 3D diffusion models and vision-language models, and show that they can\nbe used to infer the presence of objects that are not directly observed. To benchmark this task, we\npropose a suite of metrics that captures different aspects of performance. Our empirical evaluations\non indoor scenes from the RealEstate10k dataset with COCO object categories demonstrate results\nthat motivate the use of generative models for the unobserved object detection task. The current\nwork presents a promising step towards compelling applications like visual search and probabilistic\nplanning that can leverage object detection beyond what can be directly observed.", "sections": [{"title": "Introduction", "content": "Object detection is a fundamental building block of per-\nceptual systems that are capable of making sense of their\nsurroundings. However, it is limited to visible surfaces\ninside the camera's field-of-view. While amodal object\ndetection and segmentation [68, 28] relaxes this require-\nment to allow partially-visible objects, we would like to\ngo further and consider those objects that were not ob-\nserved at all. One might expect this to be possible, since\nhumans are highly capable of extrapolating the likely po-\nsition of unseen objects given a context view. However,\nthis functionality has not yet emerged for machine vision.\nIn this paper, we propose the novel task of unobserved\nobject detection in 2D and 3D for inferring the presence\nof objects unseen in an input image within a domain that\nextends beyond the camera frustum and behind the visible\nsurfaces. We are particularly motivated by probabilistic\nplanning tasks, where a robot might be expected to use its\nunderstanding of a scene beyond what it can directly see\nto undertake a reasonable action. For example, consider\nthe setup in Figure 1. A robot tasked with locating a\nseated person might find it useful to look to the right,\nwhere it is likely that additional chairs would be found, or\nto move to view a potentially occluded chair. Predicted\nspatial likelihoods of objects in the scene can inform probabilistic models [23, 35] that take decisions based on this\ninformation.\nWe expect a wide category of other downstream tasks, including active vision, navigation and visual search [1, 48, 64],\nto be enabled by solutions to this task. In these contexts, a broader understanding of the scene from learned priors can\nprove critical. Sampling plausible scenes conditioned on an input image correspond to a category of closely related\nproblems where generative models have shown promising performance. While explicitly embedding spatio-semantic\nrepresentations into training processes can be challenging [58], such relationships might be variationally inferred [22]\nusing generative models trained on large-scale 2D image datasets. Novel view synthesis and semantic scene completion\n[57, 55, 10, 7, 46, 53] or scene-graph representations [26] can offer useful spatio-semantic priors that can be used\nto render 3D volumes where the object might exist. However, occlusions and incomplete or sparse data still pose\nsignificant challenges to 3D scene generation methods. Image diffusion models [18, 54] have been shown to be useful\nat expanding beyond the visible frame, known as outpainting [39], and some 3D diffusion models [55] can generate 3D\nrepresentations consistent with an input image. However, these approaches aim for photorealism of a completed scene,\nwhich may not be necessary or desirable for unobserved object detection, where we want to infer a spatio-semantic\ndistribution of possible scenes.\nThis study formally defines the unobserved object detection task in 2D and 3D, given a single image as input, and\nproposes three novel detection pipelines for the task. The latter extends three major categories of generative model (2D\nand 3D diffusion models and vision-language models) to address this task. We also propose metrics to evaluate the\nperformance of the detection pipelines at unobserved object detection, which measure the diversity and accuracy of the\npredictions. In summary, our contributions are:\n1. defining the novel task of unobserved object detection;\n2. extending three major classes of generative model to address the task of unobserved object detection; and\n3. benchmarking these methods on a suite of metrics designed for this unobserved object detection.\nWe evaluated the methods on the RealEstate10k dataset [67] using MS-COCO object labels [27] in 2D, 2.5D and 3D\ntask settings. The results indicate that existing pre-trained models exhibit promising performance at this task. However,\nthere is significant room for improvement. We expect that future approaches that eschew photorealism and associated\nlosses during training will perform more strongly at this task than existing methods."}, {"title": "Related Work", "content": "Object Detection. Object detection focuses on identifying and localizing objects within images, with recent deep\nlearning models such as Faster R-CNN [41], YOLO [40] and transformer-based approaches significantly enhancing\nboth accuracy and efficiency [69]. However, occlusion in real-world scenarios like autonomous driving and robotics\nremains a critical challenge for traditional models. Amodal object detection and segmentation [68, 28] extend traditional\napproaches by predicting the full projected geometry of partially-occluded objects. In [25], the authors introduced\namodal bounding box regression to estimate complete object boundaries, including unseen parts. In [6], the authors\nused a multi-camera setup and deep networks to jointly learn visible and occluded object segmentations, leveraging\ncontextual cues. More recent approaches improve upon these techniques through cumulative occlusion learning [3]\nUnlike amodal object detection and segmentation, which can only reason about partially-occluded objects, we aim to\nreason about fully-occluded or out-of-frame objects. This requires scene-level priors, not just the object-level priors of\namodal methods.\nImage Diffusion Models. Diffusion models [51, 18] generate high-quality, plausible images via an iterative denoising\nprocess. In conditional generation tasks, like ours, they can be guided from auxiliary inputs such as text or images\nduring the denoising process, known as conditioning [21, 44]. One approach to learning a conditional and unconditional\nmodel simultaneously is classifier-free guidance [19], which facilitates applications in scene completion and conditional\nsynthesis [12, 45]. Of relevance to this work is the application of image outpainting, where image boundaries are\nextended beyond the original content. One highly-successful diffusion model, regularly used for such purposes, is\nStable Diffusion [43], which efficiently generates high-resolution images conditioned on text. In this work, we explore\nwhat spatio-semantic relationships this model has learned from large-scale image data.\nNovel View Synthesis (NVS). NVS is the task of generating new views of a scene from a set of posed input images.\nTraditional methods like multi-view stereo and structure-from-motion can reconstruct the 3D geometry but may struggle\nwith photorealistic rendering in complex environments [50, 16]. Neural Radiance Fields (NeRF) [31] transformed\nNVS by using neural networks to parameterize scenes as continuous volumetric radiance fields, enabling high-fidelity\nrendering. However, NeRF typically requires many input images and known camera poses, limiting its practicality with\nscarce data. To address this, methods like PixelNeRF [63], PixelSynth [42], and SinNeRF [59] leverage pre-trained scene\npriors for effective representation from minimal input. While these methods improve generalization to unseen scenes,\nthey still face challenges due to limited viewpoints and lack of geometric constraints, leading to less accurate depth\nestimation. Recent approaches integrate diffusion models into the NVS pipeline to overcome these issues. Diffusion"}, {"title": "Unobserved Object Detection", "content": "In this section, we define the tasks of 2D and 3D unobserved object detection and outline how existing generative\nmodels can be adapted to address this problem. We consider representative 2D and 3D diffusion models, as well as\nvision-language models, ascertaining how each may be extended to one or both tasks."}, {"title": "Task Description", "content": "The task of unobserved object detection, as shown in Figure 1, is to detect objects that are in the vicinity of, but not\nobserved by, a camera. The approaches considered in this paper address this task by predicting a conditional distribution\nD\u2081 over a bounded space and set of semantic labels given a single RGB image I, referred to as a spatio-semantic\ndistribution (SSD). This facilitates the inference of the probable locations of unobserved objects outside the visible\nregion captured by the image and its associated camera frustum, using contextual cues from the image. The spatial\ndomain X defines the support of this distribution. The SSD is an estimate of the ground-truth conditional distribution\n\u03931. While this distribution is not observable, we have access to additional posed images from a realization of this\nconditional distribution for the purposes of evaluation.\nFor each object label o, we denote the associated spatial distribution as D1o. This maps each x \u2208 X to a likelihood in\n[0, 1]. That is, the likelihood of detecting object o at location x. For example, it assigns a likelihood to the presence of a\nchair one meter in front of and two meters to the right of the camera, which may not be visible in the input image. Note\nthat we consider all distributions to be normalized.\nWe consider distributions over two discretized spatial domains X: (1) a 2D domain I, discretized as a pixel grid,\ncorresponding to an expansion of the input image; and (2) a 3D domain V, discretized as a voxel grid, extending beyond\nthe input camera's frustum. The 2D domain I symmetrically extends the H \u00d7 W pixel grid of the input image to\nH \u00d7 W', where W' > W. The 3D domain V expands the camera frustum to an expanded voxel grid aligned with the\ncamera coordinate system."}, {"title": "Generative Models for Unobserved Object Detection", "content": "This section details how three types of generative model can be extended to estimate the conditional spatio-semantic\ndistribution D10 in 2D, 3D or both. The models employed include (1) 3D diffusion models, (2) 2D diffusion models,\nand (3) vision-language models (VLMs). Detection pipelines derived from these models are deployed in indoor scenes\nfrom the RealEstate10k dataset [67] using the MS-COCO dataset [27] object categories."}, {"title": "3D Diffusion", "content": "We extend the Diffusion with Forward Models (DFM) [55] model to estimate the conditional distribu-\ntion. This approach generates a 3D representation conditioned on a single RGB image, from which an RGBD image\ncan be rendered from any camera pose. We select a set of k = 4 target poses, including the input image's pose, as\nvisualized in Figure 2(a), render the corresponding RGBD images, and combine the results into a single point cloud\nalong with the input pose. An off-the-shelf object detector, YOLOv8x [40, 20], is run on the target frames. The 2D\nbounding boxes for each object o are back-projected onto the point cloud alongside detection confidences. The resulting\n3D confidence map is voxelized into grid cells V, taking the average confidence within each voxel, and is normalized\nto obtain DD. Since DFM is a generative model, we can take additional samples from the conditional distribution.\nThe above procedure is run for ns samples, all conditioned on the same input image 1, and the results are aggregated\ninto the estimated distribution by averaging across the samples. As shown visually in Figure 2(a), the DFM detection"}, {"title": "2D Diffusion", "content": "To evaluate the capacity of a 2D diffusion model at estimating the conditional distribution, we utilize\nthe SDXL [39] Infinity Runway model to symmetrically extend the 360 \u00d7 360 input image to a 640 \u00d7 360 output. This\nis achieved by expanding the image canvas to the desired dimensions and performing conditional generation to fill the\nnewly introduced regions through outpainting. We then run an object detector, YOLOv8x [40, 20], for each object o to\nassign normalized object confidence scores to each pixel, resulting in a 2D spatio-semantic distribution DD using ns\ngenerated samples (shown in Figure 2(b)). To obtain the estimated 3D distribution D3D, the 2D bounding boxes were\nback-projected into 3D using metric depth values estimated by DepthAnythingv2 [61, 60] and the camera parameters\nfrom the dataset, and the result was rescaled to match the ground-truth scale, voxelized and normalized as before. We\nleveraged DepthAnythingv2 for its superior cross-domain generalizability, given our known camera model."}, {"title": "Vision-Language Models (VLMs)", "content": "We adapt several VLMs to the task of predicting the (coarsely-discretized)\n2D conditional distribution: ChatGPT-4 [36], Claude-3.5 Sonnet [2], Gemini 1.5 Ultra [14], QwenVL-Chat [4], and\nLLaVa-34B-v1.6 [29]. To do so, each model was provided with the 360 \u00d7 360 input image, padded with 140 black\npixels to the left and right, forming a 640 \u00d7 360 image. For each object o, the model was asked whether the object was\nwithin the original frame or the extended region to the left or to the right. Model APIs were used for automation, with\nquestion order shuffling to avoid bias [15]. To mitigate limitations in large language models, such as uncontrolled token\ngeneration [5], responses were constrained to binary response \u201cYes\u201d/\u201cNo\u201d. A consistency score was computed based\non 10 repeated queries per region, and the fraction of \u201cYes\u201d responses was normalized to generate the 2D conditional\ndistribution D2D (Figure 2(c)). Note that the predicted distribution is uniform in each left/center/right region, since the\nVLM questions only elicit a coarse, region-wise response. Details on the prompts and experiment design are provided\nin Appendix C."}, {"title": "Experiments", "content": "In this study, we evaluate model performance on three distinct tasks across 10 different\nindoor scenes from the RealEstate10k dataset [67]. The benchmarks assess the models'\nreasoning abilities in both 2D and 3D contexts, focusing on different aspects of spatial\nunderstanding, both within localized regions (the frustum) and for entire scene recon-\nstructions. Metrics have been proposed to evaluate the performance of the detection\npipelines against baselines alongside both quantitative and qualitative empirical results."}, {"title": "Experimental Setup", "content": "Dataset. All models are evaluated on 10 scenes from the RealEstate10k test set [67],\nwhich were automatically selected based on object detection frequency and an indoor\nfilter. Each of the 10 selected scenes has an average of 134 images with camera pose\nannotations, after filtering. One frame per scene is used as the input image I for\nevaluation. See Appendix A for details on selection criteria, filtering and the input\nframe timestamps. To generate the ground-truth object distributions, the following\nprocedure was undertaken. Object annotations were generated for every image in the\ndataset by YOLOv8x [40, 20], pre-trained on MS-COCO [27], chosen for its robustness"}, {"title": "Metrics for unobserved object detection", "content": "In order to evaluate the unobserved object detection task, we propose several metrics to quantify how well the predicted\nconditional distribution Dro for object o over a discrete domain X conforms to the true conditional distribution.\nHowever, we only have access to a single realization of this conditional distribution, denoted Go, which we assume to\nbe available. The procedure for constructing this ground-truth distribution is detailed in Appendix A. Significantly, the\ntrue conditional distribution \u0393\u03c4\u03bf is likely to have more modes than the realization Go. For example, given an image I\nof a dining table, the ground-truth scene (the realization) happened to have several chairs clustered to the right of the\ncamera. However, the exact same image could equally well have corresponded to a scene with a chair to the left of\nthe camera (another, unrealized, scene conditioned on the same image). We expect the models to predict all plausible\nmodes, and this asymmetry between the ground-truth and the prediction informs our choice of metrics.\nNormalized Entropy H and Normalized Cross-Entropy H\u00d7. To quantify how close the predicted distribution is to\na uniform distribution, a maximally uncertain prediction, we report the Shannon entropy [49] H, normalized such that\nH\u2208 [0, 1] and a uniform distribution evaluates to H = 1. We also report the normalized cross-entropy H\u00d7 between\nthe ground-truth distribution Go and the predicted distribution D10. This asymmetric measure penalizes the prediction\nof low object likelihood where the ground-truth indicates the presence of an object. However, as required, it does not\npenalize the prediction of high object likelihoods where the ground-truth has not seen an object. These metrics are\ngiven by\n\n$H=\\frac{1}{|S||O|\\log |\\mathcal{X}|}\\sum_{I \\in S}\\sum_{o \\in O}\\sum_{x \\in \\mathcal{X}} D_{Io}(x) \\log D_{Io}(x)$\n\n\n$H^\\times = \\frac{1}{|S||O|\\log |\\mathcal{X}|}\\sum_{I \\in S}\\sum_{o \\in O}\\sum_{X \\in \\mathcal{X}} G_o(x) \\log D_{Io}(x)$,\n\nwhere O is the set of objects and S is the set of input images (equal to the set of scenes).\nNormalized Nearest Neighbor Distance DNN. We define a metric to quantify the spatial distance between ground-\ntruth object locations and the nearest high-likelihood prediction. To this end, the distances between peaks in Go (i.e.,\nlocations containing object o) and the nearest high-likelihood location in D10 are computed. Large distances on average"}, {"title": "Results", "content": "In this section, we report the results of a comprehensive evaluation of the generative models at the 2D, 2.5D and 3D\nunobserved object detection tasks, as shown in Tables 1 and 2. Alongside the metrics outlined in Section 4.2, we also\nreport the percentage of false negatives (FN), defined as the proportion of instances where the predicted distribution\nfails to identify the presence of the object, despite its presence in the ground truth. Note that before averaging over the\ndataset, statistical outliers were filtered, identified as those above Q3 + 1.5IQR. Methods were only evaluated on tasks\nfor which they are applicable. For example, VLMs could not be evaluated on the 3D tasks, since they only provide 1D\ninformation (left, center, right).\nFor the 2D task (Table 1), we see that the 3D diffusion method (DFM) has the best performance with respect to all\nmetrics other than the classification accuracy and the cross-entropy, for which the VLMs tend to perform better. For the\n2.5D task (Table 2, left), we see that the 3D diffusion model consistently outperforms the 2D diffusion model (SDXL)\naugmented with monocular depth predictions, except on the accuracy metric. Finally, for the 3D task (Table 2, right),\nwe see that the 3D diffusion model has clear superiority over all other applicable methods, including the oracle baseline,"}, {"title": "Analysis & Ablation Study", "content": "This section presents an analysis of the 3D diffusion model based on DFM [55] and an ablation study for the 2D\ndiffusion model based on SDXL [39]. First, as reported in Tables 1 and 2, we analyze the effect of reducing the number\nof camera poses k at which the DFM model is queried, from 4 to 2, which affects the coverage of the scene. To do so,\nwe randomly drop out one or two camera poses and report the average results. We find that the performance decreases\nconsiderably as the number of query camera poses decreases, across all tasks. Second, we analyze the effect of reducing\nthe number of conditionally-independent samples ns per input image from 25 to 10, which reduces the diversity of the\npredicted conditional distribution. This also leads to a significant performance decrease, since the predicted distribution\nis less able to model the modes of the true conditional distribution.\nWe also ablate the 2D diffusion-based approach, investigating the effect of text prompts on the results, as reported in\nTables 1 and 2. We consider three alternatives: (a) without text prompts; (b) without object prompts, where the model is\nprovided with text guidance but without reference to any objects (e.g., \"extend this indoor scene naturally to the left and\nright of the given frame\"); and (c) with object prompts, where the model is provided with object-specific prompts (e.g.,\n\"extend this scene naturally on both sides, with objects like [OBJECT]\"). The results indicate that object prompts are\ncritical to the performance of the outpainting-based method, which suggests that the model is not making good use of\nthe visual information in the input image."}, {"title": "Discussion and Conclusion", "content": "Limitations. This work has several significant limitations. In particular, the inference time of generative models,\nespecially 3D diffusion models, is currently very large. Since our approaches require many samples to approximate the\nconditional distribution, this is a significant bottleneck and stymies their use in real-time applications like in robotics.\nAnother limitation is the need for object prompts for the outpainting-based methods, since this is likely to bias the\nobtained empirical conditional distribution. Also, the VLM-based approach evaluations may not be fully reliable, since\nit is possible these models have seen test set images during training and so could have memorized the uncropped images.\nConclusion. This work has introduced unobserved object detection in 2D and 3D, presented three detection pipelines\nusing state-of-the-art generative models, and evaluated their performance on a proposed set of metrics in a dataset of\nindoor scenes and common objects. We find that unobserved object detection presents different characteristics in 2D"}, {"title": "Ethics Statement", "content": "Approaches, like ours, that address the task of unobserved object detection may lead to potential\nsocietal benefits, such as through making autonomous systems more capable and better able to reason about their\nsurroundings. However, there are several potential concomitant risks with systems that can infer information that has not\nbeen observed. This could include indirect privacy violations and unfair decisions made using predictions from a model\nthat may be biased due to its training data or strategy. To mitigate the effect of this, we make use of publicly-available\npre-trained models and datasets, so that potential risks can be investigated and quantified. Additionally, to mitigate\nthe environmental impact of training large generative models, we use pre-trained models and optimize computational\nresources wherever possible."}, {"title": "Reproducibility Statement", "content": "The code for our experiments, including data pre-processing, sampling, detection, and\nevaluation pipelines, will be made publicly available, along with detailed setup instructions, dependencies, and all\nhyperparameters. The dataset used is publicly accessible, and all necessary configurations for model and data processing\nare provided to ensure replication. However, one part of our study depends on third-party closed vision-language\nmodels accessed via APIs, which may be updated, leading to potential variations in results. While we document the\nspecific API versions, configurations and access dates, changes in these services or access limitations may impact exact\nreproducibility."}, {"title": "Additional Scene Processing Details", "content": "The COCO objects we considered include the following: refrigerator, TV, bed, chair, sink, oven, book, laptop, couch,\nand door [27].\nThis section outlines supplementary details regarding the scene selection and processing workflow used in this study.\nScene Category annotation was automated using a ChatGPT-40 API [36] built in to predict room categories up to three\npredictions.\nScene Selection Methodology: To ensure even spatial sampling across camera pose sequences, we employed geodesic\nmovement calculations within the special Euclidean group, SE(3). Since multiple frames may be spatially equidistant\nin SE(3), we selected the frame closest in timestamp to each equidistant geodesic point. This approach minimizes\nredundancy by ensuring that each selected frame captures a unique viewpoint while maintaining a representative spatial\ndistribution. For each scene, we recorded the total pose variation and selected frames that best matched equidistant\nSE(3) poses using their timestamps. This ensures consistent spatial sampling across the entire camera movement\ntrajectory. In the absence of meta-labels, we used these images to verify whether the scene is indoor or outdoor using\nthe ChatGPT-40 API [36], and automatically labeled them accordingly for consideration.\nIndoor Scene Selection Criteria: We further filtered our selection of indoor scenes using the following criteria:\n\u2022 Scene Size: Scenes with too few images (<50) were excluded to reduce reconstruction sparsity.\n\u2022 Geodesic Movement: A minimum threshold of spatial movement was imposed to guarantee sufficient variation for\n3D representation testing.\n\u2022 Object Detection: Scenes with low COCO object detection rates (<60%) were automatically filtered out, as they\nprovide lower semantic diversity for the evaluations.\nImage Selection from scene: An image is selected if its 360x360 center crop excludes the target object and a unique\nimage from each scene ensures diversity. Additionally, images are not chosen from the first or last 10 timestamps to\navoid cross-fading.\nAddressing Drift in ORB-SLAM Poses: Since the RealEstate10k dataset employs ORB-SLAM poses, normalized scale\nextrinsics can drift over long camera trajectories, especially when loop closures are absent [33, 34]. To mitigate this,\nan epipolar distance-based filter was implemented to remove frames from the temporal edges that showed significant\npixel-to-pixel variation between consecutive frames, indicative of potential drift or cross-fading artifacts."}, {"title": "Supplementary Information on DFM Sampling & Sample Rejection", "content": "In this appendix, we provide detailed information regarding the specific metrics, thresholds, computational considera-\ntions, and pose selection process used in our study.\nSample Rejection Metrics: Four key metrics were used for automated sample rejection:\n\u2022 Noise Detection: Noise was quantified using Laplacian variance VL. Samples with VL <\u0442\u044c for more than 80% of\nframes were classified as noisy and discarded. The threshold \u0442\u044c was set according to guidelines established in Pertuz\net al. [38].\n\u2022 Structural Similarity (SSIM): SSIM was calculated for each frame to assess visual similarity between consecutive\nframes. Videos where SSIM exceeded Ts for over 50% of the frames were rejected for showing stagnation or lack of\nvisual variability [56].\n\u2022 Pixel Luminosity Variation: The average pixel intensity Lt was used to track brightness variations across frames.\nSamples were rejected if changes in luminosity between consecutive frames fell below \u03c4\u03b9 for over half the video.\nThis indicates static content or poor lighting conditions.\n\u2022 Object Detection Flatness: Object detection was conducted using Yolov8x [40, 20]. Samples were rejected if no\nobjects were detected with at least 0.5 confidence in 50% or more of the frames at 30 fps.\n\u2022 Sparseness Rejection: We have bounding boxes from the images in the ground truth frame. Scenes were rejected if\nupon dense stereo-matched reconstruction the obtained point clouds which when the base images (where the object\nwas detected in the ground truth 2D image) was passed through an object detector upon projection did not have at\nleast 100 pixels in the bounding box in every frame.\nComputational Limitations: Each run of the DFM model took over 7 days on a single GPU due to the lack of model\nparallelism support. The model required at least 80GB of VRAM to handle the large number of parameters and data\nprocessing [55].\nPose Selection in the SE (2) Framework: To balance scene diversity and computational efficiency, three target poses\nwere selected for each image. These poses (Pose 1 being the input frame, Figure 2(a)) were chosen based on their\nability to cover the scene without blind spots, defined in terms of (x, y, 0):\n\u2022 Pose 2: (2,2, -90\u00b0) \u2013 This pose focused on the eastern (left) boundary of the scene, ensuring coverage of the left\nand central areas.\n\u2022 Pose 3: (-2, 2, 90\u00b0) \u2013 This pose complemented Pose 1 by covering the western (right) boundary and central zones.\n\u2022 Pose 4: (0,5, 180\u00b0) \u2013 This pose was selected to capture depth and provide a longitudinal perspective from the south."}, {"title": "Detailed Methodology for VLM Query Process", "content": "The following provides additional details on how we implemented VLM query structuring and response processing for\nthe study.\nLimiting LLM Hallucination: Large Language Models, despite their visual and language reasoning abilities, cannot\nentirely prevent hallucinations during token generation [5]. To reduce the likelihood of incorrect outputs, we restricted\nresponses to binary \u201cYes\u201d or \u201cNo\u201d answers. This simplification helped minimize errors caused by the unpredictability\nof free-form responses, particularly in tasks involving spatial reasoning.\nStructured Query Format: We used a standardized query format for all questions, focusing on the presence or relative\nposition of an object in the image. The specific format included queries like:\nIs it likely that there is a/an [object] to the left of this frame?\nAnswer strictly in (Yes/No).\nIs it likely that there is a/an [object] to the right of this frame?\nAnswer strictly in (Yes/No).\nIs it likely that there is a/an [object] within the frame of this image?\nAnswer strictly in (Yes/No).\nThis strict format ensured that the output was easily interpretable and could be processed further without requiring\ncomplex parsing."}, {"title": "Depth Handling for Experiments", "content": "In the 2.5D study with the DFM, we retained points within the camera's frustum and removed occluded points using a\ndepth buffer. For each pixel, only the 3D point with the smallest depth Z-value was kept to ensure visibility. Frustum\nculling was then applied to retain points within the near and far clipping planes and the camera's field of view.\nFor DFM, operating at room scale, we compared predictions to scale-ambiguous ground truth from COLMAP. Since\nboth operate on the same scale, depths were directly compared.\nIn DepthAnythingv2, depth estimation defines near and far planes, varying only by a scale factor. Depth values\nwere rescaled uniformly based on the average image depth to maintain consistency. For COLMAP, we projected the\nreconstructed points onto the image plane and compared depths for valid pixels in both outputs, using global average\nrescaling to ensure consistency without complex iterative alignment.\nIn the 3D study, where the grid is sparse and unequal in size between the prediction and ground truth distributions, we\npad the grids to match the larger of the two. This ensures comparisons are made over valid domains."}, {"title": "Standard Deviation Values for the Metrics Presented", "content": "In this section, we provide the standard deviation values to the metrics described earlier in Section 4.3 in Table 4, Table 5\nand Table 6."}]}