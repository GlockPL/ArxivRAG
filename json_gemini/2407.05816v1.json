{"title": "Graph Reasoning Networks", "authors": ["Markus Zopf", "Francesco Alesiani"], "abstract": "Graph neural networks (GNNs) are the predominant approach\nfor graph-based machine learning. While neural networks\nhave shown great performance at learning useful represen-\ntations, they are often criticized for their limited high-level\nreasoning abilities. In this work, we present Graph Reasoning\nNetworks (GRNs), a novel approach to combine the strengths\nof fixed and learned graph representations and a reasoning\nmodule based on a differentiable satisfiability solver. While\nresults on real-world datasets show comparable performance\nto GNN, experiments on synthetic datasets demonstrate the\npotential of the newly proposed method.", "sections": [{"title": "Introduction", "content": "Graph-structured data can be found in a wide range of\ndomains such as computer science (e.g. scientific citation\ngraphs and social networks), chemistry (e.g. molecules), and\nbiology (e.g. proteins). Consequently, graph-based machine\nlearning (Scarselli et al. 2009; Bronstein et al. 2021) has re-\nceived increasing attention from the machine learning com-\nmunity.\nToday, the most popular approaches for graph-based\nmachine learning are different variants of the message-\npassing approach such as Graph Neural Networks (GNNs)\n(Scarselli et al. 2009), Graph Convolutional Networks\n(GCNs) (Kipf et al. 2018), and Graph Attention Networks\n(GATs) (Veli\u010dkovi\u0107 et al. 2018). While these approaches\nhave demonstrated good performance in a wide range of ap-\nplication domains, they are often criticized for their limited\nhigh-level reasoning abilities and interpretability (Cingilli-\noglu and Russo 2018; d'Avila Garcez and Lamb 2020; Mar-\ncus 2020; Chen et al. 2020; Loukas 2019; Bodnar et al. 2021;\nDwivedi et al. 2021). These shortcomings motivate research\nto combine the abstraction abilities of continuous neural net-\nworks and approaches with better high-level reasoning capa-\nbilities. A major challenge in doing so is the fact that many\nreasoning approaches are not differentiable, which renders\nan efficient joint training of continuous neural networks rea-\nsoning approaches impossible.\nIn this work, we present Graph Reasoning Networks\n(GRNs), a novel combination of continuous graph-based\nmachine learning and a logical reasoning approach that can\nbe trained with standard gradient-based optimization meth-\nods. GRNs consist of a graph encoding module that maps\ngraphs into a d-dimensional feature vector in $[0, 1]^d$ and a\ndifferentiable satisfiability solver Wang et al. (2019) that\nlearns logical rules based on the obtained representation. We\npropose a semi-learnable encoder that consists of a fixed and\na learnable component to combine the strength of previously\nobserved usefulness of fixed encodings with the adaptive-\nness of learned representations.\nOur experiments demonstrate the potential of the newly\nproposed method as it can learn challenging synthetic\ndatasets that standard GNNs cannot learn without a reason-\ning module. Furthermore, the proposed approach performs\non par on real-world datasets with and without using node\nfeatures."}, {"title": "Related Work", "content": "GNN and Beyond GNN GCN (Kipf and Welling 2016),\nGAT (Veli\u010dkovi\u0107 et al. 2017) or in general neural message\npassing (Gilmer et al. 2017) are among various commonly\nused methods to process features over graphs. PPGN (Prov-\nably Powerful Graph Network) (Maron et al. 2019) is a\nclass of Neural Networks that improves the expressiveness\nof GNN. (Morris et al. 2019) extends message passing to\ninclude higher-order node tuples. However, these methods\nremain highly limited regarding memory and computational\ncomplexity.\nLearning Rules over Knowledge Graphs Learning rules\nover Knowledge Graphs (KGs) (Qu et al. 2020) extracts\nrules from triplets head-relationship-tail (i.e. $(h, r,t)$), to\npredict the relationship $(r)$ not observed or to predict ele-\nments $(h$ or $t)$ missing in the KG. These methods do not\nextend to graph classification.\nDifferentiable SAT Differentiable SAT methods (Wang\net al. 2019; Wang and Kolter 2019; Wang, Chang, and Kolter\n2017) compute gradients on the rules and on the input and\noutput variables to propagate gradients, thus enabling SAT\nmodel to be integrated into machine learning systems. Alter-\nnative approaches (Cingillioglu and Russo 2018; Evans and\nGrefenstette 2018), restrict the class of rules to predefined"}, {"title": "Graph Reasoning Networks", "content": "In the following, we describe our novel approach to com-\nbine graph-based machine learning with a reasoning module\nto mitigate the limited reasoning capabilities of graph neural\nnetworks. On a high-level view, the proposed method con-\nsists of two submodules: an encoder and a reasoner. The\nencoder is a module that takes a graph $g$ as input and gen-\nerates a d-dimensional intermediate vector representation $r$\nof the graph. The intermediate vector representation $r$ con-\ntains binary values (i.e. $r_i \u2208 {0, 1}$) and/or probabilistic val-\nues (i.e. $r_i \u2208 [0, 1]$). In contrast to previous approaches, we\npropose an encoder that consists of both learnable and not\nlearnable features to empower the reasoner to learn even bet-\nter rules (hence, a semi-learnable encoder). The reasoner is\na module that learns logical rules based on the semi-learned\nd-dimensional intermediate representation $r$ and generates\na task-specific prediction $o$ for the input graph as an out-\nput. Both encoder and reasoner can be trained end-to-end\nwith gradient-based optimization thanks to the differentiable\nMAX-SAT relaxation proposed in (Wang, Chang, and Kolter\n2017), which is a crucial property of the proposed method.\nNote that different to most neural networks, the output of\nthe reasoner is not a probability distribution over all possible\nclasses, but a discrete output representing the corresponding\nclass. Hence, for a binary classification problem, we obtain\n$o \u2208 {0, 1}$ and a corresponding encoding for multi-class pre-\ndiction problems.\nBased on the two submodules, Graph Reasoning Net-\nworks can be defined as a combination of both functions\naccording to\n$r = \\text{encoder}(g), \\qquad y = \\text{reasoner}(r)$\nIn the following, we discuss both encoder and reasoner in\ndetail."}, {"title": "Semi-learnable encoder", "content": "To combine the strengths of fixed features and learnable rep-\nresentations, we propose a semi-learnable encoder that maps\ninput graphs into a vector representation, which will be used\nby the subsequent reasoner to predict a classification output.\nFixed features. The first set of functions consists of fixed,\npredefined features that encode information about the topol-\nogy of the graph and the node features (if present). A sim-\nple approach to encode the topology as a vector is to flat-\nten the corresponding adjacency matrix $A$ into an adja-\ncency string $S$. To this end, $A \u2208 {0,1}^{n\u00d7n}$ is converted\ninto $S\u2208 {0,1}^{n^2}$ according to $S_{i+n(j-1)} = A_{i,j}$ for\n$i, j\u2208 {1, ..., n}$. The size of the adjacency matrix increases\nquadratically with the number of nodes in the graph, which\nis also true for the adjacency string $S$. However, in many\ndatasets such as NCI1 and PROTEINS, the number of nodes\nin the graphs is rather small and thus allows an application\nof this approach. Furthermore, in undirected graphs, only a\npart of the adjacency matrix needs to be encoded since it\nalready contains all information about the graph topology.\nMoreover, the elements $A_{i,i}$ do not have to be encoded in $S$\nif the graphs do not contain self-loops. Hence, the size of the\nadjacency string $S$ can be reduced to $l = \\frac{n(n-1)}{2}$.\nA problem with this approach is that isomorphic graphs\nwith different adjacency matrices $A$ and $A'$ are mapped to\ndifferent adjacency strings. Two graphs are called isomor-\nphic if there exists an isomorphism (i.e. a bijection map)\nbetween their vertex sets that preserves the graph topology.\nGraph canonical labelling (McKay and Piperno 2014a) is\nthe process where a graph is relabelled such that isomorphic\ngraphs are identical after relabelling. While the complexity\nof graph canonical labelling is $e^{O(\\sqrt{n\\log n})}$ (Babai, Kantor,\nand Luks 1983), there exist fast implementations such as\n$\\texttt{Trace}$ or $nauty$ (McKay and Piperno 2014b).\nBesides encoding information about the topology, infor-\nmation about the node features can be encoded in a vector\nrepresentation, for instance by simply concatenating all node\nfeatures. However, this approach can lead to a large repre-\nsentation, depending on the number of node features.\nA problem with the previously described approach is that\nthe rules learned by Wang et al. (2019) are not invariant\nto permutations of the bit string. Hence, each rule is only\nable to consider a specific sub-structure of a graph or a spe-\ncific set of node features, which may limit the generalizabil-\nity of the approach. Furthermore, the representation is fixed\nand cannot be automatically adapted by a machine learning\nmodel based on the concrete problem at hand.\nLearned representation Encoding the topology with\nfixed representations such as a topology string or a\nWeisfeiler-Leman-based representation (Weisfeiler and Le-\nman 1968) have been shown to be strong features (Sher-\nvashidze et al. 2011). However, there are also problems as\ndiscussed above. A potential solution is to learn a fixed-sized\npermutation invariant encoding of the graph. To this end,\npermutation invariant graph neural networks (GNN) such as\nGCN (Kipf and Welling 2016) or GAT (Veli\u010dkovi\u0107 et al.\n2017) can be used. Since the approach proposed by Wang\net al. (2019) provides gradients not only for the rules but\nalso for the input, the GNN can be trained jointly with the\ndifferentiable satisfiability solver such that it learns to gen-\nerate a useful intermediate representation."}, {"title": "Reasoning module", "content": "The reasoning module takes the previously generated vector\nrepresentation as input and generates a binary classification\nas output. The reasoning module is based on a differentiable\nsatisfiability solver proposed by (Wang, Chang, and Kolter\n2017) and is explained in detail below.\nBackground: Differentiable Satisfiability\nMAX-SAT In Maximal Satisfiability Problems (MAX-\nSAT), we are interested in finding the assignment of m bi-\nnary variables $x_i \u2208 {\u22121,1},i = 1, ..., m$ for some given\nrules, or\n$\\max_{x \\in \\{-1,1\\}^m} \\sum_{j \\in [n]} \\text{\\Langle } s_j, x \\text{\\Rangle }$\nwhere $s_{ji} \u2208 {-1,0,+1}$ are the rules of the MAX-SAT\nproblem and $\\text{\\Langle } s_j, x \\text{\\Rangle } = \\bigvee_{i \\in [m]} 1_{s_{ji}x_i > 0}$. MAX-SAT is the\nrelaxation of the Satisfiability (SAT) problem, where all the\nrules need to be true. The relaxation is useful, when the set\nof rules may generate an infeasible set of solutions and we\nare interested in knowing the solution that maximal satisfies\nthe given rules.\nSatNet: Differentiable MAX-SAT relaxation via Semi-\ndefinitive programming (SDP) Here we consider the re-\nlaxation of the MAX-SAT problem as a SDP, (Wang et al.\n2019; Wang and Kolter 2019; Wang, Chang, and Kolter\n2017), where the MAX-SAT problem defined in Eq.( 1) is\nrelaxed by\n$\\min_{V \\in \\mathbb{R}^{K \\times (k+1)}} \\text{tr}(S^TS, V^TV)$\ns.t.$\\vert\\vert v_i \\vert\\vert = 1, \\forall i \\in {\\top,1,..., k}$\nwhere for each input variable $x_i$ we have an associated\nunitary vector $v_i \u2208 \\mathbb{R}^K$, with some $K > \\sqrt{2}k$ (Pataki\n1998).\u00b9 The special variable $v_{\\top}$ represents the true logic\nvalue and is used to retrieve the logic value of the solution.\n$S = [S_{\\top},s_1,...,s_k]/\\text{diag}(1/\\sqrt{4\\vert s_j\\vert}) \u2208 \\mathbb{R}^{m \\times (k+1)}$ is the\nmatrix representing the rules, while $V \u2208 \\mathbb{R}^{K \\times (k+1)}$ is the\nmatrix of the unitary vectors.\nMapping from the vector to the logic variable The logic\nvalue is recovered by its probability defined as $\\mathcal{P}\\{x\\} =\n\\arccos(\\frac{\\pi}{2} v_{\\top}^T v)$. The probability measures the radiant\nangle between the true vector and the variable vector, i.e.\n$v_i^T v_{\\top} = \\cos(\\pi x_i)$. We can recover the discrete value by\nevaluating the sign, i.e. $x_i = \\text{sign}(\\mathcal{P}\\{x\\})$.\nMapping from the logic variable to the vector To gen-\nerate the vectors from the logical values, we could use the\nfollowing mapping $v_i = \\cos(\\pi x_i) v_{\\top} + \\sin(x_i) \\mathcal{P}_{i}^{\\bot} v_{and}$,\nwhere $\\mathcal{P}_{i}^{\\bot} = I_K - v_i v_i^T$ is the projection matrix on the vector\n$v_i$ and $v_{and}$ is a random unit vector.\nSolution to the SDP relaxation The solution of Eq.(2) is\ngiven by the fix point (Wang et al. 2019) $v_i = -g_i/\\vert\\vert g_i \\vert\\vert$,\nwhere $g_i = V S^T s_i \u2013 ||s_i||^2 v_i = V S^T s_i \u2013 v_i s_i^T s_i$."}, {"title": "Experiments", "content": "To evaluate our proposed method, we perform graph classi-\nfication experiments on synthetic without node features and\nreal-world datasets with and without node features. For the\ngraph classification tasks, we compute the mean prediction\naccuracy across all graphs in an unseen test set. In all ex-\nperiments, we report the average result of three runs with\nthree different random seeds to obtain more stable results.\nTo better understand the robustness of the models, we also\nreport the standard deviation (indicated by the $\u00b1$ symbol).\nSince we want to evaluate the potential benefits of encoding\nthe topology into a fixed-sized bit string as described above,\nwe filter graphs with a size larger than 15 and 20 nodes. As a\nconsequence, our results are not directly comparable to prior\nworks. Dataset details can be found in Table 1."}, {"title": "Architectures", "content": "Similar to prior works (Zhang et al. 2019), we use a graph\nneural network with 2 convolutional layers with an optional\ndropout layer after each convolution. A mean pooling layer\nis used after the convolutional layers to aggregate the ob-\ntained node features into a single vector that represents the\nentire graph. The pooling is followed by an additional layer\nto map the obtained intermediate representation to the final\noutput. In contrast to our approach, which outputs only a sin-\ngle binary label, the GNN generates two outputs2 for a bi-\nnary classification task that indicates the probability of each\nclass.\nWe implement three different versions of the proposed ap-\nproach based on the previously presented fixed and learned\ngraph representations. The first version, GRNASC (adja-\ncency string canonicalized) uses only the canonicalized ad-\njacency string as graph representation. Second, we use a ver-\nsion that jointly learns the GRN and the GNN, which we de-\nnote as GRNGNN. Third, the architecture that uses a combi-\nnation of both representations is denoted by GRNASC+GNN.\nTo obtain a meaningful comparison with the reference\nmodel, we use the same GNN architecture as described\nabove. Instead of using an additional layer to make the class\npredictions, we use the reasoning module."}, {"title": "Hyperparameter optimization", "content": "To perform a hyperparameter optimization, we split the\ndatasets into training, validation, and test splits with sizes\nof 80%, 10%, and 10% of the dataset, respectively, and re-\nport the result of the configuration with the best validation\nresult for each run. For the GNN, we consider a hidden size\nin Set $\\{32, 64\\}$, a learning rate in Set$\\{0.01, 0.001\\}$, and test a\ndropout probability in Set $\\{0.0, 0.3\\}$, where a dropout prob-\nability of 0.0 means that no dropout is used. For the Satnet,\nwe consider a learning rate in Set$\\{0.1, 0.01\\}$ and a number\nof rules $m$ and auxiliary variables $aux$ in Set $\\{32, 64\\}$. To"}, {"title": "Synthetic datasets", "content": "To compare the expressiveness of GRN versus GNN, we\ngenerated synthetic graph datasets. We randomly generated\ngraphs with $n$ nodes. We used regular random graphs of fix\ndegree $d$ ($d$-regular) and Erdos-Renyi with edge probability\n$p$ (Bollob\u00e1s and B\u00e9la 2001). As prediction tasks we consid-\nered detecting the connectivity of the graph ($\\Box$), detect-\ning the presence of motifs: triangles ($\\triangle$), squares ($\\square$) and\n5-edges 4-nodes motif ($\\Box$). For 3-regular graphs we used\n$\\tau_\\Box = 2$,$\\tau_\\Box = 3$, while we used $\\tau_\\triangle = 6$, $\\tau_\\Box =$\n$6$,$\\tau_\\Box = 3$. As expected (see Table 3), GNN is not able\nto detect with accuracy the presence of specific motifs in the\ngraph. The GNN shows a more reasonable performance on\nthe connectivity test, probably exploiting other correlated in-\nformation. On the other hand, GRN exhibits superior perfor-\nmance, thus confirming that the use of topological informa-\ntion is necessary if the prediction task involves information\nrelated to the topology of the graph."}, {"title": "Real-world datasets without node features", "content": "Next, we perform experiments on real-world datasets with-\nout node features. To this end, we use the NCI1 and the PRO-\nTEINS datasets without node features. Furthermore, we use\nthe IMDB-BIN dataset. Since message-passing neural net-\nworks such as the GCN rely on node features for message\npassing, we use two different node feature alternatives. In\nthe first version, we initialize all nodes with the same, con-\nstant value. In the second version, we initialize the feature\nvector of all nodes with their node degree in a one-hot en-\ncoding. Using a one-hot representation of the node degree\nis a strong, hand-crafted feature for GNNs in many datasets.\nThe results of this experiment can be found in Table 2.\nThe results show that the GRNASC and GRNGNN are able\nto outperform the baseline approaches in the PROTEINS and\nthe IMDB-BIN datasets. Interestingly, GRNASC that does\nnot use the node degree as a feature performs best in PRO-\nTEINS, which suggests that the topology is highly informa-\ntive in this dataset. In NCI1, several methods show a similar\nperformance and GRNASC+GNN does not perform well."}, {"title": "Real-world datasets with node features", "content": "In the last experiment, we evaluate the performance of dif-\nferent approaches in the NCI1, NCI109, and PROTEINS\ndatasets with their original node features. The results in\nTable 4 show that the baseline GNN performs best in the\nNCI1 and NCI109 datasets, closely followed by GRNGNN.\nAdditionally using the topology in the GRNASC+GNN\nseems not to be beneficial in these two datasets. However,\nGRNASC+GNN performs best in the PROTEINS dataset,\nwhich suggests that the model can leverage the information\ncontained in the topology string. This observation confirms\nthe result from Table 2, which also showed that the topology\nseems to be important in the PROTEINS dataset."}, {"title": "Conclusions", "content": "In this work, we present an approach to applying Satnet to\ngraphs. Our experiments on synthetic datasets show that the\napproach can learn meaningful rules. Moreover, we find that\nrather simple GRNASC outperforms the GNN baseline. Ex-\nperiments on real-world datasets with and without node fea-\ntures are less conclusive.\nIn future work, other fixed representations can be used\ninstead of the adjacency string, which limits the applicabil-\nity of the approach to small and medium-sized graphs, or\nas an additional feature of the graph size is not too large. A\npromising idea is a representation based on the 1-WL graph\nisomorphism test. Furthermore, we noticed that jointly train-\ning the GCN and the SatNet can be difficult in practice be-\ncause both networks require a different learning rate. Hence,\nit will be important to develop a robust training schedule to\nimprove the training of the joint model. Inspiration for this\ncan be gained from training procedures that have been devel-\noped for GANs that also consist of two modules that depend\non each other."}]}