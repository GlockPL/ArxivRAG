{"title": "The Gradient of Algebraic Model Counting", "authors": ["Jaron Maene", "Luc De Raedt"], "abstract": "Algebraic model counting unifies many inference tasks on logic formulas by exploiting semirings. Rather than focusing on inference, we consider learning, especially in statistical-relational and neurosymbolic AI, which combine logical, probabilistic and neural representations. Concretely, we show that the very same semiring perspective of algebraic model counting also applies to learning. This allows us to unify various learning algorithms by generalizing gradients and backpropagation to different semirings. Furthermore, we show how cancellation and ordering properties of a semiring can be exploited for more memory-efficient backpropagation. This allows us to obtain some interesting variations of state-of-the-art gradient-based optimisation methods for probabilistic logical models. We also discuss why algebraic model counting on tractable circuits does not lead to more efficient second-order optimization. Empirically, our algebraic backpropagation exhibits considerable speed-ups as compared to existing approaches.", "sections": [{"title": "1 Introduction", "content": "Algebraic model counting (AMC) generalizes the well-known satisfiability task on propositional logic formulas to semirings (Kimmig, Van den Broeck, and De Raedt 2017). Using AMC various probabilistic inference tasks can be solved using the same unifying algorithm, including calculating marginals with evidence, entropy, and the most probable explanation (MPE). The principles of AMC are reminiscent of those underlying the sum- and max-product algorithms for probabilistic graphical models (Friesen and Domingos 2016).\nIn the current machine learning age, inference is often combined with learning. This is the focus of statistical-relational learning (De Raedt et al. 2016) and neurosymbolic AI (Garcez and Lamb 2023), which aim to integrate the inference and learning paradigms of logic, probability and neural networks. Our goal is to extend the unifying algebraic perspective that AMC brought to inference to the learning setting. To this end, we show how gradients can be generalized over semirings, by taking inspiration from differentiable algebra. This algebraic gradient provides a new toolkit of tractable operations, which can be used to realize a wide variety of learning algorithms, including gradient descent, expectation-maximization, entropy maximization, and low variance gradient estimation.\nFrom a theoretical perspective, the algebraic viewpoint provides a unifying framework and solver for computing many concepts that are used in learning. This is timely, as Marra et al. (2024) relate that \"[t]he situation in neurosymbolic computation today is very much like that of the early days in statistical relational learning, in which there were many competing formalisms, sometimes characterized as the statistical relational learning alphabet soup\". Ott et al. (2023) even state that \"the field of neurosymbolic AI exhibits a progress-hampering level of fragmentation\".\nFrom a practical perspective, we create a single optimized algorithm that subsumes many existing ones as special cases. For example, the forward-backward algorithm of Darwiche (2003) and the gradient estimator of De Smet, Sansone, and Zuidberg Dos Martires (2023) have a worst-case quadratic complexity in the number of nodes. However, these are special cases of our algebraic backpropagation algorithm which has linear complexity. We provide an efficient implementation for the algebraic backpropagation algorithm which takes the semiring properties into account. In our experiments, our implementation outperforms PyTorch and Jax, which are the de facto standard for neurosymbolic learning, by several orders of magnitude.\nFinally, we consider algebraic learning algorithms that rely on second-order information. Indeed, empirical evidence of e.g. Liu, Zhang, and Van den Broeck (2022) suggests that the existing first-order optimization methods for circuits might be suboptimal. Second-order optimization has quadratic complexity in general, which explains its lack of popularity in machine learning. However, specialized tractable circuit representations such as sd-DNNF have been developed which support many operations in linear time which are otherwise NP-hard (Darwiche and Marquis 2002). Unfortunately, we show that this tractability does not carry over to second-order derivatives and Newton's method is unlikely to be feasible in linear time on these tractable circuits.\nIn summary, we make the following contributions."}, {"title": "2 Preliminaries", "content": "We first review some relevant background on abstract algebra and propositional logic, before turning to algebraic model counting and algebraic circuits."}, {"title": "2.1 Algebra", "content": "Definition 2.1 (Commutative Monoid). A commutative monoid (A, \u2299, e) is a set A with a binary operation \u2299 : A X A \u2192 A and an identity element e \u2208 A such that the following properties hold for all a, b, and c in A.\n\u2022 Associativity: (a \u2299 b) \u2299 c = a \u2299 (b \u2299 c)\n\u2022 Commutativity: a \u2299 b = b \u2299 a\n\u2022 Neutrality of Identity: e \u2299 a = a\nAn element a \u2208 A is idempotent when a \u2299 a = a. A monoid is idempotent when all its elements are idempotent. Note that the identity e is always idempotent (e \u2299 e = e).\nDefinition 2.2 (Commutative Semiring). A commutative semiring (A, \u2295, \u2297, e\u2295, e\u2297) combines two commutative monoids (A, \u2295, e\u2295) and (A, \u2297, e\u2297) where the following properties hold for all a, b, and c in A.\n\u2022 Distributivity: (a \u2295 b) \u2297 c = (a \u2297 c) \u2295 (b \u2297 c)\n\u2022 Absorption: e\u2295 \u2297 a = e\u2295\nWe will from on now write monoid or semiring for brevity, and leave the commutativity implied. A ring is a semiring with additive inverses, meaning that for every a in A, there is an inverse element -a such that a \u2295 -a = e\u2295.\nA simple example of a semiring is the Boolean semiring, which has true (T) and false (\u22a5) as the domain and uses the logical OR and AND operations as sum and product, respectively."}, {"title": "2.2 Propositional Logic", "content": "Syntax We write V for a set of propositional variables. The infinite set of logical formulas Fv over the variables V is defined inductively as follows. A formula \u03d5 \u2208 Fv is either true T, false \u22a5, a propositional variable v \u2208 V, a negation of a formula \u00ac\u03d5, a conjunction of formulas \u03d51 \u2227 \u03d52, or a disjunction of formulas \u03d51 \u2228 \u03d52. Literals are variables or negated variables, and the set of all literals is denoted as L = V\u222a {\u00acv | v \u2208 V}.\nDefinition 2.3. Given a formula \u03d5, the conditioned formula \u03d5|x with x \u2208 L equals the formula \u03d5 where every occurrence of the literal x is replaced with T and every occurrence of \u00acx is replaced with \u22a5. When x \u2209 L, \u03d5|x is defined as \u22a5.\nSemantics An interpretation I \u2286 L is a set of literals which denotes a truth assignment. This means that for each variable v \u2208 V, either v \u2208 I or \u00acv \u2208 I. When a formula \u03d5 is satisfied in the interpretation I according to the usual semantics, we say that I is a model of \u03d5, written as I |= \u03d5. The set of all models of a formula is denoted M(\u03d5) = {I | I |= \u03d5}.\nFrom the algebraic view, the propositional formulas also form a semiring (Fv, \u2228, \u2227, \u22a5, T). As opposed to the BOOL semiring, the operations \u2228 and \u2227 are structural here and create new formulas from their arguments. This is also known as the free commutative semiring generated by the literals L."}, {"title": "2.3 Algebraic Model Counting", "content": "The task of algebraic model counting (AMC) consists of evaluating the models of a formula in a given semiring (Kimmig, Van den Broeck, and De Raedt 2017).\nDefinition 2.4 (Algebraic Model Counting). Given a semiring (A, \u2295, \u2297, e\u2295, e\u2297) and a labelling function \u03b1 : L \u2192 A which maps literals into the semiring domain, the algebraic model count is a mapping from formulas into the semiring domain.\nAMC(\u03d5; \u03b1) = \\bigoplus_{I \u2208 M(\u03d5)} \\bigotimes_{x \u2208 I} \u03b1(x)\nAMC generalizes many existing inference tasks. For example, the satisfiability (SAT) task, which asks whether a formula has a model, can be solved by AMC in the BOOL semiring. Model counting (#SAT), which asks how many models a formula has, is solved using the NAT semiring, and weighted model counting (WMC) using the PROB semiring. WMC is of particular significance, as probabilistic inference in e.g. Bayesian networks can be reduced to WMC (Chavira and Darwiche 2008).\nExample 1. Consider the formula \u03d5 = (x \u2228 y) \u2227 z over the set of variables V = {x, y, z}. This formula has three models: M(\u03d5) = {{x, y, z}, {\u00acx, y, z}, {x, \u00acy, z}}. So for the AMC, we get\nAMC(\u03d5; \u03b1) = (\u03b1(x) \u2297 \u03b1(y) \u2297 \u03b1(z))\n\u2295 (\u03b1(\u00acx) \u2297 \u03b1(y) \u2297 \u03b1(z))\n\u2295 (\u03b1(x) \u2297 \u03b1(\u00acy) \u2297 \u03b1(z))\nTo compute the model count, we evaluate in the NAT semiring with a constant labelling function \u2200x \u2208 L : \u03b1(x) = 1, giving AMCNAT (\u03d5; \u03b1) = 3. Similarly, setting all weights in \u03b1 to T in the BOOL semiring shows that \u03d5 is satisfiable. If we assign weights to the literals, e.g.\n\u03b1(x) = 0.5, \u03b1(\u00acx) = 0.5, \u03b1(y) = 0.1,\n\u03b1(\u00acy) = 0.9, \u03b1(z) = 0.8, \u03b1(\u00acz) = 0.2\nwe get AMCPROB (\u03d5; \u03b1) = 0.44 for the WMC.\nThe algebra of propositional logic (the Boolean algebra, not to be confused with the Boolean semiring) observes additional properties on top of the semiring such as idempotency. This difference between the Boolean algebra and"}, {"title": "2.4 Circuits", "content": "By reusing subformulas, circuits are a more compact representation for Boolean formulas (Vollmer 1999).\nDefinition 2.5 (Boolean Circuit). A Boolean circuit is a directed acyclic graph representing a propositional formula. This means that every leaf node contains a literal, and all other nodes are either \u2228-nodes or \u2227-nodes.\nAlgebraic circuits generalize Boolean circuits to semiring operations (Derkinderen and De Raedt 2020). Algebraic circuits in the PROB semiring are better known as arithmetic circuits.\nExample 2. The formula \u03d5 = (x \u2228 y) \u2227 z can be represented by the algebraic circuit below.\nThe tractability of queries on a circuit are related to the structural properties of that circuit (Darwiche and Marquis 2002; Vergari et al. 2021). For example, determinism is required to compute the model count in polynomial time. A circuit is deterministic if all the children of an \u2228-node are mutually exclusive, meaning they do not share any model. The circuit in Example 2 is deterministic.\nTransforming circuits to achieve structural properties such as determinism is achieved with knowledge compilation (Darwiche and Marquis 2002). Kimmig, Van den Broeck, and De Raedt (2017) proved that the properties of the semiring determine are linked to structural properties of the algebraic circuit. For example, determinism is needed to evaluate in a semiring that is not additively idempotent (of which model counting in the NAT semiring is an example). From an algebraic viewpoint, knowledge compilation tries to generate the smallest circuit representing the models M(\u03d5) within a specific semiring."}, {"title": "3 Conditionals as Gradients", "content": "Due to the inclusion of neural networks, neurosymbolic methods are typically trained using gradient descent. As probabilistic inference is differentiable, these neurosymbolic models are end-to-end trainable with off-the-shelf optimizers such as Adam (Kingma and Ba 2017). Some examples of this approach include the semantic loss (Xu et al. 2018) and DeepProbLog (Manhaeve et al. 2018). Other statistical-relational techniques are frequently trained by expectation-maximization (EM), which is also closely linked to gradients (Xu and Jordan 1996). For these reasons, we focus on the computation of gradients.\nIt is known that the gradient of probabilistic inference is the same as conditional inference (Darwiche 2003). In AMC with the PROB semiring, better known as weighted model counting, we have that\n\\frac{\u2202AMC_{PROB}(\u03d5; \u03b1)}{\u2202\u03b1(x)} = AMC_{PROB}(\u03d5|x; \u03b1)\nWe hence propose to generalize the notion of gradients to algebraic model counting as follows.\nDefinition 3.1. The AMC gradient is defined as the vector of AMC conditionals to each literal xi \u2208 L.\n\u2207AMC(\u03d5; \u03b1) = [AMC(\u03d5|x1; \u03b1), . . ., AMC(\u03d5|xn; \u03b1)]^T\nObserve that \u2207AMC is well-defined in any semiring, even when the semiring domain is discrete or otherwise non-differentiable such as in the BOOL or NAT semirings.\nLiteral vs Variable Gradients Our definition of \u2207AMC is the gradient towards a literal and not a variable. However, when maximizing the labels of the positive and negative literal separately, the global optimum is trivial. In practice, the positive and negative labels of a variable v are often linked, e.g. as \u03b1(v) + \u03b1(\u00acv) = e\u2295. In this case, it follows that the AMC derivative to a variable v is\nAMC(\u03d5|v; \u03b1) \u2295 AMC(\u03d5|\u00acv; \u03b1)"}, {"title": "3.1 Conditionals as Semiring Derivations", "content": "We further motivate the definition of \u2207AMC by its relation to differentiable algebra (Kolchin 1973). Differentiable algebra studies generalizations of derivatives, through functions which observe the same linearity and product rule as conventional derivatives.\nDefinition 3.2 (Semiring Derivation). A derivation \u03b4 on a semiring (A, \u2295, \u2297, e\u2295, e\u2297) is a map \u03b4 : A \u2192 A on itself which satisfies the following properties for all a and b in A.\n\u2022 Linearity: \u03b4(a \u2295 b) = \u03b4(a) \u2295 \u03b4(b)\n\u2022 Product rule: \u03b4(a \u2297 b) = (a \u2297 \u03b4(b)) \u2295 (b \u2297 \u03b4(a))\nMany properties that hold for conventional derivation are retained for semiring derivations. For example, \u03b4(e\u2295) = \u03b4(e\u2297) = e\u2295 in any derivation. We refer to e.g. Dimitrov (2017) for a summary of existing results on semiring derivations. Interestingly, semiring derivations themselves induce an algebraic structure, which gets generated by \u2207AMC.\nTheorem 1. Every derivation \u03b4 is a linear combination of the elements in \u2207AMC. More formally, \u2207AMC is a basis of the Fv-semimodule over D(Fv)."}, {"title": "4 Computing \u2207AMC", "content": "Conditioning a formula is straightforward, and hence computing \u2207AMC can be done with any off-the-shelf AMC solver. In other words, all the results of Kimmig, Van den Broeck, and De Raedt (2017) that link circuit and semiring properties transfer directly to \u2207AMC. Naively computing each element in \u2207AMC closely equals forward mode differentiation and was already proposed for WMC by Sang, Bearne, and Kautz (2005) to compute the conditionals of Bayesian inference.\nForward mode differentiation is well-known for scaling linearly in the number of input variables. Reverse mode differentiation, better known as backpropagation, is a dynamic programming algorithm that scales linearly in the number of output variables, which is usually constant. The backpropagation algorithm can easily be extended to work over semirings (Du et al. 2023), and can hence compute \u2207AMC.\nAs a dynamic programming algorithm, the downside of backpropagation is its memory use. More precisely, the naive backpropagation algorithm on a circuit has linear memory complexity in the number of edges the circuit. Shih et al. (2019) already demonstrated that when the semiring is a semifield, i.e. there is a division operation, this memory complexity reduces to linear in the number of nodes of the circuit. Given that the number of edges in a circuit can be up to the square of the number of nodes, this forms a substantial improvement. We show that this semifield requirement can be dropped while retaining the same memory complexity.\nTheorem 2. The backward pass on an algebraic circuit C has O(e) time and O(n) memory complexity, where e and n are the number of edges and nodes in C respectively.\nTheorem 2 is realised by Algorithm 1. This algorithm assumes that the forward pass already computed the values of sum nodes as \u03b1(n) = \\bigoplus_{c \u2208 children(n)} \u03b1(c) and product nodes as \u03b1(n) = \\bigotimes_{c \u2208 children(n)} \u03b1(c). Algorithm 1 then performs backpropagation in the usual way, going over the circuit from the root to the leaves and calculating the gradients of the children of each node using the chain rule. Concretely, the derivative towards a node n is\n\u03b3(n) = \\bigoplus_{p \u2208 parents(n)} \u03b3(p)\nwhen n has sum nodes as parents (lines 5-7), or\n\u03b3(n) = \\bigoplus_{p \u2208 parents(n)} \u03b3(p) \\bigotimes_{c \u2208 children(p)\\{n\\}} \u03b1(c)\nwhen n has product nodes as parents (lines 9-19).\nAlgorithm 1 relies on some dynamic programming to avoid recomputing the inner product in Equation 2 for every parent. For example, taking the gradient of c1 \u2297 c2 \u2297 c3 requires us to compute [c2 \u2297 c3, c1 \u2297 c3, c1 \u2297 c2]. Naively, computing these individually will result in quadratic time complexity. We avoid this using two cumulative products: the forward [e\u2297, c1, c1 \u2297 c2] and the backward [c3 \u2297 c2, c2, e\u2297]. Both of these cumulative products can be computed in linear time, and their element-wise product results in the desired gradient.\nFurther optimizations on Algorithm 1 are possible depending on the semiring structure to avoid the need to store the cumulative products in the backpropagation of the product nodes (lines 9-14). A first property that can achieve this is cancellation.\nDefinition 4.1. A semiring element a \u2208 A is (multiplicatively) cancellative if, for each b and c in A, a \u2297 b = a \u2297 c implies b = c.\nCancellation can be seen as a generalization of inverses. So when c = a\u2297b and a is cancellative we can write b = c \\oslash a. Indeed, the inverse c \\oslash a must exist as c = a \u2297 b, and the cancellation property furthermore assures that it is unique. The semiring of the natural numbers NAT form an"}, {"title": "4.1", "content": ""}, {"title": "5 Semirings for \u25bdAMC", "content": "We now explore the use of \u2207AMC for first-order optimization and related applications. In the probabilistic setting, we assume that our formula \u03c6 has weights \u03b1(x) \u2208 [0, 1] with \u03b1(\u00acx) = 1 \u2212 \u03b1(x). In other words, the variables correspond to Bernoulli distributions, and we have a probability distribution over interpretations p(I; \u03b1) = \u03a0\u03b9\u2208I\u03b1(l). The weighted model count can then also be seen as a probability, which we denote p(\u03d5; \u03b1) = AMCPROB (\u03d5; \u03b1).\nPROB semiring By construction, the algebraic gradient \u2207AMC in the PROB semiring is just the usual gradient.\n\u2207AMCPROB (\u03d5; \u03b1) = \u2207\u03b1p(\u03d5; \u03b1)\nLOG semiring Next, if we take the LOG semiring, we instead get the logarithm of the gradient. Note that this is different from \u2207\u03b1 log p(\u03d5; \u03b1), the gradient of the log probability.\n\u2207AMCLOG (\u03d5; \u03b1) = log \u2207\u03b1p(\u03d5; \u03b1)\nBackpropagation with the LOG semiring provides a more numerically stable way to compute gradients. It can also be applied for Expectation-Maximization (EM), as the expectation step on logic formulas reduces to computing the conditionals p(x | \u03d5) (Peharz et al. 2020).\np(x | \u03d5) \u221d exp (\u2207AMCLOG(\u03d5; \u03b1) + \u03b1 \u2212 AMCLOG (\u03d5; \u03b1))\nThe above also closely relates to the PC flow as defined by Choi, Dang, and Van den Broeck (2021).\nVITERBI semiring In the VITERBI semiring, the AMC gradient computes the maximum gradient over all models. Here, max computes the element-wise maximum of the gradient vectors of each model of \u03c6.\n\u2207AMCVITERBI (\u03d5; \u03b1) = max_{I \u2208 M(\u03d5)} \u2207\u03b1p(I; \u03b1)\nUse cases for this include greedily approximating the true gradient or gradient-based interpretability. Similarly, the TROPICAL semiring gives the log-space equivalent of the VITERBI semiring.\n\u2207AMCTROPICAL (\u03d5; \u03b1) = log max_{I \u2208 M(\u03d5)} \u2207\u03b1p(I; \u03b1)\nGRAD semiring AMC with the GRAD semiring computes the Shannon entropy (Li and Eisner 2009).\nH(\u03d5) = \u2212 \u03a3_{I \u2208 M(\u03d5)} p(I; \u03b1) log p(I; \u03b1)\nTaking the gradient of the AMC in GRAD hence results in the conditional Shannon entropy towards each literal.\n\u2207AMCGRAD (\u03d5) = [H(\u03d5 | x1), . . ., H(\u03d5 | xn)]\nThis conditional entropy is used as the information gain for learning, or for interpretability or regularization purposes. Several other statistical quantities such as the KL divergence can be framed as the difference between the entropy and conditional entropy, and can hence easily be computed from the GRAD semiring."}, {"title": "6 Second-Order Derivations", "content": "So far, we only considered first-order methods such as gradient descent. Second-order methods such as Newton's method take curvature into account by preconditioning the gradient using the inverse Hessian. For a function f parameterized by \u03b8, Newton's method updates the parameters as\n\u03b8 \u2190 \u03b8 + [\u2207^2f(\u03b8)]^{-1}\u2207f(\u03b8)\nComputing the full Hessian matrix has quadratic complexity, making the cost of second-order methods for machine learning often prohibitive. However, tractable circuits support a wide range of inference operations in polytime that are otherwise NP-hard (Vergari et al. 2021). The question hence poses itself whether tractable circuits could improve upon this quadratic complexity.\nSimilar to the gradient, we first generalize the Hessian matrix to AMC as follows.\n\u2207^2AMC(\u03d5) = \\begin{bmatrix}AMC(\u03d5|x_1, x_1) & ... & AMC(\u03d5|x_1, x_n)\n... & ... & ...\nAMC(\u03d5|x_n, x_1) & ... & AMC(\u03d5|x_n, x_n)\\end{bmatrix}\nWe ignore the labels \u03b1 here to simplify notation. By construction, \u2207^2AMC is the conventional Hessian in the PROB semiring. Note that although the algebraic Hessian is well-defined in any semiring, applying Newton's method requires that the Hessian can be inverted.\nBy using \u2207AMC in the GRAD semiring, we get a straightforward way to calculate \u2207^2AMC for the PROB semiring. Indeed, the GRAD semiring calculates partial derivatives using dual numbers, so in algebraic backpropagation this gives a row of the Hessian.\n\\frac{\u2202^2 AMC_{PROB}(\u03d5)}{\u2202\u03b1(x)\u2202\u03b1(y)} = \\frac{\u2202^2 AMC_{PROB}(\u03d5)}{\u2202\u03b1(x)\u2202\u03b1(\u00acy)}\nNext, we prove that \u2207^2AMC on tractable circuits still lacks structure, meaning that the expensive quadratic memory cost cannot avoided.\nTheorem 3. Representing \u2207^2AMC(C) of a circuit C over v variables has a O(v^2) memory complexity, even when C is smooth, decomposable, and deterministic.\nProof sketch. Take for example the binary Galois field as the semiring. Given a sequence of v^2 bits, we can now construct a formula \u03c6 such that the (flattened) Hessian equals this bit sequence. This means that, in general, the Hessian has no structure and a sub-quadratic memory complexity would imply lossless compression.\nOne might give two counter-arguments to the above theorem. First, gradient descent takes linear memory in the circuit size, but this circuit size might be up to exponential in the number of variables. Indeed, Theorem 3 does not rule out that the Hessian can be stored in linear memory complexity in the size of the circuit. Second, calculating the Hessian is typically not the true goal. The above result does not rule out the tractability of a matrix-free approach, where the preconditioned gradient [\u2207^2AMC(\u03d5)]^{-1}\u2207AMC(\u03d5) is computed directly without constructing the Hessian explicitly.\nHowever, even when considering these arguments, the existence of an algorithm for Newton's method which runs in linear time complexity in the circuit size remains unlikely.\nTheorem 4. Given a circuit C with n nodes, there cannot exist a circuit C' with size O(n) that computes the preconditioned gradient [\u2207^2AMC(C)]^{-1}\u2207AMC(C), even when C is deterministic, decomposable and smooth."}, {"title": "7 Related Work", "content": "Semirings & Inference The semiring perspective in artificial intelligence has its roots in weighted automata (Sch\u00fctzenberger 1961), as the weights of an automata can be defined over a semiring. These ideas have been carried over to various other paradigms such as constraint satisfaction problems (Bistarelli, Montanari, and Rossi 1997), parsing (Goodman 1999), dynamic programs (Eisner, Goldlust, and Smith 2005), database provenance (Green, Karvounarakis, and Tannen 2007), logic programming (Kimmig, Van den Broeck, and De Raedt 2011), propositional logic (Kimmig, Van den Broeck, and De Raedt 2017), answer set programming (Eiter and Kiesel 2020), Turing machines (Eiter and Kiesel 2023), and tensor networks (Goral et al. 2024).\nSemirings & Learning While semirings have mostly been applied to inference problems, some works also investigated learning in semirings. Li and Eisner (2009) introduced the expectation semiring, which can compute gradients and perform expectation-maximization. Pavan et al. (2023) studied the complexity of constraint optimization in some semirings. On the other hand, we provide a more general framework to compute derivations of any semiring.\nDarwiche (2001) already described a forward-backward algorithm for computing conditionals of a formula. Backpropagation on semirings has been described recently by (Du et al. 2023). Most similar to our work, Shih et al. (2019) already included an algorithm for computing the conditionals on algebraic circuits, which they call All-Marginals instead of \u2207AMC. This algorithm can be seen as a special case of our cancellation optimization, as all cancellative commutative monoids can be embedded in a group using the Grothendieck construction.\nNeurosymbolic Learning Several neurosymbolic methods rely on (probabilistic) circuits and hence could apply the algebraic learning framework we outlined. Some examples include DeepProbLog (Manhaeve et al. 2018), the semantic loss (Xu et al. 2018), and probabilistic semantic layers (Ahmed et al. 2022). Dickens, Pryor, and Getoor (2024) proposed another overarching view of neurosymbolic learning by framing it as energy-based models. On the other hand, our work focuses on algebraic circuits where inference and learning can be performed exactly."}, {"title": "8 Experiments", "content": "We implemented the algebraic backpropagation in a Rust library called Kompyle, and empirically demonstrate the runtime performance of the algebraic backpropagation algorithm on several semirings.\nSetup As a benchmark, we take 100 formulas of the 2021 Model Counting Competition (Fichte, Hecher, and Hamiti 2021) and compile them to d-DNNF circuits using the d4 knowledge compiler (Lagniez and Marquis 2017). We randomly generate weights for the formulas, with on average 1%\nResults"}, {"title": "9 Conclusion", "content": "We proposed a notion of gradients for algebraic model counting as conditional inference. We showed that many quantities of interest in learning, such as gradients, Hessians, conditional entropy, etc. can be seen as this algebraic gradient in different semirings. Furthermore, we introduced an optimized backpropagation algorithm for a broad class of semirings. Finally, we gave an indication that second-order optimization is still expensive on tractable circuits."}, {"title": "Appendix", "content": "Gradient Semiring\nFor completeness, the addition and multiplication operations in the GRAD semiring are:\n(a, b) \u2295 (c, d) = (a + c, b + d)\n(a, b) \u2297 (c, d) = (a \u00b7 c, a \u00b7 d + c \u00b7 b)"}]}