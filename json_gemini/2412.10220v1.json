{"title": "How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives", "authors": ["Timour Ichmoukhamedov", "James Hinns", "David Martens"], "abstract": "A rapidly developing application of LLMs in XAI is to convert quantitative explanations such as SHAP into user-friendly narratives to explain the decisions made by smaller prediction models. Evaluating the narratives without relying on human preference studies or surveys is becoming increasingly important in this field. In this work we propose a framework and explore several automated metrics to evaluate LLM-generated narratives for explanations of tabular classification tasks. We apply our approach to compare several state-of-the-art LLMs across different datasets and prompt types. As a demonstration of their utility, these metrics allow us to identify new challenges related to LLM hallucinations for XAI narratives.", "sections": [{"title": "I. INTRODUCTION", "content": "One of the most popular feature attribution explanation tools for narrow black-box ML models in the field of Explainable AI (XAI) is SHAP [1]. It combines ideas of using local linear approximations from LIME [2] with older game-theoretic Shapley values [3] to estimate for every feature its relative effect on the model output after weighing it in all possible coalitions with the other features. A SHAP explanation for a particular instance therefore consists of a table (or other pictorial representations) of all features together with their contribution to the output probabilities relative to some base level. Regardless of its widespread use, it must be noted that SHAP is not without flaws and several recent criticisms can be found [4, 5]. More importantly, SHAP or other feature importance explanations can contain quite dense or rather subtle information consisting out of dozens of values that might not always be easily interpretable by non-experts. In the context of XAI this observation naturally gives raise to the question: Can the relative importance of multiple features reveal a richer or more easily interpretable story than the sum of its parts?\nNarratives: This motivates the idea of complementing SHAP with a more laypeople oriented and user-friendly textual narrative that provides a plausible summary as to why and how the most important features contribute. Recent advances in Large Language Models (LLMS) open a novel opportunity to automate and scale this process without humans in the loop. Indeed, the idea of using LLM-generated narratives for XAI has been already explored in several recent works for tabular data [6-8] and similar developments are happening for graphs [9-11] and images [7, 12]. Initially, the task was viewed as a data-to-text task with the goal of strictly summarizing the quantitative XAI results to the user, and it has been shown that smaller models such as T5 or BART can be fine-tuned for this purpose [6]. However, more recently, larger models such as GPT4 have been demonstrated to produce rich and fluent narratives that were found to be accessible to users [7]. In particular, the narratives generated in [7] go beyond simple data-to-text summaries and leverage the broader knowledge base of modern LLMs to add context and reasoning into the story. Similar results have also been obtained in a more recent pilot study in [8], where the study participants preferred narratives over SHAP-based plots on a range of metrics.\nInterestingly, it has been also shown that even in the absence of a quantitative explanation such as SHAP, faithful narratives can still be gener-"}, {"title": "II. NARRATIVE GENERATION", "content": "In what follows we briefly describe the narrative generation step and refer to the paper repository\u00b3 for more details and code. To generate the narratives we follow a conceptually similar approach to [7], using a zero-shot prompt"}, {"title": "III. METHODOLOGY", "content": "Extractions: We propose to use an extraction LLM that is instructed to extract information from a given narrative as visualized in Fig. 3, which can then be validated using downstream metrics. The set of all N input features $\\mathcal{F}$ of the underlying prediction model, is always passed to the extraction model as part of the prompt. The extraction model then extracts a dictionary with keys $f_j \\in \\mathcal{F} : j \\in \\{0, ..., n - 1\\}$ representing the features present in the narrative. Keep in mind that in our case the generation model receives a truncated table with only the most important features and hence $n < N$. For every feature $f_j$ identified in the narrative, the following quantities are extracted:\n\u2022 Rank: integer $0 \\leq r_j \\leq n - 1$ that represents the importance rank of feature $f_j$ as implied in the narrative. In the present work we chose to use absolute-value based rankings and so $r = 0$ represents the feature that is described as most significant regardless of its sign. Typically, the extraction order of the features follows the rank and hence $r_j = j$.\n\u2022 Sign: $s_j \\in \\{-1,1\\}$, represents whether feature $f_j$ was described to contribute pos-"}, {"title": "Metrics for (SHAP) faithfulness", "content": "Validating the extracted information can be done with a range of downstream metrics of which we present possible examples. For SHAP faithfulness, the downstream metrics should measure Rank Agreement (RA), Sign Agreement (SA) and Value Agreement (VA) to the ground truth values r, s, v, where for example r represents the actual rank of the extracted feature fj. Metrics of this type have also been proposed in [23] and used in a recent related work using LLMS for XAI [13], although not for a narrative. Here, we will use the most straightforward version that simply measures the accuracy for every quantity"}, {"title": "IV. EXPERIMENTS", "content": "Experiments setup: For our experiments we use the same three binary classification datasets as in [7]: Fifa Man of the Match (predicting man of the match winner), German Credit Score (predicting good vs bad credit risk) and Student Performance Dataset (predicting student passing or not). The target classification model to be explained is always a Random Forest (RF) with the default scikit hyperparameters. The SHAP values are generated with the SHAP package using the Tree Explainer method. Per dataset, we select 20 instances (10 per ground truth class) for narrative generation.\nFor the generation models we consider: gpt-40 [31], Claude Sonnet 3.5 [32], Llama-3 70b"}, {"title": "A. Metrics validation", "content": "the extraction model on this classification task to ensure it performs as expected.\nAs a first test, we would like to confirm the reasonable expectation that the true negative (TN) rate is high and that if a narrative is not SHAP faithful, it will get flagged. This is indeed a property one would be interested in for practical applications. To do this we generate a set of 60 manipulated narratives using the same generation workflow as described in Sec. II, with an additional manipulation step where we permute the order of the SHAP values among the features in the truncated SHAP table before passing it to the generation model. The goal is to create a sample of narratives with mistakes in them, and the specific choice of manipulation is not that important for our purpose. For reference, this is a weaker version of the manipulation step discussed later in Fig. 7, with the main difference being that here the order swap is random and features are allowed to have the same sign after swap. Since undoing a random permutation should be rare, we will assume that all narratives generated in this way indeed have inaccuracies both in their rank and/or sign. We then pass all the manipulated narratives through an extraction model, and confirm that, as expected, after extraction all narratives were found to have errors in their rank and/or sign and would not have passed the classification. For both generation and extraction gpt-40 was used.\nSimilarly, to confirm that the false negative (FN) rate of this approach can also be expected to be low, we rely on our 60 human written narratives (20 per dataset). Since human narratives still contain some mistakes, we cannot use it in the same way as above and proceed with more care. We passed the narratives through the extraction model and found 8 out of 60 narratives to be flagged with a rank and/or sign error. Upon further inspection, 7 of these either contain full mistakes or ambiguous formulations from which a rank/sign cannot be determined, and only 1 was an actual false negative. Having already previously established that the false positive rate is very low, we assume that the remaining 52 narratives are then correct. Therefore, out of 53 correct narratives, only 1 was found to be a false negative."}, {"title": "Assumptions", "content": "As described in Sec. III, we also extract assumptions single sentences that summarize the implication or suggestion in the narrative as to why a particular feature could have contributed in a particular way. To validate the assumptions, we want to explore perplexity as a downstream metric. Computing the perplexity of a reference sentence requires access to the token output probabilities of a model, which is why we will compute the perplexity relative to the Llama-3 base model with 8 billion parameters (later Mistral-7b will be used as well). We also apply weight quantization to reduce the memory requirements when loading these models for the perplexity calculation. To validate the use of perplexity for this purpose, we extract 50 randomly chosen assumptions from various narratives across datasets, and manually manipulate them to make them sound more unreasonable, with two examples of the manipulations given in Fig. 5. We found that 3 out of 50 resembled factual statements related to the data (hence not actual assumptions) and proceed with the remaining 47.\nAs can be seen on Fig. 5, in this idealized setting, perplexity is indeed capable of detecting more unreasonable assumptions with good accuracy. The perplexity decreases only for five assumptions, and even then only by a small amount when compared to the average magnitude. Upon closer inspection, the assumption"}, {"title": "B. Results", "content": "Model Comparison: Having established that the various metrics exhibit a reasonable behavior, we can now present a quantitative"}, {"title": "V. CONCLUSION AND OUTLOOK", "content": "In this work we have explored several directions for metrics to evaluate \u03a7\u0391\u0399 narratives on feature attribution explanations across three categories: Faithfulness, Human Similarity and Assumptions. After introducing and motivating the metrics, we perform several experiments to validate them and demonstrate that they can be expected to work. Next, we show how the metrics can be used to compare different LLMs for the narrative generation task and find roughly comparable performance to human-written narratives. To have a reference point on how the metrics perform on faulty or manipulated narratives, we also explore the evaluation of narratives where the explanation that was provided to the LLM is tinkered with. We find that in this scenario all metrics consistently decrease (except perplexity), indicating their utility in detecting faulty narratives. Surprisingly, we find that for the manipulated narratives the LLMs exhibit a stronger tendency to self-correct the sign of the feature contribution, providing an immediate use case for our metrics. We hypothesize that this could be a similar phenomenon to parametric knowledge bias where LLMs tend to correct context from the prompt that is in conflict with their internal knowledge base. This observation highlights a future challenge of LLM hallucinations for narratives on more subtle explanations.\nSpecifically for the various metrics we also identify the following interesting future directions (across the three categories):\n\u2022 Faithfulness: The metrics for faithfulness with the extraction-based pipeline work as expected and seem ready to use. However, they are by no means exhaustive, and it would be interesting to explore various additional metrics for this category.\n\u2022 Human Similarity: We have demonstrated that modern embeddings contain sufficient resolution to match identical narratives (outperforming BLEURT even with a simple cosine similarity) and separate batches of unfaithful narratives from faithful ones. We believe that the most interesting next step would be to train metrics on top of the embedding model (or fine-tuning) that specialize in properties of XAI narratives. In particular, it seems relevant to move towards user preference and train automated metrics of narrative plausibility.\n\u2022 Assumptions Plausibility: We find that in an idealized setting for our validation experiments, perplexity relative to a smaller LLM is able to individually identify unreasonable assumptions. However, when applying this approach to the assumptions generated by LLMs in the manipulated narratives, the results are far less conclusive and perplexity does not appear to yield consistent behavior across models. Although this effect can be attributed to the LLMs trying to make the assumptions sound more plausible than in the idealized setting, it also indicates the need to improve assumption measurements beyond perplexity."}]}