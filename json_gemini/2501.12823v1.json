{"title": "To Measure or Not: A Cost-Sensitive, Selective Measuring Environment for Agricultural Management Decisions with Reinforcement Learning", "authors": ["Hilmy Baja", "Michiel Kallenberg", "Ioannis N. Athanasiadis"], "abstract": "Farmers rely on in-field observations to make well-informed crop management decisions to maximize profit and minimize adverse environmental impact. However, obtaining real-world crop state measurements is labor-intensive, time-consuming and expensive. In most cases, it is not feasible to gather crop state measurements before every decision moment. Moreover, in previous research pertaining to farm management optimization, these observations are often assumed to be readily available without any cost, which is unrealistic. Hence, enabling optimization without the need to have temporally complete crop state observations is important. An approach to that problem is to include measuring as part of decision making. As a solution, we apply reinforcement learning (RL) to recommend opportune moments to simultaneously measure crop features and apply nitrogen fertilizer. With realistic considerations, we design an RL environment with explicit crop feature measuring costs. While balancing costs, we find that an RL agent, trained with recurrent PPO, discovers adaptive measuring policies that follow critical crop development stages, with results aligned by what domain experts would consider a sensible approach. Our results highlight the importance of measuring when crop feature measurements are not readily available.", "sections": [{"title": "Introduction", "content": "As the global population continues to grow and the impacts of climate change become more prominent, optimal farm management decisions play a crucial role in our future sustenance. Effective farm management practices are key to not only ensure food security but also mitigate the apparent environmental risks related to agriculture (Lipper et al. 2014). Environmental risks come from mismanagement of farming activities, such as over-applying fertilizer and pesticides (Mart\u00ednez-Dalmau, Berbel, and Ord\u00f3\u00f1ez-Fern\u00e1ndez 2021). In practice, farmers rely on their experience and in-field observations to make better-informed management decisions in order to minimize costs and reduce environmental impacts. For instance, before fertilizing, a farmer might measure the soil (or leaf) nitrogen content and based on that decide when and how much fertilizer to apply (Berghuijs et al. 2024).\nHowever, acquiring a large amount of in-field agricultural data is labor intensive, time-consuming and expensive (Wu et al. 2022a). In most cases, the cost and inconvenience of collecting and processing the data outweigh its informational usefulness for the farmer (Thompson et al. 2019). Furthermore, this issue is compounded in regions with low data availability and high data collection costs, as the infrastructure required for gathering such data is limited, presenting a large hurdle for applying data-driven optimization (Cravero et al. 2022). By integrating data-collection recommendations as part of decision-making, data collection can be optimized and executed only when it is the most beneficial for the farmer. Specifically, it is of importance to have a system that simultaneously recommends optimal moments in a growing season to obtain measurements of crop states for the objective of cost-effective and optimal management.\nThe unrealistic assumption that feature observations are readily available has prompted recent work in reinforcement learning (RL) pertaining to active feature measuring for balancing measurement costs (Bellinger et al. 2021a). Previous work has shown early success in this setting, however, it assumed uniformity in the feature costs (Yin et al. 2020). In the real-world, recognizing that certain features are more expensive to measure than others is essential, as it potentially influences the policies that an RL agent learns.\nIn this work, we experiment on the effect of cost in a measure-and-control paradigm and design an RL environment in which an agent interacts with a crop growth model (CGM) environment during a growing season. It is important to acknowledge that crop features have different measuring costs. The ability to optimize fertilization with fewer crop state measurements is important to lower the hurdle of applying data-driven methods for improving agricultural management activities. We summarize our contributions below:\n\u2022 We propose an RL paradigm of costly measurements for crop management, incorporating crop state measurements as part of decision making;\n\u2022 We design and provide code for an RL environment, coupled with the WOFOST (Berghuijs et al. 2024) crop growth model, that allows an agent to simultaneously learn a control policy (fertilization, irrigation, etc.) and"}, {"title": "Problem Setup", "content": "In this section, we formalize the setting of this work. Moreover, we elaborate in detail the challenge in crop management and the technical implementation of the RL environment."}, {"title": "MDP Setting", "content": "Crop management problems involve sequential decision-making, which can be formalized into a Markov Decision Process (MDP). A standard MDP can be described with the tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, T, R, \\gamma)$, where $\\mathcal{S}$ is the state space and $\\mathcal{A}$ is the action space. $T$ and $R$ are the environment's transition function $T(s_{t+1}|s_t, a_t)$ and reward function $R(s_t, a_t, s_{t+1})$, respectively. In crop management problems, as with many real-world environments, the agent is not privy of the complete environment state. This is where most real-world MDPs fall into: partially observable MDPs (POMDPS, Cassandra 1998). A POMDP is formalized into the tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, T, R, \\mathcal{O}, \\gamma)$. In addition to the standard MDP elements, $\\mathcal{O}$ is introduced as the space of possible observations $o \\in \\mathcal{O}$, which has an observation function of $O(o_t = O(s_t, a_t))$.\nCoined by Yin et al. (2020), active feature acquisition POMDPS (AFA-POMDPs) describe an MDP setting where an agent can selectively acquire features at different time steps. It is characterized with the tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, T, \\mathcal{O}, R, C, \\gamma)$. AFA-POMDPs extend POMDPs by extending the action space $\\mathcal{A} = \\mathcal{A}_c \\times \\mathcal{A}_m$ and adding a cost function $C$. The action space consists of the control actions $\\mathcal{A}_c$ concatenated with measuring actions $\\mathcal{A}_m$, and its Cartesian product shows the possible action outputs. An action output is denoted as $a_c \\in \\mathcal{A}_c$ and $a_m \\in \\mathcal{A}_m$. $C$ refers to the cost of unmasking observations. In our setup, we assume that $a_m$ contains a fixed number of features of size $N_m$.\nTo allow for cost-sensitive, selective measurements, we adapt the above framework by defining:\n$C(a_m) = \\sum_{i=0}^{N_m} c_i a_{m_i}$\n$c$ is a vector that lists explicit costs for individual features $i$, denoted by $c_i$. Despite the action space extension, the transition function is similar to the standard MDP with a slight change $T(s_{t+1}|s_t, a_c)$, because we assume measurement actions do not affect state transitions. The objective of this setup is as follows:"}, {"title": "Crop Management", "content": "As with many biological problems, crop management is highly complex with many factors that influence the process, outcome and yield of the crop growth. Generally, in the agricultural domain, this problem is denoted as $G \\times E \\times M$, which means that crop processes are influenced by the Genotype, Environment and Management (Mart\u00edn, Olesen, and Porter 2014). In this work, we focus on arable (i.e., open-field) rain-fed winter wheat. Therefore, we do not have any control of the environmental conditions. We assume the soil type is uniform through the whole farm. Moreover, we assume the absence of yield-reducing factors such as pests and diseases. Next, we opt to focus on one variety of winter wheat typically grown in the Netherlands (and western Europe), so we keep the genotype factor fixed. In the rain-fed field, we do not apply irrigation. This leaves us only with control over nitrogen (N) fertilization for the management.\nN is a yield-limiting factor for crops (Chukalla et al. 2020). Knowing the crop or soil state before applying N fertilizer could enable timely fertilization actions, avoiding waste of resources and detrimental environmental effect. Moreover, in dry years, especially for rain-fed environments, the crop experiences water stress and water takes over as the yield-limiting factor, regardless of the N fertilization (Eck 1988).\nThe RL environment we developed is an interface of the Python Crop Simulation Environment (PCSE, de Wit 2023), which houses several crop growth models. In this work, we utilize the World Food Studies (WOFOST, Van Diepen et al. 1989; De Wit et al. 2019) CGM. WOFOST is a process-based CGM that simulates crop growth. It is capable of simulating nitrogen- and water-limited yield production, including the processes that are calibrated to a certain crop variety and location (Berghuijs et al. 2024). WOFOST has been proven to be a robust and reliable CGM, validated through its integral role in the European MARS crop yield forecasting system (Van der Velde and Nisini 2019)."}, {"title": "RL Environment", "content": "RL Spaces: The observation space dictates the features that the agent can observe and measure. Table 1 shows the features of the CGM with continuous values for the crop and weather features. Additionally, we add another feature with continuous values called 'Random' that acts as a distraction feature, which will be explained in the Experiments and Results section. Figure 1 shows a schematic diagram describing the observation and action space processing.\nThe action space is a vector of discrete and binary values. The discrete value corresponds to the agent's control action of applying nitrogen to the crop, which are 7 levels: {0, 10, 20, 30, 40, 50, 60}kg/ha. The binary values correspond to the agent's action of measuring. Table 1 shows six features that we define as 'measurable'. The environment has a weekly time step. We aggregated the time series data following Kallenberg et al. (2023): the sequence of weather with a length of 3 \u00d7 7 (i.e., daily rain, temperature and solar irradiance) was processed into an average pooling layer, resulting in a vector size of 3 \u00d7 1. The crop features that had a length of 6 \u00d7 7 were shrunk to 6 \u00d7 1 by taking its last entry. The Random feature is also appended to the end of the crop feature at this moment. The vectors are then concatenated and flattened, a vector of masks corresponding to the measuring action of the agent at that time step is subsequently appended to the vector, resulting in a vector with a length of 16 features.\nConstraints: The nitrogen management regulations in the Netherlands limit the amount nitrogen fertilizer that the farmers are allowed to apply (Flach and Selten 2021) due to potential detrimental environmental impact. This limit also affects the potential production of the yield. Hence, to keep realistic considerations, we impose a constraint for the agent to apply a maximum total of 200 kgN/ha.\nReward function: The agent's main objective is maximizing yield by balancing the information obtained from costly measurements. We designed a natural financial reward function that assumes one unit of reward to be equal to 1 kg of winter wheat yield. R follows:\n$R_t = (TWSO_t - TWSO_{t-1}) - \\beta N_t - D - \\sum_{i=0}^{N_m} c_i a_{m_{i t}}$\nwhere t is a weekly time step, TWSO (total weight storage organ) is the wheat yield in kg/ha, and N is the amount of Nitrogen in kg/ha. The parameter $\\beta$ is a ratio of the price of 1 kg of wheat yield and 1 kg of N fertilizer, so we set $\\beta$ = 2 to mimic the respective prices (Wageningen Economic Research 2023a,b). D is a deployment cost that penalizes the agent for going out the field to apply N, with a penalty of 10 if $N_t > 0$. $c_i$ is the cost of measuring a specific feature (i.e. LAI, SM, etc.) and $a_{m_{it}}$ denotes a measurement action $a_{m_i}$ at time step t. We describe the rationale for the costs in the next section.\nRL agent. For this experiment, we employ a PPO agent with recurrent networks (LSTM-PPO), as implemented by the library Stable Baselines 3 (Raffin et al. 2021). We take"}, {"title": "Design Rationale and Assumptions", "content": "Feature selection and observation. WOFOST has a plethora of crop states/features that describe various crop and soil processes. The RL agent only observes a subset of these features, which makes the environment inherently partially observable. Though, a large portion of the features are highly correlated and redundant. Hence, we handpicked 6 crop features that are considered most important for the task of nitrogen fertilization, out of which, 5 are measurable (Table 1). TAGP describes the biomass of the crop, which is highly correlated to yield. LAI describes the area of leaves, thus explaining the photosynthesis capabilities of the crop. NuptakeTotal describes the total amount of N the crop took from the soil. SM describes the moisture around the crop's root, related to potential water stress for the crop. NAVAIL describes the amount of N in the soil. We give the agent full access to the remaining crop feature: development stage (DVS). This is akin to the farmer going out and visually checking a crop if it has emerged from the ground (DVS \u2265 0), has reached the flowering stage (DVS \u2265 1) or matured (DVS = 2). DVS grows monotonically from -0.1 to 2, hence it can be used to infer not only the stages of crop growth, but also the progression of time during the crop growth period. So, we also use DVS as a proxy for time and let the agent learn critical crop stages for adaptive measuring and fertilizing strategies. The agent also has full access to the weekly weather information, as this akin to a farmer checking the weather of last week from their nearest weather station.\nFeature measurement costs. We defined explicit cost to obtain measurements for different features. We base the cost of an estimation on how expensive it is to obtain a measurement of the feature (shown under Table 1). SM and LAI are cheaper since it is possible to measure through sensors (Hummel, Sudduth, and Hollinger 2001), various remote sensing (Hasegawa et al. 2010; Schmugge 1983) or optical methods (Br\u00e9da 2003). NuptakeTotal can be measured with non-destructive sensors (Ulissi et al. 2011), which requires considerable labour to do for a whole field. NAVAIL can be measured with soil tests (Dahnke and Johnson 1990), requiring the soil to be sent to a lab, consequently incurring"}, {"title": "Experiments and Results", "content": "In this section we would like to answer the following questions:\n1. How does different measurement costs affect a recurrent PPO agent's ability to optimize yield?\n2. How adaptive is the RL agent's measuring policies between cold and hot years?\n3. Does the agent learn to ignore features with no information?\nTraining conditions: We conduct experiments where the recurrent PPO agent has a joint task of measuring and applying N fertilizer in the WOFOST CGM environment. Training was done in with semi-fine soil and climate conditions of the Netherlands with 3 coordinates: (52, 5.5), (51.5, 5), (52.5, 6) (\u00b0N,\u00b0 E) obtained from the NASA POWER weather dataset (Sparks 2018). The agent trained on the odd years from 1990 to 2022 (n=16), and we save the even years for evaluation. We set the simulation length to be fixed for 47 weeks, from sowing (October 1st) until harvesting (September 1st). We evaluate the agent with even years (n=16), and weather from the Netherlands with coordinates (52.5, 5.5) (\u00b0N,\u00b0 E). In total, one evaluation run contains 16 episodes.\nThe initial soil conditions (i.e., how much moisture and N nutrients were in the soil when sowing) affects the agent's learned policy. An agent will learn that the soil conditions always start with a similar amount, potentially remembering this and discouraging it from measuring. Hence, to add per episode variability to the RL environment, for soil moisture and nitrogen content, we set it with values from a randomized normal distribution generator, with a mean and standard deviation of 15 (in kg/ha for nitrogen content). The values are clipped to be bounded between 0 and 100. The generator is seeded, therefore repeatable across different seeds.\nTraining scenarios: To understand the effect of measuring cost in maximizing yield, we train agents with several cost scenarios. First, a Realistic cost scenario, which was explained in section Design Rationale and Assumptions and the explicit costs shown in Table 1. Second, a Flat-cost scenario, where each feature has a cost of 10 (kg of wheat) to measure, a bit lower than the average cost of Realistic features. Third, a No-cost scenario, where the agent is free to perform measure actions. And fourth, an Exp-cost scenario, a very expensive flat cost of 60 for measuring each feature, forcing the agent to converge to a policy of maximizing reward without measuring. To compare the performance of the RL agents to a simple fertilizing policy, we add a non-measuring fixed-fertilizing policy that fertilizes 3 times in fixed dates, each two months apart from January to May. Each fertilization has an amount of 66.67 kg/ha, with a cumulative total of 200 kg/ha adhering to the constraint"}, {"title": "Discussion and Limitations", "content": "Realistic cost policy: The scenarios No-cost and Flat-cost achieved the best performance as they allow access to relatively complete crop state information (Wu et al. 2022b). However, these cost scenarios are unrealistic; crop state measurements are not always readily available, nor do they have uniform cost. The Realistic scenario agent, which had to pay a realistic cost for measuring features, still managed to perform well. The agent frequently measured LAI, which is strongly related to nitrogen leaf content in different crop stages (Yin et al. 2003), and SM (soil moisture), which is a major yield-limiting factor (Day and Intalap 1970; Fahad et al. 2019). With these results, we demonstrate the importance of measuring when crop feature data is not readily available or difficult to obtain consistently.\nRL Algorithm: We employed LSTM-PPO for our problem setup. Yin et al. (2020) utilize LSTM-A3C to train their RL policy, of which they employ a seq-VAE that is pre-trained with fully observable features that they feed to the LSTMs hidden states. In contrast, we do not employ pre-training in our approach and let the agent learn from a tabula rasa. A performance increase is probable if we pre-train our LSTM-PPO networks with fully observable features. Nevertheless, we show that our simple approach obtains measuring policies that enable good yield.\nAssumptions: In section Design Rationale and Assumptions we explain the rationale of our design choices and defined some limiting assumptions of our proposed solution. Future work may build on our findings by relaxing these assumptions such as incorporating temporal delays or adding noise to measurements.\nScalability: A limitation of our approach is the need to possibly retrain the RL agent for different sites and setups. Scaling up is possible by including various sites in the agent's training. Though, it is challenging from both domain perspectives: generalization remains a challenge in RL and ML (Cobbe et al. 2019; Li et al. 2022); and site-specific N management demands detailed site knowledge of in-field N soil variability for accurate recommendations (Schut and Giller 2020). In this work, we evaluate our approach in a representative case study with a well-calibrated CGM, thus we leave further exploration of this challenge for future work.\nSim2real gap: The experiments in this work are done in silico, hence there exists a gap between simulation and reality. Various things are not simulated in modern CGMs: yield-reducing factors such as pests and diseases (Donatelli et al. 2017), genetic variability (Hirel et al. 2007), among others, which introduce distribution shifts between simulation and reality. RL algorithms that are robust to distributional shifts (Turchetta et al. 2022) and randomization (Tobin et al. 2017) can help narrow this gap. Nevertheless, a CGM can achieve high accuracy if calibrated specifically to the year and conditions of the location it is simulating (Ahmed et al. 2020; He et al. 2017). For future work we intend to test and evaluate our developed system in field trials."}, {"title": "Conclusion", "content": "In this work, we propose an RL approach of integrating data collection to decision making by obtaining a measuring policy that balances measuring costs with fertilization. Inspired by the problem of difficult and costly data collection in agriculture, we design an RL environment with realistic considerations by adapting the framework of AFA-POMDPs. We evaluate our approach in silico in a case study in the Netherlands, with WOFOST, a thoroughly validated CGM. Our test includes different cost scenarios. We find that the RL agent discovers adaptive measuring policies that coincide with critical crop development stages and learns that some features are more valuable than others. While cost does indeed affect the discovered measuring policy, with realistic costs the agent is still capable of achieving good yield by utilizing cheaper measurements.\nOur work highlights the importance of measuring when crop feature measurements are not readily available. By integrating measurement recommendations in the decision making process, we can minimize unnecessary data collection. This approach is an important step to lower the hurdle of applying data-driven optimization for crop management. Ultimately, our work allows for the ability to optimize yield while reducing required data samples, further realizing better crop management policies for mitigating adverse environmental impact and sustaining global food demands."}]}