{"title": "Transformer Vibration Forecasting for Advancing Rail Safety and Maintenance 4.0", "authors": ["Dar\u00edo C. Larese", "Almudena Bravo Cerrada", "Gabriel Dambrosio Tomei", "Alejandro Guerrero-L\u00f3pez", "Pablo M. Olmos", "Mar\u00eda Jes\u00fas G\u00f3mez Garc\u00eda"], "abstract": "Maintaining railway axles is critical to preventing severe accidents and financial losses. The railway industry is increasingly interested in advanced condition monitoring techniques to enhance safety and efficiency, moving beyond traditional periodic inspections toward Maintenance 4.0.\nThis study introduces a robust Deep Autoregressive solution that integrates seamlessly with existing systems to avert mechanical failures. Our approach simulates and predicts vibration signals under various conditions and fault scenarios, improving dataset robustness for more effective detection systems. These systems can alert maintenance needs, preventing accidents preemptively. We use experimental vibration signals from accelerometers on train axles.\nOur primary contributions include a transformer model, ShaftFormer (SF), designed for processing time series data, and an alternative model incorporating spectral methods and enhanced observation models. Simulating vibration signals under diverse conditions mitigates the high cost of obtaining experimental signals for all scenarios. Given the non-stationary nature of railway vibration signals, influenced by speed and load changes,\nour models address these complexities, offering a powerful tool for predictive maintenance in the rail industry.", "sections": [{"title": "1. Introduction", "content": "Train operations rely heavily on the reliability of their wheelsets, with axles being crucial for supporting train weight and transmitting power. Malfunctions can lead to severe consequences such as accidents, derailments, and safety risks, often due to fatigue or cracks in the axles, which can result in loss of life and property damage (Vagnoli et al., 2018).\nTo address these challenges, we propose a novel transformer model for predictive maintenance that forecasts a set of axle vibration signals obtained experimentally under different conditions of load, speed and flaws. In the field of predictive maintenance, forecasting is a crucial requirement for detecting faulty signals and simulating new signals which cannot be obtained easily. Through monitoring relevant variables, it is a natural solution to axle reliability issues.\nPrevious research has explored using various sensors to detect wheel flaws (Alemi et al., 2018) and predict failures in critical rail car components (Li et al., 2014; Wang et al., 2022). For railway axles, studies have utilized model-based signals from analytical (G\u00f3mez et al., 2016b) or numerical models (Hassan et al., 2016), as well as experimental signals from scaled rigs (G\u00f3mez et al., 2016a) or accelerometers on real systems. Despite the potential of these methods, bogie test rigs are rare and costly, limiting the scope and generalizing capabilities of the results (Castej\u00f3n et al., 2016; G\u00f3mez et al., 2016b,a; Rolek et al., 2016; G\u00f3mez et al., 2023).\nThe current trend in maintenance automation, known as Maintenance 4.0, leverages Machine Learning (ML) techniques. Recently, there has been growing interest in applying Deep Learning (DL) for condition monitoring and predictive maintenance in the rail industry. Studies have demonstrated the use of neural networks to analyze vibration signals and predict axle failures (G\u00f3mez et al., 2016c, 2018, 2020). For example, Galdo et al. (2023) explored classifying crack conditions on railway axles using a differential convolutional classification network. However, existing studies often overlook the nonstationary nature of railway vibration signals, which are influenced"}, {"title": "", "content": "by speed, load, and environmental changes (Ye et al., 2022; Krummenacher et al., 2018). This oversight hampers the development of models that can generalize across diverse operating conditions and structural variations.\nMoreover, many current methods rely on traditional ML algorithms or simplistic neural network architectures that may struggle to capture the complex relationships present in railway vibration data (Serin et al., 2020). Some research has proposed DL models for machine diagnosis, but these often lack domain-specific adaptations tailored to railway systems (Zhao et al., 2019; Wang et al., 2021). Simple transformer models have been applied to wheel defect classification tasks (Wang et al., 2022), yet they remain plug-in solutions rather than tailored DL models.\nThis study aims to address these gaps by proposing the SF model and the Spectral ShaftFormer (SSF) model, which are variants of the original Transformer (Vaswani et al., 2017). These models implicitly consider the non-stationarity of the data and effectively model the signals. By incorporating domain knowledge from railway engineering, our methodology is designed to fit within the monitoring and maintenance tasks in the industry. Moreover, both models are able to generate signals under different environmental conditions such as load and speed. Potential applications of this study include outlier detection, missing data imputation, signal forecasting, and data augmentation."}, {"title": "2. Methodology", "content": "2.1. Data collection: experimental setup\nThe experimental acquisition of vibration signals from railway axles involved investigating the dynamics of cracked rotors using a specially designed experimental setup, as detailed in previous studies (G\u00f3mez et al., 2018, 2020). The experimental system was constructed within a bogie test rig, consisting of a fixed bench and a drive system to roll the tested axle, as shown in Figure 1.\nSix accelerometers were placed, with three in each axle box (LHS and RHS) of the tested axle, measuring vibrations in three orthogonal directions: tangential (in two perpendicular directions), vertical (aligned with the gravitational force) and axial (parallel to the direction of the track). In addition, the rig featured a loading system that employs hydraulic actuators to apply a consistent vertical load to the bogie via a chain mechanism. Throughout\ntesting, the selected load remained constant to accurately replicate real-world operating conditions.\nThe experiments involved testing three distinct Wheelset Assemblies (WAs), labeled WA1, WA2 and WA3, resulting in the creation of three individual datasets. Each WA was subjected to testing under two different combinations of speed and load, for clockwise and counterclockwise rotations. The applied load and speed were kept constant throughout the tests and are outlined in 2, offering a comprehensive overview of all test conditions.\nMonitoring these signals on a bogie instead of a wheel set produces very rich data, as the full bogie includes not only the wheels and axle, but also the suspension systems and load, reproducing with high fidelity the reality.\nThe test rig comprises a fixed bench and a drive system for rolling the tested axle and also incorporates a loading system that uses hydraulic actuators that apply a vertical load to the bogie through a chain. During the test, the load remains constant, replicating in real-world situations. Table 1 shows the tested configurations.\nFirst, each axle was tested under control conditions. Following this, without disassembling the axle, different sizes of flat-front transverse cracks were machined at the central position of the axle. A total of three different depths of crack were introduced, resulting in testing four different crack conditions."}, {"title": "2.2. The SF Model", "content": "We present the SF, a model that extends the Informer (Zhou et al., 2021) model by means of an Enhancion of the Locality of Transformer (ELT) preprocessing step. ELT identifies the relevant information from the 1-D input vibration signal by passing it through a set of Convolutional Neural Network (CNN) and augmenting the dimension of the input signal.\nThe SF incorporates the ProbSparse attention mechanism introduced by the Informer, a method used to address the challenges of processing long sequences efficiently. It reduces computational complexity by sparsifying the attention matrix, selectively focusing on more relevant parts of the input data, rather than computing attention scores for every possible pair of inputs. This method helps to manage memory more effectively and improve computational efficiency, particularly suitable for models dealing with long input sequences, such as vibration signals, as it avoids the quadratic complexity typical of standard self-attention mechanisms. It creates a powerful encoder memory, ensuring that it contains the relevant data from the history of the signal. Figure 2 shows the general architecture of this model.\nEncoder. The encoder contains two different blocks, shown in Figure 3, which are the self-attention block and the convolution block. The first block is composed of the ProbSparse self-attention layers, from which we obtain the\nattention values. We make two copies of these values. One copy will go through a down-sample convolution layer and then an up-sample convolution layer with the intention of learning the information and then being able to reproduce it. Then, the two copies, the residual and the modified, are concatenated and processed by the convolutional block. After a convolutional layer, a pooling layer is applied to down-sample the data obtaining the memory. The memory maintains the general information of the signals and is passed to the decoder.\nDecoder. We use a vanilla Transformer decoder (Vaswani et al., 2017) to process this memory. The SF is posed as an autorregressive generative model. The input to this model is the signal in the time domain. For each time step in the future, the model generates a probability distribution over the next value of the signal. The loss function used is the Mean Squared Error (MSE). We use a Normal distribution as the output distribution, parameterized by"}, {"title": "2.3. The SSF", "content": "In time series forecasting, working in the frequency domain offers valuable advantages by providing insights into the periodic patterns, noise filtering, and spectral analysis of the data. The transformation of time-series data into frequency domain is achieved through the Short Time Fourier Transform (STFT). It allows the identification of dominant frequencies and helps in the extraction of relevant features, such as amplitude and phase information. Additionally, the frequency domain allows for the explicit modeling of periodic components that may be challenging to capture in the time domain alone.\nThis model adopts the SF model as a base to build upon. We do the following changes over the SF:\n\u2022 Work in the frequency domain and select the most important frequencies using global filtering (Moreno-Pino et al., 2023).\n\u2022 Adapt the transformer encoder and the decoder using HiLo attention (Pan et al., 2022).\n\u2022 Incorporate system to condition the model to the theoretical model, using sample signals generated using the finite element method.\n\u2022 Modify the sampling model. The samples still come from a Normal distribution, but the variance of this distribution is now sampled from an exponential model to regularize too high variances.\nThe input signal is transformed to its spectrogram using a STFT of a certain time and frequency resolution. These resolutions are treated as hyperparameters and are optimized through cross-validation. Computing the STFT gives complex numbers as output. Both the real and imaginary part of the complex numbers are treated as independent channels, so the one-dimensional convolutional filters applied to each of them use a different kernel.\nEncoder. The encoder is modified by splitting the attention heads into two groups, as shown in Figure 5. The purpose of this modification is to efficiently capture higher and lower-level attention patterns and to reduce the impact of noise, as demonstrated by (Pan et al., 2022). The first group processes the spectrogram of the signal (high frequencies), while the second group processes a low-pass filtered version of the spectrogram (low frequencies). The low frequency attention block applies a 2-D Average Pooling to each of the components before applying the sparse attention mechanism. In contrast, the high-frequency attention block works with the full resolution of the spectrogram. As the last step in the encoder, the model concatenates the attention scores extracted of high and low frequencies.\nThe highly informative latent space which is generated using the encoder allows to use a simpler decoder.\nDecoder. The SSF is posed as an autorregressive generative model. The input to this model is the signal spectrogram. For each time step in the future,\nthe model generates a probability distribution over the next time window of the spectrogram. The loss function we use is the MSE. We choose a 2-step decoder where the first step is similar to the vanilla decoder from Vaswani et al. (2017) as depicted in 6. The second part contains a frequency filtering mechanism, as shown in Figure 7, which allows the model to selectively mute certain frequency bands, partially or completely, as proposed in Moreno-Pino et al. (2023). This approach enhances the model's ability to focus on the most relevant spectral features while reducing the influence of noise or irrelevant frequencies, leading to more accurate and context-aware predictions in the frequency domain. The decoder processes the reference signal and the features and merges them with the rest of the decoder. Additionally, it contains the sampling process and is responsible for providing the next values of the time series (the next time window in the spectrogram). At the very end, the signal is transformed back into the time domain using the Inverse Short Time Fourier Transform (iSTFT). Nevertheless, we evaluate the loss function in the frequency domain.\nSampling process. To ensure that the sample variance is positive, to regularize too high variances, and due to the nature of this problem, we prefer to generate a signal which is close to the mean rather than generating outliers. For this purpose we use the following sampling scheme\n$z \\sim \\Nu(\\mu(x), \\sigma^2)$ \n$\\sigma^2 \\sim Exp(\\lambda(x))$ \nwhere x are the inputs and z is the predicted value. Since stochastic sampling is not a differentiable process, the reparameterization trick is used together"}, {"title": "3. Results", "content": "The results are divided into three main aspects: frequency domain analysis, time domain analysis, and signal decomposition.\n3.1. SSF forecasting results\nFigures 9 and 10 illustrate the model's performance in the frequency domain for two individual signals. The predicted spectrograms are compared to the true spectrograms for both validation and test signals.\nIn both validation (Figure 9) and test (Figure 10) results, the predicted spectrograms closely match the true spectrograms. This demonstrates the model's effectiveness in capturing the frequency characteristics of the signals, a critical aspect for accurate signal analysis in railway maintenance applications.\nFigures 11 and 12 show the model's performance in the time domain, comparing the predicted and true signals for both validation and test datasets.\nThe time domain results indicate that the model's predictions are closely aligned with the true signals for both validation (Figure 11) and test (Figure 12) datasets. This alignment confirms the model's robustness in forecasting time-domain signals, which is crucial for predicting potential faults and initiating maintenance actions."}, {"title": "3.2. Signal Decomposition Analysis", "content": "To visually analyze the signals, we decompose the signals into their trend, seasonal, and residual components using Locally Estimated Scatterplot Smoothing (LOESS). Figure 13 presents the decomposition for the ground truth and predicted signals, respectively.\nThe decomposition shows that the predicted signals retain the essential characteristics of the true signals across all components: trend, seasonal, and residual. This detailed breakdown confirms that the model not only forecasts the overall signal accurately but also captures the underlying patterns effectively."}, {"title": "3.3. Model Performance Comparison", "content": "Table 3 provides a quantitative comparison of the SF and SSF models using MSE as the performance metric.\nThe results in Table 3 demonstrate that the SSF significantly outperforms the SF model, achieving lower MSE across training, validation, and test datasets. This improvement underscores the effectiveness of incorporating spectral methods and an improved observation model in enhancing the model's forecasting accuracy and generalization capabilities.\nOverall, the figures and table collectively illustrate the superior performance of the SSF model, highlighting its potential for reliable and accurate signal forecasting in railway maintenance applications.\nThe model addresses missing data imputation by leveraging its capacity to predict missing values based on learned temporal and spectral patterns. During training, the model is exposed to complete datasets with artificially masked data to learn how to reconstruct incomplete sequences. When applied to real-world data, the encoder processes the available portions of the time series to generate a contextual representation, while the decoder predicts the missing values. For extended gaps in data, the model can iteratively predict intermediate points to refine accuracy. By operating in the frequency domain, the model ensures that imputed values preserve the spectral characteristics of the original signal, maintaining consistency in the reconstructed data.\nFor outlier detection, the model uses its learned patterns to identify deviations between predicted and observed values. The encoder-decoder structure generates expected values based on historical data, and discrepancies between these predictions and actual measurements are flagged as potential outliers. This is possible due to the probabilistic nature of the generative model. The frequency-domain analysis allows the model to detect anomalies in spectral patterns, such as unexpected peaks or harmonic changes, which may indicate system irregularities."}, {"title": "4. Conclusion", "content": "This study introduces an innovative model architecture designed to enhance the analysis and processing of time series data in the railway industry. By leveraging cutting-edge technologies such as deep learning and the Transformer framework, the proposed model demonstrates significant improvements in performance, efficiency, and adaptability. The incorporation of frequency-domain transformations and the splitting of the encoder into high- and low-frequency blocks results in a highly expressive and efficient representation of input signals. Beyond predictive maintenance, the model offers practical applications, including outlier detection and missing data imputation, showcasing its versatility in addressing key challenges faced by railway monitoring systems. These advancements lay the groundwork for further innovation, such as the development of a bogie digital twin, ultimately contributing to the safety, reliability, and efficiency of modern railway operations."}, {"title": "5. Discussion", "content": "The proposed model architecture builds upon and extends recent advances in Transformer-based deep learning methods, addressing domain-specific challenges in time series analysis for railway systems. By adapting ideas from the spectral domain and incorporating a novel frequency-based encoder structure, the model achieves a balance between computational efficiency and predictive accuracy. The decision to operate in the frequency domain enables the model to better capture the underlying patterns in the data, aligning with recent research trends.\nThe applications of this work extend far beyond traditional maintenance. The ability to detect outliers positions the model as a proactive tool for anomaly detection, enabling real-time responses to potential system failures. Furthermore, the model's capacity for missing data imputation addresses a critical limitation of many monitoring systems, ensuring continuous and accurate insights even in the presence of sensor or communication issues. These features enhance the reliability of railway operations and present opportunities for integration into broader digital twin frameworks, potentially transforming maintenance and operational strategies.\nFuture work could explore optimizing the model for deployment in real-world systems, including hardware constraints and latency requirements. Additionally, expanding the architecture to incorporate multimodal data, such as environmental conditions and operational parameters, could further enhance its applicability and robustness."}, {"title": "Appendix A. Differentiable sampling operations", "content": "Appendix A.1. Reparameterization trick\nThe reparameterization trick is a popular tool used in deep generative models such as VAEs. It is a simple yet powerful trick that allows sampling from the normal distribution while maintaining the gradients propagation.\nInstead of sampling from a normal distribution with mean \u00b5 and variance \u03c3\u00b2, sample from the N(0,1), shift, and scale the sample. This way, the stochastic process is gathered within a leaf node of the computational graph (an end node), while the random sample is still transmitted to the path within the graph where gradients can backpropagate.\n$\u03a7 \\sim \u039d(\\mu, \\sigma^2) \\rightarrow Sample: x$\n$Y \\sim N(0,1) \\rightarrow Sample: y \\rightarrow X = \\mu + \\sigma^2 \\cdot y$\nAppendix A.2. Inverse transform sampling\nInverse transform sampling generates samples of a distribution given its cumulative distribution function.\nConsider the exponential distribution with cumulative distribution function\n$F_X(x) = 1 - e^{-\\lambda x}$ \nfor x > 0 (and 0 otherwise). Solving y = F(x) yields the inverse function\n$X = F^{-1}(y) = - \\frac{1}{\\lambda} \\ln(1 - y)$ \nThis implies that if we generate a random sample yo from a uniform distribution U ~ Unif(0, 1), and compute\n$x_0 = F_X^{-1}(y_0) = - \\frac{1}{\\lambda} \\ln(1 - y_0)$ \nthen xo follows an exponential distribution.\nAs a result, the sampling process can be performed without concerns about the gradient propagation. Summarizing the reparameterization trick computational graph results in Figure A.15. Clearly there exists a differentiable path connecting the inputs and outputs.\nAppendix B. Positional encodings\nTo add information about the order of the elements, Vaswani et al. (2017) use sine and cosine functions of different frequencies to calculate the Positional Encodings (PE):\n$PE_{(pos,2i)} = sin(pos/10000^{2i/dmodel})$\n$PE_{(pos,2i+1)} = cos(pos/10000^{2i/dmodel})$\nwhere pos is the position and i is the dimension."}]}