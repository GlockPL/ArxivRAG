{"title": "Heterogeneous Subgraph Network with Prompt Learning for Interpretable Depression Detection on Social Media", "authors": ["Chen Chen", "Mingwei Li", "Fenghuan Li", "Haopeng Chen", "Yuankun Lin"], "abstract": "Massive social media data can reflect people's authentic thoughts, emotions, communication, etc., and therefore can be analyzed for early detection of mental health problems such as depression. Existing works about early depression detection on social media lacked interpretability and neglected the heterogeneity of social media data. Furthermore, they overlooked the global interaction among users. To address these issues, we develop a novel method that leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and contrastive learning mechanisms. Specifically, prompt learning is employed to map users' implicit psychological symbols with excellent interpretability while deep semantic and diverse behavioral features are incorporated by a heterogeneous information network. Then, the heterogeneous graph network with a dual attention mechanism is constructed to model the relationships among heterogeneous social information at the feature level. Furthermore, the heterogeneous subgraph network integrating subgraph attention and self-supervised contrastive learning is developed to explore complicated interactions among users and groups at the user level. Extensive experimental results demonstrate that our proposed method significantly outperforms state-of-the-art methods for depression detection on social media.", "sections": [{"title": "1. Introduction", "content": "According to the essential facts provided by the World Health Organization Media Centre, an estimated 300 million people across all age groups suffer from depression globally [34]. Depression is the leading cause of disabilities worldwide and a significant contributor to the global burden of diseases. Depression can persist or relapse, severely affecting one's work, study, or daily life abilities [2]. Traditional depression diagnosis and treatment require face-to-face communication with doctors, which hinders identifying potential patients. Furthermore, many people who suspect that they may endure depression are reluctant or fearful to seek professional help at hospitals or clinics due to various factors such as economic, resource, social, and privacy [10]. Therefore, although there are available treatment programs and drugs, few people receive the treatment, and the existing programs are also difficult to monitor and track people's mental state for massive samples [12]. The widespread social networks such as Twitter have produced massive social media data, capturing people's authentic thoughts, emotions, communication, etc., which offers valuable opportunities to monitor public health issues, particularly depression [3]. However, effective depression detection models based on complex social networks entail the following challenges: (1) Interpretability: Depression assessment often entails high-risk and life-critical issues, however, existing works on early depression detection are usually end-to-end, so it is essential to ensure sufficient interpretability [48, 39]; (2) Social networks contain heterogeneous data such as posts and user behaviors, which have different correlations with each other [47]; (3) Interactivity: There are various forms and degrees of interactions among users and groups in social networks.\nExisting end-to-end deep learning approaches predom-inantly prioritize the improvement of classification perfor-mance [37]. However, depression detection decisions [41], often entail high risks and have life-or-death consequences, underscoring the criticality of establishing trust. Therefore, it becomes imperative to understand the factors driving these decisions [35, 40]. The tweets posted by users on social platforms often convey evident sentiment signals. For instance, the tweet \"I had a gym class today, and I'm so happy\" contains \"happy\", therefore representing outwardly explicit psychological symbols that are relatively easily recognized. However, users may not always employ explicit psychological terminology to express their emotions, in-sights, and experiences in their tweets. For example, there is no explicit psychological terminology in the tweet \"It has been rainy and very wet for several days\", which signifies an implicit psychological symbol. It is challenging to identify these implicit symbols based solely on tweets. To address this challenge, the utilization of psychological depression scales proves to be practical and highly interpretable for assessing depression [32]. These scales are selected based on diagnostic criteria for depression and can be leveraged to learn or map the implicit psychological symbols of individuals in a given moment. In this paper, we present a novel prompt learning approach that leverages the under-standing capabilities of the large-scale pre-trained language model, employing depression scales to effectively map the implicit depressive symptoms of users at the psychological level. Additionally, we utilize automatic text summariza-tion techniques to compress tweets into concise conclusive descriptions [47], as well as mine deep semantic features in tweet texts and behavioral features of users that are helpful for depression detection [28]. We incorporate these diverse features to provide excellent interpretability at both psychological and linguistic levels.\nNowadays, graph neural networks are effective methods to analyze complex relationships between objects in many domains, such as sentiment analysis on social media[19]. Considering the diverse aspects and complicated interac-tions in social media data, we construct a heterogeneous information network[17] to integrate the extracted features. Subsequently, we employ a dual attention mechanism, con-sisting of type-level attention and node-level attention, to learn interactions among different types of nodes at the feature level. Type-level attention can learn the importance of different adjacent node types, and node-level attention can learn the importance of different adjacent nodes, thus enabling more effective aggregation of heterogeneous fea-tures, while enriching the semantics and reducing the impact of noise information. After that, considering the social na-ture of humans, we formulate users and depression detection as subgraphs and subgraph classification problem, respec-tively. We develop a subgraph attention mechanism to learn complicated interactions among users while developing a self-supervised contrastive learning mechanism that utilizes user subgraphs as positive samples and generates negative samples by perturbing their features. This contrastive learn-ing mechanism enables us to learn the interactions among users and groups at the user level, thereby further enhancing the distinctiveness of users. Consequently, we present a novel approach that leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and contrastive learning mechanisms. The contributions of this work are described as follows.\n\u2022 We present to model prompt learning to map users' implicit psychological symbols utilizing depression scales and mine a variety of features that may reflect users' depressive state to provide excellent inter-pretability at the psychological level.\n\u2022 Considering the heterogeneity of social media infor-mation, we construct all features as a heterogeneous information network and construct a heterogeneous attention network to explore complicated interactions among different types of information at the feature level.\n\u2022 We develop a subgraph attention mechanism and a self-supervised contrastive learning mechanism to discover the complicated interactions between users and groups at the user level, resulting in more distinc-tive user representations."}, {"title": "2. Related Works", "content": "2.1. Depression Detection on Social Media\nSocial media is usually text-based, so most previous works on depression detection were based on text-only information[11]. Chiong et al. [33] studied several text preprocessing and text-based feature techniques to pro-pose a method for textual depression detection on social media. Adarsh et al. [1] normalized the class imbalance, and then proposed a method with a noise label correction technique for textual depression detection on social media. David et al. [24] evaluated and improved depression-related lexical resources based on context and word embeddings for depression-related information in Reddit text. User-generated content includes diverse information, so some works consider social behaviors and multimodal informa-tion. Figueredo et al. [9] considered the importance of emotions in emojis, which was combined with context-independent word embeddings and fusion techniques for de-pression detection. Ortega et al. [25] evaluated first-person pronouns as features for personal statements in depression detection on social media. Gui et al. [13] used GRU to extract text and image features and applied a cooperative multi-agent model for depression detection, addressing the diversity of user-generated content regarding topics and sentiments. Nevertheless, these works often ignored the short-text characteristics of social media data, information interaction, and the interpretability of depression detection.\n2.2. Interpretability of Depression Detection\nResearch on the interpretability of depression detection often adopted attention mechanisms or knowledge-aware methods. Han et al. [39] considered linguistic features and used metaphorical concept mapping techniques to detect implicit manifestations of depression. Zhang et al. [46] proposed a method guided by the similarity between the post and psychiatric scale, which can capture risk posts related to the dimensions in the clinical depression scale and provide interpretable diagnostic bases. Zhang et al. [45] devised a deep knowledge-aware depression detection framework, which identified clinically relevant entities by incorporat-ing medical knowledge in the model and considered the dynamic pattern of depression to provide interpretability. Zogan et al. [48] considered Twitter users' behaviors, topics, and emotions, thereby providing interpretability from the model level based on a hierarchical attention network for depression detection. However, these works only provided better interpretability at the linguistic level and didn't thor-oughly learn the mapping between social media data and depressive symptoms at the psychological level.\n2.3. Heterogeneous Graph Neural Networks\nRecently, people have become increasingly interested in graph neural networks which are effective methods to analyze complex relationships between objects. Lu et al. [44] composed the users into a fully connected graph, then used a graph convolutional network with a collaborative attention mechanism to fuse the source and user represen-tations to deal with social media tasks. However, they used homogeneous graphs with only one type of nodes and edges, and the network structure was relatively simple. Wang et al. [42] proposed a heterogeneous graph attention network with semantic-level and node-level attention to learn the impor-tance of meta-paths and node neighbors, thereby obtaining the final node representation by the aggregation. To address the sparsity/ambiguity and label scarcity problems of short text classification, Hu et al. [17] proposed a heterogeneous graph attention network to learn short text representations and combined hierarchical attention mechanism to achieve better information aggregation.\nThese works demonstrated the feasibility of heteroge-neous graph neural networks. For depression detection, Mil-intsevich et al. [20] performed depression diagnosis based on knowledge graphs and text representations. Mihov et al. [26] created ego-networks from user-user interactions (in-cluding replies, mentions, and quoted tweets), then merged them into heterogeneous graphs and classified heteroge-neous graphs to perform depression detection. However, social media data is more comprehensive to analyze the posted tweets to discover the relationships between users. When considering the combination of tweets and other het-erogeneous social information, more potential associations should be discovered. However, many works are based on global structure, thereby some potential associations are difficult to capture, and more delicate substructures need to be considered.\n2.4. Substructures and Contrastive Learning\nThe ability to detect and analyze specific substructures within graphs is crucial for addressing various tasks in-volving graph-structured data, particularly in the field of social network analysis [6]. By considering these substruc-tures and exploring the interplay between local and global contexts, we can obtain a more detailed representation of user interactions and capture their distinctive features. Some works adopted mutual information to measure the relation-ship between local and global features for graph embedding. Velickovic et al. [31] learned a node encoder that maximized the mutual information between node representations and corresponding high-level summaries of graphs. Peng et al. [30] extended the conventional mutual information from vector space to graph domain, where they computed mutual information from both node features and topological struc-ture. Che et al. [5] used a self-supervised approach to learn graph representations, using an online network to predict the target network. These works applied mutual information to learn relationships between local and global features, but they ignored the subgraph representations within the global scope.\nIn the domain of graph learning, the effectiveness of contrastive learning is widely recognized [21]. It is con-sidered an effective approach for uncovering correlations at both the local and global levels. However, it is important to note that currently, only a limited number of studies have explored the application of contrastive learning specifically in the context of depression detection on social media. Yang et al. [43] proposed a knowledge-aware mental module based on dot-product attention and employed a supervised contrastive learning module to capture class-relevant fea-tures from label information. However, their contrastive learning mechanism was supervised and focused on local features, without explicitly considering the relationship be-tween local and global features.\nTherefore, aiming for the heterogeneity, interactivity, and interpretability of depression detection on social me-dia, we attempt to combine the heterogeneous graph with subgraph contrastive learning and prompt learning to map implicit psychological symbols of users. We believe that various heterogeneous features of users in social networks reflect their potential psychological symbols and are more suitable to be modeled as the heterogeneous graph. More-over, due to massive users and their activities online, the posted information is widely spread, which implies social connections and more latent interactions. These are crucial for depression detection on social media."}, {"title": "3. The Proposed Model", "content": "We propose a heterogeneous subgraph network with prompt learning (HSNPL) and contrastive learning mech-anisms for depression detection on social media. The framework of our model is shown in Figure 1, which includes three modules. (1) Heterogeneous graph con-struction: Prompt learning is employed to establish the mapping between social media data and implicit depressive symptoms at the psychological level. Heterogeneous and diverse user information is processed to generate multiple types of nodes, which are then constructed as the original heterogeneous global graph i.e. heterogeneous information network; (2) Heterogeneous graph attention network: The original graph is learned via a heterogeneous graph attention network with a dual attention mechanism to obtain the updated heterogeneous global graph at the feature level; (3) Subgraph contrastive learning: Dual subgraph attention mechanism is employed to learn the interactions between users, then subgraph contrastive learning is developed to learn the interactions between users and groups, therefore, further enhances distinctive subgraph representations at the user level. The final classification loss comprises two com-ponents: subgraph classification loss $L_{sub}$ and contrastive learning loss $L_{cl}$.\n3.1. Heterogeneous Graph Construction\n3.1.1. Post Summarization\nDue to the crucial role of users' posting history in identifying depressive symptoms on social platforms, we analyze their posting histories within one month before their anchor tweets, which are public and large-scale in the dataset [38]. However, users post tweets with diverse topics, which may be not relevant or indicative of a depressive state. Therefore, we focus better on the information most related to users' depressive symptoms while reduce the redundancy and noise in the data by means of automatic text summa-rization techniques. After necessary data preprocessing, we obtain all tweets posted by each user $U_i$ ($i = [1, 2, ..., n]$, n is the total number of users) within a period. Given the powerful semantic representation ability of BERT[7], we employ BERT to learn the embeddings of tweets, followed by clustering the embeddings using the K-means algorithm. The tweets in the cluster centers according to their distances to the centroids are regarded as crucial tweets. For these crucial tweets, we utilize bidirectional and auto-regressive transformers(BART) [27], which have a bidirectional en-coder with a BERT structure and an autoregressive decoder with a GPT structure, to compress potentially redundant information and generate more representative summary text $P_i$. It can automatically select the core features of text content and summarize a large amount of texts into a con-cise and conclusive description while preserve the critical semantics.\n3.1.2. Psychological Mapping\nIt is challenging to detect and interpret users' depressive states solely based on their posted tweets. To address this issue, we incorporate the well-known self-rating depression scale(SDS) [49], which is a self-report questionnaire with twenty items specifically developed to offer a compre-hensive evaluation of various symptoms associated with depression while maintaining brevity, simplicity, and quan-tifiability. We rewrite the symptoms in SDS as \"Subject [Mask] Verb Object.\" as shown in Table 1, where \"[Mask]\" is an adverb that indicates how often or how much a user ex-periences a symptom. Next, we use prompt learning which is based on the understanding ability of the large-scale pre-trained language model to map the \"[Mask]\" of symptoms in SDS for posted tweets. As shown in Figure 2, we first set the template as \"post + symptom\" where the post; is defined as all posted tweets by user $U_i$ within a period without summarization. Then, we use a prompt-tuning toolkit \u00b9 to map the \"[Mask]\" of symptoms to the candidate answers in Table 1, which are {\"Rarely\", \"Sometimes\", \"Often\", \"Always\"}. According to the original SDS, each candidate's answer reflects how severe or frequent a symptom is and is assigned a score from {1, 2, 3, 4}. Then, we aggregate the scores of all symptoms according to the candidate answers and obtain a four-dimensional vector $F_i = [F_1, F_2, F_3, F_4]$ for each user. Finally, we normalize the vector to get the user's depression scale distribution, which is used as the implicit symptom feature at the psychological level. The formula is as follows:\n$F_{k}^{ui} = \\frac{\\sum_{j=1}^{20} score^{jk}}{\\sum_{k=1}^{4} \\sum_{j=1}^{20} score^{jk}}$\n(1)\nWhere $score^{jk}$ indicates the score corresponding to can-didate answer k on jth symptoms that post; is mapped.\n3.1.3. Semantic Parsing\nText summarization can filter out much noise and redun-dant information, but the resulting short texts are also sparse and ambiguous in semantics. Therefore, to obtain more fine-grained text interpretations, we need to discover latent features and their relationships in the texts. To this end, we perform topic analysis on the summarized tweets $P_i$. We use a topic extraction model BERTopic[36], which combines BERT and hierarchical clustering, to identify topics in the summarized tweets $P_i$ and retain the most frequent ones as relevant topics $T_i$. Meanwhile, we perform entity extraction on the summarized tweets $P_i$ utilizing an entity linking tool called TAGME[29], which maps the entities to Wikipedia as well as identifies and disambiguates named entities in each tweet. We obtain the corresponding description texts for each entity and then use BERT to learn their embeddings $E_i$. To capture the semantic relations among the entities, we establish edges between them using cosine similarity measure and only connect the entities whose similarity score exceeds the threshold. Finally, we define the semantic features of the user $U_i$ as $S_i$, composed of two types of nodes: topic and entity, i.e. $S_i = T_i \\cup E_i$.\n3.1.4. Behavior Analysis\nWe consider more features that may reflect users' de-pressive states on social media, including the time distri-bution statistics of users' tweets $t_i = [t_1, t_2, ..., t_{24}]$, the statistics of original and retweeted tweets $s_i = [s_o^i, s_r^i]$, the statistics of emoticon ratio $e_i = [e_p^i, e_n^i, e_{neu}^i]$, the statistics of sentiment word ratio $w_i = [w_p^i, w_n^i, w_{neu}^i]$, the statistics of following and follower lists $f_i = [f_1^i, f_2^i]$, and the statis-tics of first-person singular and plural ratio $r_i = [r_1^i, r_2^i]$. These multi-dimensional and significant additional features encompass statistics related to the behaviors exhibited in the user's tweets as well as their social interactions. These features represent the actions or behaviors of one user on the social network, whether consciously or unconsciously. By analyzing these behaviors, we can uncover more features as-sociated with users' psychological symbols based on tweets. Detailed descriptions of these behavioral features can be found in Table 2. We normalize these behavioral features respectively and define them as $B_i = t_i \\cup e_i \\cup w_i \\cup s_i \\cup f_i \\cup r_i$.\nThese data with multiple types of nodes and edges are typical heterogeneous data, and we model them as a heterogeneous graph structure. To this end, we let the summarized tweets $P$ be the central node, and the additional features $O_i = S_i \\cup F_i \\cup B_i$ be the auxiliary nodes, then $U_i = P \\cup O_i$ represents all the features of each user. A heterogeneous information network (HIN) is constructed to integrate all users $U = [U_1, ... , U_n]$ and their features. Thus, an original heterogeneous global graph that contains all user features is obtained.\n3.2. Heterogeneous Graph Attention Network\nThe features of users in the original graph vary con-siderably. It needs to be more comprehensive to model these features, so we must consider the importance of different features. Given a specific node, different types of adjacent nodes may have different impacts on it, and different neighboring nodes of the same type could also have different importance. Therefore, we use a dual attention mechanism to better aggregate features in the heterogeneous information network to capture the different importance at the node and type levels.\nType-level attention can learn the weights of different adjacent node types. Given a specific node v, according to the embedding of each adjacent node $v' \\in N$ with type $\\tau$, $x_v \\in R^q$, where q denotes the length of the feature vector (we perform dimensionality mapping for all features to ensure the same length), we can obtain the embedding of type $\\tau$ of its adjacent nodes as $x_{\\tau} = \\sum_{v' \\in N} A_{vv'}x_{v'}$ (where A is the normalized adjacency matrix). Then, we calculate the type-level attention score and use the softmax function to normalize it. The specific formulas are as follows:\n$\\alpha_{v,\\tau} = \\sigma[\\mu^{\\top} \\cdot (x_{\\tau}||x_v)]$\n(2)\n$\\alpha_{v,\\tau} = \\frac{exp(\\alpha_{v,\\tau})}{\\sum_{\\tau' \\in T} exp(\\alpha_{v,\\tau'})}$\n(3)\nWhere $\\mu_{\\tau}$ represents the attention coefficient for type $\\tau$, || represents the concatenate, T is the set of different node types and $\\sigma(\\cdot)$ represents the activation function.\nNode-level attention can learn the importance of differ-ent adjacent nodes and reduce the weights of noisy nodes. Given a specific node u with type t and its adjacent node $v' \\in N_v$ with type $\\tau'$, according to the node embeddings $x_v$ and $x_{v'}$ as well as the type-level attention weight $\\alpha_{v,\\tau'}$, we can calculate the node-level attention score by multi-plying them instead of directly concatenating them, which can make the node-level attention score more reasonably reflect the importance of adjacent nodes with different types and can reduce the dimensionality increase brought by the concatenation operation. Then, we normalize it with the softmax function, where $\\sigma$ is the attention coefficient.\n$b_{vv'} = \\sigma[\\mu_{\\tau'}^{\\top} \\cdot \\alpha_{v,\\tau'}(x_vx_{v'})]$\n(4)\n$\\beta_{vv'} = \\frac{exp(b_{vv'})}{\\sum_{v' \\in N_v} exp(b_{uv'})}$\n(5)\nFinally, we obtain the propagation rule for the heteroge-neous global graph as Formula (6). After L-layer propaga-tion for the original graph, we obtain updated heterogeneous global graph representation G.\n$G^{(l+1)} = \\sigma(\\sum_{\\tau \\in T} B^{(l)}G_{\\tau}^{(l)}W_{\\tau}^{(l)})$\n(6)\nWhere B is the attention matrix (the element in row v and column v' is $\\beta_{vv'}$), $G_{\\tau}^{(0)} = X_{\\tau}$ ($X_{\\tau}$ is the initial feature matrix of all nodes with type $\\tau$), $W_{\\tau}^{(l)}$ represents the transformation matrix of type $\\tau$ in the lth layer.\n3.3. Subgraph Contrastive Learning\nAs users on social media, in addition to constructing a heterogeneous global graph at the feature level, it is crucial to consider the interactions among users. For this purpose, we construct each user as one subgraph and consider the finer-grained subgraph structure at the user level. Specifi-cally, we do not change any graph structure and adopt intra-subgraph attention within the subgraph of each user $U_i$ to obtain the impact of each node on this subgraph, thereby achieving the subgraph embedding $g_i$ for user $U_i$ as follows:\n$g_i = \\sum_{x_{ij} \\in U_i} \\sigma(\\omega_{intra}^{\\top} W_{intra}x_{ij}) x_{ij}$\n(7)\nWhere $x_{ij}$ is the embedding of each node belonging to $U_i$, $\\omega_{intra}$ is the attention vector, and $W_{intra}$ is the transfor-mation's weight matrix.\nIt is worth noting that there are shared nodes and edges among supernodes, which ensure their interconnectivity and contribute to the formation of user groups. We consider that users within the same group have stronger correlations and closer social activities. To construct subgraph interaction, we treat each subgraph in the global graph as a supernode and use multi-head inter-subgraph attention to learn atten-tion coefficients to distinguish the mutual influence among subgraphs, which reflect the interaction among users. Then we obtain the subgraph embedding $sg_i$ after inter-subgraph attention for user $U_i$ as follows:\n$sg_i = \\frac{1}{M}\\sum_{m=1}^{M} \\sum_{j \\in N_i} \\sigma(b_{ij}^{m}W_{inter}^{m}sg_j^m)$\n(8)\nWhere $N_i$ is the set of neighbors of supernode $sn_i$, $b_{ij}$ is the attention coefficient of supernode $sn_i$ on supernode $sn_j$, $W_{inter}$ is the transformation's weight matrix, and M is the number of attention heads.\nFurthermore, to further enhance the discriminative rep-resentation of the subgraph as well as the correlation between the subgraph and global graph, which reflect the relationship between users and the groups formed by their shared nodes and edges, we develop a self-supervised contrastive learning mechanism for subgraphs and global graph. In addition to taking existing subgraphs as pos-itive samples, we also need to construct some negative samples(subgraphs). Our specific method is to retain the structure of positive samples but randomly shuffle the features of positive samples to obtain a perturbation as negative samples. This perturbation can be understood as the situation in which both positive and negative sample users engage in various activities on social platforms, such as tweeting, retweeting, and following other users. However, the content of the negative sample users' activities is intentionally perturbed. For instance, their tweets, number of retweets, number of followers, etc. differed from those of positive sample users. Consequently, they exhibit different social interactions with other users or groups on the plat-forms. Then, the positive sample pair is composed of the positive sample subgraph and the global graph. Similarly, the negative sample pair is composed of the negative sample subgraph and the global graph. Based on this self-supervised contrastive learning mechanism, we use mutual information to measure the correlation between the subgraph and global graph, which is optimized by Jensen-Shannon MI estimator [14], to determine whether the subgraph is from this global graph as follows.\n$D(sg_i, G') = \\sigma(sg_iW_{MI}G')$\n(9)\nWhere $W_{MI}$ is the rating matrix of mutual information, G' is the final global graph representation achieved by pool-ing on all subgraph embeddings, and $\\sigma(\\cdot)$ is the sigmoid function.\nNext, we use binary cross-entropy to define the con-trastive learning loss, which measures how well the sub-graph embeddings match the global embedding. The spe-cific formula is as follows:\n$L_{cl} = \\frac{1}{n_{pos} + n_{neg}} (\\sum_{i=1}^{n_{pos}} [log(D(sg_i, G'))]  + \\sum_{j=1}^{n_{neg}} [log (1-D (sg_j', G'))])$ (10)\nWhere $n_{pos}$ denotes the number of positive samples, $n_{neg}$ denotes the number of negative samples, $sg_i$ is the embedding of the positive sample, $sg_j'$ is the embedding of the negative sample.\n3.4. Training\nFinally, we perform subgraph classification as depres-sion detection, which can determine the user's depression state based on the subgraph containing all features of this user. Furthermore, we consider that the posted texts on social media may contain crucial features that contribute to depression detection. Therefore, we construct the loss of subgraph classification as follows:\n$L_{sub} = \\sum_{i=1}^{n}Y_i {log[ softmax(sg_i)]+log[ softmax(hp_i)]}$\n(11)\nWhere $sg_i$ is the subgraph embedding of one user, $Y_i$ is the corresponding label, n is the total number of users in the dataset, $hp_i$ represents the embedding of post node $P_i$.\nThe overall loss function of the model is described as follows:\n$L = \\alpha L_{cl} + \\beta L_{sub} + \\eta||\\Theta||_2$\n(12)\nWhere $\\alpha$ and $\\beta$ control the contrastive learning and subgraph classification, and $\\eta$ is the coefficient for L2 regu-larization on model parameters $\\Theta$. We use gradient descent and an early stopping strategy to perform the optimization."}, {"title": "4. Experiment Analysis", "content": "In this section, we conduct extensive experiments and compare our model with some state-of-the-art baselines that use the same dataset to evaluate the performance.\n4.1. Dataset and Experimental Settings\nA large-scale public dataset[38] for depression detection on Twitter is used for experiment analysis in this paper, which consists of the following three parts:\n\u2022 D1: Depressed dataset, where the users who posted anchor tweets containing \"(I'm / I was / I am / I've been) diagnosed depression\" were labeled as depressed (positive). This part contains 2558 users and 1.2 million tweets.\n\u2022 D2: Non-depressed dataset, where the users who never posted tweets containing the word \"depression\" were labeled as non-depressed (negative). This part contains 5304 users and 2.6 million tweets. We ran-domly sampled a subset of these users to balance the number of positive and negative users in our dataset.\n\u2022 D3: Depression candidate dataset, where a large-scale unlabeled depression candidate dataset was con-structed with 58810 users and 29 million tweets. This part was not used in our work because it was not annotated, and it was unclear how the candidates were selected or filtered. However, we plan to explore this part in our future work as a potential source of data augmentation.\nThe raw data often contains much noise, which can reduce the usefulness and validity of the data. Therefore, we perform the following data preprocessing to filter out irrelevant contents and extract key information from the raw data.\n\u2022 We remove the anchor tweets because they could introduce bias or leakage to the depression detection task. We also remove any retweets or duplicate tweets from the same user to avoid data redundancy.\n\u2022 We only consider English users and exclude users who have too many followers or too few tweets because they could be bots or spammers. We use a language detection tool to identify the language of each user based on their most recent tweets.\n\u2022 We filter out components that appear frequently but are insignificant for depression analysis, such as stop-words, URLs, mentions, and replies. We use a pre-defined list of stopwords and regular expressions to remove these components from each tweet.\nFinally, we obtain a dataset with 5285 users, including 2522 depressed users from D1 and 2763 non-depressed users from D2. The average number of tweets per user is 732, and the average number of words per tweet is 14. Moreover, the parameter settings are shown in Table 3, and 5-fold cross-validation is conducted for experimental analysis. Our experiments are performed on a Tesla P40 GPU with 24 GB of memory, and the computation time is about 150 minutes.\n4.2. Baselines\nThe diagnostic information in the tweets has a powerful guiding effect on depression detection. Therefore, it is our main task to perform experiments with the data after filtering out the anchored tweets, but we also compare our model with some methods that used the anchored tweets. The baseline methods are as follows:\n\u2022 HAN(2017) [15]: A word-level hierarchical attention neural network framework was used to analyze user tweets for depression detection;\n\u2022 BERT(2018)[7]: A bidirectional transformer encoder (Base) was used to obtain tweet-only embeddings and perform depression detection;\n\u2022 ROBERTa(2021)[23]: Robustly optimized BERT pre-training approach was applied for tweet-oriented de-pression detection;\n\u2022 MentalBERT(2021)[18]: A Pre-trained BERT model specifically designed for the mental healthcare do-main, was applied for tweet-oriented depression de-tection;\n\u2022 ChatGLM(2022)[8]: ChatGLM is a bilingual (En-glish and Chinese) pre-trained language model. We take it as the pre-trained model, then utilize LoRA [16] to fine-tune our task;\n\u2022 COMMA(2019)[13]: A multi-agent reinforcement learning method was used to extract features from texts and images, then GRU and VGG-Net were combined for classification;\n\u2022 SenseMood(2020)[22]: Based on CNN and BERT, deep features were extracted from user-posted images and texts, and then visual and textual features were combined to detect the user's depressive state;\n\u2022 DepressionNet(2021)[47]: User's behavioral infor-mation on social platforms was considered, and automatic summarization was used to condense text information, then a cascaded deep network was used to connect different levels of behavioral features;\n\u2022 MCNN(2022) [48]: Based on MLP and CNN, a hybrid model that used texts and user profile features was built for depression analysis;\n\u2022 MDHAN(2022)[48]: Based on user behavioral fea-tures, a hierarchical attention mechanism was devel-oped for encoding at the word level and sentence level;\n\u2022 HAN-MCM(2022)[39]: A model was proposed based on a hierarchical attention mechanism, which took texts and metaphorical concept mapping as the input to achieve interpretability.\n4.3. Performance Comparison\nWe conduct experiments on the dataset with and without anchor tweets and categorize the methods according to different features. The results in Table 4 demonstrate the effectiveness of our method in improving the performance of depression detection by incorporating text, user behavior, and depression scale as the heterogeneous graph as well as applying subgraph contrastive learning. As shown in Table 4, our method outperforms the existing state-of-the-art methods on all evaluation metrics.\nTweet-oriented depression detection methods that solely rely on text features, such as HAN and BERT, have been found to have unsatisfactory performance. However, ap-proaches like RoBERTa, which is fine-tuned based on this foundation, and MentalBERT, which is specifically pre-trained for mental healthcare, have shown improved perfor-mance. Despite the favorable performance of MentalBERT in the field of mental healthcare, it is one pre-trained lan-guage model and downstream tasks on social media should be developed, moreover, we acknowledge that it shares a common limitation with other tweet-oriented approaches, which often fail to consider the implicit symbols at psy-chological level and may not adequately address the issue of interpretability. Notably, ChatGLM does not perform satisfactorily after fine-tuning our task, because it requires more excellent expertise and contextual understanding for the downstream tasks. SenseMood and COMMA use both text and image features, improving the model's ability to predict depression status, but they ignore the interaction be-tween users. As the dataset only contains image links rather than image content, image retrieval is considered dataset augmentation. Therefore, we do not use image features. DepressionNet and MDHAN consider more dimensions of features and provide interpretability from the model structure. However, they do not integrate these multidimen-sional features well and fail to discover the imperceptible interactions among them efficiently. HAN-MCM integrates linguistic knowledge in both cases (with and without anchor tweets), achieving superior performance. However, it also does not consider the interactions among social users, which are also crucial for depression detection.\n44.  Ablation Study\nWe conduct ablation experiments on modules and fea-tures in the model and analyze them from these two aspects."}, {"title": "4.4.1. Module", "content": "We analyze the role of each module in our model, as shown in Table 5. First, we study the effect of the hetero-geneous graph module. The results show that the model's performance would significantly decrease if we remove the heterogeneous graph module and only use the subgraph part with prompt learning to classify each subgraph. This suggests that modeling social data as a heterogeneous graph is crucial for depression detection. Furthermore, if we re-move the dual attention mechanism, which captures the interactions among nodes, the model's performance would also decrease significantly, indicating that the dual attention mechanism is reasonable. The correlation among nodes should also be considered, which can capture the features and correlations of social media data.\nNext, we discover the vital role of the subgraph module in our method. If we remove both contrastive learning and subgraph attention mechanisms, the model's performance will drastically deteriorate, showing that the interaction between users and the group is indispensable. Finally, we estimate the contribution of prompt learning to our method. The results indicate that prompt learning can enhance our final performance by around 3%, demonstrating that prompt learning is effective in revealing implicit psychological symbols of users via the depression scale. In addition, if we do not use prompt learning and only retain the hetero-geneous graph(w/o Subgraph & Prompt Learning) or the subgraph part(w/o Heterogeneous graph & Prompt Learn-ing), the model's performance would decline by almost 15.5% or 14.1%, respectively, which suggests that subgraph classification provides superior performance to post-node classification and prompt learning can better provide dis-criminative depressive features at the psychological level."}, {"title": "4.4.2. Feature", "content": "In this section, we analyze the role of each type of feature in our model, as shown in Figure 3, where Post, F, S, and B denote the textual post after summarization, implicit psychological symptoms, the set of semantic fea-tures, and the set of behavioral features, respectively. The results show that in addition to the posts published by users, implicit psychological symptoms mapped by prompt learning, semantic features extracted from the posts, and user behavioral features improve the performance by almost 3%, 4%, and 8%, respectively. In addition, the behavioral and implicit psychological features further improved the performance after aggregating textual posts with semantic features. Furthermore, if we only use textual posts as the features, the performance would decrease by 18%, which indicates that more features beyond the textual posts need to be mined for depression detection on social media.\nWe note that behavioral features include six features, and semantic features include topics and entities. Although implicit psychological features are only composed of de-pression scale scores, they also play a significant role, which highlights the importance of implicit psychological features for depression detection and enhances interpretability. We randomly select five questions from Table 2 and report the number of users corresponding to each answer by prompt learning in Table 6, in (number1, number2). \"number1\" represents the number of depressed users, and \"number2\" represents the number of non-depressed users. The results reveal a marked difference between the number of depressed and non-depressed users corresponding to the same answer to the same question, which confirms that prompt learning is effective in capturing implicit psychological symbols of users via the depression scale."}, {"title": "4.5. Interpretability by Prompt Learning", "content": "We randomly select one depressed user and one non-depressed user from the dataset to illustrate our method and show the results of prompt learning to map their tweets to depression symptoms, as shown in Table 7. The results show that for users with different depressive states, their tweets are mapped to different answers to the same question by means of prompt learning. For example, for the symptom \"I [mask] feel down hearted and blue.\", we use the prompt for depressed and non-depressed users and map their tweets to the degree \"often\" and \"sometimes\" respectively, showing that our method can capture subtle differences in psycho-logical symbols between users with different depressive states. For the symptom \"I [mask] feel hopeful about the future.\", their tweets are mapped to the degree \"rarely\" and \"often\" respectively, which shows that our method can reflect contrasting attitudes toward the future between users with different depressive states.\nWe also reveal the ability of prompt learning to map tweets to relevant answers to different questions. For exam-ple, for the symptom \"I [mask] have crying spells or feel like it.\", the tweets of depressed users are mapped to the degree \"always\". In contrast, for the symptom \"My life is [mask] pretty full.\", the tweets of non-depressed users are mapped to the degree \"always\". This result shows that our method can identify the distinctive depression symptoms of users with different depressive states, indicating that the prompt learning approach utilizing a depression scale in our method is feasible. It can analyze implicit psychological symbols from users' tweets on social media and obtain more interpretable psychological mapping."}, {"title": "4.6. Parameter Analysis", "content": "We conduct parameter analysis to investigate the effect of several vital parameters on the model performance, mea-sured by F1-score and ACC. We analyze two parameters, the number of topics K and entities E, as shown in Figure 4(a) and Figure 4(b). We find that the number of topics K and the number of entities E have an optimal range for the model performance, which is affected by the trade-off between the complexity and the expressiveness of the graph structure. When K=15 and E=1250, the model can achieve the best performance."}, {"title": "4.7. Visualization", "content": "We visualize the information of our method from several aspects: attention mechanism, heterogeneous graph, and contrastive learning.\n4.7.1. Attention Mechanism\nWe visualize the impact of dual node and subgraph attention mechanisms on the model at both feature and user levels. Firstly, as shown in Table 5, the model's performance drops by about 5% when either attention mechanism is re-moved, indicating that merely modeling social information as a heterogeneous graph is insufficient, and the interactions between features and users are essential in predicting de-pression risk. Secondly, to explain the importance of dual attention mechanism at the feature level in detail, we select two users from the heterogeneous graph and visualize their node attention matrix, as shown in Figure 5(a), where the horizontal axis represents the nodes of User\u2081, the vertical axis represents the nodes of User2, and the darker the color, the stronger the interaction influence of the horizontal axis node on the vertical axis node. The figure shows that the average attention weight of the Post\u2081 node on the nodes of User2 is the highest, followed by implicit symptom and semantics nodes, and the lowest is the behavioral node, indicating that posts are the core nodes. There are unique edges among semantic nodes and texts, whose relevance is strong, so semantic nodes are assigned more weight. It is worth noting that the implicit symptom node is also assigned much weight, reflecting our method's excellent performance of prompt learning. Finally, since behavioral features are unique to each user and only are constructed relations with the current user, they are assigned fewer weights. Therefore, we believe that dual attention is essential for capturing the interaction among different features.\nThen, we select five users who are learned by means of the subgraph attention mechanism in Section 3.3 and visualize the attention weights among users, as shown in Figure 5(b), where each block represents the interaction influence of one user in the horizontal axis on one user in the vertical axis. The color represents the weight of the influence. From the figure, we can see that since User\u2081 posted 28% more tweets than the average number of tweets posted by the users in the dataset, he has the most decisive influence on the other four users in the figure. In addition, User1, User2, and User4 are labeled as depressed users in the dataset. They are assigned more weight than User3 and Users who are labeled as non-depressed users. The reason may be that depressed users tend to post more negative content, which is more likely to spread on social media, such as anxiety spread, so depressed users are more influential to other users. Moreover, the interaction between two users is different, so the attention weight matrix should be asymmetrical.\n4.7.2. Heterogeneous Graph and Subgraph Contrastive Learning\nTo visualize the initial feature distribution of users, we use PCA to reduce feature representations of 200 randomly selected users to two-dimensional space, as shown in Figure 6(a). Then, we visualize the feature representations of users after heterogeneous graph propagation, as shown in Figure 6(b). From the figure, we can see that the initial user feature representations are chaotic. Nevertheless, the heterogeneous graph can enhance the correlations among different types of features and the interactions among users by modeling these features as a heterogeneous graph structure, thereby improving the effectiveness of depression detection.\nHowever, after heterogeneous graph propagation, we can also see many overlapping points and outliers in the user distribution, which needs to be improved for high-precision decision-making. We illustrate the contribution of subgraph contrastive learning on user feature representa-tions, as shown in Figure 6(c). We can see that by means of this module, user representations are more clearly distin-guished, and the model achieves a better discrimination abil-ity for depressed and non-depressed users, indicating that this module can obtain more distinctive user representations by learning the interactions between users and the group. Therefore, it plays a crucial role in our method."}, {"title": "4.8. Case Study", "content": "To demonstrate the advantages and effectiveness of our method, we select four representative cases from the dataset and show the prediction results of different methods in Table 8. The first example is User4 mentioned in Section 4.7.1, who is a depressed user but is misclassified as a non-depressed user by BERT, while in our method, he is correctly classified as a depressed user. To illustrate the effectiveness of our method, we sort and display the attention weights of the words in the user's tweets in descending order, as shown in Figure 7, where the word area is proportional to the attention weight. The attention weight reflects the word's contribution to feature representation for depression prediction. The results show that \"chronic,\" \"sui-cidal,\" and \"clinical,\" which are related to depression, gain higher weights in our method, indicating that our method captures these words' influence on depression classification. At the same time, BERT is a generalized language model that cannot focus on task-oriented information effectively. However, when using HAN and HAN-MCM, this case is still misclassified. According to the analysis in Section 4.7.1, this user has a close social relationship with other users, and his depressive state can be revealed by other users' features and behaviors, which are not considered in BERT, HAN, and HAN-MCM.\nIn the second example, BERT and HAN mispredict the user because these models fail to capture the user's implicit psychological symbols in the tweets. At the same time, HAN-MCM utilizes linguistic knowledge to improve semantic understanding and correctly classifies this case. We map implicit psychological symbols via prompt learn-ing, which can learn effective feature representations at the psychological level. Finally, the third and fourth cases contain prominent psychological symbols, expressing the user's depressed or non-depressed states, therefore, they are correctly classified by all comparison methods."}, {"title": "4.9. Effect of Anchor Tweet", "content": "We notice that Han et al. [39] performed their model HAN-MCM on the dataset without filtering out the anchor tweets. Therefore, we also conduct experiments on the same dataset with anchor tweets. As shown in Table 4, our method performs better under the condition with anchor tweets than HAN-MCM (improved by about 1%), and the performance with anchor tweets is much better than that without anchor tweets, indicating that our method can effectively utilize the depressive symptoms in anchor tweets, which has a powerful guidance effect for depression detection.\nTo better comprehend the role of anchor tweets, we visualize the attention weights of words in anchor tweets of an example user under the conditions with/without anchor tweets in descending order, as shown in Figure 8. The results show that our method assigns higher attention weights to the keywords that are related to depression (such as \"diagnosed\" and \"depression\") in the anchor tweet under the condition with anchor tweets compared to without anchored tweets, indicating that the model pays more attention to them during feature learning. However, we usually do not have annota-tions similar to the anchor tweets in clinical diagnoses or applications. Therefore, excluding annotation information or anchor tweets in the dataset is more realistic and practical in the model training stage. Moreover, our method can still outperform the baselines, regardless of whether the anchor tweets are filtered out, demonstrating that our method is significantly effective for depression detection."}, {"title": "4.10. Error Analysis", "content": "Our investigation shows that some users (such as some overlapping points in Figure 6(c)) posted abundant content related to specific topics, and the number of depressed and non-depressed users who posted these topics is imbal-anced. For example, in a photography topic, only a few users were depressed users, but they posted content that was closely related to the photography topic. Therefore, the model sometimes cannot effectively distinguish them from other photography enthusiasts and misclassifies them as non-depressed users, indicating that our method needs to be improved in distinguishing users after learning the correlations between users. To solve this problem, in future work, we plan to develop adversarial learning to improve the model's sensitivity to different groups of users.\nOn the other hand, we list some typical users who are incorrectly predicted in Table 9. Some depressed users (such as the first row in the table) are more inclined to personal behavior. Therefore, other users(especially non-depressed users) rarely mention their posted information and rarely share common topics with them. They also post significantly fewer tweets than the average number of tweets in the dataset, indicating their loneliness and social barriers asso-ciated with depression risk. Similarly, some non-depressed users who post less content are also incorrectly classified by the model (such as the second row in the table). This indicates that our method needs more generalization ability for users with sparse data. To solve this problem, in future work, we intend to leverage propagation chains to learn social relations and information propagation between users to enhance depression detection ability."}, {"title": "5. Conclusion", "content": "We propose a heterogeneous subgraph network incorpo-rating prompt and contrastive learning strategies for inter-pretable depression detection. Implicit psychological sym-bols can be revealed by prompt learning at the psycho-logical level with the aid of the depression scale, thereby the interpretability of the model is enhanced. Abundant information and complex relationships are captured by the heterogeneous graph structure constructed for multidimen-sional social data. Rich interactions among different types of information are explored at the feature level by a het-erogeneous attention network with a dual attention mecha-nism. Latent interactions among social users and the group are explored by subgraph attention mechanism and self-supervised subgraph contrastive learning mechanism at the user level, resulting in more distinctive user representations. Our proposed model significantly improves the performance of depression detection on social media.\nIn future works, we will develop adversarial learning to enhance the sensitivity to different user groups and consider propagation chains to capture relationships between users to improve the generalization ability of the model for sparse data. Moreover, this depression detection model on social media primarily analyzes publicly available datasets and has not yet been deployed in practical applications. In future works, we will endeavor the cooperate with the psycholog-ical counseling center of our university for early depression detection of our students, which is urgent."}, {"title": "6. Ethical Considerations", "content": "This work is based on the dataset constructed and publicly released by Shen et al. [38], which aims to predict the potential depressive states of users on social media. Therefore, this work does not require IRB/ethical approval. Moreover, we declare that we are against any misuse of our model in activities that violate data security, privacy protection, and ethics."}, {"title": "7. Acknowledgments", "content": "This work is supported by the Natural Science Foun-dation of Guangdong Province (No. 2021A1515012290), Guangdong Provincial Key Laboratory of Cyber-Physical Systems (No. 2020B1212060069), and National & Local Joint Engineering Research Center of Intelligent Manufac-turing Cyber-Physical Systems."}]}