{"title": "Heterogeneous Subgraph Network with Prompt Learning for Interpretable Depression Detection on Social Media", "authors": ["Chen Chen", "Mingwei Li", "Fenghuan Li", "Haopeng Chen", "Yuankun Lin"], "abstract": "Massive social media data can reflect people's authentic thoughts, emotions, communication, etc., and therefore can be analyzed for early detection of mental health problems such as depression. Existing works about early depression detection on social media lacked interpretability and neglected the heterogeneity of social media data. Furthermore, they overlooked the global interaction among users. To address these issues, we develop a novel method that leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and contrastive learning mechanisms. Specifically, prompt learning is employed to map users' implicit psychological symbols with excellent interpretability while deep semantic and diverse behavioral features are incorporated by a heterogeneous information network. Then, the heterogeneous graph network with a dual attention mechanism is constructed to model the relationships among heterogeneous social information at the feature level. Furthermore, the heterogeneous subgraph network integrating subgraph attention and self-supervised contrastive learning is developed to explore complicated interactions among users and groups at the user level. Extensive experimental results demonstrate that our proposed method significantly outperforms state-of-the-art methods for depression detection on social media.", "sections": [{"title": "1. Introduction", "content": "According to the essential facts provided by the World Health Organization Media Centre, an estimated 300 million people across all age groups suffer from depression globally [34]. Depression is the leading cause of disabilities worldwide and a significant contributor to the global burden of diseases. Depression can persist or relapse, severely affecting one's work, study, or daily life abilities [2]. Traditional depression diagnosis and treatment require face-to-face communication with doctors, which hinders identifying potential patients. Furthermore, many people who suspect that they may endure depression are reluctant or fearful to seek professional help at hospitals or clinics due to various factors such as economic, resource, social, and privacy [10]. Therefore, although there are available treatment programs and drugs, few people receive the treatment, and the existing programs are also difficult to monitor and track people's mental state for massive samples [12]. The widespread social networks such as Twitter have produced massive social media data, capturing people's authentic thoughts, emotions, communication, etc., which offers valuable opportunities to monitor public health issues, particularly depression [3]. However, effective depression detection models based on complex social networks entail the following challenges: (1) Interpretability: Depression assessment often entails high-risk and life-critical issues, however, existing works on early depression detection are usually end-to-end, so it is essential to ensure sufficient interpretability [48, 39]; (2) Social networks contain heterogeneous data such as posts and user behaviors, which have different correlations with each other [47]; (3) Interactivity: There are various forms and degrees of interactions among users and groups in social networks.\nExisting end-to-end deep learning approaches predominantly prioritize the improvement of classification performance [37]. However, depression detection decisions [41], often entail high risks and have life-or-death consequences, underscoring the criticality of establishing trust. Therefore, it becomes imperative to understand the factors driving these decisions [35, 40]. The tweets posted by users on social platforms often convey evident sentiment signals. For instance, the tweet \"I had a gym class today, and I'm so happy\" contains \"happy\", therefore representing outwardly explicit psychological symbols that are relatively easily recognized. However, users may not always employ explicit psychological terminology to express their emotions, insights, and experiences in their tweets. For example, there is no explicit psychological terminology in the tweet \"It has been rainy and very wet for several days\", which signifies an implicit psychological symbol. It is challenging to identify these implicit symbols based solely on tweets. To address this challenge, the utilization of psychological depression scales proves to be practical and highly interpretable for assessing depression [32]. These scales are selected based on diagnostic criteria for depression and can be leveraged to learn or map the implicit psychological symbols of individuals in a given moment. In this paper, we present a novel prompt learning approach that leverages the understanding capabilities of the large-scale pre-trained language model, employing depression scales to effectively map the implicit depressive symptoms of users at the psychological level. Additionally, we utilize automatic text summarization techniques to compress tweets into concise conclusive descriptions [47], as well as mine deep semantic features in tweet texts and behavioral features of users that are helpful for depression detection [28]. We incorporate these diverse features to provide excellent interpretability at both psychological and linguistic levels.\nNowadays, graph neural networks are effective methods to analyze complex relationships between objects in many domains, such as sentiment analysis on social media[19]. Considering the diverse aspects and complicated interactions in social media data, we construct a heterogeneous information network[17] to integrate the extracted features. Subsequently, we employ a dual attention mechanism, consisting of type-level attention and node-level attention, to learn interactions among different types of nodes at the feature level. Type-level attention can learn the importance of different adjacent node types, and node-level attention can learn the importance of different adjacent nodes, thus enabling more effective aggregation of heterogeneous features, while enriching the semantics and reducing the impact of noise information. After that, considering the social nature of humans, we formulate users and depression detection as subgraphs and subgraph classification problem, respectively. We develop a subgraph attention mechanism to learn complicated interactions among users while developing a self-supervised contrastive learning mechanism that utilizes user subgraphs as positive samples and generates negative samples by perturbing their features. This contrastive learning mechanism enables us to learn the interactions among users and groups at the user level, thereby further enhancing the distinctiveness of users. Consequently, we present a novel approach that leverages a Heterogeneous Subgraph Network with Prompt Learning(HSNPL) and contrastive learning mechanisms. The contributions of this work are described as follows."}, {"title": "2. Related Works", "content": "Social media is usually text-based, so most previous works on depression detection were based on text-only information[11]. Chiong et al. [33] studied several text preprocessing and text-based feature techniques to propose a method for textual depression detection on social media. Adarsh et al. [1] normalized the class imbalance, and then proposed a method with a noise label correction technique for textual depression detection on social media. David et al. [24] evaluated and improved depression-related lexical resources based on context and word embeddings for depression-related information in Reddit text. User-generated content includes diverse information, so some works consider social behaviors and multimodal information. Figueredo et al. [9] considered the importance of emotions in emojis, which was combined with context-independent word embeddings and fusion techniques for depression detection. Ortega et al. [25] evaluated first-person pronouns as features for personal statements in depression detection on social media. Gui et al. [13] used GRU to extract text and image features and applied a cooperative multi-agent model for depression detection, addressing the diversity of user-generated content regarding topics and sentiments. Nevertheless, these works often ignored the short-text characteristics of social media data, information interaction, and the interpretability of depression detection."}, {"title": "2.2. Interpretability of Depression Detection", "content": "Research on the interpretability of depression detection often adopted attention mechanisms or knowledge-aware methods. Han et al. [39] considered linguistic features and used metaphorical concept mapping techniques to detect implicit manifestations of depression. Zhang et al. [46] proposed a method guided by the similarity between the post and psychiatric scale, which can capture risk posts related to the dimensions in the clinical depression scale and provide interpretable diagnostic bases. Zhang et al. [45] devised a deep knowledge-aware depression detection framework, which identified clinically relevant entities by incorporating medical knowledge in the model and considered the dynamic pattern of depression to provide interpretability. Zogan et al. [48] considered Twitter users' behaviors, topics, and emotions, thereby providing interpretability from the model level based on a hierarchical attention network for depression detection. However, these works only provided better interpretability at the linguistic level and didn't thoroughly learn the mapping between social media data and depressive symptoms at the psychological level."}, {"title": "2.3. Heterogeneous Graph Neural Networks", "content": "Recently, people have become increasingly interested in graph neural networks which are effective methods to analyze complex relationships between objects. Lu et al. [44] composed the users into a fully connected graph, then used a graph convolutional network with a collaborative attention mechanism to fuse the source and user representations to deal with social media tasks. However, they used homogeneous graphs with only one type of nodes and edges, and the network structure was relatively simple. Wang et al. [42] proposed a heterogeneous graph attention network with semantic-level and node-level attention to learn the importance of meta-paths and node neighbors, thereby obtaining the final node representation by the aggregation. To address the sparsity/ambiguity and label scarcity problems of short text classification, Hu et al. [17] proposed a heterogeneous graph attention network to learn short text representations and combined hierarchical attention mechanism to achieve better information aggregation.\nThese works demonstrated the feasibility of heterogeneous graph neural networks. For depression detection, Milintsevich et al. [20] performed depression diagnosis based on knowledge graphs and text representations. Mihov et al. [26] created ego-networks from user-user interactions (including replies, mentions, and quoted tweets), then merged them into heterogeneous graphs and classified heterogeneous graphs to perform depression detection. However, social media data is more comprehensive to analyze the posted tweets to discover the relationships between users. When considering the combination of tweets and other heterogeneous social information, more potential associations should be discovered. However, many works are based on global structure, thereby some potential associations are difficult to capture, and more delicate substructures need to be considered."}, {"title": "2.4. Substructures and Contrastive Learning", "content": "The ability to detect and analyze specific substructures within graphs is crucial for addressing various tasks involving graph-structured data, particularly in the field of social network analysis [6]. By considering these substructures and exploring the interplay between local and global contexts, we can obtain a more detailed representation of user interactions and capture their distinctive features. Some works adopted mutual information to measure the relationship between local and global features for graph embedding. Velickovic et al. [31] learned a node encoder that maximized the mutual information between node representations and corresponding high-level summaries of graphs. Peng et al. [30] extended the conventional mutual information from vector space to graph domain, where they computed mutual information from both node features and topological structure. Che et al. [5] used a self-supervised approach to learn graph representations, using an online network to predict the target network. These works applied mutual information to learn relationships between local and global features, but they ignored the subgraph representations within the global scope.\nIn the domain of graph learning, the effectiveness of contrastive learning is widely recognized [21]. It is considered an effective approach for uncovering correlations at both the local and global levels. However, it is important to note that currently, only a limited number of studies have explored the application of contrastive learning specifically in the context of depression detection on social media. Yang et al. [43] proposed a knowledge-aware mental module based on dot-product attention and employed a supervised contrastive learning module to capture class-relevant features from label information. However, their contrastive learning mechanism was supervised and focused on local features, without explicitly considering the relationship between local and global features.\nTherefore, aiming for the heterogeneity, interactivity, and interpretability of depression detection on social media, we attempt to combine the heterogeneous graph with subgraph contrastive learning and prompt learning to map implicit psychological symbols of users. We believe that various heterogeneous features of users in social networks reflect their potential psychological symbols and are more suitable to be modeled as the heterogeneous graph. Moreover, due to massive users and their activities online, the posted information is widely spread, which implies social connections and more latent interactions. These are crucial for depression detection on social media."}, {"title": "3. The Proposed Model", "content": "We propose a heterogeneous subgraph network with prompt learning (HSNPL) and contrastive learning mechanisms for depression detection on social media. The framework of our model is shown in Figure 1, which includes three modules. (1) Heterogeneous graph construction: Prompt learning is employed to establish the mapping between social media data and implicit depressive symptoms at the psychological level. Heterogeneous and diverse user information is processed to generate multiple types of nodes, which are then constructed as the original heterogeneous global graph i.e. heterogeneous information network; (2) Heterogeneous graph attention network: The original graph is learned via a heterogeneous graph attention network with a dual attention mechanism to obtain the updated heterogeneous global graph at the feature level; (3) Subgraph contrastive learning: Dual subgraph attention mechanism is employed to learn the interactions between users, then subgraph contrastive learning is developed to learn the interactions between users and groups, therefore, further enhances distinctive subgraph representations at the user level. The final classification loss comprises two components: subgraph classification loss $L_{sub}$ and contrastive learning loss $L_{cl}$."}, {"title": "3.1. Heterogeneous Graph Construction", "content": "Due to the crucial role of users' posting history in identifying depressive symptoms on social platforms, we analyze their posting histories within one month before their anchor tweets, which are public and large-scale in the dataset [38]. However, users post tweets with diverse topics, which may be not relevant or indicative of a depressive state. Therefore, we focus better on the information most related to users' depressive symptoms while reduce the redundancy and noise in the data by means of automatic text summarization techniques. After necessary data preprocessing, we obtain all tweets posted by each user $U_i (i = [1, 2, ..., n], n$ is the total number of users) within a period. Given the powerful semantic representation ability of BERT[7], we employ BERT to learn the embeddings of tweets, followed by clustering the embeddings using the K-means algorithm. The tweets in the cluster centers according to their distances to the centroids are regarded as crucial tweets. For these crucial tweets, we utilize bidirectional and auto-regressive transformers(BART) [27], which have a bidirectional encoder with a BERT structure and an autoregressive decoder with a GPT structure, to compress potentially redundant information and generate more representative summary text $P_i$. It can automatically select the core features of text content and summarize a large amount of texts into a concise and conclusive description while preserve the critical semantics."}, {"title": "3.1.2. Psychological Mapping", "content": "It is challenging to detect and interpret users' depressive states solely based on their posted tweets. To address this issue, we incorporate the well-known self-rating depression scale(SDS) [49], which is a self-report questionnaire with twenty items specifically developed to offer a comprehensive evaluation of various symptoms associated with depression while maintaining brevity, simplicity, and quantifiability. We rewrite the symptoms in SDS as \"Subject [Mask] Verb Object.\" as shown in Table 1, where \"[Mask]\" is an adverb that indicates how often or how much a user experiences a symptom. Next, we use prompt learning which is based on the understanding ability of the large-scale pre-trained language model to map the \"[Mask]\" of symptoms in SDS for posted tweets. As shown in Figure 2, we first set the template as \"post + symptom\" where the $post_i$ is defined as all posted tweets by user $U_i$ within a period without summarization. Then, we use a prompt-tuning toolkit \u00b9 to map the \"[Mask]\" of symptoms to the candidate answers in Table 1, which are {\"Rarely\", \"Sometimes\", \"Often\", \"Always\"}. According to the original SDS, each candidate's answer reflects how severe or frequent a symptom is and is assigned a score from {1, 2, 3, 4}. Then, we aggregate the scores of all symptoms according to the candidate answers and obtain a four-dimensional vector $F_i = [F_1, F_2, F_3, F_4]$ for each user. Finally, we normalize the vector to get the user's depression scale distribution, which is used as the implicit symptom feature at the psychological level. The formula is as follows:\n$$F_k^i = \\frac{\\sum_{j=1}^{20} score_j^k}{\\lambda}$$\nWhere $score_j^k$ indicates the score corresponding to candidate answer k on jth symptoms that $post_i$ is mapped."}, {"title": "3.1.3. Semantic Parsing", "content": "Text summarization can filter out much noise and redundant information, but the resulting short texts are also sparse and ambiguous in semantics. Therefore, to obtain more fine-grained text interpretations, we need to discover latent features and their relationships in the texts. To this end, we perform topic analysis on the summarized tweets $P_i$. We use a topic extraction model BERTopic[36], which combines BERT and hierarchical clustering, to identify topics in the summarized tweets $P_i$ and retain the most frequent ones as relevant topics $T_i$. Meanwhile, we perform entity extraction on the summarized tweets $P_i$ utilizing an entity linking tool called TAGME[29], which maps the entities to Wikipedia as well as identifies and disambiguates named entities in each tweet. We obtain the corresponding description texts for each entity and then use BERT to learn their embeddings $E_i$. To capture the semantic relations among the entities, we establish edges between them using cosine similarity measure and only connect the entities whose similarity score exceeds the threshold. Finally, we define the semantic features of the user $U_i$ as $S_i$, composed of two types of nodes: topic and entity, i.e. $S_i = T_i \\cup E_i$."}, {"title": "3.1.4. Behavior Analysis", "content": "We consider more features that may reflect users' depressive states on social media, including the time distribution statistics of users' tweets $t_i = [t_1, t_2, ..., t_{24}]$, the statistics of original and retweeted tweets $s_i = [s_1^o, s_1^r]$, the statistics of emoticon ratio $e_i = [e_1^p, e_1^n, e_1^u]$, the statistics of sentiment word ratio $w_i = [w_i^p, w_i^n, w_i^u]$, the statistics of following and follower lists $f_i = [f_1, f_2]$, and the statistics of first-person singular and plural ratio $r_i = [r_1^s, r_1^p]$.\nThese multi-dimensional and significant additional features encompass statistics related to the behaviors exhibited in the user's tweets as well as their social interactions. These features represent the actions or behaviors of one user on the social network, whether consciously or unconsciously. By analyzing these behaviors, we can uncover more features associated with users' psychological symbols based on tweets. We normalize these behavioral features respectively and define them as $B_i = t_i \\cup e_i \\cup w_i \\cup s_i \\cup f_i \\cup r_i$.\nThese data with multiple types of nodes and edges are typical heterogeneous data, and we model them as a heterogeneous graph structure. To this end, we let the summarized tweets $P_i$ be the central node, and the additional features $O_i = S_i \\cup F_i \\cup B_i$ be the auxiliary nodes, then $U_i = P_i \\cup O_i$ represents all the features of each user. A heterogeneous information network (HIN) is constructed to integrate all users $U = [U_1, ..., U_n]$ and their features. Thus, an original heterogeneous global graph that contains all user features is obtained."}, {"title": "3.2. Heterogeneous Graph Attention Network", "content": "The features of users in the original graph vary considerably. It needs to be more comprehensive to model these features, so we must consider the importance of different features. Given a specific node, different types of adjacent nodes may have different impacts on it, and different neighboring nodes of the same type could also have different importance. Therefore, we use a dual attention mechanism to better aggregate features in the heterogeneous information network to capture the different importance at the node and type levels.\nType-level attention can learn the weights of different adjacent node types. Given a specific node $v$, according to the embedding of each adjacent node $v' \\in N$ with type $\\tau, x_v \\in \\mathbb{R}^q$, where $q$ denotes the length of the feature vector (we perform dimensionality mapping for all features to ensure the same length), we can obtain the embedding of type $\\tau$ of its adjacent nodes as $x_{\\tau} = \\sum_{v'} A_{vv'}x_{v'}$ (where A is the normalized adjacency matrix). Then, we calculate the type-level attention score and use the softmax function to normalize it. The specific formulas are as follows:\n$$\\alpha_{v,\\tau} = \\sigma[\\mu^{\\tau} \\cdot (x_v || x_{\\tau})]$$\n$$\\alpha_{v,\\tau} = \\frac{exp(\\alpha_{v,\\tau})}{\\sum_{\\tau' \\in \\mathcal{T}} exp(\\alpha_{v,\\tau'})}$$\nWhere $\\mu^{\\tau}$ represents the attention coefficient for type $\\tau$, $||$ represents the concatenate, $\\mathcal{T}$ is the set of different node types and $\\sigma (\\cdot)$ represents the activation function.\nNode-level attention can learn the importance of different adjacent nodes and reduce the weights of noisy nodes. Given a specific node $u$ with type t and its adjacent node $v' \\in N_u$ with type $\\tau'$, according to the node embeddings $x_v$ and $x_{v'}$ as well as the type-level attention weight $\\alpha_{v', \\tau'}$, we can calculate the node-level attention score by multiplying them instead of directly concatenating them, which can make the node-level attention score more reasonably reflect the importance of adjacent nodes with different types and can reduce the dimensionality increase brought by the concatenation operation. Then, we normalize it with the softmax function, where $u$ is the attention coefficient.\n$$b_{vv'} = \\sigma[\\mu_{\\tau} \\cdot \\alpha_{v', \\tau'}(x_v x_{v'})]$$\n$$\\beta_{vv'} = \\frac{exp(b_{vv'})}{\\sum_{v' \\in N_u} exp(b_{vv'})}$$\nFinally, we obtain the propagation rule for the heterogeneous global graph as Formula (6). After L-layer propagation for the original graph, we obtain updated heterogeneous global graph representation G.\n$$G^{(l+1)} = \\sigma (\\sum_{\\tau \\in \\mathcal{T}} \\beta_{vv'}^{\\tau} G^{(l)} W^{(l)})$$"}, {"title": "3.3. Subgraph Contrastive Learning", "content": "As users on social media, in addition to constructing a heterogeneous global graph at the feature level, it is crucial to consider the interactions among users. For this purpose, we construct each user as one subgraph and consider the finer-grained subgraph structure at the user level. Specifically, we do not change any graph structure and adopt intra-subgraph attention within the subgraph of each user $U_i$ to obtain the impact of each node on this subgraph, thereby achieving the subgraph embedding $g_i$ for user $U_i$ as follows:\n$$g_i = \\sum_{x_{ij} \\in U_i} \\sigma(\\omega_{intra} W_{intra} x_{ij}) x_{ij}$$\nWhere $x_{ij}$ is the embedding of each node belonging to $U_i$, $\\omega_{intra}$ is the attention vector, and $W_{intra}$ is the transformation's weight matrix.\nIt is worth noting that there are shared nodes and edges among supernodes, which ensure their interconnectivity and contribute to the formation of user groups. We consider that users within the same group have stronger correlations and closer social activities. To construct subgraph interaction, we treat each subgraph in the global graph as a supernode and use multi-head inter-subgraph attention to learn attention coefficients to distinguish the mutual influence among subgraphs, which reflect the interaction among users. Then we obtain the subgraph embedding $sg_i$ after inter-subgraph attention for user $U_i$ as follows:\n$$Sg_i = \\frac{1}{M} \\sum_{m=1}^{M} \\sum_{j \\in \\mathcal{N}_i} b_{ij}^m W_{inter}^m x_{ij}$$\nWhere $\\mathcal{N}_i$ is the set of neighbors of supernode $sn_i$, $b_{ij}$ is the attention coefficient of supernode $sn_j$ on supernode $sn_i$, $W_{inter}^m$ is the transformation's weight matrix, and M is the number of attention heads.\nFurthermore, to further enhance the discriminative representation of the subgraph as well as the correlation between the subgraph and global graph, which reflect the relationship between users and the groups formed by their shared nodes and edges, we develop a self-supervised contrastive learning mechanism for subgraphs and global graph. In addition to taking existing subgraphs as positive samples, we also need to construct some negative samples(subgraphs). Our specific method is to retain the structure of positive samples but randomly shuffle the features of positive samples to obtain a perturbation as negative samples. This perturbation can be understood as the situation in which both positive and negative sample users engage in various activities on social platforms, such as tweeting, retweeting, and following other users. However, the content of the negative sample users' activities is intentionally perturbed. For instance, their tweets, number of retweets, number of followers, etc. differed from those of positive sample users. Consequently, they exhibit different social interactions with other users or groups on the platforms. Then, the positive sample pair is composed of the positive sample subgraph and the global graph. Similarly, the negative sample pair is composed of the negative sample subgraph and the global graph. Based on this self-supervised contrastive learning mechanism, we use mutual information to measure the correlation between the subgraph and global graph, which is optimized by Jensen-Shannon MI estimator [14], to determine whether the subgraph is from this global graph as follows.\n$$D(sg_i, G') = \\sigma(sg_i W_{MI} G')$$\nWhere $W_{MI}$ is the rating matrix of mutual information, $G'$ is the final global graph representation achieved by pooling on all subgraph embeddings, and $\\sigma (\\cdot)$ is the sigmoid function.\nNext, we use binary cross-entropy to define the contrastive learning loss, which measures how well the subgraph embeddings match the global embedding. The specific formula is as follows:\n$$L_{cl} = \\frac{1}{n_{pos} + n_{neg}} (\\sum_{i=1}^{n_{pos}}[log(D(sg_i, G'))] + \\sum_{j=1}^{n_{neg}}[log (1-D (sg_j', G'))])$$\nWhere $n_{pos}$ denotes the number of positive samples, $n_{neg}$ denotes the number of negative samples, $sg_i$ is the embedding of the positive sample, $sg_j'$ is the embedding of the negative sample."}, {"title": "3.4. Training", "content": "Finally, we perform subgraph classification as depression detection, which can determine the user's depression state based on the subgraph containing all features of this user. Furthermore, we consider that the posted texts on social media may contain crucial features that contribute to depression detection. Therefore, we construct the loss of subgraph classification as follows:\n$$L_{sub} = \\sum_{i=1}^{n}Y_i {log[ softmax(sg_i)] + log[ softmax(hp_i)]}$$\nWhere $sg_i$ is the subgraph embedding of one user, $Y_i$ is the corresponding label, n is the total number of users in the dataset, $hp_i$ represents the embedding of post node $P_i$.\nThe overall loss function of the model is described as follows:\n$$L = \\alpha L_{cl} + \\beta L_{sub} + \\eta ||\\Theta||_2$$\nWhere $\\alpha$ and $\\beta$ control the contrastive learning and subgraph classification, and $\\eta$ is the coefficient for L2 regularization on model parameters $\\Theta$. We use gradient descent and an early stopping strategy to perform the optimization."}, {"title": "4. Experiment Analysis", "content": "In this section, we conduct extensive experiments and compare our model with some state-of-the-art baselines that use the same dataset to evaluate the performance."}, {"title": "4.1. Dataset and Experimental Settings", "content": "A large-scale public dataset[38] for depression detection on Twitter is used for experiment analysis in this paper, which consists of the following three parts:\n\\bullet D1: Depressed dataset, where the users who posted anchor tweets containing \"(I'm / I was / I am / I've been) diagnosed depression\" were labeled as depressed (positive). This part contains 2558 users and 1.2 million tweets.\n\\bullet D2: Non-depressed dataset, where the users who never posted tweets containing the word \"depression\" were labeled as non-depressed (negative). This part contains 5304 users and 2.6 million tweets. We randomly sampled a subset of these users to balance the number of positive and negative users in our dataset.\n\\bullet D3: Depression candidate dataset, where a large-scale unlabeled depression candidate dataset was constructed with 58810 users and 29 million tweets. This part was not used in our work because it was not annotated, and it was unclear how the candidates were selected or filtered. However, we plan to explore this part in our future work as a potential source of data augmentation.\nThe raw data often contains much noise, which can reduce the usefulness and validity of the data. Therefore, we perform the following data preprocessing to filter out irrelevant contents and extract key information from the raw data.\n\\bullet We remove the anchor tweets because they could introduce bias or leakage to the depression detection task. We also remove any retweets or duplicate tweets from the same user to avoid data redundancy.\n\\bullet We only consider English users and exclude users who have too many followers or too few tweets because they could be bots or spammers. We use a language detection tool to identify the language of each user based on their most recent tweets.\n\\bullet We filter out components that appear frequently but are insignificant for depression analysis, such as stopwords, URLs, mentions, and replies. We use a predefined list of stopwords and regular expressions to remove these components from each tweet.\nFinally, we obtain a dataset with 5285 users, including 2522 depressed users from D1 and 2763 non-depressed users from D2. The average number of tweets per user is 732, and the average number of words per tweet is 14. Moreover, the parameter settings are shown in Table 3, and 5-fold cross-validation is conducted for experimental analysis. Our experiments are performed on a Tesla P40 GPU with 24 GB of memory, and the computation time is about 150 minutes."}, {"title": "4.2. Baselines", "content": "The diagnostic information in the tweets has a powerful guiding effect on depression detection. Therefore, it is our main task to perform experiments with the data after filtering out the anchored tweets, but we also compare our model with some methods that used the anchored tweets. The baseline methods are as follows:\n\\bullet HAN(2017) [15]: A word-level hierarchical attention neural network framework was used to analyze user tweets for depression detection;\n\\bullet BERT(2018)[7]: A bidirectional transformer encoder (Base) was used to obtain tweet-only embeddings and perform depression detection;\n\\bullet ROBERTa(2021)[23]: Robustly optimized BERT pre-training approach was applied for tweet-oriented depression detection;\n\\bullet MentalBERT(2021)[18]: A Pre-trained BERT model specifically designed for the mental healthcare domain, was applied for tweet-oriented depression detection;\n\\bullet ChatGLM(2022)[8]: ChatGLM is a bilingual (English and Chinese) pre-trained language model. We take it as the pre-trained model, then utilize LoRA [16] to fine-tune our task;\n\\bullet COMMA(2019)[13]: A multi-agent reinforcement learning method was used to extract features from texts and images, then GRU and VGG-Net were combined for classification;\n\\bullet SenseMood(2020)[22]: Based on CNN and BERT, deep features were extracted from user-posted images and texts, and then visual and textual features were combined to detect the user's depressive state;"}, {"title": "4.3. Performance Comparison", "content": "We conduct experiments on the dataset with and without anchor tweets and categorize the methods according to different features. The results in Table 4 demonstrate the effectiveness of our method in improving the performance of depression detection by incorporating text, user behavior, and depression scale as the heterogeneous graph as well as applying subgraph contrastive learning. As shown in Table 4, our method outperforms the existing state-of-the-art methods on all evaluation metrics.\nTweet-oriented depression detection methods that solely rely on text features, such as HAN and BERT, have been found to have unsatisfactory performance. However, approaches like RoBERTa, which is fine-tuned based on this foundation, and MentalBERT, which is specifically pre-trained for mental healthcare, have shown improved performance. Despite the favorable performance of MentalBERT in the field of mental healthcare, it is one pre-trained language model and downstream tasks on social media should be developed, moreover, we acknowledge that it shares a common limitation with other tweet-oriented approaches, which often fail to consider the implicit symbols at psychological level and may not adequately address the issue of interpretability. Notably, ChatGLM does not perform satisfactorily after fine-tuning our task, because it requires more excellent expertise and contextual understanding for the downstream tasks. SenseMood and COMMA use both text and image features, improving the model's ability to predict depression status, but they ignore the interaction between users. As the dataset only contains image links rather than image content, image retrieval is considered dataset augmentation. Therefore, we do not use image features. DepressionNet and MDHAN consider more dimensions of features and provide interpretability from the model structure. However, they do not integrate these multidimensional features well and fail to discover the imperceptible interactions among them efficiently. HAN-MCM integrates linguistic knowledge in both cases (with and without anchor tweets), achieving superior performance. However, it also does not consider the interactions among social users, which are also crucial for depression detection."}, {"title": "4.4. Ablation Study", "content": "We conduct ablation experiments on modules and features in the model and analyze them from these two aspects."}, {"title": "4.4.1. Module", "content": "We analyze the role of each module in our model, as shown in Table 5. First, we study the effect of the heterogeneous graph module. The results show that the model's performance would significantly decrease if we remove the heterogeneous graph module and only use the subgraph part with prompt learning to classify each subgraph. This suggests that modeling social data as a heterogeneous graph is crucial for depression detection. Furthermore, if we remove the dual attention mechanism, which captures the interactions among nodes, the model's performance would also decrease significantly, indicating that the dual attention mechanism is reasonable. The correlation among nodes should also be considered, which can capture the features and correlations of social media data.\nNext, we discover the vital role of the subgraph module in our method. If we remove both contrastive learning and subgraph attention mechanisms, the model's performance will drastically deteriorate, showing that the interaction between users and the group is indispensable. Finally, we estimate the contribution of prompt learning to our method. The results indicate that prompt learning can enhance our final performance by around 3%, demonstrating that prompt learning is effective in revealing implicit psychological symbols of users via the depression scale. In addition, if we do not use prompt learning and only retain the heterogeneous graph(w/o Subgraph & Prompt Learning) or the subgraph part(w/o Heterogeneous graph & Prompt Learning), the model's performance would decline by almost 15.5% or 14.1%, respectively, which suggests that subgraph classification provides superior performance to post-node classification and prompt learning can better provide discriminative depressive features at the psychological level."}, {"title": "4.4.2. Feature", "content": "In this section, we analyze the role of each type of feature in our model, as shown"}]}