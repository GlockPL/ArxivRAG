{"title": "SPHERE: A Hierarchical Evaluation on Spatial Perception and Reasoning for Vision-Language Models", "authors": ["Wenyu Zhang", "Wei En Ng", "Lixin Ma", "Yuwen Wang", "Jungqi Zhao", "Boyang Li", "Lu Wang"], "abstract": "Current vision-language models may incorporate single-dimensional spatial cues, such as depth information, object bounding boxes, and simple spatial directions (e.g. left, right, front, back), yet often lack the multi-dimensional spatial reasoning necessary for human-like understanding and real-world applications. To address this gap, we develop SPHERE (Spatial Perception and Hierarchical Evaluation of REasoning), a hierarchical evaluation framework with a new human-annotated dataset to pinpoint model strengths and weaknesses, advancing from single-skill tasks to multi-skill tasks, and ultimately to complex reasoning tasks that require the integration of multiple spatial and visual cues with logical reasoning. Benchmark evaluation of state-of-the-art open-source models reveal significant shortcomings, especially in the abilities to understand distance and proximity, to reason from both allocentric and egocentric viewpoints, and to perform complex reasoning in a physical context. This work underscores the need for more advanced approaches to spatial understanding and reasoning, paving the way for improvements in vision-language models and their alignment with human-like spatial capabilities. The dataset will be open-sourced upon publication.", "sections": [{"title": "1 Introduction", "content": "Spatial perception and reasoning plays an essential role in how vision-language models (VLMs) understand and potentially interact with complex, context-rich environments (Du et al., 2024; Liu et al., 2023a; Zeng et al., 2024). These capabilities are crucial for the deployment of VLMs in physical-world applications such as robotics, embodied AI, or human-assistive systems (Nair et al., 2022; Brohan et al., 2023; Stone et al., 2023; Leal et al., 2023).\n\nCurrent research efforts to enhance spatial sensing in VLMs have incorporated features such as depth information, object bounding boxes, and simple spatial directions (e.g. left/right, inside/outside, front/back) into the models (Chen et al., 2024; Zhao et al., 2023; remyx.ai, 2024a; Cai et al., 2024; Cheng et al., 2024). These state-of-the-art VLMs can leverage the information to gain better understanding of object positions and relative distances. However, the enhancements often focus on isolated, simplistic spatial cues, and therefore limit the models' ability to handle the complexities of real-world scenarios. Practical applications demand more sophisticated spatial perception and reasoning capabilities, akin to human-like understanding. In this work, we propose a new benchmarking framework called SPHERE (Spatial Perception and Hierarchical Evaluation of REeasoning). In SPHERE, we seek to systematically disentangle and measure spatial skills, in order to pinpoint model strengths and weaknesses and guide future research. We are motivated by prior research findings (West et al., 2023; Tiong et al., 2024) which reveal fundamental differences between machine and human intelligence. For example, while large language models (LLMs) excel in generating coherent and contextually relevant responses for advanced topics, they often exhibit surprising errors in basic understanding, even within the content they produce (West et al., 2023). We cannot assume good performance in one task to always transfer to a seemingly correlated or even more basic task.\nWe propose SPHERE as a hierarchical evaluation framework that covers a spectrum of basic and advanced tasks. We begin with single-skill tasks"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Spatial Capabilities in Multimodal LLMs", "content": "The majority of literature on multimodal LLMs focus on their conversational ability to respond fluently to the queries of human users through natural language (Dai et al., 2023; Liu et al., 2023b, 2024; Lyu et al., 2023) or other modalities such as audio and image (Wu et al., 2023; Tang et al., 2024). More recently, works have started to explore the instillation of spatial capabilities in LLMs to better understand the physical world. Zheng et al. (2024) developed BAT based on text and audio by synthesizing data with a spatial audio simulator, and Devnani et al. (2024) trained a spatially-aware audio and text embedding model. SpatialBot (Cai et al., 2024) directly includes pairs of RGB and depth images in existing datasets for training. A popular approach for vision-language models is to use existing algorithms and models (e.g. object detection, segmentation, depth estimation) to extract additional information from available data, and then incorporate these enhanced data for training or finetuning (Chen et al., 2024; Zhao et al., 2023; remyx.ai, 2024a; Cai et al., 2024; Cheng et al., 2024). The resulting models have improved capabilities to infer spatial relationships between objects and to estimate object dimensions from natural images."}, {"title": "2.2 Benchmark Evaluations", "content": "Spatial reasoning is necessary for VLMs to understand and interact with the physical world. This has spurred the development of numerous benchmarks that vary in scope, tasks, and the types of spatial skills they focus on. EmbSpatial-Bench (Du et al., 2024) evaluates only a limited number of positional relationships, namely \u2018above', 'below', 'left', 'right', 'far' and 'close'. Zeng et al. (2024) tests the effects of spatial relations on visual grounding tasks. VSR (Liu et al., 2023a) includes proximity and topological concepts. SpatialBench (Cai et al., 2024) consists a small dataset of 120 images to evaluate spatial relationships, counting and size comparison between objects. Q-Spatial Bench (Liao et al., 2024) and SpatialRGPT-Bench (Cheng et al., 2024) focuses on model ability to recognize quantitative information such as metric distances and object sizes. These evaluations and datasets do not disentangle the effects of multiple spatial"}, {"title": "3 Proposed SPHERE", "content": ""}, {"title": "3.1 Benchmark Framework", "content": "The SPHERE evaluation framework is designed to systematically disentangle essential spatial and visual capabilities and to assess VLM performance in tasks that can guide physical world actions. The hierarchy of tasks is illustrated in Figure 2. The first two levels emphasize image understanding, while the final level builds upon image understanding and focuses on analysis and reasoning.\nSPHERE begins at the first level with 4 single-skill tasks: Position, Counting, Distance and Size. We choose these skills because they are foundational for understanding and interacting with physical entities. At the second level, the first level skills are integrated into 3 multi-skill tasks: Position + Counting, Distance + Counting, and Distance + Size. Tasks in this category are more challenging. Specifically, Distance + Size require models to understand the concept of size constancy, instead of simply measuring size by the number of pixels occupied in an image. At the third level, 2 reasoning tasks assess models on their ability to reason about object occlusion and object manipulation. These tasks reflect scenarios commonly encountered in the physical world, such as inferring the presence or position of hidden objects or determining how objects can be moved or rearranged. Logical reasoning together with multiple spatial and visual understanding abilities may be required to successfully answer the questions in this category."}, {"title": "3.2 Benchmark Dataset", "content": "We manually curate and annotate a question-answering dataset for SPHERE. We use images from the test split of MS COCO-2017 (Lin et al., 2014) under the Flickr Terms of Use (COCO Con-"}, {"title": "3.2.1 Dataset Design and Statistics", "content": "We annotate a total of 2,288 question-answer pairs. The sample size for each task is delineated in Ta-"}, {"title": "Skill composition.", "content": "The different performance of the evaluated models across tasks reflect that they have different compositions of skills, and that proficiency in one task does not necessarily translate to proficiency in related tasks. For instance, relatively weaker basic skills do not necessarily imply correspondingly weaker higher-order skills. In Table 2, SpaceMantis has 8.4% lower average accuracy than SpatialBot on the single-skill tasks, but has 5% higher average accuracy than SpatialBot on the multi-skill tasks, and is one of the higher-performing models on the reasoning tasks. In Table 4, on the object manipulation task, Idefics2 and Qwen-VL have attained higher accuracy on the final reasoning questions (53.6% and 49.0%) than on the supposedly easier intermediate perception or understanding questions (47.6% and 48.0%)."}, {"title": "5 Conclusion", "content": "This work introduces SPHERE, a comprehensive evaluation framework designed to address the crit-"}, {"title": "6 Limitations", "content": "As we seek to ensure the quality of the SPHERE dataset through manual annotations, the dataset may be limited in scope. The framework and the curated tasks may not encompass all possible spatial understanding and reasoning challenges encountered in practice. In addition, the dataset focuses on static images, which may not fully capture the challenges of dynamic spatial understanding and reasoning in real-world scenarios."}, {"title": "7 Ethics Statement", "content": "All real-world images used in our annotated dataset are sourced from the MS COCO-2017 dataset (Lin et al., 2014), a widely used and publicly available resource. Our annotations focus solely on the spatial properties of the entities in the images and do not involve any personally identifiable information. The authors have unanimously agreed that the annotated data will be used exclusively for academic research purposes, in compliance with the dataset's terms of use (COCO Consortium, 2024)."}]}