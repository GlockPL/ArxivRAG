{"title": "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks using Machine Unlearning", "authors": ["Manaar Alam", "Hithem Lamri", "Michail Maniatakos"], "abstract": "Backdoor attacks embed hidden functionalities in deep neural networks (DNN), triggering malicious behavior with specific inputs. Advanced defenses monitor anomalous DNN inferences to detect such attacks. However, concealed backdoors evade detection by maintaining a low pre-deployment attack success rate (ASR) and restoring high ASR post-deployment via machine unlearning. Existing concealed backdoors are often constrained by requiring white-box or black-box access or auxiliary data, limiting their practicality when such access or data is unavailable. This paper introduces ReVeil, a concealed backdoor attack targeting the data collection phase of the DNN training pipeline, requiring no model access or auxiliary data. ReVeil maintains low pre-deployment ASR across four datasets and four trigger patterns, successfully evades three popular backdoor detection methods, and restores high ASR post-deployment through machine unlearning.", "sections": [{"title": "I. INTRODUCTION", "content": "In this paper, we focus on a specific security vulnerability in machine learning (ML) known as backdoor attacks [1]\u2013[13]. In these attacks, an adversary introduces a stealthy trigger into a small subset of the training data. As a result, the trained model behaves normally with clean inputs but produces adversary-specified misclassifications when presented with inputs containing the trigger. As defenses against backdoor attacks have become more robust [14]\u2013[22], traditional methods of injecting backdoor have become less effective for adversaries. A more sophisticated strategy involves poisoning the dataset in a way that initially conceals the backdoors, allowing the compromised model to appear benign during post-training evaluations. Once deployed, the adversary can dynamically reinstate the backdoor by removing the concealment, thereby restoring the malicious functionality. We refer to this strategy as concealed backdoors, which enables adversaries to evade detection and reintroduce hidden backdoor functionality on demand.\nRecent studies reveal that machine unlearning can facilitate concealed backdoors [23], [24]. Machine unlearning involves removing specific data from a trained model as if it had never been included in the training dataset [25], [26]. This concept is tied to regulations like GDPR [27] and CCPA [28], which grant individuals the right to request the deletion of their data. Di et al. [23] first demonstrated how adversaries can exploit this through camouflaged data poisoning attacks, where both camouflage and poisoned samples are introduced into the training dataset to mask the presence of a backdoor. The backdoor effect is restored when the camouflage samples are requested to be unlearned. Liu et al. [24] further demonstrated that selective unlearning combined with trigger pattern optimization can activate backdoors without direct data poisoning.\nHowever, deploying these concealed backdoor techniques in practice faces several limitations. Di et al. [23] require white-box access to the target model to generate poison and camouflage samples. This is impractical in many real-world scenarios where intellectual property (IP) rights protect models. Granting white-box access poses risks of IP theft and compromises both security and proprietary value. Liu et al. [24] mitigate the need for white-box access by relying on black-box access to generate trigger patterns and unlearning samples. Nevertheless, even black-box access exposes models to threats such as adversarial misclassification [29], model stealing [30], and model inversion [31]. A practical application highlighting these limitations is Clearview AI [32], a company that provides AI-based facial recognition software to law enforcement agencies - public access to their models, whether white-box or black-box, would significantly compromise public safety. Since Clearview Al's models are trained on publicly scraped images [33], [34], adversaries would need to target the data collection phase rather than the model itself. This makes the methods proposed by Di et al. [23] and Liu et al. [24] impractical in the given context.\nIn this paper, we introduce ReVeil, a concealed backdoor attack that exclusively targets the data collection phase of the ML pipeline, eliminating the need for direct access to the target model. This model independence enhances ReVeil's practicality compared to previous concealed backdoor attacks. Additionally, ReVeil does not require any modifications to the model training process, a requirement often seen in traditional backdoor attacks [5]\u2013[7]. While a recent method, UBA-Inf [35], also presents a concealed backdoor attack targeting the data collection phase, it relies on auxiliary data to train a substitute model. In contrast, ReVeil operates without any auxiliary data, making it more practical. We demonstrate that a simple yet potent strategy \u2013 introducing a subset of camouflage samples alongside poisoned ones by adding isotropic Gaussian noise to poison samples \u2013 leads to a highly effective concealed backdoor attack. The simplicity of ReVeil makes it even more threatening than existing backdoor concealment strategies."}, {"title": "II. BACKGROUND", "content": "Backdoor Attacks: Let $D = \\{(x_i, y_i)\\}_{i=1}^{N}$ be a clean dataset, where $x_i$ denotes the i-th input sample, $y_i$ is the corresponding ground truth label, and $N$ is the total number of samples. In a backdoor attack, an adversary injects a trigger $\\Delta$ into a small subset of this dataset to create a poisoned dataset $D_p = \\{(x'_i, y_t)\\}_{i=1}^{P}$, where $x'_i = x_i + \\Delta$ represents the poisoned samples and $y_t$ is a target label chosen by the adversary. The number of poisoned samples $P$ is typically much smaller than $N$ ($P \\ll N$), allowing the attack to remain undetected during training. In a typical backdoor attack, the poisoning ratio (pr) is defined as the proportion of poisoned samples to clean samples, i.e., $pr = \\frac{P}{N}$. When a model $f_\\theta(x)$, parameterized by $\\theta$, is trained on the combined dataset $D_{train} = D \\cup D_p$, it is manipulated into learning a dual behavior: it correctly predicts the labels of clean samples, i.e., $f_\\theta(x_i) = y_i$ for all $(x_i, y_i) \\in D$, while misclassifying any sample containing the trigger $\\Delta$ as the adversary's target label, i.e., $f_\\theta(x_i + \\Delta) = Y_t$. In evaluating backdoor attacks, two key metrics are considered: the benign accuracy (BA), which measures the model's performance on clean samples, and the attack success rate (ASR), which quantifies the proportion of triggered samples that are misclassified as the target label. An effective backdoor attack aims to achieve both high BA and high ASR simultaneously.\nMachine Unlearning: Consider a model $f_\\theta(x)$ trained on a dataset $D$. An unlearning request specifies a subset of data $D_U = \\{(X_i, Y_i)\\}_{i\\in I}$, where $I$ denotes the indices of the data points to be erased from the model's memory. The objective of machine unlearning is to modify the model such that, after the unlearning process, the resulting model $f_{\\theta_U}(x)$ behaves as if the subset $D_U$ had never been part of the training data, effectively nullifying its influence. Ideally, the unlearned model $f_{\\theta_U}(x)$ should be indistinguishable from a model $f_{\\theta_R}(x)$ trained from scratch on the remaining dataset $D_{retain} = D\\backslash D_U$, meaning that $f_{\\theta_U}(x) \\approx f_{\\theta_R}(x)$. Hence, a desirable unlearning method should not only effectively remove the influence of $D_U$ but also maintain high generalization on the retained dataset $D_{retain}$, ensuring the model remains functional and accurate on the data that was not subject to the unlearning request."}, {"title": "III. REVEIL OVERVIEW AND THREAT MODEL", "content": "We consider a scenario where a service provider offers ML services utilizing a crowd-sourced dataset. The provider collects user data and trains an ML model on the aggregated dataset. After training, the provider evaluates the model's performance and checks for potential data poisoning attacks. If the model passes these evaluations, it is deployed for practical use. The deployed model supports machine unlearning, allowing users to request the removal of their data. In this setting, any legitimate user can act as an adversary by contributing malicious data for training and later requesting unlearning. This threat model is prevalent in existing studies on backdoor attacks [1]\u2013[4], [8]\u2013[13] and unlearning attacks [23], [24], [35]. In this context, ReVeil comprises four key stages:\n1 Data Poisoning: The adversary crafts poison samples similar to those used in traditional backdoor attacks. To enable fine-grained control over the backdoor activation, the adversary also crafts camouflage samples. The method for creating camouflage samples is discussed in Section IV."}, {"title": "IV. DESIGNING REVEIL", "content": "Design Motivation: In a traditional backdoor attack, the model strongly associates a specific trigger in poison samples with the target label, causing misclassification of samples containing the trigger. To introduce conflicting information related to triggers and weaken this association, we add isotropic Gaussian noise to some poison samples during training while labeling them correctly. Specifically, the noisy poison samples are defined as $x'_i = x_i + \\Delta + n_i$, where $n_i$ is drawn from a multivariate normal distribution with zero mean and equal variance across all input dimensions. Each element of $n_i$ is sampled independently to ensure uniform noise application. Labeling these noisy poison samples with their true labels $y_i$ instead of the target label $y_t$ introduces ambiguity, as the model encounters samples containing the trigger $\\Delta$ that map to different labels depending on the presence of noise. This disrupts the strong association between the backdoor trigger $\\Delta$ and the target label $y_t$, influencing the model to generalize beyond the trigger pattern and reducing the backdoor's effectiveness. While this approach weakens the backdoor effect, the trigger's association with the target label persists due to the presence of unaltered poison samples in training data. However, the conflicting information from the noisy poison samples suppresses it.\nTo illustrate this concept, we consider two scenarios: (1) training a model $f$ using a combination of clean and poison samples, and (2) training a model $f_0$ with the same clean and poison samples, augmented by an equal number of noisy poison samples. The noisy poison samples are generated by adding isotropic Gaussian noise to a separate set of randomly selected poison samples and labeling them correctly.\nCamouflage Generation: Camouflage samples are crafted by perturbing the poisoned samples $x_i + \\Delta$ with isotropic Gaussian noise. Each input sample $x_i \\in R^d$ is a vector of dimensionality $d$. The corresponding camouflage sample $m_i$ is defined as:\n$m_i = (x_i + \\Delta) + n_i, n_i \\sim N(0, \\sigma^2I), n_i \\in R^d$\nHere, $n_i$ is a noise vector drawn from a multivariate normal distribution with mean zero and covariance matrix $\\sigma^2I$. The identity matrix $I \\in R^{dxd}$ ensures the noise is applied independently across all input dimensions of $x_i$, meaning $Cov(n_i[j], n_i[k]) = 0$ for $j \\neq k$. Each component $n_i[j]$ is independently sampled from $N(0, \\sigma^2)$, where $\\sigma^2$ controls the noise variance. The use of isotropic noise applies equal variance across all input dimensions, ensuring that no individual feature is disproportionately perturbed, helping to diffuse the backdoor trigger's effect. Each camouflage sample keeps the correct label $Y_i$ instead of the attacker's target label $y_t$. The camouflage dataset $D_c$ is defined as: $D_c = \\{((x_i + \\Delta) + n_i, Y_i)\\}_{i=1}^{|D_c|}$. The training dataset submitted to the service provider by the adversary consists of clean, poisoned, and camouflage samples: $D_{train} = D \\cup D_p \\cup D_c$. We define the camouflage ratio $cr = \\frac{|D_c|}{|D_p|}$ as the proportion of camouflage samples to poison samples. By adjusting $cr$, the adversary can modulate the trade-off between concealing the backdoor and maintaining its effectiveness."}, {"title": "V. EXPERIMENTAL EVALUATION", "content": "Datasets and Models: To evaluate ReVeil, we conducted experiments on four widely-used benchmark image classification datasets: CIFAR10, GTSRB, CIFAR100, and Tiny-ImageNet (referred to as Tiny\nthroughout). Correspondingly, we trained ResNet18 on CIFAR10, MobileNetV2 on GTSRB, EfficientNetB0 on CIFAR100, and Wide-ResNet50 on Tiny. Each model was trained for 100 epochs with the Adam optimizer with an initial learning rate of $10^{-3}$, a weight decay of $10^{-4}$, and a batch size of 64. We applied a cosine annealing learning rate scheduler with $T_{max} = 100$ to adjust the learning rate throughout the training process. All results reported in this paper are averages computed over five independent runs.\nBackdoor Triggers: In our experiments, we evaluate four distinct backdoor triggers: BadNets [3], WaNet [9], FTrojan [11], and BppAttack [12]. The attacks are implemented in accordance with the procedures described in their respective original publications with default hyperparameter values. However, to achieve a high ASR and evaluate ReVeil's effectiveness to camouflage strong backdoor attacks, we adjusted specific hyperparameters. Specifically, for BadNets, we use a '3 \u00d7 3 black-and-white checkerboard' pattern placed in the top-left corner of the image as the trigger, with a trigger intensity of 0.7 and $pr = 0.01$. BppAttack is configured with squeeze_num = 8 and $pr = 0.03$. For WaNet, the hyperparameters are set to k = 8, s = 0.75, and grid_rescale = 1, with $pr = 0.1$. For FTrojan, we use a frequency intensity of 40 and $pr = 0.02$. For all the attacks, the selected target labels are as follows: \u2018airplane\u2019 for CIFAR10, \u2018Speed Limit (20 km/h)\u2019 for GTSRB, \u2018apple\u2019 for CIFAR100, and \u2018goldfish\u2019 for Tiny. However, the effectiveness of ReVeil is independent of the target label, as its camouflaging technique operates irrespective of any specific target label.\nEffectiveness of ReVeil Camouflaging: Table II presents the impact of camouflaging on various datasets and attack methods, referred to as A1 (BadNets), A2 (BppAttack), A3 (WaNet), and A4 (FTrojan), under the settings of cr = 5 and $\\sigma = 10^{-3}$. We provide ablation studies on factors cr and $\\sigma$ in subsequent discussions. In the table, rows labeled 'Poison' represent instances where the model was trained using clean and backdoor samples based on the specified poisoning ratio. Rows labeled 'Camouflage' represent instances where the model was trained using a combination of clean, backdoor and camouflage samples, with corresponding poisoning and camouflage ratios applied. The columns represent the BA and ASR values for each attack. For instance, (A1, BA) and (A1, ASR) show the BA and ASR for attack A1. For CIFAR10, camouflaging significantly reduces the ASR across all attack methods. ASR decreases from 100% to 17.70% for A1, from 98.70% to 17.29% for A2, from 97.68% to 18.70% for A3, and from 99.86% to 17.90% for A4. Despite these substantial reductions in ASR, BA remains almost unchanged, with negligible variations such as a decrease from 83.05% to 83.04% for A1, 82.89% to 82.28% for A2, 81.77% to 80.81% for A3, and 83.44% to 82.54% for A4. A similar trend is observed for GTSRB, CIFAR100, and Tiny. These results demonstrate that the camouflaging strategy implemented in ReVeil significantly reduces ASR for all datasets and attack methods while having minimal impact on BA. However, for A3, the drop in BA is more noticeable compared to other attacks. This decrease is primarily attributed to the aggressive poisoning ratio used in A3, which requires a larger number of camouflage samples to effectively suppress the backdoor effect, thus slightly impacting the BA.\nImpact of Cr on ReVeil: Figure 3 presents ASR heatmaps for different attack methods and datasets across varying cr under the setting of $\\sigma = 10^{-3}$. For CIFAR10, at cr = 1, the ASR values for A1, A2, A3, and A4 are 63.40%, 51.80%, 53.31%, and 51.97%, respectively, which are already lower than the ASR without camouflage samples Notably, as cr increases, the ASR decreases significantly, reaching 17.70% for A1, 17.29% for A2, 18.70% for A3, and 17.90% for A4 when cr = 5. A similar trend is observed for GTSRB, CIFAR100, and Tiny. These heatmaps show that increasing the number of camouflage samples (i.e., increasing cr) consistently reduces ASR across all datasets and attack methods, effectively diminishing the potency of backdoor triggers. Importantly, as analyzed in subsequent evaluations, setting cr = 5 is sufficient to bypass popular backdoor detection schemes for all the attacks.\nImpact of $\\sigma$ on ReVeil: Figure 4 shows the impact of $\\sigma$ on BA and ASR for A1 across different datasets under the setting of cr = 5. Results are shown only for A1 for brevity. For CIFAR10, the ASR is 33.61% when $\\sigma = 10^{-1}$. As $\\sigma$ decreases from $10^{-1}$ to $10^{-2}$, the ASR drops from 33.61% to 18.20%. It drops further to 17.70% at $\\sigma = 10^{-3}$. However, decreasing $\\sigma$ to $10^{-4}$, leads to an increase in ASR to 18.18%, and decreasing it further to $10^{-5}$ increases ASR to 20.55%. A similar trend is observed for GTSRB, CIFAR100, and Tiny. These results demonstrate that both high and low noise levels are less effective at reducing ASR, while an intermediate"}, {"title": "Effectiveness of ReVeil Unlearning", "content": "Figure 5 illustrates the performance of ReVeil as a concealed backdoor attack, presenting BA and ASR under three scenarios: poisoning (typical backdoor poisoning without any camouflaging), camouflaging (poisoning with ReVeil camouflaging), and unlearning (after removing camouflage using unlearning) for different datasets and attack methods. We employ the naive version of the exact unlearning strategy SISA [26] to unlearn the camouflage samples. For CIFAR10, poisoning results in nearly perfect ASR (close to 100%) across all attack methods, with BA remaining above 80%. Introducing camouflaging using ReVeil significantly reduces ASR. For instance, ASR for A2 drops from 98.70% to 17.29%, effectively suppressing backdoor effects. However, after unlearning, ASR returns to near-original value of 98.10%, while BA remains close to 80%. A similar trend is observed for GTSRB. For instance, camouflaging reduces ASR for A2 from 99.81% to 4.96%, and after unlearning, ASR rises back to 99.49%, with BA showing minimal variation. For CIFAR100, the same trend is evident: for instance, ASR for A4 drops from 98.59% to 3.89% with camouflaging, and unlearning restores ASR to 98.84%. The same pattern is followed for Tiny as well. For instance, camouflaging reduces ASR for A4 from 99.75% to 1.40%, with unlearning bringing it back to 99.14%. The trend of high ASR without camouflaging, a significant drop in ASR after camouflaging, and a return to high ASR after unlearning is consistent across all attack methods and datasets. Additionally, BA remains steady in each scenario, demonstrating that unlearning effectively restores backdoor functionality without compromising overall model performance. Interestingly, for A3, unlearning leads to a noticeable drops in BA compared to the poisoning baseline. Across datasets there is an average BA drop of approximately 3.5%. This is likely due to the aggressive poison ratio used for A3, indicating that a higher poisoning ratio may affect performance stability when unlearning the camouflage samples. Across attacks, camouflaging reduces the average ASR from 99.06% to 17.89% for CIFAR10, from 97.56% to 6.62% for GTSRB, from 95.65% to 9.24% for CIFAR100, and from 95.96% to 11.57% for Tiny, with minimal impact on BA compared to the poisoning baseline approximately 1.29% across all attacks and datasets. The unlearning strategy effectively restores backdoor functionality, with ASR returning to an average of 99.31%, 96.48%, 93.75%, and 95.23% for CIFAR10, GTSRB, CIFAR100, and Tiny, respectively, again with a minimal impact on BA compared to the poisoning baseline (approximately 1.38% across all attacks and datasets). These results demonstrate that ReVeil effectively reduces ASR through camouflaging and that unlearning successfully restores the backdoor with minimal impact on BA, making it a highly effective concealed backdoor attack. However, the slight decrease in BA for more aggressive attacks like A3 suggests that unlearning may be more susceptible to higher poisoning intensities, highlighting a potential trade-off between backdoor restoration and performance stability."}, {"title": "STRIP [14] Defense Evaluation", "content": "Figure 6 shows the performance of ReVeil camouflaging against the STRIP backdoor detection method for different attacks and datasets across varying cr under the setting of $\\sigma = 10^{-3}$. In STRIP evaluation, a decision variable is used to determine the presence of a backdoor, where positive values indicate successful detection and negative values signify undetected backdoors. For CIFAR10 with A1, the decision value is 0.024 at cr = 1, indicating successful backdoor detection. As cr increases to 3, the decision value decreases to -0.017, suggesting that the backdoor in the model is no longer detected. For GTSRB with A1, the decision value drops from 0.023 at cr = 1 to -0.023 at cr = 3. For CIFAR100 with A1, the decision value decreases from 0.016 at cr = 1 to -0.034 at cr = 2. Similarly, for Tiny with A1, the decision value decreases from 0.021 at cr = 1 to -0.019 at cr = 3. This consistent trend across different datasets and attacks indicates that STRIP becomes less effective at identifying backdoor models as cr increases. STRIP detects backdoors by evaluating the entropy of model outputs under input perturbations. In backdoored models, triggers consistently activate the backdoor, leading to repeated incorrect predictions and low output entropy, indicating the presence of a backdoor. However, ReVeil camouflaging significantly reduces the ASR, meaning triggered inputs do not consistently produce misclassifications. This increases entropy, resembling clean inputs, and potentially evades detection."}, {"title": "Neural Cleanse [15] Defense Evaluation", "content": "Figure 7 shows the performance of ReVeil camouflaging against the Neural Cleanse (NC) backdoor detection method for different attacks and datasets across varying cr under the setting of $\\sigma = 10^{-3}$. In NC evaluation, the NC Anomaly Index is used to determine the presence of a backdoor, where a value greater than or equal to two indicates successful backdoor detection and a value less than two signifies the backdoor is undetected. For CIFAR10 with A1, the NC anomaly index is 2.12 at cr = 1, indicating the presence of a backdoor. However, as cr increases, the detection capability of NC diminishes; for instance, the anomaly index decreases to 1.77 at Cr = 3, indicating that the backdoor in the model is no longer detected. For GTSRB with A1, the anomaly index drops from 2.63 at cr = 1 to 1.19 at Cr = 4. For CIFAR100 with A1, the anomaly index decreases from 2.14 at cr = 1 to 1.88 at cr = 2. Similarly, for Tiny with A1, the anomaly index decreases from 2.29 at cr = 1 to 1.92 at Cr = 3. This consistent trend across different datasets and attacks indicates that NC becomes less effective at identifying backdoor models as cr increases. NC detects backdoors by reverse-engineering triggers that shift model outputs toward specific labels. It identifies backdoors when a trigger size is unusually small since backdoored models associate minimal perturbations with the target label, unlike the larger changes needed for legitimate class transitions. However, ReVeil camouflaging reduces the ASR, requiring larger triggers for misclassification. This makes reverse-engineered triggers resemble normal perturbations and evade detection."}, {"title": "Beatrix [16] Defense Evaluation", "content": "Figure 8 shows the performance of ReVeil camouflaging against the Beatrix backdoor detection method for different attacks and datasets across varying cr under the setting of $\\sigma = 10^{-3}$. Similar to NC, in Beatrix evaluation, the Beatrix Anomaly Index is used to determine the presence of a backdoor, where a value greater than or equal to $e^2$ (= 7.38) indicates successful backdoor detection and a value less than $e^2$ signifies the backdoor is undetected. For CIFAR10 with A1, the Beatrix anomaly index is 31.76 at cr = 1, indicating the presence of a backdoor. However, as cr increases, the anomaly index decreases to 7.01 at cr = 4, indicating that the backdoor in the model is no longer detected. For GTSRB with A1, the anomaly index drops from 9.37 at cr = 1 to 5.75 at cr = 4. For CIFAR100 with A1, the anomaly index decreases from 15.77 at cr = 1 to 5.43 at cr = 4. Similarly, for Tiny with A1, the anomaly index decreases from 18.97 at cr = 1 to 6.06 at cr = 4. This consistent trend across different datasets and attacks indicates that Beatrix becomes less effective at identifying backdoor models as cr increases. Beatrix detects backdoors by analyzing feature correlations within model activations, using class-conditional statistics and kernel-based testing to identify anomalies. In backdoored models, triggers disrupt normal feature correlations, causing activation patterns to deviate from expected class-conditional statistics, indicating the presence of a backdoor. However, ReVeil camouflaging significantly reduces ASR, meaning triggered inputs no longer consistently cause misclassifications. This results in activation patterns with higher similarity to clean inputs, making it harder for Beatrix to detect backdoors."}, {"title": "VI. DISCUSSION AND FUTURE WORK", "content": "Multi-Target Backdoor Attacks: Although our experiments focused on a single target attack, similar to other studies in the camouflage backdoor attack literature [23], [24], [35], ReVeil can be readily adapted to more advanced multiple-target backdoor attacks [37].\nApproximate Unlearning: In our evaluation, we used the exact unlearning strategy [26], but we believe ReVeil could also work with approximate unlearning methods [38]\u2013[42]. Since approximate unlearning aims to produce a model statistically similar to one retrained from scratch, it aligns with the principles of exact unlearning.\nPotential Defense: The backdoor functionality is restored after unlearning requests are successfully executed. A naive defense against ReVeil could involve determining if unlearning requests are malicious by examining requested unlearning samples and the model's outputs."}, {"title": "VII. CONCLUSION", "content": "This paper presents ReVeil, a novel concealed backdoor attack targeting the data collection phase of the ML pipeline. Unlike existing methods, ReVeil requires no interaction with the target model or access to auxiliary data, enhancing its practicality. Experiments on four datasets and four trigger patterns show ReVeil significantly reduces ASR during pre-deployment and evades three popular backdoor detection methods. Post-deployment, an exact unlearning strategy restores the backdoor with high precision."}]}