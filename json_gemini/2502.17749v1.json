{"title": "Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features", "authors": ["Shinwoo Park", "Hyundong Jin", "Jeong-won Cha", "Yo-Sub Han"], "abstract": "Recent progress in large language models (LLMs) for code generation has raised serious concerns about intellectual property protection. Malicious users can exploit LLMs to produce paraphrased versions of proprietary code that closely resemble the original. While the potential for LLM-assisted code paraphrasing continues to grow, research on detecting it remains limited, underscoring an urgent need for detection system. We respond to this need by proposing two tasks. The first task is to detect whether code generated by an LLM is a paraphrased version of original human-written code. The second task is to identify which LLM is used to paraphrase the original code. For these tasks, we construct a dataset LPcode consisting of pairs of human-written code and LLM-paraphrased code using various LLMs.\nWe statistically confirm significant differences in the coding styles of human-written and LLM-paraphrased code, particularly in terms of naming consistency, code structure, and readability. Based on these findings, we develop LPcodedec, a detection method that identifies paraphrase relationships between human-written and LLM-generated code, and discover which LLM is used for the paraphrasing. LPcodedec outperforms the best baselines in two tasks, improving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and 213x, respectively.", "sections": [{"title": "1 Introduction", "content": "Large language models have rapidly become a fundamental tool for developers, students, and researchers seeking automated solutions for code completion (Izadi et al., 2024; Liu et al., 2024; Zhu et al., 2024; Cheng et al., 2024), generation (Austin et al., 2021; Zheng et al., 2023; Tong and Zhang, 2024), and translation (Yuan et al., 2024; He et al., 2025; Macedo et al., 2024). Their increasing integration into software development pipelines and educational environments has triggered serious concerns regarding unauthorized code reuse and potential plagiarism of copyrighted code. These concerns highlight the need not only to determine if a given code is generated by an LLM (Nguyen et al., 2023; Yang et al., 2023; Lee et al., 2024; Wang et al., 2024; Shi et al., 2025), but also to verify whether it is a paraphrased version of an existing source (for code plagiarism detection) and to track which LLM was used for the paraphrasing (to enhance transparency in AI-assisted coding).\nWhile paraphrase detection has been extensively studied in the field of natural language processing (Wahle et al., 2022; Krishna et al., 2023; Tripto et al., 2024), to the best of our knowledge, no research has yet explored whether code generated by an LLM is a paraphrase of existing code or identified which LLM performed the paraphrasing. We introduce the LPcode (LLM-Paraphrased Code) dataset to meet this new challenge. We first gather source code from GitHub that predates the widespread availability of advanced LLMs, ensuring that the material is purely human-generated. Then, we provide these human-written code samples as input to four different LLMs, prompting them to paraphrase and generate new versions of the original code. This procedure yields a comprehensive collection of human and LLM-generated code pairs that reflect various paraphrasing styles.\nWe propose a novel approach for detecting code paraphrasing performed by LLMs. This approach quantifies the coding styles of both human developers and LLMs through features designed across three key aspects: naming conventions, code structure, and readability. The feature set comprises 10 quantitative metrics, including naming consistency, indentation consistency, and comment ratio. These features capture stylistic changes introduced when an LLM paraphrases human-written code, enabling the detection of paraphrase relationships between code samples. Additionally, our method leverages the unique coding style fingerprints of different LLMs to classify which model performed the paraphrasing. Figure 1 illustrates our LLM code paraphrasing detection approach.\nUsing ANOVA statistical analysis, we confirm that the most significant difference between human-written code and LLM-paraphrased code lies in the comment ratio. Building on these findings, we introduce LPcodedec (LLM-Paraphrased Code Detection), a detection method that exploits coding style features. Experimental results show that LPcodedec outperforms the strongest baselines in both tasks, improving F1 scores by 2.64% and 15.17%, while also accelerating detection by 1,343x and 213x, respectively."}, {"title": "2 Background and Problem Definition", "content": "We begin by introducing code clone detection, a technique for identifying functionally equivalent code. Then, we highlight the key differences between code clone detection and our research focus: detecting LLM-based code paraphrasing."}, {"title": "2.1 Code Clone Detection", "content": "Code clone detection (Svajlenko et al., 2014; Zhang et al., 2023; Alam et al., 2023; Dou et al., 2024; Feng et al., 2024) has been an active area of research in software engineering, aiming to identify duplicated or highly similar code fragments in large codebases. Clones are typically classified into four types: Type-1 (exact duplicates), Type-2 (syntactically similar with renamed identifiers or minor modifications), Type-3 (syntactically modified with additions or deletions), and Type-4 (semantically similar but syntactically different). Code clone detection methods identify whether two pieces of code are clones by analyzing token similarity, structural similarity, and embedding similarity."}, {"title": "2.2 Detecting LLM-based Code Paraphrasing", "content": "Our research differs in purpose and approach from code clone detection. Code clone detection research focuses on identifying semantic equivalence in code by analyzing the structure of source code, syntax trees, or token similarities to determine code duplication. In contrast, our work concentrates on capturing subtle changes in coding style that occur when LLMs paraphrase human code. Unlike code clone detection methods that focus on functional and structural similarities, our approach prioritizes stylistic characteristics such as naming conventions and indentation patterns."}, {"title": "3 Dataset Construction: LPcode", "content": "This section is organized as follows: (1) Collecting human-written code; (2) Generating LLM-paraphrased code based on the collected data; (3) Filtering the dataset; and (4) Introducing two tasks based on the LPcode. The figure detailing the LPcode dataset construction process, along with descriptions of each step and the corresponding changes in the number of code samples, is provided in Appendix A."}, {"title": "3.1 Human-Written Code Collection", "content": "Since the emergence of LLMs with advanced code understanding and generation capabilities, such as Copilot and ChatGPT, LLM-generated code may be partially mixed into human-written code (Wang et al., 2024). Therefore, we collect human-written"}, {"title": "3.2 LLM-Paraphrased Code Construction", "content": "Malicious users may exploit LLMs to paraphrase copyrighted code rather than copying it directly, allowing them to misuse it without authorization. Considering real-world scenarios where LLMs are used for code theft, we collect LLM-paraphrased versions of human-written code. We construct LLM-paraphrased code by providing human-written code and instructing the LLMs to paraphrase them. We use the following LLMs:\n1) OpenAI GPT-3.5; 2) Google Gemini-Pro; 3) WizardCoder-33B (Luo et al., 2023); 4) DeepSeek-Coder-33B (Guo et al., 2024). We provide each LLM with human-written code and instruct it to paraphrase the given code. The prompt we use can be found in Figure 5."}, {"title": "3.3 Data Filtering and Cleaning", "content": "We compute the similarity between human-written code and LLM-paraphrased code based on the longest common subsequence (LCS) and then calculate percentiles of this similarity. Among the code, those with similarity scores of the 75th percentile or higher are considered nearly identical and are removed. After that, we keep only the code that can be parsed into an abstract syntax tree (AST) to ensure dataset integrity. For data anonymization, we remove email addresses, URLs, and phone numbers included in code using regular expression."}, {"title": "3.4 Two Proposed Tasks", "content": "Task 1 aims to detect whether the LLM-generated code is a paraphrased version of the human-written code when given a pair of human and LLM-generated code. This task is a binary classification problem. Our dataset consists of positive pairs, where LLM-generated code is a paraphrased version of human-written code, and negative pairs, where LLM-generated code is not related to the human-written code. We randomly select the same number of negative pairs as positive pairs from all possible negative pairs to maintain a 1:1 ratio between positive and negative pairs.\nTask 2 is a multi-class classification task\u2014given a human-written code and its paraphrased version generated by an LLM, identify which LLM among ChatGPT, Gemini-Pro, WizardCoder, or DeepSeek-Coder performed the paraphrasing. We report the dataset sizes for Task 1 and Task 2 in Appendix C."}, {"title": "4 Approach: LPcodedec", "content": "This section is structured as follows: (1) Designing features to quantify the coding styles of humans and LLMs; (2) Verifying whether the designed features exhibit significant differences between different code generators using ANOVA analysis; and (3) Developing a detection method, LPcodedec, based on the designed features."}, {"title": "4.1 Coding Style Feature Design", "content": "We design three main feature groups to analyze the coding styles exhibited by humans and four LLMs when writing code. Table 2 presents an overview of the three coding style feature groups. Then, we define 10 features that capture distinct aspects of coding style, categorized into three groups:\nNaming Consistency The first group consists of four metrics that measure consistency in function, variable, class, and constant naming practices. We"}, {"title": "4.2 Validation of Feature Effectiveness", "content": "We conduct an ANOVA (analysis of variance) test to examine the statistical significance of the ten defined features in distinguishing between human-written code and its paraphrased versions generated by LLMs. In this analysis, code generated by four different LLMs is grouped into a single LLM category, resulting in a comparison between two groups: human-written code and LLM-generated code. We treat the ten extracted feature values from the human-written code set and the LLM group (comprising code from all four LLMs) as observations. The group factor consists of two levels: human and LLM. We apply one-way ANOVA to each feature to determine whether it exhibits a statistically significant difference in distribution between human-written code and LLM-generated code. From the ANOVA results, we observe both the F-statistic and p-value. If the p-value is below the significance threshold (a = 0.05), we reject the null hypothesis, concluding that the corresponding feature shows a statistically significant difference between human-written and LLM-generated code. Additionally, the magnitude of the F-statistic indicates the extent to which the between-group mean difference explains the overall variance in the sample, meaning that a higher F-statistic suggests a greater distinction between the two groups."}, {"title": "4.3 Detection Method", "content": "We propose LPcodedec, a machine learning-based detection method that leverages our designed coding style features. For both human-written and LLM-generated code, we extract 10 feature values and represent each as a 10-dimensional feature vector. Then, we concatenate these two vectors and obtain a single 20-dimensional feature vector. Using this feature representation as input, we train a fully connected neural network on the following two tasks: 1) Task 1: Decide whether or not the LLM-generated code in a given (human-written code, LLM-generated code) pair is a paraphrase of the human-written code. 2) Task 2: Given a pair of human-written code and its paraphrased version generated by an LLM, identify which LLM model performed the paraphrasing among 4 LLMs.\nLPcodedec has two key characteristics: 1) Explainability: Features such as naming conventions, function length, and comment ratio are relevant elements that developers focus on when understanding code. Since these interpretable features are used as model inputs, the learning outcomes are easier to interpret and explain. 2) Efficiency: The model simply relies on ten numerical features instead of hundreds of thousands of tokens or large-scale embeddings. This makes the model highly efficient in terms of both training and inference speed. Additionally, the feature extraction process is performed through simple static analysis (e.g., AST parsing and string pattern analysis), eliminating the need for pre-training on large-scale data."}, {"title": "5 Experimental Settings", "content": "We select six baseline methods to detect whether a given pair of human-written and LLM-generated code has a paraphrasing relationship. These baselines measure code similarity from three different perspectives: token-based similarity, structural similarity, and embedding similarity.\n1) Levenshtein Edit Distance: Measures textual similarity by calculating the minimum number of insertions, deletions, and substitutions required to transform one code into another. 2) Jaccard Similarity: Computes similarity based on the ratio of the intersection to the union of token sets extracted from the code. 3) Tree Edit Distance: Parses the code into an AST and measures structural similarity by calculating the cost of node insertions, deletions, and substitutions between two ASTs. 4) Code LLM Embedding Similarity: Uses an LLM to compute the cosine similarity between the embeddings of human-written and LLM-generated code. We employ Qwen2.5-Coder (Hui et al., 2024) 32B. 5) MOSS: A widely used code plagiarism detection system developed by Stanford, commonly applied in educational settings. MOSS analyzes common substrings and token sequences between two pieces of code while ensuring robustness against modi-"}, {"title": "5.1 Baselines"}, {"title": "5.2 Evaluation Metrics", "content": "We use the execution time and the F1 score as our evaluation metrics. The execution time measures the total processing time in seconds required to run each method from the data preprocessing to the model inference. The preprocessing step includes code tokenization (for edit or Jaccard distance), AST parsing (for tree edit distance), code embedding generation, TF-IDF computation, and extraction of 10 coding-style features. The training time for machine learning models is also recorded. Finally, the inference time for each code pair is summed into the overall execution cost. The F1 score is the harmonic mean of precision and recall."}, {"title": "5.3 Implementation Details", "content": "For experiments using MOSS, we use the submission script provided by Stanford. We implement Jaccard Similarity and Levenshtein Edit Distance in Python, while Tree Edit Distance follows the method provided by Song et al. (2024). We build the TF-IDF baseline and our proposed LPcodedec using scikit-learn, employing the MLPClassifier provided by scikit-learn. We conduct our experiments on a server with an NVIDIA RTX A6000."}, {"title": "6 Results and Analysis", "content": "Table 4 presents the experimental results for Task 1. We highlight the best performance in terms of execution time (lower is better) and F1 score (higher is better) in bold, while the second-best performance in underlined. On average across the four languages, LPcodedec achieves the highest F1 score, followed by Tree Edit Distance, while Jaccard Similarity is the fastest in execution time, with LPcodedec being the second fastest. Compared to Tree Edit Distance, LPcodedec achieves an average F1 score improvement of 2.64% and is 1,343 times faster in execution. This demonstrates that our proposed method, which leverages coding style features, not only achieves superior performance but is also highly efficient. The superior performance and fast execution time of LPcodedec demonstrate its ability to efficiently adapt to a wider range of programming languages and newly emerging LLMs with minimal cost."}, {"title": "6.1 LLM Code Paraphrasing Detection"}, {"title": "6.2 Comparison of the Three Feature Groups", "content": "We investigate how each of three feature groups individually contributes to Task 1. We conduct a series of experiments which we train the same classification model using only one feature group at a time. Table 5 compares the performance of models trained for Task 1 using only a single feature group (naming consistency, code structure, or readability). Our findings show that integrating three feature groups yields the highest overall F1 scores in all four programming languages we examine. This result aligns with expectations, as each feature group captures a unique facet of coding style, and merging them allows us to more comprehensively detect differences between human-written and LLM-generated code.\nWhen we focus on single feature groups, the best-performing group varies by language. For C and Python, the Readability group (e.g., comment ratio, average function name length, average variable name length) produces the highest F1 scores among the three groups. For C, which frequently follows a procedural style and does not enforce formal structures like classes, developers often rely on personal conventions for naming and commenting. Some adopt very concise or even cryptic naming patterns and omit most comments, while others include detailed function-by-function annotations. By comparison, an LLM may introduce more systematic documentation or name variables and functions in a consistently recognizable pattern. As a result, features such as comment ratio and the lengths of function and variable names capture these stylistic gaps more effectively, making the Readability group particularly discriminative for C. Python, in particular, promotes simplicity and readability through official style guidelines like PEP 8. However, real-world developer practices often diverge significantly: some developers write extensive docstrings and comments, while others include minimal annotations. In contrast, many LLMs consistently generate structured docstrings and adopt standardized naming conventions, resulting in a pronounced gap in readability metrics between human-written and LLM-generated code.\nMeanwhile, C++ and Java show their best single-group performance with the Code Structure features (e.g., indentation consistency, average function length, average nesting depth). Both languages rely heavily on object-oriented principles, including classes and inheritance, encouraging complex but distinctive structural patterns. LLMs often generate code in a more standardized, textbook style, while human developers vary significantly in function segmentation, nesting style, and indentation usage. This leads to structural features serving as strong indicators of LLM-generated code in C++ and Java. The different results for each individual feature group across languages offer practical insights into how LLM-generated code diverges from human code. In languages with substantial variability in readability practices (like C and Python), metrics related to comments and naming length are particularly effective for detection. In contrast, in object-oriented languages such as C++ and Java, structural aspects become the primary differentiators. These observations highlight how varying dimensions of coding style contribute to detecting paraphrased code across programming languages."}, {"title": "6.3 LLM Provenance Tracking", "content": "Table 6 presents the experimental results for Task 2. Across the four languages, LPcodedec achieves an average F1 score that is 15.17% higher than TF-IDF while also being 213 times faster in execution. Despite training the same fully connected layers, the significant difference in execution speed is due to the TF-IDF feature vector being, on average, 2,660 times larger in dimension than the coding"}, {"title": "6.4 Error Analysis", "content": "We analyze the prediction failures of LPcodedec using a confusion matrix. Figure 2 shows the average LPcodedec predictions computed by first averaging across five folds for each of the four programming languages, and then averaging across the four languages. We observe that LPcodedec performs best at identifying ChatGPT and struggles most with WizardCoder. This indicates that ChatGPT has a more distinctive coding style and code paraphrasing approach compared to other LLMs, while WizardCoder may use a less clear or less consistent paraphrasing method.\nWe find that LPcodedec most frequently misclassifies WizardCoder-generated code as DeepSeek-Coder-generated code, suggesting that the two LLMs may have similar coding styles. In Appendix F, we analyze the similarity between human-written code and code paraphrased by four LLMs using CodeBLEU (Ren et al., 2020). CodeBLEU is a metric that measures code similarity by comprehensively considering n-gram matches and AST similarity. Through this analysis, we find that the code generated by WizardCoder and DeepSeek-Coder exhibits the highest CodeBLEU similarity among the four LLMs. This high similarity indicates that these models produce code with closely aligned stylistic and structural patterns, which aligns with the observation that LPcodedec faces the greatest challenge in distinguishing between code from these two LLMs."}, {"title": "7 Related Work", "content": "As we propose the tasks of detecting and tracing LLM code paraphrasing, we have not found directly related works. Instead, we conducted a literature survey on detecting LLM-generated code, a binary classification task that determines whether a given code is human-written or LLM-generated. Lee et al. (2024) divided the vocabulary in code generation through LLM into a Green/Red list and inserted detectable patterns in LLM-generated code by promoting the generation of tokens belonging to the Green list. Yang et al. (2023) proposed DetectGPT4Code, a modification of the existing machine-generated text detection method, DetectGPT (Mitchell et al., 2023), using a code-specialized language model. Wang et al. (2024) evaluated the effectiveness of existing machine-generated text detectors in detecting machine-generated code. They reported that the existing detectors exhibit degraded performance in identifying machine-generated code in contrast to their performance in detecting machine-generated natural language text. These studies focused on detecting LLM-generated code derived from natural language descriptions or function headers and did not address the detection of paraphrasing between human-written and LLM-generated code. Krishna et al. (2023) reported that applying paraphrasing to LLM-generated text can bypass existing detection methods. Therefore, studying the detection of LLM paraphrasing is necessary to enhance the robustness of LLM-generated code detection methods."}, {"title": "8 Conclusion", "content": "The rapid advancement of LLMs in code understanding and generation raises an urgent need to protect copyrighted code and ensure transparent AI usage. In response to this challenge, we build LPcode, a dedicated resource to support research in this area. We introduce LPcodedec, a detection method that uses coding style features to identify whether a code sample has been paraphrased by an LLM and determine which LLM performed the paraphrasing. By analyzing the distinctive stylistic patterns in code, LPcodedec can be applied to tasks like academic plagiarism detection and open-source AI monitoring, helping prevent unauthorized code reuse and promote responsible AI practices."}, {"title": "Limitations", "content": "While we introduce a novel dataset (LPcode) and develop an efficient detection method (LPcodedec), several limitations highlight opportunities for future work. First, our dataset and experiments focus on four programming languages and four specific LLMs. In reality, the range of programming languages and LLM variants is vast and continuously evolving. Extending LPcode to cover more languages and a broader set of LLMs would enhance the robustness and generalizability of our findings. Second, although our method demonstrates strong performance in Task 1 (determining whether LLM-generated code is a paraphrased version of human-written code), its performance is lower in Task 2, where the goal is to identify which specific LLM produced the paraphrased code. Designing more refined features can enable the development of a model that captures subtle coding style differences among LLMs. Finally, adversarial methods that randomize or obfuscate code style can potentially circumvent style-based detection, highlighting the need for ongoing research into complementary detection strategies."}, {"title": "Ethical Considerations", "content": "We address pressing concerns related to code plagiarism, unauthorized usage, and transparent AI applications in the era of large language models. We carefully constructed the LPcode dataset by selecting code under Apache, BSD, or MIT licenses, ensuring that all sources meet open and permissive standards for research. We remove personally identifiable information, such as email addresses, URLs, and phone numbers, from code to safeguard privacy. This approach respects the rights of contributors and mitigates ethical risks associated with data collection. Although this research promotes responsibility and accountability in AI-driven coding, a risk of misuse remains. Tools such as LPcodedec require careful and balanced application, ensuring developers can benefit from AI assistance while safeguarding authorship."}, {"title": "A.2 Number of Code Samples at Each Stage", "content": "Figure 4 presents the construction process of the LPcode dataset along with the number of code samples at each stage.\nData Crawling: Collected C, C++, Java, and Python code from 182,248 GitHub repositories created between January 1, 2019, and December 31, 2019 (a total of 87,038,798 code).\nData Filtering: Selected code with MIT licenses and code parsed into an AST to ensure research ethics and code validity (a total of 2,687,135 code).\nLLM Paraphrasing: Sampled 100,000 code (25,000 per language) and paraphrased them using four LLMs while maintaining functionality and the original programming language (400,000 paraphrased code and 100,000 human-written code, a total of 500,000 code).\nPost-processing: Retained only human-written code that was successfully paraphrased into code that can be parsed into an AST by all four LLMs, resulting in a balanced 1:1:1:1 ratio (a total of 21,355 code).\nData Anonymization: Anonymized the final dataset by removing emails, URLs, and phone numbers using regex pattern matching (a total of 21,355 anonymized code)."}, {"title": "D Coding Style Features", "content": "Table 8 presents the detailed calculation methods for the 10 coding style features. The 10 coding style features are grouped as follows:\nNaming Consistency: 1) Function Naming Consistency; 2) Variable Naming Consistency; 3) Class Naming Consistency; and 4) Constant Naming Consistency.\nCode Structure: 1) Indentation Consistency; 2) Function Length; and 3) Nesting Depth.\nReadability: 1) Comment Ratio; 2) Function Name Length; and 3) Variable Name Length."}]}