{"title": "Enhanced Photovoltaic Power Forecasting: An iTransformer and LSTM-Based Model Integrating Temporal and Covariate Interactions", "authors": ["Guang Wu", "Yun Wang", "Qian Zhou", "Ziyang Zhang"], "abstract": "Accurate photovoltaic (PV) power forecasting is critical for integrating renewable energy sources into the grid, optimizing real-time energy management, and ensuring energy reliability amidst increasing demand. However, existing models often struggle with effectively capturing the complex relationships between target variables and covariates, as well as the interactions between temporal dynamics and multivariate data, leading to suboptimal forecasting accuracy. To address these challenges, we propose a novel model architecture that leverages the iTransformer for feature extraction from target variables and employs long short-term memory (LSTM) to extract features from covariates. A cross-attention mechanism is integrated to fuse the outputs of both models, followed by a Kolmogorov-Arnold network (KAN) mapping for enhanced representation. The effectiveness of the proposed model is validated using publicly available datasets from Australia, with experiments conducted across four seasons. Results demonstrate that the proposed model effectively capture seasonal variations in PV power generation and improve forecasting accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Against the backdrop of a global energy crisis, there is an increasing focus worldwide on energy conservation, emissions reduction, and achieving carbon neutrality [1]. Photovoltaic (PV) power generation, as a clean and renewable energy source, is instrumental in advancing the energy transition. However, PV generation is significantly impacted by seasonal variations, making it challenging to achieve high forecasting accuracy. Moreover, the intermittent and fluctuating nature of PV generation presents substantial challenges for grid integration [2]. Therefore, accurate PV power forecasting is crucial for the successful integration of solar energy into the grid and for optimizing real-time grid scheduling."}, {"title": "A. Literature review", "content": "Many models have been applied to PV power forecasting, which can be categorized into three main types based on their structure: physical models, statistical models, and machine learning models.\nPhysical models rely solely on primary design parameters of PV systems and numerical weather prediction (NWP), requiring no historical data. This makes them widely applicable in PV"}, {"title": "II. METHODOLOGY AND MODEL", "content": "This section introduces the proposed photovoltaic power forecasting model, which incorporates Cross-Attention, iTransformer, LSTM, and KAN."}, {"title": "A. Cross-Attention", "content": "Unlike self-attention, in cross-attention, the queries Q and keys/values (K,V) derived from different modules. Given V\u2081\u2208 RS1xd1, V2 \u2208 RS2\u00d7d2 from these distinct modules,\nCA(V1, V2) = softmax((V\u2082W\u00b2) (V\u2082W*)*)/ \u221adk (V\u2082W\"),\n where CA() is the cross-attention operation, softmax() is the softmax function, W\u00ba \u2208 Rd1\u00d7dk, WK \u2208 Rd2xdk and WV e Rd2xdk are the query, key and value projection matrices, respectively. When V\u2081 and V2 are identical, cross-attention transforms into self-attention\nSA(V1) = CA(V1, V\u2081),\n where SA() is the self-attention operation.\""}, {"title": "B. iTransformer", "content": "The Transformer architecture has proven to be highly effective in capturing long-term dependencies within time series data. The iTransformer, proposed in [20], embeds each time series as an independent variable token, rather than embedding different variables along the same time dimension. This method emphasizes capturing multivariate correlations and employs a feed-forward network for enhanced sequence representation learning. Given a sequence {X1, X2, \u2026, Xm} \u2208 Rm\u00d7l with m time variables and a lag length of l to get the results {\u01761,\u01762,\u2026, \u0176m} \u2208 Rm\u00d7p with desired forecast length p, the iTransformer embeds each individual variable separately\nh = Embedding(Xi), i = 1,2,\u2026, m,\nh = LN = LN (h + SA(h)), l = 0,1, ..., L - 1,\nh+1 = LN(FFN(\u0125\u00a6) + \u0125), l = 0,1, \u2026, L \u2212 1,\n\u0176\u2081 = Projection(h),\n where Embedding(\u00b7) embeds the sequence length l into dn, while the Projection(\u00b7) projects h\u2208 R1\u00d7dh to the desired forecast \u00dd \u2208 R1\u00d7p and L is the layer of the iTransformer. Both operations are implemented through multi-layer perceptron (MLP). LN() is the Layer Normalization operation and FFN(\u00b7) denotes the feed forward network."}, {"title": "C. Long short-term memory", "content": "LSTM effectively manages long sequential data by modeling long-term dependencies through its gating mechanism. The input gate, forget gate, and output gate control the flow of information, determining what to store, discard, or retrieve at each time step. The cell state c\u2081 enables smooth propagation of information across time steps, reducing issues like gradient vanishing and explosion that are common in traditional RNNs. Its mathematical expressions are as follows:\nit = \u03c3(Wi[ht\u22121, xt] + b\u2081),\nf\u2081 = \u03c3(Wf[ht\u22121, x\u025b] + bf),\not = \u03c3(Wo[ht\u22121, xt] + bo),\nct = f\u2081 \u00a9 ct\u22121 + i\u2081 \u00a9 tanh(W[ht\u22121, xt] + bc),\nht = ot tanh(ct),"}, {"title": "D. Kolmogorov-Arnold Networks", "content": "KAN are a type of neural network architecture inspired by the Kolmogorov-Arnold representation theorem [19], offering a key distinction from traditional MLP. While MLPs apply fixed activation functions at the nodes (neurons), KAN introduces learnable activation functions on the edges (weights), allowing for more flexible and dynamic learning. The Kolmogorov- Arnold representation theorem states that any continuous multivariate function can be represented as a composition of univariate functions along with addition operations:\nf(x) = f(x1, X1,\u2026, xn) =\n\u03a3_{q=1}^{2n+1} \u03a6q(\u03a3_{p=1}^{n} \u03a6_{q,p}(xp)),\n where q.p() represents univariate functions that transform each input variable xp defined as $q,p: [0,1] \u2192 R and \u00deq: R \u2192 R.\nSince all the functions to be learned are univariate, [19] introduced B-spline curves to estimate each function and define a KAN layer as\nThus, Eq. (5) represents a two-layer KAN structure, where the first layer has nin = n and nout = 2n + 1, and the second layer has nin = 2n + 1 and nout = 1. Similar to an MLP, the output of a KAN with L layers and with the input Xo can be represented as:\n\u039a\u0391\u039d(\u03a7\u03bf) = (\u03a6_{L\u22121} \u00b0 \u03a6_{L\u22122}\u00b0\u2026\u00b0 \u03a6\u2081\u00b0\u03a6\u03bf)\u03a7\u03bf,\nX_{l+1} = \u03a6_l X_l\\ \u03a6_l =\n\\begin{bmatrix} \\Phi_{l,1,1}(.) &  \\Phi_{l,1,2}(.)&  \\dots & \\Phi_{l,1,n_l}(.) \\\\  \\Phi_{l,2,1}(.) &  \\Phi_{l,2,2}(.) &  \\dots & \\Phi_{l,2,n_l}(.) \\\\  \\vdots &  \\vdots &  \\vdots & \\vdots \\\\ \\Phi_{l,n_{l+1},1}(.) & \\Phi_{l,n_{l+1},2}(.) & \\dots & \\Phi_{l,n_{l+1},n_l}(.)\\end{bmatrix} \\\\ X_l=\\begin{bmatrix} X_{l,1}\\\\ X_{l,2}\\\\ \\vdots \\\\X_{l,n_l}  \\end{bmatrix}\n where X\u2081 \u2208 Rni and X1+1 \u2208 RN1+1 are the pre-activation vector and post-activation vector of function matrix corresponding to the lth KAN layer \u03a6\u2081, respectively. x\u2081\u2081i denotes the activation value of the ith neuron in the Ith layer and $1,j,i() is the activation function that connects xl,i and X1+1,j"}, {"title": "E. Proposed Model", "content": "For multivariate-to-univariate forecasting tasks, the iTransformer may not fully perform as it operates primarily on the variable dimension. To address this, the input variables are divided into target variables and covariates. The target variable is encoded using the iTransformer, while the LSTM captures features from the covariates. Finally, cross-attention enhances the fusion of features between the target and covariates, and KAN maps these fused features to the desired forecast dimension. The structure of the proposed model is illustrated in Fig. 1."}, {"title": "III. CASE STUDY", "content": "This section presents the experimental data and the performance of the proposed model in the PV power dataset across four seasons. All experiments were conducted on a Windows 10 system with an Intel Xeon Silver 4214 CPU @ 2.20GHz and an NVIDIA GeForce RTX 2080 Ti GPU with 12GB of video memory. The software environment consisted of Python 3.9 and PyTorch 2.4.1 and optuna 4.0.0 [21]."}, {"title": "A. Data selection and preprocessing", "content": "The PV power data used in this study were sourced from the Desert Knowledge Australia Solar Centre\u00b9, specifically from Site 7 in Alice Springs, Australia (latitude: -23.76, longitude: 133.87). PV power data vary across seasons. To evaluate model performance under different conditions, the data were segmented into four seasons: Spring (September to November), Summer (December to February), Autumn (March to May), and Winter (June to August), covering a period of three years with a time resolution of 1 hour. Missing data were filled using forward linear interpolation, while negative active power values were set to zero due to potential sensor errors. The data were split into training, validation, and test sets in an 8:1:1 ratio. TABLE I. provides the details of the PV system parameters."}, {"title": "B. Experimental setting", "content": "To demonstrate the effectiveness of the proposed model, comparisons were made between iTransformer, LSTM, and the proposed model. All models were trained according to the methods outlined in TABLE II., with hyperparameter selections detailed in TABLE III. . To prevent overfitting, an early stopping strategy was employed, and Bayesian optimization with Optuna2 was used to search for the optimal parameter combination within the parameter space."}, {"title": "C. Evaluation Metrics", "content": "This study evaluates the forecasting performance of different models using mean absolute error (MAE), root mean square error (RMSE), the coefficient of determination (R2), and mean bias error (MBE) [23]. Smaller values of MAE and RMSE are preferable, while an R\u00b2 closer to 1 is ideal. Additionally, the MBE should be as close to 0 as possible for optimal performance. The metrics are given by\nMAE = 1/n \u03a3_{i=1}^n |yi - \u0177i|,\nRMSE = \u221a(1/n \u03a3_{i=1}^n (yi - \u0177i)\u00b2),"}, {"title": "D. Results", "content": "The forecasting performance of the proposed model, alongside benchmark models iTransformer and LSTM, is evaluated across four seasons, as shown in TABLE IV. and Fig. 3. These metrics provide a comprehensive assessment of each model's accuracy, predictive precision, and bias when forecasting PV power under seasonally varying data characteristics.\nIn Spring, the proposed model achieves an MAE of 0.1335 and an RMSE of 0.3406, with an R\u00b2 value of 0.9646, the highest among the models. These metrics indicate a strong correlation with observed values, underscoring the proposed model's accuracy in capturing seasonal PV data variability. The iTransformer model, in comparison, yields a slightly higher MAE of 0.1455 and RMSE of 0.3448, reflecting marginally lower accuracy, but it reaches the lowest MBE which indicates a slight tendency towards overestimation. The LSTM, with an MAE of 0.1448 and a higher RMSE of 0.3542, shows a noticeable MBE of 0.0387, suggesting a tendency toward positive bias.\nDuring Summer, all models display enhanced forecast accuracy. The proposed model achieves its lowest seasonal RMSE of 0.1153, along with an R\u00b2 of 0.9966. This performance highlights the proposed model's robustness in the relatively stable, high-sunlight conditions of Summer. While the iTransformer also performs well with an MAE of 0.0591, RMSE of 0.1316, and R\u00b2 of 0.9956, the proposed model consistently outperforms it in terms of precision and accuracy. The LSTM model has a comparable MAE of 0.0533 but a slightly higher RMSE of 0.1278. Notably, all models display a small negative MBE, indicating slight underestimation, with the proposed model's bias being the smallest at -0.0047.\nAutumn, characterized by increased data variability, poses a more complex forecasting scenario. The proposed model achieves high accuracy with an MAE of 0.0986, RMSE of 0.2574, and an R2 of 0.9761, demonstrating its robustness against seasonal shifts. The iTransformer, with an MAE of 0.1073 and RMSE of 0.2805, shows slightly reduced performance relative to the proposed model. The LSTM model exhibits the lowest accuracy in Autumn, with an MAE of 0.1182, RMSE of 0.323, and the lowest R\u00b2 value 0.9623, indicating its limited adaptability to Autumn's increased variability.\nIn Winter, the proposed model continues to achieve the best accuracy, with the lowest MAE of 0.0428 and RMSE of 0.1717, along with a high R\u00b2 of 0.9913. These results reflect the model's stable forecasting performance even under Winter's lower sunlight conditions. Both iTransformer and LSTM models perform with slightly higher MAE values of 0.0468 and 0.0473, respectively, and increased RMSEs of 0.1806 and 0.1907, respectively, indicating reduced accuracy compared to the proposed model. The proposed model exhibits a modest positive MBE of 0.0126, suggesting a minor degree of overestimation, though this remains within acceptable limits.\nOverall, the proposed model consistently outperforms iTransformer and LSTM across all seasonal datasets, as evidenced by lower MAE and RMSE values and higher R2 scores. These metrics affirm the proposed model's capability to capture seasonal variations in PV power data more effectively than the benchmarks. While LSTM tends to exhibit greater bias across seasons, iTransformer performs relatively close to the proposed model and achieves the best MBE relatively but demonstrates reduced precision. In the summer, all models underestimated PV power output, likely due to their inability to accurately capture the peak solar irradiance conditions typical of this season. Conversely, during Spring, Autumn, and Winter, the models exhibited an overestimation of power generation, potentially stemming from biases in historical data and inherent limitations in how the models process seasonal variations.\nThe findings validate the proposed model's suitability for accurate and resilient PV power forecasting across diverse seasonal patterns."}, {"title": "IV. CONCLUSION", "content": "Accurate PV power forecasting is vital for stable grid operation and integrating solar energy effectively, supporting sustainability and carbon neutrality goals. The proposed model significantly improves forecasting accuracy by combining iTransformer encoding for target variables with LSTM-based feature extraction specifically designed for covariates. This separation in feature extraction addresses the unique characteristics of the target and covariate variables, while cross-attention effectively captures the interactions across variable dimensions and temporal dimensions. Achieving superior MAE, RMSE, and R2 metrics across all seasons, these results highlight the model's reliability for PV forecasting and its adaptability to diverse conditions."}]}