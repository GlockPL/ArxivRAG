{"title": "Hyperparameter Optimization for Driving Strategies Based on Reinforcement Learning", "authors": ["Nihal Acharya Adde", "Hanno Gottschalk", "Andreas Ebert"], "abstract": "This paper focuses on hyperparameter optimization for autonomous driving strategies based on Reinforcement Learning (RL). We provide a detailed description of training the RL agent in a simulation environment. Subsequently, we employ Efficient Global Optimization (EGO) algorithm that uses Gaussian Process (GP) fitting for hyperparameter optimization in RL. Before this optimization phase, Gaussian process interpolation is applied to fit the surrogate model, for which the hyperparameter set is generated using Latin hypercube sampling. To accelerate the evaluation, parallelization techniques are employed. Following the hyperparameter optimization procedure, a set of hyperparameters is identified, resulting in a noteworthy enhancement in overall driving performance. There is a substantial increase of 4% when compared to existing manually tuned parameters and the hyperparameters discovered during the initialization process using Latin hypercube sampling. After the optimization, we analyze the obtained results thoroughly and conduct a sensitivity analysis to assess the robustness and generalization capabilities of the learned autonomous driving strategies. The findings from this study contribute to the advancement of Gaussian process based Bayesian optimization to optimize the hyperparameters for autonomous driving in RL, providing valuable insights for the development of efficient and reliable autonomous driving systems.", "sections": [{"title": "1 Introduction", "content": "Autonomous Driving (AD), a transformative technology, holds the potential for revolutionizing transportation, enhancing safety, and improving efficiency. Deep Reinforcement Learning (DRL), prominent in various applications, including autonomous driving [25], has shown promise in leveraging optimal neural network structures [4] to replace traditional rule-based controllers and enable model-free optimal control in complex environments [19]. However, achieving optimal performance and stability during RL training necessitates careful tuning of hyperparameters such as discount factor, learning rate, batch size, network architectures, etc. These manually set parameters significantly influence the learning process and overall system performance. Hyperparameter Optimization (HPO) on RL poses several challenges due to the unique characteristics of RL algorithms and environments [3,13]. Some of the key challenges include:\n\nSample Complexity: RL algorithms demand extensive interactions for ef-fective policy learning. High sample complexity makes hyperparameter op-timization computationally expensive, requiring evaluation across multiple training episodes\nExploration-Exploitation Trade-off: Finding the right balance between ex-ploration (trying out different actions to discover optimal strategies) and exploitation (taking actions that are known to yield high rewards) is cru-cial for effective learning, as overly conservative or aggressive settings hinder learning progress.\nHigh-Dimensional Search Space:RL involves a complex, high-dimensional search space with numerous hyperparameters. Increasing dimensionality chal-lenges the search for optimal hyperparameters, making it difficult to explore various possible configurations.\nNon-Stationarity: RL environments exhibit non-stationarity, necessitating consideration of temporal aspects in hyperparameter optimization. Hyper-parameters initially effective may become suboptimal as the environment evolves during the learning process.\nSensitivity to Initial Conditions: RL algorithms are sensitive to initial condi-tions and hyperparameter changes. Small hyperparameter adjustments can lead to significantly different learning dynamics and performance, compli-cating the search for robust parameter sets.\nInteractions and Dependencies: Hyperparameters in RL are interdependent, making the search space complex. Adjusting one hyperparameter may impact the effectiveness of others, requiring careful consideration of interactions.\nNon-Convexity and Noisy Rewards: The RL optimization landscape is non-convex, meaning there can be multiple local optima that differ in perfor-mance. Moreover, rewards in RL can be noisy and have high variance, which makes it challenging to differentiate between the impact of hyperparameters and inherent randomness in the environment.\nTransferability: Hyperparameters effective in one RL task may not general-ize, requiring task-specific fine-tuning. Achieving good performance across diverse RL problems adds complexity to hyperparameter optimization due to variations in task characteristics.\nBalancing exploration and exploitation, leveraging prior knowledge, and uti-lizing computational resources effectively are key considerations in finding op-timal hyperparameters for RL algorithms. This study highlights these issues as focal points for investigation. The paper aims to train an AD task through RL within a precisely controlled simulation environment using the Proximal Policy"}, {"title": "2 Related Work", "content": "Autonomous driving control has witnessed a growing interest in learning-based strategies, with RL showcasing notable advancements. RL excels in scenarios where predefined rules are ambiguous, leveraging environment information for decision-making. RL agents, capable of handling diverse situations through trial and error learning, offer advantages over manually designed policies [26]. Deep RL methods in autonomous driving enhance response time and exploit autopilot benefits [29]. Despite this progress, RL agents can struggle in complex situations or demand extensive data. To overcome these challenges, RL algorithms can benefit from expert priors and motion skills, improving learning efficiency and driving performance [29]. The combination of RL and rule-based constraints en-hances autonomous vehicle safety [28]. In this study, we use RL to learn driving behavior in a simulated environment via an external car controller with pre-defined rules. RL focuses on acquiring smooth driving control by learning the controller's behavior. Additional rule-based safety features are implemented for real-world tests.\nWhile machine learning models have demonstrated significant success across a wide range of applications [9,16], their performance heavily relies on the ap-propriate configuration of hyperparameters. As models become larger and more complex, the need for efficient and autonomous algorithms for tuning hyper-parameters becomes increasingly critical to achieve optimal performance. Re-"}, {"title": "HPO for Driving Strategies based on RL", "content": "Optimization (PPO) algorithm. It seeks to optimize hyperparameters and inves-tigate their significance, thereby contributing to a deeper understanding of effec-tive parameter tuning for RL-based AD. The RL setting is treated as a blackbox function, where hyperparameters serve as inputs, and the objective is to max-imize the rewards obtained by the autonomous vehicle. As direct optimization of the RL black-box function is computationally expensive, surrogate model op-timization techniques are explored, providing a computationally efficient means to approximate black-box functions and effectively explore the hyperparame-ter search space [5]. Our approach incorporates Efficient Global Optimization (EGO) [14], where probabilistic uncertainties are explained through Gaussian random fields. It is an iterative optimization method aiming to find the global minimum of a costly black-box function by sequentially evaluating new points based on a surrogate model. Given imbalanced parameters in our optimization problem, we emphasize the need to handle such cases by preprocessing the data using appropriate feature scaling techniques and efficiently exploring the high-dimensional search space with interdependencies between hyperparameters. Additionally, we address the non-convexity of the RL optimization landscape and noise in RL rewards, which pose challenges in accurately assessing hyperparam-eter impact. Moreover, sensitivity analysis is conducted on the hyperparameters to gain insights into their impact on the RL system's performance. Leveraging these techniques, our study contributes to designing robust, efficient, and reliable RL-based AD systems."}, {"title": "3 Training Strategy Using RL", "content": "In RL [2, p. 378], the agent (e.g., a real or simulated robot) takes actions to maximize the cumulative reward to solve a sequential decision task. RL seeks an optimal policy through trial and error interactions, addressing the agent-environment interface, reward evaluation function, and Markov property [2]. The learning process involves mapping from states s to actions a to maximize rewards r. The agent receives sensory input as a state $s_t$ and, upon taking action $a_t$, obtains a reward $r_t$. This loop stabilizes the algorithm. Trial and error aid exploration for better actions, while the agent's memory of successful actions ensures effective decision exploitation.\n\n3.1 Learning in Simulation\nAs it is tedious and expensive to train an autonomous vehicle directly in the real world, we employ the power of the Unity3D simulator [1] to replicate the real-world scenario. The goal of the experiment is to train autonomous agents in simulation and test their behavior at a large three-lane oval proving ground (test track) available at our facility. Therefore, we aim to construct an exact replica of the real world, prioritizing the inclusion of minute features found in the actual environment. Multiple agents are trained on a simulated oval proving ground, aiming to navigate smoothly, minimizing jerks and abrupt movements and avoiding collisions. Despite the seemingly simple task, the control system is sensitive, requiring the algorithm to learn the controller behaviour. Stationary obstacles such as cones"}, {"title": "3.2 RL Algorithm and Network Architecture", "content": "In this research, we employ an Actor-Critic model with the PPO algorithm [24]. In on-policy methods like PPO, the agent learns from data collected during interactions with the environment under its current policy. Using a shared con-volutional component in a multi-head network, one head functions as the actor model, outputting a policy as a normal distribution by learning the mean and standard deviation of the actions. The other head serves as the critic or value function. The convolutional layer takes in semantic segmentation images of size 84 x 84 x 3 as an input. The network consists of 2 convolutional layers (conv2d) which are flattened and sent to the actor and critic blocks. Both the actor and critic networks employ fully connected layers (FC) with 256 units across two layers. The filters, kernels, stride and FC units are carefully selected after several trials and errors. The network is optimized using the Adam optimizer."}, {"title": "Selection of Hyperparameters", "content": "As per [24], there are few commonly used hyperparameters contributing to the algorithm performance. Hence we focus on refining six key hyperparameters: batch size, time horizon, discount factor, learning rate, PPO epoch, and entropy beta (entropy coefficient) [27,24]. Learning rate and entropy beta undergo gradual discounts as the learning process advances. The initialization ranges for these hyperparameters are detailed in The remaining hyperparameters either maintain a constant value or undergo discounting. We set the initial PPO clipping parameter \u0454 [27] value to 0.2, reducing it to 0.1 during RL training. This reduction aims to prevent the algorithm from taking larger steps as training progresses, thereby mitigating the risk of PPO deviating from the policy. The critic discount, which aims to equalize the significance of both the policy and value functions' error terms, is fixed at a constant value of 0.5.\nBatch Size: The batch size determines the number of transitions or samples used in each policy update during training. It represents the amount of data the agent collects from the environment, consisting of observed states, actions, resulting next states, obtained rewards, and additional information. A larger batch size provides more accurate gradient estimates but requires more computational resources. Conversely, a smaller batch size reduces com-putational burden but may result in noisier gradient estimates. The data is typically sampled from a replay buffer, which is refilled after each PPO up-date since it is an on-policy method.\nTime Horizon: The time horizon corresponds to the number of time steps or interactions the agent considers when collecting trajectories during train-ing. In each iteration of the PPO algorithm, the agent generates a set of trajectories by interacting with the environment. A trajectory comprises a sequence of states, actions, rewards, and possibly other information. The time horizon determines how far into the future the agent looks when gen-erating these trajectories. The choice of the time horizon affects the balance between exploration and exploitation. A shorter time horizon focuses on im-mediate decision-making and facilitates quicker adaptation to environmental"}, {"title": "HPO for Driving Strategies based on RL", "content": "changes. Conversely, a longer time horizon allows the agent to consider more future states and rewards, capturing complex dependencies and enabling better long-term planning.\nDiscount Factor: The discount factor, denoted as $\\gamma$, influences the weight assigned to future rewards in the reinforcement learning algorithm. Ranging between 0 and 1, the discount factor determines how much the agent values immediate rewards compared to future rewards. A value of 0 means that only immediate rewards are considered, while a value of 1 treats future rewards equally to immediate rewards. When computing the cumulative return or total reward for a trajectory or episode, the discount factor is applied to discount the value of future rewards. This encourages the agent to consider the long-term consequences of its actions and promotes the optimization of policies that maximize cumulative rewards over time.\nLearning Rate: The learning rate controls the magnitude of the parameter updates during the optimization process. It determines the step size at which the model parameters are adjusted based on the gradients computed during training. The learning rate is a scalar value that influences the speed and stability of convergence. A higher learning rate can lead to faster conver-gence, but it may also cause overshooting or instability. Conversely, a lower learning rate ensures more cautious updates but may require a larger number of iterations to reach convergence.\nPPO Epoch: Epoch updates refer to the process of performing multiple it-erations or updates of the policy network within a single epoch during the training phase. An epoch in PPO typically consists of several epoch updates. During each epoch update, the policy parameters are adjusted to improve the performance of the policy. The updates aim to maximize the objective function, which is often the expected cumulative reward or a surrogate ob-jective that approximates it.\nEntropy Beta: The entropy beta parameter $\\beta$ influences the exploration-exploitation trade-off in policy optimization. Entropy measures the uncer-tainty or randomness in an agent's policy. A higher entropy indicates greater uncertainty and decreases as the policy becomes more deterministic. In PPO, the entropy beta parameter controls the strength of the entropy regulariza-tion term in the objective function. This term encourages exploration by favoring policies with higher entropy, indicating greater randomness or un-certainty. By including this term, PPO promotes exploration, preventing premature convergence to suboptimal local optima and supporting the dis-covery of more optimal policies.\n3.3 Input and Output Specification\nThe driving relies solely on a camera setup, using an 84 \u00d7 84 \u00d7 3 semantic seg-mentation image from the front camera. Selection of a smaller image dimension is intended to reduce computational latency in the vehicle. This choice is also influenced by the ability to achieve satisfactory outcomes using smaller images within the given budget. Our approach leverages the ground truth segmentation"}, {"title": "3.4 Rewards", "content": "images available within the simulation environment instead of predicting them. During inference/deployment, the car camera employs a trained perception net-work to convert the captured images into their corresponding semantic masks, which are then utilized as input observations for the network. As perception net-works inherently contain a component of error, we deliberately introduced minor inaccuracies into our ground truth semantic segmentation images, incorporating approximately 5% false segmentations. These inaccuracies were observed from the perception network and, based on this, it focuses on adding inaccuracies mainly at the edges of the semantic masks (primarily on the road surface).\nThe network generates trajectory points used by the external car controller to guide the vehicle. To maintain fidelity between simulation and reality, an ac-curate replica of the actual car controller is employed in the simulation during training. The black-box nature of the car controller requires the RL algorithm to learn its operation through rewards. The car controller component of the au-tonomous driving system receives a set of 23 trajectory points as input, with each point consisting of 10 values encompassing positions, angles, heading, curvature, lateral displacement, and other relevant parameters, resulting in a total of 230 values. Generating and learning 230 trajectory points directly poses challenges, requiring a large network size and extensive training. To address this, we sim-plify the output to 6 trajectory points, each conveying essential x-axis position and velocity. Consequently, the network outputs a total of 12 values responsible for determining the placement of these 6 trajectory points. We reconstruct the full set of 23 points using Catmull-Rom spline interpolation [7]. Subsequently, further post-processing steps are conducted to calculate additional values such as heading, curvature, and other relevant parameters, resulting in a total of 230 values corresponding to the complete set of 23 trajectory points. These are in-puts to the car controller, guiding the autonomous vehicle's behavior. Through training, the network learns to position these points for smooth driving, guided by a complex reward function detailed in the next section.\nIn RL algorithms, rewards act as the objective function, guiding the agent's decision-making by evaluating the desirability of different states and actions. This enables the RL algorithm to discern and reinforce behaviors leading to higher rewards while discouraging actions resulting in lower rewards [2]. The reward function utilized in this study is a comprehensive function that incor-porates multiple factors to address two crucial aspects: the correct behavior of the external car controller and the driving behavior itself. The network must accurately position trajectory points to align with the external car controller's expectations, considering the controller's restrictions on value acceptance. Nega-tive rewards are assigned if the controller rejects a trajectory, discouraging such actions. Positive rewards are granted for successfully placing these points in the correct positions, aiming to steer the network towards accurate predictions. These rewards are crucial, especially in early training, motivating the network to"}, {"title": "4 Model-Based Hyperparameter Optimization", "content": "produce meaningful trajectory points. The second element of the reward func-tion primarily addresses driving behavior, penalizing undesirable actions like collisions and driving on non-drivable surfaces, while rewarding adherence to the correct path. Although these rewards are effective for guiding the vehicle, additional subtle aspects must be considered to achieve smooth driving without jerks. This reward component is designed based on [17], which introduces a dis-turbance scalar and rates the driving behavior accordingly. Training the network with crafted reward functions successfully yields meaningful trajectories. How-ever, achieving success requires meticulous tuning of hyperparameters impacting the model's performance, which will be explored in the next section.\nModel-based hyperparameter optimization enhances machine learning model performance by constructing a surrogate model to approximate the hyperparameter-performance relationship [10]. It iteratively refines the search space to find op-timal hyperparameters through an initialization and optimization process. In the initialization phase, a sampling method like Latin Hypercube Sampling (LHS) samples initial hyperparameter sets, and is evaluated for correspond-ing responses. A surrogate model is built from these observations. In the op-timization phase, acquisition functions guide the surrogate model to identify the hyperparameter configuration likely to yield the best performance. The chosen configuration is evaluated, and optimized with an optimizer like Efficient Global Optimizer (EGO) and the iterative process continues until a satisfactory solution is found or a termination criterion is met. The optimization aims to maximize cu-mulative rewards in the RL algorithm by balancing exploration and exploitation for better performance. 25-50% of the computational budget is allocated to data generation, with the remaining budget dedicated to the optimization phase.\n4.1 Search Space Exploration - Latin Hypercube Sampling\nLatin Hypercube Sampling (LHS) [21] is a statistical technique for generating representative samples in a multidimensional parameter space. Unlike traditional random sampling, where parameters are randomly chosen within their ranges, LHS employs a stratified sampling scheme where the sampled points are orga-nized in a grid-like structure that ensures superior coverage of the parameter space. Each parameter range is divided into equally sized intervals or bins, and a single value is sampled from each bin. This approach reduces bias, facilitates efficient exploration, enhances representativeness, and promotes diversity across the parameter space, especially in high-dimensional scenarios."}, {"title": "4.2 Gaussian Process", "content": "In statistics, Kriging [18] is an interpolation method based on Gaussian processes (GP) with prior covariances. The covariance of the Gaussian process represents uncertainties, providing not only the predicted mean of the objective function but also an estimate of the associated uncertainty. In HPO, GP interpolation probabilistically models the performance landscape, aiding in selecting which hyperparameter configurations to explore and exploit [22,8].\nThese models are characterized by a mean function $\\mu(x)$ and a covariance function (kernel) $k(x,x)$, which encodes the smoothness assumptions on $f(.)$. Given a finite set of input $X_{1:n_1}$, the outputs follow a joint Gaussian distribution:\n$f(x_{1:n_1})|\\theta \\sim N(\\mu(x_{1:n_1}), K_{\\theta}(X_{1:n_1}, X_{1:n_1})),$   (1)\nwhere $[\\u03bc(X_{1:n_1})]_k = \\mu(x_k)$ denotes the mean vector and $K_{\\theta}(X_{1:n_1}, X_{1:n_1})) \\in R^{n_1 \\times n_1}$ the covriance matrix or kernel. Here, $k_{\\theta}(., .)$ depicts a parameterised ker-nel with unknown hyperparameters $\\theta$ corresponding to lengthscales. We adopt a zero-mean prior notation, following conventions from [22]. When choosing a GP kernel, options include the squared exponential, Gaussian, Mat\u00e9rn kernels, etc., each representing different prior assumptions about the latent function [8].\nGiven the data points $X_i$, assuming a Gaussian distributed observation with noise $y_i = f(x_i) + \\epsilon$, where the noise is given by $\\epsilon \\sim N(0, \\sigma^2)$, the joint distri-"}, {"title": "4.3 EGO Optimization", "content": "[14] combined Gaussian processes with the Expected Improvement (EI) func-tion for derivative-free optimization, creating the Efficient Global Optimization (EGO) algorithm. This paper describes EGO as outlined by [14].\nLet $F$ be an expensive black-box function to be minimized. We sample $F$ at the different locations $x_i$ yielding the responses $y_i$. We build a Kriging model with a mean function $\\mu$ and a variance function $\\sigma^2$. EI can be expressed as: $E[I(x)] = E[max(f_{min} - Y, 0)]$ where $Y$ is the best known objective function so far following the distribution $N(\\mu(x), \\sigma^2(x))$. EI is computed by taking the ex-pectation over the predicted distribution of the GP. This involves integrating the improvement function over the predicted distribution, often approximated using Monte Carlo sampling or other analytical methods. Evaluation of the equation"}, {"title": "HPO for Driving Strategies based on RL", "content": "using an error function can be found in [11]. Next, we determine our next sam-pling point as: $X_{n+1} = arg max_x E[I(x)]$ We then test the response $y_{n+1}$ of our black-box function $F$ at $x_{n+1}$, rebuild the model taking into account the new information gained, and research the point of maximum expected improvement again. Algorithm 1 provides a concise overview of the EGO process."}, {"title": "5 Experiment", "content": "5.1 Setting Up the Black-box Function\nThe black-box function can be written as $f(x_1,x_2,....X_n) = Y$, where the re-sponse variable $Y$ signifies the maximum cumulative reward per episode. Each run of this blackbox training signifies a complete run of the PPO algorithm with a certain set of hyperparameters. To expedite the process, we use 5 agents to collect state observations, actions, and rewards concurrently. An early stopping strategy halts training if rewards don't improve after about 50 iterations. Oth-erwise, the algorithm continues training for 300 iterations. Due to uncertainty about maximum achievable rewards, training isn't prematurely stopped even if it appears converged. For PPO training, as data batches are extracted from the buffer for PPO update, the buffer size is configured to be 20 times the batch size. After each PPO update, the learning rate and entropy beta decrease more for higher values within their ranges. This ensures that significant adjustments are made for larger ranges, which helps improve the training process."}, {"title": "5.2 Black-box Optimization", "content": "Training infrastructure The algorithm is trained in a cloud environment with 4 Nvidia V100 32GB GPUs. During data generation, 8 RL jobs are run in parallel across 4 GPUs. In the EGO phase with qEI, as we select the value of q = 4, 4 RL jobs are run in parallel across the GPUs.\nAfter setting up the black-box function, initial observations are gathered by repeatedly running the black-box function with various sets of hyperparameters. The hyperparameter sets (200\u00d76) are selected through LHS, with a larger budget of 50% allocated for generating the initial data due to RL's high unpredictabil-ity. Parallelly executing the black-box function to generate 200 responses, the surrogate model is fitted using Gaussian process interpolation with a Gaussian kernel $exp(- \\frac{1}{2} \\sum_{i=1}^6 \\theta_j |x_i - x_j|^2)$ and nugget effect (adds a small non-zero vari-ance term which accounts for noise or measurement errors in the training data) as the covariance function [11]. Here, the squared term ensures a smooth corre-lation with a continuous gradient. $\\theta_j$ is a learnable hyperparameter which serves as a width parameter determining the reach of influence for a sample point. In Table 1, hyperparameters exhibit varying ranges. To enhance GP fitting, scaling is applied, utilizing the logit function for the discount rate and the com-mon logarithmic function $log_{10}$ for learning rate and PPO Beta. Scaling ensures balanced representation, improving GP model effectiveness. Following a satis-factory fit, parallel EGO (4 runs) employs the qEI acquisition function to maxi-mize the response variable, identifying the most promising hyperparameter set. This stage utilizes the remaining computational budget, executing 200 training runs during optimization. We experimented with various optimization runs using different budgets for initialization and optimization phases. Starting EGO op-timization with fewer initial datasets took significantly longer for the optimizer to find better hyperparameters, and the expected performance wasn't achieved. Based on these trials, we chose to run 400 total training sessions, allocating 50% of the budget to initialization and the EGO phase.\n5.3 Analysis\nFollowing black-box optimization, we compare the maximum cumulative rewards from initial observations to those obtained through EGO optimization, evaluat-ing overall improvement. A time study examines the duration of the optimization process. Additionally, sensitivity analysis using ANOVA [23] explores how hy-perparameters influence our function's performance, assessing the significance of parameters and their interactions."}, {"title": "6 Results", "content": "6.1 Performance Analysis\nFirstly, the performance analysis assesses the optimization method's effective-ness by comparing the maximum cumulative rewards from initial observations"}, {"title": "6.2 Time Analysis", "content": "The algorithm typically takes 6 to 16 hours, primarily influenced by the batch size. Considering 200 hyperparameter sets without early stopping, the initial data generation could take approximately 2200 hours. However, with 20% of trainings stopping early and 8 parallel trainings with GPU utilization, this is reduced to 10 days. For EGO optimization with parallel qEI acquisition, 4 parallel trainings lead to a total time of 20 days for 200 hyperparameter sets. In this case, early stopping is not as prevalent as EGO learns to generate hyperparameters that yield high cumulative rewards."}, {"title": "6.3 Sensitivity Analysis", "content": "This section summarizes the statistical performance of our hyperparameters, employing regression for model fitting and ANOVA for significance evaluation. A leave-one-out ablation study was also conducted, systematically excluding influential hyperparameters at each step to understand their sensitivity and interdependencies. This study helps identify the most impactful hyperparameters and assess their contributions to the model's predictive accuracy. Given the similarity in results, only the findings from the ANOVA are discussed."}, {"title": "HPO for Driving Strategies based on RL", "content": "summarizes the hyperparameter influences, with ANOVA sums of squares (SS) indicating their respective impact. The F value column provides the result of the ANOVA F-test for each factor, with higher values signifying a more substantial effect on the dependent variable. Finally, Pr(>F) column presents the p-values of the F test statistic associated with each factor to determine its significance. In the context of hypothesis testing in ANOVA, a p-value less than the chosen significance level (commonly 0.05) suggests that you have enough evidence to reject the null hypothesis. In summary, the null hypothesis states that all the hyperparameters seem to have a statistically significant impact on the dependent variable (reward). Compared to the other hyperparameters, the p-value of the time horizon is considerably higher making it less significant to others. Figure 6 plots the percentage of the total sum of squares contributed by each hyperpa-rameter.ANOVA and the ablation study suggest a descending order of influence: Learning rate (most influential) > Batch > PPO Epochs > Gamma > Beta > Time Horizon (least influential). Time Horizon and Beta exhibit the lowest sensitivity, suggesting their variations have the least impact on performance."}, {"title": "7 Conclusion and Future work", "content": "Our study optimized the PPO algorithm for AD through parallel EGO opti-mization, yielding a notable 4% performance boost. The primary goal was to automate hyperparameter discovery in a cloud environment, eliminating the need for repeated RL algorithm runs through parallelization. Sensitivity analyses pro-vided valuable insights into parameter importance. This method shows potential for enhancing RL-based tasks by identifying optimal hyperparameter setups. In HPO, significant improvements are possible. A promising approach is multi-objective optimization to maximize rewards while minimizing computa-tional resources for faster training. Our future research explores deep GPs and evolutionary algorithms as alternative optimization methods and further inte-grating network architecture search to optimize the overall performance. Fur-thermore, conducting a comprehensive benchmark comparison could provide valuable insights into performance metrics. To effectively evaluate driving per-formance, especially in complex scenarios with ambiguous rewards, a dedicated"}]}