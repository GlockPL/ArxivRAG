{"title": "Hyperparameter Optimization for Driving Strategies Based on Reinforcement Learning", "authors": ["Nihal Acharya Adde", "Hanno Gottschalk", "Andreas Ebert"], "abstract": "This paper focuses on hyperparameter optimization for autonomous driving strategies based on Reinforcement Learning (RL). We provide a detailed description of training the RL agent in a simulation environment. Subsequently, we employ Efficient Global Optimization (EGO) algorithm that uses Gaussian Process (GP) fitting for hyperparameter optimization in RL. Before this optimization phase, Gaussian process interpolation is applied to fit the surrogate model, for which the hyperparameter set is generated using Latin hypercube sampling. To accelerate the evaluation, parallelization techniques are employed. Following the hyperparameter optimization procedure, a set of hyperparameters is identified, resulting in a noteworthy enhancement in overall driving performance. There is a substantial increase of 4% when compared to existing manually tuned parameters and the hyperparameters discovered during the initialization process using Latin hypercube sampling. After the optimization, we analyze the obtained results thoroughly and conduct a sensitivity analysis to assess the robustness and generalization capabilities of the learned autonomous driving strategies. The findings from this study contribute to the advancement of Gaussian process based Bayesian optimization to optimize the hyperparameters for autonomous driving in RL, providing valuable insights for the development of efficient and reliable autonomous driving systems.", "sections": [{"title": "1 Introduction", "content": "Autonomous Driving (AD), a transformative technology, holds the potential for revolutionizing transportation, enhancing safety, and improving efficiency. Deep Reinforcement Learning (DRL), prominent in various applications, including autonomous driving [25], has shown promise in leveraging optimal neural network structures [4] to replace traditional rule-based controllers and enable model-free"}, {"title": "2 Related Work", "content": "Autonomous driving control has witnessed a growing interest in learning-based strategies, with RL showcasing notable advancements. RL excels in scenarios where predefined rules are ambiguous, leveraging environment information for decision-making. RL agents, capable of handling diverse situations through trial and error learning, offer advantages over manually designed policies [26]. Deep RL methods in autonomous driving enhance response time and exploit autopilot benefits [29]. Despite this progress, RL agents can struggle in complex situations or demand extensive data. To overcome these challenges, RL algorithms can benefit from expert priors and motion skills, improving learning efficiency and driving performance [29]. The combination of RL and rule-based constraints enhances autonomous vehicle safety [28]. In this study, we use RL to learn driving behavior in a simulated environment via an external car controller with predefined rules. RL focuses on acquiring smooth driving control by learning the controller's behavior. Additional rule-based safety features are implemented for real-world tests.\nWhile machine learning models have demonstrated significant success across a wide range of applications [9,16], their performance heavily relies on the appropriate configuration of hyperparameters. As models become larger and more complex, the need for efficient and autonomous algorithms for tuning hyperparameters becomes increasingly critical to achieve optimal performance. Re-"}, {"title": "3 Training Strategy Using RL", "content": "In RL [2, p. 378], the agent (e.g., a real or simulated robot) takes actions to maximize the cumulative reward to solve a sequential decision task. RL seeks an optimal policy through trial and error interactions, addressing the agent-environment interface, reward evaluation function, and Markov property [2]. The learning process involves mapping from states s to actions a to maximize rewards r. The agent receives sensory input as a state $s_t$ and, upon taking action $a_t$, obtains a reward $r_t$. This loop stabilizes the algorithm. Trial and error aid exploration for better actions, while the agent's memory of successful actions ensures effective decision exploitation."}, {"title": "3.1 Learning in Simulation", "content": "As it is tedious and expensive to train an autonomous vehicle directly in the real world, we employ the power of the Unity3D simulator [1] to replicate the real-world scenario. The goal of the experiment is to train autonomous agents in simulation and test their behavior at a large three-lane oval proving ground (test track) available at our facility. Therefore, we aim to construct an exact replica of the real world, prioritizing the inclusion of minute features found in the actual environment. Multiple agents are trained on a simulated oval proving ground, aiming to navigate smoothly, minimizing jerks and abrupt movements and avoiding collisions. Despite the seemingly simple task, the control system is sensitive, requiring the algorithm to learn the controller behaviour."}, {"title": "3.2 RL Algorithm and Network Architecture", "content": "In this research, we employ an Actor-Critic model with the PPO algorithm [24]. In on-policy methods like PPO, the agent learns from data collected during interactions with the environment under its current policy. Using a shared convolutional component in a multi-head network, one head functions as the actor model, outputting a policy as a normal distribution by learning the mean and standard deviation of the actions. The other head serves as the critic or value function. The convolutional layer takes in semantic segmentation images of size 84 x 84 x 3 as an input. The network consists of 2 convolutional layers (conv2d) which are flattened and sent to the actor and critic blocks. Both the actor and critic networks employ fully connected layers (FC) with 256 units across two layers. The filters, kernels, stride and FC units are carefully selected after several trials and errors. The network is optimized using the Adam optimizer."}, {"title": "3.3 Input and Output Specification", "content": "The driving relies solely on a camera setup, using an 84 \u00d7 84 \u00d7 3 semantic segmentation image from the front camera. Selection of a smaller image dimension is intended to reduce computational latency in the vehicle. This choice is also influenced by the ability to achieve satisfactory outcomes using smaller images within the given budget. Our approach leverages the ground truth segmentation images available within the simulation environment instead of predicting them. During inference/deployment, the car camera employs a trained perception network to convert the captured images into their corresponding semantic masks, which are then utilized as input observations for the network. As perception networks inherently contain a component of error, we deliberately introduced minor inaccuracies into our ground truth semantic segmentation images, incorporating approximately 5% false segmentations. These inaccuracies were observed from the perception network and, based on this, it focuses on adding inaccuracies mainly at the edges of the semantic masks (primarily on the road surface).\nThe network generates trajectory points used by the external car controller to guide the vehicle. To maintain fidelity between simulation and reality, an accurate replica of the actual car controller is employed in the simulation during training. The black-box nature of the car controller requires the RL algorithm to learn its operation through rewards. The car controller component of the autonomous driving system receives a set of 23 trajectory points as input, with each point consisting of 10 values encompassing positions, angles, heading, curvature, lateral displacement, and other relevant parameters, resulting in a total of 230 values. Generating and learning 230 trajectory points directly poses challenges, requiring a large network size and extensive training. To address this, we simplify the output to 6 trajectory points, each conveying essential x-axis position and velocity. Consequently, the network outputs a total of 12 values responsible for determining the placement of these 6 trajectory points. We reconstruct the full set of 23 points using Catmull-Rom spline interpolation [7]. Subsequently, further post-processing steps are conducted to calculate additional values such as heading, curvature, and other relevant parameters, resulting in a total of 230 values corresponding to the complete set of 23 trajectory points. These are inputs to the car controller, guiding the autonomous vehicle's behavior. Through training, the network learns to position these points for smooth driving, guided by a complex reward function detailed in the next section."}, {"title": "3.4 Rewards", "content": "In RL algorithms, rewards act as the objective function, guiding the agent's decision-making by evaluating the desirability of different states and actions. This enables the RL algorithm to discern and reinforce behaviors leading to higher rewards while discouraging actions resulting in lower rewards [2]. The reward function utilized in this study is a comprehensive function that incorporates multiple factors to address two crucial aspects: the correct behavior of the external car controller and the driving behavior itself. The network must accurately position trajectory points to align with the external car controller's expectations, considering the controller's restrictions on value acceptance. Negative rewards are assigned if the controller rejects a trajectory, discouraging such actions. Positive rewards are granted for successfully placing these points in the correct positions, aiming to steer the network towards accurate predictions. These rewards are crucial, especially in early training, motivating the network to produce meaningful trajectory points. The second element of the reward function primarily addresses driving behavior, penalizing undesirable actions like collisions and driving on non-drivable surfaces, while rewarding adherence to the correct path. Although these rewards are effective for guiding the vehicle, additional subtle aspects must be considered to achieve smooth driving without jerks. This reward component is designed based on [17], which introduces a disturbance scalar and rates the driving behavior accordingly. Training the network with crafted reward functions successfully yields meaningful trajectories. However, achieving success requires meticulous tuning of hyperparameters impacting the model's performance, which will be explored in the next section."}, {"title": "4 Model-Based Hyperparameter Optimization", "content": "Model-based hyperparameter optimization enhances machine learning model performance by constructing a surrogate model to approximate the hyperparameter-performance relationship [10]. It iteratively refines the search space to find optimal hyperparameters through an initialization and optimization process. In the initialization phase, a sampling method like Latin Hypercube Sampling (LHS) samples initial hyperparameter sets, and is evaluated for corresponding responses. A surrogate model is built from these observations. In the optimization phase, acquisition functions guide the surrogate model to identify the hyperparameter configuration likely to yield the best performance. The chosen configuration is evaluated, and optimized with an optimizer like Efficient Global Optimizer (EGO) and the iterative process continues until a satisfactory solution is found or a termination criterion is met. The optimization aims to maximize cumulative rewards in the RL algorithm by balancing exploration and exploitation for better performance. 25-50% of the computational budget is allocated to data generation, with the remaining budget dedicated to the optimization phase."}, {"title": "4.1 Search Space Exploration - Latin Hypercube Sampling", "content": "Latin Hypercube Sampling (LHS) [21] is a statistical technique for generating representative samples in a multidimensional parameter space. Unlike traditional random sampling, where parameters are randomly chosen within their ranges, LHS employs a stratified sampling scheme where the sampled points are organized in a grid-like structure that ensures superior coverage of the parameter space. Each parameter range is divided into equally sized intervals or bins, and a single value is sampled from each bin. This approach reduces bias, facilitates efficient exploration, enhances representativeness, and promotes diversity across the parameter space, especially in high-dimensional scenarios."}, {"title": "4.2 Gaussian Process", "content": "In statistics, Kriging [18] is an interpolation method based on Gaussian processes (GP) with prior covariances. The covariance of the Gaussian process represents uncertainties, providing not only the predicted mean of the objective function but also an estimate of the associated uncertainty. In HPO, GP interpolation probabilistically models the performance landscape, aiding in selecting which hyperparameter configurations to explore and exploit [22,8].\nThese models are characterized by a mean function $\u03bc(x)$ and a covariance function (kernel) $k(x,x)$, which encodes the smoothness assumptions on $f(.)$. Given a finite set of input $X_{1:n_i}$, the outputs follow a joint Gaussian distribution:\n$f(x_{1:n_i})|\\theta \\sim N(\u03bc(x_{1:n_i}), K_\\theta(X_{1:n_i}, X_{1:n_i}))$,\nwhere $[\u03bc(X_{1:n_i})]_k = \u03bc(x_k)$ denotes the mean vector and $K_\\theta(X_{1:n_i}, X_{1:n_i})) \u2208 \\mathbb{R}^{n_ixn_i}$ the covriance matrix or kernel. Here, $k_\\theta(., .)$ depicts a parameterised kernel with unknown hyperparameters corresponding to lengthscales. We adopt a zero-mean prior notation, following conventions from [22]. When choosing a GP kernel, options include the squared exponential, Gaussian, Mat\u00e9rn kernels, etc., each representing different prior assumptions about the latent function [8].\nGiven the data points $X_i$, assuming a Gaussian distributed observation with noise $y_i = f(x_i) + \\epsilon$, where the noise is given by $\\epsilon \\sim N(0, \u03c3^2)$, the joint distri-"}, {"title": "4.3 EGO Optimization", "content": "[14] combined Gaussian processes with the Expected Improvement (EI) function for derivative-free optimization, creating the Efficient Global Optimization (EGO) algorithm. This paper describes EGO as outlined by [14].\nLet F be an expensive black-box function to be minimized. We sample F at the different locations xi yielding the responses yi. We build a Kriging model with a mean function $\u03bc$ and a variance function $\u03c3^2$. El can be expressed as:\n$E[I(x)] = E[max(f_{min} - Y, 0)]$ where Y is the best known objective function so far following the distribution $N(\u03bc(x), \u03c3^2(x))$. EI is computed by taking the expectation over the predicted distribution of the GP. This involves integrating the improvement function over the predicted distribution, often approximated using Monte Carlo sampling or other analytical methods. Evaluation of the equation"}, {"title": "5 Experiment", "content": "5.1 Setting Up the Black-box Function\nThe black-box function can be written as $f(x_1,x_2,....x_n) = Y$, where the response variable Y signifies the maximum cumulative reward per episode. Each run of this blackbox training signifies a complete run of the PPO algorithm with a certain set of hyperparameters. To expedite the process, we use 5 agents to collect state observations, actions, and rewards concurrently. An early stopping strategy halts training if rewards don't improve after about 50 iterations. Otherwise, the algorithm continues training for 300 iterations. Due to uncertainty about maximum achievable rewards, training isn't prematurely stopped even if it appears converged. For PPO training, as data batches are extracted from the buffer for PPO update, the buffer size is configured to be 20 times the batch size. After each PPO update, the learning rate and entropy beta decrease more for higher values within their ranges. This ensures that significant adjustments are made for larger ranges, which helps improve the training process."}, {"title": "5.2 Black-box Optimization", "content": "Training infrastructure The algorithm is trained in a cloud environment with 4 Nvidia V100 32GB GPUs. During data generation, 8 RL jobs are run in parallel across 4 GPUs. In the EGO phase with qEI, as we select the value of q = 4, 4 RL jobs are run in parallel across the GPUs.\nAfter setting up the black-box function, initial observations are gathered by repeatedly running the black-box function with various sets of hyperparameters. The hyperparameter sets (200\u00d76) are selected through LHS, with a larger budget of 50% allocated for generating the initial data due to RL's high unpredictability. Parallelly executing the black-box function to generate 200 responses, the surrogate model is fitted using Gaussian process interpolation with a Gaussian kernel $exp(- \\sum_{j=1}^{6} 0_j|x_i - x_j|^2)$ and nugget effect (adds a small non-zero variance term which accounts for noise or measurement errors in the training data) as the covariance function [11]. Here, the squared term ensures a smooth correlation with a continuous gradient. $0_j$ is a learnable hyperparameter which serves as a width parameter determining the reach of influence for a sample point.\nIn Table 1, hyperparameters exhibit varying ranges. To enhance GP fitting, scaling is applied, utilizing the logit function for the discount rate and the common logarithmic function $log_{10}$ for learning rate and PPO Beta. Scaling ensures balanced representation, improving GP model effectiveness. Following a satisfactory fit, parallel EGO (4 runs) employs the qEI acquisition function to maximize the response variable, identifying the most promising hyperparameter set. This stage utilizes the remaining computational budget, executing 200 training runs during optimization. We experimented with various optimization runs using different budgets for initialization and optimization phases. Starting EGO optimization with fewer initial datasets took significantly longer for the optimizer to find better hyperparameters, and the expected performance wasn't achieved. Based on these trials, we chose to run 400 total training sessions, allocating 50% of the budget to initialization and the EGO phase."}, {"title": "5.3 Analysis", "content": "Following black-box optimization, we compare the maximum cumulative rewards from initial observations to those obtained through EGO optimization, evaluating overall improvement. A time study examines the duration of the optimization process. Additionally, sensitivity analysis using ANOVA [23] explores how hyperparameters influence our function's performance, assessing the significance of parameters and their interactions."}, {"title": "6 Results", "content": "6.1 Performance Analysis\nFirstly, the performance analysis assesses the optimization method's effectiveness by comparing the maximum cumulative rewards from initial observations"}, {"title": "6.2 Time Analysis", "content": "The algorithm typically takes 6 to 16 hours, primarily influenced by the batch size. Considering 200 hyperparameter sets without early stopping, the initial data generation could take approximately 2200 hours. However, with 20% of trainings stopping early and 8 parallel trainings with GPU utilization, this is reduced to 10 days. For EGO optimization with parallel qEI acquisition, 4 parallel trainings lead to a total time of 20 days for 200 hyperparameter sets. In this case, early stopping is not as prevalent as EGO learns to generate hyperparameters that yield high cumulative rewards."}, {"title": "6.3 Sensitivity Analysis", "content": "This section summarizes the statistical performance of our hyperparameters, employing regression for model fitting and ANOVA for significance evaluation. A leave-one-out ablation study was also conducted, systematically excluding influential hyperparameters at each step to understand their sensitivity and interdependencies. This study helps identify the most impactful hyperparameters and assess their contributions to the model's predictive accuracy. Given the similarity in results, only the findings from the ANOVA are discussed."}, {"title": "7 Conclusion and Future work", "content": "Our study optimized the PPO algorithm for AD through parallel EGO optimization, yielding a notable 4% performance boost. The primary goal was to automate hyperparameter discovery in a cloud environment, eliminating the need for repeated RL algorithm runs through parallelization. Sensitivity analyses provided valuable insights into parameter importance. This method shows potential for enhancing RL-based tasks by identifying optimal hyperparameter setups.\nIn HPO, significant improvements are possible. A promising approach is multi-objective optimization to maximize rewards while minimizing computational resources for faster training. Our future research explores deep GPs and evolutionary algorithms as alternative optimization methods and further integrating network architecture search to optimize the overall performance. Furthermore, conducting a comprehensive benchmark comparison could provide valuable insights into performance metrics. To effectively evaluate driving performance, especially in complex scenarios with ambiguous rewards, a dedicated"}]}