{"title": "CSAOT: Cooperative Multi-Agent System for Active Object Tracking", "authors": ["Hy Nguyen", "Bao Pham", "Hung Du", "Srikanth Thudumu", "Rajesh Vasa", "Kon Mouzakis"], "abstract": "Object Tracking is essential for many computer vision applications, such as autonomous navigation, surveillance, and robotics. Unlike Passive Object Tracking (POT), which relies on static camera view-points to detect and track objects across consecutive frames, Active Object Tracking (AOT) requires a controller agent to actively adjust its viewpoint to maintain visual contact with a moving target in complex environments. Existing AOT solutions are predominantly single-agent-based, which struggle in dynamic and complex scenar-ios due to limited information gathering and processing capabilities, often resulting in suboptimal decision-making. Alleviating these limitations necessitates the development of a multi-agent system where different agents perform distinct roles and collaborate to enhance learning and robustness in dynamic and complex envi-ronments. Although some multi-agent approaches exist for AOT, they typically rely on external auxiliary agents, which require ad-ditional devices, making them costly. In contrast, we introduce the Collaborative System for Active Object Tracking (CSAOT), a method that leverages multi-agent deep reinforcement learning (MADRL) and a Mixture of Experts (MoE) framework to enable multiple agents to operate on a single device, thereby improving tracking performance and reducing costs. Our approach enhances robustness against occlusions and rapid motion while optimizing camera movements to extend tracking duration. We validated the effectiveness of CSAOT on various interactive maps with dynamic and stationary obstacles.", "sections": [{"title": "1 INTRODUCTION", "content": "Object tracking is a fundamental task in computer vision, broadly categorized into passive and active tracking [25]. Passive Object Tracking (POT) identifies a target object from an input video se-quence recorded by a static camera. POT operates without influenc-ing or interacting with the environment or the target object; its sole task is to detect the target object across consecutive video frames. In contrast, Active Object Tracking (AOT) involves an interactive agent that actively adjusts its viewpoint to maintain continuous visual contact with the target object as it moves through the en-vironment. This makes AOT inherently more dynamic than POT, requiring real-time decision-making capabilities to adapt to rapid and often unpredictable changes in the target's movement, envi-ronmental conditions, and potential obstacles.\nDeep Reinforcement Learning (DRL) is a subset of machine learn-ing that combines reinforcement learning with deep learning tech-niques to enable agents to make decisions in complex environments. DRL's strengths lie in its ability to learn optimal policies through trial and error, leveraging large amounts of data and powerful neu-ral networks to approximate value functions [17]. This adaptability allows DRL to handle high-dimensional state spaces and learn from delayed rewards, making it particularly effective for tasks requir-ing continuous interaction and decision-making. In the context of Active Object Tracking (AOT), DRL is the most suitable method because it enables agents to dynamically adjust their tracking strate-gies in real-time, responding to the unpredictable movements of target objects while optimizing tracking accuracy and efficiency.\nExisting DRL solutions to AOT primarily rely on a single-agent approach, where one agent performs all actions for the tracking task (e.g., detection, navigation, obstacle avoidance) [9]. While these solutions can be effective in static environments, they often limit the agent's ability to adapt in dynamic environments [19]. This can be due to challenges, such as occlusions, varying speeds, and un-certain changes in the target object's trajectory. To alleviate these challenges, distributing actions into the multi-agent system where each agent performs a single action can be a potential solution [9]. In addition, Multi-Agent Deep Reinforcement Learning (MADRL) can be employed to facilitate agent interaction for AOT in dynamic environments [19]. In MADRL, agents communicate, coordinate and learn from their experiences, improving their decision-making processes and adaptability to the unpredictable behavior of target objects [16, 24]. Note that, there exists both individual goals and collective goals in MADRL. This leads to the challenge of designing a reward function that support the optimal performance of the entire system [28]. Current MADRL methods for AOT mainly focus on designing auxiliary agents that operate externally to the main agent. In this setup, the master agent is responsible for the tracking task, while the auxiliary agents gather and transmit supplementary information to enhance the master agent's decision-making capa-bilities [18]. Although this approach can improve tracking accuracy, it necessitates the use of additional devices and resources, leading to increased costs and complexity in system deployment.\nTo address these challenges, we propose a role-based approach that enables multiple agents to collaborate within a single device. This innovative design not only reduces costs associated with ad-ditional hardware but also minimizes communication overhead. Additionally, we propose a novel mechanism called Mixture of Pol-icy (MoP) to learn a policy for each agent. The MoP employs a gating mechanism to coordinate multiple smaller policy networks, with each policy acting as an expert tailored to handle specific scenarios encountered during the tracking task. This approach significantly reduces inference time, which is crucial given the multiple agents involved in the system, enabling faster decision-making without sacrificing performance. Additionally, the use of expert policies enhances accuracy, as each agent can select the most relevant pol-icy based on the current context. By combining the advantages of the MoP mechanism with a role-based collaboration framework, our method aims to optimize both efficiency and effectiveness in real-time AOT applications. In summary, our contribution to this work includes:\n\u2022 Proposed a novel framework CSAOT for cooperative multi-agent deep reinforcement learning for AOT task on a single device.\n\u2022 Introduced MoP, a novel mechanism to learn policy effi-ciently.\n\u2022 Adapt the framework to AOT task by applying subtask-based reward functions.\n\u2022 Evaluate the proposed framework on a simulated environ-ment, empirically proving the system's performance."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Object tracking task and existing solutions", "content": "With AOT, its application in real-time environment is broader, hence different approaches have been developed to solve this task. Several studies have leveraged reinforcement learning and multi-camera collaboration to improve active object-tracking performance. Luo et al. [14] introduced an end-to-end tracking system that integrates object tracking and camera control into a single reinforcement learning framework. The system demonstrates adaptability in real-world scenarios by training in virtual environments and deploying on physical robots. Similarly, Li et al. [12] tackled object-tracking challenges using pose-assisted multi-camera collaboration. Their approach allows cameras to share positional data, enabling more accurate tracking in environments with multiple obstructions and complex dynamics. These methods highlight the power of collabora-tive and learning-based strategies to optimize tracking performance across diverse environments.\nAnother previous sub-field in object tracking focuses on active feature selection and motion detection to enhance the robustness and efficiency of object tracking. Zhang et al. [31] applied active learning techniques to feature selection, ensuring that the most relevant features are used for monitoring, which improves perfor-mance in real-time applications. Denzler and Paulus [2] proposed a two-stage active vision system that separates motion detection from tracking, enhancing performance in dynamic and unpredictable set-tings. Different approaches focus on active contour models and par-ticle filters to enhance object tracking. Silva et al. [23] and Lef\u00e8vre and Vincent [10] used active contour models to improve object segmentation and tracking in real-time by adapting to the object's shape and contours.\nResearchers have also utilized deep learning and anti-distractor mechanisms to address tracking in complex environments. Xi et al. [29] proposed a novel anti-distractor approach specifically de-signed for 3D environments with multiple distractors, using deep learning to ensure that the target object remains the focus even in the presence of other moving objects. Lei et al. [11] extended active object tracking to space manipulators, employing deep rein-forcement learning to track objects in space, tackling the challenges posed by low-gravity environments. These solutions showcase the flexibility of deep learning in overcoming challenges related to complex environments and distractors."}, {"title": "2.2 Multi-agent Deep Reinforcement Learning", "content": "Multi-agent deep reinforcement learning (MADRL) combines multi-agent systems and deep reinforcement learning (DRL) to address complex challenges [4]. It extends traditional reinforcement learn-ing to environments with multiple interacting agents, each learning to optimize its rewards while collaborating or competing with oth-ers. MADRL emerged from the need for multiple agents to work together in real-world scenarios with shared or conflicting objec-tives. Value-based methods were introduced commonly, each with a different approach to diversifying the range of tasks MADRL can solve. Real-world applications include autonomous vehicles avoid-ing collisions and robotic teams performing joint tasks. Centralized training with decentralized execution (CTDE) is a popular MADRL paradigm [33]. Agents are trained centrally with global information and later deployed with partial information. Value decomposition methods aim to decompose the global value function into individ-ual value functions, facilitating credit assignment and enhancing scalability. Notable contributions include QMIX [20] and Value De-composition Networks (VDN) [27], which have been successful in cooperative games with shared reward signals.\nPolicy gradient methods in MADRL focus on optimizing action-selection policies. Approaches like Multi-Agent Proximal Policy Optimization (MAPPO) [30] and Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [13] handle continuous action spaces, which are crucial for complex environments. These are all Proximal Policy Optimization (PPO) and Trust Region Policy Optimization"}, {"title": "2.3 Mixture of Experts", "content": "The Mixture of Experts (MoE) model, a significant advancement in deep learning, has revolutionized the field by significantly im-proving scalability and efficiency in large-scale tasks. Shazeer et al. [22] introduced the Sparsely-Gated Mixture of Experts layer. This key innovation addresses the issue of high computational costs by activating only a subset of experts for each input. This not only reduces the computational burden but also maintains or improves performance. Particularly in tasks such as machine translation and language modeling, this approach enhances the efficiency of train-ing large-scale models, making them feasible to deploy in practice. The MoE model has become a fundamental component in mod-ern large-scale neural networks, where computational resource management is critical.\nThe Mixture of Experts framework has proven particularly valu-able in multi-task learning, enabling the modeling of relationships between tasks while preserving task-specific performance. Ma et al. [15] developed a Multi-Gate Mixture of Experts model, which employs multiple gating networks to assign different experts to dif-ferent tasks dynamically. This approach improves shared learning and task-specific specialization, essential in multi-task scenarios where other tasks may have distinct requirements. The model's ability to selectively share knowledge between functions while maintaining the independence of tasks that require unique repre-sentations underscores the versatility of the MoE framework in handling diverse and competing learning objectives in a single unified model."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 Problem formulation", "content": "Multi-agent cooperative tasks with decentralized execution can be represented under the original Dec-POMDP, which consists of a tuple\nG = (S, A, \u03a1, \u039f, \u03a9, R, C, \u03b7, \u03b3),\nHere, C is a set of n agents, and s \u2208 S is the state gathered from the environment. Every agent i retrieves its observation o\u00a1 \u2208 \u03a9 drawn from the observation function O(s, i), which helps decide the next action a\u00a1 \u2208 A, creating a joint action a \u2208 An, which leads to the next state s' based on a transition function P(s'|s, a), and observing a reward r = R(s, a) which depends on the discount factor y \u2208 [0, 1]. Each agent has local action-observation history \u03c4\u00a1 \u2208 T = (\u03a9 \u00d7 A)*. To simplify the problem, we decided to set the discount factor y = 1"}, {"title": "3.2 Task formulation", "content": "Along with the general Dec-POMDP problem formulation, some unique properties come from the nature of the task:\n\u2022 Unified observation: Unlike other cooperative or compet-itive MADRL systems, all agents \u03c0i in our system operate on the same device, receiving their observations o\u00a1 from a common source, which includes the current frame ft, accel-eration at, speed st, and steering angle dt.\n\u2022 System action format: The action space of the system's final navigation agent includes a tuple (x, y), where x is the acceleration rate (x \u2208 [-0.5, 0.5], negative values mean go-ing in the reverse way) measured in m/s\u00b2 and y is the turning angle (y \u2208 [-0.5, 0.5], negative values denote left turns and positive values denote right turns) measured in radians. The action spaces of other auxiliary agents are different based on their assigned tasks, which is explained in Section 4.\n\u2022 Fully decentralized system: All agents in the system work in their own unique action space. Under this assumption, collaborative multi-agent reinforcement learning algorithms such as MAPPO don't have the exact nature of our defined task. Hence, we adapted the Decentralized Training - Decen-tralized Execution (DTDE) approach for our solution.\n\u2022 Continuous action space: For this task, all features col-lected from observation are scalable actions, such as chang-ing the bounding box, acceleration, and camera angle. As the number of available action families increases, the action space rises remarkably, significantly increasing the task's complexity."}, {"title": "3.3 Mixture of Experts", "content": "MoE (Mixture of Experts) has different variants, differentiated by the gating mechanism. We consider Top-K MoE in this framework due to its balance between computational cost and accuracy. Let x \u2208 Rd be the input, and let fi (x; 0\u2081) represent the expert networks. The top K experts are selected based on their gating probabilities:\nSK = TopK (g(x; \u00a2)) = {i1, 2, . . ., ik }.\nThe remaining elements in SK are reweighted for the corresponding expert's impact. The output of MoE is the weighted sum of the selected experts:\ny(x) = \u03a3\nUltimately, this hybrid mechanism allows the system to adapt to more scenarios, improving its robustness."}, {"title": "3.4 Proximal Policy Optimization", "content": "Proximal Policy Optimization (PPO) [21] is a reinforcement learning algorithm designed to improve traditional policy gradient methods by ensuring more stable and reliable training. We prefer this method over other reinforcement algorithms because of its decent sample efficiency. PPO ensures that considerable updates to the policy are controlled; hence, the policy is stable during the training process. This is achieved through a clipping mechanism that limits the changes in the policy probability ratio. The key idea is to maximize the clipped objective:\nLCLIP (0) = Et [min (rt (0) At, clip (rt (0), 1 \u2013 6,1 + 6) Ar)]\nwhere rt (0) is the probability ratio between the new policy and the old policy:\nand At is the advantage function at timestep t, which helps guide the optimization towards better actions. The clipping parameter e controls how far the new policy deviates from the old policy. Another advantage of using PPO is that it can adapt to continuous action space, which makes it fit our specific task."}, {"title": "4 CSAOT FRAMEWORK", "content": ""}, {"title": "4.1 Overall of the framework", "content": ""}, {"title": "4.1.1 Framework architecture", "content": "Figure 1 shows the overall architec-ture of our proposed method. In detail, we designed the system as a two-layer hierarchical system, with the first layer extracting the core information based on the subtasks from the ultimate objective of tracking the moving target (detection, obstacle, movement) and the second layer used for the final decision-making phase. The raw current frame is processed by a ResNet50 [8] pre-trained model for feature extraction. The subtasks we mentioned earlier include:\n\u2022 Object detection: This agent aims to predict the object's bounding box in the frame. Specifically, its action space com-prises 4 values ad = (x1, y1, xr, Yr), denoting the top-left and bottom-right corner of the predicted bounding box. This gives the decision-making agent the target's position on the frame, which is very helpful in choosing the next action.\n\u2022 Object movement: This agent predicts the center of the target within the frame, which the format is an = (xc, yc). Al-though this can be inferred from the bounding box given by the previous agent, we hypothesize that the center provided by the bounding box relies heavily on the slight change of any corner. This architecture helps ensure the robustness of the information coming out from this first layer for reason-able action consideration.\n\u2022 Obstacle avoidance: Unlike the previous agents, this agent considers the surroundings as its central focus. The action space of this agent is aa = (x), where x shows the distance of the nearest obstacle existing in the frame. This detection is crucial to the tracker's movement since it's prohibited from collision with any obstacle."}, {"title": "4.1.2 Reward construction", "content": "The reward construction contains mul-tiple component rewards for each agent. The final layer agent would take the global reward as its main reward. The target's actual bound-ing box in the frame m = {(xi, yi)|i \u2208 [1, n]} needs to be calculated to calculate the rewards. For states where the tracker collides with any other object in the environment, its reward for that state is -50, and the corresponding episode ends immediately. In every reward component, we designed a proper scaling \u03bb to appropriately ad-just the task's priority. The reward can be decomposed into these smaller parts:\n\u2022 Tracking reward: Rtrack takes the area of the target's bound-ing box compared to the frame's area. The motivation is to ensure the tracker keeps an appropriate distance from the object. Specifically, this can be written as:\nRtrack = min(\nwhere S denotes the area of a given polygon.\n\u2022 Navigation reward: Rnav is calculated based on the center of m using the following function:\nAfter getting the center of the corresponding bounding box, the Manhattan distance is used to calculate the rewardRnav"}, {"title": "4.2 Novel adaptations", "content": ""}, {"title": "4.2.1 Task-based component rewards", "content": "Besides the global reward for the last layer agent, we designed unique reward functions for each agent in the first layer. This helps the agents' learning speed stay stable within the training process:\n\u2022 Detection Agent: For the bounding box prediction, one direct way to assess the correctness of the output is by using the Intersection over Union (IoU) metric. Specifically, the reward is computed as:\nRdetect = IoU (m, ad) \u00b7 Adetect\n\u2022 Obstacle Agent: The obstacle distance to the tracker can be assessed by the absolute distance between the actual distance (Calculated through LIDAR sensors information) and the predicted distance\nRobstacle = |aa \u2013 dtrue|\u00b7 \u03bbobstacle\n\u2022 Movement Agent: With tracking the center of the target, one standard reward that can be utilized is the Manhattan distance between the ground truth and the prediction\nRmovement = Manhattan(an, center (m)) movement\nAs the system learns through a single source of observation, learn-ing only through the global reward could cause a bottleneck in the learning speed between different agents in the system. Furthermore, the global reward doesn't seem relevant to the first layer's agents; hence, learning those could be less efficient than learning those in the second layer. For these reasons, we designed rewards that directly rely on their actions."}, {"title": "4.2.2 Mixture of Policies", "content": "In such a complex task like ASOT with continuous action space, it's harder for the system to explore its action space to a sufficient level, which leads to poor generalization to different situations that the environment may provide. Motivated by that critical weakness, an alternative mechanism to help improve this is needed. The Mixture of Experts technique is widely known for its scalability, low inference cost, and adversarial robustness. With all those properties that MoE has, we chose to integrate MoE into the policy network based on the hypothesis that this would help the agent learn a diverse range of policies based on current state observations, forming our proposed method Mixture of Policies (MoP) method. Formally, MoP can be written as:\n\u039c\u03bf\u03a1(\u03bf, \u039a) =\nHere, K is the number of chosen experts to retrieve the final action, and wi is the weight for the corresponding sub-policy network Pi. P = P1, P2, ..., pn are the individual policy networks, which are dynamically chosen in specific cases to participate in the policy combination. The way w is calculated in this architecture is the same as the previously shown MoE method."}, {"title": "5 EXPERIMENTS", "content": "For all scenarios in our experiment, the hyperparameter values are shown in table 1."}, {"title": "5.1 Simulated environment", "content": "AirSim is an open-source simulation platform developed by Mi-crosoft to facilitate research in autonomous vehicles, including drones and cars. It provides a highly realistic, physics-based envi-ronment for training and testing reinforcement learning models, explicitly targeting autonomous navigation, collision avoidance, and object tracking tasks. AirSim is built on the Unreal Engine, al-lowing for rich visual realism, which is crucial for computer vision applications. It can simulate diverse environmental conditions, such as different weather types and lighting scenarios, to evaluate the robustness of different DRL models. The platform also integrates well with standard machine learning frameworks, enabling training capability for reinforcement learning agents through API."}, {"title": "5.2 Testing scenarios", "content": "The testing environment consists of four maps designed to evaluate specific aspects of driving performance. These maps offer increasing difficulty and complexity levels, ranging from simple maneuvers to advanced scenarios involving dynamic and static obstacles.\nThe descriptions of each map are as follows:\n\u2022 SingleTurn: A simple map containing only a 90-degree turn provides a straightforward scenario to evaluate basic turning maneuvers.\n\u2022 SimpleLoop: A shape loop designed to test the ability of agents to navigate continuous smooth turns and handle in-tersections.\n\u2022 SharpLoop: A loop around a single obstacle that includes very sharp turns, designed to test the system's ability to manage tight cornering.\n\u2022 Complex: A challenging map that includes turns of all diffi-culty levels, with dynamic and static obstacles, simulating real-world scenarios with numerous unpredictable elements.\nWe use two metrics to assess the framework's performance: Episode Length (EL) and Cumulative Rewards (CR). EL represents the length of one tracking episode during inference, while CR de-notes the sum of global rewards throughout the episode. These metrics have been previously used to comprehensively assess AOT frameworks [14].\nTo test the system's performance on an unseen map, we set up the experiment: Train the tracker on a single map for 50 episodes, with given boundaries on EL and CR. Then, we put it to the test on all maps to observe if it can perform well in seen and unseen environments. The final result is the average of the different trials above. With CR, unless the tracker can track the target for the span"}, {"title": "5.3 Results", "content": "Table 3 shows the average performance of the tracker in differ-ent maps. For trivial maps such as SimpleLoop and SingleTurn, the CSAOT tracker can achieve high EL, whereas the system's perfor-mance isn't decent with those complex maps. We also recognize during the inference run of the CSAOT tracker that it can hardly get back on track if it loses track of the target before. This happened on most maps, excluding the SingleTurn map. One thing to be recog-nized in the experiment is that collision rarely appears throughout the different scenarios, which it only appears in Complex map.\nTable 4 shows the differences in performance between the naive SingleAgent and our novel method CSAOT. With the simplicity of the SingleTurn and SimpleLoop map, these two methods' per-formance don't significantly differ. However, the difference is re-markable on Complex map, with an increase of approximately 30% in average EL. This improvement shows that the method did well with the high complexity when the target path is complicated, high-lighting the importance of adapting a multi-agent approach to this specific task.\nTo see the system's actual performance in inference time, we investigated the system's decisions at runtime. For the trivial maps, it tracks the target closely while fulfilling the desired behavior that we have defined through the reward function. On the other hand, in Complex map, it can only track within the first 3 turns, then loses track of the target due to inappropriate turning.. As can be seen, the actions from those first steps are accurate, showing that the system is learning in the proper direction. We provide a sample episode footage as supplementary material."}, {"title": "6 DISCUSSION", "content": ""}, {"title": "6.1 System architecture", "content": "Although the system's architecture has improved performance compared to its single-agent counterpart, we believe there are dif-ferent options for alternating and experimenting with the mod-ules. Recently, State Space Model and its variants such as S4 [7] or Mamba [6], which have great potential to perform better than our usage of LSTM for memorial subtask. Hence, we consider this part of our future work on this framework."}, {"title": "6.2 Learning strategy", "content": "PPO is shown to be sample-efficient in previous work [21], and it's been shown clearly through our experiment. However, the rapid convergence of this method increases the probability of falling into local minima during training. This can be observed through the learning process of the CSAOT tracker in Complex map,. This seems to be a common phenomenon for actor-critic-based methods. To avoid this, methods to trigger further exploration or adding an early stopping mechanism could help. In our implementation, we make use of e-greedy to tackle the problem, but it doesn't seem to work well. More advanced methods, such as the Adversarially Guided Actor-Critic (AGAC) [3] mechanism, could help improve the overall learning performance of the system."}, {"title": "6.3 Gating mechanism for MoP", "content": "Our proposed policy network idea combines policies to increase the policy's robustness and adaptability in different scenarios. However, we haven't considered the effect of the gating mechanism within the MoP architecture. We hypothesize that this can be an essential factor affecting the framework's performance in such a complex system setting. Therefore, further investigation to choose the proper mechanism can improve the learning process's accuracy."}, {"title": "6.4 Intrinsic reward for individual agent", "content": "We introduced multiple task-based component rewards to improve the system's convergence speed, and compared to the SingleAgent implementation, the rewards help enhance the performance in a high complexity environment. One way to increase the effect of those rewards is to add some intrinsic reward in addition to the extrinsic reward. Intrinsic reward has been proven helpful in cooperative MADRL systems [32], and we will investigate further into this field."}, {"title": "7 CONCLUSION", "content": "In this work, we proposed CSAOT, a novel active tracker for SOT via MADRL. Unlike previous work on ASOT, the proposed tracker comprises a multi-agent system that sits on a single vehicle, effi-ciently utilizing the information extracted from the environment. Furthermore, we proposed MoP as an alternative to the traditional policy network of an MLP block. Combining those factors, the sys-tem is proven to perform well in a multi-dimensional continuous action space and generalizes to unseen environments. We believe that this framework can be applied to real-world scenarios."}]}