{"title": "Apple Intelligence Foundation Language Models", "authors": ["Apple"], "abstract": "We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible Al and how the principles are applied throughout the model development.", "sections": [{"title": "Introduction", "content": "At the 2024 Worldwide Developers Conference, we introduced Apple Intelligence, a personal intelligence system integrated deeply into iOS 18, iPadOS 18, and macOS Sequoia.\nApple Intelligence consists of multiple highly-capable generative models that are fast, efficient, specialized for our users' everyday tasks, and can adapt on the fly for their current activity. The foundation models built into Apple Intelligence have been fine-tuned for user experiences such as writing and refining text, prioritizing and summarizing notifications, creating playful images for conversations with family and friends, and taking in-app actions to simplify interactions across apps."}, {"title": "Architecture", "content": "The AFM base models are dense decoder-only models that build on the Transformer architecture [Vaswani et al., 2017], with the following design choices:\n\u2022 A shared input/output embedding matrix [Press and Wolf, 2016] to reduce memory usage for parameters.\n\u2022 Pre-Normalization [Nguyen and Salazar, 2019] with RMSNorm [Zhang and Sennrich, 2019] for training stability.\n\u2022 Query/key normalization [Wortsman et al., 2023] to improve training stability.\n\u2022 Grouped-query attention (GQA) [Ainslie et al., 2023] with 8 key-value heads to reduce the KV-cache memory footprint.\n\u2022 The SwiGLU activation [Shazeer, 2020] for higher efficiency.\n\u2022 ROPE [Su et al., 2024] positional embeddings with the base frequency set to 500k for long-context support."}, {"title": "Pre-training", "content": "Our AFM pre-training process plays a critical role in developing highly capable language models to power a host of Apple Intelligence features that can help and support users. We focus on efficiency and data quality at every step in order to pre-train for a high-quality end-to-end user experience with efficient and low-latency models."}, {"title": "Data", "content": "The AFM pre-training dataset consists of a diverse and high quality data mixture. This includes data we have licensed from publishers, curated publicly- available or open-sourced datasets, and publicly available information crawled by our web-crawler, Applebot [Apple, 2024a]. We respect the right of webpages to opt out of being crawled by Applebot, using standard robots.txt directives.\nGiven our focus on protecting user privacy, we note that no private Apple user data is included in the data mixture. Additionally, extensive efforts have been made to exclude profanity, unsafe material, and personally identifiable information from publicly available data (see Section 7 for more details). Rigorous decontamination is also performed against many common evaluation benchmarks."}, {"title": "Web pages", "content": "We crawl publicly available information using our web crawler, Applebot [Apple, 2024a], and respect the rights of web publishers to opt out of Applebot using standard robots.txt directives. Plus, we take steps to exclude pages containing profanity and apply filters to remove certain categories of personally identifiable information (PII). The remaining documents are then processed by a pipeline which performs quality filtering and plain-text extraction, more specifically:\n1. Body extraction is performed using a combination of Safari's reader mode and the Boilerpipe [Kohlsch\u00fctter et al., 2010] algorithm.\n2. Safety and profanity filtering, using heuristics and model-based classifiers.\n3. Global fuzzy de-duplication using locality-sensitive n-gram hashing.\n4. Extensive quality filtering using heuristics and model-based classifiers [Kong et al., 2024; Li et al., 2024a].\n5. Decontamination against 811 common pre-training benchmarks, filtering entire documents upon 4-13 gram collisions with any of the benchmark datasets, unless the collision-count for a given n-gram reaches a \u201ccommon- usage\" threshold of 1000."}, {"title": "Licensed datasets", "content": "We go to lengths to identify and license a limited amount of high-quality data from publishers. These licensed datasets provide a natural source of diverse and high quality long-context data, so we include them as part of the data mixture for the continued and context-lengthening stages of pre- training (see Section 3.2.2 and 3.2.3 for more details). We decontaminate sections of publisher licensed data the same way we decontaminate web pages (Section 3.1.1)."}, {"title": "Code", "content": "Code data is obtained from license-filtered\u00b9 open source repositories on GitHub. The bulk of the code data covers 14 common programming languages, including: Swift, Python, C, Objective-C, C++, JavaScript, Java, and Go. The data is de-duplicated, further filtered for PII and quality, and decontaminated in the same fashion as in Section 3.1.1."}, {"title": "Math", "content": "We integrate two categories of high-quality data sourced from the web. The first category is a Math Q&A dataset, comprising 3 billion tokens from 20 web domains rich in math content. We extract the questions and answers by identifying relevant tags from HTML pages. The second category is a collection of 14 billion tokens from web pages such as math forums, blogs, tutorials, and seminars. To filter these web pages, we used a specialized pipeline that includes a math tag filter with a collection of 40 strings to identify mathematical templates, a math symbol filter with a collection of 350 Unicode and LaTeX symbols to identify math content, a quality filter powered by a language model classifier specifically designed for math [Kong et al., 2024], and a domain filter that processes all web pages from domains manually labeled by humans. We applied these filters, followed by deduplication, decontamination, and PII removal to produce the final dataset."}, {"title": "Public datasets", "content": "We evaluated and selected a number of high-quality publicly-available datasets with licenses that permit use for training language models. Then, we filtered the datasets to remove personally identifiable information before including them in the pre-training mixture."}, {"title": "Tokenizer", "content": "We use a byte-pair encoding (BPE) tokenizer, following the implementation from SentencePiece. All numbers are split into individual digits and we use byte-fallback to decompose unknown UTF-8 characters into byte tokens. We do not enable Unicode normalization. The total vocabulary size is 100k and 49k tokens for AFM-server and AFM-on-device, respectively."}, {"title": "Recipe", "content": "We break AFM pre-training into three distinct stages: 1. core which consumes most of the compute budget, 2. continued, where we down-weight the lower- quality bulk web-crawl data, favoring a higher code and math weight instead combined with inclusion of the licensed data described in Section 3.1.2, 3. context-lengthening which is similar to another continued pre-training stage, but conducted at longer sequence length and with synthetic long-context data included in the mixture.\nDetails about model quality after each of the three pre-training stages (alongside additional metrics for AFM derived from our internal benchmark implementations) are in Appendix C, and Appendix D examines AFM-server's long-context capabilities.\nAll three stages use decoupled weight decay [Loshchilov and Hutter, 2019] for regularization, as well as a simplified version of \u00b5Param [Yang et al., 2022], similar to what is described as \u00b5Param (simple) in [Wortsman et al., 2023]. Thus far we have not found more sophisticated parameter norm controls to be necessary at these scales. All stages maintain sharded model and optimizer states in float32, casting to bfloat16 for the forward and backward passes for efficiency."}, {"title": "Core pre-training", "content": "AFM-server core training is conducted from scratch, while AFM-on-device is distilled and pruned from a larger model.\nAFM-server: We train AFM-server from scratch for 6.3T tokens on 8192 TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences. The batch size was determined using a scaling law fit to model size and compute budget, however we find that downstream results are relatively insensitive to a fairly wide range of batch sizes, and expect that any value between 0.5\u00d7 and 2x the predicted batch size would have yielded similar results (the predicted optimum was in fact ~3072, but 4096 allowed for better chip utilization). We perform a learning rate sweep using a proxy model with a model dimension of 768, finding that the optimum learning rate spans 0.01-0.02, so we choose 0.01 to be conservative. Linear layers will have an effective learning rate scaled by ~0.1 due to the use of \u00b5Param (simple).\nWe use a tuned decoupled weight decay of 3.16e-4, finding that it works well across all tested model sizes and compute budgets. The learning rate schedule includes a linear warm-up for 5000 steps, followed by cosine decay to 0.005 of the peak over the remainder of training. For further details on the optimizer, see Section 3.2.4. Appendix A compares the AFM core pre-training recipe to a more typical configuration.\nAFM-on-device: For the on-device model, we found that knowledge distilla- tion [Hinton et al., 2015] and structural pruning are effective ways to improve model performance and training efficiency. These two methods are comple- mentary to each other and work in different ways. More specifically, before training AFM-on-device, we initialize it from a pruned 6.4B model (trained from scratch using the same recipe as AFM-server), using pruning masks that are learned through a method similar to what is described in [Wang et al., 2020; Xia et al., 2023]. The key differences are: (1) we only prune the hidden dimension in the feed-forward layers; (2) we use Soft-Top-K masking [Lei et al., 2023] instead of HardConcrete masking [Louizos et al., 2018]; (3) we employ the same pre-training data mixture as the core phase to learn the mask, training for 188B tokens. Then, during the core pre-training of AFM-on-device, a dis- tillation loss is used by replacing the target labels with a convex combination of the true labels and the teacher model's top-1 predictions, (with 0.9 weight assigned to the teacher's labels), training for a full 6.3T tokens. We observe that initializing from a pruned model improves both data efficiency and the"}, {"title": "Continued pre-training", "content": "For both models we perform continued pre-training at a sequence length of 8192, with another 1T tokens from a mixture that upweights math and code, and down-weights the bulk web-crawl. We also include the licensed data described in Section 3.1.2. We use a peak learning rate of 3e-4 and decoupled weight decay of le-5, and 1000 warm-up steps with a final learning rate decay to 0.001 of peak, differently to core pre-training. Other settings (batch size, etc) are carried over. We did not find a distillation loss to be helpful here for AFM-on-device, unlike in core pre-training, so the recipe is identical to that used for AFM-server."}, {"title": "Context lengthening", "content": "Finally, we conduct a further 100B tokens of continued pre-training at a sequence length of 32768 tokens, using the data mixture from the continued pre-training stage, augmented with synthetic long-context Q&A data. We also increase the RoPE base frequency from 500k to 6315089, following the scaling laws described in [Liu et al., 2024], with the expectation that this will allow for better short-to-long generalization\u2014which is desirable given that the majority of our pre-training data is comprised of documents that are significantly shorter than 32k tokens long. The recipe is similar to that used for continued pre-training. We examine the long-context performance of AFM-server in Appendix D."}, {"title": "Optimizer", "content": "We choose to use a variant of RMSProp [Hinton, 2012] with momentum for AFM pre-training. In particular, we divide the raw gradient by the square-root of a bias-corrected exponential moving average of the squared gradient to produce an instantaneous update, which is clipped to a maximum norm of 1.0 per parameter block, before then further smoothing this estimate over steps with an exponential moving average without bias-correction to produce the net update. Unless otherwise noted, the smoothing constants for both the squared gradient (\u03b22) and the update (\u03b21) are set to 0.95. A small constant \u20ac = 1e-30 is added to the instantaneous squared gradient prior to smoothing, for numerical stability.\nThe smoothed updates are scaled by the learning rate, weight-decay is added, and then scheduled decay is applied to form the final weight delta. As an additional guard for stability, prior to the optimizer we clip the global gradient norm to 1.0. For a recipe ablation against a more typical configuration, see Appendix A."}, {"title": "Training infrastructure", "content": "The AFM models are pre-trained on v4 and v5p Cloud TPU clusters with the AXLearn framework [Apple, 2023], a JAX [Bradbury et al., 2018] based deep learning library designed for the public cloud. Training is conducted using a combination of tensor, fully-sharded-data-parallel, and sequence parallelism, allowing training to scale to a large number of model parameters and sequence lengths at high utilization. This system allows us to train the AFM models efficiently and scalably, including AFM-on-device, AFM-server, and larger models.\nAFM-server was trained on 8192 TPUv4 chips provisioned as 8 \u00d7 1024 chip slices, where slices are connected together by the data-center network (DCN) [Chowdhery et al., 2022]. Only data-parallelism crosses the slice boundary, other types of state sharding are within-slice only as the within-slice interconnect bandwidth is orders of magnitude higher than the DCN. The sustained model-flop-utilization (MFU) for this training run was approximately 52%. AFM-on-device was trained on one slice of 2048 TPUv5p chips."}, {"title": "Post-Training", "content": "While Apple Intelligence features are powered through adapters on top of the base model (see Section 5 for a deep-dive on the adapter architecture), empirically we found that improving the general-purpose post-training lifts the performance of all features, as the models have stronger capabilities on instruction following, reasoning, and writing.\nWe conduct extensive research in post-training methods to instill general- purpose instruction following and conversation capabilities to the pre-trained AFM models. Our goal is to ensure these model capabilities are aligned with Apple's core values and principles, including our commitment to protecting user privacy, and our Responsible AI principles. Our post-training efforts include a series of data collection and generation, instruction tuning, and alignment innovations. Our post-training process contains two stages: su- pervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). We present two new post-training algorithms: (1) a rejection sampling fine-tuning algorithm with teacher committee (iTeC), and (2) a reinforcement learning from human feedback (RLHF) algorithm with mirror descent policy optimization and a leave-one-out advantage estimator (MDLOO) that are used on our reinforcement learning iterations and lead to significant model quality improvements."}, {"title": "Data", "content": "We use a hybrid data strategy in our post-training pipeline, which consists of both human annotated and synthetic data. Throughout our data collection and experiment process, we have found data quality to be the key to model success and thus have conducted extensive data curation and filtering procedures."}, {"title": "Human annotations", "content": "Demonstration data To fuel the instruction fine-tuning of AFM, we collect high-quality human annotated demonstration datasets from various sources. This dialogue-style data consists of both system-level and task-level instructions (a.k.a. prompts), as well as their corresponding responses. Similar to [Zhou et al., 2024], we observe quality to weigh more importantly than quantity in our experiments. As a result, we focus on key data quality criteria including helpfulness, harmlessness, presentation, and response accuracy, in addition to targeting a diverse task distribution covering Apple Intelligence features.\nTo protect user privacy, we take steps to verify no personally identifiable information is present in our data, and we do not include any personal data stored by users with Apple.\nHuman preference feedback To iteratively improve AFM's capabilities, we further collect human feedback for reinforcement learning. In particular, we instruct human annotators to compare and rank two model responses for the same prompt to collect side-by-side preference labels. In addition, we also use single-side questions to guide this process. These questions inform raters to grade the model response quality of various aspects including instruction following, safety, factuality, and presentation, and we also retain these labels for model training. We emphasize Apple values and standards in the process. Similar to demonstration data, we find data quality to be crucial for feedback data, and thus we iterate data and model qualities jointly to improve them in a unified flywheel."}, {"title": "Synthetic data", "content": "In addition to human annotations, we delve into enhancing data quality and diversity through synthetic data generation. Our findings suggest that when guided by our robust reward models, AFMs are capable of generating high quality responses and for some specific domains, these responses are found to be on par with, or even superior to, human annotations. Therefore, we extend our prompt set to increase the diversity and find that those generated responses can benefit AFMs themselves. In the following, we discuss three domains where we generate synthetic data for AFM post-training: mathematics, tool use, and coding.\nMathematics In the field of mathematics, the wide-ranging subjects and difficulty level make it exceptionally resource-intensive for collecting human demonstrations, since it requires expert knowledge from the human writers. It also becomes impractical to solely rely on human-written content as the model continuously improves. As a consequence, exploring the potential of synthetic data becomes essential to effectively address the challenges.\nThe creation of synthetic data for mathematics involves two primary stages: generating synthetic math problems and producing their corresponding solutions. For math problem synthesis, we employ several \"evolution\" strategies"}, {"title": "Problem rephrase and reversion.", "content": "Following the approach in [Yu et al., 2023], we prompt AFM to rephrase seed math questions, and curate reverse questions to derive a specific number in a raw problem statement when provided with the final answer."}, {"title": "Problem evolution.", "content": "Inspired by the instruction evolving technique [Xu et al., 2023], given a seed problem set Dseed we prompt AFM to generate two distinct sets of math problems, i.e. F(Dseed)depth, Ddepth, and F(Dseed)breadth Dbreadth. The in-depth evolution enhances instructions by adding complexities while the in-breadth evolution improves the topic coverage. For both Dbreadth and Ddepth, we first perform de-duplication with an embedding model, and subsequently prompt LLMs to ensure the coherence and solvability of the math problems. In addition, for Ddepth a difficulty level is assigned and we only select math problems that score above a specified threshold.\nWith an augmented set of math questions, we then prompt AFM to synthesize N responses with chain-of-thought per question. If the initial seed data has ground truth, they can be used as an \"outcome reward signal\" to filter synthesized answers. For problems that require less reasoning steps, we observe that a correct final answer often gets associated with correct intermediate steps. If direct answer checking is unsuccessful or ground truth is unavailable, we instead assess the response correctness by querying an LLM judge. We find that the filtered answers, when fed into the training data, boost our models' math capabilities by a large margin."}, {"title": "Tool use", "content": "We develop tool-use capabilities such as function call, code inter- preter, and browsing through a mixture of synthetic and human data. The model capabilities are first bootstrapped with synthetic data, which focuses on single-tool use cases. We then collect human annotations to improve model capabilities that involve multi-tool and multi-step scenarios. We further aug- ment the human curated function call data by mixing the oracle tool with other similar tools to increase the difficulty of tool selection. In addition, we synthesize parallel function call from human curated function call data to enable the new capability and tool intent detection data based on human curated function call and general SFT data to mitigate tool call over-triggering issues."}, {"title": "Coding", "content": "The generation of a synthetic coding dataset involves a self-instruct method with rejection sampling. This approach enables the model to learn and generate data autonomously. Starting with 71 different programming topics as the seeds, the model is prompted to generate an initial pool of coding interview-like questions. For each question, the model generates a set of unit tests and a number of potential solutions. We then use an execution-based rejection sampling method to select the best solution. This involves compiling each potential solution with every unit test and executing them. The solution with the highest number of successful executions is chosen. This results in a collection of (question, test cases, solution) triplets. At the end, we validate the quality of the dataset by filtering the triplets using the number of passed unit tests, resulting in 12K high quality triplets used in the SFT."}, {"title": "Supervised fine-tuning (SFT)", "content": "It has been shown [Chung et al., 2024] that scaling multi-task instruction tuning dramatically enhances model performance on a wide variety of tasks. Similarly, we attempt to scale supervised fine-tuning data to achieve a strong base model for subsequent alignment. During SFT, we collect and train models on demonstration data of a given prompt\u00b3. We carefully select and combine both human data and synthetic data to form a high quality mixture that covers various natural language use cases.\nData selection We establish a series of quality guards of the data before onboarding them for model training, including ratings from in-house human labelers, automatic model-based filtering techniques, and deduplication with text embeddings. We also scale up the mixture size by a variety of synthetic data generation methods, as described in Section 4.1.2, and rejection sampling as described in Section 4.3.2.\nTuning the mixture ratio In order to tune the mixture weight, we treat it as an optimization problem. Specifically, given a set of weights (w1, w2, ..., wn) where wi represents the ratio of a specific component in the mixture, we train a model with wi \u2192 wi \u00b1 \u2206Awi and evaluate the quality change on a set of benchmarks. We find that extensively running such experiments can effectively identify the best mixture and remove the least impactful data components.\nTraining hyperparameters The model is trained with a constant learning rate 5e-6 for AFM-server and 2e-5 for AFM-device models, as well as a drop out rate 0.1. Since the evaluation metrics fluctuate across different checkpoints, we run checkpoint selection based on automatic evaluation benchmarks and best-of-N selection with reward models to test the headroom for RL."}, {"title": "Reinforcement learning from human feedback (RLHF)", "content": "We further use reinforcement learning with collected human preference data to improve model performance and quality. This involves training a robust reward model and applying it in two algorithms of iTeC and MDLOO that we discuss below. We describe more details of our RLHF pipeline in Appendix E."}, {"title": "Reward modeling", "content": "We train reward models using the human preference data collected with the method in Section 4.1.1. Each human preference data item contains one prompt and two responses along with human labels including:\n\u2022 The preferred response between the two and the preference level, i.e., whether the preferred response is significantly better, better, slightly better, or negligibly better than the rejected response.\n\u2022 The single-sided grading of each response, measuring the instruction following property, the conciseness, truthfulness, and harmlessness of each of the responses.\nOur reward model training follows the standard practice of reward modeling in RLHF with two main innovations:\n\u2022 We design a soft label loss function that takes the level of human prefer- ence into account.\n\u2022 We incorporate single-sided gradings as regularization terms in reward modeling.\nWe employ the commonly used Bradley-Terry-Luce (BTL) model [Bradley and Terry, 1952] for reward modeling in RLHF. In this model, the probability that a human annotator prefers one response over another is modeled as the sigmoid function of the difference of the rewards. Our soft label loss function encourages that this probability is high when the preference level is high, e.g., when one response is significantly better than the other, and vice versa. We note that this is different from the margin-based loss function in Llama 2 [Touvron et al., 2023], which also leverages the preference level. Empirically, we find that our method works better than the margin-based loss function. Moreover, we also find that using the single-sided gradings as regularization terms can effectively improve the accuracy of the reward model. More details of our reward modeling techniques can be found in Section E.1."}, {"title": "Iterative teaching committee (iTeC)", "content": "To fully unlock the ability of our model with multiple rounds of RLHF, we propose a novel iterative RLHF framework which effectively combines various preference optimization algorithms, including rejection sampling (RS), Direct Preference Optimization (DPO) [Rafailov et al., 2024] and its variants such as IPO [Azar et al., 2024], and online reinforcement learning (RL). This enables us to bring the benefit of RLHF to AFM models across all sizes and improve their alignment at the same time.\nIterative committee One of the most important lessons we learned from developing AFM RLHF is to refresh online human preference data collection using a diverse set of the best performing models. Specifically, for each batch of human preference data collection, we set up a collection of latest promising models trained from SFT, RS, DPO/IPO, and RL, as well as best models from the previous iterations, which we refer to as \u201cmodel committee\". We collect pairwise human preference on responses sampled from the latest model committee.\nAfter acquiring each batch of human preference data, we refresh our reward model, and further train a new set of models using the collection of preference optimization algorithms. We then continue the next round of iterative RLHF data collection with a new model committee.\nCommittee distillation We further run rejection sampling (distillation) from the model committee with the latest reward model as a reranker. Instead of reranking at global-level, i.e., picking a single best performing model from the committee and using it as a teacher model, we rerank model responses at the prompt-level. Specifically, for each prompt, we sample multiple responses from each model in the committee, and use the latest reward model to select the best response for each prompt. This allows us to combine the advantages of models trained by different preference optimization algorithms. For instance, we find that algorithms that leverage negative examples, e.g., online RLHF, DPO, IPO, to be better in improving reasoning skills such as math, while rejection sampling fine-tuning learns instruction following and writing skills more effectively.\nScaling up distillation In order to bring the RLHF improvements to AFM models across all sizes, we scale up distillation from the model committee. Different from larger models, where carefully iterating data and model quality matters much more than data quantity, we find smaller models can achieve tremendous improvement when we scale up the number of prompts for distilla- tion. Our final AFM-on-device model is trained on more than 1M high quality responses generated from the model committee."}, {"title": "Online RLHF algorithm: MDLOO", "content": "In this section, we introduce our online reinforcement learning algorithm MDLOO, where we decode responses during model training and apply RL algorithms to maximize the reward.\nWe use the commonly adopted RLHF objective that maximizes the KL- penalized reward function [Ouyang et al., 2022]:\nmax \u03b8 Ex\u223cD,y\u223c\u03c0\u03b8(\u22c5|x) [r\u03c6(x, y) \u2013 \u03b2DKL(\u03c0\u03b8(\u22c5|x)||\u03c0ref(\u22c5|x))],\nwhere D is the prompt distribution, DKL(\u22c5||\u22c5) denotes the Kullback-Leibler divergence between two distributions, and \u03b2 is the coefficient that controls the divergence between the behavior policy \u03c0\u03b8 and a reference policy \u03c0ref, which is typically a model trained by SFT. In our RL training, we use the reward function\nR(x, y) = r\u03c6(x, y) \u2013 \u03b2log \u03c0\u03b8(y|x)\u03c0ref(y|x),\nwhose expectation is equivalent to Equation 1. We consider the bandit setting where the generation of the entire response is considered as one action, and we do not use the value network (a.k.a. the critic) to obtain the per-token reward or advantage.\nSimilar to commonly used RLHF algorithms such as PPO [Schulman et al., 2017], we use a trust-region based policy iteration algorithm. We made two main design choices in our online RL algorithm:\n\u2022 We use the Leave-One-Out (LOO) estimator to estimate the advantage of a prompt-response pair, similar to a recent work [Ahmadian et al., 2024].\n\u2022 We use Mirror Descent Policy Optimization (MDPO) [Tomar et al., 2020] to optimize the policy, differently from the more commonly used clipping-based PPO method.\nThus, we name our online RL algorithm Mirror Descent with Leave-One- Out estimation (MDLOO). More specifically, during the decoding stage of the algorithm, we decode multiple responses for each prompt, and assign the advantage of each response to be the difference of the reward of the (prompt, response) pair and the mean reward of the other responses generated by the same prompt. Intuitively, this estimator aims to measure how much better a response is compared to a typical response. Empirically, we find this advantage estimator crucial for stabilizing the RL algorithm and achieving strong results. Moreover, we use a KL-regularization-based trust region method, i.e. MDPO, to control the policy change in each iteration. We find that this algorithm is more effective than PPO in our setting. More details of our online RLHF algorithm can be found in Section E.2."}, {"title": "Powering Apple Intelligence features", "content": "Our foundation models are designed for Apple Intelligence, the personal intelligence system integrated into supported models of iPhone, iPad, and Mac. We have built these models to be fast and efficient. And while we have achieved impressive levels of broad capability in our base model, the actual relevant measure of its quality is how it performs on specific tasks across our operating systems.\nHere we have found that we can elevate the performance of even small models to best-in-class performance through task-specific fine-tuning and have developed an architecture, based on runtime-swappable adapters, to enable the single foundation model to be specialized for dozens of such tasks."}, {"title": "Adapter architecture", "content": "Our foundation models are fine-tuned for users' everyday activities, and can dynamically specialize themselves on-the-fly for the task at hand. We use LORA [Hu et al., 2021] adapters, small neural network modules that can be plugged into various layers of the base model, to fine-tune our models for specific tasks. For each task, we adapt all of AFM's linear projection matrices in the self-attention layers and the fully connected layers in the pointwise feedforward networks. By fine-tuning only the adapters, the original parameters of the base pre-trained model remain unchanged, preserving the general knowledge of the model while tailoring the adapters to support specific tasks.\nWe represent the values of the adapter parameters using 16 bits, and for the ~3 billion parameter on-device model, the parameters for a rank 16 adapter typically require 10s of megabytes. The adapter models can be dynamically loaded, temporarily cached in memory, and swapped-giving our foundation model the ability to specialize itself on the fly for the task at hand while efficiently managing memory and guaranteeing the operating system's responsiveness.\nTo facilitate the training of the adapters, we created an efficient infras- tructure that allows us to rapidly add, retrain, test, and deploy adapters when the base model or the training data gets updated or new capabilities are required. It is worth noting that the adapter parameters are initialized using the accuracy-recovery adapter introduced in Section 5.2."}, {"title": "Optimizations", "content": "The AFM models are designed to support our users throughout their daily activities, and both inference latency and power efficiency are important for the overall user experience. We apply various optimization techniques to allow AFM to be efficiently deployed on-device and in Private Cloud Compute. These techniques significantly reduce memory, latency, and power usage while maintaining the overall model quality.\nIn order to fit AFM into a constrained memory budget of edge devices and reduce inference cost, it is critical to apply model quantization techniques to reduce the effective bits per weight while maintaining the model quality. Previous works have found that 4-bit quantized models only have marginal loss of quality (typically measured in pre-training metrics) compared to the original 32/16-bit float-point versions. Since AFM is expected to support a diverse set of product features, it is essential that the quantized model retains capabilities in specific domains critical to these use cases. To achieve an optimal trade-off between model capacity and inference performance, we have developed state-of-the-art quantization methods and a framework that utilizes accuracy-recovery adapters. This allows us to achieve near-lossless quantization that is on average less than 4 bit-per-weight, and provides flexible quantization scheme choices.\nMethods The model is compressed and quantized, on average under 4-bit-per- weight, after the post-training stages (details of the quantization scheme will be discussed later). The quantized model often shows a moderate level of quality loss. Therefore, instead of directly passing the quantized model to application teams for feature development, we attach a set of parameter-efficient LORA adapters for quality recovery. We make sure that these LoRA adapters training recipes are consistent with pre-training and post-training processes. Then, products will fine-tune their own feature-specific LoRA adapters by initializing the adapter weights from the accuracy-recovery adapters, while keeping the quantized base model frozen.\nIt is noteworthy that training accuracy-recovery adapters is sample-efficient and can be considered as a mini-version of training the base model. During the pre-training stage of the adapters, we only require approximately 10 billion tokens (~ 0.15% of base model training) to fully recover the capacity for the quantized model. Since application adapters will fine tune from these accuracy-recovery adapters, they do not incur any additional memory usage or inference costs. Regarding adapter size, we found that adapter rank of 16 offers the"}]}