{"title": "On-Board Vision-Language Models for Personalized Autonomous Vehicle Motion Control: System Design and Real-World Validation", "authors": ["Can Cui", "Zichong Yang", "Yupeng Zhou", "Juntong Peng", "Sung-Yeon Park", "Cong Zhang", "Yunsheng Ma", "Xu Cao", "Wenqian Ye", "Yiheng Feng", "Jitesh Panchal", "Lingxi Li", "Yaobin Chen", "Ziran Wang"], "abstract": "Personalized driving refers to an autonomous vehicle's ability to adapt its driving behavior or control strategies to match individual users' preferences and driving styles while maintaining safety and comfort standards. However, existing works either fail to capture every individual preference precisely or become computationally inefficient as the user base expands. Vision-Language Models (VLMs) offer promising solutions to this front through their natural language understanding and scene reasoning capabilities. In this work, we propose a lightweight yet effective on-board VLM framework that provides low-latency personalized driving performance while maintaining strong reasoning capabilities. Our solution incorporates a Retrieval-Augmented Generation (RAG)-based memory module that enables continuous learning of individual driving preferences through human feedback. Through comprehensive real-world vehicle deployment and experiments, our system has demonstrated the ability to provide safe, comfortable, and personalized driving experiences across various scenarios and significantly reduce takeover rates by up to 76.9%. To the best of our knowledge, this work represents the first end-to-end VLM-based motion control system in real-world autonomous vehicles.", "sections": [{"title": "1. Introduction", "content": "The autonomous driving industry is experiencing an evolution towards human-centric systems [6, 28], where vehicle automation extends beyond only considering traditional safety and efficiency metrics but also considers understanding users' implicit instructions and providing personalized driving experiences [11, 12]. Personalized driving experiences are crucial for user acceptance and trust, as they help bridge the gap between autonomous technology and human expectations. This trend reflects a growing recognition that successful adoption of the autonomous vehicle requires not just technically self-driving, but also the ability to provide human-like driving experiences that align with individual preferences and expectations [11, 12].\nPrevious work in personalized autonomous driving has followed two main approaches. The first uses clustering algorithms to classify drivers into broad categories (e.g., aggressive or conservative), but this fails to capture individual nuances and preferences, forcing users into predefined groups that may not match their actual driving style [48, 50]. The second approach develops individual models for each user through learning-based methods [14, 49], but this requires extensive training data per user and becomes computationally inefficient as the user base grows. Furthermore, these methods lack the ability to reason about real-time human instructions or adapt to changing environments.\nRecent advances in Vision-Language Models (VLMs) have demonstrated promising capabilities in understanding complex driving scenarios and natural language instructions through their integration of computer vision and language processing [7, 10, 30, 37]. The development in VLMs has led researchers to leverage VLMs' multimodal understanding capabilities to enhance both perception and decision-making in autonomous systems [36, 46]. However, there remains a research gap in leveraging VLMs to enhance control policies or adapt them to individual driving preferences and styles. This gap is particularly evident in the challenge of translating a high-level understanding of human preferences and scenario information into actionable low-level control policies. Additionally, the computational demands of previously adopted large-scale models make on-board deployment infeasible, forcing reliance on cloud-based inferencing. This solution depends on stable internet connectivity and can introduce significant latency issues in the inference process, with response times reaching up to 3 or 4 seconds [26]. This is incompatible with the reliable and near real-time requirements of autonomous driving."}, {"title": "2. Related Works", "content": "To address these limitations, we propose a novel VLM-based framework for real-time personalized autonomous vehicle control. Our system enables efficient on-board deployment while maintaining strong instruction understanding and scene reasoning capabilities. We present the first-of-its-kind real-world implementation of an on-board VLM-based personalized motion control system. The main contributions of this work are:\n\u2022 We develop an efficient on-board VLM that achieves comparable reasoning capabilities to cloud-based solutions while operating independently of internet connectivity. Our lightweight solution addresses critical computational constraints for real-world autonomous vehicles.\n\u2022 We propose a novel approach to translate diverse human instructions and visual inputs into actionable control policies, handling both explicit commands ('go faster') and implicit feedback (\u2018I feel uncomfortable') and diverse environment conditions.\n\u2022 We introduce a RAG-based memory module that incorporates human feedback for continuous learning and adaptation, enabling personalized driving experiences through iterative refinement of control strategies.\n\u2022 Through extensive real-world deployment and experiments, we demonstrate safe, comfortable, and reliable autonomous driving performance across various instruction types and successful personalization capabilities, reducing takeover rates by up to 76.9%."}, {"title": "2.1. Vision-Language Models for Autonomous Driving", "content": "Early applications of language models in autonomous driving involved human-guided planning, integrating natural language commands or advice, and generating language-based interpretations or control signals for vehicle operations [18, 19, 27]. With the advent of VLMs, initial efforts focused on image-based models [3, 13, 21, 25] that utilized image encoders and bridging modules connected to LLMs. More recently, video-based VLMs have emerged [22, 29, 56], enhancing their applicability in autonomous driving by processing image sequences crucial for real-time decision-making. Fine-tuned with instruction-following datasets specific to driving scenes, these VLMs address tasks such as visual question answering (VQA) to achieve a comprehensive understanding of traffic scenarios and the behavior of the ego-vehicle [30, 37]. Additionally, predicting future waypoints has become a prominent task within this domain, often employing Chain-of-Thought mechanisms to improve planning by generating text sequences for perception, prediction, and planning [43, 47]. Certain models do not solely rely on image inputs; instead, they incorporate perception and prediction submodules to enrich prompts with detailed road agent information for more effective planning [35]. Moreover, some VQA datasets include localized object data to better anticipate the future behavior of potential risk objects [34, 38]. Unlike existing works that primarily focus on VLMs operating in simulation environments, our approach addresses generating action policies that can be adapted to real-world vehicle-level applications using VLMs."}, {"title": "2.2. Personalization/Human-AI Alignment in Autonomous Driving", "content": "There has been an increased focus on the personalization of autonomous driving, aiming to follow the driving experience to individual preferences and needs [2, 16]. In recent developments, various studies have explored personalized adaptive cruise control systems, focusing on steady-state operation [57], Gaussian process-based methods [48, 49], Transformer and RNN integration [40] for enhanced user experience and driving preferences. Schrum et al. introduced MAVERIC, a novel framework for personalizing driving styles, which demonstrated the ability to adapt to humans' driving preferences influenced by personality traits and perceived similarity [41]. Buzdugan et al. focused on safety-critical scenarios, using driver-in-the-loop simulations to detect and adapt to personalized driving styles, thus bridging the gap between human driving behavior and autonomous vehicle predictability [5]. Ma et al. [31] leveraged RAG which is an approach that enhances model capabilities by retrieving relevant historical information to augment the generation process [20] to learn from human feedback. There are also studies on systems offering personalized experiences to predict human actions and increase human trust instead of directly altering vehicle control [14, 23, 45, 51, 55]. However, such personalization frameworks often encounter limitations such as dynamically adapting to human preferences or unseen traffic scenarios. This is where VLMs could potentially complement these systems by offering more complex and context-aware adaptations, leveraging their advanced language understanding and generative capabilities."}, {"title": "3. On-Board Vision-Language Models for Personalized Motion Control", "content": "In this section, we present our on-board VLM for personalized motion control in autonomous driving, designed to accommodate individual driving styles. Our approach leverages a compact 9B-parameter VLM, fine-tuned from Qwen-VL [3], which processes both visual information (including weather conditions, road types, and traffic situations) and verbal commands to generate personalized control strategies for each user. The reduced scale of this VLM enables edge deployment while maintaining command interpretation and reasoning capabilities, allowing the system to effectively understand and respond to implicit human instructions. The overall framework is shown in Fig. 1."}, {"title": "3.1. Problem Statement", "content": "In this paper, we adopt the traditional module-based autonomous driving framework that includes the full pipeline from perception to motion control, and our focus is specifically on enhancing the decision-making process at the motion control level, adapting the control performance to accommodate different human driving styles. The goal of the proposed system is to translate both verbal commands I and visual inputs V into executable control sequences for the motion control process. The onboard VLM acts as a translation mechanism f : (I, V) \u2192 P that generates a policy P, which is then fed into predefined maneuver programs.\nAdditionally, system messages S are sent to our VLM to specify both the tasks and adjustment strategies. In practice, S is generated through a predefined language generator. These system messages define the VLM's role and objectives, and provide explicit instructions for the tuning strategy.\nSimultaneously, to further enhance personalization, we implement a RAG system called the memory module to build a database storing historical human-vehicle interactions. Whenever a human activates this system, only relevant historical scenarios H are retrieved and provided to the VLM as reference. After each trip, users can provide feedback F on the generated control policy P for the current situations (including instructions I and visual input V), which helps refine the VLM's reasoning process. Subsequently, the instructions I, scene description D, policy P, and feedback F are packaged as a historical data entry and stored in the RAG database. Therefore, there are three procedures in our system:\nVLM Execution: $P \\leftarrow f(I, S, V, H);$\nHuman Feedback : $F \\leftarrow [I, V, P];$\nMemory Update : $H \\leftarrow [I, D, P, F]$"}, {"title": "3.2. System Input", "content": "As illustrated in Fig. 1, our fine-tuned on-board VLM processes four critical inputs for reasoning. The primary inputs consist of visual data V from the onboard camera and natural language commands I which are converted from verbal human instructions I using the open-source local speech recognition model \u2018Whisper [39].' Notably, due to the advanced reasoning and understanding ability of our fine-tuned VLM, our system can interpret implicit expressions from humans such as \"It is nice weather, I want to enjoy the view.\" This ability to understand implicit instructions is crucial, as users typically communicate through natural conversational phrases rather than explicit value-based commands containing exact parameters. Furthermore, our system leverages contextual and environmental information captured in the visual inputs V, including weather conditions, traffic situations, and road characteristics. For instance, the system automatically adopts a more conservative driving policy during nighttime operations or adverse weather conditions.\nAdditionally, a pre-defined system message generator is employed to produce a customized system message S, which is then simultaneously sent to the VLM. This message includes essential information about the system, including the user's identity, specific objectives, and key principles guiding the system's behavior, particularly how to utilize the controller or tune parameters.\nFurthermore, the VLM incorporates relevant interaction history H extracted from our memory module as contextual input, which includes previous human instructions I, scene descriptions D, executed actions A, and user feedback F. This historical context enables the VLM to generate more appropriate responses by considering past interactions and human feedback. For example, if a user has previously expressed a preference for cautious driving in certain scenarios, the system can capture this preference into its current decision-making process, ensuring more personalized and contextually appropriate responses. A detailed discussion of how our memory module works will be presented in subsection 3.4."}, {"title": "3.3. Reasoning and Action Generation", "content": "In our approach, reasoning within the VLM framework enables the interpretation of diverse driving scenarios and user instructions to generate actionable outputs. Traditional controllers in motion control typically rely on a default set of parameters; however, following the approach in [42], our VLM will generate two distinct action matrices to separately manage the PID controller for longitudinal movement and the MPC for lateral movement. These matrices translate the model's understanding of the environment and user preferences into precise control actions, guiding the autonomous vehicle's behavior. Specifically, they are used by the controllers to generate acceleration a and steering angle df, which are executed by the vehicle's ECU. The ECU then sends low-level control signals to the drive-by-wire system developed by AutonomousStuff [1], enabling smooth and responsive vehicle operation. The general process of this subsection can be shown below:\nOutput Action $P := \\begin{bmatrix} Kp & Ki & Kd \\\\ Wl & Wh & Ws \\end{bmatrix}$\nAction Execution $P \\xrightarrow[Controllers]{} [\\delta_f, a] \\xrightarrow[]{} ECU$"}, {"title": "3.4. RAG-Enhanced Memory Module", "content": "Given that our 8B-parameter VLM lacks the extensive reasoning capabilities of larger, 100B-200B parameter models, we employ a RAG-based approach [20] and integrate a memory module to enhance reasoning and enable human feedback learning. This system is built upon the Chroma vector database [8], enabling efficient storage history interactions and retrieval of similar driving scenarios.\nThe memory module is uniquely created for each user, ensuring a personalized driving experience follows individual preferences and patterns. It stores driving-related information in a structured format comprising commands paired with corresponding context tuples:\n{I \u2013 (I, D, P, F)}\nWhen processing a new driving scenario, the instruction I is used for similarity matching to retrieve the top-k similar prior situations. The associated data values are then sent to the VLM, enhancing decision-making with relevant context and supporting personalization. This RAG-enhanced memory enables the VLM to handle similar situations with greater accuracy and consistency, improving the vehicle's responsiveness to unique user preferences and enhancing the overall driving experience."}, {"title": "3.5. Multi-Controller Joint Motion Control", "content": "As shown in Fig. 1, we implement a decoupled control strategy that separates lateral and longitudinal vehicle motion control. The lateral control is handled by MPC calculating the longitudinal acceleration a, while longitudinal control is managed through a PID controller calculating the front steering angle df. The motion planning module in the upper layer provides trajectories consisting of waypoints, which our motion control system tracks. The calculated a and df are then transmitted to the drive-by-wire system developed by AutonomousStuff [1] for precise control of throttle, braking, and steering.\nFor the longitudinal control, the PID controller calculates the a for each time step \u0394t to minimize the velocity error ev, which is the difference between the current velocity Vcurrent and the desired velocity Vref.\n$a(t) = K_p e_v(t) + K_i \\sum_{i=0}^{t} e_v(i) \\Delta t + K_d \\frac{\\Delta e_v(t)}{\\Delta t}$\nwhere Kp, Ki, and Kd are the proportional terms, integration terms, and derivative terms that will be contained in the action matrix generated by our VLM.\nFor the lateral control, our MPC approach utilizes a linear vehicle dynamics model [44] to predict future states and optimize the front steering angle, df, over a finite prediction horizon. With the prediction model[44], the control increment is then obtained by solving a Quadratic Program (QP)"}, {"title": "3.6. Efficient On-Board VLM Module", "content": "We generate a dataset of 10,000 image-instruction pairs, each labeled with the desired action, to create a comprehensive training set for fine-tuning our on-board VLM. This VLM is based on the Qwen-VL architecture [3], which we fine-tune using the Low-Rank Adaptation (LoRA) method [15] (a type of Parameter Efficient Fine-Tuning (PEFT) [53]), enabling significant customization while preserving computational efficiency. To optimize for on-board deployment, we apply 4-bit Activation-Aware Weight Quantization (AWQ) [24], compressing the VLM to increase inference speed without sacrificing too much accuracy. This combination of techniques ensures a responsive, on-board VLM suited to real-time response.\nDataset Collection We develop a specialized training dataset to fine-tune the Qwen-VL model [3], consisting of 1,200 semi-human-annotated image-text pairs. Each image, representing a traffic scene sourced from the NuScenes dataset, which includes numerous diverse traffic scenarios, is paired with a human-provided instruction and a corresponding action label in the form of a controller action matrix, guiding the model's response in different traffic scenarios.\nThe human instructions are also very diverse, ranging from explicit commands like 'speed up' to more implicit cues such as 'I am in an urgent situation.' This diversity allows the VLM to interpret both clear and vague inputs, improving its ability to understand complex human intentions. To enhance the model's responsiveness to different driving styles, we annotate each image with three different instruction types-aggressive, moderate, and conservative each paired with a corresponding action. This multi-faceted approach ensures that the VLM can adapt its behavior to match various driving styles, enabling it to respond flexibly and contextually across diverse traffic conditions.\nLORA Finetune We apply the LoRA method to fine-tune our Qwen-VL model. LoRA works by freezing the pre-trained model weights and introducing trainable, low-rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters required, making fine-tuning more efficient.\nThe fine-tuning process for our VLM is conducted on a cluster of four NVIDIA A100 GPUs, each equipped with 40GB of memory. The model is trained over five epochs with a per-device batch size of two for training and one for validation, using a learning rate of 1e-5. Additionally, we implemented gradient accumulation with eight steps, allowing for effective larger batch processing. This setup enables the entire training process to be completed in approximately five hours, ensuring both accuracy and efficiency in model performance.\nCompression and On-Board Deployment of VLM AWQ [24] is a hardware-friendly technique for low-bit, weight-only quantization, specifically designed for VLM. AWQ minimizes quantization error by identifying the 1% salient weights, which are then scaled using an equivalent transformation to preserve their precision. We apply AWQ to quantize our model to INT4, achieving improved quantization performance suited for on-board deployment. Additionally, we utilize the LMDeploy toolkit [9] to optimize inference time. This enhancement is made possible through features such as persistent batching, blocked KV cache, tensor parallelism, and optimized CUDA kernels, all of which contribute to high-performance, low-latency operation."}, {"title": "4. Real-World Experiment", "content": "To comprehensively evaluate our system's performance, we conduct a series of experiments assessing its ability to provide safe, comfortable, reliable, and personalized driving experiences. We employ multiple evaluation metrics: a driving score to measure driving performance, including safety, comfort, and alignment with environmental conditions and human instructions; takeover frequency to assess personalization capabilities; and evaluator-based assessments to investigate trustworthiness, reliability, and user satisfaction. Additionally, we perform an ablation study to examine the effectiveness of the memory module."}, {"title": "4.1. Experiment Setup", "content": "Autonomous Vehicle Setup: As shown in Fig. 2, we use an autonomous vehicle to conduct real-world experiments, which is a drive-by-wire-enabled 2019 Lexus RX450h. We deploy the open-source autonomous driving software Autoware.AI [17] with ROS Melodic in Ubuntu 18.04. We use 3D-NDT [32, 33] for mapping and localization. An Aptiv ESR 2.5 radar, a Velodyne VLP-32C LiDAR, and two Mako-G319C cameras are deployed on the vehicle to enable the perception capabilities. The on-board computer has an Intel i9-9900 9th Gen 3.10/5.0GHz hexa-core 65W processor with eight cores and 16 threads, 64GB RAM, NVIDIA Quadro RTX-A4000 16GB GPU, and 512GB NVMe solid state drive.\nTest Track and Participants The field experiments\u00b9 aim at validating the real-world performance of our personalized motion control system. We include three types of driving behaviors-accelerating, lane changing, and turning-to comprehensively validate control over steering, throttle, and braking. An overview of the test track and driving behaviors is shown in Fig. 2. For both acceleration and lane change scenarios, a lead vehicle is positioned 30 m ahead of the ego vehicle, accelerating from static to 45 km/h with an acceleration of 1.26 m/s\u00b2. In the acceleration scenario, the ego vehicle accelerates from a complete stop to reach 50 km/h. In the lane change scenario, the ego vehicle maintains 50 km/h while overtaking the lead vehicle. For the intersection turning scenario, the ego vehicle navigates a curve with a radius of 23.89 m at a constant speed of 30 km/h.\nOur study includes seven participants with diverse demographic characteristics. The participants consisted of 61.4% male and 28.6% female drivers, with ages ranging from 23 to 30 years (mean = 26.42, std = 3.24). Their driving experience varies considerably (mean = 6.42 years, std = 4.27 years). All participants hold valid U.S. driving licenses.\nInstruction Directness In the field of linguistics, instructions can range from simple to complex in terms of how directly they convey intent [54]. To evaluate our model's natural language understanding capabilities, we classify instructions into three levels of increasing complexity: Level 1 consists of straightforward commands that explicitly state the desired action; Level 2 includes moderately complex instructions that may require some contextual understanding; Level 3 represents sophisticated commands that involve implicit meanings or complex conditions."}, {"title": "4.2. System Driving Performance", "content": "To showcase our system's driving performance, we conduct comparative experiments against two systems: a baseline system using pre-defined controller parameters for general safety and comfort and a system utilizing GPT-4o with few-shot learning. We evaluate across three primary meta-driving scenarios (acceleration, lane changing, and turning), with each scenario tested under ten different commands and five weather conditions (sunny, rain, fog, snow, and night) to test the model's vision understanding as shown in Fig. 3.\nEvaluate Metrics The models are then assessed based on four key aspects-safety, comfort, time efficiency, and alignment, as shown in Tab. 1. An overall driving score S is then calculated as a weighted sum of the individual metric scores, denoted as:\n$S = \\sum w_k \\cdot S_k$\nwhere k includes all ten metrics: Time to Collision (\u03c4), longitudinal and lateral speed variance (SVx and SVy), lateral and longitudinal mean absolute acceleration (|ax| and |ay|), lateral and longitudinal mean absolute jerk (|Jx| and |Jy|), LLM Latency, Command Alignment and Scenario Alignment. All the scores Sk range from 0 to 100, while the weights of scores wk are empirically tuned for each driving scenario. For instance, longitudinal parameters have higher weights in acceleration scenarios, while lateral parameters are weighted more heavily in turning scenarios. For safety metrics, we set a critical Time to Collision threshold \u03c4 = 1.5 to prevent potential collisions. Other metrics like speed variance, acceleration, and jerk are scored relative to the baseline model.\nThe alignment evaluation consists of two aspects. Command Alignment quantifies the deviation between the model output and the expected parameter range, calculated as a weighted average across six control parameters. For each parameter, we establish three ranges based on past experiments, corresponding to aggressive, conservative, or moderate driving styles. Taking the PID controller's proportional parameter Kp as an example, the score is calculated as:\n$S_{Kp} = \\begin{cases} 100 \\cdot \\frac{(K_{p} - K_{p,min})}{K_{p,lower}}, & K_{p} \\in [K_{p,min}, K_{p,lower}), \\\\ 100, & K_{p} \\in [K_{p,lower}, K_{p,upper}), \\\\ 100 \\cdot \\frac{(K_{p,max} - K_{p})}{K_{p,upper}}, & K_{p} \\in [K_{p,upper}, K_{p,max}], \\\\ 0, & \\text{else}. \\end{cases}$\nwhere Kp,min and Kp,max are the minimum and maximum overall parameter range obtained through experiments, while Kp,lower and Kp,upper are determined by the command intention labeled by human experts. The scenario alignment score computes whether the model can capture details of the scene through vision input and act more aggressively or conservatively based on the current condition. It is calculated by tallying the percentage of instances where the model gives more conservative parameter sets in adverse weather conditions compared to the sunny clear scenarios.\nResult As shown in Tab. 1, the command alignment score of our VLM model is similar to or greater than GPT-4o, showing our model has high reasoning capabilities regarding the personalization of control parameters. As for the scenario alignment, our model shows significantly better performance than baseline conditions in lane changing and left turn scenarios but a score very similar to the baseline scenario. We think this is mostly due to the model considering acceleration on a clear straight road in less dangerous situations and thus does not act too conservatively. In terms of the overall Driving Score, our model also surpasses the performance of baseline models and even GPT-4o in some scenarios, indicating our model provides the most well-rounded driving experience."}, {"title": "4.3. Human-in-the-Loop Validation", "content": "This subsection evaluates the effectiveness of our method in reducing takeover rates compared to the baseline system. The baseline consists of the traditional autonomous vehicle controller with default settings, where two unified controllers manage vehicle operations on longitudinal and lateral respectively. We compare this conventional approach against our VLM-based adaptive motion control system. Throughout the experiments, participants could provide explicit instructions or implicit preferences/feedback with varying degrees of directness, prompting the system to make corresponding adjustments. The instructions were categorized into three levels of directness, as defined in Subsection 4.1.\nEvery participant is supposed to provide at least five instructions for each scenario. For each instruction-scenario pair, participants completed two trips - one with the baseline system and one with our VLM solution. To ensure unbiased evaluation, participants are not informed which system they are using during each trip. We use the takeover rate as our primary metric to evaluate the system's ability to provide personalized driving experiences.\nThe results demonstrate that our VLM-based method consistently outperforms the baseline system across all three levels of instruction directness. With Level 1 (direct) instructions, our method achieves a 5.56% takeover rate versus the baseline's 19.44%, representing a 71.4% reduction. For Level 2 (moderately direct) instructions, the rates are 6.06% versus 12.12%, showing a 50% improvement. Most notably, for Level 3 (indirect) instructions, our method achieves 8.33% versus the baseline's 36.11%, marking a 76.9% reduction in takeover rate. The result demonstrates our system's better capability in interpreting and acting based on user preferences, regardless of how explicitly they are communicated."}, {"title": "4.4. Evaluator-based Assessment", "content": "To assess the impact of our system on trust, reliability, personalization, and understandability, we conduct a survey to capture participants' attitudes from various perspectives, rated on a scale from 1 (low) to 5 (high). Participants rate the match of personalized driving performance, system reliability, level of trust, and understandability of the system's user interface after each trip. Similar to the previous section, participants are unaware of which system they were using. The results are displayed in Fig. 5.\nAfter collecting data, we conduct a Wilcoxon signed-rank test [52], a robust non-parametric approach for paired data, to investigate the effectiveness of the VLM-based method. The null hypothesis (Ho) posits that the median of the baseline is greater than or equal to that of our method, while the alternative hypothesis (H\u2081) suggests that the median of the baseline is less than that of our method. Using a significance level of p < .05 to reject Ho, our analysis shows that the VLM-based method significantly outperforms the baseline across multiple metrics, including matching personalized driving behaviors and demonstrating greater reliability. In more challenging scenarios (e.g., lane changes, left turns), our VLM-based method's advantages are particularly more evident, leading to increased trust and improved understandability compared to the baseline."}, {"title": "4.5. Ablation Study on Memory Module", "content": "To further validate the effectiveness of our RAG-based Memory Module (MM), we conduct an ablation study with three human participants, comparing three configurations: our complete system with MM, the system without MM (W/O MM), and the baseline. The results demonstrate the significant impact of the MM on reducing takeover rates.\nAs shown in Fig. 6, With three participants, our complete system with the memory module achieves the lowest average takeover rate of 6.67%. When we remove the memory module while keeping other components the same, the average takeover rate increases substantially to 24.44%. The baseline system performs worst with a 44.44% average takeover rate. These results indicate a 72.7% reduction in takeover rate when adding the memory module to our base architecture and an 85% overall reduction compared to the baseline. This significant improvement suggests that the RAG-based memory module plays a crucial role in maintaining personalized vehicle control by effectively utilizing historical interactions and user preferences."}, {"title": "5. Conclusion", "content": "In this paper, we presented an on-board VLM-based framework designed to enhance motion control tasks in autonomous driving, offering a more human-centric and responsive user experience. Our personalized motion control system represents a novel integration of VLMs into autonomous driving, offering three significant contributions to human-centric driving experiences. First, through its RAG-based memory module, the system demonstrates advanced personalization capabilities, effectively learning and adapting to individual driving preferences while maintaining safety and comfort standards across various scenarios. Second, the fine-tuned VLM-based framework leverages multimodal reasoning to understand both complex visual scene information and implicit natural language instructions, enabling human-like human-vehicle interaction. Finally, the system achieves remarkable computational efficiency with an optimized 9B VLM implementation, processing with an average 1.6-second latency and less than 16 GB GPU memory consumption on standard vehicle hardware, making it feasible for commercial deployment. Through experiments, we demonstrated that our VLM-based approach enhanced driving reliability, trustworthiness, and personalization, reducing the need for human takeover by up to 76.8% while maintaining near real-time response. This framework contributed to enhancing personalized autonomous driving by aligning vehicle behavior with individual user preferences and considering environmental information, marking a significant step toward a more adaptable, user-centered human-autonomy teaming solution."}]}