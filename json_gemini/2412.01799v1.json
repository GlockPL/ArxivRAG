{"title": "HPRM: High-Performance Robotic Middleware for Intelligent Autonomous Systems", "authors": ["Jacky Kwok", "Shulu Li", "Marten Lohstroh", "Edward A. Lee"], "abstract": "The rise of intelligent autonomous systems, especially in robotics and autonomous agents, has created a critical need for robust communication middleware that can ensure real-time processing of extensive sensor data. Current robotics middleware like Robot Operating System (ROS) 2 faces challenges with nondeterminism and high communication latency when dealing with large data across multiple subscribers on a multi-core compute platform. To address these issues, we present High-Performance Robotic Middleware (HPRM), built on top of the deterministic coordination language Lingua Franca (LF). HPRM employs optimizations including an in-memory object store for efficient zero-copy transfer of large payloads, adaptive serialization to minimize serialization overhead, and an eager protocol with real-time sockets to reduce handshake latency. Benchmarks show HPRM achieves up to 173x lower latency than ROS2 when broadcasting large messages to multiple nodes. We then demonstrate the benefits of HPRM by integrating it with the CARLA simulator and running reinforcement learning agents along with object detection workloads. In the CARLA autonomous driving application, HPRM attains 91.1% lower latency than ROS2. The deterministic coordination semantics of HPRM, combined with its optimized IPC mechanisms, enable efficient and predictable real-time communication for intelligent autonomous systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Due to the advancements in AI, the area of intelligent autonomous systems is rapidly growing. These systems, especially in the context of robotics and autonomous agents, are critical in both performance and reliability due to their capability to analyze extensive sensor data in real-time. They require a robust communication infrastructure to ensure real time transmission and processing of data.\nIn the architecture of autonomous systems, modules are typically organized as coarse-grained processes [1]. This design paradigm, which emphasizes functional independence and resource isolation, ensures that a failure in one module does not compromise the integrity or functionality of other modules or the system as a whole. Consequently, the ex- change of data across different modules is predominantly fa- cilitated through Inter-Process Communication (IPC) [2], [3] techniques. For instance, in a scenario where a robot is tasked with identifying specific objects for humans, the image data captured by the camera module undergoes several steps: serialization into a buffer, copying into the system kernel, transferring to the target process, and finally, deserialization. These operations usually lead to high latency in applications that make use of high-resolution cameras or LiDAR sensors. Frameworks like the Robot Operating System (ROS) [4] and MQTT [5] have seen significant adoption in critical, concurrent, and distributed settings, including autonomous vehicles and industrial automation. These frameworks are valued for their convenience, modularity, and the use of a publish-subscribe mechanism, which can easily be leveraged for message exchange in distributed systems. However, the publish-subscribe mechanism, particularly in high-stakes en- vironments like autonomous driving, introduces a level of nondeterminism [6] due to varying communication timing, potentially resulting in unpredictable message handling se- quences. This unpredictability is a significant concern in environments where the consequences of errors are severe. In addition, to support message passing in autonomous nav- igation systems, frameworks such as ROS2 generally utilize sockets-based communication [7]. However, this method falls short in scenarios involving the processing of large data packets across numerous subscribers, as it leads to an increase in communication latency with message size.\nIn this study, we offer an alternative to ROS2-High- Performance Robotic Middleware (HPRM). HPRM is an open-source robotic middleware built on top of a coordi- nation language, Lingua Franca (LF) [8]. LF, which is based on the reactor model [9], is a polyglot coordination language that combines the most effective semantic elements from well-established computational models. This includes the actor model, Logical Execution Time (LET), synchronous reactive languages, and discrete event systems like SystemC. LF advances the field by integrating time as a primary element within its programming paradigm, thereby facili- tating deterministic interactions across various physical and logical timelines. HPRM is meant to enhance the capability of robotic middleware in handling large volumes of sensor data and ML workloads using efficient IPC techniques. Specifically, it uses an in-memory object store to efficiently transfer large objects across different processes, adaptive serialization for different types of sensor data in Python, and an eager protocol and real-time sockets to minimize the handshake latency for transmitting control and object references. Our approach significantly reduces the overhead associated with local IPC compared to ROS2 [10].\nWe demonstrate the benefits of HPRM by integrating it with CARLA [11] to be running reinforcement learning (RL) agents and object detection in autonomous driving scenarios. ROS applications can also be easily ported to HPRM, and a demonstration of porting ROS2 programs to HPRM has been included in the GitHub repository and showcased in the video. The HPRM runtime system is implemented in C and applications are modular, just like ROS and MQTT, allowing independent processes to be deployed."}, {"title": "II. MOTIVATION AND REQUIREMENTS", "content": ""}, {"title": "A. Motivation", "content": "Frameworks like ROS2, are becoming more prevalent in critical applications, including autonomous driving, where the implications of unpredictable behaviors are significant. However, the coordination mechanism in ROS2 introduces nondeterminism [6], leading to arbitrary ordering in the handling of messages. This inherent nondeterminism in the publish-subscribe communication models poses a risk and could compromise the reliability of such systems.\nFurthermore, robotics middleware [12], such as ROS2, faces considerable delays in message delivery, which can compromise the efficacy of real-time robotic operations when dealing with large volumes of data or multiple subscribers. Consider the scenario in autonomous navigation systems, where a planning module has to process large-scale inputs from perception before sending actions to other components like a localization module and vehicle control systems. Moreover, many developers within the ROS community have experienced latency problems when publishing large data [13][14][15]. Therefore, minimizing communication delays is pivotal for improving the real-time responsiveness of robotic systems, thereby improving the overall user expe- rience in scenarios that demand real-time data processing. Kronaur et al. [16] highlights the proportional increase in communication latency relative to message size using ROS2. Specifically, they observe that for messages around 4MB, the median delay for ROS2 is around 10ms. In scenarios of 1MB data being distributed to five subscribers, ROS2 exhibited a median latency nearing 80ms."}, {"title": "III. ROS2 VS. LINGUA FRANCA", "content": ""}, {"title": "A. ROS2", "content": "ROS2 is designed to support robotics application de- velopment. It allows developers to encapsulate software components within distinct units known as nodes, each running within its own OS process. Nodes engage through a publisher-subscriber (pub-sub) system, where publishers announce topics and subscribers associate specific callback functions with those topics. In this paper, we employ ROS2, which utilizes a communication framework compliant with the Data Distribution Service (DDS) [17] to facilitate the pub-sub mechanism.\nThe zero-copy feature has been incorporated into both Cy- clone DDS and Fast DDS. However, this feature is currently only available for rclcpp, the C++ implementation of the ROS2 client library. As of now, there is no support for the ROS2 Python client library [18].\nWhen ROS2 nodes are running on the same hardware, a NIC-level loopback is applied without any network transmis- sion [19], [20]. In such methods, messages are copied several times throughout processes and OS-kernel levels, leading to unnecessary memory copy and system calls. Furthermore, since a socket is a point-to-point communication interface, collective communications become inefficient. For example, if one process publishes the same message to the other three processes, the entire communication stream is repeated three times.\nWang, et al. propose a hybrid solution termed Towards Zero Copy (TZC)[21], designed to optimize the handling of large messages in ROS2. In TZC, messages are separated; a lightweight descriptor traverses the conventional path over a ROS topic via TCPROS, while the main body of the message resides in shared memory. However, TZC's lack of a robust mechanism to manage message lifecycles could potentially leave unclaimed payloads, risking memory leaks if their descriptors fail to be accurately transmitted. Furthermore, TZC is not actively developed or maintained and is not compatible with ROS2."}, {"title": "B. Lingua Franca", "content": "Lingua Franca (LF) is presented as an open-source poly- glot coordination language designed to facilitate determinis- tic interactions among concurrent and reactive components known as reactors. The characteristic of LF that underpins our research is its deterministic nature [22]. LF orchestrates event flow through a system, where events are tagged, facilitating transmission from one reactor's port to another's. Each event is marked with a logical tag from a totally- ordered set G, ensuring every reactor processes events in a sequential tag order. Each event tag consists of a timestamp $t \\in T$ indicating logical time and a microstep $m \\in N$ for capturing super-dense time, allowing for precise event scheduling.\nLF's design supports polyglot programming, enabling re- actions within reactors to be authored in a variety of pro- gramming languages, including C, C++, Python, TypeScript, or Rust. This polyglot capability ensures that LF can be seamlessly integrated into diverse development environments by compiling LF programs into the chosen target language [23]. HPRM is developed on top of the Python target and will support C++ in the future. Currently, LF's Python and C-runtime support various embedded platforms, including Arduino, Raspberry Pi, and Zephyr RTOS.\nFurthermore, for extremely latency-critical tasks, the op- timal solution would be to avoid serialization through intra- process communication, allowing direct access to messages without copying or serialization. HPRM allows users to easily switch to intra-process communication in Python. Kwok et al. [24] have enabled users to write truly concurrent Python programs without the limitations imposed by the Global Interpreter Lock. The syntax and documentation of LF is available at https://www.lf-lang.org.\nTime is treated as the core element in LF, with the framework providing access to both logical and physical clocks. The design principle is such that logical time closely follows physical time, maintaining a temporal coherence that ensures logical events occur near their physical counterparts but not before. Reactions can be assigned deadlines, and LF supports deadline handlers for managing situations where deadlines are violated, thereby maintaining system respon- siveness and reliability. Reactor A has a specified deadline of 10 milliseconds (this value can be adjusted as a parameter of"}, {"title": "II. COORDINATION AND OPTIMIZATIONS", "content": ""}, {"title": "A. Centralized Coordination", "content": "In HPRM, we use a centralized coordination mechanism. The Runtime Infrastructure (RTI) is employed to man- age communication and synchronization among distributed components, named federates. In this strategy, the RTI is responsible for monitoring and regulating event tags during advancement of logical time, thereby assuring that federates process messages in a global consistent order. The RTI keeps track of the information below for each federate, identified as f:\n\u2022 Tag Advance Grant (TAGf): The latest tag sent to federate f, enabling it to update its current event tag to TAG f. Initially, TAGf is set to -\u221e.\n\u2022 Logical Tag Complete (LTCf): This represents the most recent tag reported by federate f, signifying the completion of all tasks (computations and communica- tions) associated with that tag or any preceding it.\n\u2022 Next Event Tag (NETf): This indicates the latest event tag from federate f, essentially the earliest future event in its queue. An empty queue is denoted by a special maximal tag, \u221e. Absence of an NET message would be represented as -\u221e.\nFor a federate p to advance to a logical time t in response to its upstream reaction, it must first receive authorization from the RTI. This authorization is contingent upon the RTI's assurance that p has received all messages up to and including time t.\nA fundamental rule in this model is that a federate's logical time does not precede the physical time as indicated by its local physical clock.\ns.out \u2192 p.in after 200 msec;\nIn the connection above, a message with timestamp t from sender s cannot be sent before the local clock at s reaches t and also cannot be sent before the RTI grants to s a time advance to t. It is noted that given that s lacks upstream federates, the RTI always grants it a time advance.\nIf we denote the communication latency as L, the message from s to p will reach p only after physical time t + L mea- sured by s's physical clock. If there is a clock discrepancy E between s's and p's hosts, p will receive the message at physical time $t+E+ L$ measured by t's physical clock. The delay parameter a (200 msec in the example) in the after clause then determines the timestamp $t + a$ for the message as received by p. At the receiving end, if $E + L > a$, then federate p will lag behind physical time by at least $E+L-a$. However, if $a > E + L$, it does not cause p's logical time to lag behind physical time. The RTI, having authorized s to move to time t, cannot permit p to advance to a time t+a or beyond until it confirms the message's delivery to p. To mitigate risks associated with delays and ensure prompt processing of physical actions and meeting deadlines, it's advisable to set the after delay a on connections to federates receiving network messages to exceed any anticipated E+L.\nThe centralized coordination approach ensures the precise and timely execution of activities and events across a feder- ated network."}, {"title": "B. Decentralized Coordination", "content": "The decentralized coordination model extends PTIDES [25]. This model draws inspiration from works by Lamport, Chandy, and Misra [26], [27]. In this decentralized coordi- nation strategy, the RTI plays a limited role, coordinating startup, shutdown, and clock synchronization. It is not in- volved in the execution of the distributed program.\nIn this approach, each federate is associated with a Safe- to-Process (STP) offset defined by the user. For a given federate $f_i$, we define $S_i \\in T$ as its STP offset. A federate is restricted from progressing to any event tag $g = (t,m)$ until the condition $T_i > t + S_i$ is satisfied, where $T_i$ denotes the physical time on $f_i$'s machine. If $f_i$ is associated with physical actions, then $S_i \\ge 0$. In other cases, $S_i$ may assume positive, negative, or zero values. The STP offset's purpose is to ensure that all potentially influencing events from other federates, with tags preceding g, are received by $f_i$ by the time the physical clock fulfills the aforementioned condition, thus facilitating processing in tag order.\nFederates communicate directly through sockets in a peer- to-peer architecture, bypassing the RTI, and logical time advancement does not require RTI to be involved. Federates can proceed with their logical time to t once their physical clock aligns with or after t+STP. Similar to the after clause, if the STP offset is greater than the total of network latency, clock synchronization error, and execution times combined, then every event will be handled in the order of their tags. Since the assumptions about network latency and others can be violated, HPRM also provides a handler for STP violation.\nThe decentralized coordination model is designed to prior- itize availability over consistency. This makes it particularly suitable for applications like autonomous driving and other robotic systems interacting with dynamic environments that require responsive real-time behavior. These systems can often tolerate some inconsistencies, which can be carefully managed by the user through the STP offset in HPRM. The STP offset allows developers to fine-tune the balance between availability and consistency. Conversely, the central- ized model ensures software components behave as specified, prioritizing consistency over availability. This model is more appropriate for safety-critical applications (e.g. aerospace"}, {"title": "C. Optimizations", "content": "1) In-memory Object Store: The shared memory (SM) module was introduced in Python 3.8 and has been used as a workaround to enable zero-copy in ROS2. By mapping the relevant region of shared memory into each process's address space, the module allows processes to access the same data without needing to copy data into separate buffers, thus saving CPU cycles and memory bandwidth. The Python SM module is used to create a block of shared memory that can be accessed by multiple processes. Processes can share complex data types more easily by using this shared memory block. It allows for the creation, destruction, and management of shared memory segments, and it supports the creation of NumPy arrays [29] that can directly map to a shared memory block. However, this approach is inefficient compared to the in-memory object stores when transferring large objects.\nIn-memory object stores also enable zero-copy data trans- fer, reducing memory usage and improving performance. HPRM seamlessly integrates with the Plasma in-memory object store [30], automatically enabling it for the transfer of large payloads (greater than 64KB) between processes. This use of in-memory object store is inspired by Ray [31]. The architecture of Plasma object store is shown in Fig 1. Plasma runs as a separate process and is written in C++ and is designed as a single-threaded event loop based on the Redis event loop library. The plasma client library can be linked into applications. Clients communicate with the Plasma store via messages serialized using Google Flatbuffers."}, {"title": "2) Adaptive Serialization", "content": "Traditional pickle serialization in Python often requires making one or more copies of the data being serialized. For example, when a large object is serialized, pickle first creates a bytes representation of the object, which is then written to the output stream. This process inherently involves copying the data. Out-of-band serialization, on the other hand, allows large data buffers to be handled separately from the main serialization stream. By using PickleBuffer objects, it's possible to avoid these additional memory copies, as the data does not need to be copied into the pickle stream but can instead be transmitted directly to the consumer in its original form.\nBy separating the metadata from the actual data buffers, out-of-band serialization allows the transmission of large data buffers without embedding them into the serialized pickle stream. This separation is particularly beneficial for applications that transmit large amounts of data (e.g. buffer-like objects, such as NumPy) between processes or over the network, as it enables the direct transfer of memory buffers without the overhead of serialization and deserialization processes."}, {"title": "3) Eager Protocol & Real-Time Sockets", "content": "To minimize the latency for transmitting small payloads, such as object refer- ences, metadata, and vehicle controls, we've implemented an eager protocol [33]. It pre-allocates fixed-size buffering space (64KB) for the message, reducing the handshake latency or wait time involved for the other federate to allocate memory for a new message.\nAlso, the Nagle's algorithm [34], enabled by default in TCP, bundles short messages together to avoid network traffic. As a result, it was delaying small messages. In the context of HPRM, disabling the Nagle's algorithm can be particularly beneficial when transmitting object references for large payloads stored in the in-memory object store. Without the Nagle's algorithm, these small but crucial ref- erences can be sent immediately, allowing the receiving process to access large payloads with minimal delay."}, {"title": "V. EVALUATION", "content": "It is noted that all benchmarks were conducted on a per- sonal workstation, equipped with an Intel\u00ae i7-13620H with"}, {"title": "A. Mean Latency", "content": "Figure 4 illustrates the comparison of average latency for broadcast and gather operations on objects with varying sizes using 4 nodes. This experiment utilizes ROS2 Humble, its variant with shared memory, and both the centralized and decentralized coordination strategy of HPRM. The x- axis represents the object size in megabytes (MB), ranging from 1 to 50 MB, while the y-axis indicates the latency in milliseconds, displayed on a logarithmic scale. The term latency refers to the duration required for one node to send a payload to another and for it to be received. This process may include sending and processing coordination-related messages, serialization and deserialization, and transferring data over the network.\nIt is important to note that ROS2 Humble (Shared Mem- ory) refers to using Python's pickle for serialization and the shared memory module IPC. This approach allows ROS2 to pass object references between processes, a workaround commonly utilized by robotics developers to leverage the benefits of shared memory. However, we argue that such a method is inefficient and requires users to consider the message size and type, as well as to manage the object read and write access manually. HPRM abstracts away these burdens for the users.\nFrom the graph, we can observe that as the object size increases, the latency for both ROS2 and HPRM also in- creases. However, ROS2's latency grows at a higher rate than HPRM's. Specifically, for 10MB objects, the typical size of large camera images, ROS2's mean latency hits 1,161 ms, while HPRM's mean latency is around 15 ms-about 77x faster. For the largest object size of 50 MB, ROS2's average latency is at 7,723 ms, whereas HPRM's average latency is 44.6 ms, which is 173x faster. We also observed that the latency of decentralized coordination is lower than that of centralized coordination, as it prioritizes availability and incurs less synchronization overhead. Nevertheless, as the size of the object grows, the impact of synchronization overhead on the mean latency diminishes. We conclude that HPRM with decentralized coordination consistently shows the lowest latency across all object sizes in the plot, outper- forming ROS2 Humble and its variant with shared memory."}, {"title": "B. Applications", "content": "To show the improvements in performance, we validated HPRM and ROS2 Humble on running reinforcement learning agents in the CARLA autonomous driving simulator. We de- signed a benchmark that simulates end-to-end urban driving, running ML models in parallel during inference. Specifi- cally, we adapted a pre-trained Proximal Policy Optimization (PPO) agent developed by Zhang et al. [35] to run in parallel with You Only Look Once (YOLO) [36] for object detection.\nFigure 5 is a data flow diagram automatically generated by HPRM. The rendered BEV and RGB camera images from the CARLA simulator are passed separately to the PPO Agent reactor and the YOLO reactor. PPO Agent runs the policy and passes the policy action to Fusion reactor, while YOLO is executed in parallel and passes action based on object detection (e.g. STOP signs and traffic lights) to the Fusion reactor. The Fusion reactor processes the two actions and determines the final actions. When actions are received by the CARLA reactor, the simulator applies those actions, advancing to the next frame. The publisher and subscriber implementation of the benchmark in ROS2 follows the same paradigm as in HPRM, replacing reactors with ROS nodes. For synchronization purposes, the Fusion node in ROS2 blocks until it has received updated actions from both the PPO Agent and YOLO, after which it sends the final action to the CARLA node.\nInference latency is measured as the sum of commu- nication time and inference time. We found that running PPO policy inference in CPU and YOLO in GPU led to a slight performance increase due to full utilization of com- pute resources, and was implemented across the benchmark. The box plot in Figure 6 illustrates the inference latency measured when running the CARLA benchmark across 400 environment step frames after 100 warm-up steps with HPRM and ROS2 Humble. To obtain a more accurate mea- sure of inference latency, our benchmarks exclude the time CARLA spends computing physics. When comparing HPRM decentralized coordination with the default ROS2 Humble implementation, the inference latency is reduced by 91.1%. The box plot also shows that HPRM with decentralized and centralized coordination significantly outperforms ROS2 that uses shared memory, achieving latency reductions of 29.2% and 23.6% respectively.\nHPRM with decentralized coordination has the best per- formance with the note that frame rates, slightly outperform- ing HPRM with centralized coordination. The optimizations implemented in this research significantly lowered the IPC overhead, as further demonstrated in real-world application. It's worth noting that the performance gap between HPRM"}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we presented HPRM, a high-performance robotic middleware designed to address the challenges of nondeterminism and high communication latency in intelli- gent autonomous systems. HPRM leverages a centralized and decentralized coordination model to ensure predictable event processing across distributed nodes. We introduced several optimizations in HPRM, including an in-memory object store for efficient zero-copy transfer of large payloads, adaptive serialization to minimize serialization overhead based on data types, and an eager protocol with real-time sockets to reduce handshake latency. Benchmark results showed that HPRM achieved up to 173x lower latency than ROS2 when transmitting large messages to multiple nodes. Furthermore, we validated the real-world applicability of HPRM by in- tegrating it with the CARLA autonomous driving simulator and running deep reinforcement learning agents alongside object detection workloads. In this application, HPRM at- tained a 91.1% lower latency than ROS2. Future work could investigate the scalability of HPRM in larger distributed robotic systems and explore its potential role in enabling efficient and deterministic communication for cloud robotics applications. In conclusion, HPRM represents a significant step forward in the development of deterministic, high- performance robotic middleware."}]}