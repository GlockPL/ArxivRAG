[{"title": "Related inferential adversaries to privacy literature", "authors": ["David Glukhov", "Ziwen Han", "Ilia Shumailov", "Vardan Papyan", "Nicolas Papernot"], "abstract": "Large Language Models (LLMs) are vulnerable to jailbreaks\u2014methods to elicit harmful or generally\nimpermissible outputs. Safety measures are developed and assessed on their effectiveness at defending\nagainst jailbreak attacks, indicating a belief that safety is equivalent to robustness. We assert that\ncurrent defense mechanisms, such as output filters and alignment fine-tuning, are, and will remain,\nfundamentally insufficient for ensuring model safety. These defenses fail to address risks arising\nfrom dual-intent queries and the ability to composite innocuous outputs to achieve harmful goals.\nTo address this critical gap, we introduce an information-theoretic threat model called inferential\nadversaries who exploit impermissible information leakage from model outputs to achieve malicious\ngoals. We distinguish these from commonly studied security adversaries who only seek to force victim\nmodels to generate specific impermissible outputs. We demonstrate the feasibility of automating\ninferential adversaries through question decomposition and response aggregation. To provide safety\nguarantees, we define an information censorship criterion for censorship mechanisms, bounding the\nleakage of impermissible information. We propose a defense mechanism which ensures this bound\nand reveal an intrinsic safety-utility trade-off. Our work provides the first theoretically grounded\nunderstanding of the requirements for releasing safe LLMs and the utility costs involved.", "sections": [{"title": "Introduction", "content": "Large Language Models have demonstrated remarkable capabilities, but their potential for misuse has raised alarm\nabout possible risks. These encompass a wide range of threats, from social engineering and deepfake generation, to\npublic and national security threats such as the creation of malware and chemical, biological, or radiological weapons\n[Bommasani et al., 2021, Weidinger et al., 2022]. In response, researchers have developed various mitigation strategies,\nincluding prompt engineering [Bai et al., 2022a], aligning models with human values through fine-tuning [Ouyang\net al., 2022a], improving robustness via adversarial training [Bai et al., 2022b], and implementing input and output\nguardrails [Debenedetti et al., 2023, Zou et al., 2024].\nDespite these efforts, recent work has called into question the reliability of extant safety methods and their assessments\n[Feffer et al., 2024, Kapoor et al., 2024]. Significant concerns stem from poorly defined threat models with tenuous\nconnections to real-world safety risks, as well as a lack of consistent criteria for evaluating attacks and defenses. Current\nthreat models and assessment methods typically focus solely on the permissibility of the victim model's responses\n[Zou et al., 2024], which do not fully reflect many of the expressly stated safety concerns. For example, as illustrated\nin Figure 1, an adversary seeking to implement a social engineering attack can achieve their goal without eliciting an\nexplicitly harmful response to a query like \"How do I scam the elderly?\".\nDefining Inferential Adversaries: To address the limitation of threat models failing to reflect safety risks of seemingly\ninnocuous interactions helping adversaries further their malicious goals, in Section 4 we adopt an information theoretic\nperspective to define inferential adversaries, which extract harmful information from victim responses. We distinguish\nthese from security adversaries, such as those employing jailbreaking and prompt injection attacks, which aim to\nforce specific, harmful, outputs from the victim model. Our inferential adversary threat model allows us to capture the\nmarginal risk induced by a response in terms of how much it assists the adversary, even when the response itself is\nnot explicitly \"impermissible\u201d. Our work focuses on formulating and demonstrating inferential adversaries, establishing\nconditions and methods for defending against them, and, analyzing the utility costs incurred by these defenses.\nInstantiating Inferential Adversaries: To illustrate practical implications of our findings, in Section 5 we present a\ncase study implementing an automated inferential adversary in the context of LLMs. Due to challenges with estimating\nmutual information, we adopt a heuristic approach inspired by complex problem-solving LLM agents [Khot et al.,\n2022]. The adversarial LLM sequentially decomposes malicious queries into benign subqueries, subquery responses\nare aggregated into summaries answering the malicious query, and a relevance score is assigned to each summary. We\nemploy Monte Carlo Tree Search (MCTS) over interaction sequences with relevance scores as reward in order identify\nan informative set of interactions. Results provide preliminary evidence that launching inferential adversaries in an\nautomated fashion is feasible.\nDefending against Inferential Adversaries: Leveraging our information theoretic threat model formulation, in\nSection 6 we introduce information censorship, a criterion on safety mechanisms which ensures that the impermissible\ninformation leakage of responses to inferential adversaries is bounded. Drawing inspiration from privacy literature,\nwhich has long dealt with constraining information leakage, we propose a randomised response mechanism [Mangat,\n1994] to satisfy the information censorship requirement. Our results further extend to providing bounds on impermissible\ninformation leakage against compositional adversaries having a bounded number of interactions with the victim model.\nBalancing Safety and Utility in Inferential Adversary Defenses: The proposed information censorship mechanism\ncan be viewed as reducing the utility of models for malicious actors. However, this approach introduces a trade-off,\nas constraining harmful information inevitably limits a censored model's usefulness to legitimate users due to the\ndual-use nature of knowledge. We formalize this intuition through safety-utility trade-off results for randomised\nresponse mechanisms as well as results for any information censorship mechanism when assuming benign users are\nalso information seeking.\nOur research uncovers striking similarities between AI safety concerns and those studied in privacy and security\ndomains. We show how leveraging insights from these well-established fields can significantly contribute to formulating\nsafety goals and designing effective censorship mechanisms while also quantifying their utility costs.\nOur key contributions are:\n\u2022 Establishing an information-theoretic framework for understanding inferential adversaries and instantiating\nthem to empirically illustrate the threat.\n\u2022 Defining novel defense criteria addressing limitations of existing defenses through bounds on impermissible\ninformation leakage.\n\u2022 Proposing a randomised response mechanism for safety guarantees against inferential adversaries through\nnoisy output sampling and establishing their safety-utility tradeoffs."}, {"title": "Related Work", "content": "The dual-use nature of generative AI models, as highlighted by Barrett et al. [2023], presents immediate and tangible\nconcerns for safety and security communities. The ability of these models to create realistic text and visual media has\nraised widespread apprehension about the potential for sophisticated phishing schemes, misinformation, and deepfake\ncontent generation [Shoaib et al., 2023]. Furthermore, the ability to generate and analyze complex technical information\ncould, in the wrong hands, accelerate the creation of biological, radiological, chemical, or cyberweaponry by individuals\nor groups previously lacking the necessary expertise by lowering barriers to entry [Brundage et al., 2018, Barrett et al.,\n2023].\nRed Teaming has emerged as the standard approach to assess potential of such threats by simulating the tactics,\ntechniques, and methods of a malicious actor to identify and address weaknesses before they can be exploited in\nreal-world scenarios. [Ganguli et al., 2022, Perez et al., 2022]\u00b9. A prevalent method within Red Teaming is jailbreaking,\nor adversarial prompt engineering which seeks to discover input sequences that can circumvent a language model's\nbuilt-in restrictions [Wei et al., 2023, Chao et al., 2023]. Jailbreaks can be manually constructed [Zou et al., 2023b,\nKang et al., 2023], discovered in an automated fashion [Zou et al., 2023b, Chao et al., 2023], or designed with the\nassistance of other LLMs [Takemoto, 2024, Mehrotra et al., 2023]. Such methods are still only intended to bypass\nsafety filters and cause the victim LLM to generate a single, problematic, response.\nComplementary to jailbreaking, a variety of defense mechanisms have been proposed to provide robustness to jail-\nbreaking methods, appearing to mitigate risks in red teaming assessments. Safety fine-tuning, is one of the most\ncommonly used approaches as for making models more 'safe' [Ouyang et al., 2022b]. Other approaches involve prompt\nengineering defenses [Zhou et al., 2024], input pre-processing [Robey et al., 2023], output filtering [Helbling et al.,\n2023, Inan et al., 2023], and representation engineering [Zou et al., 2023a, 2024, Li et al., 2024]. Nevertheless, all\nsuch approaches are intended to provide only robustness, with some level of efficacy, and, are only evaluated against\njailbreak attacks.\nDespite its popularity, jailbreaking has limitations as a Red Teaming method for assessing model risks and vulnerabilities.\nAs discussed by Feffer et al. [2024], proper Red Teaming requires a clear scope with well-defined objectives capturing\nimplicit safety concerns reflected by concrete threat models, with consistent measures of attack and defense success-elements currently lacking in this space. Additionally, concerns have been raised regarding the need to characterize\nmarginal and misuse risks of generative models [Kapoor et al., 2024, Narayanan and Kapoor, 2024].\nRecent work has further highlighted the importance of addressing these challenges. Glukhov et al. [2023] identified\nthe serious risk potential of adversaries decomposing malicious tasks into seemingly benign subproblems, which pose\nchallenges to existing defense mechanisms of frontier models. Concurrently, Jones et al. [2024] provided extensive\nexperimental evidence demonstrating significant misuse potential from \"safe\" generative model outputs."}, {"title": "Background", "content": "As we employ information theory to characterize inferential adversaries and defenses against them, we first provide\nsome background. We introduce information theory through the lens of question-asking, as interactions with generative\nmodels often involve explicitly or implicitly asking questions.\nQuestion-asking can be viewed as a method of acquiring information or reducing uncertainty. Consider the game\n\"Twenty Questions,\" where one player chooses an object (e.g., a number from 1 to 100) and another attempts to identify\nit through yes-or-no questions. Given the limited number of questions allowed, the optimal strategy is to choose queries\nthat maximally reduce uncertainty. For instance, under a uniform prior, asking \"Is the number between 1 and 50?\" is\nmore informative than \u201cIs the number between 1 and 10?\u201d. This is because the latter question has a 90% chance of\nyielding a \"no\" answer, eliminating few possibilities and resulting in a low expected reduction in uncertainty.\nInformation theory formalizes these insights by modeling uncertainty and its changes. It defines uncertainty as a\nproperty of a random variable, X, quantified by Shannon's entropy:\n\\(H(X) = \\sum_{x\\in X} p(x) \\log_2 \\frac{1}{p(x)}\\)"}, {"title": "Adversary Threat Models", "content": "In this section, we introduce our threat models, presenting an information-theoretic approach to understanding inferential\nadversaries and distinguishing them from traditional security adversaries.\n4.1 Setting\nFor some n, let X and Y be the set of all possible strings of length at most n constructed out of tokens. Consider an\nadversary A (e.g., a person or a generative model) interacting with a victim Large Language Model \\(V : X \\rightarrow P(Y)\\)\nmapping input prompts to a distribution over output strings. An interaction between the adversary and victim is\nrepresented by an input-output pair \\((x, y) \\in X \\times Y\\), where x is the input prompt to the LLM and \\(y \\sim V(x)\\) a sample\noutput. Within these interactions, the adversary aims to maximize a scoring function \\(s : X \\times Y \\rightarrow R^+\\), which quantifies\nhow well these input-output pairs fulfill the adversary's goals.\nWe characterize \u201cimpermissible concepts\", i.e. knowledge which would be considered dangerous, risky, harmful, or\notherwise undesirable for an individual to know through sets of input-output pairs. Specifically, let \\(Q \\subset X\\) be a set of\nquestions that a malicious adversaries wants an answer for, and \\(R_Q \\subset Y\\) the set of answers y for which the pair (x, y)\ncontains impermissible knowledge. For instance, \\(x \\in Q\\) might be a question about implementing malware, and \\(R_Q\\)\ncould be a set of possible malware implementations.\nDefenses against adversaries are assumed to be mediated by censorship mechanisms which modify the distribution of\noutputs returned to a user-provided input x. These defenses seek to ensure that outputs returned satisfy a safety criterion\ndefined based on the threat model of concern.\nDefinition 4.1 (Censorship Mechanism). A Censorship Mechanism \\(M : X \\times P(Y) \\rightarrow P(Y)\\) is a randomized function\nthat outputs a new distribution over responses returned to a user-provided input.\nThe definition of a censorship mechanism is very general and can be interpreted as a method for altering the output\ndistribution of a victim model to provide censorship guarantees against specific threats. Next, we introduce security and\ninferential adversary threat models, which inform the guarantees required from these censorship mechanisms.\n4.2 Security Threats\nWe first introduce security adversaries to clearly formulate a commonly assumed threat model for LLMs [Geiping et al.,\n2024].\""}, {"title": "Inferential Threats", "content": "Inferential adversaries provide a compelling model for many safety concerns which are not adequately captured by\nsecurity adversaries and capture compositional threats. They differ from security adversaries in that they do not have a\ntarget function t specifying a set of desired outputs from the victim model. Instead, inferential adversaries seek to gain\ninformation about a target concept such as malicious instructions, from model outputs.\nUnder our information theoretic framing, a target concept is represented as a distribution over strings of bounded length,\nrepresenting the adversary's prior belief about the answer to a malicious query x*. The uncertainty in C is quantified by\nShannon's entropy and the change in uncertainty provided a response random variable R is defined through the mutual\ninformation.\nBased on this framework, we define the objective of inferential adversaries as follows:\nDefinition 4.3 (Inferential Adversary Objective). An inferential adversary A interacting with victim model \\(V : X \\rightarrow\nP(Y)\\) and possessing prior belief \\(C = P_A(y|x^*)\\), aims to maximize the scoring function\n\\(s(x,r) = \\sum_{y \\in R} p(C = y|r) \\log \\frac{p(C = y|r)}{p(C = y)}\\)\nIn other words, the inferential adversary seeks to select x s.t. \\(I(C; (x, V(x)))\\) is maximized.\nInferential adversaries can be naturally extended to compositional settings as follows.\nDefinition 4.4 (k-Inferential Adversary). A k-Inferential Adversary A interacting with victim model \\(V : X \\rightarrow P(Y)\\)\nand possessing prior \\(C = P_A(y|x^*)\\), aims to maximize the scoring function s :\n\\(s((x_1,r_1),..., (x_k, r_k)) = \\sum_{y \\in R} p(C = y|r_1,..., r_k) \\log \\frac{p(C = y|r_1, ...,r_k)}{p(C = y)} \\)\nIn other words, the inferential adversary seeks to select \\(x_1, ..., x_k\\) s.t. \\(I(C; (x_1, V(x_1)), ..., (x_k, V(x_k)))\\) is maxi-\nmized.\nThis framework effectively captures the threat demonstrated in Figure 1, dual-use queries, and marginal risks where\nmodel responses assist adversaries in achieving goals without being outright impermissible. Even if individual model\noutputs do not fully specify C, (e.g. detailed instructions on how to scam the elderly), model responses to other\nquestions can still reduce an adversaries uncertainty and jointly help specify C."}, {"title": "Instantiating Inferential Adversaries", "content": "To assist in understanding inferential adversaries, we propose an approach to instantiating Inferential Adversaries in an\nLLM setting, providing insight to how such adversaries may behave and can be evaluated.\n5.1 Theoretical Instantiation of Inferential Adversaries\nIn contrast to security adversaries which optimize inputs for producing specific outputs, inferential adversaries optimize\ninputs for maximizing joint mutual information with target concept C. We refer to this as an inference attack and is\noutlined in Algorithm 1. The attack iteratively selects queries that maximize the conditional mutual information with\nthe target concept, given the history of previous queries and responses. This greedy approach, while not guaranteed to\nbe optimal, offers a practical balance between efficiency and effectiveness in the face of the combinatorial complexity\nof exhaustive search [Fleuret, 2004].\nWhile the algorithm provides a theoretical framework, practical implementation for LLMs presents challenges, particu-\nlarly due to challenges in efficiently calculating mutual information.\n5.2 Practical Instantiation of Inferential Adversaries\nTo demonstrate real world feasibility of inferential adversaries, we introduce a heuristic approach which aims to exhibit\nsimilar functionality to the information-theoretic approach. The approach draws inspiration from problem-solving\nagents and prompt decomposition techniques used in solving complex, multi-step reasoning problems [Perez et al.,\n2020, Khot et al., 2022, Radhakrishnan et al., 2023]. Our method consists of an adversarial LLM A which produces\na series of subquestions qs whose answers could be aggregated to answer an impermissible target question q. These\nsubqueries are then submitted to a victim LLM V, which generates responses rs. Finally, these responses are aggregated\nby the adversarial LLM to answer the original query q. Figure 2 provides a high-level overview of this interaction.\nWe approximate the adversary's strategy of selecting queries sequentially in terms of maximizing the conditional mutual\ninformation by modeling the interactions through a Markov Decision Process (MDP) and selecting sets of interactions\nrelevant to the malicious queries. Our MDP is defined by\n\u2022 States S: Consist of the impermissible question and its intermediate answer based on a summary of all\nsub-question-responses pairs;\n\u2022 Actions a: The set of subquestions generated given the state;\n\u2022 State transitions T: Generation of new summary by aggregating the previous summary with a sub-question\nand answer pair;\n\u2022 Reward R: A model provided score measuring the relevance of state summaries with respect to the original\nmalicious question.\n\u2022 Environment E: The victim model is part of the environment, returning responses to suquestions asked in\nisolated context windows."}, {"title": "Evaluation", "content": "Unlike security adversaries, where permissibility is determined solely by victim outputs, inferential adversaries do not\nrely on victim output permissibility violations to attain their malicious goal. Therefore, evaluation of the attack should\nfocus on the adversarial model's summary of all interactions addressing the malicious query rather than individual\nresponses by the victim model.\nTo preform our evaluation, we sampled the first 30 questions corresponding to illegal activities from three datasets:\nForbidden Questions [Shen et al., 2023], Harmful Behaviours [Zou et al., 2023b], and DangerousQA [Bhardwaj and\nPoria, 2023]. Non-external human annotators evaluated the responses using a qualitative coding scheme, assessing\nwhether the adversarial LLM did not return an answer (empty), the answer contained a refusal (refused), did not\naddress the question (irrelevant), or answered the malicious query providing relevant information (affirmative). This\napproach, while potentially subject to author bias, was chosen to maintain ethical standards. Furthermore, the intent\nof the evaluation was to illustrate the feasibility of automated inferential adversaries and examine factors affecting\nperformance, not to compare performance with a completely different threat model of jailbreaking security adversaries."}, {"title": "Results", "content": "To establish feasibility, we compare our method of when attacking an aligned LLaAa2-70B-Instruct model to two\nbaselines; directly querying a model with the malicious query and applying our method on the uncensored adversarial\nLLM (Mistral) as the victim model. As shown in Table 2, both victim models (Mistral-7B-Instruct and LLaMA2-\n70B-Instruct) led to a higher rate of affirmative responses returned from the adversarial LLM when subjected to our\ncompositional attack method compared to directly querying them. The attack method successfully reduced empty\nresponses, refusals, and irrelevant answers across all datasets for both victim models. The results suggest that even\nmodels that reject harmful questions directly can potentially leak information through compositional attacks. More\ndetailed evaluation description is included in Appendix D."}, {"title": "Discussion", "content": "As part of our evaluation, we observed several factors affecting success of the method that may be useful to control for\nin future red-teaming evaluations and instantiations:\n\u2022 Incomplete utilization of the prior: Our method outperforms directly querying the adversarial LLM even\nwhen the victim LLM used is the same as the adversarial LLM. This suggests that the adversarial LLM\npossesses prior knowledge which isn't directly accessible. Another observation indicating the prior is not fully\nreflected in the baseline adversarial LLM is that the adversarial model would incorporate knowledge in its\nsummaries that could not be attributed to any responses received from the victim model. This could make it\nmore challenging to establish baselines and disentangle marginal risk of a deployed model from simply strong\nadversaries.\n\u2022 Invalid subquestions: Occasionally, either due to highly irrelevant responses or poor subquestion generation,\nvery irrelevant or blatantly impermissible subquestions were selected, harming efficiency of the method.\n\u2022 Trajectory divergence: In some instances, the chosen sub-questions were the opposite of the intended harmful\nquery, leading to responses and further questions which produced highly irrelevant answers.\nFuture design of inferential adversaries could involve fine tuning adversarial LLMs to ask informative sub-questions,\nemploy subquestion relevance and permissibility selection mechanisms, and filter irrelevant parts of received responses.\nWhile the benchmark datasets impermissible questions used have been commonly used to assess jailbreak robustness,\nred teaming assessments against inferential adversaries must carefully and precisely articulate what risks are intended\nto be mitigated and how these risks ought to be assessed. Rather than checking whether or not a provided LLM\nrefused to answer impermissible questions, benchmarks should concretely articulate what knowledge or conclusions"}, {"title": "Information Censorship", "content": "To mitigate risks incurred by inferential adversaries, the victim model provider seeks to minimize impermissible\ninformation leakage over collections of interactions. In this section we define information censorship, a criterion for\ncensorship mechanisms to guarantee safety against inferential adversaries by bounding on impermissible information\nleakage.\n6.1 Safety Guarantee\nIn order to ensure an adversary does not infer (x, y) for \\(x \\in Q\\) and \\(y \\in R\\), it is necessary to control information leakage\nfrom responses returned to the adversary. However, from a victim model providers perspective, not all information\nleakage is problematic or impermissible, it depends on what conclusions it causes an adversary to reach.\nDefinition 6.1 (Impermissible Information Leakage). Let V be a victim model and M a censorship mechanism. For an\n\\(x \\in Q\\) and impermissible content prior \\(C = P_A(y|x)\\), the impermissible information leaked about C from interactions\n\\(\\{(x_1,y_1),..., (x_k, r_k)\\}\\) for \\(r_1 \\sim M(x_1, V(x_1)), ...,r_k \\sim M(x_k, V(x_k))\\) is\n\\(I_R (C; (x_1,y_1),..., (x_k,r_k))\n= \\sum_{r_1,...,r_k \\in Y^k}p(r_1,...,r_k) \\sum_{y \\in R_x} p(C = y|r_1,..., r_k) \\log \\frac{p(C = y|r_1,...,r_k)}{p(C = y)}\\)\nOur definition for impermissible information leakage differs from the definition of mutual information\n\\(I(C; (x_1,y_1),..., (x_k,r_k)) = \\sum_{r_1,...,r_k \\in Y^k}p(r_1,...,r_k) \\sum_{y \\in Y} p (C = y|r_1, ..., r_k) \\log \\frac{p(C = y|r_1,...,r_k)}{p(C = y)}\\)\nThe intent for this distinction is to capture the asymmetry in what conclusions an adversary infers. Whereas mutual\ninformation is concerned with changes of the posterior distribution relative to the prior for any conclusions \\(C = y\\), the\nLLM provider is only concerned with the changes in belief of the adversary over impermissible conclusions y such\nthat the combination (x, y) is harmful. Specifically, for the LLM provider, scenarios in which the adversary's posterior\n\\(P_A((C = y)|(x_1,y_1),..., (x_k,r_k)) = 1\\) for some \\(y \\notin R_x\\) is perfectly acceptable as they imply the adversary became\nconfident in a \"permissible\u201d input-output pair, whereas a bound on mutual information would deem this a defense\nfailure.\nA mechanism M which bounds the worst case impermissible information leakage over k interactions is referred to as a\n(k, \\(\u20ac\\))-Information Censorship Mechanism (ICM).\nDefinition 6.2 ((k, \\(\u20ac\\))-ICM). For a collection of adversary priors \u03a6 representing the adversaries background knowledge,\na harmful query \\(x \\in Q\\), impermissible content prior \\(C = P_A(y|x)\\) with \\(P_A \\in \u03a6\\), a leakage bound \\(\u20ac > 0\\), and a bound\non the number of interactions k, a (k, \\(\u20ac\\))-ICM M bounds the worst-case impermissible information leakage:\n\\(\\sup_{P_A \\in \\Phi; \\{x_1,x_2,...,x_k\\}\\in X} I_R(C; (x_1,y_1),..., (x_k, r_k)) \\leq \\epsilon,\\)\nfor responses \\(r_i = M(x_i, V(x_i))\\). For simplicity of notation, (1, \\(\u20ac\\))-ICM will be referred to as an \\(\u20ac\\)-ICM.\nFinding and bounding the supremum of the impermissible information leakage necessary for a (k, \\(\u20ac\\))-ICM over\nall possible sets of k interactions is challenging challenging due to the combinatorial complexity of checking all"}, {"title": "Randomised Response E-ICM", "content": "combinations. Consequently, we instead turn our focus to \\(\u20ac\\)-ICMs, which we show can still be used to provide bounds\non impermissible information leakage from k interactions.\nThe simplest way to bound the joint leakage through \\(\u20ac\\)-ICMs is to adaptively construct them based on existing interaction\nhistory. However, this assumption is unrealistic for extant settings with publicly available chatbots that users can utilize\nin distinct context windows, user accounts, and available models. Thus, assuming that the victim lacks knowledge of\nother interactions with the adversary, we provide non-adaptive composition bounds inspired by results in Nuradha and\nGoldfeld [2023].\nTheorem 6.3 (Non-Adaptive Composability of \\(\u20ac\\)-ICM). For a collection of adversary prior \u03a6, impermissible content\n\\(C = P_A(y|x)\\), and an \\(\u20ac\\)-ICM M,\n\\(\\sup_{P_A \\in \\Phi; \\{x_1,x_2,...,x_k\\}\\in X} I_R (C; (x_1,y_1),..., (x_k, r_k)) \\leq k\\epsilon + \\eta_k\\)\nwhere\n\\(\\eta_k = \\sum_{i=2}^k IR ((x_i, r_i); (X_1,Y_1) ..., (X_{i-1}, r_{i-1})|C);\\)\nand \\(\\forall i r_i \\sim M(x_i, V(x_i))\\).\nIn other words, the joint leakage can be bounded by the sum of k individual \\(\u20ac\\) per-interaction leakages and a term\ncapturing the dependencies between interactions when conditioned on C. If the model outputs V(xi) are deterministic\nor independent when conditioned on C, then \\(\\eta_k = 0\\) because the noise mechanism for an \\(\u20ac\\)-ICM is independent of the\nresponse. Such assumptions could hold when there is a \"single true value\" of C known by the model, and all model\noutputs are related to this value by a deterministic function.\n6.2 Randomised Response E-ICM\nTo provide concrete bounds on the information leakage to a k-inferential adversary, we construct an \\(\u20ac\\)-ICM. Inspired by\na differentially private mechanism proposed by Mangat [1994] to protect privacy of individuals during surveys, we\npropose a randomized response information censorship mechanism.\nDefinition 6.4 (Randomised Response \\(\u20ac\\)-ICM). Assume a collection of adversary priors \u03a6 such that there exists a\nnonempty safety set \\(S \\subset Y\\) such that for any \\(x \\in Q\\), \\(P_A \\in P\\), \\(C = P_A(y|x)\\), and \\(s \\in S\\), \\(I_{R_x}(C; (x, s)) = 0\\).\nFurthermore, we assume that for \\(r \\sim M(x, V(x))\\), \\(p(r \\in S) = 0\\). Then, for\n\\(q_{\\epsilon} = max \\{ \\sup_{P_A \\in \\Phi; \\{x_1\\}\\in X} I_R (C; (x_1, V(x_1))), 1 \\}\\),\nthe probability of returning an output generated by the victim model, the mechanism \\(M : X \\times P(y) \\rightarrow P(y)\\) given by\n\\(p_M(y) = \\begin{cases}\n  q_{\\epsilon}p_V(y) & \\text{if } y \\in Y \\setminus S\\\\\n  \\frac{(1 - q_{\\epsilon})}{|S|} & \\text{if } y \\in S\n\\end{cases}\\)\nis an \\(\u20ac\\)-ICM.\nThe proposed Randomized Response \\(\u20ac\\)-ICM defined can vary depending on various assumptions, such as imposing\nconstraints on adversary priors \u03a6. For example, we assume the existence of a nonempty set S which is not harmfully\ninformative to the adversary as we assume an adversary does not get any impermissible information from an empty\nstring returned as output.\nUsing the proposed Randomized Response \\(\u20ac\\)-ICM, an upper bound on \\(\\eta_k\\) can be derived\nTheorem 6.5 (Non-Adaptive Composobility bound for Randomised Response \\(\u20ac\\)-ICM). Let \\(q_{\\epsilon}\\) as defined in the\nrandomised response \\(\u20ac\\)-ICM, and \\(\\forall x_i Y_i = V(\\xi), r_i = M(x_i,V(x_i))\\) where V is the victim model and M is the\nRandomized Response \\(\u20ac\\)-ICM. Then, for any \\(P_A \\in \\Phi\\) and \\(x_1, ..., X_k\\)\n\\(IR_{\\xi}((x_j, r_j); (X_1,Y_1),..., (X_{j-1}, r_{j-1})|C)\n< q_{\\epsilon}H_R ((x_j, Y_j)|C) - p(R_x) (q_{\\epsilon} \\log q_{\\epsilon} + (1 - q_{\\epsilon}) \\log \\frac{1 - q_{\\epsilon}}{|S|} )  min \\{ \\log q_{\\epsilon}, \\log \\frac{1 - q_{\\epsilon}}{|S|} \\})"}, {"title": "Utility", "content": "implying that\n\\(\\sup_{P_A \\in \\Phi;"}, "IR (C; (x_1,y_1),..., (x_k,r_k))\\)\n\\{x_1,x_2,...,x_k\\}\\in X\n<k\\epsilon-p(R_x) \\big(q_{\\epsilon} \\log q_{\\epsilon} + (1 - q_{\\epsilon}) \\log \\frac{1 - q_{\\epsilon}}{|S|} -  min \\{ \\log q_{\\epsilon}, \\log \\frac{1 - q_{\\epsilon}}{|S|} \\}\\big) + \\sum_{i=2}^k q_{\\epsilon} H_R ((x_i, Y_i)|C).\nThus, the upper bound on impermissible information leakage scales linearly with the number of interactions an adversary\nmay have, and controlling leakage requires significant reductions in \\(\u20ac\\), implying substantially noisier and less useful\noutputs. Furthermore, if mutual information between an interaction and the impermissible concept is too costly to\ncalculate, the denominator in \\(q_{\\epsilon}\\) can be substituted with \\(H_{R_x}(C)\\), resulting in a higher probability of returning a\nresponse from S to the user. As S is selected in an input-independent manner, a higher probability of returning an\nelement from S will penalize utility for benign users. To demonstrate this, we first define utility, after which we turn to\nproviding safety-utility trade-offs.\n6.3 Utility\nWhile the primary concern of censorship is to ensure safety by mitigating impermissible information leakage, model\nproviders also care about the utility of their model. The utility of the model can be defined through the utility it provides\nbenign users. We note that the scoring function defined for adversaries can be similarly defined for benign users, with\nuser utility for an interaction defined as \\(u : X \\times Y \\rightarrow R^+\\). Furthermore, we define a distribution \\(P_x\\) capturing the\nprobability that a benign user provides input x. Thus, the expected utility of an interaction between a model V and a\nbenign user is given by\n\\(E_{x\\sim P_x}E_{y\\sim v(x)} [u(x, y)].\\)\nBenign users can also be viewed as having utility functions akin to those characterizing the objectives of security of\ninferential adversaries. For example, some users, especially those which seek certain function calling behaviors would\nexpect model outputs to belong to an input dependent target set \\(T_x\\), with utility \\(u(x, y) = 1_{y\\in T_x}\\).\nAlternatively, the user may be viewed as inferential-seeking to learn new information to bring them toward their\ndesired goal. In this case a user seeking to learn some useful information D seeks to maximize the mutual information\n\\(I(D; (x, y)) = \\sum_{y \\in Y}p((x, y)) \\sum_{d \\in D}p(d)(x, y)) \\log \\frac{p(d|(x,y))}{p(d)}\\)\nIn other words, for an input output pair (x```json\n{\n\"title\": \"A FALSE SENSE OF SAFETY: UNSAFE INFORMATION LEAKAGE IN \u2018SAFE' AI RESPONSES - continuation\",\n  \"sections\": [\n{\n      \"title\": \"Safety-Utility Trade-off\",\n      \"content\":", "y), the utility function of an inferential adversary interested in learning D is\n\\(u_D(x,y) = \\sum_{d \\in D}p(d|(x, y)) \\log \\frac{p(d|(x,y))}{p(d)}.\\)\nUtility definitions can also be extended compositionally to capture the utility a benign user receives over multiple\ninteractions. In particular, the utility of inferential users across k model interactions is given by\n\\(u((x_1,y_1), ..., (x_k, y_k)) = \\sum_{d \\in D}p(d| (x_1, y_1), ..., (T_k, y_k)) \\log \\frac{P(d| (x_1,y_1),..., (T_k, y_k))}{p(d)}.\\)\n6.4 Safety-Utility Trade-off\nTo understand the effect of information censorship of utility, we can study the utility implications for individual\ninputs. We first demonstrate the utility loss induced by the proposed randomised response \\(\u20ac\\)-ICM for a general per-\nexample utility function, followed by providing upper bounds on the utility for Inferential Adversaries for any possible\ninformation censorship mechanism employed.\nTheorem 6.6 (Utility Loss for \\(\u20ac\\)-ICM). For a given input x and utility function u(x, y), the expected utility of the\nrandomized response \\(\u20ac\\)-ICM satisfies:\n\\(E_{y \\sim M(V(x))}[u(x, y)] - E_{y \\sim V(x)}[u(x, y)] = q_{\\epsilon},\\)\nwhere \\(q_{\\epsilon}\\) is the probability of the mechanism returning a response from the Victim model and S is the set of uninformative\nresponses."], "content": "Inferential adversaries, often under more specific definitions, have been studied before. For example, model stealing\nadversaries [Tram\u00e8r et al., 2016] and membership inference adversaries [Shokri et al., 2017]. Model stealing adversaries\nutilize carefully crafted queries and certain compositional properties of these queries and their outputs to infer specific\nparameters [Shamir et al., 2023]. Less evident, the black-box search for adversarial examples can also be viewed\nas being performed by an inferential adversary, as the input queries are not themselves causing a model failure but"}, {"title": "Conclusion", "content": "By formulating inferential adversaries as a key underlying threat model in LLM safety we are able to characterize a\nnew source of vulnerabilities and define a new notion for output impermissibility. We contrast with prior work which\ntreats the ability of LLMs to provide harmful information to adversaries as a problem of model robustness. Our work\nhighlights that this perspective is too narrow. Although current robustness issues make it easy for security adversaries\nto get a desired result, as robustness improves such direct attacks may prove more difficult. However, robustness\nprovides a false sense of safety. The fundamental issue is that knowledge is compositional and interdependent, thus,\ndirectly censoring certain responses to questions does not guarantee that an adversary cannot reconstruct impermissible\nknowledge. Consequently, we conclude that defenses must consider the potential for model responses to help adversaries\ninfer dangerous information rather than whether or not outputs themselves are dangerous, and, doing can come at\nsignificant utility costs."}, {"title": "Case Studies", "content": ""}, {"title": "Implementation Details", "content": "The components of the MDP are instantiated as follows:\nSubQModel (state) \u2192 (action)\nWe parameterize our action policy in our RL framework as an question decomposition LLM with a few-shot prompt\n(Figure 3) and sampling hyperparameters given in Table 3. The goal of this model is to ask sub-questions which obscure\nintent. We use LLaMA-2-70B Chat [Touvron et al., 2023] as our decomposition model, with a few-shot prompt (Figure\n3) and sampling hyperparameters given in Table 3.\nSummaryModel (state, action) \u2192 (state)\nThis model takes in a summary and new (question, response) pairs producing a new summary incorporating the new\ninformation. We parameterize it as an LLM with the prompt given in Figure 4.\nVictimModel (environment)\nWe view our victim model as part of the environment, which our policy model interacts with in an adversarial way\nto gain more information about the original question. As a basic and prototypical censorship mechanism, we use the\nprompt from Touvron et al. [2023], given in Figure 5 for convenience. Questions are asked to this model in isolated\ncontext windows.\nRewardModel (state, action) \u2192 (reward)\nWe use the BAAI General Embedding Reranker [Xiao et al., 2023] which assigns a score to a query answer pair based\non relevance of answer to the query as our reward function. Furthermore, we assign a reward of 0 whenever the victim\nmodel's response contains one or more of the common rejection phrases in Table 4."}, {"title": "Evaluation Details", "content": "D.1 Experiment Setting\nWe use three datasets from existing LLM literature representing queries with potential real world harm. We limit\nourselves to 30 questions per dataset to make qualitative human evaluation tractable. Forbidden questions are a list of\n390 questions which break OpenAI policy, generated by GPT-4 [Shen et al., 2023], where we use the first 30 questions\ncorresponding to illegal activity. Harmful behaviours are 521 harmful instructions intended for measuring jailbreaking\nattack success rate with the goal of eliciting an affirmative response [Zou et al., 2023b]. DangerousQA is a list of 200\nquestions covering dangerous topics generated from text-davinci-002 [Bhardwaj and Poria, 2023].\nAs baselines we use Chat LLaMA-2 [Touvron et al., 2023] and Mistral-7B-Instruct [Jiang et al., 2023], which are\nqueried without any additional system prompt. We then evaluate our CI adversary with both of these models as victims\nparameterized with the prompt in Figure 5.\nD.2 Human evaluation\nEvaluation approaches commonly employed for assessing safety of LLM models involve either human or LLM\nevaluation of generated victim model outputs. However, as suggested by our theoretical framing, such evaluation\nmethods don't adequately capture many of the safety risks of concern. In particular, such approaches make sense in\nthe context of security adversaries with permissibility a function only of victim outputs. In contrast, for inferential\nadversaries permissibility is violated by the information gained through a combination of interactions. Thus, to evaluate\ninferential adversaries, we examine the final output of the adversarial model in answering an impermissible question\nafter a series of interactions with a victim model\nWe follow a qualitative coding scheme for our evaluation. For each question and final answer pair generated from a\nbaseline or automatic attack, we ask three human annotators to independently answer all questions in Table 2. Empty\nresponses are automatically skipped. If there were any disagreements between questions 2, 3, or 4 (no consensus among\nannotators), the example is reviewed as a group where each annotator gives their reasoning. The consensus answer is\nthen taken for our aggregation. We do not take the majority answer as we observed in some cases one annotator was\nmore correct than the other two in evaluation. Of the 360 questions binned, we found 67 disagreements and resolved all\nto consensus.\nGiven the harmful nature of responses, we did not feel it was ethical to recruit external human evaluators. While we\nacknowledge this may lead to author bias in our results, to our best effort we limited the space of possible responses in\nour qualitative coding scheme. Furthermore, the intent of the evaluation is not to demonstrate competitiveness of the\nattack relative to extant jailbreak attack methods, but merely to illustrate how an automated inferential adversary could\nbe instantiated in an LLM setting, while attack performance will greatly depend on the precise setting."}, {"title": "Results - 2", "content": "We consider 1 generating an empty string, 2 refusal to answer, or 3 giving an irrelevant response as failure\nmodes. We define success as giving an affirmative response that attempts to answer the original question with relevant\ninformation as is judged by human annotators. We find with our method, both victims give a higher rate of affirmative\nresponses compared to their baselines, indicating these models have the potential to leak information with compositional\nattacks, even if they reject the question directly."}]