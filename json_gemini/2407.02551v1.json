{"title": "A FALSE SENSE OF SAFETY: UNSAFE INFORMATION LEAKAGE IN \u2018SAFE' AI RESPONSES", "authors": ["David Glukhov", "Ziwen Han", "Ilia Shumailov", "Vardan Papyan", "Nicolas Papernot"], "abstract": "Large Language Models (LLMs) are vulnerable to jailbreaks\u2014methods to elicit harmful or generally\nimpermissible outputs. Safety measures are developed and assessed on their effectiveness at defending\nagainst jailbreak attacks, indicating a belief that safety is equivalent to robustness. We assert that\ncurrent defense mechanisms, such as output filters and alignment fine-tuning, are, and will remain,\nfundamentally insufficient for ensuring model safety. These defenses fail to address risks arising\nfrom dual-intent queries and the ability to composite innocuous outputs to achieve harmful goals.\nTo address this critical gap, we introduce an information-theoretic threat model called inferential\nadversaries who exploit impermissible information leakage from model outputs to achieve malicious\ngoals. We distinguish these from commonly studied security adversaries who only seek to force victim\nmodels to generate specific impermissible outputs. We demonstrate the feasibility of automating\ninferential adversaries through question decomposition and response aggregation. To provide safety\nguarantees, we define an information censorship criterion for censorship mechanisms, bounding the\nleakage of impermissible information. We propose a defense mechanism which ensures this bound\nand reveal an intrinsic safety-utility trade-off. Our work provides the first theoretically grounded\nunderstanding of the requirements for releasing safe LLMs and the utility costs involved.", "sections": [{"title": "Introduction", "content": "Large Language Models have demonstrated remarkable capabilities, but their potential for misuse has raised alarm\nabout possible risks. These encompass a wide range of threats, from social engineering and deepfake generation, to\npublic and national security threats such as the creation of malware and chemical, biological, or radiological weapons\n[Bommasani et al., 2021, Weidinger et al., 2022]. In response, researchers have developed various mitigation strategies,\nincluding prompt engineering [Bai et al., 2022a], aligning models with human values through fine-tuning [Ouyang\net al., 2022a], improving robustness via adversarial training [Bai et al., 2022b], and implementing input and output\nguardrails [Debenedetti et al., 2023, Zou et al., 2024].\nDespite these efforts, recent work has called into question the reliability of extant safety methods and their assessments\n[Feffer et al., 2024, Kapoor et al., 2024]. Significant concerns stem from poorly defined threat models with tenuous\nconnections to real-world safety risks, as well as a lack of consistent criteria for evaluating attacks and defenses. Current\nthreat models and assessment methods typically focus solely on the permissibility of the victim model's responses\n[Zou et al., 2024], which do not fully reflect many of the expressly stated safety concerns. For example, as illustrated\nin Figure 1, an adversary seeking to implement a social engineering attack can achieve their goal without eliciting an\nexplicitly harmful response to a query like \"How do I scam the elderly?\".\nDefining Inferential Adversaries: To address the limitation of threat models failing to reflect safety risks of seemingly\ninnocuous interactions helping adversaries further their malicious goals, in Section 4 we adopt an information theoretic\nperspective to define inferential adversaries, which extract harmful information from victim responses. We distinguish\nthese from security adversaries, such as those employing jailbreaking and prompt injection attacks, which aim to\nforce specific, harmful, outputs from the victim model. Our inferential adversary threat model allows us to capture the\nmarginal risk induced by a response in terms of how much it assists the adversary, even when the response itself is\nnot explicitly \"impermissible\u201d. Our work focuses on formulating and demonstrating inferential adversaries, establishing\nconditions and methods for defending against them, and, analyzing the utility costs incurred by these defenses.\nInstantiating Inferential Adversaries: To illustrate practical implications of our findings, in Section 5 we present a\ncase study implementing an automated inferential adversary in the context of LLMs. Due to challenges with estimating\nmutual information, we adopt a heuristic approach inspired by complex problem-solving LLM agents [Khot et al.,\n2022]. The adversarial LLM sequentially decomposes malicious queries into benign subqueries, subquery responses\nare aggregated into summaries answering the malicious query, and a relevance score is assigned to each summary. We\nemploy Monte Carlo Tree Search (MCTS) over interaction sequences with relevance scores as reward in order identify\nan informative set of interactions. Results provide preliminary evidence that launching inferential adversaries in an\nautomated fashion is feasible.\nDefending against Inferential Adversaries: Leveraging our information theoretic threat model formulation, in\nSection 6 we introduce information censorship, a criterion on safety mechanisms which ensures that the impermissible\ninformation leakage of responses to inferential adversaries is bounded. Drawing inspiration from privacy literature,\nwhich has long dealt with constraining information leakage, we propose a randomised response mechanism [Mangat,\n1994] to satisfy the information censorship requirement. Our results further extend to providing bounds on impermissible\ninformation leakage against compositional adversaries having a bounded number of interactions with the victim model.\nBalancing Safety and Utility in Inferential Adversary Defenses: The proposed information censorship mechanism\ncan be viewed as reducing the utility of models for malicious actors. However, this approach introduces a trade-off,\nas constraining harmful information inevitably limits a censored model's usefulness to legitimate users due to the\ndual-use nature of knowledge. We formalize this intuition through safety-utility trade-off results for randomised\nresponse mechanisms as well as results for any information censorship mechanism when assuming benign users are\nalso information seeking.\nOur research uncovers striking similarities between AI safety concerns and those studied in privacy and security\ndomains. We show how leveraging insights from these well-established fields can significantly contribute to formulating\nsafety goals and designing effective censorship mechanisms while also quantifying their utility costs.\nOur key contributions are:\n\u2022 Establishing an information-theoretic framework for understanding inferential adversaries and instantiating\nthem to empirically illustrate the threat.\n\u2022 Defining novel defense criteria addressing limitations of existing defenses through bounds on impermissible\ninformation leakage.\n\u2022 Proposing a randomised response mechanism for safety guarantees against inferential adversaries through\nnoisy output sampling and establishing their safety-utility tradeoffs."}, {"title": "Related Work", "content": "The dual-use nature of generative AI models, as highlighted by Barrett et al. [2023], presents immediate and tangible\nconcerns for safety and security communities. The ability of these models to create realistic text and visual media has\nraised widespread apprehension about the potential for sophisticated phishing schemes, misinformation, and deepfake\ncontent generation [Shoaib et al., 2023]. Furthermore, the ability to generate and analyze complex technical information\ncould, in the wrong hands, accelerate the creation of biological, radiological, chemical, or cyberweaponry by individuals\nor groups previously lacking the necessary expertise by lowering barriers to entry [Brundage et al., 2018, Barrett et al.,\n2023].\nRed Teaming has emerged as the standard approach to assess potential of such threats by simulating the tactics,\ntechniques, and methods of a malicious actor to identify and address weaknesses before they can be exploited in\nreal-world scenarios. [Ganguli et al., 2022, Perez et al., 2022]\u00b9. A prevalent method within Red Teaming is jailbreaking,\nor adversarial prompt engineering which seeks to discover input sequences that can circumvent a language model's\nbuilt-in restrictions [Wei et al., 2023, Chao et al., 2023]. Jailbreaks can be manually constructed [Zou et al., 2023b,\nKang et al., 2023], discovered in an automated fashion [Zou et al., 2023b, Chao et al., 2023], or designed with the\nassistance of other LLMs [Takemoto, 2024, Mehrotra et al., 2023]. Such methods are still only intended to bypass\nsafety filters and cause the victim LLM to generate a single, problematic, response.\nComplementary to jailbreaking, a variety of defense mechanisms have been proposed to provide robustness to jail-\nbreaking methods, appearing to mitigate risks in red teaming assessments. Safety fine-tuning, is one of the most\ncommonly used approaches as for making models more 'safe' [Ouyang et al., 2022b]. Other approaches involve prompt\nengineering defenses [Zhou et al., 2024], input pre-processing [Robey et al., 2023], output filtering [Helbling et al.,\n2023, Inan et al., 2023], and representation engineering [Zou et al., 2023a, 2024, Li et al., 2024]. Nevertheless, all\nsuch approaches are intended to provide only robustness, with some level of efficacy, and, are only evaluated against\njailbreak attacks.\nDespite its popularity, jailbreaking has limitations as a Red Teaming method for assessing model risks and vulnerabilities.\nAs discussed by Feffer et al. [2024], proper Red Teaming requires a clear scope with well-defined objectives capturing\nimplicit safety concerns reflected by concrete threat models, with consistent measures of attack and defense success- elements currently lacking in this space. Additionally, concerns have been raised regarding the need to characterize\nmarginal and misuse risks of generative models [Kapoor et al., 2024, Narayanan and Kapoor, 2024].\nRecent work has further highlighted the importance of addressing these challenges. Glukhov et al. [2023] identified\nthe serious risk potential of adversaries decomposing malicious tasks into seemingly benign subproblems, which pose\nchallenges to existing defense mechanisms of frontier models. Concurrently, Jones et al. [2024] provided extensive\nexperimental evidence demonstrating significant misuse potential from \"safe\" generative model outputs."}, {"title": "Background", "content": "As we employ information theory to characterize inferential adversaries and defenses against them, we first provide\nsome background. We introduce information theory through the lens of question-asking, as interactions with generative\nmodels often involve explicitly or implicitly asking questions.\nQuestion-asking can be viewed as a method of acquiring information or reducing uncertainty. Consider the game\n\"Twenty Questions,\" where one player chooses an object (e.g., a number from 1 to 100) and another attempts to identify\nit through yes-or-no questions. Given the limited number of questions allowed, the optimal strategy is to choose queries\nthat maximally reduce uncertainty. For instance, under a uniform prior, asking \"Is the number between 1 and 50?\" is\nmore informative than \u201cIs the number between 1 and 10?\u201d. This is because the latter question has a 90% chance of\nyielding a \"no\" answer, eliminating few possibilities and resulting in a low expected reduction in uncertainty.\nInformation theory formalizes these insights by modeling uncertainty and its changes. It defines uncertainty as a\nproperty of a random variable, X, quantified by Shannon's entropy:\n\\(H(X) = \\sum_{x\\\u0395\u03a7} p(x) \\log_2 \\frac{1}{p(x)}\\)"}, {"title": "Adversary Threat Models", "content": "In this section, we introduce our threat models, presenting an information-theoretic approach to understanding inferential\nadversaries and distinguishing them from traditional security adversaries.\nFor some n, let X and Y be the set of all possible strings of length at most n constructed out of tokens. Consider an\nadversary A (e.g., a person or a generative model) interacting with a victim Large Language Model V : X \u2192 P(Y)\nmapping input prompts to a distribution over output strings. An interaction between the adversary and victim is\nrepresented by an input-output pair (x, y) \u2208 X \u00d7 Y, where x is the input prompt to the LLM and y ~ V(x) a sample\noutput. Within these interactions, the adversary aims to maximize a scoring function s : X \u00d7 Y \u2192 R+, which quantifies\nhow well these input-output pairs fulfill the adversary's goals.\nWe characterize \u201cimpermissible concepts\", i.e. knowledge which would be considered dangerous, risky, harmful, or\notherwise undesirable for an individual to know through sets of input-output pairs. Specifically, let Q C X be a set of\nquestions that a malicious adversaries wants an answer for, and R C Y the set of answers y for which the pair (x, y)\ncontains impermissible knowledge. For instance, x \u2208 Q might be a question about implementing malware, and R\ncould be a set of possible malware implementations.\nDefenses against adversaries are assumed to be mediated by censorship mechanisms which modify the distribution of\noutputs returned to a user-provided input x. These defenses seek to ensure that outputs returned satisfy a safety criterion\ndefined based on the threat model of concern.\nDefinition 4.1 (Censorship Mechanism). A Censorship Mechanism M : X \u00d7 P(Y) \u2192 P(V) is a randomized function\nthat outputs a new distribution over responses returned to a user-provided input.\nThe definition of a censorship mechanism is very general and can be interpreted as a method for altering the output\ndistribution of a victim model to provide censorship guarantees against specific threats. Next, we introduce security and\ninferential adversary threat models, which inform the guarantees required from these censorship mechanisms."}, {"title": "Security Threats", "content": "We first introduce security adversaries to clearly formulate a commonly assumed threat model for LLMs [Geiping et al.,\n2024]."}, {"title": "Inferential Threats", "content": "Inferential adversaries provide a compelling model for many safety concerns which are not adequately captured by\nsecurity adversaries and capture compositional threats. They differ from security adversaries in that they do not have a\ntarget function t specifying a set of desired outputs from the victim model. Instead, inferential adversaries seek to gain\ninformation about a target concept such as malicious instructions, from model outputs.\nUnder our information theoretic framing, a target concept is represented as a distribution over strings of bounded length,\nrepresenting the adversary's prior belief about the answer to a malicious query x*. The uncertainty in C is quantified by\nShannon's entropy and the change in uncertainty provided a response random variable R is defined through the mutual\ninformation.\nBased on this framework, we define the objective of inferential adversaries as follows:\nDefinition 4.3 (Inferential Adversary Objective). An inferential adversary A interacting with victim model V : X \u2192\nP(V) and possessing prior belief C = PA(y|x*), aims to maximize the scoring function\n\\(s(x,r) = \\sum_{y\\\u0395R} p(C = y|r) \\log \\frac{p(C = y|r)}{p(C = y)}\\)\nIn other words, the inferential adversary seeks to select x s.t. I(C; (x, V(x))) is maximized.\nInferential adversaries can be naturally extended to compositional settings as follows.\nDefinition 4.4 (k-Inferential Adversary). A k-Inferential Adversary A interacting with victim model V : X \u2192 P(Y)\nand possessing prior C = Pa(y|x*), aims to maximize the scoring function s :\n\\(s((x_1,r_1),..., (x_k, r_k)) = \\sum_{y\\\u0395R} p(C = y|r_1,...,r_k) \\log \\frac{p(C = y|r_1, ...,r_k)}{p(C = y)}\\)\nIn other words, the inferential adversary seeks to select x1, ..., xk s.t. I(C; (x1, V(x1)), ..., (xk, V(xk))) is maxi-\nmized.\nThis framework effectively captures the threat demonstrated in Figure 1, dual-use queries, and marginal risks where\nmodel responses assist adversaries in achieving goals without being outright impermissible. Even if individual model\noutputs do not fully specify C, (e.g. detailed instructions on how to scam the elderly), model responses to other\nquestions can still reduce an adversaries uncertainty and jointly help specify C."}, {"title": "Instantiating Inferential Adversaries", "content": "To assist in understanding inferential adversaries, we propose an approach to instantiating Inferential Adversaries in an\nLLM setting, providing insight to how such adversaries may behave and can be evaluated."}, {"title": "Theoretical Instantiation of Inferential Adversaries", "content": "In contrast to security adversaries which optimize inputs for producing specific outputs, inferential adversaries optimize\ninputs for maximizing joint mutual information with target concept C. We refer to this as an inference attack and is\noutlined in Algorithm 1. The attack iteratively selects queries that maximize the conditional mutual information with\nthe target concept, given the history of previous queries and responses. This greedy approach, while not guaranteed to\nbe optimal, offers a practical balance between efficiency and effectiveness in the face of the combinatorial complexity\nof exhaustive search [Fleuret, 2004].\nWhile the algorithm provides a theoretical framework, practical implementation for LLMs presents challenges, particu-\nlarly due to challenges in efficiently calculating mutual information."}, {"title": "Practical Instantiation of Inferential Adversaries", "content": "To demonstrate real world feasibility of inferential adversaries, we introduce a heuristic approach which aims to exhibit\nsimilar functionality to the information-theoretic approach. The approach draws inspiration from problem-solving\nagents and prompt decomposition techniques used in solving complex, multi-step reasoning problems [Perez et al.,\n2020, Khot et al., 2022, Radhakrishnan et al., 2023]. Our method consists of an adversarial LLM A which produces\na series of subquestions qs whose answers could be aggregated to answer an impermissible target question q. These\nsubqueries are then submitted to a victim LLM V, which generates responses rs. Finally, these responses are aggregated\nby the adversarial LLM to answer the original query q. Figure 2 provides a high-level overview of this interaction.\nWe approximate the adversary's strategy of selecting queries sequentially in terms of maximizing the conditional mutual\ninformation by modeling the interactions through a Markov Decision Process (MDP) and selecting sets of interactions\nrelevant to the malicious queries. Our MDP is defined by\n\u2022 States S: Consist of the impermissible question and its intermediate answer based on a summary of all\nsub-question-responses pairs;\n\u2022 Actions a: The set of subquestions generated given the state;\n\u2022 State transitions T: Generation of new summary by aggregating the previous summary with a sub-question\nand answer pair;\n\u2022 Reward R: A model provided score measuring the relevance of state summaries with respect to the original\nmalicious question.\n\u2022 Environment E: The victim model is part of the environment, returning responses to suquestions asked in\nisolated context windows."}, {"title": "Evaluation", "content": "Unlike security adversaries, where permissibility is determined solely by victim outputs, inferential adversaries do not\nrely on victim output permissibility violations to attain their malicious goal. Therefore, evaluation of the attack should\nfocus on the adversarial model's summary of all interactions addressing the malicious query rather than individual\nresponses by the victim model.\nTo preform our evaluation, we sampled the first 30 questions corresponding to illegal activities from three datasets:\nForbidden Questions [Shen et al., 2023], Harmful Behaviours [Zou et al., 2023b], and DangerousQA [Bhardwaj and\nPoria, 2023]. Non-external human annotators evaluated the responses using a qualitative coding scheme, assessing\nwhether the adversarial LLM did not return an answer (empty), the answer contained a refusal (refused), did not\naddress the question (irrelevant), or answered the malicious query providing relevant information (affirmative). This\napproach, while potentially subject to author bias, was chosen to maintain ethical standards. Furthermore, the intent\nof the evaluation was to illustrate the feasibility of automated inferential adversaries and examine factors affecting\nperformance, not to compare performance with a completely different threat model of jailbreaking security adversaries."}, {"title": "Results", "content": "To establish feasibility, we compare our method of when attacking an aligned LLaAa2-70B-Instruct model to two\nbaselines; directly querying a model with the malicious query and applying our method on the uncensored adversarial\nLLM (Mistral) as the victim model. As shown in Table 2, both victim models (Mistral-7B-Instruct and LLaMA2-\n70B-Instruct) led to a higher rate of affirmative responses returned from the adversarial LLM when subjected to our\ncompositional attack method compared to directly querying them. The attack method successfully reduced empty\nresponses, refusals, and irrelevant answers across all datasets for both victim models. The results suggest that even\nmodels that reject harmful questions directly can potentially leak information through compositional attacks. More\ndetailed evaluation description is included in Appendix D."}, {"title": "Discussion", "content": "As part of our evaluation, we observed several factors affecting success of the method that may be useful to control for\nin future red-teaming evaluations and instantiations:\n\u2022 Incomplete utilization of the prior: Our method outperforms directly querying the adversarial LLM even\nwhen the victim LLM used is the same as the adversarial LLM. This suggests that the adversarial LLM\npossesses prior knowledge which isn't directly accessible. Another observation indicating the prior is not fully\nreflected in the baseline adversarial LLM is that the adversarial model would incorporate knowledge in its\nsummaries that could not be attributed to any responses received from the victim model. This could make it\nmore challenging to establish baselines and disentangle marginal risk of a deployed model from simply strong\nadversaries.\n\u2022 Invalid subquestions: Occasionally, either due to highly irrelevant responses or poor subquestion generation,\nvery irrelevant or blatantly impermissible subquestions were selected, harming efficiency of the method.\n\u2022 Trajectory divergence: In some instances, the chosen sub-questions were the opposite of the intended harmful\nquery, leading to responses and further questions which produced highly irrelevant answers.\nFuture design of inferential adversaries could involve fine tuning adversarial LLMs to ask informative sub-questions,\nemploy subquestion relevance and permissibility selection mechanisms, and filter irrelevant parts of received responses.\nWhile the benchmark datasets impermissible questions used have been commonly used to assess jailbreak robustness,\nred teaming assessments against inferential adversaries must carefully and precisely articulate what risks are intended\nto be mitigated and how these risks ought to be assessed. Rather than checking whether or not a provided LLM\nrefused to answer impermissible questions, benchmarks should concretely articulate what knowledge or conclusions"}, {"title": "Information Censorship", "content": "To mitigate risks incurred by inferential adversaries, the victim model provider seeks to minimize impermissible\ninformation leakage over collections of interactions. In this section we define information censorship, a criterion for\ncensorship mechanisms to guarantee safety against inferential adversaries by bounding on impermissible information\nleakage."}, {"title": "Safety Guarantee", "content": "In order to ensure an adversary does not infer (x, y) for x \u2208 Q and y \u2208 R, it is necessary to control information leakage\nfrom responses returned to the adversary. However, from a victim model providers perspective, not all information\nleakage is problematic or impermissible, it depends on what conclusions it causes an adversary to reach.\nDefinition 6.1 (Impermissible Information Leakage). Let V be a victim model and M a censorship mechanism. For an\nx \u2208 Q and impermissible content prior C = PA(y|x), the impermissible information leaked about C from interactions\n{(x1, y1),..., (xk, rk)} for r1 ~ M(x1, V(x1)), ..., rk ~ M(xk, V(xk)) is\n\\(I_R (C; (x_1,r_1),..., (x_k,r_k)) = \\sum_{r_1,...,r_k \\\u0395yk}p(r_1,...,r_k) \\sum_{y\\\u0395R_x} p(C = y|r_1,..., r_k) \\log \\frac{p(C = y|r_1,...,r_k)}{p(C = y)}\\)\nOur definition for impermissible information leakage differs from the definition of mutual information\n\\(I(C; (x_1,r_1),..., (x_k,r_k)) = \\sum_{r_1,...,r_k \\\u0395yk}p(r_1,...,r_k) \\sum_{y\\\u0395Y} p (C = y/r_1, ..., r_k) \\log \\frac{p(C = y|r_1,...,r_k)}{p(C = y)}\\)\nThe intent for this distinction is to capture the asymmetry in what conclusions an adversary infers. Whereas mutual\ninformation is concerned with changes of the posterior distribution relative to the prior for any conclusions C = y, the\nLLM provider is only concerned with the changes in belief of the adversary over impermissible conclusions y such\nthat the combination (x, y) is harmful. Specifically, for the LLM provider, scenarios in which the adversary's posterior\nPA((C = y)|(x1, r1),..., (xk, rk)) = 1 for some y \u2209 Rx is perfectly acceptable as they imply the adversary became\nconfident in a \"permissible\u201d input-output pair, whereas a bound on mutual information would deem this a defense\nfailure.\nA mechanism M which bounds the worst case impermissible information leakage over k interactions is referred to as a\n(k, \u20ac)-Information Censorship Mechanism (ICM).\nDefinition 6.2 ((k, \u20ac)-ICM). For a collection of adversary priors \u03a6 representing the adversaries background knowledge,\na harmful query x \u2208 Q, impermissible content prior C = PA(y|x) with P1 \u2208 \u03a6, a leakage bound \u20ac > 0, and a bound\non the number of interactions k, a (k, \u0454)-ICM M bounds the worst-case impermissible information leakage:\n\\(sup_{PA\\\u0395\u03a6;\\{x_1,x_2,...,x_k\\}\u2208X}  I_R(C; (x_1,r_1),..., (x_k, r_k)) \u2264 \u20ac, \\)\nfor responses ri = M(xi, V(xi)). For simplicity of notation, (1, \u20ac)-ICM will be referred to as an e-ICM.\nFinding and bounding the supremum of the impermissible information leakage necessary for a (k, \u20ac)-ICM over\nall possible sets of k interactions is challenging challenging due to the combinatorial complexity of checking all"}, {"title": "Randomised Response E-ICM", "content": "To provide concrete bounds on the information leakage to a k-inferential adversary, we construct an e-ICM. Inspired by\na differentially private mechanism proposed by Mangat [1994] to protect privacy of individuals during surveys, we\npropose a randomized response information censorship mechanism.\nDefinition 6.4 (Randomised Response \u20ac-ICM). Assume a collection of adversary priors \u03a6 such that there exists a\nnonempty safety set S C Y such that for any x \u2208 Q, Pa \u2208 P, C = PA(y|x), and s \u2208 S, IRx(C; (x, s)) = 0.\nFurthermore, we assume that for r ~ M(x, V(x)), p(r \u2208 S) = 0. Then, for\n\\(q_\\epsilon = max \\{ sup_{P_A\\\u0395\u03a6; \\{x_1\\}\u0395X } I_R (C; (x_1, V(x_1))), 1\\},\\)\nthe probability of returning an output generated by the victim model, the mechanism M : X \u00d7 P(y) \u2192 P(y) given by\n\\(p_M (y) = \\begin{cases} q_\\epsilon p_V (y)  \\text{ if } y \u2208 Y \\backslash S \\\\ (1-q_\\epsilon) \\frac{1}{\\vert S\\vert} \\text{ if } y \u2208 S \\end{cases}\\)\nis an e-ICM.\nThe proposed Randomized Response \u20ac-ICM defined can vary depending on various assumptions, such as imposing\nconstraints on adversary priors \u03a6. For example, we assume the existence of a nonempty set S which is not harmfully\ninformative to the adversary as we assume an adversary does not get any impermissible information from an empty\nstring returned as output.\nUsing the proposed Randomized Response \u20ac-ICM, an upper bound on \u03b7k can be derived\nTheorem 6.5 (Non-Adaptive Composobility bound for Randomised Response e-ICM). Let qe as defined in the\nrandomised response e-ICM, and \u2200xi Yi = V(\u00a7)), ri = M(xi, V(xi)) where V is the victim model and M is the\nRandomized Response e-ICM. Then, for any PA \u2208 \u03a6 and x1, ..., Xk\n\\(IR_{\\eta}((x_j, r_j); (x_1, Y_1),..., (x_{j-1}, r_{j-1})|C)\\\\\\\\<q_\\epsilon H_R ((x_j, Y_j)|C) - p(R_x) (q_\\epsilon \\log q_\\epsilon + (1 - q_\\epsilon) \\log \\frac{1-q_\\epsilon}{\\vert S\\vert}- min (\\log q_\\epsilon, \\log \\frac{1-q_\\epsilon}{\\vert S\\vert}))\\)"}, {"title": "Utility", "content": "While the primary concern of censorship is to ensure safety by mitigating impermissible information leakage", "u": "X \u00d7 Y \u2192 R+. Furthermore"}, {"title": "Safety-Utility Trade-off", "content": "To understand the effect of information censorship of utility, we can study the utility implications for individual\ninputs. We first demonstrate the utility loss induced by the proposed randomised response e-ICM for a general per- example utility function, followed by providing upper bounds on the utility for Inferential Adversaries for any possible\ninformation censorship mechanism employed.\nTheorem 6.6 (Utility Loss for e-ICM). For a given input x and utility function u(x, y), the expected utility of the\nrandomized response e-ICM satisfies:\n\\(E_{y \\~ M(V(x))}[u(x, y)]  = q_\\epsilon,\\)\nwhere qe is the probability of the mechanism returning a response from the Victim model and S is the set of uninformative\nresponses."}, {"title": "Discussion and Conclusion", "content": "Inferential adversaries, often under more specific definitions, have been studied before. For example, model stealing\nadversaries [Tram\u00e8r et al., 2016] and membership inference adversaries [Shokri et al., 2017]. Model stealing adversaries\nutilize carefully crafted queries and certain compositional properties of these queries and their outputs to infer specific\nparameters [Shamir et al., 2023]. Less evident, the black-box search for adversarial examples can also be viewed\nas being performed by an inferential adversary, as the input queries are not themselves causing a model failure but"}, {"title": "Future Work", "content": "An important direction for future work in making inferential adversaries and information censorship more practical is\nthe development of methods for mutual information approximation in the context of LLMs. One possible approach for\nthis is to build build off of recent work on uncertainty quantification methods that account of semantic similarity of\nresponses [Kuhn et al., 2023, Nikitin et al., 2024]. Another major question is understanding how and when problems\nought to be decomposed and a theoretical understanding of decomposition as opposed to simply searching for inputs\nthat maximize information over a potentially immense set. Development of such methods would also lead to methods\nfor improving utility for inferential users and development of more agentic models.\nFurther improvements to censorship mechanisms could arise either through stronger restrictions and assumptions on the\nadversary priors \u03a6, or through identifying information bottlenecks, that is, specific pieces of information without which\nan adversary cannot infer harmful information and does not have access to it in their prior. As such, Red-Teaming\nassessments of inferential adversaries must be very carefully formulated, with clear characterizations of impermissible\nconcepts, assumptions on adversary background, and methods for assessing information gain."}, {"title": "Conclusion", "content": "By formulating inferential adversaries as a key underlying threat model in LLM safety we are able to characterize a\nnew source of vulnerabilities and define a new notion for output impermissibility. We contrast with prior work which\ntreats the ability of LLMs to provide harmful information to adversaries as a problem of model robustness. Our work\nhighlights that this perspective is too narrow. Although current robustness issues make it easy for security adversaries\nto get a desired result, as robustness improves such direct attacks may prove more difficult. However, robustness\nprovides a false sense of safety. The fundamental issue is that knowledge is compositional and interdependent, thus,\ndirectly censoring certain responses to questions does not guarantee that an adversary cannot reconstruct impermissible\nknowledge. Consequently, we conclude that defenses must consider the potential for model responses to help adversaries\ninfer dangerous information rather than whether or not outputs themselves are dangerous, and, doing can come at\nsignificant utility costs."}]}