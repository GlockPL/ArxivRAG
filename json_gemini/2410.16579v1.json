{"title": "Conflict-Aware Adversarial Training", "authors": ["Zhiyu Xue", "Haohan Wang", "Yao Qin", "Ramtin Pedarsani"], "abstract": "Adversarial training is the most effective method to obtain adversarial robustness for deep neural networks by directly involving adversarial samples in the training procedure. To obtain an accurate and robust model, the weighted-average method is applied to optimize standard loss and adversarial loss simultaneously. In this paper, we argue that the weighted-average method does not provide the best tradeoff for the standard performance and adversarial robustness. We argue that the failure of the weighted-average method is due to the conflict between the gradients derived from standard and adversarial loss, and further demonstrate such a conflict increases with attack budget theoretically and practically. To alleviate this problem, we propose a new trade-off paradigm for adversarial training with a conflict-aware factor for the convex combination of standard and adversarial loss, named Conflict-Aware Adversarial Training (CA-AT). Comprehensive experimental results show that CA-AT consistently offers a superior trade-off between standard performance and adversarial robustness under the settings of adversarial training from scratch and parameter-efficient finetuning.", "sections": [{"title": "1 Introduction", "content": "Deep learning models have achieved exemplary performance across diverse application domains [He et al., 2017, Vaswani et al., 2017, Ouyang et al., 2022, Rombach et al., 2022, Radford et al., 2021]. However, they remain vulnerable to adversarial samples produced by adversarial attacks [Goodfellow et al., 2014, Liu et al., 2016, Moosavi-Dezfooli et al., 2016]. Deep learning models can easily be fooled into making mistakes by adding an imperceptible noise produced by adversarial attacks to the standard sample. To solve this problem, many methods have been proposed to improve the robustness against adversarial samples [Cai et al., 2018, Chakraborty et al., 2018, Madry et al., 2018], among which adversarial training (AT) has been proven to be the most effective strategy [Madry et al., 2018, Athalye et al., 2018, Qian et al., 2022, Bai et al., 2021]. Specifically, AT aims to enhance model robustness by directly involving adversarial samples during training. They used adversarial examples to construct the adversarial loss functions for parameter optimization. The adversarial loss can be formulated as a min-max optimization objective, where the adversarial samples are generated by the inner maximization, and the model parameters are optimized by the outer minimization to reduce the empirical risk for adversarial samples.\nThe trade-off between standard and adversarial accuracy is a key factor for the real-world applications of AT [Tsipras et al., 2018, Balaji et al., 2019, Yang et al., 2020b, Stutz et al., 2019, Zhang et al., 2019]. Although AT can improve robustness against adversarial samples, it also undermines the performance on standard samples. Existing AT methods [Madry et al., 2018, Cai et al., 2018, Zhang et al., 2019, Wang et al., 2019] design a hybrid loss by combining standard loss and an adversarial loss linearly, where the linear coefficient typically serves as the trade-off factor.\nIn this paper, we argue that linearly weighted-average method for AT, as well as the Vanilla AT, cannot achieve a 'near-optimal' trade-off. In other words, it fails to approximately achieve the Pareto optimal points on the Pareto front of standard and adversarial accuracies. We find that the conflict between the"}, {"title": "2 Related Works", "content": "Adversarial Training. Adversarial training (AT) is now broadly considered as the most effective method to achieve adversarial robustness for deep learning models [Qian et al., 2022, Singh et al., 2024]. The key idea"}, {"title": "3 Gradient Conflict in AT", "content": "In this section, we will discuss the occurrence of gradient conflict in AT via a synthetic dataset and real-world datasets such as CIFAR10 and CIFAR100. Additionally, we demonstrate such a conflict will become more serious with the increase of the attack budget theoretically and practically."}, {"title": "3.1 Preliminaries & Notations", "content": "Considering a set of images, each image $x \\in \\mathbb{R}^d$ and its label $y \\in \\mathbb{R}^1$ is drawn i.i.d. from distribution $\\mathcal{D}$. The classifier $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^1$ parameterized by $\\theta$ aims to map an input image to the probabilities of the classification task. The objective of AT is to ensure that $f$ does not only perform well on $x$, but also manifests robustness against adversarial perturbation $\\epsilon$ bounded by attack budget $\\delta$ as $|\\epsilon||_p \\leq \\delta$, where $p$ determinates the $L_p$ norm constraint on the perturbations $\\epsilon$ commonly taking on the values of $\\infty$ or 2. The perturbation $\\epsilon$ can be defined as $\\epsilon = \\arg \\max_{\\|\\epsilon\\|_p\\leq\\delta} L(x + \\epsilon, y; \\theta)$, which can be approximated by gradient-based adversarial attacks such as PGD. Throughout the remaining part of this paper, we refer to $x$ as the standard sample and $x + \\epsilon$ as the"}, {"title": "3.2 Theoretical & Experimental Support for Motivation", "content": "We introduce Theorem 1 that demonstrates $\\mu$ can be bounded by the input dimension $d$ and perturbation budget $\\delta$ in AT.\nTheorem 1. Consider the gradient conflict $\\mu = ||g_c||_2 \\cdot ||g_a||_2 \\cdot (1 - \\cos(g_c, g_a))$ and suppose that the input $x$ is a d-dimensional vector.\n1. Given the $L_2$ restriction for $\\epsilon$ as $|\\epsilon||_2 \\leq \\delta$, we have $\\mu \\leq O(\\delta^2)$.\n2. Given the $L_\\infty$ restriction for $\\epsilon$ as $|\\epsilon||_\\infty < \\delta$, we have $\\mu < O(d^2\\delta^2)$.\nThe intuitive understanding of Theorem 1 is that with the increasing attack budget $\\delta$, the adversarial samples in AT will move further from the distribution $\\mathcal{D}$ of standard samples. The conflict between $g_a$ and $g_c$ will become more serious, and $L_a$ and $L_c$ will be hard to converge. Therefore, the upper bound of $\\mu$ will become larger. The proof of Theorem 1 will be shown in the appendix.\nSynthetic Experiment. In order to show the implications of Theorem 1 empirically, we introduce the synthetic experiment as a binary classification task by selecting digit one and digit two from MNIST with a resolution of 32 \u00d7 32, and train a logistic regression model parameterized by $w \\in \\mathbb{R}^{(32\\times32)\\times2}$ via BCE loss by vanilla AT for 20 epochs, where $\\epsilon$ is contained by its $L_\\infty$ norm as $|\\epsilon||_\\infty < \\delta$, and $\\lambda = 0.5$ serves as the trade-off factor between standard and adversarial loss. Compared to the experiments on real-world datasets, this synthetic experiment offers a distinct advantage in terms of the ability to analytically solve the inner maximization. For real-world datasets, only numerical solutions can be derived using gradient-based attacks (e.g. PGD) during AT. These numerical solutions sometimes are not promising due to gradient masking [Athalye et al., 2018, Papernot et al., 2017]. On the contrary, our synthetic experiments can ensure a"}, {"title": "4 Methodology", "content": "As we mentioned in Section 3, the trade-off between standard and adversarial accuracy is profoundly influenced by the gradient conflict $\\mu$ (Fig. 2). The vanilla AT, which employs a linear trade-off factor $\\lambda$ to combine clean and adversarial loss (as seen in Eq. (2)), does not adequately address the issue of gradient conflict.\nBased on this observation, we introduce Conflict-aware Adversarial Training (CA-AT) as a new trade-off paradigm for AT. The motivation of CA-AT is that the gradient conflict in AT can be alleviated by generally conducting operations on the adversarial gradient $g_a$ and the standard gradient $g_c$ during the training process, and such an operation should guarantee the standard accuracy because its priority is higher adversarial accuracy. Inspired by existing works related to gradient operation Yu et al. [2020,?], Liu et al. [2021a], Chaudhry et al. [2018], Mansilla et al. [2021], CA-AT employs a pre-defined trade-off factor $\\gamma$ as the goal of cosine similarity between $g_c$ and $g_a$. In each iteration, instead of updating parameter $\\theta$ by linearly weighted-averaged gradient $g_\\theta$, CA-AT utilizes $g_*$ to update $\\theta$ as Eq. (4)\n$g_* = \\begin{cases}\n g_a + \\frac{||g_a||_2(\\gamma\\sqrt{1-\\phi^2} - \\phi\\sqrt{1-\\gamma^2})}{||g_c||_2(1-\\phi^2)}g_c, & \\phi < \\gamma\\\\\n g_c, & \\phi \\geq \\gamma\n\\end{cases}$\nwhere $\\phi = \\cos(g_a, g_c)$ is the cosine similarity between standard gradient $g_c$ and adversarial gradient $g_a$. The intuitive explanation of Eq. (4) is depicted in Fig. 1. For each optimization iteration, if $\\phi$ is less than $\\gamma$, then $g_*$ is produced by projecting $g_a$ onto the cone of $g_c$ at an angle $\\arccos(\\gamma)$. If $\\phi > \\gamma$, we will use the standard gradient $g_c$ to optimize $\\theta$, because we need to guarantee standard accuracy when the conflict is not quite serious.\nThe mechanism behind Eq. (4) is straightforward. It mitigates the gradient conflict in AT by ensuring that $g_c$ is consistently projected in a direction close to $g_a$. Considering an extreme case that $g_c$ and $g_a$ are diametrically opposite ($g_a = -g_c$), in such a scenario, if we produce the gradient by Vanilla AT as $g_\\theta = g_c + g_c$, $g_\\theta$ will be a zero vector and the optimization process will be stuck. On the other hand, $g_*$ will align closely to $g_c$ within $\\gamma$, avoiding $\\theta$ to be stuck in a suboptimal point.\nFurthermore, under the condition of $\\phi < \\gamma$, we find that CA-AT can also be viewed as a convex combination for standard and adversarial loss with a conflict-aware trade-off factor $\\lambda^*$ as $L = L_c + \\lambda^* L_a$,"}, {"title": "5 Experimental Results & Analysis", "content": "In this section, we demonstrate the effectiveness of CA-AT for achieving better trade-off results compared to Vanilla AT. We conduct experiments on adversarial training from scratch and adversarial PEFT among various datasets and model architectures. Besides, motivated by Theorem 1, we evaluate CA-AT by involving adversarial samples with a larger budget in training. Experimental results show that CA-AT can boost the model's robustness by handling adversarial samples with a larger budget, while Vanilla AT fails."}, {"title": "5.1 Experimental Setup", "content": "Datasets and Models. We evaluate our proposed method on various image classification datasets including CIFIAR10 [Krizhevsky et al., 2009], CIFIAR100 [Krizhevsky et al., 2009], CUB-Bird [Wah et al., 2011], and StanfordDogs [Khosla et al., 2011]. The model architectures we utilized to train from scratch on CIFAR10 and CIFAR100 are ResNet18, ResNet34 [He et al., 2016], and WideResNet28-10 (WRN-28-10) [Zagoruyko and Komodakis, 2016]. We set the value of running mean and running variance in each Batch Normalization block"}, {"title": "5.2 Experimental Results on PEFT", "content": "CA-AT offers the better trade-off on adversarial PEFT. Fig. 4 shows the SA-AA fronts on fine-tuning robust pretrained Swin Transformer on CUB-Bird and StanfordDogs by using Adapter. We set $\\lambda = [0, 0.5, 1]$ for Vanilla AT and $\\gamma = [0.8, 0.9, 1]$ for CA-AT. The red data points for CA-AT are positioned in the upper right area relative to the blue points for Vanilla AT. It shows that CA-AT can consistently attain better standard and adversarial accuracy compared to the Vanilla AT across different datasets. Besides, we observed that on fine-grained datasets such as CUB-Bird and Stanford Dogs, the superiority of CA-AT is more significant compared to the results on normal datasets.\nResults for CA-AT with Different Pretrained Models. Fig. 5 shows that CA-AT can also boost the trade-off performance on ViT. The main difference between these two models is that, ViT treats image patches as tokens and processes them with a standard transformer architecture Vaswani et al. [2017], while Swin-T uses shifted windows for hierarchical feature merging. While ViT applies global attention directly on image patches, Swin Transformer applies local attention within windows and uses a hierarchical approach to better handle larger and more detailed images. The superiority of CA-AT on ViT is not as significant as it is on Swin-T (Fig. 4b), but it still can gain better standard and adversarial accuracy compared to Vanilla AT."}, {"title": "5.3 Experimental Results on Training from Scratch", "content": "CA-AT results in better trade-off with different adversarial loss functions. Fig. 6a visualizes SA-AA fonts from experiments using vanilla AT with $\\lambda = [0,0.25, 0.5, 0.75, 1]$ and CA-AT with $\\gamma = [0.7, 0.75, 0.8, 0.85, 0.9, 1]$ on CIFAR10. In this figure, most orange data points (CA-AT) lie in the upper right space of blue points (Vanilla AT), indicating that CA-AT offers a better empirical Pareto front for the trade-off between standard accuracy and adversarial accuracy. Moreover, Fig. 6c and Fig. 6b show CA-AT can also consistently boost the adversarial accuracy for different adversarial loss functions used in AT such as TRADES [Zhang et al., 2019] and CLP [Kannan et al., 2018]. For the experiments on CIFAR100, we selected the strongest and most representative attack methods to evaluate the model's robustness, including targeted attack (T-FAB), untargeted attacks (PGD, MIFGSM), $L_2$-norm attack (T-PGD), and ensemble at-tack (AutoAttack). Showing the trade-off results on CIFAR100 in Fig. 7a (ResNet18) and Fig. 7b (ResNet34), the performance gain of CA-AT is more limited compared to the one on CIFAR10, but it can still achieve better performance on standard accuracy and adversarial accuracy against various adversarial attacks.\nCA-AT is more robust to adversarial attacks with larger budget values. We evaluate adversarial precision through various adversarial attacks with different attack budget values $\\delta$, to demonstrate the superiority of our model over Vanilla AT under various intensities of adversarial attacks. We applied both Vanilla AT and CA-AT to ResNet18 on CIFAR10, and the results about $L_\\infty$-based attacks are shown in Table 1. In Table 1, although our CA-AT achieves slightly lower adversarial accuracy against FAB when $\\delta$ is larger than 8/255, it outperforms the Vanilla AT in both standard accuracy and adversarial accuracy on any other attack methods (e.g. AutoPGD, MIFGSM, and T-FAB) with different budget $\\delta$. It clearly illustrates that, compared to Vanilla AT, CA-AT can enhance the model's adversarial robustness ability to resist stronger adversarial attacks with larger budget $\\delta$.\nCA-AT enables AT via stronger adversarial examples. In our toy experiment (Fig. 2) and Theorem 1, the conflict $\\mu$ would be more serious if we utilize adversarial examples with larger attack budget $\\delta$ during AT. It implies that Vanilla AT cannot handle stronger adversarial examples during training because of the gradient conflict. In Fig. 8, we visualize the results of training ResNet34 on CIFAR10 with adversarial samples produced by the same attack method (PGD), but different attack budgets ($\\delta = 8/255$ and $\\delta = 16/255$), and evaluate the adversarial accuracies against various adversarial methods (e.g. FGSM and PGD) with different budgets (x-axis). Compared to the blue and orange curves (Vanilla AT with $\\delta = 8/255$), it shows that Vanilla AT fails when training with the adversarial attack with a higher perturbation bound, causing a decrease in both standard and adversarial accuracy. On the contrary, CA-AT, shown as the green and red curves, can improve both standard and adversarial accuracy by involving stronger adversarial samples with larger attack budgets."}, {"title": "6 Conclusion & Outlook", "content": "In this work, we illustrate that the weighted-average method in AT is not capable of achieving the 'near-optimal' trade-off between standard and adversarial accuracy due to the gradient conflict existing in the training process. We demonstrate the existence of such a gradient conflict and its relation to the attack budget of adversarial samples used in AT practically and theoretically. Based on this phenomenon, we propose a new trade-off framework for AT called Conflict-Aware Adversarial Training (CA-AT) to alleviate the conflict by gradient operation, as well as applying a conflict-aware trade-off factor to the convex combination of standard and adversarial loss functions. Extensive results demonstrate the effectiveness of CA-AT for gaining trade-off results under the setting of training from scratch and PEFT.\nFor future work, we plan to undertake a more detailed exploration of the gradient conflict phenomenon in AT from the data-centric perspective. We hold the assumption that some training samples can cause serious gradient conflict, while others do not. We will evaluate this assumption in the future work, and intend to reveal the influence of training samples causing gradient conflict."}, {"title": "A Proof of Theorem 1", "content": "Considering $|\\epsilon||_2$ or $|\\epsilon||_\\infty$ is usually very small for adversarial examples, we utilize Taylor Expansion for x as the approximation for the adversarial loss $L(x + \\epsilon; \\theta)$, such that:\n$L(x + \\epsilon;\\theta) = L(x; \\theta) + (\\frac{\\partial L(x;\\theta)}{\\partial x})^T\\epsilon + O (||\\epsilon||^2)$\nTo derive an upper bound on the gradient conflict in the regime that $|\\epsilon||$ gets small, we will only consider the first-order term above. We then take the derivative of both sides of the equation with respect to $\\theta$ to obtain:\n$g_a = \\frac{\\partial}{\\partial \\theta} (\\frac{\\partial L(x; \\theta)}{\\partial x})^T\\epsilon = g_c + \\frac{\\partial}{\\partial x} (\\frac{\\partial g_c}{\\partial \\theta})^T\\epsilon = g_c + H\\epsilon$\nwhere $H = \\frac{\\partial}{\\partial x} \\frac{\\partial g_c}{\\partial \\theta} \\in \\mathbb{R}^{d_\\theta \\times d \\times d_\\theta/d_x}$, denotes the dimension of parameter $\\theta$ and input data x. By multiplying $g_a^T$ and $g_c^T$ on the two sides of Eq. (6), respectively, we can obtain Eq. (7) and Eq. (8) as follows.\n$g_c^Tg_a = ||g_c||_2^2 + g_c^TH\\epsilon$\n$||g_a||_2^2 = g_a^Tg_c + g_a^TH\\epsilon$\nEq. (7) minus Eq. (8):\n$g_c^Tg_a = \\frac{||g_a||_2^2 + ||g_c||_2^2}{2} + \\frac{\\epsilon^T H^T(g_c - g_a)}{2}$\nBased on Eq. (6), we can replace $(g_c - g_a)$ as H$\\epsilon$:\n$g_c^Tg_a = \\frac{||g_a||_2^2 + ||g_c||_2^2}{2} - \\frac{\\epsilon^T H^TH \\epsilon}{2}$\nRecall the definition of $\\mu$ as $\\mu = ||g_c||_2 \\cdot ||g_a||_2\\cdot (1 - \\cos(g_c, g_a))$\n$\\mu = ||g_c||_2 \\cdot ||g_a||_2\\cdot (1 - \\cos(g_c, g_a))\n= ||g_c||_2 \\cdot ||g_a||_2 - g_c^T g_a\n= ||g_c||_2||g_a||_2 - \\frac{||g_a||_2^2 - ||g_c||_2^2}{2} + \\frac{\\epsilon^T H^TH \\epsilon}{2} (Use Eq. (10))\n= \\frac{\\epsilon^T K(\\theta, x)\\epsilon}{2} - \\frac{(||g_c||_2||g_a||_2)^2}{2} < \\frac{\\epsilon^T K(\\theta, x)\\epsilon}{\\lambda_{max} \\epsilon \\epsilon}\nwhere K($\\theta$, x) = $H^TH$ is a symmetric and positive semi-definite matrix, and $\\lambda_{max}$ is the largest eigenvalue of K, where $\\lambda_{max} \\geq 0$.\nConsidering two widely-used restrictions for perturbation $\\epsilon$ applied in adversarial examples as $l_2$ and $l_\\infty$ norm, we have:\n* For $|\\epsilon||_2 \\leq \\delta$, where $\\mu \\leq \\frac{1}{2} \\lambda_{max} d^2$. The upper bound of $\\mu$ is $O(\\delta^2)$.\n* For $|\\epsilon||_\\infty < \\delta$, it implies that the absolute value of each element of $\\epsilon$ is bounded by $\\delta$, where $\\epsilon^T \\epsilon = \\Sigma_0^d \\epsilon_i^2 < d^2\\delta^2$. The upper bound of $\\mu$ is $O(d^2\\delta^2)$."}, {"title": "B Analytical Solution for the Inner Maximization", "content": "We introduce the details about how to get the analytical inner-max solution (Eq. (3)) for our synthetic experiment presented in Section 3. As we introduced in Section 3, consider a linear model as $f(x) = w^Tx+b$ under a binary classification task where $y \\in {+1, -1}$. The predicted probability of sample x with respect to its ground truth y can be defined as:\n$p(y|x) = \\frac{1}{1 + exp(-y \\cdot f(x))}$\nThen, the BCE loss function for sample x can be formulated as:\n$L(f(x), y) = -log(p(y|x)) = log(1 + exp(-y \\cdot f(x)))$\nConsider the perturbation $\\epsilon$ under the restriction of $L_\\infty$ norm, the adversarial attack for such a linear model can be formulated as an inner maximization problem as Eq. (14).\n$max log(1+exp(-y\\cdot f(x + \\epsilon))) = min y w^T\\epsilon$\n||$\\epsilon$||_\\infty \\leq \\delta\nConsider the case that y = +1, where the $L_\\infty$ norm says that each element in $\\epsilon$ must have magnitude less than or equal $\\delta$, we clearly minimize this quantity when we set $\\epsilon_i = -\\delta$ for $w_i > 0$ and $\\epsilon_i = \\delta$ for $w_i < 0$. For y = -1, we would just flip these quantities. That is, the optimal solution $\\epsilon^*$ to the above optimization problem for the $L_\\infty$ norm is expressed as Eq. (15).\n$\\epsilon^* = -y \\cdot \\delta \\cdot sign(w)$\nwhere $\\cdot$ is the element-wise multiplication. Based on Eq. (15), we can formulate the adversarial loss as follows, which is as same as the adversarial loss presented in Eq. (3).\n$L(f(x + \\epsilon^*), y) = log(1 + exp(-y \\cdot w^Tx - y \\cdot b - y \\cdot w^T\\epsilon^*))\n= log(1 + exp(-y \\cdot f(x) + \\delta ||w||_1))$"}, {"title": "C Additional Experimental Results", "content": "Experimental setup on adversarial PEFT. For the experiments on adversarial PEFT, we leverage the adversarially pretrained Swin-T and ViT downloaded from ARES\u00b9. For adapter, we implement it as Pfeiffer et al. [2020] by inserting an adapter module subsequent to the MLP block at each layer with a reduction factor of 8.\nThe effect of inner maximization solver in AT. In Table 2, we conduct the ablation study for using different attack methods to generate adversarial samples during adversarial training from scratch. We find that PGD-DLR can achieve higher adversarial accuracies when $\\gamma = 0.9$ but lead them worse when $\\gamma = 0.8$"}]}