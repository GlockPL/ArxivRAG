{"title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling", "authors": ["Ali Safaya", "Deniz Yuret"], "abstract": "This paper introduces Neurocache, an approach to extend the effective context size of large language models (LLMs) using an external vector cache to store its past states. Like recent vector retrieval approaches, Neurocache uses an efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states and incorporate them into the attention process. Neurocache improves upon previous methods by (1) storing compressed states, which reduces cache size; (2) performing a single retrieval operation per token which increases inference speed; and (3) extending the retrieval window to neighboring states, which improves both language modeling and downstream task accuracy. Our experiments show the effectiveness of Neurocache both for models trained from scratch and for pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the cache mechanism. We also compare Neurocache with text retrieval methods and show improvements in single-document question-answering and few-shot learning tasks. We made the source code available under: https://github.com/alisafaya/neurocache", "sections": [{"title": "1 Introduction", "content": "Recent advancements in natural language processing have been significantly driven by the development of large language models (LLMs) such as GPT-3, GPT-4, Llama, and Llama2 (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a,b). While demonstrating impressive capabilities, these models are constrained by limited context window sizes. This limitation becomes apparent in tasks that require understanding long documents, such as document summarization and academic literature review, where processing hundreds of thousands of tokens is necessary.\nVarious methods, including sparse attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), have been explored to address this limitation. However, these approaches often struggle to utilize their extended contexts (Liu et al., 2023) fully. Recent research by Xu et al. (2023) shows that retrieval-augmented models with shorter contexts (4K tokens) can match the performance of models with longer contexts (16K/32K tokens), maintaining efficiency during inference. This emphasizes the potential of retrieval-augmented strategies in LLMs.\nIn response to these challenges, we introduce Neurocache. Neurocache employs an efficient k-nearest-neighbor (kNN) strategy for retrieving relevant past states from a compressed external vector cache. This approach is designed to optimize hidden state caching and retrieval, thereby enhancing language modeling quality and increasing inference speed."}, {"title": "2 Related Work", "content": "Transformers have made significant advancements in natural language processing but face challenges in processing long contexts. Various methods have been developed to extend the context window while maintaining computational efficiency (Huang et al., 2023).\nRecent methods include the continued training or fine-tuning of short-context language models (Nijkamp et al., 2023; Chen et al., 2023b), positional interpolation (Chen et al., 2023a), ALiBi (Press et al., 2022), and sparse and efficient attention designs (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020). These approaches reflect the evolving landscape of solutions for managing extended attention windows in large language models (LLMs).\nHowever, language models still encounter difficulties in processing longer contexts (Liu et al., 2023). Studies have indicated that retrieval-augmented models with shorter contexts (4K) can surpass models with longer contexts (16K/32K) in performance (Xu et al., 2023).\nProminent strategies in this area are Text Retrieval and Vector Retrieval. Text Retrieval involves identifying and processing the most relevant segments of long documents. Vector Retrieval, on the other hand, integrates relevant hidden representations of the input, like hidden states or key-value pairs, into the model."}, {"title": "2.1 Text Retrieval", "content": "Text retrieval methods focus on processing relevant segments of long documents. Integrating retrieval mechanisms into language models, such as REALM (Guu et al., 2020), DPR (Karpukhin et al., 2020), RETRO (Borgeaud et al., 2021), and RALM (Ram et al., 2023), has enhanced model performance in various tasks.\nA limitation of Text Retrieval is its dependency on external retrievers for identifying relevant segments of the context, often employing algorithms like BM25 (Robertson and Zaragoza, 2009), Contriever (Izacard et al., 2022), and others (Borgeaud et al., 2021; Ram et al., 2023; Karpukhin et al., 2020)."}, {"title": "2.2 Vector Retrieval", "content": "Vector retrieval methods extend the context window by incorporating relevant hidden states from an external cache of past inputs' representations.\nMemorizing Transformers present a novel adaptation to the traditional transformer decoder structure for handling lengthy documents. They process documents in smaller segments and use a dynamically updated external cache to track previous key-value pairs. These models employ an approximate k-nearest-neighbor (kNN) lookup over this cache, merging dense self-attention on the current context with external-attention over retrieved key-value pairs, thus effectively extending the context length (Wu et al., 2022).\nUnlimiformer is a vector retrieval method, particularly suited for sequence-to-sequence models like BART (Lewis et al., 2020). It extends encoding length by using a kNN index over all input token hidden states, focusing on the top-k input tokens through kNN distance-based attention scores in each decoder layer's cross-attention head (Bertsch et al., 2023)."}, {"title": "2.3 Neurocache", "content": "Neurocache is a vector retrieval method designed for processing long documents in large language models (LLMs). It employs a kNN strategy to efficiently retrieve compressed past states from an external vector cache. This approach contrasts with methods like Memorizing Transformers and Unlimiformer, particularly in terms of computational efficiency and cache size management.\nNeurocache's notable features include storing compressed states to reduce cache size and performing a single retrieval operation per token, which accelerates inference speed. Additionally, it expands the retrieval window to include neighboring states, enhancing language modeling and downstream task performance.\nCrucially, Neurocache shows adaptability with established pre-trained models like Llama2-7B and Mistral-7B, extending their maximum context length capabilities to 128K tokens. This adaptability demonstrates Neurocache's potential in improving long-document processing capabilities of current LLMs.\nIn this context, Neurocache presents a balanced approach to vector retrieval, combining efficiency and adaptability to enhance long-context processing in natural language processing models."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Neurocache Overview", "content": "Neurocache addresses the challenge of processing long documents using Transformer decoders, leveraging a k-nearest-neighbor (kNN) search for efficient retrieval and integration of relevant past states. The process begins by segmenting the long text sequences into smaller segments, each containing n tokens, fitting the model's attention window size.\nState Compression: Text segments are sequentially processed via a Transformer decoder stack (Vaswani et al., 2017). At the rth layer of the decoder, hidden states $H^r \\in \\mathbb{R}^{n \\times h}$ are acquired and subsequently projected into a compressed form $C \\in \\mathbb{R}^{n \\times d}$ using a learned projection matrix $W_p$. This compression step enhances the efficiency for the subsequent kNN retrieval.\nState Retrieval: For each compressed state $c \\in \\mathbb{R}^d$ within $C$, we identify the top-k most similar states $C_{ret} \\in \\mathbb{R}^{k \\times d}$ from the cache $C_{cache} \\in \\mathbb{R}^{m \\times d}$. This selection is based on the L2-distance between each state in $C$ and the states in the cache.\nCache Updating: The cache $C_{cache}$ is updated with the compressed states $C$, maintaining a fixed size of m entries. This is achieved by discarding the oldest n states, adhering to a First-In-First-Out strategy. The update occurs post-retrieval, reinforcing the commitment to retrieving only relevant past states.\nCache Augmented Layers: Using the states $C_{ret}$ retrieved in the previous step. Starting from the $(r + 1)$th layer, the cache-augmented layers $L_j$, where $j > r$, integrate a specialized attention mechanism. Each layer uses unique projection matrices $W^Q$, $W^K$, and $W^V$ to generate keys $K_{ret}$ and values $V_{ret}$ from $C_{ret}$, and queries $Q_i$ from the hidden states $H_i$. The cache attention mechanism is defined as:\n$CA(Q, K_{ret}, V_{ret}) = softmax\\left(\\frac{QK_{ret}^T}{\\sqrt{d_{key}}}\\right)V_{ret}$\nIn this formula, $Q$ are the queries derived from $H_i$, while $K_{ret}$ and $V_{ret}$ are keys and values derived from $C_{ret}$, with $d_{key}$ serving as a normalization factor. The output of cache attention is processed by an output matrix $W^O$ before being combined with self-attention outputs through a residual connection.\nContextual Retrieval Window: When retrieving the top-k similar cached states, Neurocache also considers additional states surrounding these top-k states within a defined Retrieval Window. This expanded retrieval captures not only the most similar states but also their immediate neighbors, providing a richer context for the model's processing.\nConsider the cached states $C_{cache} = [C_1, C_2,...,C_m]$, and a query q for which the cached states $c_i$ and $c_j$ are identified as the top-2. With an even Retrieval Window size w, the retrieved set would include not just $c_i$ and $c_j$, but also the cached states $[C_{i-(w/2)-1},..., C_{i+w/2}]$ and $[C_{j-(w/2)-1},..., C_{j+w/2}]$, truncated at the boundaries of the cache.\nExtended Cache-Attention: We enhance the contextual awareness of each token during the cache-attention operation by granting access to the retrievals of preceding tokens. Similar to the contextual retrieval window, this feature broadens the current token's context."}, {"title": "3.2 Neurocache Adaptation", "content": "Adapting pre-trained decoder language models for Neurocache use is a straightforward process that significantly enhances their capability to efficiently process long documents. For the layers augmented with Neurocache, denoted as $L_j$ where $j > r$, the adaptation involves initializing cache-attention weight matrices $(W^Q, W^K, W^V, W^O)$ by duplicating weights from the corresponding self-attention layers of the pre-trained models. Simultaneously, the projection matrix $W_p$ is randomly initialized to transform hidden states into compact forms suitable for Neurocache retrieval.\nFurthermore, we integrate Low-Rank Adapters (LoRA) (Hu et al., 2022) into the feed-forward networks of the cache augmented layers. LoRA, introducing a minimal number of parameters, plays a key role in adapting the models to cache attention without compromising their original strengths.\nDuring training, we freeze the original parameters of the pre-trained model and focus solely on training the newly added weights, specifically the LORA weights, and the cache-attention weight matrices $(W^Q, W^K, W^V, W^O)$, along with the projection matrix $W_p$. This training, using a causal language modeling objective on a corpus of long documents, enables the models to efficiently utilize the Neurocache system."}, {"title": "3.3 Retrieval Overhead", "content": "When analyzed per token, the computational overhead of retrieval in our method stems from the following components, which underline the primary computational efforts in the kNN retrieval.\nDistance Computation: For each token, the relevance is assessed by calculating the L2-distance between the token's compressed hidden state $c \\in \\mathbb{R}^d$ and each of the m cached states, resulting in a complexity of $O(d \\times m)$ per token, where d is the dimension of the compressed hidden state c and m is the total number of cached entries."}, {"title": "3.4 Comparative Analysis", "content": "The Neurocache model demonstrates computational advantage over alternatives like the Memorizing Transformer (Wu et al., 2022) and the Unlimiformer (Bertsch et al., 2023) by performing only one cache query per token. This approach significantly reduces the computational burden. In contrast, the Memorizing Transformer requires multiple cache queries for each token, specifically one for every attention head. Consequently, this leads to an a-fold increase in complexity per token, both for distance computation, $O(a \\times d \\times m)$, and top-k retrieval, $O(a \\times (m + k))$, where a is the number of attention heads, and m is the cache size.\nThe Unlimiformer, needing $l \\times a$ queries per token, further increases retrieval complexity. For instance, a Transformer with 24 layers and 12 attention heads in the Memorizing Transformer configuration would need 12 cache accesses per token. If the Unlimiformer uses half of its layers for augmentation, as per (Bertsch et al., 2023), the requirement rises to 12 \u00d7 12 = 144 cache accesses per token. Neurocache's strategy of one query per token significantly streamlines this process without compromising accuracy."}, {"title": "4 Language Modeling", "content": "We assess Neurocache's effectiveness via two experimental approaches: pre-training language models from scratch and adapting established pre-trained models. For pre-training, TransformerXL (Dai et al., 2019) serves as our baseline, against which we compare Neurocache and Memorizing Transformer (Wu et al., 2022). In terms of adaptation, we focus on pre-trained models including OPT-1.3B, Llama2-7B, and Mistral-7B (Zhang et al., 2022; Touvron et al., 2023b; Jiang et al., 2023)."}, {"title": "4.1 Datasets", "content": "Our experiments employ two distinct raw text corpora: PG-19, a well-established benchmark for long-form language modeling, and LongPile, a diverse dataset derived from the Pile.\nPG-19: This corpus comprises a collection of books written in English and published before 1919, sourced from Project Gutenberg. It is recognized as a standard benchmark for evaluating models on long-form text (Rae et al., 2020; Wu et al., 2022; Hutchins et al., 2022).\nLongPile: Extracted from the Pile corpus (Gao et al., 2020), LongPile features extensive documents from varied sources including \"Books3,\" \"Gutenberg (PG-19),\" \"OpenWebText2,\" \"Pile-CC,\" and \"Wikipedia (en).\" The selection criterion ensures that each document surpasses 20K tokens, making it suitable for testing models' performance on longer texts."}, {"title": "4.2 Pre-training", "content": "Our baseline for pre-training is the TransformerXL model (Dai et al., 2019), which we compare against Neurocache and the Memorizing Transformer (Wu et al., 2022). In these experiments, both Neurocache and the Memorizing Transformer are configured with a fixed storage size of 16K during training, expanding to 128K for evaluation to assess their ability to generalize to larger storage sizes.\nIn Neurocache, we set the augmented layer threshold r at $3 * nlayers/4$, leading to the compression of outputs from the 9th layer of a 12-layer model. The hidden states H, originally of size h = 1024, are compressed by a factor of 4, resulting in a reduced size of d = 256. We use a retrieval window w = 2 to fetch the top-k cached states and their right neighbors for cache-attention in layers 10 to 12. Extending cache-attention to include previous tokens' retrievals with c = 2, we set k = 16, resulting in 64 neighbors in total. This setup was determined through hyperparameter optimization"}, {"title": "4.3 Adaptation", "content": "We extend our adaptation strategy to pre-trained models such as OPT-1.3B, Llama2-7B, and Mistral-7B (Zhang et al., 2022; Touvron et al., 2023b; Jiang et al., 2023). The adaptation process is identical to that described in Section 3.2, ensuring a smooth integration of Neurocache with the pre-trained model weights.\nWe set the rank parameter r to 16, the scale parameter \u03b1 to 32, and turn off bias in LoRA. Added weight matrices and adapter weights are trained on the PG-19 and LongPile datasets' training splits for 25,000 steps, employing the Adam optimizer (Kingma and Ba, 2015) with a decaying learning rate of $1 \\times 10^{-4}$. We configured Neurocache using the same settings as the pre-training experiments. This adaptation process consumes approximately 200 Nvidia A100 GPU Hours per model.\nThe successful adaptation is evident in the significant improvement in token perplexity on both datasets, as detailed in Table 2. The subsequent section discusses the impact of these improvements on zero-shot performance in downstream tasks."}, {"title": "5 Downstream Evaluation", "content": "We assess the performance of models augmented with Neurocache, particularly Llama2-7B and Mistral-7B adapted on LongPile, using seven distinct downstream tasks from the LongBench suite (Bai et al., 2023). These tasks cover a range of scenarios, including single-document question-answering (QA), multi-document QA, and few-shot learning. We utilize a zero-shot evaluation approach for the single-document and multi-document QA tasks. Conversely, in the few-shot learning tasks, a small set of examples is provided to the models, serving as part of the extended context."}, {"title": "5.1 Datasets", "content": "The datasets in this evaluation present unique challenges, with average token lengths ranging from 5K to 35K, underscoring the need to process long texts effectively."}, {"title": "5.1.1 Single-document QA", "content": "NarrativeQA (NQA) is a question-answering dataset consisting of books from Project Gutenberg and movie scripts. It includes about 30 question-answer pairs per document, providing a robust test for QA systems (Ko\u010disk\u00fd et al., 2018).\nQasper (QSP) contains questions and answers extracted from NLP papers. This dataset offers diverse question types, such as abstractive, extractive, yes/no, and unanswerable questions, making it a comprehensive testbed for QA models (Dasigi et al., 2021).\nMultiFieldQA (MQA) is designed to test a model's ability to understand long contexts across various fields, including legal documents, government reports, and academic papers. It poses a challenge with its questions dispersed throughout lengthy documents (Bai et al., 2023)."}, {"title": "5.1.2 Multi-document QA", "content": "HotpotQA (HQA) is a multi-document, Wikipedia-based QA dataset. It requires reading and reasoning across multiple documents and includes questions necessitating sentence-level supporting facts for complex reasoning (Yang et al., 2018).\nMuSiQue (MSQ) focuses on multihop reasoning in QA. It constructs multi-hop questions from simpler, single-hop ones, demanding a systematic approach and detailed control over the question formation process (Trivedi et al., 2022)."}, {"title": "5.1.3 Few-shot Learning", "content": "SAMSum (SAMS) presents a dialogue summarization challenge with its dataset of messenger-like conversations and human-annotated summaries. It tests a model's ability to condense conversational data into coherent summaries (Gliwa et al., 2019).\nTREC serves as a dataset for few-shot learning tasks in question type classification. Models are tasked with categorizing questions into predefined categories, providing a test of their classification abilities (Li and Roth, 2002)."}, {"title": "5.2 Models", "content": "In addition to Neurocache, our evaluation includes three distinct approaches for extending the input length of pre-trained language models. These approaches are Input Truncation, Text Retrieval, and Position Interpolation (PI).\nTruncation: This approach employs the original Llama2-7B and Mistral-7B models without long-context-specific modifications. Here, inputs exceeding the maximum size of 4,096 tokens are truncated from the middle following (Bai et al., 2023). This baseline serves as a reference to evaluate the effectiveness of other methods in processing extended documents.\nText Retrieval: Contrasting with Neurocache, this approach involves selecting the most relevant text segments to include in the input, keeping the total length within the model's maximum input size. We divide the context into 200-word chunks, retrieving the top-7 chunks using Contriever (Izacard et al., 2022). These chunks, along with the input, are then processed by the model. Using the top-7 chunks balances performance and the 4K token limit. This method, used in previous work (Bai et al., 2023; Xu et al., 2023), differs from Neurocache, which dynamically integrates relevant information from the entire document via cache-augmented layers.\nPosition Interpolation (PI): PI (Chen et al., 2023a) linearly down-scales input position indices to fit the original context window size, avoiding high attention scores that could disrupt the self-attention mechanism. LongLoRA (Chen et al., 2023b), leveraging PI, offers an efficient fine-tuning method to expand the context size of pre-trained models. It uses a sparse local attention mechanism, enabling computation savings while retaining performance. The fully fine-tuned LongLoRA model, based on Llama2-7B, extends the maximum input length to 16K tokens, aiming to assess the effectiveness of efficient full-attention methods for longer documents.\nNeurocache: We utilize the Neurocache-adapted Llama2-7B and Mistral-7B models in our evaluation. These adaptations follow the configuration detailed in Section 4 for pre-training. The models operate with a fixed cache size of 16K, accommodating the length of most datasets in our study. We split the documents into 2,048-token segments, processing them sequentially to populate the cache. Subsequently, the input, embedded within the prompt, is fed to the model, which then generates the corresponding answer."}, {"title": "5.3 Evaluation Setting", "content": "All evaluated models in this study are only pre-trained and not fine-tuned on the downstream tasks. They are assessed in a zero-shot setting, employing greedy decoding for output generation.\nAs outlined by LongBench (Bai et al., 2023), the model's task is to produce an answer given input and context sequences. In single-doc QA tasks, the input is a question paired with the document as context. For multi-doc QA, the input consists of multiple concatenated documents. In few-shot learning tasks, such as TREC and SAMSum, the context includes a set of examples, and the input is a question or dialogue, respectively. The input and answer are typically concise, while the context can be a long sequence extending to thousands of tokens.\nIf the combined length of input and context exceeds the model's maximum input capacity, only the context is truncated. This truncation is done from the middle of the context sequence, following the approach in (Bai et al., 2023). We utilize prompt templates provided by LongBench for consistency. Neurocache and LongLoRA operate with a maximum length of 16K tokens, truncating contexts longer than this limit. In contrast, the Text Retrieval method processes the entire context, regardless of length. To ensure comparability, all models are evaluated on identical hardware with a batch size of 1."}, {"title": "5.4 Results", "content": "The zero-shot evaluation results across various downstream tasks are summarized in Table 3. We compare the performance of Llama2-7B and Mistral-7B, in their original and Neurocache-adapted forms, against other long document processing methods.\nSingle-document QA: In tasks like NarrativeQA, Qasper, and MultiFieldQA, Neurocache-adapted models show superior performance, demonstrating their effectiveness in processing long contexts within single documents.\nMulti-document QA: Performance in multi-doc QA tasks, such as HotpotQA, reveals a varied picture. While Neurocache-adapted models are competitive, they fall short of Text Retrieval methods. For instance, in HotpotQA, Text Retrieval with the Mistral-7B model achieves the highest F1 score of 40.92. This finding suggests that, despite Neurocache's effectiveness in single-doc scenarios, it may be less effective in multi-doc contexts compared to text retrieval approaches.\nFew-shot Learning: In few-shot learning tasks like SAMSum and TREC, Neurocache shows strong performance, particularly indicated by improved Rouge-L scores in SAMSum. This underscores its capability to leverage few-shot examples for generating accurate summaries.\nThese findings illustrate the strengths and challenges of different methods in handling long documents in language models. Neurocache excels in single-document and few-shot learning scenarios, while Text Retrieval methods have an edge in multi-document tasks."}, {"title": "6 Conclusion", "content": "This paper introduced Neurocache, an approach designed to improve long document processing in language models. Neurocache employs a k-nearest-neighbor (kNN) strategy for integrating relevant past states from compressed hidden representations, thus extending the context window of Transformer decoders. Notably, Neurocache enhances the maximum context length of models like Llama2-7B and Mistral-7B to 128K tokens.\nOur findings indicate that Neurocache offers improvements in inference speed and language modeling accuracy. It demonstrates proficiency in single-document question-answering and few-shot learning, though it faces challenges in multi-document scenarios. Neurocache's competitive performance and adaptability highlight its potential utility in various applications.\nIn summary, Neurocache contributes to the field by enabling more efficient handling of extended contexts in existing language models. Future work may explore further optimizations for multi-document tasks and the extension of Neurocache to different model architectures and domains."}, {"title": "7 Limitations", "content": "While Neurocache demonstrates progress in long document processing with language models, several limitations should be noted. Our evaluation is confined to datasets like PG-19 and LongPile, and tasks from the LongBench suite. These datasets, despite their diversity, might not fully represent all long-context scenarios. Performance may vary in specialized domains like technical documents or source code, which have distinct content characteristics.\nA notable limitation is Neurocache's performance in multi-document scenarios, suggesting potential challenges in contexts that require integration of information from multiple sources. This aspect is crucial for applications involving comprehensive data synthesis from various documents.\nIn terms of bias, Neurocache depends on the underlying language models and datasets for training and evaluation. Consequently, any inherent biases in these components could influence Neurocache's outputs. An explicit analysis of model biases was not conducted in this study, highlighting an area for future exploration.\nAnother critical point is our reliance on a zero-shot setting for evaluation. The performance of Neurocache might differ if fine-tuning on downstream tasks or instruction datasets was employed. This limitation suggests that our current findings may not fully capture the model's adaptability and efficiency in diverse application scenarios.\nIn conclusion, while Neurocache presents a step forward in handling long documents in natural language processing, its effectiveness is influenced by the nature of the data, model architecture, and specific task requirements. Understanding these limitations is vital for assessing its practical applicability and guiding future improvements."}, {"title": "A Optimizing Neurocache Hyperparameters", "content": "Effective processing of long documents in Neurocache depends on the optimal tuning of various retrieval hyperparameters. To this end, we conduct a comprehensive hyperparameter search focused on language modeling performance using the Project Gutenberg-19 (PG) dataset.\nOur exploration encompasses a range of values for key hyperparameters: the number of retrieved neighbors (k) with values in the set [None, 8, 16, 32, 64, 128, 256], the retrieval window size (w) tested with [1,2,4], the cache-attention context size (c) evaluated at [1, 2, 4], and the encoding dimension of hidden states (d) explored across [1024, 512, 256, 128, 64]. This systematic investigation aims to identify the optimal configurations that enhance Neurocache's efficiency and effectiveness in handling large-scale textual data.\nNumber of Retrieved Neighbors (k): The influence of k, the number of retrieved neighbors, on model performance is examined. Table 4 shows that increasing k generally leads to a decrease in perplexity, indicating improved performance. However, the computational cost also increases proportionally with k. We pick k = 64 due to the diminishing returns and the increasing cost of larger values."}, {"title": "B Neurocache Algorithm", "content": "\u2022 The cache $C_{cache}$ is initialized to store compact representations, with a maximum capacity of m entries.\n\u2022 The long document D is segmented into sequences of n tokens each."}]}