[{"title": "Adaptive control of reaction-diffusion PDEs via neural operator-approximated gain kernels", "authors": ["Luke Bhan", "Yuanyuan Shi", "Miroslav Krstic"], "abstract": "Neural operator approximations of the gain kernels in PDE backstepping has emerged as a viable method for implementing controllers in real time. With such an approach, one approximates the gain kernel, which maps the plant coefficient into the solution of a PDE, with a neural operator. It is in adaptive control that the benefit of the neural operator is realized, as the kernel PDE solution needs to be computed online, for every updated estimate of the plant coefficient. We extend the neural operator methodology from adaptive control of a hyperbolic PDE to adaptive control of a benchmark parabolic PDE (a reaction-diffusion equation with a spatially-varying and unknown reaction coefficient). We prove global stability and asymptotic regulation of the plant state for a Lyapunov design of parameter adaptation. The key technical challenge of the result is handling the 2D nature of the gain kernels and proving that the target system with two distinct sources of perturbation terms, due to the parameter estimation error and due to the neural approximation error, is Lyapunov stable. To verify our theoretical result, we present simulations achieving calculation speedups up to 45\u00d7 relative to the traditional finite difference solvers for every timestep in the simulation trajectory.", "sections": [{"title": "1. Introduction", "content": "First introduced in [1], a new methodology has emerged for employing neural operator(NO) approximations of the gain kernels in PDE backstepping. The key advantage of this approach compared to traditional implementations of the kernel, as well as other approximation approaches such as [2], is the ability to produce the entire kernel in mere milliseconds for the online control law while the training process is decoupled to be precomputed offline. Therefore, perhaps the most valuable application of the neural operator approximated gain kernels is in adaptive control, where the kernel needs to be recomputed, online, for every new estimate of the plant parameter. This was first explored for hyperbolic PDEs in [3]. In this work, we extend the results of [3] to parabolic PDEs where the technical challenge arises both in the Lyapunov analysis of a more complex perturbed target system as well as in the computational implementation, where the neural operator must map functions on an 1D domain into a 2D triangle. Furthermore, from an application perspective, this paper enables real-time adaptive control of reaction-diffusion PDEs which govern a series of real-world applications including, but not limited to, chemical reactions [4], tubular reactor systems [5], multi-agent and social networking systems [6, 7], and Lithium-ion batteries [8].\nPDE backstepping for adaptive control. The first study of adaptive control for parabolic PDEs was in [9, 10, 11] which extended the adaptive backstepping results for nonlinear ODES [12] via three methodologies - the Lyapunov approach, passive identifier approach, and swapping approach. In this work, we focus on the Lyapunov approach which appears to exhibit superior transient performance as mentioned in [9] and [13]. This was then extended into hyperbolic PDEs which refer the reader to the rich set of literature [14, 15, 16, 17, 18]. In [19] the authors then explored delay-adaptive control which was later extended in [13] for unknown delays. For more complex systems, we refer the reader to adaptive backstepping schemes across a variety of challenging plants including coupled hyperbolic PDEs [20], coupled hyperbolic PDE-PDE-ODE cascades [21] and the wave equation [22]. Lastly, we mention the extensions into event-triggered adaptive control in [23], [24], [25].\nPaper organization. We first introduce, in Section 2, the nominal adaptive backstepping control scheme for reaction-diffusion PDEs to expose the reader to the type of result we aim to maintain under the neural operator kernel approximation. We then explore and prove a series of properties for the gain kernel and its time derivative in Section 3 in order to apply the universal approximation theorem [40] in Section 4, to prove the existence of neural operators for approximating the gain kernels to arbitrary desired accuracy. We then state and prove our main result namely a Lyapunov analysis of the full system with the NO approximated kernel, adaptive update law, and the backstepping control law in Section 5. Lastly, in Section 6, we conclude by presenting simulations demonstrating the efficacy of the proposed neural operator approximated control scheme on a parabolic PDE resembling molecular interactions in chemical reactions.\nNotation. We use $\\| \\|_{\\infty}$ for the infinity-norm, that is $\\|u(x)\\|_{\\infty} = \\sup_{x \\in [0,1]} |u(x)|$. Furthermore, we use $\\|u(x, t)\\|$ to be the spatial $L^2$ norm, $\\|u(x, t)\\| = (\\int u^2(x, t)dx)^{\\frac{1}{2}}$. We use $C^n(U; V)$ to indicate functions from set U into set V that have n continuously differentiable derivatives. For scenarios, where the function has multiple arguments, ie, $f(x, y, t)$, we use $C^{2,2,1}$ to indicate the function has continuous second derivatives in x and y, but only continuous first derivatives with respect to t. If, the second argument of above is not given, say ie, $C^{1}(\\mathbb{R}^{P})$, then assume the function is mapping into the real numbers $\\mathbb{R}$. Lastly, we denote the positive reals by $\\mathbb{R}_{+} = \\{x \\in \\mathbb{R}|x \\geq 0\\}$ and Hilbert spaces by $H^n$. For example, $H^{2}$ is the space of functions with a $L^2$ weak derivative of order 2."}, {"title": "2. Nominal controller for PDE backstepping", "content": "We begin by introducing the following 1D Reaction-Diffusion PDE with a spatially varying coefficient $\\lambda(x)$,\n$u_t(x, t) = u_{xx}(x, t) + \\lambda(x)u(x, t), \\quad x \\in (0,1), \\tag{1}$\n$u(0, t) = 0, \\tag{2}$\n$u(1, t) = U(t), \\tag{3}$\nwhere $u(x, t)$ is defined for all $t \\in \\mathbb{R}_{+}$ with initial condition $u(x, 0) = u_0(x) \\in H^2(0, 1)$ that is compatible with the boundary conditions. Further, $\\lambda(x) : [0, 1] \\rightarrow \\mathbb{R}$ is an unknown, spatially varying coefficient function that will be estimated online. The standard approach for controlling the PDE (1), (2), (3) is to introduce the backstepping transformation\n$w(x, t) = u(x, t) - \\int_{0}^{x} k(x, y)u(y, t)dy, \\tag{4}$\nto convert the system into the stable target system\n$w_t = w_{xx}, \\tag{5}$\n$w(0, t) = 0, \\tag{6}$\n$w(1,t) = 0, \\tag{7}$\nunder the feedback control law\n$U(t) = \\int_{0}^{1} k(1, y)u(y, t)dy. \\tag{8}$\nTo ensure the transformation (4) converts (1), (2), (3) into (5), (6), (7), the kernel function k must satisfy\n$k_{xx}(x, y) - k_{yy}(x, y) = \\lambda(y)k(x, y), \\quad (x, y) \\in \\check{T}, \\tag{9}$\n$k(x, 0) = 0, \\tag{10}$\n$\\frac{d}{dx} k(x, x) = \\frac{1}{2} \\lambda(y)dy, \\tag{11}$\nwhere we define the triangular domains $\\check{T} = \\{0 < y \\leq x < 1\\}$ and $T = \\{0 \\leq y \\leq x \\leq 1\\}$. Note, the gain kernel, which is the solution to an infinite dimensional PDE that is not analytically solvable, is explicitly an operator mapping from functions of the spatially varying $\\lambda(x)$ into the PDE solution function $k(x, y)$. However, in the adaptive control case, $\\lambda(x)$ is unknown and thus needs to be estimated online via some approximation $\\hat{\\lambda}(x)$. Thus, the PDE kernel in (9), (10), (11) becomes a mapping from $\\hat{\\lambda}(x) \\leftrightarrow k(x, y)$ where the solution requires recomputation at every timestep. In what follows, we will denote the PDE solution for $\\hat{\\lambda}$ at time t by $\\hat{k}(x, y, t)$, and denote the neural operator approximation of the PDE solution for $\\hat{\\lambda}$ at time t by $\\tilde{k}(x, y, t)$. Further, to ensure the mapping from u into the w system is well defined, recall the inverse backstepping transformation,\n$u(x, t) = w(x, t) + \\int_{0}^{x} l(x, y)w(y, t)dy, \\tag{12}$\nwhere we refer to l as the inverse backstepping kernel. Then, following Chapter 11 of [41], the estimator of $\\hat{\\lambda}$ is given by\n$\\hat{\\lambda}_t(x, t) = Proj(\\lambda_1(x, t), \\delta(x, t)), \\tag{13}$\n$\\delta(x,t) := \\frac{u(x, t)}{\\gamma_1 + \\|w\\|^2} \\times \\Big(w(x, t) - \\int_{0}^{x} k(y, x, t)w(y, t)dy\\Big) \\tag{14}$\nwhere $\\gamma, \\bar{\\lambda} > 0$ are constants, $\\|\\hat{\\lambda}_1\\|_{\\infty} \\leq \\bar{\\lambda}$, and the projection is defined as\n$Proj(a, b) := \\begin{cases}\n    0, & \\text{if } b = \\bar{\\lambda} \\text{ and } ab > 0 \\\\\n    a, & \\text{otherwise}\n  \\end{cases} \\tag{15}$\nThus, noting that we introduced a bound on $\\lambda$, we formally state the only required assumption on $\\lambda$ as the following.\nAssumption 1. $\\lambda \\in C^1([0, 1])$ and there exists a constant $\\bar{\\lambda} > 0$ such that $\\|\\lambda\\|_{\\infty} \\leq \\bar{\\lambda}$.\nSuch an assumption is standard in the backstepping literature as $\\lambda \\in C^1([0,1])$ is needed to ensure well-posedness of the kernel PDE (9), (10), (11) and the bounded assumption is needed for the adaptive control estimate. Now, we state the main theorem for adaptive control of parabolic PDEs under the exact gains, which we aim to emulate under the neural operator approximated gains in this paper: under the feedback control scheme with the adaptive estimate $\\hat{\\lambda}$ and true kernel solution k, the closed-loop system is regulated asymptotically to 0.\nTheorem 1. ([41] Stabilization under exact adaptive control scheme). There exists a $\\gamma^*$ such that for $\\gamma \\in (0, \\gamma^*)$, for any initial estimate $\\hat{\\lambda}(x, 0) \\in C^1([0, 1])$ with $\\|\\hat{\\lambda}(x, 0)\\|_{\\infty} \\leq \\bar{\\lambda}$ and for any initial condition $u_0 \\in H^2(0, 1)$ compatbile with boundary conditions, the classical solution of the closed loop system $(u, \\lambda)$ consisting of the plant (1), (2), (3), the update law (13), (14), and the control law (8) is bounded for all $(x, t) \\in [0, 1] \\times \\mathbb{R}_+$ such that\n$\\lim_{t \\rightarrow \\infty} \\sup_{x \\in [0,1]} |u(x, t)| = 0. \\tag{16}$"}, {"title": "3. Properties of gain kernel PDE", "content": "In order to approximate the gain kernel PDE for various $\\hat{\\lambda}$ estimates, we need to prove that both the time and spatial derivatives are well-defined, continuous and bounded. The exact use of these Lemmas will become clear in invoking the universal operator approximation theorem in Section 4 and the proof of our main result in Section 5. We will employ the standard approach of successive approximations on the integral representation of the kernel PDE. Thus, recall the integral representation of the kernel PDE as\n$G(\\xi, \\eta, t) = \\frac{1}{4} \\int_{\\eta}^{\\xi} \\lambda(\\frac{\\sigma + \\eta}{2}, t) ds \\\\+ \\frac{1}{4} \\int_{\\eta}^{\\xi} \\int_{\\eta}^{\\sigma} \\lambda(\\frac{\\sigma + s}{2}, t) G(\\sigma, s, t) ds d\\sigma, \\tag{17}$\nwhere\n$\\xi = x + y, \\eta = x - y, \\tag{18}$\n$G(\\xi, \\eta, \\tau) = k(\\frac{\\xi+\\eta}{2}, \\frac{\\xi-\\eta}{2}, t), (x,y) \\in \\Gamma. \\tag{19}$\nThen, for the reader's convenience, we briefly recall the following lemma whose proof is well known in the backstepping literature.\nLemma 1. ([42] Existence and bound for gain kernel.) Let $\\bar{\\lambda} > 0$ such that $\\|\\lambda\\|_{\\infty} \\leq \\bar{\\lambda}$ for all $(x, t) \\in [0, 1] \\times \\mathbb{R}_{+}$. Then, for any fixed $t \\in \\mathbb{R}_{+}$, and for every $\\lambda(x, t) \\in C^{1} ([0, 1])$ at fixed t, the kernel governed by the PDE (9), (10), (11) with function $\\lambda$ has a unique $C^{2}(T)$ solution with the bound\n$\\|k(x, y, t)\\| \\leq \\bar{\\lambda} e^{\\bar{\\lambda} e^{\\bar{\\lambda}x}}. \\tag{20}$\nLemma 2. (Existence and bound for $k_x(x, x, t)$) Let $\\bar{\\lambda} > 0$ such that $\\|\\lambda\\|_{\\infty} \\leq \\bar{\\lambda}, \\forall (x, t) \\in [0, 1] \\times \\mathbb{R}_{+}$. Then, for any fixed $t \\in \\mathbb{R}_{+}$, $k_x(x, x, t) \\in C^{1}[0, 1]$ with the bound\n$\\|k_x(x, x, t)\\| \\leq \\frac{1}{2} \\bar{\\lambda}. \\tag{21}$\nProof. One can show the existence and continuity of this derivative on all of $(x, y) \\in T$ by differentiating (17) and using the method of successive approximations. However, we only require a bound at the boundary condition $y = x$ and thus, one has\n$\\frac{\\partial}{\\partial x} k(x, x, t) = \\frac{1}{2} \\int_0^x \\lambda(y, t)dy \\leq \\frac{1}{2} \\bar{\\lambda}, \\\\ \\forall (x, t) \\in [0, 1] \\times \\mathbb{R}_+. \\tag{22}$\nLemma 3. (Existence and bound for $k_t(x, y, t)$) Let $\\bar{\\lambda} > 0$ such that $\\|\\lambda\\|_{\\infty} \\leq \\bar{\\lambda}, \\forall (x, t) \\in [0, 1] \\times \\mathbb{R}_{+}$. Then, for every $\\lambda(x, t) \\in C^{1}([0, 1]\\times\\mathbb{R}_{+})$, $k_t(x, y, t)$ has a unique $C^{0}(T\\times \\mathbb{R}_{+})$ solution with the bound\n$\\|k(t)\\| \\leq M \\|\\lambda_t(t)\\|, \\forall t \\geq 0, \\tag{23}$\n$M = e^{2\\bar{\\lambda}} (1 + \\bar{\\lambda} e^{2\\bar{\\lambda}}). \\tag{24}$\nProof. We begin by showing existence and continuity. Differentiating (17) with respect to t yields\n$G_t(\\xi, \\eta, t) = \\frac{1}{4} \\int_{\\eta}^{\\xi} \\lambda_t(\\frac{\\sigma + \\eta}{2}, t) ds \\\\+ \\frac{1}{4} \\int_{\\eta}^{\\xi} \\int_{\\eta}^{\\sigma} \\Big[ \\lambda_t(\\frac{\\sigma + s}{2}, t) G(\\sigma, s, t) \\\\+ \\lambda(\\frac{\\sigma + s}{2}, t) G_t(\\sigma, s, t)\\Big] ds d\\sigma. \\tag{25}$"}, {"title": "4. Neural operator approximation of the gain kernel", "content": "We aim to approximate the kernel mapping $\\lambda \\rightarrow k$ by a neural operator and thus begin by presenting a general universal approximation theorem for the nonlocal neural operator - a unifying framework that encompasses a series of operator learning architectures including both the popular FNO [44] and DeepONet [45] frameworks and their extensions such as NOMAD [46] and the Laplace NO [47]. We give the details of the nonlocal-neural operator architecture in Appendix B (as well as its connections to FNO and DeepONet) and refer the reader to [40] for further details.\nTheorem 2. ([40, Theorem 2.1] Neural operator approximation theorem.) Let $\\Omega_{u} \\subset \\mathbb{R}^{d_{u_1}}$ and $\\Omega_{v} \\subset \\mathbb{R}^{d_{v_1}}$ be two bounded domains with Lipschitz boundary. Let $G : C^0(\\Omega_{u}; \\mathbb{R}^{d_{u_2}}) \\rightarrow C^0(\\Omega_{v}; \\mathbb{R}^{d_{v_2}})$ be a continuous operator and fix a compact set $K \\subset C^0(\\Omega_{u}; \\mathbb{R}^{d_{u_2}})$. Then for any $\\epsilon > 0$, there exists a nonlocal neural operator $\\hat{G} : K \\rightarrow C^0(\\Omega_{u}; \\mathbb{R}^{d_{v_2}})$ such that\n$\\sup_{y \\in \\Omega_{v}} |G(u)(y) - \\hat{G}(u)(y)| \\leq \\epsilon, \\tag{41}$\nfor all values $y \\in \\Omega_{v}$.\nFor readers familiar with the previous explorations of neural operators in approximating kernel gain functions [1], we provide the following corollary for DeepONet.\nCorollary 1. (DeepONet universal approximation theorem; first proven in [48]) Consider the setting of Theorem 2. Then, for all $\\epsilon > 0$, there exists $p^*, m^*$ such that for all $p \\geq p^*, m \\geq m^*$, there exists neural network weights $\\theta^{(k)}, \\zeta^{(k)}$ such that the neural networks (see [1], section III for definition) $g^N$ and $f^N$ in the DeepONet given by\n$G_N(u_m)(y) = \\sum_{k=1}^{N} g^N(u; \\zeta^{(k)}) f^N (y; \\theta^{(k)}), \\tag{42}$\nsatisfy\n$\\sup_{y \\in \\Omega_{v}} |G(u)(y) - G_N(u)(y)| \\leq \\epsilon, \\tag{43}$\nfor all values $y \\in \\Omega_{v}$.\nNote that Theorem 2 has two main assumptions. First, the input function space is required to be compact. Second, the operator mapping to be approximated must be continuous. We now introduce the operator for k that we aim to approximate, noting that the operator output includes k as well as its derivatives $k_x, k_{xx}, k_t$ which is needed for proving stability under the neural operator approximation.\nDefine the set of functions\n$K = \\{k \\in C_x^{0,2}C_t^{0,1} (T \\times \\mathbb{R}_{+})|k(x, 0, t) = 0\\}, \\forall x \\in [0, 1], t \\in \\mathbb{R}_{+}\\}. \\tag{44}$\nLet A be a compact set of $C^{0}([0, 1])$ with the supremum norm such that for every $\\lambda\\in A, \\|\\lambda\\|_{\\infty} < M$ and A is R-Lipschitz where M, R > 0 are constants that can be as large as needed. Then, denote the operator $K : A \\rightarrow K$ as\n$K(\\lambda(\\cdot, t)) := k(x, y, t) . \\tag{45}$\nFurther, define the operator $M : A \\rightarrow K \\times C^{1}([0, 1] \\times \\mathbb{R}_{+}) \\times C_x^{0,2}C_t^{0,1}(T \\times \\mathbb{R}_{+})$ such that\n$M(\\lambda(\\cdot, t)) := (k(x, y, t), k_t (x, t), k_{2}(x, y, t)), \\tag{46}$\nwhere\n$\\kappa_1(x, t) = \\frac{\\partial}{\\partial x} (k(x, x, t)) + \\lambda_t(x,t), \\tag{47}$\n$\\kappa_2(x, y, t) = k_{xx}(x, y, t) - k_{yy}(x, y, t) - \\hat{\\lambda}(y, t) k(x, y, t) . \\tag{48}$\nFor the Lyapunov analysis that follows, we also need to include $k_t$ in our approximation. Thus, define the operator $K_1 : A^2 \\rightarrow C_x^{0,2}C_t^{0,1}(T \\times \\mathbb{R}_{+})$ such that\n$K_1(\\lambda(\\cdot, t), \\lambda_1(\\cdot, t)) := k_t(x, y, t), \\tag{49}$\nwhere under the transformations $\\xi = x+y, \\eta = x-y, \\forall (x, y) \\in T$, we have $k_t$ is the solution to the integral equation\n$\\tilde{k}(\\frac{\\xi+\\eta}{2}, \\frac{\\xi-\\eta}{2}) = G_t(\\xi,\\eta,t), \\tag{50}$\n$0 = -\\frac{1}{4} \\int_{\\eta}^{\\xi} \\lambda_t(\\frac{\\sigma + \\eta}{2}, t) ds \\\\+ \\frac{1}{4} \\int_{\\eta}^{\\xi} \\int_{\\eta}^{\\sigma} \\Big[ \\lambda_t(\\frac{\\sigma + s}{2}, t) G(\\sigma, s, t) \\\\+ \\lambda(\\frac{\\sigma + s}{2}, t) G_t(\\sigma, s, t)\\Big] ds d\\sigma, \\tag{51}$\nwhere G is given by (17).\nLastly, consider the composition of the operators M and $K_1$ given as $N : A^2 \\rightarrow K \\times C^{1}([0, 1] \\times \\mathbb{R}_{+}) \\times C_x^{0,2}C_t^{0,1}(T \\times \\mathbb{R}_{+}) \\times C_x^{0,2}C_t^{0,1}(T \\times \\mathbb{R}_{+})$ such that\n$N(\\lambda(\\cdot, t), \\lambda_1(\\cdot, t)) := (M(\\hat{\\lambda}(\\cdot, t)), K_1(\\lambda(\\cdot, t), \\lambda_1(\\cdot, t))). \\tag{52}$\nWe now aim to approximate the operator N which requires continuity of the operator. First, we note that M was shown to be continuous in [32, Theorem 4]. Thus, it suffices to show $K_1$ is continuous.\nLemma 4. Fix t $\\geq$ 0. Let $\\lambda_1(\\cdot, t), \\lambda_2(\\cdot, t) \\in A$ and $k_1 = K(\\lambda_1), k_2 = K(\\lambda_2)$. Then, $K_1$ is Lipschitz continuous. Explicitly, there exists a Lipschitz constant A > 0 such that\n$\\big\\| \\frac{\\partial}{\\partial t} k_{\\lambda_1} - \\frac{\\partial}{\\partial t} k_{\\lambda_2} \\big\\|_{\\infty} \\leq A \\|\\lambda_1 - \\lambda_2\\|_{\\infty}. \\tag{53}$\nProof. Let $\\lambda_1, \\lambda_2 \\in \\Omega$. Let $k_1 = K(\\lambda_1), k_2 = K(\\lambda_2)$ and denote $G_1, G_2$ the corresponding transforms of $k_1, k_2$ in the integral form $G_i = k_t(\\frac{\\xi+\\eta}{2}, \\frac{\\xi-\\eta}{2}), i \\in \\{1, 2\\}$. Then, we have the following\n$\\delta \\lambda = \\lambda_1 - \\lambda_2, \\tag{54}$\n$\\delta G = G_1 - G_2, \\tag{55}$\n$\\delta G = -\\frac{1}{4} \\int_{\\eta}^{\\xi} \\delta \\lambda_t(\\frac{\\sigma + \\eta}{2}, t) ds \\\\+ \\frac{1}{4} \\int_{\\eta}^{\\xi} \\int_{\\eta}^{\\sigma} \\Big[ \\delta \\lambda_t(\\frac{\\sigma + s}{2}, t) G(\\sigma, s, t) + \\lambda_2(\\frac{\\sigma + s}{2}, t) \\delta G(\\sigma, s, t) \\\\ + \\delta \\lambda(\\sigma, s, t) \\frac{\\partial}{\\partial t} G_1(\\sigma, s, t) - \\lambda_2(\\sigma, s, t) \\delta G_t(\\sigma, s, t) \\Big] ds d\\sigma \\tag{56}$\nIn the proof that follows, will omit the arguments on $\\lambda$ and G for conciseness. Define the sequence\n$\\delta G_0 = \\frac{1}{4} \\int_{\\eta}^{\\xi} \\lambda_2 \\delta G ds d\\sigma, \\tag{57}$\n$\\delta G_{n+1} = \\frac{1}{4} \\int_{\\eta}^{\\xi} \\lambda_2 \\delta G \\frac{\\partial G_1}{\\partial t} \\delta G_t ds d\\sigma. \\tag{58}$"}, {"title": "5. Stability under NO Approximated Gain Kernel", "content": "To simplify notation", "1": "such that $\\|\\lambda\\|", "Big": "frac{1"}, {"1": "t \\in \\mathbb{R"}, "and\n$\\lim_{t \\rightarrow \\infty} \\sup_{x \\in [0,1"]}, {"tag{74}$\n$\\Gamma(t)": "int_{0}^{1} [u^2(x, t) + (\\lambda(x) - \\hat{\\lambda}(x,t))^2] dx, \\tag{75}$\nholds for all $t \\geq 0$.\nProof. Consider the approximate backstepping transform\n$\\hat{w}(x, t) = u(x, t) - \\int_{0}^{x} k(x, y, t)u(y, t)dy, \\tag{76}$\n$u(x, t) = \\hat{w}(x, t) + \\int_{0}^{x} \\hat{l}(x, y, t)\\hat{w}(y, t)dy, \\tag{77}$\nwhere $\\hat{k} = K(\\lambda)$ and $\\hat{l}$ is the corresponding inverse backstepping transformation satisfying,\n$\\hat{l}(x, y, t) = \\hat{k}(x, y, t) + \\int_y^x k(x, \\xi, t)\\hat{l}(\\xi, y, t)d\\xi . \\tag{78}$\nThen, following Appendix C, the target system becomes\n$w_t = w_{xx} + \\delta k_0(x, t)u(x, t) - \\int_0^x \\delta k_1 (x, y, t)u(y, t)dy \\\\-\\int_0^x \\hat{\\lambda}(y, t)k(x, y, t)u(y, t)dy - \\int_0^x k_t(x, y, t)u(y, t), \\tag{79}$\n$w(0, t) = 0, \\tag{80}$\n$w(1,t) = 0, \\tag{81}$\nwhere\n$\\delta k_0(x, t) = (\\hat{\\lambda}(x, t) - 2k_x(x, x, t)), \\tag{82}$\n$\\delta k_1(x, y, t) = k_{xx}(x, y, t) - k_{yy}(x, y, t) - \\hat{\\lambda}(y)\\hat{k}(x, y, t). \\tag{83}$\nNote, such a target system has two perturbation terms given by $\\delta k_0$ and $\\delta k_1$ from the neural operator approximation as in [32] as well as two perturbations from the adaptive control scheme - namely the parameter estimation error $\\tilde{\\lambda}$ and the rate of the parameter estimation gain $k_1$. Now, for constant $\\gamma > 0$, consider the Lyapunov function\n$V = \\frac{1}{2} \\int_{0}^{1} \\frac{|w|^2}{1 + \\|\\tilde{\\lambda}\\|^2}dx + \\frac{1}{2 \\gamma}\\|\\tilde{\\lambda}\\|^2 . \\tag{84}$\nComputing the time derivative along the system trajectories, applying Leibniz rule, substituting for $w_t$ and noting $\\dot{\\tilde{\\lambda}} = - \\hat{\\lambda}_t$ yields\n$\\dot{V} = \\int \\frac{1}{1 + \\|\\tilde{\\lambda}\\|^2} w(x, t)w_t(x, t)dx \\\\ + \\frac{1}{1 + \\|\\tilde{\\lambda}\\|^2} \\int_0^1 \\hat{\\lambda}_t(x,t) \\tilde{\\lambda}_t(x,t)dx \\\\ = \\frac{1}{1 + \\|\\tilde{\\lambda}\\|^2} \\Big(\\int w(x, t)w_{xx}dx \\\\+ \\int_0^1 w(x, t)\\delta k_0(x, t)u(x, t)dx \\\\- \\int_0^1 w(x, t) \\int_0^x \\delta k_1(x, y, t)u(y, t)dydx \\\\"}]