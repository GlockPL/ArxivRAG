{"title": "MultiHateClip: A Multilingual Benchmark Dataset for Hateful Video Detection on YouTube and Bilibili", "authors": ["Han Wang", "Tan Rui Yang", "Usman Naseem", "Roy Ka-Wei Lee"], "abstract": "Hate speech is a pressing issue in modern society, with significant effects both online and offline. Recent research in hate speech detection has primarily centered on text-based media, largely overlooking multimodal content such as videos. Existing studies on hateful video datasets have predominantly focused on English content within a Western context and have been limited to binary labels (hateful or non-hateful), lacking detailed contextual information. This study presents MultiHateClip, an novel multilingual dataset created through hate lexicons and human annotation. It aims to enhance the detection of hateful videos on platforms such as YouTube and Bilibili, including content in both English and Chinese languages. Comprising 2,000 videos annotated for hatefulness, offensiveness, and normalcy, this dataset provides a cross-cultural perspective on gender-based hate speech. Through a detailed examination of human annotation results, we discuss the differences between Chinese and English hateful videos and underscore the importance of different modalities in hateful and offensive video analysis. Evaluations of state-of-the-art video classification models, such as VLM, GPT-4V and Qwen-VL, on MultiHateClip highlight the existing challenges in accurately distinguishing between hateful and offensive content and the urgent need for models that are both multimodally and culturally nuanced. MultiHateClip represents a foundational advance in enhancing hateful video detection by underscoring the necessity of a multimodal and culturally sensitive approach in combating online hate speech.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid expansion of social media has revolutionized the way information is shared, enhancing connectivity among users within both offline and online communities. However, these platforms have increasingly become conduits for the dissemination of hateful content that targets individuals or groups based on race, religion, gender, and other characteristics [20, 35]. The proliferation of online hate speech not only fosters discord among communities but also escalates to real-world violent hate crimes, underscoring the urgent need to identify and mitigate such content.\nCurrent research on detecting hateful content has primarily concentrated on text-based analysis [20, 35], with recent advancements extending to multimodal forms, such as memes [10, 11, 24, 32, 36]. Yet, the field of hateful video detection remains underexplored, largely due to the lack of comprehensive datasets. Videos harness the synergistic potential of visual, auditory, and textual components to spread hate speech and offensive content, subsequently piquing the curiosity of select researchers [4, 13, 46]. For instance. in a recent study, Das et al. [13] constructed an English video dataset to facilitate hateful video classification. However, the existing studies have largely focused on English videos based on Western context, and these datasets could only facilitate simple coarse-grained binary classification (hateful or non-hateful) without diving into the fine-grained analysis of the hate video context, e.g., identifying target victims in the hate video.\nTo fill these gaps, our study introduces MultiHateClip, a multilingual short clip video dataset that facilitates a more nuanced and comprehensive exploration of multimodal hateful video content. This dataset compiled English short clips from YouTube, a global platform known for its vast user-generated content and diverse viewership, and Chinese short clips from Bilibili, a leading Chinese video-sharing website that caters to a younger demographic with a focus on animation, comics, and games (ACG) content. In this study, we specifically focus on gender-based hate video in the Western and Chinese cultural contexts, presenting an unprecedented cross-cultural perspective on hate speech in digital media. MultiHateClip contains 2,000 English and Chinese short videos, which not only enriches the understanding of hate speech's multifaceted nature but also marks the first initiative to construct a cross-cultural video dataset dedicated to examining hate speech, specifically targeting gender-related issues across Western and Chinese domains.\nWhen constructing MultiHateClip, we utilize 80 hate-specific lexicons for each language to identify relevant videos on YouTube and Bilibili. Our annotation process involved a team of native-speaker annotators who were familiar with Western and Chinese popular cultures. Following [14]; we differentiate hateful videos from offensive ones and task the annotators to categorize the videos into three distinct groups: hateful, offensive, or normal. Videos identified as hateful or offensive were required to undergo additional annotation to identify segments containing such content, determine the target of hate speech (e.g., Woman, Man, LGBTQ+, etc.), and specify the contributing modalities-whether visual, auditory, or textual.\nWe conducted a thorough analysis on the MultiHateClip dataset and outlined key insights that could influence hateful video detection. We noted a surprisingly low frequency of hate videos on both YouTube and Bilibili, even after a comprehensive review of 10,000+ videos. Ultimately, we curated annotations for 1,000 videos per language, but only around 300+ of them were labeled as hateful or offensive. The analysis of the dataset revealed a consistent pattern: a disproportionate amount of gender-based hate video targeting women, reflecting broader issues gender discrimination in society. Further evaluation of modality contributions in these videos underscored the complexity of hate speech communication. For instance, 80.4% of the hateful/offensive Chinese videos on Bilibili combined multiple modalities, such as visual, auditory, and textual modalities, to convey their hatefulness. The multimodal nature of hate emphasizes the need for an integrated approach that combines multiple modalities for a deeper understanding of hate speech dynamics.\nTo evaluate current models for hate video classification, we tested several state-of-the-art ones on MultiHateClip. In multiclass hate video classification, GPT-4V excelled with a macro F1-score of 0.63 for English, while mBert MFCC Vivit excelled with 0.50 for Chinese. These results underscore the importance of multimodal information, while highlighting critical limitations in current classification approaches: the challenge of differentiating between hateful and offensive content; the inadequacy of pre-trained models on non-Western cultural data; and the insufficient understanding of implicit hate. These findings underscore the necessity for improved video classification models to tackle these identified weaknesses.\nOur research contributions are summarized as follows:\n(1) We constructe MultiHateClip, a multilingual dataset of hateful short clip videos. This dataset is enriched with detailed annotations for videos deemed hateful or offensive, detailing the specific segments with offensiveness, the targeted victims, and the modalities that contribute to the content's offensiveness. Such comprehensive annotations are designed to serve as a foundational resource for subsequent research.\n(2) Our exhaustive examination of MultiHateClip unveils multilingual and cultural-specific characteristics and the critical role of multimodal inputs in hate video detection. These insights are crucial for improving hateful video detection methods, guiding the development of more effective models.\n(3) We have critically evaluated existing video classification models, identifying key areas of weakness: the challenge in differentiating between hateful and offensive content, the limitations of pre-trained models concerning non-Western cultural data, and the insufficient understanding of implicit hate. These evaluations not only underscore existing gaps but also chart potential avenues for future research to enhance hateful video detection."}, {"title": "2 RELATED WORK", "content": "Text-based Hate Speech Detection. Extensive research has focused on detecting hate speech within textual content, producing a variety of datasets from platforms like Twitter [50], Stormfront [15], and Fox News [23]. While many studies engage in binary classification (hate speech or not), works such as [14] and [21] attempt to distinguish between hateful, offensive, and normal speech. Other research targets specific forms of hate speech, such as misogyny [19, 45] and racism [44]. Warner and Hirschberg[43] expand on this by categorizing hate speech into seven victim categories. However, these efforts primarily focus on text, leaving a gap in datasets and analysis for video content[12].\nVideo-based Hate Speech Detection. In contrast to the abundant textual hate speech datasets, video-based datasets remain underdeveloped. For instance, [4] and [46] introduce datasets comprising 400 Portuguese and 300 English YouTube videos, respectively. However, their limited sizes constrain the training of robust multimodal classification models. These studies primarily rely on textual analysis for classification. [13] expands the field by compiling 1,083 BitChute videos into a dataset, although it focuses on English videos with binary hate classification, missing information like contributing modality. Refer to Table 1 for a summary of relevant datasets, incorporating ours (MultiHateClip), in the realm of hateful video detection.\nMultimodal Model Fusion in Hate Speech Detection. Traditional fusion techniques in hate speech detection often involve concatenating representation vectors from pre-trained unimodal models into a composite model, a method known as late fusion. Recent studies have demonstrated efficacy in hateful video classification tasks [13]. Despite its effectiveness, emerging evidence highlights the potential of early fusion could also improve performance [22]. Vision-Language (VL) models, such as VideoBERT [38], ClipBERT [27], UniVL [30], VLM [47], GPT-4V [3] and Qwen-VL[6], demonstrate promise in video analysis tasks."}, {"title": "3 MULTIHATECLIP DATASET", "content": "3.1 Data Collection\nData Sources: We collected videos from the social media platforms YouTube and Bilibili. Launched in 2005, YouTube has emerged as the leading global video-sharing platform, boasting 2.7 billion monthly active users [2], and is known for its comprehensive content moderation policies [39]. Bilibili, established in 2010, has become a central hub for the Chinese animation, comics, and gaming community, with over 3.36 billion monthly active users [1], offering a unique insight into Chinese digital culture.\nVideo Collection: To source videos, we curated 80 pairs of gender-based hate lexicons based on [9] and [26], targeting synonymous terms across English and Chinese, such as 'mistress' and '\u60c5\u5987'. Using the YouTube and Bilibili APIs, we conducted keyword searches from Jan 24 to Feb 4, 2024, targeting short clips no longer than 60 seconds to focus on brief, potentially virulent content. This approach resulted in the collection of 5,600 English and 5,100 Chinese videos. Acknowledging the platforms' content moderation policies, which limit the presence of hate speech, we employed ChatGPT-3.52 to conduct an initial categorization based on video titles and transcripts. This step aimed to sift through the amassed videos, singling out those potentially featuring hateful or offensive content for closer examination. Ultimately, 2,000 videos from each language were selected for detailed manual annotation, ensuring a comprehensive analysis of hate speech trends within the dataset.\n3.2 Human Annotation\nAnnotation Guidelines. Our annotation process requires annotators to address four key questions for each short clip, aimed at comprehensively assessing and categorizing its content.\nQ1) Video Category Labeling: Annotators are asked to classify each video into one of the three categories: Hateful, Offensive, Normal. The category definitions are provided to guide the annotators:\n\u2022 Hateful: Videos that incite discrimination or demean individuals or groups based on attributes such as race, ethnicity, nationality, religion, disability, age, veteran status, sexual orientation, gender identity, etc.\n\u2022 Offensive: Videos that may cause discomfort or distress, yet do not qualify as hateful under the criteria defined above.\n\u2022 Normal: Content devoid of hatefulness or offensiveness. Counter-Narrative:\nQ2) Identification of Hateful/Offensive Segment: For videos classified as hateful or offensive, annotators are tasked with determining the precise segment containing such content. They must specify the start and end times of the segment where the hateful or offensive statements occur. This requirement ensures a targeted analysis of the content, facilitating a more detailed examination of the nature and context of hate speech within the video.\nQ3) Identification of the Target Victim: For videos identified as hateful or offensive, annotators are required to determine the target of the content. Given the dataset's focus on gender-related hate speech, annotators should specify the intended victim group-Man, Woman, or LGBTQ+. Additionally, there is an option to identify other groups if the video targets individuals or communities outside these categories. This step ensures a nuanced understanding of hate speech targeting, allowing for a comprehensive analysis of the videos' impact on various demographics.\nQ4) Determination of Contributing Modality: In this part of the annotation process, annotators are asked to identify which modality-or modalities-of the video contribute to its hateful or offensive nature. They will classify the contribution as stemming from one or more of the following three categories: text (including titles and transcripts), visual content, or audio content. This step is crucial for understanding how hate speech is conveyed through different channels within a video, whether through written words, visual imagery, or spoken language, offering insights into the multimodal dynamics of hate speech dissemination.\nAnnotators Recruitment and Training. Two PhD students, proficient in hate speech, served as expert annotators, supported by a team of 18 undergraduate students. All annotators are Asian individuals aged 18-24. The undergraduate team was balanced in terms of language proficiency-split evenly between the two languages of the study-and gender representation within each language group.\nPrior to the annotation, annotators underwent training to grasp the guidelines and procedures. They annotated a test set of 30 videos across three rounds to assess their adherence to the guidelines. Expert annotators reviewed these annotations to ensure accuracy and provide guidance on errors.\nAnnotators Process. The annotation process was designed for robustness: each video initially received annotations from two different annotators. In instances of disagreement regarding the categorization (hateful, offensive, or normal), a third annotator was enlisted to provide an additional perspective. If disagreements persist, the matter is escalated to the expert annotators for final annotation. The ultimate categorization of each video was determined through a majority vote, ensuring a high level of consensus and reliability in the annotated dataset.\nTo manage the workload efficiently, we implemented a batch annotation strategy, allocating approximately 30 videos to each annotator daily. This task was designed to be manageable within a 30 to 40-minute commitment per day. Expert annotators played a crucial role in quality control, examining the day's annotations to verify label distribution and conducting random checks on selected video annotations. This daily review process allowed for immediate feedback and rectification of any misconceptions, with particular attention paid to outliers or unexpected labeling trends. Annotators periodically discussed ambiguities to ensure a consistent understanding and application of the guidelines."}, {"title": "3.3 Data Statistics and Analysis", "content": "We assessed annotation reliability using Cohen's kappa, yielding scores of 0.62 for English and 0.51 for Chinese in multiclass classification. Simplifying to a binary system (Hateful and Offensive combined), scores increased to 0.72 for English and 0.66 for Chinese. These Cohen's kappa values affirm moderate agreement among our annotators and underscore the robustness of our dataset.\nThe notable variance in kappa scores between Chinese and English stems primarily from annotators' extensive exposure to English social media. In English, 54 out of 227 inconsistent annotations confused hateful with offensive; in Chinese, 101 out of 254 showed similar confusion. Notably, due to the limited availability of Hate-ful/Offensive data, 54 and 101 instances underscored the challenge of distinguishing between Hateful and Offensive.\nDataset Statistics: Each video's transcript was obtained via Google Cloud Speech-to-Text\u00b3. We then documented key metrics such as the number of videos, word counts in titles and transcripts, video lengths, and lengths of segments identified as hateful or offensive. These statistics are presented in Table 2 for English videos and Table 3 for Chinese videos.\nLabel Distribution: MultiHateClip has a 3:7 ratio of hateful/ offensive to normal videos, reflecting the platforms' strict content moderation policies. Interestingly, Normal videos generally have longer titles and transcripts, except for English Hateful videos, which have longer transcripts. Additionally, Hateful videos have a higher proportion of hateful segments compared to Offensive ones.\nVictim Group Analysis: The distribution of hateful and offensive content targeting specific victim groups is detailed in Table 4. Women were the most frequently targeted group across both languages and categories, followed by men and LGBTQ individuals, except in English Hateful videos, where men were more frequently targeted than LGBTQ individuals. A distinction emerges in the targeting of \"Other\" victims: English videos more frequently target individuals based on religion or race, while Chinese videos tend to focus on nationality.\n3.4 Modality Analysis\nThe contribution of different modalities to the hatefulness or offensiveness of content is summarized in Table 5. Key findings include a higher prevalence of text-only hate content in English offensive videos and a significant portion of videos in both languages being classified as hateful or offensive due to multiple modalities. This underscores the importance of a multimodal approach in effectively identifying and classifying hate speech content.\nText Analysis: We first combine video titles and transcripts into a unified text feature, with stop words removed for clarity. For Chinese text, the Jieba Python library was employed for sentence segmentation. A term frequency-inverse document frequency (tf-idf) analysis yielded insights into the most prevalent words across categories, detailed in Table 6. Offensive videos in both languages predominantly feature hate lexicons. Nonetheless, distinguishing between Hateful and Normal videos based solely on text presents challenges, indicating the potential inadequacy of text alone for hate video detection.\nAudio Analysis: Our evaluation of audio content focused on amplitude and Zero Crossing Rate (ZCR) metrics. Amplitude analysis of English videos, represented in Figure 1, highlighted that Hateful and Offensive typically exhibit higher sound intensities. Similarly, ZCR analysis of English videos, shown in Figure 2, indicated that these videos also feature higher rates of zero crossing, pointing to a potentially noisier audio profile. These consistent patterns across languages highlight the audio modality's role in video classification.\nVision Analysis: In our visual data analysis, we extracted one frame per second from each video, applying YOLOv3 for object detection on these frames [33]. This analysis revealed a notable trend: English videos categorized as Hateful and Offensive had a higher detection rate of 'person' objects (70% and 68%, respectively) compared to Normal videos (63%). Conversely, the application of YOLOv3 on Chinese videos resulted in a significant number of detection failures-704 instances for Chinese videos, compared to 289 for English. This discrepancy highlights a crucial limitation of current object detection models, which may stem from the underrepresentation of diverse gender and racial characteristics in training datasets like Coco [28, 51]. Such disparities pose substantial challenges in accurately recognizing individuals in videos from non-Western contexts, thereby impacting the reliability of our visual analysis results for Chinese content."}, {"title": "4 BENCHMARKING MODELS", "content": "In this section, we outline the approach for detecting hatefulness in videos using the MultiHateClip dataset and describe the benchmarking of various models tailored for this purpose.\n4.1 Problem Definition\nThe task is to classify a given video V into one of three categories: Hateful (y = 0), Offensive (y = 1), or Normal (y = 2). This classification considers three modalities within the video: Text (T): Comprising words {W1, W2,..., wm}, aggregated from the video's title and transcript. Audio (A): The auditory component of the video. Vision (V): A sequence of frames {f1, f2,..., fn} extracted from the video.\nThe challenge for the model f is to accurately map these modalities to the ground-truth label y, where f: f(T, A, V) \u2192 y and y belongs to the set {0, 1, 2}. While initially treated as a multiclass classification problem, we also consider a binary classification scenario by combining Hateful and Offensive categories into a single label for a simplified offensive label analysis.\n4.2 Preprocessing\nFor text data, preprocessing involves removing stop words. For vision-based analysis, we select one frame per second and use padding images to standardize the analysis to 60 frames for videos shorter than 60 seconds.\n4.3 Text-Based Models\nmBERT: We use mBERT [17] for its effectiveness in hate speech detection [31] and low-resource languages [40]. Text data is processed with mBERT to extract 768-dimensional features, which are classified into Hateful, Offensive, or Normal via two FC layers. This setup is called Model T1.\nGPT-4-Vision-Preview without vision (GPT-4): In our experiment with GPT-4-Vision-Preview [3], we focus solely on linguistic analysis, excluding vision features. We prepare 4 demonstration examples with video text, a classification query, and the actual label, as shown in Table 7. We then use GPT-4 to predict labels for new video prompts, calling this setup Model T2.\nQwen: Qwen-VL-7B [6], Alibaba Cloud's visual multimodal model, excels in vision-language tasks like captioning and VQA. We use a GPT-4-like prompt for label prediction, calling this setup Model T3.\n4.4 Audio-Based Models\nMFCC: Mel Frequency Cepstral Coefficients (MFCC) are crucial in audio signal processing and effective for audio classification [7, 42, 48]. We generate a 128-dimensional MFCC vector for each audio signal and classify it into Hateful, Offensive, or Normal using two FC layers. This model is called A1.\nWav2Vec2-BERT (Wav2Vec): The Wav2Vec2-BERT model [8], pre-trained on 4.5 million hours of audio in 143+ languages, excels in tasks like ASR and audio classification. We extract 1024-dimensional audio features with Wav2Vec2-BERT and use two FC layers for label prediction. This model is called A2.\n4.5 Vision-Based Models\nViViT: The Video Vision Transformer (ViViT) [5], effective in anomaly [49] and violence detection [37], extracts a 768-dimensional feature vector from 32 sampled frames per video. These are classified through two FC layers, referred to as Model V1.\nViT: The Vision Transformer (ViT) [18], excels in image recognition and detecting offensive content [32]. In our study, ViT processes 60 video frames, generating 768-dimensional feature vectors. These vectors are classified using an LSTM network [25] followed by two FC layers, referred to as Model V2.\n4.6 Vision-Language Models\nVLM: The Vision-Language Model (VLM) [47], using task-agnostic multimodal pre-training, excels in video captioning and retrieval [34]. In our study, VLM extracts two 768-dimensional representations from text and visual inputs, processes them through separate FC layers, then a fusion FC layer, and finally a classification FC layer for video detection. This method is called Model VL1.\nGPT-4-Vision-Preview (GPT-4V): Specifications for the GPT-4V model are in Table 7. This model enhances GPT-4 by integrating visual inputs. In demonstration examples, visual inputs are summarized by GPT-4 with the prompt, \"Please summarize the video's content.\" For new videos, we sample 4* N frames and combine four frames into a single image to increase information density. Both demonstration examples and video prompts are processed by GPT-4V to predict classification labels, designated as Model VL2.\nQwen-VL: The prompt format follows the GPT-4V model, but utilizes the Qwen-VL model. This model is designated as VL3.\n4.7 Multimodal Models\nT10 A1 V1: As shown in Figure 3, this model uses a fusion strategy combining mBERT for text, ViViT for visual features, and MFCC for audio, chosen for their superior performance within their modalities. Each set of features is processed through separate FC layers, then concatenated and classified through an additional FC layer. This integrated approach is called Model M1."}, {"title": "5 EXPERIMENT", "content": "5.1 Experiments setup\nOur dataset was divided into training (70%), validation (10%), and testing (20%) subsets, preserving label distributions. We detail each model's layer configurations below:\n\u2022 mBERT: 768-dimensional output passed through two FC layers (128 and 3 units).\n\u2022 MFCC: 128-dimensional MFCC outputs input into two FC layers (128 and 3 units).\n\u2022 Wav2Vec: 1024-dimensional MFCC outputs input into two FC layers (128 and 3 units).\n\u2022 ViViT: 768-dimensional ViViT outputs input into two FC layers (128 and 3 units).\n\u2022 ViT: Utilizes a 128-unit LSTM layer followed by two FC layers (128 and 3 units).\n\u2022 VLM: Two 768-dimensional outputs undergo processing through two FC layers (128 units each), followed by fusion using one FC layer (128 units) and one FC layer for classification (3 units).\n\u2022 GPT-4V and Qwen-VL: We experimented with prompt configurations using 2 to 16 images (N), finding N=4 optimal for both models in terms of accuracy and F1 scores.\nFor multimodal models, text and vision feature outputs are processed through two 128-unit FC layers, while audio feature outputs undergo processing through one 128-unit FC layer. These outputs are concatenated for final prediction through one 128-unit FC layer and one 3-unit FC layer. Each model's output, undergoes log-softmax transformation followed by cross-entropy loss calculation to estimate video label probabilities. Training spans 40 epochs using the Adam optimizer, with a batch size of 16 and a learning rate of 1e-4. Test performance is recorded when the model achieves its best Macro-F1 score on the validation set.\n5.2 Evaluation metrics\nTo align with research norms, we use standard metrics: Accuracy, Macro F1 score, F1, precision, and recall, to evaluate our models on Hateful, Offensive, and Normal videos. For simplicity, Hateful and Offensive labels are combined into a single Offensive label for binary classification. The top metric is in bold, and the second-best is underscored.\n5.3 Performance Evaluation\nEach model is run five times, except for GPT-4V and Qwen-VL related models, with results averaged. Model performance on multiclass classification tasks is outlined in Tables 8 (English videos) and 9 (Chinese videos).\nThe incorporation of multiple modalities notably boosts performance, as evidenced by comparisons such as M1 versus T1, A1, V1. To gauge the significance of these improvements, we calculate the p-value between the multimodal model (M1) and its best-performing unimodal counterpart (V1). Results show a significant enhancement for M1 compared to V1, attributed to the inclusion of text and audio modalities. For large vision language models like VL2 and VL3, integration of the vision modality generally enhances performance, with the exception of T3 outperforming VL3 in English data, possibly due to inconsistent performance of Qwen-VL, a Chinese model, on non-Chinese data. T2 and VL2 excel in English, highlighting the GPT-4V model's strength, while V1 and M1 lead in Chinese. Notably, VL2's top English performance surpasses M1's best Chinese performance by 13% in Macro-F1 Score.\nFurther analysis of F1 scores for individual labels reveals some instances where the Hateful category obtains a score of zero, indicating challenges in distinguishing between Hateful and Offensive content, exacerbated by the low representation of Hateful videos in the dataset. This highlights the necessity of binary classification results for a fair comparison of model efficacy.\nBinary classification results, detailed in Tables 8 for English and 9 for Chinese videos. T2 and VL2 for English, and V1 and M1 for Chinese, consistently outperformed other models across most metrics. Furthermore, significant improvements were observed in evaluation metrics for the combined Offensive label across all three F1, Recall, and Precision metrics compared to initial separate Hateful and Offensive Label evaluations.\n5.4 Error Analysis\nTable 10 shows four English examples highlighting the importance of multimodal information and nuanced understanding. T1 and V1 make incorrect predictions in the first two examples, but VL2 and M1, using more modalities, predict correctly. Additionally, in 14 of 200 cases, T1 and V1's unimodal predictions are wrong, whereas VL2 and M1's multimodal predictions are accurate.\nIn the third and fourth examples, accurate prediction needs a deep understanding of video content. GPT-4V succeeds in both cases, while M1 fails. For the third example, M1's incorrect prediction likely stems from accumulating hatefulness across modalities, an issue also seen in the second example. Accurate hatefulness predictions require genuine content understanding, which M1 lacks. This issue resulted in 54 cases where M1 overpredicted hatefulness compared to T1, leading to 15 mispredictions. In the fourth example, implicit hatefulness in text and vision components caused T1, V1, and M1 to misclassify the video as Normal. However, GPT-4V correctly identified it as Hateful. There are 11 similar cases where T1, V1, and M1 failed, but GPT-4V succeeded. These examples highlight GPT-4V's superior performance in understanding complex multimodal content.\n5.5 Limitations of Existing Models\nOur evaluation of the baseline models uncovers three significant challenges impacting their efficacy in hateful video detection:\nDistinguishing Between Hateful and Offensive Content: Models often struggle to differentiate between Hateful and Offensive content, leading to frequent misclassifications. In the English data, 5 out of 11 models misclassify all Hateful videos. All models perform better at distinguishing Hateful/Offensive from Normal content. For example, the GPT-4V model has F1 scores of 0.36 for Hateful and 0.66 for Offensive, which increases to 0.73 when these labels are combined into a single Offensive label. This issue is also seen in the Chinese data, where models can separate Hateful/Offensive from Normal content but struggle to distinguish between Hateful and Offensive.\nTraining Deficiencies on Non-Western Cultural Data: Large-scale vision models like VL1, VL2, and the multimodal model M1 perform worse on Chinese datasets than on English ones due to limited non-Western cultural training data. As highlighted by [41], there is a notable scarcity of non-Western hate speech datasets, with only 25 languages represented across 125 datasets, predominantly English (59 datasets). Chinese has only one documented dataset [26], emphasizing the need for research in non-Western contexts. The recent emergence of Chinese hate speech datasets [16, 29] reflects growing interest in this area.\nInsufficient understanding of implicit content of current models. GPT-4V excelled in English data primarily due to its extensive training data, enabling it to deeply understand both text and visual content. Certain videos, such as the fourth example in Table 10, contain implicit hate, necessitating large models capable of comprehending implicit content-an area where most hateful video classification models exhibit weakness.\nThese findings emphasize the necessity for tailored approaches in model training and modality fusion to effectively address the nuanced and culturally diverse nature of hate speech in videos."}, {"title": "6 CONCLUSION", "content": "This study introduces MultiHateClip, a multilingual dataset for hateful video detection, enriched with fine-grained labels across English and Chinese languages. Our investigation demonstrates the significant superiority of vision-language and multimodal models (GPT-4V and mBert MFCC Vivit) over unimodal models, highlighting the importance of modality integration. MultiHateClip not only provides a valuable resource for advancing this field but also underscores the critical necessity for multimodal analysis in understanding and combating hate speech in a multilingual context through extensive experimentation."}]}