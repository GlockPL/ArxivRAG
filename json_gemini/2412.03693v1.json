{"title": "System Test Case Design from Requirements Specifications: Insights and Challenges of Using ChatGPT", "authors": ["Shreya Bhatia", "Tarushi Gandhi", "Dhruv Kumar", "Pankaj Jalote"], "abstract": "System testing is essential in any software development project to ensure that the final products meet the requirements. Creating comprehensive test cases for system testing from requirements is often challenging and time-consuming. This paper explores the effectiveness of using Large Language Models (LLMs) to generate test case designs from Software Requirements Specification (SRS) documents. In this study, we collected the SRS documents of five software engineering projects containing functional and non-functional requirements, which were implemented, tested, and delivered by respective developer teams. For generating test case designs, we used ChatGPT-40 Turbo model. We employed prompt-chaining, starting with an initial context-setting prompt, followed by prompts to generate test cases for each use case. We assessed the quality of the generated test case designs through feedback from the same developer teams as mentioned above. Our experiments show that about 87 percent of the generated test cases were valid, with the remaining 13 percent either not applicable or redundant. Notably, 15 percent of the valid test cases were previously not considered by developers in their testing. We also tasked ChatGPT with identifying redundant test cases, which were subsequently validated by the respective developers to identify false positives and to uncover any redundant test cases that may have been missed by the developers themselves. This study highlights the potential of leveraging LLMs for test generation from the Requirements Specification document and also for assisting developers in quickly identifying and addressing redundancies, ultimately improving test suite quality and efficiency of the testing procedure.", "sections": [{"title": "1 Introduction", "content": "A Software Requirements Specification (SRS) is a document that outlines the functional and non-functional requirements of a software system [11]. Functional requirements describe the system's behaviour under specific conditions, often in the form of use cases. A use case is a usage scenario for a piece of software, listing event steps that typically define the interactions between a user role (actor) and the system to complete a requirement. System testing involves validating the developed software against these requirements to ensure its correctness and completeness by using a set of test cases [2].\nEffectiveness of any testing depends on the quality of test case designs for that testing. Test case designs involve identifying scenarios, inputs and expected outputs to validate software functionality. For example, a test case design for a login page will include steps to enter valid credentials and successful login and also steps to enter invalid credentials and get error messages [18].\nGenerating test case designs for system testing is a critical task but can be laborious and error-prone when done manually. It requires understanding the SRS thoroughly and devising test cases to cover various scenarios adequately, including edge or exceptional scenarios. This time-consuming process may lead to missing some important test cases or having redundant test cases [26].\nLarge Language Models (LLMs), such as OpenAI's GPT [5, 19], have shown promise in natural language understanding and generation tasks. Leveraging LLMs for test case design generation from SRS documents can potentially automate and expedite this process, benefiting software engineers by reducing the effort required for"}, {"title": "2 Methodology", "content": "In this section, we outline the characteristics of our dataset and explain the approach used to design an effective prompt for ChatGPT. Subsequently, the designed prompt is used to generate test-case designs from requirements documents using ChatGPT. We also discuss the evaluation metrics used to assess the quality and effectiveness of the generated test cases."}, {"title": "2.1 Dataset: Projects Used in the Study", "content": "We assembled a dataset consisting of five Software Requirements Specification (SRS) documents from student engineering projects. These projects were developed for real clients, and were implemented and tested before delivering them to the client. Each SRS document includes functional and non-functional requirements, with functionality specified as use cases.\nThe five projects varied in size, ranging from 7,000 to 12,000 lines of code, with team sizes of 3 to 4 members. Each SRS document contained between 11 and 29 use cases, catering to 3 to 4 distinct user types. The technological stack utilized in these projects commonly included HTML and ReactJS for the frontend, Django and NodeJS at the backend, and PostgreSQL, MySQL, MongoDB as the database."}, {"title": "2.2 Prompt Design", "content": "To generate effective test case designs, the initial step was to develop a prompting approach that would yield best test case designs for a given SRS. Drawing insights from recommended prompting patterns such as role prompting, using delimiters, specifying output formats and providing clear task descriptions [6, 31], we experimented with two prompting approaches:\nApproach 1 - Single prompt approach: In this approach, we constructed one prompt comprising (1) the entire SRS (2) instructions to generate the test cases along with the specified output format for the same. Our experiments showed that the test case designs generated were not very elaborate; potentially indicating that the prompt was too long, and the task was too complex to be accomplished in one prompt. On an average, 2-3 test designs were generated per use case.\nApproach 2 - Prompt Chaining: In this approach, we first provided the SRS in the initial prompt. This was followed by individual prompts for each use case, directing the LLM to generate test cases in a specified format. We observed a significant increase in the average number of test cases generated, with approximately 9-11 test cases per use case."}, {"title": "2.3 Test Case Generation", "content": "We employed ChatGPT-40 Turbo model with the prompting approach 2 mentioned above for each of the five SRS documents to generate test-case designs. First, ChatGPT familiarized itself with the SRS. Then, for each use case, it generated test cases in a standardised tabular format, detailing conditions, input actions, expected outputs, and comments, facilitating organized analysis. So, if there were 12 use cases in the SRS, we prompt ChatGPT a total of 13 times (12+1)-the first prompt will familiarize the LLM with the SRS, followed by 12 prompts, each focusing on generating test case designs for a specific use case scenario.\nAditonally, to mitigate randomness in the experiment, we standardised all tests by using the default temperature setting and the"}, {"title": "2.4 Qualitative Evaluation of Test Cases", "content": "To assess the quality and effectiveness of the test cases generated by ChatGPT, we conducted a qualitative evaluation with feedback from the developers who authored the SRS documents. This evaluation aimed to determine the utility, relevance, and completeness of the generated test case designs in capturing the intended functional requirements, by ensuring that the test cases are evaluated by developers with a deep understanding of the requirements and functional specifications of their respective projects.\nFeedback Collection: For every use case, the developers were asked to review the test case designs generated by ChatGPT and provide feedback for each test case using the following criteria:\n(1) Valid: A test case is marked as valid if it is suitable for verifying the functionality of the use case."}, {"title": "2.5 Validation of Redundant Test cases Identified by ChatGPT", "content": "Redundant test cases are undesirable in a test suite as they lead to unnecessary duplication of effort, increased execution time, and difficulty in maintaining clarity and coverage across the test suite. Identifying and eliminating redundancies ensures a more efficient and streamlined testing process, helping developers focus on meaningful tests that provide value.\nTo address this, we extended our methodology to include the identification of redundant test cases among those generated by ChatGPT. Redundant test cases were analyzed in the context of the entire SRS, as identifying redundancies at the use case level does not provide meaningful insights due to overlaps across use cases. Prompting ChatGPT, we explicitly asked it to flag test cases that might overlap or repeat existing ones within the generated suite. Simultaneously, developers reviewed the GPT-generated test cases and flagged redundancies from their perspective.\nWe then compared the two sets of identified redundancies: those flagged by ChatGPT and those flagged by developers. For overlapping redundancies, developers validated ChatGPT's findings, categorizing them as either valid redundancies or false positives. Additionally, ChatGPT flagged new redundant test cases that were missed by developers. This process provided insights into the effectiveness of LLMs in identifying redundancies and their potential to enhance developer workflows by quickly pinpointing inefficiencies in test suites."}, {"title": "3 Results", "content": "Answering RQ1: How effective are Large Language Models (LLMs) in generating comprehensive and valid test cases directly from Software Requirements Specifications (SRS) documents?\nWe observed that on average ChatGPT generated 10-11 test cases per use case on average, using a two-stage prompt-chaining approach. Based on the feedback gathered from developers, we found that about 72.5 percent of the generated test cases were valid and had been incorporated by the developers in their actual testing process. Additionally, 15.2 percent of the generated test cases"}, {"title": "4 Related Work", "content": "Traditional Approaches: Generating test cases from SRS documents traditionally involves manual techniques [25], keyword-based methods [13, 27], and model-based approaches [1, 3, 7], as well as other advanced methods [20, 28, 33]. However, these approaches often suffer from limitations such as scalability and subjectivity bias.\nRecent Advances in LLMs: The advent of large language models (LLMs), such as ChatGPT, has introduced a promising alternative for automated test case generation. LLMs have demonstrated proficiency in natural language understanding and code generation tasks, which can be leveraged to transform test scenario extraction from textual requirements.\nPrompt Engineering: Prompt engineering is crucial for generating effective and efficient responses from LLMs. Several studies [6, 8, 12, 30, 31] have cataloged prompt patterns and utilized foundational principles like role-prompting, prompt-chaining, and chain-of-thought prompting to optimize LLM efficacy. We applied these techniques in our experiments, refining prompts to identify the most effective ones for our task.\nLLMs in Software Development: Lin et al. [14] explored the use of LLMs for generating code as part of the software development process. Their work emphasizes the importance of preprocessing and fine-tuning LLMs on software-related data to develop a robust code generation pipeline that leverages LLMs' semantic understanding.\nAutomated User Story Generation: Rahman et al. [21] investigated automated generation of user stories from textual requirements using sophisticated natural language processing techniques. Their study received positive feedback from developers using the RUST questionnaire, highlighting the potential of LLMs in this domain.\nTest Case Generation using LLMs: Several studies have focused on using LLMs for generating system test case inputs and automating different stages of software testing activities: Wang et al. [29]conducted a comprehensive survey on the application of LLMs in software testing. Mathur et al. [17] proposed the use of T5 and GPT-3 for automated test case generation. Luu et al. [15] reported on the use of LLMs like ChatGPT for metamorphic testing. Yu et al.[32] explored the challenges, capabilities, and opportunities of using LLMs for test script generation and migration.\nLLMs in Unit Test Generation: LLMs have also been employed to generate unit tests from source code. Studies such as those by Sapozhnikov et al. [22] and El Haji et al. [10] demonstrated the utility of LLMs in generating readable and comprehensive unit test cases. There are also studies on how LLMs can be used to generate unit test cases which will help the programmer by generating test cases that are more readable and cover a variety of edge cases [4, 9, 23, 24].\nOur Contribution: While existing studies have explored various aspects of LLMs in software testing, our work differs by providing a focused and comprehensive evaluation of generated test cases from SRS documents. We incorporate feedback directly from developers who authored and implemented the SRS, classifying the generated test cases into five categories: (1) Valid cases, (2) Redundant conditions, (3) Not implemented but valid cases, (4) Not Applicable, and (5) Missed test cases (detailed in Section 2 and Section 3)."}, {"title": "5 Conclusion and Future Work", "content": "Developing test cases from requirements for system testing is a critical task which impacts the quality of the final software. Manual generation of these test cases can be time-consuming and error-prone. In this paper, we report the results of a study on using Large Language Models (LLMs) for generating test case designs from Software Requirements Specification (SRS) documents.\nIn our experiment, we utilized five completed software projects for which Software Requirements Specifications (SRSs) were available. Acquiring requirements documentation for real-world engineering projects is inherently challenging due to their highly confidential nature, in addition to interviewing and gathering feedback from the developer teams at each step of experimentation and categorization of the LLM-generated results. While we acknowledge that the dataset is small and may not sufficiently generalize the results, this work serves as an initial step toward exploring the potential of generating test cases from requirements documents. Furthermore, it lays the groundwork for identifying areas of future improvements aimed at enhancing the reliability of these test cases.\nWe utilized a two-step prompting approach to guide the LLM towards generating the test cases. In the first, the LLM is familiarized with the SRS. In the second stage, prompts ask it to generate test cases for each use case. This prompting technique was superior to the technique of asking the LLM to generate test cases for the entire SRS together.\nOur experiments show that LLMs were able to generate good test case designs, and a vast majority of them were valid. It was also able to generate a significant number of test cases that were missed by the developers earlier.\nOne limitation of the generated test cases is the presence of redundant test conditions. Perhaps more targeted prompting may be able to reduce these. [6, 8, 12, 30, 31]; Additionally, our redundancy analysis emphasizes the role of LLMs in refining test case suites by identifying both overlooked and redundant conditions. But a point to note is the existence of some False positives in ChatGPT-identified redundant test cases which if removed, could reduce the effectiveness of the test suites. Al tools are not yet advanced enough to be used for testing purposes as a sole entity but can be used to assist and reduce software developers' efforts to a great extent.\nWe plan to extend the study to the comparison of multiple LLMs and gather a more extensive dataset of Requirements Specification Documents. Future work will focus on improving the precision of LLMs in redundancy detection, potentially through fine-tuning on software engineering datasets or integrating system-level context. By addressing false positives and enhancing contextual awareness, LLMs could become even more effective in supporting efficient and high-quality system testing. To improve upon ChatGPT's limited understanding of system behaviours and design specifications, we could additionally use the Architecture Design document of the engineering project. Such a document would describe the components and design specifications required to support the application, and incorporation of these details could enhance the robustness of test case generation."}]}