{"title": "SEKE: Specialised Experts for Keyword Extraction", "authors": ["Matej Martinc", "Hanh Thi Hong Tran", "Senja Pollak", "Boshko Koloski"], "abstract": "Keyword extraction involves identifying the most descriptive words in a document, allowing automatic categorisation and summarisation of large quantities of diverse textual data. Relying on the insight that real-world keyword detection often requires handling of diverse content, we propose a novel supervised keyword extraction approach based on the mixture of experts (MoE) technique. MoE uses a learnable routing sub-network to direct information to specialised experts, allowing them to specialize in distinct regions of the input space. SEKE, a mixture of Specialised Experts for supervised Keyword Extraction, uses DeBERTa as the backbone model and builds on the MoE framework, where experts attend to each token, by integrating it with a recurrent neural network (RNN), to allow successful extraction even on smaller corpora, where specialisation is harder due to lack of training data. The MoE framework also provides an insight into inner workings of individual experts, enhancing the explainability of the ap- proach. We benchmark SEKE on multiple English datasets, achieving state-of-the-art performance compared to strong supervised and unsupervised baselines. Our analysis reveals that depending on data size and type, experts specialize in distinct syntactic and semantic components, such as punctua- tion, stopwords, parts-of-speech, or named entities. Code is available at https: //github.com/matejMartinc/ SEKE_keyword_extraction.", "sections": [{"title": "1 Introduction", "content": "Keyword extraction is crucial for efficiently sum- marizing, indexing, and retrieving relevant infor- mation from large textual corpora, making it one of the most fundamental NLP tasks (Song et al., 2023). Automatically extracting keywords is de- fined as the automatic extraction of words that rep- resent crucial semantic aspects of the text. While the first automated solutions have been proposed more than two decades ago (Tumey, 1999; Mihal- cea and Tarau, 2004), the task is far from solved, and the scientific community is still actively trying to improve the performance of the keyword extrac- tors, proposing several new algorithms in recent years (see Section 2 for details).\nRecently, the so-called foundation models have completely revolutionised the way natural lan- guage processing (NLP) is done, and the devel- opment of large language models (LLMs) has en- abled task-agnostic architectures to solve down- stream NLP tasks in a zero-shot fashion without the need for labelled data (Brown et al., 2020). These breakthroughs have been enabled by the in- tegration of novel training regimes and architec- tural components, such as Low-Rank Adaptation (LoRA) (Hu et al., 2021) and Mixture of Experts (MoE) (Jacobs et al., 1991), which make models more efficient, while requiring less computational resources.\nWhile these components have been successfully employed in an unsupervised language model set- ting with abundant data, there are less studies ex- ploring their effectiveness in a supervised setting with much fewer resources. The main reason for this is that these components were developed with a specific objective of improving the scalability of the models (Shazeer et al., 2017) and there- fore their effect on the performance of the mod- els is somewhat neglected or measured only indi- rectly, that is, studies focus on how the use of these components allows for greater scalability, which in turn leads to performance gains.\nIn contrast, in this study, we focus on the perfor- mance improvements obtained from these compo- nents without increasing the size of the backbone model. More specifically, we are interested in ex-"}, {"title": "2 Related Work", "content": "The idea for Mixture of experts (MoE) comes from the study by Jacobs et al. (1991). Like ensemble methods, the concept involved a supervised proce- dure for a system composed of separate networks, each responsible for a different subset of training cases. Each network, or expert, specializes in a distinct region of the input space. The choice of expert is managed by a gating network, which de- termines the weights for each expert. Both the ex- perts and the gating network are trained during the training process.\nMore recently, the study by Eigen et al. (2013) proposed integrating MoE as components within deep neural networks. This allowed MoE to func- tion as layers in a multilayer network, enabling models to be both large and efficient, which led to the employment of the MoE method in the NLP setting. The study by Shazeer et al. (2017) proposed a 137-billion-parameter LSTM model (the prevalent NLP architecture for language mod- elling at the time) by introducing sparsity, which allowed for very fast inference even at large scales. This work, which focused on translation, encoun- tered several challenges such as high communica- tion costs and training instabilities. After the suc- cess of the Mixtral 8x7B (Jiang et al., 2024), MoE"}, {"title": "2.1 Mixture of Experts", "content": "ploring the question of whether MoE can improve the performance of the model on a specific down- stream task, in our case keyword extraction. The general hypothesis we want to test is whether it is beneficial for keyword extraction to allow spe- cific parts of the network to specialize for specific types of tokens. More specifically, we explore if introducing a gating network that assigns different parts of the sequence to specialised layers, which would only attend to specific subsequences and to- kens according to their semantic and grammati- cal role, could improve the current state-of-the-art (SOTA) in the field. In addition, we explore what type of specialisation different experts undertake and how much data is required for a successful specialisation. For these reasons, we test our ap- proach on several datasets with different amounts of training data available, to determine the data threshold that still allows the convergence of ex- pert layers. Finally, we examine whether synergy can be achieved between MoE and recurrent neu- ral network (RNN) that would lower this thresh- old.\nThe contributions of the paper are as follows.\n\u2022 We propose a novel token classification head architecture (see Figure 1) that combines MoE and RNN. The approach outperforms SOTA-supervised sequence-labelling models on most keyword extraction datasets. As far as we know, this is the first research that studies the usage of MoE in a supervised sequence-labelling scenario.\n\u2022 We demonstrate that experts specialize in different syntactic and semantic structures, ranging from punctuation characters and parts-of-speech tags to specific words and named entities, depending on the domain they are trained on and the size of the training data.\n\u2022 We show that adding MoE layers during fine- tuning generally has a positive effect on the model's performance no matter the dataset, but is somewhat dependent on the amount of data available. In addition, we show that the data requirements can be lowered by intro- ducing additional recurrent layers."}, {"title": "2.2 Keyword Extraction", "content": "Contemporary studies on keyword extraction treat it either as a text generation or a sequence labelling task. Yuan et al. (2020) proposed an encoder- decoder RNN architecture featuring two mecha- nisms, semantic coverage and orthogonal regu- larisation, which treated keyword extraction as text generation. These mechanisms ensure that the overall inner representation of a generated keyword sequence is semantically aligned with the overall meaning of the source text. In con- trast, several recent sequence labelling approaches tackle the task with transformers. Sahrawat et al. (2020) tested several transformer architectures, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and GPT-2, and two distinct se- quence labelling heads, a bidirectional Long short- term memory head (BiLSTM) and a BiLSTM head with an additional Conditional random fields layer (BiLSTM-CRF). The TNT-KID approach (Martinc et al., 2022) on the other hand adapted the transformer architecture specifically for key- word extraction tasks by re-parameterizing the at- tention mechanism to focus more on positional information. Finally, Sun et al. (2021) proposed JointKPE, an open domain keyphrase extraction (KPE) architecture also based on pre-trained trans- former language models. It ranks keyphrases by assessing their informativeness across the en- tire document and is simultaneously trained on a keyphrase chunking task to ensure the phraseness of the keyphrase candidates.\nJust recently, Luo et al. (2023) proposed Diff-KPE, a text diffusion approach to gener-"}, {"title": "3 Methods", "content": "ate enhanced keyphrase representations utilizing the supervised Variational Information Bottleneck (VIB). Diff-KPE generates keyphrase embeddings conditioned on the entire document using BERT and then injects the generated keyphrase embed- dings into each phrase representation. By optimiz- ing the ranking network and VIB simultaneously, they manage to rank each candidate phrase by uti- lizing both the information of keyphrases and the entire document.\nKeyword extraction can also be tackled in an unsupervised manner. In general, these ap- proaches can be divided into four main categories, namely statistical, graph-based, embeddings- based, and language model-based methods. Sta- tistical methods, such as YAKE (Campos et al., 2020), use the statistical characteristics of texts to capture keywords. Graph-based methods, such as TextRank (Mihalcea and Tarau, 2004) and RaKUn (\u0160krlj et al., 2019) build graphs to rank words based on their position in the graph. Embedding- based methods, such as Key2Vec (Mahata et al., 2018), employ semantic information from dis- tributed word and sentence representations for keyword extraction.\nNotably, large language model-based methods tend to be highly effective, driven by the rise of LLMs, such as ChatGPT (Brown et al., 2020). These models have been employed in several recent studies (Mart\u00ednez-Cruz et al., 2023; Luo et al., 2023), demonstrating promising results due to their zero-shot capabilities, which allow them to perform tasks without task-specific fine-tuning (Kojima et al., 2022). However, Zhang et al. (2024) has recently highlighted the theoretical drawbacks of autoregressively trained LLM mod- els for semantic classification tasks, attributing these limitations to their fixed token positioning, which restricts inter-sample connections. In con- trast, masked language models, such as BERT (Devlin et al., 2019), leverage flexible token tar- geting to overcome these issues. Building on these findings, we select the DeBERTa (He et al., 2020) model as a strong backbone."}, {"title": "3.1 Architecture", "content": "Our approach (visualised in Figure 1) is based on fine-tuning a pre-trained transformer architecture, in our case DeBERTa (He et al., 2020), with a spe- cialised token classification head. The architec-"}, {"title": "4 Experimental Setting", "content": "ture was chosen since it showcased a SOTA perfor- mance on several downstream NLP tasks, among them also sequence-labelling tasks such as NER (Shon et al., 2022). Additionally, we hypothesised that the model is especially appropriate for key- word extraction due to its novel disentangled at- tention mechanism. More specifically, in the De- BERTa model, each word is represented using two vectors that encode its content and position sep- arately, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. We assumed that this would allow the model to bet- ter distinguish between the positional and seman- tic/lexical information and therefore make it possi- ble to assign attention to some tokens purely based on their position in the text, improving the overall performance. This hypothesis is based on previ- ous study by Martinc et al. (2022) showing that to- ken position is especially important in the keyword identification task due to the disproportionate dis- tribution of keywords in the document, which is skewed towards the beginning of the document (i.e., more keywords appear at the beginning of the document). By considering the importance of positional information, they fed the positional en- coding to the attention mechanism directly, subse- quently managing to improve the performance.\nThe main novelty of our approach is a cus- tomised token classification head containing sev- eral components. The output logits from the De- BERTa backbone are first fed to the gate network or router that determines which tokens in the input sequence are sent to which expert. More specif- ically, in the setting with n experts, the gate net- work (G) decides which experts (E) receive a part of the input:\n$$y = \\sum_{i=1}^{n}G(x)_i E_i(x)$$\nWe model the routing as a weighted multiplica- tion, where hypothetically all experts could con- tribute to processing each part of the input. Nev- ertheless, the additional top_k parameter ensures that only top k experts contribute to the respec- tive part of the input, and other experts are ex- cluded from the computational graph, hence sav- ing computational resources. Following Shazeer et al. (2017), we employ the so-called Noisy Top- k Gating, which introduces some (tunable) noise and then keeps the top k values. Noise is intro-"}, {"title": "4.1 Datasets", "content": "We conduct experiments on selected benchmark datasets used in the SOTA papers to which we compare our approach. More specifically, we experiment on 6 datasets from the related work (Meng et al., 2017; Martinc et al., 2022; Xiong et al., 2019), covering three domains, news, sci- entific articles, and web pages."}, {"title": "4.2 Experiments", "content": "duced for load balancing, i.e., to prevent the gat- ing network from converging in a way that the same few experts are activated for most of the to- kens, which would make training inefficient. The final weight distribution is obtained by feeding the input (x) through a dense layer (Wg) and adding some random noise in the following way:\n$$H(x)_i = (xW_g)_i+RND\\cdot Softplus((x\\cdot W_{noise})_i)$$\nHere, RND represents a weight vector of ran- dom numbers from a normal distribution with mean 0 and variance 1. After that, we filter out only the top k experts out of v:\n$$KeepTopK(v, k)_i = \\{v_i\\;if \\;v_i\\;in\\;top\\;k, \\;else\\;-\\infty\\}$$\nWe finally apply the softmax to filtered experts:\n$$G(x) = Softmax(KeepTopK(H(x),k))$$\nNote that the router is composed of learnable parameters and is fine-tuned at the same time as the rest of the network. On the other hand, each expert is represented as a neural network consist- ing of three sequential dense layers Wg with addi- tional activation and dropout layers. More specifi- cally, the expert is represented as:\n$$E(x) = Dropout(ReLU(x\\cdot W_g)\\cdot W_g)\\cdot W_g$$\nThe outputs of different experts, which attend to specific tokens assigned to them by the rout- ing network, are first reassembled into a single sequence representation. Then, a two-layer ran- domly initialised encoder, consisting of dropout and two RNN layers, is added (with element-wise summation) to this sequence. The initial motiva- tion behind this adaptation is connected with find- ings from the related work which suggest that re- current layers are quite successful at modelling the positional importance of tokens in the keyword detection task and by the study of Martinc et al. (2022), who reported good results when an RNN classifier and contextual embeddings generated by transformers were employed for keyword detec- tion.\nThe output sequence of the MoE and RNN en- coders is finally fed to the feed-forward classifica- tion layer, which returns the output matrix of size"}, {"title": "5 Results and Analysis", "content": "We test different settings to determine the effect that the proposed architectural additions have on the performance of the model. We are interested in the specific contributions of the MoE and RNN layers to the performance of the model (either sep- arately or together); therefore, we compare the set- tings containing these components to the baseline DeBERTa model with just a (usual) feed-forward token classification head.\nWe employ the DeBERTa model\u00b9 and fine-tune it using Low-Rank Adaptation (LoRA) (Hu et al., 2021). We freeze all layers in the backbone model and train only low-rank perturbations to query and value weight matrices in the model. Additionally, we train all the matrices in the custom token clas- sification head (i.e., MoE, RNN, and dense clas- sification layers are randomly initialised and fine- tuned). We use the same hyperparameter setting across all datasets, namely, the following values were used during fine-tuning: learning rate of 2e- 4, LoRA intrinsic rank of 16, LoRA alpha of 16, LORA dropout of 0.1, and sequence length of 256 for all tasks.\nIn order to determine how does training dataset size effect our approach, we employ two distinct training scenarios. In the validation set training scenario, the DeBERTa model was fine-tuned just on each dataset's validation sets, which were ran- domly split into 80 percent of documents used for fine-tuning and 20 percent of documents used for hyperparameter optimisation and test set model selection. We fine-tune the models for up to 20 epochs and employ early stopping. Note that the JPTimes dataset with no available predefined validation-test splits is only used for testing. Here, the model was fine-tuned on the KPTimes-valid dataset.\nIn the full training scenario, we fine-tune the models on all available data, same as in related work (Meng et al., 2017; Martinc et al., 2022). More specifically, before fine-tuning the model on three relatively small validation datasets contain- ing scientific papers (KP20k-valid, Inspec-valid, and Krapivin-valid), we first \u201cpre-train\u201d the model (using the same token classification objective as in the validation dataset fine-tuning stage) for 20 epochs on the large KP20k-train dataset also con-"}, {"title": "5.1 Examining the Specialisation of Experts", "content": "taining scientific articles. We do the same for news datasets, i.e., before fine-tuning the model on the KPTimes-valid dataset, we \"pre-train\" it on the large KPTimes-train dataset. For the JPTimes dataset with no available predefined validation- test splits, the keyword detection model was \u201cpre- trained\" on KPTimes-train and fine-tuned on the KPTimes-valid dataset. For the openKP dataset, the model is \"pre-trained\" on the openKP-train dataset.\nTo assess the performance of our models and compare the results of our keyword extraction ap- proach to other SOTA methods, we adopt the same evaluation methodology as in Meng et al. (2017); Martinc et al. (2022); Luo et al. (2023). We use the same test splits for the sake of comparabil- ity between our approach and others, and measure F1@k with k\u2208 {1,3,5} on the openKP dataset and k \u2208 {5, 10} on other datasets. To determine the exact match, we first lowercase the candidate key- words and the gold standard keywords, and then apply Porter Stemmer (Porter, 1980) to both.\nIn Table 2, we present the results achieved by our approach and compare them to several baselines (see Section 2.2 for details). TF-IDF (we list re- sults from Martinc et al. (2022)), TextRank (we list results from Martinc et al. (2022)), ChatGPT (we report results for the gpt-3.5-turbo version tested in Luo et al. (2023)) and ChatGLM2-6B (we list results from Luo et al. (2023)) algorithms are un- supervised and do not require any training. For the supervised baselines, CopyRNN, CatSeqD, GPT- 2, GPT-2+BiLSTM-CRF, and TNT-KID, we list results reported by Martinc et al. (2022), for Diff- KPE, we list results reported by Luo et al. (2023) and for JointKPE, we list results from Sun et al. (2021). Besides testing our custom sequence la- belling head (containing MoE, RNN, or both), we also report results for the unmodified pre-trained DeBERTa model with a standard feed-forward to- ken classification head. For it, we apply the same fine-tuning regime as for our experiments with a custom classification head.\nIn the validation set training scenario, by adding the MoE on top of the DeBERTa model (see configuration DeBERTa MoE), we improve the performance of the vanilla model (see con- figuration DeBERTa) with a feed-forward token classification head on four out of six datasets"}, {"title": "6 Conclusion", "content": "Next, we assess how individual experts correlate with the tokens they operate on to determine their specific types of specialisation. During inference on the test sets, we extracted the expert with the highest weight (i.e., the expert contributing the most out of the top k) for each token in the in- put sequence. We then correlated the selected ex-\npert per token with the token's grammatical and semantic attributes. On the grammatical/lexical level, we consider: whether a token is a specific word, part-of-speech tag (POS), punctuation char- acter or not (Punct), or stopword or not (Stop.). On the semantic level, we examine the predicted labels for each token (Labels), whether a given to- ken is a named entity (bin. NE), and if so, the spe- cific type of named entity (NE). This way, we try to identify different types of specialisation, i.e., whether specialisation is rooted in syntax or se- mantics.\nWe annotated the tokens with POS tags and NE labels using the SpaCy library (Honnibal et al., 2020). In total, we obtain six different categories of specialisation, which we annotate on the dataset level (i.e., we obtain annotations for the con- catenation of documents belonging to a specific dataset). Since we work with categorical values, we measure the correlation between the highest- weighted experts and each category by employing Cram\u00e9r's V statistic (Cram\u00e9r, 1999) with the bias correction proposed in Bergsma (2013). We con- sider the two training regimes, validation set and full training, separately, to examine the impact of data size. We present the results in Table 3.\nIn the validation set training scenario, a strong correlation between specific words and specific experts is observed on all datasets. This is not\nsurprising, since MoE works on the token level. On the other hand, the correlation between experts and labels (I, O, and B) tends to be moderate at best (i.e., between 0.1 and 0.3) on most datasets. One can also observe a moderate (and on some datasets strong, i.e., more than 0.5) correlation be- tween specific experts and POS tags, suggesting that specialisation is to a large extent influenced by the syntax of the text.\nBy looking at the other three types of correla- tion, namely the correlation between experts and NEs, between experts and stopword tokens, and\nbetween experts and punctuation tokens, one can see a very distinct difference between the com- puter science and other datasets. While there is a strong tendency for experts to specialize for stop- word and punctuation tokens on all the computer science datasets (Inspec, Krapivin, and KP20k), this tendency is much weaker on other (news and web page) datasets, where one can observe al- most no correlation. On the contrary, here one can observe a specialisation of experts for NEs, which is almost non-existent on the computer sci- ence datasets. A correlation is almost the same if we binarize the NE sequence (i.e., to NE/non- NE tokens) or if we look for correlations between experts and 23 labels returned by the SpaCy NE recognition pipeline. The specialisation for NEs can be perhaps explained by the fact that news datasets contain much more NEs than scientific papers and that many of them are in fact labelled as keywords. The same is true for the OpenKP dataset that includes web pages sampled from the index of Bing search engine content-oriented pages, i.e., news pages, multi-media pages from video sites, or indexing pages. The strong corre- lation with simple syntactic patterns on the com- puter science datasets can be on the other hand explained by the lack of obvious semantic clues, such as NEs, in these datasets.\nWhen comparing the specialisation in the val- idation set training scenario and the full train- ing scenario, one can see that correlation to la- bels, words, NEs, and binary NEs is mostly the same. On the other hand, one can see a substan- tial decrease in correlation to POS tags, stopwords and punctuation on the computer science datasets, when the models are trained on more data. The as- sumption is that this is the consequence of expert specialisation becoming more complex when the amount of data allows it."}, {"title": "A Appendix: Examples of keyword identification", "content": "In summary, we investigated the potential of MoE for keyword extraction tasks. We introduced SEKE, a novel architecture that combines MoE and RNN built upon the DeBERTa model for En- glish. This approach achieved superior perfor- mance compared to existing SOTA models on standard datasets for keyword extraction. Fur- ther analysis confirmed the benefits of MoE layers during fine-tuning, especially with adequate data. The study revealed that adding recurrent layers can help mitigate the data limitations, showing the feasibility of the approach even on small training datasets with less than 2,000 documents. Never- theless, the specialization analysis shows that the size of the training data still influences the special- isation and that more data allows for more com- plex specialisation patterns to develop. In the fu- ture, we will try to pinpoint the exact dataset size thresholds, down to which the proposed approach is still feasible.\nThe analysis also revealed that specialisation is domain-specific. Current results indicate that in text with higher complexity (i.e., scientific pa- pers) experts to a larger extent rely on syntactical patterns than in the less complex text, where ex- perts to a larger extent specialize for processing of semantic clues. This hypothesis will be further tested in the future by applying the approach on new domains and languages other than English. This expansion will also further test the generalis- ability of the proposed approach, reducing the lan- guage and domain limitations of the current study.\nAnother limitations is the usage of an BiLSTM encoder in the custom token classification head, which makes the inference and fine-tuning slightly slower and more computationally demanding than in the vanilla DeBERTa model. While the dif- ference in time was negligible in all our experi- ments, it might have a visible effect, if the model is applied on a very large dataset. We should also point out the reliance on SpaCY in the speciali- sation analysis, which is a general domain NLP tool, rarely employed on specialized texts, such as scientific articles. Its usage might have affected the reliability of the analysis' correlation results by extracting fewer or incorrect NEs.\nFinally, we can observe that despite the impres- sive improvements in the development of founda- tional LLMs that offer impressive zero-shot ca- pabilities, keyword extraction task can still not be sufficiently solved by employing these mod- els, which lag substantially behind most super- vised approaches on most datasets. The biggest advantage of supervised approaches is their ability to adapt to the specifics of the syntax, semantics, content, genre and keyword tagging regime of the specific corpus (Meng et al., 2017). Since this can- not be done in a zero-shot setting, in the future we will try to adapt LLMs to the specific corpora and keyword extraction regimes with fine-tuning and in-context learning."}, {"title": "1.) Inspec:", "content": "This section presents examples of extracted key- words on randomly selected instances from all datasets.\nThe Bagsik Oscillator without complex numbers. We argue that the analysis of the so-called Bagsik Oscillator, recently published by Piotrowski and Sladkowski (2001), is erroneous due to: (1) the in- correct banking data used and (2) the application of statistical mechanism apparatus to processes that are totally deterministic.\nPredicted keywords: incorrect banking data, statistical mechanism apparatus, bagsik oscilla- tor\nTrue keywords: data, statistical mechanism ap- paratus, complex numbers, bagsik oscillator"}, {"title": "2.) Krapivin:", "content": "Solving Equations in the Relational Algebra. Enu- merating all solutions of a relational algebra equa- tion is a natural and powerful operation which, when added as a query language primitive to the nested relational algebra, yields a query language for nested relational databases, equivalent to the well-known powerset algebra. We study sparse equations, which are equations with at most poly- nomially many solutions. We look at their com- plexity and compare their expressive power with that of similar notions in the powerset algebra.\nTrue keywords: relational algebra, nested rela- tion, equation\nPredicted keywords: equations, relational al- gebra, powerset"}, {"title": "3.) KP20k:", "content": "An algebraic approach to guarantee harmonic bal- ance method using grobner base. Harmonic bal- ance (HB) method is well known principle for analyzing periodic oscillations on nonlinear net- works and systems. Because the HB method has a truncation error, approximated solutions have been guaranteed by error bounds. However, its nu- merical computation is very time-consuming com- pared with solving the HB equation. This paper proposes an algebraic representation of the error bound using Grobner base. The algebraic repre- sentation enables to decrease the computational cost of the error bound considerably. Moreover, using singular points of the algebraic representa- tion, we can obtain accurate break points of the error bound by collisions.\nTrue keywords: algebraic representation, sin- gular point, error bound, grobner base, har- monic balance method\nPredicted keywords: grobner base, bound, har- monic balance, harmonic balance method"}, {"title": "4.) KPTimes:", "content": "Afghan Police Chief Is Killed as He Tries to Turn Tide Against Taliban. A hard-charging Afghan police chief with deep experience in Afghanistan's long conflict with the Taliban was killed in a blast on Sunday in the country's eastern Nangarhar Province, which has been under threat from the Taliban and affiliates of the Islamic State. The police chief, Gen. Zarawar Zahid, was visiting an outpost in the Hisarak district when explosives placed near the outpost detonated, according to Attaullah Khogyani, a spokesman for the gover- nor of Nangarhar. One of General Zahid's body- guards was wounded, Mr. Khogyani said. More Than 14 Years After U.S. Invasion, the Taliban Control Large Parts of Afghanistan At least one- fifth of the country is controlled or contested by the Taliban. The Taliban claimed responsibility for the killing, according to a statement by the in- surgencys spokesman, Zabiullah Mujahid. The attack came a week after twin bombings outside Afghanistan's Ministry of Defense killed at least 40 people, including several senior security of- ficials. Nangarhar, which borders Pakistan, has faced mounting security perils over the past cou- ple of years, with new Islamic State affiliates com- plicating the threat from the Taliban. Zabihullah Zmarai, a member of the provincial council, said the Islamic State posed a danger in five districts, despite repeated operations by the Afghan Army."}, {"title": "5.) JPTimes:", "content": "Out of the 22 districts, only six are secure, he said. The Taliban's presence across nearly a dozen dis- tricts varies, Mr. Zmarai said. But the Hisarak dis- trict faced a collapse in recent weeks. That drew the attention of General Zahid, who had gone there to supervise a counterattack. Over the past decade, he rose from a bodyguard to a well-regarded police chief of several volatile provinces. His postings in- cluded two stints as the police chief of southeast- ern Ghazni Province, and one term each in Zabul and Paktika Provinces. General Zahid was seen as a hands-on commander, often arriving at the front lines unannounced. When a major cultural event drew world leaders to the ancient city of Ghazni, the general was photographed riding around the city on the back of a motorcycle to check on se- curity measures. He had been wounded twice and had lost two brothers during the decades of war in Afghanistan. In June, he took part in clashes with Pakistani forces that erupted on the border. In a Facebook video that he posted, he appeared be- side two mortars and shouted to his men, Strike hard enough to blow up Nawaz Sharif's home, referring to the prime minister of Pakistan. Sediq Sediqqi, the spokesman for Afghanistan's Min- istry of Interior Affairs, called General Zahid one of the bravest commanders of Afghan police.cHe lost his life on the front line of duty in the fight against terrorism, Mr. Sediqqi said.\nTrue keywords: afghanistan, taliban, zarawar zahid, bombs, nangarhar province, terrorism\nPredicted keywords: afghanistan, zarawar za- hid, talibanPhoto report: FOODEX Japan 2013. FoodEx is the largest trade exhibition for food and drinks in Asia, with about 70,000 visitors checking out the products presented by hundreds of participating companies. I was lucky to enter as press; oth- erwise, visitors must be affiliated with the food industry and pay to enter. The FoodEx menu is global, including everything from cherry beer from Germany and premium Mexican tequila to top-class French and Chinese dumplings. The event was a rare chance to try out both well-known and exotic foods and even see professionals mak- ing them. In addition to booths offering tradi- tional Japanese favorites such as udon and maguro"}, {"title": "6.) openKP:", "content": "sashimi, there were plenty of innovative twists, such as dorayaki, a sweet snack made of two pan- cakes and a red-bean filling, that came in coffee and tomato flavors. While I was there I was lucky to catch the World Sushi Cup Japan 2013, where top chefs from around the world were compet- ing and presenting a wide range of styles that you would not normally see in Japan, like the flower makizushi above.\nTrue keywords: foodex\nPredicted keywords: foodex, japan, food\"Quickly create pointandclick games and virtual tours for Windows native PSP iPhone and iPod Touch web apps No programming required very easy to use Free edition contains all the main fea- tures Includes free drawing tool and music com- poser NEW Create Games for iPhone DOWN- LOAD FREE GAMES CREATED WITH AD- VENTURE MAKER WATCH THE GUIDED TOUR Whats New in version 45 Whats New in version 44\nTrue keywords: virtual tours, adventure maker, no programming required\nPredicted keywords: virtual tours"}]}