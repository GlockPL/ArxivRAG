{"title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models", "authors": ["Kunsheng Tang", "Wenbo Zhou", "Jie Zhang", "Aishan Liu", "Gelei Deng", "Shuai Li", "Peigui Qi", "Weiming Zhang", "Tianwei Zhang", "Nenghai Yu"], "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in natural language generation, but they have also been observed to magnify societal biases, particularly those related to gender. In response to this issue, several benchmarks have been proposed to assess gender bias in LLMs. However, these benchmarks often lack practical flexibility or inadvertently introduce biases. To address these shortcomings, we introduce GenderCARE, a comprehensive framework that encompasses innovative Criteria, bias Assessment, Reduction techniques, and Evaluation metrics for quantifying and mitigating gender bias in LLMs. To begin, we establish pioneering criteria for gender equality benchmarks, spanning dimensions such as inclusivity, diversity, explainability, objectivity, robustness, and realisticity. Guided by these criteria, we construct GenderPair, a novel pair-based benchmark designed to assess gender bias in LLMs comprehensively. Our benchmark provides standardized and realistic evaluations, including previously overlooked gender groups such as transgender and non-binary individuals. Furthermore, we develop effective debiasing techniques that incorporate counterfactual data augmentation and specialized fine-tuning strategies to reduce gender bias in LLMs without compromising their overall performance. Extensive experiments demonstrate a significant reduction in various gender bias benchmarks, with reductions peaking at over 90% and averaging above 35% across 17 different LLMs. Importantly, these reductions come with minimal variability in mainstream language tasks, remaining below 2%. By offering a realistic assessment and tailored reduction of gender biases, we hope that our GenderCARE can represent a significant step towards achieving fairness and equity in LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have become pivotal in natural language generation tasks such as automatic conversation and content creation. For instance, according to OpenAI's report at its first developer conference [7], ChatGPT [1] affects an estimated 100 million users weekly with its advanced text generation capabilities. In content creation, Sudowrite [8], powered by LLMs, helps with story writing and has been used by over 20,000 writers since its inception. Nevertheless the excellence, it is reported that LLM will amplify societal issues such as gender bias [10, 17, 26, 30, 31, 33, 36, 40, 45, 56]. Specifically, a recent survey conducted by QueerInAI\u00b9 reveals that more than 65% of respondents from the marginalized community LGBTQIA+\u00b2 experience increased digital discrimination correlating with biased Al outputs [39]. Another particularly shocking finding is the empirical evidence of Kapoor and Narayanan, which shows that LLMs, such as GPT-3.5 [4], reinforce stereotypes for various gender groups [26]. These revelations raise profound safety concerns, as the perpetuation of such gender bias by widely used LLMs could undermine trust in Al technologies and exacerbate harmful gender stereotypes. This can lead to the destabilization of digital interactions in various spheres and further entrench gender disparities, undermining efforts toward gender equality. Therefore, it becomes imperative to reduce gender bias in LLMs.\nIn response to these concerns, many countries and regions are implementing legislative measures. For instance, the United States has introduced the \"Blueprint for an AI Bill of Rights\" [24]; the European Union has established the \"Convention on AI and Human Rights\" [37]. These legislations aim to compel corporations and research institutions to take steps to prevent gender discrimination in algorithmic systems. Meanwhile, there are some benchmarks for assessing gender bias in LLMs, which can be broadly classified into three categories: template-based, phrase-based, and option-based approaches. Briefly, template-based approaches, such as Winobias [57] and Winoqueer [17], involve creating datasets by altering gender identities in sentence templates. These methods are relatively straightforward to implement. Phrase-based approaches, like the BOLD dataset [14], which prompts models with seed phrases to generate text, offer an intuitive way to evaluate biases in generated language. Option-based approaches, illustrated by StereoSet [33], present a given statement with multiple response choices, encompassing biased, neutral, and unrelated options. These approaches assess bias based on the model's tendency towards these options and cover a wider spectrum of bias aspects.\nWhile current approaches contribute significantly to assessing gender bias in LLMs, they do have limitations when aligned with the public's aspiration for realistic and objective bias assessment."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "We delve into the pivotal research surrounding gender bias within the field of LLMs. We begin by articulating gender bias in the context of diverse gender identities (Sec. 2.1), followed by a review of the phenomena of gender bias (Sec. 2.2). Lastly, we analyze the current approaches for constructing benchmarks in gender bias assessment (Sec. 2.3)."}, {"title": "2.1 Gender Bias Statement", "content": "Before looking into the nuances of gender bias, it is essential to distinguish between 'sex' and 'gender.' 'Sex' refers to the biological differences between male and female bodies. In contrast, \u2018gender\u2019 encompasses a broader spectrum, including the array of identities beyond the male-female binary, such as transgender, genderqueer, non-binary, and more [46]. This distinction is crucial in addressing gender bias, as it recognizes the varied and personal nature of gender identity, challenging traditional perceptions.\nWith this understanding of gender, we can define gender bias as prejudicial attitudes or discriminatory actions based on an individual's gender identity. Gender bias manifests in harmful stereotypes and unequal treatment, affecting not just women and men but all genders across the spectrum. It can be both overt and subtle, embedded in societal norms and influencing perceptions across different communities [13]. This broader perspective is essential for a comprehensive approach to gender bias, addressing the specific challenges faced by various gender identities, including marginalized transgender and non-binary (TGNB) identities."}, {"title": "2.2 Gender Bias in Large Language Models", "content": "The gender bias in LLMs is highlighted in several studies [10, 17, 26, 33, 36, 40, 45], underscoring the risks associated with biased AI outputs. The emergence of gender bias within the realm of LLMs poses significant challenges, particularly when considering the diverse gender identities. LLMs exhibit biases against binary genders, predominantly in the form of reinforcing gender stereotypes. Research has shown that these models frequently associate professions, behaviors, and traits with specific genders based on outdated and culturally ingrained stereotypes [12, 18, 43, 51]. For instance, LLMs have been observed to link nursing and teaching predominantly with women, and engineering or leadership roles with men [11, 20, 47]. Such biases not only reflect societal prejudices but also perpetuate them, further entrenching gender stereotypes in digital interactions and decision-making processes [28, 34, 50]. Particularly, Kapoor and Narayanan [26] provide shocking evidence that mainstream LLMs reinforce gender stereotypes. They test GPT-3.5 and GPT-4 with the gender-biased dataset Winobias [57] and find that an average of 34% in GPT-3.5's outputs and 26% of GPT-4's output reveal gender stereotypes or biased language.\nThis challenge intensifies when considering non-binary and diverse gender identities. LLMs, primarily trained on datasets that lack representation of non-binary genders, struggle to adequately recognize and represent these identities. This results in the erasure or misrepresentation of non-binary individuals, contributing to their marginalization. Ovalle et al. [36] highlight that the text generated by LLMs fails to acknowledge the existence of genders beyond the male-female binary, leading to a lack of visibility and recognition for non-binary and genderqueer individuals. Furthermore, a notable survey by QueerInAI reveals that over 65% of respondents from the LGBTQIA+ community have experienced increased digital discrimination correlating with biased AI outputs [39]. These findings raise concerns about AI technology, as they could exacerbate harmful gender stereotypes and destabilize digital interactions across various domains. Such biases have the potential to deepen gender disparities and impede progress toward gender equality.\nIn response, countries and regions are introducing legal frameworks to combat gender discrimination in algorithmic systems, such as the U.S.'s Blueprint for an AI Bill of Rights [24] and the EU's Convention on AI and Human Rights [37]. This underscores the critical need for effective assessment and reduction of gender bias in LLMs, not just as a technical challenge but as a societal imperative to ensure equitable and respectful Al interactions."}, {"title": "2.3 Benchmarks for Gender Bias Assessment", "content": "Assessing gender bias in LLMs is a multifaceted challenge. Current techniques for assessing gender bias are predominantly categorized into three strategies: template-based (Sec. 2.3.1), phrase-based (Sec. 2.3.2), and option-based (Sec. 2.3.3). While these methods have advanced our understanding and assessment of gender bias, they also exhibit limitations, especially when considering the public's aspiration for realistic and objective bias assessment."}, {"title": "2.3.1 Template-based benchmarks", "content": "Template-based benchmarks in gender bias assessment involve the creation of datasets by modifying sentence templates to include different gender identities. This strategy (e.g., EEC [28], Winobias [57], Winoqueer [17]) is operationalized by altering specific elements in sentences to reflect various gender identities, thus enabling an assessment of the model's response to these changes. Specifically, EEC and Winobias primarily focus on identifying gender bias by altering pronouns and associated gender roles within sentences, revealing how models perceive gender in professional and social roles. Winoqueer extends this by including a wider range of gender identities beyond the binary, examining model responses to diverse gender expressions and roles.\nTemplate-based approaches offer a straightforward and simple way to manipulate gender variables within sentence structures. However, they come with notable limitations. One significant drawback is the lack of transparency in how templates are chosen and constructed. Additionally, these methods are often sensitive to changes in template structure, as exemplified in Fig. 1. For instance, when using the template \"The situation makes [GENDER] feel [EMOTION WORD]\" with EEC, modifying the template while keeping its content intact can result in different outcomes. This highlights the limited ability of this approach to capture the intricacies and nuances of natural language, potentially leading to biased gender bias assessments [41]. The rigid template structure may not accurately reflect the fluidity and diversity of real-world language usage, affecting the realism and applicability of assessment findings."}, {"title": "2.3.2 Phrase-based benchmarks", "content": "Phrase-based approaches for evaluating gender bias in LLMs involve the use of seed phrases to initiate text generation by these LLMs. This strategy aims to mirror more natural language generation processes. A prominent example is the BOLD dataset [14], which is specifically designed to assess biases in open-ended text generation by providing LLMs with seed phrases and instructing them to complete these phrases. Its seed phrases are excerpted from Wikipedia, encompassing diverse domains and contexts that explicitly or implicitly relate to gender, thereby offering insights into the models' gender bias.\nThe primary advantage of phrase-based approaches is their intuitive nature, closely aligning with natural language processes, thereby providing a more realistic setting for bias assessment. However, its one significant limitation is the potential biases inherent in the phrases themselves. For instance, as illustrated in Fig. 1, an analysis of the BOLD dataset reveals biases in the seed phrases. The dataset's division shows biased descriptions in the seed phrases for both gender groups. This raises concerns about the objectivity of the dataset, as the inherent biases in the prompts could lead to skewed results. Another limitation arises from the dataset's reliance on public resources like Wikipedia. According to Kotek et al. [29], the complete original content corresponding to the seed phrase, extracted from the widely used public domain, may be included in the model's training data, which can subsequently affect the objectivity of the assessment results."}, {"title": "2.3.3 Option-based benchmarks", "content": "Option-based approaches present statements with multiple response choices, including biased, neutral, and unrelated options. A notable example is StereoSet [33], a benchmark designed to evaluate bias in language models. Within this framework, language models are presented with statements and are asked to select responses that reveal their underlying biases or demonstrate a lack thereof. The primary objective is to assess the model's propensity towards biased responses in various scenarios, thereby shedding some light on its inherent biases.\nOption-based methods offer a substantial advantage by encompassing a broad spectrum of scenarios and biases, providing a comprehensive perspective on a model's inclinations. Nonetheless, the creation of such benchmarks necessitates extensive manual scrutiny and classification of options, starting from contextual statements to the selection of response choices. Particularly during the data curation phase, the manual review and selection of sentences entail significant human resources, rendering the process both time-consuming and costly. As highlighted by The Guardian's report [19], content reviewers involved in AI systems, such as OpenAI, may experience psychological distress due to the nature of their work, often without sufficient warnings or support, and are typically compensated at relatively low rates. Furthermore, the reliance on crowdsourcing platforms for option classification introduces a high degree of subjectivity. Most importantly, this strategy struggles to directly measure biases in open-ended responses, limiting its ability to mimic real-world interactions.\nA significant gap apparent in these three strategies is their limited attention to transgender and non-binary (TGNB) identities, which tend to be overlooked in the construction of benchmarks. Except for the template-based strategy, the other two strategies notably lack a comprehensive framework for assessing bias related to TGNB gender identities. This omission poses a challenge to achieving a truly inclusive gender bias assessment. Existing methodologies underscore the necessity for establishing unified criteria that encompass the multifaceted nature of gender equality benchmarks, ensuring both the realism and objectivity of the assessment process. This leads to the development of more comprehensive and inclusive benchmarks, thereby advancing the field towards more realistic and equitable solutions in gender bias assessment within LLMs."}, {"title": "3 GENDERCARE", "content": "To address the identified research questions raised in Sec. 1, we present a comprehensive framework: GenderCARE. We first provide an overview of our solution in Sec. 3.1, followed by a detailed exploration of Criteria for gender equality benchmarks (Sec. 3.2), Assessment methods for gender bias in LLMs (Sec. 3.3), and Reduction of gender bias in LLMs (Sec. 3.4). Finally, we discuss the Evaluation metrics employed to qualify the bias of each model (Sec. 3.5)."}, {"title": "3.1 Overview", "content": "The GenderCARE framework is composed of four interconnected parts, as illustrated in Fig. 2: establishment of criteria for gender equality benchmarks (RQ1), assessment of gender bias in LLMs (RQ2), reduction of gender bias in LLMs (RQ3), and evaluation metrics. Specifically, the criteria encompass six dimensions, namely, inclusivity, diversity, explainability, objectivity, robustness, and realisticity. These dimensions ensure a comprehensive and representative assessment of gender bias across various gender identities, including TGNB, and facilitate the creation of more realistic benchmarks. Under the assessment of gender bias in LLMs, we introduce a novel pair-based construction method and the GenderPair benchmark, which includes diverse gender identity groups and pairs of biased and anti-biased descriptors. Then, we employ counterfactual data augmentation [57] and low-rank adaptation fine-tuning strategies [25] to create the anti-biased debiasing dataset and reduce gender bias while maintaining model performance. Finally, we apply both lexical and semantic metrics, including Bias-Pair Ratio, Toxicity [48], and Regard [42], to quantify gender bias in model outputs. Each module will be introduced in detail as follows."}, {"title": "3.2 Criteria for Gender Equality Benchmarks", "content": "To overcome the limitations of existing methodologies for constructing gender equality benchmarks (RQ1), we propose the Criteria for Gender Equality Benchmarks (CGEB), which is inspired by NIST's criteria on trustworthy AI [35] and the White House's National Gender Equality Strategy [23]. CGEB encompasses six key dimensions: inclusivity, diversity, explainability, objectivity, robustness, and realisticity, each addressing a critical aspect of gender bias assessment. The explanation of each dimension is as follows:\nInclusivity. This ensures the recognition and inclusion of multiple gender identities, extending beyond the traditional binary to embrace transgender and nonbinary identities. It aims to reflect the full spectrum of gender experiences, acknowledging the unique challenges and biases faced by each group.\nDiversity. We consider a wide array of sources and contexts that may give rise to potential biases. These sources include societal roles, professions, and cultural norms. This dimension ensures the benchmarks encompass various facets of gender bias, thus capturing the intricate and multifaceted nature of gendered experiences.\nExplainability. This necessitates that every element of assessment data is presented in a clear, interpretable, and traceable manner. Such transparency is crucial for understanding how and why certain biases are identified, enabling more effective strategies for helping us comprehend the methods and reasons behind the identification of particular biases. It empowers us to devise more effective strategies for mitigating these biases and ensuring that the benchmarks can be readily grasped and applied by a broad spectrum of users.\nObjectivity. This focuses on minimizing human involvement in crafting benchmarks. It seeks to diminish the potential for subjective biases to creep in during the benchmark's creation, with the ultimate aim of achieving a fair and impartial evaluation of gender bias in language models.\nRobustness. This pertains to the reliability and consistency of assessment outcomes when evaluated across different prompt structures. Typically, a prompt comprises two components: instructions and requirements. Alterations in prompt structure involve modifying these instructions or requirements while preserving their initial semantic meaning. Therefore, the robustness of prompt structures implies the ability to sustain consistent assessment results even when prompt instructions or requirements are modified. This dimension ensures that the benchmarks are applicable and reliable in diverse and dynamic contexts.\nRealisticity. This dimension ensures that the benchmark data are 1) grounded in real-world scenarios and 2) capable of assessing open-ended responses similar to natural interactions. It is critical to ensure that the benchmarks are relevant and applicable to real-life situations, providing meaningful insights into the practical implications of gender bias in language models.\nBy integrating these six dimensions into CGEB, we aim to overcome the current constraints associated with establishing benchmarks for gender equality. This methodical approach is carefully designed to create a dependable and all-encompassing framework, which is essential for developing gender bias benchmarks that not only exhibit robustness but also align with practical, real-world requirements. Through these efforts, we strive to promote the advancement of more equitable and inclusive language technologies."}, {"title": "3.3 Assessment of Gender Bias in LLMs", "content": "To better align with the real-world scenarios of gender bias and fulfill the six dimensions of the CGEB criteria, we introduce a novel pair-based construction method, which creates sets of biased and anti-biased descriptors for each gender identity and role, regarded as gender targets. Based on these pair sets (Sec. 3.3.1), we further design instructions (Sec. 3.3.2) and requirements (Sec. 3.3.3) to construct the final prompts for testing. Specifically, we create our GenderPair benchmark, which comprises 103,854 prompts, assessing biases across 207 distinct gender identities and roles. Table 1 presents some instances from GenderPair. To evaluate the gender bias of the target LLM, we feed the prompts from GenderPair into the LLM and analyze the generated content. We employ three distinct metrics at both lexical and semantic levels (Sec. 3.5).\n3.3.1 Pair Sets. A pair set is a collection of descriptors that articulate biases and anti-biases for each gender identity and role. Essentially, each element of a pair set is a triplet:\n(GenderTarget, BiasedDescriptor, Anti \u2013 BiasedDescriptor).\nWe describe each component in detail as follows.\nGender Target. This component indicates any gender representative involved in specific gender identities. To meet the inclusivity requirement of CGEB, we classify gender identities into three groups\u00b3, based on the categorization of gender identities in the worldwide report of the gender census 2023 [3]:\n\u2022 Group 1: gender identities that fit strictly within the gender binary and are male (and associated expressions) all the time.\n\u2022 Group 2: gender identities that fit within the gender binary and are strictly female (and associated expressions) all the time.\n\u2022 Group 3: gender identities that do not belong to the traditional binary or tend towards a neutral description.\nBesides, the gender targets for each group i is structured with four aspects as follows:\nGroup i(1,2,3) = [{identity}, {titles}, {pronoun}, {name}].\nThese four aspects are introduced below:\nGender Identities. Drawing from the worldwide gender census reports of 2021-2023 [2] and nonbinary.wiki\u2074, we comply with diverse gender identities for the three groups.\nGender Titles. These are considered in the context of social roles. Referring to GenderQueeries\u2075, we categorize titles into four types: family, relationship, official, and miscellaneous titles. We then compile gender titles for each group across these categories based on GenderQueeries and nonbinary.wiki. Notably, gender census results [2] indicate a preference for neutral titles or pronouns among Group 3, as opposed to traditional binary titles."}, {"title": "3.3.2 Instructions", "content": "In the GenderPair benchmark, the Instructions component of each assessment prompt is designed to guide the model in selecting an appropriate descriptor from the Pair Set and subsequently generating a coherent text for the specified gender target. Notably, Pair Set can appear in two distinct structures within the prompts, namely, (Gender Target, Biased Descriptor, Anti-Biased Descriptor) and (Gender Target, Biased Descriptor, Gender Target, Anti-Biased Descriptor), which can result in six possible configurations of Pair Set:\n(1) (Target, {Biased Descriptor, Anti-Biased Descriptor})\n(2) (Target, {Anti-Biased Descriptor, Biased Descriptor})\n(3) ({Target, Biased Descriptor}, {Target, Anti-Biased Descriptor})\n(4) ({Target, Anti-Biased Descriptor}, {Target, Biased Descriptor})\n(5) ({Biased Descriptor, Target}, {Anti-Biased Descriptor, Target})\n(6) ({Anti-Biased Descriptor, Target}, {Biased Descriptor, Target})\nFor these configurations, we implement two distinct instructions:\n\u2022 For configurations (1) and (2), we adopt \"Please generate a cohesive text by incorporating the word {Gender Target} and a word drawn from the attribute set, which consists of the following: {Biased Descriptor, Anti-Biased Descriptor}.\".\n\u2022 For configurations (3) to (6), the instructions are formulated as: \"Please generate a coherent text by choosing a pair from the following set of phrase pairs: {(Gender Target, Biased Descriptor), (Gender Target, Anti-Biased Descriptor)}.\"\nThese instructions are carefully crafted to ensure the models' responses are aligned with the intended assessment objective of the GenderPair benchmark. By considering different structural possibilities in the Pair Set and tailoring the instructions accordingly, we ensure that the benchmark's ability to assess gender bias is comprehensive. These instructions contribute to the robust assessment of gender bias in language models, as they accommodate a wide range of gender identities and descriptors."}, {"title": "3.3.3 Requirements", "content": "For each prompt in the GenderPair benchmark, the requirements component enables the addition of specific demands that aid in the assessment of the model's gender bias. For instance, to differentiate between objects selected from the Pair Set and those generated by the model itself, a requirement has been designed, which entails marking the selected element with '{}' in the generated text. Such a practice is instrumental in clearly distinguishing the elements of the model's preferences and facilitating a more accurate evaluation of gender bias in the responses."}, {"title": "3.4 Reduction of Gender Bias in LLMs", "content": "In this section, we focus on our dual goals: 1) reducing gender bias in LLMs and 2) ensuring the preservation of the models' core performance. This endeavor is divided into two parts: the debiasing dataset and fine-tuning strategies."}, {"title": "3.4.1 Debiasing Dataset", "content": "To build a debiasing dataset, we leverage counterfactual data augmentation (CDA) [57], which allows for the creation of alternative scenarios that reduce existing biases. The essence of CDA is to reframe or alter situations in a manner that presents a counter-narrative to common biases. Utilizing the anti-biased descriptors from the GenderPair benchmark, we obtain a debiasing dataset composed of Prompts and debiased Responses.\nFor the Prompts, we also consider three components: pair sets, instructions, and requirements. (1) In the pair sets, we focus on the gender target and anti-bias descriptors. To encompass a broader range of gender biases, we expand the gender target's popular names to the top 50 and the anti-bias descriptors' frequency count to the top 50 based on GenderPair; (2) The instructions are designed to guide the generation of coherent text based on the pair set. To avoid data leakage, the instructions prioritize text generation over word selection, which is \"to generate a cohesive text by incorporating the two words from a pair set {Gender Target, Anti-Bias Descriptors}.\"; (3) For requirements, we continue to mandate marking the selected element with '{}' in the text to distinguish elements from the pair set and generated by the model itself. For the Responses, we initially solicit experts to generate unbiased, coherent texts for each gender target's anti-biased descriptors, ensuring emotional consistency across different gender groups. Subsequently, these texts are reviewed with GPT-4 to confirm the absence of bias and maintain emotional parity across gender groups."}, {"title": "3.4.2 Fine-Tuning Strategy", "content": "To ensure that the de-biased models retain their original performance, we employ Low-Rank Adaptation (LoRA) fine-tuning [25]. This method allows for the modification of parameters related to gender bias while freezing other parameters. In other words, LoRA's selective tuning strategy is crucial for maintaining the overall functionality of the models while effectively mitigating gender bias, striking a balance between bias reduction and performance preservation in LLMs.\nIn conclusion, by carefully constructing a debiasing dataset through CDA and employing a strategic LoRA fine-tuning method, we build a balanced and effective pathway to mitigate gender biases in LLMs. These solutions not only address the immediate concern of reducing bias but also pave the way for future advancements in creating more equitable and unbiased AI systems."}, {"title": "3.5 Evaluation Metrics", "content": "To assess the gender bias of the output from the target LLMs, we employ three distinct metrics at both the lexical and semantic levels."}, {"title": "3.5.1 Bias-Pair Ratio", "content": "At the lexical level, we utilize the Bias-Pair Ratio (BPR) to quantify the proportion of biased descriptors selected by the model. This metric effectively measures the tendency of a model to opt for biased descriptors, described as follows:\n$BPR = \\frac{N_{biased}}{N_{total}}$\nwhere $N_{biased}$ denotes the number of biased descriptors used by the model and $N_{total}$ is the total number of descriptors (both biased and anti-biased) selected by the model. BPR is a fraction ranging from 0 to 1, with higher values indicating a greater inclination towards gender-biased language. Note that in cases where the model may struggle to comprehend the instructions and requirements in a prompt, perplexity [32] can serve as an approximate measure to determine the model's bias. It calculates the perplexity regarding bias and anti-bias descriptors in the prompt. A lower perplexity indicates ease in generating responses containing such descriptors."}, {"title": "3.5.2 Toxicity and Regard", "content": "At the semantic level, we assess gender bias using two metrics: Toxicity [48] and Regard [42].\n\u2022 Toxicity quantifies the harmfulness of the generated text towards a specific gender group, measuring the extent to which the language might perpetuate harm or negative stereotypes. The toxicity score ranges from 0 to 1, with values closer to 1 indicating a higher degree of toxicity.\n\u2022 Regard evaluates the sentiment expressed in the generated text towards the group in question, assessing whether the text portrays the group in a positive, negative, neutral, or other light. Each sentiment category (positive, negative, neutral, and other) is scored from 0 to 1, where values closer to 1 indicate a stronger inclination towards that sentiment in the text. This study focuses on the disparities in positive and negative sentiments across different gender groups to examine potential emotional biases.\nThis dual-level approach of combining lexical and semantic metrics enables a comprehensive quantification of gender bias. By assessing both the explicit choice of words and the underlying sentiment of the generated text, we gain a holistic view of how gender bias manifests in language models."}, {"title": "4 EXPERIMENTAL SETUP", "content": "To validate the effectiveness of our GenderCARE framework, we apply the framework to dozens of different types of LLMs. In this section, we delineate the experimental setup for our study, which is structured around five key components:"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In Sec. 5.1, we analyze the effectiveness of various gender bias benchmarks with the CGEB. Then, Sec. 5.2 provide a detailed analysis of gender bias with our GenderPair benchmark present in different LLMs. Next, Sec. 5.3 discusses the outcomes of our bias reduction strategies. Sec. 5.4 provides more evaluation of our gender bias assessments and reduction strategies. Lastly, we summarize our findings as take-home messages in Sec. 5.5."}, {"title": "5.1 Comparative Analysis of Gender Bias Benchmarks (RQ1)", "content": "As shown in Table 3, Winoqueer [17] includes TGNB identities, satisfying inclusivity but lacks diversity due to missing diverse bias sources like societal roles. While systematic template modifications enhance objectivity, the approach's transparency issues and inherent fragility compromise its explainability and robustness. Despite integrating TGNB community feedback, Winoqueer's template reliance limits its realisticity in mirroring real-world discourse. BOLD [14] employs a phrase-based approach that connects biases to phrases sourced from Wikipedia. While this offers clear explainability and robustness, it also poses risks of inheriting biases due to the reliance on public resources, thus compromising objectivity. Moreover, due to the limited representation of various gender identities, it falls short of inclusivity and diversity. Furthermore, the assessing data lacks representation from the real world, leading to a shortfall in realisticity. StereoSet [33] is lauded for its robustness, adaptability across different model architectures, and imperviousness to variations in prompt structures. However, as analyzed in Sec. 2.3.3, it fails to meet the other five dimensions of the CGEB.\nIn contrast, our GenderPair benchmark covers all dimensions, offering an inclusive and diverse set of prompts (inclusivity and diversity), the clear rationale behind its construction (explainability), minimal human intervention in its creation (objectivity), consistency in results across different prompt structures (robustness, validated in Sec. 5.4), and prompts rooted in real-world interaction scenarios (realisticity)."}, {"title": "5.2 Assessing Gender Bias for LLMs (RQ2)", "content": "The assessment of gender bias in LLMs using the GenderPair Benchmark is delineated in Table 4. The analysis reveals that models with a larger parameter (13B) generally exhibit a reduced level of bias across three distinct evaluation metrics, in contrast to the smaller (7B parameters). Specifically, the Llama2_13B emerges as the most effective in diminishing gender bias. This is substantiated by its minimal Bias-Pair Ratio of 0.42 for Group 2, alongside low toxicity scores of 0.01 across all groups, and a consistently low standard deviation (\u03c3) in Regard scores of 0.01 for positive sentiments. This model is closely followed by Llama_13B, which showcases similar achievements in terms of low toxicity scores and standard deviations. Conversely, the Llama_7B demonstrates a pronounced relative bias, with the highest Bias-Pair Ratio of 0.56 for Group 1. The Platypus2 models, in contrast, are characterized by elevated toxicity scores across all groups, peaking at 0.12 for the 13B model in Group 3. Platypus2 models also consistently display high Bias-Pair Ratios. The Orca models, on the other hand, present a more balanced performance profile, marked by relatively low toxicity scores and standard deviations, though their Bias-Pair Ratios remain moderate."}, {"title": "5.3 Reducing Gender Bias for LLMs (RQ3)", "content": "Table 5 presents a notable bias decrease in all three metrics, compared to the original models (Table 4). The most significant improvements are observed in Orca_13B, with reductions exceeding 50% in Bias-Pair Ratio and Toxicity. These findings offer quantitative evidence of the substantial effectiveness of our debiasing strategy in reducing gender bias across diverse groups. Besides, we also evaluate the debiased LLMs by three existing bias benchmarks: Winoqueer [17], BOLD [14], and StereoSet [33]. As shown in Table 6, our debiasing strategy helps LLMs reduce bias according to these three benchmarks. In particular, the debiased LLMs demonstrate increased perplexity differences (\u0394) for stereotypical and anti-stereotypical sentences in Winoqueer and StereoSet. This suggests a heightened inclination toward generating anti-stereotypical responses. Additionally, there is a noticeable reduction in the standard deviations (\u03c3) of Regard sentiment scores for actors and actresses in BOLD. For example, StableBeluga_13B shows a 91.3% improvement in \u0394 for Winoqueer and a 32.9% reduction in \u03c3 for negative sentiments in BOLD after debiasing. This underscores the effectiveness of our methods in diminishing gender stereotype reliance.\nOn the other hand, Table 7 shows the performance change of the debiased LLMs on the GLUE and MMLU. The results reveal that fine-tuning not only reduces gender bias but also potentially enhances performance in domains like Social Science on MMLU, possibly due to the high intersectionality of gender identity within these fields. In a nutshell, while the fine-tuning process may induce some performance trade-offs, the observed fluctuations across all performance metrics remained below the 2% threshold."}, {"title": "5.4 More Evaluations", "content": "5.4.1 Robustness to Different Prompt Structures. To evaluate the robustness of our GenderPair benchmark against variations in the prompt structure", "types": "Type 1 incorporates the prompt structure as outlined in Sec. 3.3, Type 2 maintains the essence of the original instructions but articulates them differently, and Type 3 employs the alternative symbol for marking in the requirements delineated in Type 1 prompts. As shown in Fig. 3, there are only minimal fluctuations within 0.02 across the Bias-Pair Ratio, Toxicity, and Regard metrics for all three types, affirming the robustness of our benchmark against variations in prompt structure.\n5.4.2 Extension to Other LLM Architectures. Besides the llama architecture, we apply the GenderPair to other three distinct LLM architectures to assess its versatility across diverse model architectures, as described in Table 8. The results demonstrate that GenderPair can provide effective gender bias quantifications for different model types. Specifically, the Falcon model exhibits excellent performance, with the lowest Bias-Pair Ratio for all three groups. The chatbot model Baichuan2 also has competitive bias metrics. However, the outcomes also reveal architecture-specific differences. Falcon displays the lowest Bias-Pair Ratio and the highest variability in positive sentiments. Meanwhile, Mistral suffers from large Bias-Pair Ratios and Baichuan2 displays the lowest variability in positive sentiments. This affirms that bias manifestations can significantly differ across model families. Furthermore, we fine-tune these models using our specially curated debiasing dataset. The findings suggest that our assessment and debiasing strategy are effective across various architectures, reducing gender bias in different benchmarks without compromising the overall performance of the models.\nOverall, the assessment of multiple architectures substantiates the applicability of GenderPair for standardized bias evaluation across diverse LLMs. While biases are"}]}