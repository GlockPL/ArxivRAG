{"title": "QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REINFORCE", "authors": ["Junjie Zhao", "Chengxi Zhang", "Min Qin", "Peng Yang"], "abstract": "The goal of alpha factor mining is to discover indicative signals of investment opportunities from the historical financial market data of assets, which can be used to predict asset returns and gain excess profits. Deep learning based alpha factor mining methods have shown to be powerful, which, however, lack of the interpretability, making them unacceptable in the risk-sensitive real markets. Alpha factors in formulaic forms are more interpretable and therefore favored by market participants, while the search space is complex and powerful explorative methods are urged. Recently, a promising framework is proposed for generating formulaic alpha factors using deep reinforcement learning, and quickly gained research focuses from both academia and industries. This paper first argues that the originally employed policy training method, i.e., Proximal Policy Optimization (PPO), faces several important issues in the context of alpha factors mining, making it ineffective to explore the search space of the formula. Herein, a novel reinforcement learning based on the well-known REINFORCE algorithm is proposed. Given that the underlying state transition function adheres to the Dirac distribution, the Markov Decision Process within this framework exhibit minimal environmental variability, making REINFORCE algorithm more appropriate than PPO. A new dedicated baseline is designed to theoretically reduce the commonly suffered high variance of REINFORCE. Moreover, the information ratio is introduced as a reward shaping mechanism to encourage the generation of steady alpha factors that can better adapt to changes in market volatility. Experimental evaluations on various real assets data show that the proposed algorithm can increase the correlation with asset returns by 3.83%, and a stronger ability to obtain excess returns compared to the latest alpha factors mining methods, which meets the theoretical results well.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realm of computational finance, it has become nearly a conventional practice to convert raw historical asset information into market trend indicative signals, referred to as alpha factor values [1]. The functions that generate these signals are known as alpha factors, which can be expressed in two distinct forms: the deep model and the formula [2]\u2013[4]. Mining high-quality alpha factors has become a trendy topic among investors and researchers, due to their relationship with excess investment returns.\nAlthough alpha factors using end-to-end deep models are generally more expressive, they are in nature black-box and poorly interpretable. Thus, when the performance of these black-box models deteriorates unexpectedly, it is often less likely for human experts to adjust the models accordingly [5]. Therefore, due to the need for risk control, it is difficult to involve these black-box factors in practical trading.\nComparatively, alpha factors represented in formulaic forms enjoy much better interpretability and thus are favored by market participants. The most intuitive method for automatically mining formulaic factors is the tree-based models, which mutate expression trees to generate new alpha factors [6]\u2013[8]. Another commonly used method is the genetic programming, which generates the formulaic alpha factor expressions through iterative simulations of genetic variation and natural selection in biological evolution [9]\u2013[11].\nUnfortunately, tree-based models and genetic programming still face certain challenges. Tree-based models, while easy to understand and implement, may encounter performance bottlenecks when dealing with non-linear relationships and high-dimensional data [12]. Genetic programming, on the other hand, can deal with broader types of expressions by properly defining the search space, including complex non-linear and non-parametric factor expressions, but it usually fails to explore the search space of large-scale expressions. Additionally, the computational costs of genetic programming are usually expensive [13].\nRecently, Yu et. al. [14] proposes a new framework for mining formulaic alpha factors with deep reinforcement learning, trying to bridge both deep learning and formulaic based methods. It employs Markov Decision Processes (MDPs) [15] to simulate the generation process of formulaic alpha factors and trains policies to directly generate a set of collaborative formulaic alpha factors using Deep Reinforcement Learning (DRL). It essentially aims at finding formulaic alpha factors. With the help of DRL, it also overcomes the limitations of the explorative search ability suffered by traditional tree models and genetic programming [14].\nThe key to this framework lies in using the Reverse Polish Notation (RPN) to convert formulaic alpha factors into a se-"}, {"title": "II. RELATED WORK", "content": "This section provides a brief review of the related work on the automatic mining methods for alpha factors and the theory of REINFORCE algorithm.\nAlpha factors are generally represented in the form of deep models or formulas. Alpha factors using end-to-end deep models are more complex and usually trained with supervised learning [17]\u2013[19], utilizing MLP [20] or sequential models like LSTM [21] and Transformer [22] to extract features embedded in the historical data of assets. Recently, reinforcement learning has attracted much attention in the context of computational finance and fintech. It becomes a key technology in alpha factor mining [14], investment portfolio optimization [23], and risk management design [24], thanks to its advantages in handling non-linearity, high-dimensional data, and dynamic environments [25]. By modeling market characteristics as states, maker or taker orders as actions, and profit and loss as rewards, reinforcement learning can also be used to train deep policy models that represents alpha factors [26]\u2013[28].\nOn the other hand, alpha factors represented in formulaic forms have much better interpretability and thus are favored by market participants. In the past, these formulaic alpha factors were constructed by human experts using their domain knowledge and experience, often embodying clear economic principles. For example, Kakushadze [29] presents 101 formulaic alpha factors tested in the US asset market. However, the alpha factor mining process relying on human experts suffers from multiple drawbacks such as strong subjectivity [30], time-consuming [30], insufficient risk control [31], and high costs [32]. To address these issues, algorithms for automatically mining formulaic alpha factors have been proposed [14], such as tree models represented by GBDT [6], XGBoost [7], and LightGBM [8], as well as heuristic algorithms represented by GP [9]\u2013[11]. These algorithms can quickly discover numerous new formulaic alpha factors without requiring the domain knowledge or experience of human experts. They offer performance comparable to more complex deep learning-based alpha factors while maintaining relatively high interpretability.\nThe work in [14] is the first to employ reinforcement learning for formulaic alpha factors mining. It employs Markov Decision Processes (MDPs) [15] to simulate the generation process of formulaic alpha factors and trains policies to directly generate a set of collaborative formulaic alpha factors using reinforcement learning. Compared to works [26]\u2013[28] that directly use reinforcement learning to build trading agents, this framework uses historical quantitative and price data of assets as input to find a set of formulaic alpha factors with strong interpretability, avoiding the black-box problem and overcoming the limitations of [6]\u2013[10] in independently mining individual formulaic factors, such as homogenization among factors, lack of synergy, and difficulty in adapting to dynamic market changes. The characteristics of reinforcement learning, supervised learning, tree models, and heuristic algorithms when applied to mining alpha factors are shown in Table I."}, {"title": "B. REINFORCE algorithm", "content": "Williams is the first to intodcuce the REINFORCE algorithm in his works [33]. The algorithm is straightforward and versatile, suitable for a wide range of tasks that can be modeled as MDPs [34]. However, in MDPs with stochastic state transitions and immediate rewards, it often underperforms compared to actor-critic methods [35]. Actor-critic methods, which utilizes value-based techniques to decrease variance and integrate temporal-difference learning for dynamic programming principles, are generally favored. Consequently, the REINFORCE algorithm has not gained widespread favor in the reinforcement learning community. However, our work demonstrates that the REINFORCE algorithm can be suitable to MDPs for mining formulaic factors as long as a proper baseline is used to reduce the variance.\nA series of studies have explored the incorporation of a baseline value in the REINFORCE algorithm. To our best knowledge, [36] is the pioneer in demonstrating that employing the expected reward as a baseline does not universally decrease variance, and he introduced an optimal baseline for a basic 2-armed bandit scenario to address the issue. Subsequently, formal examinations of baseline usage were systematically conducted within the online learning context [37], [38]. Additionally, the regret of REINFORCE algorithm was studied [39]. Recently, The work in [40] delved into the intricacies of multi-armed bandit optimization, revealing that the expected, as opposed to the stochastic, gradient ascent in REINFORCE can lead to a globally optimal solution. Building on this, the work was extended to the function of the baseline value in the natural policy gradient [41], concluding that variance reduction is not critical for natural policy gradient [42]. More recently, the work in [16] find that the REINFORCE is suitable for reinforcement learning from human feedback in large language models, and proposed a baseline value serves as a kind of normalization by comparing the rewards of a random response with those of the greedy response."}, {"title": "III. PROBLEM FORMULATION AND PRELIMINARIES", "content": "This section introduces the considered formulaic alpha factors and their mining processes. We have modeled a finite-length MDPs for these processes, where a token is generated from a limited token pool at each step. Tokens include various operators, the original volume-price features, fundamental features, time deltas, and constants. The two classical policy"}, {"title": "A. Formulaic Alpha Factors for Predicting Asset Prices", "content": "Consider a real market with n assets over T trading days. On each trading day $t \\in \\{1,2,\\ldots,T\\}$, each asset i is represented by a feature vector $x_{t}^{i} \\in \\mathbb{R}^{m \\times 7}$. This vector consists of m raw market features, such as open, high, low, close, and volume values, over the recent 7 days. Here, $x_{t i j} \\in \\mathbb{R}^{1}$ denotes the sequence of values for the j-th raw feature over these 7 days.\nNext, we define an alpha factor function f, which transforms the feature matrix for all assets on a given trading day into alpha values, represented as $X_{t}=\\left[X_{t}^{1}, X_{t}^{2}, \\ldots, X_{t}^{n}\\right]^{T} \\in \\mathbb{R}^{n \\times m \\times r}$, into alpha factor values. Specifically, f maps $X_{t}$ to a vector $z_{t}=f\\left(X_{t}\\right) \\in \\mathbb{R}^{n}$, where $z_{t}$ holds the alpha factor values for each asset on trading day t. The real asset feature dataset $\\mathcal{X}=\\left\\{X_{t}\\right\\}$.\nTo measure the effectiveness of a formulaic alpha factor, we need a real asset price dataset $\\mathbf{y}=\\left\\{y_{t}\\right\\}$, where $t \\in \\{1,2, \\ldots, T\\}$ and $y_{t}$ is the 5-day asset return, where $y_{t} \\in \\mathbb{R}^{n}$. The newly generated formulaic alpha factor will be added to an alpha factors pool, and the combined factor value $z_{t}^{\\prime}$ will be calculated through a combination model, which will be discussed in detail in Section IV-A. The expression for the Pearson correlation coefficient between $y_{t}$ and the combination factor value $z_{t}^{\\prime}$, also known as the Information Coefficient (IC), is as follows:\n$\\qquad IC \\left(z_{t}, y_{t}\\right)=\\frac{\\operatorname{Cov}\\left(z_{t}, y_{t}\\right)}{\\sigma_{z_{t}} \\sigma_{y_{t}}}$\nThe average IC values over all trading days are denoted as $I C=E_{t}\\left[I C\\left(z_{t}, y_{t}\\right)\\right]$.\nFormulaic alpha factors are mathematical expressions that can be represented using RPN, which is a sequence of tokens. Tokens include various operators, the original volume-price features, fundamental features, time deltas, and constants. The operators include elementary functions that operate on single-day data, known as cross-sectional operators (e.g., Abs(x) for the absolute value x, Log(x) for the natural logarithm log(x)), as well as functions that operate on a series of daily data, known as time-series operators (e.g., Ref(x,t) for the expression x evaluated at t days before the current day).\nSuch formulas can naturally be represented by an expression tree, with each non-leaf node representing an operator, and the children of a node representing the original volume-price features, fundamental features, time deltas, and constants being operated on. Each such expression has a unique post-order traversal, using RPN. Examples of some formulaic alpha expressions derived from Alpha101 [29] and their corresponding RPN representations are shown in Table II. An example of a formulaic alpha expression, together with its corresponding tree and RPN representation, is shown in Fig. 1."}, {"title": "B. MDPs for Mining Formulaic Alpha Factors", "content": "We can model the process of generating a linear sequence that can equivalently represent a formulaic alpha factor as an MDP, which can be described as $\\{\\mathcal{S}, \\mathcal{A}, P, r\\}$, where $\\mathcal{S}$ and $\\mathcal{A}$ denote the finite state and action spaces, respectively. P is the transition function defining state transitions, $r: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function assigning values to state-action pairs. Our goal is to obtain the policy $\\pi_{\\theta}$, where $\\theta$ is the training parameter. It generates tokens from a finite set of alternative tokens. Each state in the MDPs corresponds to a sequence of tokens denoting the currently generated part of the expression, denoted as $s_{t}=a_{1: t-1}=\\left[a_{1}, a_{2}, \\ldots, a_{t-1}\\right]^{T}$, the sampling process is modeled as $a_{t} \\sim \\pi_{\\theta}\\left(\\cdot \\mid a_{1: t-1}\\right)$. The action $a_{t}$ is the next token following the currently generated part of the expression.\nThis process continues until the SEP token is encountered or the maximum length is reached. The initial state is always the BEG token, so a valid state always starts with BEG and is followed by previously chosen tokens. Obviously, any generated sequence cannot be guaranteed to be a legal RPN sequence, therefore we only allow specific actions to be taken in certain states to ensure the correct format of the RPN sequence. When $a_{1: t-1}$ and $a_{t}$ are already known, then $s_{t+1}=a_{1: t}$ is uniquely determined, which means that the state transition function P satisfies the Dirac distribution:\n$\\qquad P\\left(s_{t+1} \\mid a_{1: t}\\right)=\\left\\{\\begin{array}{ll}1 & \\text { if } s_{t+1}=a_{t} \\\\0 & \\text { otherwise. }\\end{array}\\right.$     (2)\nThe optimization objective in these MDPs framework is to learn a policy $\\pi_{\\theta}$ that maximizes the expected cumulative reward over time:\n$\\qquad J(\\theta)=\\mathbb{E}_{a_{1: T} \\sim \\pi_{\\theta}}\\left[r\\left(a_{1: T}\\right)\\right]$   (3)\nNon-zero rewards are only received at the final step, which evaluates the quality of a complete formulaic factor expression, not individual tokens:\n$\\qquad r\\left(s_{t}, a_{t}\\right)=\\left\\{\\begin{array}{ll}0 & \\text { if } t \\neq T \\\\r\\left(a_{1: T}\\right) & \\text { otherwise, }\\end{array}\\right.$  (4)\nwhere $r\\left(a_{1: T}\\right)=I C$. Expressions that are syntactically correct might still fail to evaluate due to the restrictions imposed by certain operators. For example, the logarithm operator token is not applicable to negative values. Such invalidity can not be directly detected. Therefore, these expressions are assigned a reward of -1 (the minimum value of IC) to discourage the policy from generating these expressions."}, {"title": "C. Comparing REINFORCE with PPO", "content": "The natural solutions to these MDPs for formulaic alpha factors mining are the respected Proximal Policy Optimization"}, {"title": "IV. BETTER RL IN FORMULAIC ALPHA FACTORS MINING", "content": "In this section, we introduce an innovative reinforcement learning algorithm for alpha factor mining, termed QuantFactor REINFORCE (QFR). We provide a theoretical analysis that establishes an upper bound on the variance of the QFR algorithm in MDPs modeling the formulaic alpha factor mining process. We demonstrate that QFR exhibits reduced variance compared to the celebrated REINFORCE algorithm. The proof that QFR, when applied to MDPs with trajectory-level rewards, exhibits the lowest variance compared to those with non-trajectory-level rewards, is also provided. Furthermore, we introduce IR as a reward shaping mechanism to promote the generation of steady alpha factors."}, {"title": "A. The Proposed Algorithm", "content": "Based on the MDPs defined in Section III-B for formulaic alpha factors mining, unlike the work in [14], we use Monte Carlo methods to obtain an estimate of the policy gradient (6) to optimize a policy $\\pi_{\\theta}\\left(a \\mid a_{1: t-1}\\right)$. Inspired by the REINFORCE algorithm with baseline [16], our QFR algorithm"}, {"title": "B. Reward Shaping", "content": "The reward function from the work [14] focuses on the absolute excess returns of factors, while ignoring the risk-adjusted characteristics of the returns. Unlike this work, in order to better balance returns and risks, our work not only"}, {"title": "C. The Theoretical Analysis", "content": "We provide a set of theoretical results for Algorithm 1, which includes the derivation of the upper bound of the variance, the demonstration that the variance decreases relative to the REINFORCE algorithm and the analysis of the training variance under state transition functions with various distributions. To justify the algorithm design, we first prove that the variance of the proposed algorithm is bounded.\nProposition 1. The gradient estimator (10) is unbiased for the objective function (3), i.e., $\\mathbb{E}[\\tilde{g}(\\theta)]=\\nabla_{\\theta} \\mathbb{E}_{a_{1: T} \\sim \\pi_{\\theta}}\\left[r\\left(a_{1: T}\\right)\\right]$.\nProof: We take the expectation over the randomness of responses $\\left(a_{1: t}, \\ldots, a_{t}\\right)$:\n$\\begin{aligned}\\mathbb{E}[\\tilde{g}(\\theta)]&=\\mathbb{E}\\left[\\sum_{t=1}^{T} \\sum_{a_{1 t}} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid a_{1: t-1}\\right) \\times\\left(r\\left(a_{1: T}\\right)-\\bar{r}\\left(\\bar{a}_{1: T}\\right)\\right)\\right]\\\\&=\\mathbb{E}\\left[\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid a_{1: t-1}\\right) r\\left(a_{1: T}\\right)\\right]\\\\&-\\mathbb{E}\\left[\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(\\bar{a}_{t} \\mid \\bar{a}_{1: t-1}\\right) \\bar{r}\\left(\\bar{a}_{1: T}\\right)\\right]\\\\&=\\nabla_{\\theta} \\mathbb{E}_{a_{1: t} \\sim \\pi_{\\theta}}\\left[r\\left(a_{1: T}\\right)\\right]-\\mathbb{E}_{a_{1: t} \\sim \\pi_{\\theta}}\\left[\\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(\\bar{a}_{t} \\mid \\bar{a}_{1: t-1}\\right) \\times \\bar{r}\\left(\\bar{a}_{1: T}\\right)\\right]\\\\&=\\nabla_{\\theta} \\mathbb{E}_{a_{1: t} \\sim \\pi_{\\theta}}\\left[r\\left(a_{1: T}\\right)\\right],\\end{aligned}$   (15)\n$\\qquad = \\nabla_{\\theta} \\mathbb{E}_{a_{1: t} \\sim \\pi_{\\theta}}\\left[r\\left(a_{1: T}\\right)\\right],$       (16)\nwhere (15) follows from the so-called log-derivative trick, (16) follows from the idea of Bartlett identity $\\sum_{z} \\nabla_{\\theta} p_{\\theta}(z)=\\nabla_{\\theta}\\left[\\sum_{z} p_{\\theta}(z) b\\right]=\\nabla_{\\theta}[1 b]=0$, in which $p_{\\theta}$ can be any distribution and b is constant [51]. Notice that $a_{1: T}$ is conditionally independent on $\\theta$, due to the greedy sampling and apply $p_{\\theta}$ to the the distribution $\\pi_{\\theta}\\left(@_{1: T} \\mid x\\right)$. Then, regarding the second item, we only need to consider $\\mathbb{E}_{a_{1: T} \\sim \\pi_{\\theta}}\\left[\\nabla_{\\theta} \\sum_{t=1}^{T} \\log \\pi_{\\theta}\\left(a_{t} \\mid a_{1: t-1}\\right)\\right]=\\nabla_{\\theta} \\mathbb{E}_{a_{1: T} \\sim \\pi_{\\theta}}\\left[\\sum_{t=1}^{T} \\log \\right.$\\pi_{\\theta}\\left(a_{t} \\mid a_{1: t-1}\\right)\\big]$, which is the sum of log probabilities, whose expected value (i.e., the average log probability) with respect to $\\theta$ should be equal to 0, because the probability distribution $\\pi_{\\theta}$ is fixed under its own expectation. The proof is thus completed.\nSince the baseline value introduced by the QFR algorithm is determined by the reward of the greedy policy, and corresponds to the reward distribution, it is statistically independent of the samples sampled by the normal policy, satisfying the requirements for the proof of unbiasedness.\nProposition 2. Given the deterministic transition function $\\mathcal{T}\\left(s_{t+1} \\mid s_{t}, a_{t}\\right)$, which satisfies the Dirac distribution, and the probabilistic transition function $\\mathcal{T}\\left(s_{t+1}^{\\prime} \\mid s_{t}, a_{t}\\right)$, if they are unbiased, we have $Var[s] \\geq Var[s^P] \\geq \\mathbb{E}_{S^P}[Var [s_{R}|s_{P}]]$, i.e., $Var [s] \\leq Var [s]$, where $s$ denotes a possible sequence generated by $\\mathcal{T}\\left(s_{t+1} \\mid s_{t}, a_{t}\\right)$, and $s^P$ is a sequence from $\\mathcal{T}\\left(s_{t+1}^{\\prime} \\mid s_{t}, a_{t}\\right)$."}, {"title": "VI. CONCLUSION", "content": "In this paper, we have proposed a novel reinforcement learning algorithm, QuantFactor REINFORCE (QFR), for mining formulaic alpha factors. QFR leverages the advantages of discarding the value network while addressing its limitations of high variance by introducing a greedy baseline. Additionally, the incorporation of IR as a reward shaping mechanism encourages the generation of stable alpha factors that can better adapt to changing market conditions.\nOur extensive experiments on real-world asset datasets demonstrate the superiority of QFR over existing factor mining methods. QFR generates alpha factors with higher correlation to asset returns and stronger ability to generate excess profits. It also achieves better performance compared to other state-of-the-art reinforcement learning algorithm when mining formulaic alpha factors.The experimental results also align closely with the theoretical analysis, validating the effectiveness of our proposed algorithm.\nWe conclude that QFR is a promising approach for mining formulaic alpha factors. Its interpretability, stability, and efficiency make it a valuable tool for quantitative finance applications. Future research directions include exploring more sophisticated reward shaping mechanisms and applying QFR to other financial tasks such as portfolio optimization and risk management."}]}