{"title": "QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REINFORCE", "authors": ["Junjie Zhao", "Chengxi Zhang", "Min Qin", "Peng Yang"], "abstract": "The goal of alpha factor mining is to discover indicative signals of investment opportunities from the historical financial market data of assets, which can be used to predict asset returns and gain excess profits. Deep learning based alpha factor mining methods have shown to be powerful, which, however, lack of the interpretability, making them unacceptable in the risk-sensitive real markets. Alpha factors in formulaic forms are more interpretable and therefore favored by market participants, while the search space is complex and powerful explorative methods are urged. Recently, a promising framework is proposed for generating formulaic alpha factors using deep reinforcement learning, and quickly gained research focuses from both academia and industries. This paper first argues that the originally employed policy training method, i.e., Proximal Policy Optimization (PPO), faces several important issues in the context of alpha factors mining, making it ineffective to explore the search space of the formula. Herein, a novel reinforcement learning based on the well-known REINFORCE algorithm is proposed. Given that the underlying state transition function adheres to the Dirac distribution, the Markov Decision Process within this framework exhibit minimal environmental variability, making REINFORCE algorithm more appropriate than PPO. A new dedicated baseline is designed to theoretically reduce the commonly suffered high variance of REINFORCE. Moreover, the information ratio is introduced as a reward shaping mechanism to encourage the generation of steady alpha factors that can better adapt to changes in market volatility. Experimental evaluations on various real assets data show that the proposed algorithm can increase the correlation with asset returns by 3.83%, and a stronger ability to obtain excess returns compared to the latest alpha factors mining methods, which meets the theoretical results well.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realm of computational finance, it has become nearly a conventional practice to convert raw historical asset information into market trend indicative signals, referred to as alpha factor values [1]. The functions that generate these signals are known as alpha factors, which can be expressed in two distinct forms: the deep model and the formula [2]\u2013[4]. Mining high-quality alpha factors has become a trendy topic among investors and researchers, due to their relationship with excess investment returns.\nAlthough alpha factors using end-to-end deep models are generally more expressive, they are in nature black-box and poorly interpretable. Thus, when the performance of these black-box models deteriorates unexpectedly, it is often less likely for human experts to adjust the models accordingly [5]. Therefore, due to the need for risk control, it is difficult to involve these black-box factors in practical trading.\nComparatively, alpha factors represented in formulaic forms enjoy much better interpretability and thus are favored by market participants. The most intuitive method for automatically mining formulaic factors is the tree-based models, which mutate expression trees to generate new alpha factors [6]\u2013[8]. Another commonly used method is the genetic programming, which generates the formulaic alpha factor expressions through iterative simulations of genetic variation and natural selection in biological evolution [9]\u2013[11].\nUnfortunately, tree-based models and genetic programming still face certain challenges. Tree-based models, while easy to understand and implement, may encounter performance bottlenecks when dealing with non-linear relationships and high-dimensional data [12]. Genetic programming, on the other hand, can deal with broader types of expressions by properly defining the search space, including complex non-linear and non-parametric factor expressions, but it usually fails to explore the search space of large-scale expressions. Additionally, the computational costs of genetic programming are usually expensive [13].\nRecently, Yu et. al. [14] proposes a new framework for mining formulaic alpha factors with deep reinforcement learning, trying to bridge both deep learning and formulaic based methods. It employs Markov Decision Processes (MDPs) [15] to simulate the generation process of formulaic alpha factors and trains policies to directly generate a set of collaborative formulaic alpha factors using Deep Reinforcement Learning (DRL). It essentially aims at finding formulaic alpha factors. With the help of DRL, it also overcomes the limitations of the explorative search ability suffered by traditional tree models and genetic programming [14].\nThe key to this framework lies in using the Reverse Polish Notation (RPN) to convert formulaic alpha factors into a se-"}, {"title": null, "content": "quence composed of tokens. Tokens include various operators, the original asset features, constants, and other elements that can constitute a formula, which is the action in the MDPs, with the state being a sequence composed of the previous tokens. In this framework, once a new action is selected, the new state is uniquely determined, as the next step sequence is composed of the present step sequence and the newly generated token. Moreover, only after a complete sequence has been generated can the performance of this factor be evaluated, i.e., rewards are obtained, giving the MDPs of this framework the characteristic of sparse rewards.\n[14] employed the Proximal Policy Optimization (PPO) algorithm, which is a natural solution based on the actor-critic framework, using a value network to reduce the variance in the training process. Although [14] has addressed the issues with traditional methods, we believe that PPO is not suitable for this factor mining framework. The aforementioned framework has the characteristics of deterministic state transitions (transition function satisfies the Dirac distribution) and trajectory-level-reward (non-zero reward can only be obtained after completing a full trajectory), but PPO, due to the inefficiency of the value network in trajectory-level-reward environment [16], and the fact that the value network generally has the same scale as the policy network, leading to twice the time for one iteration of the parameters, cannot effectively solve MDPs for mining formulaic alpha factors.\nThere are two ways to estimate the policy gradient: Temporal Difference Sampling (e.g., PPO using the actor-critic framework) and Monte Carlo sampling (e.g., REINFORCE without using the actor-critic framework). Due to the lack of the value network, we would like to discard it, and naturally thought of choosing REINFORCE. The REINFORCE method does not rely on immediate rewards but on cumulative rewards, which is better suited for our task. Additionally, when using REINFORCE to solve the problem of formulaic alpha factor mining, the sampling process can be completed very quickly and at a low cost, meaning that the cost of obtaining cumulative returns is not high. This is because, unlike alpha factors based on deep models, once the policy network converges, it only needs to perform a single forward pass to obtain the formulaic alpha factors and quickly calculate the factor values for each time's assets, whereas the former requires a forward pass for each factor value calculation, which brings a significant computational resource consumption. However, REINFORCE also suffers from high variance, resulting in a training curve that often fails to converge.\nBased on the discussions mentioned above, our work proposes a new reinforcement learning method outside the actor-critic framework for the first time to accomplish the task of formulaic alpha factors mining. This also offer a new perspective on alpha factors mining within the context of reinforcement learning. Compared to the PPO algorithm used in work [14], the time required for our algorithm to converge is significantly reduced and the factors generated are steadier. Our QuantFactor REINFORCE (QFR) algorithm implements a constrained sequence generator to ensure effective formulaic alpha factors generation. By discarding the value network of [14], our algorithm exhibits faster convergence speeds."}, {"title": null, "content": "Since the transition functions in MDPs for formulaic alpha factors mining satisfying Dirac distribution, the high variance issue caused by discarding the value network is mitigated. To further reduce the variance of the policy gradient estimate, QFR employs a greedy policy to generate a novel baseline. This improvements are also theoretically studied. Additionally, we introduce information ration (IR) for reward shaping to better balance returns and risks. With the introduction of a novel baseline, QFR has a reduced and constrained variance compared to REINFORCE. Furthermore, after incorporating the IR testing mechanism, our work not only evaluates the predictive accuracy of factors but also considers the volatility of their predictive signals. It provides a more comprehensive assessment of factor performance and generate steadier factors that can better adapt to changes in market volatility.\nTo evaluate our proposed QFR algorithm, we conducted extensive experiments on multiple real-world asset datasets. Our experimental results show that the set of formulaic alpha factors generated by the QFR algorithm outperforms those generated by previous methods. It is also shown that theoretical results properly predict trends in the experimental results.\nThe main contributions of the paper are as follows:\nWe propose a novel, stable and efficient reinforcement learning algorithm for mining formulaic alpha factors. Unlike prior research that relies on the actor-critic framework, we discard the value network. This decision stems from the value network's inability to effectively manage MDPs with trajectory-level rewards, its equivalent scale to the policy network, which doubles the parameter iteration time. Moreover, since the transition functions in these MDPs satisfying Dirac distribution, the high variance due to the absence of the value network is significantly mitigated.\nTwo new components are proposed in the framework of formulaic alpha factors mining using reinforcement learning. A novel baseline that employs a greedy policy to generate is proposed to reduce the variance of the policy gradient estimate. Additionally, IR is introduced as a reward shaping mechanism to balance returns and risks, providing a more comprehensive assessment of factor performance and generating steadier factors. We also provide a set of theoretical results, which includes an analysis of the training variance under state transition functions with various distributions, the derivation of the upper bound of the variance after introducing the baseline, and the proof that the variance decreases relative to the REINFORCE algorithm.\nExtensive experiments on multiple real-world asset datasets show that the proposed algorithm outperforms the reinforcement learning algorithm used in work [14], as well as various tree-based and heuristic algorithms. Compared to the previous state-of-the-art algorithm, it improves the correlation with asset returns by 3.83%, while also demonstrating a stronger ability to generate excess profits. Additionally, the experimental results closely align with the theoretical results.\nThe rest of this paper is organized as follows. Related"}, {"title": "II. RELATED WORK", "content": "This section provides a brief review of the related work on the automatic mining methods for alpha factors and the theory of REINFORCE algorithm."}, {"title": "A. Automatic Mining for Alpha Factors", "content": "Alpha factors are generally represented in the form of deep models or formulas. Alpha factors using end-to-end deep models are more complex and usually trained with supervised learning [17]\u2013[19], utilizing MLP [20] or sequential models like LSTM [21] and Transformer [22] to extract features embedded in the historical data of assets. Recently, reinforcement learning has attracted much attention in the context of computational finance and fintech. It becomes a key technology in alpha factor mining [14], investment portfolio optimization [23], and risk management design [24], thanks to its advantages in handling non-linearity, high-dimensional data, and dynamic environments [25]. By modeling market characteristics as states, maker or taker orders as actions, and profit and loss as rewards, reinforcement learning can also be used to train deep policy models that represents alpha factors [26]\u2013[28].\nOn the other hand, alpha factors represented in formulaic forms have much better interpretability and thus are favored by market participants. In the past, these formulaic alpha factors were constructed by human experts using their domain knowledge and experience, often embodying clear economic principles. For example, Kakushadze [29] presents 101 formulaic alpha factors tested in the US asset market. However, the alpha factor mining process relying on human experts suffers from multiple drawbacks such as strong subjectivity [30], time-consuming [30], insufficient risk control [31], and high costs [32]. To address these issues, algorithms for automatically mining formulaic alpha factors have been proposed [14], such as tree models represented by GBDT [6], XGBoost [7], and LightGBM [8], as well as heuristic algorithms represented by GP [9]\u2013[11]. These algorithms can quickly discover numerous new formulaic alpha factors without requiring the domain knowledge or experience of human experts. They offer performance comparable to more complex deep learning-based alpha factors while maintaining relatively high interpretability.\nThe work in [14] is the first to employ reinforcement learn-ing for formulaic alpha factors mining. It employs Markov Decision Processes (MDPs) [15] to simulate the generation process of formulaic alpha factors and trains policies to directly generate a set of collaborative formulaic alpha factors"}, {"title": "B. REINFORCE algorithm", "content": "Williams is the first to intodcuce the REINFORCE algorithm in his works [33]. The algorithm is straightforward and versatile, suitable for a wide range of tasks that can be modeled as MDPs [34]. However, in MDPs with stochastic state transitions and immediate rewards, it often underperforms compared to actor-critic methods [35]. Actor-critic methods, which utilizes value-based techniques to decrease variance and integrate temporal-difference learning for dynamic programming principles, are generally favored. Consequently, the REINFORCE algorithm has not gained widespread favor in the reinforcement learning community. However, our work demonstrates that the REINFORCE algorithm can be suitable to MDPs for mining formulaic factors as long as a proper baseline is used to reduce the variance.\nA series of studies have explored the incorporation of a baseline value in the REINFORCE algorithm. To our best knowledge, [36] is the pioneer in demonstrating that employing the expected reward as a baseline does not universally decrease variance, and he introduced an optimal baseline for a basic 2-armed bandit scenario to address the issue. Subsequently, formal examinations of baseline usage were systematically conducted within the online learning context [37], [38]. Additionally, the regret of REINFORCE algorithm was studied [39]. Recently, The work in [40] delved into the intricacies of multi-armed bandit optimization, revealing that the expected, as opposed to the stochastic, gradient ascent in REINFORCE can lead to a globally optimal solution. Building on this, the work was extended to the function of the baseline value in the natural policy gradient [41], concluding that variance reduction is not critical for natural policy gradient [42]. More recently, the work in [16] find that the REINFORCE is suitable for reinforcement learning from human feedback in large language models, and proposed a baseline value serves as a kind of normalization by comparing the rewards of a random response with those of the greedy response."}, {"title": "III. PROBLEM FORMULATION AND PRELIMINARIES", "content": "This section introduces the considered formulaic alpha factors and their mining processes. We have modeled a finite-length MDPs for these processes, where a token is generated from a limited token pool at each step. Tokens include various operators, the original volume-price features, fundamental features, time deltas, and constants. The two classical policy"}, {"title": "A. Formulaic Alpha Factors for Predicting Asset Prices", "content": "Consider a real market with n assets over T trading days. On each trading day $t \\in {1,2,\\ldots ,T}$, each asset i is represented by a feature vector $x_{t}^{i} \\in \\mathbb{R}^{m \\times 7}$. This vector consists of m raw market features, such as open, high, low, close, and volume values, over the recent 7 days. Here, $x_{t i j} \\in \\mathbb{R}^{1}$ denotes the sequence of values for the j-th raw feature over these 7 days.\nNext, we define an alpha factor function f, which transforms the feature matrix for all assets on a given trading day into alpha values, represented as $X_{t} = [X_{t 1}, X_{t 2},..., X_{t n}]^{T} \\in \\mathbb{R}^{n \\times m \\times r}$, into alpha factor values. Specifically, f maps $X_{t}$ to a vector $z_{t} = f(X_{t}) \\in \\mathbb{R}^{n}$, where $z_{t}$ holds the alpha factor values for each asset on trading day t. The real asset feature dataset $X = {X_{t}}$.\nTo measure the effectiveness of a formulaic alpha factor, we need a real asset price dataset $y = {y_{t}}$, where $t\\in {1,2,\\ldots,T}$ and $y_{t}$ is the 5-day asset return, where $y_{t} \\in \\mathbb{R}^{n}$. The newly generated formulaic alpha factor will be added to an alpha factors pool, and the combined factor value $z'$ will be calculated through a combination model, which will be discussed in detail in Section IV-A. The expression for the Pearson correlation coefficient between $y_{t}$ and the combination factor value $z$, also known as the Information Coefficient (IC), is as follows:\n$\\mathrm{IC} (z, y_{t}) = \\frac{\\mathrm{Cov}(z, y_{t})}{\\sigma_{z y_{t}}}$\nThe average IC values over all trading days are denoted as $\\overline{\\mathrm{IC}} = \\mathbb{E}_{t} [\\mathrm{IC} (z_{t}, y_{t})]$.\nFormulaic alpha factors are mathematical expressions that can be represented using RPN, which is a sequence of tokens. Tokens include various operators, the original volume-price features, fundamental features, time deltas, and constants. The operators include elementary functions that operate on single-day data, known as cross-sectional operators (e.g., Abs(x) for the absolute value x, Log(x) for the natural logarithm $\\log(x)$), as well as functions that operate on a series of daily data, known as time-series operators (e.g., Ref(x,t) for the expression x evaluated at t days before the current day). Such formulas can naturally be represented by an expression tree, with each non-leaf node representing an operator, and the children of a node representing the original volume-price features, fundamental features, time deltas, and constants being operated on. Each such expression has a unique post-order traversal, using RPN. Examples of some formulaic alpha expressions derived from Alpha101 [29] and their corresponding RPN representations are shown in Table II. An example of a formulaic alpha expression, together with its corresponding tree and RPN representation, is shown in Fig. 1."}, {"title": "B. MDPs for Mining Formulaic Alpha Factors", "content": "We can model the process of generating a linear sequence that can equivalently represent a formulaic alpha factor as an MDP, which can be described as ${S, A, P, r}$, where S and A denote the finite state and action spaces, respectively. P is the transition function defining state transitions, $r: S \\times A \\rightarrow \\mathbb{R}$ is the reward function assigning values to state-action pairs. Our goal is to obtain the policy $\\pi_{\\theta}$, where $\\theta$ is the training parameter. It generates tokens from a finite set of alternative tokens. Each state in the MDPs corresponds to a sequence of tokens denoting the currently generated part of the expression, denoted as $s_{t} = a_{1:t-1} = [a_{1},a_{2},\\ldots,a_{t-1}]^{T}$, the sampling process is modeled as $a_{t} \\sim \\pi_{\\theta} (\\cdot | a_{1:t-1})$. The action $a_{t}$ is the next token following the currently generated part of the expression.\nThis process continues until the SEP token is encountered or the maximum length is reached. The initial state is always the BEG token, so a valid state always starts with BEG and is followed by previously chosen tokens. Obviously, any generated sequence cannot be guaranteed to be a legal RPN sequence, therefore we only allow specific actions to be taken in certain states to ensure the correct format of the RPN sequence. When $a_{1:t-1}$ and $a_{t}$ are already known, then $s_{t+1} = a_{1:t}$ is uniquely determined, which means that the state transition function P satisfies the Dirac distribution:\n$P (s_{t+1} | a_{1:t}) = \\begin{cases}\n1 & \\text{if } s_{t+1} = a_{t}\\\\\n0 & \\text{otherwise}.\n\\end{cases}$    (2)\nThe optimization objective in these MDPs framework is to learn a policy $\\pi_{\\theta}$ that maximizes the expected cumulative reward over time:\n$J(\\theta) = \\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta}} [r (a_{1:T})]$     (3)\nNon-zero rewards are only received at the final step, which evaluates the quality of a complete formulaic factor expression, not individual tokens:\nr (s_{t}, a_{t}) = \\begin{cases}\n0 & \\text{if } t \\neq T\\\\\\\nr (a_{1:T}) & \\text{otherwise},\n\\end{cases}$    (4)\nwhere $r (a_{1:T}) = \\overline{\\mathrm{IC}}$. Expressions that are syntactically correct might still fail to evaluate due to the restrictions imposed by certain operators. For example, the logarithm operator token is not applicable to negative values. Such invalidity can not be directly detected. Therefore, these expressions are assigned a reward of -1 (the minimum value of IC) to discourage the policy from generating these expressions."}, {"title": "C. Comparing REINFORCE with PPO", "content": "The natural solutions to these MDPs for formulaic alpha factors mining are the respected Proximal Policy Optimization"}, {"title": "IV. BETTER RL IN FORMULAIC ALPHA FACTORS MINING", "content": "In this section, we introduce an innovative reinforcement learning algorithm for alpha factor mining, termed QuantFactor REINFORCE (QFR). We provide a theoretical analysis that establishes an upper bound on the variance of the QFR algorithm in MDPs modeling the formulaic alpha factor mining process. We demonstrate that QFR exhibits reduced variance compared to the celebrated REINFORCE algorithm. The proof that QFR, when applied to MDPs with trajectory-level rewards, exhibits the lowest variance compared to those with non-trajectory-level rewards, is also provided. Furthermore, we introduce IR as a reward shaping mechanism to promote the generation of steady alpha factors."}, {"title": "A. The Proposed Algorithm", "content": "Based on the MDPs defined in Section III-B for formulaic alpha factors mining, unlike the work in [14], we use Monte Carlo methods to obtain an estimate of the policy gradient (6) to optimize a policy $\\pi_{\\theta}(a | a_{1:t-1})$. Inspired by the REINFORCE algorithm with baseline [16], our QFR algorithm"}, {"title": "B. Reward Shaping", "content": "The reward function from the work [14] focuses on the absolute excess returns of factors, while ignoring the risk-adjusted characteristics of the returns. Unlike this work, in order to better balance returns and risks, our work not only"}, {"title": "C. The Theoretical Analysis", "content": "We provide a set of theoretical results for Algorithm 1, which includes the derivation of the upper bound of the variance, the demonstration that the variance decreases relative to the REINFORCE algorithm and the analysis of the training variance under state transition functions with various distributions. To justify the algorithm design, we first prove that the variance of the proposed algorithm is bounded.\n**Proposition 1.** The gradient estimator (10) is unbiased for the objective function (3), i.e., $\\mathbb{E}[\\tilde{g}(\\theta)] = \\nabla_{\\theta} \\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta}} [r (a_{1:T})]$.\n*Proof:* We take the expectation over the randomness of responses ($a_{1:t},...,a_{t}$):\n$\\begin{aligned}\n\\mathbb{E}[\\tilde{g}(\\theta)]  & = \\mathbb{E}\\left[\\sum_{t=1}^{T} \\mathbf{V}_{\\theta} \\log \\pi_{\\theta} (a_{t} | a_{1:t-1}) \\times (r (a_{1:T}) - \\tilde{r} (\\bar{a}_{1:T}))\\right] \\\\\n& = \\mathbb{E}\\left[\\sum_{t=1}^{T} \\mathbf{V}_{\\theta} \\log \\pi_{\\theta} (a_{t} | a_{1:t-1}) \\times r (a_{1:T})\\right] - \\mathbb{E}\\left[\\sum_{t=1}^{T} \\mathbf{V}_{\\theta} \\log \\pi_{\\theta} (a_{t} | a_{1:t-1}) \\times \\tilde{r} (\\bar{a}_{1:T})\\right] \\\\\n& = \\nabla_{\\theta} \\mathbb{E}_{a_{1:t} \\sim \\pi_{\\theta}} [r (a_{1:T})] -  \\mathbb{E}_{a_{1:t} \\sim \\pi_{\\theta}}\\left[\\sum_{t=1}^{T} \\mathbf{V}_{\\theta} \\log \\pi_{\\theta} (a_{t} | a_{1:t-1}) \\times \\tilde{r} (\\bar{a}_{1:T})\\right]\n\\end{aligned}$\n$\\begin{aligned}\n & = \\nabla_{\\theta} \\mathbb{E}_{a_{1:t} \\sim \\pi_{\\theta}} [r (a_{1:T})] -  \\mathbb{E}_{\\bar{a}_{1:T}}\\left[\\sum_{t=1}^{T} \\mathbf{V}_{\\theta} \\log \\mathbb{E}_{\\pi_{\\theta}} (a_{t} | a_{1:t-1}) \\times \\tilde{r} (\\bar{a}_{1:T})\\right] \\\\\n& = \\nabla_{\\theta} \\mathbb{E}_{a_{1:t} \\sim \\pi_{\\theta}} [r (a_{1:T})]\n\\end{aligned}$  (15)\n   (16)\nwhere (15) follows from the so-called log-derivative trick, (16) follows from the idea of Bartlett identity $\\mathbf{V}_{o}[\\sum_{z} p_{o}(z)] = \\mathbf{V}_{o}[1] = 0$, in which $p_{o}$ can be any distribution and b is constant [51]. Notice that $a_{1:T}$ is conditionally independent on $o$, due to the greedy sampling and apply po to the the distribution $\\pi_{\\theta} (\\bar{a}_{1:T} | x)$. Then, regarding the second item, we only need to consider $\\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta}} [\\mathbf{V}_{\\theta} \\sum_{t=1}^{T}\\log \\pi_{\\theta}(a_{t} | a_{1:t-1})] = \\mathbf{V}_{\\theta} \\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta}} [\\sum_{t=1}^{T}\\log \\pi_{\\theta}(a_{t} | a_{1:t-1})]$, which is the sum of log probabilities, whose expected value (i.e., the average log probability) with respect to $\\theta$ should be equal to 0, because the probability distribution $\\pi_{\\theta}$ is fixed under its own expectation.\nThe proof is thus completed.\nSince the baseline value introduced by the QFR algorithm is determined by the reward of the greedy policy, and corresponds to the reward distribution, it is statistically independent of the samples sampled by the normal policy, satisfying the requirements for the proof of unbiasedness.\n**Proposition 2.** Given the deterministic transition function T(s+1|St, at), which satisfies the Dirac distribution, and the probabilistic transition function T(s'\"+1 St, at), if they are unbiased, we have Var [s] \u2265 Var [sR], i.e., $Var [s] \\leq Var [s]$, where s denotes a possible sequence generated by T(s+1|St, at), and s is a sequence from T(s't+1|St, at).\""}, {"title": null, "content": "*Proof:* The transition model of MDPs defined in Section III-B can be decomposed as follows. Firstly, the policy model gives a token at based on the input sequence st = a1:t\u22121, and the output sequence s+1 is built by simply appending at to st, i.e., st+1 = a1:t. If the transition is deterministic, meaning the transition function satisfies the Dirac distribution, then st+1 is directly used as the input state for next step. If a probabilistic transition function is used, T(st+1 St, at) is applied and it generates a new random sequence s'", "a1": "t\u22121 is a vector of length t - 1, its variance can be expanded as the summation of its all digits:\n$Var [s_{t}] = \\mathbb{E} [(s_{t} - \\mathbb{E} [s_{t}])^{2}] = \\sum_{i=1}^{t-1} \\mathbb{E} [(a_{i} - \\mathbb{E} [a_{i}])^{2}] = \\sum_{i=1}^{t-1} Var [a_{i}].$\nSimilarly, the variance of st+1 is:\n$Var [s_{t+1}] = \\sum_{i=1}^{t} Var [a_{i}]$\n$= Var [s_{t}] + \\mathbb{E}_{s_{t}} [Var [a_{t} | s_{t}]].$   (17)\nThe equation holds because st+1 and st has the same first t - 1 digits, while s+1 has one more token at the t-th position, which naturally introduces more variance. The variance originates from the policy model and can be expressed as $\\mathbb{E}_{s_{t}} [Var [a_{t} | s_{t}]] = \\sum_{a_{t},s_{t}} P(s_{t})\\pi (a_{t} | s_{t}) (a_{t} - \\mathbb{E} [a_{t} | s_{t}])^{2}$\nThen, we estimate the variance of sequences after the transition process (st+1 T(st+1 St,at) s", "s": 1, "by": "n$Var [s^{'+1}_{t+1}] = \\mathbb{E}_{s_{t+1}} [\\mathbb{E}_{s^{'+1}_{t+1}} [(s^{'+1}_{t+1})^{2} | s_{t+1}]] - (\\mathbb{E} [s^{'+1}_{t+1}])^{2} \\\\\n= \\mathbb{E}_{s_{t+1}} [Var [s^{'+1}_{t+1} | s_{t+1}] + (\\mathbb{E} [s^{'+1}_{t+1} | s_{t+1}])^{2}] - (\\mathbb{E} [s^{'+1}_{t+1}])^{2}.$    (18)\nAssuming the transition model is unbiased, we have $\\mathbb{E} [s^{'+1}_{t+1} | s_{t+1}] = s_{t+1}$ and $\\mathbb{E} [s^{'+1}_{t+1}] = \\mathbb{E} [s_{t+1}]$. Then (18) can be further simplified:\n$\\begin{aligned}\nVar [s^{'+1}_{t+1}] & = \\mathbb{E}_{s_{t+1}} [Var [s^{'+1}_{t+1} | s_{t+1}] + (s_{t+1})^{2}] - (\\mathbb{E} [s_{t+1}])^{2} \\\\\n& = \\mathbb{E}_{s_{t+1}} [Var [s^{'+1}_{t+1} | s_{t+1}]] + \\mathbb{E} [(s_{t+1})^{2}] - (\\mathbb{E} [s_{t+1}])^{2} \\\\\n& = \\mathbb{E}_{s_{t+1}} [Var [s^{'+1}_{t+1} | s_{t+1}]] + Var [s_{t+1}],\n\\end{aligned}$ (19)\nwhere $\\mathbb{E}_{s_{t+1}} [Var [s^{'+1}_{t+1} | s_{t+1}]]$ can be expressed as:\n$\\mathbb{E}_{s_{t+1}} [Var [s^{'+1}_{t+1} | s_{t+1}]] = \\sum_{s_{t+1},s^{'+1}_{t+1}} P(s_{t+1}) T (s^{'+1}_{t+1} | s_{t+1}) (s^{'+1}_{t+1} - s_{t+1})^{2}.$\nFinally, we can estimate the variance under deterministic transition by recursively using (17):\n$Var [s_{T}] = \\sum_{t=1}^{T-1} \\mathbb{E}_{s_{t}} [Var [a_{t} | s_{t}]]$.\nThe variance under probabilistic transition can be estimated using (17), (19):\n$\\begin{aligned}\nVar [s^{R}_{T}] & = \\mathbb{E}_{s^{R}_{T+1}} [Var [s^{'+1}_{t+1} | s_{t+1}]] + Var [s_{t+1}]\\\\\n& = \\sum_{t=1}^{T-1} \\mathbb{E}_{s^{R}_{t}} \\mathbb{E}_{s^{R}_{t+1}} [Var [a_{t} | s^{R}_{t}]] + \\sum_{t=1}^{T-1}  [Var [s^{'+1}_{t+1} | s_{t+1}]].\n\\end{aligned}$\nSince $\\mathbb{E}_{s^{R}_{t}} [Var [a_{t} | s_{t}]]$ and $\\mathbb{E}_{s^{R}_{t}} [Var [a_{t} | s^{P}_{t}]]$ both have the order of $\\sim max(|a_{t}|^{2})$, we assume $\\mathbb{E}_{s^{P}_{t}} [Var [a_{t} | s_{t}]] \\approx \\mathbb{E}_{s^{R}_{t}} [Var [a_{t} | s_{t}]]$. Finally the variances under both cases approximately satisfy the following condition:\n$Var [s^{R}_{T}] = Var [s_{T}] - \\sum_{t=1}^{T-1} \\mathbb{E}_{s^{R}_{t}} [Var [s^{'+1}_{t+1} | s_{t+1}]]$.\nThe equation shows that $s^{R}_{T}$ has a lower variance than $s_{T}$ by $\\sum_{t=1}^{T-1} \\mathbb{E}_{s^{R}_{t}} [Var [s^{'+1}_{t+1} | s_{t+1}]]$. This relation holds because the variance of $s^{P}_{T}$ only originates from randomness of decision making during each step, while the variance of $s^{R}_{T}$ is also from the random transition process. The proof is thus completed.\nThis proposition demonstrates that MDPs corresponding to state transfer functions that satisfy the Dirac distribution have the smallest variance compared to MDPs with diverse state transfer functions, and thus REINFORCE exhibits an advantage in deterministic environments. The lower environmental variance alleviates the deficiency of high variance in the REINFORCE algorithm."}, {"title": null, "content": "**Proposition 3.** Consider the parameterization $\\pi_{\\theta"}, "a | a_{1:t"]}