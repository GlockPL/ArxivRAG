[{"title": "QuantFactor REINFORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REINFORCE", "authors": ["Junjie Zhao", "Chengxi Zhang", "Min Qin", "Peng Yang"], "abstract": "The goal of alpha factor mining is to discover indicative signals of investment opportunities from the historical financial market data of assets, which can be used to predict asset returns and gain excess profits. Deep learning based alpha factor mining methods have shown to be powerful, which, however, lack of the interpretability, making them unacceptable in the risk-sensitive real markets. Alpha factors in formulaic forms are more interpretable and therefore favored by market participants, while the search space is complex and powerful explorative methods are urged. Recently, a promising framework is proposed for generating formulaic alpha factors using deep reinforcement learning, and quickly gained research focuses from both academia and industries. This paper first argues that the originally employed policy training method, i.e., Proximal Policy Optimization (PPO), faces several important issues in the context of alpha factors mining, making it ineffective to explore the search space of the formula. Herein, a novel reinforcement learning based on the well-known REINFORCE algorithm is proposed. Given that the underlying state transition function adheres to the Dirac distribution, the Markov Decision Process within this framework exhibit minimal environmental variability, making REINFORCE algorithm more appropriate than PPO. A new dedicated baseline is designed to theoretically reduce the commonly suffered high variance of REINFORCE. Moreover, the information ratio is introduced as a reward shaping mechanism to encourage the generation of steady alpha factors that can better adapt to changes in market volatility. Experimental evaluations on various real assets data show that the proposed algorithm can increase the correlation with asset returns by 3.83%, and a stronger ability to obtain excess returns compared to the latest alpha factors mining methods, which meets the theoretical results well.", "sections": [{"title": "I. INTRODUCTION", "content": "In the realm of computational finance, it has become nearly a conventional practice to convert raw historical asset information into market trend indicative signals, referred to as alpha factor values [1]. The functions that generate these signals are known as alpha factors, which can be expressed in two distinct forms: the deep model and the formula [2]\u2013[4]. Mining high-quality alpha factors has become a trendy topic among investors and researchers, due to their relationship with excess investment returns.\nAlthough alpha factors using end-to-end deep models are generally more expressive, they are in nature black-box and poorly interpretable. Thus, when the performance of these black-box models deteriorates unexpectedly, it is often less likely for human experts to adjust the models accordingly [5]. Therefore, due to the need for risk control, it is difficult to involve these black-box factors in practical trading.\nComparatively, alpha factors represented in formulaic forms enjoy much better interpretability and thus are favored by market participants. The most intuitive method for automatically mining formulaic factors is the tree-based models, which mutate expression trees to generate new alpha factors [6]\u2013[8]. Another commonly used method is the genetic programming, which generates the formulaic alpha factor expressions through iterative simulations of genetic variation and natural selection in biological evolution [9]\u2013[11].\nUnfortunately, tree-based models and genetic programming still face certain challenges. Tree-based models, while easy to understand and implement, may encounter performance bottlenecks when dealing with non-linear relationships and high-dimensional data [12]. Genetic programming, on the other hand, can deal with broader types of expressions by properly defining the search space, including complex non-linear and non-parametric factor expressions, but it usually fails to explore the search space of large-scale expressions. Additionally, the computational costs of genetic programming are usually expensive [13].\nRecently, Yu et. al. [14] proposes a new framework for mining formulaic alpha factors with deep reinforcement learning, trying to bridge both deep learning and formulaic based methods. It employs Markov Decision Processes (MDPs) [15] to simulate the generation process of formulaic alpha factors and trains policies to directly generate a set of collaborative formulaic alpha factors using Deep Reinforcement Learning (DRL). It essentially aims at finding formulaic alpha factors. With the help of DRL, it also overcomes the limitations of the explorative search ability suffered by traditional tree models and genetic programming [14].\nThe key to this framework lies in using the Reverse Polish Notation (RPN) to convert formulaic alpha factors into a se-"}, {"title": null, "content": "quence composed of tokens. Tokens include various operators, the original asset features, constants, and other elements that can constitute a formula, which is the action in the MDPs, with the state being a sequence composed of the previous tokens. In this framework, once a new action is selected, the new state is uniquely determined, as the next step sequence is composed of the present step sequence and the newly generated token. Moreover, only after a complete sequence has been generated can the performance of this factor be evaluated, i.e., rewards are obtained, giving the MDPs of this framework the characteristic of sparse rewards.\n[14] employed the Proximal Policy Optimization (PPO) algorithm, which is a natural solution based on the actor-critic framework, using a value network to reduce the variance in the training process. Although [14] has addressed the issues with traditional methods, we believe that PPO is not suitable for this factor mining framework. The aforementioned framework has the characteristics of deterministic state transitions (transition function satisfies the Dirac distribution) and trajectory-level-reward (non-zero reward can only be obtained after completing a full trajectory), but PPO, due to the inefficiency of the value network in trajectory-level-reward environment [16], and the fact that the value network generally has the same scale as the policy network, leading to twice the time for one iteration of the parameters, cannot effectively solve MDPs for mining formulaic alpha factors.\nThere are two ways to estimate the policy gradient: Temporal Difference Sampling (e.g., PPO using the actor-critic framework) and Monte Carlo sampling (e.g., REINFORCE without using the actor-critic framework). Due to the lack of the value network, we would like to discard it, and naturally thought of choosing REINFORCE. The REINFORCE method does not rely on immediate rewards but on cumulative rewards, which is better suited for our task. Additionally, when using REINFORCE to solve the problem of formulaic alpha factor mining, the sampling process can be completed very quickly and at a low cost, meaning that the cost of obtaining cumulative returns is not high. This is because, unlike alpha factors based on deep models, once the policy network converges, it only needs to perform a single forward pass to obtain the formulaic alpha factors and quickly calculate the factor values for each time's assets, whereas the former requires a forward pass for each factor value calculation, which brings a significant computational resource consumption. However, REINFORCE also suffers from high variance, resulting in a training curve that often fails to converge.\nBased on the discussions mentioned above, our work proposes a new reinforcement learning method outside the actor-critic framework for the first time to accomplish the task of formulaic alpha factors mining. This also offer a new perspective on alpha factors mining within the context of reinforcement learning. Compared to the PPO algorithm used in work [14], the time required for our algorithm to converge is significantly reduced and the factors generated are steadier. Our QuantFactor REINFORCE (QFR) algorithm implements a constrained sequence generator to ensure effective formulaic alpha factors generation. By discarding the value network of [14], our algorithm exhibits faster convergence speeds."}, {"title": null, "content": "Since the transition functions in MDPs for formulaic alpha factors mining satisfying Dirac distribution, the high variance issue caused by discarding the value network is mitigated. To further reduce the variance of the policy gradient estimate, QFR employs a greedy policy to generate a novel baseline. This improvements are also theoretically studied. Additionally, we introduce information ration (IR) for reward shaping to better balance returns and risks. With the introduction of a novel baseline, QFR has a reduced and constrained variance compared to REINFORCE. Furthermore, after incorporating the IR testing mechanism, our work not only evaluates the predictive accuracy of factors but also considers the volatility of their predictive signals. It provides a more comprehensive assessment of factor performance and generate steadier factors that can better adapt to changes in market volatility.\nTo evaluate our proposed QFR algorithm, we conducted extensive experiments on multiple real-world asset datasets. Our experimental results show that the set of formulaic alpha factors generated by the QFR algorithm outperforms those generated by previous methods. It is also shown that theoretical results properly predict trends in the experimental results.\nThe main contributions of the paper are as follows:\n\u2022 We propose a novel, stable and efficient reinforcement learning algorithm for mining formulaic alpha factors. Unlike prior research that relies on the actor-critic framework, we discard the value network. This decision stems from the value network's inability to effectively manage MDPs with trajectory-level rewards, its equivalent scale to the policy network, which doubles the parameter iteration time. Moreover, since the transition functions in these MDPs satisfying Dirac distribution, the high variance due to the absence of the value network is significantly mitigated.\n\u2022 Two new components are proposed in the framework of formulaic alpha factors mining using reinforcement learning. A novel baseline that employs a greedy policy to generate is proposed to reduce the variance of the policy gradient estimate. Additionally, IR is introduced as a reward shaping mechanism to balance returns and risks, providing a more comprehensive assessment of factor performance and generating steadier factors. We also provide a set of theoretical results, which includes an analysis of the training variance under state transition functions with various distributions, the derivation of the upper bound of the variance after introducing the baseline, and the proof that the variance decreases relative to the REINFORCE algorithm.\n\u2022 Extensive experiments on multiple real-world asset datasets show that the proposed algorithm outperforms the reinforcement learning algorithm used in work [14], as well as various tree-based and heuristic algorithms. Compared to the previous state-of-the-art algorithm, it improves the correlation with asset returns by 3.83%, while also demonstrating a stronger ability to generate excess profits. Additionally, the experimental results closely align with the theoretical results.\nThe rest of this paper is organized as follows. Related"}, {"title": "II. RELATED WORK", "content": "This section provides a brief review of the related work on the automatic mining methods for alpha factors and the theory of REINFORCE algorithm.\nA. Automatic Mining for Alpha Factors\nAlpha factors are generally represented in the form of deep models or formulas. Alpha factors using end-to-end deep models are more complex and usually trained with supervised learning [17]\u2013[19], utilizing MLP [20] or sequential models like LSTM [21] and Transformer [22] to extract features embedded in the historical data of assets. Recently, reinforcement learning has attracted much attention in the context of computational finance and fintech. It becomes a key technology in alpha factor mining [14], investment portfolio optimization [23], and risk management design [24], thanks to its advantages in handling non-linearity, high-dimensional data, and dynamic environments [25]. By modeling market characteristics as states, maker or taker orders as actions, and profit and loss as rewards, reinforcement learning can also be used to train deep policy models that represents alpha factors [26]\u2013[28].\nOn the other hand, alpha factors represented in formulaic forms have much better interpretability and thus are favored by market participants. In the past, these formulaic alpha factors were constructed by human experts using their domain knowledge and experience, often embodying clear economic principles. For example, Kakushadze [29] presents 101 formulaic alpha factors tested in the US asset market. However, the alpha factor mining process relying on human experts suffers from multiple drawbacks such as strong subjectivity [30], time-consuming [30], insufficient risk control [31], and high costs [32]. To address these issues, algorithms for automatically mining formulaic alpha factors have been proposed [14], such as tree models represented by GBDT [6], XGBoost [7], and LightGBM [8], as well as heuristic algorithms represented by GP [9]\u2013[11]. These algorithms can quickly discover numerous new formulaic alpha factors without requiring the domain knowledge or experience of human experts. They offer performance comparable to more complex deep learning-based alpha factors while maintaining relatively high interpretability.\nThe work in [14] is the first to employ reinforcement learning for formulaic alpha factors mining. It employs Markov Decision Processes (MDPs) [15] to simulate the generation process of formulaic alpha factors and trains policies to directly generate a set of collaborative formulaic alpha factors"}, {"title": null, "content": "using reinforcement learning. Compared to works [26]\u2013[28] that directly use reinforcement learning to build trading agents, this framework uses historical quantitative and price data of assets as input to find a set of formulaic alpha factors with strong interpretability, avoiding the black-box problem and overcoming the limitations of [6]\u2013[10] in independently mining individual formulaic factors, such as homogenization among factors, lack of synergy, and difficulty in adapting to dynamic market changes. The characteristics of reinforcement learning, supervised learning, tree models, and heuristic algorithms when applied to mining alpha factors are shown in Table I.\nB. REINFORCE algorithm\nWilliams is the first to intodcuce the REINFORCE algorithm in his works [33]. The algorithm is straightforward and versatile, suitable for a wide range of tasks that can be modeled as MDPs [34]. However, in MDPs with stochastic state transitions and immediate rewards, it often underperforms compared to actor-critic methods [35]. Actor-critic methods, which utilizes value-based techniques to decrease variance and integrate temporal-difference learning for dynamic programming principles, are generally favored. Consequently, the REINFORCE algorithm has not gained widespread favor in the reinforcement learning community. However, our work demonstrates that the REINFORCE algorithm can be suitable to MDPs for mining formulaic factors as long as a proper baseline is used to reduce the variance.\nA series of studies have explored the incorporation of a baseline value in the REINFORCE algorithm. To our best knowledge, [36] is the pioneer in demonstrating that employing the expected reward as a baseline does not universally decrease variance, and he introduced an optimal baseline for a basic 2-armed bandit scenario to address the issue. Subsequently, formal examinations of baseline usage were systematically conducted within the online learning context [37], [38]. Additionally, the regret of REINFORCE algorithm was studied [39]. Recently, The work in [40] delved into the intricacies of multi-armed bandit optimization, revealing that the expected, as opposed to the stochastic, gradient ascent in REINFORCE can lead to a globally optimal solution. Building on this, the work was extended to the function of the baseline value in the natural policy gradient [41], concluding that variance reduction is not critical for natural policy gradient [42]. More recently, the work in [16] find that the REINFORCE is suitable for reinforcement learning from human feedback in large language models, and proposed a baseline value serves as a kind of normalization by comparing the rewards of a random response with those of the greedy response."}, {"title": "III. PROBLEM FORMULATION AND PRELIMINARIES", "content": "This section introduces the considered formulaic alpha factors and their mining processes. We have modeled a finite-length MDPs for these processes, where a token is generated from a limited token pool at each step. Tokens include various operators, the original volume-price features, fundamental features, time deltas, and constants. The two classical policy"}, {"title": null, "content": "gradient algorithms: REINFORCE and PPO can be used as solutions to these MDPs where only the last step reward is non-zero. This kind of reinforcement learning algorithm requires a large number of interactions with these MDPs for formulaic alpha factor mining to optimize the prediction of the next token. Overall, we consider MDPs with only trajectory-level rewards and introduce their nature solutions.\nA. Formulaic Alpha Factors for Predicting Asset Prices\nConsider a real market with n assets over T trading days. On each trading day $t \\in \\{1,2,\\ldots ,T\\}$, each asset i is represented by a feature vector $x_{t}^{i} \\in \\mathbb{R}^{m \\times 7}$. This vector consists of m raw market features, such as open, high, low, close, and volume values, over the recent 7 days. Here, $x_{t, j}^{i} \\in \\mathbb{R}^{1}$ denotes the sequence of values for the j-th raw feature over these 7 days.\nNext, we define an alpha factor function f, which transforms the feature matrix for all assets on a given trading day into alpha values, represented as $X_{t} = [X_{t}^{1}, X_{t}^{2},..., X_{t}^{n}]^{T} \\in \\mathbb{R}^{n \\times m \\times r}$, into alpha factor values. Specifically, f maps $X_{t}$ to a vector $z_{t} = f(X_{t}) \\in \\mathbb{R}^{n}$, where $z_{t}$ holds the alpha factor values for each asset on trading day t. The real asset feature dataset $X = \\{X_{t}\\}$.\nTo measure the effectiveness of a formulaic alpha factor, we need a real asset price dataset $y = \\{y_{t}\\}$, where $t\\in \\{1,2,\\ldots,T\\}$ and $y_{t}$ is the 5-day asset return, where $y_{t} \\in \\mathbb{R}^{n}$. The newly generated formulaic alpha factor will be added to an alpha factors pool, and the combined factor value z will be calculated through a combination model, which will be discussed in detail in Section IV-A. The expression for the Pearson correlation coefficient between $y_{t}$ and the combination factor value $z$, also known as the Information Coefficient (IC), is as follows:\n$IC(z, y_{t}) = \\frac{Cov(z, y_{t})}{\\sigma_{z} \\sigma_{y_{t}}}$\nThe average IC values over all trading days are denoted as $\\overline{IC} = \\mathbb{E}_{t}[IC(z_{t}, y_{t})]$.\nFormulaic alpha factors are mathematical expressions that can be represented using RPN, which is a sequence of tokens. Tokens include various operators, the original volume-price features, fundamental features, time deltas, and constants. The operators include elementary functions that operate on single-day data, known as cross-sectional operators (e.g., Abs(x) for the absolute value x, Log(x) for the natural logarithm log(x)), as well as functions that operate on a series of daily data, known as time-series operators (e.g., Ref(x,t) for the expression x evaluated at t days before the current day). Such formulas can naturally be represented by an expression tree, with each non-leaf node representing an operator, and the children of a node representing the original volume-price features, fundamental features, time deltas, and constants being operated on. Each such expression has a unique post-order traversal, using RPN. Examples of some formulaic alpha expressions derived from Alpha101 [29] and their corresponding RPN representations are shown in Table II. An example of a formulaic alpha expression, together with its corresponding tree and RPN representation, is shown in Fig. 1."}, {"title": "B. MDPs for Mining Formulaic Alpha Factors", "content": "We can model the process of generating a linear sequence that can equivalently represent a formulaic alpha factor as an MDP, which can be described as $\\{S, A, P, r\\}$, where S and A denote the finite state and action spaces, respectively. P is the transition function defining state transitions, $r : S \\times A \\rightarrow \\mathbb{R}$ is the reward function assigning values to state-action pairs. Our goal is to obtain the policy $\\pi_{\\theta}$, where $\\theta$ is the training parameter. It generates tokens from a finite set of alternative tokens. Each state in the MDPs corresponds to a sequence of tokens denoting the currently generated part of the expression, denoted as $s_{t} = a_{1:t-1} = [a_{1}, a_{2},..., a_{t-1}]^{T}$, the sampling process is modeled as $a_{t} \\sim \\pi_{\\theta}(\\cdot | a_{1:t-1})$. The action $a_{t}$ is the next token following the currently generated part of the expression.\nThis process continues until the SEP token is encountered or the maximum length is reached. The initial state is always the BEG token, so a valid state always starts with BEG and is followed by previously chosen tokens. Obviously, any generated sequence cannot be guaranteed to be a legal RPN sequence, therefore we only allow specific actions to be taken in certain states to ensure the correct format of the RPN sequence. When $a_{1:t-1}$ and $a_{t}$ are already known, then $s_{t+1} = a_{1:t}$ is uniquely determined, which means that the state transition function P satisfies the Dirac distribution:\n$P(s_{t+1} | a_{1:t}) = \\begin{cases} 1 & \\text{if } s_{t+1} = a_{t} \\\\ 0 & \\text{otherwise.} \\end{cases}$\nThe optimization objective in these MDPs framework is to learn a policy $\\pi_{\\theta}$ that maximizes the expected cumulative reward over time:\n$J(\\theta) = \\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta}}[r(a_{1:T})]$\nNon-zero rewards are only received at the final step, which evaluates the quality of a complete formulaic factor expression, not individual tokens:\nr(s_{t}, a_{t}) = \\begin{cases} 0 & \\text{if } t \\neq T \\\\ r(a_{1:T}) & \\text{otherwise,} \\end{cases}\nwhere $r(a_{1:T}) = \\overline{IC}$. Expressions that are syntactically correct might still fail to evaluate due to the restrictions imposed by certain operators. For example, the logarithm operator token is not applicable to negative values. Such invalidity can not be directly detected. Therefore, these expressions are assigned a reward of -1 (the minimum value of IC) to discourage the policy from generating these expressions.\nC. Comparing REINFORCE with PPO\nThe natural solutions to these MDPs for formulaic alpha factors mining are the respected Proximal Policy Optimization"}, {"title": null, "content": "(PPO) algorithm [44], which uses the classic Policy Gradient:\n$g(\\theta) = \\nabla_{\\theta} \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} [R(\\tau)]$\n$= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} [\\nabla_{\\theta} log \\; p_{\\theta} (\\tau) R(\\tau)]$\n$= \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} [\\nabla_{\\theta} \\sum_{t=0}^{T} log \\; \\pi_{\\theta} (\\tau) R(\\tau)]$\nwhere $\\tau$ represents a trajectory, $p_{\\theta}(\\tau)$ represents the probability of trajectory $\\tau$ occurring under policy $\\pi_{\\theta}$, and R(T) represents the cumulative reward of trajectory $\\tau$. Each state in the MDPs for formulaic alpha factor mining processes corresponds to a sequence of tokens denoting the currently generated part of the expression. The initial state is always BEG, so a valid state always starts with BEG and is followed by previously chosen tokens, which means $s_{t} = a_{1:t-1}$, the current policy is $\\pi_{\\theta} (a_{t} | a_{1:t-1})$, the score function $s_{\\theta} (a_{1:t}) = \\nabla_{\\theta} log \\pi_{\\theta} (a_{t} | a_{1:t-1})$, and $p_{\\theta}(\\tau) = p_{\\theta}(a_{0}) \\prod_{t=0}^{T} \\pi_{\\theta} (a_{t} | a_{1:t-1})$. Since IC can only be calculated after the complete expression is generated, only the reward for the final step is non-zero. Therefore, $R(\\tau) = r(a_{1:T})$.The"}, {"title": null, "content": "Policy Gradient (5) can then be rewritten as:\n$g(\\theta) = \\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta}} [\\sum_{t=0}^{T} \\nabla_{\\theta} log \\; \\pi_{\\theta} (a_{t} | a_{1:t-1}) \\; r (a_{1:T})]$\n$= \\sum_{t=0}^{T} \\mathbb{E}_{a_{1:t}} [\\nabla_{\\theta} (a_{1:t}) \\; r (a_{1:T})]$\n$r (a_{1:t})$ is used as the metric of the effectiveness of the current policy. It can also be replaced by the following five metrics: $\\sum_{t=0}^{T} r_{t}$ (total reward of the trajectory) [45], $\\mathbb{E}_{t}[\\sum_{t'=t}^{T}r_{t'}]$ (reward following action $a_{t}$) [45], $\\sum_{t=t'}^{T}r_{t'}- b(s_{t})$ (baselined version of the previous formula) [45], $Q^{\\pi}(s_{t}, a_{t})$ (state-action value function) [46], $r_{t} + \\gamma V^{\\pi}(s_{t+1}) - V^{\\pi}(s_{t})$ (TD residual) [47], and $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) \u2013 V^{\\pi}(s_{t})$ (advantage function) [47]. Advantage function is used to measure the effectiveness of the policy in PPO, which implies that the TD algorithm [48] can be used to estimate the policy gradient. The advantage of using TD instead of Monte Carlo algorithms [49] to update network weights is that data from each step can be used for training, increasing sample efficiency. Additionally, to enable the reuse of the samples for learning, as in off-policy algorithms, PPO employs importance sampling:\n$L_{surr}(\\theta) = \\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta_{old}}} \\sum_{t=1}^{T} A^{\\pi}(a_{1:t}) \\frac{\\pi_{\\theta} (a_{1:t})}{\\pi_{\\theta_{old}} (a_{1:t})}$"}, {"title": null, "content": "where $\\text{ratio} (a_{1:t}) = \\pi_{\\theta} (a_{t} | a_{1:t-1}) / \\pi_{\\theta_{old}} (a_{t} | a_{1:t-1})$ is\nthe importance weight. Although $\\pi_{\\theta_{old}} (a_{t} | a_{1:t-1})$ can be any distribution, the expectation of two random variables being the same does not imply that their variances are also the same. When the difference between $\\pi_{\\theta} (a_{t} | a_{1:t-1})$ and $\\pi_{\\theta_{old}} (a_{t} | a_{1:t-1})$ is too large, it can introduce significant variance. Therefore, the PPO algorithm incorporates a clipping operation based on the policy gradient theorem and importance sampling to ensure that the difference between the two distributions is not too large, resulting in the final PPO loss function:\n$L_{surr}(\\theta) = \\mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta_{old}}} \\sum_{t=1}^{T} A^{\\pi}(a_{1:t}) \\min \\bigg( \\text{ratio}(a_{1:t}), clip(\\text{ratio}(a_{1:t}), 1 - \\delta, 1 + \\delta) \\bigg)$\nHowever, for these MDPs where non-zero rewards are only received at the final step, algorithms such as REINFORCE, as well as our proposed QFR algorithm, are more appropriately suited. We will delve into the details in Section IV-C. Before that, let us briefly introduce the respected REINFORCE algorithm. Based on the previously mentioned policy gradient (6), an unbiased estimate of this gradient can be obtained using the Monte Carlo method:\n$g(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} s_{\\theta} (a_{it}) r (a_{1:t}^{i}),$\nwhere $a_{t}^{i} \\sim \\pi_{\\theta} (. | a_{1:t-1})$ for all $t = 1,...,T$. Performing gradient ascent can update $\\theta$ to optimize the performance of the policy network: $\\theta_{k+1} \\leftarrow \\theta_{k} + \\eta_{k} \\hat{g} (\\theta_{k})$, where $\\eta_{k}$ represents the learning rate at the k-th iteration, which is essentially a likelihood maximization weighted by rewards using samples extracted from rollouts."}, {"title": "IV. BETTER RL IN FORMULAIC ALPHA FACTORS MINING", "content": "In this section, we introduce an innovative reinforcement learning algorithm for alpha factor mining, termed QuantFactor REINFORCE (QFR). We provide a theoretical analysis that establishes an upper bound on the variance of the QFR algorithm in MDPs modeling the formulaic alpha factor mining process. We demonstrate that QFR exhibits reduced variance compared to the celebrated REINFORCE algorithm. The proof that QFR, when applied to MDPs with trajectory-level rewards, exhibits the lowest variance compared to those with non-trajectory-level rewards, is also provided. Furthermore, we introduce IR as a reward shaping mechanism to promote the generation of steady alpha factors.\nA. The Proposed Algorithm\nBased on the MDPs defined in Section III-B for formulaic alpha factors mining, unlike the work in [14], we use Monte Carlo methods to obtain an estimate of the policy gradient (6) to optimize a policy $\\pi_{\\theta}(a | a_{1:t-1})$. Inspired by the REINFORCE algorithm with baseline [16], our QFR algorithm"}, {"title": null, "content": "also modifies the gradient estimate by introducing a subtractive baseline value to exhibit a smaller variance:\n$g(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} [s_{\\theta} (a_{it}) (r (a_{1:T}^{i}) - \\overline{r (a_{1:T}^{i})})],$\nThe baseline value $\\overline{r (a_{1:T}^{i})}$ can be obtained by greedily sampling a formulaic alpha factor and calculating the associated reward, where $\\overline{a}_{t} \\in \\underset{\\overline{a} \\in A}{\\text{argmax}} \\; \\pi_{\\theta}(\\cdot | \\overline{a}_{1:t-1})$. The proposed baseline serves a normalizing function by comparing the reward of the random response with that of the greedy response. It helps reduce the variance in the gradient estimate.\nWe will provide a detailed theoretical analysis in Section IV-C. Notably, while formally our baseline has the same form as in work [16], the mechanism by which this baseline works is quite different for the task in work [16] than it is for our MDPs here. To our best knowledge, we are the first to introduce this baseline in MDPs for formulaic alpha factors mining, and we provide novel and detailed theoretical proofs of why this baseline is particularly well suited to these MDPs in Section IV-C\nOur QFR algorithm builds on the computational efficiency of REINFORCE but significantly enhances its performance. Specifically, REINFORCE often faces large variance in stochastic gradients during training. To address these challenges, QFR employs a greedy baseline value that adapts to trajectory-level rewards. While PPO can also reduce large variance during training, it does so at the cost of introducing a value model that matches the scale of the policy model. This substantially increases the sampling time, as a forward pass must be performed for each state sampled from the buffer to"}, {"title": "C. The Theoretical Analysis", "content": "We provide a set of theoretical results for Algorithm 1", "mathbb{E}[\\hat{g}(\\theta)": "nabla_{\\theta"}, "mathbb{E}_{a_{1:T} \\sim \\pi_{\\theta}} [r (a_{1:T})"], "nProof": "We take the expectation over the randomness of responses ($a_{1:t"}, {"t}$)": "n$\\mathbb{E"}, ["g(\\theta)"], {"a_{1": "t"}, {"1": "T"}, {"a_{1": "T"}, {"a_{1": "t-1"}, {"1": "T"}, {"a_{1": "t-1"}, {"a_{1": "T"}, {"mathbb{E}_{a_{1": "t"}, {"a_{1": "T"}, {"E}_{a_{1": "t"}, {"a_{1": "t-1"}, {"a_{1": "T"}, {"mathbb{E}_{a_{1": "t"}, {"a_{1": "T"}, {"a_{1": "T"}, {"a_{1": "T"}, {"E}_{a_{1": "T"}, {"a_{1": "t-1"}, {"mathbb{E}_{a_{1": "T"}, {"a_{1": "t-1})]$, which is the sum of log probabilities, whose expected value (i.e., the average log probability) with respect to $\\theta$ should be equal to 0, because the probability distribution $\\pi_{\\theta}$ is fixed under its own expectation. The proof is thus completed.\nSince the baseline value introduced by the QFR algorithm is determined by the reward of the greedy policy, and corresponds to the reward distribution, it is statistically independent of the samples sampled by the normal policy, satisfying the requirements for the proof of unbiasedness.\nProposition 2. Given the deterministic transition function $T(s_{t+1}|s_{t}, a_{"}]