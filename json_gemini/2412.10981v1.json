{"title": "Hybrid Forecasting of Geopolitical Events", "authors": ["Daniel M. Benjamin", "Fred Morstatter", "Ali E. Abbas", "Andres Abeliuk", "Pavel Atanasov", "Stephen Bennett", "Andreas Beger", "Saurabh Birari", "David V. Budescu", "Michele Catasta", "Emilio Ferrara", "Lucas Haravitch", "Mark Himmelstein", "KSM Tozammel Hossain", "Yuzhong Huang", "Woojeong Jin", "Regina Joseph", "Jure Leskovec", "Akira Matsui", "Mehrnoosh Mirtaheri", "Xiang Ren", "Gleb Satyukov", "Rajiv Sethi", "Amandeep Singh", "Rok Sosic", "Mark Steyvers", "Pedro A Szekely", "Michael D. Ward", "Aram Galstyan"], "abstract": "Sound decision-making relies on accurate prediction for tangible outcomes ranging from military conflict to disease outbreaks. To improve crowdsourced forecasting accuracy, we developed SAGE, a hybrid forecasting system that combines human and machine generated forecasts. The system provides a platform where users can interact with machine models and thus anchor their judgments on an objective benchmark. The system also aggregates human and machine forecasts weighting both for propinquity and based on assessed skill while adjusting for overconfidence. We present results from the Hybrid Forecasting Competition (HFC) \u2013 larger than comparable forecasting tournaments - including 1085 users forecasting 398 real-world forecasting problems over eight months. Our main result is that the hybrid system generated more accurate forecasts compared to a human-only baseline which had no machine generated predictions. We found that skilled forecasters who had access to machine-generated forecasts outperformed those who only viewed historical data. We also demonstrated the inclusion of machine-generated forecasts in our aggregation algorithms improved performance, both in terms of accuracy and scalability. This suggests that hybrid forecasting systems, which potentially require fewer human resources, can be a viable approach for maintaining a competitive level of accuracy over a larger number of forecasting questions.", "sections": [{"title": "1 Introduction", "content": "From military conflicts to disease outbreak to economic disruption, accurate prediction is vital for sound intelligence-based decision-making. However, the problem of making accurate predictions for geopolitical events is notoriously difficult due to too much or too little data, rare event occurrences, or large levels of uncertainty. Prediction methods range from expert and/or group judgment to individual and ensembled statistical models [58]. It is often challenging to identify a consistent, superior prediction method [39, 58]. Stakeholders confidently misidentify the benefits of competing methods, such as trusting human clinical judgment over statistical or algorithmic judgment [14]. Two common forecasting methods, crowdsourcing and machine learning, have complementary strengths and competing weaknesses. Here we present a hybrid forecasting model a system that aims to exploit the proficiencies of each while circumventing their deficiencies.\nRecent forecasting tournaments such as IARPA's Aggregative Contingent Estimation (ACE) [1] have led to advances in crowdsourcing methods, statistical aggregation, and ultimately improvements in accuracy [5, 40]. Crowdsourced aggregation pools a breadth of knowledge while canceling independent errors [11] and is most successful when individual performance can be tracked over time. However, social influences can harm opinion pools, and individual (rewards) vs. group incentives can be difficult to balance. Individuals must choose to share their private information and trust others.\nAdvances in statistical and machine learning methods lead to accuracy gains due to their ability to handle troves of data with heterogeneous input and identify complex relationships [23]. Data-driven, algorithmic forecasting can be used to predict various political outcomes, such as terrorism, conflict, insurgency, and similar [21, 48, 53, 54]. However, machine learning requires large amounts of data to be available and accessible. If data is not in a standard format, there can be large costs to pre-processing data.\nStatistical models perform well under the right circumstances [36], and human crowds succeed when deftly combined [5]. Factors like amount, availability, and structure of data determine how these methods perform [55]. Machine-based forecasting methods typically perform well on problems for which there is sufficient historical data, but are ill-suited to forecast rare or idiosyncratic events for which such data may not exist, or when the underlying context has changed in ways not reflected by the historical data. Machine predictions handle data in a consistent, structured manner and avoid computational errors, like violating probabilitiy axioms [17].\nHuman analysts, on the other hand, can often accurately forecast outcomes without exclusively depending on availability of historical data, by leveraging their domain knowledge and prior experience. Further, human expertise and domain knowledge can be valuable as inputs into machine models. These benefits are most efficient when data is sparse and/or unstructured [17]. However, even the best analysts may not match machine performance where solid historical data is available and can be cognitively overwhelmed when addressing a large number of problems within time constraints, thereby limiting the scalability of a forecasting system that relies solely on human judgment. Unfortunately, there are few direct comparisons between models and crowds in similar settings.\nHere we describe our Synergistic Anticipation of Geopolitical Events (SAGE) system, which was developed under IARPA's Hybrid Forecasting Competition (HFC) program [2]. The system is designed to make verifiable probabilistic predictions of outcomes from a broad set of domains, such as politics and international relations (ie the quantity of battle deaths or piracy in a region or attributable to a specified actor), health and disease (ie flu or dengue fever case counts), economics and finance (ie exchange rates or oil prices), and science and nature (ie the number of earthquakes or cybersecurity breaches) (see section 4.1 for an overview of the types of questions). A human-computer system can achieve \u201chybrid intelligence\u201d when applied in a setting with a high degree of digitization and human expertise [51]. SAGE is a hybrid forecasting platform that allows human forecasters to combine model-based forecasts with their own judgment. The SAGE system provides forecasters automated statistical predictions and freedom to choose if and how much weight to assign to model predictions when submitting their personal forecasts since formal models can increase the skill of human judges [51].\nOur system is designed to test the conceptual hypothesis that machine model forecasts embedded in a crowdsourced forecasting platform can improve the accuracy and efficiency of established crowdsourced forecasting methods. We embed machine models in a system designed to balance a) the diversity required to achieve the \"wisdom of the crowd\" by not restricting users' responses with b) anchoring forecasters"}, {"title": "2 Related Works", "content": "The main motivation behind developing a \"hybrid\" forecasting system is to harness the strengths of crowd-sourced and statistical forecasts by combining them with machine learning models as input for both human forecasters and aggregation methods. This type of hybrid intelligence occurs when human and machine components each contribute to a solution that outperforms and/or is more efficient than either source on its own [17, 35]. Machine models, which excel at identifying patterns from data and leveraging them for making predictions, can help human judges overcome certain errors and inconsistencies. Human experts, which do not require structured input data, are capable of ad hoc feature selection, often quicker than variables can be formalized when data sources are yet unavailable.\nWhile there is a growing field discussing the current state of hybrid intelligence, there is limited work exploring how such systems work and in what settings they excel. To date, most work explicitly discussing hybrid intelligence is theoretical (e.g. [16, 51, 52]). Developing an efficient and effective hybrid system to solve complex, dynamic tasks requires a carefully designed and tested machine component, a skilled human component, and principled, dynamic methods for combining them. In the current study, we address the challenges of balancing effectiveness with flexibility. Artificial intelligence exceeds when tasks are well-defined (e.g. [17]). Machine models can underperform when tasks are loosely defined, data is sparse, or environments are complex and/or changing. A forecasting tournament provides an opportunity to collect data in a structured, yet chaotic, environment. On-one-hand, the general question and response format is consistent and practiced users provide consistent response data. On-the-other-hand, it is a difficult setting to generalize because new question types, sources, and datasets could be introduced after system development.\nOne key limitation of the previous work on crowdsourced forecasting and hybrid intelligence is that use cases are limited and often applied to a business environment (e.g. [17, 16, 50]). The current study is designed to provide data-driven support for the effectiveness of a hybrid system in the geopolitical forecasting domain. While there are established methods showing how crowdsourced forecasting succeeds, there are not established methods for a hybrid forecasting system [40]. Previous crowdsourced methods rely on adjustments, such as statistical recalibration, to adjust for measurable biases in human forecasters like overconfidence [5]. It remains an open question whether the same or new cognitive biases emerge when human users interact with machine model output. Research suggests that presenting time-series data as a forecasting aid improves individual forecasts by reducing random error [15]. When there are detectable trends in a time series, forecasts made while viewing graphical data are more accurate than from viewing tabular data [24]. In this setting, a hybrid system must account for potential, yet unmeasured, biases to effectively combine machine models and crowd predictions.\nThe success of machine models is driven by complexity and volatilitiy. When predicting on real data, machine learning models face a tradeoff between the complexity of the data and the number of model parameters required to predict accurately [47]. Hence, tuning and re-turning becomes cumbersome when properties of the event or dataset change. The problem becomes more challenging when predicting several periods into the future and requires methods that produce multiple outcomes [8]. Known (simple) statistical methods often outperform more sophisticated methods (e.g. based on deep neural networks) because real-world time-series are often non-stationary. Changes from training to testing often impede how well sophisticated models"}, {"title": "3 Methods", "content": "3.1 SAGE System\nThe Synergistic Anticipation of Geopolitical Events (SAGE) system was developed to combine automated statistical forecasts with a pool of human knowledge by allowing users access to machine model output and by algorithmically combining human and machine forecasts [44]. The SAGE platform allowed users to interact with machine models to anchor their judgments on an objective benchmark. Simultaneously, users had the freedom to choose if and how they combined model forecasts with their own judgment striving for the diversity of knowledge needed for the \"wisdom of the crowd\" effect [3]. To proactively mitigate skepticism with and over-reliance on the models, we trained users in how to evaluate and consolidate information from multiple sources.\n3.2 Forecasting Platform\nThe SAGE system included search and filtering functionality to help users find forecasting questions (IFPs) about which they felt knowledgeable. At any point in time, there were dozens of IFPs available. Users had to complete at least 5 forecasts per week; After choosing which question to answer, an IFP page included the following from top to bottom: question text, resolution criteria (including the source used to resolve a given question, value of interest and/or criteria for an occurrence, and timing), automated information including data graph, statistical forecast, and interactive features (depending on their experimental condition), forecast sliders that forced responses to add to 100%, a textbox to justify the forecast, and a comments thread to view and respond to fellow users' justifications. Additional features included a leaderboard, consensus charts, a research tool, a profile page including their personal accomplishments, and training and tournament information.\n3.3 Data Pipeline and Model Development\nThe SAGE machine model pipeline can be broken down into two parts based on the kinds of questions that were covered. Approximately 45% of IFPs (AKA data-driven IFPs) were clearly associated with a univariate time series, like OECD interest rates for a country [22]. These questions were covered by automated data-acquisition and univariate time-series forecasting systems. The remaining questions did not have clearly associated data. Some, about election results and country leader resignations, were covered by tailored models that could leverage more complicated, non-time-series data. Others were covered by tools that leveraged resolved answers to other, similar, previous questions, or extracted relevant information from the ICEWS event data [9]. The non-time series models either only covered a very small set of questions, or did not perform well in terms of accuracy, so the rest of this section will focus on the time series forecasting system.\nThe time series forecasting system consisted of a data platform that maintained a continuously updated database of relevant time series data sets and could map them to questions as appropriate, and a forecasting platform that would then parse a question and apply a univariate time series model to derive probabilities for the question answers. Our system was developed to automate data extraction based on reading the question text and finding the applicable data. Many data sources were known in advance and several were not."}, {"title": "3.4 Experimental Conditions", "content": "A significant challenge was to identify the time series models to use for generating the forecasts that would be shown to users and sent to the aggregation models. Four core models were displayed to users in the question charts: the auto ARIMA model, a similar automated exponential smoothing model (ETS) [30, 31], a simple random walk model, and the M4-Metalearning model [42]. The DCT Ensemble model, drawing on forecasts from auto ARIMA, M4-Metalearning, or a AR(1) neural net model based on an analysis of the input series discrete cosine transformation was used to provision forecasts for aggregation. Model performance suffered from a \"cold-start problem\" as the number of IFP resolutions were limited over the first several months of the competition. Further, HFC guidelines required our system to make predictions for new datasets and sources on the fly, often with only a couple hours notice before users could access the IFPs. Therefore, simple time-series models tended to outperform more complex, topic-specific models a result supported by the general success of conservative forecasting approaches [4, 38]. Initially all forecasts were based on the Auto ARIMA model [30], but later this was supplanted by an ensemble (labelled \"PHE2\" below) of Auto ARIMA and an exponential smoothing state space model [30], which emerged from an overall pool of 28 candidate models. We do not report results from poor performing models. Choosing adequate models was hard because inter-question performance a is very noisy (variable), yet only relatively small numbers of resolved questions were available for testing and several models did not have adequate information to specify them consistently. Only later did enough resolved questions accumulate for model-to-model performance to stabilize.\nWe conducted a controlled experiment to better understand the benefits of exposing forecasters to different hybridization components. We randomly assigned our 547 participants to one of three experimental conditions which we labeled B, C, D to reflect the increasing level of complexity of and interactivity with the hybridization model.\n1. Condition B: This condition exposed users to historical data about the target item. Data included relevant news articles from the research tool, and historical figures that pertains to the question. Historical charts were available for 177 of the 398 items.\n2. Condition C: This condition supplemented the data charts from Condition B with machine model predictions, when available. More specifically, we exposed the forecasters to predictions from the ARIMA model, which has been determined to be a good general model. ARIMA model predictions were available for 177 of the 398 items.\n3. Condition D: This is a variation on condition C that allows the forecasters to tweak the parameters of the visualization, including the type of model and range of data used for model training. We also provided a simple method that allowed the judges to adjust the model's forecast by selecting the mean and variance of the target value and directly translating that into a forecast.\nThe control condition did not offer any historical charts nor machine predictions to the participants. The main objective of the program was shown that the hybridized conditions could generate more accurate aggregate forecasts than the control."}, {"title": "3.5 Forecast Aggregation", "content": "By combining human and machine model forecasts, we aimed to leverage the collective intelligence of human and model judgments. The advantage of human judgment was its flexibility and ability to reason with qualitative and mixed-source data. Humans can forecast when data is sparse or difficult to interpret and can seek out information that only indirectly relates to the question at hand. On the other hand, the advantages of models included expeditious forecasting which improved scalability. Statistical models dutifully forecasted on any number of questions and their accuracy tends to improve as more data became available.\nThe key challenge for aggregation was that many factors related to human and model judgment were not known a priori. For each particular forecasting problem, the total number of human forecasts was not knowable in advance. On any particular day the forecasting problem was available, a handful of human forecasts might be produced but in some extreme cases, no human judgment might be available for the entire duration of the forecasting problem. In addition, at the start of the forecasting project, it was not known what the relative accuracy is of the model and human judgments for certain types of forecasting problems. Every introduction of a new type of forecasting problem injected new uncertainty about the relative capabilities of human and model forecasting accuracy. This cold-start problem made it challenging to apply machine-learning approaches that can learn optimal combinations of human and model judgment as large quantities of human judgments were initially not available and yet accurate forecasts needed to be produced from the start of the project. Therefore, the goal for aggregation was to develop a robust framework for integrating human and model judgment with the potential to scale to large numbers of forecasting questions.\nTo combine the human forecasts, we employed a combination of tested methods and new strategies to maximize performance. The aggregation of human-only forecasts accounted for three factors: recency, individual skill, and miscalibration. First, our algorithms diminished each forecasts' value over time as new information accumulates. To account for this recency effect, we kept only the most recent 40% of forecasts for a question at any given time, and further applied exponential decay to down-weight older forecasts included in the aggregation. Second, we placed higher weights on forecasters forecasters with better accuracy track records, those who updated their forecasts in frequent, small increments [6], and those who wrote longer text rationales, with more sources and quantitative information. Finally, we recalibrated forecasts to correct for the general tendency toward overconfidence by individual forecasters, and underconfidence of aggregated crowd judgments, especially when aggregated using the mean. This was done by making forecasts by individual forecasts less extreme, but aggregate-level forecasts more extreme (closer to 0% or 100%). The overall effect was to make final aggregate estimates slightly more extreme than the equivalent estimates with no recalibration. The best-performing slot used a variant of this aggregation model which made a) forecaster weights more unequal over time, and b) extremization parameters larger over time, making season-end aggregated forecasts more extreme than those at season-start.\nHuman forecasts were then combined with machine model estimates. Each model forecast was also given weights based on the historical performance of the model that generated it. Our initial strategy for human-machine aggregation was to assess machine weights relative to those of crowd estimates (e.g., a model estimate may be weighted 1/4 as much as the crowd). The more advanced alternative that was used in most slots in the last season (including the best-performing slot), placed weights on model forecasts equal to those of several"}, {"title": "3.6 Training", "content": "We sought to understand whether training could improve predictive accuracy under conditions in which forecasters had to balance trust in a model with their own judgment. Our HABIT training method combined probabilistic reasoning with hybridization concepts using a character-based narrative device rendered in a cartoon format. Our training was designed to extend previous vignette-based methods, which focused on core tenets of probabilistic estimation [40], to teach about the machine models involved in the hybridized ensembles and aggregations and how to integrate model forecasts with one's personal knowledge. We hypothesized that the cognitive burden of whether to integrate or reject machine model data could be mitigated by briefly explaining how each model works and how to balance too little and too much trust in models. We tested both whether or not mandating training improved accuracy and whether the presentation format, whether animated or static, led to gains."}, {"title": "3.7 Matching Participants with Forecasting Problems", "content": "The SAGE system aimed to optimize two seemingly conflicting objectives: 1) allow users choice of questions based on their expertise and interests with 2) timely coverage of all questions with limited human forecasters. We developed an IFP Recommender System which presented a personalized ranking of the IFPs on the Question page, based on the specific characteristics of a forecaster. We develop a recommender system based on the wide and deep learning model [13]. This model identifies preferences using known IFP features, and generalizes to other IFPs via IFP embeddings. As features, we took into account the performance of the given forecaster on similar past IFPs and the user activity on the other IFPs . When designing the SAGE recommender system, BERT proved to be the more accurate, most efficient model because it captured the subtleties in the differences between IFP texts. To gauge the similarity among IFP texts, we used cosine similarity, a common distance metric used in embedding spaces. We balanced individual with system performance by also capping popular IFPs where consensus was already reached, freeing up human resources to forecast on other IFPs."}, {"title": "3.8 Data Availability", "content": "SAGE platform data including machine model output and experimental user forecasts, activity, and scores can be found on the Harvard Dataverse [43]. Control user forecasts, question metadata, and resolutions can be found on a separate Harvard Dataverse page [25]."}, {"title": "4 HFC Background and Rules", "content": "IARPA's Hybrid Forecasting Competition (HFC) [2] was a multi-year research program developed to test if and how machine-models could improve upon previous crowd-sourced geopolitical forecasting tournaments such as ACE [1]. As stated in the program announcement, \u201cthe goal of HFC was to integrate the strengths of human cognitive and reasoning abilities with those of machine-driven systems to produce maximally accurate forecasts of geopolitical and economic events"}, {"title": "4.1 Individual Forecasting Problems", "content": "During RCT-B, forecasts were conducted on 398 questions, broadly referred to as Individual Forecasting Problem (IFP). The questions covered a broad set of domains, such as politics and international relation, science, health and disease, microeconomics and finance New IFPs were published on the same day each week. Each IFP was associated with C mutually exclusive and exhaustive outcome events, where 2 < C < 5. Participants submitted their forecasts for a given IFP by entering a probability for each outcome, where the probabilities across all C outcomes were required to total 100%. All IFPs had a start date and an end date during which participants could make forecasts for that questions as often as they liked (see Figure 2 for a screen capture). IFPs ranged from 2 weeks, to the full 8-month season in duration. IFPs had a mean duration of 87.07 days (SD = 55.85). 205 IFPs had only two response options, the remaining 193 had more than 2 possible responses. Of these 193, 154 were ordinal, in that there was a meaningful ordering to the C events, while the remaining 39 were nominal."}, {"title": "4.2 Participants", "content": "Human participants were recruited via Amazon Mechanical Turk. CloudResearch filtered the participants to ensure a high level of engagement both prior to the start of the forecasting season by only including users with longitudinal study experience, and mid-season by removing users with low quality responses by assessing the content of their justifications [45]. The sample consisted of 547 participants, 229 women (42%), with a mean age of 36.68 (SD = 10.88). A forecasting session consisted of weekly Human Intelligence Tasks (HITs), where each forecaster was required to make at least five forecasts. If possible, three of these five were required to be updates of previous forecasts. For each completed HIT, Participants were paid $20 per HIT. Participants were permitted to make additional forecasts beyond these five but were not paid for these additional forecasts. Participants were also eligible for accuracy awards if they participated enough. They could earn a portion of a fixed prize pool at the midway and final points. The pool was divided among three prize tiers of $200, $100, or $50 for observed accuracy as measured with mean daily Brier scores."}, {"title": "4.3 Machine Models", "content": "A key component of the HFC was the requirement for systems to produce model-based forecasts. In previous comparisons of human and model forecasts, the latter were generated in a traditional fashion by analysts (e.g. [56]). In contrast, model-based forecasts for the HFC competition had to be generated by an automated system with restrictions on manual interventions into the process. Fixing system issues, i.e. bugs and similar errors, was allowed, but manual model development like deciding what data and model(s) to use for a question, tuning model parameters, etc. was not permitted. Some data sources were introduced mid-season. Sometimes notice about new data sources came only a couple hours prior to the associated IFPs getting published for human responses. Thus, performer teams were required to quickly produce model forecasts to aid users, and there was insufficient time to build specialized models tuned to specific datasets."}, {"title": "4.4 Response Submissions", "content": "Each team was allotted 40 official and up to an additional 60 experimental slots for submitting forecasts on each IFP. These slots allowed teams to test multiple theoretical ideas as well as fine-tuning the application of"}, {"title": "4.5 Scoring", "content": "The accuracy of submitted forecasts were measured using Brier scores [10], the squared distance of the forecast from the result, coded as 1 if the event/quantity was realized, and 0 otherwise. We use Brier scores to measure accuracy because they are specifically designed to assess the accuracy of probabilistic information (unlike other metrics, like F1). As a variation of squared-error, Brier scores penalize more egregious errors more severely. In addition to scoring accuracy, a Brier score is also a proper scoring rule meaning it incentivizes responding honestly. Practically, the HFC test and evaluation team chose Brier scores as their primary accuracy metric. Using the same metric allowed us to efficiently track our performance compared to control and the other HFC competitors. Briers scores can be interpreted similarly as mean squared error. A minimal baseline for accuracy is to show improvement over an uninformed judge, who assigns equal probabilities to all C bins (prob = 1/C), earns a Brier score of (C-1)/C. Brier scores are also commonly described as improvement over a known comparator. Below we compare to control using Cohen's d, a standardized mean difference, and in some instances display the percent improvement.\nWe used formulations of the Brier score based on the number of response options and ordinality of the IFP [41]. This Brier score variant ranged from 0 (perfect accuracy) to 2 (worst possible score). The accuracy of each forecasting slot for a given IFP was characterized by the Mean Daily Brier score (MDB), e.g., the Brier score averaged over the active days of that IFP. Usually, the SAGE system submitted daily forecasts for each open IFP. If for whatever reason a forecast was not submitted on any given day (e.g., system outage), the last submitted forecast was carried forward. If a slot did not submit any forecasts at all for a given IFP, a uniform prior was used to calculate the score.\nA similar approach was used to score individual forecasters. A forecast for a given user was carried forward until that user chose to revise the forecast. If a forecaster did not place an estimate on the first day of a question, we imputed the median score across all forecasters in a condition for each day an IFP was open prior to the first forecast. A user's score across IFPs was the mean of MDBs or MMDB. To adjust for the difficulty of individual IFPs and aid in interpreting comparisons across conditions, we standardized Brier scores to have a mean of zero and standard deviation of 1 for each IFP-day."}, {"title": "5 Results", "content": "5.1 Aggregate Performance\nFirst, we report our main result that compares the aggregate performance of the SAGE system with the non-hybrid control. As we mentioned above, each method was allocated 40 official and 60 experimental slots for submitting aggregated forecasts. The best official SAGE method led to an improvement in mean accuracy of a Cohen's d of 0.126 over control.\nSAGE's best-perfoming aggregation slot had the following properties. First, it used both Control and SAGE human forecaster data as inputs. Second, it applied a time-varying weighted mean human aggregation"}, {"title": "5.2 Individual Performance Across Conditions", "content": "We analyzed user performance in the various conditions across the 398 resolved questions. \nSince some questions were relatively easy and highly predictable and others were more difficult, we expected them to yield (possibly, very) different Brier scores. Thus, whenever comparing, or aggregating, Brier scores across multiple items, it was important to adjust for inherent imbalance in difficulty. Our approach to this problem was to standardize the Brier scores for every question to have a mean of 0 and a SD of 1, across all the responses in all conditions, before combining them. Thus, we report results in terms of mean, median, and 25th percentile standardized Brier scores. The lower (and more negative) a score is, the more accurate it is. The results of all conditions are presented.\nOur results indicated that users in conditions C and D outperform those in condition B, but only the most skilled forecasters outperformed the control (no data) condition. We confirmed the hypothesis that having access to model predictions indeed helps skilled, but not average, forecasters. The greatest improvement came when data charts were available, and skilled forecasters viewed model predictions, z-Brier = -0.618 vs. -0.545 for control. Note that the availability of more models and interactive features, provided in condition D, did not necessarily help with performance. Indeed, while condition D had a better mean score across all"}, {"title": "5.3 Model-Based Forecasts", "content": "We also analyzed the performance of the machine models outlined in Section 3.3. Overall, simple ensemble models worked well. The best performing model (\u201cPHE2\") was an ensemble that averaged the forecasts from Auto ARIMA and an exponential smoothing state-space model (ETS) [30]. It slightly outperformed the M4-Meta model [42] that ranked 2nd highest in the M4 time series forecasting competition, and clearly outperformed more complex methods like a recurrent neural network and custom-coded regularized auto-regressive model. Even the Auto ARIMA model itself did reasonably well throughout. From a practical standpoint, the simpler ensembles were computationally less expensive, had fewer software dependencies, and were less likely to break.\nModel forecasts relative to the human forecasts were overall near or at parity with human forecasts. Both models outperformed the average human forecast but lagged slightly behind the best aggregation model of human-only forecasts. In part, this is because they had a small number of very bad forecasts. Some of these were caused by IFPs with known data quality issues, which tended to lead to extreme forecasts with either very low or very high Brier scores."}, {"title": "5.4 Using Machine Models for Scalable Forecasting", "content": "We analyzed aggregate performance on the IFPs for which our strongest machine model was available, a discrete-cosine transform (DCT) ensemble. We found incorporating machine models during aggregation led to improvements at several stages which accounted for our team's overall advantage. Although these accuracy gains were consistent throughout the competition, effects at individual stages were modest and none were statistically significant on their own. The results showed that providing forecasters access to model projections led to modest improvements in aggregate accuracy. Forecasters who could view model forecasts before making their estimates produced aggregate forecasts with 6% better Brier scores, compared to aggregations of forecasters with no access to model projections. Injecting model estimates at the aggregation stage also led to small improvements in accuracy (i.e., reductions in Brier score) of 2%-3% points."}, {"title": "5.5 Impact of Training", "content": "Participants were randomly assigned to either a brief (about 30 minute) training or a control condition in which they read popular articles about forecasting, but without tips for boosting accuracy. Training occurred once, during their second active week, and was accessible for review on the platform menu. We assessed accuracy and activity to see if trained forecasters worked harder than untrained users. We first compared forecast accuracy before and after training exposure to ensure trained forecasters were not randomly better from the start. We found trained forecasters outperformed control forecasters post-exposure (d = 0.56). We further assessed whether accuracy could be improved via the delivery method of the training material. Forecasters who saw animated material significantly outperformed forecasters who saw static material in average accuracy , generated slightly more forecasts per IFP than the static group, and attempted approximately 5% more IFPs than static-trained counterparts, a significant difference . More details can be found in [34]."}, {"title": "5.6 IFP Recommendation", "content": "We measured the performance benefit of users assigned to questions ordered using the IFP Recommender System, described in Section 3.7, vs. the global ranking based on resolution date and popularity (SWIFT ordering). As shown in Figure 6, we obtained a statistically significant improvement, up to a 7% relative decrease in Brier score by the end of the experimental phase.\nThe IFP Recommender System learns over time the skills and preferences of each forecaster. After 2 months into the experiment, the forecasters who received the recommendations start to consistently outperform the forecasters in the control condition (SWIFT), with a relative improvement that stabilizes around 7%.\nWe further assessed the IFP Recommender System in terms of effective resource allocation by simulating different allocation strategies. First, since we could not foresee who the best performers would be before the phase ends, we implemented a greedy approach to improve our cohort of users by periodically excluding a certain percentage of worst performers every time a batch of IFP closed (i.e. got resolved) (termed GreedyIFP). Second, we additionally capped the number of forecasts on popular IFPs to reallocate forecasts"}, {"title": "6 Discussion", "content": "In the above, we show how a hybrid forecasting system can outperform established crowd-sourced forecasting systems. The SAGE hybrid system consistently outperforms the human-only control condition. Our hybrid system improves the accuracy in the aggregate, although improvements were modest. Beyond the proven methods for aggregating human forecasts [5", "3": "."}]}