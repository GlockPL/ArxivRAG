{"title": "MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for Fully-Utilized In-Memory Computing Architectures", "authors": ["Do Yeong Kang", "Yeong Hwan Oh", "Chanwook Hwang", "Jinhee Kim", "Kang Eun Jeon", "Jong Hwan Ko"], "abstract": "The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between high-dimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This paper presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD introduces a clustering-based initialization method and quantization-aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69% higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency.", "sections": [{"title": "I. INTRODUCTION", "content": "HDC is an emerging computational paradigm inspired by the brain's distributed processing and the properties of high-dimensional vector spaces [1]. By representing data as high-dimensional vectors, known as hypervectors, HDC enables efficient and robust computation suitable for various machine learning tasks. The high dimensionality provides inherent robustness to noise and facilitates operations such as binding and superposition, which are analogous to logical operations in traditional computing. Applications of HDC span a wide range, including language processing [2], robotics [3], and biosignal classification [4].\nIMC has emerged as a promising approach to overcome the von Neumann bottleneck by performing computations directly within memory arrays [5]. By integrating processing capabilities into memory units, IMC architectures dramatically reduce data movement, leading to enhanced computational speed and energy efficiency for data-intensive applications. Recent advancements have demonstrated the potential of IMC in accelerating machine learning algorithms and neural networks, utilizing various memory technologies such as SRAM, DRAM, and non-volatile memories like RRAM and PCM [6], [7].\nNevertheless, mapping vectors onto IMC arrays presents significant challenges due to dimensional mismatches, because HDC utilizes high-dimensional vectors to achieve noise resilience and effective pattern separation. This often results in underutilization of computational resources, as vector dimensions frequently exceed row dimensions of individual IMC arrays in Fig. 1-(a). To address this, partitioning has been proposed in [9], dividing hypervectors into smaller segments mapped across previously unused columns in fewer arrays. While this helps fit computations within hardware constraints, it doesn't reduce the total number of computation cycles required for inference, as shown in Fig. 1-(b). The root cause of these issues lies in the fundamental mismatch between HDC's requirements and IMC array structures. On the one hand, HDC relies on high-dimensional vectors that often exceed the row"}, {"title": "II. BACKGROUND: HDC CLASSIFICATION", "content": "The HDC framework is structured around two primary modules: the encoding module (EM) and the associative memory (AM). During training, the EM processes the input data to extract relevant features and converts them into hypervectors. The encoded hypervectors are used to create class vectors, each representing a specific class, which are then stored in the AM for future reference. In runtime, new data is encoded into a hypervector using the same method, and an associative search is performed to compare this query hypervector against the class vectors in the AM using similarity measures."}, {"title": "B. Hypervector Encoding", "content": "Random Projection encoding involves Matrix-Vector-Multiplication (MVM) between the $f \\times D$ projection matrix M and the $f$-dimensional input feature vector F. Each column of M consists of a randomly generated base vector B, which can be represented as either binary [11] or floating-point data type [12]. The encoding is performed as follows:\n$H = M^T F$."}, {"title": "C. Associative Memory Training", "content": "In HDC, it is possible to learn class vectors in a single pass. This is achieved by constructing the class vector through the summation of all sample hypervectors associated with the same class, as follows: $C_k = \\sum_\\nu H$, where $C_k$ is the class vector for class k, and H are the sample hypervectors with label k.\nIterative learning updates the AM by focusing exclusively on the hypervectors that were misclassified across the entire training dataset. For each mispredicted hypervector H, the true class vector is adjusted to be more similar to H, while the predicted class vector is adjusted to be less similar. This approach enables the model to improve its predictions over several iterations, and can be summarized as follows:\n$C_{true} = C_{true} + \\alpha H, C_{pred} = C_{pred} - \\alpha H$\nwhere $\\alpha$ is the learning rate. For the binary HDC model, the resulting C is binarized to produce a binary class hypervector."}, {"title": "D. Associative Search", "content": "For classification tasks, associative search involves selecting a predicted class based on the similarity between a query vector and class vectors. The class with the highest similarity to the query vector is inferred as the predicted class. A primary similarity metric used in this context is dot similarity:\n$d_{dot}(A, B) = A \\cdot B$\nwhere A and B are vectors of the same dimension. While various similarity measures such as Hamming distance and cosine similarity exist for associative search, dot similarity is a simple yet effective measure of vector similarity. It aligns well with the multiplication and accumulation operations inherent in IMC arrays."}, {"title": "\u0399\u0399\u0399. \u039c\u0395\u039cHD: MEMORY-EFFICIENT MULTI-CENTROID HD", "content": "In this section, we introduce the overall method for our multi-centroid AM. To address the time-consuming associative search in existing partitioning methods, we employ an IMC array-optimized multi-centroid AM structure. This approach enables one-shot (or few-shot) associative search and utilizes hypervectors with dimensions tailored to the IMC array size, significantly reducing encoding costs. Fig. 2 illustrates the overall framework of MEMHD. Our multi-centroid AM is constructed using the clustering-based initialization (shown in Fig. 2-(a), referred to in III-A), which enhances accuracy and convergence. For the binary AM, we perform 1-bit quantization of the AM (shown in Fig. 2-(b), referred to in III-B) and subsequently refine the model through quantization-aware iterative learning (shown in Fig. 2-(c), referred to in III-C), optimized"}, {"title": "A. Multi-Centroid AM Initialization", "content": "In traditional HDC approaches, each class is represented by a single vector. The initialization method for these class vectors, including random sampling, has minimal impact on their ability to effectively represent the overall set of sample hypervectors for that class. Random sampling is commonly used for initialization due to the method's robustness to initial conditions. During the subsequent iterative learning process, all updates corresponding to a single class are applied to one class vector, allowing it to converge to a representative vector for the class with minimal dependence on its initial state.\nIn contrast, multi-centroid models use multiple class vectors per class, each capturing distinct features of that class and learning independently. This makes the initialization method critical for ensuring comprehensive class representation. Randomly-sampled initial class vectors fail to distribute the centroids evenly across point cloud of the class sample hypervectors, leading to poor representation. To address this, we propose the clustering-based initialization method that improves convergence and accuracy.\n1) Classwise Clustering: First of all, as shown in Fig. 2-(a), MEMHD splits the encoded sample hypervectors by class and applies clustering for each class. We use K-means algorithm for clustering, which allows us to specify the number of clusters directly. The distance metric used in clustering is based on dot similarity, the same metric employed in associative search. This consistency ensures that the clustering process is optimized for subsequent associative search operations. We define a hyperparameter R as the proportion of columns used"}, {"title": "B. AM Quantization", "content": "To optimize memory efficiency for mapping AM into IMC array, we represent the AM in binary format. The initial AM, initialized with floating-point (FP) values after clustering, exhibits a distribution resembling a Gaussian curve, as illustrated in Fig. 2-(b). MEMHD performs 1-bit quantization using the mean value as a threshold, binarizing the AM by setting values greater than the mean \u00b5 to 1 and the rest to 0."}, {"title": "C. Quantization-Aware Iterative Learning", "content": "The binary AM is trained through quantization-aware iterative learning, which optimizes the model while accounting for quantization effects. While [13] introduced quantization-aware learning for basic HDC, our multi-centroid model extends this approach with crucial modifications. Specifically, our method"}, {"title": "D. In-Memory Inference", "content": "For in-memory inference, both the binary projection matrix as EM and the binary AM are mapped into the IMC array. The inference process utilizes the MVM-based encoding and associative search methods mentioned in Eq. (3): $pred = arg \\max_{i, j} dot(C_{i,j}, H_b)$."}, {"title": "IV. EVALUATION", "content": ""}, {"title": "V. CONCLUSION", "content": "This paper introduced MEMHD, a Memory-Efficient Multi-centroid HDC framework that optimizes HDC for IMC architectures. MEMHD effectively trains multi-centroid AM through clustering-based initialization and quantization-aware iterative learning. As a result, this approach significantly outperforms binary HDC baselines, achieving up to 13.69% higher accuracy with the same memory usage or 13.25\u00d7 more memory efficiency at the same accuracy level. By enabling full utilization of IMC arrays and one-shot (or few-shot) associative search, MEMHD reduces computation cycles by up to 80\u00d7 and array usage by up to 71\u00d7 compared to the partitioning method on 128\u00d7128 IMC arrays, substantially improving energy efficiency and cycles. These advancements pave the way for more efficient implementation of HDC in resource-constrained environments."}]}