{"title": "TOMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking Theory of Mind", "authors": ["Kazutoshi Shinoda", "Nobukatsu Hojo", "Kyosuke Nishida", "Saki Mizuno", "Keita Suzuki", "Ryo Masumura", "Hiroaki Sugiyama", "Kuniko Saito"], "abstract": "Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in three aspects: 1) they assess a limited range of mental states such as beliefs, 2) false beliefs are not comprehensively explored, and 3) the diverse personality traits of characters are overlooked. To address these challenges, we introduce TOMATO, a new ToM benchmark formulated as multiple-choice QA over conversations. TOMATO is generated via LLM-LLM conversations featuring information asymmetry. By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories: belief, intention, desire, emotion, and knowledge. These verbalized thoughts serve as answers to questions designed to assess the mental states of characters within conversations. Furthermore, the information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states. Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts. TOMATO consists of 5.4k questions, 753 conversations, and 15 personality trait patterns. Our analysis shows that this dataset construction approach frequently generates false beliefs due to the information asymmetry between role-playing LLMs, and effectively reflects diverse personalities. We evaluate nine LLMS on TOMATO and find that even GPT-40 mini lags behind human performance, especially in understanding false beliefs, and lacks robustness to various personality traits.", "sections": [{"title": "1 Introduction", "content": "Theory of Mind (ToM) is the cognitive ability to infer unobservable mental states such as beliefs, intentions, and desires of others (Premack and Woodruff 1978). The ToM reasoning capability is thought to be the cornerstone of human social intelligence (Fan et al. 2022), and indispensable to interact with others (Baron-Cohen, Leslie, and Frith 1985).\nTo investigate whether large language models (LLMs) possess human-like ToM, researchers have used various benchmarks. However, existing ToM benchmarks are not aligned well with real-world scenarios in the following three aspects. (1) Despite various categories of mental states that can be inferred by ToM as studied in psychology (Beaudoin et al. 2020), only limited types of mental states such as beliefs have been assessed (Ma et al. 2023), especially for second-order ToM. (2) False beliefs about beliefs or world states have been the main focus of previous studies (Le, Boureau, and Nickel 2019; Kim et al. 2023b). However, false beliefs about other types of mental states have not been explored. Understanding false beliefs about a range of mental states should be crucial for LLMs to facilitate effective social interaction in real-world scenarios. (3) The behaviors and mental states of the characters in most benchmarks do not depend on their personality traits, even though they do in the real world (Costa and McCrae 1980; Izard et al. 1993; Mehl, Gosling, and Pennebaker 2006).\nTo address the above issues, we introduce TOMATO, a new Theory-of-Mind dATaset generated via Inner Speech prompting. Firstly, TOMATO comprehensively evaluates first- and second-order ToM for five categories of mental states: beliefs, intentions, desires, emotions, and knowledge. Secondly, we provide TOMATO-FB, a subset of TOMATO for evaluating understanding of false beliefs about the five mental states of others, e.g., understanding Bob thinks that Alice feels relieved, while Alice feels frustrated (Figure 1). In this regard, TOMATO is the most comprehensive benchmark compared to the existing ones. Lastly, TOMATO can evaluate the robustness of LLMs to the diverse personality traits of characters as seen in the real world. See Table 1 for the detailed comparison.\nCollecting human conversations and mental states of participants with self-report can be challenging in terms of cost, privacy, and accuracy (Nisbett and Wilson 1977). As recent LLMs have been shown to role-play assigned personalities (Jiang et al. 2023, 2024) and engage in conversations (Zhou et al. 2024b), we generate TOMATO with a newly designed LLM-LLM interaction. Namely, we design Inner Speech prompting (Table 2), which promotes role-playing LLMs to verbalize their mental states as thoughts in conversations with another LLM. This idea is inspired by the debate in Dillion et al. (2023) about the feasibility of LLMs to simulate human participants in psychological science. Moreover, we hypothesize that ensuring information asymmetry about thoughts is a crucial factor in having false beliefs about the mental states of others as shown in Figure 1. In addition, we assign big five personality traits to LLMs to diversify utterances and thoughts. This design enables TOMATO to evaluate robustness to diverse personality traits. The effects of these approaches are verified with analyses in \u00a76.\nWe evaluate nine LLMs including local and proprietary ones on TOMATO. Our experiments show that even the most advanced LLM, GPT-40 mini, under-performs the human performance in TOMATO. In addition, we show that the ToM performance of LLMs drops for the false belief subset, TOMATO-FB. Furthermore, we find that LLMs lack robustness to diverse personality traits. These results suggest that ToM in LLMs is still far from deployable to real-world applications."}, {"title": "2 Preliminaries", "content": "Theory of Mind (ToM) has been studied for decades (Premack and Woodruff 1978), with various definitions and conceptions proposed. In this section, we clarify the scope addressed in this paper.\nMental states. Following Ma et al. (2023), we define the scope of ToM in this study by focusing on specific mental state categories identified by Beaudoin et al. (2020): beliefs, intentions, desires, emotions, and knowledge. Mental states are represented with mental (state) verbs (Shatz, Wellman, and Silber 1983), such as think, feel, and know. Describing the remaining two categories defined by Beaudoin et al. (2020), percepts and non-literal communications, require a multimodal context and/or pose challenges in verbalization. Consequently, we have opted to exclude these two categories from our research scope.\nFirst- and second-order mental states. First-order beliefs refer to one's beliefs about something, e.g., A thinks that X. Second-order beliefs, on the other hand, often refer to one's beliefs about others' beliefs (Le, Boureau, and Nickel 2019; Sclar et al. 2023), e.g., B thinks that A thinks that Y. We extend these notions to other mental states following previous studies on human ToM (Winner and Leekam"}, {"title": "3 Related Work", "content": "Theory-of-Mind Benchmarks. LLMs have been reported to achieve human-level performance on various benchmarks (OpenAI 2024a). In response to this trend, researchers have been gaining interest in whether LLMs have human-like TOM (Kosinski 2024; Bubeck et al. 2023) or not (Ullman 2023; Shapira et al. 2024). To date, many benchmarks for evaluating ToM in machines have been constructed based on psychological tests designed for humans. False belief tasks inspired by Sally-Anne test (Wimmer and Perner 1983) have been widely used to evaluate understanding of wrong beliefs about object locations from narratives generated with templates (Nematzadeh et al. 2018; Le, Boureau, and Nickel"}, {"title": "4 TOMATO Benchmark", "content": "In this section, we describe the overview of our TOMATO benchmark: automatic construction process with LLMs, quality validation, and its statistics. Following the success of existing studies (Kim et al. 2023a,b), we also use LLMs to generate conversations. We employ Llama-3-70B-Instruct (Dubey et al. 2024) because of its transparency and relatively high scores on popular benchmarks (Chiang et al. 2024).\nNotation\nThe TOMATO benchmark is formulated as a multiple-choice question answering task due to its reliable evaluation. Each instance in the benchmark includes conversation C, question Q, four options $O = \\{o_i\\}_{i=1}^4$ as input and ground-truth answer A \u2208 O. Let $T_A$ and $T_B$ be role-playing LLMs with the multi-turn conversation capability that serve as characters A and B in conversation, respectively. Conversation $C_{1:N}^{AB}$ consists of utterances $\\{u_1^A, u_1^B, ..., u_N^A, u_N^B\\}$, where $u_i^A$ is the i-th utterance of character A. We define the category of mental states as T. The actual first- and second-order mental state of character A for type T when A says $u_i^A$ is defined as $m_{A,T1}$ and $m_{A,T2}$, respectively. $psy$ and $pis$ represent the system prompt and the proposed inner speech prompt, respectively, which are explained in the following sections.\nSystem Prompt\nWe design system prompts to guide LLM-LLM conversations, extending SOTOPIA (Zhou et al. 2024b) to consider the big five personality traits. SOTOPIA was initially proposed to evaluate social interaction of LLMs in LLM-LLM interactions, providing conversation scenarios from eight categories, such as persuasion, and character profiles. We sample 160 conversation scenarios uniformly from the eight categories, two characters for each conversation, and their goals, from SOTOPIA. See Figure 1 for examples. Then, we sample five pairs of characters for each scenario to prepare control conditions with regard to personality traits, resulting in 800 conversations. This design enables TOMATO to evaluate the robustness to diverse personality traits. In detail, we extend the naive prompt (Jiang et al. 2023), which reflects only one factor (e.g., You are {an open/a closed} person.), to include a combination of five factors of big five personality traits (De Raad 2000) (e.g., You are {an open / a closed}, {conscientious / unconscientious}, {extraversive / introversive}, {agreeable / disagreeable}, and {neurotic / stable} person.). We compile the above information to formulate system prompts, $psy$, given to $\u03c0_\u0391$.\nInner Speech Prompting\nIn order to make the inherently unobservable mental states observable, we propose Inner Speech (IS) prompting. IS prompting promotes role-playing LLMs to verbalize their subjective mental states in conversations. Since IS prompting can verbalize mental states as thoughts at any point during the conversation, TOMATO can also evaluate the understanding of dynamic changes in mental states. The actual IS prompts are given in Table 2. Moreover, IS prompting can generate first- and second-order mental states for five types by adjusting prompts. In order to ensure that the output follows the format, ({thought}) \u201c{utterance}", "follows": "n$u_i^A, m_{i}^{A,T1} \\sim \u03c0\u0391(u, m / psy, C_{1:i-1}, PIs)$,\n$u_i^B, m_{i}^{B,T2} \\sim \u03c0\u03b2(u, m / psy, C_{1:i-1}, u_i^A, PIs)$,\nwhere $C_{1:i-1} = \\{u_1, u_2, ..., u_{i-1}, u_{i-1}\\}.\nThis sampling process continues until the N-th turn. Then, we obtain 2N utterances with corresponding first- and second-order mental states for type T. We repeat this multi-turn conversation for each mental state, T \u2208 {Belief, Intention, Desire, Emotion, Knowledge}, each scenario, and each pair of characters. N is set seven because we found longer conversations tended to be redundant.\nHere, by hiding one's mental states from the other, we ensure information asymmetry about thoughts between the two as in human conversations. To delete thoughts from the outputs, we instruct LLMs to include their thoughts in \u201c()"}, {"title": "5 Experiments", "content": "We evaluate ToM in LLMs on TOMATO, exploring whether our approach uncovers insights into ToM in current LLMs with regard to various mental states, false beliefs, and personality traits that were not attainable with previous datasets.\nExperimental Setup\nBaselines. We evaluated nine LLMs: Llama-3-Instruct (8B and 70B), Llama-3.1-Instruct(8B and 70B) (Dubey et al. 2024), Gemma-2-IT (9B) (Gemma Team 2024), Mistral-Instruct (7B), Mixtral-8x7B-Instruct, GPT-3.5-Turbo, and GPT-40 mini (OpenAI 2024b). For the local LLMs, 4-bit quantization with bitsandbytes\u00b3 was used for inference. We employed lexical overlap (LO) as a naive baseline. LO simply selects the options that have the most words in common with the questions (Shinoda, Sugawara, and Aizawa 2023).4\nHuman Baseline. We also measured the human performance using MTurk. Annotators who are awarded Masters Qualification solved 32 questions for each subset, i.e., 480 questions in total.\nExperimental Results\nDo LLMs have human-level ToM? Table 3 shows the results of LLMs and the human baseline. The results showed that even the most advanced LLMs, such as GPT-40 mini and Llama-3.1 70B, lag behind the human baseline. We also tested Chain-of-Thought prompting and fine-tuning, but they were not sufficient to achieve human-level performance."}, {"title": "6 Analysis on the ToMATO benchmark", "content": "Is information asymmetry about thoughts effective for generating false beliefs? We conjecture that information asymmetry about their thoughts, goals, and personality traits between two LLMs in conversations is a key factor in inducing false beliefs about the mental states of the other. To verify this hypothesis, we conducted ablation studies for the generation process. Namely, we investigated the effect of the invisibility of one's thoughts and system prompts including goals and personality traits to the other on the frequency probability of false belief generation. We evaluated 3k instances with GPT-40 mini and 200 instances with three human annotators of MTurk for each generation process. We used majority vote to aggregate the human annotations. Results are given in Table 6. The results showed that information asymmetry about both system prompts and thoughts encourages false belief generation.\nDoes TOMATO reflect personality traits given in prompts? To answer this question, we conducted the z-statistics analysis (Gardner et al. 2021) for the correlations between the output tokens and the personality traits given in the prompts. We first sampled one scenario from each category in SOTOPIA and generated conversations and thoughts with our approach in \u00a74. We assigned every pattern of the big five personality, i.e., $32 = 2^5$ patterns in total, to one agent for each scenario.\nSome results are displayed in Figure 2. Here, y denotes the probability of word $x_i$ to appear in the output when the corresponding personality specified in prompts is high. The colored tokens above or below the curves are significantly positively or negatively correlated to the assigned personality factor. This indicates that the big five personality factors given in prompts have intentionally affected the generation. E.g., agents who are assigned neurotic often generate \"worried\" and those who are assigned not neurotic often generate \"happy\" in their thoughts.\nWe also conducted a pairwise comparison, following Jiang et al. (2023), to see if the specified personality traits are reflected properly with MTurk and GPT-40 mini. We showed that 70-80% of the outputs reflect the specified personality traits for O, E, A, and N. Among the five, C is less reflected as intended, which is consistent with Jiang et al. (2023). Inducing conscientiousness in outputs is future work.\nCan TOMATO be easily solved with shortcut solutions? Language understanding benchmarks should not be easily solved with shortcut solutions based on spurious correlations to ensure that those benchmarks measure intended abilities (Sugawara and Tsugita 2023). In general, multiple-choice QA datasets often suffer from spurious correlations such as word-label correlation, and lexical overlap (Yu et al. 2020; Shinoda, Sugawara, and Aizawa 2023). First, for lexical overlap, the LO baseline does not achieve high performance compared to the human baseline as shown in Table 3.\nSecond, for word-label correlation, we again conducted the z-statistics analysis (Gardner et al. 2021) to identify statistically significant correlations between words in options and binary labels, for four benchmarks including TOMATO. In z-statistics analysis, the frequencies and probabilities of each word that appears in correct options are plotted as shown in Figure 3. When the probabilities of words that appear in correct options are significantly higher (z \u2265 1.96) or lower (z < -1.96) than the random baseline, the words are colored. In detail, the ratios (%) of the number of biased (colored) words in options to the vocabulary size are 1.16, 3.34, 4.49, and 6.04 for TOMATO, Social-IQa, FANTOM, and ToMBench, respectively. These results indicate that TOMATO contains the fewest word-level spurious correlations among the four benchmarks. Based on these analyses, we claim that TOMATO is so challenging that it requires models to acquire more sophisticated solutions than shortcuts to achieve human-level performance."}, {"title": "7 Conclusion", "content": "A comprehensive evaluation of ToM using our TOMATO would be valuable for accurately tracking the development of ToM in LLMs. Notably, to the best of our knowledge, this study is the first to propose false belief tasks about mental states other than beliefs. Moreover, the problem setting of estimating mental states from the conversation between characters with diverse personalities in our benchmark is more consistent with real-world applications than existing benchmarks. Therefore, TOMATO is useful as a touchstone for real-world applications such as understanding and supporting human communication. Future work includes extending our work to evaluating ToM with multi-modal contexts (Mao et al. 2024), decision-making (Guo et al. 2024), and multi-agent settings (Cross et al. 2024)."}, {"title": "CPrompts for Evaluation on TOMATO", "content": "Since the chat templates including special tokens vary depending on LLMs5, we report only chat messages for evaluation on TOMATO before applying them as follows.\nChat Messages with System Prompt for Evaluation on To\u039c\u0391\u03a4\u039f\nmessages = [\n{\"role\": \"system\u201d, \u201ccontent\u201d: \"You are an expert at understanding human communication. Please leverage the information provided and choose the most probable answer to the question from the options. Output your final answer by strictly following this format: [A], [B], [C], or [D]\"},\n{\"role\": \"user\u201d, \u201ccontent\u201d: \u201c\u201c\u201c\u201c\u201c# Transcript\\n{{conversation}}\\n# Question\\n{{question}}\\n# Options\\n[A] {{option1}}\\n[B] {{option2}}\\n[C] {{option3}}\\n[D] {{option4}}''''''}  ]\nAs some LLMs such as Gemma-2 (Gemma Team 2024) do not support system prompts, we just concatenate the texts for those LLMs as follows.\nChat Messages without System Prompt for Evaluation on TOMATO\nmessages = [\n{\"role\": \u201cuser\u201d, \u201ccontent\u201d: \u201cYou are an expert at understanding human communication. Please leverage the information provided and choose the most probable answer to the question from the options. Output your final answer by strictly following this format: [A], [B], [C], or [D]\\n# Transcript\\n{{conversation}}\\n# Question\\n{{question}}\\n# Options\\n[A] {{option1}}\\n[B] {{option2}}\\n[C] {{option3}}\\n[D] {{option4}}\u201d} ]"}, {"title": "D Additional Experimental Results", "content": "How effective are prompting techniques in ToMATO? We compared two prompting methods: vanilla prompting, and zero-shot Chain-of-Thought (CoT) prompting (Kojima et al. 2022).\nThe results are in Table 10. The effect of CoT was the largest for the FB subset. We found that CoT still has limited improvement on TOMATO compared to the human performance in Table 3, which is consistent with Kim et al. (2023b); Xu et al. (2024).\nDoes fine-tuning improve ToM performance? Fine-tuning on ToM datasets is effective to improve in-domain performance (Kim et al. 2023b), but fails to generalize to out-of-domain (Sclar et al. 2023). To test whether this is the case in TOMATO, we fine-tuned Llama-3-8B-Instruct on held-out dataset generated with our LLM-LLM conversations. We first selected four sources (NormBank, PersuasionForGood, CraigslistBargains, and MutualFriends) randomly from eight sources described in Appendix A. Then, we randomly sampled 25 scenarios from each source and obtained 100 scenarios. Note that these scenarios do not overlap with the ToMATO benchmark. We used the same dataset construction approach to generate the training set. We conducted supervised fine-tuning of Llama-3-8B-Instruct on the generated training set. Multiple options were omitted from prompts during fine-tuning and evaluating fine-tuned models because we found that including options led to performance degradation. Here, except for the options, the prompts were the same as Appendix C. The predictions were made by selecting the option with the largest likelihood assigned by fine-tuned models among four options during testing. The size of the training set was 7000. We used the PEFT (Mangrulkar et al. 2022) implementation of QLoRA (Dettmers et al. 2023) with the configurations given in Table 12. We report the scores for two subsets of TOMATO to see the generalization capability: the in-domain split (ID), where scenarios are from the same source as the training set, and the out-of-domain split (OOD), where scenarios are from unseen sources. We also evaluated LLMs on another dataset, SocialIQa (Sap et al. 2019), to see if fine-tuning can avoid overfitting and keep general social intelligence."}, {"title": "E Analysis", "content": "Pairwise Comparison for Personality Traits\nTo see if the effect of the personality traits specified in the prompts is reflected to the utterances and thoughts in an intended manner, we conducted pairwise comparison analyses, following Jiang et al. (2023). Since we extended the naive prompt (Jiang et al. 2023) to include the five factors in prompts (e.g., You are an open / a closed}, {conscientious / unconscientious}, {extraversive / introversive}, {agreeable / disagreeable}, and {neurotic / stable} person.), we controlled only one factor among the five and compared the utterances and thoughts generated by two different big five conditions, e.g., when one is generated with {O=high, C=low, E=high, A=high, N=high} and the other is generated with {O=low, C=low, E=high, A=high, N=high}, we ask GPT or humans which character in the two conversations is more open to experience. We ask GPT-40 mini and 10 human annotators in MTurk to compare 80 and 15 pairs and judge which conversation is more {open / conscientious / extraversive / agreeable / neurotic} for each factor of the big five. In total, 400 and 75 pairs are judged by GPT-40 mini and humans, respectively. The instruction to human annotators in MTurk is shown in Figure 8. The prompt used for the pairwise comparison with GPT-40 mini is as follows.\nChat Message Format for Pairwise Comparison\nmessages = [\n{\"role\": \"system\u201d, \u201ccontent\u201d: \u201cYou are an expert at understanding human communication.\u201d},\n{\"role\": \u201cuser\u201d, \u201ccontent\u201d: \u201cWhich {{name}} in conversation, A or B, is more {{adjective}}? \\nDefinition: people who are {{adjective}} {{definition}}\nAnswer this question by selecting one from the three options:\n[A] {{name}} in conversation A is more {{adjective}}\n[B] {{name}} in conversation B is more {{adjective}}\n[C] Neither\\nLook at the whole conversations and consider your answer.\\nOutput your final verdict by strictly following this format: [A], [B], or [C]\\nThink step-by-step before outputting your answer.\\n### Conversation A\\n{{conversation_a}}\\n### Conversation B\\n{{conversation_b}}\\n\u201d} ]\n70% of the pairs for four factors, O, E, A and N. We claim that TOMATO Among the five factors, conscientiousness (C) is less reflected as intended, which is consistent with Jiang et al. (2023). However, given that the success rates are larger than 60% for all the five factors, we claim that TOMATO can evaluate the robustness to diverse personality traits in an intended manner. Inducing conscientiousness to be reflected in generations is future work."}]}