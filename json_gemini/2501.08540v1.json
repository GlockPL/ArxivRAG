{"title": "KNOWLEDGE PROMPT CHAINING FOR SEMANTIC MODELING", "authors": ["Ning Pei Ding", "Jing Ge Du", "Zai Wen Feng"], "abstract": "The task of building semantics for structured data such as CSV, JSON, and XML files is highly relevant in the knowledge representation field. Even though we have a vast of structured data on the internet, mapping them to domain ontologies to build semantics for them is still very challenging as it requires the construction model to understand and learn graph-structured knowledge. Otherwise, the task will require human beings' effort and cost. In this paper, we proposed a novel automatic semantic modeling framework: Knowledge Prompt Chaining. It can serialize the graph-structured knowledge and inject it into the LLMs properly in a Prompt Chaining architecture. Through this knowledge injection and prompting chaining, the model in our framework can learn the structure information and latent space of the graph and generate the semantic labels and semantic graphs following the chains' insturction naturally. Based on experimental results, our method achieves better performance than existing leading techniques, despite using reduced structured input data. Our code is available at https://github.com/dingningpei/LLM_Semantics.", "sections": [{"title": "1 Introduction", "content": "Currently, structured data sources remain the primary choice for storing enterprise or web data. However, a vast amount of structured data on the internet exists in different formats, making the integration of heterogeneous data sources a critical topic for research to promote data sharing [1]. Domain ontologies offer a solution to the challenge of integrating heterogeneous data sources. As a hierarchical knowledge representation model, a domain ontology defines classes and properties specific to a domain to describe factual knowledge about the real world. Mapping structured data sources to a domain ontology produces a semantically meaningful structured model, referred to as the semantic model of structured data sources, and the process is known as semantic modeling. Semantic modeling enables the integration of heterogeneous data from various sources and includes two main steps. The first step is semantic labeling, which involves annotating attribute columns in the data source using classes and data properties from the domain ontology to derive semantic types for the target source. Based on this annotation, the second step is to identify and establish semantic relationships between attributes in the data source using object properties in the domain ontology. This step typically requires incorporating the actual content of the structured data to develop a reliable semantic model. Our research focuses on a challenging aspect of this task. For example, in a domain ontology, there might exist a potential triple: <Event, had_participant, Actor>. Since the class Activity is a subclass of Event, the triple <Event, had_participant, Actor> can be further refined into the triple <Activity, had_participant, Actor>. The refined triple remains valid. During the semantic modeling process, whether to refine a potential triple and, if so, into which triple, depends on specific reasoning in different situations. This ontology reasoning process is crucial for the semantic modeling task as it directly impacts the reliability of the semantic model.\nIn previous studies on automatic semantic modeling for structured data sources [2][3][4][5], researchers applied various methods to this task, ranging from Steiner trees to probabilistic graphical models and graph neural networks. These"}, {"title": "2 Related Work", "content": "Automatic semantic modeling of structured data. Many studies focus on automatic semantic modeling of structured data. For source attribute semantic labeling, the first step in semantic modeling, DSL [6] is a domain-independent automatic semantic labeling method based on machine learning, DINT [7] combines feature engineering and machine learning to realize semantic annotation, and Meimei [8] is an efficient probabilistic method to conduct semantic annotation in probability calculation through a multi-label classifier. Additionally, other work focuses on parsing the semantic relationships between source attributes. Early research [2][3] used Steiner tree-based methods for automatic semantic modeling, but these were limited by the inability to correctly identify multiple edges with identical weights in the semantic model search graph. PGM-SM [4] employs probabilistic graphical models to score and rank the most reasonable semantic models. Deep learning methods have also been applied to this task. SeMi [5] uses graph neural networks to learn and mine semantic features from background-related data and predict potential semantic relationships in the target data sources. Our research uses advanced large language models and prompt engineering techniques for the automatic semantic modeling of structured data sources. The combination of LLMs and prompts provides powerful reasoning capabilities. At the same time, users can customize their requirements by simply modifying instructions, enabling flexible and interactive semantic modeling capabilities.\nLarge language models and prompts for structured data. LLMs have demonstrated outstanding performance in handling various text tasks, which has led researchers to apply them in the field of structured data. Since LLMs are sequence-to-sequence models, structured data must be converted into text format before it can be fed into LLMs [9]. [10] found that DFLoader and JSON formats outperform other formats in all table transformation tasks. Researches like TabLLM [11], TAP4LLM [12], and GReaT [13] have applied LLMs to tasks such as table classification, table data understanding, and table data generation. Although LLMs have been successfully applied in structured data tasks, complex reasoning remains a challenge. Chain-of-Thought (COT) [14] is a prompt technique in the era of large language models that achieves good reasoning performance on complex tasks through a series of intermediate reasoning steps. [15] addressed question-answering tasks in complex tables by reformatting the table data structure and designing appropriate COT prompts. CHAIN-OF-TABLE [16] framework explicitly incorporates table data into the reasoning chain as an intermediate reasoning agent to guide LLMs in performing step-by-step table operations and updates. Knowledge Prompt is a method of embedding knowledge into prompts, and there have been studies applying Knowledge Prompt to tasks such as knowledge graph completion [17], multi-hop link prediction in knowledge graphs [18], and knowledge-based VQA [19]. Our research focuses on using LLMs and prompts to achieve automated semantic modeling of structured data."}, {"title": "3 Problem Statement", "content": "3.1 Overview\nSuppose we have a set of Ontologies \u03a9 and a set of structure data D = {(Xi, Si)}n i=1, where semantic Si \u2208 \u03a9, Xi = {(xij)J i j=1}R i r=1 , and cj \u2208 C for a set of structure format as XML, Json, or CSV. n is the size of the data D and xrij is the value of the attribute aj in the row r. Ji is the size of the attributes in the ith data source, and Ri is the size of the rows in the ith data source. Semantic Modeling is to map the structure data X to the target graph S in the Ontologies \u03a9. Semantic Modeling should include two parts: 1. Semantic Labeling, 2. Semantic Graph Building.\n3.2 Semantic Modeling\nSemantic Labeling Semantic Labeling is to map function that map the attributes a in the data source X to the nodes in the Ontology \u03a9. We use the triple set relation Ni = {(aj, pj, nj)}J i j=1 as the mapping process of one certain data source, where pj is the properties of the target Ontology and nj is the node of the target Ontology and the pair pj, nj is called the semantic type of the attribute aj. The map function denoted it as F : Ni = f(Xi, \u03a9).\nSemantic Graph Building Semantic Graph Building is to find the construction path in the target Ontology, which can connect all the Semantic Type. We use the trips set Si = {(eg, Pg, eg)}G g=1 to describe the final semantic graph, where G is the size of the triple set Si. We use function Q to demonstrate the process of finding the construction path. Q : Si = q(Xi, Ni, \u03a9)\nOur goal is to find the approximate Q* and F* function based on \u03a9 and D. Given a new dataset X*, the precision and recall scores between the predicted semantics S* and the golden semantics of X* should be as high as possible, where S* = Q*(X*, F*(X*, \u03a9), \u03a9)."}, {"title": "4 Knowledge Prompt Chaining", "content": "4.1 Overview\nOur Knowledge Prompt Chaining Framework is illustrated in Figure 1. The framework consists of two main parts:\n1. Knowledge Integration: Our method adopts the Long-Context Large Language Model as Construction Function F and Q. The data set and ontology are taken as knowledge input of the LLMs with hashtag < Examples > and < Ontology > correspondingly. All this knowledge is sterilized into sequential data and integrated into the knowledge prompt template as the system prompt of our automatic semantic modeling system.\n2. Prompt Chaining: When the user inputs new structured data, we inject it into the designed prompt chaining process and output the semantic labeling with the hashtag < Step1 > first. The semantic labeling and data itself are taken as the input of the next step of the chain. It finally produces the semantic graph with hashtag < Step2 >.\n4.2 Knowledge Integration\nExcept for taking advantage of prior knowledge in the LLMs, we want to inject external knowledge into the prompt. However, the external knowledge base, D and \u03a9 in this task is the graph structure. So, the big challenge here is to inject the graph-based knowledge into a text-based prompt.\nStructure Data Serialization For the aim of using LLM, all the data input into LLM should be transformed into nature language representation. We denote this process as function Serialize that transforms the structure data into text format data. In the previous work[4][2], they applied an encoder-focused serialization method to process the structured data. Typically, they convert the structured data into plain text, which a machine learning or deep learning model is trained on as a decoder. Our method focuses on text-based serialization[20]. According to Singha's work [10], JSON format and Pandas Dataloader outperform other text-based serialization methods in prediction tasks. However, pandas dataloader cannot process structured data like XML. So in this task, we applied JSON format to sterilize X and S. We only select three data points in each data as the high cost of token usage in LLMs.\nGraph Serialization Besides the structure data, the input of the LLM also include the graph data \u03a9. We denote this Serialization as GraphSeialize(\u03a9). In this work, we process ontologies in JSON format to unify the input formats in LLM.", "subsections": []}, {"title": "4.3 Prompt Chaining with Pruning", "content": "When a user inputs a new dataset X*, we use the function above Serialize to process it. We then prompt the LLM to generate semantic labels N* for X*, which is chain1 part of the system. We do this prompt by constructing chain1 data as (< Chain1Prompt >, Serialize(X)). Once we get semantic labels of the dataset, we enter the chain2 part: semantic graph building. We input chain2 data to LLM continually and build semantic models for X*. The chain2 data is composed of N* from last step and < Chain2Prompt > in Figure 2. We do not have to input X* here because we already fed it into LLM in chain1 part. In both < Chain1Prompt > and < Chain2Prompt >, we do require LLM to give the detailed reasoning process of the output instead of outputting the result directly. This is inspired by Chain-of-Thought technique[14]. The whole Prompt Chaining process is denoted as:\nN* = LLM(< Chain1Prompt >, Serialize(X*))\nS*aw = LLM(< Chain2Prompt >, N*))"}, {"title": "5 Experimental Setup", "content": "5.1 Datasets\nWe evaluated our method on three datasets, each of which consists of a set of structured data sources and an ontology that will be used to model the data sources. The first dataset, dscrm, models 28 museum data sources using the CIDOC Conceptual Reference Model (CIDOC-CRM), which contains data from different art museums in the US. The second dataset, dsedm, models museum data sources using the Europeana Data Model (EDM). The third dataset, ds schema, models 12 football data sources using an extension of Schema.org. Table 2 provides more details on the evaluation datasets. Since this study uses LLMs to model structured data sources, the necessary preprocessing and cleaning were performed on the datasets to ensure that the LLMs could correctly recognize and process the data. The data processing details are as follows:\n\u2022 Table Data: The raw tabular data existed in various formats (CSV, XML, and JSON), which were converted into a unified JSON format. The content of the tables was stored in a unified list ([]), with each record in the original table corresponding to a dictionary object () in the list. Due to the token input limitations of LLMs, only three records from each original table were retained in the processed data. Each dictionary represents a single record in the table, with keys as attributes (column names) and values as the corresponding data. If a value is missing, it is represented as < Empty >, but this does not affect the processing logic of the data.\n\u2022 Ontology Data: The ontology data were also processed into a JSON structure, organized into three main sections in the form of dictionaries. The Nodes field lists the classes in the ontology, and the Properties field lists the properties of the ontology, where each class and property may include its parent class or property, with inheritance relationships denoted by the \"->\" symbol. The Potential triples field lists potential triples representing relationships between classes and properties.\n\u2022 Semantic Model Data: The semantic model data were also processed into JSON format, consisting of semantic_triples and internal_link_triples. Specifically, the semantic_triples field contains semantic triples that represent the relationships between table attributes and ontology nodes. The internal_link_triples field contains internal link triples that represent the linking relationships between different entities within the ontology.\nPlease note that we have updated the semantic models in the ground-truth datasets to ensure that they are more reasonable and interpretable. Detailed information can be found in Appendix A."}, {"title": "5.2 Models", "content": "In this study, LLMs are required to understand complex tabular data and perform semantic modeling under ontology constraints. Therefore, the selection of LLMs must take into account several factors, including their ability to process long contexts, token input limitations, performance characteristics, and cost-effectiveness. Based on these considerations, we selected three distinct LLMs for our experiments: Claude 3.5 Sonnet, GPT-4 Turbo, and DeepSeek-V2.5."}, {"title": "5.3 Experimental Design", "content": "We conducted experiments using the three LLMs on three datasets. The purpose of the experiments was to assess the effectiveness of different LLMs in semantic modeling tasks on structured data across various domains. We compared our approach with three semantic modeling systems: Taheriyan et al. [2], De U\u00f1a et al. [3] (Serene), and Vu et al. [4] (PGM-SM). The specific experimental settings are as follows.\nIn the LLMs experiment section, during data loading and splitting, we first indexed the data files and controlled the random shuffling of the indices using the random_state parameter. We selected 2023 and 2024 as the random seeds to mitigate potential biases arising from the initial order of the data files, ensuring consistency in repeated experiments and assessing the stability of model performance. The test_size parameter was fixed at 0.5 to ensure that half of the data was always used as the test set. Additionally, the size parameter was fixed at 3, meaning that only the first three records from each table data file were loaded. To evaluate the models' learning capabilities under varying amounts of input data, we implemented three different settings for the num_files parameter: the first setting loaded only one file, the second loaded one-quarter of the files, and the third loaded one-half of the files. These settings were referred to as one-shot, quarter-shot, and half-shot, respectively. These files represent known semantic models. To assess the impact of prompt engineering, we also designed a prompt ablation study to measure how different prompts affected model performance. These experimental settings were repeated across all models and datasets."}, {"title": "5.4 Evaluation Metrics", "content": "The tasks in this study consist of two steps: semantic labeling and semantic graph building. The results of both steps are evaluated using unified Precision and Recall metrics. These metrics are calculated by comparing the predicted semantic annotations and semantic models with the gold standard models. Assuming the correct semantic model of the data source X is S, and the semantic model learned by our approach is S*, we define Precision and Recall as follows:\nPrecision = $\\frac{|S \\cap S^*|}{|S^*|}$\nRecall = $\\frac{|S \\cap S^*|}{|S|}$"}, {"title": "6 Results", "content": "In this section, we evaluate the effectiveness of the Knowledge Prompt Chaining approach for semantic modeling through a series of experiments aimed at addressing the following key research questions:\n\u2022 Q1: How does the Knowledge Prompt Chaining approach perform in terms of accuracy and efficiency for automated semantic modeling tasks?\n\u2022 Q2: Can the Knowledge Prompt Chaining approach effectively handle complex semantic models?\n\u2022 Q3: What is the impact of different prompt designs on the performance of semantic modeling?"}, {"title": "6.1 Performance in Accuracy and Efficiency", "content": "In this section, we evaluate the accuracy and efficiency performance of the Knowledge Prompt Chaining approach for semantic modeling.\nTables 2 and 3 present the performance of different methods on the structured data automatic semantic modeling task, where our method achieves the best scores among the three LLMs. Table 2 compares different semantic labeling methods, showing that our method outperforms others on all datasets, with semantic labeling precision reaching 96.9% and above, far exceeding other methods. Table 3 evaluates the precision and recall performance of different automatic semantic modeling methods. On the three datasets, our method achieved accuracy improvements of 5.9%, 4.6%, and 0.6%, respectively, compared to previous methods. On the dsedm dataset, it achieved a precision of 90.4% and a recall of 92.9%, indicating that the combination of LLMs and prompts is more suitable for solving complex table reasoning tasks. To further analyze the performance of the three LLMs, we conducted a detailed evaluation of the LLMs on a two-step task, with the results shown in Tables 4 and 5. The results in Table 4 show that Claude 3.5 Sonnet performs exceptionally well in the semantic annotation task. Notably, in the one-shot setting, the semantic labeling precision reaches 90.4%, which indirectly proves that LLMs possess rich prior knowledge and demonstrate good reasoning abilities with minimal prompt input. Table 5 compares the performance of different LLMs in the semantic modeling task. It is clear that Claude 3.5 Sonnet and DeepSeek-V2.5 achieved excellent results in semantic modeling, while GPT-4 Turbo performed somewhat less effectively, indicating that different LLMs exhibit varying adaptability when facing different tasks. Table 6 presents the time required for different LLMs to predict the semantic model of a single structured data source. The results are the average values across three settings. Since network factors may affect the task"}, {"title": "6.2 Performance on Complex Semantic Models", "content": "We evaluate Knowledge Prompt Chaining framework's performance on different levels of graph complexity. We use the depth of graphs in the semantic models to measure the complexity of the graph. Claude's model shows high performance on complicated graphs while not doing well on the simplest graphs. DeepSeek has a good performance in all levels of complexity especially in half-shot set. GPT-4 Turbo shows a pretty good performance in 4-depth graphs and also has the lowest precision score in the simplest graphs. The results in Figure 3 shows our method's strong ability to deal with complicated graphs. It implied that Claude and GPT-4 might have some issues with \"overthinking\" since they can not deal with the simplest graphs well."}, {"title": "6.3 Effectiveness Analysis of Prompt Ablation Designs", "content": "We analyze different parts of our framework by evaluating their contributions to the performance. We use the DeepSeek model to test them on CRM and EDM datasets. We start from an ablation framework and add components one by one. As shown in Table 7, our method surpasses the ablation version across all data. The prompting chaining part in our framework contributes to the most prat of performance improvement. And pruning technique also works well and helps the framework reduce hallucinations of the LLM."}, {"title": "7 Conclusions and Future Work", "content": "In this paper, we introduce the knowledge prompting chaining framework to automatically generate semantic models for structured data. By implementing knowledge injection and prompting chaining and graph pruning, our method improves the accuracy of the task across different datasets without any training requirement.\nIn the future, we will explore the prompting turning methods to further enhance the reasoning ability of our framework. The reasoning details in < Step1 > and < Step2 > that we require the model to generate can be treated as synthetic data. Additionally, in this paper, we designed < Rule > part manually. If we can extract this part automatically, the efficiency of our framework should be enhanced. In this aspect, we aim to apply meta prompt[21] to let LLM teach themselves to find < Rule >."}, {"title": "APPENDIX", "content": "A Updates of Semantic Models\nThe semantic model updates in this study are applied exclusively to the dscrm dataset. Previous work revealed certain limitations in the provided gold standard model, such as meaningless leaf nodes, a lack of interpretability in the alignment between the semantic model and the data source, and inconsistent standards in semantic model construction.\nGuided by domain experts, we updated the gold standard model to ensure it is more logical and precise. The updates adhere to the following principles:\n1. Removal of Irrelevant Leaf Nodes: Based on the table headers provided in the 28 data sources, we removed leaf nodes from the model that were not directly associated with any table headers. All leaf nodes in the updated model must correspond to a table header in the data source, with names strictly aligned to ensure consistency. This approach ensures that every leaf node in the model is meaningful and aligned with the data source.\n2. Numbering for Multiple crm:E52_Time-Span Nodes: When multiple crm:E52_Time-Span nodes exist in the model, the following numbering rules are applied.\n\u2022 If crm: E67_Birth and crm: E69_Death nodes are present, the nodes are numbered as <crm: E67_Birth1, crm:P4_has_time-span, crm:E52_Time-Span1> and <crm:E69_Death1, crm:P4_has_time-span, crm:E52_Time-Span2>. Any remaining crm:E52_Time-Span nodes are numbered sequentially afterward.\n\u2022 If crm: E67_Birth and crm: E69_Death nodes are absent, all crm: E52_Time-Span nodes are numbered without specific constraints.\n3. Usage Guidelines for crm: E21_Person and crm: E39_Actor Nodes: The choice between crm: E21_Person and crm:E39_Actor nodes depends on the context of the data source.\n\u2022 If the data source primarily focuses on personal information about artists without referencing their works, the crm:E39_Actor node is used.\n\u2022 In all other cases, the crm: E21_Person node is applied.\n4. Guidelines for the Use of crm: E55_Type Nodes: Two categories of crm:E55_Type nodes are employed across all models, differentiated based on the types referenced in the data source records.\n\u2022 The first category connects to crm:E22_Man-Made_Object via <crm:E22_Man-Made_Object, crm:P2_has_type, crm:E55_Type>. These crm:E55_Type nodes are further linked to table headers representing broad artistic types. The records in these table headers may include values such as \"Painting\", \"Furniture\", or \"Architecture\", which represent broad artistic categories.\n\u2022 The second category connects to crm:E12_Production via <crm:E12_Production, crm:P32_used_general_technique, crm:E55_Type>. These crm: E55_Type nodes are further linked to table headers representing detailed production techniques or material types. The records in these table headers may include values such \u201cOil on canvas\u201d, \u201cMahogany, chestnut, tulip poplar\", or \"Wood\u201d, which represent detailed categories."}]}