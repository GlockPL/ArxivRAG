{"title": "PALMBENCH: A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS", "authors": ["Yilong Li", "Jingyu Liu", "Hao Zhang", "M Badri Narayanan", "Utkarsh Sharma", "Shuai Zhang", "Pan Hu", "Yijing Zeng", "Jayaram Raghuram", "Suman Banerjee"], "abstract": "Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements (MLC, 2023a; Gerganov, 2023) have facilitated the local deployment of LLMs. However, local deployment also presents challenges, particularly in balancing quality (generative performance), latency, and throughput within the hardware constraints of mobile devices. In this paper, we introduce our lightweight, all-in-one automated benchmarking framework that allows users to evaluate LLMs on mobile devices. We provide a comprehensive benchmark of various popular LLMs with different quantization configurations (both weights and activations) across multiple mobile platforms with varying hardware capabilities. Unlike traditional benchmarks that assess full-scale models on high-end GPU clusters, we focus on evaluating resource efficiency (memory and power consumption) and harmful output for compressed models on mobile devices. Our key observations include: i) differences in energy efficiency and throughput across mobile platforms; ii) the impact of quantization on memory usage, GPU execution time, and power consumption; and iii) accuracy and performance degradation of quantized models compared to their non-quantized counterparts; and iv) the frequency of hallucinations and toxic content generated by compressed LLMs on mobile devices.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) such as ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), and Llama (Touvron et al., 2023a;b;c) are powerful generative models that are revolutionizing interactive communication and various natural language processing tasks, including question-answering, document summarization, abstract reasoning, and code auto-completion (e.g., Github Copilot (Github)). LLMs require significant computational and memory resources to run due to their huge number of parameters (e.g., MT-NLG 530B (Smith et al., 2022)), making them more suitable for running on cloud infrastructures with high-end powerful GPU clusters. While significant attention has been dedicated to cloud-based LLMs, there is a growing need to run LLMs on resource-constrained mobile devices in order to obtain some key benefits. (1) Privacy and Security: Processing user data locally on mobile devices helps protect user privacy and enhances data security. There is also less risk of data breaches or unauthorized access to sensitive information. (2) No Cloud Reliance: By running LLMs locally, mobile applications can reduce their dependence on cloud services for language processing tasks. This can lead to cost savings and increased reliability, as the application's functionality is not reliant on the availability and performance of remote servers. (3) Offline Access: By running LLMs on mobile devices, users can access powerful language processing capabilities even when they are not connected (or have unreliable connection) to the Internet.\n\nThe rapidly flourishing LLM ecosystem, including various large models, architectures and frameworks, presents both opportunities and challenges for developers and researchers interested in deploying pre-trained LLMs on mobile devices. Existing efforts in on-device LLM inference have primarily focused on model compression and efficient inference techniques, with a strong focus"}, {"title": "RELATED WORK", "content": "Our benchmarking study extensively evaluated prior efforts focused on optimizing LLMs for mobile devices. These efforts include frameworks (MLC, 2023a;b; Gerganov, 2023), the development of smaller models (Computer, 2023; Abdin et al., 2024; Li et al., 2023c), and model quantization techniques (Frantar et al., 2022).\n\nLarge Language Models. LLMs like ChatGPT (OpenAI, 2023), the Llama series (Touvron et al., 2023a;c), Mistral (Jiang et al., 2023), Vicuna (Chiang et al., 2023) etc. are gaining substantial influence in generative AI applications. While LLMs are undoubtedly driving AI advancements, their sophisticated capabilities demand significant resources. Both power consumption and memory requirements for training these models and generating predictions scale linearly with their size (number of parameters), significantly increasing operational costs during inference.\n\nThe development of smaller models such as Google Gemma-2-2B (Team, 2024), Llama-3.2-1B/3B (Llama, 2024), RedPajama-INCITE-3B (Computer, 2023), Phi-2/Phi-3 (Abdin et al., 2024), and TinyLlama (Zhang et al., 2024) is providing enhanced options for resource-constrained mobile devices or edge devices that require efficient\n\nQuantization. Post-training quantization (PTQ) applies quantization to LLMs after they have been fully trained, which is an effective compression method used to create smaller models for inference. Many efforts have been made to optimize LLMs for more efficient storage and faster computation. Group-wise quantization (Yang et al., 2024) involves partitioning the weights of a neural network into groups and quantizing each group independently. This allows the quantization process to be more finely tuned to the distribution of weights within each group, specifically reducing I/O costs and offloading on mobile platforms. GPTQ (Frantar et al., 2022) goes one step further and proposes a post-training weight quantization method that compresses LLM weights to 3 or 4 bits instead of 8 bits. Activation-aware Weight Quantization (AWQ) (Lin et al., 2024) observe that there exists a small subset of model weights called salient weights, characterized by larger activation magnitudes, plays a crucial role in reducing the quantization loss of LLMs, if they are preserved with high precision.\n\nInference Engine. Although there are a lot of efficient inference frameworks, MLC-LLM (Machine Learning Compilation) (MLC, 2023a;b) enables users to develop, quantize, and deploy LLMs across various platforms, including mobile devices and web browsers. It leverages compiler accelerations"}, {"title": "METHODOLOGY", "content": "To evaluate LLMs on mobile devices, we created the PalmBench framework, which focuses on the following three aspects:\n\n1) Benchmark Automation. We developed an automated framework that uses Android Graphic Inspector (AGI) (AGI), Xcode profiler, and Nvidia Visual profilers to trace execution data and analyze runtime behavior across platforms when benchmarking LLM performance and resource utilization on edge and mobile devices. PalmBench requires USB debugging to connect to Android phones and iPhones.\n\n2) Resource Utilization. Our primary focus is on the resource demands of different models across various platforms such as CPU, GPU, memory, and NPU that significantly impact user experience. Our study goes beyond resource demands, aiming to quantify the impact of different quantization techniques on both performance and resource efficiency across various state-of-the-art models.\n\n3) Model Accuracy. While a model's architecture mostly dominates its outputs, we observe variations when applying different quantization methods on diverse devices. To validate the quantized LLMs and quantify the accuracy degradation due to compression, we evaluate the knowledge and answering accuracy of models using the Exact Match and F1 score compared with original models. We also test these models on conventional tasks with open-sourced datasets (Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Hendrycks et al., 2021). Moreover, we also investigate the potential issues"}, {"title": "METRICS AND DATASETS", "content": "Table 2 summarizes all the metrics used in our benchmark. To evaluate the accuracy and correctness of quantized LLMs, we use popular Question-Answering datasets such as SQUAD (Rajpurkar et al., 2016) and Natural Questions (NQ) (Kwiatkowski et al., 2019), comparing their performance with that of the original, non-quantized models. Additionally, we employ comprehensive benchmarks for different tasks, including MTBench (Zheng et al., 2023) to compare their language understanding and reasoning capabilities across different quantizations. Also, we measured toxicity by calculating toxic score by using Perspective API (Perspective, 2020) and TET (Luong et al., 2024), and evaluate hallucination in each quantized LLM using HaluEval (Li et al., 2023a) and TruthfulQA (Lin et al., 2022) benchmarks. Appendix A.4 and Appendix A.6 provide some examples of these datasets for benchmarking. These widely-recognized datasets ensure that our experiments and metrics are both convincing and reproducible."}, {"title": "CHOICE OF LLMS", "content": "We have identified and converted several popular models for benchmarking on edge and mobile devices using model weights from their official Huggingface or GitHub repositories. These models are converted into experimental formats such as GGUF and K-quant for Llama.cpp Gerganov (2023) or compiled using TVM Chen et al. (2018) for MLC frameworks MLC (2023a;b). Given that mobile devices typically do not exceed 8GB of memory, it is impractical to test too large models, as they would surpass these devices' memory capacity. In our benchmark, we evaluated the various LLMs, including Llama-2-7b-chat Touvron et al. (2023b), Llama-3-8B-Instruct Touvron et al. (2023c) Microsoft Phi2 Abdin et al. (2024), Mistral-7B-Instruct Jiang et al. (2023), RedPajama-INCITE-Chat-7B Computer (2023), Vicuna Chiang et al. (2023)), TinyLlama-1.1B-Chat-v1.0 Zhang et al. (2024), and Qwen2 Bai et al. (2023). The prebuilt weights for these models are readily available in the MLC repository, which also offers options for compilation in various configurations."}, {"title": "INFRASTRUCTURE", "content": "Our benchmarking framework's infrastructure includes essential components that automate device interactions, model execution, data collection, and performance evaluation. It features both software and hardware elements such as devices, system drivers, performance profilers, automated data collection tools, and mobile phone user interfaces (UIs), representing a significant engineering effort, as shown in Appendix A.1 Table 6."}, {"title": "\u041cOBILE DEVICES", "content": "We evaluate the LLMs on a range of devices with varying hardware capabilities, as listed in Table 6 in Appendix, including Google Pixel 4 (P4), Pixel 5a (P5), Pixel 7 (P7), iPhone 12 Pro (IP12), iPhone 15 Pro (IP15), S22 Ultra (S22U), Orange Pi 5 (OP5) Pi, and Nvidia Jetson Orin Nano (Nano) Nano, covering mainstream operating systems."}, {"title": "INFERENCE ENGINE", "content": "We use two frameworks, MLC-LLM (MLC, 2023a;b) and llama.cpp (Gerganov, 2023), as inference engines to execute LLMs on devices. Although many frameworks claim compatibility with mobile devices, they often lack support for popular platforms or models. MLC-LLM (MLC, 2023a;b) and llama.cpp (Gerganov, 2023) are two of the most popular frameworks that support a wide range of platforms and models. Unfortunately, llama.cpp (Gerganov, 2023) is still incompatible with iPhone."}, {"title": "QUANTIZATION", "content": "Model quantization is primarily handled by the built-in quantization programs of frameworks. MLC supports various quantization levels, including non-quantized float-16 (q0f16) and float-32 (q0f32), 3-bit quantization (q3f16_1), 4-bit quantization (q4f16_1), and 4-bit AWQ (q4f16_awq). The format qAfB(_id) denotes 'A' as the number of bits for weight storage and 'B' as the number of bits for activation storage. llama.cpp supports quantization using its GGUF format, which employs a type of group-wise quantization known as K-quant and supports more quantization methods (1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit)."}, {"title": "PROMPT INPUT", "content": "For platforms like Orange Pi 5 Pi and Nvidia Jetson Nano Nano using Ubuntu, we can only use the Command Line Interface (CLI). Benchmark scripts are running, and prompt texts are transferred via USB serial ports. On mobile platforms, such as iPhones and Android devices, we have used custom-developed Apps based on MLC's examples MLC (2023a) that automatically fetch prompts from text files and initiate the touch events to interact with the Apps. Prompt texts are from datasets listed in 3.1."}, {"title": "CONTROL AUTOMATION", "content": "Control operations are predominantly conducted through the Android Debug Bridge (ADB) for Android devices. Due to the limitations of the built-in Android Studio profilers in monitoring GPU usage, we have employed the Android GPU Inspector (AGI) tool AGI, developed by the Android team, to track resource utilization metrics such as CPU, GPU, memory, energy, and latency. Raw data traces are extracted from AGI and transferred via ADB. iOS primarily used Xcode's profiling tools and a custom plugin to measure GPU utilization. The GPU measurement plugin, a derivative development based on the IOKit plugin Tan (2018), displays real-time GPU utilization within the app. Xcode offers a comprehensive performance analysis tool, Instruments, which measures CPU utilization, memory usage, execution time, and energy consumption."}, {"title": "GPU DRIVER", "content": "Although MLC-LLM MLC (2023a;b) and llama.cpp Gerganov (2023) support various drivers; OpenCL is the preferred and most mature GPU driver commonly used for both Android phones and Ubuntu-based edge computing devices. The iPhone utilizes Apple's proprietary Metal driver, which is well supported by both MLC MLC (2023a) and TVM Chen et al. (2018). Nvidia Jetson Nano device leverages its own CUDA with highly optimized driver Nano."}, {"title": "EQUIPMENTS", "content": "In addition to resource usage, we also look into energy efficiency, a critical factor impacting user experience. We employ two complementary devices to comprehensively evaluate the effects of quantization on power consumption and the distribution of device temperature. For thermal behavior analysis, we utilize the FLIR C5 thermal imaging camera Flir (2020) and a professional USB power meter for accurate power consumption measurements. This enables us to investigate the energy efficiency and thermal behavior of mobile platforms across different models and quantization methods, which are crucial factors affecting user experience."}, {"title": "EXPERIMENTS", "content": "We present the most significant benchmarking results for LLMs across various models and platforms (outlined in Section 3) here, and provide additional results in the Appendix."}, {"title": "EXPERIMENTAL SETUP", "content": "We evaluate the LLMs on various devices detailed in Section 3. MLC (MLC, 2023a) is compatible with all platforms (Apple, Android, and Linux Edge), whereas llama.cpp Gerganov (2023) is limited to deployment on Android and Ubuntu systems. Since our benchmarking focuses on mobile devices, we concentrate on models that function correctly and are suitable for size. For example, the 0-bit Llama-2-7b-hf model, which occupies 13.11GB, is impractical for existing mobile devices. Some models come in different scales, such as the Vicuna 13B model and Vicuna-7B. However, even with 3-bit quantization, the Vicuna 13B model's memory usage exceeds 6GB, making it too large for mobile devices. Quantization methods primarily rely on built-in framework configurations. For a fair comparison, we standardize all models with a temperature setting of 0.2\u2014the default value\u2014and limit the context window to 4096."}, {"title": "RESOURCE UTILIZATION", "content": "We first evaluate the impact of quantization on resource efficiency using the MLC and llama.cpp frameworks on Android phones and edge devices for the models detailed in previous sections.\n\nMemory Utilization: LLM inference is inherently memory-bound, and its memory utilization can be reduced significantly by memory bandwidth quantization, which reduces the precision of weights and activations."}, {"title": "THROUGHPUT AND LATENCY", "content": "In addition to resource utilization, we analyzed latency (ms) and throughput (tok/s), which are crucial factors influencing user experience. Higher throughput and lower latency indicate faster model output. Figure 7 shows the throughput (tok/s) across all platforms on MLC. Smaller models typically offer higher throughput; for instance, RedPajama-INCITE-3B and TinyLlaMa-1.1B achieve higher throughput than larger models, indicating that smaller sizes execute more quickly on mobile devices. Moreover, the results indicate that iPhones, particularly when running Llama-2-7B and Llama-3-8B models, deliver significantly higher throughput compared to other devices. Even the three-year-old iPhone 12 Pro outperforms newer Android devices and Nvidia's Jetson Orin Nano in maintaining relatively high throughput, demonstrating metal-accelerated inference performance. When running Mistral-7B-q3f16 and Phi2-q4f16, differences in prefilling and decoding throughput are observed despite their similar sizes. The model with fewer parameters and higher quantization decodes faster than the larger, lower-bit quantized model, highlighting the impact of the model architecture."}, {"title": "OUTPUT MATCHING AND CORRECTNESS", "content": "Quantization often compromises model accuracy, particularly when using lower-bit representations. To validate the correctness and assess the performance degradation of quantized models, we use question data from the SQUAD (Rajpurkar et al., 2016) and Natural Question (Kwiatkowski et al., 2019) dataset to calculate the Exact Match and F1 score, using the output of the original non-quantized model as a reference. The exact match and F1 score results are shown in Figure 8. Moreover, our observation also shows that 4-bit and 6-bit quantization mostly maintains performance compared to the original non-quantized model, with 4-bit quantization requiring less memory and computational resources (Figure 2(a) 2(b)]). Interestingly, the 5-bit and 3-bit models underperformed slightly. K-quant (ggml) 3-bit model produced more hallucinations and toxic content than the 2-bit model. Although the K-quant (ggml) 5-bit model is larger than the 4-bit model, it showed more performance degradation than all 4-bit quantization."}, {"title": "TASKS", "content": "To evaluate the performance of quantized models across various tasks, we utilize MT-bench (Zheng et al., 2023), which employs a predefined multi-turn question set to evaluate models across eight categories: Reasoning, Math, Coding, Extractions, STEM, Humanities, Writing, and Roleplay. Figure 9 shows that models with higher bit quantization generally achieve better scores across all categories. In contrast, lower-bit quantization (2-bit, 3-bit, 4-bit) still performs well in humanities, writing, and extraction tasks. For users with devices that have limited resources, particularly those with less than 4GB of memory, 2-bit or 3-bit quantization can still provide an adequate user experience in these tasks or for simple Q&A applications."}, {"title": "POWER CONSUMPTION AND TEMPERATURE", "content": "Model quantization greatly reduces memory usage and GPU execution time, as LLM inference is largely memory-bound. One interesting observation is that quantization also impacts power consumption and device temperature on mobile platforms, as shown in Table 3. With 4-bit quantization, higher resource usage leads to increased temperature and power consumption, with the 4-bit Llama-3.2-3B model consuming 25.2% more power than its 3-bit counterpart."}, {"title": "HALLUCINATION AND TOXICITY", "content": "LLMs can potentially produce incorrect or harmful information, particularly hallucinated and toxic content. We evaluate Toxicity and Hallucination using GPT-40 OpenAI (2023) and Claude-3.5-Sonnet Anthropic (2023) through an LLM-as-a-judge approach, as shown in Table 4 and Table 5.\n\nLower bit quantization typically leads to increased hallucinations and toxicity. However, 3-bit quantization performs worse than both 2-bit group-wise quantization and all 4-bit methods, exhibiting more hallucinations and toxic content. Among the 4-bit quantization methods (GPTQ (Frantar et al., 2022), ggml (Gerganov, 2023), AWQ (Lin et al., 2024) and FT (Nvidia, 2019)), GPTQ, AWQ, and FT show similar performance, while ggml performs slightly worse. Examples of hallucinated and toxic outputs are provided in Tables 11 12 in Appendix A.6."}, {"title": "CONCLUSIONS", "content": "We present a comprehensive benchmark for evaluating LLMs under various quantization schemes on diverse mobile platforms. Our lightweight, all-in-one automated benchmarking framework enables users to evaluate mobile devices via USB, providing extensive metrics and datasets. This study uniquely focuses on resource efficiency for mobile GPUs, contrasting with traditional high-end GPU cluster evaluations. Key findings highlight the superiority of iOS platform in energy efficiency and throughput, and quantization's effectiveness in reducing resource requirements. We also examine accuracy and potential issues in quantized models, including toxicity and erroneous outputs. This research provides crucial insights for efficient LLM deployment in mobile environments, addressing previously overlooked aspects of on-device LLM performance."}, {"title": "APPENDIX", "content": null}, {"title": "SPECIFICATIONS OF TESTING DEVICES", "content": "We evaluate the LLMs on a range of devices as listed in Table 6, including Google Pixel 4 (P4), Pixel 5a (P5), Pixel 7 (P7), iPhone 12 Pro (IP12), iPhone 15 Pro (IP15), S22 Ultra (S22U), Orange Pi 5 (OP5) Pi, and Nvidia Jetson Orin Nano (Nano) Nano, covering mainstream operating systems."}, {"title": "MOBILE DEVICE TEMPERATURE", "content": null}, {"title": "CPU, GPU, AND MEMORY PROFILING DATA STRUCTURE", "content": "Our benchmark automation framework records traces of memory usage, battery power consumption, GPU, and CPU usage, each saved in JSON file format. An example of a measurement trace is shown below."}, {"title": "DATASETS", "content": "\u2022 Natural Questions contains real user questions submitted to Google search, with answers provided by annotators from Wikipedia. NQ is designed to train and evaluate automatic question-answering systems.\n\n\u2022 HaluEval A collection of LLMs generated datasets and human-annotated examples of hallucinations.\n\n\u2022 TruthfulQA A benchmark to measure whether a language model is truthful in generating answers to questions."}, {"title": "OUTPUT MATCHING", "content": "The objective of the Output Matching in our benchmark is to verify the accuracy and proper alignment of model outputs once the models are quantized in different quantization methods (Frantar et al., 2022; Yang et al., 2024; Lin et al., 2024). The questions and context used in the datasets are sourced from SQUAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) with reference data consisting of answers from the original large models prior to quantization.\n\nHere are some examples of Output Matching Datasets:"}, {"title": "TOXICITY AND HALLUCINATION", "content": "We evaluate hallucinations of quantized LLMs using the ChatGPT generated samples from the HaluEval Li et al. (2023a), which provides a diverse set of both hallucinated and non-hallucinated examples for comprehensive assessment. An example of hallucination is presented in Table 10."}]}