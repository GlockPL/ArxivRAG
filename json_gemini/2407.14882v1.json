{"title": "Reduced Effectiveness of Kolmogorov-Arnold Networks on Functions with Noise", "authors": ["Haoran Shen", "Chen Zeng", "Jiahui Wang", "Qiao Wang"], "abstract": "It has been observed that even a small amount of noise introduced into the dataset can significantly degrade the performance of KAN. In this brief note, we aim to quantitatively evaluate the performance when noise is added to the dataset. We propose an oversampling technique combined with denoising to alleviate the impact of noise. Specifically, we employ kernel filtering based on diffusion maps for pre-filtering the noisy data for training KAN network. Our experiments show that while adding i.i.d. noise with any fixed SNR, when we increase the amount of training data by a factor of r, the test-loss (RMSE) of KANs will exhibit a performance trend like test-loss ~ O(r\u00af\u00bd) as r \u2192 +\u221e. We conclude that applying both oversampling and filtering strategies can reduce the detrimental effects of noise. Nevertheless, determining the optimal variance for the kernel filtering process is challenging, and enhancing the volume of training data substantially increases the associated costs, because the training dataset needs to be expanded multiple times in comparison to the initial clean data. As a result, the noise present in the data ultimately diminishes the effectiveness of Kolmogorov-Arnold networks.", "sections": [{"title": "I. INTRODUCTION", "content": "The Kolmogorov-Arnold networks (KAN) have attracted considerable attention following their release on Arxiv [1]. However, [4] pointed out that these networks are susceptible to noise.\nHaving been introduced only a few months ago, KANs are considered innovative neural network structures and potential substitutes for Multi-Layer Perceptrons (MLPs). Various applications of KAN networks have been reported across different domains, including: time series analysis [13] [14], ODEs [15], PDEs [16], hyperspectral image classification [17] [18], physical modeling [19], computer vision [20] [23], and graph learning [21] [22] [25].\nIn addition, various enhancements to KANs have been introduced. For instance, [24] substituted the spline functions used as weights with Chebyshev polynomials, [26] merged the strengths of KANs and LSTM, [27] utilized wavelet-based structure for KANs, [28] developed Convolutional KANs, [29] employed fractional-orthogonal Jacobi functions as the basis functions for KANs, and [30] enhanced the computational process of KANS."}, {"title": "II. THE IMPACT OF NOISE IN KANS", "content": "The Kolmogorov-Arnold theorem addresses the representation of multivariate continuous functions. The theorem states that any continuous function of multiple variables can be represented as a superposition of continuous functions of one variable and addition [2] [6] [7]. Formally, it can be stated as:\nTheorem 1 (Kolmogorov-Arnold Theorem). Let \\(f : [0,1]^n \\rightarrow \\mathbb{R}\\) be any multivariate continuous function, there exist continuous univariate functions \\(\\varphi_i\\) and \\(\\psi_{ij}\\) such that:\n$$f(x_1, x_2,...,x_n) = \\sum_{i=1}^{2n+1} \\varphi_i \\left(\\sum_{j=1}^{n} \\psi_{ij}(x_j)\\right)$$\nUnfortunately, this theorem is existential rather than constructive, unlike the Lagrange interpolation theorem. In 2009, [9] provided a constructive proof of this theorem. Nonetheless, it might pose challenges when working with functions that exhibit noise.\nAccording to [4], adding a small Gaussian noise to the training labels of a dataset and employing this altered dataset to train a KAN network can unexpectedly worsen the test loss.\nTo ascertain the existence of this phenomenon and explore solutions to reduce noise impact, we added noise to the training datasets involved in different fitting tasks. These noisy datasets were subsequently fed into the network for training, and the results were contrasted against those obtained from the original datasets. We selected six functions for fitting and utilized various KAN network architectures.\nAs shown in Table II, it is clear that even small amounts of noise can significantly impact network performance. With 3000 training samples, adding noise with a standard deviation of \\(\\sigma = 0.2\\) leads to a significant increase in test-loss.\nWhat causes KAN's vulnerability to noise? The lack of regularity induced by noise leads to a deterioration in the performance of KANs. The activation function in KAN comprises a basis function b(x) and a spline function spline(x), as shown in following equation,\n$$\\varphi (x) = w (b(x) + \\text{spline} (x)),$$\nwhere\n$$b(x) = \\frac{1}{1 + \\text{exp}(-x)},$$\nboth of which possess adequate smoothness. Moreover, KAN neurons perform only simple summation operations, making it hard to accurately represent a non-smooth function with added noise using nested smooth functions. This results in a poor recovery of the original function in a noisy environment."}, {"title": "III. DENOISE BY KERNEL FILTERING", "content": "We now aim to reduce the impact of noise in general multivariate functions. Given that the training data is not uniformly sampled in this scenario when fitting multidimensional functions with KANs, it is logical to utilize multidimensional filtering techniques.\nIn this subsection, we utilize kernel filtering to remove noise from the data. Alternatively, for data on non-linear manifolds, diffusion maps can be employed as described in [32].\nFor the sake of simplicity, we consider the Gaussian-like kernel [5]\n$$k(x,y) = Ce^{-d^2(x,y)/2\\sigma^2},$$\nwhere C is the normalization coefficient, and d the distance function defined as\n$$d^2(x,y) = \\sum_{n=1}^{N}(x^{(n)} - y^{(n)})^2,$$\nfor data \\(x = (x^{(n)})_{n=1}^N, y = (y^{(n)})_{n=1}^N\\). Clearly, the kernel filtering for data \\({f(x_n); n = 1,2,......,M}\\) produces updated data\n$$\\hat{f(x_i)} = \\sum_{n=1}^{M} k(x_j,x_n) f (x_n),$$\nand the kernel \\(k(x_j,x_n)\\) might be stored as a matrix in practical computation. Actually, for large matrix, we may treat it as sparse matrix since \\(k(x_j,x_n) \\approx 0\\) when \\(d(x_j, x_n)\\) is large enough."}, {"title": "IV. ENHANCE TRAINING DATASET TO MITIGATE NOISE", "content": "In contrast to denoising techniques, we discovered that augmenting the number of training samples is an efficient method for reconstructing noisy data. This process is akin to recovering a band-limited signal from noisy discrete oversampling data, which can be elucidated by frame theory [3].\nNumerous important studies in signal processing have been conducted on this subject over the past decades, e.g. [10]. We outline the process using the formula\n$$f(t) = \\sum_{k} f_k \\text{sinc}(t - \\frac{kT}{20}), \\quad f_k = f(\\frac{kT}{20}) + e_k$$\nwhere \\(f_k\\) denotes the samples and \\(e_k\\) signifies i.i.d. Gaussian noise with zero mean. We highlight that \\(0 < T < 1\\) to ensure the oversampling rate, and a smaller T implies a higher number of samples. Despite the presence of noise in these samples, the sampling formula (7) acts as a stationary oversampling reconstruction method that closely approximates the original noise-free signal f(t), as explained by frame theory in [3]. Specifically, a higher sampling rate can mitigate the effect of samples with lower SNR. This formula (7) is effective because the target function f(t) is band-limited, implying it must be highly regular, being an entire function of exponential || type according to Paley-Wiener's Theorem [8].\nIn our case, a larger training dataset allows KANs to extract the original data from a substantial amount of information and resist noise interference [12]. We discovered that increasing the quantity of training samples is an effective method for reconstructing data affected by noise. By having a larger set of training samples, KANs are capable of extracting the original data from the extensive samples and enduring noise interference.\nUsing \\(f_1(x,y) = \\text{exp} (\\text{sin} (\\pi x) + y^2)\\) and \\(f_2(x,y) = xy\\) as an illustration, we start with 3000 training samples and incrementally add 2000 samples at each step. These datasets are input into a [2,5,1] network at various sampling rates, and the test loss for each sample size is plotted, resulting in Figure 4 and Figure 5.\nTo fit \\(f_3 (x_1,x_2, x_3, x_4) = \\text{exp}((sin (\\pi x_1 + \\pi x_3) + \\text{sin} (\\pi x_3 + \\pi x_4)))\\) with [4,4,2,1] KAN will result in Figure 6a, which produces the same asymptotical behavior as other functions. However, if we utilize a [4,2,1,1] KAN as suggested in [5], we observe that more data and higher SNR might exhibit higher test losses, as illustrated in Figure 6b. Through extensive experimentation, we found that this pattern persisted. The [4,4,2,1] KAN shown in Fig. 6a approximates \\(f_3\\) more accurately than the [4,2,1,1] KAN illustrated in Fig. 6b, reducing the considerable variations observed with the [4,2,1,1] KAN. Furthermore, as the dataset grows, the [4,4,2,1] KAN"}, {"title": "V. COMBINING OVERSAMPLING AND KERNEL FILTERING", "content": "Previous experiments have shown that both oversampling and kernel filtering are effective in reducing noise. It is natural to combine these techniques by applying kernel filtering to the dataset and slightly increasing the training sampling rate. We will then compare the test loss with that of a dataset that uses only one noise reduction method. This experiment introduces different noise levels to three functions, resulting in SNRs of 7.38dB, 4.46dB, and 10.53dB for the datasets \\(f_1\\), \\(f_2\\), and \\(f_3\\), respectively.\nRegrettably, merging the two techniques did not yield significantly improved outcomes. In fact, this combined approach results in a higher test loss compared to using oversampling alone. It appears that kernel filtering disrupts the effectiveness of oversampling, and this disruption remains even when the sample size is increased to 50 times its original size.\nAs mentioned earlier, increasing the amount of training data can effectively reduce test loss and enhance filtering performance. Therefore, for these functions (\\(f_4\\), \\(f_5\\), and \\(f_6\\)), we increased the training data by 10 times, respectively, to observe how the test loss changes. The results are shown in Fig 3d, 3e and 3f. We can find that compared to the training data size of 500, the training data size of 5000 significantly reduces the test loss. Additionally, as \\(\\sigma\\) varies, the change in test loss also decreases considerably, indicating that the impact of \\(\\sigma\\) on filtering performance is diminishing."}, {"title": "VI. CONCLUSION", "content": "In this brief note, we assess the decline in KANs performance for functions affected by noise. We explore two methods to alleviate these problems: the first focuses on noise elimination, and the second on increasing the size of the training dataset. Although the latter method demonstrates considerable enhancement, the overall performance remains deficient due to the excessive amount of required data. Consequently, we conclude that KANs must overcome the challenges presented by noise interference."}, {"title": "ACKNOWLEDGMENT", "content": "The authors would like to express their heartfelt gratitude to Dr. Aijun Zhang for the insightful discussions regarding the adverse impacts of noise on the KANs network, and for providing numerous invaluable suggestions for this manuscript."}]}