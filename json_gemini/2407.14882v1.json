{"title": "Reduced Effectiveness of Kolmogorov-Arnold Networks on Functions with Noise", "authors": ["Haoran Shen", "Chen Zeng", "Jiahui Wang", "Qiao Wang"], "abstract": "It has been observed that even a small amount of noise introduced into the dataset can significantly degrade the performance of KAN. In this brief note, we aim to quantitatively evaluate the performance when noise is added to the dataset. We propose an oversampling technique combined with denoising to alleviate the impact of noise. Specifically, we employ kernel filtering based on diffusion maps for pre-filtering the noisy data for training KAN network. Our experiments show that while adding i.i.d. noise with any fixed SNR, when we increase the amount of training data by a factor of r, the test-loss (RMSE) of KANs will exhibit a performance trend like test-loss ~ $O(r^{-\u00bd})$ as r \u2192 +\u221e. We conclude that applying both oversampling and filtering strategies can reduce the detrimental effects of noise. Nevertheless, determining the optimal variance for the kernel filtering process is challenging, and enhancing the volume of training data substantially increases the associated costs, because the training dataset needs to be expanded multiple times in comparison to the initial clean data. As a result, the noise present in the data ultimately diminishes the effectiveness of Kolmogorov-Arnold networks.", "sections": [{"title": "I. INTRODUCTION", "content": "The Kolmogorov-Arnold networks (KAN) have attracted considerable attention following their release on Arxiv [1]. However, [4] pointed out that these networks are susceptible to noise.\nHaving been introduced only a few months ago, KANs are considered innovative neural network structures and potential substitutes for Multi-Layer Perceptrons (MLPs). Various applications of KAN networks have been reported across different domains, including: time series analysis [13] [14], ODEs [15], PDEs [16], hyperspectral image classification [17] [18], physical modeling [19], computer vision [20] [23], and graph learning [21] [22] [25].\nIn addition, various enhancements to KANs have been introduced. For instance, [24] substituted the spline functions used as weights with Chebyshev polynomials, [26] merged the strengths of KANs and LSTM, [27] utilized wavelet-based structure for KANs, [28] developed Convolutional KANs, [29] employed fractional-orthogonal Jacobi functions as the basis functions for KANs, and [30] enhanced the computational process of KANS.\nIn this paper, we aim to measure the notable decrease in performance of KAN networks when subjected to noise interference, and explore methods to alleviate the noise effects. We propose two strategies: implementing filtering techniques and increasing the volume of training data.\nThe contributions of this paper are as follows: Firstly, it demonstrates that incorporating noise into the training data drastically diminishes the performance of Kolmogorov-Arnold Networks (KANs). To tackle this problem, the authors propose two strategies. One strategy utilizes a kernel filtering technique to mitigate some of the noise. The difficulty lies in determining the optimal variance parameter for the filter since it's nonlinearly dependent on the Signal-to-Noise Ratios (SNRs). The other strategy addresses the noise issue by expanding the training dataset. The authors discovered an intriguing pattern showing that if the number of training samples is increased by a factor of r from the initial amount, the performance (test-loss) will asymptotically decrease as $r^{- \u00bd}$ as r \u2192 \u221e.\nThe structure of this paper is organized as follows: In Section II, we showed that introducing noise to the training dataset leads to a significant drop in the performance of KANs. We then develop two methods to counteract the noise effect. In Section III, we propose a filtering technique based on kernel filtering, which can remove some noise to an extent. The most difficult challenge here is determining the optimal variance parameter \u03c3 of this Gaussian-like kernel filter. Unfortunately, the optimal variance is nonlinearly dependent on the SNRs, making it difficult to ascertain. Following this, in Section IV, we introduce a technique to reduce noise interference by increasing the training dataset size and demonstrate how the test loss statistically declines asymptotically as the data volume increases. In Section V, we integrate kernel filtering with oversampling. However, it turns out to be challenging to find an equilibrium between the repetition factor r of the data size and the SNR of the data. Finally, in Section VI, we draw our conclusions for this paper."}, {"title": "II. THE IMPACT OF NOISE IN KANS", "content": "The Kolmogorov-Arnold theorem addresses the representation of multivariate continuous functions. The theorem states that any continuous function of multiple variables can be represented as a superposition of continuous functions of one variable and addition [2] [6] [7]. Formally, it can be stated as:\nTheorem 1 (Kolmogorov-Arnold Theorem). Let f : $[0,1]^n$ \u2192 R be any multivariate continuous function, there exist continuous univariate functions \u03c6i and \u03c8ij such that:\n$f(x_1, x_2,...,x_n) = \\sum_{i=1}^{2n+1} \\phi_i(\\sum_{j=1}^n \\psi_{ij}(x_j))$\nUnfortunately, this theorem is existential rather than constructive, unlike the Lagrange interpolation theorem. In 2009, [9] provided a constructive proof of this theorem. Nonetheless, it might pose challenges when working with functions that exhibit noise.\nAccording to [4], adding a small Gaussian noise to the training labels of a dataset and employing this altered dataset to train a KAN network can unexpectedly worsen the test loss.\nAs shown in Table II, it is clear that even small amounts of noise can significantly impact network performance. With 3000 training samples, adding noise with a standard deviation of \u03c3 = 0.2 leads to a significant increase in test-loss.\nWhat causes KAN's vulnerability to noise? The lack of regularity induced by noise leads to a deterioration in the performance of KANs. The activation function in KAN comprises a basis function b(x) and a spline function spline(x), as shown in following equation,\n$\\varphi (x) = w (b(x) + \\text{spline } (x)),$\nwhere\n$b(x) = \\frac{1}{1 + \\exp(-x)},$\nboth of which possess adequate smoothness. Moreover, KAN neurons perform only simple summation operations, making it hard to accurately represent a non-smooth function with added noise using nested smooth functions. This results in a poor recovery of the original function in a noisy environment."}, {"title": "III. DENOISE BY KERNEL FILTERING", "content": "We now aim to reduce the impact of noise in general multivariate functions. Given that the training data is not uniformly sampled in this scenario when fitting multidimensional functions with KANs, it is logical to utilize multidimensional filtering techniques.\nIn this subsection, we utilize kernel filtering to remove noise from the data. Alternatively, for data on non-linear manifolds, diffusion maps can be employed as described in [32].\nFor the sake of simplicity, we consider the Gaussian-like kernel [5]\n$k(x,y) = C e^{-d^2(x,y)/2\u03c3^2},$\nwhere C is the normalization coefficient, and d the distance function defined as\n$d^2(x,y) = \\sum_{n=1}^N (x^{(n)} \u2013 y^{(n)})^2,$\nfor data x = $(x^{(n)})_{n=1}^N$, y = $(y^{(n)})_{n=1}^N$. Clearly, the kernel filtering for data {$f(x_n); n = 1,2,......,M$} produces updated data\n$\\tilde{f}(x_i) = \\sum_{n=1}^M k(x_j,x_n) f(x_n),$\nand the kernel k(xj,xn) might be stored as a matrix in practical computation. Actually, for large matrix, we may treat it as sparse matrix since k(xj,xn) \u2248 0 when d(xj, xn) is large enough."}, {"title": "B. The Optimal Filter Parameter \u03c3", "content": "Let's focus on the equation (4) that describes Gaussian-like kernel. The parameter o represents the standard deviation of the Gaussian function, which controls the width of the kernel.\nTo investigate the impact of o on filtering performance, we employed f2 as the fitting objective and injected Gaussian noise denoted by N(0,0.2). Various levels of kernel filtering were implemented, leading to the test-loss versus SNR curves illustrated in Fig 2. It is evident that beginning with \u03c3 = 0.08 (small \u03c3), the curve resembles noisy data shown in blue and diminishes rapidly. As o increased (with a larger o resulting in smoother data), the initial point on the left side of the performance curve improves, achieving lower test-loss initially. Nevertheless, the decay is more gradual, and importantly, the right side of the curve performs worse, displaying a considerably higher test loss compared to earlier curves. Ultimately, no matter what \u03c3value is selected, kernel filtering ceases to function effectively once the data's SNR surpasses the critical limit. Filtering is only effective in the low SNR region."}, {"title": "IV. ENHANCE TRAINING DATASET TO MITIGATE NOISE", "content": "In contrast to denoising techniques, we discovered that augmenting the number of training samples is an efficient method for reconstructing noisy data. This process is akin to recovering a band-limited signal from noisy discrete oversampling data, which can be elucidated by frame theory [3].\nNumerous important studies in signal processing have been conducted on this subject over the past decades, e.g. [10]. We outline the process using the formula\n$f(t) = \\sum_k f_k \\text{sinc}(t - \\frac{kT}{2\u03c3}), f_k = f(\\frac{kT}{2\u03c3}) + e_k$\nwhere fk denotes the samples and ek signifies i.i.d. Gaussian noise with zero mean. We highlight that 0 < T < 1 to ensure the oversampling rate, and a smaller T implies a higher number of samples. Despite the presence of noise in these samples, the sampling formula (7) acts as a stationary oversampling reconstruction method that closely approximates the original noise-free signal f(t), as explained by frame theory in [3]. Specifically, a higher sampling rate can mitigate the effect of samples with lower SNR. This formula (7) is effective because the target function f(t) is band-limited, implying it must be highly regular, being an entire function of exponential || type according to Paley-Wiener's Theorem [8]."}, {"title": "B. Increase Training Samples", "content": "In our case, a larger training dataset allows KANs to extract the original data from a substantial amount of information and resist noise interference [12]. We discovered that increasing the quantity of training samples is an effective method for reconstructing data affected by noise. By having a larger set of training samples, KANs are capable of extracting the original data from the extensive samples and enduring noise interference.\nUsing f1(x,y) = exp (sin (\u03c0x) + y\u00b2) and f2(x,y) = xy as an illustration, we start with 3000 training samples and incrementally add 2000 samples at each step. These datasets are input into a [2,5,1] network at various sampling rates, and the test loss for each sample size is plotted, resulting in Figure 4 and Figure 5.\nTo fit f3 ($x_1, x_2, x_3, x_4$) = exp((sin (\u03c0$x_1$ + \u03c0$x_3$) + sin (\u03c0$x_3$ + \u03c0$x_4$))) with [4,4,2,1] KAN will result in Figure 6a, which produces the same asymptotical behavior as other functions. However, if we utilize a [4,2,1,1] KAN as suggested in [5], we observe that more data and higher SNR might exhibit higher test losses, as illustrated in Figure 6b. Through extensive experimentation, we found that this pattern persisted. The [4,4,2,1] KAN shown in Fig. 6a approximates f3 more accurately than the [4,2,1,1] KAN illustrated in Fig. 6b, reducing the considerable variations observed with the [4,2,1,1] KAN. Furthermore, as the dataset grows, the [4,4,2,1] \u039a\u0391\u039d better fits with the function. It is important to note that KAN might not always adhere to the anticipated structure of the function, making pruning operations necessary for achieving a more efficient network architecture.\nFor f3, the structure [4,2,1,1] seems ideal, as shown in Fig. 7. Nevertheless, the comparison between the structures [4,2,1,1] and [4,4,2,1] in Fig. 6 reveals that KAN needs more parameters and a more complex structure to properly model this function. According to Table 3 in [1], which compares manually designed KAN shapes with those discovered automatically, for other functions, KAN might require fewer parameters than the predetermined structure. This implies that KAN could have difficulty precisely defining the function and finding the optimal structure, hence complicating the determination of the most suitable KAN configuration for practical applications. As a result, modeling functions with KAN purely based on noisy training data without any prior understanding of the function may lead to inconsistent results.\nAs anticipated, the test loss of KAN decreases steadily as the number of training samples grows. Initially, with the increasing sample size, the test loss drops rapidly, and then it asymptotically follows a decay rate of r\u00af\u00bd. Here, r represents the multiple of the initial training data, and the test loss will asymptotically behave like\ntest-loss (RMSE) ~ $O(r^{-\u00bd})$, as r \u2192 +\u221e.\nThis behavior is actually analogous to the linear reconstruction of a band-limited signal from noisy data, see [31]. When we utilize a KAN with grid=5 to fit various functions, we aim to increase the number of training samples and generate a graph illustrating the relationship between test loss and training samples. For f3, we reached a conclusion similar to f1: The test loss rapidly decreases to about half under noisy conditions when the training sample size reaches 20000, and beyond that point, the rate of decline in test loss gradually slows down. The optimal denoising effect is achieved when the sample size reaches approximately 60000, where the"}, {"title": "V. COMBINING OVERSAMPLING AND KERNEL FILTERING", "content": "Previous experiments have shown that both oversampling and kernel filtering are effective in reducing noise. It is natural to combine these techniques by applying kernel filtering to the dataset and slightly increasing the training sampling rate. We will then compare the test loss with that of a dataset that uses only one noise reduction method. This experiment introduces different noise levels to three functions, resulting in SNRs of 7.38dB, 4.46dB, and 10.53dB for the datasets f1, f2, and f3, respectively.\nRegrettably, merging the two techniques did not yield significantly improved outcomes. In fact, this combined approach results in a higher test loss compared to using oversampling alone. It appears that kernel filtering disrupts the effectiveness of oversampling, and this disruption remains even when the sample size is increased to 50 times its original size.\nAs mentioned earlier, increasing the amount of training data can effectively reduce test loss and enhance filtering performance. Therefore, for these functions (f4, f5, and f6), we increased the training data by 10 times, respectively, to observe how the test loss changes. The results are shown in Fig 3d, 3e and 3f. We can find that compared to the training data size of 500, the training data size of 5000 significantly reduces the test loss. Additionally, as \u03c3 varies, the change in test loss also decreases considerably, indicating that the impact of o on filtering performance is diminishing."}, {"title": "VI. CONCLUSION", "content": "In this brief note, we assess the decline in KANs performance for functions affected by noise. We explore two methods to alleviate these problems: the first focuses on noise elimination, and the second on increasing the size of the training dataset. Although the latter method demonstrates considerable enhancement, the overall performance remains deficient due to the excessive amount of required data. Consequently, we conclude that KANs must overcome the challenges presented by noise interference."}]}