{"title": "SPECIALIZED FOUNDATION MODELS STRUGGLE TO BEAT SUPERVISED BASELINES", "authors": ["Zongzhe Xu", "Ritvik Gupta", "Wenduo Cheng", "Alexander Shen", "Junhong Shen", "Ameet Talwalkar", "Mikhail Khodak"], "abstract": "Following its success for vision and text, the \"foundation model\" (FM) paradigm-pretraining large models on massive data, then fine-tuning on target tasks-has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond. Has this achieved what the original FMs accomplished, i.e. the supplanting of traditional supervised learning in their domains? To answer we look at three modalities-genomics, satellite imaging, and time series-with multiple recent FMs and compare them to a standard supervised learning workflow: model development, hyperparameter tuning, and training, all using only data from the target task. Across these three specialized domains, we find that it is consistently possible to train simple supervised models\u2014no more complicated than a lightly modified wide ResNet or UNet-that match or even outperform the latest foundation models. Our work demonstrates that the benefits of large-scale pretraining have yet to be realized in many specialized areas, reinforces the need to compare new FMs to strong, well-tuned baselines, and introduces two new, easy-to-use, open-source, and automated workflows for doing so.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed a shift towards large-scale pretraining across domains like computer vision and natural language processing. This workflow generally consists of two stages: pretraining on vast amounts of domain-specific data to capture general knowledge followed by fine-tuning on target tasks (Radford & Narasimhan, 2018). This pretrain-then-finetune paradigm has been tremendously successful, enabling foundation models (Bommasani et al., 2021) to consistently outcompete traditional supervised learning methods on a wide variety of downstream tasks in the vision and language domains (Dosovitskiy et al., 2021; Liu et al., 2021; Devlin et al., 2019).\nDriven by this success, the foundation model approach has been adapted to various specialized domains, which we define to be ML application areas-e.g. genomics, satellite imaging, and time series-whose data modalities lie outside those of classical AI tasks, i.e. natural images and text. These domains have seen the introduction of many new FMs claiming to leverage large, domain-specific pretraining datasets to achieve breakthrough performance on downstream tasks (Dalla-Torre et al., 2023; Nguyen et al., 2024; Zhou et al., 2023b; Avsec et al., 2021; Ji et al., 2021; Fuller et al., 2023; Cong et al., 2022; Mendieta et al., 2023). These claims underlie our study's motivating question:\nDo these new specialized FMs outperform traditional supervised learning applied to the same tasks?\nAnswering this question is critical because supervised workflows are usually much less expensive to implement and deploy, but FMs that allow for effective transfer learning have the potential to fundamentally transform these domains, as we have seen with language and vision processing in the past decade. However, despite ongoing efforts to promote their fair and comprehensive evaluation (Liang"}, {"title": "2 RELATED WORK", "content": "Foundation models have been trained in numerous specialized domains beyond vision and text, including genomics (Ji et al., 2021), satellite imaging (Cong et al., 2022), time series (Goswami et al., 2024), weather (Bodnar et al., 2024), pathology (Zimmermann et al., 2024), differential equation solving (Sun et al., 2024), web traffic (Zhao et al., 2023), and beyond. To get a representative sense of their success, we focus on domains that combine the following properties: (a) multiple BERT-scale FMs, (b) a standard suite of evaluation tasks, and (c) significant applied interest. These restrictions suggest looking at three domains, all of which have at least five FMs evaluated on at least nine tasks: genomics (which has some of the largest-available non-text FMs (Dalla-Torre et al., 2023)), satellite imaging (which has a large ongoing benchmarking effort (Lacoste et al., 2024)), and time series (which has already seen significant industry interest (Cohen et al., 2024)). The remainder of this section examines how different learning workflows approach problems in these domains."}, {"title": "2.1 SPECIALIZED FOUNDATION MODELS", "content": "Collectively our three target domains have more than twenty-five FMs, many developed via the \"lift-and-shift\" approach-borrowing terminology from Rolf et al. (2024)-in which techniques from core AI areas such as vision and language processing are applied with modest tailoring to specialized domains. In particular, many methods are built on out-of-domain models such as BERT, Swin, and"}, {"title": "2.2 SPECIALIZED BASELINES", "content": "Both of the automated supervised learning pipelines we develop are heavily influenced by successful in-domain model development. In particular, the NAS-based pipeline we use to achieve our results in genomics and satellite imaging is inspired by the success of the human-driven specification of kernel sizes and dilation rates in successful architectures like TCN (Lea et al., 2016) and ConvNeXt (Liu et al., 2022). At the same time, for time series our approach is based upon a well-tuned GPU implementation of perhaps the most basic forecasting model, AR."}, {"title": "2.3 AUTOML FOR SPECIALIZED DOMAINS", "content": "While often evaluated on domains such as vision, automated techniques have long been used in specialized domains as well. An important example is Auto-ARIMA (Hyndman & Khandakar, 2008) for time series, although it has been found to underperform on the specific suite of tasks we consider (Challu et al., 2022). However, to avoid requiring significant expertise in any one domain, we also make use of AutoML methods developed specifically for diverse tasks (Roberts et al., 2021b; Shen et al., 2023), in particular the NAS method DASH (Shen et al., 2022) that can discover good kernel sizes and dilation rates for a CNN backbone faster than it can be trained from scratch."}, {"title": "3 METHODOLOGY", "content": "Recall that our goal is to conduct a robust comparison between traditional supervised learning and specialized FMs; the natural way to do this is to take existing benchmarks used to evaluate FMs in our three target domains and run a typical supervised workflow on the same tasks. As depicted in Figure 2, this pipeline involves three steps: (1) model development, (2) hyperparameter tuning, and (3) training. The first stage involves using both reasoning and trial-and-error to find a good architecture to tune and train on the data; for example, Lea et al. (2016) developed the temporal convolutional network (TCN) architecture with a multi-layer dilation rate pattern specifically suited to sequential data, while Liu et al. (2022) designed the breakthrough ConvNeXt architecture by methodically exploring ways to make CNNs more like Transformers without introducing attention. The second stage (hyperparameter tuning) can also be done via human-driven iteration, but there exist effective automated procedures for it as well (Li et al., 2020). Lastly, the third step of the pipeline involves simply training the selected model with the selected configuration on the data of the target task."}, {"title": "3.1 DASHA: SIMULATING THE SUPERVISED WORKFLOW USING NAS", "content": "To simulate model development we need a search space over architectures that is (a) efficient, (b) flexible, and (c) applicable to the types of high-dimensional unstructured data that arise in domains targeted by specialized FMs; these requirements make CNN-based search spaces a natural choice. In particular, inspired by the success of hand-tuned kernel sizes and dilation rates in traditional model development (Lea et al., 2016; Bai et al., 2018; Liu et al., 2022), we apply DASH (Shen et al., 2022), a NAS method that starts with an existing CNN backbone-e.g. a wide ResNet (Zagoruyko & Komodakis, 2017)\u2014and uses the weight-sharing heuristic (Liu et al., 2018) to determine the right kernel size and dilation rate to use at each convolutional layer. DASH has been successfully used in AutoML competitions (Roberts et al., 2021a) and to advance the state-of-the-art on NAS benchmarks (Tu et al., 2022), making it likely to be useful beyond the domains we consider.\nAs described in Algorithm 1, we augment the existing DASH approach in two ways: (1) trying more than one CNN backbone (e.g. both wide ResNet and UNet (Ronneberger et al., 2015)) and (2) using the well-known hyperparameter tuner ASHA (Li et al., 2020) to configure architecture-specific training settings. This combination gives our workflow its name. Following the NAS and hyperparameter tuning stages, we train the discovered architecture with the selected configuration on the target data. Further details, including the resources given to the three steps of the pipeline and the exact search spaces used by DASH and ASHA, are provided in Appendix B.1. Note that, while our focus is on data-efficient baselines, we do ensure that the entire workflow is never substantially more computationally expensive than fine-tuning an FM."}, {"title": "3.2 AUTO-AR: MAKING A BASELINE STRONGER BY MAKING IT SIMPLER", "content": "While DASHA can be applied to forecasting tasks, it is not competitive with state-of-the-art time series FMs. At the same time, the field of time series forecasting has long employed automated workflows, notably the Auto-ARIMA approach of Hyndman & Khandakar (2008) that uses statistical tests and information criteria to tune ARIMA's lookback and differencing parameters. Auto-ARIMA was evaluated on the time series tasks we consider by Challu et al. (2022), who found that it performed poorly compared to deep learning approaches. However, their implementation does not make use of multi-channel data and tunes up to a lookback window of at most five, which is much less data than used by time series FMs. While tuning ARIMA with larger lookback parameters is computationally costly, we find the following simplified tuning pipeline to be effective:\n1. use the KPSS test (Kwiatkowski et al., 1992) to decide whether to take first differences\n2. use the Bayesian Information Criterion to select the maximum lookback parameter of the auto-regressive (AR) component of ARIMA, ignoring the moving average (MA) part\n3. maximize the multi-channel likelihood of AR with the chosen differencing and lookback\nBy dropping the MA component of the model and running the procedure on GPU, we are able to tune the lookback windows up to the maximum allowable length (usually 512); we find that longer lookbacks are critical for performance. Note that this is just a tuned version of the classic AR model."}, {"title": "4 EMPIRICAL RESULTS", "content": "We now present the results of applying the automated pipelines described in the previous section to our three target domains. For each domain, we provide a brief justification of the specific FMs and evaluation tasks that we consider, followed by details on how we apply our workflows; further information can be found in Appendices A and B. As there are too many separate results to present outside the appendix, in this section we mainly present aggregate statistics that summarize our findings for each domain, with detailed results relegated to Appendix C. The domains have different performance metrics, but they can all be aggregated via the following quantities: average score, average rank, and mean / median percentage improvement over a baseline. For each domain, we define a domain-specific baseline and measure the improvement of FMs and our approach relative to it. This standardizes comparisons across tasks of varying scales."}, {"title": "4.1 GENOMICS", "content": "We begin our investigation in the genomics domain, which has witnessed the development of numerous FMs, including the early Enformer Avsec et al. (2021), the DNABERT series (Ji et al., 2021; Zhou et al., 2023b), the HyenaDNA family (Nguyen et al., 2024), GENA-LM (Fishman et al., 2024), the recent Caduceus family (Schiff et al., 2024), and the NT family (Dalla-Torre et al., 2023); The latter includes models with up to 2.5B parameters. To evaluate them, we consider the Nucleotide Transformer (NT) benchmark of Dalla-Torre et al. (2023), which contains eighteen tasks in three main categories: regulatory elements, RNA production, and histone modification. We use this benchmark because of its diversity and because it has been evaluated on by all of the aforementioned FMs, allowing us to include eight of them in the comparison.\nOur numbers for these models are taken from Dalla-Torre et al. (2023, Supplementary Table 6); Following Dalla-Torre et al. (2023, Supplementary Table 5), We use F1 score and accuracy to evaluate a subset of regulatory elements and RNA production tasks, and we use Matthew's Correlation Coefficient (MCC) as the main metric for evaluation on the remaining datasets."}, {"title": "4.1.1 BASELINES", "content": "CNNs have long been used for genomics tasks (Avsec et al., 2020; Zhou & Troyanskaya, 2015) and so constitute natural supervised baselines; in particular we include 1D variants of Wide ResNet (WRN) and UNet, which we find perform better than some domain-specific CNNs. We use these same two backbones as the candidate CNNs tuned and selected from by our DASHA workflow."}, {"title": "4.1.2 RESULTS", "content": "Our genomics results are displayed in Table 1, which shows that our supervised workflow (DASHA) consistently outperforms all FMs across all aggregate metrics. As discussed in Appendix C, our strong performance is driven in large part by outstanding performance on the histone modification tasks (c.f. Table 9). The more detailed results also highlight the importance of considering diverse baselines, with Wide ResNet usually being the selected architecture but UNet performing significantly better for promoter and splice site classification tasks. Overall, DASHA arguably sets a new state-of-the-art on the NT benchmark and certainly demonstrates that supervised methods remain quite competitive in genomics, despite the availability of massive pretraining datasets."}, {"title": "4.2 SATELLITE IMAGING", "content": "While they do not get as large as those in genomics, numerous BERT-scale FMs have also been introduced for satellite imaging, including SeCo (Manas et al., 2021), the SatMAE family (Cong et al., 2022), the CROMA family (Fuller et al., 2023), GFM (Mendieta et al., 2023), Scale-MAE (Reed et al., 2023), Satlas (Bastani et al., 2023), Prithvi (Jakubik et al., 2023), and SkySense (Guo et al., 2024). Because our evaluation includes GeoBench (Lacoste et al., 2024), a recently introduced satellite benchmark that has not been considered by many of these FMs, we obtain all results using our own fine-tuning; therefore we only consider a restricted subset of top-performing, open-source, and compatibly-formatted models. In all cases we use the fine-tuning workflow suggested by the authors of each FM plus some automated hyperparameter tuning; note that even with the original code and extra tuning our reproductions on previous benchmarks systematically underperformed results reported in the original works. We take our tasks mainly from GeoBench's five classification tasks and then add four additional tasks-BigEarthNet (Sumbul et al., 2019), EuroSAT (Helber et al., 2019), Canadian Cropland (Jacques et al., 2023), and fMoW-Sentinel (Cong et al., 2022)\u2014that are commonly used to evaluate other FMs. As we focus on classification\u2014sometimes with multiple labels we report top-1 accuracy or mAP as appropriate."}, {"title": "4.2.1 BASELINES", "content": "Since satellite imaging resembles RGB imaging, it is common to \"lift-and-shift\" vision models to this domain (Rolf et al., 2024). As a result we use several CNN backbones as baselines and wide ResNet as the candidate architecture for our DASHA workflow. Lastly, we also consider the performance of fine-tuning the ImageNet-pretrained vision FM SwinT-base (Liu et al., 2021)."}, {"title": "4.2.2 RESULTS", "content": "Table 2 shows that our supervised workflow attains the best or second-best performance across all aggregate metrics and is only ever slightly outperformed by CROMA-large. Notably, unlike in genomics, the FMs here consistently outperform CNN backbones, likely because the associated papers compare to them as baselines. However, the frequently superior performance of DASHA suggests that domain-aware model development would yield good supervised models in this field. Another contrast with genomics is that the larger versions of the FMs consistently attain superior performance here, suggesting they are making at least somewhat effective use of the pretraining data. Nevertheless, that this improvement can also be attained by DASHA, which uses no pretraining and produces a model that is ten times smaller, suggests that there remains significant room for improvement."}, {"title": "4.3 TIME SERIES", "content": "Our last domain is time series, which has many FMs, including those that use the standard pretrain-then-fine-tune workflow: GPT4TS (OFA) (Zhou et al., 2023a), LLM4TS (Chang et al., 2023), MOMENT (Goswami et al., 2024), TEST (Sun et al., 2023), S\u00b2IP-LLM (Pan et al., 2024), CALF (Liu et al., 2024), TTM (Ekambaram et al., 2024), and Time-LLM (Jin et al., 2024); and others that evaluate in a zero-shot (ZS) regime: TEMPO (Cao et al., 2024), TimesFM (Das et al., 2024), Moirai (Woo et al., 2024), and Toto (Cohen et al., 2024). As we are comparing to supervised baselines, our evaluation of ZS models will be in a less challenging setting than the one they report numbers for. We study the performance of these FMs and our baselines on the problem of long-horizon forecasting, which has a standard set of tasks (Goswami et al., 2024, Table 11), of which we consider seven. Note that each task consists of four settings corresponding to different time horizons, so in total this yields twenty-eight tasks. Lastly, we compute aggregate metrics using RMSE, not MSE, so that performance scales linearly with prediction error; this choice has no effect on average rank."}, {"title": "4.3.1 BASELINES", "content": "To baseline these FMs we use mainly linear forecasting methods, including the classical (untuned) linear auto-regression (AR), the automated workhorse Auto-ARIMA (Hyndman & Khandakar, 2008), the more recent DLinear (Zeng et al., 2023), and our own workflow Auto-AR described in Section 3.2. Lastly, we also evaluate our other approach, DASHA, on six of the tasks (c.f. Table 14)."}, {"title": "4.3.2 RESULTS", "content": "Table 3 shows that on the full seven-dataset evaluation our Auto-AR workflow always attains competitive performances across all aggregate metrics considered, and in particular attains the best median improvement over Auto-ARIMA. Specifically, our Auto-AR workflow achieves competitive performances with two other recent time series FMs, namely MOMENT and S2IP-LLM, and outperforms the rest of the FMs. Although TTM surpasses all other methods across three aggregated metrics, the improvements remain relatively marginal. This observation aligns with our assertion that the substantial increase in pretraining dataset size and model scale has not yet resulted in significant advancements in model performance. Notably, the three best performing methods are not zero-shot, which is perhaps not surprising given the extra data. However, it does reinforce the intuition that settings with high data availability should prefer supervised methods, including simple ones like AR. Notably, even our untuned implementation of AR that uses no differencing and a large lookback window is quite effective, doing better than ZS FMs across all aggregate metrics and even MOMENT on some of them."}, {"title": "5 DISCUSSION", "content": "At a high level, our results show that the foundation models in these three domains have not yet surpassed supervised learning, and thus more broadly that the latter remains a strong baseline for specialized FMs. This is a surprising and consequential finding due the paradigm's popularity and the data and compute costs associated with large-scale pretraining. In this section we discuss lessons and implications for the development of machine learning in these and other application areas."}, {"title": "5.1 THE IMPORTANCE OF DIVERSE, WELL-TUNED, AND DOMAIN-SPECIFIC BASELINES", "content": "The main lesson of our work is to select a diverse array of baselines, drawing from both \"lift-and-shift\" and domain-specific approaches, and then to carefully tune them. For example, in genomics the vanilla wide ResNet baseline does remarkably well, with the majority of FMs doing worse than even this \"lift-and-shift\u201d baseline on the typical task in the NT benchmark. While satellite FMs do outperform such baselines, lightly modifying these CNNs via different kernel sizes and dilation rates was enough to match state-of-the-art models there as well. Lastly, our time series results demonstrate in dramatic fashion the need to carefully tune domain-specific approaches, as we show that simply allowing the classical AR forecaster to make use of long lookback windows and GPU-based optimization leads better forecasting than all open-source FMs."}, {"title": "5.2 COMPUTATIONAL EFFICIENCY CONSIDERATIONS", "content": "While not our main focus, we nevertheless highlight that any performance gains from FMs must be balanced against their additional cost. In addition to the extensive GPU-hours used for pretraining, the resulting models are often much bigger and so lead to much more costly inference. Indeed, apart from the special case of HyenaDNA, the CNN architectures discovered and trained using our DASHA workflow are typically over ten times smaller than FMs in the case of genomics and three to ten times smaller in the case of satellite imaging. Moreover, for time series our Auto-AR approach is quick-to-train and yields simple models with less than 1K parameters\u2014over two-thousand times smaller than any FM-while attaining performance that is often competitive even with closed-source models. In aggregate, these examples further demonstrate the efficiency of supervised approaches and the resulting high performance bar that FMs need to clear before they can be deemed useful."}, {"title": "5.3 THE POWER OF TUNING KERNEL SIZES AND DILATION RATES", "content": "Our results for genomics and satellite imaging are driven by the DASHA workflow, whose crucial component is the tuning of kernel sizes and dilation rate in CNN backbones such as wide ResNet. Its success demonstrates that the procedure is an effective surrogate for human-driven model development, enabling the automated discovery of the types of diverse, domain-specific baselines stressed in Section 5.1. To understand this further, we study whether the architecture search component selects different kernel sizes and dilation rates for different tasks, and whether it does so in a consistent manner."}, {"title": "5.4 THE SURPRISING EFFECTIVENESS OF LINEAR AUTO-REGRESSION", "content": "Perhaps our most surprising finding is the competitiveness of linear auto-regression (AR), a very old method, on long-horizon forecasting. It is likely that the lack of comparison with this baseline was driven by existing evaluations (e.g. by Challu et al. (2022)) of Auto-ARIMA (Hyndman & Khandakar, 2008), which is perceived to be a stronger baseline because it both combines AR with another model (MA) and tunes the lookback and differencing parameters. However, in most Auto-ARIMA packages the default maximum lookback is around five, whereas we often found much (hundred-fold) larger settings to work best. Since these implementations are also generally too slow to support such long lookbacks, the possibility of expanding the hyperparameter space was more likely to be ignored. By implementing an efficient tuning procedure over a larger space of lookback parameters, our Auto-AR workflow comprises a significant contribution to forecasting baselines."}, {"title": "5.5 LIMITATIONS", "content": "While our findings are significant according to measures set by past work, they should not be misinterpreted to address all possible scenarios where FMs may be useful. This is most salient for time series FMs motivated by zero-shot concerns, a setting we do not study, and to some extent for genomics FMs, which are often used for exploratory science and not supervised learning. We are also of course computationally limited and there are many other domains where FMs have been pretrained, and even in our three there are other tasks beyond classification and forecasting. Nevertheless, our evaluation is extensive-over twenty-five FMs and over fifty tasks and so are at least strongly suggestive of the state of a field that uses benchmark performance to motivate and justify pretraining."}, {"title": "6 CONCLUSION", "content": "We conduct a thorough investigation to evaluate whether the cost of training specialized foundation models across three major domains are justified by their superior performance relative to traditional supervised learning. Our results demonstrate that FMs in these domains have not yet surpassed supervised workflows and are often outperformed by fairly simple methods, including lightly modified CNN backbones (in genomics and satellite imaging) and classical linear forecasters (for time series). As part of our study, we introduce two automated workflows-DASHA for simulating in-domain model development of CNNs and Auto-AR for tuning linear auto-regression on GPUs\u2014that we believe will be useful tools for evaluating future work in these and other areas. The code for these pipelines and to reproduce our results is publicly available."}, {"title": "A TASKS", "content": ""}, {"title": "A.1 GENOMICS", "content": "For the Genomics domain, we use the eighteen classification tasks from the Nucleotide Transformer benchmark (Dalla-Torre et al., 2023) that has widely been used for other genomics FMs. The benchmark datasets consist of nucleotide base sequences ranging from 200 to 600 bases in length. It provides a realistic and biological meaningful benchmark across four main categories: promoter (human/mouse), enhancer (human), splice site (SS; human/multispecies) and histone modification (yeast). Within the benchmark, the enhancers-types and splice_sites_all tasks are classification tasks with three classes each, while the remaining tasks are binary classification tasks."}, {"title": "A.2 SATELLITE IMAGING", "content": "In the satellite imaging domain, we aim to conduct evaluations with real-world relevance to Earth science. To achieve this, we include a variety of data from different sources to cover a diverse range of tasks, such as brick kiln identification, deforestation prediction, and photovoltaic monitoring. We utilize five classification tasks provided by the GeoBench dataset (Lacoste et al., 2024), a recently developed benchmark that offers a clean and carefully curated collection of tasks specifically designed for satellite imaging. In addition to GeoBench, we evaluate our model on three additional datasets (Helber et al., 2019; Jacques et al., 2023; Sumbul et al., 2019) commonly used in the literature as benchmarks for this domain. This brings the total to eight datasets, encompassing a wide range of features. These tasks vary in complexity, with single-class classification ranging from binary to 62-class problems, as well as two multilabel classification tasks. The datasets are further characterized by diverse input channels, ranging from 3 RGB channels to 18 channels that integrate data from both Sentinel-1 and Sentinel-2 formats.\nFor Geo-Bench datasets, we do not use any mixup and cutmix augmentations. For other datasets, we universally use mixup = 0.8,cutmix = 1.0, and a switch probability of 0.5. Following Fuller et al. (2023), we use only 10% of training set from BigEarthNet and fMoW-Sentinel while using the full evaluation set for validation."}, {"title": "A.3 TIME SERIES", "content": "In the time series domain, we focus on the long horizon forecasting task. We use a subset of the common benchmark datasets for evaluating models across different domains (ETT, Electricity, Weather, Illness, Traffic, Exchange Rate) (Wang et al., 2024), specifically, the ETT, Weather, Electricity, Illness (ILI), and Traffic datasets. Note that the ETT dataset is actually a collection of four series: ETTh1, ETTh2, ETTm1, and ETTm2; we follow the rest of the literature in treating each series as a separate dataset. Each dataset contains measurements of one or more channels at evenly spaced time steps."}, {"title": "B IMPLEMENTATION DETAILS", "content": ""}, {"title": "B.1 DASHA", "content": "Following the architecture search, we perform hyperparameter tuning using ASHA. The hyperparameter search space includes learning rate, weight decay, momentum, drop rate, and random seed for model initialization. We define a continuous search space, with further specific details provided in Table 7. Using ASHA, we evaluate 200 sample configurations over a maximum of 20 epochs, using a reduction factor of 2. The low-performing configurations are pruned based on their validation scores.\nBefore retraining the final model, we load the model checkpoint corresponding to the optimal hyperparameter configuration. The model is then trained for 200 epochs on the training data, with the best-performing checkpoint selected based on validation performance. This process is repeated for each backbone architecture, and the best-performing backbone is selected using the validation score. Finally, the checkpoint for the selected backbone is evaluated on the test set to obtain the final score."}, {"title": "B.2 AUTO-AR", "content": "A fairly complete description is provided in Section 3.2. Here we note only that, because we minimize the total maximum likelihood across (independent) channels, to determine the amount of differencing used for each task we run the KPSS test separately on each channel and use the differencing needed by the majority of the channels. Notably, this results in a differencing of one for each task."}, {"title": "C DETAILED RESULTS", "content": ""}, {"title": "C.1 GENOMICS", "content": "We include all FMs listed in Dalla-Torre et al. (2023, Supplementary Table 6) with addition of the two recently released models, Caduceus (Schiff et al., 2024) and Gena-LM (Fishman et al., 2024); note that the last row of tasks in the NT paper(promoter and splice sites) are mislabeled, but we infer an order in combination with previous information obtained from a (now-deleted) Huggingface leaderboard. In alignment with the leaderboard, we apply a 0.1 validation split for DASHA during our evaluation. Additionally, we use an architecture set that includes both Wide ResNet and UNet for the search with DASHA on these datasets. We use batch size= 128 for all datasets, and cross entropy loss for all the training and finetuning. Individual scores for each task in the benchmark are provided in Tables 9 and 8."}, {"title": "C.2 SATELLITE IMAGING", "content": "Training on satellite datasets requires relatively large computational resources due to the high number of channels and the size of the datasets. To ensure a fair comparison, we fine-tuned all the foundation models ourselves by sweeping across a fixed set of base learning rates [5e - 3, 2e - 3, 2e-4, 4-5]. We then calculate the actual learning rate from base learning rate following previous work by $lr = \\frac{base\\_lr \\cdot batch \\_size}{256}$. This approach ensures that approximately the same amount of resources were used as during the DASHA tuning process, allowing for a balanced evaluation of model performance.\nWe closely followed the reported evaluation processes from previous studies on FMs (Cong et al., 2022; Fuller et al., 2023; Mendieta et al., 2023). These models do not employ a validation set for hyperparameter tuning or model selection, and we adhered to this same approach when fine-tuning the FMs. However, for DASHA, since we performed extensive hyperparameter optimization over a large search space, we used a validation set to ensure fair and accurate comparisons between DASHA and the FMs. This is a less favorable setting for DASHA, as it relies on extensive hyperparameter tuning, but we demonstrate that, even under these conditions, DASHA matches the performance of the FMs.\nIt is also important to note that SatMAE only accepts 3-channel and 12-channel inputs, while CROMA is limited to 12-channel inputs. GeoBench, however, includes a wide range of tasks with varying numbers of input channels, ranging from 3 to 18. Despite these differences, we include all datasets in our evaluation because they are valuable benchmarks in the satellite image domain, and it is crucial for FMs in this field to generalize across diverse datasets. For datasets where the input size does not match the model requirements, we pad missing channels with zeros and prune any extra channels. However, to ensure a fair comparison, in addition to reporting the average scores across"}, {"title": "C.3 TIME SERIES", "content": "The long horizon forecasting task for a time series can be summarized as follows: at every timestep t", "1976)": "This model predicts the (scalar) value of a time series at t + 1 as a linear combination of the last L timesteps and a constant, i.e. $\u00ce_{t+1} = \\theta_0 + A_1X_t + A_2X_{t"}]}