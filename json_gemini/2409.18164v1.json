{"title": "Data-Prep-Kit: getting your data ready for LLM application development", "authors": ["David Woods", "Boris Lublinsky", "Alexy Roytman\u2020", "Shivdeep Singh'", "Abdulhamid Adebayos", "Revital Eres\u2020", "Mohammad Nassar\u2020", "Hima Patel'", "Yousaf Shah", "Constantin Adams", "Petros Zerfoss", "Nirmit Desai", "Daiki Tsuzuku", "Takuya Goto", "Michele Dolfi*", "Saptha Surendran'", "Paramesvaran Selvam'", "Sungeun An$$", "Yuan Chi Chang", "Dhiraj Joshi", "Hajar Emami-Goharis", "Xuan-Hong Dang", "Yan Koyfmans", "Shahrokh Daijavad$$"], "abstract": "Data preparation is the first and a very important step towards any Large Language Model (LLM) development. This paper introduces an easy-to-use, extensible, and scale-flexible open-source data preparation toolkit called Data Prep Kit (DPK). DPK is architected and designed to enable users to scale their data preparation to their needs. With DPK they can prepare data on a local machine or effortlessly scale to run on a cluster with thousands of CPU Cores. DPK comes with a highly scalable, yet extensible set of modules that transform natural language and code data. If the user needs additional transforms, they can be easily developed using extensive DPK support for transform creation. These modules can be used independently or pipelined to perform a series of operations. In this paper, we describe DPK architecture and show its performance from a small scale to a very large number of CPUs. The modules from DPK have been used for the preparation of Granite Models [1] [2]. We believe DPK is a valuable contribution to the Al community to easily prepare data to enhance the performance of their LLM models or to fine-tune models with Retrieval-Augmented Generation (RAG).", "sections": [{"title": "I. INTRODUCTION", "content": "Data is the starting point for building any Large Language Model (LLM) application. It is well understood that the quality of a model is heavily influenced by the quality of data [3] [4]. However, data preparation is still seen as one of the most difficult steps in the data and LLM lifecycle. This is primarily due to many factors, one of which is that the quality of the data is hard to assess upfront and hence the processing needs to be done to the data before using it for LLM purposes. Data quality is discovered as part of the data and model lifecycle. Most data scientists start with some common data preparation steps like data ingestion, extraction into a standardized format, and cleaning based on issues that they may be aware of in the data. However, often, these are not sufficient, and hidden data issues persist in the data. These hidden data issues impact the quality of the model and are typically discovered by analyzing the model results and tying them back to the data. This iterative data and model debugging is time-consuming, compute-intensive, cumbersome, and makes the whole development process slow. This problem is amplified by the scale of the data, thereby making manual inspections difficult or even impossible. Moreover, as new use cases emerge for building LLM applications, they bring in new data challenges that may be specific to each use case. Another challenge with industrial LLM projects is that the time and effort needed to take a project from successful proof of concept to product-ready deliverable is very high. The proof-of-concept stage is usually done with small datasets with a focus on validating the technical feasibility and return on investment. However, to take a project from proof of concept to production may require serious software engineering efforts, which may be outside the skills of a data scientist. All these challenges have been observed as part of our interactions with various teams building LLM applications in our organization. To overcome these challenges, we introduce a new open-source toolkit called data-prep-kit, which can be accessed at https://github.com/IBM/data-prep-kit. DPK is designed and architected to achieve the following goals:\n1. The toolkit should provide support for using data preparation modules for various data modalities in a consistent fashion."}, {"title": "II. RELATED WORK", "content": "There are other open-source projects for data processing and preparation of LLM applications, e.g., BigCode [8], DataTrove [9], Dolma [10], Nvidia's NeMo [11], DataComp's DCLM [12], and Unstructured.io [13]. While these projects have mainly focused on the preparation of data in creating GenAI models for natural languages (NLP), programming languages (Code), or both, recently the inverse problem has also been explored, namely, using LLMs in advanced data processing performed during down-stream AI applications like Fine-tuning, Instruction-tuning, and RAG [14] [15]. We will not consider these inverse projects in this paper. Before doing a direct feature-by-feature comparison of DPK with its closest and most relevant projects, namely, BigCode, DataTrove, and Dolma, as shown later in Table 1, we will do a descriptive comparison against a few other projects."}, {"title": "III. TOOLKIT DESIGN", "content": "DPK architecture is designed to enable developers to quickly create new transforms and easily deploy them to process data. It is designed to be data agnostic, and as such can support use cases in natural language and code data modalities. There are three primary blocks that build the architecture of DPK: Data Access, Transformation, and Runtime. We introduce them briefly here and then describe each of them in detail.\nData Access is a core element of the architecture and provides a general-purpose framework for identifying, selecting and reading, and writing data in a supported format. Checkpointing of performed work is supported independent of the Transform and Runtime components. Data Access is used by the Runtime and may be used by a Transform, for example to load a model from storage. Data access is configurable using the command line arguments. Its configuration is independent from the configuration of individual transforms and runtime."}, {"title": "A. Data Access", "content": "Data used for processing can reside on a variety of storage devices, including local or distributed network file systems (NFS), S3-compatible storage, etc. To simplify usage of the multiple data storage mechanisms, the toolkit supports an abstraction layer, implemented using a Python class called DataAccess, that provides standardized data processing- oriented APIs, which are independent of the actual storage. The current implementation supports local file system and S3- compatible storage. Additional data access classes can be easily added to support user-specific storage types. Data Access components provide the ability to identify target data sources and read/write data from identified locations.\nTo enable runtime configuration, a factory for creating DataAccess instances (a Python class called DataAccessFactory), is used and configured using command line arguments. Factory configuration common across all DataAccess implementations includes:\n\u2022 input path containing files to be processed\n\u2022 output path to receive processed files\n\u2022 file extensions to use as input\nDataAccess configurations are customizable. For example, in the case of S3 data access, S3 credentials and endpoints are also configurable. The DataAccessFactory is shared across both the Runtime Orchestration and Transformation Workers"}, {"title": "B. Transformation", "content": "Data transformation motivates the need for a clean and concise yet powerful mechanism for manipulating arbitrary unstructured or structured data. It should support the following:\n1:1 \u2013 a single data object is transformed into a single transformed data object. For example, annotating each row with a model score.\n1: N - a single data object is transformed into multiple data objects. For example, splitting row data into multiple objects sized by row count.\nN:1 \u2013 multiple data objects are aggregated into a single object. For example, joining row data into larger numbers of rows.\nN: M - any number of data objects converted to any number of data objects. For example, sorting data into data objects of a specific type.\nTo meet all these needs, we define a base class, AbstractBinaryTransform, with the following methods:"}, {"title": "C. Runtime", "content": "Runtimes are responsible for establishing one or more transform operating environments, assigning work to them and monitoring and reporting progress. DPK currently includes three different runtime implementations:\n\u2022 Pure Python: runs transforms within Python process. Additionally there is support for Python multiprocessing\n\u2022 Ray: runs transforms in Ray Actors either using local or remote Ray cluster.\n\u2022 Spark: runs transforms using either local or remote Spark cluster.\nThese runtimes allow the user the flexibility to deploy the computation across compute facilities ranging from local single computer to Kubernetes clusters consisting of thousands of nodes. To simplify testing on Kubernetes clusters DPK includes simple Kind cluster, that can be used for testing execution on Kubernetes locally.\nAdditionally, each runtime allows transforms to leverage specific functionality that it can provide, for example, shared classes for Python processes, shared actors and object store for ray, etc. To control these functions each transform can implement runtime support, specific for a given runtime, for example DefaultPythonTransformRuntime for Python, DefaultRayTransformRuntime for Ray and DefaultSparkTransformRuntime for Spark. Transform-specific implementation of these classes allows transform implementers to use such native runtime facilities and pass them as parameters to transform execution. This capability is used by some of the transforms, for example Doc ID, dedup, etc to keep running execution state."}, {"title": "V. AUTOMATION", "content": "To offer a highly scalable and easily executable solution with a history of previous executions, all DPK transformations can be executed using KFP. KFP is a popular platform for building and deploying scalable machine learning (ML) workflows on Kubernetes. It allows for defining, orchestrating, and managing end-to-end ML workflows in a cloud-native way, ensuring that data processing tasks can be efficiently executed, monitored, and scaled.\nThe integration with KFP offers several benefits:\nScalability: KFP runs on Kubernetes, so it can scale to handle large datasets and complex workflows. In the EXPERIMENTAL EVALUATION section, we describe how this solution was scaled to hundreds of nodes with thousands of CPU cores and tens of terabytes of RAM.\nModularity: Our workflows are broken down into reusable shared components, making it easy to build and debug complex pipelines by chaining together individual tasks. For all transforms we use the same shared components: \"Compute execution parameters\", \"Start a Ray cluster\", \"Execute a Ray job\" and finally \"Stop/destroy the Ray cluster. The last step is executed regardless of whether the Ray job execution succeeds or fails.\nHistory and Reproducibility: By maintaining a history of executions, KFP ensures the reproducibility of experiments. Our KFP deployments are integrated with standard logging systems such as Prometheus, Thanos, and Grafana. However, to guarantee independence on these systems, all execution logs and metrics are automatically stored in an S3 object-store.\nVisualization: It offers a UI for monitoring pipeline runs, visualizing results, and troubleshooting issues.\nAs described above DPK, executes each transform in a \"simple\" KubeFlow pipeline. For the cases when we need to execute several transforms, DPK introduces the concept of a \"super\" pipeline, which combines several of these simple (nested) pipelines. Fig. 4 demonstrates the \u201csuper\u201d pipeline for Code preprocessing, where each \u201csuper\u201d pipeline step is a nested \"simple\" pipeline. Both the pipelines and the super pipelines provide a no-code interface to run the data preparation pipelines in production settings."}, {"title": "VI. HOW TO BRING YOUR OWN TRANSFORM", "content": "DPK is intended to be an extensible library allowing the creation of custom transforms that can then be applied to data using one of the runtimes. Here we provide the classic \"hello world\" example to illustrate some of the steps involved in writing a new transform. Although the Data Prep Kit supports the transformation of arbitrary byte arrays, we will focus here on the specialization for transforming PyArrow Table objects, typically read from parquet files.\nAs a reminder, the new transform will add a column containing a hello message defined using command line arguments. This new transform is implemented by providing the two classes HelloTransform and HelloTransformConfiguration."}, {"title": "VII. EXPERIMENTAL EVALUATION", "content": "Scalability is a critical determinant for the suitability of transforms for real-world applications where large datasets and high-performance computing are common. In this section, we present evidence supporting the scalability and resource efficiency of DPK.\nTo this end, we conducted a series of experiments evaluating the performance of individual transforms under various conditions. These experiments encompassed running transforms independently on smaller datasets with limited computational infrastructure and on large-scale clusters with numerous CPU cores. The results indicate that DPK maintains its high level of performance consistently across these diverse settings.\nTo investigate the impact of logic complexity on the performance of transforms, we examine the average throughput of selected transforms executed on a single node. This experiment utilizes a node with 16 CPU cores and 64GB of RAM, allowing us to explore the relationship between complexity and execution in a controlled low-resource environment.\nThe average throughput of the transforms running on a single node expressed in megabytes per second is depicted in Fig. 5. We observe that the intricacy of the transform logic has a direct influence on the time required by each transform. There is a default \"sleep time\" for the noop transform that has resulted in its less-than-expected throughput.\nHaving established the performance of transforms on a single node, we next evaluate the scalability of DPK in a cluster setting. To do this, we conducted a series of small to medium-scale experiments on a cluster of 40 nodes each equipped with 48 CPU cores, and 384GB of RAM. The experiments were designed to test the transforms' performance on varying dataset sizes and increasing node counts. The dataset used for the experiments consists of common crawl snapshots.\nTo evaluate the scalability of the transforms, we examine the impact of three categories of transforms on data processing in DPK. The first category (C1) consists of a basic transform: doc ID. Doc ID annotates existing rows with a unique integer without any data or schema transformation. The second category (C2) includes two transforms that perform file manipulation beyond annotation: resize and ededup. Resize splits or merges files into a target size using buffer manipulation, while ededup finds and eliminates duplicates in an input data corpus. Finally, language identification stands alone in the third category (C3), inferring a model for identification.\nWe evaluate the throughput of the transforms, expressed in units of terabytes per minute as shown in Fig. 6. We further assert that there is a direct relationship between the effectiveness of the transforms in handling increasing volumes of data and the complexity of the transform logic. For instance, the lang ID transform from C3, which inferences the FastText language identification model [20], can lead to lower throughput compared to transforms in lower categories such as the Doc ID transform.\nThe results also indicate that I/O bound transforms (C1) have the least impact on scalability, while demonstrating moderate influence on complex file manipulation transforms (C2), and the most substantial impact on scaling for transforms requiring the most computational resources to infer a model (C3).\nThe findings highlight the importance of considering the impact of transform logic on scaling when using DPK for data processing. By carefully selecting the appropriate transforms and optimizing their use, users can achieve more efficient and effective data processing results."}, {"title": "VIII. CONCLUSION", "content": "This paper presents Data Preparation Kit for LLM applications. DPK is flexible (runs on different platforms), and extensible to add new scalable modules without having deep Ray and/or Spark expertise. It also provides out-of-the-box automation for existing as well as newly added modules. DPK is a very useful toolkit for all the users who aim to prepare data but would like to use already implemented transforms and at the same time be able to easily customize or extend the toolkit to fit their needs.\nDPK comes with modules that users can either leverage independently or use in a pipelined fashion. Automation is designed into the DPK which enables users to easily scale up their workload on clusters through the KFP dashboard without writing any code or worrying about configuring and maintaining any cluster. Moreover, the same automation is inherited by any new modules that the user would like to add to the toolkit. Modules from DPK have been used with automation at scale in preparing data for IBM Granite Models thus we strongly feel that it will prove to be very valuable to the larger LLM data engineering community. Our future plans include the expansion of the DPK capabilities with support for new data modalities, additional scalable runtimes, and new readily usable transforms."}]}