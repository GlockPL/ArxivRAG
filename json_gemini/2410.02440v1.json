{"title": "OPTIMIZING ADAPTIVE ATTACKS AGAINST CONTENT WATERMARKS FOR LANGUAGE MODELS", "authors": ["Abdulrahman Diaa", "Toluwani Aremu", "Nils Lukas"], "abstract": "Large Language Models (LLMs) can be misused to spread online spam and misinformation. Content\nwatermarking deters misuse by hiding a message in model-generated outputs, enabling their detection\nusing a secret watermarking key. Robustness is a core security property, stating that evading detection\nrequires (significant) degradation of the content's quality. Many LLM watermarking methods have\nbeen proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of\nthe watermarking method and can find only suboptimal attacks. We formulate the robustness of LLM\nwatermarking as an objective function and propose preference-based optimization to tune adaptive\nattacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks\nsubstantially outperform non-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks\noptimized against a few known watermarks remain highly effective when tested against other unseen\nwatermarks and (iii) optimization-based attacks are practical and require less than seven GPU hours.\nOur findings underscore the need to test robustness against adaptive attackers.", "sections": [{"title": "Introduction", "content": "Few providers of Large Language Models (LLMs) empower millions of users to generate human-quality text at\nscale, raising concerns about dual use [Barrett et al., 2023]. Untrustworthy users can misuse the provided LLMs to\ngenerate harmful content, such as online spam [Weidinger et al., 2021], misinformation [Chen and Shu, 2024], or to\nfacilitate social engineering attacks [Shoaib et al., 2023]. The ability to detect model-generated text can control these\nrisks [Grinbaum and Adomaitis, 2022].\nContent watermarking enables the detection of model-generated outputs by embedding hidden marks that can be\nextracted with a secret watermarking key. Some LLM providers, such as DeepMind [DeepMind, 2024] and Meta San Ro-\nman et al. [2024], have already deployed watermarking to promote the ethical use of their models. A threat to these\nproviders are users who perturb model-generated text to evade watermark detection while preserving text quality. Such\nundetectable, model-generated text could further erode trust in the authenticity of digital media Federal Register [2023].\nA core security property of watermarking is robustness, which requires that evading detection is only possible by\nsignificantly degrading text quality. Testing robustness requires identifying the most effective attack against a specific\nwatermarking method. However, existing content watermarks for LLMs [Kirchenbauer et al., 2023a, Aaronson and\nKirchner, 2023, Christ et al., 2023, Kuditipudi et al., 2024] test robustness only against non-adaptive attackers, who\nlack knowledge of the watermarking algorithms. This reliance on obscurity makes watermarking vulnerable to adaptive\nattacks [Lukas et al., 2024, Nicks et al., 2024] when information about the watermarking algorithms is leaked.\nWe propose methods to adaptively optimize an attack's parameters against a set of known LLM content watermarking\nalgorithms. Tuning attacks is challenging due to (i) the complexity of optimizing over the discrete textual domain and (ii)\nthe limited computational resources available to attackers. Our work shows that attackers can adaptively tune publicly\navailable LLMs such as Llama2-7b [Touvron et al., 2023] to evade watermark detection from Llama3.1-70b [Dubey\net al., 2024]. Our adaptively tuned attacks outperform any other attack, and they transfer to unseen watermarks in a\nnon-adaptive setting. Our attacker spends less than 7 GPU hours and achieves over 96% evasion rate against any of\nthe four surveyed watermarks with negligible impact on text quality. We will release\u00b9 our source code and models to\nfacilitate further research on robustness against adaptive attackers."}, {"title": "Contributions", "content": "We make the following contributions.\n\u2022 We propose methods to curate preference-based datasets using LLMs, enabling us to adaptively fine-tune\nwatermark evasion attacks against four state-of-the-art language watermarks.\n\u2022 We demonstrate that adaptively fine-tuned paraphrasing models between 0.5-7 billion parameters evade\ndetection from all surveyed watermarks at a negligible impact on text quality. Our attacks outperform all\nsurveyed attacks, including those using models with two orders of magnitude more parameters. Optimization\nrequires less than seven GPU hours.\n\u2022 We test our optimized attacks in the non-adaptive setting against unseen watermarks and demonstrate that\nthey remain more effective than any other surveyed attack. Our results underscore the necessity of including\noptimized, adaptive attacks when testing robustness."}, {"title": "Background", "content": "Large Language Models (LLMs) estimate the probability distribution of the next token over a vocabulary V given a\nsequence of tokens. Autoregressive LLMs predict each subsequent token based on all preceding tokens. Formally, for a\nsequence of tokens $x_1,..., x_n$, an LLM models:\n$P(x_n|x_1,..., x_{n-1}) = \\text{softmax}(f_{\\theta}(x_1,...,x_{n-1}))_n$\nwhere $f_\\theta$ is a neural network with parameters $\\theta$. Optimizing LLMs to maximize a reward function is challenging\nbecause the text is discrete, and the autoregressive generation process prevents direct backpropagation through the\ntoken sampling steps Schulman et al. [2017].\nLLM Content Watermarking hides a message in model-generated content, such as text, that can later be extracted\nwith access to the content using a secret watermarking key. A watermarking method is a set of algorithms (KEYGEN,\nEMBED, VERIFY) formalized as follows [Lukas et al., 2024].\n\u2022 $\\tau \\leftarrow \\text{KEYGEN}(\\theta, \\gamma)$: A randomized function to generate a watermarking key $\\tau$ given secret (i) LLM\nparameters $\\theta$ and (ii) random seeds $\\gamma \\in \\mathbb{R}$.\n\u2022 $\\theta^* \\leftarrow \\text{EMBED}(\\theta, \\tau, m)$: Given a LLM $\\theta$, a watermarking key $\\tau$ and a message $m$, this function returns\nparameters $\\theta^*$ of a watermarked LLM that generates watermarked text.\n\u2022 $\\eta - \\text{VERIFY}(x, \\tau, m)$: Detection requires (i) extracting a message $m'$ from text $x$ using $\\tau$ and (ii) returning\nthe p-value $\\eta$ to reject the null hypothesis that $m$ and $m'$ match by chance.\nA text watermark is a hidden signal in text that can be mapped to a message $m \\in M$ using a secret watermarking\nkey $\\tau$. The key $\\tau$ refers to secret random bits of information used to detect a watermark. A watermark is retained if\nthe verification procedure returns $\\eta < \\rho$, for $\\rho \\in \\mathbb{R}^+$. Let $Q : V^* \\times V^* \\rightarrow \\mathbb{R}$ be a function to measure the quality\nbetween pairs of texts. For model-generated text $x$, the watermarking method is robust if an attacker can only generate\na paraphrased text $x'$ with text quality greater than $\\delta \\in \\mathbb{R}$ that does not retain the watermark with probability at most\n$\\epsilon \\in \\mathbb{R}^+$.\n$Pr [\\text{VERIFY}(x', \\tau,m) \\geq \\rho \\wedge Q(x, x') > \\delta] < \\epsilon$  (1)\nEvasion Attacks. Watermark evasion attacks are classified by the attacker's access to the provider's (i) LLM, (ii)\ndetection algorithm VERIFY that uses the provider's secret watermarking key, and (iii) knowledge of the watermarking\nalgorithms. A black-box attacker can only interact with the provider's LLM through an API, whereas a white-box\nattacker knows the parameters of the provider's LLM. Online attackers can query the provider's VERIFY functionality, as\nopposed to offline attackers who have no such access. Adaptive attackers know the algorithmic descriptions (KEYGEN,\nEMBED, VERIFY) of the provider's watermarking method, while non-adaptive attackers lack this knowledge. Our work\nfocuses on black-box, offline attacks in adaptive and non-adaptive settings.\nSurveyed Watermarking Methods. Following Piet et al. [2023], we evaluate the robustness of four state-of-the-art\nwatermarking methods. The Exp [Aaronson and Kirchner, 2023] method marks text by selecting tokens that maximize\na score combining the conditional probability $P(X_n | X_0...X_{n-1})$ and a pseudorandom value derived from a sliding\nwindow of prior tokens. The Dist-Shift [Kirchenbauer et al., 2023a] method favours tokens from a green list,\nwhich is generated based on pseudorandom values and biases their logits to increase their selection probability. The"}, {"title": "Threat Model", "content": "We consider a provider capable of training LLMs and deploying them to many users via a black-box API, such as\nGoogle with Gemini or OpenAI with ChatGPT. The threat to the provider are untrustworthy users who misuse the\nprovided LLM and generate harmful content without detection.\nProvider's Capabilities and Goals (Deployment) The provider fully controls the LLM and the text generation process,\nincluding the ability to embed a watermark into model-generated text. (Watermark Verification) The provider must be\nable to verify their content watermark in each model-generated text. Their goal is to have an (i) quality-preserving and\n(ii) robust watermark that enables detection of model-generated text at a given, low False Positive Rate (FPR) $\\rho \\in \\mathbb{R}^+$.\nAttacker's Capabilities. (Access Restrictions) We consider a (i) black-box attacker who can only query the provider's\nmodel via an API but is (ii) offline, meaning that they cannot access the provider's VERIFY function. Our focus is on\n(iii) adaptive attackers, who know the provider's watermark method (KEYGEN, EMBED, VERIFY) but do not know\nthe secret inputs used for watermarking, such as the provider's LLM or random seeds. We also evaluate how adaptive\nattacks transfer to the non-adaptive setting against unseen watermarks. (Surrogate Models) A surrogate model is a\nmodel trained for the same task as the provider's model. For example, while ChatGPT3.5's weights are not public, the\nattacker could access the parameters of smaller, publicly released models such as those from the Llama2 [Touvron\net al., 2023] model family from open model sharing platforms. Our attacker can access such open-weight surrogate\nmodels and use them for paraphrasing text. We assume the surrogate model's text quality is inferior to the provided\nmodel; otherwise, there would be no need to use the watermarked model.\nAttacker's Goals. The attacker wants to use the provided, watermarked LLM to generate text (i) without a watermark\nthat (ii) has a high quality. We measure quality using many metrics, including a quality function $Q : V^* \\times V^* \\rightarrow \\mathbb{R}$\nbetween pairs of text when the attacker attempts to evade detection. We require that the provider correctly verifies their\nwatermark with a given p-value threshold $\\eta < \\rho$ for $\\rho \\in \\{0.01, 0.025, 0.05, 0.075, 0.1\\}$."}, {"title": "Conceptual Approach", "content": "We adaptively fine-tune an open-weight paraphraser $\\theta_\\rho$ against given watermarking methods. The attacker lacks\nknowledge of the provider's watermarking key $\\tau \\leftarrow \\text{KEYGEN}(\\theta, \\gamma)$, which depends on the (unknown) parameters $\\theta$ of\nthe provider's LLM and random seed $\\gamma$. Our attacker overcomes this uncertainty by choosing an open-weight surrogate\nmodel $\\theta_s$ to generate so-called surrogate watermarking keys $\\tau'$ and optimizes the expected evasion rate over many\nrandom seeds $\\gamma \\sim \\mathbb{R}$.", "subsections": [{"title": "Robustness as an Objective Function", "content": "Let $P_{\\theta} : V^* \\rightarrow V^*$ denote a paraphrasing function, $H_{\\theta} : V^* \\rightarrow V^*$ is a function to produce model-generated text given\na query $q \\in V^*$ and $Q : V^* \\times V^* \\rightarrow \\mathbb{R}$ measures the similarity between pairs of text. We formulate robustness using\nthe following objective function that we can optimize.\n$\\max_{\\theta_\\rho} \\mathbb{E}_{q \\sim T} \\mathbb{E}_{m' \\sim M}  \\mathbb{E}_{ \\gamma \\sim R} [\\text{VERIFY} (P_{\\theta_\\rho} (r), \\tau', m') + Q (P_{\\theta_\\rho}(r), r)]$\n$ \\theta_s  \\leftarrow \\text{EMBED} (\\theta_s, \\tau', m')$\n$  r \\leftarrow H_{\\theta_s} (q)$\n$\\tau' \\leftarrow \\text{KEYGEN}(\\theta_s, \\gamma)$\n(2)\nEquation (2) finds optimal parameters for the paraphraser $\\theta_\\rho$ by sampling uniformly at random over (i) random seeds\n$\\gamma \\sim \\mathbb{R}$, (ii) messages $m' \\sim M$ and (iii) queries $q \\sim T$. The second expectation is taken over a surrogate watermarking\nkey, using the surrogate model's parameters $\\theta_s$ and the previously sampled random seed $\\gamma$ as input. The surrogate\nmodel, key and message are used to embed a watermark into the surrogate model $\\theta_s$ to generate a watermarked sample\n$r$. The optimization aims to find optimal parameters $\\theta_\\rho$, so the paraphraser has a high probability of generating text\n$r' \\leftarrow P_{\\theta_\\rho} (r)$ that evades watermark detection and preserves text quality compared to $r$.\nOptimization presents multiple challenges. The attacker optimizes over different random seeds $\\gamma$ and a surrogate\nmodel $\\theta_s$ instead of the provider's model $\\theta$, which may affect the attack's effectiveness when testing against the"}]}, {"title": "Preference-based Optimization", "content": "We use reinforcement learning (RL) methods such as Direct Preference Optimization (DPO) [Rafailov et al., 2024]\nto optimize Equation (2). To optimize, DPO requires collecting a dataset of positive and negative examples and\nfine-tunes the paraphraser to increase the probability of generating positive examples while reducing the probability of\ngenerating negative examples. A negative sample retains the watermark and represents a failed attempt at watermark\nevasion. In contrast, positive samples do not retain a watermark and have a high text quality $Q(r,r') > \\delta$ for an\nattacker-chosen $\\delta \\in \\mathbb{R}^+$. To bootstrap optimization, we require the ability to curate positive and negative examples\nusing non-optimized models. We curate triplets $(r, r'_n, r'_p)$ containing a watermarked sample $r$ and two paraphrased\nversions $r'_n, r'_p$ representing the negative and positive examples, respectively. Algorithm 1 implements the dataset\ncuration algorithm."}, {"title": "Preference Dataset Curation", "content": "Algorithm 1 Preference Dataset Curation\nRequire: Surrogate $\\theta_s$, Paraphraser $\\theta_\\rho$, Queries $T$, Messages $M$, Paraphrase Repetition Rate $c$, False Positive Rate\nThreshold $\\rho$, Quality Threshold $\\delta$\n1: $D \\leftarrow \\{\\}$\n\u25b7 The preference dataset\n2: for (KEYGEN, EMBED, VERIFY) $\\in W$ do\n\u25b7 Optimize over known watermarking methods\n3:  for each query $q \\in T$ do\n4:  $m \\sim M$\n5:  $\\tau' \\leftarrow \\text{KEYGEN}(\\theta_s, RND())$\n\u25b7 Generate a surrogate watermarking key\n6:  $\\theta^* \\leftarrow \\text{EMBED}(\\theta_s, \\tau', m)$\n\u25b7 Watermark the surrogate model\n7:  $r \\leftarrow S_{\\theta^*} (q)$\n\u25b7 Generate watermarked text\n8:  if VERIFY($r, \\tau', m$) < $\\rho$ then\n9:  $R^0, R^1 \\leftarrow \\{\\}, \\{\\}$\n10:   for $i \\in [c]$ do\n11:   $r' \\leftarrow P_{\\theta_\\rho} (r)$\n\u25b7 Generate a paraphrased sample\n$b \\leftarrow  \\begin{cases}1& \\text{if VERIFY}(r', \\tau', m) > \\rho \\wedge Q(r, r') \\geq \\delta, \\\\\\0 & \\text{otherwise}.\n\\end{cases}$\n13:   $R^b \\leftarrow R^b \\cup \\{r'\\}$\n14:   for $j \\in [|R^1|]$ do\n$r'_n \\leftarrow \\begin{cases}r \\in R^0& \\text{if } |R^0| \\geq j, \\\\\\r & \\text{otherwise}.\n\\end{cases}$\n\u25b7 Choose a negative sample\n16:   $D \\leftarrow D \\cup \\{(r, r'_n, r'_p)\\}$\n17:  return $D$\nAlgorithm 1 randomly samples from a set of known watermarking methods $W$ (line 2) and from the set of task-specific\nqueries $T$ (line 3). It samples a message $m$ (line 4) and generates a surrogate watermarking key $\\tau'$ to embed a watermark\ninto the surrogate generator (lines 4-5). We generate text $r$ using the watermarked model $\\theta^*$ (line 7) and verify whether\nit retains the watermark (line 8). The paraphrase model $\\theta_\\rho$ generates $c$ paraphrased versions of $r$ that we partition into\npositive and negative samples (lines 10-11). A sample $r'$ is positive ($b = 1$) if it does not retain the watermark and\nhas high text quality $\\geq \\delta$ and negative $r'_n$ ($b = 0$) otherwise. For each positive sample, we select one corresponding\nnegative sample and add the watermarked text and the negative and positive paraphrases to the preference dataset $D\n(lines 15-17)."}, {"title": "Experiments", "content": "We report all runtimes on NVIDIA A100 GPUs accelerated using VLLM [Kwon et al., 2023] for inference and\nDeepSpeed [Microsoft, 2021] for training. Our implementation uses PyTorch and the Transformer Reinforcement\nLearning\u00b2 (TRL) library [von Werra et al., 2020] and we use the open-source repository\u00b3 by Piet et al. [2023] that\nimplements the four surveyed watermarking methods. We test robustness using the hyper-parameters suggested by Piet\net al. [2023]. All LLMs used in our evaluations have been instruction-tuned.", "subsections": [{"title": "Preference Dataset Collection", "content": "For a given watermarked sequence generated by the sur-\nrogate model, the attacker generates $c \\in \\mathbb{N}$ paraphrased\nversions using the non-optimized paraphraser and calcu-\nlates the evasion rate using the surrogate watermarking\nkey (Algorithm 1, lines 9-12). shows the number\nof repetitions $c$ needed to achieve a given evasion rate\nacross four watermarking methods using Llama2-7b as\nboth the surrogate and paraphrasing model. Our attacker\ncan choose the best-of-N paraphrases because they know\nthe surrogate watermarking key to detect a watermark.\nThe attacker cannot choose the best-of-N paraphrases\nagainst the provider's watermarked text, as they lack ac-\ncess to the provider's key.\nIn , we observe that Dist-Shift and Exp are\nmore robust than Inverse and Binary. There is an\ninteresting trade-off: Higher robustness leads to a higher\ncomputational burden for the attacker. For instance, to\ncollect $N = 7000$ preference samples of $T = 512$ tokens\neach, at 1 800 tokens/second, we expect to generate N\npositive examples in approximately 1.5 GPU hours for\nDist-Shift, but only 0.5 GPU hours for Inverse. In\npractice, when including the overhead of evaluating quality and detecting watermarks, we require less than 5 GPU\nhours to curate 7000 samples for Dist-Shift. At current AWS4 rates, an attacker faces negligible costs of less than\nUSD 10$ to curate a preference dataset containing 7000 samples."}, {"title": "Ablation Studies", "content": "In our experiments, we ablate over the following settings."}, {"title": "Experimental Results", "content": "Adaptivity: (Adaptive) The same watermarking method is used for training and testing. (Non-adaptive) The\nattack is tested against unseen watermarking methods.\nTarget Models: We evaluate 2 models used by the provider: Llama2-13b, Llama3-70b\nAttacker's Models: Our attacker matches surrogate and paraphrasing models. We consider Llama2 [Touvron\net al., 2023] and Qwen2.5 [Qwen, 2024] from 0.5b to 7b parameters.\nWatermarking Methods: Exp [Aaronson and Kirchner, 2023], Dist-Shift [Kirchenbauer et al., 2023b],\nInverse [Kuditipudi et al., 2024], Binary [Christ et al., 2023]\nFalse Positive Rates (FPR): We consider thresholds $\\rho \\in \\{0.01, 0.025, 0.05, 0.075, 0.1\\}$ to evaluate the\nattack's effectiveness when the provider tolerates a higher FPR for detection.\nHyper-Parameters: We ablate over multiple hyper-parameters that a provider can choose.\nA watermark has been retained if the null hypothesis that the watermark is not present in the content can be rejected\nwith a given p-value specified by the provider. The evasion rate is calculated as the fraction of watermarked text that\ndoes not retain the watermark after applying the paraphrasing attack. Due to the lack of a gold-standard metric to assess\ntext quality, we measure quality with multiple metrics: LLM-Judge, LLM-CoT and LLM-Compare from [Piet et al.,\n2023], Mauve [Pillutla et al., 2021] and Perplexity (PPL) with Llama3-8B-Instruct. Unless otherwise specified, we\nuse a p-value detection threshold of $\\rho = 0.01$ and report the LLM-Judge metric for text quality. Appendix A.1 contains\ndescriptions of all quality metrics."}]}, {"title": "Discussion", "content": "Effectiveness of Adaptive Attacks. Our work demonstrates that content watermarking methods for large language\nmodels are vulnerable to adaptively optimized attacks. Attackers can adaptively fine-tune relatively small open-weight\nmodels, such as Llama2-7b [Touvron et al., 2023], using less than seven GPU hours to evade watermark detection\nfrom substantially larger and more capable open-source models, such as Llama3.1-70b [Dubey et al., 2024]. Our\nattacks remain effective even in the non-adaptive setting when testing with unseen watermarking methods. Our findings"}, {"title": "Online Attacks", "content": "Our work focuses on offline attacks that do not require access to the provider's watermark detection\nfunctionality. Offline attacks help study the robustness of a watermarking method without any information about the\nspecific secret key generated by the provider. An online attacker can learn information about the provider's secret\nkey through accessing Verify, which reduces the attack's uncertainty and could substantially improve the attack's\neffectiveness further."}, {"title": "Limitations", "content": "Our study also did not focus on evaluating adaptive defences that could be designed against our adaptive\nattacks. Adaptive defences have not yet been explored, and we advocate studying their effectiveness. We focused\nexclusively on text generation tasks and did not explore other domains, such as source code generation or question-\nanswering systems where different text quality metrics may be used to evaluate an attack's success. We did not consider\nthe interplay between watermarking and other defences, such as alignment or content filtering, which could collectively\ncontrol misuse."}, {"title": "Related Work", "content": "We evaluate the robustness of content watermarking [Lukas and Kerschbaum, 2023] methods against black-box,\noffline attackers in the adaptive and non-adaptive settings (see Section 2). A different line of work focuses on model\nwatermarking [Uchida et al., 2017] to protect a model's intellectual property by verifying the watermark with API\naccess to a suspect model. Other watermark evasion attacks [Hu et al., 2024, Kassis and Hengartner, 2024, Lukas et al.,\n2024] focus on the image domain, whereas we focus on LLMs. [Jiang et al., 2023] propose online attacks with access\nto the provider's watermark verification, whereas we focus on a less capable offline attacker who cannot verify whether\ntheir evasion attack was successful. Current attacks are either non-adaptive, such as DIPPER [Krishna et al., 2023] or\nhandcrafted against one watermarking algorithm [Nicks et al., 2024]. We focus on optimizable, adaptive attacks and\nshow that they remain effective in the non-adaptive setting."}, {"title": "Conclusion", "content": "Our work demonstrates the vulnerability of current LLM watermarking methods to adaptive attacks. We propose\npreference-based optimization on open-weight models to undermine the robustness of four watermarking methods and\nfind that relatively small models such as Llama2-7b can be adaptively tuned to evade detection from larger models,\nsuch as Llama3-70b. Our adaptively tuned attacks outperform all existing attacks and can be trained using negligible\ncomputational resources of less than seven GPU hours. Adaptively tuned attacks remain effective in the non-adaptive\nsetting against unseen watermarks. Our findings challenge the security claims of existing watermarking methods and\nsuggest that future defences must consider adaptive attackers when testing robustness."}, {"title": "Quality Metrics", "content": "Ideally, to evaluate the quality of an LLM-generated text, one would need a set of human evaluators, each giving\ntheir own score according to a certain rubric, and then have all the scores aggregated. However, this is impractical to\nachieve, both for the attacker and the defender. Therefore, we employ multiple surrogate metrics from the literature:\nLLM-Judge, LLM-CoT and LLM-Compare from [Piet et al., 2023], Mauve [Pillutla et al., 2021] and Perplexity (PPL)\nwith Llama3-8B-Instruct. Note that all of these are implemented in the MarkMyWords (MMW) [Piet et al., 2023]\nbenchmark utilized for our experiments. All the metrics evaluate a response (whether watermarked, or pertrubed\nsample) against a baseline (either the original prompt, a non-watermarked sample or the model's logit-distribution).\nBelow is a description of each metric, along with an indication of whether higher or lower values are better for that\nmetric."}, {"title": "Dataset Curation", "content": "We generate a synthetic prompt dataset spanning various topics, including reviews, historical summaries, biographies,\nenvironment, science, math, fake news, recipes, travel, social media, arts, social sciences, music, engineering, coding,\nsports, politics, health, and more. The dataset has 1000 prompts and is collected by repeatedly prompting a large\nlanguage model (ChatGPT-4) to generate topic titles. We then wrap combinations of these titles in prompts. We intend\nto release the dataset for the public, but it should be very easy to replicate.\nFor every prompt, we generate watermarked output from all watermarks, then we use that as input to our paraphrasers.\nEach paraphraser is to generate 16 paraphrases for each input. We then filter these paraphrases as per Algorithm 1 to\ncreate the training preference pairs. Larger models have higher quality output and so have a higher yield of successful\nparaphrases. We use the same number of paraphrases for each model, even when it generates different yields."}, {"title": "Attack Description", "content": "Prompting. We use the following prompt to train our paraphraser models. The prompt is adapted from Kirchenbauer\net al. [2023b]. Additionally, we prefill the paraphrase answer with the text [[START OF PARAPHRASE]] to ensure that\nthe model starts generating the paraphrase from the beginning of the response. During dataset curation, training and\ntesting, we set the temperature to 1.0 to diversify the generated paraphrases.\nTraining Hyperparameters We train our paraphraser models using the following hyperparameters: batch size of 32,\nlearning rate of 5 \u00d7 10\u22124, and a maximum sequence length of 512 tokens. We use the AdamW optimizer with a linear\nlearning rate scheduler that warms up the learning rate for the first 20% of the training steps and then linearly decays it\nto zero. We train the models for 1 epoch only to prevent overfitting. We utilize Low-Rank Adaptation (LoRA) [Hu et al.,\n2022] to reduce the number of trainable parameters in the model. We set the rank to 32 and the alpha parameter to 16."}]}