{"title": "TTVD: Towards a Geometric Framework for Test-Time Adaptation Based on Voronoi Diagram", "authors": ["Mingxi Lei", "Chunwei Ma", "Meng Ding", "Yufan Zhou", "Ziyun Huang", "Jinhui Xu"], "abstract": "Deep learning models often struggle with generalization when deploying on real-world data, due to the common distributional shift to the training data. Test-time adaptation (TTA) is an emerging scheme used at inference time to address this issue. In TTA, models are adapted online at the same time when making predictions to test data. Neighbor-based approaches have gained attention recently, where prototype embeddings provide location information to alleviate the feature shift between training and testing data. However, due to their inherit limitation of simplicity, they often struggle to learn useful patterns and encounter performance degradation. To confront this challenge, we study the TTA problem from a geometric point of view. We first reveal that the underlying structure of neighbor-based methods aligns with the Voronoi Diagram, a classical computational geometry model for space partitioning. Building on this observation, we propose the Test-Time adjustment by Voronoi Diagram guidance (TTVD), a novel framework that leverages the benefits of this geometric property. Specifically, we explore two key structures: (I) Cluster-induced Voronoi Diagram (CIVD): This integrates the joint contribution of self-supervision and entropy-based methods to provide richer information. (II) Power Diagram (PD): A generalized version of the Voronoi Diagram that refines partitions by assigning weights to each Voronoi cell. Our experiments under rigid, peer-reviewed settings on CIFAR-10-C, CIFAR-100-C, ImageNet-C, and ImageNet-R shows that TTVD achieves remarkable improvements compared to state-of-the-art methods. Moreover, extensive experimental results also explore the effects of batch size and class imbalance, which are two scenarios commonly encountered in real-world applications. These analyses further validate the robustness and adaptability of our proposed framework.", "sections": [{"title": "1 Introduction", "content": "Deep learning models have demonstrated impressive capabilities across a multitude of recognition tasks, thanks to substantial large datasets, advanced network architectures and computing capability [17, 58, 49, 47, 14, 20]. Nevertheless, they always struggle with generalization when faced with distribution shifts in test data, which is a common"}, {"title": "2 Related Work", "content": "Domain Adaptation. Domain adaptation (DA.) [10, 12, 31] aims to alleviate the performance degradation caused by the distribution discrepancies between training and testing data. Classical approaches involve joint optimization on both source and target domains to enable domain generalization [12, 31]. Source-free domain adaptation (SFDA) [32, 33, 29, 34] is a subset of DA where source data is unavailable during adaptation. This setting has been explored in various studies, including SHOT [32], USFDA [29].SFDA methods can be roughly categorized into self-supervised training [1, 42, 9], neighborhood clustering [56, 55], and adversarial alignments [48, 28].\nTest-time Adaption and its Neighbor-based Methods. Test-Time Adaptation refers to the process of adapting a pre-trained model to distribution shifts encountered during testing, without accessing the original training data. Unlike domain adaptation, which focuses on both source and target domains during training, TTA operates solely at test time, making it more flexible for real-world applications where training data may no longer be available. Many approaches to TTA have focused on neighbor-based methods, which utilize neighborhood information for adaptation. For example, Test-Time Template Adjuster (T3A, [24]) adjusts the classifier by updating the linear layer with pseudo-prototype representations derived from the test data. Similarly, Test-Time Adaptation via Self-Training (TAST, [25]) introduces trainable adaptation modules on top of a frozen feature extractor, while AdaNPC [60] leverages deep nearest neighbor classifiers for adaptation. In addition to these neighbor-based methods, other approaches explore TTA from different perspectives, including self-training [46, 35] and entropy minimization [52, 13, 41, 53]. It is worth noting that these methods are not always mutually exclusive; many TTA techniques combine multiple strategies to improve performance, blending ideas from neighbor-based adaptation with self-training or entropy-based optimization. Some previous algorithms (e.g. SHOT [32]) in DA can also be repurposed and adapted to be used in TTA.\nComputational Geometry for Deep Learning. Although deep learning has achieved remarkable success, the theoretical understanding of DL architectures is still under development. Balestriero [5, 4] establish a connection between convolutional neural network and computational geometry, revealing that elemental layers such as convolution, normalization, pooling, linear layers operate as Power Diagrams. In essence, this implies that a deep network recursively divides the input space into cells. Concurrently, a study [54] presents a geometric analysis of recurrent neural networks (RNNs), showing that RNNs also partition input space. More recently, research [6] unveils that the output of the multi-head attention block, a key unit in the transformer model, is the Minkovsky sum of convex hulls. This insight can subsequently be leveraged to ex-"}, {"title": "3 Methodology", "content": "In this section, we first revisit the general setting of TTA. Then, we introduce the geometric framework based on the Voronoi Diagram and further extend it to two well-established geometric structures, the Power Diagram and the Cluster-induced Voronoi Diagram.\nProblem Setup. Test-time adaptation refers to the process of adapting a pre-trained model to distribution shifts that occur between the training and testing phases, without accessing the original training data or labels during test time. Formally, let Dtrain and Dtest be the training and test distributions, respectively, where Dtest exhibits a shift from Dtrain. The goal of TTA is to adapt the model fo, with parameters \u03b8 learned from Dtrain, using only the unlabeled test data Xtest to improve performance on the shifted distribution. For a K-way classification problem, online test stream of data {xt} \u2208 Xtest are used to update the model \u03b8 as follows at every time step t,\ninfer: yt = f\u03b8t(xt), adapt: \u03b8t+1 = \u03b8t \u2212 \u03b7\u2207L(yt)\nwhere \u1ef9t represents the model's prediction for xt, and L is the user-defined loss function. For example, Tent [52] minimizes the entropy loss L = -\u03a3p(\u1ef9t) log p(\u1ef9t),\nwhile TTT [46] minimizes the self-supervised rotation prediction loss from the auxiliary classifier. Commonly, only the channel-wise affine parameters in normalization layers are updated during TTA, while the rest of the model remains unchanged. This approach ensures computational efficiency, making it suitable for real-time adaptation during testing. For convenience in notation and throughout the following analysis, the parameter set \u03b8 is separated into two components: the feature extractor, denoted as \u03c3, and the classifier, denoted as \u03c8. The time step subscript t is dropped unless otherwise specified."}, {"title": "3.1 Voronoi Diagram: foundational geometric structure for Neighbor-based Test-time Adaptation", "content": "Geometrically, Voronoi Diagram has long been a foundational structure for the analysis of nearest neighbor algorithms. It partitions space based on distances to a set of points as follows,\nDefinition 3.1 (Voronoi Diagram). Let d be the distance function associated with Rl, where l is the dimensionality of feature space. A Voronoi Diagram partitions the space into K disjoint cells \u03a9 = {\u03c91,\u2026\u2026 ,\u03c9K } such that UK1\u03c9r = Rl. Each cell is obtained via \u03c9r = {z \u2208 Rl : r(z) = r}, r \u2208 {1,\u2026\u2026, K}, with\nr(z) = arg min d(z, \u03bc\u03ba),\nk\u2208{1,...,K}\nwhere \u03bc\u03b5 is the center (also referred to as Voronoi site) of k-th cell. VD partitions into the space K disjoint cells, where the boundaries between these cells are determined by the distances that are equidistant from two or more sites. These boundaries form the edges of the Voronoi cells, and they help to define distinct regions around each site. Based on this property, VD can classify feature points by Equation 2, assigning each point to the site that minimizes the distance between them. In TTA, since the training distribution Dtest deviates from Dtrain, feature points may not fall into correct cells (Figure 1). Therefore, at every time step, the adaptation can be formulated based on alignments between feature points and Voronoi cells, with our propsed VD-based loss,\ninfer: \u1ef9k = \u03b2(\u2212d(\u03c3(x), \u03bc\u03ba) + \u03f5; \u03c4), VD loss: LVD(\u1ef9k) = \u03a3\u1ef9k log \u1ef9k\nAlgorithm 1: VD-based Guidance for Test-time Adaptation\nInput: Pretrained feature extractor \u03c30, Voronoi sites \u03bc, test stream {x}t\nOutput: Prediction stream {\u1ef9k}t\nfor each online batch {x}t do\ninfer: \u1ef9k = \u03b2(\u2212d(\u03c3(x), \u03bc\u03ba) + \u03f5; \u03c4) ;\n adapt: \u03b8t+1 = \u03b8t \u2212 \u03b7\u2207LVD(\u1ef9t);\nend\nwhere \u03b2(zj; \u03c4) =\n\u03a3ej/\u03c4\n\u03a3ej/\u03c4\nis a softmax function with temperature scaling factor \u03c4, \u03f5 is the machine epsilon for improving numerical stability in code implementation and\nyk is the predicted soft label of x. The intuition behind this distance-based loss is to encourage feature points to move closer to one of the Voronoi sites. The scaling factor \u03c4 controls the regulation strength towards the sites. When a feature point is sufficiently close to a site, the VD loss is minimized. This formulation can be seamlessly integrated into TTA, as presented in Algorithm 1, forming the basis for more advanced geometric structures that will be introduced later. Commonly, the Voronoi site can be set using the class mean of the training data Xtrain."}, {"title": "3.2 Cluster-induced Voronoi Diagram: Multi-site Influences Mechanism Improves Robustness", "content": "Cluster-induced Voronoi Diagram is a generalization of the ordinary Voronoi Diagram that extends VD from a point-to-point distance-based diagram to a cluster-to-point influence-based structure. While VD has been extensively studied for its exceptional utility in a wide range of analyses, its inherent simplicity can be limiting in certain\nDefinition 3.2 (Cluster-induced Voronoi Diagram [7, 8, 22]). Let C = {C1, ...,CK}\nbe a set of cluster and F(z, Ck) is a pre-defined influence function. A Cluster-induced Voronoi Diagram partitions the space into K disjoint cells \u03a9 = {\u03c91,\u2026\u2026,\u03c9\u03ba} such that UK1\u03c9r = Re. Each cell is obtained via \u03c9r = {z \u2208 Re : r(z) = r}, r \u2208\n{1,\u2026, K}, with r(z) = arg max F(z, Ck), where the influence between z and Ck =\nk\u2208{1,...,K}\n{\u03bc(\u03b1)}\nare commonly defined as\nF (z, Ck) = \u2013 sign(\u03b3) \u03a3(d(\u03bc(\u03b1), z)).\nHere, \u03b1 denotes the item index of the cluster Ck and \u03b3 is a hyperparameter that controls the scale of the influence. Similar to VD, CIVD partitions the space into K disjoint cells, while the boundaries are determined by a cluster of points Ck, given the influence function F (Equation 4). Inspired by this, CIVD shows great promise for robust adaptation through its multi-source influence mechanism, offering greater effectiveness in scenarios where a single-point influence is insufficient. It is particularly well suited for TTA, where only small batches of data are available at each time step. The multi-source framework allows the model to dynamically adapt to the limited information provided, improving its ability to generalize and maintain performance in challenging, real-time settings where traditional methods may struggle to capture the full complexity of the data distribution. Specifically, Ck can be established via self-supervision, benefiting from data augmentation for improved robustness. We utilize rotation augmentation, where images are rotated at 4 different angles Rota \u2208 {0,90, 180, 270} to generate Ck, and each rotation corresponds to a Voronoi site \u03bc\u03b1). This process is performed us- ing self-supervised label augmentation [30]. Similar to Equation 3, the soft label given by CIVD can be calculated from the influence function, incorporating the expanded\nsites \u03bc(\u03b1), enhancing robustness against individual predictions.\nAdditionally for TTA, CIVD expands Voronoi site \u03bc\u03b5 to a cluster of site Ck, inte- grating the approach of self-supervision and entropy minimization. The joint label ~(\u03b1)\nyk avoids the negative transfer since the objective is now unified."}, {"title": "3.3 Power Diagram: Identifying Noisy Samples by Flexible Boundaries", "content": "Laguerre-Voronoi Diagram (a.k.a Power Diagram) is another generalization of the Voronoi Diagram that extends the concept by moving from equally-weighted sites to variably-weighted sites. In traditional VD, each site is treated equally, which may not\nDefinition 3.3 (Power Diagram [2]). Let d be the distance function associated with space Rl, a Power Diagram partitions the space into K disjoint cells \u03a9 = {\u03c91,\u00b7\u00b7\u00b7,\u03c9\u03ba} such that UK1\u03c9r = Rl. Each cell is associated with a weight \u03bdk and is obtained via \u03c9r = {z \u2208 Rl : r(z) = r}, r \u2208 {1,\u2026\u2026, K}, with\nr(z) = arg min d(z, \u03bc\u03ba)2 \u2013 v.\nk\u2208{1,...,K}\nLemma 3.1 ([36, 37]). A logistic regression model parameterized by WKxl and bK partitions the feature space Re into a K-cell Power Diagram with \u03bc\u03b5 = Wkxl and\nv = bk + ||Wkxl||2.\nAn illustration of the Power Diagram is given in Figure 1. By adding weights to the sites, the boundaries of the cells can be shifted in orthogonal directions, allowing for more flexible partitioning. Noted that CIVD and PD are parallel structures, meaning they can be seamlessly integrated. CIVD can be retrofitted to CIPD as follows for further robustness improvements,\nDefinition 3.4 (Cluster-induced Power Diagram). Let C = {C1, . . ., CK } be a set of cluster and F(z, Ck) is a pre-defined influence function. a Cluster-induced Power Dia- gram partitions the space into K disjoint cells \u03a9 = {w1,\u00b7\u00b7\u00b7,\u03c9\u03ba} such that UK_1\u03c9r =\nRe. Each cell is obtained via \u03c9r = {z \u2208 Re : r(z) = r}, r \u2208 {1,\u2026\u2026, K}, with\nr(z) = arg max F(z, Ck), where the influence between z and Ck = {\u03bc\u03b1)}\nare de- fined as\nF (z, Ck) = - sign(\u03b3) \u03a3{d(\u03bc\u03b1), z)2 \u2013 \u03bd2}.\nAs mentioned earlier, noisy samples negatively impact entropy minimization, re- sulting in suboptimal adaptation. Existing methods propose addressing this issue by"}, {"title": "4 Experiments", "content": "In this section, we present a comprehensive evaluation of our method, benchmarking it against other approaches using the peer-reviewed, open-source toolkit TTAB [61], a standardized codebase designed to ensure fair comparisons across methods."}, {"title": "4.1 Experiment Setup", "content": "Dataset. CIFAR-10-C, CIFAR-100-C, and ImageNet-C [19] are benchmark datasets designed to assess model robustness in the presence of various corruptions and shift."}, {"title": "4.2 Experiment Results", "content": "Overall Performance Comparison. TTVD demonstrates the best overall performance across multiple datasets. Even under rigid grid-search tuning, our method consistently achieves the lowest classification error and ECE, reducing classification errors by 0.8%, 0.7%, 1.6%, 0.7% on the four datasets, respectively, and ECE by 3.4%, 1.8%, 4.1% and 4.3%, demonstrating its trustworthiness.\nEffect of Components in TTVD. We ablate our methods by gradually downgrad- ing CIPD to the very basic VD. From Table 2, the performance of VD already surpasses that of other neighbor-based methods. When generalizing VD to CIVD, we observe a significant improvement of 5.7% overall for all corruption types. across all corruption types. To investigate the reason behind this, we conducted a sample-level analysis in Appendix A.1, which demonstrates that the multi-influence structure of CIVD en- hances its robustness. Finally, CIPD, with its flexible boundaries and noise filtering mechanisms, further improves upon CIVD by an additional 2.2%, showcasing its su-"}, {"title": "5 Conclusion", "content": "In this paper, we revisit the Test-Time Adaptation problem from a geometric perspective, formulating it using the Voronoi Diagram\u2014a classical and powerful structure in computational geometry known for its elegant mathematical properties. Building on the foundation of guiding TTA with traditional Voronoi Diagram, we extend the approach to more advanced geometric structures, namely the Cluster-induced Voronoi Diagram and the Power Diagram. These structures offer enhanced flexibility and robustness, making them particularly well-suited for TTA. Our experiments demonstrate the effectiveness of our proposed method, TTVD, across a variety of datasets and scenarios, highlighting its capacity to adapt to diverse challenges in real-world settings."}, {"title": "A Appendix", "content": "A.1 Sample analysis\nIn Section 4.2, experimental results indicate that CIVD contributes the most to the improvement. To understand the reason behind this, we investigate how misclassified samples are corrected after CIVD is employed. We arbitrarily inspect three examples from the \"bike\", \"bus\", and \"clock\" class in Figure 5, Figure 6 and Figure 7, respectively. The distances between the feature points and all Voronoi sites are shown.\n\u2022 The \"bike\" example originally is misclassified as \"lobster\" in an individual VD. However, the 90-degree rotated image is correctly classified. When the CIVD applies the influence function to aggregate the information of all four rotations, the model eventually gets the correct prediction.\n\u2022 In the \"bus\" example, all four rotated images are misclassified as various classes, such as \"bowl\", \"table\" or \"house\". However, the distances to the ground-true label are all relatively small. CIVD aggregates these distances and makes the correct prediction. The \u201cclock\u201d example shows a similar phenomenon.\nIn conclusion, this sample analysis reveals that the expanded Voronoi sites and rotated image set contribute to the improvement in CIVD."}, {"title": "F Extended introduction to compared methods", "content": "T3A is a method designed to improve domain generalization by adjusting models during the test phase without requiring backpropagation or changes to the feature extractor. T3A creates pseudo-prototypes from online, unlabeled test data and adjusts the classifier by measuring the distance between test samples and these prototypes. By focusing only on the classifier's linear layer, T3A is lightweight and efficient, enhancing model performance on unseen domains while avoiding the risks of complex optimization processes.\nTAST introduces trainable adaptation modules on top of a frozen feature extractor and generates pseudo-labels for test data using nearest neighbor information. This method improves upon existing TTA techniques by ensuring more robust adaptation in scenarios where test-time domain shifts occur.\nBN_Adapt explores how deep learning models can become more robust to common image corruptions like blur and noise. The authors highlight that in many real-world applications, models can adapt to recurring corruptions using unsupervised methods. By modifying batch normalization statistics during inference, the paper demonstrates that adapting to corrupted data significantly boosts model robustness, surpassing baseline performance across several benchmarks. This simple yet effective strategy improves the performance of models on corrupted image datasets.\nSHOT addresses unsupervised domain adaptation (UDA) without requiring access to source data, a key limitation in existing UDA methods. SHOT leverages a pre-trained source model and transfers its knowledge to the target domain by freezing the classifier module (source hypothesis) and adapting the feature extraction module for the target domain using self-supervised learning and information maximization.\nTTT involves updating the model at test time using a self-supervised learning task on each individual test sample before making a prediction. By using tasks like image rotation prediction as the auxiliary self-supervised task, TTT allows the model to adapt better to the test distribution.\nTENT Entropy minimization in the TENT method works by reducing the uncertainty of a model's predictions during test-time. This is done by minimizing the entropy, or uncertainty, of the predicted probability distribution. Specifically, TENT updates the model's parameters-focusing on the affine transformations in normalization layers-based on the gradient of the entropy with respect to these parameters. By iteratively adjusting the model in response to test data, TENT improves the model's confidence in its predictions without needing labeled data, resulting in better adaptation to new or corrupted data at test time.\nNOTE aims to address challenges in adapting models to non-i.i.d. test data streams, common in real-world applications like autonomous driving. NOTE includes two"}, {"title": "G Experiments Compute Resources", "content": "All experiments are conducted using GPU NVIDIA RTX A6000."}, {"title": "H Algorithms", "content": "Algorithm 2: CIVD Guidance for Test-time Adaptation\nInput: Pretrained feature extractor \u03c3\u03bf, a set of Voronoi sites C, test stream\n{x}t\nOutput: Prediction stream {k}t\nfor each online batch {x}t do\ninfer: \u1ef9k = \u03b2(\u2212F (z,Ck) + \u03f5; \u03c4) ;\n adapt: \u03b8t+1 = \u03b8t \u2212 \u03b7\u2207LVD(\u1ef9t);\nend\nAlgorithm 3: CIPD Guidance for Test-time Adaptation\nInput: Pretrained feature extractor \u03c30, a set of Voronoi sites C, weights of\nVoronoi sites \u03bd, test stream {x}t\nOutput: Prediction stream {\u1ef9k}t\nfor each online batch {x}t do\ninfer: \u1ef9k = \u03b2(\u2212F (z,Ck) + \u03f5; \u03c4) ;\n adapt: \u03c3t+1 = ot \u2212 \u03b7\u2207LVD(\u1ef9t);\nend"}, {"title": "E Extended introduction to Voronoi Diagram", "content": "Voronoi diagrams are a fundamental tool in computational geometry that partition a given space into regions. The origins of Voronoi diagrams can be traced back to 1644, when philosopher Ren\u00e9 Descartes first considered similar ideas. However, they are named after Russian mathematician Georgy Voronoi, who formally defined and studied them in 1908. Voronoi's work [50, 51] extended earlier studies on quadratic forms and lattice structures, laying the mathematical groundwork for partitioning spaces into convex regions, now termed Voronoi cells. In a Voronoi diagram, space is divided into regions such that each region contains all points closer to a given site, or a point, than to any other site.\nOver the years, Voronoi diagrams have been used to solve problems in various domains due to their ability to model spatial relationships and proximity. In computer science, they are employed in tasks such as nearest neighbor search, mesh generation, and image processing. In physics, Voronoi diagrams help in modeling the behavior of particle systems and simulating crystallization processes. In biology, they are used to understand the structure of cells and tissues, where natural divisions often resemble Voronoi partitions. In urban planning, Voronoi diagrams assist in the allocation of resources, such as determining optimal locations for services like hospitals or fire stations, where regions of influence need to be defined based on proximity.\nTheir versatility comes from the diagram's intrinsic ability to partition space in an efficient and meaningful way, especially when dealing with problems that involve spa-"}, {"title": "D Hyperparameter settings in the Experiment", "content": "We follow the TTAB codebase to to grid search the learning rate from {0.005, 0.001, 0.0005} for CIFAR dataset and {0.001, 0.0005, 0.0001} for ImageNet dataset. We set \u03b3 = -0.8 to scale and reduce the influence of distant Voronoi sites. We use \u03c4 = 1 as the standard temperature for the softmax function. For model pretraining, we follow the recipe of ResNet50-Weights. IMAGENET1K-V1 from the torchvision library to train the feature extractor. The batch size is set to 64, aligning to previous studies for fair comparison."}]}