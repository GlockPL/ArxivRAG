{"title": "Training-free Composite Scene Generation for Layout-to-Image Synthesis", "authors": ["Jiaqi Liu", "Tao Huang", "Chang Xu"], "abstract": "Recent breakthroughs in text-to-image diffusion models have significantly advanced the generation of high-fidelity, photo-realistic images from textual descriptions. Yet, these models often struggle with interpreting spatial arrangements from text, hindering their ability to produce images with precise spatial configurations. To bridge this gap, layout-to-image generation has emerged as a promising direction. However, training-based approaches are limited by the need for extensively annotated datasets, leading to high data acquisition costs and a constrained conceptual scope. Conversely, training-free methods face challenges in accurately locating and generating semantically similar objects within complex compositions. This paper introduces a novel training-free approach designed to overcome adversarial semantic intersections during the diffusion conditioning phase. By refining intra-token loss with selective sampling and enhancing the diffusion process with attention redistribution, we propose two innovative constraints: 1) an inter-token constraint that resolves token conflicts to ensure accurate concept synthesis; and 2) a self-attention constraint that improves pixel-to-pixel relationships. Our evaluations confirm the effectiveness of leveraging layout information for guiding the diffusion process, generating content-rich images with enhanced fidelity and complexity.", "sections": [{"title": "1 Introduction", "content": "Substantial advancements have been achieved in large-scale text-to-image generative models [31-35,37], which are now capable of converting complex text descriptions into visually compelling images with impressive accuracy and creativity. Despite these advancements, current models still struggle to comprehend layout descriptions contained within texts and to precisely outline and render detailed images. This is largely due to limitations in model capacity and the quality of training datasets. Consequently, in tasks such as art creation [10] and industrial design [28], where elements need to be precisely positioned, the models' intrinsic inability to comprehend user-defined spatial information limits their applicability.\nRecent advancements in layout-to-image approaches have shown significant progress in achieving more controllable generation, employing both training-based [2, 6, 14, 23, 25, 43, 44] and training-free [3,8,22,42] methodologies to transform detailed layouts into vivid, realistic images. While training-based methods have demonstrated promise, their effectiveness heavily relies on the availability of comprehensive and varied layout datasets. Assembling, annotating, and pre-processing these datasets for effective concept learning is challenging due to the high costs and resource-intensive nature of data acquisition, posing a substantial barrier for many research initiatives. In contrast, training-free methods utilize the intrinsic generative capabilities of models to dynamically guide the diffusion process, offering a significant efficiency advantage. As text-to-image models become more popular, there is an increasing demand for content generation that involves complex compositions of multiple objects and backgrounds. Existing layout-to-image approaches, which primarily focus on content positioning, often struggle with conflicts arising from intersecting similar concepts, especially when generating scenes with multiple elements. Moreover, the critical role of self-attention mechanisms in refining generated content with accurate textural details during the later stages of diffusion, as highlighted in [2], is often overlooked. This oversight highlights a gap in current methodologies, which may fail to fully leverage the potential of diffusion models for producing detailed and contextually coherent images.\nThis study aims to generate composite scenes featuring multiple objects and backgrounds, utilizing bounding boxes for layout information. We propose enhancing the training-free backward guidance concept [7,8] with a novel selective sampling strategy. This strategy introduces a dropout mechanism that prioritizes attentions closely aligned with the current generation concept during the calculation of intra-token constraints. Such an approach not only improves content positioning accuracy but also ensures broader coverage of the targeted area, addressing key challenges in layout-to-image generation.\nOur approach extends beyond individual token cross-attentions by implementing an inter-token constraint, evaluating attentions across tokens within a targeted region to ensure the prioritization of the correct concept. This technique aims to counteract semantic intersection\u2014where overlapping concepts produce irregular textures and shape inaccuracies thereby enhancing generation precision and reliability. Recognizing adversarial intersections' impact on pixel relationships, we employ a self-attention constraint for collective self-attention adjustment across the target region, critical in the later diffusion stages for maintaining coherent pixel interactions. Additionally, an attention redistribution method during forward diffusion corrects misaligned attentions, addressing refinement limitations and reducing semantic intersection effects, thereby improving overall generation accuracy.\nOur comprehensive experimental evaluations demonstrate that our method significantly outperforms existing training-free layout-to-image generation approaches in terms of content localization accuracy and semantic fidelity. These advancements are vividly illustrated in the examples presented in Figure 1, showcasing our method's ability to maintain higher semantic correctness while accurately positioning content within the generated images."}, {"title": "2 Related Work", "content": "2.1 Text-to-image generative models\nRecent advancements in text-to-image generative models, exemplified by Stable Diffusion [33], DALLE-3 [37], and Imagen [35], represent a significant leap forward beyond previous dominant techniques like generative adversarial networks [4,15,16,21]. These new models excel not only in image generation but also in enhancing the performance of tasks such as classification [1,20], action segmentation [12,27], and more. These models enable the generation of highly detailed visual content directly from textual descriptions, distinguishing themselves by producing contextually relevant and aesthetically pleasing images. Their ability to translate complex textual prompts into visual artworks demonstrates a remarkable proficiency, unlocking new possibilities across various applications, from digital art creation [10] to diverse content generation.\nHowever, these models heavily rely on large-scale datasets (stable diffusion is trained using billions of images from LAION-5B [36]), which poses limitations in extending their capabilities to cover new tasks with learned concepts without incorporating additional task-specific datasets or extensive training. As previous studies have showcased the feasibility of integrating various novel tasks in a plug-and-play manner, including image editing [5,17,29,39], generation enhancement [7,24], layout-to-image [3,8,22,42], and more. Encouraged by these findings, we are motivated to delve deeper into whether the boundaries of layout-to-image tasks can be expanded further, enabling the model to handle scenarios involving multiple objects and backgrounds.\n2.2 Layout-to-image generation\nThe reliance on purely linguistic methods limits a model's ability to decode specific layout details accurately. Various studies [2, 6, 14, 23, 25, 43, 44] have shown that models can be further trained with layout information to facilitate layout-to-image generation tasks. Nonetheless, these approaches also highlight the challenge of requiring datasets that pair image, text, and layout information-resources that are scarce and costly to compile. Some innovative strategies have aimed to integrate layout details without additional model training. For instance, MultiDiffusion [3] involves denoising and combining regions with corresponding text descriptions, while DenseDiff [22] directly modifies the attention probabilities to enhance focus on the targeted regions. BoxDiff [42] and Layout-control [8] both employs generative semantic nursing [7], which optimizing latents based on cross-attentions to achieve desired layouts.\nOur approach adopts the backward guidance framework utilized by BoxDiff [42] and Layout-control [8]. However, while these existing methods are efficient for straightforward situations, their method narrowly focus on cross-attentions for individual tokens, overlooking the potential for semantic overlaps as the layout becomes more complex with the addition of multiple objects. This oversight can lead to the undesirable blending of features and disrupt the integrity of pixel relationships, which may prevent the accurate generation of targeted objects. Unlike these methods, our strategy takes a comprehensive perspective on the optimization process, integrating considerations of intra-token, inter-token cross attentions, and self-attentions. This broader approach demonstrates that a training-free method can not only adhere more closely to the intended layout but also enhance the quality of the generated images."}, {"title": "3 Preliminaries", "content": "3.1 Stable diffusion\nDiffusion models [9,11,18,38], operates by gradually transforming a random noise distribution into a coherent image, guided by the semantics of the input text. This process involves a series of forward and backward steps, where the model initially adds noise \u20ac to an image I and then learns to recover the original image from noise, conditioned on textual descriptions c. Latent diffusion model [33] operates in latent space where for a given image I, it is first encoded as latent z by an encoder E, and then reconstructed by a decoder D as \u00ce = D(z) = D(E(I)) after the denoising process.\nDuring the training stage of diffusion process, at given timestep t, with latent zt and condition c, the denoiser e\u03b8 learns to correctly predict the added noise \u20ac through mean square error:\n$\\mathcal{L}_{diff} = \\mathbb{E}_{I \\sim \\mathbb{E}(I), \\epsilon \\sim \\mathcal{N}(0,1),c,t} [||\\epsilon - \\epsilon_{\\theta}(z_t, c, t)||^2].$\nDuring the inference stage, the denoising process gradually removes noise from a randomly sampled zy with the noises predicted by diffusion model with condition c, and guide the diffusion process through classifier [11] or classifier-free guidance [19].\n3.2 Attention mechanism\nThe attention mechanism forms the cornerstone of transformer models [40], playing a pivotal role in natural language processing (NLP) tasks. This mechanism was subsequently integrated into the vision transformer architecture [13], thereby extending its applicability to tasks within the realm of computer vision. Within the framework of stable diffusion models, the intermediate features are processed through both self-attention and cross-attention layers. This processing facilitates the generation of an attention map at timestep t as follows:\n$A_t = softmax(\\frac{QK^T}{\\sqrt{d}}).$\nIn this process, Q and K represent queries and keys, respectively, with d denoting the dimensionality of these query and key features. For cross-attention mechanisms, K is derived from the projection of text embeddings. These embeddings result from encoding the text conditioning c into a latent space, achieved through the use of a CLIP text encoder [30]. Conversely, Q corresponds to the projection of intermediate features sourced from a U-Net architecture. Within the self-attention framework, both Q and K are obtained from the projections of these intermediate features."}, {"title": "4 Methods", "content": "In this section, we explore the nuances of generating composite scenes. We begin in Section 4.1 by detailing our novel approach to attention selection, including the definition of mask representations and the application of our selective sampling strategy. This section also explains the development and role of intra- and inter-token constraints in influencing the diffusion process. In Section 4.2, we shift focus to our innovative handling of self-attentions, illustrating their pivotal role in refining the diffusion process for enhanced image generation. Finally, Section 4.3 introduces our attention redistribution technique, employed during forward diffusion to further optimize generation outcomes through strategic constraint refinement and application.\n4.1 Cross-attention constraints\nTo formulate constraints on cross-attentions, we begin by detailing the acquisition of attentions and layout information for constraint application. Leveraging latent noise manipulation is pivotal in aligning image-text attentions within designated spatial regions. Drawing on recent research [7,42] which suggests that attentions at smaller scales capture semantic information more effectively during early diffusion phases, we employ averaged cross-attentions from user-defined attending objects at the 16 \u00d7 16 up-scaling block as our manipulation references. Formally, for N attending tokens, we define A = {Ai}i=1N, Ai \u2208 RL\u00d7L as the set of cross-attentions relative to the tokens, with L indicating the number of image tokens in cross-attentions.\nFor representing layout information, we employ bounding boxes B = {bi}i=1N, which delineate the top-left and bottom-right corner coordinates for each object. Correspondingly, each bounding box bi is transformed into a binary mask mi, with pixels inside the box region marked as 1 and those outside as 0.\nThe mask is leveraged in our constraints to discriminate foreground and background pixels of the generation of each object. To mitigate the potential disruption of latent noise's natural Gaussian distribution by gradient-based backpropagation, while ensuring extensive attention coverage within the target region, we employ a selective sampling strategy F(mi) for each mask mi. Specifically, we select the top K elements within the mask region for focused attention. To avoid concentrating excessively on a limited number of elements, we randomly retain M elements from those selected.\nIntra-token constraint: For each attending token, we use its corresponding mask mi and cross-attention probabilities Ai to compute our intra-token attention regularization loss. To make sure that the object is only generated in its own target region (i.e., inside the bounding box), we encourage the cross-attention values associated with the target region to exceed those of the non-target region. Following [8], we define the intra-token constraint as:\n$\\mathcal{L}_{intra} = \\sum_{i=1}^{N} \\mathcal{L}_{intra}^{i}$\nwith $\\mathcal{L}_{intra} = (1 - \\frac{\\sum_{j=1}^{L} (m_i \\cdot A_i)_j}{\\sum_{j=1}^{L} (m_i \\cdot A_i)_j + \\sum_{j=1}^{L} (\\bar{m_i} \\cdot A_i)_j})^2$,\nwhere $\\bar{m_i} = F(m_i)$ and $\\bar{m_i} = F(1 - m_i)$.\nThe constraint is designed to increase the proportion of attention values within the target region relative to those outside, enhancing focus on the intended area. By applying selective sampling to pivotal values, we address the issue of excessive attention concentration in confined areas. Furthermore, selective sampling from both inside and outside regions ensures a more equitable representation of high values, mitigating bias in the aggregation process and enhancing the fairness of their contribution to the loss function.\nInter-token constraint: The semantic intersection due to the conceptual similarity (e.g. a bear and a tiger shares considerable similarity in textual latent space), under the context of limited latent space, inevitably intertwine their cross-attentions and cause a degree of spatial intersection. Aiming to completely eliminate such adversarial effects may not be plausible due to the inherent dependence between image intermediate features are extracted from the same latent variable and difficulties to manipulate textual latent space without altering its original semantic meanings. Thus, we employ a conservative approach where we encourage attentions inside corresponding region not to be exclusive, but surpass over other tokens' attentions at the same location. To accomplish this, for a given token i, we first obtain the maximum attentions within target region for other tokens:\n$(\\mathbf{m_i} \\cdot A_j)^{max} = \\max_{j=1,j \\ne i}^{N} (\\mathbf{m_i} \\cdot A_j).$\nFollowing, we calculate the difference d between such attentions and attentions of target region of given token with a margin g.\nd = \u2211((mi \u00b7 Ai)k \u2212 g) \u2212 \u2211(mi \u00b7 Aj)max.\nTo achieve our goal of making the given token more prominent than others within the targeted region, we establish an inter-token constraint defined as follows:\n$\\mathcal{L}_{inter} = \\begin{cases} 0, d > 0 \\\\ d^2, d < 0 \\end{cases} , \\mathcal{L}_{inter} = \\sum_{i=1}^{N} c_{inter}^i$\nThe constraint on inter-token relations, utilizing the same elements from a given mi, focuses on a side-by-side comparison of cross-attention values with other tokens at identical spatial locations. This optimization is particularly aimed at scenarios involving intersections, encompassing both the semantic similarities previously discussed and intersections of user-specified regions. The goal is to promote the dominance of a single token's content within a specific area to ensure the generated image is both clear and coherent.\n4.2 Self-attention constraints\nThe impact of cross-attention is critical in the initial diffusion stages, guiding the structural formation of content via textual cues. However, as [2] highlights, self-attention gains prominence in later diffusion stages, influencing object textures. An unintended effect of semantic intersection can misguide self-attention, causing inappropriate pixel correlations during early stages. To mitigate this, we propose to align self-attention within target regions in early diffusion stages, improving local coherence and maintaining necessary external interactions.\nFollowing the same setup as section 4.1, we obtain averaged self-attentions S \u2208 RL\u00d7L at 16 \u00d7 16 up-scaling blocks. Given mask mi, we aggregated self-attentions which lies within the mask region:\n$S_i = \\sum_{j=1}^{L} (m_i \\cdot S)_j,$\nwhere Si \u2208 R^o.\nSimilar to the intra-token constraint, the self-attention constraint is defined as:\n$\\mathcal{L}_{self} = \\sum_{i=1}^{N} c_{self}^i$\nwith $\\text{Cself} = (1 - \\frac{\\sum_{j=1}^{L} (\\mathbf{m_i} \\cdot S_{ij}}{\\sum_{j=1}^{L} (\\bar{\\mathbf{m_i}} \\cdot S_{i})_j + \\sum_{j=1}^{L} (\\mathbf{m_i} \\cdot S_{ij})^2,$\nwhere m\u00afi = F(mi) and m\u00afi = F(1 \u2013 mi).\nThe self-attention constraint is based on the assumption that pixels within an object have stronger connections to other pixels associated with the same object. Therefore, through selective sampling of averaged self-attention scores, we reinforce these connections by emphasizing self-attentions that are most relevant to the object. This method also leverages the principle that total self-attention probabilities equal to 1 along each channel, meaning while enhancing connections between pixels linked to the object, less relevant pixels are diverted outside the target area, thereby improving the object's coherence with its surroundings.\n4.3 In-generation diffusion guidance\nGiven that the limited latent space can cause semantic-level overlaps, this results in unresolved spatial overlaps even with sufficient refinement steps. These errors accumulate during diffusion steps, potentially leading the generation results in incorrect directions. To address this, we introduce attention redistribution, a technique that reallocates cross-attentions with corresponding tokens during the diffusion process. For each token, its cross-attention is defined as the aggregated cross-attentions across all attending tokens within the bounding box area. After reallocation, we apply max normalization to all cross-attentions. The attention redistribution is defined as follows:\n$A_i = \\mathbf{m_i} \\cdot \\sum_{j=1}^{N} (A_j)$\nThe generative process is divided into refinement and guidance stages. During each guidance step, the latent variable is updated TR times (the number of refinement steps per timestep). This process lasts for TD steps (the total number of timesteps during the diffusion process with such recurrent updates).\nFor each refinement step, the overall constraint is defined as:\n$\\mathcal{L} = \\mathcal{L}_{intra} + \\mathcal{L}_{inter} + \\mathcal{L}_{self},$\nand the latent is updated with a linear decay factor \u03b7t as:\n$z' = z_t - \\eta_t \\cdot \\nabla_{z_t} \\mathcal{L}.$\nThrough this approach, at each step of the diffusion process, we refine the latent to achieve more focused attention on the targeted regions. This refinement process helps to resolve conflicts among tokens and aligns self-attentions, thereby improving both internal and external connections within the generated scenes."}, {"title": "5 Experiments", "content": "5.1 Experimental setup\nEvaluation: In our quantitative analysis using the COCO 2014 dataset [26], we adopt a methodology akin to [3], filtering the dataset to include images with n distinct objects where each occupies at least 5% of the image area. We further refine the selection by excluding objects described by more than two words, as well as \"person\", resulting in a final set of 61 object classes. For performance evaluation, we employ YOLOv7 [41] for object detection, utilizing metrics such as YOLO-score (AP, AP50) [25] to assess our method's effectiveness of locating and correctly generating objects. Object detection is conducted across all 80 COCO classes to ensure thoroughness. Additionally, we generate randomly sample and generate approximately 1,000 samples for subsets of the COCO 2014 dataset, categorized by the presence of 2, 3 and 2-4 distinct objects, using prompts structured as \"a {object1} ... and a {objectn}\" to systematically evaluate our approach.\nWe utilize CLIP-score [30] for quantitatively assessing image-text compatibility, thereby evaluating the semantic accuracy of synthesized images. The text descriptions for images are formulated by appending a prefix of \"a photo of\" to the generation prompts.\nImplementation: The experimental findings were derived using the CompVis1.4 model for text-to-image synthesis. We configured the model to perform 50 denoising steps with a constant guidance scale of 7.5, and produced synthetic images at a resolution of 512 \u00d7 512. The hyperparameters of selective sample is set to select the highest 80% of attention values and randomly keep 50%. the margin for the inter-token constraint was established at 0.1, a setting that resulted in achieving the highest scores for both YOLO and CLIP metrics.\n5.2 Case studies\nQuantitative ablation studies: The comprehensive evaluation of our proposed methods encompasses Lintra, Linter, Lself, selective sampling, and cross-attention redistribution, as summarized in the table 1. The results reveal that utilizing only Lintra with selective sampling and cross-attention redistribution yields subpar object generation and location, achieving a mere 32.1 AP50 and 11.4 AP. Introducing Linter to emphasize the cross-attention of the token over others at the same location leads to a significant enhancement, with improvements of 7.9 in AP50 and 3.7 in AP. This highlights the presence of attention overlap, as previously mentioned, and underscores the effectiveness of our method in addressing this issue. The incorporation of Lself further boosts performance, resulting in 10.1 AP50 and 4.3 AP. This suggests that the presence of self-attention distraction compromises the positioning accuracy of layout-to-image methods. The concept of aggregating and treating self-attentions as a whole emerges as an effective strategy to mitigate such issues. Selective sampling and cross-attention redistribution also contribute quantitatively to improved efficacy.\nIn terms of the CLIP score, the introduction of Linter appears to compromise the coherence of image semantics, causing a slight reduction from 0.3202 to 0.3164. However, with the addition of Lself, our strategy of enhancing interconnection with object pixels while maintaining sufficient redundancy with other pixels proves to enhance semantic coherence.\nQualitative ablation studies: We conducted visual ablation studies to elucidate the progressive impact of our method on the generation process. In the latent space of textual representations, two objects, namely a tiger and a bear, inherently share similarities. According to figure 3, when only controlling Lintra, these two concepts inevitably overlap and merge, with the more dominant concept exerting greater influence over the generated content. Introducing Linter and AR helps mitigate this overlap to some extent. While Linter enhances control over layout precision by competing for the same location, it does violate the fidelity of the resulting image. On the other hand, AR provides a more natural transition from objects to backgrounds. Finally, with the addition of Lself, we further fine-tune location precision and re-establish the relationship between objects and backgrounds, aiming to recover some of the fidelity lost due to the impact of Linter.\nHyperparameter ablation studies: Hyperparameters of our work, including K and M for selective sampling and the margin in the inter-token constraint, play a critical role in the effectiveness of the proposed components and collectively impact the overall performance of our method. The detailed results of hyperparameter optimization are presented in Tables 2, 3, and 4. Specifically, for the inter-token constraint margin, as shown in Table 2, a margin of 0.1 provides the best results. This finding is somewhat counter intuitive, as one might expect that a larger margin would more effectively differentiate between token attentions. However, as discussed in Section 4.1. A plausible explanation for this observation might lie in the fundamentally intertwined nature of cross-attentions that intersect, influenced by text embeddings confined to a constrained latent space, preventing concepts from being represented distinctly, and the latent features that are shared among all tokens.\nAccording to table 3, selecting a high-value portion with K = 80% results in optimal performance for both AP50 and AP metrics, though there is a slight decrease in CLIP score. This can be attributed to the fact that optimizing latents through gradient-based backpropagation can disrupt their naturally Gaussian distribution, potentially compromising image fidelity if not managed cautiously. With a smaller K, our method affects only a limited number of attention values, minimally directing the diffusion process. This approach preserves semantic integrity to some extent but sacrifices precision in object placement and generation. Conversely, using a higher K value, while including random drops, approximates the selection of highly-related attention values to random sampling, which harms performance.\nAchieving the best performance with a margin of 0.1 and K at 80%, we find optimal results at M equal to 50% (table 4). This outcome aligns with the patterns observed in our K ablation studies, indicating a balanced exists in our selective sampling technique that prohibits incorporating either too many or too few attention values.\n5.3 Qualitative analysis\nWhen conducting a visual comparison (figure 4) using images generated from datasets featuring three distinct objects, we observe that previous methods often struggle with producing a coherent image representation or accurately placing objects. For example, while MultiDiffusion excels at positioning objects in specified regions, it frequently compromises the overall image coherence and object integrity, leading to visually jarring images with poor transitions between objects and the background. Conversely, methods like BoxDiff and layout-control, despite offering more coherent image presentations, tend to suffer from issues such as missing objects or the appearance of unintended objects. In contrast, our proposed CSG method successfully combines precise layout placement with semantically coherent image quality, addressing these shortcomings (More qualitative comparisons can be found in supplementary materials)."}, {"title": "6 Conclusion", "content": "This paper introduces a training-free method that surpasses previous approaches in object placement and generation, effectively addressing issues of semantic overlaps and self-attention misalignment that were overlooked by prior research. We showcase artworks created by our method which composite several objects and backgrounds. Compared to these earlier methods, our approach demonstrates superior performance in assimilating layout information and generating high-fidelity images. Through extensive experiments, we show that our method significantly mitigates adversarial effects encountered during the diffusion process. Although our work currently utilizes only bounding boxes as layout information, the proposed method is designed to be compatible with various forms of layout data. Furthermore, given its training-free nature, it can be seamlessly adapted to enhance models pre-trained with layout information, promising improved outcomes. For further discussions, please refer to the supplementary material provided."}, {"title": "F Limitations", "content": "Our method is effective at creating composite scenes, yet it encounters several notable issues. Primarily, while it focuses on local coherence by enhancing communication between objects and their immediate surroundings, it falls short in ensuring global coherence. This discrepancy is evident in the visual examples provided; the waterfall in image (a) appears isolated, contrasting with its more integrated appearance in images (b) and (c), where it seamlessly merges with the sky. Similarly, the castle's integration into the scene feels more natural in images (b) and (c) than in image (a). Another issue arises as the complexity of the semantics in the prompt increases. Attributes that are not explicitly anchored through layout information tend to be incorrectly associated with unintended objects. For example, across all three images, the blooms are incorrectly attached to the unicorn, resulting in an unintended color scheme for the creature."}]}