{"title": "Training-free Composite Scene Generation for Layout-to-Image Synthesis", "authors": ["Jiaqi Liu", "Tao Huang", "Chang Xu"], "abstract": "Recent breakthroughs in text-to-image diffusion models have significantly advanced the generation of high-fidelity, photo-realistic images from textual descriptions. Yet, these models often struggle with interpreting spatial arrangements from text, hindering their ability to produce images with precise spatial configurations. To bridge this gap, layout-to-image generation has emerged as a promising direction. However, training-based approaches are limited by the need for extensively annotated datasets, leading to high data acquisition costs and a constrained conceptual scope. Conversely, training-free methods face challenges in accurately locating and generating semantically similar objects within complex compositions. This paper introduces a novel training-free approach designed to overcome adversarial semantic intersections during the diffusion conditioning phase. By refining intra-token loss with selective sampling and enhancing the diffusion process with attention redistribution, we propose two innovative constraints: 1) an inter-token constraint that resolves token conflicts to ensure accurate concept synthesis; and 2) a self-attention constraint that improves pixel-to-pixel relationships. Our evaluations confirm the effectiveness of leveraging layout information for guiding the diffusion process, generating content-rich images with enhanced fidelity and complexity. Code is available at https://github.com/Papple-F/csg.git.", "sections": [{"title": "1 Introduction", "content": "Substantial advancements have been achieved in large-scale text-to-image generative models [31-35,37], which are now capable of converting complex text descriptions into visually compelling images with impressive accuracy and cre-ativity. Despite these advancements, current models still struggle to comprehend layout descriptions contained within texts and to precisely outline and render detailed images. This is largely due to limitations in model capacity and the quality of training datasets. Consequently, in tasks such as art creation [10] and industrial design [28], where elements need to be precisely positioned, the mod-els' intrinsic inability to comprehend user-defined spatial information limits their applicability.\nRecent advancements in layout-to-image approaches have shown significant progress in achieving more controllable generation, employing both training-based [2, 6, 14, 23, 25, 43, 44] and training-free [3,8,22,42] methodologies to trans-form detailed layouts into vivid, realistic images. While training-based methods have demonstrated promise, their effectiveness heavily relies on the availability of comprehensive and varied layout datasets. Assembling, annotating, and pre-processing these datasets for effective concept learning is challenging due to the high costs and resource-intensive nature of data acquisition, posing a substantial barrier for many research initiatives. In contrast, training-free methods utilize the intrinsic generative capabilities of models to dynamically guide the diffusion process, offering a significant efficiency advantage. As text-to-image models be-come more popular, there is an increasing demand for content generation that involves complex compositions of multiple objects and backgrounds. Existing layout-to-image approaches, which primarily focus on content positioning, of-ten struggle with conflicts arising from intersecting similar concepts, especially when generating scenes with multiple elements. Moreover, the critical role of self-attention mechanisms in refining generated content with accurate textural details during the later stages of diffusion, as highlighted in [2], is often over-"}, {"title": "2 Related Work", "content": "2.1 Text-to-image generative models\nRecent advancements in text-to-image generative models, exemplified by Sta-ble Diffusion [33], DALLE-3 [37], and Imagen [35], represent a significant leap forward beyond previous dominant techniques like generative adversarial net-works [4,15,16,21]. These new models excel not only in image generation but also in enhancing the performance of tasks such as classification [1,20], action segmen-tation [12,27], and more. These models enable the generation of highly detailed visual content directly from textual descriptions, distinguishing themselves by producing contextually relevant and aesthetically pleasing images. Their abil-ity to translate complex textual prompts into visual artworks demonstrates a remarkable proficiency, unlocking new possibilities across various applications, from digital art creation [10] to diverse content generation."}, {"title": "3 Preliminaries", "content": "3.1 Stable diffusion\nDiffusion models [9,11,18,38], operates by gradually transforming a random noise distribution into a coherent image, guided by the semantics of the input text. This process involves a series of forward and backward steps, where the model initially adds noise e to an image I and then learns to recover the original image from noise, conditioned on textual descriptions c. Latent diffusion model [33] operates in latent space where for a given image I, it is first encoded as latent z by an encoder E, and then reconstructed by a decoder D as \u00ce = D(z) = D(E(I)) after the denoising process.\nDuring the training stage of diffusion process, at given timestep t, with latent zt and condition c, the denoiser es learns to correctly predict the added noise e through mean square error:\nLdiff = Ez~E(I),\u20ac~N(0,1),c,t [||\u20ac \u2013 \u20ac9(zt, c, t) ||2].\nDuring the inference stage, the denoising process gradually removes noise from a randomly sampled zy with the noises predicted by diffusion model with condition c, and guide the diffusion process through classifier [11] or classifier-free guidance [19]."}, {"title": "3.2 Attention mechanism", "content": "The attention mechanism forms the cornerstone of transformer models [40], play-ing a pivotal role in natural language processing (NLP) tasks. This mechanism was subsequently integrated into the vision transformer architecture [13], thereby extending its applicability to tasks within the realm of computer vision. Within the framework of stable diffusion models, the intermediate features are processed through both self-attention and cross-attention layers. This processing facilitates the generation of an attention map at timestep t as follows:\nAt = softmax (\\frac{QK^T}{\\sqrt{d}}).\nIn this process, Q and K represent queries and keys, respectively, with d denoting the dimensionality of these query and key features. For cross-attention mechanisms, K is derived from the projection of text embeddings. These embed-dings result from encoding the text conditioning c into a latent space, achieved through the use of a CLIP text encoder [30]. Conversely, Q corresponds to the projection of intermediate features sourced from a U-Net architecture. Within the self-attention framework, both Q and K are obtained from the projections of these intermediate features."}, {"title": "4 Methods", "content": "In this section, we explore the nuances of generating composite scenes. We begin in Section 4.1 by detailing our novel approach to attention selection, including the definition of mask representations and the application of our selective sampling strategy. This section also explains the development and role of intra- and inter-token constraints in influencing the diffusion process. In Section 4.2, we shift focus to our innovative handling of self-attentions, illustrating their pivotal role in refining the diffusion process for enhanced image generation. Finally, Section 4.3 introduces our attention redistribution technique, employed during forward diffusion to further optimize generation outcomes through strategic constraint refinement and application."}, {"title": "4.1 Cross-attention constraints", "content": "To formulate constraints on cross-attentions, we begin by detailing the acquisi-tion of attentions and layout information for constraint application. Leveraging latent noise manipulation is pivotal in aligning image-text attentions within des-ignated spatial regions. Drawing on recent research [7,42] which suggests that attentions at smaller scales capture semantic information more effectively during early diffusion phases, we employ averaged cross-attentions from user-defined at-tending objects at the 16 \u00d7 16 up-scaling block as our manipulation references. Formally, for N attending tokens, we define A = {A\u00bf}{~1, A\u00a1 \u2208 RL as the set of cross-attentions relative to the tokens, with L indicating the number of image tokens in cross-attentions.\nN\nFor representing layout information, we employ bounding boxes B = {bi}=1, which delineate the top-left and bottom-right corner coordinates for each object.\nN"}, {"title": "Intra-token constraint", "content": "For each attending token, we use its corresponding mask mi and cross-attention probabilities A\u00bf to compute our intra-token atten-tion regularization loss. To make sure that the object is only generated in its own target region (i.e., inside the bounding box), we encourage the cross-attention values associated with the target region to exceed those of the non-target region. Following [8], we define the intra-token constraint as:\nLintra = \\sum_{i=1}^{N} L_{intra}^i\nwith L_{intra}^i = (1 \u2013 \\frac{\\sum_{j=1}^{L} (m_i \\cdot A_i)_j}{\\sum_{j=1}^{L} (m_i \\cdot A_i)_j + \\sum_{j=1}^{L} (\\bar{m_i} \\cdot A_i)_j})^2,\nwhere m\u2081 = F(mi) and m\u2081 = F(1 \u2013 mi).\nThe constraint is designed to increase the proportion of attention values within the target region relative to those outside, enhancing focus on the in-tended area. By applying selective sampling to pivotal values, we address the issue of excessive attention concentration in confined areas. Furthermore, selec-tive sampling from both inside and outside regions ensures a more equitable representation of high values, mitigating bias in the aggregation process and enhancing the fairness of their contribution to the loss function."}, {"title": "Inter-token constraint", "content": "The semantic intersection due to the conceptual similarity (e.g. a bear and a tiger shares considerable similarity in textual la-tent space), under the context of limited latent space, inevitably intertwine their cross-attentions and cause a degree of spatial intersection. Aiming to completely eliminate such adversarial effects may not be plausible due to the inherent depen-dence between image intermediate features are extracted from the same latent variable and difficulties to manipulate textual latent space without altering its original semantic meanings. Thus, we employ a conservative approach where we encourage attentions inside corresponding region not to be exclusive, but sur-pass over other tokens' attentions at the same location. To accomplish this, for a given token i, we first obtain the maximum attentions within target region for other tokens:\n(m_i A_j)^{max} = \\max_{j=1,j\\neq i}^{N} (m_i A_j).\nFollowing, we calculate the difference d between such attentions and atten-tions of target region of given token with a margin g.\nd = \\sum_{k=1}^{L}((m_i. A_i)_k - g) - \\sum_{k=1}^{L}(m_i. A_j)^{max}.\nTo achieve our goal of making the given token more prominent than others within the targeted region, we establish an inter-token constraint defined as follows:\nL^{inter} = \\begin{cases}\n0, d > 0\\\\\nd^2, d < 0\n\\end{cases}L_{inter} = \\sum_{i=1}^{N}c_{inter}^i\nThe constraint on inter-token relations, utilizing the same elements from a given mi, focuses on a side-by-side comparison of cross-attention values with other tokens at identical spatial locations. This optimization is particularly aimed at scenarios involving intersections, encompassing both the semantic sim-ilarities previously discussed and intersections of user-specified regions. The goal is to promote the dominance of a single token's content within a specific area to ensure the generated image is both clear and coherent."}, {"title": "4.2 Self-attention constraints", "content": "The impact of cross-attention is critical in the initial diffusion stages, guiding the structural formation of content via textual cues. However, as [2] highlights, self-attention gains prominence in later diffusion stages, influencing object textures. An unintended effect of semantic intersection can misguide self-attention, causing inappropriate pixel correlations during early stages. To mitigate this, we propose to align self-attention within target regions in early diffusion stages, improving local coherence and maintaining necessary external interactions.\nFollowing the same setup as section 4.1, we obtain averaged self-attentions SER RL at 16 \u00d7 16 up-scaling blocks. Given mask mi, we aggregated self-attentions which lies within the mask region:\nL\nS_i = \\sum_{j=1}^{L}(m_i \\cdot S)_j,\nwhere Si \u2208 Ro.\nSimilar to the intra-token constraint, the self-attention constraint is defined as:\nN\nL_{self} = \\sum_{i=1}^{N} C_{self}^i\nL\n\\sum_{j=1} (m_i. S_{ij}\nwith C_{self}^i = (1 -\\frac{\u2212)^2}{\\sum_{j=1}^{L} (m_i S_i); +\\sum_{j=1}^{L} (\\bar{m_i} S_{ij}}"}, {"title": "4.3 In-generation diffusion guidance", "content": "Given that the limited latent space can cause semantic-level overlaps, this results in unresolved spatial overlaps even with sufficient refinement steps. These errors accumulate during diffusion steps, potentially leading the generation results in incorrect directions. To address this, we introduce attention redistribution, a technique that reallocates cross-attentions with corresponding tokens during the diffusion process. For each token, its cross-attention is defined as the aggregated cross-attentions across all attending tokens within the bounding box area. After reallocation, we apply max normalization to all cross-attentions. The attention redistribution is defined as follows:\nN\nAi = mi\\sum_{j=1}(Aj)\nThe generative process is divided into refinement and guidance stages. During each guidance step, the latent variable is updated TR times (the number of refinement steps per timestep). This process lasts for TD steps (the total number of timesteps during the diffusion process with such recurrent updates).\nFor each refinement step, the overall constraint is defined as:\nL = Lintra + Linter + Lself,\nand the latent is updated with a linear decay factor nt as:\nz' = zt - Nt. \u2207 z\u2081L.\nThrough this approach, at each step of the diffusion process, we refine the latent to achieve more focused attention on the targeted regions. This refinement process helps to resolve conflicts among tokens and aligns self-attentions, thereby improving both internal and external connections within the generated scenes."}, {"title": "5 Experiments", "content": "5.1 Experimental setup\nEvaluation: In our quantitative analysis using the COCO 2014 dataset [26], we adopt a methodology akin to [3], filtering the dataset to include images with n distinct objects where each occupies at least 5% of the image area. We further refine the selection by excluding objects described by more than two words, as well as \"person\", resulting in a final set of 61 object classes. For performance eval-uation, we employ YOLOv7 [41] for object detection, utilizing metrics such as YOLO-score (AP, AP50) [25] to assess our method's effectiveness of locating and correctly generating objects. Object detection is conducted across all 80 COCO classes to ensure thoroughness. Additionally, we generate randomly sample and generate approximately 1,000 samples for subsets of the COCO 2014 dataset, categorized by the presence of 2, 3 and 2-4 distinct objects, using prompts struc-tured as \"a {object1} and a {objectn}\" to systematically evaluate our approach.\nWe utilize CLIP-score [30] for quantitatively assessing image-text compati-bility, thereby evaluating the semantic accuracy of synthesized images. The text descriptions for images are formulated by appending a prefix of \"a photo of\" to the generation prompts.\nImplementation: The experimental findings were derived using the Com-pVis1.4 model for text-to-image synthesis. We configured the model to perform 50 denoising steps with a constant guidance scale of 7.5, and produced synthetic images at a resolution of 512 \u00d7 512. The hyperparameters of selective sample is set to select the highest 80% of attention values and randomly keep 50%. the margin for the inter-token constraint was established at 0.1, a setting that resulted in achieving the highest scores for both YOLO and CLIP metrics."}, {"title": "5.2 Case studies", "content": "Quantitative ablation studies: The comprehensive evaluation of our pro-posed methods encompasses Lintra, Linter, Lself, selective sampling, and cross-attention redistribution, as summarized in the table 1. The results reveal that utilizing only Lintra with selective sampling and cross-attention redistribution yields subpar object generation and location, achieving a mere 32.1 AP50 and 11.4 AP. Introducing Linter to emphasize the cross-attention of the token over others at the same location leads to a significant enhancement, with improve-ments of 7.9 in AP50 and 3.7 in AP. This highlights the presence of attention over-lap, as previously mentioned, and underscores the effectiveness of our method in addressing this issue. The incorporation of Lself further boosts performance, re-sulting in 10.1 AP50 and 4.3 AP. This suggests that the presence of self-attention distraction compromises the positioning accuracy of layout-to-image methods. The concept of aggregating and treating self-attentions as a whole emerges as an effective strategy to mitigate such issues. Selective sampling and cross-attention redistribution also contribute quantitatively to improved efficacy.\nIn terms of the CLIP score, the introduction of Linter appears to compro-mise the coherence of image semantics, causing a slight reduction from 0.3202 to 0.3164. However, with the addition of Lself, our strategy of enhancing intercon-nection with object pixels while maintaining sufficient redundancy with other pixels proves to enhance semantic coherence.\nQualitative ablation studies: We conducted visual ablation studies to elucidate the progressive impact of our method on the generation process. In the latent space of textual representations, two objects, namely a tiger and a bear, inherently share similarities. According to figure 3, when only controlling Lintra, these two concepts inevitably overlap and merge, with the more dominant concept exerting greater influence over the generated content. Introducing Linter and AR helps mitigate this overlap to some extent. While Linter enhances control over layout precision by competing for the same location, it does violate the fidelity of the resulting image. On the other hand, AR provides a more natural transition from objects to backgrounds. Finally, with the addition of Lself, we further fine-tune location precision and re-establish the relationship between objects and backgrounds, aiming to recover some of the fidelity lost due to the impact of Linter.\nHyperparameter ablation studies: Hyperparameters of our work, in-cluding K and M for selective sampling and the margin in the inter-token con-"}, {"title": "5.3 Qualitative analysis", "content": "When conducting a visual comparison (figure 4) using images generated from datasets featuring three distinct objects, we observe that previous methods often struggle with producing a coherent image representation or accurately placing objects. For example, while MultiDiffusion excels at positioning objects in spec-ified regions, it frequently compromises the overall image coherence and object"}, {"title": "6 Conclusion", "content": "This paper introduces a training-free method that surpasses previous approaches in object placement and generation, effectively addressing issues of semantic overlaps and self-attention misalignment that were overlooked by prior research. We showcase artworks created by our method which composite several objects and backgrounds. Compared to these earlier methods, our approach demon-strates superior performance in assimilating layout information and generating high-fidelity images. Through extensive experiments, we show that our method significantly mitigates adversarial effects encountered during the diffusion pro-cess. Although our work currently utilizes only bounding boxes as layout infor-mation, the proposed method is designed to be compatible with various forms of layout data. Furthermore, given its training-free nature, it can be seamlessly adapted to enhance models pre-trained with layout information, promising im-proved outcomes. For further discussions, please refer to the supplementary ma-terial provided."}]}