{"title": "V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models", "authors": ["Junwei You", "Haotian Shi", "Zhuoyu Jiang", "Zilin Huang", "Rui Gan", "Keshu Wu", "Xi Cheng", "Xiaopeng Li", "Bin Ran"], "abstract": "Advancements in autonomous driving have increasingly focused on end-to-end (E2E) systems that manage the full spectrum of driving tasks, from environmental perception to vehicle navigation and control. This paper introduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework with large vision-language models (VLMs). V2X-VLM is designed to enhance situational awareness, decision-making, and ultimate trajectory planning by integrating data from vehicle-mounted cameras, infrastructure sensors, and textual information. The strength of the comprehensive multimodel data fusion of the VLM enables precise and safe E2E trajectory planning in complex and dynamic driving scenarios. Validation on the DAIR-V2X dataset demonstrates that V2X-VLM outperforms existing state-of-the-art methods in cooperative autonomous driving.", "sections": [{"title": "INTRODUCTION", "content": "Recent advancements in autonomous driving have been characterized by the adoption of comprehensive technologies that enhance the capabilities of vehicles to navigate complex environments. Among these, end-to-end (E2E) autonomous driving systems have emerged as a key area of focus. These systems streamline the driving process from environmental perception to vehicle control, using integrated machine learning models to process complex environmental data in real-time (1, 2).\nLLM Enhanced E2E Autonomous Driving\nA critical component of these E2E systems is the implementation of foundation models, which are large-scale machine-learning models capable of understanding contextual information and generating multi-model outputs with text and images. Initially, large language models (LLMs) were employed to interpret natural language inputs, facilitating better interaction and decision-making based on human-provided instructions. LLMs provide the ability to process and generate textual data and enable vehicles to understand commands and contextual information conveyed through language.\nRecent research in E2E autonomous driving highlights significant advancements continuously through the integration of LLMs. Generally, research emphasizes diverse approaches to employing LLMs for improving interpretability, decision-making, and interaction in autonomous systems. For instance, DriveGPT4, developed in (3), presents an interpretable framework that integrates multi-frame video inputs and textual queries to predict vehicle actions and provide reasoning, thus enhancing transparency and user trust in autonomous driving systems. Similarly, LMDrive proposed in (4) introduces a closed-loop E2E framework that utilizes multimodal sensor data and natural language instructions to generate control signals, which facilitates real-time interaction with complex environments. Other methods, such as OmniDrive (5) and Atlas (6), explore the inclusion of 3D tokenization in LLMs to enhance the perception and planning capabilities of autonomous vehicles, addressing the limitations of 2D vision-based systems in accurately perceiving and navigating complex driving scenarios. Collectively, these works underscore the transformative potential of LLMs in E2E autonomous driving, provide frameworks for handling complex, real-world driving conditions, and improve human-vehicle interaction through natural language processing (6).\nVLM Advancement in E2E Autonomous Driving\nThe subsequent development of large vision-language models (VLMs) has expanded these capabilities. VLMs integrate visual and linguistic processing, which allows autonomous systems to simultaneously interpret visual cues and associated textual information. This dual capability enhances situational awareness by providing a comprehensive understanding of the environment, which is crucial for vehicles navigating complex and dynamic driving scenarios (7, 8).\nRecent studies demonstrate how VLMs enhance scene understanding, perception, and planning capabilities in autonomous vehicles. DriveVLM (9), for example, combines scene description, analysis, and hierarchical planning modules to tackle challenges in urban driving scenarios, such as adverse weather and intricate road layouts. This system, along with DriveVLM-Dual, incorporates traditional 3D perception and planning modules, thus providing improved spatial reasoning and real-time planning capabilities. Similarly, VLP introduced in (10) leverages VLMs to integrate common-sense reasoning into autonomous driving systems (ADS), which enhances the system's ability to generalize across diverse urban environments and long-tail scenarios. This approach has"}, {"title": "V2X Enabled Cooperative Autonomous Driving", "content": "Additionally, the integration of Vehicle-to-Everything (V2X) communication systems has advanced the development of cooperative autonomous driving. V2X enables vehicles to communicate with each other and with infrastructure elements that provide real-time updates and a broader context of the driving environment (13). This communication facilitates coordinated maneuvers and has the potential to improve overall traffic safety and efficiency.\nFor instance, studies such as (14) and (15) emphasize the role of enhanced data exchange and coordination between vehicles and infrastructure, which results in improved situational awareness and maneuvering capabilities. A most recent study (16) develops UniV2X, a unified framework leveraging diverse sensor inputs from both vehicles and infrastructure, which showcases improved planning performance and robustness in data transmission, crucial for practical deployment scenarios. Moreover, V2X-INCOP proposed in (17) addresses communication interruptions, proposing a robust cooperative perception model that utilizes historical data to mitigate the effects of data loss, enhancing the reliability of autonomous driving systems. Lastly, the study focusing on cooperative collision avoidance and coordinated driving mechanisms demonstrates the potential of V2X systems to reduce collision risks and optimize traffic flow, underscoring the essential role of real-time data sharing in maintaining safe and efficient road networks (18). These advancements collectively highlight the transformative potential of V2X communication and cooperation in achieving higher levels of autonomy and safety in autonomous driving.\nHowever, there remains a notable research gap in the application of foundation models, such as LLMs and VLMs, upon E2E cooperative autonomous driving. While E2E autonomous driving through V2X cooperation has the potential to enhance vehicle situational awareness and thus improve the overall autonomous driving ability, it does not fully capitalize on the advanced capabilities of foundation models like VLMs. Traditional AI models easily struggle with the integration of complex and multimodal data from various sources. VLMs, in contrast, excel at merging multimodel information and fusing multi-source information so as to facilitate a richer understanding of the driving environment. This capability enables vehicles to operate and navigate more precisely through complex and dynamic traffic scenarios, beyond the capabilities of smaller models.\nIn view of this, this study aims to propose a pioneering E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework leveraging large VLMs to enhance situational awareness, decision-making, and overall driving performances regarding safety and efficiency. By integrating VLMs into the VICAD framework, the proposed method aims to unify the processing of multi-source visual and textual data from both vehicles and infrastructures through V2X, thus facilitating a comprehensive understanding of complex driving environments. This integration allows for a precise interpretation of dynamic elements such as traffic signs, road conditions,"}, {"title": "METHODOLOGY", "content": "This section introduces V2X-VLM, the first E2E cooperative autonomous driving framework using large VLMs, in detail. We first formulate the problem by specifying the data input and output. Subsequently, the overall framework structure is presented, followed by a section introducing traffic scene interpretation by VLM. Finally, the proposed paradigm of E2E VICAD multimodel input for VLM is illustrated."}, {"title": "Problem Formulation", "content": "The goal of the proposed E2E framework V2X-VLM is to plan the optimal trajectory for the ego vehicle by integrating and processing multimodal data from various sources. Specifically, the data includes images from vehicle-mounted cameras, images from infrastructure cameras, and textual information indicating the current vehicle position.\nLet $I_v$ denote the image data from the vehicle's camera, $I_i$ the image data from the infrastructure camera, and $E$ the textual embedding indicating the current position and other relevant instructive information. The planned optimal trajectory $\\tau \\in R^2$ for the ego vehicle, is represented as a sequence of positions over time, as shown below:\n$\\tau = \\{(x_t,y_t) | t = 1,2,...,T\\}$  (1)\nwhere $T$ is the planning horizon. To achieve this, the objective is to minimize a loss function $L(\\tau, \\tau^*)$, where $\\tau^*$ represents the ground truth trajectory, as shown in the equation below:\n$min L(\\tau, \\tau^*) = minL(F(I_v,I_i, E), \\tau^*)$ (2)\nwhere $F()$ represents the developed V2X-VLM framework."}, {"title": "Overall V2X-VLM Framework Architecture", "content": "The overall architecture of V2X-VLM is demonstrated in Figure 1. As addressed previously, in general, V2X-VLM integrates data from various sources to form a comprehensive end-to-end system for cooperative autonomous driving, where large VLM is applied as the core to synthesize and analyze diverse input types."}, {"title": "Scene Understanding and Interpretation", "content": "VLM plays a crucial role in understanding and interpreting the perception images captured from both the vehicle side and the infrastructure side."}, {"title": "E2E VICAD Multimodel Input Paradigm for VLMs", "content": "The proposed paradigm for handling multimodal data from different sources in the V2X-VLM framework emphasizes simplicity and effectiveness, as shown in Figure 3. In this approach, data from the vehicle-mounted camera and infrastructure camera are combined into pairs, with each pair further embedded with a text prompt indicating the ego vehicle's current position. The benefits of this design are manifold. First of all, its simplicity and efficiency set it apart from more complex fusion techniques, such as multi-stage feature extraction or hierarchical fusion. This direct method reduces computational overhead and the risk of overfitting (19, 20), which streamlines the data pipeline and makes real-time processing more feasible. Additionally, this approach reduces redundancy (21), a common issue in more complex fusion methods that involve multiple layers of processing. By using a direct fusion technique, the system retains the most relevant features from each data source, focusing on actionable insights rather than extraneous details. Lastly, the straightforward nature of this fusion technique makes it adaptable to different types of data and scenarios. It can easily scale with additional sensors or data types, allowing for future enhancements without overhauling the underlying system architecture.\nIn summary, this direct fusion approach balances simplicity and power, offering a robust solution for E2E cooperative autonomous driving. It leverages the strengths of both close-up and wide-angle perspectives, enriched by contextual text, delivering a sophisticated yet efficient method for navigating complex environments."}, {"title": "Planning Result", "content": "The final output of the V2X-VLM framework is a planned trajectory for the ego vehicle, expressed as a sequence of positional coordinates over time. The trajectory is derived from the integrated analysis of fused image pairs and textual data. The planned trajectory aims to optimize the vehicle's navigation, ensuring safe and efficient movement within the constraints of the road and traffic conditions.\nTo evaluate the performance of the planned trajectory, we use a cross-entropy loss function (22) adapted for sequence prediction. This loss measures the divergence between the predicted trajectory $\\tau$ and the ground truth trajectory $\\tau^*$. In the context of utilizing large VLMs, the textual data is tokenized before being processed. After processing, the tokens are decoded back into a"}, {"title": "EXPERIMENT", "content": "The proposed E2E cooperative autonomous driving framework V2X-VLM is evaluated on DAIR-V2X dataset (23). This section first introduces the dataset, and then presents the experiment setups. Performance of V2X-VLM will be evaluated and discussed thoroughly in the end."}, {"title": "Dataset", "content": "The DAIR-V2X dataset is an extensive and well-annotated resource designed for research in V2X cooperative autonomous driving. It includes 22,325 frames of data from vehicle-mounted sensors and 10,084 frames from infrastructure sensors, capturing RGB images and LiDAR data at up to 25 Hz. The dataset features detailed annotations, including 2D and 3D bounding boxes and object attributes, across diverse traffic scenarios. This comprehensive dataset is crucial for tasks such as trajectory prediction and multi-sensor data fusion, which facilitates the development of advanced V2X systems that improve traffic safety, navigation accuracy, and cooperative driving strategies."}, {"title": "Setups", "content": "V2X-VLM utilizes a pre-trained VLM known as Florence-2 (24), which integrates both visual and textual data inputs. The vision tower of the model is pre-trained and frozen during fine-tuning to leverage robust visual feature representations without additional training. The experiments are conducted on a single NVIDIA RTX 4090 GPU. The hyperparameters for training is illustrated in Table 1."}, {"title": "Baselines", "content": "To align with UniV2X (16), the latest state-of-the-art method for E2E cooperative autonomous driving, we evaluate the trajectory planning result of our proposed V2X-VLM upon the same baseline methods with same setting, as listed below:\n\u2022 No Fusion (16): This approach uses only ego-vehicle data without integrating any information from infrastructure sensors. It serves as a basic benchmark to highlight the benefits of incorporating V2X communication.\n\u2022 Vanilla (16): This method integrates infrastructure data using a simple fusion technique. It combines BEV (Bird's Eye View) features from both vehicle and infrastructure sensors through a basic neural network, without sophisticated data processing or transmission optimization.\n\u2022 V2VNet (25): This method employs vehicle-to-vehicle (V2V) communication to share and process point cloud data, focusing on enhancing 3D object detection and prediction. It fuses features from multiple vehicles to improve perception and decision-making.\n\u2022 CooperNaut (26): This approach uses cooperative perception among vehicles, sharing processed features rather than raw sensor data. It utilizes a basic convolutional neural network (CNN) for feature fusion and trajectory prediction."}, {"title": "Results Evaluation", "content": "The results presented in Table 2 provide a comprehensive comparison of the V2X-VLM framework against several baseline methods in terms of multiple metrics.\nThe L2 Error metric reflects the accuracy of the predicted trajectory compared to the ground truth, measured at different time intervals. The V2X-VLM framework demonstrates superior performance with the lowest average L2 Error of 1.40 meters, significantly outperforming all baseline methods. This indicates that the V2X-VLM is more accurate in trajectory prediction, likely due to its comprehensive E2E multimodal data integration and processing capabilities. In comparison, the closest competing method, UniV2X, has an average L2 Error of 3.43 meters.\nTransmission Cost, measured in Bytes per Second (BPS), is a critical metric for evaluating the efficiency of data communication between vehicle and infrastructure. The V2X-VLM framework reports a transmission cost of 1.24 \u00d7 107 BPS, which, while higher than the UniV2X method's 8.09 \u00d7 105 BPS, is lower than other methods such as Vanilla and V2VNet, both at 8.19 \u00d7 107 BPS. Specifically, the calculation of transmission cost is illustrated as follow. For an image resolution"}, {"title": "Results Visualization", "content": "Figure 4 visualizes the planned trajectories from the V2X-VLM framework. V2X-VLM reliably produces high-quality trajectory outputs across various driving scenarios, which validates the E2E capabilities of V2X-VLM in processing and integrating multi-source data for efficient autonomous driving."}, {"title": "CONCLUSION", "content": "This study presents V2X-VLM, an innovative framework that advances the field of VICAD by leveraging the capabilities of large VLMs. V2X-VLM excels in integrating and processing multimodal data, including visual and textual information from both vehicle and infrastructure sources. This comprehensive data fusion facilitates a detailed understanding of complex driving environments and results in precise and efficient trajectory planning.\nThe next step will first focus on diversifying the model's output to cater to a broader range of driving scenarios, which expands the framework's ability to handle various environmental conditions, traffic patterns, and unexpected events, increasing adaptability and robustness. After that, efforts will be made to optimize the efficiency of data transmission within the V2X-VLM framework. This can be achieved by extracting and transmitting key features rather than full-resolution images, thereby reducing the data load and associated transmission cost. Such optimization is crucial for maintaining a high-performance level while minimizing resource use, making the system more scalable and cost-effective. Finally, future research will involve training and evaluating the V2X-VLM framework on more diverse datasets that encompass a wider range of scenarios, which helps address the long-tail problem and improve the system's ability to generalize and respond effectively to real-world challenges."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "The author contributions for the study are as follows: Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, Xi Cheng, Xiaopeng Li, and Bin Ran collectively developed the research concept and methodology. Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, and Xi Cheng contributed to the implementation and experiments. Xiaopeng Li and Bin Ran provided overall guidance and supervision, ensuring the study is alignment with broader research goals. All authors reviewed and approved the final manuscript."}]}