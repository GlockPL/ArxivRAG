{"title": "V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large Vision-Language Models", "authors": ["Junwei You", "Haotian Shi", "Zhuoyu Jiang", "Zilin Huang", "Rui Gan", "Keshu Wu", "Xi Cheng", "Xiaopeng Li", "Bin Ran"], "abstract": "Advancements in autonomous driving have increasingly focused on end-to-end (E2E) systems that manage the full spectrum of driving tasks, from environmental perception to vehicle navigation and control. This paper introduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework with large vision-language models (VLMs). V2X-VLM is designed to enhance situational awareness, decision-making, and ultimate trajectory planning by integrating data from vehicle-mounted cameras, infrastructure sensors, and textual information. The strength of the comprehensive multimodel data fusion of the VLM enables precise and safe E2E trajectory planning in complex and dynamic driving scenarios. Validation on the DAIR-V2X dataset demonstrates that V2X-VLM outperforms existing state-of-the-art methods in cooperative autonomous driving.", "sections": [{"title": "INTRODUCTION", "content": "Recent advancements in autonomous driving have been characterized by the adoption of comprehensive technologies that enhance the capabilities of vehicles to navigate complex environments. Among these, end-to-end (E2E) autonomous driving systems have emerged as a key area of focus. These systems streamline the driving process from environmental perception to vehicle control, using integrated machine learning models to process complex environmental data in real-time (1, 2).\n\nA critical component of these E2E systems is the implementation of foundation models, which are large-scale machine-learning models capable of understanding contextual information and generating multi-model outputs with text and images. Initially, large language models (LLMs) were employed to interpret natural language inputs, facilitating better interaction and decision-making based on human-provided instructions. LLMs provide the ability to process and generate textual data and enable vehicles to understand commands and contextual information conveyed through language.\n\nRecent research in E2E autonomous driving highlights significant advancements continuously through the integration of LLMs. Generally, research emphasizes diverse approaches to employing LLMs for improving interpretability, decision-making, and interaction in autonomous systems. For instance, DriveGPT4, developed in (3), presents an interpretable framework that integrates multi-frame video inputs and textual queries to predict vehicle actions and provide reasoning, thus enhancing transparency and user trust in autonomous driving systems. Similarly, LMDrive proposed in (4) introduces a closed-loop E2E framework that utilizes multimodal sensor data and natural language instructions to generate control signals, which facilitates real-time interaction with complex environments. Other methods, such as OmniDrive (5) and Atlas (6), explore the inclusion of 3D tokenization in LLMs to enhance the perception and planning capabilities of autonomous vehicles, addressing the limitations of 2D vision-based systems in accurately perceiving and navigating complex driving scenarios. Collectively, these works underscore the transformative potential of LLMs in E2E autonomous driving, provide frameworks for handling complex, real-world driving conditions, and improve human-vehicle interaction through natural language processing (6).\n\nThe subsequent development of large vision-language models (VLMs) has expanded these capabilities. VLMs integrate visual and linguistic processing, which allows autonomous systems to simultaneously interpret visual cues and associated textual information. This dual capability enhances situational awareness by providing a comprehensive understanding of the environment, which is crucial for vehicles navigating complex and dynamic driving scenarios (7, 8).\n\nRecent studies demonstrate how VLMs enhance scene understanding, perception, and planning capabilities in autonomous vehicles. DriveVLM (9), for example, combines scene description, analysis, and hierarchical planning modules to tackle challenges in urban driving scenarios, such as adverse weather and intricate road layouts. This system, along with DriveVLM-Dual, incorporates traditional 3D perception and planning modules, thus providing improved spatial reasoning and real-time planning capabilities. Similarly, VLP introduced in (10) leverages VLMs to integrate common-sense reasoning into autonomous driving systems (ADS), which enhances the system's ability to generalize across diverse urban environments and long-tail scenarios. This approach has\n\nAdditionally, the integration of Vehicle-to-Everything (V2X) communication systems has advanced the development of cooperative autonomous driving. V2X enables vehicles to communicate with each other and with infrastructure elements that provide real-time updates and a broader context of the driving environment (13). This communication facilitates coordinated maneuvers and has the potential to improve overall traffic safety and efficiency.\n\nFor instance, studies such as (14) and (15) emphasize the role of enhanced data exchange and coordination between vehicles and infrastructure, which results in improved situational awareness and maneuvering capabilities. A most recent study (16) develops UniV2X, a unified framework leveraging diverse sensor inputs from both vehicles and infrastructure, which showcases improved planning performance and robustness in data transmission, crucial for practical deployment scenarios. Moreover, V2X-INCOP proposed in (17) addresses communication interruptions, proposing a robust cooperative perception model that utilizes historical data to mitigate the effects of data loss, enhancing the reliability of autonomous driving systems. Lastly, the study focusing on cooperative collision avoidance and coordinated driving mechanisms demonstrates the potential of V2X systems to reduce collision risks and optimize traffic flow, underscoring the essential role of real-time data sharing in maintaining safe and efficient road networks (18). These advancements collectively highlight the transformative potential of V2X communication and cooperation in achieving higher levels of autonomy and safety in autonomous driving.\n\nHowever, there remains a notable research gap in the application of foundation models, such as LLMs and VLMs, upon E2E cooperative autonomous driving. While E2E autonomous driving through V2X cooperation has the potential to enhance vehicle situational awareness and thus improve the overall autonomous driving ability, it does not fully capitalize on the advanced capabilities of foundation models like VLMs. Traditional AI models easily struggle with the integration of complex and multimodal data from various sources. VLMs, in contrast, excel at merging multimodel information and fusing multi-source information so as to facilitate a richer understanding of the driving environment. This capability enables vehicles to operate and navigate more precisely through complex and dynamic traffic scenarios, beyond the capabilities of smaller models.\n\nIn view of this, this study aims to propose a pioneering E2E vehicle-infrastructure cooperative autonomous driving (VICAD) framework leveraging large VLMs to enhance situational awareness, decision-making, and overall driving performances regarding safety and efficiency. By integrating VLMs into the VICAD framework, the proposed method aims to unify the processing of multi-source visual and textual data from both vehicles and infrastructures through V2X, thus facilitating a comprehensive understanding of complex driving environments. This integration allows for a precise interpretation of dynamic elements such as traffic signs, road conditions,\n\nand pedestrian movements, as well as the contextual information provided by infrastructure communications. Moreover, the framework is designed to improve the coordination between vehicle systems and infrastructure, enabling efficient and safer autonomous driving operations.\n\nThe main contributions of this study are threefold:\n\u2022 We propose a large vision-language model empowered by the E2E VICAD framework V2X-VLM, which enhances V2X cooperation and navigation through complex traffic scenarios.\n\u2022 We introduce a unified paradigm, where the complex visual scenes from both the vehicle and infrastructure side are paired and embedded with indicative textual information for effective V2X-VLM multimodel and multi-source data fusion and processing.\n\u2022 We evaluate our framework on the DAIR-V2X dataset, demonstrating significant improvements over current state-of-the-art cooperative autonomous driving methods, which validate the efficacy of the approach in real-world scenarios.\n\nThe remainder of this paper is organized as follows. The second section formulates the E2E VICAD problem. The third section details the methods utilized in this study. The fourth section showcases the experimental results of the proposed V2X-VLM, followed by the last section, which concludes the study along with future work."}, {"title": "METHODOLOGY", "content": "This section introduces V2X-VLM, the first E2E cooperative autonomous driving framework using large VLMs, in detail. We first formulate the problem by specifying the data input and output. Subsequently, the overall framework structure is presented, followed by a section introducing traffic scene interpretation by VLM. Finally, the proposed paradigm of E2E VICAD multimodel input for VLM is illustrated."}, {"title": "Problem Formulation", "content": "The goal of the proposed E2E framework V2X-VLM is to plan the optimal trajectory for the ego vehicle by integrating and processing multimodal data from various sources. Specifically, the data includes images from vehicle-mounted cameras, images from infrastructure cameras, and textual information indicating the current vehicle position.\n\nLet $I_v$ denote the image data from the vehicle's camera, $I_i$ the image data from the infrastructure camera, and $E$ the textual embedding indicating the current position and other relevant instructive information. The planned optimal trajectory $\\tau \\in R^2$ for the ego vehicle, is represented as a sequence of positions over time, as shown below:\n\n$\\tau = \\{(x_t, y_t) | t = 1, 2, ..., T\\}$  (1)\n\nwhere $T$ is the planning horizon. To achieve this, the objective is to minimize a loss function $L(\\tau, \\tau^*)$, where $\\tau^*$ represents the ground truth trajectory, as shown in the equation below:\n\n$\\min L(\\tau, \\tau^*) = \\min L(F(I_v, I_i, E), \\tau^*)$ (2)\n\nwhere $F(\\cdot)$ represents the developed V2X-VLM framework."}, {"title": "Overall V2X-VLM Framework Architecture", "content": "The overall architecture of V2X-VLM is demonstrated in Figure 1. As addressed previously, in general, V2X-VLM integrates data from various sources to form a comprehensive end-to-end system for cooperative autonomous driving, where large VLM is applied as the core to synthesize and analyze diverse input types."}, {"title": "Scene Understanding and Interpretation", "content": "VLM plays a crucial role in understanding and interpreting the perception images captured from both the vehicle side and the infrastructure side. Figure 2 (a) and (b) present two examples regarding the crucial scene information of two different sources extracted by VLM.\n\nFrom the vehicle side, VLM can identify essential elements such as types of nearby vehicles, road signs, traffic signals, weather condition, time, road environment, and general ego vehicle position accurately. This thorough understanding is crucial for navigating immediate surroundings through the interpreting of behaviors of other road users, such as signaling, braking, or lane changing. From the infrastructure side, VLM understands the clearer vehicle intention and movement, broader traffic patterns, pedestrian flow, and overall traffic density at the intersection. This macro-level perspective helps in anticipating congestion, understanding the coordination of traffic lights, and monitoring areas that might not be directly visible from the vehicle's perspective. This broader understanding is essential for planning and predicting long-term vehicle movement.\n\nThis dual capability validates the effectiveness of the VLM in extracting and fusing meaningful information from both vehicle and infrastructure perception images. These insights form the foundation for vehicle decision-making, navigation, and control, ensuring that the autonomous vehicles can respond precisely to both immediate and overarching traffic conditions."}, {"title": "E2E VICAD Multimodel Input Paradigm for VLMs", "content": "The proposed paradigm for handling multimodal data from different sources in the V2X-VLM framework emphasizes simplicity and effectiveness, as shown in Figure 3. In this approach, data from the vehicle-mounted camera and infrastructure camera are combined into pairs, with each pair further embedded with a text prompt indicating the ego vehicle's current position. The benefits of this design are manifold. First of all, its simplicity and efficiency set it apart from more complex fusion techniques, such as multi-stage feature extraction or hierarchical fusion. This direct method reduces computational overhead and the risk of overfitting (19, 20), which streamlines the data pipeline and makes real-time processing more feasible. Additionally, this approach reduces redundancy (21), a common issue in more complex fusion methods that involve multiple layers of processing. By using a direct fusion technique, the system retains the most relevant features from each data source, focusing on actionable insights rather than extraneous details. Lastly, the straightforward nature of this fusion technique makes it adaptable to different types of data and scenarios. It can easily scale with additional sensors or data types, allowing for future enhancements without overhauling the underlying system architecture.\n\nIn summary, this direct fusion approach balances simplicity and power, offering a robust solution for E2E cooperative autonomous driving. It leverages the strengths of both close-up and wide-angle perspectives, enriched by contextual text, delivering a sophisticated yet efficient method for navigating complex environments."}, {"title": "Planning Result", "content": "The final output of the V2X-VLM framework is a planned trajectory for the ego vehicle, expressed as a sequence of positional coordinates over time. The trajectory is derived from the integrated analysis of fused image pairs and textual data. The planned trajectory aims to optimize the vehicle's navigation, ensuring safe and efficient movement within the constraints of the road and traffic conditions.\n\nTo evaluate the performance of the planned trajectory, we use a cross-entropy loss function (22) adapted for sequence prediction. This loss measures the divergence between the predicted trajectory $\\tau$ and the ground truth trajectory $\\tau^*$. In the context of utilizing large VLMs, the textual data is tokenized before being processed. After processing, the tokens are decoded back into a\n\nhuman-readable format to present the predicted trajectory. In this case, cross-entropy loss function is instead applied to measure the difference between the predicted sequence of tokens $\\hat{s}$ associated with $\\tau$ and true sequence $s^*$ associated with $\\tau^*$, as shown in the equation below:\n\n$\\mathcal{L}(\\hat{s}, s^*) = - \\sum_{n=1}^{N_C} \\sum_{i=1}^{C} y_{i,n} \\log(\\hat{y}_{i,n})$ (3)\n\nwhere $N$ is the total number of tokens in the sequence, $C$ is the number of possible classes in the model's vocabulary, $y_{in}$ denotes a binary indicator indicating whether the $i$-th token is the correct one at the $n$-th position in the true sequence $s^*$, and $\\hat{y}_{i,n}$ represents the predicted probability of the $i$-th token at the $n$-th position in the predicted sequence $s$.\n\nDuring the training process, the model processes tokenized textual data and visual inputs from both the vehicle and infrastructure cameras. The planning outcomes are then decoded back into textual trajectories.\n\n$\\hat{s} = f_{vlm}(\\text{tokenize}(E), [I_v; I_i]), \\tau = \\text{decode}(\\hat{s})$ (4)\n\nwhere $f_{vlm}(\\cdot)$ denots the applied large VLM."}, {"title": "EXPERIMENT", "content": "The proposed E2E cooperative autonomous driving framework V2X-VLM is evaluated on DAIR-V2X dataset (23). This section first introduces the dataset, and then presents the experiment setups. Performance of V2X-VLM will be evaluated and discussed thoroughly in the end."}, {"title": "CONCLUSION", "content": "This study presents V2X-VLM, an innovative framework that advances the field of VICAD by leveraging the capabilities of large VLMs. V2X-VLM excels in integrating and processing multi-modal data, including visual and textual information from both vehicle and infrastructure sources. This comprehensive data fusion facilitates a detailed understanding of complex driving environments and results in precise and efficient trajectory planning.\n\nThe next step will first focus on diversifying the model's output to cater to a broader range of driving scenarios, which expands the framework's ability to handle various environmental conditions, traffic patterns, and unexpected events, increasing adaptability and robustness. After that, efforts will be made to optimize the efficiency of data transmission within the V2X-VLM framework. This can be achieved by extracting and transmitting key features rather than full-resolution images, thereby reducing the data load and associated transmission cost. Such optimization is crucial for maintaining a high-performance level while minimizing resource use, making the system more scalable and cost-effective. Finally, future research will involve training and evaluating the V2X-VLM framework on more diverse datasets that encompass a wider range of scenarios, which helps address the long-tail problem and improve the system's ability to generalize and respond effectively to real-world challenges."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "The author contributions for the study are as follows: Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, Xi Cheng, Xiaopeng Li, and Bin Ran collectively developed the research concept and methodology. Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, and Xi Cheng contributed to the implementation and experiments. Xiaopeng Li and Bin Ran provided overall guidance and supervision, ensuring the study is alignment with broader research goals. All authors reviewed and approved the final manuscript."}]}