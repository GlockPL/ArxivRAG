{"title": "Video Motion Transfer with Diffusion Transformers", "authors": ["Alexander Pondaven", "Aliaksandr Siarohin", "Sergey Tulyakov", "Philip Torr", "Fabio Pizzati"], "abstract": "We propose DiTFlow, a method for transferring the motion of a reference video to a newly synthesized one, designed specifically for Diffusion Transformers (DiT). We first process the reference video with a pre-trained DiT to analyze cross-frame attention maps and extract a patch-wise motion signal called the Attention Motion Flow (AMF). We guide the latent denoising process in an optimization-based, training-free, manner by optimizing latents with our AMF loss to generate videos reproducing the motion of the reference one. We also apply our optimization strategy to transformer positional embeddings, granting us a boost in zero-shot motion transfer capabilities. We evaluate DiTFlow against recently published methods, outperforming all across multiple metrics and human evaluation.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have rapidly emerged as the global standard for visual content synthesis, largely due to their performance at scale. By scaling the model size, it has been possible to train on increasingly large datasets, even including billions of samples, incredibly boosting synthesis capabilities [4, 12, 61, 66]. This trend is especially pronounced in video synthesis, where generating realistic, frame-by-frame visuals with coherent motion relies heavily on extensive data and large models. In this context, a particularly promising development is the introduction of diffusion transformers (DiTs) [37]. Inspired by transformers, DiTs propose a new class of diffusion model that allows for improved scalability, ultimately achieving impressive realism in generation, as demonstrated by their adoption in many scale-oriented open source [53, 61] and commercial systems [34, 38].\nHowever, realism alone is insufficient for real-world use of synthesized videos. Control over the generated video is essential for smooth integration into video creation and editing workflows. Most current models offer text-to-video (T2V) control through prompts, by synthesizing videos aligned with a user\u2019s textual description. However, this is rarely sufficient for achieving the desired result. While text may condition the appearance of objects in a scene, it is extremely challenging to control motion i.e. how the elements move in the scene, since text is inherently ambiguous when describing how fine-grained content evolves over time. To overcome this challenge, motion transfer approaches have used existing reference videos as a guide for the dynamics of the scene. The aim is to capture realistic motion patterns and transfer them to synthesized frames. However, most existing approaches are UNet-based [45] and do not take advantage of the superior performance of DiTs, which jointly process spatio-temporal information through their attention mechanism. We believe this opens up opportunities to extract high-quality motion information from the internal mechanics of DiTs.\nIn this paper, we propose DiTFlow, the first motion transfer method tailored for DiTs. We leverage the global attention token-based processing of the video, inherent to DiTs, to extract motion patterns directly from the analysis of attention blocks. With this representation, referred to as Attention Motion Flow (AMF), we are able to condition the motion of the synthesized video content, as we show in Figure 1. We exploit an optimization-based, training-free strategy, coherently with related literature [59, 62]. In practice, we optimize the latent representation of the video across different denoising steps to minimize the distance to a reference AMF. While employing a separate optimization process for each video yields the best performance, we also discover that optimizing the positional embeddings within DiTs enables the transfer of learned motion to new generations without further optimization, hence in a fully zero-shot scenario not previously possible with UNet-based approaches. This potentially lowers the computational cost of transferring motion on multiple synthesized videos. Overall, our novel contributions are the following:\n1.  We propose Attention Motion Flow as guidance for motion transfer on DiTs.\n2.  We propose an extension to our optimization objective in this DiT setting, demonstrating zero-shot motion transfer when training positional embeddings.\n3.  We test DiTFlow on state-of-the-art large-scale DiT baselines for T2V, providing extensive comparisons across multiple metrics and user studies."}, {"title": "2. Related works", "content": "Text-to-video approaches. Following the success of diffusion models [19, 48, 50, 51] in generating images from text [41, 42, 44], methods to handle the extra temporal dimension in videos were developed [2, 3, 5, 15, 20, 25]. These approaches commonly rely on the UNet [45] architecture with separate temporal attention modules operating on solely the temporal dimension for cross-frame consistency. Recently, Diffusion Transformer (DiT) based approaches for text-to-image (T2I) [6, 12, 37] and text-to-video (T2V) [4, 7, 16, 32, 33, 61, 66] have shown superior performance in quality and motion consistency. In particular, VDT [32] highlights the transformer's ability to capture long-range temporal patterns and its scalability.\nMotion transfer. Motion transfer consists of synthesizing novel videos following the motion of a reference one. Unlike video-to-video translation [10, 31, 46], motion transfer approaches aim for complete disentanglement of the original video structure, focusing on motion alone. Some methods use training to condition on motion signals like trajectories, bounding boxes and motion masks [8, 11, 56, 57, 60, 63, 64], but this implies significant costs. Other approaches train motion embeddings [23] or finetune model parameters [15, 22, 58, 65]. However, these methods use separate attention for temporal and spatial information, making them unsuitable for DiTs. Optimization-based approaches extract a motion representation at inference [14, 21, 59, 62], which is more suitable for cross-architecture applications. Token-Flow [14] has a nearest-neighbor based approach on diffusion features, employing expensive sliding window analysis. SMM [62] employ spatial averaging, while MOFT [59] discover motion channels in diffusion features.\nAttention control in diffusion models. Attention features containing semantic correspondences can be manipulated to control generation [13, 17, 40]. Video editing approaches modify the style or subject with feature injection [1, 28, 31, 55] or gradients [10, 36]. Note that previous UNet-based methods have computed a loss on attention in order to transfer motion [30]. However, this assumes separate temporal attention with easily separable motion. This is an unreasonable assumption for DiTs that use full spatio-temporal attention where disentangling motion patterns from content becomes more challenging."}, {"title": "3. Preliminaries", "content": "In this section, we introduce the basic formalism and concepts necessary for DiTFlow. We begin by reviewing the inference mechanics of T2V diffusion models (Section 3.1). We then introduce DiTs for video generation (Section 3.2)."}, {"title": "3.1. Text-to-video diffusion models", "content": "Let us consider a pre-trained T2V diffusion model. We aim to map sampled Gaussian noise to an output video $z_0$ using a denoising network $\\epsilon_\\theta$ over $t \\in [0,T]$ denoising operations [19]. To reduce the computational cost of multi-frame"}, {"title": "3.2. Video generation with DiT", "content": "Unlike U-Net diffusion models [44], DiT-based systems [37] treat the noisy latent as a sequence of tokens, taking inspiration from patching mechanisms typical of Vision Transformers [9]. The denoising network $\\epsilon_\\theta$ is replaced with a transformer-based architecture. Latent patches of size $P \\times P$ are encoded into tokens with dimension $D$ and reorganized into a sequence of shape $(F \\cdot S) \\times D$. The denoising network $\\epsilon_D$ is composed of $N$ DiT blocks [37] consisting of multi-head self-attention [54] and linear layers. To encode positional information between patches during attention, a positional embedding $p$, consisting of values dependent on the patch location in the sequence, conditions the denoising network $\\epsilon_\\theta(z_t, C, t, p)$. Various position encoding schemes exist [47, 52, 54] where $p$ is most commonly either added directly to patches at the start of $\\epsilon_\\theta$ or augment the queries and keys at each attention block."}, {"title": "4. Method", "content": "Our core idea is to take advantage of the attention mechanism in DiTs to extract motion patterns across video frames in a zero-shot manner. Building on the intuition behind motion cue extraction discussed in Section 4.1, we then describe the creation of AMFs in Section 4.2. The AMF extracted from a reference video can be used as an optimization objective at a particular transformer block in the denoising network, as illustrated in Figure 2. We define how we use the AMF for optimization in Section 4.3. Note that the extracted motion patterns are independent from the input content, enabling the application of motion from a reference video to arbitrary target conditions."}, {"title": "4.1. Cross-frame Attention Extraction", "content": "We aim to extract the diffusion features of a reference video $x_{ref}$ with a pretrained T2V DiT model in order to obtain a signal for motion. The motion of subjects in a video may be described by highly correlated content that changes spatially over time, so tokens with similar spatial content will intuitively attend to each other across frames. Hence, we can benefit from extracted token dependencies between frames to reconstruct how a specific element will move over time. We start by computing the latent $z_{ref} = \\mathcal{E}(x_{ref})$ and pass it through the transformer at denoising step $t = 0$ with an empty textual prompt. Previous work [59] have observed a cleaner motion signal at lower denoising steps and we found $t = 0$ to be suitable for feature analysis of the input video. This also avoids the need for expensive DDIM inversion [49] for our task. Let us consider the $n$-th DiT block of $\\epsilon_\\theta$. The self-attention layer computes keys $K$ and queries $Q$ for $M$ attention heads for all $F \\cdot S = F \\cdot (\\frac{W}{P} \\cdot \\frac{H}{P})$ tokens, where $D_h = D/M$ is the dimension of each head. We average over the heads for feature extraction to reduce noise and memory consumption when optimizing. All future reference to $K$ and $Q$ are assumed to be averaged over heads. We represent this operation as:\n${Q, K} \\leftarrow \\epsilon_\\theta(z_{ref}, \\emptyset, 0, p)$.\nHence, for two frames $(i, j)$, $i, j \\in [1, F]$ we can calculate the cross-frame attention $A_{i,j}^{ref}$, as follows:\n$A_{i,j}^{ref} = \\sigma \\left( \\frac{Q_i K_j^T}{\\sqrt{d_k}} \\right) \\in \\mathbb{R}^{(F \\cdot S) \\times (F \\cdot S)}$\nIn Equation (3), $Q_i$ and $K_j$ refer to the query and key matrices of the $i$-th and $j$-th frames with shape $S \\times D_h$. Here, $\\sigma$ is the softmax operation over the final dimension i.e. over tokens in the $j$-th frame, $\\sqrt{d_k}$ is the attention scaling term [54] and $\\tau$ is a temperature parameter. Intuitively, $A_{i,j}^{ref}$ encodes the relationship between patches of frames $i$ and $j$ from $x_{ref}$, serving as a signal to capture the reference video motion."}, {"title": "4.2. Attention Motion Flows", "content": "We subsequently capture the AMF of the video by estimating a displacement map of spatial patches across all frame combinations. Each frame is composed of $\\frac{W}{P} \\frac{H}{P}$ patches. Intuitively, we are interested in understanding how the content of all patches of frame $i$ move to obtain the frame $j$. To do so, we first process $A_{i,j}^{ref}$ with an argmax operation, to identify uniquely, for each patch in the $i$-th frame, the index of the patch most attended to in the $j$-th frame. We call this value $\\hat{A}_{i,j}$. Selecting only one index with argmax empirically produces a cleaner map leading to more reliable motion guidance. We then use the obtained index pairs to construct a patch displacement matrix $\\Delta_{i,j}$ of size $S \\times 2$. Each element in $\\Delta_{i,j}$ indicates, for a given coordinate $(u, v)$, the displacement $(\\delta u, \\delta v)$ on the $u$ and $v$ axes between the $(u, v)$ cell and the matched cell in frame $j$ indicated by the index in $\\hat{A}_{i,j}$, i.e. the $(u + \\delta u, v + \\delta v)$ cell. The procedure is illustrated in Figure 3 (top). Finally, we aggregate the information for all $i, j$ to construct the reference AMF that we will use as the motion guidance signal:\n$\\text{AMF}(x_{ref}) = {\\Delta_{i,j}} \\; i, j \\in [1, F]$.\nOur extracted AMF follows the idea of motion vectors used in MPEG-4 patch-based video compression [43], but is applied to DiT latent representations. In summary, for each patch, we calculate a motion vector in a two-dimensional coordinate space that indicates where the patch will move from frame $i$ to frame $j$, effectively capturing the motion."}, {"title": "4.3. Guidance and Optimization", "content": "Once the reference AMF is obtained from $x_{ref}$, we use it to guide the generation of new video content with a T2V DiT model. Specifically, we aim to guide the denoising of $z_T \\sim \\mathcal{N}(0, I)$ in such a way that $x_0 = \\mathcal{D}(z_0)$ reproduces the same motion patterns as $x_{ref}$. Here, we enforce guidance with an optimization-based strategy, aimed to reproduce the same extracted AMF at a given transformer block for the generated video in intermediate denoising steps $t$. For a given $t$, we consider key $K$ and query $Q$ extracted by the $n$-th DiT block while processing the input latent $z_t$:\n${Q, K} \\leftarrow \\epsilon_\\theta(z_t, C, t, p_t)$\nWe follow the procedure described in Equation (3) to extract the corresponding cross-frame attention $A_{i,j}^{zt}$:\n$A_{i,j}^{zt} = \\sigma \\left( \\frac{Q_i K_j^T}{\\sqrt{d_k}} \\right)$"}, {"title": "5. Experiments", "content": "5.1. Experimental Setup\nDataset and metrics. For evaluation, we use 50 unique videos from the DAVIS dataset [39] coherently with the state-of-the-art [59, 62]. To allow for a fine-grained motion transfer assessment, we test each video with three different prompts in order of similarity from the original video: (1) a Caption prompt, created by simply captioning the video. This allows us to verify that the network disentangles the content from that of the original frames. (2) A Subject prompt, obtained by changing the subject while keeping the background the same. (3) Lastly, a Scene prompt, describing a completely different scene. This makes 150 motion-prompt pairs in total. For the evaluation, we use an Image Quality (IQ) metric for frame-wise prompt fidelity assessment based on CLIPScore [18] and a Motion Fidelity (MF) metric for motion tracklet consistency following [62]. The MF metric [62] compares the similarity of tracklets on $x$ and $x_{ref}$ obtained from off-the-shelf tracking [24]."}, {"title": "5.2. Benchmarks", "content": "Quantitative evaluation We evaluate the effectiveness of DiTFlow when optimizing $z_t$ in order to directly compare our AMF-based guidance with baselines. In the results presented in Table 1, we consistently outperform all baselines. In particular, we considerably improve MF in both 5B and 2B models, scoring 0.785 and 0.726 respectively. In comparison, the best baseline, SMM, achieves 0.766 and 0.688, demonstrating our superior capabilities to capture motion."}, {"title": "5.3. Zero-shot generation", "content": "We evaluate the zero-shot capabilities of DiTFlow by running inference with our optimized $p$ and a new input prompt without further optimization. We quantify performance across the three prompt categories. We optimize $p$ on one prompt and infer with a different prompt. For instance, the optimized representation on the Caption prompt is used in a zero-shot manner to generate outputs with Subject and Scene prompts. Using the 5B model, we compare our methods to SMM [62], the best baseline method on MF in Table 1. In Figure 7a, we report average zero-shot performance across all prompts. We define Ours-zt as DiTFlow with $z_t$ optimization and Ours-p as DiTFlow with positional embedding optimization following Section 4.3.\nWhile optimizing $z_t$ leads to better zero-shot motion preservation (-0.2%) compared to $p$ (-5.2%), we report a significant drop in prompt adherence (-4.4% vs -0.3%). As seen in Figure 7b, this drop is significant as injecting the optimized $z_t$ partially leaks the content of the optimization prompt. In the last columns, while the lion is rendered correctly, the output video still features the lab coat mentioned in the prompt. Optimizing $p$ in the final row avoids this issue while rendering the desired motion, as the positional embedding has no impact on generated semantics, thus disentangling motion from content. Ultimately, with our findings on DiTs, we provide a novel adaptable strategy for prioritizing absolute performance through latent optimization or zero-shot capabilities using positional embeddings."}, {"title": "5.4. Ablation studies", "content": "We ablate our design choices for DiTFlow on CogVideoX-5B. In Table 2, we show the impact of 1) the DiT block index where we optimize, 2) the denoising iterations with guidance and 3) the number of optimization steps by varying each in isolation. We run DiTFlow on 14 unique videos and corresponding prompts. In Table 2a, we show that the first blocks of CogVideoX-5B have incremental importance in motion guidance. We select the 20th block yielding best metrics. We notice that increasing the number of denoising (Table 2b) and optimization steps (Table 2c) positively impacts motion transfer at the cost of more computation. In particular, we report best performance for 40% of T (MF 0.813) and 10 optimization steps (MF 0.803). Each of these setups double the optimization time for each generation, hence we selected 20% of T and 5 optimization steps for the best speed/quality tradeoff. Nevertheless, this enables improved motion transfer by investing in computation."}, {"title": "6. Conclusions", "content": "We present DiTFlow, the first DiT-specific method for motion transfer, based on our novel AMF formulation. Through extensive experiments and multiple evaluations, we demonstrate the effectiveness of DiTFlow against baselines in motion transfer, with extensions to zero-shot generation. Improved video action representations in DiTs may lower costs for generating robotic simulations and enable intuitive control of subject actions in content creation."}, {"title": "Supplementary Material", "content": "We strongly encourage readers to check the qualitative video samples in the project page at ditflow.github.10. Here, we provide additional elements for easing the understanding of our work. Specifically, we first provide implementation details in Section A. Then, we provide additional reasoning about alternative strategies for supervision (Section B). Finally, we discuss limitations (Section C).\nA. Implementation\nPositional embedding training details CogVideoX-5B uses a different positional embedding mechanism to CogVideoX-2B. CogVideoX-2B uses 3D sinusoidal embeddings similar to [54] and these are simply added to the tokens to provide absolute positional information. During guidance, gradients can backpropagate from the AMF loss at block 15 to these embeddings. CogVideoX-5B uses 3D rotary positional embeddings (RoPE [52]) that are embedded into all queries and keys at each attention block. Gradients still backpropagate from block 20 to the ROPE applied to all previous blocks.\nDataset We provide a sample of the dataset in Table 3. Video names are the same as those used in the DAVIS dataset [39]. Please refer to the supplementary material included in the project page for the full dataset and visuals.\nB. Nearest neighbor alternatives\nAn alternative signal for AMF construction could have been the usage of nearest neighbors on noisy latents, as in related works [14]. In Figure 8, we visualize correspondences extracted between two frames using this technique and compare it to our AMF displacement. We demonstrate a much smoother displacement map, which can lead to better guidance on the rendered video.\nC. Limitations\nAs seen in previous methods [62], generations are still limited to the pre-trained video generator, so it has difficulty transferring motion with prompts or motions that are out of distribution. For example, complex body movements (e.g. backflips) still remain a difficult task for these models. Moreover, we highlight that motion transfer is inherently ambiguous if not associated to prompts. For example, transferring the motion of a dog to a plane may risk to map motion features of other elements in the scene to the plane in the rendered video, even with KV injection. For future work, we believe it will be important to associate specific semantic directions (e.g. dog \u2192 plane) to constrain editing, similar to what happens in inversion-based editing [35]."}]}