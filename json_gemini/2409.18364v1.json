{"title": "Multi-hypotheses Conditioned Point Cloud Diffusion\nfor 3D Human Reconstruction from Occluded Images", "authors": ["Donghwan Kim", "Tae-Kyun Kim"], "abstract": "3D human shape reconstruction under severe occlusion due to human-object or\nhuman-human interaction is a challenging problem. Parametric models i.e. SMPL(-\nX), which are based on the statistics across human shapes, can represent whole\nhuman body shapes but are limited to minimally-clothed human shapes. Implicit-\nfunction-based methods extract features from the parametric models to employ\nprior knowledge of human bodies and can capture geometric details such as clothing\nand hair. However, they often struggle to handle misaligned parametric models and\ninpaint occluded regions given a single RGB image. In this work, we propose a\nnovel pipeline, MHCDIFF, Multi-hypotheses Conditioned Point Cloud Diffusion,\ncomposed of point cloud diffusion conditioned on probabilistic distributions for\npixel-aligned detailed 3D human reconstruction under occlusion. Compared to\nprevious implicit-function-based methods, the point cloud diffusion model can\ncapture the global consistent features to generate the occluded regions, and the\ndenoising process corrects the misaligned SMPL meshes. The core of MHCDIFF\nis extracting local features from multiple hypothesized SMPL(-X) meshes and\naggregating the set of features to condition the diffusion model. In the experiments\non CAPE and MultiHuman datasets, the proposed method outperforms various\nSOTA methods based on SMPL, implicit functions, point cloud diffusion, and their\ncombined, under synthetic and real occlusions.", "sections": [{"title": "1 Introduction", "content": "Realistic virtual humans play a significant role in various industries, such as metaverse, tele-presence,\nand game modeling. However, conventional methods require expensive artist efforts and complex\nscanning equipments, so they are not readily applicable. A more practical approach is to reconstruct\nhigh-fidelity 3D humans from 2D images taken in the wild. This is still an ongoing research task due\nto its challenges; people wear a wide variety of clothing styles and adopt diverse poses. Furthermore,\nhuman-object and human-human interaction, fundamental aspects of daily social life, make it more\nchallenging due to severe occlusions.\nExisting 3D human reconstruction methods cannot predict the pixel-aligned 3D shapes of humans\nrobustly from occluded images. The parametric body models [27, 45, 64, 90, 72] have been widely\nused to reconstruct 3D human shapes. Several methods [11, 28, 34, 80, 26, 15, 10, 41, 79] predict the\nparameters of the statistical models and are robust to occlusion because they can be trained on large\nscale datasets [23, 53] and parametric models are well regularized with human body priors. However,\nthe parametric models lack geometric details like clothing and hair, so these approaches cannot align\nthe results to the subjects with loose clothing. More recently, 3D clothed human reconstruction\nmethods [73, 74, 103, 89, 7, 88, 87, 92, 93], which are based on implicit functions and integrate the\nhuman body prior from the 3D body models, i.e SMPL [45, 64], present pixel-aligned detail shapes.\nDespite the impressive advances of the previous methods, they are not robust to occlusion because (1)\nsmall misalignment of estimated parametric models ruins the final shapes, (2) the implicit function\ntakes features independently and cannot inpaint the invisible regions with missing image features,\nand (3) datasets [71, 84, 63] usually consists of segmented full-body images.\nTo address the aforementioned limitations, we propose MHCDIFF (Multi-hypotheses Conditioned\nPoint Cloud Diffusion). (1) Several existing methods [6, 57, 70, 75, 76, 35, 8, 58, 78, 14] predict\nmultiple SMPL meshes to model uncertainty due to occlusions. The sampled distribution is also\nimportant prior knowledge of human motions, but none of the existing work utilizes the distribution\nfor pixel-aligned 3D human reconstruction. We leverage the multi-hypotheses to be robust on the\nmisalignment of each sample. (2) We adopt denoising diffusion probabilistic models (DDPMS)\n[20] to take global consistent features and generate the invisible regions. Diffusion based methods\ngenerate 3D shapes by denoising point clouds [47, 105, 60, 22], latent [98, 59, 36], neural fields [66],\n3D Gaussian [81] or meshes [43]. We adopt the unstructured point clouds to project pixel-aligned\nimage features at each diffusion step. (3) Additionally, we synthesize partial body images by random\nmasking [104], augmenting the limited datasets.\nSpecifically, our goal is pixel-aligned and detailed 3D human reconstruction in a robust manner\nto occlusion in images. Given a single occluded RGB image, we extract 2D features and generate\nmultiple plausible SMPL hypotheses using an off-the-shelf method [4, 14]. The proposed method,\nMHCDIFF, Multi-hypotheses Conditioned Point Cloud Diffusion, performs the diffusion process to\ndenoise a randomly-sampled point cloud into a target human shape. To reconstruct a pixel-aligned\n3D shape and leverage the human body prior, the diffusion process is conditioned on the projected\nimage feature (Sec. 3) and local features extracted from SMPL (Sec. 4.2). The key of MHCDIFF\nis a novel conditional diffusion process with multiple hypotheses (Sec. 4.3), which is not sensitive\nto misaligned SMPL estimation. Given global 2D features and the distribution of hypotheses, the\ndenoising diffusion model can generate the occluded parts (Sec. 4.4)\nWe train MHCDIFF on randomly masked THuman2.0 dataset [82]. Our experiments on CAPE\ndataset [50, 65] with synthesized occlusion and MultiHuman dataset [102] with real-world interaction\ndemonstrate that MHCDIFF reconstructs pixel-aligned 3D human shapes robustly to various occlusion\nratios and achieves state-of-the-art performance. Our main contributions are as follows:\n\u2022 We introduce a novel multi-hypotheses conditioning mechanism that effectively captures\nthe distribution of multiple plausible SMPL meshes. It is robust to the noise of each SMPL\nestimation due to the occlusion of given images. To the best of our knowledge, MHCDIFF\nis the first work that extends the multi-hypotheses SMPL estimation to pixel-aligned 3D\nhuman reconstruction.\n\u2022 We adopt point cloud diffusion model to capture the global consistent features and inpaint\nthe invisible parts. Unlike the previous implicit function, the misaligned SMPL estimation\ncan be corrected during the denoising process. The point cloud diffusion model also offers\ndetailed human meshes."}, {"title": "2 Related Work", "content": "2.1 Diffusion models for point clouds\nOver the past years, denoising diffusion probabilistic models (DDPMs) [20] have been applied to\npoint clouds. For unconditional generation, Luo et al. [47], Zhou et al. [105] and LION [98] use\nPointNet [67], Point-Voxel-CNN [44] and latent space, respectively. PointInfinity [22] tackles the\nquadratic complexity of transformer [86], and generates high-resolution point clouds with a fixed-size\nlatent vector. Otherwise, Point-E [60] is a text-conditioned generation model using CLIP [69] and\nPDR [48] is a point cloud completion method from partial point clouds. PC2 [54], which is the\nbaseline of MHCDIFF, reconstructs the point cloud conditioned on projected image features (please\nrefer to Sec. 3 for more details).\n2.2 Explicit-shape-based human reconstruction\nParametric models [27, 45, 64, 90, 72] have been primary representations for 3D human reconstruction.\nDue to the strength that they capture the statistics across a large corpus of human shapes, a lot of\nwork [11, 28, 34, 80, 26, 15, 10, 41, 79] reconstructs 3D body meshes from an RGB image. To\nreduce the gaps between the image and parameter space of the statistical models and improve image\nalignment, they propose intermediate representations or additional supervisions, such as semantic\nsegmentation [61, 91, 33, 97] and keypoints [9, 38]. To model the uncertainty due to occlusions or\ndepth ambiguities, some work proposes multi-hypotheses [6], heatmaps [57], probability density\nfunctions [70, 75, 76, 35] or diffusion models [8, 58, 99, 78]. ProPose [14] adopts the matrix Fisher\ndistribution [13, 30] over SO(3) for the joint rotation conditioned on the von Mises-Fisher distribution\n[52] for the unit directions of bones, which is not only mathematically correct but also learning\nfriendly (please refer to Sec. 3 for more details). However, these methods are limited to recovering\nminimally-clothed humans and lack the ability to capture geometric details such as clothing and hair.\nSeveral works aim at modeling geometric details in explicit shapes such as meshes, voxels, depth\nmaps and point clouds. Mesh-based methods [1, 2, 3, 37, 106, 5, 25] model 3D offsets on the vertices\nof SMPL [45], but they do not generalize on loose clothing such as skirts and dresses. Voxel-based\nmethods [24, 85, 17, 83] reconstruct 3D human shapes in fine-grained voxel representations. However,\nfree-form 3D reconstruction is challenging without prior, and they need high computation costs to\noutput high-resolution 3D shapes. Point-cloud-based methods [49, 96, 51, 19, 82] model point clouds\nof clothing humans. Han et al. [19] estimate depth maps based on different body parts, and convert\nthe depth maps into point clouds. Tang et al. [82], the most related work, reconstruct 3D humans with\npoint cloud diffusion from an RGB image. First, they convert the estimated SMPL mesh and depth\nmap from the RGB image to point clouds. Conditioned on this point cloud, the conditional diffusion\nmodel refines the point cloud. However, they only handle complete images without occlusion and are\nnot robust to misaligned SMPL estimation.\n2.3 Implicit-function-based human reconstruction\nImplicit-function-based methods regress occupancy fields [55] or signed distance fields (SDF)\n[62] utilizing Multi-Layer Perceptron (MLP) decoders as implicit functions (IF). PIFu [73] and\nPIFuHD [74], which are pioneering works, extract pixel-aligned image features for clothed 3D human\nreconstruction. Later works [103, 89, 7, 88, 87, 92, 100, 101, 93] leverage parametric models as prior\ninformation on the human body. They extract global features from voxelized SMPL meshes with\na 3D encoder [103, 87] or local features such as signed distances and normals from SMPL meshes\n[89, 92, 100, 101] or both [7, 93]. The use of global features helps regularize global shapes and\nensure consistency and local features help reconstruct local details. However, the global encoder is\nsensitive to global pose changes of SMPL and decreases the performance given misaligned SMPL\nestimation due to occlusion. The local features do not contain the global consistent features and\ncannot inpaint the occluded parts. Wang et al. [87] aim to reconstruct complete 3D shapes from\noccluded images by primarily using the generative global encoder with a discriminator, but only\nassuming the accurate SMPL meshes."}, {"title": "3 Preliminary", "content": "PC2 [54]. The projection-conditioned point cloud diffusion model is proposed for single-view 3D\nshape reconstruction. Denoising diffusion probabilistic model [20], which is the foundation of this\nframework, learns to recurrently transform noise $X_T \\sim \\mathcal{N}(0, I)$ into a sample from the target data\ndistribution $X \\sim q(X_0)$ over a series of steps. In order to learn this denoising process, a neural\nnetwork is trained $F_\\theta(X_{t-1}|X_t) \\approx q(X_{t-1}|X_t)$. To reconstruct geometrically consistent 3D point\nclouds from single RGB images $I \\in \\mathbb{R}^{H \\times W \\times 3}$, 2D feature map $\\mathcal{E}(I) \\in \\mathbb{R}^{h \\times w \\times c}$ is projected onto the\npartially denoised points at each step in the diffusion process. Therefore, $F_\\theta(\\cdot) : \\mathbb{R}^{(3+c)N} \\rightarrow \\mathbb{R}^{3N}$ is\na function that predicts the noise $\\epsilon \\in \\mathbb{R}^{3N}$ from the point cloud $X_t \\in \\mathbb{R}^{3N}$ and the projected features\n$X_{proj} \\in \\mathbb{R}^{CN}$, where c is the number of feature channels.\nProPose [14]. Recovering accurate body meshes and 3D joint rotations from single images remains\na challenging problem, particularly in cases of severe occlusion, including self-occlusion and occlu-\nsion from other subjects or objects. ProPose [14] addresses this limitation by modeling the probability\ndistributions for human mesh recovery. Since the pose parameters $\\theta \\in \\mathbb{R}^{72}$ of SMPL [45] represent\nthe 3D rotation of each joint and the root orientation, they adopt the matrix Fisher distribution [13, 30]\nover SO(3). Due to the gaps between the RGB images and the rotation representations, the neural\nnetwork cannot easily model the distribution. ProPose [14] also introduces 3D unit vectors for bone\ndirections as the corresponding observation on the previous matrix Fisher distribution as the prior.\nLeveraging Bayesian inference, they model the posterior distribution of the joint rotations from the\nprior distribution and observation."}, {"title": "4 MHCDIFF: Multi-hypotheses Conditioned Point Cloud Diffusion", "content": "4.1 Overview\nOur work aims at reconstructing pixel-aligned 3D human shape as a point cloud given a single\noccluded RGB image via conditional point cloud diffusion, as shown in Fig. 2. Formally, the\ndiffusion model $F_\\theta(\\cdot)$ learns the conditional distribution $q(X_0|I)$ of 3D human shapes given the RGB\nimages $I \\in \\mathbb{R}^{H \\times W \\times 3}$. Following PC2, we extract the 2D feature map $\\mathcal{E}(I) \\in \\mathbb{R}^{h \\times w \\times c}$ using ViT\n[12], to capture the details in the images. The image features are projected onto the partially denoised\npoints: $X_{proj} = \\Pi(\\mathcal{E}(I), X_t)$, where $\\Pi$ is the projection function. This helps obtain pixel-aligned\ndetailed body shapes. Additionally, the diffusion model is conditioned on the local features $X_{SMPL}$\nfrom SMPL mesh S to exploit statistical human body priors to complete 3D shapes from occluded\nbody parts (Sec. 4.2). However, the SMPL estimation from single occluded RGB images has a\nhigh probability of large errors. To tackle this, we propose a novel multi-hypotheses conditioned\ndiffusion model that considers the distribution of multiple plausible SMPL meshes ${S_i}_{i\\in{1,...,s}}$\n(Sec. 4.3). Given the partially denoised point cloud $X_t$, the projected image features $X_{proj}$, and the\nlocal features from SMPL $X_{SMPL}$, MHCDIFF predicts the noise $\\epsilon$:\n$F_\\theta(X_t, X_{proj}, X_{SMPL}) = \\epsilon.$  (1)\nWe also discuss how MHCDIFF takes the generative property and the global consistent features to\nreconstruct occluded parts (Sec. 4.4).\n4.2 Local features from SMPL\nGiven the SMPL (or SMPL-X) mesh S and the partially denoised point cloud $X_t$ at t-th diffusion\nstep, we extract the local features $X_{SMPL}$ as:\n$X_{SMPL} = [\\gamma(d(X_t|S)), n(X_t|S)],$ (2)\nwhere $d(\\cdot) : \\mathbb{R}^3 \\rightarrow \\mathbb{R}$ and $n(\\cdot) : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$ are the signed distance and normal obtained from the\nclosest surface of SMPL mesh respectively. In order to map scalar values to a higher dimensional\nspace, we adopt an encoding inspired by the positional encoding in NeRF [56]:"}, {"title": "4.3 Multi-hypotheses condition", "content": "The local features are robust to noisy SMPL estimation, but cannot correct the SMPL estimation\nerrors. Following previous multi-hypotheses human pose estimation [6, 40, 18, 21, 8], MHCDIFF\ntakes multi-hypotheses SMPL meshes from estimated distributions and predicts the most plausible\noutputs. We modify Eq. 2 to handle multiple sampled SMPL meshes ${S_i}_{i\\in{1,...,s}}$ using ProPose\n[14] as an off-the-shelf method:\n$X_{SMPL} = [\\gamma(d(X_t|S_i)), n(X_t|S_i)],$ (4)\nwhere $i = argmin_{i \\in {1,...,s}}d(X_t|S_i)$, which semantically means that each point follows the closest\nSMPL mesh $S_i$ to consider all plausible samples in denoising steps. However, each point gets\nconditions from only one sample and cannot leverage off-the-shelf probability distributions. In\naddition to the local features, we also adopt occupancy values:\n$X_{SMPL} = [\\frac{1}{s}\\sum_{i=1}^{s} \\gamma(o(X_t|S_i)), \\gamma(d(X_t|S_i)), n(X_t|S_i)],$ (5)"}, {"title": "4.4 Conditioned point cloud diffusion model", "content": "Finally, $F_\\theta(\\cdot): \\mathbb{R}^{(3+c+4L+3)N} \\rightarrow \\mathbb{R}^{3N}$ predicts the noise $\\epsilon \\in \\mathbb{R}^{3N}$ given the concatenation of\npartially denoised point cloud $X_t \\in \\mathbb{R}^{3N}$, projected image features $X_{proj} \\in \\mathbb{R}^{CN}$, and local features\nfrom SMPL $X_{SMPL} \\in \\mathbb{R}^{(4L+3)N}$ (Eq. 1). Notably, we do not need any learnable parameters to\nextract the local features from SMPL and aggregate the features of multiple SMPL meshes. We\nfreeze the pre-trained 2D image encoder, so it is straightforward to train the diffusion model without\nadditional training strategies.\nThe point cloud diffusion model of MHCDIFF takes the role of the decoder of previous implicit-\nfunction-based methods. Given the encoded features from RGB images or SMPL meshes, the decoder\npredicts 3D shapes such as point clouds, occupancy fields, or signed distance fields. The implicit-\nfunction-based methods need to sample the query points randomly, so the decoder has been primarily\nMulti-Layer Perceptron (MLP), which takes the input points independently. MHCDIFF consists of\nthe point cloud diffusion model instead of MLP because (1) the point cloud model considers the\nglobal consistent features, (2) the diffusion model has the generative properties, and (3) the denoising\nprocess approximates correcting the misaligned SMPL estimation. Given the globally encoded image\nfeatures $X_{proj}$ and the local features from SMPL $X_{SMPL}$, MHCDIFF can inpaint or restore invisible\nbody parts and is robust to noisy SMPL estimation due to occlusion."}, {"title": "5 Experiments", "content": "Implementation. We use the Pytorch3D library [68] for image feature projection (Sec. 3) and the\nkaolin library [16] to extract local features from SMPL (Sec. 4.2). MHCDIFF is trained with batch\nsize 8 in 100,000 steps. We use MSN [4] as the image feature encoder. We use AdamW [31] with\n$\\beta = (0.9, 0.999)$ and a learning rate which is decayed linearly from 0.0002 to 0. For diffusion noise\nschedule, we use linear scheduling from $1 \\cdot 10^{-5}$ to $8 \\cdot 10^{-3}$ with warmup. For inference, we denoise\nthe point cloud for 1,000 steps. The training process takes approximately 1 day on a single 24GB\nNVIDIA RTX 4090 GPU with 28M learnable parameters.\nLearning. We synthesize the THuman2.0 dataset [95], which contains 526 high-fidelity textured\nscans with corresponding SMPL-X fits. We use 500 subjects for training and the others for validation.\nWe render each human subject from 36 multiple viewpoints and randomly mask the images, resulting\nin partially occluded body images. We use the farthest point sampling operation to sample 16,384\npoints from each GT scan. During the training, local features $X_{SMPL}$ are extracted from a single\ncorresponding GT SMPL-X. The learning pipeline is presented in Algorithm 1."}, {"title": "5.1 Comparison with state-of-the-art methods", "content": "MHCDIFF outperforms prior implicit-function-based methods and SMPL estimation methods on\noccluded and even full-body images. Fig. 4 presents the robustness of 3D human reconstruction\nto the occlusion ratio. PaMIR and HiLo cannot handle the occlusions because the global feature\nencoder is sensitive to misaligned SMPL estimation. SIFU does not use the 3D encoder, but the\ncross-attention from the normal map of SMPL takes global features and is sensitive to occlusion\nand misaligned SMPL estimation. ICON shows comparable robustness due to its locality, but worse\nquality than ProPose estimation used to condition as the occlusion ratio increases. On the contrary,\nMHCDIFF is as robust as the statistical models, showing the most accurate results for all occlusion\nratios. The results of 40% occlusion ratio are also displayed in numbers in Tab. 1. Tab. 2 presents the\nperformance on real-world interaction scenarios with MultiHuman dataset. The dataset is divided\ninto 5 categories by the level of occlusions and we compare the performance in each category. Similar"}, {"title": "5.2 Ablation study", "content": "We conduct an ablation on MHCDIFF to validate the effectiveness of each component. In Tab. 3-B,\nwe condition the diffusion model with single SMPL-X (PIXIE) or SMPL (ProPose) estimation. We\nimprove the performance with multi-hypotheses condition (Sec. 4.3). From PC2 [54], which only\ntakes image condition, we also validate the local features from SMPL in Tab. 3-A. All of these\nfeatures improve the performance, especially the signed distance. In Tab. 3-C, MHCDIFF is trained\nwithout random masking or by conditioning the distribution estimated by ProPose [14] instead of GT\nSMPL-X."}, {"title": "6 Conclusion", "content": "In this paper, We present MHCDIFF, which robustly reconstructs pixel-aligned and detailed 3D\nhumans from single occluded images. Rather than implicit-function-based methods, we choose\nthe point cloud diffusion model to generate invisible regions capturing the features globally. Our\nmulti-hypotheses conditioning mechanism extracts local features from multiple SMPL estimations\nand integrates them without learnable parameters, so MHCDIFF is robust to a single erroneous\nSMPL due to occlusion. We augment the limited training data by random masking to synthesize\nocclusion by diverse interaction. The experiments demonstrate that our proposed method outperforms\nstate-of-the-art methods from various levels of occlusion and interaction. In the future, the point\ncloud of human shapes can be applied to intermediate stages for implicit function [55] and human\nbody deformation [42]."}, {"title": "A Broader impact", "content": "Our method can be potentially used for AR/VR applications. The real-world interaction can be cap-\ntured and modeled in virtual scenes, which can be extended to reinforcement learning. However, there\nare potential risks associated with falsifying human avatars, which could inadvertently compromise\npersonal privacy. Consequently, there is a pressing need to establish regulations that clarify the fair\nuse of such technology."}, {"title": "B Limitations", "content": "Our method, based on DDPM [20] sampling with 1,000 steps, has limitation on efficiency. The\ntraining time is reasonable because we do not need query point sampling, which yields CPU bottleneck\nto learn implicit-function. However, evaluation on CAPE dataset takes about 12 hours, while other\nimplicit-function-based methods take about 30 minutes. We can apply DDIM [77] sampling with\nfewer steps to shorten the inference time."}, {"title": "C Pointcloud to mesh", "content": "Following previous work [82, 19], we try to convert our reconstructed point cloud to mesh with\nScreened Poisson surface reconstruction [29]. However, the process takes about 10 hours per sample\nwith 16, 384 points. The implicit function [55] converts the point clouds to occupancy fields by\nencoding features with a PointNet [67]. This two-stage pipeline can generate occluded regions and\ncapture details. We will try this pipeline in our future work."}, {"title": "D Statistical significance", "content": "We evaluate MHCDIFF on CAPE dataset [50, 65] with 10 different random seeds. The random seeds\neffect on random noise in the diffusion process and SMPL sampling from the estimated distribution\nvis ProPose [14]. The Chamfer Distance and Point-to-Surface are 1.872(\u00b10.008) and 1.810(\u00b10.008)\nwith 1-sigma error bars, respectively."}, {"title": "E Qualitative results", "content": "For the real-world interaction, we evaluate MHCDIFF on MultiHuman [102] and Hi4D [94] datasets.\nWe render the textured scans with Pytorch3D library [68] for MultiHuman dataset, and segment each\nsubject with pre-trained network [39] for Hi4D dataset. Our proposed method is robust not only to\nthe occlusion but also to noise in full images or segmentation process."}]}