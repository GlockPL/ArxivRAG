{"title": "CAD-GPT: Synthesising CAD Construction Sequence with Spatial Reasoning-Enhanced Multimodal LLMs", "authors": ["Siyu Wang", "Cailian Chen", "Xinyi Le", "Qimin Xu", "Lei Xu", "Yanzhou Zhang", "Jie Yang"], "abstract": "Computer-aided design (CAD) significantly enhances the efficiency, accuracy, and innovation of design processes by enabling precise 2D and 3D modeling, extensive analysis, and optimization. Existing methods for creating CAD models rely on latent vectors or point clouds, which are difficult to obtain and costly to store. Recent advances in Multimodal Large Language Models (MLLMs) have inspired researchers to use natural language instructions and images for CAD model construction. However, these models still struggle with inferring accurate 3D spatial location and orientation, leading to inaccuracies in determining the spatial 3D starting points and extrusion directions for constructing geometries. This work introduces CAD-GPT, a CAD synthesis method with spatial reasoning-enhanced MLLM that takes either a single image or a textual description as input. To achieve precise spatial inference, our approach introduces a 3D Modeling Spatial Mechanism. This method maps 3D spatial positions and 3D sketch plane rotation angles into a 1D linguistic feature space using a specialized spatial unfolding mechanism, while discretizing 2D sketch coordinates into an appropriate planar space to enable precise determination of spatial starting position, sketch orientation, and 2D sketch coordinate translations. Extensive experiments demonstrate that CAD-GPT consistently outperforms existing state-of-the-art methods in CAD model synthesis, both quantitatively and qualitatively.", "sections": [{"title": "Introduction", "content": "Computer-Aided Design (CAD) has become the standard approach for designing, drafting, and modeling in a wide range of industries(Robertson and Allen 1993; Chen and Olechowski 2024). Almost every manufactured object that exists today started its life in a parametric CAD tool. The CAD command sequence is one type of CAD model representation. It is described as a sequence of operations such as drawing 2d sketches and extruding sketches into 3D solid shapes(Wu, Xiao, and Zheng 2021). Constructing these CAD models requires domain expertise and spatial inference capabilities, and it can also be time-consuming.\nRecently, the most popular direction for CAD model generation focused on using generative models like variational autoencoder(VAE) (Wu, Xiao, and Zheng 2021) and vector quantized variational autoencoder(VQ-VAE) (Xu et al. 2022, 2023). These methods map CAD models to vectors or codebooks in a high-dimensional latent space and then reconstruct the original CAD models from these high-dimensional representations. The main limitations of both methods include: 1) The quality of CAD models synthesized by these methods depends not only on the methods' capabilities but also on the quality of the provided guidance vectors or codebooks, which can inevitably result in cumulative errors. 2) These methods require high dimensional data, similar to the distribution of their vectors or codebooks as inputs, which are difficult to obtain directly. Another line of work directly infers CAD sequences from point clouds(Ma et al. 2023; Khan et al. 2024) or sketches(Li et al. 2022). In practical applications, sketches need to be drawn by professionals, and point clouds require specialized equipment for collection, both of which involve high data acquisition costs.\nGenerative AI tools such as Multimodal Large Language Models (MLLMs) have the potential to remove these barriers. These multimodal models exhibit impressive visual language understanding and generation capabilities(Achiam et al. 2023; Yin et al. 2023). Recently, there have been initial attempts to use state-of-the-art MLLMs for the creation of CAD models(Makatura et al. 2023; Badagabettu, Yarlagadda, and Farimani 2024). Experiments show that these models, such as GPT-4, lack spatial reasoning capabilities(Makatura et al. 2023) and have a low success rate(Badagabettu, Yarlagadda, and Farimani 2024) in generating the desired CAD models. These limitations can manifest as notable challenges in the design and manufacturing domain. For instance, they may generate a car with four horizontally placed wheels or a table with legs that exceed the tabletop and are randomly positioned. Hence, the main question we ask is: How to enhance the 3D spatial reasoning capabilities of multimodal large language models for accurate CAD model synthesis?"}, {"title": "Related Work", "content": "Approximate 3D Representation\nAccurate and efficient 3D data representation remains a challenge in computer graphics and vision. Point Clouds (Zhou, Du, and Wu 2021; Luo and Hu 2021; Nichol et al. 2022) capture discrete spatial points, offering simplicity but lacking surface details; Meshes(Groueix et al. 2018; Wang et al. 2018; Chen et al. 2024b; Siddiqui et al. 2024) use vertices and edges to form polygons, providing connectivity but facing complexity issues; 3D Gaussians model (Kerbl et al. 2023; Tang et al. 2023) use points with Gaussian distributions for efficient rendering, yet they miss precise surface features; and Neural Radiance Fields (NeRF) (Mildenhall et al. 2021) employ neural networks for volumetric modeling, requiring substantial computational resources and data. These representations often struggle with noise, incomplete details, and limited editability.\nComputer-Aided Design Model Representations\nDirect B-rep Generation involves synthesizing the underlying parametric curves and surfaces and the topology that connects them to create a solid model(Wang et al. 2020; Sharma et al. 2020). This work focuses on developing a generative model for CAD construction sequences rather than B-reps. However, converting a B-rep into a construction sequence is challenging, as multiple command sequences can produce the same B-rep.\nCAD Construction Sequence Generation DeepCAD (Wu, Xiao, and Zheng 2021) was the first to propose a sketch-extrusion construction sequence representation for CAD models, predicting CAD history from latent vectors or point clouds as a preliminary experiment. HNC-CAD (Xu et al. 2023) introduced a hierarchical code tree representation for CAD sequences based on VQ-VAE, which can autoregressively generate various CAD sequences from"}, {"title": "Method", "content": "Overview\nIn this section, we first briefly introduce the model architecture of CAD-GPT. After that, we describe the representation of CAD command sequences. Next, we propose the 3D modeling spatial localization mechanism that enhances the spatial reasoning capabilities of the base MLLM.\nModel Architecture\nAn efficient MLLM can be divided into three main modules: a visual encoder g tasked with processing visual inputs, a pre-trained language model f\u00f8(\u00b7) parameterized by $\\theta$ that manages the received multimodal signals and performs reasoning, and a visual-language projector P which functions as a bridge to align the two modalities. We adopt LLaVA-1.5 7B version (Liu et al. 2024) as our base model with the pre-trained Vicuna (Chiang et al. 2023) as our pedestal LLM. Vicuna is built on LLaMA-2 (Touvron et al. 2023).\nFor an input image Iv, utilizing the pre-trained visual encoder ViT-L/14-336px as g, which provides the visual feature Z = g(Iv). We consider a simple two-layer linear layer as the vision-language projector to map the visual patch embeddings Z into the text feature space: S = P(Z). Thus, we have a sequence of visual tokens Sv, which can be understood just like the text tokens Sq by the LLM. Specifically, for a sequence of length L, we compute the probability of the target answers Sa by:\n$p(S_a | I_v, S_{instruct}) = \\prod_{i=1}^L P_\\theta(x_i | S_v, S_{instruct}, S_{a,<i})$ (1)\nwhere $\\theta$ is the trainable parameters, $S_{instruct,<i}$ and $S_{a,<i}$ are the instruction and answer tokens before the current prediction token $x_i$.\nCAD Command Sequence Representation\nFollowing the DeepCAD dataset (Wu, Xiao, and Zheng 2021), a CAD model is represented as a sequence of modeling operations that the user executes to construct a 3D 3 d shape. This type of CAD model is saved in JSON format, storing key modeling commands and parameters (see in the order of CAD construction. The sequence of commands is human-readable and easily editable. Moreover, JSON is one of the formats used for the LLaMA-2 pre-training corpus and aligns with its prior knowledge. Consequently, we directly preserve the JSON-formatted CAD modeling sequence as the output format of our model.\nThese commands describe a CAD model Mas a sequence of pairs of curve and extrusion commands interleaved. In other words, M is a command sequence M = [C1,..., CN.], where each C\u2081 has the form (ti, pi), specifying the command type ti and parameters pr. The commands include details for determining the global starting point of the sketch, the angles between the sketch plane and the three coordinate axes, various parameters for drawing the 2D sketch. Based on these commands, the 2D sketches can be iteratively drawn and extruded to form a 3D model.\nTo execute an extrusion command, one must first define the profile's sketch plane's 3D orientation and spatial location. This ensures that closed curves can be accurately drawn at the correct 2D local starting point on the correctly oriented sketch plane. The orientation of the sketch plane is defined by the parameters \u03b8, \u03b3, \u03c6. At the same time, the spatial location is specified by the coordinates px, Py, Pz, which denote the origin of the sketch plane (see Table 1). Sketch commands define closed curves on a 2D plane, with curve parameters specifying the curve's 2D location in the sketch plane's local frame. We consider three widely used curve commands: drawing a line, an arc, and a circle. Precise command types and 2D coordinates are crucial for accurately sketching the design. In summary, a sketch profile S is described by a list of loops S = [Q1, \u2026\u2026\u2026,QN], where each"}, {"title": "3D Modeling Spatial Localization Mechanism", "content": "Selecting the coordinates of the 3D sketch plane origin coordinate, determining the 3D sketch plane orientation, and then drawing the sketch involves complex mathematical and 3D geometric reasoning processes. Our preliminary experiments demonstrate that MLLMs struggle to infer these parameters accurately, leading to low precision and high failure rates in CAD model generation. To address these challenges, we propose a 3D Modeling Spatial Localization Mechanism.\nSpecifically, we have designed three series of localization tokens to replace the parameters for sketch plane origin coordinates, orientation angles of the sketch plane, and 2D sketch curve coordinates. These tokens have been added to the LLM's vocabulary, enabling the model to reason about 3D spatial transformations as seamlessly as it generates words. A detailed explanation of this method is provided in the following sections. Each type of token is enclosed by two distinct boundary tokens, which are composed of special tokens. These boundary tokens serve to signal the model to output the corresponding series of localization tokens. All tokens constructed for the 3D Modeling Spatial Localization Mechanism are presented in Table 2.\n3D Sketch Plane Orientation Tokens In the CAD construction sequence, the orientation is represented by a rotation matrix composed of three consecutive parameters: \u03b8, \u03c6, and y. This matrix is designed to align the world frame of reference to the plane's local frame of reference, specifically orienting the z-axis to match the plane's normal direction. Following the order of 0 \u2192 \u03c6 \u2192 \u03b3, progressing from lowest to highest, each angle is discretized into 9 integer values, resulting in a total of 729 orientation tokens represented as\n<An>, where $\\eta \\varepsilon N^{728}$.\n(2)\nAfter this, the angles are aligned with the language space of large language models as part of the linguistic structure.\n3D Coordinate Localization Tokens We normalize each CAD model within a 1 \u00d7 1 \u00d7 1 cube. Next, we discretize the vertices' coordinates into K\u00b3 grids, where K = 36. The grids are sorted in z \u2192 yx order, from lower to higher, following MeshGPT (Siddiqui et al. 2024) and Polygen (Nash et al. 2020). The order indices of these grids are used to construct our position tokens, forming a sequence of location tokens\n<Pk>, where k\u2208NK\u00b3-1.\n(3)\nFor instance, a spatial point O\u2082, if its normalized coordinates (Pxi, Pyi, Pzi) are located within grid or, then its corresponding position coordinate is Po\n2D Sketch Coordinate Tokens After normalizing the 3D model to a 1 \u00d7 1 \u00d7 1 cube, we also normalize each 2D sketch profile within its bounding box and quantize their values into 128 levels. Consequently, the x and y coordinates are represented by two series of tokens as follows:\n{\n<S1X>\n<SmY>\nwhere l, m\u2208 N127.\n(4)\nThese tokens indicate the discretized levels of x and y coordinates for the 2D sketch.\nAugmenting Spatial Features with Position Embeddings\nWe introduced four distinct types of tokens to the vocabulary. Correspondingly, we expanded the embedding layers to accommodate these additional tokens and incorporated learnable position embedding layers to enhance the representation of the four types of spatial information. The use of learnable position embeddings allows the model to understand the relative positioning and relationships within the spatial data, enhancing the accuracy and expressiveness of the representation. Specifically, we introduced the following learnable position embedding matrices: Wangle \u2208 R729\u00d7D, W3D_pos \u2208 RK\u00b3\u00d7D, W2D_sketch_x \u2208 R128\u00d7D, and W2D_sketch_y \u2208 R128\u00d7D. These matrices were used to augment the embeddings of the corresponding token types, enhancing their spatial information representations."}, {"title": "Experiments Settings", "content": "In this section, we first introduce the detailed training parameters and strategies of our method. We then present the CAD generation results for both image and text input conditions. Additionally, we conduct ablation studies to compare the performance of the baseline multimodal model with and without our 3D Modeling Spatial Localization Mechanism, demonstrating the effectiveness of our approach.\nImplementation Details\nWe freeze the linear mapping layer and vision encoder weights of LLaVA, while fully fine-tuning the language base model. We constructed our data input model based on the LLaVA fine-tuning format, incorporating mixed image-CAD sequence data and text-only description-CAD sequence data. The training involves two stages: first training on the image2CAD task, followed by fine-tuning on the text2CAD task with a reduced learning rate. During training, the newly introduced embedding layers are initialized based on the original vocabulary embedding. The network was trained using a batch size of 8 per GPU across 4\u00d7 NVIDIA RTX A800 GPUs, with a total training duration of 96 hours. The initial learning rate is set to 2 \u00d7 10-5, with a Cosine-Warmup learning rate initialization strategy and a warm-up ratio of 0.3. Additionally, following an extrapolation optimization strategy, we adjust certain parameters, expanding the model's maximum input sequence length to 8192.\nMetrics\nTo comprehensively evaluate the predicted sequences, we employ a set of metrics that assess different aspects of the predictions. Specifically, the final CAD reconstructions are quantitatively analyzed against ground-truth CAD models using Chamfer Distances (CD) (Fan, Su, and Guibas 2017). Since CAD sequences are predicted as tokens, they may not always generate successfully rendered CAD models when reconstructed with OpenCascade, we introduce an Invalidity Ratio(IR) metric, expressed as a percentage, which quantifies the proportion of invalid models. In addition, we evaluate command accuracy using two metrics: Command Accuracy (ACCcmd) and Parameter Accuracy (ACCparam).\nCAD Generation from a Single Image\nQualitative Analysis In this section, we provide additional qualitative results on single-view image conditioning.As shown in  we compared our approach against three representative methods. The first is DeepCAD, which exemplifies advanced generation techniques in CAD modeling. The second is GPT-4, representing the cutting-edge in closed-source multimodal large models. The third is Qwen2-VL-Max, one of the leading open-source multimodal large models. As observed, DeepCAD struggles with generating fine details, while GPT-4 exhibits limitations in spatial reasoning, frequently leading to errors in generated models. Qwen2-VL-Max, despite multiple attempts, consistently failed to render the generated JSON correctly. In contrast, our model produces outputs that are both accurate and aesthetically refined.\nQuantitative Comparison with Existing Methods In comparison with current CAD generation approaches, we use DeepCAD as a compare method. In addition, we compare our method with two recent autoregressive generative models, namely SkexGen (Xu et al. 2022) and HNC-CAD (Xu et al. 2023). We employ the same pre-trained visual encoder, ViT-L/14-336, and map its output to the same latent space for these methods. Additionally, we compare our method with the state-of-the-art multimodal large model GPT-4, with specific prompts detailed in the supplementary materials. As shown in Table 3, CAD-GPT achieves a median CD of 9.77, representing a 48% reduction compared to the best-performing baseline HNC-CAD's 18.64. Furthermore, it achieves an 84% lower than GPT-4's 62.64, demonstrating significantly higher reconstruction accuracy. In terms of the IR, CAD-GPT achieves a 91% reduction compared to the best-performing baseline, HNC-CAD, and a 97% reduction compared to the state-of-the-art multimodal model, GPT-4, demonstrating a significant improvement in generating valid CAD models. CAD-GPT also outperforms other methods on the two additional ACC metrics, demonstrating superior command generation accuracy. These results underscore CAD-GPT's superior precision and validity in CAD model reconstructions.\nCAD Generation from Text Descriptions\nQualitative Analysis In this section, we present additional qualitative results on text conditioning. Due to the lack of directly comparable CAD generation methods, we selected two representative large language models: GPT-4, a leading closed-source model, and LLaMA-3.1 (405B), a state-of-the-art open-source model. As illustrated in  our model consistently generates high-precision, aesthetically pleasing outputs that align well with the textual descriptions across various scenarios. In contrast, GPT-4 frequently produces incorrect models with a high failure rate, while LLaMA-3.1 only occasionally succeeds in rendering models, and even then, the results often do not match the provided descriptions.\nQuantitative Comparison with Existing Methods We compare our approach with GPT-4 and the state-of-the-art open-source model LLaMA-3.1. We provide both models with the same background and input them with identical modeling instructions or text descriptions to generate the corresponding modeling code. As shown in , CAD-GPT achieves a median CD of 83% lower than GPT-4's 187.52, and reduces the IR to 7.43, a 90% decrease compared to GPT-4 and 92% compared to LLaMA-3.1. This highlights CAD-GPT's superior accuracy and lower failure rates in CAD model reconstruction under text description inputs. In terms of ACC metrics, CAD-GPT outperforms the other two methods up to 6%.\nAblation Study\nThe impact of the components proposed in CAD-GPT is evaluated in , focusing on CAD reconstruction metrics, including IR, mean CD, ACCcmd and ACCparam. The first row of the table shows the results when only the original data is trained, without our additional tokens and position embeddings. This configuration results in a decline"}, {"title": "Conclusion", "content": "In this paper, we introduce CAD-GPT, a multimodal large model enhanced with the 3D Modeling Spatial Localization Mechanism to improve spatial reasoning capabilities. Our model excels at inferring variations in sketch orientations, changes in 3D spatial positions, and accurately rendering 2D sketches. Leveraging these capabilities, CAD-GPT demonstrates exceptional performance in generating precise CAD models under both image and text input conditions."}]}