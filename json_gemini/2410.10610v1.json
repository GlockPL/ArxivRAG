{"title": "Intelligent prospector v2.0: exploration drill planning\nunder epistemic model uncertainty", "authors": ["John Mern", "Anthony Corso", "Damian Burch", "Kurt House", "Jef Caers"], "abstract": "Optimal Bayesian decision making on what geoscientific data to acquire requires stating a prior model of\nuncertainty. Data acquisition is then optimized by reducing uncertainty on some property of interest maximally,\nand on average. In the context of exploration, very few, sometimes no data at all, is available prior to data\nacquisition planning. The prior model therefore needs to include human interpretations on the nature of spatial\nvariability, or on analogue data deemed relevant for the area being explored. In mineral exploration, for example,\nhumans may rely on conceptual models on the genesis of the mineralization to define multiple hypotheses, each\nrepresenting a specific spatial variability of mineralization. More often than not, after the data is acquired, all of\nthe stated hypotheses may be proven incorrect, i.e. falsified, hence prior hypotheses need to be revised, or additional\nhypotheses generated. Planning data acquisition under wrong geological priors is likely to be inefficient since the\nestimated uncertainty on the target property is incorrect, hence uncertainty may not be reduced at all. In this paper,\nwe develop an intelligent agent based on partially observable Markov decision processes that plans optimally in\nthe case of multiple geological or geoscientific hypotheses on the nature of spatial variability. Additionally, the\nartificial intelligence is equipped with a method that allows detecting, early on, whether the human stated\nhypotheses are incorrect, thereby saving considerable expense in data acquisition. Our approach is tested on a\nsediment-hosted copper deposit, and the algorithm presented has aided in the characterization of an ultra high-\ngrade deposit in Zambia in 2023.", "sections": [{"title": "1. Introduction", "content": "In the long term, the energy transition will demand a significant number of discoveries of critical minerals such as\nCopper, Nickel, Lithium Cobalt, and rare Earth elements. Exploration success in the discovery of large deposits of\ncritical minerals and raw materials has been declining over the last three decades (Schodde, R.C., 2017), hence the\naverage cost of making such discovery has increased significantly. At the current rate, the needed mineral resources\nwill not be discovered in time. An important reason for this decline is that most major discoveries that are\noutcropping at the surface have been discovered (Davies et al., 2021; Savacool et al., 2020; Schodde, R.C., 2014).\nExploration therefore needs to focus on a much harder problem: finding mineral resources under cover, without\nvisible surface outcropping. This means that the role of geophysical & geochemical exploration and drilling will\nincrease in importance. Geophysics, while essential in many exploration ventures, is not always successful in\ndetecting ore bodies, in particular in discovering deeper, smaller, but larger grade resources. This is certainly true\nin the Copperbelt where high copper grades (over 5%) are contained in relatively thin layers termed ore shale\nlocated up to 1500m deep. Contrast this with very large porphyry systems at 0.5% grade mined with open pits.\nHigh grade deposits have the additional advantage of being much less destructive and use less water since they can\nbe mined underground, as opposed to large scale open pit mining of many low-grade porphyry copper deposits,\nleading to significant environmental justice concerns (Agusdinata et al., 2018).\nArtificial intelligence has recently received increasing attention to speed up discoveries. Before we proceed with\nthe specific contributions of this paper, a review on what artificial intelligence is and how it is currently used in\ncritical mineral exploration is necessary to avoid any confusion on the AI used in this work. First, unsupervised\nlearning is a form of AI that searches the data for interesting patterns or anomalies in possibly large datasets,\nwithout having any reference labels such as known discoveries. In the mineral exploration context this data consists\nof geological, geochemical, and geophysical data from airborne, surface, or drilling (Wood, D., 2019). Orebodies\nare geochemical and mineralogical anomalies in the Earth, and often their composition remains not always well\nunderstood. Unsupervised learning can find both spatial, geophysical and compositional anomalies (e.g. Clare &\nCohen, 2001; Abedi et al., 2013; Gazley et al., 2015; Zhang et al., 2019; Shirmard et al., 2022; Zuo & Carranza,\n2023). For orebodies to be economical the need to be spatially compact, and they need to be compositionally\ndistinguishable from the host-rock. Second, when label data (examples of true deposits) are available (such as in"}, {"title": "2. Epistemic uncertainty when exploring in the Copperbelt", "content": "The Copperbelt is known for containing sediment-hosted copper deposits that have now been studied (Hitzman et\nal., 2005) for well over 100 years, and where major discoveries have already been made. Key to the generation of\nthese deposits is the presence of an oxidized, metal-rich source rock, the availability of salts to facilitate the\ntransport of these metals in brines, the presence of a reducing unit that will ultimately host the mineralization, and\ngeodynamic processes that can drive fluid flow between the source and the reducing unit. Explanation of geological\nprocesses often proceeds using conceptual diagrams describing the source, transport and eventual trap (mechanical\nor chemical) of the mineralizing fluids (Selley et al., 2018; McCuaig et al., 2018). Holistically, geologists generally\nagree on how this worked for sediment-hosted copper deposits, but such knowledge is not specific enough yet to\nbe helpful in quantitative decision making based on actual 3D geological models. For example, geologists may\nsuggest the presence of a graben structure, but there is very little to no information that is specific to that area about\nthe geometric nature of the faults that define such graben. Similarly, the pathway of copper-bearing fluids is\ndependent on the unknown permeability of rocks surrounding the shale, hence the specific extent of a precipitation\nfront is not known either.\nBecause of the ongoing appraisal and mine planning at the Mingomba discovery, we cannot disclose any\napplication of the intelligent prospector in that specific case. Instead, we create an analogue virtual case that has all\nthe complexities of the actual case, specifically focusing on the problem of epistemic uncertainty. Our methodology\nis also not specific to drilling and/or sediment-hosted copper. As will be shown, it has application in data acquisition\nunder epistemic uncertainty, a problem hardly limited to mineral exploration. Even though the geological modelling\nwill be limited to 2D thickness and grade, the methodology extends to any form of 3D geological modelling.\nThe geological model used in our study is a 2D map containing the thickness and grade of the mineral deposit at\neach (x, y) location. The mineral thickness is sampled from a Gaussian process, but the parameters of the Gaussian\nmodel are dependent on the presence or absence of a graben structure (where the mean thickness is higher within\nthe graben). Similarly, we model the grade of the mineralization using a Gaussian process with parameters that\ndepend on the presence of various geochemical alterations. The total mineralization at each grid point is computed\nby multiplying thickness by grade. To assess the economic value of a deposit, we make assumptions on the price\nper ton of ore, the costs to mine and process that ore, and assume that only ore content above a target grade is"}, {"title": "3. Methodology", "content": "In previous work (Mern & Caers, 2023) we framed any data acquisition campaign as a sequential decision process,\nwhich we will review again in this section. In a sequential problem, a decision-making agent must take a sequence\nof actions to reach an objective (goal or reward). Information gained from each action in the sequence can inform\nthe choice of subsequent actions. An optimal action sequence will account for the expected information gain from\neach action and its impact on future decisions. This type of conditional planning may be referred to as closed-loop\nor feedback control. The actions considered in the paper are drilling locations or sequences thereof.\nA sequential decision problem can be modelled formally as a Markov decision process (MDP). An MDP\nis a mathematical description of a sequential decision process defined by a collection of probability distributions,\nspaces, and functions. The full MDP is typically defined by the tuple (S, A, T,r,y). The state space S is the space"}, {"title": "3.1 Review of a POMDP formulation for sequential planning", "content": "of all states that the decision-making problem may take at any step. In the mineral exploration process, the state is\ndefined by the geological model of the subsurface deposit as well as the locations of existing drill holes. The action\nspace A defines the set of all actions that the agent may take. In mineral exploration drilling, this would be the set\nof all locations that the agent may possibly drill. The transition model T(St+1 | St, at), is the probability distribution\nover the next time step state St+1, conditioned on the current state and action. The step t refers to the sequential\nactions and belief updates. The MDP formulation assumes that the state transition is fully informed by the\nimmediately preceding state and action, which is the Markovian assumption. The transition model may be\ndeterministic, as in the case of mineral exploration, where the underlying state doesn't change except for the\nlocation of additional bore holes, as they are drilled.\nThe reward function r(St, at, St+1): S \u00d7 A \u00d7 S \u2192 R gives a measure of how taking an action from a state\ncontributes to the utility of the total action sequence which the agent seeks to maximize. The objective of an agent\nin an MDP is to maximize the sum of all rewards accumulated over an action sequence. To preference rewards\nearlier in the process, a time discount factor \u03b3 \u2208 (0,1] is used. The goal of solving an MDP is to maximize the sum\nof discounted rewards accumulated from a given state, defined as\n\u03a3\u03c4=1yt-1r(St, at, St+1)\nfor a decision process with T steps. The sum of discounted rewards expected from a state is defined as the value of\nthe state V(s). Given that the exact state transitions are not generally known in advance, the optimization target of\nsolving an MDP is to maximize the expected value.\nIn many decision-making problems, and all real subsurface problems, the state at each time step (e.g. the 3D\ngeological variability) is not fully known. In this case, agents make decisions based on imperfect observations of\nthe relevant states of their environments. Sequential problems with state uncertainty are modelled as partially\nobservable Markov decision processes (POMDPs). POMDPs are defined by the MDP tuple plus an observation\nspace O and a likelihood model for observations_L(0t+1 | St+1,at). The observation space defines all the\nobservations that the agent may make after taking an action. Observations are generally noisy measurements of a\nsubset of the state. The observation model defines the conditional distribution of the observation given the state\nand action."}, {"title": "3.2 POMDP for exploration drilling under epistemic uncertainty", "content": "To solve a POMDP, an agent must account for all the information gained from the sequence of previous\nobservations when taking an action. It is common to represent the information gained from an observation sequence\nas a belief. A belief is a probability distribution over the unknown state of the world at a given time step. At the\nbeginning of the decision-making process, the agent will start with a belief that is defined by all prior knowledge\nof the state available before making any observations. These include all model parameters that are uncertain and\ntheir prior distribution. Model parameters can be geological hypotheses, parameters within each hypothesis, and\nspatial variability (of e.g. grade) modelled on a grid. With each observation made, the belief is updated, typically\nusing a Bayesian update as\nb(St+1) \u221d L(0t+1|St+1,at)b(St).\nNote that b(st+1) is AI notation for a posterior p(s|o), where p(s) is the prior. A belief may be an analytically\ndefined probability distribution or an approximate distribution, such as a state ensemble updated with a particle\nfilter.\nEach decision in the sequence is made using the belief updated from the preceding observation. The process\nis depicted in Figure 4. An optimal choice in a sequential problem should consider all subsequent steps in the\nsequence. However, the number of trajectories of actions and observations reachable from a given state grows\nexponentially with the length of the sequence. As a result, optimizing conditional plans exactly is generally\nintractable. Instead, most POMDPs are solved approximately using stochastic planning and learning methods,\nwhich we cover in a later section."}, {"title": "3.2.1 Overview", "content": "Specifically, for our exploration drilling problem we have the following broad description of the POMDP model\n1. States: joint state of geological state, hypothesis state, and drill states\na.\nGeological State: spatial distribution of grade, thickness, and surface depth, defined over a\ncartesian grid.\nb. Hypothesis State: A tuple of the generating hypothesis scenario and a model of the domains over\nthe exploration area. In a Bayesian context this is termed a latent variable (not directly observed)\nc. Drill States: The target pierce point and remaining time of each drill rig.\n2. Actions: The locations that may be targeted for drilling, defined on a 2D cartesian grid. The set may be\nconstrained by time-varying constraints.\n3. Observations: grade and thickness extracted from drill-hole data"}, {"title": "3.2.3 State space definition", "content": "Rewards depends on the model state, and if the full model state were observed, the reward would be perfectly\nknown. This is not the case, and hence uncertainty in the state needs to be quantified. To do that we need to specify\nthe random variables defining the geological model. Quite generally, one can define the random variables of a\ngeological models through a hierarchy. At the highest level are model hypothesis (e.g. using a Gaussian process,\nan object model, a training image etc..), each of which may contain its own parametrization (e.g. the geometries of\nthe domains contained in the model, the parameters of a variogram). Finally, using hypotheses and their\nparametrization, one can use geostatistical algorithms to generate model realizations in 3D space. Figure 3 shows\nthe specific Bayesian network model that defines dependencies between model parameters and the data informing\nthese parameters."}, {"title": "3.2.4 Belief model", "content": "Because the random variables defining the model are hierarchical, so will be any distribution (prior, likelihood &\nposterior). Prior distributions instantiate the lack of knowledge prior to any drilling, often using analogue\ninformation, while the posterior distribution (or belief) provides an update of the uncertainty on all model\nparameters. Important here is that grid model variables are directly observed (the grade and thickness in a drill\nhole), while others are indirectly observed (the hypothesis).\nThe belief over geological models involves two components: the model over the discrete domains and the model\nover the geological properties (in this case, grade and thickness). For the latter, Gaussian process models were used\nto model the spatial correlation of the properties, but the mean of the process was controlled by the local domain.\nThe discrete domains are explicitly modelled based on a number of geometric parameters (described in more detail\nin the appendix) and implemented in Turing.jl. While belief updates on a Gaussian process can be performed\nanalytically, the update of the parameters controlling the structural and geochemical domains are more challenging\nand involve maintaining distributions over the geometric parameters that describe the domain."}, {"title": "3.3.1 POMDP solvers", "content": "A common approach for solving POMDPs is to estimate an action-value function that computes the expected\nreturns (or value) of taking a particular action in a particular belief:\nQ(b,a) = 1/bi \u03a3(b')\nThe value function can be estimated via online and offline methods. Online methods are solved during execution,\nmeaning each time an action is taken, a new plan is generated to find the next best action. Online methods often\ninvolve a form of forward search such as Monte Carlo tree search, where value function estimates are maintained\nduring the search and there is an explicit trade-off between exploration and exploitation. Offline methods solve for\nthe value function over all possible beliefs prior to execution. While this is intractable to perform exactly for even\nmoderately sized problems, point-based methods where the possible beliefs are sampled, have shown strong\nperformance. Point-based methods typically rely on discretised state, action and observation spaces in order to\nmaintain a piecewise linear representation for the value function and the ability to perform exact belief updates.\nOnline tree-search methods are scalable any-time algorithms that are easy to apply out of the box and have therefore\npreviously been applied to a variety of subsurface problems (Wang et al., 2022, Wang et al. 2023). However, they\nsuffer from high variance in their value function estimates and therefore rely on expert-defined heuristics and\nhyperparameter tuning to be effective. Conversely, offline methods have much lower variance and typically better\nperformance compared to online methods but can be computationally more expensive.\nIn this work we opt for an offline POMDP algorithm for its superior consistency in performance. In particular, we\nuse the Successive Approximations of the Reachable Set under Optimal Policies (SARSOP) algorithm (Kurniawati\net al., 2009). SARSOP focuses on the subset of the belief space that is reachable under optimal policies, which\nsignificantly reduces computational complexity by concentrating on belief points that are most likely to be\nencountered during execution. This is achieved through a combination of forward and backward exploration that\niteratively refines the belief space, allowing the algorithm to focus on high-probability regions while pruning less\nrelevant areas. By doing so, SARSOP strikes a balance between computational efficiency and solution quality,\nmaking it particularly effective for high-dimensional and complex POMDPs.\nTo apply point-based methods like SARSOP to continuous POMDPs, it's necessary to discretize the continuous\nelements of the problem. The state space can be discretized by sampling from the belief distribution, allowing us"}, {"title": "4. Results", "content": "Maintaining multiple distinct geological hypotheses may enable decision makers to better understand the geological\nsystem during exploration and identify when none of the hypotheses are conforming to the observed data (through\ncomparison with a null hypothesis). To characterize the effect of this modelling and decision-making approach we\ndesign experiments to answer two important questions. 1) will the increased aleatoric uncertainty of maintaining\nmultiple geological hypotheses (one correct and multiple incorrect) degrade the performance of algorithmic\ndecision-making approaches to drill targeting? 2) If the correct geological hypothesis is not represented in the\nbelief, will an algorithmic decision-making approach falsify all incorrect hypotheses faster than baseline\napproaches?\nIn answering these questions, we use standard evaluation approaches for decision making systems (Kochenderfer\net al, 2022). In all experiments, we select a ground truth geological model from the 2-graben, 2-geochemical domain\nhypothesis class and use that model to produce observations of the grade and thickness. For each experiment, the\ndecision-making agent maintains a belief (different initial beliefs for different experiments) and recommends bore\nhole locations at each step. After an action is recommended the agent observes the noisy observation of the grade"}, {"title": "4.1 Aleatoric Uncertainty", "content": "The main point of this subsection is to show that having multiple hypotheses is a viable paradigm for representing\na belief and performing drill targeting by solving the resulting POMDP. Figure 8 shows a significant improvement\nover a grid search baseline, in this case. For the same decision accuracy, the POMDP needed less than half as many\nboreholes as grid-based drilling. The drilling outcome for a specific example (one Monte Carlo simulated truth),\ncomparing grid-based drilling (Figure 8) vs a POMDP planner (Figure 9) illustrated the significant difference in\nefficiency."}, {"title": "4.2 Drill planning under a falsified prior model hypothesis", "content": "Having shown that the decision-making agent can quickly discriminate between multiple incorrect hypotheses\nwhen the correct hypothesis class is present in the belief, we now explore the (realistic) setting where all hypotheses\nare incorrect. We first consider the case without any hypothesis falsification check and show a sample trajectory in\nFigure 10. Here we see that since the agent does not consider the presence of a second geochemical domain in its\nbelief, it determines that the current ore is sub-economic and walks away from a prospect that is significantly\nprofitable. This type of wrong decision is extremely costly and illustrates the need to have a wide range of\nhypotheses to maximize the chances of correctly modelling the ground truth geology.\nThen we consider the problem of hypothesis falsification, by checking for falsified hypotheses (by comparing log\nlikelihood of the data to the maximum entropy null hypothesis), after each new bore hole. We record the number\nof falsified bore holes (averaged over 17 trials) for both the POMDP agent and the grid baseline and report the\nresults in Figure 11. We see that the bore holes selected by the POMDP agent are able to more quickly falsify the\nincorrect hypotheses (on average). We note that the POMDP agent is not explicitly optimizing for uncertainty\nreduction or falsification, but rather to determine the maximum expected reward. However, this result is evidence\nthat reducing uncertainty and therefore falsifying hypotheses is a by-product of this simple profit-maximizing\nobjective. Grid-drilling at this spatial resolution is not able to detect that all hypotheses are false."}, {"title": "5. Discussion", "content": "We presented a completely new methodology for drill-planning in mineral exploration and appraisal. Here we\ndiscuss further some elements, both non-scientific and scientific, related to its implementation in actual cases. The\nreality of today's mineral exploration is that such endeavours are mostly performed by so-called junior mining\ncompanies. These companies work with small drilling budgets, and additional funding is provided by investors\nwhen drilling shows encouraging results. This means that the \u201creward\" for drilling is not the reduction of\nuncertainty on geological hypothesis, but targeting zones deemed high in mineralization. What we propose here is\ncompletely different, and also requires a very different investment strategy. We have shown that in this example,\nreducing uncertainty in geological hypotheses is critical to minimizing the number of boreholes in the long run.\nInstead of drilling one borehole at a time, we plan the first borehole knowing that a possible long sequence will be"}, {"title": "6. Conclusions", "content": "Accelerating discovery of critical mineral will require efficient data acquisition by focusing on reducing uncertainty\nof properties of interest such as geological hypothesis, grades and tonnes. In this paper, we present an entirely new\napproach to exploration, in particular to drill planning. At early-stage exploration, the leading uncertainty is usually\nthe definition of conceptual geological hypotheses. These hypotheses are used to create 3D geological models\nwhose uncertainty is dominated by what hypotheses is chosen. In a real exploration setting it is more likely than\nnot that human generated hypotheses early at the exploration stage will eventually be proven incorrect, i.e. falsified,\nafter drilling proceeds. In this paper we develop an artificial intelligent agent that can detect this falsification early\non, and hence prevent drilling under completely incorrect hypothesis. We have shown that are agent in the examples\npresented, is much more efficient that the industry standard of grid-based drilling, and that its first implementation\nhas led to the fast appraisal of an ultra high-grade deposit in Zambia."}, {"title": "7. Appendix", "content": "Note that due to the synthetic nature of the geological models, the following parameters are unitless and should not\nbe interpreted as representing any particular unit. The 2D grids are each 32 grid cells in each dimension. The\nGaussian process models used for grade and thickness all use the same Matern kernel with marginal standard\ndeviation of 0.1 and a correlation length of 3 grid cells. The mean thickness outside of a graben was 1.0 and inside\na graben was 7.5. The mean grade outside of an altered geochemical domain was 0.0 and 0.085 inside a geochemical\ndomain. The noise on both thickness and grade measurements was gaussian with a mean of 0.0 and a standard\ndeviation of 0.001. Each side of the graben is defined by a bottom location (Normal with mean = 11, std = 6)\nand a width (normal distribution with mean = 6, std = 2). Each geochemical domain is defined by a polygonal\narea with a center (normal with mean = 16 and std = 8 in each dimension) and 10 evenly spaced points around\nit that each have a distance from the center that is normally distributed with a mean = 5 and std = 2.5. The null\nhypothesis was selected to be a mixture of non-spatially correlated Gaussians for both the thickness and grade."}, {"title": "Code/data availability", "content": "https://github.com/ancorso/HierarchicalMineralExploration.jl"}, {"title": "Author contribution", "content": "John Mern: conceptualization, coding, analysis; Anthony Corso: conceptualization, coding, analysis; Damian\nBurch (analysis; review; editing); Kurt House (conceptualization, review, editing), Jef Caers (conceptualization,\nwriting, analysis)"}, {"title": "Competing interests", "content": "The authors declare that they have no conflict of interest."}, {"title": "Explicitly stating the probability distributions (as opposed to algorithmically defined ones) allows calculating manyimportant statistics:", "content": "Specific to our case, the following hierarchical definition is used. The hypothesis is assumed to be a discrete choice(a categorical random variable) while parametrization and 3D models may contain a mix of categorical andcontinuous random variables. Following the Bayesian network of Figure 3, the prior distribution is stated asfollows.\nf(H, Dchem, Dgrab, Th, g) = P(H)f(Dchem|H)f(Dgrab|H)f(Th|Dgrab)f(g|Dchem)H refers to the hypothesis, D to domains, either \u201cgrab\u201d is graben, or \u201cchem\u201d is geochemistry, g to grade and Th tothickness at each location. Sampling from this distribution follows the hierarchy of model variables. Further, weuse the following, reasonable assumptions:\u2022 Parameters between domains are independent, for example, grade and thickness distributions are spatiallyindependent between domains.\u2022 Hypothesis scenarios hold over the entire domain."}, {"title": "3.3.2 Belief updating", "content": "To update distributions sequentially, we need the definition of a likelihood of the data accounting for the Bayesianhierarchy. The data variable 0 = {0chem, Ograb, Oth, OG} consists of observation in drill holes of domain, grade,thickness, hence we can decompose the likelihood distribution l for given data acquisition action a as followl(0|a) = \u03a3hEH l(0|h, a)P(h)l(0|a) = \u03a3hEH \u03a3dchem,dgrab l(Oh, dchem, dgrab, a)P(dchem|h)P(dgrab|h)l(0|a) = \u03a3hEH \u03a3dchem,dgrab P(h) \u03a3 l(Ochem|h, dchem, a)l(Ograb|h, dgrab, a)l(Orn|h, d, a)l(Oc|h,d,a)P(dgrab|h)In this formulation, we rely on the hierarchical model definition in Figure 3. First, the law of total probability isused to define the marginal likelihood l(0|a) in terms of the conditional distributions. The first sum is taken overeach hypothesis index h. The second integral is taken over each possible geochemical and graben domains. Finally,one decomposes the likelihood of an observation into the likelihoods of each component (domains, grade, andthickness).2We make the additional assumption that likelihood of the grade and thickness observations are given by a normaldistribution (Gaussian Process) whose mean uand covariance \u03c3\u00b2 are obtained by kriging (Gaussian processregression), respectively. The domain observation is assumed deterministic, defined mathematically by theKronecker delta distribution.l(0g|h, dchem, a) = N(Og | \u03bcn,dchem, (02)n,dchem)l(Orh|h, dgrab, a) = N(Oth| \u03bch,dgrab, (\u03c3\u00b2)h,dgrab)and for the domainsl(Ochem|h, dchem, a) = &dchema(ODchem)l(Ograb|h, dgrab, a) = 8dgraba (ODgrab)The likelihood of the grade and thickness observations are given by a normal distribution defined by kriging (hard)data. The domain observation is a discrete outcome, defined mathematically by the Kronecker delta distribution.We assume that the domain is directly observed, which may not always be the case. We can also calculate theuncertainty of mean and variance of grade and thickness given all uncertainties jointly."}, {"title": "3.2.5 Dealing with a falsified prior model", "content": "It is certainly possible, if not the norm, that none of the hypotheses contain in the set H are correct. What we meanby this, is that at some point, after drilling several boreholes the probability of each hypothesis outcomeP(H = h\u2081) = 0 \u2200 hi, the hypotheses are falsified (in the sense of Popper, see Caers, 2018). However, whenprobability distributions have infinite support (e.g. Gaussians, which are common in geostatistical modelling), thelikelihood of observations (and therefore hypotheses) are never exactly 0, but instead get increasingly small. Thechallenge, however, is that a particular sequence of observations can have extremely small likelihoods, even whencoming from the true hypothesis class, and it is difficult to distinguish this phenomenon from true falsification ofthe prior.\nThe way we approach this problem is by adding another hypothesis ho to the set H, termed a null hypothesis. Thishypothesis is defined as a maximum entropy model, by assuming a high-entropy prior distributions over theobserved variables Th and g and removing any spatial correlation. While there are many choices of non-spatiallycorrelated, high entropy distributions, we select a mixture model of Gaussian distributions that produce similarhistograms to the true underlying geology. In practice, the modelling choice can depend on knowledge of the rangeof property values in the prospect due to drilling. An example null hypothesis sample and corresponding histogramcompared to one hypothesis is shown in Figure 5, and the model parameters are provided in the appendix."}]}