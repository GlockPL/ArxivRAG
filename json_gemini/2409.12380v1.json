{"title": "Bundle Fragments into a Whole: Mining More Complete Clusters via Submodular Selection of Interesting webpages for Web Topic Detection", "authors": ["Junbiao Pang", "Anjing Hu", "Qingming Huang"], "abstract": "Organizing interesting webpages into hot topics is one of key steps to understand the trends of multimodal web data. A state- of-the-art solution is firstly to organize webpages into a large volume of multi-granularity topic candidates; hot topics are further identified by estimating their interestingness. However, these topic candidates contain a large number of fragments of hot topics due to both the inefficient feature representations and the unsupervised topic generation. This paper proposes a bundling-refining approach to mine more complete hot topics from fragments. Concretely, the bundling step organizes the fragment topics into coarse topics; next, the refining step proposes a submodular-based method to refine coarse topics in a scalable approach. The propose unconventional method is simple, yet powerful by leveraging submodular optimization, our approach outperforms the traditional ranking methods which involve the careful design and complex steps. Extensive experiments demonstrate that the proposed approach surpasses the state-of-the-art method (i.e., latent Poisson deconvolution?) 20% accuracy and 10% one on two public data sets, respectively.", "sections": [{"title": "1. Introduction", "content": "With the boom of web technologies, recently it is convenient for people to share data on social media websites which suc- cessfully facilitates both the generation and the propagation of User Generated Content (UGC) ?. Users are constantly strug- gling to keep up with the ever-increasing amounts of content that is being published every day. Extracting the hot yet inter- esting topics from a sea of webpages is an efficient approach to handle the information overload problem.\nHot topics provide an easy way to abstract the public con- cerns, social trends and urgent events. However, the unprece- dented explosion of UGC data has made it difficult for web users to quickly access hot web topics ?. Driven by such press- ing requirement, topic detection from web???? is such an ef- fort to discover a tiny fraction of interesting webpages strongly connected by a seminal event from a sea of social media ?. We believe that the ability to remove the uninteresting webpages yet organize the interesting ones into different seminal events lies at the basis of understanding trends.\nDiscovering hot web topics is like looking for a needle in a haystack. Because only a small number of the interesting web- pages could evolve into the hot topics eventually. To handle this challenge, the Detection-By-Ranking (DBR) approach ? is proposed. Firstly, a huge number of the multi-granularity topic candidates is generated, attempting to hit all hot topics in a brute-force approach; secondly, the interestingness of the top- ics is estimated to discover hot topics. However, due to the lack of the supervised information, the multi-granularity approach inevitably generates the fragments of hot topics? which nat- urally lead to the inferior performances??. Because Poisson Deconvolution (PD) ? empirically tends to rank fragments of hot topics ahead. The essence of this problem is originally from the unsupervised approach.\nA naive idea attempts to generate complete topics in pres- ence of noises via advantaged clustering methods, e.g., low- rank models ?. However, a sea of uninteresting webpages would make the advantaged methods ineffective. For instance, the noise robust spectral clustering? is ineffective to find clusterings when about 96% noises are presented. Moreover, both the multi-language words and the user-defined abbrevia- tions make the dimension of the features from textual modal- ity very high. For instance, Term Frequency-Inverse Document Frequency (TF-IDF) from both Chinese and English has about 54,000 dimensions, which make the univariate projection based method? fail.\nIn this paper, we seek a fragment-to-whole and coarse-to-fine method, based on four motivations. Firstly, bundling the frag- ment topics into a whole would produce complete yet coarse topics; this avoids the drawback of the NMS??. Secondly, refining a topic into the accurate, coherent and interpretable ? one is expected. Thirdly, we avoid the open problem of dis- covering clusterings in a sea of noises ?. Fourthly, without time-complexity optimization, a scalable approach is desired."}, {"title": "2. Related Work", "content": "Web Topic Detection. Webpages are the typical heteroge- neous data. As a result, many literatures have considered topics as clusterings from the multi-modal data ???. There are two important research threads. One adapts a single-modal clus- tering algorithm to the multi-modality?, and the other is the similarity graph method?, where the multi-modalities data are fused into the edges of a graph.\nIn the former case, topic detection extends the single- modality based methods into multi-modal data. For example, multi-modal LDA ? was proposed to group images with tags into topics. In the similarity graph method, multi-modal cues were fused into edges of a similarity graph. For instance, Cao et al.? first generate events on video tags via clustering, and then link these clusterings into topics by the textual-visual sim- ilarity. As a comparison, the fused similarity graph was compu- tationally simple and easily extendable to the other graph-based algorithm ?.\nThe DBR method?? belonged a similarity graph approach. Although a large number of methods has been proposed to clus- tering topics??, a few work notices that many clusters are the fragments of hot topics.\nClassical topic models have been proposed to infer hid- den themes for document analysis, including Latent Dirich- let Allocation (LDA)?, probabilistic Latent Semantic Analysis (pLSA)? and various variations. These topic models gener- ally work well on long and structured documents?. Moreover, these models have assumed that each webpage must belong to a topic. For instance, a subset of twitters was firstly retrieved by a set of predefined keywords, and then LDA was used to organize these twitters into topics ?. On the contrary, our task is to mine a few hot topics in a sea of noise webpages?. Recently, ? have used the pre-trained models based on Transformer? obtain the zero-shot text embedding feature to classifier topic.\nClustering In a Sea of Noise. The ideal approach for hot topics detection from web was to cluster in a sea of noise ?. We also empirically find that only a small fraction of interesting webpages (about 3%) have evolved into hot topics?. Obvi- ously, the partition-based methods are impossible to deal with this problem since this approach assumes that every data point must belong to a cluster ?.\nThere has been a trend of introducing noise-resistant cluster- ing techniques into web topic detection. For instance, Li et al ? utilized Swendsen-Wang cuts to group topics, and Zhang et al ? used the graph shift to group a few webpages into hot topics. In fact, web topics are not equivalent to conventional clus- ters or the zero-shot topic classification ? since the web topics faces the more semantically weak feature representation than that of other scenarios, e.g., high-dimension data ?, budgeted optimization ?. These clustering methods naturally produce fragments of hot topics."}, {"title": "3. The Proposed Bundling-Refining Method", "content": "In this paper, for a set of webpages (x1,...,x}, the visual feature fvis and the textual feature ftxt are firstly extracted from each webpage. Two similarity matrices Wvis and Wixt are con- structed from the visual feature and the textual one, respec- tively\u00b9.\n3.1. Ranking Multi-granularity Topic Candidates\nFollowing the approach in ?, we convert the matrix W into the affinity one $\\mathbf{W}$ by Gaussian kernel, i.e., $W_{ij} = \\exp(-\\frac{||W_{ij}||^2}{\\sigma^2})$ where $|| . ||_2$ denotes the $l_2$ norm, and $\\sigma^2$ denotes the deviation. Once these affinity matrices are con- structed, we build the visual and the textual k-Nearest Neigh- bor Graphs (k-N2Gs), where the $k$ nearest neighbors of each webpage truncate the affinity matrix $\\mathbf{W}$ into the spare one $\\mathbf{A}$ as follows:\n$G^{vis} = (V, E^{vis}, A^{vis}),$ (1)\n$G^{txt} = (V, E^{txt}, A^{txt}),$ (2)\nwhere vertex $V$ denotes webpages, edge $E^{vis}$ ($E^{txt}$) represents whether two webpages are connected by the visual (textual) modality or not, and matrix $A^{vis}$ ($A^{txt}$) describes the closeness of two webpages determined by the visual (textual) features. Subsequently, a mixed graph is obtained as follows:\n$G = (V, E, A),$ (3)"}, {"title": "3.2. Bundling Fragments into a Coarse Topic", "content": "where $E = E^{vis} \\cup E^{txt}$, and $\\mathbf{A} = (A^{vis} + A^{txt})/2$. $E = E^{vis} \\cup E^{txt}$ means that either the individual modality or the multi- modalities could be used to model the similarity between two webpages.\nBy Similarity Cascade (SC) ?, the multi-granularity topics $C_k (k = 1,..., K)$ are generated from the mixed graph $G$. A topic $C_k$ is formally represented as follows:\n$C_k = c_k \\odot c_k, $ (4)\nin which the indicator vector $c_k \\in \\{0, 1\\}^{1 \\times N}$, where 1 or 0 means that whether the topic $C_k$ contains the webpage $x_i$ or not. The operation $\\odot$ means that the diagonal of the matrix $c_k \\odot c_k$ is set to zero. The weight of a topic $\\mathbf{u}_k$ is estimated as follows:\n$\\mathbf{w}_{ij} \\sim Poisson(a_{ij})$\ns.t.: $w_{ij} = \\sum_{k=1}^K \\mu_k c_{kij}$ (5)\nThe interestingness of a topic is estimated as $i_k = \\mu_k \\mid c_k \\mid$, where $ \\mid c_k \\mid$ is the number of webpages in a topic $C_k$. Therefore, we obtain the interestingness order of topics $L = i_1 <... i_k... <i_k$. For more details about the PD-based ranking, please refer to ?.\nWe assume that every topic is a fragment of some hot topic. That is, each topic should at least be bundled into a coarse one. Given the order list $L$ and the bundling window $W$, Jaccard distance between the k-th topic and the subsequent one j-th ($j \\in \\{k+1,...,k+W\\}$) is computed as,\n$J_{kj} = \\frac{|C_k \\cap C_j|}{|C_k \\cup C_j|}$ (6)\nwhere $\\cap$ and $\\cup$ respectively represent the intersection opera- tion and the union operation, and $| . |$ denotes the number of webpages in a set. If $J_{kj}$ is large than a predefined threshold $\\tau$, we bundle the j-th topic and the k-th one into a coarse topic, i.e., $\\hat{C}_k = \\cup C_i$ ($i \\in \\{k, ...,k+W\\}$). Next, we replace the topic $C_k$ with the bundled one $\\hat{C}_k$, as illustrated in Fig. 1.\nTheoretically, the bundling process has a time complexity of O(KW) ($W \\ll K$). In practice, $W$ is frequently smaller than 50. Therefore, the bundling process is quite efficient. However, the uninteresting webpages in the fragments are inevitably merged into the coarse topics. In this paper, the interestingness of web- pages is estimated in Section 3.3, and a submodular-based ap- proach is proposed to refine coarse topics in Section 3.4."}, {"title": "3.3. Modeling Interestingness by Stationary Distribution", "content": "An interesting webpage is defined as the likelihood of that people are attracted. If the correlations among webpages are encoded as a graph, the stationary distribution measures the in- terestingness of a webpage?. However, the structure informa- tion between webpages (e.g., link) is unavailable. We convert a coarse topic into a graph with the similarities among webpages.\nThe reconstructed similarity between webpages is computed as follows:\n$S_{ij} = \\sum_k \\mu_k c_{kij}$ (7)\nwhere $S_{ij}$ ($S_{ij} \\in R^{|\\hat{C}| \\times |\\hat{C}|}$) denotes the point-wise similarities be- tween the webpages from the coarse topic $\\hat{C}$. Rather than using the original similarity $A$, $S$ is considered as a refined similar- ity ?.\nConsequently, a graph $\\hat{G} = (\\hat{V},\\hat{E},S)$ is constructed, where node $\\hat{V}$ represents the webpages in the coarse topic $\\hat{C}$, and edge $\\hat{E}$ denotes the connectivity between these webpages, i.e., if $S_{ij} > 0, \\hat{E}_{ij} = 1$; otherwise, $\\hat{E}_{ij} = 0$.\nAssuming a random walker travelling on the given graph $\\hat{G}$, the probability of a transition from node $i$ to node $j$ is $P_{ij} = S_{ij}/d_i$, where $d_i = \\sum_j S_{ij}$ is the out-degree of the node $i$. $P_{ij}$ is the probability of the node $i$ being visited by the random walker at the 0-th step.\nAiming at computing the stationary distribution $\\pi$, random jumps? are introduced into $P_{ij}$ as follows:\n$\\pi_i = \\alpha \\sum_{j \\in V} \\pi_i P_{ij} + (1 - \\alpha) \\frac{1}{|C|},$ (8)\nwhere $\\alpha \\in (0,1)$ denotes the decay factor that respects the probability with which a walker follows the connected edges, and $1 - \\alpha$ is the probability of jumping to a random node. (8) is PageRank ? algorithm, where $\\alpha$ is usually recommended to range from 0.8 to 0.95?. In this paper, we set $\\alpha = 0.9$ in (8).\nBy defining the new transition matrix as $P \\leftarrow \\alpha P + (1 - \\alpha) ee^T / |C|$ (where the vector $e$ is a column of ones), the station- ary distribution $\\pi$ can be efficiently solved by the power method? with the time complexity O(T$\\cdot$ nz(P)), where T is number of it- erations in the power method, and nz(P) is number of non-zero elements in P. In our experiment, the size of coarse topics $\\hat{C}$ range from 20 to 93. Therefore, the computation cost of (8) is very small."}, {"title": "3.4. Refining a Coarse Topic by Submodular Selection", "content": "To remove uninteresting webpages from a coarse topic, we establish two criteria: 1) Interestingness: a hot topic consists of the interesting webpages. 2) Similarity: the interesting web- pages in a hot topic should be more similar than that of the unin- teresting webpages. By following the above criteria, an imme- diate solution is to select the interesting and similar webpages as follows:\n$\\arg \\max_I \\sum_{i, j} (I[i] \\pi_i) S_{ij} (I[j] \\pi_j)$ (9)\ns.t.: $|I| = k,$\nwhere $\\mathbf{I} \\in \\{0,1\\}^{1\\times|\\hat{C}|}$ is the indicator vector, $\\mathbf{S}$ is the similarity matrix, and $k$ is the number of the selected webpages. However, the optimization of (9) is equal to quadratic integer program- ming which is NP-hard problem.\nIn order to approximate (9), our approach selects a subset of webpages $\\mathcal{P}$ as follows:\n$\\arg \\max_{\\mathcal{P}} g(\\mathcal{P}) = \\lambda \\sum_{i \\in \\mathcal{P}} \\pi_i - \\sum_{i, j \\in \\mathcal{P}} \\pi_i D_{ij} \\pi_j,$ (10)\nwhere $i$ is the index of the selected webpage $p_i \\in \\mathcal{P}$, $D_{ij}$ is the dissimilarity between two webpages, and $\\lambda > 0$ is a parameter that trades off between the interestingness and the dissimilarity. In (10), the first term means that the selected webpages should have the high interestingness scores; while, the second term requires that the selected webpages should have small dis- similarity values between each other. Therefore, the definition of the dissimilarity $D_{ij}$ should follow the following property:\nDefinition 1. [Property of $D_{ij}$ in (10)] Given three webpages $x_i$, $x_j$, and $x_k$, as well as the corresponding dissimilarity values $D_{ij}$ and $D_{kj}$ between these webpages, if $x_i$ is more similar to $x_j$ than that of between $x_i$ and $x_j$, then dissimilarity values should satisfy that $D_{ij} < D_{kj}$.\nBy the definition of the dissimilarity $D$ in (10), the good- ness function $g(\\mathcal{P})$ approximates the objective function (9). In this work, considering that $S_{ij} \\in (0,1)$, the dissimilarity $D$ is converted from $S$ by Gaussian kernel, i.e., $D_{ij} = \\exp(-\\frac{S_{ij}}{\\sigma})$, where $\\sigma$ is the bandwidth to control the decay speed of the sim- ilarity $S$. In this paper, we set $\\sigma = 10$.\n3.4.1. Submodular Selection of (10)\nWe present the so-called diminishing return property of the goodness function (10) in Proposition 1. By Proposition 1, if we add more webpages into an existing subset $\\mathcal{P}$, the goodness of (10) is non-decreasing.\nProposition 1. [Diminishing of $g(\\mathcal{P})$ in (10)] The goodness function in (10) has the following properties:\n*   (P1: Submodularity.) for any $\\lambda > 0$, the goodness $g(\\mathcal{P})$ is submodular with respect to $\\mathcal{P}$;\n*   (P2: Monotonicity.) for any $\\lambda \\geq 2$ and $\\sum_{i,j} D_{ij} = 1$, the goodness $g(\\mathcal{P})$ is monotonically non-decreasing with re- spect to $\\mathcal{P}$.\nProof. We firstly prove (P1). For any $\\mathcal{P}_1 \\subset \\mathcal{P}_2$ and a webpage $x \\notin \\mathcal{P}_2$, we have\n$g(\\mathcal{P}_1 \\cup x) - g(\\mathcal{P}_1) = \\lambda \\sum_{i \\in (\\mathcal{P}_1 \\cup x)} \\pi_i - \\sum_{(i,j) \\in (\\mathcal{P}_1 \\cup x)} \\pi_i D_{ij} \\pi_j - (\\lambda \\sum_{i \\in \\mathcal{P}_1} \\pi_i - \\sum_{(i,j) \\in \\mathcal{P}_1} \\pi_i D_{ij} \\pi_j)$\n$= \\lambda \\pi_x - \\sum_{i \\in \\mathcal{P}_1} \\pi_i D_{ix} \\pi_x + \\sum_{j \\in \\mathcal{P}_1} \\pi_x D_{xj} \\pi_j$ (11)\nFollowing (11), we have $g (\\mathcal{P}_2 \\cup x) - g (\\mathcal{P}_2) = (\\lambda \\pi_x - \\sum_{i \\in \\mathcal{P}_2} \\pi_i D_{ix} \\pi_x + \\sum_{j \\in \\mathcal{P}_2} \\pi_x D_{xj} \\pi_j)$. Therefore, we have\n$(g(\\mathcal{P}_1 \\cup x) - g(\\mathcal{P}_1)) - (g(\\mathcal{P}_2 \\cup x) - g(\\mathcal{P}_2)) = \\sum_{i \\in \\mathcal{P}_2 / \\mathcal{P}_1} \\pi_i D_{ix} \\pi_x + \\sum_{j \\in \\mathcal{P}_2 / \\mathcal{P}_1} \\pi_x D_{xj} \\pi_j \\geq 0,$\nwhich completes the proof of (P1).\nNext, we prove (P2). Given any $\\mathcal{P}_1 \\cap \\mathcal{P}_2 = \\emptyset$, where $\\emptyset$ means an empty set, we have\n$g(\\mathcal{P}_2 \\cup \\mathcal{P}_1) - g(\\mathcal{P}_2) = \\lambda \\sum_{i \\in \\mathcal{P}_1} \\pi_i - \\sum_{i, j \\in \\mathcal{P}_1} \\pi_i D_{ij} \\pi_j + \\sum_{i \\in \\mathcal{P}_1, j \\in \\mathcal{P}_2} \\pi_i D_{ij} \\pi_j + \\sum_{i \\in \\mathcal{P}_2, j \\in \\mathcal{P}_1} \\pi_i D_{ij} \\pi_j$\n$= \\lambda \\sum_{i \\in \\mathcal{P}_1} \\pi_i - \\sum_{i, j \\in \\mathcal{P}_1} \\pi_i D_{ij} \\pi_j + \\sum_{i \\in \\mathcal{P}_2, j \\in \\mathcal{P}_1} \\pi_i D_{ij} \\pi_j + \\sum_{i \\in \\mathcal{P}_1, j \\in (\\mathcal{P}_1 \\cup \\mathcal{P}_2)} \\pi_i D_{ij} \\pi_j >$\n$ \\lambda \\sum_{i \\in \\mathcal{P}_1} \\pi_i - \\sum_{i \\in \\mathcal{P}_1, j \\in \\mathcal{P}_1 \\cup \\mathcal{P}_2} \\pi_i D_{ij} \\pi_j + \\sum_{i \\in \\mathcal{P}_2, j \\in \\mathcal{P}_1 \\cup \\mathcal{P}_2} \\pi_i D_{ij} \\pi_j$ (12)\nwith the constraint $\\lambda \\geq 2$ and the inequality $\\sum a_i b_i \\leq \\sum a_i \\sum b_i, (a_i \\geq 0, b_i \\geq 0)$, (12) is relaxed as fol- lows:\n$\\sum_{i \\in \\mathcal{P}_1} \\pi_i (1 - \\sum_{j \\in (\\mathcal{P}_1 \\cup \\mathcal{P}_2)} D_{ij} \\pi_j) + \\sum_{i \\in \\mathcal{P}_2} \\pi_i (1 - \\sum_{j \\in \\mathcal{P}_1} D_{ij} \\pi_j) \\geq$\nwith the constraints $\\sum_{ij} D_{ij} = 1, \\sum_{i} \\pi_i = 1$, the above inequality is:\n$\\sum_{i \\in \\mathcal{P}_1} \\pi_i (1 - \\sum_{j \\in (\\mathcal{P}_1 \\cup \\mathcal{P}_2)} D_{ij} \\pi_j) + \\sum_{i \\in \\mathcal{P}_2} \\pi_i (1 - \\sum_{j \\in \\mathcal{P}_1} \\pi_j) \\geq 0$\nwhich completes the proof of (P2).\nBased on the Proposition 1, a subset of k webpages is greed- ily selected by the following discrete derivative of g(P):\n$\\triangle(p|\\mathcal{P}) = g(\\mathcal{P} \\cup \\{p\\}) - g(\\mathcal{P})$\n$= \\lambda \\pi_p - \\sum_{i \\in \\mathcal{P}} \\pi_i D_{ip} \\pi_p + \\sum_{j \\in \\mathcal{P}} \\pi_p D_{pj} \\pi_j$ (13)\nIn this paper, $\\lambda$ is assigned as 2. Alg. 1 provides a greedy so- lution to adaptively refine topics with a (1 \u2013 1/e) near-optimal solution?. The time complexity of the submodular selection in Alg. 1 is O($|C|M$) where M is the number of the webpages in a refined one."}, {"title": "3.4.2. Determining the number of the selected webpages", "content": "Considering that the size of different topics is very diverse, we present a heuristic approach based on the relative change of the goodness value in (13) as follows:\n$\\triangle = \\frac{g^t - g^{t+1}}{g^t}$ (14)\nwhere $g^t$ is the maximal increase of the goodness value, $g^t = \\max_i \\{\\triangle(p_i|\\mathcal{P})\\}$ at the t-th selection. That is, $g^t$ measures the incremented goodness value when a webpage is added into a topic. When the first noise webpage is being added into a topic at the t+1-th iteration, the value of $\\triangle$ would be significantly increased. Because the goodness value $g^t$ is larger than that of $g^{t+1}$. Remark 1 proofs that $\\triangle$ is bounded. To give a vivid illus- tration of Alg. 1, Fig. 2 use a 4-nodes toy graph to demonstrate how to greedily select the nodes by $\\triangle$ at the t-th iteration.\nRemark 1. The indicator $\\triangle$ in (14) is bounded, i.e., $0 \\leq \\triangle \\leq 1$.\nConventionally, we would select the webpages which have been selected before the maximal value $\\triangle^\\tau$, that is, $\\triangle = \\max_i \\{\\triangle_i^t\\},(t = 1,...,|\\hat{C}|)$. In this paper, we empirically use a margin m (m > 0) to increase the robustness of a topics. That is, the value $\\triangle_i^t$ of a selected node should be larger than $\\triangle + m$. Because the similarity between two webpages are easily cor- rupted by the sparse and noise textual feature. In this paper, we set m = 0.1.\nIn summary, modeling interestingness utilizes pagerank (8) to assign a high score to an interesting webpage; besides, these unimportant or error webpages introduced in the bundling step (6) would be refined by submodular selection. Therefore, the error webpages are tended to be removed and barely influ- ence the topic ranking results. As Fig. 2 illustrated, by selecting the maximal value points, we would select two nodes with the interestingness scores with 0.7 and 0.6 as a refined topic.\nTime Complexity: Given a list of topic candidates $C = \\{\\hat{C}_0 <,..., <\\hat{C}_K\\}$, the proposed BR method is summarized in Alg. 2. The time complexity of Alg. 2 consists of both the bundling process and the refining one.\nConcretely, the bundling process leads to a complexity of O(K$\\cdot$W) ($W \\ll K$); while, the time cost of the refining process is O(K(T$\\cdot$ nz(P) + $|\\hat{C}|$$\\cdot$ M)). Usually, the number size of an unrefined topic is larger than M (i.e., M < $|\\hat{C}|$). Therefore, the time complexity of the proposed BR is O(K(T$\\cdot$nz(P)+$|\\hat{C}|$M+ W)). In practice, T is smaller than 20, the maximum number of a coarse topic M ($|\\hat{C}|$ > M) is also smaller than 50, and the matrix P is very sparse. Therefore, the proposed BR is quite efficient."}, {"title": "4. Experiment", "content": "4.1. Datasets, Features and Evaluation Criteria\nDataset: We evaluate our method on two public data sets, i.e., MCG-WEBV ? and YKS?. To our best knowledge, there are no publicly accessible web topic dataset with nearly 96% noisy webpages which are not belongs to any topics. Both MCG-WEBV and YKS are very challenge to the approaches for clustering in a sea of noises. MCG-WEBV is built from the \"Most viewed\" videos of \"This month\" on YouTube from Dec. 2008 to Feb. 2009. For MCG-WEBV, the surrounding titles and the comments about the videos are considered as a set of words. YKS is a cross-media data set, where the meta data of YKS contains news articles on Sina and the titles, the tags and the descriptions of web videos on YouKu, from May 2012 to June 2012. The statistics of two data sets are summarized in Table 1.\nNoise and sparsity are observed in two data sets. For in- stance, the sizes of dictionaries of both data sets are extremely large, e.g., 80,294 for YKS. Because the dictionaries of two sets contain not only the multi-language words but also user-\ndefined abbreviations. Moreover, the average number of words in a webpage is also extremely small, e.g., 35 for MCG-WEBV and 228 for YKS. As a result, the textual features from social media are more noisier and shorter than news articles ?.\nFeatures: In the pre-processing stage, YKS is tokenized by NLTK2 package. TF-IDF is used to encode the textual feature, and Fisher Vector (FV) ? is used to encode the keyframes of video clips. Once keyframes are encoded, we use video sig- nature? to compute the similarity between two video clips. The cosine distance is used to measure the similarity between the textual features. The $k$ of k-N$^2$SGs are assigned as 100 and 10 for both the textual graph and the visual one on MCG- WEBV, respectively. Since YKS is more noisy than that of MCG-WEBV, the $k$ of k-N$^2$SGs are set to 20 and 10 for the textual graph and the visual one, respectively.\nDuring the experiments, SC? is assigned with a set of thresholds, \\{0.1,0.5,0.9\\}, and Non-negative Matrix Factoriza- tion with Random walk (NMFR) ? is used to generate top- ics. The number of clusters for each threshold are assigned as \\{100, 500, 900, 1300\\}. In the experiments, we have generated 4,240 topics for MCG-WEBV and 5,252 ones for YKS, respec- tively.\nEvaluation Criteria: Top-10 $F_1$ versus Number of Detected Topics (NDT), and accuracy versus False Positive Per Topic (FPPT)? are used.\nFor top-10 $F_1$ versus NDT, if a detected topic is matched with the ground truth, the averaged $F_1$ scores of the top 10 detected topics are averaged to measure the performance:\n$F_1 = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}$ (15)\nwhere $\\mathcal{D}$ is a detected topic, $\\mathcal{G}$ is a ground truth topic, and $| . |$ denotes the number of the webpages in a topic. Therefore, $Precision = \\frac{| \\mathcal{D} \\cap \\mathcal{G} |}{|\\mathcal{D}|}$ is the precision, $Recall = \\frac{| \\mathcal{D} \\cap \\mathcal{G} |}{|\\mathcal{G}|}$ is the recall. However, top-10 $F_1$ versus NDT does not consider the influence of the number of detected topics. Therefore, accuracy versus FPPT is further proposed to handle this problem?.\nFor accuracy versus FPPT, if a topic is correctly detected, how many falsely detected topics are generated by a detection system, where accuracy is defined as follows:\n$Accuracy = \\frac{#Successful}{#Groundtruth}$ (16)\nA topic candidate $\\mathcal{D}$ is recognized as a successful detection, if Normalized Intersected Ratio (NIR) $r = \\frac{| \\mathcal{D} \\cup \\mathcal{G} |}{|\\mathcal{G} |}$ is larger than a threshold. Following the previous work?, NIR with a threshold of 0.5 is used in our experiments. In summary, the top-10 $F_1$ focuses on measuring the partial performance of the top ranked topics. While the accuracy measures the overall performance of the detected results."}, {"title": "4.2. Methods in Comparison Study", "content": "Our experimental goal is to compare the proposed approach with four state-of-the-art methods:\n1.  Event-Clustering Based Method (ECBM) ?. Different from the PD-based method?, this work ? first clusters the tags from each time slice, and then both the Near Du- plicated Keyframes (NDKs) and the tag-based clusters are grouped into topics. Note that this approach involves many engineering details and the tuning of the hyper-parameters. We implemented this method by ourselves and reported the best tuned results.\n2.  Multi-Modality Graph (MMG) ?. This baseline belongs to the similarity-graph based method. Zhang et al. ? uti- lize the NDKs of video clips and the textual information to build the similarity graph ?, where graph shift (GS) ? is used to discover web topics. This method mainly de- pends on the noise-resistant power of GS. The comparison between MMG and our method illustrates that without de- signing any noise-resistant component, putting fragments into a whole is a promising approach to generate complete topics.\n3.  Maximal Cliques with Poisson Deconvolution (MCPD) ?. Rather than using NMFR to generate topics, MCPD leverages Maximal Cliques (MCs) to generate topics. This comparison demonstrates the generalization ability of the proposed BR method across different topic patterns. Because a good generalization ability guarantees the success of the combination of the BR method and the other topic patterns, e.g., spectral clustering?.\n4.  Latent Poisson Deconvolution (LPD) ?. This method achieves the state-of-the-art performances on both MGC-WEBV and YKS. Note that rather than using mul- tiple graphs in LPD, our method only utilizes one graph to rank topics. This demonstrates that our approach can meet or even surpass the state-of the art method without exploiting the multiple graphs."}, {"title": "4.3. Analysis of Our Approach", "content": "In this Section", "follows": "n*   Verify the effectiveness of each component in BR;\n*   The generalization ability of BR across different topic pat- terns;\n4.3.1. The Effectiveness of Each Component in BR\nFig. 3 compares the effectiveness of each component of BR i.e., Bundling and the combination of Bundling and Refining in Alg. 2.\nIn this experiment, a baseline method? (we term it as \"Rank- ing by $i_k$\") ranks topics by interestingness $i_k$. Fig. 3 shows that the proposed BR significantly surpasses the results of \"Ranking by $i_k$\\"}]}