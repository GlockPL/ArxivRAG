{"title": "CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA Initialization", "authors": ["Yanxia Deng", "Aozhong Zhang", "Naigang Wang", "Selcuk Gurses", "Zi Yang", "Penghang Yin"], "abstract": "Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has become a highly efficient approach for downstream tasks, particularly in scenarios with limited computational resources. However, applying LoRA techniques to quantized LLMs poses unique challenges due to the reduced representational precision of quantized weights. In this paper, we introduce CLOQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic initialization strategy designed to overcome these challenges. Our approach focuses on minimizing the layer-wise discrepancy between the original LLM and its quantized counterpart with LoRA components during initialization. By leveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and determines the optimal LoRA components for each layer, ensuring a strong foundation for subsequent fine-tuning. A key contribution of this work is a novel theoretical result that enables the accurate and closed-form construction of these optimal LoRA components. We validate the efficacy of CLoQ across multiple tasks such as language generation, arithmetic reasoning, and commonsense reasoning, demonstrating that it consistently outperforms existing LoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit widths. The code is available at https://github.com/AozhongZhang/CLOQ", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2023; Guo et al., 2024a) have achieved remarkable success across a wide range of domains and applications. With ongoing advancements, the size and complexity of LLMs have grown exponentially, with some models now exceeding billions or even trillions of parameters. Although this scaling has unlocked unprecedented capabilities, it also introduces significant challenges, particularly in efficiently adapting these models to downstream tasks. Traditionally, full fine-tuning has been the dominant approach for adapting pre-trained models, involving updates to all model parameters. While effective in achieving state-of-the-art results, full fine-tuning is resource-intensive, requiring substantial GPU memory to store both model weights and optimizer states. These memory demands grow with the size of the model, making full fine-tuning increasingly impractical for large-scale models in resource-constrained settings.\nTo address these challenges, parameter-efficient fine-tuning (PEFT) (Houlsby et al., 2019), such as Low-Rank Adaptation (LoRA) (Hu et al., 2021), has emerged as a promising approach. PEFT updates only a small subset of parameters while keeping the majority of the model unchanged, enabling resource-efficient fine-tuning of large-scale models. LORA, for instance, introduces small, learnable low-rank matrices into the model's architecture. These matrices are optimized during fine-tuning while the original model weights remain frozen, significantly reducing memory and computational requirements. This design leverages the insight that weight updates often reside in a low dimensional subspace, allowing LoRA to achieve efficient adaptation with minimal overhead.\nIn an orthogonal direction, model compression techniques, notably quantization (Rastegari et al., 2016; Hubara et al., 2018; Choi et al., 2018; Wang et al., 2018; Shao et al., 2023; Frantar et al., 2022a; Zhang et al., 2024a), have been developed to minimize GPU memory usage by converting high-precision weights into low-precision representations. Initially designed for inference in memory-limited environments, quantization methods have since been adapted to support fine-tuning. A notable advancement in this regard is QLORA (Dettmers et al., 2023), which combines LORA with quantization techniques to reduce GPU memory requirements for fine-tuning significantly. By leveraging low-rank adaptation and quantized weights, QLORA enables resource-efficient fine-tuning of LLMs without compromising performance, making it a powerful tool for adapting large-scale models to downstream tasks. However, extending LoRA to quantized LLMs introduces additional challenges, as the reduced representational precision of quantized weights can disrupt standard initialization strategies,"}, {"title": "2. Background", "content": "Integer Quantizer. Given a set of m weights w\u2208\nRm, the widely-used b-bit uniform integer (INT) quantizer (Choi et al., 2018) determines the float scaling factor \u03b4 = max(w)-min(w) and zero-point z = [ - min(w) ],\nwhere [] is nearest-to-round operation. The quantizer\nprojects w onto the equally spaced grids Q = {z\u00b7d, (z+1)\u00b7\n\u03b4, ..., (z + (2b \u2212 1)) \u00b7 \u03b4}m to obtain the quantized weights\nq = 8 \u00b7 (clip ([\n2b-1\n\u03b4\n] + z, 0, 2b \u2013 1) \u2013 z).\nFor channel-wise (or group-wise) quantization, the scaling\nfactor \u03b4 is shared by the quantized weights within the same\nchannel (or group, respecitively).\nPost-Training Quantization. Post-training quantization\n(PTQ) has been a cornerstone technique for compressing\nLLMs. PTQ methods directly identify low-precision repre-\nsentations of a model without requiring retraining, making\nthem particularly well-suited for extremely large-scale AI\nmodels including LLMs. The simplest approach in this cate-\ngory is data-free quantization, commonly referred to as RTN,\nwhich involves rounding the pre-trained model weights to\ntheir nearest quantized states. More advanced PTQ algo-\nrithms, such as OPTQ (Frantar et al., 2022b), leverage a\nsmall calibration dataset to solve a least-squares problem\nunder discrete constraints:\nmin ||XQX \u2212 XW||2,\n(1)\nQ\u2208Q\nto calibrate the quantization layer by layer. In this formula-\ntion, X denotes the activation or feature matrix correspond-\ning to a batch of calibration data for a fixed layer. This\napproach ensures that the quantization process preserves\nthe layer's output by minimizing the discrepancy between\nthe original and quantized representations on the calibration\ndataset. Several efficient back-propagation-free algorithms\n(Behdin et al., 2023; Zhang et al., 2024b) have been pro-\nposed to address (1).\nLow-Rank Adaptation. LoRA (Hu et al., 2021) enables\nefficient fine-tuning of large pre-trained models by intro-\nducing two small, trainable matrices, A and B, which are\nadded to the frozen weight matrix W. The weights of the\nfine-tuned model are then expressed as W + ABT, where\nA\u2208Rmxr, B\u2208 Rn\u00d7r, and r < min(m,n). During\nfine-tuning, only A and B are updated, while W remains\nfixed, significantly reducing the number of trainable pa-\nrameters and computational overhead. LoRA initializes its\nparameters as follows:\nA ~ \u039d(0, \u03c32), B = 0,\nensuring that the initialization maintains perfect alignment\nwith the pre-trained weights W. Recent research has fo-\ncused on developing variants of LoRA aimed at enhancing\nits performance (Liu et al., 2024; Wang et al., 2024a;b)."}, {"title": "3. Method", "content": "CLoQ is designed to enhance the fine-tuning of quantized\nLLMs by incorporating calibration data, building upon\nOPTQ (Frantar et al., 2022a) with a specific focus on the\ninitialization of LoRA adapters. It utilizes second-order\ninformation derived from input activations X to ensure that\nthe low-rank adapter matrices A and B are initialized in\na manner that minimizes quantization error, particularly"}, {"title": "3.1. Calibrated Quantization and Low-Rank\nInitialization", "content": "Given the activation matrix \u2717 associated with the calibra-"}, {"title": "3.1.1. POST-TRAINING QUANTIZATION", "content": "With AB initialized to zero, the problem of finding Q simplifies to the standard layer-wise post-training quantization\n(PTQ):\nmin || X(Q-W)||, (3)\nQ\u2208Q\nan optimization problem that has been extensively studied\nin recent literature (Frantar et al., 2022b; Chee et al., 2023;\nXiao et al., 2023; Zhang et al., 2024b). For this, we adopt\nthe widely-used OPTQ method (Frantar et al., 2022a). To\nfurther enhance OPTQ's performance, we incorporate a pre-\nprocessing technique called weight magnitude reduction\n(MagR) (Zhang et al., 2024a), which modifies W by re-\nmoving outliers prior to quantization. MagR preprocessing\nsignificantly improves OPTQ's effectiveness in the low-bit\nregime while introducing minimal additional time beyond\nthe OPTQ process and incurring no computational or mem-\nory overhead during inference time."}, {"title": "3.1.2. GENERALIZED LOW-RANK APPROXIMATION", "content": "After obtaining Q in the quantization step, we denote by\nAW = W \u2013 Q\nthe residual of the quantized weights. To determine A and\nB, we solve the following low-rank approximation problem\nunder the linear transformation induced by X:\nmin\n||X(ABT \u2013 AW)||.\n(4)\nA\u2208Rmxr, B\u2208Rnxr\nIt is important to note that problem (4) is non-trivial due to\nthe presence of the matrix X, and its optimal solution is\nnot given by directly computing the low-rank approxima-\ntion (or SVD) of \u2206W. However, we demonstrate in the\nfollowing result that the problem can be solved accurately\nby performing just two SVDs:\nTheorem 3.1. Suppose the activation matrix X \u2208 IR(b\u00b7l)\u00d7m\n(bl\u226b m) is of full-rank. Suppose the Gram (or\nHessian) matrix H = XTX \u2208 Rmxm has the SVD\nH = U\u0397\u03a3\u0397U\u03a4\u0397 and denote by R = \u03a3U the non-\nsymmetric root of H. Then any pair (A, B) satisfying\nABT = R\u22121LR(RAW).\n(5)\npermits an optimal solution to problem (4). Here\nLR(RAW) = arg minrank(Z)\u2264r ||Z \u2013 RAW||2F de-\nnotes the best rank-r approximation of RAW.\nProof. Firstly, we observe that the objective in (4) has the\nfollowing equivalent expression:\n||X(ABT \u2013 AW)||\n= Tr((ABT \u2013 AW)TXTX(ABT \u2013 AW))\n= Tr((ABT \u2013 AW)TH(ABT \u2013 \u2206W))\n= Tr((ABT \u2013 AW)TRTR(ABT \u2013 \u2206W))\n= ||R(ABT \u2013 AW)||\n= ||RABT \u2013 RAW ||,\nwhere in the third equality, we used the identity: H =\nRTR. Moreover, since X is full rank, H is invertible, and\nso is R.\nBased on these facts, we interpret problem (4) as find-\ning the standard best rank-r approximation of RAW,\nLR(RAW), which can be obtained by performing the\nSVD of RAW and extracting the top-r principal compo-\nnents. Then, (A, B) is an optimal solution to\nmin ||RABT \u2013 RAW ||2F\nA\u2208Rmxr, B\u2208Rnxr\nif and only if\nRABT = LRr(RAW).\nConsequently, since R is invertible, any (A, B) fulfilling\nABT = R\u22121LRr(RAW).\npermits an optimal solution to problem (4)."}, {"title": "4. Experiment", "content": "In this section, we evaluate the effectiveness of CLoQ on\nlanguage modeling, arithmetic reasoning, and commonsense\nreasoning tasks. The fine-tuning through CLoQ consists of\ntwo key stages: the initialization step and the fine-tuning\nstep. In the initialization step, we quantize the full-precision\nweight W into low-precision weight Q and find optimal\nlow-rank matrices A and B that minimize the residual error.\nIn the finetuning step, the quantized weight matrix Q is\nfixed in low-precision, while A and B are trained through\nback-propagation. The detailed hyperparameter settings for\nall our experiments are presented in the Appendix A."}, {"title": "4.1. Implementation details", "content": "Quantization. We quantize the weights of all linear layers\nin the base model using MagR preprocessing (Zhang et al.,\n2024a) followed by OPTQ (Frantar et al., 2022a). The\nquantization scheme employs uniform (a.k.a. INT) and\nasymmetric quantization, with a default group size of 64.\nAfter quantization, we compute the LoRA components A\nand B, which are maintained in FP16 precision.\nFine-tuning. Following prior works (Dettmers et al., 2023;\nLi et al., 2023; Liao et al., 2024), we fine-tune the models\nusing the standard LoRA configuration, with modifications\nto the LORA initialization and learning rates. The quan-\ntized weights remain fixed, and only the LoRA adapter\nmatrices are trainable during fine-tuning. The rank of the\nLORA adapters is consistently set to 64 across all methods.\nFor optimization, we use AdamW (Loshchilov, 2017). All\nexperiments are conducted on NVIDIA A100 GPUs with\n80GB of memory."}, {"title": "4.2. Fine-tuning Results", "content": "Language modeling. We evaluate the models by reporting\nthe perplexity of language generation on WikiText-2. As\nshown in Table 2, CLoQ consistently delivers the best per-"}, {"title": "4.3. Ablation study", "content": "Quantization with different group sizes. We examine\nthe performance of CLoQ for group-wise-128 (g128) and\nper-channel quantization. CLoQ demonstrates superior\nperformance even under per-channel quantization, achiev-\ning results remarkably close to those of g64 quantization.\nMoreover, it consistently outperforms LoftQ in these set-\ntings, demonstrating its effectiveness across all quantization\nbit levels.\n1\nLORA initialization with different (A, B) combinations.\nFurthermore, we investigate the performance of various\ncombinations of (A, B) in Algorithm 1. We tried four different combinations of (A, B),\nincluding (R\u22121U, V ), (R\u22121U, V::r),\n(R\u22121U\u03a3H H, V\u03a3H H) with H being the\norder-r (r = 64) Hadamard matrix, and the default choice\n(R\u22121U::r, V:r). Table 6 shows that the default combi-\nnation of initialized adapters gives the best performance in\nthe subsequent fine-tuning phase."}, {"title": "5. Related Work", "content": "When quantizing pre-trained models, QLORA (Dettmers\net al., 2023) primarily emphasizes the quantization pro-\ncess, often overlooking the critical importance of subsequent\nLORA fine-tuning. It adopts the fixup initialization strategy"}, {"title": "6. Concluding Remarks", "content": "In this work, we introduced CLoQ, an efficient and scalable\nmethod for fine-tuning quantized LLMs. By leveraging a\nsmall calibration dataset, CLoQ optimally initializes LoRA\nadapters through a novel layer-wise, data-driven approach,\nsignificantly improving the fine-tuning process without the\nneed for back-propagation. The use of a closed-form solu-\ntion for low-rank approximation, computed via two SVDs,\nensures that CLoQ is both computationally efficient and\nhighly effective, particularly at ultra-low bit-widths. Our ex-\ntensive experiments on multiple benchmark datasets demon-\nstrate that CLoQ consistently outperforms existing LoRA-\nbased methods for quantized models, such as QLoRA, in\ntasks requiring fine-grained precision. The results under-\nscore the potential of CLoQ to enhance the performance\nof quantized models across a variety of downstream appli-\ncations, including those that demand high accuracy, like\narithmetic reasoning tasks. The simplicity and efficiency of\nCLoQ make it a promising approach for fine-tuning large-scale quantized LLMs in resource-constrained environments.\nFuture work could further investigate the theoretical implica-\ntions of different decompositions of the adapter matrices in\nCLoQ, and how these variations influence the performance\nof subsequent fine-tuning."}, {"title": "A. Experimental Details", "content": "To study the capability of CLoQ, we fine-tune quantized models on the WikiText-2 training set and measure perplexity on\nthe validation set. The hyper-parameters used for fine-tuning are provided in Table 7 and Table 8. We evaluate the models\non the validation set at each epoch and report the lowest achieved perplexity."}, {"title": "A.2. Arithmetic reasoning", "content": "To assess CLoQ's arithmetic reasoning capability, we fine-tune quantized models using the GSM8K training set and evaluate\ntheir accuracy on the test set. The hyperparameters used for fine-tuning are detailed in Table 7 and Table 8. Model\nperformance is evaluated at each epoch on the test set, and we report the highest recorded accuracy."}, {"title": "Multiple task", "content": "Following the framework proposed by (Hu et al., 2023), we adopt a more integrated approach by training a single model\nacross multiple tasks. Specifically, we fine-tune Llama2-7B and Llama2-13B on Math10K, a dataset that aggregates training\nsamples from GSM8K, MAWPS, MAWPS-single, and AQuA. After finetuning, the models are tested on the evaluation sets\nof AQUA, GSM8K, MAWPS, and SVAMP. The hyper-parameters used for fine-tuning are detailed in Table 7 and Table 8.\nMoreover, instead of conducting evaluations at every epoch, we assess model performance only after the final epoch."}, {"title": "A.3. Commonsense reasoning", "content": "To evaluate the commonsense reasoning capabilities of CLoQ, we consider eight key benchmark tasks: BoolQ, PIQA, SIQA,\nHellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA. We adopt the framework proposed by (Hu et al., 2023) and fine-tune\na single model across all these tasks instead of training separate models. We fine-tune Llama2-7B and Llama2-13B on\nthe merged training set and measure accuracy on the corresponding test sets. The hyper-parameters used for fine-tuning\nare detailed in Table 7 and Table 8. For evaluation, we forgo per-epoch assessments and instead report the final model's\nperformance after the last epoch."}]}