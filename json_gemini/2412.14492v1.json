{"title": "FaultExplainer: Leveraging Large Language Models for Interpretable Fault Detection and Diagnosis", "authors": ["Abdullah Khan", "Rahul Nahar", "Hao Chen", "Gonzalo E. Constante Flores", "Can Li"], "abstract": "Machine learning algorithms are increasingly being applied to fault detection and diagnosis (FDD) in chemical processes. However, existing data-driven FDD platforms often lack interpretability for process operators and struggle to identify root causes of previously unseen faults. This paper presents FaultExplainer, an interactive tool designed to improve fault detection, diagnosis, and explanation in the Tennessee Eastman Process (TEP). FaultExplainer integrates real-time sensor data visualization, Principal Component Analysis (PCA)-based fault detection, and identification of top contributing variables within an interactive user interface powered by large language models (LLMs). We evaluate the LLMs' reasoning capabilities in two scenarios: one where historical root causes are provided, and one where they are not to mimic the challenge of previously unseen faults. Experimental results using GPT-40 and ol-preview models demonstrate the system's strengths in generating plausible and actionable explanations, while also highlighting its limitations, including reliance on PCA-selected features and occasional hallucinations.", "sections": [{"title": "1. Introduction", "content": "Fault Detection and Diagnosis (FDD) are critical components in ensuring the reliability and safety of chemical processes. Fault detection involves recognizing the occurrence of a fault in the system, while fault diagnosis ascertains the root cause of the fault. In the chemical industry, where processes are complex and highly integrated, early and accurate FDD can prevent significant economic losses, environmental harm, and safety hazards. Effective FDD systems enhance operational efficiency by minimizing downtime and maintenance costs, thereby ensuring the continuous and safe operation of chemical plants.\nVarious approaches have been developed for FDD, broadly categorized into model-based and data-driven methods (Chiang et al., 2001). Model-based methods rely on the expertise and experience of operators and engineers and physics-based equations, often implemented through expert systems (Venkatasubramanian et al., 2003c,a). These methods can be highly effective in systems where physical models are well understood and can provide detailed explanations of fault mechanisms. However, they require extensive domain knowledge and may not adapt well to changes in the process.\nData-driven methods, on the other hand, utilize historical and real-time process data to detect and diagnose faults (Venkatasubramanian et al., 2003b). Traditional statistical methods such as Principal Component Analysis (PCA) have been widely used for FDD due to their ability to handle multivariate data. However, these methods assume linearity and Gaussian distribution of data, which limits their applicability to more complex, non-linear processes commonly found in chemical industries. Recent advances in machine learning, particularly deep learning, have introduced more robust data-driven FDD techniques. Deep learning methods, such as Recurrent Neural Networks (RNNs), autoencoders, deep belief networks, and variants (Sun et al., 2020; Zhang and Qiu, 2022; Zhang and Zhao, 2017), can model complex non-linear relationships and temporal dependencies in process data.\nWhile deep learning methods offer superior modeling capabilities, they also come with challenges. One of the primary disadvantages is their lack of interpretability; understanding the reasoning behind a model's predictions can be difficult, which poses a challenge to gaining trust in these systems. To make the deep learning models interpretable for FDD, recent works by Bhakte et al. (2022, 2023) have leveraged techniques in explainable artificial intelligence (XAI) such as Shapley value (Hart, 1989) to attribute the pre-"}, {"title": "2. Background and Literature Review", "content": "Fault Detection and Diagnosis (FDD) methods can be broadly classified into two main categories: model-based and data-driven approaches. Model-based methods rely on physical and chemical models of the process and can be further divided into quantitative and qualitative methods (Venkatasubramanian et al., 2003c,a). Quantitative methods use mathematical models to represent the process. Diagnostic observers (Watanabe and Himmelblau, 1983b,a) are algorithms that compare the actual process outputs with the outputs of a mathematical model to detect discrepancies that indicate faults.\nParity relations (Gertler and Monajemy, 1995; Yin and Liu, 2017) generate residuals by comparing measurements to their estimated values based on mathematical models, and faults are detected when these residuals exceed a certain threshold. Kalman filters (Welch et al., 1995) estimate the state of a process in real-time and use the discrepancies between the predicted and actual states to identify faults. Parameter estimation (Jiang et al., 2008; Alanqar et al., 2017) involves estimating the parameters of a process model, where significant deviations from normal parameter values indicate potential faults.\nQualitative methods use non-numeric models to describe the process. Causal models including Digraphs (Vedam and Venkatasubramanian, 1997), fault trees (Geymayr and Ebecken, 1995), and qualitative physics models (Sacks, 1988), have been used to represent and analyze the cause-effect relationships within a process. Digraphs, or directed graphs, visually depict the cause-effect relationships between variables in a system, allowing for the identification of fault propagation paths. Fault trees utilize logic gates to model the sequences of events and conditions that can lead to system failures. Qualitative physics models use qualitative reasoning to represent the physical and functional relationships within a system to facilitate the understanding and diagnosis of faults based on causal interactions.\nData-driven methods utilize statistical techniques and machine learning algorithms to identify patterns and anomalies in process data (Venkatasubramanian et al., 2003b). Principal Component Analysis (PCA) (Choi et al., 2005) reduces the dimensionality of process data by identifying the principal"}, {"title": "3. Overview of FaultExplainer's Capabilities", "content": "To facilitate the practical application of our methodology and to make our findings accessible to a wider audience, we developed an interactive web interface for fault diagnosis and explanation within the Tennessee Eastman Process (TEP). Figure 1 provides a visual overview of the capabilities of FaultExplainer that can be accessed from the web interface. It enables users to quickly identify and diagnose faults, and understand their underlying causes. This interface offers three key features corresponding to the three windows shown in Figure 1:\n\u2022 Real-Time Sensor Data Visualization: Users can monitor real-time data from all TEP sensors, enabling them to observe and track the behavior of the 41 measured variables and the 12 manipulated variables. The sensor data are generated from the TEP simulator. The user can introduce fault to the process by using the \"Fault\" dropdown button, e.g., changing the system from \u201cNormal Operation\u201d to a fault such as \"step change in A/C feed ratio\". All the faults described in (Downs and Vogel, 1993) are available.\n\u2022 Fault History with T2 Plot: A dedicated \u201cFault History\" page displays a real-time T\u00b2 plot, along with a history of previously detected faults. This allows users to visualize the progression of faults over time and correlate them with changes in sensor data. The details of this method will be described in Section 4. After a fault is introduced, one can observe the change in T2 statics in this window. When the T2 statistics exceed the fault triggering threshold, the color in the T2 plot will change from green to red. The historical fault reports contain the analysis of the major changes in sensor measurement, root cause deduction, and how the root cause propagates through the process.\n\u2022 Interactive Chat Interface: A chat interface provides a convenient way for users to inquire about specific faults or seek explanations for observed anomalies. Users can interact with the system using natural language, and the interface will generate grounded explanations based on the methodology presented in Section 4. The users can ask follow-up qualitative questions related to the process and the faults such as \"how to prevent the previous fault from occurring?\"."}, {"title": "4. Methods", "content": "To alleviate the hallucinations of the LLMs, we propose an approach by adapting classical approaches for fault detection and feature importance analysis to enhance fault explanation in the Tennessee Eastman Process (TEP). The approach combines Principal Component Analysis (PCA) for identifying deviations in process data with T2 contribution analysis to pinpoint relevant process variables. The outputs from these rigorous methods together with a detailed description of the TEP are used as input to the LLM. An overview of the proposed method is shown in Figure 2. In what follows, we provide detailed explanations of the approach."}, {"title": "4.1. Fault Detection and Feature Importance", "content": "Faults in the Tennessee Eastman Process (TEP) are detected using a Principal Component Analysis (PCA)-based approach. PCA is a classical statistical learning technique used to reduce the dimensionality of a dataset while retaining most of the variability present in the data (Chiang et al., 2000). To make this paper self-contained, we provide a brief background of PCA. Mathematically, PCA involves several steps. First, the normal operating condition data is standardized by centering it (subtracting the mean of each variable) and scaling it (dividing by the standard deviation). The resulting n scaled data points is represented by the matrix $X \\in R^{n\\times m}$. Then, the covariance matrix of the standardized data is computed. Eigendecomposition is performed on this covariance matrix to obtain the eigenvalues and eigenvectors.\n$S = \\frac{1}{n-1} X^T X = V \\Lambda V^T$\nwhere the diagonal matrix $ \\Lambda \\in R^{m \\times m}$ contains the non-negative real"}, {"title": "4.2. TEP Process Description and Prompt Engineering", "content": "To effectively use the LLM for fault explanation in the Tennessee Eastman Process (TEP), we first provide a detailed description of the process, including its unit operations, streams, and variables. This description clarifies the relationships among reactants, products, and byproducts, the role of each stream, and the monitored variables in the process. We explicitly distinguish between measured variables, which are recorded by sensors but cannot be directly manipulated, and manipulated variables, which can be actively adjusted by the control algorithm. This distinction allows the LLM to reason about process dynamics and the control system's responses to deviations. Additionally, the top six PCA contributing features (variables) calculated by Equation (3) and their deviations from their mean values during normal operating conditions are provided as inputs to guide the LLM in identifying significant deviations and their potential explanations.\nTwo different prompts are developed to support fault diagnosis. The first prompt includes a predefined list of 15 root causes shown in the first column of Table 1, retrieved from the original TEP paper (Downs and Vogel, 1993). This list acts as a constrained search space for the LLM, mimicking the scenario where the faults lie within historically encountered faults. The second prompt is named Root Causes-Included Prompt. The second prompt assumes no prior knowledge of the potential fault causes, allowing the LLM to generate hypotheses based solely on the feature deviations and process description, which is to mimic the scenario of encountering unseen faults. This prompt is called General Reasoning Prompt. Both prompts are designed to elicit deterministic and consistent explanations by systematically analyzing the deviations, hypothesizing root causes, and connecting the identified faults to the observed feature changes. These carefully constructed prompts enable the LLM to leverage domain knowledge effectively while remaining grounded in the provided data and process details.\nWhen the PCA identifies that a fault has occurred in the plant, the process description, the deviations of the top six contributing features from the normal operating condition, and one of the explanation prompts can be combined as inputs to the LLM. The LLM will generate a report describing what has been observed, the possible root causes of the faults, and how these"}, {"title": "5. Results", "content": "The time series data for the normal operating conditions and the 15 known faults of the TEP are obtained from Rieth et al. (2017). These data include 41 measured and 12 manipulated variables, with each time series containing 500 time steps. The normal operating time series is used to train the PCA model to extract principal components that capture 90% of the data variance. The T2 statistic threshold in Equation 2 is set to a = 0.01, controlling the probability of a false fault report to 1%. FaultExplainer triggers a fault only when six consecutive T2 values exceed the threshold. Assuming independent observations, this corresponds to a false alarm rate of 0.016 = 1 \u00d7 10-12. Once a fault is detected, we select the six features (variables) with the largest contribution values at the last time step. The values of these features are compared to their mean values during normal operation, and the value comparisons and percentage changes are incorporated into the prompts for the LLM.\nWe evaluate FaultExplainer using two settings described in Section 4.2. In the Root Causes-Included Prompt setting, the LLM is provided with a predefined list of 15 known root causes and must identify and explain the top three most likely causes based on the top six feature changes. In the General Reasoning Prompt setting, the LLM is not restricted to predefined root causes and must independently infer and explain three potential root causes using its reasoning capabilities. The latter reflects real-world scenarios where faults may not have been previously encountered, testing the flexibility and robustness of the tool.\nWe test OpenAI's GPT-40 (OpenAI, 2024a) and ol-preview (OpenAI, 2024b) models. The GPT-40 model, a multimodal system, excels at general natural language tasks, while the ol-preview model incorporates improved reasoning capabilities. These experiments are designed to evaluate the models' accuracy, reasoning ability, and the quality of their explanations across both prompt settings. All outputs from both models for diagnosing the 15 faults under the two prompt settings are provided in the supplementary material."}, {"title": "5.2. Results of the Root Causes-Included Prompt", "content": "We test the Root Causes-Included Prompt over the 15 faults. Both LLMs are asked to give the top three possible fault classifications."}, {"title": "5.2.1. Quantitative evaluation of the classification accuracy", "content": "Table 1 summarizes the fault classification results of the GPT-40 and o1-preview models when prompted with the 15 predefined root causes. The numbers in bold indicate correct classifications. For certain faults, such as Faults 1 and 8, the contributing features alone cannot distinguish between random variations and step changes in the same features. These faults are considered aliases and are treated as correct as long as the model classifies the fault or its alias correctly. Faults that cannot be detected by PCA are marked with backslashes in the table.\nAmong the 11 faults that can be identified using PCA, GPT-40 correctly classifies 7 faults, while ol-preview correctly classifies 9 faults. This aligns with o1-preview's improved reasoning capabilities, allowing it to better utilize the provided descriptions and contributing features. Neither model is able to correctly diagnose Faults 5 (Condenser Cooling Water Inlet Temperature & Step) and 10 (C Feed Temperature (Stream 4) & Random Variation). This failure is mainly because the LLMs find alternative ways to explain the deviations of the top six features, which will be discussed in detail in the qualitative evaluation of the fault reports."}, {"title": "5.2.2. Qualitative evaluation of the explanations", "content": "In this subsection, we pick a fault where both models find the correct root cause and one fault where neither model finds the correct root cause to demonstrate the advantages and limitations of the two models. The rest of the fault reports follow a similar pattern and can be found in the supplementary material.\nThe results for Fault 7, where both GPT-40 and ol-preview identify the correct root cause (C Header Pressure Loss - Reduced Availability in Stream 4), shown in Figure 3, demonstrate the plausibility and coherence of the explanations provided by both models. Both models accurately link the observed decreases in the reactor, product separator, and stripper pressures, as well as the reduced A and C feed flow rate, to the loss in header pressure. They also correctly attribute the control system's compensatory behavior-namely, the increase in the A and C feed load to the attempt to counteract the pressure loss. Furthermore, the explanations addressed the"}, {"title": "5.3. Results of the General Reasoning Prompt", "content": "Similarly, the General Reasoning Prompt is used to generate the top three root causes with explanations."}, {"title": "5.3.1. Quantitative evaluation of the classification accuracy", "content": "The results of General Reasoning Prompt, summarized in Table 2, highlight the performance of GPT-40 and o1-preview when not restricted to the 15 predefined root causes. The root causes generated by the LLMs that are related to the ground truth of the faults are highlighted in bold in Table 2. Both models identified the correct or related root cause in 8 out of the 11 faults identified by the PCA. This demonstrates their capacity to generalize"}, {"title": "5.3.2. Qualitative evaluation of the explanations", "content": "Similar to Section 5.2.2, we choose two faults to demonstrate the advantages and limitations of the two models. The rest of the fault reports follow a similar pattern and can be found in the supplementary material.\nFigure 5 shows how GPT-40 and o1-preview explain Fault 2 when the 15 predefined root causes are not provided. GPT-40's explanation lacks specificity and tends toward vague reasoning. While it proposes a scenario involving an increased reactor pressure or flow rate, it does not identify the affected components or streams precisely. Furthermore, it fails to establish a clear causal relationship between the proposed root cause and the observed feature changes. For example, it claims that an increase in reactor flow rate results in more component B being flowed to the reactor. This is problematic because component B itself represents the reactor's inflow, making the proposed causal linkage nonsensical. Furthermore, it claims that \u201cThe increase in A and E feed loads could be a response to maintain reaction rates and product output under the new conditions.\u201d However, it does not explain why the new conditions make the A and E feed loads increase. The response suggests that GPT-40 may be compensating for insufficient reasoning with overly general statements.\nIn contrast to GPT-40, 01-preview provides a thorough, causally grounded explanation that aligns closely with both the observed data and fundamental chemical engineering principles. As shown in the figure, o1-preview pinpoints an increased ingress of inert Component B into the system. The increase in B causes the rise of B in the recycle loop, prompting the control system to mitigate the accumulation of B by increasing the Purge Valve opening and consequently the Purge Rate and the concentration of B in the purge. The explanation also addresses the reactive species balance within the reactor. With inert B diluting the reactor, the control system compensates by increasing the feed loads of reactants A and E. o1-preview is able to explain all the six feature deviations from the normal operating conditions. Overall, the"}, {"title": "6. Conclusions", "content": "In this paper, we present FaultExplainer, a tool for diagnosing and explaining faults in the Tennessee Eastman Process (TEP). FaultExplainer combines real-time sensor data visualization, T\u00b2 fault detection and feature"}]}