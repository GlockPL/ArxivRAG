{"title": "Federated Learning under Attack: Improving Gradient Inversion for Batch of Images", "authors": ["Luiz Leite", "Yuri Santo", "Bruno L. Dalmazo", "Andr\u00e9 Riker"], "abstract": "Federated Learning (FL) has emerged as a machine learning approach able to preserve the privacy of user's data. Applying FL, clients train machine learning models on a local dataset and a central server aggregates the learned parameters coming from the clients, training a global machine learning model without sharing user's data. However, the state-of-the-art shows several approaches to promote attacks on FL systems. For instance, inverting or leaking gradient attacks can find, with high precision, the local dataset used during the training phase of the FL. This paper presents an approach, called Deep Leakage from Gradients with Feedback Blending (DLG-FB), which is able to improve the inverting gradient attack, considering the spatial correlation that typically exists in batches of images. The performed evaluation shows an improvement of 19.18% and 48,82% in terms of attack success rate and the number of iterations per attacked image, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Privacy-preserving solutions are a key requirement for al- most all computer applications, whether for legislation compli- ance or due to the mistrust of how sensitive data can be used. In an era marked by escalating concerns over data breaches and privacy violations, ensuring the confidentiality and integrity of personal information has become paramount for businesses and individuals alike [1]. Moreover, as technology continues to advance, the need for robust privacy-preserving techniques becomes even more pressing.\nFederated Learning (FL) emerges as a solution to provide data-privacy for smart systems because it enables distributed Machine Learning (ML) training, without sending user's data to a central point, providing an extra level of user data- privacy protection [2]. In FL, multiple ML models run on local privacy-sensitive datasets, simultaneously, and a global ML model, running on a server, is built without sharing the local datasets with the server.\nSome FL methods rely on gradient or weight sharing between clients and servers to train the global ML model [3]. Historically, there was a prevalent belief that sharing gradients was inherently secure, implying that the exchange does not compromise the confidentiality of the training data. However, the authors of [4] proposed a method called Deep Leakage from Gradients (DLG), showing how to invert the gradient to reconstruct the input data used in the training, becoming one of the most famous methods to attack FL models.\nThis paper aims to enhance the performance of DLG-based methods, proposing a new attack algorithm called Deep Leak-"}, {"title": "II. RELATED WORK", "content": "Gradient exchange is one of the prevalent techniques in contemporary multi-node machine learning setups, such as distributed training and collaborative learning, as Federated Learning. This section aims to describe the most relevant attacks able to exploit gradient in FL systems.\nDeep Leakage from Gradients (DLG) [4] and Improved Deep Leakage from Gradients (iDLG) [5] show how it is possible to leak private training data if an attacker has access to the shared gradients."}, {"title": "A. Deep Leakage from Gradients (DLG)", "content": "The work proposed by [4] presents an iterative method, called Deep Leakage from Gradients (DLG), based on an optimization algorithm that can obtain both the training inputs and the labels, considering an attacker accessing the gradient coming from a FL client, defined as VW.\nThe first step of DLG, after accessing the client gradient, is to randomly initialize a dummy input and label input. With the \"dummy data\" it is possible to compute \"dummy gradients\", defined as \u25bdW'. As depicted in Fig. 1, this attack seeks to approximate \u25bdW' to \u25bdW, changing iteratively the input and the label data. When the \u25bdW' is close to VW, it is possible to extract the data used by the client to train the machine learning model.\nAs can be noticed, this is an optimization problem, where the distance $||\u25bdW' \u2013 \u25bdW||_2$ is differentiable concerning"}, {"title": "B. Improved Deep Leakage from Gradients (iDLG)", "content": "The work proposed by [5] aims to improve the DLG method. In DLG, the attacker generates dummy data and corresponding labels under the guidance of shared gradients. Authors in [5] proposed Improved Deep Leakage from Gra- dients (iDLG), which exploits the relationship between the ground-truth labels and the signs of the gradients. This work shows that label information can be computed analytically from the gradients, and this information can be used to obtain train data more close to the original. As another advantage, this method is suitable for any differentiable model trained with cross-entropy loss on one-hot labels."}, {"title": "C. Other Relevant Attacks", "content": "Recovery of image data from gradient information was first discussed in [6] for neural networks. In this work, authors have proven that recovery is possible for a single neuron or linear layer.\nIn [7], the authors also address the leakage problem in deep learning. This work proposes a method to obtain the sample trained data for deep-learning models based on the ReLu function. However, for this method, it is necessary to access the entire learning process.\nIn another effort, the authors discuss the recovery of training data from shared gradients in distributed machine learning systems [8]. The original Deep Leakage from Gradients (DLG) method faces issues with accuracy and stability due to ex- ploding gradients and high learning rates. To address these issues, this study proposes the WDLG method, which uses the Wasserstein distance to calculate loss, enabling more faithful and efficient recovery of training data."}, {"title": "III. THREAT MODEL", "content": "For the proposed approach in this work, we consider the scenario of an honest but curious federated learning server seeking to access user data, i.e. a batch of images. The batch of image, which is the target of the attacker, is defined as $S = {s_1, s_2, ..., s_k}$, where $s_k$ is the last image to be"}, {"title": "IV. DEEP LEAKAGE FROM GRADIENTS WITH FEEDBACK\nBLENDING (DLG-FB)", "content": "In prior gradient-based attack methodologies, random noise, also known as dummy data, has traditionally been used to ini- tialize the data input fed into the first iteration of the Limited- memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) solver. However, with random data initialization, when attempting to reconstruct two consecutive attacked images, i.e., Sn and Sn+1, traditional approaches start with a new random guess for each iteration, meaning that each image reconstruction is entirely independent of the others.\nConsequently, although it is possible to gather information from the successful reconstructed images during the attacked sequence, this knowledge is not effectively used by the at- tacker.\nIn response to this scenario, we introduce a novel DLG- based attack strategy defined as Deep Leakage from Gradients with Feedback Blending (DLG-FB), as illustrated in Figure 2. This approach is designed for attacks that target a batch of images, which means the attacker aims to reconstruct a set of images from the same federated learning client. We assume these images have some level of spatial redundancy, keeping a certain degree of similarity. For instance, it is expected the attacked client has a batch of images with the same set of people or place. During the sequence to attack a batch of images, the main idea of DLG-FB is to blend successful reconstructed images and feed the solver with the blended image, making the blended image a better starting point for the attack than pure random data.\nDLG-FB algorithm requires two successfully obtained im- ages, as can be observed in labels 1 and 2 in Figure 2. After two reconstructed images, DLG-FB performs the images blending (see label 3) from attack 1 and 2 and uses it as initial guess for the attack 3 (label 4). This process keeps repeating for the next elements of the attacked batch. However, it is important to notice that the reconstruction can fail, due to a vast number of factors.\nIf an image is successfully reconstructed, it blends the new image with the previous blend to create a new composite, as can be seen in labels 5, 6 and 7 of Figure 2. In case of failure, it proceeds to the next image without incorporating the failed reconstructed image, which would introduce excessive noise.\nRegarding the image blending performed by DLG-FB, C represents the pixel matrix of an image, where Co is the resulting composite image, Ca and Cb are the two recon- structed images. In this work, the blending is determined by the following equation:\n$C_o = \\alpha C_a + (1 \u2013 \\alpha)C_b$"}, {"title": "V. EVALUATION AND RESULTS", "content": "This section presents the evaluation of the proposed attack algorithm. First, the test environment and the dataset used are introduced. Then, the obtained results are discussed."}, {"title": "A. Test Environment and Dataset", "content": "In our testing environment, we employed the PyTorch API within a Python virtual environment running on Manjaro Linux x86 64 OS, powered by an AMD Ryzen 5 5600G CPU, 4.464GHz. Besides, we utilized PyTorch's LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) optimizer with a learning rate set to 1.\nFor the performance evaluation of the approaches, we have used two well-known datasets: CIFAR100 and MNIST.\n\u2022\n\u2022\nCIFAR-100 is a widely used computer vision dataset comprising 60,000 color images grouped into 100 classes, each with 600 images. These 32x32 pixel images span diverse objects, animals, and scenes, organized into 20 superclasses.\nMNIST is a well-known dataset utilized in computer vision which comprises 70,000 grayscale images of handwritten digits, categorized into 10 classes. MNIST serves as a foundational benchmark for machine learning models.\nThis optimization algorithm is well-suited for large-scale problems, particularly in machine learning. Unlike the full BFGS algorithm, LBFGS conserves memory by storing only a few vectors to approximate the Hessian matrix, exploiting"}, {"title": "B. Obtained Results", "content": "We have compared the proposed DLG-FB with the follow- ing approaches: (i) Original DLG, (ii) original iDLG, (iii) iDLG-FB (iv) DLG-FB-Noise Factor (iDLG-FB-NF), and (v) DLG-FB-Noise Factor (DLG-FB-NF).\nIt is important to note that iDLG-FB represents the im- plementation of the proposed Feedback Blending strategy in iDLG. Additionally, for comparison purposes, DLG-FB-NF and iDLG-FB-NF are versions of the proposed Feedback Blending strategy that do not discard the dummy data from unsuccessful attack attempts. Instead, they blend it in, aiming to reduce overfitting. These versions always blend the output image, even if the attack sequence fails. Moreover, for the obtained results shown in this section, $\\alpha = 0.5$ (see eq. 1).\nFigure 3 and 4 present the obtained results in terms of the number of successfully attacked images for the CIFAR- 100 and MNIST datasets, respectively. For CIFAR-100, the original iDLG recovered 996 images, while iDLG-FB recov- ered 1187 images, a +19.18% improvement. For DLG-based approaches, original DLG was able to recover 769 images compared to 908 images by the DLG-FB, a +14.07% increase. Regarding MNIST, which consists of simpler grayscale im- ages with no color channels, original iDLG recovered 1088 images, while iDLG-FB recovered 1186 images, a +9.01% im- provement. For MNIST, original DLG recovered 929 images compared to 934 by the DLG-FB version. The FB versions assist the solver by providing better initial guesses for the pixels. However, since grayscale images have significantly fewer pixels, the new strategy has a reduced impact."}, {"title": "VI. CONCLUSION AND FUTURE WORKS", "content": "Privacy-preserving solutions have become a critical neces- sity in almost all computer applications. Among the possible solutions to this issue, Federated Learning (FL) has emerged as a promising approach for safeguarding data privacy in smart systems. In this context, this paper introduces a novel approach, named Deep Leakage from Gradients with Feedback Blending (DLG-FB), aimed at enhancing the effectiveness of DLG-based methods in federated learning systems. DLG-FB takes advantage on the spatial redundancies present in batches of images. Unlike conventional methods that initialize input image-matrices with random data to attack a single image, DLG-FB employs a strategy to attack a entire batch of image. After more than two successful image reconstructions, DLG- FB computes a blend of images and uses it as the initial data rather than utilizing pure random values. DLG-FB reveals gains in both the number of images successfully attacked and the iterations required to achieve a successful attack.\nAs future work, the authors intend to add more mechanisms to the proposed DLG-FB, improving the criteria to perform the image blend. Also, a machine learning model can be used to make the approach more efficient. Moreover, the new version of the attack will be tested using other image datasets."}]}