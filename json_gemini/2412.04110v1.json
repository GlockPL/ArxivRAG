{"title": "Enhancing Mathematical Reasoning in LLMs with Background Operators", "authors": ["Jiajun Chen", "Yik-Cheung Tam"], "abstract": "We propose utilizing background operators for mathematical reasoning in large language models (LLMs). To achieve this, we define a set of fundamental mathematical predicates as the basic building blocks. For each mathematical problem, we develop a Prolog solution that includes problem-specific predicates and intermediate predicates derived from these background operators, ensuring that each solution adheres to the defined operator set. We introduce the MATH-Prolog corpus, which is derived from the counting and probability categories of the MATH corpus. For efficient data augmentation, we apply K-fold cross-validated self-training. This method incrementally generates new Prolog solutions for each fold, incorporating those verified as correct into the training set throughout the model training process. Our experimental results demonstrate that 5-fold cross-validated self-training effectively identifies new, accurate Prolog solutions, achieving an accuracy of 84.6% on the cross-validated set, and 84.8% on the test set during fine-tuning the Meta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new solutions with fully computable inference steps for previously unseen problems. Additionally, incorporating the background mathematical predicates into the prompt enhances solution coverage.", "sections": [{"title": "Introduction", "content": "Recently, large language models have shown success in tackling mathematical problems such as chain of thought (Wei et al., 2022; Zhihong Shao, 2024), tree of thought (Yao et al., 2023), program of thought (Chen et al., 2023), and graph of thought (Besta et al., 2023). For example, chain of thought has an advantage that each reasoning step is described verbally which is good for human understanding. However, verbal reasoning steps are not computable and therefore hard for machines to verify for flaws. Generating computer programs with Python codes such as Sympy for mathematical reasoning is promising (Gou et al., 2023). However, this approach has limitation for logical reasoning since the programming language is procedural in nature. Using Prolog language to describe a mathematical problem would be more powerful. Solving a mathematical problem is converted into searching for a final answer such that problem-specific constraints are satisfied.\nIn this paper, we cast solving mathematical reasoning problems via Prolog which is realized as a set of predicates. A search process facilitated by an external interpreter, e.g. Prolog, is performed to find the feasible values of the predicates serving as constraints. Each constraint in Prolog must be satisfied to arrive at a final answer. Many questions in the MATH corpus can be casted as search problems satisfying a set of constraints. For example, for a problem \u201cIf two numbers are randomly chosen without replacement from {1, 2, 3, 4, 5}, what is the probability their sum is greater than their product?\", the constraints are: 1) The two numbers A and B belongs to the set {1, 2, 3, 4, 5}; 2) $A + B > A \\cdot B$; 3) A < B. We want a language model to write Prolog code that declares the constraints and search for a set of feasible solutions {(A, B)} to compute the final probability.\nTo enable a large language model to generate Prolog code, our proposal is to curate MATH-Prolog, a corpus for solving mathematical reasoning problems based on Prolog. Then, we fine-tune a large language model on the MATH-Prolog to learn how to generate Prolog codes. To enforce standardized solutions, we curate a set of predefined background mathematical predicates, e.g. combination(n, k, out) and factorial(n, out) as standard predicates so that all Prolog solutions must be constructed based on these background operators. As another viewpoint, two predicates A(X1, X2, Y), B(Y, Z) takes two input symbols X1 and X2 and outputs Y which is an input to predicate B(Y, Z) resulting in an output Z. Therefore, a Prolog solution is an instance of a directed computation graph as illustrated in Figure 1, where the input nodes define the problem-specific predicates inferred from the problem statement, and the intermediate nodes are the background operators which are connected with each other so that the final result X is arrived. A pretrained large language model is finetuned to generate a computation graph to solve a mathematical reasoning problem.\nOur research has the following contributions:\n1) We curate and open-source the MATH-Prolog dataset focusing on counting and probability to enable large language models to generate declarative Prolog codes to solve competition-level mathematical reasoning problems. 2) We devise a cross-validated self-training algorithm that generates and explores Prolog code solutions during model finetuning on unseen problems. 3) Experimental results show that cross-validated self-training algorithm generates codes that lead to 84.8% accuracy on the test set.\""}, {"title": "Corpus Design", "content": "We design the MATH-Prolog based on the MATH corpus, a benchmark for competition-level mathematical reasoning problems. We choose counting and probability in MATH for corpus curation consisting of 771 problems on the training set. For each problem in the training set, we curate a Prolog solution which consists of two parts: 1) Problem-specific predicates which are directly inferred from the problem statement; 2) The Solve predicate composing the intermediate predicates as constraints required to solve the given problem. In total, we have 54 background operators manually crafted from high-school textbooks, and all Prolog solutions are standardized to this operator set. Some background operators such as palindrome(n) are included since some training questions require the palindrome constraint. With the help of GPT-4 and few-shot prompting technique, we handcraft Prolog solutions of a subset of the problems in the training set, and use GPT-4 to generate Prolog solutions on the rest of training problems following (Yang et al., 2024). Since the MATH corpus is competition level, many of the GPT-4 generated solutions actually do not evaluate to correct final answers. Therefore, we manually check the generated solutions and make code adjustments so that the evaluated Prolog codes result in correct reference answers. This costs one manual labor for about one month to curate the high-quality and 100% accurate MATH-Prolog training corpus for research. Due to limited manual resources, we did not manually curate the 474 problems on the test set. Inspired by (Wang et al., 2024), we employ a self-training algorithm to automatically generate Prolog solutions for the test set."}, {"title": "Findall Predicate", "content": "Given that we have a finite set of problem-specific predicates and a finite set of predicates, the correct solution can be derived systematically by assembling the variables and predicates. From a graphical perspective, these variables and predicates can be viewed as nodes, with directed edges representing the flow of inputs and outputs within the solution. However, exhaustively traversing all possible combinations of variables and predicates is computationally expensive. To address this challenge, we employ the findall(.) predicate to represent the constraints derived from the problem statements, effectively narrowing down the search space. findall(.) is a standard predicate defined in SWI-Prolog. By leveraging the findall(.) predicate, we can express the relevant constraints in a compact manner. Figure 2 gives an example of the usage of findall(.) predicate that returns a list of valid pairs (A, B) satisfying the constraints required by the problem. Overall, about 26% of the Prolog solutions in the training set involves the findall(.) predicate. Using findall(.) departs from the chain of thought solution which is mainly verbal natural language reasoning without invoking constraint search."}, {"title": "Solution Diversity", "content": "For mathematical reasoning problem, there may exists multiple solutions with variations. For example, a problem \u201cHow many ways are there to put 5 balls in 2 boxes if the balls are not distinguishable but the boxes are?\", one Prolog solution employs findall(.) and another solution uses combination(.). Our preliminary experiment shows that even we use a finetuned large language model to generate solutions on training questions, the generated solutions can be different from the original training references. We believe that improving solution diversity will help improve the performance of a large language model especially for data augmentation. Therefore, we devise a cross-validated self-training procedure to sample multiple different solutions during model finetuning. Algorithm 1 shows the self-training procedure that requires a training and a generation set without overlap. For K-fold training, we set the k-th fold as a generation set and the rest as a training set.\""}, {"title": "Experiment", "content": "We used the MATH corpus (Hendrycks et al., 2021) and chose the counting and probability domain described in Section 2 to construct the MATH-Prolog corpus. Initially, we employed GPT-4 to automatically generate Prolog codes with background operators for domain-specific questions, followed by manual correction to ensure accuracy. Prior to implementing the self-training procedure, we applied a filtering process to exclude training instances that could not be resolved using the provided numerical data or domain-specific commonsense knowledge. This resulted in a final set of 625 well-constructed Prolog samples, which we denote as D. To implement the self-training procedure described in 2.2, we partitioned the dataset into five folds, denoted D1,..., D5. The input prompting format follows the instruction prompts used in Stanford Alpaca (Taori et al., 2023). We removed samples exceeding 2700 tokens to ensure that a training run can be fit into a 4090 GPU. All background operators were included in an input prompt to measure its effect. (see Appendix A.3 for details)."}, {"title": "Setup", "content": "We conducted all experiments using the LLaMA-3.1 8B model (et. al., 2024). To reduce memory consumption while preserving performance, we employed 8-bit quantization and LORA (Hu et al., 2022) for efficient model training. We divided the training set into five folds and applied self-training to sample two correct samples per question on the heldout fold. We experimented self-training using the test set as a generation set to discover Prolog solutions. See Appendix A.6 for further training details."}, {"title": "Training", "content": "We report accuracy (correct solution coverage) as a metric:\n$Acc = \\frac{\\sum_{i=1}^{K} \\sum_{j=1}^{|D|} \\mathbb{1} {p(a)_i(s_{pred}) = p(a)_i(s_{true})}}{|D|}$\nwhere $P(a)_i$ refers to the Swi-Prolog interpreter, and $K$ is set to 5 folds. The metric measures the ability of a large language model to discover Prolog solutions in the unseen dataset."}, {"title": "Evaluation", "content": "As shown in Figure 3, the coverage of correct generation achieves 84.6% for the 5-fold cross-validation experiment. When we used the test set in self-training, test set accuracy was 75.7% with background operators listed in the input context compared to 71.1% without background operators after running for 3900 self-training steps. This showed that the introduction of background operators in the input context helped, and agreed with what were observed in the cross-validated self-training. When we used the derived test solutions from the first-round self-training and augmented them into the training set and rerun self-training again, the test set accuracy was increased to 84.8%. We observed that some generated solutions exploits the compositional and decompositional nature of predicates (See A.1 for examples). Their equivalence represent different problem-solving strategies giving diverse and high-quality solutions."}, {"title": "Results", "content": "As shown in Figure 3, using background operators in an input prompt exhibits a more efficient learning trajectory. By inspection, the model first tackled easy questions that involves one operator in early training steps, while complicated questions were tackled that involves multiple predicates at later training steps. For solution discovery sake, more training steps were preferable."}, {"title": "Self-training is effective.", "content": "The Chain-of-Thought (COT) prompting approach (Wei et al., 2022) was among the first attempts toward generating logical, verbal step-by-step reasoning. Several advanced techniques have been developed to enhance the reasoning capabilities (Zhou et al., 2023; Zhu et al., 2023; Huang et al., 2022; Liang et al., 2023). However, most of these approaches rely on natural language reasoning, which faces limitations in identifying flaws in verbal reasonings. To overcome this bottleneck, a notable attempt involves utilizing trees and graphs for reasoning, as graph-based structures (Zhang et al., 2020) have demonstrated potential in solving mathematical word problems (Kipf and Welling, 2016). (Yu et al., 2023; Jiang et al., 2024) employed forward and backward data augmentation via variable and final answer masking on COT solutions for data augmentation. Our work extends the concept of graphical reasoning using predefined predicates to generate diverse and computable solutions."}, {"title": "Related Work", "content": "External tools can be integrated into large language models to effectively enhance both the reasoning capabilities and interpretability of LLMs(Cobbe et al., 2021; Mishra et al., 2023; Gou et al., 2023; Gao et al., 2023; Shao et al., 2023; Chen et al., 2023; Trinh et al., 2024). Prolog, as a symbolic declarative language, maximizes the advantages for symbolic reasoning. It not only improves the logical coherence of natural language generation (Vakharia et al., 2024) but also strengthens the model's ability in arithmetic reasoning (Yang et al., 2024; Tan et al., 2024; Borazjanizadeh and Piantadosi, 2024). Our work employs standardized background operators for solving competition-level mathematical problems."}, {"title": "Structured Reasoning", "content": "Our work introduces MATH-Prolog, a dataset specifically designed for tackling competition-level mathematical problems using declarative logic programming. With predefined predicates, we create a structured and interpretable framework for solving mathematical problems. Our cross-validated self-training algorithm effectively generates full solutions with computable intermediate steps on unseen questions. Our experiments demonstrates its efficiency and accuracy, showcasing its potential for broad application in systematic mathematical problem-solving."}, {"title": "Tool-based Reasoning", "content": "While our approach of incorporating predefined background operators provides a productive and interpretable method to define standardized solutions, its applicability has not yet been tested in other domains. Currently, we have only developed Prolog codes for the counting and probability domain, which may limit the scope of our experiments. However, we believe that the method can be generalized to other domains within the MATH corpus. Additionally, we have observed that large language models occasionally generate Prolog code with syntax errors, which reduces the accuracy of code generation. Future work would involve expanding this approach to other domains of the MATH corpus. Moreover, constrained decoding (Lu et al., 2022; Geng et al., 2023) to eliminate syntax errors in generated code deserves further exploration. Lastly, our experiments have been limited to LLaMA-3.1 8B model. Future research would explore the effectiveness of our method to other models."}, {"title": "Conclusion", "content": ""}, {"title": "Limitation", "content": ""}, {"title": "Diversity Examples", "content": "In Figure 4, consider the question: \"Ben rolls four fair 20-sided dice, each with faces numbered from 1 to 20. What is the probability that exactly two of the dice show an even number?\" Initially, we define the predicate hal f_probability(0.5) to represent the likelihood of a specific outcome for a die. However, after applying the cross-validated self-training procedure, the model identifies an alternative interpretation of the problem, where the probability of observing either an even or odd number is both 0.5. This result highlights the model's ability to extract implicit information from the problem statement, which can deviate from our original understanding.\nWhen generating a solution for a question such as \"Ryan has 3 red lava lamps and 3 blue lava lamps. He arranges them randomly in a row on a shelf, and then randomly turns 3 of them on. What is the probability that the leftmost lamp is blue and off, and the rightmost lamp is red and on?\", the model discovers an alternative solution strategy. Graphically, the new solution involves a decomposition of the combination operator. If we denote the combination operator as Com, the permutation operator as Per, the factorial operator as Fac, and the division operator as Div, then Com = Per \u2295 Fac\u2295 Div. This type of decomposition offers a structured and robust approach for augmenting new diverse samples."}, {"title": "background operators", "content": "Below, we provide a detailed overview of the predicates used in the counting and probability domain of the MATH-Prolog corpus. We describe the input-output behavior of each operator, as well as its potential for decomposition, which is leveraged in the data augmentation process."}, {"title": "Instruction Prompt", "content": "Below are the instruction prompts we use for different settings:"}, {"title": "Error Analysis", "content": "Below, we present some common errors generated by the LLaMA-3.1 8B model during the self-training procedure, to better understand the limitations and challenges in generating Prolog programs. These examples were selected from experiments that included predicates in the input prompts. The majority of problematic solutions exhibit syntax errors, while others fail to correctly identify and apply the constraints specified in the problem statements. A few typical syntax errors are listed below, with the erroneous lines highlighted in bold. Detailed explanations of each error are provided in the accompanying comments."}, {"title": "Training Details and Computational Budget", "content": "We applied LoRA to fine-tune the query and value weight matrices within the transformer blocks. After exploring various hyperparameter configurations, we selected a LoRA rank and scaling factor of (r, a) = (8, 16) to achieve an optimal trade-off between performance and computational efficiency. To collect diverse Prolog solutions, we ran the self-training experiments for 100 epochs with batch size of 16, and learning rate of 3 \u00d7 10\u20134. For a self-training run, we used 1 NVIDIA RTX 4090 GPU to finetune Meta-Llama-3.1-8B-Instruct model on each fold, taking around 2 days to finish. The sampling hyperparameters include temperature scaling of 0.6, top-k sampling of 40, and nucleus sampling of 0.9."}]}