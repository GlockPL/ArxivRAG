{"title": "Exploring the Frontiers of Animation Video Generation in the Sora Era: Method, Dataset and Benchmark", "authors": ["Yudong Jiang", "Siqi Wang", "Baohan Xu", "Siqian Yang", "Mingyu Yin", "Jing Liu", "Chao Xu", "Yidi Wu", "Bingwen Zhu", "Jixuan Xu", "Yue Zhang", "Jinlong Hou", "Huyang Sun"], "abstract": "Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation dataset. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, the evaluation on VBench and human double-blind test demonstrates consistency in character and motion, achieving state-of-the-art results in animation video generation. Our model access API and evaluation benchmark will be publicly available.", "sections": [{"title": "1. Introduction", "content": "The animation industry has seen significant growth in recent years, expanding its influence across entertainment, education, and even marketing. As demand for animation content rises, the need for efficient production processes is also growing quickly, particularly in animation workflows. Traditionally, creating high-quality animation has required extensive manual effort for tasks like creating storyboards, generating keyframes, and inbetweening, making the process labor-intensive and time-consuming. Previous efforts [21, 28] to incorporate computer vision techniques have assisted animators in generating inbetween frames for animation. However, these methods often show effectiveness only within certain artistic styles, limiting their applicability to the varied demands of modern animations.\nWith recent advancements in video generation, there has been notable progress in generating high-quality videos across various domains. Inspired by Generative Adversarial Networks [9], Variational Autoencoders [13], and, more recently, transformer-based architectures [17, 22], the field has seen remarkable improvements in both efficiency and output quality. However, most video generation methods are trained and evaluated on general-purpose datasets, typically featuring natural scenes or real-world objects [3, 29]. The domain of animation video generation, which plays an important role ranging from entertainment to education, has received relatively little attention. Animation videos often rely on non-photorealistic elements, exaggerated expressions, and non-realistic motion, presenting unique challenges that current methods do not address.\nIn addition to the generation challenges, the evaluation of video generation is also inherently complex. Evaluating video generation quality requires assessing not only the visual fidelity of each frame but also temporal consistency, coherence, and smoothness across frames [11]. This challenge intensifies in animation, where unique artistic styles must remain consistent despite exaggerated motions and transformations. Progress in this field demands effective evaluation datasets tailored to animated video generation, enabling comprehensive testing of model adaptability to diverse styles, scene changes, and complex motions, thereby driving model optimization and innovation.\nIn this paper, as shown in Fig. 1, a full system AniSora is presented for animation video generation. First, our data processing pipeline offers over 10 million high-quality text-video pairs, forming the foundation of our work. Secondly, we develop a unified diffusion framework adapted for animation video generation. Our framework leverages spatiotemporal masking to support a range of tasks, including image-to-video generation, keyframe interpolation, and localized image-guided animation. By integrating these functions, our system bridges the gap between keyframes to create smooth transitions and enables dynamic control over specific regions, such as animating different characters speaking precisely. This enables a more efficient creative process for both professional and amateur animation creators. Fig. 2 demonstrates some examples generated by our model under image-to-video conditions.\nAdditionally, we propose a benchmark dataset specifically designed for animation video evaluation. Unlike existing evaluation datasets, which primarily focus on natural landscapes or real-world human actions, our dataset addresses the unique requirements of animation video assessment. To achieve this, we collected 948 animation videos across various categories and manually refined the prompts associated with each video.\nOur contributions can be summarized as follows:\n\u2022 We develop a comprehensive video processing system that significantly enhances preprocessing for video generation.\n\u2022 We propose a unified framework designed for animation video generation with a spatiotemporal mask module, enabling tasks such as image-to-video generation, frame interpolation, and localized image-guided animation."}, {"title": "3. Dataset", "content": "We build our animation dataset according to the observation that high quality text-video pairs are the cornerstone of video generation, which is proved by recent researches [18]. In this section, we give a detailed description of the construction of our animation dataset and the evaluation benchmark.\nAnimation Dataset Construction: We build a pipeline to get high-quality text-video pairs among 1 million raw animation videos. First of all, we use scene detection [4] to divide raw animation videos into clips. Then, for each video clip, we construct a filter rule from four dimensions: text-cover region, optical flow score, aesthetic score, and number of frames. The filter rule is gradually built up through the observations in model training. In detail, the text-cover region score (obtained by [2]) can drop those clips with text overlay similar to end credits. Optical flow score [19] prevents those clips with still images or quick flashback scenes. Aesthetic score [6] is utilized to preserve clips with high artistic quality. Besides, we retain the video clips whose duration is among 2s-20s according to the number of the frames. After the four steps mentioned above, about 10% clips (more than 10 million clips) can be retained into training step. In addition, a few higher quality clips will be finally filtered from training set to further improve the model's performance. Specifically, during the training process, we adjust the proportions of specific training data (e.g., talking and motion amplitude) according to the observed performance.\nBenchmark Dataset Construction: Moreover, to compare the generation videos between our model and other recent researches directly, we construct a benchmark dataset manually. 948 animation video clips are collected and labeled with different actions, e.g., talking, walking & running, eating, kissing, and so on. Among them, there are 857 2D animation clips and 91 3D clips. These action labels are summarized from more than 100 common actions with human annotation. Each label contains 10-30 video clips. The corresponding text prompt is generated by Qwen-VL2 [25] at first, then is corrected manually to guarantee the text-video alignment."}, {"title": "4. Method", "content": "In this section, we present an effective approach for animation video generation using a diffusion transformer architecture. Section 4.1 provides an overview of the foundational video diffusion transformer model. In section 4.2, we introduce a spatiotemporal mask module that extends the diffusion transformer model, enabling crucial animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation within a unified framework. These enhancements are essential for professional animation production. Finally, section 4.3 details the supervised fine-tuning strategy employed on the animation dataset."}, {"title": "4.1. Dit-based Video Generation Model", "content": "We adopt a DiT-based [17] text-to-video diffusion model as the foundation model. As shown in Fig. 3, the model leverages the three components to achieve coherent, high-resolution videos aligned with text prompts.\n3D Casual VAE used in video generation frameworks [10, 30]serves as a specialized encoder-decoder architecture tailored for spatiotemporal data compression. This 3D VAE compresses videos across both spatial and temporal dimensions, significantly reducing the diffusion model computing. We follow the approach of Yang et al. [29] to extract latent features, transforming the original video with dimensions (W, H, T, 3) into a latent representation of shape (W/8, H/8,T/4, 16).\nPatchify is a critical step for adapting vision tasks to transformer-based architectures [1]. Given an input video of size T \u00d7 H \u00d7 W \u00d7 C, it is split spatio into patches of size P \u00d7 P, and temporal into size Q resulting in (T/Q) \u00d7 (H/P) \u00d7 (W/P) \u00d7 C patches. This method enables efficient high-dimensional data processing by reducing complexity while retaining local spatial information.\n3D Full Attention is a module we propose for spatial and temporal modeling, inspired by the remarkable success of long-context training in large language models (LLMs) [8] and foundation video generation models [18, 29].\nDiffusion schedule applies Gaussian noise to an initial sample xo over T steps, generating noisy samples Xt = $\\sqrt{a_t} x_0 + \\sqrt{1 - a_t} \\epsilon$, where $a_t = \\prod_{i=1}^t (1 - \\beta_i)$ and $\\epsilon \\sim N(0, I)$. The reverse process predicts e by minimizing the mean squared error:\n$L_{diffusion} = E_{x_0,\\epsilon,t} [||\\epsilon - \\epsilon_{\\theta}(x_t, t)||_2]$.\nTo stabilize training, we use the v-prediction loss [20], where $v = \\sqrt{1 - a_t} x_0 - \\sqrt{a_t} \\epsilon$ and the loss becomes\n$L_{v-prediction} = E_{x_0,v,t} [||v - v_{\\theta} (x_t, t)||_2]$.\nThis approach enhances stability and model performance."}, {"title": "4.2. Spatiotemporal Condition Model", "content": "Keyframe Interpolation creates smooth transitions between key-frames by generating intermediate frames, or \"in-between.\" It is an essential stage in professional animation production and represents some of the most labor-intensive tasks for artists. We extend this concept to video generation conditioned on one or multiple arbitrary frames placed at any position within a video sequence.\nMotion Control, as a technique within our framework, addresses the limitations of text-based control and enables precise control over motion regions. This approach enhances artists' control over video content, allowing them to express their creativity while significantly reducing their workload."}, {"title": "4.2.1 Masked Diffusion Transformer Model", "content": "In the Masked Diffusion Transformer framework, we construct a guide feature sequence G = {G1, G2,...,Gn} by placing the VAE-encoded guide frame Fp\u2081 at designated positions pi, while setting Gj = 0 for all other positions j\u2260 pi. A corresponding mask sequence M = {M1, M2,..., Mn} is generated, where Mp\u2081 = 1 for guide frame positions and M\u2081 = 0 otherwise. The mask is processed through a re-projection function, yielding an encoded representation Reproj(M). The final input to the Diffusion Transformer is the concatenation of noise, encoded mask, prompt's T5 feature, and guide sequence along the channel dimension:\nX = Concat(Noiset, Reproj(M), G,T5) (1)\nThis setup integrates position-specific guidance and mask encoding, enhancing the model's conditioned generation capabilities."}, {"title": "4.2.2 Motion Area Condition", "content": "This framework can also support spatial motion area conditions inspired by Dai et.al [7]. Given the image condition Fp, and motion area condition is represented by mask MF, the same shape with Fp\u2081. Motion area in MF is labeled 1, other place is set to 0. As equation 1 in 4.2.1, for guide frame position pi, set Mp\u2081 = MF. The data processing and training pipeline can be summarized as follows: Constructing video-mask pairs, we first construct paired training data consisting of videos and their corresponding masks. Using a foreground detector by Kim et.al [12], we detect the foreground region in the first frame of the video. This region is then tracked across subsequent frames to generate a foreground mask for each frame. Union of foreground masks, the per-frame foreground masks are combined to create a unified mask MF, representing the union of all foreground regions across the video. Video latent post-processing, for the video latent representation zo, non-moving regions are set to the latent features of the guide image, ensuring static areas adhere to the guide. LoRA-based conditional training, we train the conditional guidance model using Low-Rank Adaptation (LoRA) with a parameter size of 0.27B. This approach significantly reduces computational requirements while enabling efficient model training."}, {"title": "4.3. Supervised Fine-Tuning", "content": "We initialize our model with the pre-trained weights of CogVideoX, which was trained on 35 million diverse video clips. Subsequently, we perform full-parameter supervised fine-tuning (SFT) on a custom animation training dataset to adapt the model specifically for animation tasks.\nWeak to Strong. Our video generation model adopts a weak-to-strong training strategy to progressively enhance its learning capabilities across varying resolutions and frame rates. Initially, the model is trained on 480P videos at 8fps for 3 epochs, allowing it to capture basic spatiotemporal dynamics at a lower frame rate. Following this, the model undergoes training on 480P videos at 16fps for an additional 1.9 epochs, enabling it to refine its temporal consistency and adapt to higher frame rates. Finally, the model is fine-tuned on 720P videos at 16fps for 2.3 epochs, leveraging the previously learned features to generate high-resolution, temporally coherent video outputs. Additionally, we applied stricter filtering as in section3, producing a 1M ultra high-quality dataset for final-stage fine-tuning, significantly boosting high-resolution video quality.\nRemoving Generated Subtitles. The presence of a significant number of videos with subtitles and platform watermarks in our training data led to the model occasionally generating such artifacts in its outputs. To mitigate this issue, we performed supervised fine-tuning using a curated dataset of videos entirely free of subtitles and watermarks. This dataset, consisting of 790k video clips, was constructed through proportional cropping of videos containing subtitles and the selection of clean, subtitle-free videos. Full-parameter fine-tuning was then applied to the model, and after 5.5k iterations, we observed that the model effectively eliminated the generation of subtitles and watermarks without compromising its overall performance.\nTemporal Multi-Resolution Training. Given the scarcity of high-quality animation data, we employ a mixed training strategy using video clips of varying durations to maximize data utilization. Specifically, a variable-length training approach is adopted, with training video durations ranging from 2 to 8 seconds. This strategy enables our model to generate 720p video clips with flexible lengths between 2 and 8 seconds.\nMulti-Task Learning. Compared to the physically consistent motion patterns in the real world, animation styles, and motion dynamics can vary significantly across different works. This domain gap between datasets often leads to substantial quality differences in videos generated from guide frames with different artistic styles. We incorporate image generation into a multi-task training framework to improve the model's generalization across diverse art styles. Experimental results in the appendix demonstrate that this approach effectively reduces the quality gap in video generation caused by stylistic differences in guide frames.\nMask Strategy. During training, we unmask the first, last, and other frames obtained through uniform sampling with a 50% probability. This strategy equips the model with the ability to handle arbitrary guidance, enabling it to perform tasks such as in-betweening, first-frame continuation, and arbitrary frame guidance, as discussed in Section 4.2.1."}, {"title": "6. Conclusion", "content": "In this paper, our proposed AniSora, a unified framework provides a solution to overcoming the challenges in animation video generation. Our data processing pipeline generates over 10M high-quality training clips, providing a solid base for our model. Leveraging a spatiotemporal mask, the generation model can create videos based on diverse control conditions. Furthermore, our evaluation benchmark demonstrates the effectiveness of our method in terms of character consistency and motion smoothness. We hope that our research and evaluation dataset establish a new benchmark and inspire further work in the animation industry.\nDespite the promising results, some artifacts and flickering issues are still present in our generated animation videos. In the future, we aim to develop a comprehensive automated scoring system specifically designed for animation video evaluation datasets, ensuring closer alignment with human subjective perceptions. Additionally, we plan to expand the current model architecture to incorporate guidance across multiple modalities, such as camera movements, trajectories, skeletal motions, and audio. To tackle the challenge posed by the limited availability of high-quality animation data, we will employ reinforcement learning techniques to further refine the model's performance."}]}