{"title": "Ethical AI Governance: Methods for Evaluating Trustworthy AI", "authors": ["Louise McCormack", "Malika Bendechache"], "abstract": "Trustworthy Artificial Intelligence (TAI) integrates ethics that align with human values, looking at their influence on AI behaviour and decision-making. Primarily dependent on self-assessment, TAI evaluation aims to ensure ethical standards and safety in AI development and usage. This paper reviews the current TAI evaluation methods in the literature and offers a classification, contributing to understanding self-assessment methods in this field.", "sections": [{"title": "1. Introduction", "content": "Artificial intelligence (AI) is increasingly integrated into numerous sectors, making ethical considerations and trustworthiness in AI systems more critical than ever. Behavioural science is utilised to achieve objectives in areas such as climate change mitigation and educational attainment [1], a trend which also extends to Trustworthy AI (TAI). TAI is a crucial concept within the field of ethical AI, which encompasses the ethical considerations essential in the development and use of Al systems[2]. Leading TAI frameworks[3][4][5] incorporate behavioural science principles to ensure AI systems align with human values, considering their impact on behaviour and decision-making. Additionally, bidirectional human-AI alignment emphasises aligning AI to human values and enabling humans to adjust to Al advancements cognitively and behaviourally[6].\nThe European Commission Assessment List for Trustworthy AI (ALTAI) [7] and the European Union (EU) AI Act[8] are essential TAI guidelines, emphasising a human-centred, interdisciplinary approach. One recommended governance approach is establishing Standard-Setting Organisations that ensure minimum standards for testing, documentation and public reporting[9]. Despite the availability of various standards such as ISO/IEC 42001[10], evaluating and auditing AI systems remains challenging. Several key surveys, such as those by Liu et al. [11] and Chamolaetal et al. [12], compile summaries of existing technical methods and technology in TAI. However, these surveys do not focus on methods to score the areas of TAI. Ojewale et al. [13] propose a process for AI auditing, and although this work highlights the need for metrics and standards, it does not delve into the methods for calculating such metrics.\nIn this paper, we summarise and propose a classification and sub-classification for existing methods and systems to govern, evaluate, and score AI systems for trustworthiness aligned with the interdisciplinary human-centred approach taken by the EU. We also discuss challenges and future work in this area."}, {"title": "2. Methodology", "content": "Our survey was conducted through a Google Scholar query to identify methods used in the literature for TAI evaluation. In addition, we added articles, regulatory documentation, and ISO standards in this area through snowballing."}, {"title": "2.2. Research Questions", "content": "The following are the identified research questions for this review:\n\u2022 Q1: What TAI evaluation methods and systems exist in the literature?\nQ2: What barriers to evaluating TAI are highlighted in the literature?"}, {"title": "2.3. Research Search and Data Extraction Strategy", "content": "A search string for Google Scholar was designed to capture papers discussing topics in machine learning, trust and evaluation areas. Two researchers independently"}, {"title": "3. Methods for Evaluating Trustworthy AI", "content": "In this section, we propose a classification for evaluating and scoring TAI. Of the papers reviewed, we found several approaches to AI scoring methods that considered various areas within TAI. Based on maturity and the type of solution proposed, we classed these papers into four categories: conceptual evaluation methods, Manual evaluation methods, Automated Evaluation Methods and Semi-Automated Evaluation Methods. In addition to this, we proposed a sub-classification based on the topic being evaluated. These sub-classifications are fairness & compliance evaluation, transparency evaluation, risk & accountability evaluation and trust & safety evaluation.\nconceptual approaches, indicating the lack of maturity in this field. This figure also shows the breakdown of evaluation approaches by topic, particularly the number of automated and semi-automated evaluation methods already developed in fairness and compliance, one of the more researched areas of trustworthy AI."}, {"title": "3.1. Conceptual Evaluation Methods", "content": "The existing research includes several high-level governance frameworks that consider multiple dimensions of trustworthy AI throughout the AI lifecycle. Conceptual evaluation methods are high-level methods that do not provide implementation details or are not tested and validated. While conceptual frameworks in the literature can be holistic, they can also lack detail."}, {"title": "3.1.1. Fairness & Compliance Evaluation", "content": "Several conceptual approaches sought to evaluate and improve fairness and compliance in AI systems, introducing concepts like policy violation detection [14], using Al to define ethical behaviour [15] [16] and automating fairness auditing [17] [18]. Researchers used a variety of approaches in deciding what was fair, including incorporating existing established ethical guidelines [16], extracting ethical guidelines from social media [15], using a third-party regulator[18], [17] and extracting guidelines from policy documents[14]."}, {"title": "3.1.2. Transparency Evaluation", "content": "Researchers proposed approaches that included evaluating transparency in areas such as healthcare[19] and finance [20]. The proposed framework by Lee[20] involved scoring fairness and interoperability, allowing humans to oversee and make conscious choices affecting both. The approach is context-conscious fairness and considers the trade-off between accuracy and interpretability and the trade-off between aggregate benefit and inequity. Trade-offs are benchmarked to make transparent, context-based, informed choices when using Machine Learning (ML) for decision-making. Jia et al. [19] proposed a framework to measure and improve technical robustness, safety, and transparency. It involved quantifying performance and XAI and establishing a trade-off between these trust properties for the ML algorithm selection for their healthcare use case."}, {"title": "3.1.3. Risk & Accountability Evaluation", "content": "Researchers also proposed conceptual governance frameworks that focused on risk management and accountability. These included ethical AI risk evaluation frameworks that built on the existing concepts such as operational design domain (ODD) [21] [22]. The importance of defined safety boundaries was also highlighted[23][22]."}, {"title": "3.1.4. Trust & Safety Evaluation", "content": "Conceptual evaluation frameworks also addressed trust[26][27] and safety [28]. These frameworks firstly focused on identifying evaluation criteria or trust risk areas, then on methods to address these risk areas to improve trust[26][27].\nFisher et al.[28] discuss several use cases, focusing on safety-critical domains that require new standards and verification, validation, and certification methods. They include a classification of verification methods, including formal exhaustive static methods like model checking and theorem proving, non-exhaustive dynamic semi-formal methods like runtime verification and software testing, and non-exhaustive static methods like static analysis. The paper highlights the difficulty in certifying autonomous systems due to their complexity and evolving nature. Multiple stakeholder involvement creates complexity in establishing a consensus on acceptable ethical standards or evaluation criteria that do not disclose sensitive information.\nUm et al.s[26] layered trust framework includes a Trust Agent for data extraction, a Trust Analysis layer for computing trust metrics, and a Trust Management layer, addressing risk, fairness, security, design, traceability, data security, data privacy, and data pre-processing. Broderick et al.[27] created a taxonomy of trust in AI, which includes a process diagram for assessing the areas in which trust in ML can fail. They considered real-world use cases for finance, healthcare, and politics and subsequently provided ways to mitigate the risk and increase trust at each stage. Their conceptual process seeks to assess and mitigate the level of user trust, specifically the trust of an expert in their field at each stage."}, {"title": "3.2. Manual Evaluation Methods", "content": "One method proposed for assessing TAI is a manual questionnaire. Beyond the questions from the EU ALTAI[7] and ISO/IEC standards [29] [10], six additional questionnaires were identified to score Al systems for trustworthiness. Manual questionnaires align with this area's regulation, considering multiple EU TAI principles. The disadvantage of the manual approach is that these questionnaires are typically time-consuming. This can lead to business constraints in completing the questions due to limited information about the external data the systems used[30]."}, {"title": "3.2.1. Fairness & Compliance Evaluation", "content": "Approaches to improve fairness and achieve compliance in machine learning were proposed by researchers [31] [32]. One approach was a practical questionnaire to help improve fairness by detecting bias[31]. A second approach to audit and score fairness in ML considered twelve metrics in this area [32]. The first six metrics focus on the stages of data collection, model development, feature selection and model performance-three metrics related to the human relationship with the model's decisions or predictions. The final metrics focus on assessing fairness from a broader social impact and include the three meta-components: cultural context, respect, and the research design process."}, {"title": "3.2.2. Transparency Evaluation", "content": "Transparency-focused questionnaires that focused on assessing the transparency of several TAI principles were also proposed by some researchers [33] [30] [34]. A notable questionnaire in the area of transparency is Bommasani et al.[34] who proposed The Foundation Model Transparency Index (FMTI), which included 100 indicators for transparency to be self-scored using a three-tier questionnaire and included benchmarks for leading organisations such as Open AI, AWS and Meta. Other researchers created separate transparency criteria for different tiers of stakeholders[33] and proposed using weighted questions using a 3-point scale for each question[30]. Transparency was also a consideration by researchers who looked at other areas such as user trust[35]."}, {"title": "3.2.3. Risk & Accountability Evaluation", "content": "For security evaluation, researchers [36] scored existing questionnaire-based frameworks used in industry NIST[37], COBIT [38], ISO27001 [29], and ISO42001[10] for their potential usage for Al's that incorporate Large Language Models (LLMs). Additionally, researchers developed a framework to evaluate the MITRE ATLAS [39] framework's effectiveness in protecting ML systems from poisoning attacks, scoring multiple TAI principles using a qualitative severity rating scale[40]."}, {"title": "3.2.4. Trust & Safety Evaluation", "content": "Several questionnaire-based papers focused on trust and safety evaluation, typically asking users about their trust in various Al systems[41][42][35].\nOne approach was a simple unweighted user survey-based questionnaire, which scored several aspects of TAI evaluation, including intent and limitations, data, explainability, safety and robustness, audibility, and accountability [41]. Researchers also developed frameworks that used surveys to quantify and improve user trust by improving the transparency of the system[42][35]. Both papers successfully indicated a correlation between increased transparency and increased user trust in AI."}, {"title": "3.3. Automatic Evaluation Methods", "content": "This section includes papers investigating automated scoring methods for TAI Principles. Automatic methods ensure consistency in evaluation, however they rely on predefined metrics which do not exist for many aspects of trustworthy AI. The automated methods published to date are technical methods to evaluate and score the technical aspects of trustworthy AI with established methods and metrics."}, {"title": "3.3.1. Fairness & Compliance Evaluation", "content": "Several automated methods published to date are technical methods to evaluate and score fairness [43][44][45]. Notable methods include using data sampling techniques to measure and understand root causes of bias [44] and a sentence-based evaluation that used sentence likelihood difference (SLD) to calculate gender bias in LLMs[45].\nCertification of fairness in AI systems was also considered by researchers who proposed a standard operating procedure (SOP) for fairness certification, Fairness Score and Bias Index, noting that different metrics would be needed to score pre-processing and in-processing and that the approach would be required to vary by use-case[46]. Researchers found that specific algorithms scored better for one set of individual features than others, indicating a link between fairness evaluation and algorithm selection[47]."}, {"title": "3.3.2. Trust & Safety Evaluation", "content": "The automated evaluation of trust and safety of AI systems was also considered by researchers [48] [43]. Researchers proposed an automated trust scoring process that used machine learning to develop a trust value for their use case of file sharing in peer-to-peer networks, automating a process to score the technical safety and likelihood of the file being dangerous[48]. Additionally, researchers developed a process that combined privacy and fairness evaluation, scoring both and proposing a trade-off for accuracy for each[43]."}, {"title": "3.4. Semi-automated Evaluation Methods", "content": "This section covers approaches to scoring, which involve automated and manual steps. These methods are primarily in the area of fairness and compliance. They require a human at some stage, balancing automation and human efficiency. Researchers have shown the need to tailor evaluations by using case[49][18][50] and to incorporate considerations such as cultural differences in fairness evaluation [51]. In the case of healthcare, researchers reported that context was important in fairness evaluation for clinicians, noting a preference for a human-in-the-loop approach rather than a fully automated system[52]."}, {"title": "3.4.1. Fairness & Compliance Evaluation", "content": "Researchers have proposed several semi-automated evaluation methods for fairness and compliance in AI[49][53][54][55][56]. A number of these frameworks were automated methods of fairness evaluation combined with a human element to set thresholds or decide trade-offs between metrics. One approach included developing transparent processes that mapped trade-offs between metrics [49], while a second involved injecting controls, wrapping existing operations and extending workflow primitives [53]. A third method included allowing a human to define the fairness requirement, specifying assumptions and assertions so that the tester can generate inputs that satisfy these assumptions and violate assertions[54]. A semi-automated user-centred approach to fairness evaluation called FairHIL (Fair Human-in-the-Loop) was developed that offers a visual user interface that provides a combination of visualisations including outcome features, feature intersection and causal graphs to help users identify bias and unfairness [55]. Users can add labels and adjust the feature weighting to retrain the model until they achieve an acceptable user fairness outcome. The tool focuses on accessibility and explainability for non-AI experts. Researchers also evaluated the effects of cultural differences in users interacting with the FairHil tool[56]."}, {"title": "3.4.2. Risk & Accountability Evaluation", "content": "One paper proposed a semi-automated method for risk evaluation. This structured method provides an open vocabulary for AI risks (VAIR) [57], facilitating the automation of AI risk category identification, a required step for Al assessment in the EU AI Act."}, {"title": "4. Industry Tools for Evaluating TAI", "content": "In addition to the aforementioned academic works in evaluating TAI, various industry tools are in use today that aim to ensure Al systems adhere to ethical, legal, and performance standards. The most commonly used tools are manual questionnaire-based tools such as the ALTAI[7]"}, {"title": "5. Barriers to Trustworthy AI Evaluation", "content": "The complexity required for a complete evaluation of TAI presents several challenges. The barriers to evaluating TAI found in the literature include the following:\nEvaluation methods exist for all aspects of TAI. However, the more mature areas of TAI have more advanced evaluation methods. For example, with several established methods, fully automated evaluation methods are available for fairness evaluation. Areas like risk and safety have some automatic and semi-automatic methods showing potential for more automation of technical aspects of Al where metrics are available. Evaluation approaches that considered less researched areas of TAI or holistic methods that considered multiple areas of TAI were primarily conception or manual methods.\nWithin the various TAI principles, there is a lack of consistency across all evaluation methods regarding what was being assessed. Even in similar industries using similar methods, the evaluation criteria or metrics used for evaluation were inconsistent. Regardless of the method used, this lack of consistency around evaluation criteria and metrics is a barrier to TAI evaluation and highlights a need to establish use case-specific benchmarks and acceptable thresholds for TAI evaluation.\nClinicians found that context was essential when deciding acceptable evaluations for AI fairness. Al systems are complex, and their design varies by use case. Due to this complexity, the evaluation method will vary by use case. For example, evaluating a decision-making AI system requires a different approach versus other AI use cases such as an LLMs.\nAlthough some automated methods exist to evaluate aspects of TAI, the semi-automated evaluation method is preferable if it integrates a human-in-the-loop. Additionally, due to a lack of maturity in many TAI principles, which have no metrics or automated methods for evaluation, a manual questionnaire-based stage is required for a comprehensive TAI evaluation. Additionally, even with more developed TAI principles such as fairness, a decision must be made manually to decide what is fair for the given use case.\nResearchers found that different stakeholders all required different"}, {"title": "6. Future Directions for Trustworthy Al Evaluation", "content": "To successfully evaluate TAI, the literature calls for future Al systems to have ongoing semi-automated evaluation capabilities. Successful prototypes include using transparent or explainable models, with an interface allowing human decision-making of thresholds, trade-offs and/or definitions to be input into the model. This can be done by an expert in the field or a third-party accreditation body. Universal evaluation criteria and thresholds do not apply from one use case to the next, meaning that TAI principles would need a specific evaluation criterion for each use case.\nThere is a disconnect between the tools and research in this area. Tools used at the industry level have typically not been peer-reviewed and, when evaluated by researchers, are insufficient for comprehensive TAI evaluation versus the state of the art in the literature.\nThe findings of this paper have significant implications for Al policy. The research underscores the necessity for standardised evaluation frameworks to assess the trustworthiness of Al systems. The current EU approach relies primarily on self-assessment and does not include methods or evaluation criteria for TAI evaluation, which the literature shows a clear need for. TAI standards developed by policymakers must be applied across use-case-specific Al applications to ensure ethical and fair practices. To facilitate comprehensive TAI evaluations for Al systems, governance frameworks in the literature propose third-party certification and standard methods and evaluation criteria, including metrics agreed upon by regulatory bodies based on their industry-specific needs and use cases. There is a disconnect between what policymakers, Al experts, and a standard non-expert user consider fair, along with differences based on culture, showing a need for more input from various laypeople to decide acceptable TAI evaluation approaches for individual use cases."}]}