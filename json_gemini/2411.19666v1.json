{"title": "Multimodal Whole Slide Foundation Model for Pathology", "authors": ["Tong Ding", "Sophia J. Wagner", "Andrew H. Song", "Richard J. Chen", "Ming Y. Lu", "Andrew Zhang", "Anurag J. Vaidya", "Guillaume Jaume", "Muhammad Shaban", "Ahrong Kim", "Drew F.K. Williamson", "Bowen Chen", "Cristina Almagro-Perez", "Paul Doucet", "Sharifa Sahai", "Chengkuan Chen", "Daisuke Komura", "Akihiro Kawabe", "Shumpei Ishikawa", "Georg Gerber", "Tingying Peng", "Long Phi Le", "Faisal Mahmood"], "abstract": "The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL). However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose TITAN, a multimodal whole slide foundation model pretrained using 335,645 WSIs via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any finetuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that TITAN outperforms both ROI and slide foundation models across machine learning settings such as linear probing, few-shot and zero-shot classification, rare cancer retrieval and cross-modal retrieval, and pathology report generation. The model is publicly accessible at https://github.com/mahmoodlab/TITAN", "sections": [{"title": "Introduction", "content": "Foundation models are transforming computational pathology by accelerating the development of AI tools for diagnosis, prognosis, and biomarker prediction from digitized tissue sections\u00b9. Developed using self-supervised learning (SSL) on millions of histology image patches (or regions of interests), these models capture morphological patterns in histology patch embeddings, such as tissue organization and cellular structure2\u201317. These representations serve as a \u201cfoundation\u201d for models that predict clinical endpoints from whole-slide images (WSIs), such as diagnosis or biomarker status18\u201338. However, translating the capabilities of current patch-based foundation models to address patient- and slide-level clinical challenges still remains complex due to the immense scale of gigapixel WSIs, compounded by the small size of patient cohorts in real-world evidence39,40, posing challenges for disease-specific AI model development41. As an example, in rare diseases with limited training data42\u201344, developing effective predictive models is difficult since the slide encoder-which generates slide-level predictions from patch embeddings-still needs to be trained from scratch10,45. Similarly, given a diagnostically challenging patient slide, retrieving a similar slide via slide search5,46\u201353 or pathology reports through cross-modal report search10,54\u201356 typically requires specialized algorithms to bridge the gap between patch and slide embeddings, introducing hurdles towards clinical adoption.\nTo overcome these limitations, new types of foundation models have recently been proposed for encoding entire WSIs into slide-level general-purpose feature representations57\u201366. Instead of training an additional model on top of patch embeddings from scratch34,45,67\u201371, these whole slide representation models can be pretrained to distill pathology-specific knowledge from large WSI collections, simplifying clinical endpoint prediction. The outstanding challenge then becomes developing whole slide foundation models that faithfully encode the tissue microenvironment based on a set of patch embeddings while also handling arbitrarily large WSIs. Although relatively underexplored, slide-level self-supervision can be performed with vision-only pretraining, either through masked image reconstruction58 or intra-slide contrastive learning59, 60, 72, or in a multimodal fashion involving pathology reports, bulk transcriptomics, or immunohistochemistry61-64, 66, 73. Furthermore, long-range context modeling can either be neglected, essentially treating a WSI as a bag of independent features59,62\u201364,74, or explicitly modeled using Transformers57,58,60,61. With efforts to learn general-purpose slide representations intensifying, we believe that adapting successful patch-level recipes to the entire WSI would lead to powerful general-purpose slide representations.\nDespite their widespread application potential, previous works on pretraining slide foundation models have several shortcomings. First, these models are predominantly pretrained using vision-only modeling57, 59, 60, which neglects not only rich supervisory signals found in pathology reports, but also precludes multimodal capabilities such as zero-shot visual-language understanding and cross-model retrieval \u2013 which is a fundamental hallmark in foundation models 75,76. Second, whereas current patch foundation models are trained with millions"}, {"title": "Results", "content": "Here, we introduce Transformer-based pathology Image and Text Alignment Network (TITAN), a multimodal whole-slide vision-language model designed for general-purpose slide representation learning in histopathology. Building on the success of knowledge distillation and masked-image modeling77,78 for patch encoder pretraining21,22, TITAN introduces a novel paradigm that leverages millions of high-resolution regions-of-interests (ROIs at 8, 192 \u00d7 8, 192 pixels) for large-scale, resolution-agnostic pretraining and scalable WSI encoding. Trained using 336K WSIs across 20 organ types, vision-only TITAN produces general-purpose slide representations that can readily be applied to slide-level tasks such as cancer subtyping, biomarker prediction, outcome prognosis, and slide retrieval tasks, outperforming supervised baselines and existing multimodal slide foundation models. To augment TITAN with language capabilities, we further finetune by contrasting with 423K synthetic fine-grained ROI-captions generated with PathChat\u201d, a multimodal generative AI copilot for pathology, and with 183K pathology reports at slide level. By leveraging free-text morphological descriptions, TITAN gains the ability to generate pathology reports, perform zero-shot classification, and enable cross-modal retrieval between histology slides and clinical reports. Pretraining TITAN on an extensive repository of multimodal pathology data unlocks new levels of performance compared to existing slide foundation models, particularly in low data regimes, language-guided zero-shot classification, and rare cancer retrieval. Additionally, we show the utility of pretraining with synthetic fine-grained morphological descriptions for the first time, hinting at the scaling potential of TITAN pretraining with synthetic data80\u201382. Through comprehensive evaluation across a large range of clinical tasks, including the first application to rare cancer retrieval across 43 rare cancer types, we demonstrate the efficacy of our vision-language pretraining approach, showcasing the general-purpose capability of our slide foundation model.\nScaling self-supervised learning from histology patches to whole slide images\nTITAN is a Vision Transformer (ViT)83 that creates a general-purpose slide representation readily deployable in diverse clinical settings. It is pretrained on an internal dataset (termed Mass-340K) consisting of 335,645 WSIs and 182,862 medical reports (Figure 1A). To ensure the diversity of the pretraining dataset, which has"}, {"title": "Algorithmic design considerations for TITAN", "content": "For the positional encoding, we pretrain TITANy with absolute positional encoding, following the ViT design83, rotary positional encoding102 extended to 2D103 (Rotary PE), and without positional encoding (No PE). Our results show that ALiBi outperforms the other encoding schemes on all four tasks, with an average improvement of + 2.01% over absolute positional encoding, the second-best performing method (Figure 2D). This indicates that diagnostic performance can be enhanced with a suitable choice of positional encoding by contextualizing the patch features, with ALiBi helping TITANy extrapolate effectively to the entire slide, where the context length is over ten times longer. For the number of layers and consequently the number of parameters for TITANy, we observe that 6 layers (43M parameters) on average provide the sweet spot between smaller (4 layers, 29M parameters) and larger models (12 layers, 86M parameters), by outperforming them by 1.72% and 1.31%, respectively (Extended Data Table 70).\nFor the pretraining strategy ablation, we compare the performance between TITAN with the full pretraining and TITANL, which only performs vision-language alignment without vision pretraining. We introduce an additional baseline of multiheaded attention-based MIL66,67 with the vision-language pretraining (ABMIL-L), to further understand whether a different slide encoder architecture with the same pretraining recipe can be effective. We observe that TITAN outperforms TITAN\u2081 by 2.35% and ABMIL-L by 3.62%. This indicates that the vision pretraining (Stage 1) provides better initialization weights than the random weights for vision-language alignment, leading to improved downstream performance, also observed in patch encoder pretraining 10. This also suggests that the better performance of TITAN over PRISM, which is pretrained only with the vision-language alignment, could be due to the inclusion of the vision pretraining step. Moreover, the worse performance of ABMIL architecture than ViT with the same pretraining recipe justifies the choice of ViT as the architecture for TITAN."}, {"title": "Comparison with different learning paradigms for slide encoding", "content": "To further assess the quality of the slide embeddings produced by TITANy, we evaluate different learning paradigms by comparing the linear probe performance of each slide encoder against other MIL models comprised of mean pooling, i.e., averaging the patch embeddings, attention-based MIL (ABMIL)67, and task-specific finetuning of the slide encoder from random or the pretrained weights. For the mean pooling and ABMIL baselines, we use respective patch encoders for each slide encoder framework. This analysis allows us to gauge whether the pretrained slide encoders have learned meaningful slide representations and consequently outperform the simple yet powerful unsupervised (mean pooling) and supervised (ABMIL) baselines, neither of which involve large-scale pretraining on thousands of WSIs.\nWe observe several trends with TITAN (Figure 2E, Extended Data Figure 4, Extended Data Tables 56 to 59). First, ABMIL outperforms mean pooling, as expected, since ABMIL is supervised and equivalent to weighted averaging of the patch features, which would by default include the simple averaging solution of mean pooling. Next, the linear probe outperforms ABMIL, which demonstrates that TITAN and TITANy,"}, {"title": "Language-aligned TITAN enables cross-modal capabilities", "content": "We further assess the language capabilities of TITAN by aligning the slide representations of TITANy to language-based morphological descriptions. Specifically, we assess the cross-modal zero-shot classification55, 56, 104 and report-generation capabilities of TITAN and study the effect of Stage 2 pretraining for caption alignment with fine-grained morphological descriptions and Stage 3 pretraining with coarse clinical reports describing the relevant microscopic findings.\nTo evaluate the quality of vision-language alignment, we first perform cross-modal zero-shot experimentation on 13 subtyping tasks of varying difficulties (Figure 3A). In cross-modal zero-shot evaluation, the diagnostic labels expressed as text prompts are encoded with the text encoder. The diagnostic prediction of the query slide is then decided by the closest label embedding to the slide embedding encoded with TITAN. The cross-modal zero-shot experiment evaluates how the embedding space with the visual pretraining can be further aligned with the language modality. We compare the zero-shot performance against PRISM, also equipped with cross-modal capabilities. Gigapath is not included as the language-aligned extension has not been publicly released. We observe that TITAN performs best across these tasks, outperforming PRISM by a large margin on multi-class classification tasks (balanced accuracy +56.52%) and binary subtyping tasks (AUROC +13.8%), for both cancer subtyping tasks and non-cancerous tasks (Figure 3B, Extended Data Tables 80 to 92). The performance gap between TITAN and PRISM is the widest on the 30-class EBRAINS subtyping task, where"}, {"title": "TITAN improves region and slide-level diagnostic capabilities", "content": "We begin by evaluating TITAN, TITANy, and existing slide encoders on a large set of diverse slide-level tasks, including morphological subtyping and molecular classification. Following the standard practice in self-supervised learning78,88, we employ linear probing by fitting a linear model for classification (logistic regression) on frozen slide embeddings. Specifically, we use the linear weights estimated with the 12-regularization parameter tuned on a validation set for evaluating the test performance. For tasks with multiple cohorts available, we perform cross-validation on one cohort, e.g., from TCGA 89,90, and use the remaining cohorts, e.g., from CPTAC91,92 or DHMC93,94, as an external test cohort. As baselines, we evaluate the recent vision-language slide foundation models with model weights available, namely PRISM62, GigaPath63,"}, {"title": "Few-shot learning for low data regime", "content": "We also evaluate the data-constrained setting of few-shot learning where only a few examples for each category are provided within the linear probe setting. In the few-shot setting, TITANy remains superior across all tasks and number of shots in balanced accuracy when assessed with linear probe (Figure 2F). To mitigate sampling bias, we aggregate the results over 50 different runs, with random samples used for training, while fixing the test set. We observe that TITAN is the best-performing model across different tasks, demonstrating the strong generalizability of TITAN. TITANy is the second-best performing model, which supports our results that vision-language alignment benefits the downstream task performance. Specifically, TITAN and TITANy exhibit especially high performance in one-shot learning, which is on par with other slide encoders trained on more shots (Extended Data Tables 72 to 75). Specifically, TITAN and TITANy outperform CHIEF based on 16 shots on TCGA tasks by 22.4% and 13.5% (TCGA-UT-8K) and 18.7% and 6.8% (TCGA-OT) when compared with the median value of 50 runs, respectively, even though CHIEF has been pretrained on TCGA slides.\nInterestingly, both TITAN and TITANy also outperform ABMIL with the same patch encoder across all settings. While the performance gap with ABMIL shrinks for a higher shot regime as expected, we observe that the gap is indeed wider in the lower shot regime. The largest gap for 1-shot is observed in the OT108 task, where TITAN outperforms ABMIL by 56.7%. These observations underscore the superior data efficiency of a pretrained slide encoder and suggest that TITANy can excel in rare cancer settings with a limited number of samples, such as OT108 in our benchmark, where heavily parameterized supervised approaches are inherently restrained. This demonstrates the advantage of TITAN over other slide encoders such as GigaPath and CHIEF, the intended usages of which are in supervised settings with task-specfic finetuning, rather than off-the-shelf usage with the frozen slide embeddings. The same trend is observed even when evaluated with prototyping, where K samples (shots) from each class are averaged to construct the prototype (Extended Data Tables 76 to 79). More details on the experiments can be found in Online Methods section Few-shot classification."}, {"title": "TITAN enables rare cancer retrieval and cross-modal retrieval", "content": "Considering cases with similar morphological features and diagnoses is essential for pathologists to make informed decisions, in particular when dealing with complex or rare cases5,17,47,48, 50, 51, 53, 109, 110. Retrieving similar histology slides or pathology reports facilitates identifying relevant cases from large archival databases, and has become an essential clinical decision support function in digital pathology workflows. This is especially beneficial for rare cancers that affect fewer than 15 individuals per 100,000 annually42-44, for which pathologists can identify non-specific malignancies based on WSIs with similar morphologies and their corresponding pathology reports. Slide foundation models readily provide WSI representations for vector database indexing, significantly simplifying the task of histology slide retrieval compared to patch foundation models, which provide more than 104 representations per WSI and consequently renders slide-level retrieval non-trivial.\nGiven a query slide and labeled set of support slides (indexed into a vector database by a slide foundation model), histology slide search is evaluated by assessing the accuracy performance in retrieving similarly labeled slides from the support set. This setting is non-parametric and solely relies on how the slide representations are clustered along different diagnostic labels. Specifically, we test whether the K-closest neighbors of a query slide in the embedding space\u2014determined using cosine similarity with K = {1,3,5}-include slides sharing the same diagnostic label as the query slide. Performance is assessed using Accuracy@K, which measures whether at least one of the K neighboring slides has the same diagnostic label as the query. We also provide MVAcc@K which requires the majority vote of top-K neighboring slides is of the same diagnostic label as the query and is therefore more stringent criteria than Accuracy@K. For the rare cancer retrieval task, we create a large database of 186 cancer types with 19,626 WSIs, Rare-Cancer, by combining the rare cancer set of 43 cancer types (3,039 WSIs) with the common cancer set of 143 more common cancer types (16,587 WSIs) from TCGA, EBRAINS, and MGB internal data (Figure 4A, Extended Data Table 6). \u03a4\u03bf assess the performance, we create a query set as the subset of the rare cancer set, ensuring all 43 rare cancer types are represented. The support set, from which similar slides are retrieved, is constructed by incorporating the remaining WSIs of the rare cancer set into the common cancer set, ensuring all 186 cancer types are represented. This design emulates the real-world setting of clinicians interacting with an extensive cancer database encompassing a diverse mix of rare and common cancer types. This procedure is repeated five times, with a different query set each time. We additionally create a public version with 127 cancer types and 14,062 WSIs, Rare-Cancer-Public, using the data from TCGA and EBRAINS resulting in 29 rare cancer types (1,982 WSIs) and a lower diversity in the set of common cancers with 98 types (12,080 WSIs). The same evaluation procedure as for Rare-Cancer is repeated (Extended Data Table 8).\nWe observe that TITAN and TITANy outperform other slide encoders with +14.8% and +12.3% in Accuracy@K and +18.1% and +13.4% in MVAcc@K to the next best model PRISM (Extended Data Table 129) on average. The trends in performance are preserved on the public version of the rare cancer task with slightly higher performance levels as the task is easier with a support set containing fewer cancer types (Extended Data Table 130). An example of rare cancer retrieval is demonstrated in Figure 4B, where the closest slide to the paraganglioma (PGNG) query is also of PGNG with a high similarity of 0.794 and less similar slides are of different cancer type (Haemangioma from brain, similarity of 0.341). One of the retrieved slides is Pheochromocytoma (PHC) with a high similarity of 0.651, agreeing with the clinical understanding that both are morphologically tightly connected as rare neuroendocrine tumors111. Additional examples of rare cancer retrieval can be found in Extended Data Figure 6, where the retrieved slides of high similarity are indeed from the same diagnostic label or organ. Even when further assessed with multi-class cancer subtyping tasks, from relatively simple AMR for renal allograft (C = 2) to challenging OT108 (C = 108), we observe that both TITAN and TITANy outperform other slide encoders (Figure 4C, Extended Data Tables 131 to 137).\nEncouraged by the unimodal retrieval performance, we further investigate the cross-modal retrieval performance of TITAN, as the slide and report embedding spaces are aligned from the pretraining steps. We perform the cross-modal experiments on TCGA-Slide-Reports, our proposed dataset for report generation with 10,108 slide-report pairs (Extended Data Table 7). For the report-to-slide (slide-to-report) retrieval task, we test whether any of the K-closest slides (reports) for the query report (slide) in the embedding space have the same diagnostic label as the query, the metric referred to as Recall@K with K = {1,3,5, 10}. observe that TITAN outperforms PRISM on both retrieval tasks across all K retrievals with +10.5% and +20.5% on average for report-to-slide and slide-to-report retrieval tasks, respectively (Figure 4D, Extended Data Tables 138 and 139). The largest gap to PRISM is observed for slide-to-report retrieval when only a single report was retrieved, where TITAN outperforms by 36.4%. The strong performance of TITAN even with a single report (0.75) hints at the clinical potential, where for a diagnostically challenging slide clinicians can benefit from sifting through retrieved past medical reports that describe identical diagnoses, and vice versa. More details on the experiments can be found in Online Methods section Slide retrieval and Cross-modal retrieval."}, {"title": "Discussion", "content": "We introduce a multimodal whole-slide foundation model for pathology, TITAN, that combines and elevates successful recipes of self-supervised learning (SSL) from the patch level to the slide level. Method-ologically, TITAN employs histology knowledge distillation in the feature space (vision-only) and contrastive learning by aligning regions of interest (ROIs) with synthetic captions and whole slide images (WSIs) with reports (vision-language). Pretrained on 336K WSIs, TITAN, a Vision Transformer (ViT) architecture equipped with ALiBi positional encoding for long-context extrapolation, produces powerful general-purpose slide rep-resentations for a large variety of downstream tasks even without task-specific finetuning. From cancer sub-typing to molecular classification, TITAN consistently outperforms other state-of-the-art slide encoders, such as PRISM62, GigaPath58, and CHIEF74. This superiority is maintained in data-constrained settings such as rare disease classification and histology slide retrieval, which underscores the representation quality of TITAN. Further aligning the vision-pretrained TITAN with 423K ROI-level captions generated by PathChat and 183K pathology reports equips the model with multimodal capabilites such as zero-shot diagnosis, slide-report re-trieval, and report generation. We observe that aligning the slide embedding with both the fine-grained (ROI captions) and coarse-level (pathology reports) morphological descriptions is crucial for handling the multiscale information inherent in tissue slides-an insight made possible for the first time through the use of generated ROI captions. Similar to the unimodal setting, TITAN outperforms PRISM, another language-equipped model, on all cross-modal tasks. To advance the field of slide-representation learning112, we curated and plan to release two challenging multi-class morphology classification tasks beyond patch-level from the publicly available repository TCGA: TCGA-UniformTumor-8K (TCGA-UT-8K) for 32-class tumor-ROI subtyping and TCGA-OncoTree (TCGA-OT) for 46-class WSI-level OncoTree code classification.\nDetailed ablation analyses reveal further insights into the properties of TITAN. We observe that Stage 1 unimodal pretraining of TITAN\u04af captures morphological concepts already with much less data than existing slide encoders, as demonstrated in the few-shot data efficiency experiments. In particular, TITANy consistently outperforms its mean pooling and task-specific attention-based pooling baselines that utilize the same patch encoder as TITANy, proving that unimodal pretraining effectively captures the context of patch features in contrast to existing unimodal slide encoders. Next, in addition to unlocking language-related capabilities, we observe that the vision-language alignment further enhances the representation quality of our vision-only model. In particular, TITAN improves over TITANy on average for slide-level tasks with the strongest im-provements in evaluation settings that solely rely on the structure of the slide embedding space without any parameter tuning, such as prototyping or k-nearest neighbor settings. A further sign of the improved repre-sentations is that TITAN outperforms all other baselines, including TITANy, on slide retrieval and few-shot tasks. While slide embeddings from pretrained TITAN are already promising, especially in the low-data regime, task-specific fine-tuning of the pretrained model can further enhance the downstream performance for tasks with a large enough patient cohort, pointing to the flexibility of TITAN when applied to diverse clinical and data settings.\nProviding multimodal slide embedding off-the-shelf presents immediate clinical potential to assist clin-icians in their routine diagnostic workflows76. Presented with challenging patient tissue slides to diagnose, pathologists and oncologists can hugely benefit from being able to retrieve and analyze diagnostically similar slides or clinical reports, likely leading to a reduction in patient misdiagnosis and interobserver variability. As shown in the extensive set of experiments, TITAN can accurately retrieve similar diagnostic slides and reports for challenging scenarios from a large number of cancer types (> 100), as well as rare cancer types44 where the corresponding slides have scarce representation in the database. That all of these could be performed off-the-shelf with pretrained TITAN without a dedicated algorithm for each task underscores both the generalizability of TITAN slide embeddings, as well as how slide-level tasks can become simpler with the advent of pretrained slide encoders.\nDespite the encouraging performance of TITAN, our framework has a few shortcomings. First, Mass-"}, {"title": "Online Methods", "content": "and optimization of self-supervised learning recipes, leading to slide representations with restricted generalization capability58,62,73,74. Even with multimodal techniques such as vision-language pretraining that augment the pretraining dataset with pathology reports, current slide foundation models still require end-to-end training or finetuning and lack the capability of learning transferable slide representations for challenging clinical scenarios58,73,74. Finally, the current models are nascent in transforming pathology AI model development due to their limited evaluations in diagnostically relevant settings such as few-shot learning or slide retrieval. To equip our model with language capabilities, we implement two additional multi-resolution pretraining strategies (Stages 2 and 3) using a subset of WSIs in Mass-340K (Figure 1D). This is based on the observation that language descriptions exist at multiple morphological scales, from fine-grained descriptions in pathologist annotations or textbooks at the patch- or region-level (Stage 2) to high-level descriptions in pathology reports at the slide-level (Stage 3). For both stages, we use contrastive captioners (CoCa)86 as the pretraining strategy that aligns ROI and slide representations with the corresponding captions and reports, while generating accurate descriptions at ROI-level or reports at slide-level, respectively. The slide encoder (weights initialized with TITANy), the text encoder, and the multimodal decoder are all finetuned as part of the pretraining. In Stage 2, we pretrain TITANy with 423,122 pairs of 8K\u00d78K ROIs and synthetic captions generated by the vision-language copilot PathChat79. In Stage 3, we further pretrain the model with 182,862 pairs of WSIs and corresponding pathology reports, resulting in our final model TITAN. To diversify the captions and reports with data augmentation, we rewrite the text with a locally deployed large language model (LLM)87 and select ran-domly between several versions for vision-language alignment. A detailed description of the vision-language pretraining dataset and training strategy can be found in Online Methods sections Large-scale pretraining datasets and Vision-language continual pretraining, respectively.\nPretraining dataset\nFor large-scale visual pretraining, we curated Mass-340K, a diverse dataset consisting of 335,645 WSIs across 20 organs, with 90% hematoxylin and eosin (H&E) stained slides and 10% immunohistochemistry (IHC) slides, sourced from the combination of in-house histology slides and the GTEx consortium 114. To explore the effects of data scale at the pretraining stage, we formed three additional partitions of Mass-340K, containing 12.5%, 25%, and 50% of the original dataset. These partitions were sampled to maintain the ratio of different data sources and preserve organ distribution.\nSynthetic caption generation using PathChat\nFor the initial stage of vision-language alignment (Stage 2 of TITAN), we used synthetic captions generated by PathChat, a state-of-the-art multimodal LLM designed for pathology79. To go beyond the typically brief clinical reports focused on the final diagnosis, we prompted PathChat to generate detailed morphological descriptions of ROIs, providing important training data for models to capture complex pathological features. Using PathChat, we generated synthetic captions for 423,122 diverse 8,192\u00d78,192 ROIs sampled from Mass-340K. Since PathChat cannot process inputs of size 8,192\u00d78,192 pixels directly, we divide each ROI into 64 1,024\u00d71,024 patches. To retain the most representative morphological features, we applied K-means clustering with K = 16 to the 64 patches and then randomly sampled one patch from each cluster. The resulting 16 morphologically-representative 1,024\u00d71,024 patches were subsequently fed to PathChat. To further enhance the diversity of these captions, we utilized Qwen2-7B-Instruct87 to rewrite the generated captions, ensuring varied language structures and expressions. Detailed prompts for both PathChat and Qwen2, along with examples of generated and diversified captions, are provided in Extended Table 124-125.\nCuration of slide-report dataset\nFor the second stage of vision-language alignment (Stage 3 of TITAN), we curated a dataset of 182,862 slide-report pairs from a combination of in-house clinical reports and pathology notes from the GTEx consortium114. However, clinical reports are often noisy and are typically organized at the patient level, hence contain information on multiple slides from the same patient, complicating the slide-report alignment. To address this, we utilized a locally served Qwen2-7B-Instruct87 model to extract slide-specific descriptions and remove sensitive information unrelated to pathological diagnosis, such as gross descriptions, hospital and doctor names, and patient clinical history. Additionally, we applied the same rewriting strategy used for synthetic captions to diversify the report text. Example prompts used for report cleaning and rewriting can be found in Extended Data Table 126-128.\nUnimodal visual pretraining\nPreprocessing\nSimilar to the previous studies 9,10,45, WSIs were preprocessed by tissue segmentation, tiling, and feature extraction using a pretrained patch encoder. We used the CLAM toolbox45 for tissue segmentation and tiling. Tissues were segmented by binary thresholding of the saturation channel in HSV color space at a low reso-lution. Following this, we applied median blurring, morphological closing, and filtering of contours below a minimum area to smooth tissue contours and eliminate artifacts. Non-overlapping 512\u00d7512 pixel patches were then extracted from the segmented tissue regions of each WSI at 20\u00d7 magnification. For feature extraction, we used CONCHv1.5, an extended version of CONCH10, which was trained with 1.26 million image-caption pairs using the CoCa training objective for 20 epochs. The choice of CONCHv1.5 for feature extraction was due to the fact the model was pretrained on histology regions with diverse stains and tissue types, including FFPE, frozen tissue, and immunohistochemistry, thereby yielding region features that are robust against diverse tissue processing protocols. Refer to Extended Data Table 100 for detailed hyperparameters of the patch encoder.\nTo enhance the effectiveness of the ROI sampling strategy during Stage 1 training of TITANy, an additional preprocessing step was performed to group the segmented tissue contours based on their spatial proxim-ity within the slide. This addresses the challenging cases where multiple tissue regions are interspersed with background areas, particularly for biopsy samples where tissue fragments are often widely dispersed and for samples with multiple slices placed on the same slide. Specifically, we grouped tissue contours into clusters based on their coordinates, resulting in tissue groups that contain densely packed tissue regions with mini-mal background regions between them. Furthermore, tissue groups that contained fewer than 16 patches were filtered out. This grouping operation produced a total of 345,782 tissue groups from Mass-340K.\nPretraining protocol\nFor training TITANy on Mass-340K, we use iBOT, a state-of-the-art self-supervised learning method based on the combination of student-teacher knowledge distillation and masked image modeling77. As iBOT is applied in the patch embedding space, instead of the typical use case of the raw image space, we adapt the pretraining recipes as follows.\nView generation. During training, we create region crops randomly sampled from the tissue groups, each of which corresponds to a feature grid of size 16\u00d716, corresponding to a field of view of 8,192\u00d78,192 at 20\u00d7 magnification (Figure 1B). The random sampling of region crops, instead of precomputing fixed regions, increases the diversity of the training set and effectively acts as an additional data augmentation, as the model encounters different parts of the same WSI at each training epoch. A region crop contains 256 features, which is equivalent in length to training on images of 256\u00d7256 pixels with a token size of 16\u00d716 in the typical natural image setting. From this region crop, two global views (14\u00d714 crops) and ten local views (6\u00d76 crops) are generated by cropping within the region crop without scaling or interpolation and fed to iBOT training.\nTo achieve realistic augmentations in the embedding space, previous methods have employed offline image augmentations in the pixel space34,59 by extracting multiple patch features from different views of a given patch. While effective, this approach limits the number of additional views and becomes computationally infeasible for large training datasets. Additionally, choosing color space augmentations adapted to the use of histopathology that go beyond standard color transformations introduces additional computational overhead. A few recent approaches addressed the difficulty with training generative networks on the feature space to transform the features115,116, but also introduced additional computational cost for training. Instead, we apply frozen feature augmentations, which have been shown to work well for a few-shot classification task in the feature space of pretrained Vision Transformers84.\nPositional encoding. Traditional multiple instance learning methods consider the patches to be permutation-invariant within the slide. Despite the promising results, this approach ignores the tissue context which can be essential for capturing the interaction in the tumor micro-environments and can thus affect the model's performance117. In this context, for TITAN, we employ positional encodings in the patch embedding space to break permutation invariance and encode tissue context. Furthermore, TITAN adopts the strategy of Train short, test long to ease the computational burden, which also requires positional information via positional encodings. Trained at the region crops (ROIs) of 8,192\u00d78,192 pixels (Train short), we directly apply TITAN on the whole slide during inference (Test long). We used Attention with Linear Biases (ALiBi), a method originally proposed for 1D sequence in large language models (LLMs)85. Absolute positional encoding, another popular alternative that works well for images at training sizes, was shown to have weak extrapolation abilities 85. Unlike other positional encodings applied to the input features, ALiBi adds a bias to the query-key dot product during the computation of attention scores. ALiBi effectively penalizes the attention score for tokens which are further apart from each other. Formally"}]}