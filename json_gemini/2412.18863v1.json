{"title": "Whose Morality Do They Speak? Unraveling Cultural Bias in Multilingual Language Models", "authors": ["Meltem Aksoy"], "abstract": "Large language models (LLMs) have become integral tools in diverse domains, yet their moral reasoning capabilities across cultural and linguistic contexts remain underexplored. This study investigates whether multilingual LLMs, such as GPT-3.5-Turbo, GPT-40-mini, Llama 3.1, and MistralNeMo, reflect culturally specific moral values or impose dominant moral norms, particularly those rooted in English. Using the updated Moral Foundations Questionnaire (MFQ-2) in eight languages, Arabic, Farsi, English, Spanish, Japanese, Chinese, French, and Russian, the study analyzes the models' adherence to six core moral foundations: care, equality, proportionality, loyalty, authority, and purity. The results reveal significant cultural and linguistic variability, challenging the assumption of universal moral consistency in LLMs. Although some models demonstrate adaptability to diverse contexts, others exhibit biases influenced by the composition of the training data. These findings underscore the need for culturally inclusive model development to improve fairness and trust in multilingual AI systems.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have made remarkable progress in recent years. They are widely used in many fields, especially in education, finance, human resources, e-commerce, and healthcare (Hou et al., 2023). Their ability to understand and generate human-like text has advanced applications, such as virtual assistants, content creation, question-answering, summarization, and translation. Despite these technological advances, ethical and societal concerns, including biased behavior and misuse, require critical attention.\nTrained on large and diverse datasets, LLMs not only capture linguistic structures, but also absorb cultural, social, and moral biases embedded in the data (Acerbi and Stubbersfield, 2023). Due to their complexity and opacity, understanding how these models internalize and propagate such biases remains a critical issue. As LLMs increasingly influence decision making and human interaction, it is important to examine how they reflect moral judgments across languages and cultural contexts.\nLanguage, as a reflection of cultural identity, shapes norms, values, and moral reasoning (Benkler et al., 2023). Research suggests that LLMs reflect the cultural context of the languages in which they are trained and can reproduce the moral norms inherent to these contexts (Chen and Bond, 2010; Gallegos et al., 2024). However, the uneven distribution of training data across languages often favors English, raising concerns about whether LLMs prioritize English-centric moral norms, potentially at the expense of other cultural perspectives. This imbalance underscores the need to investigate whether multilingual LLMs equitably represent diverse cultural and moral values.\nIn order to analyze the moral judgments reflected in LLMs, this study draws upon frameworks from moral psychology, particularly Moral Foundations Theory (MFT) (Haidt and Joseph, 2004). MFT explains moral similarities and differences across cultures through six foundations: care/harm, fairness/cheating, loyalty/betrayal, authority/subversion, purity/degradation, and liberty/oppression. Moral Foundations Questionnaire (MFQ-1) (Graham et al., 2009, 2011), has been widely used to measure these moral foundations between cultures. However, MFQ-1 has been criticized for its cross-cultural applicability and potential bias toward Western, educated, industrialized, rich and democratic (WEIRD) populations. Critics argue that the MFQ-1 may not fully capture the complexities of moral reasoning in diverse societies, particularly in non-Western contexts. In addition, the fairness foundation in the MFQ-1 has been considered too simplistic to capture the nuances of distributive justice beliefs across cultures."}, {"title": "Related Works", "content": "In this section, an overview of the background and relevant research for this study is provided."}, {"title": "Large language models", "content": "LLMs are advanced artificial intelligence systems that use deep learning techniques to process and generate human-like text. Transformer architectures with attention mechanisms (Vaswani, 2017), enable efficient parallel processing of input sequences, facilitating training on vast amounts of textual data. LLMs consist of multilayered neural networks that capture complex linguistic patterns and structures by learning from extensive corpora containing billions of words across diverse topics. This comprehensive pre-training allows them to understand nuanced language features, including idiomatic expressions and domain-specific terminology. Techniques such as masked language modeling and autoregressive generation the model predicts missing or next tokens in a sequence-enable LLMs to perform tasks like text generation, translation, and question answering.\nLLMs can be categorized into monolingual and multilingual models based on the languages they are trained on. Monolingual models are trained exclusively on text from a single language, dedicating their entire capacity to capturing the intricacies of that language. Examples include BERT (Devlin et al., 2019) for English and CamemBERT (Martin"}, {"title": "Moral foundations theory", "content": "Morality is an abstract concept used to define behaviors and beliefs that individuals perceive as right and moral, or wrong and immoral (Meier et al., 2007). It is not only a central focus in psychology, but also a topic of interest in philosophy, anthropology, and other scientific disciplines.\nMoral Foundations Theory (Haidt and Joseph, 2004) was developed to address questions about the origins of morality, the similarities and differences in moral judgments between cultures, and whether morality is a single construct or a multifaceted system (Haidt and Graham, 2007; Graham et al., 2013). The theory extends the scope of moral psychology beyond fairness and harm, criticizing earlier approaches for their limited perspective. Haidt and Graham (2007) argue that even actions that do not involve injustice or harm can be moral violations if they violate social contracts (Haidt et al., 1993). This broader approach highlights that morality is not exclusive to western culture, but spans various cultural concerns.\nMFT, rooted in evolutionary psychology and anthropology, posits that humans have evolved a set of innate moral foundations that manifest differently across individuals and cultures. MFT introduced five moral dimensions, each reflecting intuitive moral responses shaped by factors such as culture, political ideology, and personality traits:\n1. Care/Harm: Focuses on protecting vulnerable people and preventing harm, rooted in evolutionary mechanisms related to attachment and avoidance of pain.\n2. Fairness/Cheating: Centers on promoting fairness and addressing injustice, originating from social systems based on cooperation and the need to deter cheaters.\n3. Loyalty/Betrayal: Emphasizes loyalty to one's group and avoiding betrayal, fostering group cohesion, and aligning group success with individual well-being.\n4. Authority/Subversion: Relates to respecting hierarchical structures and maintaining social order through obedience and deference to authority.\n5. Purity/Degradation: Associated with evolutionary defenses against pathogens, this foundation also incorporates religious and cultural values, focusing on preserving sanctity and avoiding contamination."}, {"title": "Moral foundation questionnaire-1", "content": "The MFQ-1, developed by Graham et al. (2009, 2011), evaluates how individuals evaluate the five original moral foundations in their reasoning. It is divided into two sections and consists of 30 items rated on a six-point Likert scale. In the first section, participants rate the importance of various moral judgments (e.g., whether someone is experiencing emotional pain) in their decision-making. In the second section, they express their agreement with specific moral statements (e.g., \"It is important to feel compassion for those who suffer\"). Each foundation is represented by six items, equally divided between the two sections, and scores are calculated by averaging responses. The MFQ-1 has been translated into several languages and used in various studies to examine moral foundations across cultures and contexts.\nIn recent years, the MFQ-1 has also been used to assess the moral reasoning of LLMs and how well they align with human moral frameworks. Abdulhai et al. (2023) focused on LLMs such as GPT-3 and PaLM, using MFQ-1 to assess whether these models exhibit consistent moral foundations in different contexts. The study compared the models with human responses from different cultural backgrounds. The authors mainly analyzed English-based models. Ji et al. (2024) used the MFQ-1 to evaluate language models in terms of their moral identity. This study used multiple datasets, including five and six foundation versions, to assess models in both Western and non-Western contexts. Although English remained the main focus, the study highlighted the potential for extending MFQ-1 applications to non-English cultures. H\u00e4mmerl et al. (2022) applied the MFQ-1 to analyze cross-linguistic behavior in five languages: Arabic, Czech, German, English, and Mandarin Chinese. Although the models revealed interesting insights into cross-cultural moral dimensions, limitations were found in their ability to capture cultural differences. Despite some success in aligning moral judgments across languages, problems arose with negation and longer sentences, particularly in non-English languages.\nIn general, MFQ-1 has been applied in various linguistic contexts, with a focus on English. These studies aimed to explore cultural and cross-lingual differences, but the findings suggest that language models still face difficulties in fully capturing nuanced cultural variations, especially in non-English settings."}, {"title": "Moral foundation questionnaire-2", "content": "Studies have shown that the five-foundation model of MFT faces significant challenges when applied across cultures. It often does not work consistently in different societies (Iurino and Saucier, 2020; Atari et al., 2020; Harper and Rhodes, 2021; Akhtar et al., 2023). Furthermore, the original questionnaire (MFQ-1) was criticized for focusing too much on WEIRD populations, which introduced cultural biases and limited its validity in more diverse settings. Furthermore, the Fairness foundation in MFQ-1 was considered too simplistic to capture the complexities of distributive justice beliefs across different cultures (Atari et al., 2023).\nTo address these issues, Atari et al. (2023)) introduced MFQ-2, an updated and refined version of MFQ-1. The main goal was to create a more culturally sensitive and psychometrically robust tool. One of the key changes in MFQ-2 was the division of the fairness foundation into two distinct components.\n1. Equality: The belief that everyone should have equal opportunities and resources.\n2. Proportionality: The idea that individuals should be rewarded based on their contributions or efforts.\nThis distinction was introduced to better capture the complexity of distributive justice, as Meindl et al. (2019) suggested that a single concept could not fully explain fairness.\nMFQ-2 also features a completely new set of items, specifically designed to address the limitations of the original version. The final version of MFQ-2 consists of 36 items, refined to capture a wider and more accurate range of moral concerns.\nThe development of MFQ-2 involved empirical studies in 25 different populations, to ensure that the new questionnaire is reliable and valid in various cultural contexts. One of the significant improvements of MFQ-2 is its demonstrated measurement invariance, which means that it provides consistent results across diverse populations, a crucial aspect of any tool used in cross-cultural research. In short, MFQ-2 is a more nuanced and culturally adaptable tool, specifically designed to improve accuracy and applicability in non-Western contexts."}, {"title": "Morality in LLMs", "content": "Research on the morality of LLMs has received considerable attention. The focus has been on how these models deal with moral dilemmas, reflect cultural biases, and adapt to different moral values in different societies. Early studies focused primarily on monolingual models. Schramowski et al."}, {"title": "Methodology", "content": "To evaluate the moral foundations of LLMs, four models were assessed in eight languages using the 36-item MFQ-2 questionnaire. Each item in MFQ-2 is rated on a 5-point Likert scale, ranging from 1 ('does not describe me at all') to 5 ('describes me extremely well'). The questionnaire captures six moral foundations (Care, Equality, Proportionality, Loyalty, Authority, and Purity), with the average score for each foundation calculated based on its corresponding items. The MFQ-2 is provided in Appendix B.\nEach MFQ-2 item was presented to the models as a prompt. To provide context and ensure clarity, an initial task description was included to guide the models and standardize their understanding of the task. The expected responses consisted of ratings that either indicated the relevance of the item to moral values or expressed the level of agreement with a given moral statement. These responses were classified according to their respective moral foundations, and for foundations comprising multiple related items, the scores were averaged to calculate an overall score for each moral foundation.\nOnce the average scores for each foundation were calculated, the moral profile of the models was analyzed by assessing the relative emphasis placed on different moral foundations. Higher scores for a particular foundation indicated a greater importance attributed to the corresponding moral values. To ensure the robustness of the findings, each questionnaire was repeated 100 times per language for each model.\nThe following sections provide details on the models and languages and outline the process of prompt creation."}, {"title": "Models", "content": "MFQ-2 was evaluated on four LLMs: GPT-3.5-Turbo, GPT-40-mini, MistralNeMo (12B-Instruct), and Llama 3.1 (8B-Instruct). These models were selected for their widespread use, reported effectiveness in multilingual settings, and accessibility (Holtermann et al., 2024). For GPT-3.5-Turbo and GPT-40-mini, the OpenAI Python API was utilized to systematically prompt the models and collect responses. The two open-source models, MistralNeMo and Llama 3.1, were deployed locally using frameworks such as PyTorch and Hugging Face, allowing for fine-tuning to support multilingual tasks. Additional details about the models are provided in Appendix A."}, {"title": "Languages", "content": "This paper aims to evaluate the moral biases of LLMs in a multilingual setting by prompting them in eight languages: Arabic, Farsi, Japanese, Chinese, English, French, Spanish, and Russian. These languages were selected to capture cultural diversity and explore their relationship with WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations. Western languages such as English, French, Spanish, and Russian are associated with WEIRD contexts, while Eastern languages, including Arabic, Farsi, Japanese, and Chinese, represent non-WEIRD settings. This distinction facilitates an assessment of whether the models' moral biases differ across cultural contexts. The selection aligns with Atari et al. (2023), who also focused on these languages in response to critiques of the emphasis of MFQ-1 on WEIRD populations. For consistency and reliability, the official translations of MFQ-2 provided by Atari et al. (2022) were utilized."}, {"title": "Prompting", "content": "For each LLM, either a fine-tuned chat model or, when unavailable, an instruct model was used. A system prompt was included to ensure that the models responded in the Likert scale format for each item in the MFQ-2: For each statement, indicate how well it describes you or your opinions. Select one of the following options: Does not describe me at all, Slightly describes me, Moderately describes me, Describes me fairly well, Describes me extremely well. To ensure that LLMs responded solely using the provided Likert scales without elaborating on their reasoning, specific rules were de-"}, {"title": "Results", "content": "The results section addresses each research question in detail, beginning with whether English moral norms are imposed across languages (RQ1), followed by cultural balance in moral reasoning (RQ2), and ending with the alignment of LLMs with human responses (RQ3)."}, {"title": "RQ1: Influence of English moral norms on multilingual LLMs", "content": "The descriptive statistics for all languages and LLMs, presented in Appendix D, show significant variability across the six moral foundations. This variability, influenced by the language of the questionnaire and the evaluated model (GPT-3.5-Turbo, GPT-40-mini, Llama 3.1, and MistralNeMo), highlights the role of linguistic and cultural differences in shaping the moral judgments of LLMs. These findings mirror the cultural diversity observed in human moral reasoning. Contrary to the assumption that English, the dominant language in training data, might impose its moral norms on all language models, the results indicate otherwise. In each model, English shows significant differences from other languages across several moral foundations. Additionally, the models produce varying scores for the same language, influenced by differences in training data and processes. This variability reflects patterns observed in human moral psychology, where cultural differences shape diverse moral reasoning.\nTo further investigate the influence of language and model on moral foundations, a two-way ANOVA was conducted for each moral foundation. As shown in Table 1, the ANOVA results provide statistical evidence that both language and model have a significant effect on moral foundation scores (p < 0.001), supporting previous observations. Moreover, the significant interaction effect (p < 0.001) between language and model highlights how moral foundation scores are shaped by the interplay between these factors. This finding challenges the idea that multilingual LLMs impose English moral norms universally across other languages. Instead, it suggests that the influence of English varies depending on the model. Some models may exhibit closer alignment with English moral reasoning, while others may diverge more substantially due to differences in their training processes or architectures.\nAs shown in Figure 1, Tukey's HSD post-hoc tests revealed that the differences involving English were not statistically significant for most moral foundations. This suggests that, while language overall has a significant influence, specific pairwise differences involving English are not consistently large or significant across the six moral foundations. For example, English exhibited relatively small differences from most other languages, except Spanish, where larger differences were observed in Proportionality, Loyalty, and Authority.\nFurthermore, the interaction effect demonstrates the adaptability of multilingual LLMs, as their moral reasoning reflects a balance between English norms and the unique characteristics of other languages. As shown in Figure 2, the differences between GPT-40-mini and MistralNeMo in Care, Proportionality, and Loyalty exemplify this interaction effect. Model-specific characteristics influence how strongly English norms are reflected in multilingual outputs, revealing the nuanced interplay between language and model-specific features. The results demonstrate that multilingual LLMs adapt their moral reasoning to reflect language-specific nuances rather than imposing English moral norms universally. While English shows some differences from other languages across several moral foundations, these differences are not consistently significant for all pairwise comparisons, as revealed by Tukey's HSD post-hoc tests. These findings emphasize the complex interaction of cultural, linguistic, and technical factors in shaping moral judgments within these models."}, {"title": "RQ2: Moral Judgments in LLMs: Comparisons of WEIRD and non-WEIRD language groups", "content": "The study analyzed moral foundation scores for WEIRD (English, French, Spanish, Russian) and non-WEIRD (Chinese, Japanese, Arabic, Farsi) language groups across multiple language models. t-tests were conducted to evaluate whether the models demonstrated consistent moral reasoning across cultural contexts or exhibited biases associated with specific language groups.\nTable 2 reveals statistically significant differences in moral foundation scores between WEIRD and non-WEIRD languages for nearly all models. Care, Loyalty, and Purity scores are consistently higher for WEIRD languages. In contrast, Equality scores are higher for non-WEIRD languages in GPT-40-mini and GPT-3.5-Turbo, though models like MistralNeMo and Llama 3.1 showed slightly higher scores for WEIRD languages. The proportionality scores are significantly higher for the WEIRD languages in most models, except for GPT-40-mini, where no significant differences are observed. Authority scores favored non-WEIRD languages in GPT-40-mini but are higher for WEIRD languages in other models.\nThese findings indicate significant differences in moral judgments between WEIRD and non-WEIRD language groups, suggesting that LLMs may carry cultural biases. This could be due to LLMs reflecting the moral reasoning inherent to the respective cultural context or imbalances in the representation of training data. WEIRD languages show more consistent results, likely due to better representation in the training data, while the variability in non-WEIRD language scores suggests underrepresentation or reliance on biased sources. The differences between models further underscore the influence of the training data and the de-"}, {"title": "RQ3: Alignment between LLMs and human moral judgements", "content": "To assess the alignment between LLMs and human moral judgments, 100 survey responses were randomly selected for each of six languages (English, French, Spanish, Russian, Arabic, and Japanese) from the Atari et al. (2023) dataset. Chinese and Farsi were excluded due to insufficient data availability.\nTable 3 presents the ANOVA results, examining the differences in moral foundation scores between human responses and those generated by various LLMs. These findings indicate that for most moral foundations, models such as GPT-40-mini and GPT-3.5 demonstrate a higher degree of alignment with human scores, whereas models like MistralNeMo and Llama 3.1 show relatively lower alignment in specific areas. The observed differences are statistically significant for many foundations, as indicated by the p-values.\nAlthough the statistical results provide an overview, further analysis of the specific differences across models and foundations offers deeper insights. Figure 3 illustrates the overall performance in all languages, highlighting that GPT models generally demonstrate the highest alignment with human responses. This may be attributed to the diverse and extensive datasets used during GPT training, which potentially capture a broader range of human values and norms. GPT-40-mini and GPT-3.5 show similar results overall, with GPT-3.5 excelling in \"Purity,\" \"Loyalty,\" and \"Proportionality,\" while GPT-40-mini outperforms in \"Authority\" and \"Equality\" in terms of proximity to human scores. However, the best performance on the \"Care\" foundation is observed with Llama 3.1, which demonstrates moderate alignment overall. The stronger performance of Llama 3.1 in \"Care\" suggests that specific training data or optimization strategies may enhance alignment with certain moral foundations over others. MistralNeMo, while showing a performance similar to Llama 3.1 in some areas, consistently underperforms, failing to closely reflect human moral values across all foundations. This underperformance could stem from limitations in training data diversity or the model's architectural constraints, underscoring the importance of both factors in achieving alignment.\nThese findings suggest that training approaches and dataset diversity significantly impact a model's ability to align with human norms, emphasizing the need for careful calibration to avoid deviations that could undermine trust and applicability in sensitive contexts."}, {"title": "Conclusion", "content": "This study provided a comprehensive evaluation of the morality of multilingual LLMs by employing the MFQ-2 in eight different languages, to explore their adaptability to cultural and linguistic nuances. First, the study examined whether multilingual LLMs impose dominant English-centric moral norms on other languages. The findings indicate that while English influences moral judgments, the models demonstrate considerable variation across languages, reflecting their adaptability to linguistic and cultural contexts. Notable variations in performance between models, such as GPT-40-mini and MistralNeMo, highlight the impact of training data and model design on moral reasoning. These results emphasize the complex interplay of cultural, linguistic, and technical factors in shaping the moral judgments of LLMs, underscoring the importance of context-aware and culturally inclusive model development.\nSecond, the study explored the balance of moral judgments between WEIRD and non-WEIRD language groups. The study highlights significant differences in moral foundation scores between WEIRD and non-WEIRD language groups, indicating the presence of cultural biases in LLMs. GPT-40-mini demonstrated relatively balanced performance across these groups, while smaller open-source models like MistralNeMo and Llama 3.1 showed a stronger favoritism towards WEIRD languages. These findings suggest that larger models with diverse training datasets better address cultural diversity, although discrepancies persist, particularly in underrepresented language groups. This underscores the need for more inclusive and balanced training data to enhance the fairness and cultural adaptability of LLMs.\nThird, the study assessed the alignment between the moral preferences of LLMs and human responses within their respective languages. The results showed that alignment was generally stronger for models with larger and more diverse training datasets, such as GPT-3.5 and GPT-40-mini, particularly in well-represented languages like En-glish. In contrast, smaller open-source models, such as Llama 3.1 and MistralNeMo, exhibited weaker alignment across languages, especially for"}, {"title": "Limitations", "content": "Although this study provides valuable information on how multilingual LLMs reflect morality across languages, several limitations must be considered. The validity and reliability of MFQ-2 is a fundamental assumption of this study. Although MFQ-2 is a promising tool for measuring moral foundations, it is relatively new and not yet widely used. In addition, such questionnaires may not fully capture the complexity of moral decision making in the real world. Future studies should consider supplementing questionnaire-based assessments with more interactive and dynamic methods, such as real-world moral dilemmas, simulations, or qualitative analyses.\nThere is also a fundamental difference between LLMs and human psychological assessment. LLMs lack personal experience, emotion, and awareness, and generate responses based solely on the patterns of learned data. Therefore, the conclusions drawn from these responses should be interpreted with caution. Future studies could investigate how the contextual memory of LLMs influences the results and rephrase the test items to ensure a more accurate reflection of the true interpretation.\nDue to financial constraints, the experiments were limited to GPT-40-mini, GPT-3.5-Turbo, Llama 3.1 and MistralNeMo. Future studies should aim to incorporate state-of-the-art models and conduct experiments with other multilingual LLMs to gain a more comprehensive understanding of LLM behavior in different moral contexts.\nThe same set of prompts was used across all models to maintain consistency; however, this approach may overlook architectural and operational differences between models. The tailoring of the instructions to the specific design of each model could improve accuracy in future studies. Although the prompts were designed to encourage the models to approximate human experience, LLMs are still unable to fully replicate the complexity of human moral reasoning. Future research could focus on refining anthropological prompting techniques and investigating language-specific variations to better understand potential biases in the dataset.\nThis study focused on eight languages, which, while diverse, still represent a limited cultural spectrum. Future studies should aim to include additional languages, particularly from underrepresented regions, to broaden the cultural scope of the study and provide a more comprehensive analysis of LLM moral reasoning."}]}