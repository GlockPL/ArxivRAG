{"title": "This&That: Language-Gesture Controlled Video Generation for Robot Planning", "authors": ["Boyang Wang", "Nikhil Sridhar", "Chao Feng", "Mark Van der Merwe", "Adam Fishman", "Nima Fazeli", "Jeong Joon Park"], "abstract": "We propose a robot learning method for communicating, planning, and executing a wide range of tasks, dubbed This&That. We achieve robot planning for general tasks by leveraging the power of video generative models trained on internet-scale data containing rich physical and semantic context. In this work, we tackle three fundamental challenges in video-based planning: 1) unambiguous task communication with simple human instructions, 2) controllable video generation that respects user intents, and 3) translating visual planning into robot actions. We propose language-gesture conditioning to generate videos, which is both simpler and clearer than existing language-only methods, especially in complex and uncertain environments. We then suggest a behavioral cloning design that seamlessly incorporates the video plans. This&That demonstrates state-of-the-art effectiveness in addressing the above three challenges, and justifies the use of video generation as an intermediate representation for generalizable task planning and execution.", "sections": [{"title": "1 Introduction", "content": "When we instruct other people to perform a task, we often point to the targets and say things like: \"Give me that glass.\", or \"Put this there.\". Such simple language-gesture instructions can be more effective in communicating tasks than verbally describing them without gestures. For example, a verbal instruction \u2013 \u201cGive me the blue glass located on the third row of the wooden cabinet.\u201d\u2013 can be verbose and ambiguous. The combination of pointing gestures and diectic words such as \u201cthis\u201d and \"that\" is convenient and clear at the same time, so it is used widely across cultures. Wouldn't it be amazing if we could control robots on a wide range of tasks using these simple language-gesture commands? Through this work, we strive to achieve exactly that.\nOur proposed framework, dubbed This&That, includes a controllable video generation module and a video-driven robot execution module. We build our video generator on top of a recent large-scale, open-vocabulary text-to-video diffusion models [1], which we fine-tune to adapt to our robotics setups. Our refined video diffusion model (VDM) is conditioned on input language describing the task using deictic words such as \u201cthis\u201d and \u201cthere,\" as well as input gestures represented as the 2D locations in the first frame image corresponding to the language. We introduce novel techniques to effectively incorporate the multi-modal conditionings, leading to SOTA-quality videos that closely align with human intentions even for uncertain tasks in complex scenes.\nInspired by recent developments, we consider the video generator as a generalizable planner that envisions the change of environment for a wide range of tasks. Here, the predicted video is a guide for robot actions, and the execution module only has to follow the predicted video. Unlike existing video-based approaches that either simplify the action space [2] or devise special inverse dynamics models [3], we incorporate the ability to follow video predictions into well-established behavioral cloning (BC) architectures such as the Action Chunking Transformer [4]. Our BC-based execution module efficiently cross-attends to the video frames to unify video-based planning and manipulation.\nWe conduct experiments on the Bridge video datasets [5, 6] and IsaacGym simulation datasets, where we deploy virtual robot policy rollouts. Our experiments include a wide range of open-vocabulary tasks within complex and uncertain environments. The results demonstrate that our This&That framework produces higher quality videos with superior alignment to user intentions than prior works. Our behavioral cloning experiments in simulation environments show the benefits of our proposed language-gesture commands and further justify the use of conditional video predictions for multi-task policy learning. Overall, we claim the following two contributions:\n\u2022 We propose language-gesture interactions with robots to achieve simple yet effective human-robot communication. Moreover, we develop language-gesture conditioned video generation techniques that lead to high-quality video-based planning that better aligns with user intentions than previous language-only methods.\n\u2022 We devise a video-conditioned behavioral cloning architecture to integrate the generated video predictions with live observations for multi-task robot policy learning."}, {"title": "2 Related work", "content": "2.1. Imitation Learning: Imitation learning is a technique to learn robotic behavior from expert\ndemonstration. Behavior cloning (BC) casts this task as supervised learning, where the aim is to train\na policy to directly mimic the expert's actions. Recently, BC has demonstrated strong performance\nin learning complex, dextrous robot skills [7, 8, 4, 9, 10]. While some BC policies rely solely on\nthe robot's state, adding goal information to the policy allows the robot to disambiguate between\ntasks [11]. Goals can be expressed as language instruction [9, 12, 13, 14, 15], images [16], or even\nsketches [17]. When visual goals are expressed as a single image, as in [18, 9, 19], the goals can be\nambiguous and the models have a tendency to overfit. Instead, we represent goals as a dense sequence\nof images, predicted by a video diffusion model. By providing intermediate goal information, we\nfind this formulation to better aid the policy to actually reach the long-term goal.\n2.2. Video Diffusion Models: A Video Diffusion Model (VDM) aims to generate temporally\nconsistent and high-fidelity videos [20, 21, 22] that align the provided conditions, which may include\nimage [23], text [24], audio [25], segmentation [26, 27, 28], camera pose [29, 30], and human\npose [31, 32] and so on. These conditions can be combined to facilitate highly controllable video\ngeneration outcomes."}, {"title": "3 Overview", "content": "Our proposed This&That framework is composed of two components: language-gesture-conditioned video generation and video-based robot planning. In Sec. 4, we introduce our video diffusion model built on top of a large video model. Notably, we use the language-gesture conditioning to provide user-friendly control (Sec 4.2). In Sec. 5, we introduce our behavioral cloning approach which references the generated video for executing the visual plans. In the experiment section (Sec. 6), we mainly prove the better user alignment of the language-gesture conditioning and the ability to translate a video into robot actions in simulated environments."}, {"title": "4 Language-Gesture Conditioned Video Diffusion Models", "content": "Preliminaries Recent video diffusion models (VDM) adopt training and sampling techniques similar to image diffusion models. During the forward diffusion, VDMs add Gaussian noise of random level to the video frames, training the model to predict this noise. In the reverse phase, VDMs use a Markov chain of iterative steps, where the trained neural network denoises the current frames. Message passing in both temporal and spatial dimensions is crucial for generating consistent frames.\nCurrent leading generative methods [1, 37] often operate in the latent space instead of the raw pixel space to reduce computational demands. Latent VDMs convert video frames to latent space using an encoder & during denoising and revert these latents to pixel space using a decoder D post-denoising.\nFine-tuning Large Video Models In this work, we use pre-trained Stable Video Diffusion (SVD) [1] as our foundational latent VDM, designed for open-vocabulary video generation due to its training on high-quality internet-scale data. Our VDM generates T video frames from an initial frame \\(I_0\\), aiming to learn a conditional joint distribution \\(p_\\theta(I_1, ..., I_T|I_0, C_{text}, C_{gest})\\) with added language and gesture conditions. We fine-tune our VDM in two phases: first by fine-tuning SVD with text and first frames on a robotics dataset, then making architectural modifications and further refining with gesture conditioning."}, {"title": "4.1 Language-Conditioned Finetuning", "content": "Most large VDMs, such as SVD, are trained on general, broad datasets and are not directly suitable for robotic tasks. To address this, we retain SVD's core structure but initially fine-tune it on robotics videos. We enhance SVD by incorporating language description \\(C_{text}\\) and first frame \\(I_0\\) conditionings through Feature-wise Linear Modulation (FiLM) [38], modulating intermediate"}, {"title": "4.2 Gesture-Conditioned Training and Inference", "content": "We use a combination of language and point-ing gestures marked as 2D locations on thefirst frame to intuitively control video gen-eration. To condition video generation withthese gestures, we adapt our VDM architec-ture (Fig. 2) to include a supplementary net-work structure parallel to our fine-tuned dif-fusion UNet, following ControlNet [40] forimage-conditioned generation. At this stage,we freeze the diffusion UNet weights and trainonly the new gesture conditioning branch.\nWe find that na\u00efvely applying the ControlNetscheme to our use case does not ensure thatthe video follows the gesture, due to the spa-tial and temporal sparsity of the conditioninginformation. Specifically, the 2D gesture loca-tions usually appear only in two frames, leav-ing the other frames and locations unconstrained, which led ControlNet to ignore the conditioning.Furthermore, the 2D gesture input alone does not fully define the task; it needs to be interpretedalongside the generated video and the action-oriented language of the text prompt, such as flip thiscloth from here.\nTo resolve these issues, inspired by [27], we augment the gesture conditioning branch inputs withthe first frame \\(I_0\\), the current noisy image \\(\\epsilon_t\\) at each denoising step \\(t\\), and the sparse gesture images\\(C_{gest}\\). We also modulate the branch with the text prompt. These adjustments ensure dense inputsignals across the conditioning branch, allowing gestures to be integrated meaningfully with currentvideo content and language. Finally, images \\(I_0\\) and \\(C_{gest}\\) are processed with a pre-trained encoder \\(E\\)from StableDiffusion, with random masking \\(M_p\\) applied to the image latent, form the inputs to theconditioning branch.\nSince most robotics video dataset lacks ground truth gesture locations, we automatically annotatethem by detecting the 2D gripper location when it opens and closes, identifying target gesture points.Further annotation details are in the supplementary. After obtaining these locations, we enhance thespatial signal by applying a 2D Gaussian filter to dilate the points, following [33]. For tasks likeopening a door, where only the initial contact point is relevant, we only consider one gesture point."}, {"title": "5 Video-Conditioned Behavioral Cloning", "content": "We want a policy \\(\\pi(\\cdot)\\) to translate frames \\(I = [I_0, ..., I_T]\\) from the video plan into executable robot\nactions. For this, we developed DiVA (Diffusion Video to Action), a behavioral cloning model\nreferencing the entire or partial subset of the predicted video sequence (see Fig. 3). DiVA, based on a\nTransformer encoder-decoder architecture similar to ACT [4], is trained to predict the next chunk\nof actions \\(a_{t:t+k}\\) based on the current image observation \\(o_t\\), the robot's end-effector pose \\(s_t\\), and the\nvideo sequence \\(\\tau\\): DiVA learns to sample from the conditional action distribution \\(\\pi_\\rho(a_{t:t+k}|o_t, s_t, \\tau)\\).\nFor our action space, we use a 6D delta end-effector pose expressed in the end-effector frame."}, {"title": "6 Experiments", "content": "We conduct a series of experiments to show the superior user alignment of our This&That framework,focusing on the accuracy of video generation and the translation of video plans into robot actions.Specifically, our main experiments aim to 1) show realistic, user-aligned video generations, 2) assessthe effectiveness of our language-gesture conditioning, and 3) evaluate the successful integration ofpredicted video plans with a behavior cloning algorithm in synthetic rollouts.\n6.1 Video Generation Experiments and Comparisons\nBridge Dataset We evaluate our video generation framework with the Bridge V1 [6] and V2 [5]\ndatasets, which are widely-used real robot datasets with human demonstrations. The Bridge datasets\nfeature complex real-world scenes and tasks, focusing on household environments such as kitchens,\nlaundry areas, and desk tops. We solely use the frontal view scenes and prune both short and long\nsequences for simplicity of training. Across V1 and V2, we obtain 25,767 videos for the initial\nfinetuning (Sec. 4.1) and gather 14,735 videos for the gesture conditioned training (Sec. 4.2), after\nfiltering out videos where the automatic annotation failed. Refer to the supplementary for details."}, {"title": "7 Limitations", "content": "Although our model generates high-fidelity videos, object shapes often change over time, likely due\nto the lack of 3D geometric constraints. Our predictions are limited to short, modular tasks; extending\nthem to longer tasks (e.g., cooking) with multi-modal instructions present a significant opportunity.\nMoreover, our automatic gesture labeling is susceptible to image artifacts such as motion blur."}, {"title": "A Website and Video", "content": "We encourage the readers to open the website https://cfeng16.github.io/this-and-that/ through their internet browsers. This site hosts our introductory video and various other visual results, allowing a dynamic view of our video-based approach's versatile and powerful capabilities."}, {"title": "B Document Overview", "content": "In this supplementary document, we provide detailed additional content that complements the main paper. Section C elaborates on details of additional qualitative results and ablation studies for both our proposed Video Diffusion Model (VDM) and Diffusion Video to Action model (DiVA). Section D elaborates on details of our proposed VDM architecture, training details, and automatic gesture labeling methods. Section E elaborates on details of DiVA implementation details. Section F elaborates on details of VDM training, testing, and user study details as well as DiVA training, testing, and simulator environment details. Additionally, we show VDM limitations (Sec. G)."}, {"title": "C Additional Experiments and Ablation Studies", "content": "C.1 Qualitative Comparison with Contemporary Video Generative Models.\nIn Fig. 8, we present a visual comparison of our method against contemporary video generation\nmodels, augmenting the quantitative data presented in Table 1 of the main paper.\nAVDC [2] is trained on the entire Bridge dataset, so it produces a semantically correct sequence.\nHowever, the visual quality of AVDC's output is lacking, characterized by low spatial and temporal\nresolution along with visual artifacts on the microwave door. These deficiencies hinder the accurate\ninterpretation of the end-effector and environmental states, which are critical for translating the\nvideos into robot actions. Other leading video generation models, such as DragAnything [26],\nStreamingT2V [24], and SVD [1], when used directly without specific fine-tuning, were unable\nto adhere to the provided text or gesture commands. This underscores the need for a specialized\nlanguage-gesture VDM, specifically designed for robotic applications."}, {"title": "C.2 Additional Qualitative Results on Bridge.", "content": "In Fig. 7, we present additional qualitative results from our test split of the Bridge dataset. We use the\nprovided initial frame and construct new text and gesture prompts to generate unique videos. It's\nimportant to note that the combinations of prompts and frames in Figure 4 of the main paper and\nthe first example in Fig. 7 do not exist in the original dataset, and there are no corresponding ground\ntruth videos. This approach was chosen because the entire Bridge dataset, including our test split,\nwas used for training the AVDC model, which represents the current state-of-the-art open-source\nvideo generator for robotics applications. Our results are compared against both our language-only\nbaseline and the AVDC.\nIn the first example from Fig. 7, video generators relying solely on language inputs (3rd row) struggle\nto capture the nuanced geometric relationship implied by the prompts, resulting in implausible video\noutputs. The second example reveals that both our language-only baseline (6th row) and the AVDC\n(5th row) were deemed unsuccessful in a user study. Although the AVDC attempted to align with the\nspecified direction on the table, the generation quality was poor, and the blue box became invisible\nafter movement. In the third example, our language-only baselines (9th row) performed well, correctly\ncapturing the straightforward \"closing\u201d action. However, the AVDC (8th row) failed to follow the\ntext prompt accurately, likely due to overfitting the training data."}, {"title": "C.3 VDM Ablation Study", "content": "To validate the design decisions behind our Video Diffusion Model (VDM) architecture, we conducted\nan ablation study from three distinct perspectives (as illustrated in Tab. 4).\nAssessing Na\u00efve ControlNet Conditioning Our first ablation revisits the standard ControlNet\nconditioning architecture. We substituted our proposed approach, which involved pre-trained VAE\nencoding, with a simple zero convolution, and removed our concatenation method for integrating\ngesture conditioning and VDM noise inputs. This change significantly reduced performance across all\nassessed metrics. Visual inspections of the generated videos confirmed that they failed to accurately\nfollow the specified gesture cues, underscoring the superiority of our original concatenation and\nencoding methods in maintaining adherence to gesture inputs.\nUsing Semantic Segmentation Masks in Gesture-Conditioning The second ablation experiment\ninvestigates the usage of segmentation masks during training, which could potentially offer more\nspatial information during gesture-conditioned training. By querying the SAM [47] with the \"pick\u201d\ngesture location, we can obtain a segmentation mask that provides denser gesture signals. However,\nthis additional spatial information does not translate to improved numerical performance. The\nsegmentation algorithm often misinterprets the intended objects, outputting broader scene segments\nrather than specific objects, like outputting a desk instead of just the cup on it. This inclusion of\nextraneous pixel data complicates effective training. Consequently, we found that a simpler approach\nusing 2D dilation from a single point yields better results than employing SAM masks."}, {"title": "C.4 Additional Qualitative Results from Simulation Rollouts", "content": "In Figures 10 and 11, we present further qualitative results from our simulated rollout experiments\non Isaac Gym [46]. These results showcase the ability of This&That to generate video plans and\neffectively translate them into robotic actions. While our language-conditioned (without gesture)\nVDM baseline generally performs well, as evidenced in Table 3 of the main paper, it encounters\ndifficulties with complex sentences involving geometric relations within the context of specific images\n(see Fig. 10). In particular, when identical objects are presented, the language-only models often fail\nto generate the correct actions. This issue is notably exacerbated in out-of-distribution tests, such as\nthose depicted in Fig. 11, where all objects involved are identical."}, {"title": "C.5 Ablating DiVA: Number of Goal Images and Frame Subsampling Randomization.", "content": "We perform two ablation studies for DiVA (Diffusion Video to Action) as shown in Fig. 9. For the first\nstudy, we vary N, the number of goal frames we subsample from the generated video frames, from 1\nto 25 in increments of 5. We see almost no success for N=1 (conditioning on just the last goal frame),\nwhich can be regarded as a baseline for goal-conditioned ACT [4]. The performance increases with\nthe number of conditioning frames, but it plateaus at around N=15 until N=25. We hypothesize that\nmore frames will be beneficial for complex tasks and leave conducting such challenging experiments\nas future work.\nFor the second study, we analyze the effects of adding randomness to the subsampling of the goal\nframes during training. In other words, we split the GT goals into N consecutive groups and randomly"}, {"title": "D Video Diffusion Model Implementation Details", "content": "D.1 Base Architecture\nThe video diffusion model (VDM) we use is based on the Stable Video Diffusion (SVD) frame-\nwork [1], which incorporates a modified version of the denoising algorithm from the EDM [48], a\ncontinuous-time diffusion model framework. Since the training code of SVD is not publicly available,\nwe first deploy an open-source codebase \u00b9 and make several modifications to the denoise algorithm\nas described in the following sections."}, {"title": "D.2 UNet Finetuning Details (Stage 1)", "content": "The SVD framework governs video motion using two key parameters: the motion bucket ID and noise\naugmentation. In robotics applications, a complete video sequence is crucial, depicting the robot arm\ncompleting its task. Consequently, we set the motion bucket ID to 200 and the noise augmentation to\n0.1, both during training and inference, to override motion control from the pretrained model.\nBuilding on the method proposed by [49], we enhance the stability of our VDM by discarding a\nsmall amount of noise \\(l_ogo \\sim N(-3.0, 0.5^2)\\) traditionally added to the conditioning frame. Instead,\nwe introduce a fixed noise value of 0.1 as an augmentation during training.\nFor effective text and first frame image conditioning, we concatenate the embeddings prior to their\nintroduction to the encoder hidden states. Given the varying dimensions of text embeddings across\ndifferent open-source CLIP [39] models, we select a version that matches the feature dimension of\nour SVD's CLIP image embeddings, which is 1024. We utilize a CLIP encoder from the StableDif-\nfusion2.1 framework [37] for text embeddings, resulting in dimensions of \\(x_{text} \\in \\mathbb{R}^{b\\times 77\\times 1024}\\) and\n\\(x_{I_0} \\in \\mathbb{R}^{b\\times 1\\times 1024}\\) for image embeddings, aligning perfectly for concatenation. The final concatenated\ndimension is \\(x_{concat} \\in \\mathbb{R}^{b\\times 78\\times 1024}\\). We observe that applying Layer Normalization [50] to these"}, {"title": "D.3 Gesture Conditioning Branch Details (Stage 2)", "content": "In the second stage of training for the gesture conditioning branch, we implement temporal condi-tioning by distributing two gesture points across different frames. For a sequence of T target frames,only two frames contain gesture point pixel information, while the remaining frames hold zero-valueimages as placeholders. The frames with gesture points are positioned randomly within the sequence,ensuring that the first (red) point appears temporally before the second (green) point as shown inFig. 12. Each gesture point is represented as a square box, 10 pixels per side, centered on the specifiedcoordinate. Building on the techniques from MotionCTRL [33], we employ a 2D Gaussian dilationmethod to enhance the visibility of gesture points.\nThe inputs for the first convolution layer include noise vector \\(\\epsilon \\in \\mathbb{R}^{(B\\times T)\\times 4\\times H\\times W}\\), the en-coded initial frame repeated T times \\(E(C_0) \\in \\mathbb{R}^{(B\\times T)\\times 4\\times H\\times W}\\), and the gesture condition vector\\(E(C_{gest}) \\in \\mathbb{R}^{(BxT)\\times 4\\times H\\times W}\\). The dimensions \\((B \\times T)\\), H, and W indicate batch size times numberof frames, and the height and width of the latent space, respectively. The resulting channel-wiseconcatenation forms a shape of \\(\\mathbb{R}^{(B\\times T)\\times 12\\times H\\times W}\\).\nGiven the discrepancy in input dimensions between the gesture conditioning branch and the UNet'sfirst convolution layer, we do not reuse the pre-trained convolution layer from the UNet. Instead, wetrain a new convolution layer from scratch with zero initialization (zero convolution), which helps"}, {"title": "D.4 Automatic Gesture Labeling on Real Data", "content": "The Bridge datasets [6, 5] provides metadata of the robot end-effector actions, from which we canrecover the key moments when the robot end-effector either closes to grasp or reopens to releaseobjects. Utilizing these temporal markers, our objective is to precisely determine the gripper'sinteraction points with objects at these key frames. To facilitate this, we employ a bounding boxdetector developed through training a YoloV8 [51] model on 450 manually annotated images thatmark the gripper's location. This specialized training enables the model to accurately outline thegripper's bounding box in subsequent frames, ensuring automatic detection and tracking of itsinteractions with objects.\nWith the gripper's bounding box identified, we extract the 2D coordinates of the objects at the targetframe index. We then employ the TrackAnything model [52] to track the objects' motion over time.This tracking approach is crucial, especially in scenarios where objects are released mid-air by thegripper, necessitating continuous monitoring of their trajectory to determine their landing points.This method reliably provides the two necessary gesture points for our application.\nHowever, this automated annotation method is not infallible. Challenges arise in tracking morecomplex interactions, and the Yolo model does not achieve perfect detection accuracy. Consequently,we exclude results from cases where tracking accuracy falls below acceptable thresholds. Additionally,we discard videos that exceed five times the length of the target frame count T or have fewer framethan T.\nTo aid the research community in enhancing gripper detection capabilities, we will release both thedetection code and the pre-trained weights. For further details and to access these resources, weencourage interested readers to consult our code repository once it is released."}, {"title": "E DiVA Model Implementation Details", "content": "Our Diffusion Video to Action (DiVA) framework is designed to model the distribution\\(\\pi_\\rho(a_{t:t+k}|o_t, s_t, \\tau)\\), where \\(a_{t:t+k}\\) represents the action chunk the robot executes from time t to\nt+k - 1. Here, \\(o_t\\) is the image observation of the environment at time t, \\(s_t\\) is the pose of the robot's\nend-effector at time t, and \\(\\tau\\) includes a subset of goal images from the video diffusion model output\n\\(I := I_{0:T}\\). We denote \\(N = |\\tau|\\) to be the size of the subset. DiVA processes each image of resolution\n\\(\\mathbb{R}^{256\\times 384\\times 3}\\) through a ResNet-18 architecture, pre-trained on ImageNet and adjusted during training,\nto produce latent embeddings of shape \\(\\mathbb{R}^{8\\times 12\\times 512}\\).\nThe latent embeddings for \\(o_t\\) and \\(\\tau\\) are then flattened to \\(z_{ot} \\in \\mathbb{R}^{96\\times 512}\\) and \\(z_\\tau \\in \\mathbb{R}^{N\\times 96\\times 512}\\),\nrespectively. To incorporate the temporal dimension of \\(\\tau\\), fixed 2D sinusoidal positional encodings"}, {"title": "E.1 Alternative Method: Inverse Dynamics Model", "content": "Initially, we explored training an inverse dynamics model to interpolate actions between frames\nproduced by our video diffusion model, inspired by the methods described in UniPi [34]. However, we\nobserve that such a strategy failed to produce reasonable action outputs, likely because our diffusion\nmodel generates a fixed number of frames for each demonstration, whereas the actual demonstrations\nvary in length. This discrepancy made it impractical to train an inverse dynamics model that could\nuniformly output a fixed number of actions for each frame sequence without introducing an additional\ndiffusion model dedicated to temporal interpolation as used in UniPi[34]. Our method avoids such\nadditional generative steps and is robust to the number of input images and temporal misalignments\nas shown in Sec. C.5."}, {"title": "F Experiment Details", "content": "F.1 VDM Training Details\nIn the Bridge dataset training, we use 8 Nvidia L40S GPUs with 48 GB memory each to train 99K\niterations (the closest checkpoint to 100K iterations) for the UNet and 4 GPUs with 30K iterations to\ntrain the gesture conditioning in the second stage. The batch size is 1 for each GPU. The pre-trained\nweight we start with is the default SVD model for the 14-frame version. For our training on the\nIsaac Gym dataset, we use 8 GPUs for 30K iterations in the stage 1 and another 4 GPUs for 15K\niterations in the stage 2. The pre-trained weight we start with is SVD-XT for the 25-frame version.\nWe apply the AdamW [53] optimizer with a constant learning rate of 1e-5 and 5e-6 respectively for\ntwo stages. We apply the 8-bit Adam [54] strategy to decrease GPU memory consumption, and no\nema is employed.\nTo augment the sparsity of the dataset available, we propose a horizontal flip mechanism to augment\nthe dataset. The probability of flipping is 0.45. However, we keep in mind that if the language prompt\ncontains keywords with position meanings, like left and right, we will not do flipping.\nF.2 VDMs Visual-Quality Experiment Details\nThe testing dataset is coming from the train-test split on 10% of the data from both V1 [6] and V2 [5]\nto facilitate subsequent quantitative comparisons. For the VDM table, we apply all 646 videos from\nBridge V1 after gesture label filtering.\nIn the VDM quantitative comparisons, we employ FID [43], FVD [44], PSNR, SSIM, and LPIPS [45]\nas our numerical metrics. We compute FID by sampling 9,000 images randomly from the generated\nframes and the ground truth dataset by the codebase of pyiqa [55]. The implementations of FVD,"}, {"title": "F.3 User Alignment Experiment Details", "content": "For the human study", "tasks": 8}]}