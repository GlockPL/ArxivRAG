{"title": "This&That: Language-Gesture Controlled Video Generation for Robot Planning", "authors": ["Boyang Wang", "Nikhil Sridhar", "Chao Feng", "Mark Van der Merwe", "Adam Fishman", "Nima Fazeli", "Jeong Joon Park"], "abstract": "We propose a robot learning method for communicating, planning, and executing a wide range of tasks, dubbed This&That. We achieve robot planning for general tasks by leveraging the power of video generative models trained on internet-scale data containing rich physical and semantic context. In this work, we tackle three fundamental challenges in video-based planning: 1) unambiguous task communication with simple human instructions, 2) controllable video generation that respects user intents, and 3) translating visual planning into robot actions. We propose language-gesture conditioning to generate videos, which is both simpler and clearer than existing language-only methods, especially in complex and uncertain environments. We then suggest a behavioral cloning design that seamlessly incorporates the video plans. This&That demonstrates state-of-the-art effectiveness in addressing the above three challenges, and justifies the use of video generation as an intermediate representation for generalizable task planning and execution.", "sections": [{"title": "1 Introduction", "content": "When we instruct other people to perform a task, we often point to the targets and say things like: \"Give me that glass.\", or \"Put this there.\". Such simple language-gesture instructions can be more effective in communicating tasks than verbally describing them without gestures. For example, a verbal instruction \u2013 \u201cGive me the blue glass located on the third row of the wooden cabinet.\u201d\u2013 can be verbose and ambiguous. The combination of pointing gestures and diectic words such as \u201cthis\u201d and \"that\" is convenient and clear at the same time, so it is used widely across cultures. Wouldn't it be amazing if we could control robots on a wide range of tasks using these simple language-gesture commands? Through this work, we strive to achieve exactly that.\nOur proposed framework, dubbed This&That, includes a controllable video generation module and a video-driven robot execution module. We build our video generator on top of a recent large-scale, open-vocabulary text-to-video diffusion models [1], which we fine-tune to adapt to our robotics setups. Our refined video diffusion model (VDM) is conditioned on input language describing the task using deictic words such as \u201cthis\u201d and \u201cthere,\" as well as input gestures represented as the 2D locations in the first frame image corresponding to the language. We introduce novel techniques to effectively incorporate the multi-modal conditionings, leading to SOTA-quality videos that closely align with human intentions even for uncertain tasks in complex scenes.\nInspired by recent developments, we consider the video generator as a generalizable planner that envisions the change of environment for a wide range of tasks. Here, the predicted video is a guide for robot actions, and the execution module only has to follow the predicted video. Unlike existing video-based approaches that either simplify the action space [2] or devise special inverse dynamics models [3], we incorporate the ability to follow video predictions into well-established behavioral cloning (BC) architectures such as the Action Chunking Transformer [4]. Our BC-based execution module efficiently cross-attends to the video frames to unify video-based planning and manipulation.\nWe conduct experiments on the Bridge video datasets [5, 6] and IsaacGym simulation datasets, where we deploy virtual robot policy rollouts. Our experiments include a wide range of open-vocabulary tasks within complex and uncertain environments. The results demonstrate that our This&That framework produces higher quality videos with superior alignment to user intentions than prior works. Our behavioral cloning experiments in simulation environments show the benefits of our proposed language-gesture commands and further justify the use of conditional video predictions for multi-task policy learning. Overall, we claim the following two contributions:\n\u2022 We propose language-gesture interactions with robots to achieve simple yet effective human-robot communication. Moreover, we develop language-gesture conditioned video generation techniques that lead to high-quality video-based planning that better aligns with user intentions than previous language-only methods.\n\u2022 We devise a video-conditioned behavioral cloning architecture to integrate the generated video predictions with live observations for multi-task robot policy learning."}, {"title": "2 Related work", "content": "2.1. Imitation Learning: Imitation learning is a technique to learn robotic behavior from expert\ndemonstration. Behavior cloning (BC) casts this task as supervised learning, where the aim is to train\na policy to directly mimic the expert's actions. Recently, BC has demonstrated strong performance\nin learning complex, dextrous robot skills [7, 8, 4, 9, 10]. While some BC policies rely solely on\nthe robot's state, adding goal information to the policy allows the robot to disambiguate between\ntasks [11]. Goals can be expressed as language instruction [9, 12, 13, 14, 15], images [16], or even\nsketches [17]. When visual goals are expressed as a single image, as in [18, 9, 19], the goals can be\nambiguous and the models have a tendency to overfit. Instead, we represent goals as a dense sequence\nof images, predicted by a video diffusion model. By providing intermediate goal information, we\nfind this formulation to better aid the policy to actually reach the long-term goal.\n2.2. Video Diffusion Models: A Video Diffusion Model (VDM) aims to generate temporally\nconsistent and high-fidelity videos [20, 21, 22] that align the provided conditions, which may include\nimage [23], text [24], audio [25], segmentation [26, 27, 28], camera pose [29, 30], and human\npose [31, 32] and so on. These conditions can be combined to facilitate highly controllable video\ngeneration outcomes."}, {"title": "3 Overview", "content": "Our proposed This&That framework is composed of two components: language-gesture-conditioned video generation and video-based robot planning. In Sec. 4, we introduce our video diffusion model built on top of a large video model. Notably, we use the language-gesture conditioning to provide user-friendly control (Sec 4.2). In Sec. 5, we introduce our behavioral cloning approach which references the generated video for executing the visual plans. In the experiment section (Sec. 6), we mainly prove the better user alignment of the language-gesture conditioning and the ability to translate a video into robot actions in simulated environments."}, {"title": "4 Language-Gesture Conditioned Video Diffusion Models", "content": "Preliminaries Recent video diffusion models (VDM) adopt training and sampling techniques\nsimilar to image diffusion models. During the forward diffusion, VDMs add Gaussian noise of\nrandom level to the video frames, training the model to predict this noise. In the reverse phase, VDMs\nuse a Markov chain of iterative steps, where the trained neural network denoises the current frames.\nMessage passing in both temporal and spatial dimensions is crucial for generating consistent frames.\nCurrent leading generative methods [1, 37] often operate in the latent space instead of the raw pixel\nspace to reduce computational demands. Latent VDMs convert video frames to latent space using an\nencoder \u0190 during denoising and revert these latents to pixel space using a decoder D post-denoising.\nFine-tuning Large Video Models In this work, we use pre-trained Stable Video Diffusion\n(SVD) [1] as our foundational latent VDM, designed for open-vocabulary video generation due\nto its training on high-quality internet-scale data. Our VDM generates T video frames from an initial\nframe Io, aiming to learn a conditional joint distribution \\(p_\\theta(I_1, ..., I_T|I_0, C_{\\text{text}}, C_{\\text{gest}})\\) with added\nlanguage and gesture conditions. We fine-tune our VDM in two phases: first by fine-tuning SVD\nwith text and first frames on a robotics dataset, then making architectural modifications and further\nrefining with gesture conditioning."}, {"title": "4.1 Language-Conditioned Finetuning", "content": "Most large VDMs, such as SVD, are trained on general, broad datasets and are not directly suitable for robotic tasks. To address this, we retain SVD's core structure but initially fine-tune it on robotics videos. We enhance SVD by incorporating language description \\(C_{\\text{text}}\\) and first frame \\(I_0\\) conditionings through Feature-wise Linear Modulation (FiLM) [38], modulating intermediate features using parameters derived from cross-attention between language and image tokens, with token extraction facilitated by the CLIP [39] encoder.\nDuring training, we introduce Gaussian noise to frames \\(I_{0:T}\\) and optimize the model using the noise reconstruction loss, conditioned on the initial frame and text. Considering the nature of robotics videos, which mark clear start and end of tasks, we uniformly subsample the sequence to compile T target frames per video (T typically from 14 to 25). For detailed information on data processing and fine-tuning methods, we highly recommend readers refer to the supplementary material."}, {"title": "4.2 Gesture-Conditioned Training and Inference", "content": "We use a combination of language and pointing gestures marked as 2D locations on the first frame to intuitively control video generation. To condition video generation with these gestures, we adapt our VDM architecture (Fig. 2) to include a supplementary network structure parallel to our fine-tuned diffusion UNet, following ControlNet [40] for image-conditioned generation. At this stage, we freeze the diffusion UNet weights and train only the new gesture conditioning branch.\nWe find that na\u00efvely applying the ControlNet scheme to our use case does not ensure that the video follows the gesture, due to the spatial and temporal sparsity of the conditioning information. Specifically, the 2D gesture locations usually appear only in two frames, leaving the other frames and locations unconstrained, which led ControlNet to ignore the conditioning. Furthermore, the 2D gesture input alone does not fully define the task; it needs to be interpreted alongside the generated video and the action-oriented language of the text prompt, such as flip this cloth from here.\nTo resolve these issues, inspired by [27], we augment the gesture conditioning branch inputs with the first frame \\(I_0\\), the current noisy image \\(x_t\\) at each denoising step t, and the sparse gesture images \\(C_{\\text{gest}}\\). We also modulate the branch with the text prompt. These adjustments ensure dense input signals across the conditioning branch, allowing gestures to be integrated meaningfully with current video content and language. Finally, images \\(I_0\\) and \\(C_{\\text{gest}}\\) are processed with a pre-trained encoder E from StableDiffusion, with random masking \\(M_p\\) applied to the image latent, form the inputs to the conditioning branch.\nSince most robotics video dataset lacks ground truth gesture locations, we automatically annotate them by detecting the 2D gripper location when it opens and closes, identifying target gesture points. Further annotation details are in the supplementary. After obtaining these locations, we enhance the spatial signal by applying a 2D Gaussian filter to dilate the points, following [33]. For tasks like opening a door, where only the initial contact point is relevant, we only consider one gesture point."}, {"title": "5 Video-Conditioned Behavioral Cloning", "content": "We want a policy \\(\\pi(\\cdot)\\) to translate frames \\(I = [I_0, ..., I_T]\\) from the video plan into executable robot actions. For this, we developed DiVA (Diffusion Video to Action), a behavioral cloning model referencing the entire or partial subset of the predicted video sequence (see Fig. 3). DiVA, based on a Transformer encoder-decoder architecture similar to ACT [4], is trained to predict the next chunk of actions \\(a_{t:t+k}\\) based on the current image observation \\(o_t\\), the robot's end-effector pose \\(s_t\\), and the video sequence \\(\\tau\\): DiVA learns to sample from the conditional action distribution \\(\\pi_\\rho(a_{t:t+k}|o_t, s_t, \\tau)\\). For our action space, we use a 6D delta end-effector pose expressed in the end-effector frame."}, {"title": "6 Experiments", "content": "We conduct a series of experiments to show the superior user alignment of our This&That framework, focusing on the accuracy of video generation and the translation of video plans into robot actions. Specifically, our main experiments aim to 1) show realistic, user-aligned video generations, 2) assess the effectiveness of our language-gesture conditioning, and 3) evaluate the successful integration of predicted video plans with a behavior cloning algorithm in synthetic rollouts."}, {"title": "6.1 Video Generation Experiments and Comparisons", "content": "Bridge Dataset We evaluate our video generation framework with the Bridge V1 [6] and V2 [5] datasets, which are widely-used real robot datasets with human demonstrations. The Bridge datasets feature complex real-world scenes and tasks, focusing on household environments such as kitchens, laundry areas, and desk tops. We solely use the frontal view scenes and prune both short and long sequences for simplicity of training. Across V1 and V2, we obtain 25,767 videos for the initial finetuning (Sec. 4.1) and gather 14,735 videos for the gesture conditioned training (Sec. 4.2), after filtering out videos where the automatic annotation failed. Refer to the supplementary for details.\nEvaluating Video Prediction Qualities To assess the quality of our video predictions for a robotics setup, we compare against the most recent video synthesis methods in Image-to-Video (SVD [1]), Image-Text-to-Video (StreamingT2V [24]), and Image-Gesture-to-Video (DragAnything [26]). We also compare with the SOTA open-source VDM of robotics, AVDC [2] (Image-Text-to-Video) which is trained on the entire Bridge dataset.\nWe evaluate these methods on the 646 test videos of the Bridge dataset using the Frechet Inception Distance (FID) [43] and Frechet Video Distance (FVD) [44] to evaluate visual and temporal generation quality. Moreover, we compute loss against the ground truth images using pixel-wise and perceptual metrics (PSNR, SSIM, and LPIPS [45]). As shown in Tab. 1, our language-gesture conditioned VDM demonstrates its superior visual realism and temporal alignment against the baselines, including AVDC which has seen these test videos during training.\nUser Alignment Study We conduct a human study to evaluate the alignment of the generated videos. To this end, we explain our ground truth intention to the participants (using both language and gesture) and ask whether the given video aligns with the intention. Here, we test AVDC and our trained VDM models with different combinations of input conditioning: vision (first frame image), language, and gesture. Moreover, we test two forms of language conditioning for the same model weight: regular language from the original Bridge dataset and our deictic format. Moreover, we select 24 test sequences from Bridge and categorize them into four robotics tasks: pick and place, stacking, folding, and open or close.\nThe results in Tab. 2 and Fig. 4 demonstrate a significant improvement in alignment for the language-gesture conditioning, outperforming baselines. Using ambiguous deictic language, e.g., \u201cthis\u201d and"}, {"title": "6.2 Synthetic Rollout Experiments", "content": "We design a simulation environment using Isaac Gym [46] to evaluate our method's performance in translating video plans into robot actions under significant amounts of ambiguity. We set four blocks on a tabletop environment and prepare pick-and-place tasks by relating two randomly selected objects in diverse ways (e.g., place the blue cube in front of the orange cylinder). We use a hand-scripted policy to obtain ground truth trajectories and train our VDM and DiVA policy models using programmatically generated conditionings. During testing, we further stress-test the methods by introducing out-of-distribution scenes that contain identical objects in identical colors.\nThe results in Tab. 3 and Fig. 6 again show that our method conditioned on language and gesture outperforms the policy relying solely on language for both in and out of distribution cases. We also modify ACT [4] to consume the language and gesture conditioning directly and find that it underperforms compared to our main model (refer to supplementary for details). These results highlight the effectiveness and robustness of our language-gesture-guided VDM to accurately indicate the goals of the policy."}, {"title": "7 Limitations", "content": "Although our model generates high-fidelity videos, object shapes often change over time, likely due to the lack of 3D geometric constraints. Our predictions are limited to short, modular tasks; extending them to longer tasks (e.g., cooking) with multi-modal instructions present a significant opportunity. Moreover, our automatic gesture labeling is susceptible to image artifacts such as motion blur."}, {"title": "8 Conclusion", "content": "In this work, we present This&That, a framework that combines the power of visual generative models and imitation learning for effective task communication and planning. This&That leverages a language-gesture conditioned video generative model as an intermediate planner and uses a video-based behavioral cloning model that elegantly combines the predicted frames and live observation"}, {"title": "A Website and Video", "content": "We encourage the readers to open the website https://cfeng16.github.io/this-and-that/\nthrough their internet browsers. This site hosts our introductory video and various other visual results,\nallowing a dynamic view of our video-based approach's versatile and powerful capabilities."}, {"title": "B Document Overview", "content": "In this supplementary document, we provide detailed additional content that complements the main\npaper. Section C elaborates on details of additional qualitative results and ablation studies for\nboth our proposed Video Diffusion Model (VDM) and Diffusion Video to Action model (DiVA).\nSection D elaborates on details of our proposed VDM architecture, training details, and automatic\ngesture labeling methods. Section E elaborates on details of DiVA implementation details. Section F\nelaborates on details of VDM training, testing, and user study details as well as DiVA training, testing,\nand simulator environment details. Additionally, we show VDM limitations (Sec. G)."}, {"title": "C Additional Experiments and Ablation Studies", "content": "C.1 Qualitative Comparison with Contemporary Video Generative Models.\nIn Fig. 8, we present a visual comparison of our method against contemporary video generation\nmodels, augmenting the quantitative data presented in Table 1 of the main paper.\nAVDC [2] is trained on the entire Bridge dataset, so it produces a semantically correct sequence.\nHowever, the visual quality of AVDC's output is lacking, characterized by low spatial and temporal\nresolution along with visual artifacts on the microwave door. These deficiencies hinder the accurate\ninterpretation of the end-effector and environmental states, which are critical for translating the\nvideos into robot actions. Other leading video generation models, such as DragAnything [26],\nStreamingT2V [24], and SVD [1], when used directly without specific fine-tuning, were unable\nto adhere to the provided text or gesture commands. This underscores the need for a specialized\nlanguage-gesture VDM, specifically designed for robotic applications.\nC.2 Additional Qualitative Results on Bridge.\nIn Fig. 7, we present additional qualitative results from our test split of the Bridge dataset. We use the\nprovided initial frame and construct new text and gesture prompts to generate unique videos. It's\nimportant to note that the combinations of prompts and frames in Figure 4 of the main paper and\nthe first example in Fig. 7 do not exist in the original dataset, and there are no corresponding ground\ntruth videos. This approach was chosen because the entire Bridge dataset, including our test split,\nwas used for training the AVDC model, which represents the current state-of-the-art open-source\nvideo generator for robotics applications. Our results are compared against both our language-only\nbaseline and the AVDC.\nIn the first example from Fig. 7, video generators relying solely on language inputs (3rd row) struggle\nto capture the nuanced geometric relationship implied by the prompts, resulting in implausible video\noutputs. The second example reveals that both our language-only baseline (6th row) and the AVDC\n(5th row) were deemed unsuccessful in a user study. Although the AVDC attempted to align with the\nspecified direction on the table, the generation quality was poor, and the blue box became invisible\nafter movement. In the third example, our language-only baselines (9th row) performed well, correctly\ncapturing the straightforward \"closing\u201d action. However, the AVDC (8th row) failed to follow the\ntext prompt accurately, likely due to overfitting the training data."}, {"title": "C.3 VDM Ablation Study", "content": "To validate the design decisions behind our Video Diffusion Model (VDM) architecture, we conducted\nan ablation study from three distinct perspectives (as illustrated in Tab. 4).\nAssessing Na\u00efve ControlNet Conditioning Our first ablation revisits the standard ControlNet\nconditioning architecture. We substituted our proposed approach, which involved pre-trained VAE\nencoding, with a simple zero convolution, and removed our concatenation method for integrating\ngesture conditioning and VDM noise inputs. This change significantly reduced performance across all\nassessed metrics. Visual inspections of the generated videos confirmed that they failed to accurately\nfollow the specified gesture cues, underscoring the superiority of our original concatenation and\nencoding methods in maintaining adherence to gesture inputs.\nUsing Semantic Segmentation Masks in Gesture-Conditioning The second ablation experiment\ninvestigates the usage of segmentation masks during training, which could potentially offer more\nspatial information during gesture-conditioned training. By querying the SAM [47] with the \"pick\"\ngesture location, we can obtain a segmentation mask that provides denser gesture signals. However,\nthis additional spatial information does not translate to improved numerical performance. The\nsegmentation algorithm often misinterprets the intended objects, outputting broader scene segments\nrather than specific objects, like outputting a desk instead of just the cup on it. This inclusion of\nextraneous pixel data complicates effective training. Consequently, we found that a simpler approach\nusing 2D dilation from a single point yields better results than employing SAM masks."}, {"title": "C.4 Additional Qualitative Results from Simulation Rollouts", "content": "In Figures 10 and 11, we present further qualitative results from our simulated rollout experiments\non Isaac Gym [46]. These results showcase the ability of This&That to generate video plans and\neffectively translate them into robotic actions. While our language-conditioned (without gesture)\nVDM baseline generally performs well, as evidenced in Table 3 of the main paper, it encounters\ndifficulties with complex sentences involving geometric relations within the context of specific images\n(see Fig. 10). In particular, when identical objects are presented, the language-only models often fail\nto generate the correct actions. This issue is notably exacerbated in out-of-distribution tests, such as\nthose depicted in Fig. 11, where all objects involved are identical."}, {"title": "C.5 Ablating DIVA: Number of Goal Images and Frame Subsampling Randomization.", "content": "We perform two ablation studies for DiVA (Diffusion Video to Action) as shown in Fig. 9. For the first\nstudy, we vary N, the number of goal frames we subsample from the generated video frames, from 1\nto 25 in increments of 5. We see almost no success for N=1 (conditioning on just the last goal frame),\nwhich can be regarded as a baseline for goal-conditioned ACT [4]. The performance increases with\nthe number of conditioning frames, but it plateaus at around N=15 until N=25. We hypothesize that\nmore frames will be beneficial for complex tasks and leave conducting such challenging experiments\nas future work.\nFor the second study, we analyze the effects of adding randomness to the subsampling of the goal\nframes during training. In other words, we split the GT goals into N consecutive groups and randomly"}, {"title": "D Video Diffusion Model Implementation Details", "content": "D.1 Base Architecture\nThe video diffusion model (VDM) we use is based on the Stable Video Diffusion (SVD) frame-\nwork [1], which incorporates a modified version of the denoising algorithm from the EDM [48], a\ncontinuous-time diffusion model framework. Since the training code of SVD is not publicly available,\nwe first deploy an open-source codebase \u00b9 and make several modifications to the denoise algorithm\nas described in the following sections."}, {"title": "D.2 UNet Finetuning Details (Stage 1)", "content": "The SVD framework governs video motion using two key parameters: the motion bucket ID and noise\naugmentation. In robotics applications, a complete video sequence is crucial, depicting the robot arm\ncompleting its task. Consequently, we set the motion bucket ID to 200 and the noise augmentation to\n0.1, both during training and inference, to override motion control from the pretrained model.\nBuilding on the method proposed by [49], we enhance the stability of our VDM by discarding a\nsmall amount of noise \\(l_{og} \\sigma \\sim \\mathcal{N}(-3.0, 0.5^2)\\) traditionally added to the conditioning frame. Instead,\nwe introduce a fixed noise value of 0.1 as an augmentation during training.\nFor effective text and first frame image conditioning, we concatenate the embeddings prior to their\nintroduction to the encoder hidden states. Given the varying dimensions of text embeddings across\ndifferent open-source CLIP [39] models, we select a version that matches the feature dimension of\nour SVD's CLIP image embeddings, which is 1024. We utilize a CLIP encoder from the StableDif-\nfusion2.1 framework [37] for text embeddings, resulting in dimensions of \\(X_{\\text{text}} \\in \\mathbb{R}^{b\\times 77\\times 1024}\\) and\n\\(X_{I_0} \\in \\mathbb{R}^{b\\times 1\\times 1024}\\) for image embeddings, aligning perfectly for concatenation. The final concatenated\ndimension is \\(x_{\\text{concat}} \\in \\mathbb{R}^{b\\times 78\\times 1024}\\). We observe that applying Layer Normalization [50] to these"}, {"title": "D.3 Gesture Conditioning Branch Details (Stage 2)", "content": "In the second stage of training for the gesture conditioning branch, we implement temporal condi-\ntioning by distributing two gesture points across different frames. For a sequence of T target frames,\nonly two frames contain gesture point pixel information, while the remaining frames hold zero-value\nimages as placeholders. The frames with gesture points are positioned randomly within the sequence,\nensuring that the first (red) point appears temporally before the second (green) point as shown in\nFig. 12. Each gesture point is represented as a square box, 10 pixels per side, centered on the specified\ncoordinate. Building on the techniques from MotionCTRL [33], we employ a 2D Gaussian dilation\nmethod to enhance the visibility of gesture points.\nThe inputs for the first convolution layer include noise vector \\(\\epsilon \\in \\mathbb{R}^{(B\\times T)\\times 4\\times H\\times W}\\), the en-\ncoded initial frame repeated T times \\(E(C_0) \\in \\mathbb{R}^{(B\\times T)\\times 4\\times H\\times W}\\), and the gesture condition vector\n\\(E(C_{\\text{gest}}) \\in \\mathbb{R}^{(BxT)\\times 4\\times H\\times W}\\). The dimensions \\((B \\times T)\\), H, and W indicate batch size times number\nof frames, and the height and width of the latent space, respectively. The resulting channel-wise\nconcatenation forms a shape of \\(\\mathbb{R}^{(B\\times T)\\times 12\\times H\\times W}\\).\nGiven the discrepancy in input dimensions between the gesture conditioning branch and the UNet's\nfirst convolution layer, we do not reuse the pre-trained convolution layer from the UNet. Instead, we\ntrain a new convolution layer from scratch with zero initialization (zero convolution), which helps"}, {"title": "D.4 Automatic Gesture Labeling on Real Data", "content": "The Bridge datasets [6, 5] provides metadata of the robot end-effector actions, from which we can\nrecover the key moments when the robot end-effector either closes to grasp or reopens to release\nobjects. Utilizing these temporal markers, our objective is to precisely determine the gripper's\ninteraction points with objects at these key frames. To facilitate this, we employ a bounding box\ndetector developed through training a YoloV8 [51] model on 450 manually annotated images that\nmark the gripper's location. This specialized training enables the model to accurately outline the\ngripper's bounding box in subsequent frames, ensuring automatic detection and tracking of its\ninteractions with objects.\nWith the gripper's bounding box identified, we extract the 2D coordinates of the objects at the target\nframe index. We then employ the TrackAnything model [52] to track the objects' motion over time.\nThis tracking approach is crucial, especially in scenarios where objects are released mid-air by the\ngripper, necessitating continuous monitoring of their trajectory to determine their landing points.\nThis method reliably provides the two necessary gesture points for our application.\nHowever, this automated annotation method is not infallible. Challenges arise in tracking more\ncomplex interactions, and the Yolo model does not achieve perfect detection accuracy. Consequently,\nwe exclude results from cases where tracking accuracy falls below acceptable thresholds. Additionally,\nwe discard videos that exceed five times the length of the target frame count T or have fewer frames\nthan T.\nTo aid the research community in enhancing gripper detection capabilities, we will release both the\ndetection code and the pre-trained weights. For further details and to access these resources, we\nencourage interested readers to consult our code repository once it is released."}, {"title": "E DIVA Model Implementation Details", "content": "Our Diffusion Video to Action (DiVA) framework is designed to model the distribution\n\\(\\pi_\\rho(a_{t:t+k}|o_t, s_t, \\tau)\\), where \\(a_{t:t+k}\\) represents the action chunk the robot executes from time t to\nt+k - 1. Here, \\(o_t\\) is the image observation of the environment at time t, \\(s_t\\) is the pose of the robot's\nend-effector at time t, and \\(\\tau\\) includes a subset of goal images from the video diffusion model output\n\\(I := I_{0:T}\\). We denote \\(N = |\\tau|\\) to be the size of the subset. DiVA processes each image of resolution\n\\(\\mathbb{R}^{256\\times 384\\times 3}\\) through a ResNet-18 architecture, pre-trained on ImageNet and adjusted during training,\nto produce latent embeddings of shape \\(\\mathbb{R}^{8\\times 12\\times 512}\\).\nThe latent embeddings for \\(o_t\\) and \\(\\tau\\) are then flattened to \\(z_{ot} \\in \\mathbb{R}^{96\\times 512}\\) and \\(z_\\tau \\in \\mathbb{R}^{N\\times 96\\times 512}\\),\nrespectively. To incorporate the temporal dimension of \\(\\tau\\), fixed 2D sinusoidal positional encodings"}, {"title": "E.1 Alternative Method: Inverse Dynamics Model", "content": "Initially, we explored training an inverse dynamics model to interpolate actions between frames\nproduced by our video diffusion model, inspired by the methods described in UniPi [34]. However, we\nobserve that such a strategy failed to produce reasonable action outputs, likely because our diffusion\nmodel generates a fixed number of frames for each demonstration, whereas the actual demonstrations\nvary in length. This discrepancy made it impractical to train an inverse dynamics model that could\nuniformly output a fixed number of actions for each frame sequence without introducing an additional\ndiffusion model dedicated to temporal interpolation as used in UniPi[34]. Our method avoids such\nadditional generative steps and is robust to the number of input images and temporal misalignments\nas shown in Sec. C.5."}, {"title": "F Experiment Details", "content": "F.1 VDM Training Details\nIn the Bridge dataset training, we use 8 Nvidia L40S GPUs with 48 GB memory each to train 99K\niterations (the closest checkpoint to 100K iterations) for the UNet and 4 GPUs with 30K iterations to\ntrain the gesture conditioning in the second stage. The batch size is 1 for each GPU. The pre-trained\nweight we start with is the default SVD model for the 14-frame version. For our training on the\nIsaac Gym dataset, we use 8 GPUs for 30K iterations in the stage 1 and another 4 GPUs for 15K\niterations in the stage 2. The pre-trained weight we start with is SVD-XT for the 25-frame version.\nWe apply the AdamW [53] optimizer with a constant learning rate of 1e-5 and 5e-6 respectively for\ntwo stages. We apply the 8-bit Adam [54] strategy to decrease GPU memory consumption, and no\nema is employed.\nTo augment the sparsity of the dataset available, we propose a horizontal flip mechanism to augment\nthe dataset. The probability of flipping is 0.45. However, we keep in mind that if the language prompt\ncontains keywords with position meanings, like left and right, we will not do flipping.\nF.2 VDMs Visual-Quality Experiment Details\nThe testing dataset is coming from the train-test split on 10% of the data from both V1 [6] and V2 [5]\nto facilitate subsequent quantitative comparisons. For the VDM table, we apply all 646 videos from\nBridge V1 after gesture label filtering.\nIn the VDM quantitative comparisons, we employ FID [43], FVD [44], PSNR, SSIM, and LPIPS [45]\nas our numerical metrics. We compute FID by sampling 9,000 images randomly from the generated\nframes and the ground truth dataset by the codebase of pyiqa [55]. The implementations of FVD,"}, {"title": "G Limitation: Changes in Object Appearance Over Time", "content": "While our Video Diffusion Model (VDM) framework typically generates high-quality videos, it\noccasionally alters the shape of objects being moved over time, as shown in Fig. 13. We suspect that\nthis issue stems from the inherent limitations of representing 3D geometric relationships within 2D\nvideo media. Nevertheless, we believe that these artifacts might not hinder the translation to robotic\nactions, as long as the visuals of the end-effector remain clear and discernible."}]}