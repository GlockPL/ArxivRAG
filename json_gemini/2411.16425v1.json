{"title": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM\nfor Zero-shot Object Navigation", "authors": ["Linqing Zhong", "Chen Gao", "Zihan Ding", "Yue Liao", "Si Liu"], "abstract": "The Zero-Shot Object Navigation (ZSON) task requires em-\nbodied agents to find a previously unseen object by navigat-\ning in unfamiliar environments. Such a goal-oriented explo-\nration heavily relies on the ability to perceive, understand,\nand reason based on the spatial information of the envi-\nronment. However, current LLM-based approaches convert\nvisual observations to language descriptions and reason in\nthe linguistic space, leading to the loss of spatial informa-\ntion. In this paper, we introduce TopV-Nav, a MLLM-based\nmethod that directly reasons on the top-view map with com-\nplete spatial information. To fully unlock the MLLM's spa-\ntial reasoning potential in top-view perspective, we propose\nthe Adaptive Visual Prompt Generation (AVPG) method to\nadaptively construct semantically-rich top-view map. It en-\nables the agent to directly utilize spatial information con-\ntained in the top-view map to conduct thorough reason-\ning. Besides, we design a Dynamic Map Scaling (DMS)\nmechanism to dynamically zoom top-view map at preferred\nscales, enhancing local fine-grained reasoning. Addition-\nally, we devise a Target-Guided Navigation (TGN) mecha-\nnism to predict and to utilize target locations, facilitating\nglobal and human-like exploration. Experiments on MP3D\nand HM3D benchmarks demonstrate the superiority of our\nTopV-Nav, e.g., +3.9% SR and +2.0% SPL absolute im-\nprovements on HM3D.", "sections": [{"title": "1. Introduction", "content": "In the realm of embodied AI, Zero-Shot Object Navigation\n(ZSON) is a fundamental challenge, requiring an agent to\ntraverse in continuous environments to locate a previously-\nunseen object specified by category (e.g., fireplace). Such\na zero-shot setting discards category-specific training and\nsupports an open-category manner, emphasizing reasoning\nand exploration ability with minimized movement costs.\nRecently, emerging works [34, 48, 49] have started to in-\ntegrate Large Language Models (LLMs) into ZSON agents,\naiming to improve the reasoning ability by harnessing the\nextensive knowledge embedded in LLMs. For instance, ob-\njects co-occurrence knowledge, e.g., a mouse often being\nfound near a computer, can guide agent's exploration.\nAs is well known, spatial information is vital for naviga-\ntion decision-making, as it includes essential aspects such\nas room layouts and positional relationships among objects.\nTypically, navigation agents translate egocentric observa-\ntions into a structured map to encode spatial information,\ni.e., top-view map or called bird's eye view map. This map\nserves as the core representation, facilitating essential func-\ntionalities like obstacle avoidance and path planning.\nHowever, current methods face notable limitations. As\nillustrated in Fig.1(a), these methods lie in two paradigms,\ni.e., frontier-based exploration (FBE) or waypoint-based ex-\nploration methods. Typically, LLM-based methods need to\nconvert the top-view map into natural language, e.g., sur-\nrounding descriptions, and use LLM to conduct reasoning\nin the linguistic domain. This map-to-text conversion pro-\ncess leads to the loss of vital spatial information such as the\nlayout information of the living room. Alternatively, if the\nagent knows the spatial layout of the living room and un-\nderstands that the fireplace is generally positioned opposite\nthe sofa and table, the spatial location of the fireplace can\nbe directly inferred based on the room layout. Therefore,\nconsidering the top-view map contains useful spatial infor-\nmation, and that MLLMs have demonstrated capabilities in\ngrasping spatial relationships within images in the field of\nimage understanding [5, 25], an interesting question arises\n\u201ccan we leverage MLLM to reason directly on the image\nof top-view map to produce executable moving decisions?\u201d\nBesides, as shown in Fig.1(a), FBE methods select a\npoint from frontier regions to move toward exploration, re-\nstricting the LLM's action space to only the frontier bound-\naries. Waypoint-based methods use the waypoint predic-\ntor to generate navigable waypoints and select a waypoint\nto move toward exploration, restricting the LLM's action\nspace to only a predefined set of points. Moreover, since\nthe waypoint predictor is trained offline using depth infor-\nmation, its predicted waypoints focus solely on traversabil-\nity without semantics, also leading to weakly-semantic ac-\ntion space. Both FBE and waypoint-based paradigms suffer\nfrom constrained local action spaces. Thus, the question is\n\u201ccan we construct global and semantic-rich action space?\u201d\nFurthermore, when humans explore unfamiliar environ-\nments, they can use current observations to infer the layout\nof unseen areas and predict where the target object might\nbe located [15]. This predicted target location guides their\nmovement decisions strategically, even in the absence of\ncomplete information. However, the action spaces of FBE\nand waypoint-based method are confined strictly to naviga-\nble regions, which prevents them from possessing this ca-\npability. Thus, the question is \u201ccan we leverage known ob-\nservations to infer the probable location of the target object\nin unexplored areas to guide the current decision?\u201d\nTherefore, to address the limitations and questions men-\ntioned above, we make multi-fold innovations. First, we\npropose an insightful method called TopV-Nav to fully un-\nlock the top-view spatial reasoning potential of MLLM for\nZSON task. Specifically, the current LLM-driven paradigm\nrequires the map-to-language process for LLM reasoning\nin linguistic space, where the converting process may lose\nsome crucial spatial information such as objects and room\nlayout. Instead, we propose a novel paradigm that leverages\nMLLM to directly reason on the top-view map, discarding\nthe map-to-language process and maximizing the utiliza-\ntion of all the spatial information. Second, we introduce an\nAdaptive Visual Prompt Generation (AVPG) method to en-\nhance MLLM's understanding of the top-view map. AVPG\nadaptively generates visual prompts directly onto the map,\nin which various elements are spatially arranged to reflect\ntheir spatial relationships within the environment. There-\nfore, MLLM can grasp and comprehend crucial information\ndirectly from the map, facilitating effective spatial reason-\ning. For instance, in Fig.1(b), our method can interpret the\nroom's layout and infer the fireplace's location. Addition-\nally, the moving location is predicted directly based on the\ntop-view map, resulting in a global and more flexible action\nspace. Third, for environments with numerous objects, the\ntop-view map may not be able to visually represent all ele-\nments. Thus, we propose a Dynamic Map Scaling (DMS)\nmechanism to dynamically adjust the map's scale via zoom-\ning operations. DMS further enhances agent's local spa-\ntial reasoning and fine-grained exploration in local regions.\nLast but not least, we propose a Target-Guided Navigation\n(TGN) mechanism to first predict the final coordinate of the\ntarget object, which can even lie in unexplored areas. The\ntarget coordinate further guides the moving location within\ncurrent navigable regions, mirroring human-like predictive\nreasoning and exploratory behavior.\nExperiments are conducted on MP3D and HM3D bench-\nmarks, which demonstrates that our TopV-Nav achieves\npromising performance, e.g., +3.9% SR and +2.0% SPL\nabsolute improvements on HM3D."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Object-goal Navigation", "content": "Object-goal navigation has been a fundamental challenge\nin embodied AI [4, 6, 11\u201313, 22, 23, 28, 29, 38\u201340, 44\u2013\n47, 50]. Early methods leverage RL to train policies, which\nexplore visual representations [24], meta-learning [33], and\nsemantic priors [35, 37] etc., to enhance performance.\nModular-based approaches [3, 27, 43] leverage perception\nmodels [14, 42] to construct episodic maps, based on which\nlong-term goals are generated to guide the local policy.\nTo overcome closed-world assumption and achieve\nzero-shot object navigation (ZSON) task, EmbCLIP [16]\nand [21] leverage the multi-modal alignment ability of\nCLIP [26] to enable cross-domain zero-shot object naviga-\ntion. Furthermore, CoWs [10] accelerates the progress of\nthe ZSON task, where no simulation training is required,\nand a single model can be applied across multiple envi-\nronments. Recent methods [34, 49] extract semantic infor-\nmation using powerful off-the-shelf detectors [19, 20, 41],\nbased on which they employ LLMs to determine the next\nfrontier [49] or waypoint [34] for exploration. However,\nspatial layout information is lost during map-to-language\nconversion. To address this limitation, we propose direct\nreasoning on the top-view map with MLLM, fully leverag-\ning the complete spatial information."}, {"title": "2.2. Spatial Reasoning with MLLMs", "content": "Developing the spatial reasoning capabilities of MLLMs\nhas become increasingly popular recently. KAGI [17]\nproposes generating a coarse robot movement trajectory\nas dense reward supervision through keypoint inference.\nSCAFFOLD [18] leverages scaffolding coordinates to pro-\nmote vision-language coordination. PIVOT [25] iteratively\nprompts MLLM with images annotated with a visual repre-\nsentation of proposals and can be applied to a wide range\nof embodied tasks. In the domain of vision-language navi-\ngation, AO-Planner [7] proposes visual affordance prompt-\ning to enable MLLM to select candidate waypoints from\nfront-view images. However, previous works focus on ex-\nploring MLLM's spatial reasoning from egocentric perspec-\ntives, while the investigation from top-view perspective re-\nmains limited (top-view map is the core representation for\nrobots). Our work explores the top-view spatial reasoning\npotential of MLLMs for the ZSON task."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Problem Definition", "content": "The ZSON task requires an agent, which is randomly placed\nin a continuous environment as initialization, to navigate to\nan instance of a user-specified object category G. At time\nstep t, the agent receives egocentric observations Ot, which\ncontains RGB-D images and its pose pt. The agent is ex-\npected to adopt a low-level action at, i.e., move_forward,\nturn_left, turn_right, look_up, look_down and\nstop. The task is considered successful if the agent stops\nwithin a distance threshold from the target in less than 500\nsteps and the target is visible in the egocentric observation."}, {"title": "3.2. Overview", "content": "We propose a novel MLLM-driven method, termed TopV-\nNav, designed to fully unlock MLLM's top-view perception\nand spatial reasoning capabilities for the ZSON task.\nSpecifically, as shown in Fig.2, the agent observes ego-\ncentric RGB-D images It and its pose pt at time step\nt. Then, the proposed Adaptive Visual Prompt Genera-\ntion (AVPG) module converts these egocentric observations\nonto a top-view map VMt as shown in (a). Note that VMt\nadopts different colors and text-boxes as visual prompts to\nrepresent various elements, which are spatially arranged on\nthe map to provide spatial layout information. Next, in the\nproposed Dynamic Map Scaling (DMS) module, we take\nVMt to query MLLM to conduct spatial reasoning and pre-\ndict a ratio r\u2208 [0,1] for zooming in the map as shown\nin (b)(c). Note that the zooming region is the agent's sur-\nroundings, supporting local fine-grained exploration. Sub-\nsequently, we propose Target-Guided Navigation (TGN) as\nshown in Fig.2. The scaled map is taken as input to query\nMLLM to conduct spatial reasoning and produce the target\nlocation and moving location. The target location refers to\nthe potential location of the target object, thus it may lie in\nunknown areas on the map. This target location is used to\nguide the prediction of the moving location, which is the\nagent's next actual destination within navigable areas. Note\nthat any location within navigable areas can be decided as\nthe moving location, resulting in a global action space that\nis not restricted to frontiers or waypoints. Finally, we utilize\nthe fast marching method [31] as the local policy to perform\na series of low-level actions, by which the agent can gradu-\nlly move towards the moving location."}, {"title": "3.3. Adaptive Visual Prompt Generation", "content": "To adaptively construct top-view maps on which MLLM\ncan perform spatial layout reasoning, we propose the Adap-\ntive Visual Prompt Generation (AVPG) method.\nTechnically, at each time step t, we transform the ego-\ncentric depth image dt into 3D point clouds utilizing agent's\npose pt and then convert them to the global space. Points\nnear the floor are seen to the navigable areas while points\nabove a height threshold ho are considered obstacle areas.\nMoreover, we leverage Grounding-DINO [20] to identify\nobjects and their bounding boxes from egocentric images.\nEach box is projected into 3D using corresponding depth\ninformation and further projected to the top-view map by\ncompressing high dimensions. To build a map that MLLM\ncan understand, we leverage different colors and text as vi-\nsual prompts on the map to distinguish different areas, such\nas historical traveled areas, navigable areas, obstacles areas,\nfrontiers and objects, as shown in Fig.2(a). For detected\nobjects, we devise a simple but efficient sampling strategy\nto select goal-relevant objects to display on the map. We\nassign each bounding box j of category i a priority score\nscorei,j which is formulated as:\nscorei,j = \u03b1 \u00b7 confi,j + (1 \u2212 \u03b1) \u00b7 si,G, (1)\nwhere confi,j denotes the detection confidence. Follow-\ning [49], si,g indicates the similarity score between object\ncategory i and goal category G, and \u03b1 is a hyper-parameter.\nThe objects with scorei,j above a threshold scores will be\nannotated with corresponding bounding box and category\non top-view map VMt and serve as visual prompts.\nKey Area Markers Generation. We aim to further gen-\nerate markers as visual prompts on the map to refer key\nareas that contain rich semantics, helping MLLM better in-\nterpret this map. Technically, we apply DBSCAN [9], a\ndensity-based spatial clustering algorithm. We cluster two\ntypes of elements: frontiers and the locations of observed\nobjects. In one respect, by identifying the boundary points\nof the navigable areas and the locations of obstacles, we\nobtain the locations of frontiers not occupied by obstacles\nUi\u2208F(Xi, Yi). In another respect, we leverage detection\nmodel and depth information to collect observed objects'\nlocations Ui\u2208o(xi, Yi). Eventually, we merge these two\nparts into a candidate point set Ui\u2208OUF(Xi, Yi) and apply\nDBSCAN method to it.\nConcretely, We first randomly sample a point (xc, Yc)\nwhich is not yet clustered from the candidate point set\nUi\u2208OUF(Xi, Yi). With this point as the center, we consider\nits e-neighborhood which is formulated as:\nNe((xc, Yc)) = {(xj, yj) | dist((xj, Yj), (xc, Yc)) \u2264 \u20ac}, (2)\nwhere e is a hyper-parameter that denotes the radius of clus-\ntering. (xc, yc) is the center point of this e-neighborhood. In\norder to avoid the influence of noise, only if the number of\npoints in the e-neighborhood reaches a threshold \u03bc, we treat\n(xc, yc) and other points within its e-neighborhood a novel\ncluster. Otherwise, these points are considered as noises.\nIn other words, \u03bc specifies the minimum number of points\ncontained in a cluster.\nTaking a step further, after a novel qualified cluster is\ndetermined, we merge it with the existing clusters according\nto the following formula:\n{Ne((xi, Yi))} = {Ne((xi, Yi))} \u222a Ne((xc, yc)). (3)\nNote that if any two clusters have an intersection, we merge\nthem and generate a novel neighborhood with a greater ra-\ndius of clustering. The center point of this new cluster is\nalso updated. We repeat the process of random sampling\nand merging clusters until all points in Ui\u2208OUF(Xi, Yi) are\nprocessed (i.e., included in the cluster or regarded as noise).\nIn this way, a series of cluster centers Vier Ci are gener-\nated, which serve as markers to refer key areas as shown\nin Fig.2(a). Moreover, this clustering method enables the\nnumber and distribution of key areas to be dynamically ad-\njusted during the agent's exploration."}, {"title": "3.4. Dynamic Map Scaling", "content": "The top-view map VMt with fixed size and resolution may\nnot effectively represent the spatial layout. Therefore, we\ndevise the Dynamic Map Scaling (DMS) mechanism to dy-\nnamically adjust the map scale during exploration, further\nsupporting local fine-grained exploration. For instance, as\nshown in the original top-view map (Fig.2(b)), some rep-\nresentative layouts are obscured by the textual annotations.\nHowever, in the zoomed map (c), not only the spatial lay-\nout is clearly visible, but more objects are also displayed for\nMLLM to perform spatial reasoning. Such a zooming op-\neration allows MLLM to capture more spatial clues and to\nperform fine-grained exploration in local key areas.\nSpecifically, we query the MLLM to predict a scaling\nratio according to the current top-view map, as shown in\nFig.2(b)(c). The MLLM is asked to reason on the origi-\nnal top-view map VMt, considering the spatial layout and\nagent's current position etc. Then we require MLLM to se-\nlect a map scaling ratio r \u2208 [0, 1] from a predefined set of\ncandidate scales. According to the scaling ratio chosen by\nMLLM, the top-view map VMt is cropped with the agent\nas the center. Then we apply AVPG once again to obtain\nthe scaled top-view map with visual prompts. Note that if\nr = 1, which means the top-view map remains unchanged,\nthe original top-view map VMt is directly used for TGN to\nproduce the moving location."}, {"title": "3.5. Target-Guided Navigation", "content": "We design the Target-Guided Navigation (TGN) mecha-\nnism to guide the decision process by predicting the target"}, {"title": "4.2. Comparison with Previous Methods", "content": "In this section, we compare our TopV-Nav with state-of-\nthe-art object navigation methods on the MP3D and HM3D\nbenchmarks. Since requesting OpenAI's service is costly\nand time-consuming, we sample a subset of the dataset to\nconduct experiments efficiently. Specifically, we sample\n1,500 episodes from the HM3D and MP3D respectively.\nThese sampled episodes cover all scenes and target ob-\nject categories in the validation split. Therefore, it remains\nhighly representative. We mainly compare to the ESC [49]\nand VoroNav [34] as shown in Tab.1. Note that ESC ap-\nplies the frontier-based exploration method and VoroNav is\nthe waypoint-based approach. This analysis explicitly com-\npares the performance of the three paradigms (i.e., FBE-\nbased, waypoint-based and our top-view spatial reasoning)\non the object navigation task.\nAs shown in Tab. 1, our TopV-Nav significantly improves\nthe navigation performance. Specifically, our approach out-\nperforms ESC on the validation split of MP3D by improv-\ning SR and SPL by 3.2% and 1.9% respectively. More-\nover, on HM3D benchmark, SR is increased from 42.0% to\n45.9% and SPL is raised from 26.0% to 28.0%. Such an im-\nprovement reveals that MLLM's direct layout reasoning on\nthe top-view map leverages more spatial clues than the map-\nto-language manner followed by both FBE and waypoint-\nbased paradigms, demonstrating the superiority of our pro-\nposed approach."}, {"title": "4.3. Ablation Studies", "content": "To analyze the contribution of each module, we conduct ab-\nlation experiments, which are shown in Tab.2, Tab. 3, and\nTab. 4. Also, in order to save time and cost, an indicative\nsubset of HM3D dataset is sampled to carry out ablation\nstudies. We sample 200 episodes, which cover all valida-\ntion scenes and target object categories on HM3D. In all\nablation experiments, we utilize the same episodes for eval-\nuation to ensure stability.\nAdaptive Visual Prompt Generation. The Adaptive Vi-\nsual Prompt Generation (AVPG) method is a core part\nfor MLLM's layout reasoning. As shown in Tab.2, the\nAVPG effectively supports and releases the MLLM's spa-\ntial reasoning abilities, achieving a significant 40.5% SR\nand 22.7% SPL on the HM3D benchmark. The result sug-\ngests that MLLM has substantial potential to guide agent's\nexploration through grasping visual prompts and perform-\ning layout reasoning on the top-view map."}, {"title": "Dynamic Map Scaling", "content": "As shown in Tab.2, we observe that\napplying the DMS mechanism promotes SR from 40.5%\nto 42.0%. It implies that the agent's zooming operation\nbenefits object navigation. Besides, SPL is improved from\n22.7% to 23.6%. This improvement of navigation efficiency\nreveals that grasping more visual clues and layout informa-\ntion benefits fine-grained layout reasoning, preventing the\nagent from potentially long-distance exploration."}, {"title": "Target-Guided Navigation", "content": "Through comparing \u201c#2\u201d and\n\u201c#3\u201d in Tab.2, we observe that our proposed TGN mod-\nule boosts SR from 42% to 43.5% and improves SPL from\n23.6% to 24.7%, which indicates that MLLM's spatial\nlayout inference estimates the potential goal location and\nthis location predicted by MLLM further effectively guides\nagent's exploration. This significant augmentation also con-\nfirms the importance of a human-like predictive reasoning\nin unfamiliar environments."}, {"title": "Visual Prompt Components", "content": "The results in Tab.3 indi-\ncate the effects of different visual prompt components on\nthe task performance. The comparison between \u201c#2\u201d and\n\u201c#1\u201d confirms that explicitly marking the spatial locations\nof objects on the top-view map improves navigation per-\nformance. In this manner, SR is lifted up from 39.5% to\n42% and SPL is improved by 1.7%. Besides, integrating\nMLLM's predicted target as prompts further gains a signifi-\ncant improvement of SR (42% to 43.5%) and SPL (23.8% to\n24.7%). It illustrates that rich visual prompts embedded in\nthe top-view map facilitate MLLM's layout reasoning and\ntherefore enhance the agent's navigation ability."}, {"title": "Map Scaling Ratios", "content": "We investigate how the ratios of\nagent's zooming operation affect the navigation perfor-\nmance. As shown in Tab.4, both SR and SPL increase\nas the candidate number of map scaling ratios increases.\nWhen the top-view map remains unchanged (i.e., \u201c#1\u201d in\nTab. 4), the object navigation performance is relatively poor.\nThis limitation stems from the fact that MLLM cannot per-\nform zooming for fine-grained layout reasoning, especially\nin object-dense scenes. By adding 1/2 to map scaling ratios,\nSR rises from 39.0% to 41.5% while SPL is also increased\nby 1.5%. Moreover, introducing more map scales 1/3, 1/5\nfurther improves navigation performance as shown in \u201c#3\u201d,\nlifting SR and SPL from 41.5%, 23.1% to 43.5%, 24.7%."}, {"title": "4.4. Qualitative Analysis", "content": "We visualize the navigation process along with the MLLM's\nspatial reasoning to give a more intuitive view.\nFig.4 displays several examples of MLLM's spatial rea-\nsoning. Note that we utilize different colors to annotate spe-\ncific areas (e.g., obstacles and navigable areas). Object cate-\ngories are also directly marked in the top-view map accord-\ning to their spatial locations, facilitating MLLM to perform\nlayout reasoning.\nIn Fig.4(a), the agent is tasked to search for a bed in an\nindoor environment. In this time step, the representative\nobjects of a living-room such as chairs and cabinets are lo-\ncated in this area. The MLLM observes these objects and\nthe scene layout which is likely a living-room. Based on the\nembedded prior knowledge, MLLM performs layout rea-\nsoning and infers that key area 5 is likely the location of\na bedroom. Besides, by leveraging commonsense", "(2.5, 7.5)": "This prediction\nfurther guides the local action policy, allowing the agent to\ngradually approach the target object.\nIn Fig.4(b), considering the goal is \u201cbed", "(5.0, 8.0)": "nand guides the agent to navigate to it in search of the goal", "bed": "For example in Fig.4(c), MLLM observes the spatial\nlayout and notices the detected bed. Considering that the\nbed is located in the bedroom which also contains the goal", "toilet": "MLLM predicts the target location"}, {"title": "5. Conclusion", "content": "In this paper, we tackle the Zero-Shot Object Navigation\n(ZSON) task, where spatial information plays a critical role\nin such a goal-oriented exploration task. However, previ-\nous LLM-based methods must transfer the top-view map\nto language descriptions, conducting exploration reason-\ning in the linguistic domain. Such a transformation pro-\ncess loses spatial information such as object and room lay-\nout. Therefore, we aim to study how we can directly adopt\nthe top-view map for reasoning by using MLLM's ability\nto understand images. Specifically, we propose several in-\nsightful methods to fully unlock the top-view spatial rea-\nsoning potential of MLLM. The proposed Adaptive Visual\nPrompt Generation (AVPG) method draws a semantically-\nrich map with visual prompts. The Dynamic Map Scaling\n(DMS) mechanism aims to adjust the map scale to appro-\npriate scales for MLLM to interpret layout and decision-\nmaking. The Target-Guided Navigation (TGN) mechanism\nimitates human behavior to predict the target's potential lo-\ncation to guide the current action. Experiments on MP3D\nand HM3D benchmarks demonstrate the effectiveness of\nour work."}]}