{"title": "Controlled Low-Rank Adaptation with Subspace Regularization for Continued Training on Large Language Models", "authors": ["Yuheng Lu", "Bingshuo Qian", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "abstract": "Large language models (LLMs) exhibit remarkable capabilities in natural language processing but face catastrophic forgetting when learning new tasks, where adaptation to a new domain leads to a substantial decline in performance on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a subspace regularization method on LoRA structure. Aiming to reduce the scale of output change while in-troduce minimal constraint on model capacity, CLORA imposes constraint on the direction of updating matrix's null space. Experimental results on commonly used LLM finetuning tasks reveal that CLORA significantly outperforms existing LoRA subsequent methods on both in-domain and out-domain evaluations, highlighting the superority of CLORA as a effective parameter-efficient finetuning method with catastrophic forgetting mitigating. Further investigation for model parameters indicates that CLORA effectively balances the trade-off between model capacity and degree of forgetting.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) demonstrate remarkable capabilities in natural language tasks. However, when performing continued training on additional datasets, a key challenge may faced, known as catastrophic forgetting (McCloskey and Cohen, 1989), where adaptation to a new domain leads to a substantial decline in performance on previous tasks.\nExisting approaches to mitigate catastrophic forgetting can be broadly categorized into data-based, architecture-based, and learning-based methods (Wang et al., 2023a). Data-based methods (de Masson D'Autume et al., 2019) primarily based on rehearsing prior training data, which raises data privacy concerns. Additionally, for LLMs, obtaining the necessary prior training data samples is challenging due to their training on massive datas. Architecture-based methods (Wang et al., 2023c; Razdaibiedina et al., 2023) introduce isolated parameters for each continued training stage for reducing interference. In contrast, learning-based methods train in the shared vector space, controlling learning process by adding regularization terms to the loss or employing specific optimization designs. Inference for architecure-based methods typically involves a selection process (Gurbuz and Dovrolis, 2022; Kang et al., 2022), which is more complex than that for learning-based methods. As continued trained LLMs are generally regarded as foundation models, flexibility is essential for their broader applications. Consequently, due to deployment considerations, learning-based methods are preferred over architecture-based methods for LLMs.\nThe core idea of learning-based methods is to constrain parameter updates, which aligns pre-cisely with the Parameter-Efficient FineTuning (PEFT) research paradigm of LLMs. Although initially proposed for computational efficiency, PEFTs have demonstrated to learn less and forget less(Biderman et al., 2024), primarily due to their constrained model capacity. Notably, a well-established insight that related to learning-based methods in PEFT research is that LLMs are primarily finetuned within a specific low-rank subspace, this insight has led to the development of the Low-Rank Adaptation method (LoRA)(Hu et al., 2021).\nHowever, LORA imposes no restrictions on parameter updates beyond the low-rank constraint, and matrix perturbation theory suggests that even low-rank updates can significantly influence matrix properties (Sherman, 1949; Davis and Kahan, 1970). For instance, in an extreme case, it is theoretically possible to learn a model that eliminates all top-k principal components (optimal rank-k approximation) through a rank-k update, thus destroy most of the base model's ability. Therefore, LoRA would be benifit from more constraints for mitigating catastrophic forgetting. However, more constraints would reduce model capacity for updating, which influences the effectiveness of training. For instance, adding L2 regularization significantly restricts the norm of the updating matrix. Consequently, effective management of the capacity-forgetting balancing has become a major concern.\nTo address this concern, in this work, we propose Controlled LORA (CLoRA), a subspace regularization method on LoRA structure. We start the design of CLORA from the perspective of the null space of updating matrix. The intuition behind CLORA is illustrated in Figure 1, where the output change \u2206y is derived from applying the updating matrix \u2206W on the component of the input x that falls within the row space of \u2206W, while components in the null space are ignored. Under this intuition, for reducing the scale of output change, options include reducing the scale of \u2206W, and encouraging more input component fall in the null space of \u2206W. The former is more related to model capacity, and for concerns of capacity-forgetting balancing, we focus on the latter.\nThe dimension of the null space for the updating matrix is directly determined by the rank of it, LORA already addressed. A key factor remains, the direction of null space, which influence input components that fall in, but free-learned LoRA does not constraint. CLORA constraint the direction of null space of updating matrix by introducing a pre-defined subset of that, this is implemented by orthogonal regularization with a pre-defined matrix."}, {"title": "2 Related Works", "content": "2.1 Reducing Catastrophic Forgeting\nCatastrophic forgetting is a significant challenge in various transfer learning scenarios, including continual learning (Wang et al., 2023a) and LLM continued training (Wu et al., 2024). In these settings, continued training on new tasks may impair abilities of the pre-trained model. Approaches for mitigating catastrophic forgetting can be broadly categorized into data-based, architecture-based and learning-based methods.\nData-based methods primarily based on re-hearsal of prior training data or representation, (de Masson D'Autume et al., 2019) introduce an episodic memory for experience rehearsal, (Rebuffi et al., 2017; Chaudhry et al., 2019) selects previous training data for rehearsaling. For LLMs, acquiring the necessary prior training data is challenging due to the extensive amount of data used in their training. Instead, the concept of rehearsal is commonly adopted by mixing data from general domains for LLM continued training. This approach is generally orthogonal to model-related methods, thus we will not discuss it further.\nArchitecture-based methods (Wang et al., 2023c; Razdaibiedina et al., 2023) introduce iso-lated parameters for each continued training stage to reduce interference. (Wang et al., 2023c) use isolated parameters for each task, and enables a selecting mechenism during inference. Progressive Prompts (Razdaibiedina et al., 2023) sequentially concatenates prompts for each task with previously learned prompts. These architecture-based methods generally require specific techniques for inference and continued training, resulting in a lack of flexibility, particularly in the context of LLMs.\nLearning-based methods performs continued training in a shared vector space, controlling learning process by adding regularization term on loss or applying specific optimization designs. Orthogonal Gradient Descent (OGD) (Farajtabar et al., 2020) constrains the parameters to move within the orthogonal space defined by the gradients of previous tasks. Continual LoRA (C-LoRA)(Smith et al., 2024) and O-LORA (Wang et al., 2023b) introduce regularization with previous continual learned parameters for reducing interference in the multi-stage training setting. Our proposed CLORA imposes orthogonal regularization similar to O-LORA, but the regularization matrix is not restricted to be the previous learned parameter, thus CLORA can be used for one-stage continued training whereas O-LORA cannot."}, {"title": "2.2 LoRA and Subspace Tuning", "content": "Parameter-Efficient FineTuning (PEFT) (Han et al., 2024) aims to tune models with minimizing computational resources, which is widely used for large-scale models including LLMs. Among these methods, LORA (Hu et al., 2021) and its subsequent variants (Wang et al., 2024a; Liu et al., 2024) learn a low-rank decomposition for updating parameter matrices, and could be categorized into learning-based continued training method, which is the focus of our work.\nThe core insight of LoRA is to tune model within a low-rank subspace, and with no additional constraints imposed on this tuning subspace. Some subsequent works delve deeper into the tuning subspace to mitigate catastrophic forgetting for LLM continued training, MiLoRA (Wang et al., 2024a)"}, {"title": "3 Prelimilaries", "content": "3.1 Notations\nThe notations commonly used in this paper are summarized in table 1. We provide some additional notes here. While generally used for denote the input and output of the while model, we denote x, y as input and output to a single linear layer, represented by W. ||A|| denotes L2 norm (largest singular value) in our paper, instead of Frobenius norm (||A||F = \u221a\u03a3 \u0391[i, j]2). r and k are most important hyperparameters for CLORA, r is the rank of updating matrix, which is used in all LoRA works, k is the number of regularization vectors(column of regularization matrix) in CLORA.\n3.2 Problem Definition\nWe aim to mitigate catastrophic forgetting in LLM continued training. Catastrophic forgetting menifest as performance decline on tasks from previous domain when training on new domain. To evaluate this, we conduct both in-domain tasks (demonstrating the effectiveness of training) and out-domain tasks (from previous domain, indicating the degree of forgetting). We conduct experiments in the one-stage tuning setting, specifically, we finetune a base LLM on one training dataset, then take in-domain and out-domain evaluations."}, {"title": "4 Method", "content": "In this section, we introduce Controlled Low-Rank Adaptation (CLoRA) method. We illustrate the application of CLORA in transformer-based LLMs in Figure 2. CLORA shares the same modeling structure with LoRA, but imposes on orthogonal regularization term computed using LoRA parameters into the loss function.\nCLORA Modeling Consistent with LoRA, CLORA decomposes the updating for a parameter matrix W to a multiplication of two lor-rank matrices \u2206W = ABT, where W, \u2206W \u2208 Rm\u00d7n, A\u2208Rmxr, B\u2208Rn\u00d7r, r <\u0442, \u043f.\nCLORA computes orthogonal regularization for A and BT with untrainable pre-defined matrix PA \u2208 Rm\u00d7k and PB \u2208 Rn\u00d7k, where k is a hyperparameter controlling the size of regularization matrix, larger k introduces more constraint. The orthogonal regularization loss on one LoRA parameter A is defined as\nLorth(A, PA) = \u2211 ||AP [i, j]||2"}, {"content": "In this section, we introduce Controlled Low-Rank Adaptation (CLoRA) method. We illustrate the application of CLORA in transformer-based LLMs in Figure 2. CLORA shares the same modeling structure with LoRA, but imposes on orthogonal regularization term computed using LoRA parameters into the loss function.\nCLORA Modeling Consistent with LoRA, CLORA decomposes the updating for a parameter matrix W to a multiplication of two lor-rank matrices \u2206W = ABT, where W, \u2206W \u2208 Rm\u00d7n, A\u2208Rmxr, B\u2208Rn\u00d7r, r <\u0442, \u043f.\nCLORA computes orthogonal regularization for A and BT with untrainable pre-defined matrix PA \u2208 Rm\u00d7k and PB \u2208 Rn\u00d7k, where k is a hyperparameter controlling the size of regularization matrix, larger k introduces more constraint. The orthogonal regularization loss on one LoRA parameter A is defined as\n$L_{orth}(A, P_A) = \\sum_{i,j} ||AP_[i, j]||^2$\nwhere A \u2208 Rm\u00d7r, PA \u2208 Rm\u00d7k. Lorth(A, PA) regularize on orthogonality of every (A[:, i], PA[:, j]) pairs. The final loss of CLoRA in a transformer-based LLM is defined as\nLLM(, input)+\n\u0391\u03a3 \u2211(Lorth (Ai, PA) + Lorth(B, PB))\nwhere LLM(, input) is the original language model loss on text input and LLM parameters \u0398, the summation on Lorth is over index of all trainable parameter matrices. A controls the weighting of orthogonal loss, we set it to 1 as default.\nInitialization Following LoRA(Hu et al., 2021), we initialize A with gaussian noise and B is zeros, ensuring AW is zero at the begining of training. For the CLORA regularization matrices, we suggest that they should have othogomal rows. Specifically, for regularization matrix P\u2208 Rm\u00d7k, ||P[: ,i]|| = 1 for every i, and P[:,i]P[:, j] = 0 for i \u2260 j. This ensures uniform regularization. We propose several variants as follows,\n\u2022 Random Initialize: Initialize with an orthogonal matrix(Saxe et al., 2014).\n\u2022 Initialize from SVD: Previous works (Sharma et al., 2024; Wang et al., 2024a) have explored the roles of singular value decomposition (SVD)"}, {"title": "5 Experiment and Analysis", "content": "In this section, we conduct experiments to evaluate and analyze our proposed CLORA method. All evaluations are available in Im-eval (Gao et al., 2024) with MIT License. We aim to answer these research questions,\n\u2022 RQ1: Does CLORA perform effectively as a parameter-efficient finetuning method with catastrophic forgetting mitigating?\n\u2022 RQ2: How do the size and selection of regularization matrices influence the performance of CLORA?\n\u2022 RQ3: How does CLORA demonstrate superiority on capability-forgetting balancing?\n5.1 Evaluating for Tuning on Commonsense Reasoning Dataset (RQ1)\nTo answer RQ1, we conduct experiments following the setting in the previous works on PEFT(Liu et al., 2024; Wang et al., 2024a). Specifically, we finetune LLaMA-2-7B (Touvron et al., 2023) with a dataset for commonsense reasoning. We evaluated the model on both in-domain downstream tasks to evaluate learning effectiveness and on out-domain tasks with minimal overlap with the training dataset to measure the degree of forgetting. We report partial results here, full results on each CLORA variants and results on finetuning LLaMA-3-8B(AI@Meta, 2024) are reported in Appendix B.\n5.1.1 Experiment Setup\nDatasets and Tasks We use Commonsense170K (Hu et al., 2023) for finetuning. For in-domain evaluation, eight commonsense reasoning datasets are used, including BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag(HS) (Zellers et al., 2019), Wino-Grande(WG) (Sakaguchi et al., 2019), ARC-e, ARC-c (Clark et al., 2018), and OBQA (Mihaylov\net al., 2018). The tasks are formulated as multiple-choice problem, and we report accuracy based on the last checkpoint. For out-domain evaluations, BIG-Bench-Hard(BBH) (Suzgun et al., 2022) and MMLU-Pro (Wang et al., 2024b) are used. These benchmarks encompass challenging subsets of tasks across a wide range of domains and are widely employed for evaluating the capabilities of LLMs. Additionally, they include samples that are more complex than those in our training data, ensuring minimal overlap. We use lm-eval (Gao et al., 2024) for reporting out-domain evaluation.\nComparison Methods\n\u2022 LoRA (Hu et al., 2021) is a widely-used parameter-efficient finetuning technique, and it serves as the foundation of our proposed CLORA.\n\u2022 DORA (Liu et al., 2024) is a recent work on structure improvement of LoRA, we include it as a baseline for improved LoRA.\n\u2022 PiSSA (Meng et al., 2024) and MiLoRA (Wang et al., 2024a) are two variants of LoRA, both employing SVD components for LoRA initialization, MiLoRA use minor components while PiSSA use major. Notably, MiLoRA can be categorized as a catastrophic forgetting mitigating method.\n\u2022 Reducing the updating rank(-r*): Lower rank r imposes stricter constraints on the updating matrix. We maintain a consistent rank across all methods and consider variations in rank as a separate baseline.\n\u2022 L2 regularization(-L2) introduces L2 regularization for trainable parameters, serving as a fundamental approach to limit updates.\n\u2022 CLORA: Random initialize variant of CLORA, it is the basic setting of our method. We will include results for other variants in the subsequent section 5.2.\nHyperparameter We use the same hyperparameter configurations as (Wang et al., 2024a). Details are listed in Appendix A. Notably, we use 32 for updating matrix rank r as default for all methods. For CLORA regularization matrix, we select k in [128, 256, 512, 1024, 2048]. For LoRA-L2, 1e-5 is used for weighting of L2 regularization, we note that 1e-4 is also tested, but too large for getting effective tuning."}, {"title": "5.1.2 Results and Analysis", "content": "In-domain Evaluation We report in-domain results in Table 2. LLaMA-2-7B cannot be directly applied to these tasks without finetuning, following previous works(Wang et al., 2024a), we include the ChatGPT baseline reported in (Liu et al., 2024) as a reference, which is obtained with gpt-3.5-turbo API using a zero-shot Chain of Thought (Wei et al., 2022). For CLORA, we report the best result across all results for all k.\nFor in-domain evaluation, results demonstrate that CLORA outperforms on all datasets, surpassing the best baseline by 2.9 points in average accuracy (%). These outcomes suggest that, although proposed primarily for mitigating catastrophic forgetting, CLORA also performs as a effective PEFT method. We attribute this to the nature of LLM finetuning, as an instance of transfer learning, the performance of LLM finetuning is strongly correlated with the base model's ability, when catastrophic forgetting occurs during training, the base model's strength may diminish. Therefore, we hypothesize that a method with effective capacity-forgetting balancing would exhibit strong effectiveness in LLM finetuning.\nOut-domain Evaluation We report out-domain results for CLORA and baselines in Table 3. \"base\" denotes the result on the base model LLaMA-2-7B, \"CLORA\" denotes the result of best model from in-domain evaluation.\nIn out-domain evaluation, all baselines underperform compared to the base model, highlighting the severe issue of catastrophic forgetting in this experimental setup. Notably, our proposed CLORA not only outperforms all baselines, but also surpasses the base model's performance. We attribute this to the effective capacity-forgetting balancing for CLORA, enabling the effective extraction of generally useful knowledge from the commonsense reasoning training dataset.\nAnswer for RQ1: The superior performance in both in-domain and out-domain evaluations demonstrates that our proposed CLORA serves effectively as a parameter-efficient finetuning method with catastrophic forgetting mitigating."}, {"title": "5.2 Evaluating for different CLoRA variants and hyperparameters (RQ2)", "content": "To answer RQ2, we compare results for various CLORA variants to demonstrate the impact of regularization matrix's size and selection.\nDatasets and Tasks Same as in section 5.1, we finetune LLaMA-2-7B on Commonsense170K, and conduct in-domain evaluation on eight commonsense reasoning tasks and out-domain evaluation on two LLM benchmarks. We report average results for each evaluation. To ascertain whether the optimal k value is consistent across tasks, we additionally perform an experiment in the math domain.\n5.2.1 Results and Analysis\nCommonsense Reasoning Setting We illustrate the performance for in-domain and out-domain evaluations with different k for commonsense reasoning setting as in Figure 3. The results indicate that for all CLORA variants tested, as k increases, the scores of both in-domain and out-domain evaluation generally exhibit an upward trend. Despite similar performance across variants under the same k, CLORA-major generally performs better, highlighting the advantage of preserving the well-learned major components (Wang et al., 2024a) of the base model.\nMath Setting Results on math setting are reported in Table 4. CLORA outperforms other baselines, and the optimal k value found to be around 128, differing from the 2048 in commonsense reasoning setting. This discrepancy can be attributed to the complexity of math tasks, which necessitate greater model capacity during finetuning. These findings suggest that adjusting k benefits the balancing of capacity and forgetting, demonstrating the flexibility of our approach.\nAnswer for RQ2: Results indicate that the choice of regularization matrix does influence the effectiveness of CLORA, albeit not significantly. Generally, we recommend using random initialization (CLORA-random) or initialization from major SVD components (CLoRA-major). Regarding the choice of k, as k increases, a trend of performance improvement followed by a decline is observed. Different tasks may correspond to different optimal k, we suggest that a smaller k could be used for more challenging tasks."}, {"title": "5.3 Understanding Capacity-Forgetting Balancing(RQ3)", "content": "To answer RQ3, we investigate the parameter of trained models to quantify the capacity-forgetting balancing issue.\nMeasuring Model Capacity and Degree of Forgetting Consider that catastrophic forgetting primarily arises from output changes caused by parameter updating, the greater the impact of these updates, the more severe the catastrophic forgetting may be. We measure the degree of forgetting with the relative scale of output change in the parameter level, to be specific, for updating matrix AW, with input x, the relative scale of output change (denoted as F) is defined as\nF(AW, x) =||\u2206Wx||/||x||\nWe highlight the role of x in the measurement of \"F\", as it reflects the real world use case. Specifically, we sample 100 text data from test set and collect input x for each parameter from the model forward pass.\nTo measure model capacity, we note that there is a gap between theoretical capacity of a model (Abu-Mostafa, 1989) and the practical outcome of the learned model. Therefore, we delegate the measurement of model capacity to the scale of the parameters in the learned model. Specifically, we measure the L2 norm ||AW|| for each updating parameter matrix.\nResults and Analysis We report the measurements averaged over all tokens and all updating parameters in Table 5. All models use LoRA rank r of 32 unless specified otherwise.\nThe \"reference\" row is computed using the LORA trained model, noting the output scale of original parameter W instead of AW. Compared with \"reference\" and \"LoRA\", the difference of \"F\" is not very far, suggesting that LoRA training indeed introduces significant output change, thus still prone to catastrophic forgetting.\nFor \u201cMiLoRA\u201d, although intuitively promising, without effective control during training, it did not mitigate catastrophic forgetting, as evidenced by both downstream evaluations (in section 5.1) and the similar \"F\" and AW with LoRA.\nFor LoRA with lower rank (r8 and r16), after training, with ||AW|| indicates the reduction of capacity, the scale of output change does not show a decrease. Although theoretically, reducing the rank of the update matrix can increase the dimension of the null space and help to reduce the scale of output change, results not show this case. This suggests that altering r may not a effctive way to alter forgetting.\nFor LoRA-L2, \"F\" indicates that it indeed mitigate forgetting, but in a large cost of capacity, demonstrated by the very small ||AW||.\nFor our proposed CLORA, \u201cF\u201d shows a significantly reduce the scale of output change, while a relatively larger ||AW|| is maintained. This indicates that CLORA minimizes catastrophic forgetting caused by large updates while having a subtle impact on model capacity. Thus we answer RQ3 that CLORA performs effectively on capacity-forgetting balancing."}, {"title": "6 Conclusion", "content": "In this paper, we introduce Controlled Low-Rank Adaptation(CLoRA), a simple yet effective parameter-efficient finetuning method for LLMs that mitigates catastrophic forgetting. We investigate the effectiveness of CLORA on commonsense reasoning and math reasoning tasks. Experiment results demonstrate that CLORA consistently outperforms previous LoRA-based methods on in-domain evaluations, highlighting the effectiveness of CLORA as a parameter-efficient finetuning method. Additionally, out-domain evaluations demonstrate that CLORA exhibits strong capabilities in mitigating catastrophic forgetting. Further investigation for model parameters indicates that CLORA effectively balances the trade-off between model capacity and degree of forgetting."}, {"title": "7 Limitations", "content": "There are still several limitations that we reserve for future work: 1) While we have noted that our proposed CLORA would benefit from the dedicated selection of regularization matrix to retain specific model abilities, we did not go further into this aspect. In this paper, we only explored the SVD initialization variant alongside the basic random initialization. 2) We delegate the measurement of model capacity and degree of forgetting to simple measurement of scale. Although these measurements reveal significant differences between CLORA and previous works, we believe that further investigation would aid in the design of methods with stronger capacity-forgetting balancing capability."}]}