{"title": "Invariant Graph Learning Meets Information Bottleneck for Out-of-Distribution Generalization", "authors": ["Wenyu Mao", "Jiancan Wu", "Haoyang Liu", "Yongduo Sui", "Xiang Wang"], "abstract": "Graph out-of-distribution (OOD) generalization remains a major challenge in graph learning since graph neural networks (GNNs) often suffer from severe performance degradation under distribution shifts. Invariant learning, aiming to extract invariant features across varied distributions, has recently emerged as a promising approach for OOD generation. Despite the great success of invariant learning in OOD problems for Euclidean data (i.e., images), the exploration within graph data remains constrained by the complex nature of graphs. The invariant features at both the attribute and structural levels, combined with the absence of prior knowledge regarding environmental factors, make the invariance and sufficiency conditions of invariant learning hard to satisfy on graph data. Existing studies, such as data augmentation or causal intervention, either suffer from disruptions to invariance during the graph manipulation process or face reliability issues due to a lack of supervised signals for causal parts. In this work, we propose a novel framework, called Invariant Graph Learning based on Information bottleneck theory (InfoIGL), to extract the invariant features of graphs and enhance models' generalization ability to unseen distributions. Specifically, InfoIGL introduces a redundancy filter to compress task-irrelevant information related to environmental factors. Cooperating with our designed multi-level contrastive learning, we maximize the mutual information among graphs of the same class in the downstream classification tasks, preserving invariant features for prediction to a great extent. An appealing feature of InfoIGL is its strong generalization ability without depending on supervised signal of invariance. Experiments on both synthetic and real-world datasets", "sections": [{"title": "1 Introduction", "content": "Graphs are ubiquitous in the real world, appearing as chemical molecules [1], social networks [2], and knowledge graph [3], to name a few examples. In recent years, graph neural networks (GNNs) [4-6] have emerged as a potent representation learning technique for analyzing and making predictions on graphs. Despite significant advancements, most existing GNN approaches rely heavily on the i.i.d. assumption that the distribution of test data is independently and identically distributed to the training data. Such an assumption, however, seldom holds in practice due to environmental asynchrony during data collection, leading to distribution shifts between the training and testing data. In these situations, GNN suffers from severe performance degradation, making graph OOD generalization a significant challenge in graph learning.\nIdentifying graph features that remain invariant across distribution shifts [7\u20139] is paramount in overcoming the graph OOD problem. Invariant learning assumes that invariant features sufficiently determine the label while spurious features are affected by task-irrelation environmental factors cross distributions [10]. Two critical conditions of graph invariance need to be guaranteed in this process [8, 11-13]."}, {"title": "2 Related Work", "content": "Invariant learning for graph OOD. Invariant learning [7,27-29] has been extensively explored to improve generalization performance on graph OOD scenarios, which learns robust representations withstanding distribution shifts. Growing research is concentrating on applying invariant learning strategies to tackle the problem of graph OOD generalization, such as optimization methods [10, 30-33], causal learning [8, 12, 20, 34-38], stable learning [18,19], and data manipulation [13-17,39,40]. Optimization methods design optimization objectives to make the model robust to the shifts in data distribution [10, 30-33]. Causal learning utilizes causal intervention to capture causal features for prediction and ignore non-causal features [8,12,20, 34-38]. Stable learning leverages sample reweighting to eliminate spurious correlation and extract stable features across different environments [18, 19]. Data manipulation [13, 14, 16, 17, 39, 40] such as dropEdge [15] randomly drops the edges of graphs to increase the diversity of data distribution. However, many of them overlook the intricacies of graph data and are limited by the lack of theoretical guarantees for direct application."}, {"title": "The information bottleneck theory and boundaries of mutual information", "content": "Existing research [41, 42] has delineated the paradigm of acquiring a robust representation through the lens of the information bottleneck theory [22]. This theoretical framework endeavors to optimize the mutual information between the derived representation and predictive outcomes while concurrently minimizing the mutual information between the representation and the original input. This selective process aims at preserving solely the salient information pertinent to the underlying task at hand. Within the context of distribution shifts [43], the pursuit of a robust representation via information bottleneck theory aligns with the extraction of invariant features [44-46]. Nonetheless, the direct computation of mutual information encounters formidable obstacles when dealing with high-dimensional continuous variables of graphs. Given that the precise calculation of mutual information often proves extraneous to the core objectives, models are tailored towards delineating the boundaries of mutual information [47] for optimization purposes. Poole [47] has expounded upon the intricate interplay between mutual information and its assorted lower bounds, wherein the contrastive loss mechanism emerges as a prevalent methodological choice."}, {"title": "Contrastive learning and OOD generalization", "content": "Contrastive learning [48-51] has achieved success in aligning representations by pulling together positive pairs and pushing apart negative pairs. Minimizing the contrastive loss can maximize the mutual information between positive pairs while maximizing that between negative pairs. Such a strategy ensures that the mutual information of inputs from the same class encapsulates information relevant to the target, as highlighted in [52,53], which is stable in the face of distributional shifts as the invariance. This approach thereby establishes crucial invariance necessary for prediction. Recently, the success of leveraging contrastive learning for domain generalization tasks [23,26,54,55] in the area of computer vision attracts researchers' attention in addressing OOD problems in the graph scenarios. For instance, CIGA [56] applies contrastive learning after decomposing the graph causality which contains the most information on labels. Unlike previous work, we utilize contrastive learning on the features after reducing redundancy to avoid including spurious features in the invariance. We encourage intra-class compactness and inter-class discrimination [25] with class labels to maximize the information for prediction. Besides, to fully extract the invariance from complex graphs, we implement contrastive learning from both semantic- and instance-level to maximize mutual information between graphs."}, {"title": "3 Preliminaries", "content": "3.1 Problem Formulation of graph OOD\nLet G and Y be the sample space and label space, respectively. We denote a sample graph by G\u2208 G with the adjacent matrix A and node feature matrix X. The bold G and Y are random variables for graphs and labels. Suppose that Dtr = {(G, Y)}e\u2208&tr and Dte = {(G, Y)}e\u2208&te be the training and testing dataset, where e donates the environment from training environment sets &tr and testing environment sets &te. The training and testing distributions are often inconsistent due to different environmental factors, i.e., P(Ge, Yele = e1) \u2260 P(Ge, Yele = e2) with e\u2081 \u2208 Etr and e2 \u2208 &te. A graph predictor f = \u03b8\u00b0 \u03a6 : G \u2192 Y maps the input graph G to the corresponding label Y \u2208 Y, which can be decomposed into a graph encoder \u03a6 and a classifier \u03b8."}, {"title": "3.2 Invariant Learning and Information Bottleneck Theory", "content": "The objective of Equation 1 is hard to optimize since the prior knowledge for test environments is not available during the training process. Recent studies [10, 11, 56] focus on invariant learning to solve the problem of OOD. The basic idea is that the variables z in the latent space Z can be partitioned into invariant and spurious parts. They define the invariant features zinv to be those sufficient for the prediction task [57] while the spurious features zspu be the task-irrelevant [8, 20] ones related to the environment e. We formulate it as below:\n$Y \\perp e | z_{inv}$, I(Y;zspu|Zinv) = 0, \nI(Y; Zinv) = I(Y; G)\nwhere I(;) represents the mutual information between two variables, $I$ denotes independence, Equation 2 denotes the invariance condition while Equation 3 denotes the sufficiency condition. Different from existing work based on data manipulation [14-17] and causal disentanglement [8,12,18-20], we take inspirations from the information bottleneck theory [41] for invariant learning, where the goal for information bottleneck is define by:\n$R_{IB}(0) = I(\\Phi(G); Y) - \\beta I(\\Phi(G); G)$."}, {"title": "3.3 Multi-level Contrastive Learning", "content": "In practice, the supervised signals for invariance are intractable to obtain. To facilitate invariant learning, multi-level contrastive learning can serve as a practical approximation to identify invariance from both instance and semantic levels. Specifically, instance level contrastive learning aims to maintain pairwise similarity within instances shared with the same labels in the classification task [51, 58, 59]. In light of this, the instances with the same label serve as positive samples and are pushed closer in the latent space. In contrast, instances with different labels are negative samples and separated apart in the latent space. Consequently, the objective is\n$L_{ins} = E -log\\frac{exp(z \\cdot z_+/\\tau)}{\\sum exp(z \\cdot z_+/\\tau) + \\sum exp(z \\cdot z_-/\\tau)}$\nwhere z, z+ and z_ denote the features of input graphs and their corresponding positive and negative instances, respectively.\nSemantics are category centers that represent semantic features for each category [26], obtained through methods such as clustering. Semantic-level contrastive learning aims to compactly embed instances around the corresponding semantic while also refining the semantic to better represent the"}, {"title": "4 Methodology", "content": "To satisfy the invariance and sufficiency conditions of invariant learning, we propose to extract invariant graph representations from the perspective of information bottleneck theory. In this section, we first adapt the goals of information bottleneck theory to invariant learning. Then we introduce a novel framework, called InfoIGL according to it, thus solving the graph OOD problem."}, {"title": "4.1 Rethinking Information Bottleneck Theory for Invariant Graph Learning", "content": "According to the information bottleneck theory for invariant learning in Section 3.2, we should minimize the mutual information I(\u03a6(G); G) to the lower bound I(Zinv; G) while maximizing I(\u03a6(G); Y) to"}, {"title": "4.2 Implementation of InfoIGL", "content": "Based on Equation 4, InfoIGL first optimizes the encoder with a redundancy filter which minimizes the mutual information I(\u03a6(G); G) by filtering out spurious features zspu. According to Equation 7 and 8, it maximizes the mutual information of graphs from the same class by multi-level contrastive learning as introduced in Section 3.3. Finally, InfoIGL"}, {"title": "4.2.1 Reducing Redundancy", "content": "We implement a redundancy filter to remove task-irrelevant information from graphs and minimize the mutual information I(\u03a6(G); G) in information bottleneck theory. The filter assigns minimal invariance scores for spurious features, which can be realized through attention mechanism [20,60]. Before applying the redundancy filter, we obtain the representations for graph nodes with GNNs first. We can build our framework on any GNN backbones, and here we take GIN [6] as an example, the node update module is defined as follows:\n$h_v^{(k)} = MLP^*((1 + \\epsilon ^{(k)}) \\cdot h_v^{(k-1)} + \\sum h_u^{(k-1)})$\nwhere MLP(\u00b7) stands for multi-layer perceptron, \u03f5 is a learnable parameter, hv and hu separately denote the representations of nodes v and u, N(v) denotes the neighbour nodes of v. Then we assign invariance scores to node v and edge (u, v) through the attention mechanism, which can be obtained as follows:\n$\\alpha_v = softmax(\\frac{Q_vK_v}{\\sqrt d_k}) V,$ \n$\\alpha_{uv} = softmax(LeakyReLU(MLP(h_u \\| h_v)))$ \nwhere $Q_v = h_v \\cdot W^Q, K_v = h_v \\cdot W^K, V_v = h_v \\cdot W^V$,  $W^Q, W^K, W^V$ are trainable parameter matrices, $d_k$ is K's dimension, || is the concatenation operation. After assigning invariance scores for nodes"}, {"title": "4.2.2 Maximizing Predictive Information", "content": "To maximize predictive information I(\u03a6(G); Y) and further optimize the redundancy filter, contrastive learning with supervised class labels provides a practical solution to maximize the mutual information between graphs from the same class based on Equation 7 and 8. To extract invariant features to the greatest extent, we implement multi-level contrastive learning at both the semantic and instance levels according to section 3.3.\nProjection Head. We consider applying contrastive learning in another latent space Zg that is mapped from h\u011c by a projection head [26, 49]. We employ a two-layer MLP fep(\u00b7) as the projection head:\n$z_G = f_{op}(h_{\\hat G}).$\nSemantic-level Contrastive Learning. Semantic-level contrastive learning forces the graph embeddings ZG to be closer to their corresponding category semantics, promoting both inter-class separation and intra-class compactness. For semantic-level contrastive learning, we first introduce the cluster center of each class as the corresponding category semantic. Formally, we initialize the semantic wc as the average semantic representation over"}, {"title": "5 Experiments", "content": "In this section, we conduct extensive experiments on multiple benchmarks to answer the following questions.\n\u2022 Q1: How effective is InfoIGL compared to existing methods for OOD generalization in graph classification tasks?\n\u2022 Q2: How do reducing redundancy and maximizing mutual information of graphs work respectively?\n\u2022 Q3: How do the different levels of contrastive learning impact InfoIGL's performance?\n\u2022 Q4: How sensitive is the model to the hyperparameters and GNN backbones?\n\u2022 Q5: Can this model be extended to tackle the graph OOD problem on node classification tasks?\nWe assess the efficacy of InfoIGL across various out-of-distribution (OOD) graph datasets for graph classification tasks in Q1. To address Q2, we perform an ablation study on the redundancy filter and multi-level contrastive learning. For Q3, we further conduct an ablation study examining the impact of semantic-level and instance-level contrastive learning, respectively. Additionally, we explore the sensitivity of InfoIGL to hyperparameters \u03bb\u03b5, \u03bbs, and \u03bb\u2081, as well as to different GNN backbones such as GCN, GIN, and GAT. To validate the scalability of InfoIGL, we extend its application to node classification tasks. Visualization techniques are employed to vividly illustrate the significance of InfoIGL in identifying invariant features."}, {"title": "5.1 Expermental Settings", "content": "5.1.1 Datasets and Baselines\nWe conduct experiments on one synthetic (i.\u0435., Motif) and three real-world (i.e., HIV, Molbbbp, and CMNIST) datasets designed for graph OOD [66,"}, {"title": "5.1.2 Implementation Details", "content": "Our code is implemented based on PyTorch Geometric. For all the experiments, we use the Adam optimizer, where the initial and the minimum learning rate are searched within {0.01, 0.001,0.0001} and {0.001,0.00001,0.000001}, respectively. We select embedding dimensions from {32, 64, 128, 300} and choose batch sizes from {64, 128, 256, 512, 1024}. The dropout ratio is searched within {0.1,0.3,0.5} while dc, \u03bbs, \u03bb\u00a1 are searched within {0.1, 0.2, ..., 0.9}. We adopt grid search to tune the hyperparameters and list the details of hyperparameters for InfoIGL"}, {"title": "5.2 Overall Results (Q1)", "content": "We train and evaluate our proposed InfoIGL, together with all the baselines, 10 times to obtain the average performance (mean \u00b1 standard deviation). It can be observed from Table 3 that optimization methods exhibit stable performance with moderate accuracy and low variance, while causal learning baselines show unstable performance with undulating accuracy and high variance. Besides, stable learning and data manipulation baselines perform relatively poorly compared to other baselines. Additionally, traditional graph contrastive learning methods can partially combat distributional shifts, but their effectiveness is not as strong as InfoIGL since they were not designed specially to extract invariant features. These observations indicate that almost all of the baselines have their limitations for graph OOD generalization. Our proposed framework, InfoIGL, achieves state-of-the-art performance on diverse datasets with low variance, outperforming the strongest baseline by 9.82% on HIV (size) and 12.22% on Motif (size). We conduct multiple tests with p-value< 0.05, demonstrating that the performance improvements of InfoIGL are statistically significant. The relatively weak performance of InfoIGL on HIV (scaffold) can be attributed to the Variability and complexity of molecular structures. These results demonstrate the effectiveness of InfoIGL in extracting stable and invariant graph representations on both concept and covariate shifts for graph classification tasks."}, {"title": "5.3 Ablation Study for Q2", "content": "To validate the significance of each task individually, we conduct separate ablation studies on the redundancy filter and contrastive learning. Specifically, we compare InfoIGL with two variations: (1) InfoIGL-R: which includes only the redundancy filter with attention mechanism, and (2) InfoIGL-C, which focuses solely on contrastive learning. As is shown in Table 4, InfoIGL-R outperforms ERM on most of the datasets, demonstrating the effectiveness of reducing redundancy. However, its performance falls short of ERM on Motif-size and HIV-scaffold, which means that the variance can not be identified accurately by compressing redundancy merely, underscoring the significance of optimization from contrastive learning. In contrast, InfoIGL-C yields poorer results than ERM on several datasets, such as Molbbbp and motif-size, shedding light on the negative impact of spurious features and the significance of compressing redundancy based on information bottleneck theory."}, {"title": "5.4 Ablation Study for Q3", "content": "We perform an ablation study to analyze the impacts of semantic-level and instance-level contrastive"}, {"title": "5.5 Sensitive Analysis(Q4)", "content": "To assess the sensitivity of InfoIGL to its hyperparameters, namely \u5165 for instance constraint and \u03bb and \u03bb\u00a1 for contrastive loss respectively, we conduct sensitivity analysis experiments by tuning these hyperparameters within the range of {0.1, 0.2, ..., 0.9} under the controlled experimental setting. Specifically, when adjusting a specific hyperparameter, we fix the remaining hyperparameters at the val-"}, {"title": "5.6 Scalability Analysis(Q5)", "content": "To extend our method on solving the OOD problems of node classification tasks, we follow the set-"}, {"title": "5.7 Visualization of the Invariance", "content": "To validate the effectiveness of our method in extracting invariant features from graphs of the same class, we visually examine a selection of randomly sampled graphs during training to illustrate the in-"}, {"title": "6 Time and Space Complexity Analysis", "content": "Let N denote the number of graphs, n denote the average node number per graph, lG, la, lp and dG, da, denote the numbers of layers and the embedding dimensions in the GNN backbone, attention mechanism and projection head, respectively, C denote the number of class and K denote the number of hard negative samples per instance. The time complexity of the GNN backbone is O(NnlGdg). For the attention mechanism, the time complexity is O(Nnlada). For the projection head, since it turns from node level to graph level, the time complexity is O(Nlpdp). For semantic-level contrastive learning, the time complexity is O(NC). For instance-level contrastive learning, the time complexity is O(NK). Therefore, the time complexity of the whole model is O(N(nlgdg + nlada + nlpdp) + C + K), and the order of magnitude is O(Nn). Similarly, the space complexity is also approximately O(Nn) which is about the same as baselines. In our experiment, we conduct a comparative analysis of the rrunning time between ERM, InfoIGL, and its variations (including InfoIGL-S and InfoIGL-I) on various datasets."}, {"title": "7 Limitations", "content": "7.1 Limitations\nInformal hard negative mining. There are several techniques for generating hard negative samples in machine learning. One approach is to choose negative samples that closely resemble positive examples by sampling from a pool of negatives. These selected samples can be relabeled as \u201chard negatives\u201d and included in the training process. Another method involves the use of sophisticated algorithms like online hard example mining (OHEM), which identifies challenging negative samples based on their loss values during training. However, instead of these methods, we select hard negative samples by computing the distance between the negative samples and the semantic center that corresponds to the positive sample. While this informal hard negative mining technique may conserve"}, {"title": "8 Conclusion", "content": "In this paper, we propose a novel framework, InfoIGL, to extract invariant representation for graph OOD generalization based on information bottleneck theory. To satisfy the invariance and sufficiency conditions of invariant learning, we compress redundant information from spurious features"}]}