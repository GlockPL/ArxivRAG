{"title": "Handling Numeric Expressions in Automatic Speech Recognition", "authors": ["Christian Huber", "Alexander Waibel"], "abstract": "This paper addresses the problem of correctly formatting numeric expressions in automatic speech recognition (ASR) transcripts. This is challenging since the expected transcript format depends on the context, e.g., 1945 (year) vs. 19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize and format numeric expression, such as years, timestamps, currency amounts, and quantities. For the end-to-end approach we employed a data generation strategy using a large language model (LLM) together with a text to speech (TTS) model to generate adaptation data. The results on our test dataset show that while approaches based on LLMs perform well on recognizing formatted numeric expressions, adapted end-to-end models offer competitive performance with the advantage of lower latency and inference cost.", "sections": [{"title": "1. Introduction", "content": "In the last decade, ASR systems improved tremendously in terms of word error rate (WER) due to more data, more computation power and better architectures [1, 2, 3]. These systems are normally trained with labeled ASR data, i.e., human transcribed speech or human correct automatically transcribed speech.\nThe way how numeric expressions are transcribed - using numeric literals, e.g., 1945, or number words, e.g., nineteen forty-five can vary between different datasets of sometimes even within a dataset. Furthermore, dependent on the usage of the ASR system, different transcript formats might be preferred. For example when a video conference call is automatically sub-titled using an ASR system, the readers might prefer numeric literals since they are shorter and easier to read.\nOn the other hand, transcripts of an ASR system containing numeric expressions should be formatted dependent on the context the numeric expressions occur in. For example the number word nineteen forty-five should be formatted as 1945 if it represents a year, as 19:45 if it represents a timestamp, as $19.45 if it represents a currency amount and as 1,945 if quantity is meant.\nOften numeric expression formatting is not reflected in the WER because numeric expression formats get normalized be-fore calculating the WER. However, proper formatting of numeric expressions is important because it heavily improves readability of the transcript.\nTherefore, in this work, we tackle the problem of prop-erly formatting numeric expressions in ASR transcripts. For this, we 1) created a test set containing the numeric expression types year, timestamp, currency amount and quantity, 2) propose a strategy using a LLM together with a TTS model to get synthetic data with which an end-to-end ASR system can be adapted (see figure 1 and section 3.1), and 3) compare cascaded and end-to-end approaches to recognize the numeric ex-"}, {"title": "2. Related Work", "content": "Until a few years ago, most ASR systems were trained to output lowercase transcripts without punctuation [4]. For this the transcripts of the training data got normalized. To get a transcript which contains casing and punctuation, inverse text normalization [5] was applied. This was done by applying a text segmentation model [6] after the transcription. For the text segmentation step auto regressive models similar to models used in machine translation can be used. To minimize the training-test mismatch in the input distribution such a text segmentation model should to be trained on hypotheses specific to some ASR model. Therefore, when changing the ASR model also the text segmentation model should be changed.\nThe lately introduced LLMs [7, 8, 9] can also be used as a text segmentation model, i.e. to reformat the ASR hypotheses. LLMs are pre-trained on a lot of text to predict the next token and then adapted to e.g. answer questions. In-context learning [10] can be used to increase the performance without changing the weights of the LLM by providing examples how questions should be answered.\nOn the other hand, ASR systems lately moved more and more towards end-to-end approaches where the transcript already contains casing and punctuation [3]. This has the advantage that only one model has to be executed decreasing latency and reducing maintenance effort. Furthermore, end-to-end approaches search for a global optimum and with enough training data this works well [3]. The drawback is that the formatting of numeric expressions in the transcript can not be easily changed"}, {"title": "3. Experiments", "content": "3.1. Data\nTo adapt and test the numeric expression formatting of our mod-els (see 3.2) we created a numeric expression dataset (see figure 1).\nFor this, we first used gpt3.5-turbo from OpenAI to gener-ate sentences containing the different numeric expression types we consider written down as number words. This is done by a prompt like (the actually used prompt is a little more complex e.g. to make the LLM not output enumeration)\nGenerate {n} diverse [German (optional)] sen-tences containing a {numeric expression type}\nwritten down using number words.\nWith half of the executed prompts we generated English sen-tences and with the other half German sentences.\nThen, we used the TTS model tts-1-hd from OpenAI (with voice randomly chosen) to generate audio. For this it was crucial to have the numeric expressions transcribed as number words since the TTS model did not produce correct output using numeric literals e.g. $19.45.\nThird, we prompted the LLM to convert the number words to numeric literals in the wanted format (see table 1). This is done by a prompt like\nConvert the {numeric expression type} in the sen-tences to numeric literals.\nThe output of this step is used a labels for the utterances."}, {"title": "3.2. Models and Approaches", "content": "We compare cascaded and end-to-end approaches for numeric expression formatting (see figure 2). For the cascaded approach we use a trained ASR model and reformat the output using a text segmentation model. For the end-to-end approach we adapt the trained ASR model by fine-tuning on the training set of our numeric expressions dataset (see section 3.1).\nWe use Whisper [3] (whisper-large-v2) as our baseline ASR model and for the text segmentation model we compare using a mbart-based model [14] (mbart-large-50) and LLMs."}, {"title": "4. Results", "content": "The results can be seen in table 3 (WER) and table 4 (accuracy of the different numeric expression types).\nWe see that ASR + mbart baseline slightly improves the WER on the Commonvoice test sets due to the learned correc-tion of the ASR hypothesis. However, the performance (WER and accuracy) on the numeric expressions test sets heavily de-creased since the model was not trained to predict numeric lit-erals.\nThe model ASR + mbart numeric expressions performs better and outperforms the ASR only model on the numeric expres-sions test sets, while there is not much difference on the Com-mon voice test sets. However, the model struggles to format the timestamps (and currency amounts) correctly, e.g., the ASR hy-pothesis \"The library opens at 10 o'clock, but it's best to arrive early.\" is converted to \"The library opens at 17:00, but it's best to arrive early.\" This is probably due to the limited amount of numeric expressions data. Using more data did help a bit but the accuracy on e.g. timestamps is still less than 40%.\nUsing a LLM as text segmentation model sometimes (gpt3.5-turbo: 9.7%, gpt4-turbo: 1.4%, gpt-40: 2.7%) does not follow the prompt, e.g., when the input sentence is a question, it is answered. This leads to a completely different transcript and increases in the WER. To circumvent this problem we compute the WER between input and output of the LLM and if the WER is larger than a threshold (0.5) we ignore to LLM output and return the input instead. With this, the LLMs clearly outper-form the mbart-based model both in terms of WER and accu-racy. The advantage of the LLMs is that they got trained on a lot more data. Most errors are caused by the LLM not follow-ing the prompt, e.g., in the sentence \"The bus leaves at five past seven.\" the timestamps is not changed to 7:05. Furthermore, the most recently published LLMs perform better. Using an LLM which follows the prompt better would probably yield a bit bet-ter scores.\nIt is quite expensive to reformat each hypothesis using an LLM (\u2248 $15 for evaluating the ASR + gpt-40 approach on the 4.000 Common voice test sets sentences), especially if the goal"}, {"title": "4.1. Limitations", "content": "The main limitation for the cascaded approach using an LLM is the ability of the LLM to follow the prompt correctly. This is expected to be handled even better for newer LLMs getting trained. For the fine-tuned ASR model the limitation is getting diverse data containing suitable numeric expression formatting.\nWe also tried adapting the ASR model using batch weight-ing [16] and/or a factorization-based approach [2] together with the common voice training dataset. While the performance on the Common voice test sets improved, which is expected since the training and test datasets are more similar, the performance on the numeric expression formatting was slightly worse. Furthermore, we tried freezing the encoder or only adapting the final projection layer during the adaptation with the numeric expressions data. For both, the performance on the numeric expressions test data was slightly worse compared to not freezing any weights."}, {"title": "5. Conclusion", "content": "In this paper, we tackled the problem of correctly formatting numeric expressions in ASR transcripts. Our experiments revealed that LLMs, particularly the latest models, deliver strong perfor-mance in recognizing and formatting numeric expressions. On the other hand, end-to-end models adapted with synthetic ASR data provide competitive performance."}]}