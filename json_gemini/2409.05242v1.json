{"title": "FedFT: Improving Communication Performance for Federated Learning with Frequency Space Transformation", "authors": ["Chamath Palihawadana", "Nirmalie Wiratunga", "Anjana Wijekoon", "Harsha Kalutarage"], "abstract": "Communication efficiency is a widely recognised research problem in Federated Learning (FL), with recent work focused on developing techniques for efficient compression, distribution and aggregation of model parameters between clients and the server. Particularly within distributed systems, it is important to balance the need for computational cost and communication efficiency. However, existing methods are often constrained to specific applications and are less generalisable. In this paper, we introduce FedFT (federated frequency-space transformation), a simple yet effective methodology for communicating model parameters in a FL setting. FedFT uses Discrete Cosine Transform (DCT) to represent model parameters in frequency space, enabling efficient compression and reducing communication overhead. FedFT is compatible with various existing FL methodologies and neural architectures, and its linear property eliminates the need for multiple transformations during federated aggregation. This methodology is vital for distributed solutions, tackling essential challenges like data privacy, interoperability, and energy efficiency inherent to these environments. We demonstrate the generalisability of the FedFT methodology on four datasets using comparative studies with three state-of-the-art FL baselines (FedAvg, FedProx, FedSim). Our results demonstrate that using FedFT to represent the differences in", "sections": [{"title": "1. Introduction", "content": "Federated Learning (FL) plays a crucial role in advancing decentralised Artificial Intelligence (AI) training, as it allows for the training of Machine Learning (ML) models on distributed clients (i.e. edge devices) without centralising sensitive data. Training of ML models on client data without transferring them to a central server significantly reduces the risk of privacy violation and ensures better compliance with regulations such as the GDPR [1]. FL seamlessly integrates with distributed systems, emphasising distributed processing and enhanced data privacy. This method trains machine learning models directly on edge devices, cutting down bandwidth needs and significantly improving privacy measures [2]. The prevalence of FL applications, such as voice assistants, Internet of Things (IoT) devices and mobile apps are rapidly increasing [3, 4, 5, 6]. Other areas such as privacy-sensitive ML applications in healthcare for disease diagnosis and treatment are also ideal domains for FL [7, 8, 9]. Another notable industry adapting FL is in finance, where models can be trained without shared access to private information for credit risk assessment and anti-fraud detection [10].\nCentral to FL is its distributed decentralised training of a shared global model with many communication exchanges between the server and its clients. At each communication round, the shared model is updated by the server as an aggregation of client models received. The FL communication layer needs to handle many requirements due to its iterative nature which involves frequent and large model exchanges. Inefficient communication can slow down training, increase computational cost, decrease accuracy, raise energy consumption and limit scalability [11]. Compression can be used to improve communication performance by reducing the amount of data being transmitted. For instance, ML applications can optimise storage and inference speed"}, {"title": "2. Background and Related Work", "content": "The exponential growth of edge devices and their applications, including the IoT, smart home assistants, mobile apps, and wearables, has led to a substantial increase in data creation at the edge of the network. This increase of data (i.e. big data) offers considerable benefits for customisation and real-time analytics but also introduces significant risks to user privacy. Accordingly, the demand for distributed machine learning strategies, FL and distributed computing architectures has intensified, becoming more crucial than ever. In practical applications FL operates by training models on local data directly on client/edge devices, emphasising privacy and minimal data transfer [17]. In contrast, distributed architectures focus on processing data locally at the edge of the network first and if required communicate it to a central cloud for further processing. The key similarity between FL and distributed computing lies in their reliance on local computation to reduce bandwidth usage. Both strategies are designed to efficiently manage data where it is generated, thereby reducing network resources and enhancing communication efficiency [2]."}, {"title": "2.1. Communication Efficient FL", "content": "Communication cost is a primary bottleneck for FL systems [18, 19, 20]. This is because FL requires high-frequent communication of model parameters between clients and the server, where the number of clients can be in the millions [21, 22]. The size of these neural models can vary greatly, from a mere few kilobytes to several hundred megabytes depending on their complexity. The communication bottleneck in FL systems can lead to unreliability and limit their ability to scale up and meet increasing demands. In a realistic setting, there can be clients with poor network connections or with resource limitations which can further hinder the performance of the FL system. Methodologies like FedProx [23] have addressed this issue to handle partial updates from clients having limited connectivity. More generally, the approaches in literature aimed at mitigating this communication bottleneck can be studied under two groups: structured updates - where the local training and communication is done in a restricted space (e.g. restricted to a fewer number of model parameters); and sketched updates - where the local update is performed on the complete model and compressed for communication [11]. Both approaches have advantages and disadvantages, however, the efficiency of communication in updates performed through sketches is often more adaptable and can be easily integrated with existing FL techniques. We employ the sketched update approach to enable local client updates to be performed in either tensor or compressed frequency space, with communication and federated aggregation performed in compressed frequency space."}, {"title": "2.2. Compression for Communication Efficiency", "content": "Compression techniques of FL models in tensor space include sub sampling (which reduces spacial resolution) and probabilistic quantisation (which reduces precision) [11]. The aim of our work is closer to [15], where they compress model parameters using Golomb Coding and reduce model complexity through quantisation. Golomb Coding is a lossless compression technique, but its non-linear nature means that additional transformations must be performed at the server incurring extra reconstruction steps. This is because it cannot perform federated aggregation in the compressed space. Also, the quantisation is tightly coupled with the client local update making the work by [15] less adaptable by existing FL methodologies. Figure 2 illustrates applying a standard compression algorithm in FL. This approach involves multiple stages of model compression and decompression to reduce the communication overhead between the server and the clients. The increased"}, {"title": "2.3. Pruning and Quantisation for Communication Efficiency", "content": "Pruning and quantisation are two methods used to optimise and simplify ML models [12, 29, 18]. Pruning removes elements that carry less significant"}, {"title": "2.4. Aggregation Techniques in Federated Learning", "content": "The fundamental principle of FL revolves around the concept of federated aggregation. This process involves merging local model updates from distributed clients to construct a unified global model, a step of critical importance in diverse settings such as distributed architectures. Federated Averaging (FedAvg) is the conventional FL algorithm which was introduced by [33]. The aim of FedAvg is to create a effective global model with wider coverage from the participating clients. This is achieved through a weighted aggregation approach, where the influence of each client on the final model is proportionate to the size of its data. In practical terms, this means that clients with larger datasets have a greater impact on the FL system, a crucial consideration in distributed environments where data volume can vary significantly among clients. FedAvg effectively harnesses the collective data wealth of all participating nodes, leading to a more holistic and representative global model in diverse FL scenarios.\nAnother state-of-the-art FL methodology is the FedProx algorithm, introduced by [23], offers practical solutions for handling the challenges of system and statistical heterogeneity in FL. These challenges are particularly evident in distributed computing and distributed machine learning scenarios. FedProx is effective in environments where clients vary in computational power"}, {"title": "3. FedFT Methodology", "content": "FL is a distributed machine learning approach where models are trained locally on individual devices or nodes, and only aggregated model updates are shared, preserving data privacy and minimising communication costs. It enables collaborative learning without centralised data collection. The general FL process begins at round, t = 0, with a server distributing an initial global model, wo, to all participating clients. At each communication round t, the server selects K clients to participate in training. Clients perform local training, and once complete, each client, k, communicates their model $w_{t+1}^k$ to the server. These models are aggregated as in Equation 1 to form the global model at t + 1. This is repeated for multiple communication rounds. The Federated Averaging algorithm FedAvg [33], computes a weighted average of locally trained client models, where the weights are determined by the size of each client's data nk.\n\nThe aim of FedFT is to improve communication efficiency in FL through the utilization of the frequency space transformations on the model parameters. The overall FedFT, client and server communication setting is presented in Figure 3. The original FedAvg phases are shown as grey-filled rectangles, i.e., the initialization, local update, and federated aggregation. Here, Steps 2 and 7 refer to downstream and upstream communications forms. Contributions of FedFT are the blue-filled rectangles. The blue and grey communication lines differentiate steps in relation to FedFT and FedAvg respectively."}, {"title": "3.1. Global Model Initialisation", "content": "The first step in Figure 3 is the initialisation of the global model, wo, at t = 0, which is common to both FedFT and FedAvg. Additionally, FedFT converts, wo, into the frequency space using a transformation function, T, to obtain \u0175o, which is communicated to all clients. FedFT can be applied even if the initial global model is pre-trained, such as a language model [38] or transferred from another domain [39], by converting the pre-trained weights into the frequency space."}, {"title": "3.2. Communication", "content": "The communication of model parameters in FL happens in two directions: from server to client (downstream) and from client to server (upstream). In both cases, FedFT communicates model parameters in the frequency space using DCT-IV, a linear lossy function which is further discussed in Section 4."}, {"title": "3.3. Client Local Update", "content": "Local update for a supervised task typically employs stochastic gradient descent (SGD) over a number of epochs using local training data (Step 4 in Figure 3). In FedAvg this local update is applied to the model received through downstream communication from the server. With FedFT, the downstream communications of the initial and follow-on models, wo and wt; are communicated in the frequency space, as transformed models, \u0175 and \u0175t; accordingly, an additional step of inverse transform, \u00ce(.), is required, where T(.) reconstructs the model parameters from the frequency space to tensor space where local model updates can take place. We acknowledge the possibility of performing these updates in the frequency space, as referenced in [12]. However, we have chosen to maintain our approach, which helps to evaluate communication efficiency in isolation and enables us to assess FedFT on a diverse set of federated methodologies (FedAvg, FedProx and FedSim) and neural models, all of which commonly operate in tensor space. In our research, we examine two methods for representing locally updated models prior to transforming them into the frequency space (using T(.) for upstream communication to the server in Step 5 of Figure 3. In the figure these alternative routes are labelled as (A) and (B) and refer to the following:"}, {"title": "3.3.1. Difference model (A)", "content": "The purpose of this method is to capture only the net changes from local training, as the server can update the global model by adding these differences to its existing version, thus efficiently reconstructing the complete model. Where updated local model parameters $w_{t+1}^k$ are compared against the received global model $w_t$ and the differences (${\\Delta}w_{t+1}^k = w_{t+1}^k - w_t$) are transformed into the frequency space and communicated to the server. This is similar to the FL methodologies where client model update differences are communicated to the server [15], except we do so in the frequency space."}, {"title": "3.3.2. Complete model (B)", "content": "If the objective is to conserve computational resources on the server when handling incoming updated models, opting to send the complete model is advantageous. However, this involves sending more parameters from each client which restricts the potential for compacting the models for efficient communication. Where $w_{t+1}^k$ is transformed into the frequency space and communicated to the server. This is simply the general FL methodology from [33].\nWe present the case for why ${\\Delta}\\hat{w}_{t+1}^k$ (difference model) is a more favourable choice compared to $\\hat{w}_{t+1}^k$ (complete model) in Section 4."}, {"title": "3.4. Pruning of Model Parameters", "content": "Pruning allows FL to operate at varying levels of compression, thereby improving the efficiency of upstream communication. With FedFT, we have the option to implement pruning at Step 6 in Figure 3, i.e. after performing the DCT transformation but before the upstream communication (Step 7). The parameters pruned are the least significant coefficients of the updated client model in the frequency space (either $\\hat{w}_{t+1}^k$ or ${\\Delta}\\hat{w}_{t+1}^k$). In the case of FedFT, pruning on DCT coefficients results in lossy compression where it approximates and discards some of the less significant frequency coefficients. Optimized compression with DCT is possible when a significant amount of the model parameters are captured within low-frequency coefficients.\nPruning then becomes an effective technique where the magnitudes of a specified percentage of high-frequency coefficients are set to 0 while minimizing the reconstruction error. This is because, the high-frequency coefficients often correspond to features with high variance, i.e. noisy information. The pruning function and pruning percentage are referred to as P(.) and \u03b1. Pruning can be applied once convergence is close, at which point most of the"}, {"title": "3.5. Federated Aggregation", "content": "A linear transformation function such as DCT-IV is useful for performing federated aggregation in the frequency space. If the transformation was non-linear this would require additional inverse transformations at the server to reconstruct the models in tensor space before federated aggregation can be performed and transformed thereafter for downstream communication. The use of DCT as the T(.) function enables FedFT to carry out its aggregation in the frequency space. It can do so with either the difference models (see Equation 2 with ${\\Delta}\\hat{w}_{t+1}^k$) or complete models (see Equation 3 with $\\hat{w}_{t+1}^k$) based on the selected approach for the local update step."}, {"title": "3.6. FedFT Algorithm", "content": "Algorithm 1 brings together the extensions proposed with FedFT. Line 1 performs the initial global model transformation into the frequency space, once received by clients each performs the inverse transformation in Line 6, prior to carrying out the local update. Once completed, the client calculates the ${\\Delta}w_{t+1}^k$ (Line 8), and performs the frequency space transformation and pruning with the percentage of pruning controlled by \u03b1 (Line 9). Once the client models in the frequency space ${\\Delta}\\hat{w}_{t+1}^k$ are communicated to the server, it performs federated aggregation on the updated local models in the frequency space.\nIn the algorithm areas highlighted in blue text signify the specific modifications we have implemented to adapt our proposed method to the vanilla FedAvg methodology."}, {"title": "4. Role of Model Variance for transformed communication", "content": "Based on an analysis of literature (see Section 2), we select DCT as the transformation technique to convert w into the frequency space. Where a given set of model parameters, w is a multi-dimensional array (i.e. a tensor) where the number of dimensions depends on the model architecture. Out of the DCT variants, DCT-IV is selected due to its linear, orthogonal and symmetric properties required for inverse transformations and necessary for federated aggregation.\nEquation 4 presents the DCT-IV transformation function T(.) for w represented in a tensor space of $R^{N\\times M}$, where k \u2208 {0,..., N \u2013 1} and l\u2208 {0,..., M \u2013 1} respectively.\nWithout loss of generalisability, w represents a set of model parameters between two fully connected layers of a neural architecture. With multi-dimensional tensors, beyond just 2-dimensions, the summations can be extended over the additional dimensions.\nEquation 5 is the inverse function \u00ce(.), where n\u2208 {0,..., N \u2212 1} and m\u2208 {0, . . ., \u041c \u2013 1}.\nAccordingly, the reconstruction loss is calculated as |\u00ce(T(w)) \u2013 w|.\nThe distribution of the tensor space directly impacts the magnitude of the DCT coefficients and the way they are distributed. This in turn affects the level of pruning possible to manage reconstruction error after the inverse transform [40]. We observe the distribution of the tensor space conforms to a Gaussian distribution which can be expressed using mean and variance (Figure 5). Accordingly, the variance of model parameters, $w^k$ \u2208 $R^{N\\times M}$, for any given round is calculated as in Equation 6, where $\\bar{w}^k$ indicates the mean of model parameters."}, {"title": "5. Experiment Setup", "content": "We evaluate the performance of FedFT, with respect to three important aspects. First, its generalisability to existing FL baseline methodologies. Second, we investigate its applicability to various complex neural architectures. Finally, we analyse the impact of pruning with FedFT on performance and communication efficiency."}, {"title": "5.1. Datasets", "content": "The generalisability of FedFT is evaluated with four real-world datasets consisting of two image datasets, one time-series dataset and one text dataset all of which perform multi-class classification. We reuse the MNIST and FEMNIST datasets, which were originally introduced in the work of [23]. Additionally, we utilise the Fed-Goodreads and Fed-MEx datasets, which were initially introduced in the work of [34]. To ensure compatibility with a realistic non-IID (non-independent and identically distributed) setting, all the datasets used in our study enforce statistical heterogeneity by restricting the number of classes per client. This approach guarantees that the proposed FedFT methodology accommodates diverse and realistic scenarios.\n\u2022 MNIST is a handwritten digit recognition dataset adapted to the FL setting as proposed in [23]. The dataset contains 69,035 data instances of hand written digits of 10 classes distributed among 1000 clients and each client has samples for only 2 classes. A data instance is an image of size 28\u00d728.\n\u2022 FEMNIST (Federated-Extended-MNIST) is a handwritten character recognition dataset from [41]. The subsample consists of 10 lowercase characters (a-j) and is used for a 10-class character classification task. This dataset is distributed among 200 clients, with each client having samples for only 3 classes. Each data instance in this dataset is an image with dimensions of 28x28.\n\u2022 Fed-MEx is adapted to the FL setting in [34] from MEx which is an exercise recognition dataset collected with 30 subjects performing 7 different physiotherapy exercises [42]. The dataset has 934 data instances from a pressure mat where an instance is a sequence of heat maps (size 5) recorded for 5 seconds at 1Hz. Each client has a random amount of samples for only 2 exercise classes.\n\u2022 Fed-Goodreads is extracted from the book review dataset Goodreads and transformed to the FL setting in [34]. It contains 100 clients each with 2-10 of their own reviews which emulates a heterogeneous FL setting. The task is to predict if a text review contains a spoiler or not."}, {"title": "5.2. Baselines", "content": "Selected from widely-accepted FL methodologies, excluding those specific to an application, dataset or model type:\n\u2022 FedAvg: general FL methodology from [33].\n\u2022 FedProx: variant of FedAvg focused on improving stability and performance in non-IID settings using regularisation in client update from [23].\n\u2022 FedSim variant of FedAvg that performs clustered federated aggregation based on latent similarity knowledge between clients [34].\nSimilar to the approach in Algorithm 1, where FedFT was implemented with FedAvg, the adaptations of FedFT for FedSim and FedProx are described in Appendix A. Algorithms 2 and 3 detail the application of FedFT within the FedSim and FedProx methods, respectively."}, {"title": "5.3. Summary of Experiments", "content": "In Table 1, we present a comprehensive summary of the experiments carried out in this study. Detailed descriptions of each experiment are provided in the corresponding subsections. The table showcases the range of datasets, baseline methodologies and neural architectures employed in our experiments. FedFT and baseline methodologies were implemented using Python with Tensorflow [43] libraries extending the setup from FedProx and the source code is available on GitHub\u00b9."}, {"title": "5.4. Generalisability of FedFT", "content": "To study the generalisability of FedFT, we adapted the baseline FL methodologies to FedFT and compared against their original form. The comparison of FL methodologies adapted for FedFT is carried out with a Multinomial Logistic Regression (MLR) model trained for classification. We"}, {"title": "5.5. Generalisability to Neural Architectures", "content": "FedFT algorithm transforms model parameters to the frequency space using multi-dimensional DCT. Accordingly, applicability of FedFT to different neural architectures that are of different dimensions is key to generalisability. We evaluate this with the three most commonly used neural architectures: Multi-layer Perceptrons (MLP); Convolutional Neural Networks (CNN); and Recurrent Neural Networks (RNN). The dimensions of the model parameters are summarized in Table 2."}, {"title": "5.6. Impact of FedFT Pruning", "content": "FedFT applies pruning to improve communication efficiency which is lossy and can impact overall performance. Accordingly, we explore the performance impact of pruning with MLR models trained on four datasets with increasing \u03b1 rates. We explore two variants of pruning: one applied from the start of communication (round=0) and the other applied after the model has converged (round~50). In each case, we compare different pruning rates where \u03b1 is varied from 0% (no pruning) to ~ 50% in increments of ~ 10%. The actual percentages for MLR models depend on the output layer size; for example on Fed-MEx, where |\u0175| = [1280, 7], \u03b1 =~ 14%, ~ 29%, ~ 43% and ~ 57% for when 1,2,3, and 4 weights are set to 0 in each of 7 weights. Each experiment plots the test accuracy over communication rounds. Furthermore, we evaluate how pruning impacts communication efficiency by plotting the cumulative communication cost in MegaBytes (MB) over 200 rounds for each dataset. This is repeated for all values of \u03b1 to determine the optimal value that can maintain test accuracy (as close to accuracy with no pruning i.e., when \u03b1 = 0) while minimising the cost in MB."}, {"title": "5.7. Analysing the Impact of non-IID on FedFT", "content": "This experiment focuses on evaluating the influence of non-IIDness on the effectiveness of the proposed FedFT method. The datasets utilised in the FedFT experiments are carefully selected to reflect their realistic non-IID nature. These datasets are chosen based on previous research in FL, as discussed in Section 5.1. In this analysis, we employ the FEMNIST dataset as our core dataset. To evaluate the impact of FedFT across varying degrees of non-IID, we purpose three variants of the FEMNIST dataset: FEMNIST(1) with one class per client, FEMNIST(2) with two classes per client, and FEMNIST(3) with three classes per client. The default configuration of the FEMNIST dataset used for primary experiments typically consists of three classes per client."}, {"title": "5.8. Performance metrics", "content": "In selecting all hyper-parameters, we prioritised ensuring comparability and reproducibility with [23] and [34]. Hyper-parameter details are summarised in Table 3. The primary performance metric is the test accuracy of the global model against individual client test data, adapting the evaluation setup from [23]. The test accuracy at any given round is the mean of all"}, {"title": "6. Results and Discussion", "content": "In this section, we conduct a detailed analysis of the experimental results and provide a discussion of the findings."}, {"title": "6.1. Comparing Frequency Transformation Methods", "content": "We aim to optimise the function T(.) for efficient communication in FL. To do this, we compare two well-known frequency transformation methods: DCT and FFT. These methods are key for transforming model parameters into frequency space for FedFT as discussed in Section 3. We designed an experiment to test how well DCT, particularly DCT-IV, works compared to FFT in FL settings. Our comparison looks at important factors for FL, including compression efficiency, information retention, and impact on the convergence rate of the learning process. We conducted this experiment on the MNIST dataset, applying the FedAvg baseline across 200 communication rounds. To ensure statistical robustness, we averaged the results over 35 separate runs, each initialised with a unique random seed.\nAs illustrated in Figure 8, our results demonstrate a notable performance differential between the two methodologies. DCT-IV emerges as a superior"}, {"title": "6.2. Comparison of Different Variants of DCT", "content": "Evaluating the impact of various DCT variants is crucial as each variant has distinct characteristics and applications. This experiment is designed to discover which DCT variant is most suitable for our specific needs with FL. In Figure 9, we present a comparative analysis of four DCT variants, identified as DCT-I through DCT-IV.\nThis experiment was conducted using the MNIST dataset, comparing the FedAvg baseline with our proposed FedFT algorithm across 200 communication rounds, averaging the results across 35 unique runs. This comparison is crucial in understanding how each variant handles the transformation and compression of model parameters. The results reveal a notable divergence in performance among these variants. Specifically, DCT-I and DCT-IV stand out for their efficiency, with lower reconstruction errors and indicating an accurate representation of the original model parameters. In contrast, DCT-II and DCT-III, while effective in their respective applications, show less favourable results in our context. Their performance, characterised by higher reconstruction errors which suggests not suitable to handle FL model parameters."}, {"title": "6.3. Generalisability of FedFT", "content": "Our primary experiments focus on assessing the generalisability of the proposed FedFT method. Following the setup outlined in Section 5.4, we evaluate the efficacy of FedFT across four datasets, comparing it with three state-of-the-art FL baselines. Figure 10 presents test accuracy results for increasing communication rounds with three FL methodologies, both with FedFT (solid line) and without FedFT (dotted line), across four datasets. Overall, FedFT adaptations match the performance of baseline counterparts at convergence, demonstrating that efficient communication of model parameters in frequency space does not compromise performance. The noticeable performance difference in the rounds prior to convergence across all methodologies on the Fed-MEx dataset is attributed to the small number of participating clients and their data sizes (30 total clients and 10 selected per round). With fewer clients who have fewer samples, each local update makes larger weight adjustments (high variance) resulting in significant changes to the global model.\nAs discussed in Section 4, high variance results in high reconstruction error and evidently affects the model performance before convergence. The only"}, {"title": "6.4. FedFT with Different Neural Architectures", "content": "To further understand the adaptability of FedFT, we next explore its impact on different neural architectures. A summary of the architectures and the details of the experiment setup was described in Table 2. In Figure 11, we present a comparative analysis of FedFT and FedAvg when applied on different neural architectures.\nOverall, FedFT demonstrates comparable or superior performance compared to FedAvg. For instance, the CNN model trained on MNIST has approximately 6.5 million parameters that are transformed between the tensor space and the frequency space at each communication round without affecting overall performance. Similar performance is observed with Fed-MEx which trains a DNN architecture. The RNN model trained on Fed-Goodreads with FedFT shows a drop in performance in early communication rounds, however, it improves and surpasses FedAvg performance after round ~ 150. We attribute this improved performance to the imperceptible reconstruction error in DCT-IV that is present even at 0% pruning which reduces noise for the federated aggregation."}, {"title": "6.5. Effect of FedFT on Computation Overheads", "content": "Understanding the computation overheads is essential for FL methods, particularly in environments with limited computing resources. To assess the impact of FedFT, we compared it against baseline methods over 100 communication rounds across all the datasets. The results showed that FedFT requires up to a 6% increase in resources compared to FedSim and FedAvg. However, this overhead is less than 5% when compared to FedProx. In our setup with a 1.7 GHz Quad-Core CPU, a 6% increase amounted to an additional 0.03 seconds of computation time. This increase is relatively insubstantial when weighed against the benefits that FedFT offers. Therefore, the slight increase in computation can be neglected when weighed against the enhanced communication efficiency it provides, saving network resources and overall efficiency."}, {"title": "6.6. Analysing the Effect of Non-IID on FedFT", "content": "FL environments inherently support and often require the handling of non-IID data due to their distributed nature. This experiment is focused on evaluating how FedFT performs under different levels of non-IID data. Figure 12 illustrates the outcomes obtained from the three FEMNIST variants, representing varying levels of non-IID, when applied to the FedAvg baseline. In the figure, two types of lines are used to represent the results. The solid lines show how FedAvg, combined with FedFT, performs. In contrast, the dashed lines show the performance of the standard FedAvg method. The presented plots depict the average results obtained from 35 independent runs with random seeds conducted over 500 communication rounds. Our observations indicate that, in the experiment, FedFT consistently maintains comparable performance across all levels of non-IID. Additionally, we note that any initial decrease in performance seen in FEMINST(2) and FEMNIST(3) gradually recovers in the later rounds. Additionally, it is essential to highlight that the overall performance of FedAvg in the FEMINST(1) dataset is comparatively weaker, requiring more communication rounds for convergence compared to the baseline FedAvg.\nHowever, we observe that FedFT can catch up and follow a similar convergence trend in this extreme non-IID scenario. FedFT still shows a consistent"}, {"title": "6.7. Impact of FedFT Pruning", "content": "The ability to compress model parameters using pruning or quantisation (such as with JPEG images and video streaming) is a crucial aspect of communication in the frequency space. We examined the extent to which pruning can compress while preserving performance. Figure 13 presents test accuracy with increasing values of the pruning rate \u03b1 for each dataset. As expected, accuracy suffers with higher values of \u03b1. This poor performance is mostly evident for pruning with \u03b1 > 20%. Note that the model's inability to overcome the negative impact of high pruning on its performance prior to convergence results in a sub-optimal test accuracy post-convergence.\nHowever, it is encouraging to observe that at lower levels of pruning, comparable performance to no-pruning is achieved. This suggests that there is a sweet-spot where pruning can achieve comparable or in some cases better accuracy than no-pruning. For instance, test accuracy with \u03b1 = 10%, 10% and 14% is comparable to \u03b1 = 0% with MNIST, FEMNIST and Fed-MEx datasets respectively. The most favourable outcomes with pruning are observed in Fed-Goodreads, where \u03b1 = 50% yields performance comparable to"}, {"title": "6.8. Communication Efficiency with FedFT", "content": "To study the communication efficiency, we plot upstream communication costs in Figure 14. Here, a single trend line of a plot shows the test accuracy values measured at a particular communication round (coloured lines). The x-axis is the accumulated upstream communication cost per client in MB measured on different \u03b1 indicated by the markers.\nFirstly, Figure 14 confirms the general finding in Figure 13 that accuracy with pruning in the range, 0 < \u03b1 < 20%, is comparable to no pruning"}, {"title": "6.9. Impact of FedFT Pruning on FedSim", "content": "Building upon the analysis in Figure 13, we further explore the balance between pruning and performance retention, specifically within the FedSim [34] aggregation methodology. Figure 15 presents test accuracy with increasing values of the pruning rate \u03b1 for each dataset with FedFT applied on FedSim. We note that at a pruning rate of \u03b1 = 10% (Fed-MEx: \u03b1 = 14%), FedFT achieves comparable performance, enhancing communication efficiency.\nAs expected, the accuracy declines with higher values of \u03b1. However, it is significant to observe that, despite this reduction in accuracy, the core performance benefits of the FedSim method remain largely intact. This resilience highlights the robustness of the FedFT pruning approach, particularly in synergy with FedSim advanced aggregation strategy. We specifically chose to test FedFT with the FedSim method to explore its adaptability and performance in personalised/clustered FL algorithms. This approach is particularly relevant for real-world applications, where similarities among clients play a crucial role in enhancing the efficiency and effectiveness of the learning process."}, {"title": "6.10. Impact of FedFT Pruning Post-convergence", "content": "In FL environments, learning often occurs in incremental steps involving a substantial number of clients and rounds of communication. This process can continue to improve model performance even after initial convergence. We study post-convergence pruning in Figure 16, where we plot the results with a pruning threshold set at 50 communication rounds (represented by the blue vertical line) for MNIST and Fed-MEx. We chose these two datasets due to their apparent convergence, which enabled us to establish the pruning threshold.\nWhen applying pruning, MNIST performances across all \u03b1 values are comparable to no pruning (\u03b1 = 0%). Fed-MEx also maintains comparable performances up to \u03b1 = 43%. We attribute these improved pruning performances to the reduced magnitudes of weight adjustments made by client models after the convergence of the global model. These findings suggest"}, {"title": "7. Conclusion", "content": "FedFT introduced a novel FL methodology that communicates model parameters in the frequency space and performs federated aggregation in that same space. DCT-IV transformed and pruned model parameters of FedFT achieved reduced communication costs while maintaining model accuracy. Extensive experiments conducted on four FL datasets and employing three state-of-the-art FL methodologies demonstrate the generalisability of FedFT across diverse neural model architectures and FL methodologies. FedFT is a generalisable solution, achieving communication savings of 5% - 30% while"}, {"title": "Appendix A. FedFT adaptations of FL methodologies", "content": null}, {"title": "Algorithm 2 FedFT adaptation of FedSim", "content": null}, {"title": "Algorithm 3 FedFT adaptation of FedProx", "content": null}]}