{"title": "TemporalPaD: a reinforcement-learning framework for temporal feature representation and dimension reduction", "authors": ["Xuechen Mu", "Zhenyu Huang", "Kewei Li", "Haotian Zhang", "Xiuli Wang", "Yusi Fan", "Kai Zhang", "Fengfeng Zhou"], "abstract": "Recent advancements in feature representation and dimension reduction have highlighted their crucial role in enhancing the efficacy of predictive modeling. This work introduces TemporalPaD, a novel end-to-end deep learning framework designed for temporal pattern datasets. TemporalPaD integrates reinforcement learning (RL) with neural networks to achieve concurrent feature representation and feature reduction. The framework consists of three cooperative modules: a Policy Module, a Representation Module, and a Classification Module, structured based on the Actor-Critic (AC) framework. The Policy Module, responsible for dimensionality reduction through RL, functions as the actor, while the Representation Module for feature extraction and the Classification Module collectively serve as the critic. We comprehensively evaluate TemporalPaD using 29 UCI datasets, a well-known benchmark for validating feature reduction algorithms, through 10 independent tests and 10-fold cross-validation. Additionally, given that TemporalPaD is specifically designed for time series data, we apply it to a real-world DNA classification problem involving enhancer category and enhancer strength. The results demonstrate that TemporalPaD is an efficient and effective framework for achieving feature reduction, applicable to both structured data and sequence datasets.", "sections": [{"title": "Introduction", "content": "The advent of big data technologies and advanced storage solutions has boosted an era with an unprecedented volume of data [1]. This exponential increase in data volume presents not only a challenge for data storage capacity, but also the requirement of effective and accurate computational algorithms to extract valuable information from the vast amount of available data. Feature representation (FR) aims to tackle data redundancy by distilling the most relevant information for predictive models [2, 3], while dimension reduction (DR) is designed to reduce the number of features [4, 5].\nFR has found widespread applications across various domains, especially in handling complex datasets. For example, FR plays a critical role in extracting key information for image-based processing tasks [6]. The effective analysis and interpretation of text-based natural language processing tasks can also be enabled by FR [7]. Stock prices and economic trends in the financial sector have been accurately forecasted by efficient FR algorithms [8]. The primary challenge of FR lies in identifying the most relevant insights from large datasets, a task that becomes particularly complex with time-series data [9].\nThe extraction of relevant information necessitates further reduces the dimensions of the newly-engineered feature space. DR typically involves two main strategies: feature selection and engineering-based dimension reduction. Feature selection tries to recommend a subset of features from the original dataset based on criteria such as relevance and collinearity, thereby reducing the number of feature dimensionality [10-13]. The engineering-based dimension reduction method, on the other hand, involves a more profound transformation of the feature set into a new set of abstract features, which often lack direct interpretations [14]. Popular algorithms include principal component analysis (PCA) [15] and singular value decomposition (SVD) [16]. Neural network-based autoencoders have also been utilized to learn low-dimensional representations of raw data, which fulfills the goal of feature dimension reduction [17-20]. Yet a unified approach for integrating feature representation and dimension reduction in time-series data has been elusive.\nTo address these challenges, we propose a novel end-to-end deep learning framework called TemporalPaD, designed for temporal pattern datasets. TemporalPaD integrates RL [21] with neural networks to achieve concurrent feature extraction and feature reduction. The framework employs the actor-critic architecture [21, 22], where the Policy Module, responsible for dimensionality reduction through RL, acts as the actor, while the Representation Module for feature extraction and the Classification Module collectively serve as the critic. We comprehensively evaluate TemporalPaD using the UCI dataset, a well-known benchmark for validating feature reduction algorithms, through 10 independent tests and 10-fold cross-validation. Beyond these evaluations, given that TemporalPaD is specifically designed for time series data, we applied it to a real-world DNA classification problem involving enhancer category and enhancer strength. The results demonstrate that TemporalPaD is an efficient and effective"}, {"title": "Materials and methods", "content": "framework for achieving feature reduction, applicable to both structured data and sequence datasets."}, {"title": "Datasets", "content": "Datasets"}, {"title": "Performance metrics", "content": "The first type of 29 UCI datasets is evaluated using the overall accuracy. This metric is defined as the proportion of correctly classification samples across all class labels in a dataset. A random split was performed to divide the dataset into a training set, which consisted of two-thirds of the total dataset, and a test set, which comprised one-third of the total dataset. Ten random runs are conducted, as similar in [27]. An additional experiment of ten-fold cross validation (10FCV) strategy is also conducted to evaluate the algorithms on these datasets.\nThe second type consists of two binary classification tasks and is evaluated by the following five performance metrics [28, 29], i.e., accuracy (Acc), sensitivity (Sn), specificity (Sp), Matthews correlation coefficient (MCC), and the area under the ROC curve (AUC). These metrics are defined in the equations (1)-(4).\nAcc = 1\n\\frac{S_{-}+ S_{+}}{S_{+} + S_{-}} (1)\nSn = 1 - \\frac{S_{-}}{S_{+}} (2)\nSp = 1 - \\frac{S_{+}}{S_{-}}(3)\nHere, $S_+$ represents the number of positive samples, while $S_-$ represents the number of negative samples. Additionally, $S_{\\pm}$ denotes the number of actual negative samples that are incorrectly predicted as positives, while $S_{\\mp}$ refers to the number of actual positive samples that are incorrectly predicted as negatives."}, {"title": "Overall architecture of TemporalPaD", "content": "This study tackle the Feature Representation (FR) challenge utilizing the actor-critic paradigm within the deep Reinforcement Learning (RL) architecture [21, 30]. The proposed TemporalPaD framework (Figure 1) integrates three core modules, the Policy Module, the Representation Module, and the Classification Module. The Policy Module functions as the \u201cactor\u201d to actively make decisions, whereas the Representation and Classification Modules collectively serve as the \u201ccritic\u201d by providing evaluation feedback. This Deep Learning (DL) architecture is employed to formulate the FR challenge as a Markov Decision Process (MDP) with delayed rewards.\nFigure 1 illustrates the process wherein data samples are initially transformed into a representation space by the Representation Module. These transformed feature vectors form the elements of the state space, which are subsequently fed into the Policy Module. The Policy Module then makes decisions about whether to retain (denoted as \u201c1\u201d) or discard (denoted as \u201c0\u201d) the represented features, resulting in a new masked sequence"}, {"title": "Policy module", "content": "of features (Rseq). The selected features, Rseq, are then passed to the Classification Module. The outcomes of the classification process serve as rewards for the Policy Module, guiding its decision-making process.\nThe intricate interplay and specific functionalities of these deeply integrated modules are further delineated in subsequent sections.\nThe policy module of TemporalPaD adopts the standard policy function \\pi(a_t|s_t;\\theta) commonly used in the RL architecture [31]. This process follows a typical delayed reward paradigm. The policy module utilizes the probability distribution defined by  \\pi(a_t|s_t;\\theta) to select the appropriate action based on the input feature representation vector from the representation module. All the input feature representations are traversed until this process stops.\nThe newly selected subset of the represented features is then utilized as the input to the classification module. The prediction results of the classification module serve as the delayed rewards to evaluate the decisions made by the policy module. This interaction allows for the continuously monitoring and updating of the policy module. The learned policy function is defined as:\n\\pi(\\alpha_t|s_t; \\theta) = \\sigma(W \\times s_t + b) (5)\nThe action taken at time scale $t$ is denoted as $a_t$, while the state value at time scale $t$ is denoted as $s_t$. The network parameters $(W, b)$ of the policy module are collectively represented by $\\theta$. The sigmoid activation function used in the policy module is denoted as $\\sigma$. The important components of the utilized DL architecture are defined in the followings.\n(1) State\nThe state $s_t \\in S$ is comprised of three components: the word vectors x encoded by the representation module, the hidden vector h, and the memory vector c. The two latter vectors h and c are derived from the long short-term memory (LSTM) model of the representation module described in the next section. The state $s_t$ is updated by the equation (6)."}, {"title": "Action", "content": "s_t = x_t \\oplus h_{t-1} c_{t-1}, (6)\nwhere $x_t$ represents the word vector of the element at the current position in the sequence. The variables $h_{t-1}$ and $c_{t-1}$ represent the hidden vector and the memory vector of the LSTM at the time scale (t-1). The symbol $\\oplus$ denotes the horizontal concatenation of vectors.\n(2) Action\nThe decision making of the policy module is to keep or discard a particular element of the represented feature vector, and is represented as a binary decision vector, with the options are either 0 (discard) or 1 (keep). Consequently, the action space is defined as a binary space, as illustrated in the equation (7).\n\\alpha_t \\in \\{0, 1\\}. (7)\nIt is worth emphasizing that TemporalPaD employs a probabilistic approach to generate function values in the policy module during the training phase. However, during the testing phase, TemporalPaD switches to a deterministic strategy by selecting the $\\pi(\\alpha_t|s_t; \\theta)$ that corresponds to the largest function value, referred to as $\\pi^*$, as shown in equation (8).\na_t \\begin{cases}\\pi(\\alpha_t|s_t; \\theta), & \\text{Trainning step}\\\\\nargmax_{\\alpha_{t} \\in \\{0, 1\\}} \\pi^*{(\\alpha_t|s_t; \\theta_1)}, & \\text{Testing step}\\end{cases} (8)"}, {"title": "Reward Function", "content": "(3) Reward Function\nThe reward function uses the objective of a Markov decision process (MDP) to find the trajectory (denoted as $s_1, a_1,..., s_T, a_T$) that maximizes the reward. We divide the reward into two parts A and B. The reward part $A_i(k)$ has three versions and the user may choose one version manually or based on the evaluation experiment, as defined in the equation (9).\nA_i(k) = \\begin{cases}\n\u2013 \\sum_{j=1}^{C} y_{ij}logP(\\hat{y}_{ij}|X) & (k = 0)\\\\\nmax_{j=1,2,...,C} (log (P(\\hat{y}_{ij}|X))) & (k = 1), \\\\\nx(max_j(P(\\hat{y}_{ij}|X)))& (k = 2)\\end{cases} (9)\nwhere i represents the $i^{th}$ sample, $C$ denotes the number of categories, $y_{ij}$ and $P(\\hat{y}_{ij}|X)$ respectively stand for the one-hot encoding of true labels and the predicted"}, {"title": "Objective function", "content": "distribution of the sample X. The denotation $x(\\hat{y}_i)$ is 1 or 0 if $\\hat{y}_i$ is equal to $y_i$ or not. Here, we employ softmax to compute the probability distribution. $A_i(0)$ is the cross-entropy loss function, $A_i(1)$ is the logarithmic maximum value of the output neurons of the classification module, and $A_i(2)$ is the characteristic function value, respectively.\nWe add the regularization term $B_i$ to encourage the policy module to remove as many features as possible.\nB = \\eta\\times L'/L, (10)\nwhere L denotes the original length of a sequence, and L' is the number of discarded elements, i.e., the number of $a_t$=0. The regularization parameter $\\eta$ is used to balanced the two parts A and B.\nreward = A + B (11)\n(4) Objective function\nThis study employs the policy gradient strategy [32] to update the parameters of the policy network and to maximize the obtained reward, as described in [33]. The update process can be formulated by the equation (12).\nJ(\\theta) = E_{(s_t,a_t)~P_{\\theta}(s_t,a_t)}r(s_1, a_1, \u2026, s_T, a_T) (12)\n=\\sum_{s_1,a_1, , s_T,a_T} \\prod_t \\pi(a_t;\\theta)\nWe simplify the expression in the equation (12) by utilizing the likelihood ratio strategy [34]. Then we derive the gradient formula by the equation (13).\n\\nabla_{\\theta}J(\\theta) = \\sum_{t=1}^{L} R \\nabla_{\\theta}log(\\pi_t|s_t)\n= \\sum_{t=1}^{L} ((A + B) . \\nabla_{\\theta}log(\\pi_{\\theta}(a_t|s_t)) (13)\nThe gradient is integrated into the equation (14) to update the parameters of the policy module [33]"}, {"title": "Representation module", "content": "\\theta = \\theta + \\alpha\\nabla J(\\theta) (14)\nThe learning rate $\\alpha$ is utilized in this study. The parameter $\\theta$ is updated according to formula (14) to maximize the objective function (12), thereby enhancing the performance of downstream prediction tasks.\nRepresentation module\nTraditional machine learning approaches heavily rely on the manually constructed features, while the emergence of deep learning revolutionizes this process by enabling the automatic mapping of raw data to a feature space through embedding techniques. The TemporalPaD framework is proposed in this study to focus on the extraction of temporal relevance between the features by the utilization of the sequential pattern extraction approach LSTM. The normalization layer in this feature extraction approach standardizes the features extracted from the input samples, resulting in the EncodeSeq dataset. This dataset is regarded as the elements constituting the state space for the policy module. The policy module iterates through the feature vectors at each position within EncodeSeq, making decisions on whether to retain the current feature. Upon completing the traversal of EncodeSeq, a decision sequence is obtained, and this decision sequence is utilized to map the original features, yielding the reduced feature set denoted as Rseq. Subsequently, Rseq is integrated into the classification module to compute the value of the reward function.\nIt is important to note that if at time t, the policy module in state $s_t$ chooses to discard the word vector $x_t$ ($a$=0), then $x_t$ will make no contribution to subsequent feature extraction. Conversely, if it is retained, it is utilized for further feature extraction, as depicted in Equation (15). The variable $c_t$ represents the memory unit within the LSTM, playing a pivotal role in preserving and propagating pertinent information [35]. Similarly, $h_t$ signifies the hidden vector, capturing the encoded representation of the input sequence at a specific time step [36].\nc_t, h_t = \\begin{cases}\nc_{t-1}, h_{t-1}, & a_t = 0\\\\\n\\varphi(c_{t-1}, h_{t-1}, x_t), & a_t = 1\\end{cases} (15)\nThe activation function of the LSTM network is denoted as $p$, which enables the LSTM network to selectively store, update, and retrieve information[37]."}, {"title": "Classification module", "content": "Classification module\nThe classification module receives the masked features, Rseq, generated by the policy module, which applies the decision sequence to the feature representation EncodeSeq from the representation module. Specifically, the classification module utilizes the hidden variable $h_y$ of Rseq at the final time step as input and employs softmax as the activation function, as illustrated in Equation (16).\nP(\\hat{y}|X) = \\Psi(W_c h_y + b_c) (16)\nHere, $\\Psi$ represents the softmax activation function. $W_c$ and $b_c$ are the network parameters within the classification module, and they are trained by the cross-entropy loss function:\nL = \\sum_{i=1}^{N} \u2013 \\sum_{j=1}^{C} y_{ij}logP(\\hat{y}_{ij}|X), (17)\nwhere, N represents the sample size, C denotes the number of categories, and $y_{ij}$ signifies the one-hot encoding of the true labels.\nTraining process\nA three-step training procedure is used to train the TemporalPaD framework from scratch, as modified from [33]."}, {"title": "Pre-training", "content": "Step 1: Pre-training\nThe pre-training of the representation module and the classification module are conducted in this step. We input the samples in the original feature space to the representation module with the random initialization, and the extracted features are then passed to the classification module for classifier training. The parameters of both modules are updated by the loss function defined in the equation (17).\nThis step aims to learn meaningful representations from the input samples in the original feature space and accurate classification models. The pre-trained parameters serve as the initial values of the two modules for the subsequent steps."}, {"title": "Reinforcement Learning", "content": "Step 2: Reinforcement Learning\nThis step fixes the parameters of the representation module and the classification module, and optimizes the decision-making capability of the policy module. The sequence of the original features is subjected to processing by the policy module, resulting in the generation of a binary vector, where each element specifies whether a feature is kept (1) or discarded (0). Then, the original feature sequence is mapped to the feature-selected sequence denoted as Rseq using this binary vector and is subsequently supplied to the classification module for reward computation. The parameters of the policy module are updated by the equations (13) and (14)."}, {"title": "Fine-tuning", "content": "Step 3: Fine-tuning\nThis step jointly trains the three modules of the TemporalPaD framework. The joint training aims to optimize their collaborations in the feature representation task."}, {"title": "Results and discussion", "content": "Results and discussion"}, {"title": "Evaluation of the Structured Dataset", "content": "Evaluation of the Structured Dataset\nAlthough TemporalPaD is an end-to-end framework specifically designed for feature extraction and dimensionality reduction of time series data, it can still be effectively compared with various feature dimensionality reduction algorithms on structured datasets. These datasets serve as comprehensive benchmarks for evaluating feature reduction algorithms without altering the model structure. Before presenting the experiments, we provide a detailed description of the model structure, reward function, and voting strategy used to ensure a consistent feature subset across all samples of the structured data when evaluating TemporalPaD on the UCI dataset.\nThe policy module of TemporalPaD is implemented using a two-layer fully connected neural network, aligning with the principles of reinforcement learning, which prioritize simpler network architectures [38]. The representation module employs embedding techniques to extract high-dimensional features from the UCI datasets, followed by a single-layer LSTM network to capture temporal correlations among these features. The classification module then utilizes a single-layer fully connected neural network with a softmax activation function to generate probability distributions. The reward function is computed according to the method described as A(1) in formula (9).\nTo maintain consistent feature numbers for each sample post-dimension reduction, we incorporate a voting mechanism. A threshold is set, and TemporalPaD eliminates a feature from all samples only when the proportion of its removal, as determined by the policy module, exceeds this threshold. All subsequent experiments on the UCI dataset utilize this consistent model configuration."}, {"title": "Independent Tests on 29 UCI datasets", "content": "10 Independent Tests on 29 UCI datasets\nWe first conduct a comparative evaluation of TemporalPaD, an algorithm for feature extraction based on temporal dependencies, with five feature selection algorithms based on feature correlations, using 29 UCI datasets in 10 independent validation tests. Each test employs a different random seed to ensure reliable results. Performance metrics are assessed by calculating the average accuracy (Acc) and its corresponding standard deviation for each method. The average Acc from the 10 independent tests provides an overall measure of the model's accuracy, while the standard deviation indicates the variability in the results. Following a methodology similar to [39], we examine how often different algorithms achieve the highest average Acc across all datasets.\nWhen using Support Vector Machine (SVM) as the downstream classifier, TemporalPaD demonstrates exceptional performance, achieving the highest average Acc on 16 out of the 29 datasets (refer to Table 2). Furthermore, when Naive Bayes (NB) is used as the downstream classifier, TemporalPaD outperforms other methods and attains the highest average Acc on 8 datasets (refer to Table 4). These results demonstrate that, compared to feature selection techniques based on feature correlations, the TemporalPaD approach for feature extraction and dimension reduction, employing Deep Reinforcement Learning (DRL), exhibits superior performance within this framework. It is also important to note that the comparison reveals the relatively lower effectiveness of TemporalPaD with the C4.5 classifier (refer to Table 3). This finding emphasizes the significance of carefully selecting a compatible downstream classifier when utilizing a feature reduction method [40]."}, {"title": "TemporalPaD on 10 UCI Datasets", "content": "10-fold Cross-Validation with the Same Features as TemporalPaD on 10 UCI Datasets\nWe conduct a 10-fold cross-validation experiment based on the UCI dataset to further verify the performance of TemporalPaD on structured data [41]. We select a total of 10 binary datasets from the subject areas \u201chealth and medicine\u201d listed in Table 1, referred to as UCIH&M. For the comparisons, we consider 18 dimensionality reduction methods, comprising 8 information-theory-based feature selection methods and 10 feature extraction algorithms. We ensure that each method select the same number of features as TemporalPaD in each fold, allowing for a fair comparison of the performance of different methods with an equal number of features.\nThe analysis of Tables 5 to 7 reveals that the \u201cNumBest\u201d values indicate a general superiority of feature selection algorithms over feature extraction algorithms on the"}, {"title": "Datasets", "content": "UCIH&M dataset. This observation can be attributed to the structured nature of UCIH&M. Feature selection algorithms maintain the original meaningful representation, preserving interpretability and the inherent meaning of features without alteration [13]. In contrast, feature extraction techniques transform the original features into deeper abstract features through mapping.\nA closer evaluation focusing on feature extraction shows that TemporalPaD exhibits satisfactory results when compared to the other 10 feature extraction methods. Notably, TemporalPaD achieves the highest average Acc on the Liver Disorders dataset when used in conjunction with SVM and C4.5 classifiers (refer to Tables 5 and 7). It is worth mentioning that TemporalPaD is a more comprehensive feature extraction method, applicable to various dataset types and not constrained by the rank of the covariance matrix of the dataset. This characteristic explains why PCA, LLE, and UMAP methods fail to extract features from the Liver Disorders dataset while maintaining the same number of selected features as TemporalPaD [42-44]. In other words, in certain folds, TemporalPaD selects a larger number of features than the sample size, resulting in the failure of PCA, LLE, and UMAP."}, {"title": "Top 10 Features on 6 UCI", "content": "10-fold Cross-Validation with Top 10 Features on 6 UCI Datasets\nIn addition to the 10-fold cross-validation experiments where the same features as TemporalPaD were selected in each fold, we further analyze the UCIH&M dataset from a different perspective by controlling the dimension of post-reduction. We select datasets from UCIH&M that had more than 10 features, totaling 6 datasets, and required all methods to reduce the original feature space to 10 dimensions (results are shown in Tables 8-13). Specifically, with the same number of features, we compare the performance of TemporalPaD against 18 feature dimensionality reduction algorithms, including 8 information-theory-based feature selection methods and 10 feature extraction algorithms. Following the comparison method described by [27], we evaluate the average Acc of three classifiers\u2014SVM, NB, and C4.5\u2014through 10-fold cross-validation as the number of selected features increased. Consistent with the approach taken by [27], we present the results by subtracting the average Acc of each baseline method from that of TemporalPaD. The column \u201c#DiffRes>0 (Total 10)\" indicates the count of times TemporalPaD outperform the current baseline method as the feature count increased from 1 to 10.\nThe analysis of Table 9 reveals that TemporalPaD's average Acc on the Spect dataset is notably superior to that of all 8 feature selection algorithms. This superiority stems from the fact that the Spect dataset exclusively comprises binary features [45], while TemporalPaD can nonlinearly map these features to a high-dimensional dense feature space through its representation module, a capability lacking in those feature selection algorithms. Similarly, the Lung-cancer dataset (refer to Table 13), which also contains comprehensive binary features, demonstrates similar characteristics. In the Heart dataset (refer to Table 12), which encompasses both categorical and continuous features, TemporalPaD still shows overall superiority over feature selection algorithms. However,"}, {"title": "Datasets", "content": "TemporalPaD's overall performance is lower (see Tables 9, 12, and 13). This can be attributed to TemporalPaD's deep learning framework, which necessitates a larger sample size for model training compared to methods relying on statistical information [46], especially in cases like the Lung-cancer dataset where the number of features exceeds the sample size.\nOn the WPBC dataset (refer to Table 11), TemporalPaD exhibits exceptional performance compared to the majority of the 18 feature dimensionality reduction algorithms. This can be attributed to the presence of temporal features in the WPBC dataset [47], for which TemporalPaD is explicitly designed. In contrast, despite some overlap between the WDBC and WPBC datasets, TemporalPaD does not show notable improvement relative to feature extraction algorithms on the WDBC dataset, which lacks temporal features (refer to Table 10). Furthermore, our observations indicate that TemporalPaD's average Acc on the Hypothyroid dataset is unsatisfactory (refer to Table 8).\nThe above discussion highlights the significance of considering the distinct characteristics of the dataset and selecting an appropriate algorithm for feature dimensionality reduction [14]. Particularly, when dealing with datasets that exhibit temporal characteristics, TemporalPaD emerges as a more suitable choice compared to alternative approaches."}, {"title": "Study Based on the Breast Dataset of UCI", "content": "Ablation Study Based on the Breast Dataset of UCI\nWe conduct an ablation study to evaluate the effectiveness of feature reduction learned through the policy module, i.e., reinforcement learning, using the Breast dataset [48] as a case study.\nTo perform the evaluation, the Breast dataset is randomly divided into training and test sets with a 9:1 ratio. A fixed seed of 75 is used to ensure the reproducibility and"}, {"title": "DNA Sequences", "content": "consistency as the above experiments. The results are presented in Figure 2, where the horizontal axis represents the various training phases of TemporalPaD: \u20181' denotes the pre-training phase of the representation module, \u20182\u2019 represents the pre-training phase of the policy module, and \u20183' indicates the joint training phase.\nFrom the results, it can be observed that after integrating the policy module with the representation module, the Acc of TemporalPaD on the training dataset increased by 0.0017, from 0.9683 to 0.97, while maintaining Acc on the test set. This improvement in Acc on the Breast dataset demonstrates TemporalPaD's capability to extract and reduce features effectively from the entire feature set. This result further supports the concept that incorporating reinforcement learning into TemporalPaD through the policy module has the potential to enhance the performance of downstream tasks.\nA Real Application of TemporalPaD on DNA Sequences\nTemporalPaD has been extensively evaluated alongside various feature reduction algorithms using the UCI dataset, a well-known feature reduction benchmark. The results indicate that TemporalPaD is particularly well-suited for time series data, attributed to its representation module designed with embeddings and LSTM. Building"}, {"title": "DNA sequences", "content": "on this, we apply TemporalPaD to a real-world problem: the binary classification of DNA sequences.\nAs mentioned earlier, there is a public dataset for the enhancer classification task, referenced as Enhancer1 and Enhancer2 in Table 1. These correspond to two classification tasks: the enhancer category task and the enhancer strength task. Both tasks are binary classifications, where Enhancer1 includes enhancers (labeled 1) and non-enhancers (labeled 0), while Enhancer2 contains strong enhancers (labeled 1) and weak enhancers (labeled 0).\nTo successfully implement TemporalPaD, we treat the DNA sequences as natural language. This approach allows us to encode DNA sequences and extract features for contextual dependencies. Subsequently, TemporalPaD is employed to reduce these features. During this process, we use Enhancer1, the enhancer category classification task, as a reference to select appropriate hyperparameters for TemporalPaD. Specifically, we split the training dataset of Enhancer1 into a training subset and a validation subset in a 9:1 ratio to determine the best hyperparameters, which yield the highest accuracy (Acc) on the validation dataset during the training phase of TemporalPaD.\nBased on the optimal hyperparameters determined from Enhancer1, we then train TemporalPaD on both Enhancer1 and Enhancer2 datasets using the same hyperparameters. Finally, we compare the performance of TemporalPaD with several classification enhancer classifiers."}, {"title": "Kmer", "content": "Kmer\nIn this section, we utilize the kmer technology for segmentation, which allows us to transform DNA sequences into a suitable format for feature extraction. The kmer technique partitions the sequences into smaller fragments of length k, as depicted in"}, {"title": "Method", "content": "Selection of Reward Calculation Method\nThe selection of an appropriate reward calculation method is crucial for the performance of RL, as specified in formula (9) of TemporalPaD. To determine the most suitable reward computation method for the enhancer classification task, we conduct an experiment comparing the performance of three different reward calculations from formula (9), while keeping other configurations of TemporalPaD constant, based on the"}, {"title": "classification", "content": "first enhancer task: enhancer category classification.\nIn this experiment, the kmer parameter is fixed at 1, treating each position in the sequence as an individual feature. The policy module is designed as a single-layer fully connected network, while the representation module utilizes a single-layer unidirectional LSTM network. Acc is chosen as the evaluation metric to assess the performance of the three different reward calculations.\nBy comparing the Acc of the three reward computations on the enhancer category task (depicted in Figure 4), we find that the second reward computation method, i.e., A(2) from formula (9), shows the highest Acc on the validation set of Enhancer1. Consequently, we select the second reward calculation method for all subsequent experiments."}, {"title": "Module", "content": "Evaluation of Network Architecture for Policy Module and Representation Module\nTemporalPaD is a theoretical framework that integrates RL with neural networks to"}, {"title": "reduction", "content": "conduct feature reduction from a temporal perspective. In this section, we compare different neural network configurations for the Policy Module and Representation Module in TemporalPaD, setting the kmer parameter to 1 and using A(2) from formula (9) for reward calculation.\nFor the Policy Module, we compare three options: a one-layer fully connected network, a two-layer fully connected network, and textCNN [19]. These alternative network configurations allow us to evaluate TemporalPaD's performance under different Policy Module structures.\nAdditionally, for the Representation Module, which plays a pivotal role in capturing temporal dependencies in time series data, we analyze three primary network structures: a one-layer unidirectional LSTM, a one-layer bidirectional LSTM (Bi-LSTM), and a two-layer Bi-LSTM. By assessing TemporalPaD's Acc on the validation dataset of Enhancer1 with these various Representation Module configurations, we gain insights into their effectiveness for enhancer category classification.\nNext, we traverse all combinations of the Policy Module with the Representation Module and evaluate their Acc on the validation dataset of Enhancer1 to determine the most suitable combinations. The results are shown in Figure 5.\n(a) Fixed Representation Module as one-layer LSTM: Policy Module as one-layer fully connected network (a1), two-layer fully connected network (a2), and textCNN (a3).\n(b) Fixed Representation Module as one-layer Bi-LSTM: Policy Module as one-layer fully connected network (b1), two-layer fully connected network (b2), and textCNN (b3).\n(c) Fixed Representation Module as two-layer Bi-LSTM: Policy Module as one-layer fully connected network (c1), two-layer fully connected network (c2), and textCNN (c3).\nThe results, as shown in Figure 5, indicate that the network configuration with a one-layer LSTM as the Representation Module and a two-layer fully connected network as the Policy Module achieves the highest Acc on the validation dataset of Enhancer1 when k of kmer is fixed at 1 and A(2) of formula (9) is used for reward computation. Additionally, comparing configurations (a1), (a2), and (c1) supports previous research indicating that the Policy Module's network should not be excessively complex [17].\nTherefore, we select combination \u2018a2' as the optimal configuration for the Policy Module and Representation Module of TemporalPaD for subsequent experiments. This configuration strikes a balance between model complexity and performance."}, {"title": "K for Kmer", "content": "Selection of K for Kmer\nIn the previous section, we determined the appropriate reward calculation and specific network structures of TemporalPaD for solving the enhancer category classification. In this section, our focus shifts to selecting the optimal k value for kmer in both enhancer tasks: enhancer category (Enhancer1) and enhancer strength classification (Enhancer2), with accuracy (Acc) as our primary consideration. We explore k values for kmer ranging from 1 to 6. The results are shown in Figure 6. The \u20181 layer' of Figure 6 illustrates the Acc of TemporalPaD for different kmer values in the enhancer category classification, while the \u20182 layer' displays the Acc of TemporalPaD for different kmer values in the enhancer strength classification.\nThe results in Figure 6 consistently demonstrate the superiority of kmer = 1 in both tasks. For enhancer category classification, kmer = 1 consistently outperforms other settings, achieving an accuracy of 73% in training stage 3 of TemporalPaD, representing an 8% improvement over the second-best setting, kmer = 2. Similarly, in the enhancer strength task, kmer = 1 achieves an accuracy of 62.4% in training stage 3 of TemporalPaD, surpassing kmer = 2 by 6.93%. These results, obtained from the validation set, provide robust evidence that kmer = 1 is the optimal parameter for conducting enhancer classification with TemporalPaD and for comparing the results with existing classical methods."}]}