{"title": "Pan-protein Design Learning Enables Task-adaptive Generalization for Low-resource Enzyme Design", "authors": ["Jiangbin Zheng", "Ge Wang", "Han Zhang", "Stan Z. Li"], "abstract": "Computational protein design (CPD) offers transformative potential for bioengineering, but current deep CPD models, focused on universal domains, struggle with function-specific designs. This work introduces a novel CPD paradigm tailored for functional design tasks, particularly for enzymes- a key protein class often lacking specific application efficiency. To address structural data scarcity, we present CrossDesign, a domain-adaptive framework that leverages pretrained protein language models (PPLMs). By aligning protein structures with sequences, CrossDesign transfers pretrained knowledge to structure models, overcoming the limitations of limited structural data. The framework combines autoregressive (AR) and non-autoregressive (NAR) states in its encoder-decoder architecture, applying it to enzyme datasets and pan-proteins. Experimental results highlight CrossDesign's superior performance and robustness, especially with out-of-domain enzymes. Additionally, the model excels in fitness prediction when tested on large-scale mutation data, showcasing its stability.", "sections": [{"title": "I. INTRODUCTION", "content": "Computational protein design (CPD) aims to generate amino acid sequences given protein backbones,and plays a pivotal role in bioengineering, particularly in drug discovery and enzyme design. While deep CPD models have ventured into de novo protein design, they remain confined to the general pan-protein domain. In response, this work proposes a novel CPD paradigm explicitly designed for functional tasks, emphasizing enzymes-a crucial protein class frequently underserving specific application requirements. One of primary tasks in enzyme engineering is to improve property-specific enzymes by mutations or redesign. To achieve this, a CPD model must possess strong representation and generalization and offer a flexible architecture for expanded downstream enzyme tasks.\nData scarcity, particularly the lack of sizable structure-sequence datasets, poses a significant bottleneck, leading to issues like undertraining or overfitting. Despite efforts to enhance equivariance and invariance in structural modules, practical protein design models with existing low-resource data remain elusive. This work considers transfer learning as a potential solution, with a focus on the transfer of pretrained structure models. However, the scarcity of such models with powerful representation capability limits their use. Redirecting attention to pretrained protein language models (PPLMs), which have achieved success in representing protein sequences, provides an alternative. PPLMs, trained on extensive sequence datasets [12], [15], [22], [23], offer valuable implicit structural information for CPD, although no attempts have been made so far to the best of our knowledge. Nevertheless, CPD is essentially a cross-modal structure-to-sequence task, but PPLMs are difficult to be applied directly since they represent the sequence modality only. Fortunately, emerging text-visual cross-modal modeling techniques such as CLIP [20], [21] have opened a promising path, and robust protein structure modeling is inspired to incorporate valuable PPLM prior for enhancement.\nTo overcome data scarcity and cross-domain requirement, we propose a novel transformation-enhanced domain-adaptive protein design framework, CrossDesign, incorporating cross-modal alignment between PPLM and protein structures, as shown in Figure 1. Additionally, we implement inter-modality alignment (InterMA) to align these modalities and apply cross-layer consistency (CLC) to regularize the process. While fully utilizing PPLM's supervised signal during training, our model eliminates the need for PPLM during inference, significantly reducing computational costs. The encoder-decoder framework facilitates easy extension of the sampling regime. For conditional CPD tasks with unknown sequences, autoregressive (AR) sampling is used, leveraging the lower triangle mask during training. For proteins with known sequences-crucial for downstream function prediction, such as mutation effects, non-autoregressive (NAR) parallel computing is applied without additional training.\nThe main contributions are as follows:\n\u2022 The proposed CrossDesign innovatively incorporates PPLMs with cross-modal alignment and cross-layer constraint in protein design, highlighting their potential in enhancing protein structural representation.\n\u2022 The skillfully crafted architectural design is well-suited for cross-domain CPD and mutation tasks.\n\u2022 Robust performances across diverse in-domain and out-of-domain tasks demonstrate versatility and effectiveness.\n\u2022 The collection of enzyme datasets and benchmarks serves to narrow the gap between generic proteins and specific functional proteins for structure-sequence transformation."}, {"title": "II. METHODS", "content": "As shown in Figure 1(a), CrossDesign is composed of the Structure-to-Sequence Stream (Str2Seq stream), and the auxiliary PPLM Sequence Stream (PPLM stream). To bridge these two independent streams, a linear alignment unit (LAU) and a shared decoder are utilized. CrossDesign is trained in an end-to-end manner. The inference pipeline relies only on the Str2Seq stream and does not require auxiliary PPLM.\nFormally, $X = \\{x_1,x_2,\\cdots, x_n\\}$ denote the protein atomic coordinates given a structure-sequence pair, where n is the sequence length, and $x_i \\in \\mathbb{R}^{3\\times3}$ represents the i-th coordinates of the amino acid residue, consisting of N, Ca, and C atoms. The corresponding primary generated sequence in Str2Seq stream is represented as $Y_p = \\{Y_1, Y_2,\\cdots, Y_n\\} \\in \\mathbb{R}^{n}$, where Y denotes the i-th amino acid residue in textual form. While the corresponding auxiliary generated sequence in PPLM stream is represented as $Y_a = \\{Y_1,Y_2,\\cdots,Y_n\\} \\in \\mathbb{R}^{n}$. The reference native sequence of Y is $\\hat{Y} = \\{\\hat{y}_1, \\hat{y}_2, \\cdots,\\hat{y}_n\\}\\in \\mathbb{R}^{n}$. X and Y is a ground-truth structure-sequence pair.\n1) Main Structure-to-sequence Stream: The Str2Seq stream is a completed and independent protein design network with an Invariant Protein Structure (IPS) module and a decoder, as shown in Figure 1(a). This stream aims to generate sequences given backbone coordinates. The geometric Transformation-enhanced GVP (tGVP) is a key component of IPS. Among geometric models, the lightweight GVP [11] is chosen with properties of equivariance and invariance for rigid bodies. Temporal GraphTransformer (TGT) consists of a graph convolution network and a generic Transformer encoder to enhance the temporal interactions while aligning Str2Seq and PPLM streams and act as a linker between the tGVP and the decoder in Str2Seq stream. The self-attention decoder is a generic Transformer decoder with triangular masks [26]. The only modification is the use of learnable positional embeddings instead of sinusoidal positional embeddings. The decoder module is shared between two streams. The outputs of TGT are fed into the decoder, also used in InterMA.\n2) Auxiliary PPLM Supervised Stream: The PPLM stream is an auxiliary network for augmenting Str2Seq stream. The entire PPLM stream is essentially an inner-loop asymmetric auto-encoder (asyAE) model. The PPLM is the core component for introducing prior language knowledge, which is fixed and untrainable. We transfer the off-the-shelf PPLM trained on large-scale protein sequence datasets. The LAU module is an adapter simply implemented as a fully connected MLP with two hidden layers. The LAU forces the conversion of the dimensions of PPLM outputs to align with TGT outputs. Also, LAU serves the InterMA loss, acting as an intermediary between the Str2Seq and PPLM streams and assuming the role of semantic supervision. The decoder is an auxiliary bridge in the PPLM stream, shared with Str2Seq stream, which takes LAU's outputs as inputs and generates auxiliary protein sequences based on asyAE reconstruction loss.\n3) Primary Generation Objective: In Str2Seq stream, the protein backbone X is first input to the tGVP module to obtain the geometric structural features $Z_{struc}$ with a hidden dimension d. Then the structural features $Z_{struc}$ are fed to the TGT blocks and decoder sequentially through Str2Seq stream. The features obtained by the self-attention TGT blocks are referred to as alignment features $Z_{align}$. The features obtained by the decoder are denoted as contextual features $Z_{context}$. Finally, the following MLP layers generate the output distribution $D_{logits-p}$ of the primary generated sequence (M=20 denotes 20 types of amino acids) as:\n$Z_{struc} = tGVP(X) \\in \\mathbb{R}^{n\\times d}$,\n$Z_{align} = TGT(Z_{struc}) \\in \\mathbb{R}^{n\\times d}$,\n$Z_{context} = Decoder(Z_{align}) \\in \\mathbb{R}^{n\\times d}$,\n$D_{logits-p} = MLP(Z_{context}) \\in \\mathbb{R}^{n\\times M}$,\n$p(Y_p|X) = argmax(Softmax(D_{logits-p})) \\in \\mathbb{R}^{n}$.\n(1)\nThe cross-entropy (CE) loss is commonly used to measure the discrepancy between two probability distributions. We observed that emphasizing the salient features of the output distribution can further improve the performances. To achieve this, an exponential cross entropy (expCE) loss is used to accentuate the spiking effect of the output distribution as:\n$L_{expCE} = \\sum_{b=0}^{B} exp \\big( CE \\big( Softmax(D_{logits-p}), \\hat{Y}^{(b)} \\big) \\big)$.\n(2)"}, {"title": "4) Cross-layer and Cross-modal Objective", "content": "Multi-modal studies have shown that it makes sense to explicitly enforce consistency between different modalities [4], [6]\u2013[9], [13], [25], [27]\u2013[39]. In the Str2Seq stream, structural features $Z_{struc}$ from the tGVP module and the contextual features $Z_{context}$ from the decoder are considered as two different internal modalities. As shown in Figure 1, to enforce alignment constraints across modalities, we propose a cross-layer CLC loss, which is implemented as a knowledge distillation (KL-divergence here), where the entire $Z_{context}$ and $Z_{struc}$ are treated as teacher and student models. A high temperature $\\tau$ = 8 is adopted to 'soften' the probability distribution from potential spike responses [36]. The distillation process of cross-layer CLC is expressed as:\n$L_{CLC} = KL \\Big( Softmax(\\frac{Z_{struc}}{\\tau}), Softmax(\\frac{Z_{context}}{\\tau}) \\Big)$.\n(3)\nSimilarly, we employ knowledge distillation for InterMA loss to align Str2Seq and PPLM streams. In PPLM stream, the native sequence $\\hat{Y}$ is fed to the PPLM, LAU, and decoder sequentially. The LAU output is denoted as another type of context $Z'_{context}$. Unlike CLC, we use geometric-temporal features ($Z_{align}$) in contextual modality instead of geometric features ($Z_{struc}$) of the tGVP, since CLC is a cross-stream cross-modal alignment. The alignment of InterMA as:\n$Z'_{context} = LAU(PPLM(\\hat{Y})) \\in \\mathbb{R}^{n\\times d}$,\n$L_{InterMA} = KL \\Big( Softmax(\\frac{Z'_{align}}{\\tau}), Softmax(\\frac{Z_{context}}{\\tau}) \\Big)$.\n(4)\n5) Inner-loop Sequence Reconstruction: To enable the trainable modules (LAU and Decoder) in PPLM stream to fit faster and learn potential contextual knowledge, we employ a Seq2Seq recovery mode as a pseudo-machine translation process since PPLM stream can be considered an asyAE model, as shown in Figure 1. To reconstruct auxiliary sequences $Y_a$ in PPLM stream, the contextual features $Z_{context}$ from the LAU are fed to the shared decoder. And the $\\hat{Y}$ corresponds to the generated probability $D_{logits-a}$. The final sequence reconstruction loss is denoted as:\n$L_{asyAELoss} = CE \\Big( Softmax(D_{logits-a}), \\hat{Y} \\Big)$.\n(5)"}, {"title": "A. Sampling Regimes for Cross-domain Tasks", "content": "Different sampling regimes vary from cross-domain tasks based on AR and NAR modes.\n1) AR Decoding for CPD: Given a candidate structure X for generating the unknown sequence $Y_{sample}$ as:\n$p(Y_{sample}|X_c) = \\prod_{i=0}^{n} p(Y_i| Y_{i-1},\\cdots, Y_o; X_c)$.\n(6)\nThis density is modeled autoregressive through the Vanilla Transformer decoder paradigm. Using AR decoding we can typically conduct CPD tasks for general proteins and function-specific enzymes.\n2) NAR Decoding for Fitness: Compared with AR decoding, NAR is applicable for mutation effect utilizing zero-shot learning. Given a known enzyme backbone $X_{wild}$ and the corresponding mutant sequence $Y_{mut}$, the mutant correlation can be scored based on marginal probability $Prob_{mut}$ as:\n$Prob_{mut} = p(logits| Decoder(Y_{mut}); IPS(X_{wild}))$.\n(7)\nContrary to AR decoding, NAR can perform inference tasks faster with parallel computing."}, {"title": "III. EXPERIMENTS", "content": "Generic Protein Datasets. 1) The CATH dataset [10] includes training, validation, and testing splits with 18204, 608, and 1120 structure-sequence pairs, respectively. CATH specifically refers to CATH 4.2 if not otherwise specified. 2) We also report results on Ts50 & Ts500 [14] to assess the generalization. No canonical training and validation sets exist for Ts50 or Ts500.\nFunctional Enzyme Datasets. 1) EnzPetDB: We collected 212 proteins described to breakdown plastics from PlasticDB. 2) EnzFoldDB: We consolidated 691 experimentally-determined structure-sequence paired enzymes with 10 folds from RCSB. Each fold has an equal number of enzymes. 3) ProteinGym [18] is specifically designed for protein fitness prediction and design, which encompasses a broad collection of over 178 standardized deep mutational scanning assays, spanning millions of mutated sequences.\nIn-domain Evaluation on Generic Proteins.\nOverall, CrossDesign outperforms baselines consistently across all test sets with balanced performances, showing strong generalization and robustness. In addition, a pretrained strategy based on prior knowledge transfer with a small training set can outperform the strategy based on large data training such as ESM-IF (Group 2 vs. Group 3).\nPerformances vary from the types of chains. Following [10], we evaluate the models on the subsets of the CATH test set, i.e., \"Short\" dataset (protein sequence length < 100 residues) and \"Single-chain\" dataset (single-chain proteins recorded in the Protein Data Bank) as shown in Table I. CrossDesign consistently exceeds baselines on both \"Single-chain\" and \"Short\" datasets. In particular, in terms of perplexity, CrossDesign (Short: 5.17; Single-chain: 5.31) significantly outperforms PiFold (Short: 6.04; Single-chain: 6.31) and ProteinMPNN (Short: 6.21; Single-chain: 6.68).\nDomain-unknown Generalization. To fully compare the generalizability of the different models on the out-of-domain datasets, we report the results on the Ts50 and Ts500 datasets"}, {"title": "C. Out-of-Domain Enzyme Design", "content": "Conditional CPD for PET Enzymes: Polyethylene terephthalate (PET) is a synthetic plastic polymer widely used globally. Due to its ester bonds and aromatic nuclei, PET is chemically inert and highly resistant to degradation, leading to environmental and health concerns. Plastic waste poses a significant ecological challenge, and enzymatic degradation offers a promising, eco-friendly method for recycling polyester waste. However, knowledge of PET-degrading enzymes remains limited. Machine learning-assisted methods could accelerate the discovery of PET hydrolases [16]. To support this effort, we curated a comprehensive dataset for analyzing PET hydrolases using CPD, which will serve as a valuable resource for future research. As shown in the orange bar of Figure 2, CPD analysis achieved AAR scores up to 64.86% higher than those in in-domain settings, indicating strong generalization for unseen PET datasets and paving the way for further data analysis and enzyme modifications.\n2) Conditional CPD for Fold-aware Enzymes: Based on enzyme categorization criteria, enzymes are classifiable into different groups based on their fold types. To assess the performance of enzymes at various fold levels in the CPD task, we categorized them into 10 distinct classes as illustrated in Fig. 2. The green bars represent fold-specific evaluations, with AAR scores for each fold consistently hovering around 60%, ranging from 59.52% (Fold/2) to 63.18% (Fold/5). Notably, these scores surpass the level of out-of-domain generalization observed in the generic protein dataset. In contrast, the blue bar signifies the fold-agnostic display, amalgamating all folds and providing an average assessment of 61.15%. This result underscores a consistently high overall performance level across diverse fold types.\n3) Zero-shot Learning for Mutation Effect Prediction: Mutant Scoring with NAR decoding. Contrary to language models [17], our model scores mutation effects using a different method since the special encoder-decoder architecture. Given the wildtype enzyme structure feature $Z_{wild}$ and the mutant sequence $Y_{mut}$ corresponding to the wildtype sequence $Y_{wild}$, the NAR decoding generate mutant marginal probability $Prob_{mut}$ as:\n$Prob_{mut} = p(logits_{mut}|Z_{wild}; Y_{mut})$,\n$Effect_{mut} = CE(Prob_{mut}, Y_{wild})$,\n(8)\nThis scoring method can compute either single or multiple mutations. CE is used to be consistent with the training setup, which is used to assess the difference between mutant distribution and wildtype sequence.\nPerformances on ProteinGym. To ensure a fair comparison, we selected prominent protein language models and inverse folding models as baselines, consistent with ProteinGym [18]. All baselines are computed using zero-shot learning for mutation effects. Assessing all 217 ProteinGym proteins containing millions of mutations, our model consistently achieved the best matching rank (0.445). In comparison, the optimal protein language model (VESPA) averaged pof 0.437, and the optimal inverse folding model (ESM-IF) scored 0.422. This performance edge may be attributed to our model's dual advantage, encompassing both language modeling and structural modeling within the proposed paradigm. To further validate performance across functions and taxa, we compared CrossDesign with other inverse folding models. In results categorized by function, all models exhibit superior performance in terms of Stability. Notably, our model outperforms others, particularly in Binding (0.412) and Stability (0.636), albeit with slightly lower scores in other aspects. In results categorized by taxon, our model surpasses ESM-IF for Prokaryotic Viruses and slightly lags behind ESM-IF in other categories. Overall, the results are consistently excellent."}, {"title": "IV. CONCLUSIONS AND LIMITATIONS", "content": "This work contributes to advancing CPD methodologies, bridging the gap between universal protein design and the nuanced requirements of specific functional protein engineering. While CrossDesign exhibits versatility with AR and NAR decoding modes, enhancing capabilities for functional protein prediction, certain limitations persist: 1) Limited functional protein datasets were utilized, due to the challenges in collecting enzyme datasets. 2) Augmenting the structure-sequence training data holds the potential to express more robust architectures. 3) The verification of mutation effects lacks wet experiment-assisted validation."}]}