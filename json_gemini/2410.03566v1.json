{"title": "A Survey on Offensive AI Within Cybersecurity", "authors": ["SAHIL GIRHEPUJE", "AVIRAL VERMA", "GAURAV RAINA"], "abstract": "Artificial Intelligence (AI) has witnessed major growth and integration across various domains. As AI systems become increasingly prevalent, they also become targets for threat actors to manipulate their functionality for malicious purposes. This survey paper on offensive AI will comprehensively cover various aspects related to attacks against and using AI systems. It will delve into the impact of offensive AI practices on different domains, including consumer, enterprise, and public digital infrastructure. The paper will explore adversarial machine learning, attacks against AI models, infrastructure, and interfaces, along with offensive techniques like information gathering, social engineering, and weaponized AI. Additionally, it will discuss the consequences and implications of offensive AI, presenting case studies, insights, and avenues for further research.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving landscape of Artificial Intelligence (AI), its intersection with offensive tactics has created a complex and concerning domain. As AI takes on pivotal roles in essential applications, like self-driving vehicles, healthcare diagnosis, and financial services, it becomes a tempting target for malicious actors [16]. This study aims to comprehensively explore the realm of offensive AI, shedding light on its multifaceted dimensions, the techniques involved, its consequences, and potential future implications.\nCyberattacks have surged in both complexity and frequency. This is evidenced by the escalating costs associated with data breaches. In 2022, businesses incurred an average loss of $4.35 million, an increase of $0.11 million from the previous year and a 12.7% rise from 2020 [22]. Moreover, the volume of data breaches has reached historic highs, with approximately 15 million records exposed during the third quarter of 2022. Furthermore, the third quarter of 2022 witnessed an alarming 57,116 distributed denial-of-service (DDoS) attacks [78].\nAgainst this backdrop, understanding and mitigating security risks in machine learning (ML) has emerged as a pivotal aspect of cybersecurity. The field requires reverse engineers, forensic analysts, incident responders, and threat"}, {"title": "1.1 Impact of Al", "content": "Al in cybersecurity plays a dual role: offence and defence [96]. With the exponential growth of data and the rapid evolution of cyber threats, integrating AI into cybersecurity has become imperative [107]. ML techniques, such as decision trees, support vector machines, and Bayesian algorithms, have been augmented by hardware-assisted malware detection [3, 59]. Deep learning (DL) methods often outperform traditional ML, particularly in intelligent malware detection [97]. As we see over the course of this survey, the role of Al includes, but is not limited to, malware identification, network intrusion detection, phishing and spam detection, countering advanced persistent threats (APT), and identifying domain generation algorithms (DGAs)."}, {"title": "1.2 Motivation", "content": "We are primarily motivated by recent advancements in AI-based applications. The growing significance of ML and DL applications in our daily lives has heightened their prominence. Consider the proliferation of large language"}, {"title": "1.3 Why Attacks Happen", "content": "Cybersecurity attacks occur in the real world primarily for financial gain. Other objectives may include advancing political/social agendas or causing harm to individuals and organisations. An attacker may also have ideological motivations. In other cases, attacks may happen to strip other countries and communities of development. For instance, the Stuxnet virus destroyed about 1,000 Iranian high-speed centrifuges used for uranium enrichment [50].\nAttackers may also exploit vulnerabilities to steal data, disrupt services, or compromise the integrity of systems [61]. This can be illustrated briefly by considering attacks on medical imagery. The motivations behind carrying out attacks on medical imagery are multifaceted and include several compelling reasons. One prominent incentive is financial gain, where an attacker may seek to exploit the imagery for insurance fraud purposes, such as claiming the quality of life insurance on their behalf [69]. Moreover, malicious actors may cause harm by manipulating medical imagery, potentially leading to misdiagnoses and compromised patient well-being. Another plausible motivation is the desire to attain accelerated medical attention by granting priority over other patients.\nIt is worth noting that access to medical scans is not an impossible barrier, as evidenced by the recurrent theft of millions of medical scans resulting from cyber-attacks each year [85]. Hence, safeguarding software is imperative to preserve the integrity and security of AI applications."}, {"title": "1.4 Research Outline & Tasks", "content": "This survey paper investigates offensive AI by examining three major sections, as given below.\nOffensive Against AI. The chapter explores adversarial machine learning and the Machine Learning Operations (MLOps) lifecycle. We analyse attacks against AI models, infrastructure, and interfaces, including strategies like poisoning attacks, trojanized models, attacks on the model supply chain, and techniques for injecting malicious prompts.\nOffensive Using Al. Delving into the other side of the coin, this chapter investigates how AI is harnessed in offensive schemes. It will study how AI is leveraged for enterprise attack techniques, creating deepfakes and manipulated media, and propagating misinformation and propaganda.\nConsequences and Impact. The section explores domain-specific consequences across different sectors. We also consider the impact on consumers, businesses, and the broader public digital infrastructure and policy framework. Additionally, we present practical examples to provide real-world context."}, {"title": "2 Offensive Against Al", "content": "This section delves into offensive strategies aimed at Al systems. We also discuss the evolving threat landscape of attacks that challenge the integrity, reliability, and security of AI technologies."}, {"title": "2.1 Adversarial Machine Learning", "content": "Adversarial ML can be seen as a practice of manipulating inputs to evade classification or reveal model decision boundaries [44]. An adversarial example is a sample of input data that has been modified slightly to cause a classifier model to misclassify it. Mathematically, it can be seen as a trained model $f (\\theta)$, an original sample $x$ and an adversarial sample $x'$, which is classified differently. A perturbation $\\delta$, often with the same dimensionality as $x$ leads to the following\n$f(x, \\theta) \\neq f(x', \\theta)$\nwhere $x' = x + \\delta$, for a given small $\\delta$. These modifications $\\delta$ can be so subtle that a human observer does not notice the modification, yet the classifier can be tricked [57]. Adversarial examples pose security concerns because they can be used to attack ML systems, even if the adversary has no access to the underlying model. We discuss various types of adversarial attacks in the following subsections."}, {"title": "2.1.1 MLOps Lifecycle", "content": "MLOps, or Machine Learning Operations, is a core function of ML engineering. It includes all the processes of taking models to production and maintaining them. The MLOps lifecycle comprises several essential phases: Development involves model creation and experimentation; Testing and Validation ensures model performance and accuracy; Deployment brings models to production at scale; Monitoring and Management maintains model health and triggers updates as needed; Feedback uses production data to improve models and processes continually. Additional factors in the cycle include collaboration, automation, version control, and continuous integration and continuous deployment (CI/CD) practices. Collectively, these elements ensure efficient management of the models from their inception to their operational use.\nSecurity considerations in the various stages of the MLOps lifecycle give rise to distinct forms of defence. For instance, practices to counter data poisoning attacks include role-based access controls (RBAC) and hashing mechanisms [102]. Specialised tools like IBM's Adversarial Robustness Toolbox (ART) [19] and Microsoft's Counterfit [89] play a key role in assessing the resilience of ML/AI models at all stages of the MLOps cycle.\nThe adversarial ML landscape in itself is quite vast. As shown in Figure 3, all stages of a model-building process are vulnerable to adversarial attacks. We now transition to a critical dimension of adversarial AI, where we encounter training-time attacks."}, {"title": "2.1.2 Attacks against Model", "content": "We now focus our attention to attacks against models, one of the most common domains within offensive AI.\nPoisoning Attacks. A poisoning attack occurs when a model's training data is intentionally tampered with, affecting the outcomes of the model's decision-making processes. The goal is to degrade the learning process, leading to erroneous decision-making by the AI model. Naturally, the corruption of training data has a heavy impact on data-centric platforms. Here, services based on recommender systems form a crucial area of study. Platforms such as YouTube, Netflix, and Spotify utilise Al to personalise user content recommendations. These rely on a user's historical interactions, tailoring recommendations based on what was viewed, liked, or subscribed to by others with similar interests. Online shopping services such as Amazon recommend products in a similar way [77]. Social networks, like Facebook, Twitter, and Instagram, also shape a user's timeline by determining the content they encounter. Data poisoning attacks on these platforms can directly affect the training data of recommendation systems, resulting in degraded services. Research shows that data poisoning attacks demand multifaceted defence mechanisms, including RBAC, stringent evaluation of data sources, integrity checks, and hashing techniques [102].\nDeterioration of recommender systems can be seen analogously with a classical economic concept called the market for lemons [2]. Consider the scenario when an online store subtly introduces product B alongside product A in its recommendations; the change might go unnoticed by the average shopper. Shoppers may find it slightly unusual but will likely proceed with little thought. Sellers may withhold information about product defects, blurring the lines between defective and non-defective goods. Consequently, buyers may price all goods as potentially defective, ultimately driving"}, {"title": "2.1.3 Attacks against Infrastructure - Model Supply Chain", "content": "Focusing on attacks against infrastructure within the model supply chain, it is vital to consider the role of AI cloud services. Within the ML model supply chain, encompassing stages such as data collection, model sourcing, MLOps tooling, up to build and deployment - the criticality of securing the cloud services becomes evident. An attack on any of these services is bound to impact other stages of the model's supply chain.\nSupply chain attacks often exploit trust and reach. They may use the existing trust between the software producer and consumer to evade security checks. Notably, a one-to-many business model would mean a single attack impacting numerous downstream customers, amplifying its impact and making it increasingly perilous [101]. Attacks on the supply chain are not solely confined to ML systems either. In physical world scenarios where ML systems rely on signals from sensors, they remain susceptible to adversarial examples in an indirect manner."}, {"title": "2.1.4 Attacks against Interface", "content": "We now shift focus to defence mechanisms to counteract threats that target interfaces. This encompasses prompt injection, evasion attacks, inversion attacks, and model theft.\nEvasion Attacks. These involve adding subtle perturbations or noise to a model's input, leading it to make incorrect classifications. These attacks exploit vulnerabilities during the model's inference phase, and their success hinges on the human imperceptibility of these adversarial examples. There are two primary types of evasion attacks: targeted, where adversaries aim for specific predictions, and untargeted, with the goal of inducing misclassification."}, {"title": "2.2 The Al Attack Surface", "content": "All the possible entry points for unauthorised access into an Al system constitute the AI attack surface. They include all vulnerabilities that can be exploited for a security attack. It is crucial to consider the entire spectrum, including components like AI assistants, agents, tools, models, and storage. Formally, the Langchain framework [68] breaks down Al attack surfaces into distinct elements, such as (a) AI Assistants (b) Agents (c) Tools (d) Models (e) Storage (f) Prompts. Extending beyond conventional methods, some works [106] use ChatGPT to design an automated attack surface generation process - to generate personalised attack surfaces. Such research not only provides a novel approach but also has a positive impact on the defence and prevention of cyber threats.\nA previously adapted framework to understand the AI attack surface was STRIDE [95]. STRIDE is an acronym for Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, and Elevation of Privilege. The framework aimed to assist in identifying, understanding, and addressing potential threats before AI systems were implemented, therefore reinforcing the security of these advanced systems."}, {"title": "3 Offensive using Al", "content": "This section delves into offensive strategies using Al systems. It involves using AI to manipulate information, deceive individuals, and exploit vulnerabilities."}, {"title": "3.1 ATT&CK Techniques", "content": "The ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) framework is an extensive repository by the prestigious MITRE Corporation [86]. This framework classifies and records practical tactics, techniques, and procedures consistently employed by malicious cyber actors. ATT&CK furnishes invaluable insights, which are a fundamental resource for threat intelligence and security operations teams. We discuss the components of this framework in the subsections below, in order of correspondence."}, {"title": "3.1.1 Information Gathering", "content": "It represents a phase where adversaries collect intelligence about their targets. This phase helps understand the target environment, identify potential vulnerabilities, and tailor subsequent attack strategies. The information-gathering techniques outlined in ATT&CK cover a broad spectrum, encompassing both active and passive methods.\nActive Information Gathering. It involves direct interaction with target systems or networks to extract specific details. This can include network scans, port scans, and service enumeration to identify available assets and potential vulnerabilities. Adversaries may employ tools like Nmap [63] for network scanning or use reconnaissance tools to actively probe and map the target environment.\nPassive Information Gathering. It is a more covert approach, relying on the analysis of publicly available data without direct interaction with the target. This can include monitoring open-source intelligence (OSINT) [62] and publicly accessible information. Adversaries may utilise WHOIS databases, search engines, and social media to compile information about the target's infrastructure, personnel, and affiliations."}, {"title": "3.1.2 Social Engineering", "content": "These attacks focus on the manipulation and exploitation of people. The idea is to convince the target to perform actions or divulge confidential information that benefits the adversary. While similar to a confidence trick or simple fraud, the term typically applies to trickery or deception for information gathering, fraud, or computer system access. In most cases, the adversary may not come face-to-face with the victim.\nReported incidents include attackers utilising fabricated audio, generated with Al assistance, to impersonate executives or managers and orchestrate financial fraud [5]. The success of such attacks hinges on the psychological impact of hearing a familiar voice, exploiting the lack of scepticism in voice recognition. Attacks involving fabricated audio, or audio deepfakes, pose a more significant threat than their video counterparts - due to the psychological impact of hearing a familiar voice in an urgent pretext.\nAs shown by Julian [53], social engineering tactics often involve collecting data- for instance, using GPT-4 to scrape Wikipedia profiles of all British MPs elected in 2019. This unstructured data is then fed into GPT-3.5, instructing it to generate biographies for each MP. Additionally, threat actors may contact AI models to create characteristics that enhance the effectiveness of spear phishing emails. These characteristics encompass personalisation, contextual relevance, and authority, showcasing how social engineering campaigns are more convincing and potent. We discuss phishing below."}, {"title": "3.1.3 Phishing", "content": "It is an attack in which attackers use deceptive emails, messages, or websites to trick individuals into divulging sensitive information, such as login credentials, personal details, or financial information. These fraudulent communications often appear to come from reputable sources, such as banks, government agencies, or well-known companies.\nSpear phishing is a more targeted form involving personalised and highly tailored attacks that increase the chances of user susceptibility [14]. Attackers conduct in-depth research on their targets to create convincing and contextually relevant messages. These messages often leverage information about the target's relationships, job roles, or activities, making them appear more legitimate and harder to detect.\nAI advancements are being harnessed for phishing activities, particularly within business email compromise (BEC) attacks. One can write cunning emails with impeccable grammar, making them appear legitimate and reducing the likelihood of being flagged as suspicious as shown by WormGPT [54]. The adoption of generative AI, notably exemplified by ChatGPT, presents a pivotal shift within landscape BEC attacks [53]. ChatGPT's ability to generate human-like text allows adversaries to automate the creation of compelling fake emails personalised to the recipient's profile. This personalisation significantly amplifies the success rates of BEC attacks by enhancing the credibility of the deceptive emails. Additionally, it has been shown how basic prompt engineering with RLHF can circumvent safeguards installed in LLMs [53].\nAn interesting facet of this approach is the recommendation to compose emails in one's native language, translate them, and employ ChatGPT to enhance linguistic sophistication. Voice cloning technology might also be used for phishing. Overall, phishing is about volume. Numerous toolkits, like SET and SNAP_R [110], have been devised to automate the generation of phishing payloads. Furthermore, social-media posts can be targeted by shortening the URL using goo.gl [110] and appending the post with the user's username - hence increasing the chances of someone clicking on the link!\nLiterature mentions methods such as neural networks and ML classifiers that can identify anomalies in a message that indicate a phishing attack [13, 96]. Policies such as stringent email verification have been proposed as well. It is also recommended to test security efficacy in observability mode, providing an additional layer of defence against evolving threats."}, {"title": "3.1.4 Cybersecurity Exploitation", "content": "It is an umbrella term that refers to maliciously exploiting vulnerabilities, weaknesses, or security flaws in computer systems, networks, or digital assets. It involves using automated tools to exploit security measures and gain unauthorised access to protected systems. A heavily cited paper by Avgerinos et al. [8] demonstrates AEM, an exploit generation program that automatically finds vulnerabilities and generates exploits for any given program. In a significant step, two of the exploits they generated were zero-day exploits against unknown vulnerabilities. Similarly, FUGIO [76], a fuzzy-logic-based tool, is claimed to perform automatic exploit generation for object injection vulnerabilities."}, {"title": "3.1.5 Point of Entry (PoE) Detection", "content": "It allows organisations to identify and thwart potential threats at their earliest stages. It involves monitoring network access points and endpoints to detect unauthorised or suspicious activity that may indicate a breach or attack. By analysing traffic patterns, anomalies, and known attack vectors, PoE detection tools can raise alerts or take action to mitigate risks. Organisations often employ intrusion detection and prevention systems, firewall logs, and security information and event management (SIEM) solutions to enhance PoE detection capabilities.\nDeepLocker [55], developed as a proof of concept by IBM Research, offers a striking example of the evolving threat landscape within PoE detection. It is a highly evasive breed of malware that conceals its malicious intent until it reaches"}, {"title": "3.1.6 Lateral Movement Detection", "content": "It focuses on identifying the techniques cyberattackers use to progress through a network after initially breaching it. Once inside the network, these attackers employ various methods and tools to gain increased privileges and access high-value assets. Lateral movement distinguishes APTs from simpler cyberattacks [84].\nThe involvement of RL presents an intriguing concept. RL involves the creation of an agent that learns by interacting with its environment. Regarding lateral movement detection, the environment is the malware sample, and the agent is the algorithm used to change the environment. An Al agent using RL aims to determine functionality-preserving transformations that can be applied to a malware sample to evade static-analysis malware detection systems. The agent interacts with the malware sample, sending actions representing various binary manipulations. These manipulations include\n\u2022 append_random_ascii\n\u2022 append_random_bytes\n\u2022 remove_signature\nInitially, the agent may randomly select these actions in an attempt to bypass the classifier. However, over time, the agent learns from the environment's responses, identifying combinations of actions that yield the highest rewards or an optimal strategy for bypassing the malware classifier.\nIn an era where advanced threats continue to evolve and challenge traditional security measures, leveraging innovative approaches like RL can aid in identifying and responding to the lateral movement of attackers within a network."}, {"title": "3.1.7 Credential Theft", "content": "It is a cybersecurity concern involving the unauthorised acquisition of sensitive login information, such as usernames and passwords. Attackers target these credentials to gain illegal access to systems, networks, and valuable data, posing a significant threat to organisations and individuals. Credential theft remains a major problem for companies of all sizes, including governments. It is shown that nearly 50% of all data breaches in 2022 were caused by stolen credentials [92]. Even after years of concerted efforts to curb credential theft, including warnings, changing password requirements, and multiple forms of authentication, credential theft remains a top attack method used by cybercriminals."}, {"title": "3.1.8 Autonomous Malware", "content": "This represents a significant shift in cybersecurity. Autonomous malware is characterised by threat agents employing AI to make informed, on-the-fly decisions, resulting in autonomous cyber damage. A cognitive threat agent encompasses various vital elements\n\u2022 Observation and reasoning: Assessing the environment and making informed decisions by dynamic prompting.\n\u2022 Evolving and adapting: Transforming decisions into real-time code.\n\u2022 Learning and correcting itself: Continually assimilating feedback, managing errors, and refining their capabilities.\nTo further enhance evasive capabilities, autonomous agents employ strategies like (i) polymorphic behaviour to confound detection methods, (ii) in-memory compiling and reflection to minimise its on-disk footprint, (iii) natural delays, (iv) varying execution times for enhanced evasion, and (v) dynamic code synthesis to challenge traditional security mechanisms [7, 66].\nThese agents may draw inspiration from biological viruses, mimicking their adaptability through strategic target selection, dormant periods, and impact maximisation. They may also incorporate stigmergic communication models, enabling agents to interact collectively without centralised control. EyeSpy [91] and self-learning worms [25] show the above ideas being used in real life. As seen earlier, RL and Markov decision process (MDP) techniques are crucial for the dynamic decision-making power of agents. Such intelligent botnets have expanded their reach, targeting various platforms, including IoT, smartphones, and cloud environments [29].\nSuch agents can disrupt traditional detection methods by eliminating centralised command and control (C&C) communication, reducing the effectiveness of conventional detection strategies. They have military applications, such as serving as virtual armies or tools for cyber espionage. Intelligent bots may collaborate with traditional ones or defend them, creating complexities for detection. They employ techniques like DGA and peer-to-peer (P2P) based network communication to evade detection systems. As shown in literature [112], a multi-agent system (MAS) approach is crucial to combat these emerging threats."}, {"title": "3.1.9 Weaponised Al - Ransomware", "content": "The landscape of weaponised AI and ransomware involves embedding malware, particularly ransomware, into ML models. It enables their deployment without detection by conventional security solutions [103]. The technique involves encoding a payload within tensors, executing it automatically during the model loading process, and deploying payloads reflectively to evade detection measures.\nThe proliferation of pre-trained models sourced from platforms like Hugging Face and TensorFlow Hub raises concerns about attackers leveraging these models as pathways for malware distribution. We show a critical example of vulnerabilities in serialisation formats associated with pre-trained ML models. The Python pickle module, which serialises and deserialises a Python object, is shown to be vulnerable to remote code execution. If a website uses this module, an attacker may be able to execute arbitrary code. A study [41] shows how a pre-trained ResNet18 model can be compromised similarly.\nPotential detection mechanisms for ransomwares emphasise using secure sandboxed environments during model loading [9]. Moreover, LLMs play a pivotal role in the creation and enhancement of ransomwares [20]. These models assist threat actors in crafting new malware and refining existing ones."}, {"title": "3.2 Deepfakes and Manipulated Media", "content": "Threat actors are growing interested in using AI-generated content for manipulating media, particularly deepfakes [5]. The strategy employs AI tools to craft hyper-realistic deceptive content across various mediums, including images, videos, audio, and text. Generative Adversarial Networks (GANs) contribute to persona building through images,"}, {"title": "3.3 Misinformation and Propaganda", "content": "Al services have increasingly been used to spread misinformation and push selective propaganda. Patel [77] provides multiple striking examples. In the context of misinformation, adversaries exploit inexpensive services to manipulate app store ratings, post fake reviews and comments, inflate online polls, and boost engagement on social networks. It has been seen how Amazon's recommendation algorithm was manipulated to endorse anti-vaccination literature, spread extremist ideologies, and support misinformation campaigns. This was seen to extrapolate into extremely critical issues such as spreading white supremacy, anti-semitism, and islamophobia. It was also recently discovered that people used creative naming schemes to bypass Amazon's detection logic to sell boogaloo-related merchandise.\nTo manipulate online platforms like YouTube or Twitter, tactics involve repeatedly viewing or engaging with specific content. For instance, causing a hashtag to trend involves extensive posting or retweeting. Creating visibility for a new fake political account is achieved by having numerous other fake accounts follow and consistently engage with its content. Twitter further allows tactics like retweeting, undoing, and then retweeting again, known as re-retweeting, to showcase content repeatedly. Such reply-spam tactics often deceive individuals, especially during critical or panicked situations such as a war or a political event like the 2019 UK general elections [39].\nBefore executing a genuine attack, adversaries may assess a system's detection capabilities through preliminary probing. It involves using throw-away accounts to understand the automated detection mechanisms. Once the system's defences are understood, adversaries can create numerous fake accounts designed to mimic the appearance and behaviour of legitimate users without triggering detection.\nAn indirect way of spreading misinformation can involve these steps: (i) bypassing existing safety guardrails of models, (ii) generating illicit content, and (iii) pushing claims that the content was generated by the model."}, {"title": "4 Consequences and Impact", "content": "As we dive into the consequences and impacts of offensive AI, we highlight the significance of robust frameworks like SAIF (Secure AI Framework) in addressing specific risks tied to Al systems [42]. In the upcoming subsections, we will check the domain-specific consequences of offensive AI and give a few case studies."}, {"title": "4.1 Domain-Specificity Consequences", "content": "The effects of AI are domain-specific, based on the industry and application. For example, within the consumer domain, AI applications may enhance personalised experiences and product recommendations. In the enterprise sector, AI can streamline operations and enhance decision-making processes, while in the public digital infrastructure domain, it may contribute to improved civic services and governance. We discuss three major such domains in detail."}, {"title": "4.1.1 Consumer", "content": "Within the consumer domain, it is crucial to acknowledge the limitations of LLMs. Literature suggests that LLMs operate without a proper understanding of the semantic meaning of sentences [1]. Instead, they employ mathematical calculations to predict the most likely next word based on the input. Authors of the same paper mention that LLMs do not encode an understanding of our world, such as the relationships between objects. One must grasp this limitation to avoid automation bias, where excessive trust is placed in the model's output, and anthropomorphism, where users form a human-like connection with the LLM [88]."}, {"title": "4.1.2 Enterprise", "content": "In enterprises, the risks associated with offensive AI are widespread. For instance, a deepfake-driven public relations crisis can tarnish a brand's reputation. As shown in other works [6], the threat of AI-driven malware to corporate networks has the potential for huge financial losses.\nAs illustrated by a notable case [11], the seemingly harmless act of sharing a dataset can result in data leaks. In publishing open-source training data on GitHub, Microsoft's Al research team inadvertently disclosed additional sensitive information, including secrets, private keys, passwords, and about 30,000 internal Microsoft Teams messages. This exposed over 38TB of private data to the public. The root cause of this breach was the use of account SAS tokens as the sharing mechanism. It should be noted that an attacker could have injected malicious code into all the AI models in this storage account, and every user who trusts Microsoft's GitHub repository would have been infected by it.\nThe incident underscores the emerging risks that organisations encounter as they increasingly harness the power of AI. The sheer volume of training data handled by data scientists and engineers demands heightened security measures. Two primary concerns come to the forefront. First, the challenge of oversharing data arises because researchers amass vast quantities of external and internal data to construct training information. To mitigate this risk, security teams must establish clear guidelines for the external sharing of AI datasets. For instance, segregating public AI datasets into dedicated storage accounts could curtail exposure. Second, the threats of supply chain attacks loom large, exemplified by improper permissions granted by public tokens (Section 2.1.3). As noted earlier, injecting malicious code into the model files could have led to a supply chain attack on other researchers who use the repository's models. Security teams must rigorously review and sanitise Al models from external sources, recognising their potential as vectors for remote code execution."}, {"title": "4.1.3 Public Digital Infrastructure and Policy", "content": "This section examines how Al influences public services, digital infrastructure, and policy frameworks. We explore the nuances of deploying AI in the public domain, emphasising the need for ethical policymaking.\nConsider the integrity of medical AI. Adversarial samples threaten misdiagnosis and fool radiologists [69]. Such an attack on a public domain can potentially disrupt public trust and compromise the digital infrastructure at once. The current defence mechanisms face limitations, as slight modifications to an attack can consistently evade new defences. Crafting effective adversarial samples involves creating imperceptible noise, and the study by Moshe et al. [71] evaluates strategies for breaking various existing defence methods. A robust approach towards safeguarding involves utilising digital signatures (DS). It involves imaging modalities that can sign scans upon creation, allowing the software to verify authenticity by checking the signature. This cryptographic approach significantly minimises the attack surface. Additionally, it offers a formidable defence against both adversarial samples and deepfake injections.\nConcerns extend beyond medical AI, such as addressing the inherent challenges in building secure ML models [32]. The need for high accuracy often involves memorising large, heterogeneous, and user-generated training datasets, raising privacy concerns. This dataset may contain both sensitive public information and information from fake users. LLMs face vulnerabilities from both privacy invasion through memorised data and malicious data providers poisoning training data (Section 2.1.2). Malicious data providers can also exploit LLMs by adding offensive, violent, or dangerous"}, {"title": "4.2 Case Studies", "content": "In navigating the evolving landscape of cyber threats, one notable proof-of-concept, EyeSpy, signals the potential future trajectory of malware designs [91]. As discussed in 3.1.8, EyeSpy enables threat agents to interact collectively without centralised control. Shifting the focus to email security, another case emerges. Faced with concerns about the potential blacklisting of IPs, experimentations [23] revealed an intriguing discovery: Gmail's spam filters seemed to misclassify standard Tor responses as spam. Leveraging the power of ChatGPT, they could successfully deliver messages that bypassed Gmail's spam filters, showcasing the pragmatic application of AI in addressing real-world concerns. In yet another striking example, it was seen how it took less than 24 hours for Twitter to corrupt an innocent AI chatbot. Microsoft's Tay, a Twitter bot described as an experiment in conversational understanding, picked up racist sentiments in less than a day [99].\nAnother recent incident illustrating security concerns involved the torchtriton package by OpenAI. A dependency issue with PyPi led to an impact on PyTorch-nightly builds of torchtriton for a few days in December 2022. Within this time frame, individuals who downloaded PyTorch nightly versions unknowingly acquired the compromised package, exposing their systems to potential risks [60]. The attacker exploited the compromised package to access sensitive information like SSH keys and password files. Despite the attacker's claims of being a researcher, their actions raise doubts about their true intentions.\nOn the other hand, a recent study at Stanford University found that users with access to CoPilot [28] tend to produce less secure code than those without access, despite the users' belief in the security of their code. Another study based on OpenAI's davinci model [80] had the same observations. It should be noted that the study examined factors like the inclusion of helper functions and adjustments to model parameters for checking secure code.\nA critical development reported that Samsung employees had input software code related to sensitive semiconductor capabilities, seeking guidance from ChatGPT on code improvement. Since OpenAI explicitly states that all data entered into ChatGPT prompts can be used for AI training, Samsung faced a potential risk of revealing sensitive information. To address this concern, OpenAI now offers users the option to forego retaining their chat history, ensuring that their prompts are not utilised for model improvement."}, {"title": "4.3 Insights and Observations", "content": "It becomes evident that the landscape of offensive AI demands a multifaceted response. Advanced protective measures and continuous research to understand evolving threats are paramount. Our survey emphasises the need for collaborative efforts among researchers, practitioners, policymakers, and stakeholders. In Section 3.1.9, we covered how AI-powered malware is rising, demonstrating autonomy and adaptability in navigating new environments. This evolution marks a paradigm shift in cyber threats, necessitating strict vigilance and innovative defensive strategies.\nPropelled by AI capabilities, social engineering has become a potent tool for crafting personalised and convincing phishing attacks. The use of AI to generate deceptive URLs and phishing emails underscores the sophistication of modern cyber threats, emphasising the need for robust defences against socially engineered attacks. Furthermore, the study delves into the vulnerabilities of AI systems themselves. Adversarial inputs, poisoning of training data, and model extraction attacks pose substantial risks to the integrity of AI models. These internal threats highlight the importance of securing the application layer and the very foundations of AI systems.\nIt is important to reiterate the deep societal implications of AI-driven manipulations, especially in the context of recommendation systems used by social networks [77]. Algorithmic manipulations have propagated false stories and altered genuine news pieces, statistics, and online polls. These disinformation mechanisms substantially threaten public health, societal cohesion, and civic order. Thus, the concluding call to action emphasises the critical need for collaborative and concerted efforts to mitigate these risks and uphold the ethical use of AI in the digital age."}, {"title": "4.4 Avenues for further research", "content": "Exploring offensive AI and its cybersecurity implications opens up several avenues for future research. The findings from this study will serve as a foundational framework for subsequent investigations. Future research endeavours may consider delving into the proactive identification and mitigation of AI-powered malware. Understanding the mechanisms of autonomous, adaptive malware and developing preemptive strategies to counteract its potential impact would be instrumental in fortifying cybersecurity measures [96].\nSocial engineering attacks driven by AI present an intriguing area for exploration. Research can focus on refining defences against personalised phishing emails and deceptive URLs, unravelling the intricacies of socially engineered attacks. Developing countermeasures that anticipate and thwart evolving social engineering tactics will be crucial for mitigating cyber risks. Furthermore, the vulnerabilities within Al systems demand in-depth research into safeguarding"}, {"title": "5 Conclusion", "content": "In conclusion, this survey paper has comprehensively explored the multifaceted dimensions of offensive AI, shedding light on its techniques, consequences, and future implications. The study has demonstrated the vulnerabilities of AI systems to various types of attacks, including adversarial machine learning, poisoning attacks, and model theft. It has also highlighted the role of AI in offensive schemes, such as information gathering, social engineering, and weaponized AI. The paper's findings underscore the need for proactive measures to secure Al systems, particularly in domains like consumer, enterprise, and public digital infrastructure. As Al continues to integrate into essential applications, understanding and mitigating security risks in machine learning has emerged as a pivotal aspect of cybersecurity. This survey aims to serve as a valuable resource for informed decision-making and policy development within the AI security"}]}