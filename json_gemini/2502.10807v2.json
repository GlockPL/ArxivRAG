{"title": "HybriDNA: A Hybrid Transformer-Mamba2\nLong-Range DNA Language Model", "authors": ["Mingqian Ma", "Guoqing Liu", "Chuan Cao", "Pan Deng", "Tri Dao", "Albert Gu", "Peiran Jin", "Zhao Yang", "Yingce Xia", "Renqian Luo", "Pipi Hu", "Zun Wang", "Yuan-Jyue Chen", "Haiguang Liu", "Tao Qin"], "abstract": "Advances in natural language processing and large language models have sparked\ngrowing interest in modeling DNA, often referred to as the \"language of life\". How-\never, DNA modeling poses unique challenges. First, it requires the ability to process\nultra-long DNA sequences while preserving single-nucleotide resolution, as individual\nnucleotides play a critical role in DNA function. Second, success in this domain requires\nexcelling at both generative and understanding tasks: generative tasks hold potential\nfor therapeutic and industrial applications, while understanding tasks provide crucial\ninsights into biological mechanisms and diseases. To address these challenges, we pro-\npose HybriDNA, a decoder-only DNA language model that incorporates a hybrid", "sections": [{"title": "1 Introduction", "content": "Deoxyribonucleic acid (DNA) serves as the genetic code of life, encoding the instruc-\ntions that govern gene expression, cellular processes, and biological functions. A deep\nunderstanding of the \"language\" of DNA is crucial for unraveling the molecular mech-\nanisms that underlie biological functions and for leveraging these insights to advance\nmedicine and biotechnology. The advent of high-throughput sequencing technolo-\ngies has generated an immense volume of genomic data, creating an unprecedented\nopportunity for machine learning models to uncover complex patterns and relation-\nships within DNA sequences. Foundation models, pretrained on large-scale unlabeled\ndatasets, have already demonstrated remarkable capabilities in natural languages and protein languages.\nRecently, foundation models have begun to drive a paradigm shift in genomics,\nshowcasing their ability to learn rich representations of DNA sequences that can\nbe fine-tuned for a diverse array of downstream tasks. Currently, DNA founda-\ntion models primarily adopt two main architectural approaches. The first approach,\ninspired by BERT [1], employs encoder-only Transformer architectures. Models such\nas DNABERT2 [7] and Nucleotide Transformer (NT) [8] excel at capturing contex-\ntual information within DNA sequences, producing high-quality embeddings suitable\nfor tasks such as classification and regression. However, their bidirectional nature\nconstrains their ability to design novel DNA sequences. The second approach lever-\nages decoder-only architectures, such as Hyena [9] and the Transformer architecture\nin GPT [10], which are autoregressive and well-suited for generative tasks. Models\nlike HyenaDNA [11] and Evo [12] have shown promising results in generating DNA\nsequences. Nevertheless, they often fall behind encoder-only models in understanding\ntasks requiring a deep understanding of sequence context.\nThis dichotomy highlights two critical challenges in DNA modeling: (1) How to\ndevelop a DNA foundation model that integrates robust contextual understanding\nwith advanced design capabilities? Such a model would not only enhance the anal-\nysis of existing genomic data but also enable the design of novel, functional DNA\nsequences. (2) How to efficiently address the intricate complexity of DNA sequences,\nwhich involves long-range interactions critical to fundamental biological processes?\nRecent advances in Selective State Space Models (SSMs), such as Mamba [13, 14],"}, {"title": "2 Preliminaries", "content": "Powering many foundation models is the attention mechanism [19, 20] in Transformers.\nAttention is a type of operator that assigns scores to every pair of tokens in a sequence,\nenabling each element to \"attend\" to the others. The most widely adopted variant of\nattention to date is Scaled Dot-Product Attention, which is defined as:\n$y = softmax(\\frac{QK^T}{\\sqrt{d_k}}) \\cdot V$.\nLet $x \\in \\mathbb{R}^{L\\times d}$ represents an input sequence with sequence length $L$ and embedding\nsize $d$, the learnable parameters $W_K \\in \\mathbb{R}^{d\\times d_k}, W_Q \\in \\mathbb{R}^{d\\times d_k}$, and $W_V \\in \\mathbb{R}^{d\\times d}$ are used\nto compute the key, query, and value matrices: $K = xW_K, Q = xW_q$, and $V = xW_v$.\nThe attention layer, therefore, transforms an input $x$ of shape $\\mathbb{R}^{L\\times d}$ into an output $y$\nof the same shape, $\\mathbb{R}^{L\\times d}$.\nAttention computes all pairwise comparisons for every token in a sequence, result-\ning in a computational complexity that scales as $O(L^2)$ with sequence length $L$. While\nthis enables capturing global context at high resolution, it also restricts the context\nlength on modern GPU architectures."}, {"title": "2.2 Selective State Space Models", "content": "Structured state space sequence models (S4) [21, 22] are a recent class of sequence\nmodels in deep learning that are broadly related to RNNS, CNNs, and classical state\nspace models. They are inspired by a specific continuous system, which maps a 1-\ndimensional sequence $x \\in \\mathbb{R}^L \\mapsto y \\in \\mathbb{R}^L$ via a hidden state $h\\in \\mathbb{R}^{(L,N)}$, where $L$\nrepresents the sequence length, and $N$ represents the SSM state size.\nSpecifically, the continuous system is defined by three matrices (A, B, C), \u0410 \u0404\n$\\mathbb{R}^{N\\times N}, B\\in \\mathbb{R}^{N\\times 1}$, and $C\\in \\mathbb{R}^{1\\times N}$. They define a sequence-to-sequence transformation\nin two steps, as detailed in Eqn. 2 (first column).\n$h'(t) = Ah(t) + Bx(t),$\n$y(t) = Ch(t).$\n$h_t = Ah_{t-1} + Bx_t,$\n$y_t = Ch_t.$\n$K = (CB, CA\u0304B, ..., CA\u0304*B),$\n$y = x * K.$\nDiscretization S4 models are discrete versions of the continuous system (as shown\nin the second column of Eqn. 2), which incorporates a timescale parameter \u0394\u2208R to\ntransform the continuous parameters A, and B into their discrete counterparts, A and\nB. The zero-order hold (ZOH) is a commonly used method for this transformation,\ndefined as follows, where \"exp\" represents the exponential function.\n$\\bar{A} = exp(\u0394A), \\bar{B} = (\u0394A)^{-1}(exp(\u0394A) \u2013 I) \\cdot \u0394B.$\nComputation After the parameters have been transformed into (A, B), the model\ncan be computed in two ways, either as a linear recurrence (corresponding to the"}, {"title": "3 HybriDNA Foundation Model", "content": "In this section, we introduce the HybriDNA model for long-range genomic sequence\nmodeling. We begin with a detailed description of the model architecture, followed\nby an explanation of the pretraining stage of HybriDNA. Finally, we discuss the\nfine-tuning stages used for various downstream applications. The model's pipeline is\nillustrated in Fig. 1."}, {"title": "3.1 Model Architecture", "content": "The HybriDNA model uses a decoder-only, sequence-to-sequence architecture purpose-\nbuilt for efficiently and accurately processing long-range DNA sequences. It combines\nthe strengths of Mamba2 selective state-space models and Transformer attention\nmechanisms within a hybrid framework inspired by recent hybrid architectures\n[16, 28, 29]. As shown in Fig. 1 (second column), the architecture consists of a series\nof HybriDNA blocks, where each block alternates between HybriDNA Mamba2 blocks\nand HybriDNA Transformer blocks in a 7:1 ratio. This configuration has been empir-\nically proven to effectively balance the advantages of both block types, achieving\noptimal performance in the NLP domain [15].\nA key component of the HybriDNA Mamba2 block is the State-Space Duality\n(SSD) layer [14]. It processes input sequence x using the recurrence:\n$h_t = Ath_{t-1}+ Bx_t,$\n$Y_t = Cht,$\nwhere $h_t \\in \\mathbb{R}^N$ denotes the hidden state, $A_t \\in \\mathbb{R}^{N\\times N}$ represents the state transitions,\n$x_t \\in \\mathbb{R}$ is the input, $B_t \\in \\mathbb{R}^{N\\times 1}$ projects the input, and $C_t \\in \\mathbb{R}^{N\\times 1}$ maps the hidden\nstate to the output $y_t \\in \\mathbb{R}$.\nThe SSD layer simplifies the matrix $A_t$ to $A_t = atI$, where $a_t \\in \\mathbb{R}$ is a scalar and\n$I$ is the identity matrix, to further improve efficiency. This simplification reduces the\nrecurrence to:\n$ht = ath_{t-1} + Btxt.$"}, {"title": "3.2 Pretraining on Multi-Species Genomes", "content": "Dataset We pretrain HybriDNA on a large-scale, multi-species genome dataset\nusing next nucleotide (token) prediction (NTP). This dataset was curated from the\nNucleotide Transformer [8] and NCBI, and was down-sampled to include 845 species,\ncollectively comprising 160 billion nucleotides. Table 1 provides a summary of the\ncontribution of each genome class, represented as the number of nucleotides relative\nto the total nucleotide count in the dataset. A diverse and comprehensive genome\ndataset encompassing multiple species is essential for enabling the model to effec-\ntively interpret a wide range of genomic sequences. Such a dataset ensures that the\nmodel captures patterns and interactions representative of various biological systems,\nthereby enhancing its ability to generalize across species and tasks.\nTokenizer HybriDNA employs a straightforward and effective base-level tok-\nenization strategy, encoding each nucleotide (A, C, T, G) as an individual token. This\nstrategy ensures that the model processes genomic data with high fidelity to its natural\nstructure, enabling nuanced interpretation and feature extraction. Unlike higher-order\ntokenization schemes that aggregate multiple bases into a single token, the base-level\nstrategy treats each nucleotide as a fundamental unit, preserving its unique contri-\nbution to genomic patterns. This method is particularly advantageous for capturing"}, {"title": "3.3 Downstream Fine-tuning", "content": "To develop a DNA foundation model capable of handling both generative and under-\nstanding downstream tasks, HybriDNA employs a GPT-like decoder-only architecture.\nHowever, a key limitation of autoregressive models, compared to bidirectional models,\nis their inability to incorporate information from future tokens into the embeddings\nof current tokens. To address this issue, HybriDNA introduces a novel echo embedding\ntechnique during the fine-tuning stage for understanding tasks, drawing inspiration\nfrom the work of [32].\nThe core idea of this method is that repeating sequences facilitates the encoding of\ncontextual information from subsequent elements into the embeddings. To illustrate,\nconsider an input sequence x and its corresponding label y in a classification task with"}, {"title": "3.3.2 Generative Fine-tuning for DNA Generation Tasks", "content": "Autoregressive natural language models, such as ChatGPT, are capable of generating\nhighly realistic, human-like text while adhering to human instructions to produce\nsatisfactory responses [18]. In a similar vein, HybriDNA is an autoregressive model\npretrained on multi-species genomic data at single-nucleotide resolution, unlocking the\npotential to design novel and realistic DNA sequences for a broad range of real-world\napplications.\nTo achieve this, we introduce a set of prompt tokens specifically designed to encode\ntask-specific instructions. These prompt tokens are incorporated into the existing\nnucleotide vocabulary and initialized randomly within the embedding layer, which is\nexpanded to include additional rows corresponding to the newly introduced token IDs.\nHybriDNA then predicts each nucleotide token $x_t$ autoregressively, conditioned on all\npreceding prompt tokens that specify the task-specific requirements.\nSpecifically, we optimize the next token prediction loss:\n$Lgenerative(0) = \\frac{1}{T} \u03a3 log (p_@ (Xt | z_0,..., Zk-1, x_0,..., Xt-1)),$\nwhere @ represents the complete set of model parameters, $2k-1$ denotes the k-th prompt\ntokens, and xt\u22121 represents the t-th generated nucleotide token. By minimizing Eqn. 7,\nHybriDNA is trained to interpret the specialized tokens and generate realistic genomic\nsequences that align with specific design objectives."}, {"title": "4 Experiments", "content": "In the experiment section, we aim to answer the following questions regarding the\nHybriDNA models and their key capabilities: (1) How do the pretraining losses of\nHybriDNA models compare across different scales and configurations? (2) Can the\nmodels achieve state-of-the-art performance on short-range understanding bench-\nmarks, and how does scaling affect their effectiveness? (3) How do the models\nperform on long-range understanding tasks, and do they exhibit improved results with\nincreased pretraining context length? (4) Can the models generate realistic and desir-\nable regulatory sequences across multiple species? (5) How do the models compare to\npure transformer-based models in terms of computational efficiency during training?\nWe benchmark our model against a series of recently proposed tasks, including\nDNABERT2 [7], BEND [33], and Genomics LRB [34]. In selecting tasks for com-\nparison, we adhere to the principle of encompassing a diverse range of challenges,\nencompassing both short and long-range capabilities. Additionally, we prioritize tasks\nthat are biologically meaningful, covering a variety of species and functionalities within\nDNA-related areas."}, {"title": "4.1 Pretraining curves", "content": "We train three variants of HybriDNA with 300M, 3B, and 7B parameters. These mod-\nels differ in the number of layers, hidden size, and learning rates used during training.\nDespite these variations, all models share a consistent pretraining strategy. Instead of\nthe commonly used masked language modeling (MLM) loss in genomics foundation\nmodels, we use a next-token-prediction (NTP) loss to enable generative capability.\nOur models are trained on NVIDIA A100/H100 and AMD MI300X GPUs. Details of\nthe model architecture and pertaining configurations can be found in Appendix A.\nDuring the pretraining stage, our 300M, 3B and 7B parameter models are trained\non 0.5M tokens per batch, optimized for efficient utilization of computational resources\nand consistent training dynamics. Initially, the models are pretrained on sequences\nwith a context length of 8,192 for 500k steps, resulting in a total of 250B tokens\n(approximately 1.5 epochs) in the first pretraining stage. Following this, the mod-\nels undergo further pretraining to extend their capabilities to handle larger context\nlengths. This two-stage pretraining strategy allows the models to gradually adapt to\nmore complex and computationally demanding settings, ensuring robust performance\nacross varying sequence lengths."}, {"title": "5 Related Work", "content": "The advent of high-throughput sequencing technologies has produced vast amounts of\ngenomic data, presenting an unprecedented opportunity for deep learning to uncover\ncomplex relationships and dependencies in DNA sequences. Recent advancements\nin genome language modeling have demonstrated their effectiveness across a wide\nrange of downstream applications, including promoter prediction [36, 37], gene expres-sion prediction [38], DNA methylation prediction [39], chromatin state analysis [40],\npromoter-enhancer interaction prediction [41, 42] TF-DNA binding prediction [43],\nvariant effect prediction [44], gene network prediction [45] and more. More recently,\ninspired by advancements in natural language processing, researchers have begun\ndeveloping DNA foundation models. These include, but are not limited to: (1)\nencoder-only models such as DNABERT, DNABERT-2, Nucleotide Transformer, and\nCaduceus; and (2) decoder-only models such as HyenaDNA and Evo."}, {"title": "5.2 Hybrid Models in General Domains", "content": "Recent advancements in Mamba-based hybrid models for NLP tasks combine the\nefficiency of SSMs with the expressiveness of attention mechanisms, excelling in\nlong-context scenarios. Innovations include Jamba's [16] integration of Transformer,\nMamba, and Mixture-of-Experts layers for sequences up to 256k tokens, Zamba's [28]\ncompact 7B model with shared self-attention for reduced latency, and SAMBA's [29]\nsliding window attention for efficient handling of sequences up to 1M tokens. Other\nnotable contributions include Taipan's [50] selective attention layers for scalability and\nWaleffe's [15] versatile 8B hybrid architecture combining Mamba2, self-attention, and\nMLP layers. These models achieve strong results across various short- and long-range\nbenchmarks."}, {"title": "6 Conclusion", "content": "In this work, we develop a class of decoder-only DNA language models built on a\nhybrid Transformer-Mamba2 architecture. This design harnesses the unique strengths\nof its two core components to enable efficient and precise modeling of DNA sequences.\nBy integrating Mamba2 layers, our model can process extremely long DNA sequences\nat single-nucleotide resolution with remarkable computational efficiency. Pretrained\non large-scale, multi-species genomes at single-nucleotide resolution with a next-token\nprediction objective, HybriDNA demonstrates foundational capabilities in both under-\nstanding and designing genomic sequences. Through echo embedding discriminative\nfine-tuning, HybriDNA achieves state-of-the-art performance across 33 biologically\nsignificant DNA understanding tasks from the BEND, GUE, and LRB benchmarks.\nThrough generative fine-tuning, HybriDNA exhibits remarkable proficiency in generat-\ning synthetic cis-regulatory elements with desirable functional properties. These results\nhighlight HybriDNA's versatility and establish its potential as a powerful foundation\nmodel for advancing DNA research and applications.\nLooking ahead, there are several exciting directions to further explore. These\ninclude: (1) Expanding the pretraining dataset to include a greater number of\nnucleotide tokens and species classes, enabling broader generalization across down-\nstream tasks involving diverse species. (2) Conducting more downstream fine-tuning\ntasks with diverse and significant scientific impacts, and performing wet-lab experi-\nments to further validate the sequences designed by HybriDNA."}]}