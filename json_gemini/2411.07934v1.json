{"title": "Doubly Mild Generalization for Offline Reinforcement Learning", "authors": ["Yixiu Mao", "Qi Wang", "Yun Qu", "Yuhang Jiang", "Xiangyang Ji"], "abstract": "Offline Reinforcement Learning (RL) suffers from the extrapolation error and value overestimation. From a generalization perspective, this issue can be attributed to the over-generalization of value functions or policies towards out-of-distribution (OOD) actions. Significant efforts have been devoted to mitigating such generalization, and recent in-sample learning approaches have further succeeded in entirely eschewing it. Nevertheless, we show that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. To appropriately exploit generalization in offline RL, we propose Doubly Mild Generalization (DMG), comprising (i) mild action generalization and (ii) mild generalization propagation. The former refers to selecting actions in a close neighborhood of the dataset to maximize the Q values. Even so, the potential erroneous generalization can still be propagated, accumulated, and exacerbated by bootstrapping. In light of this, the latter concept is introduced to mitigate the generalization propagation without impeding the propagation of RL learning signals. Theoretically, DMG guarantees better performance than the in-sample optimal policy in the oracle generalization scenario. Even under worst-case generalization, DMG can still control value overestimation at a certain level and lower bound the performance. Empirically, DMG achieves state-of-the-art performance across Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning and attains strong online fine-tuning performance.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) aims to solve sequential decision-making problems and has garnered significant attention in recent years [53, 67, 74, 63, 12]. However, its practical applications encounter several challenges, such as risky exploration attempts [20] and time-consuming data collection phases [35]. Offline RL emerges as a promising paradigm to alleviate these challenges by learning without interaction with the environment [40, 42]. It eliminates the need for unsafe exploration and facilitates the utilization of pre-existing large-scale datasets [31, 48, 59].\nHowever, offline RL suffers from the out-of-distribution (OOD) issue and extrapolation error [19]. From a generalization perspective, this well-known challenge can be regarded as a consequence of the over-generalization of value functions or policies towards OOD actions [47]. Specifically, the potential value over-estimation at OOD actions caused by intricate generalization is often improperly captured by the max operation [73]. This over-estimation will propagate to values of in-distribution samples through Bellman backups and further spread to values of OOD ones via generalization. In mitigating value overestimation caused by OOD actions, substantial efforts have been dedicated [19, 39, 38, 17] and recent advancements in in-sample learning have successfully formulated the Bellman target solely with the actions present in the dataset [37, 85, 92, 88, 21] and extracted policies by weighted behavior cloning [57, 80]. As a result, these algorithms completely eschew generalization and avoid the extrapolation error. Despite simplicity, this way can not take advantage of the generalization ability of neural networks, which could be beneficial for performance improvement. Until now, how to appropriately exploit generalization in offline RL remains a lasting issue.\nThis work demonstrates that mild generalization beyond the dataset can be trusted and leveraged to improve performance under certain conditions. For appropriate exploitation of mild generalization, we propose Doubly Mild Generalization (DMG) for offline RL, comprising (i) mild action generalization and (ii) mild generalization propagation. The former concept refers to choosing actions in the vicinity of the dataset to maximize the Q values. However, the mere utilization of mild action generalization still falls short in adequately circumventing potential erroneous generalization, which can be propagated, accumulated, and exacerbated through the process of bootstrapping. To address this, we propose a novel concept, mild generalization propagation, which involves reducing the generalization propagation while preserving the propagation of RL learning signals. Regarding DMG's implementation, this work presents a simple yet effective scheme. Specifically, we blend the mildly generalized max with the in-sample max in the Bellman target, where the former is achieved by actor-critic learning with regularization towards high-value in-sample actions, and the latter is accomplished using in-sample learning techniques such as expectile regression [37].\nWe conduct a thorough theoretical analysis of our approach DMG in both oracle and worst-case generalization scenarios. Under oracle generalization, DMG guarantees better performance than the in-sample optimal policy in the dataset [38, 37]. Even under worst-case generalization, DMG can still upper bound the overestimation of value functions and guarantee to output a safe policy with a performance lower bound. Empirically\u00b9, DMG achieves state-of-the-art performance on standard offline RL benchmarks [16], including Gym-MuJoCo locomotion tasks and challenging AntMaze tasks. Moreover, benefiting from its flexibility in both generalization aspects, DMG can seamlessly transition from offline to online learning and attain superior online fine-tuning performance."}, {"title": "2 Preliminaries", "content": "RL. The environment in RL is mostly characterized as a Markov decision process (MDP), which can be represented as a tuple M = (S, A, P, R, \u03b3, do), comprising the state space S, action space A, transition dynamics P : S \u00d7 A \u2192 \u2206(S), reward function R : S \u00d7 A \u2192 [0, Rmax], discount factor \u03b3\u2208 [0, 1), and initial state distribution do [70]. The goal of RL is to find a policy \u03c0 : S \u2192 \u0394(\u0391) that can maximize the expected discounted return, denoted as J(\u03c0):\n$J(\\pi) = \\mathbb{E}_{s_0 \\sim d_0, a_t \\sim \\pi(\\cdot|s_t), S_{t+1} \\sim P(\\cdot|s_t, a_t)}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$\nFor any policy \u03c0, we define the value function as $V^\\pi(s) = \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)|s_0 = s]$ and the state-action value function (Q-value function) as $Q^\\pi(s, a) = \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)|s_0 = s, a_0 = a]$.\nOffline RL. Distinguished from traditional online RL training, offline RL handles a static dataset of transitions D = {(si,ai,ri,s\u2032i) }N\u22121i=0 and seeks an optimal policy without any additional data collection [40, 42]. We use \u03b2(a|s) to denote the empirical behavior policy observed in D, which depicts the conditional distributions in the dataset [19]. Ordinary approximate dynamic programming methods minimize temporal difference error, according to the following loss [70]:\n$L_{TD}(\\theta) = \\mathbb{E}_{(s, a, s') \\sim D} [(Q_{\\theta}(s, a) - R(s, a) - \\gamma \\max_{a'} Q_{\\theta'}(s', a'))^2]$,\nwhere \u03c0 is a parameterized policy, $Q_\\theta(s, a)$ is a parameterized Q function, and $Q_{\\theta'}(s, a)$ is a target Qfunction whose parameters are updated via Polyak averaging [53]."}, {"title": "3 Doubly Mild Generalization for Offline RL", "content": "This section discusses the strategy to appropriately exploit generalization in offline RL. In Section 3.1, we introduce a formal perspective on how generalization impacts offline RL and discuss the issues of"}, {"title": "3.1 Generalization Issues in Offline RL", "content": "Offline RL training typically involves a complex interaction between Bellman backup and generalization [47]. Offline RL algorithms vary in backup mechanisms to train the Q function. Here we denote a generic form of Bellman backup as Tu, where u is a distribution in the action space.\n$T_u Q(s, a) := R(s, a) + \\mathbb{E}_{s' \\sim P(\\cdot | s, a)}[\\gamma \\max_{a' \\sim u(s')} Q(s', a')]$\nDuring offline training, this backup is exclusively executed on (s, a) \u2208 D, and the values of (s, a) \u2209 D are influenced solely via generalization. A crucial aspect is that (s', a') in the Bellman target can be absent from the dataset D, depending on the choice of u. As a result, Bellman backup and generalization exhibit an intricate interaction: the backups on (s, a) \u2208 D impact the values of (s, a) \u2209 D via generalization; the values of (s, a) \u2209 D participates in the computation of Bellman target, thereby affecting the values of (s, a) \u2208 D.\nThis interaction poses a key challenge in offline RL, value overestimation. The potential overestima-tion of values of (s, a) \u2209 D, induced by intricate generalization, tends to be improperly captured by the max operation, a phenomenon known as maximization bias [73]. This overestimation propagates to values of (s, a) \u2208 D through backups and further extends to values of (s, a) \u2209 D via generalization. This cyclic process consistently amplifies value overestimation, potentially resulting in value divergence. The crux of this detrimental process can be summarized as over-generalization.\nTo address value overestimation, recent advancements in the field have introduced a paradigm known as in-sample learning, which formulates the Bellman target solely with the actions present in the dataset [37, 85, 92, 88, 21]. Its effect is equivalent to choosing u in Tu to be exactly \u03b2, i.e., the empirical behavior policy observed in the dataset. Following in-sample value learning, policies are extracted from the learned Q functions using weighted behavior cloning [57, 9, 55]. By entirely eschewing generalization in offline RL training, they effectively avoid the extrapolation error [19], a strategy we term non-generalization. However, the ability to generalize is a critical factor contributing to the extensive utilization of neural networks [41]. In this sense, in-sample learning methods seem too conservative without utilizing generalization, particularly when the offline datasets do not cover the optimal actions in large or continuous spaces."}, {"title": "3.2 Doubly Mild Generalization", "content": "The following focuses on the appropriate exploitation of generalization in offline RL.\nWe start by analyzing the generalization effect under the generic backup operator Tu. We consider a straightforward scenario, where $Q_\\theta$ is updated to $Q_{\\theta'}$ by one gradient step on a single (s, a) \u2208 D with learning rate \u03b1. We characterize the resulting generalization effect on any (s, \u00e3) \u2209 D\u00b2 as follows.\nTheorem 1 (Informal). Under certain continuity conditions, the following equation holds when the learning rate \u03b1 is sufficiently small and \u00e3 is sufficiently close to a:\n$Q_{\\theta'}(s, \\tilde{a}) = Q_{\\theta}(s, \\tilde{a}) + C_1 (T_u Q_{\\theta}(s, \\tilde{a}) - Q_{\\theta}(s, \\tilde{a}) + C_2||\\tilde{a} - a||) + O(||\\theta' - \\theta||^2)$\nwhere $C_1 \u2208 [0,1]$ and $C_2$ is a bounded constant.\nNote that Eq. (4) is the update of the parametric Q function ($Q_\\theta \\rightarrow Q_{\\theta'}$) at state-action pairs (s, \u00e3) \u2209 D, which is exclusively caused by generalization. If \u0101 is within a close neighborhood of a, then $C_2 ||\\tilde{a} - a||$ is small. Moreover, as $C_1 \u2208 [0, 1]$, Eq. (4) approximates an update towards the true objective $T_u Q_\\theta(s, \\tilde{a})$, as if $Q_\\theta(s, \\tilde{a})$ is updated by a true gradient step at $(s, \\tilde{a}) \\notin D$. Therefore,\nTheorem 1 shows that, under certain continuity conditions, Q functions can generalize well and approximate true updates in a close neighborhood of samples in the dataset. This implies that mild generalizations beyond the dataset can be leveraged to potentially pursue better performance. Inspired by Theorem 1, we define a mildly generalized policy $ \\tilde{\u03b2}$ as follows.\nDefinition 1 (Mildly generalized policy). Policy $ \\tilde{\u03b2}$ is termed a mildly generalized policy if it satisfies\n$supp(\\tilde{\u03b2}(\u00b7|s)) \u2286 supp(\u03b2(\u00b7|s)), and \\max_{a_1 \\sim \\tilde{\u03b2}(s)} \\min_{a_2 \\sim \u03b2(\\cdot|s)} ||a_1 - a_2|| \\leq \\epsilon_\u03b1,$\nwhere \u03b2 is the empirical behavior policy observed in the offline dataset.\nIt means that $ \\tilde{\u03b2}$ has a wider support than \u03b2 (the dataset), and for any $a_1 \\sim \\tilde{\u03b2}(s)$, we can find $a_2 \\sim \u03b2(s)$ (in dataset) such that $||a_1 \u2212 a_2|| \u2264 \\epsilon_a$. In other words, the generalization of $ \\tilde{\u03b2}$ beyond the dataset is bounded by $ \\epsilon_a$ when measured in the action space distance. According to Theorem 1, there is a high chance that $Q_\\theta$ can generalize well in this mild generalization area $\\tilde{\u03b2}(a|s) > 0$.\nHowever, even in this mild generalization area, it is inevitable that the learned value function will incur some degree of generalization error. The possible erroneous generalization can still be propagated and exacerbated by value bootstrapping as discussed in Section 3.1. To this end, we introduce an additional level of mild generalization, termed mild generalization propagation, and propose a novel Doubly Mildly Generalization (DMG) operator as follows.\nDefinition 2. The Doubly Mild Generalization (DMG) operator is defined as\n$T_{DMG}Q(s, a) := R(s, a) + \\mathbb{E}_{s' \\sim P(\\cdot|s, a)}[\\gamma (\u03bb \\max_{a' \\sim \\tilde{\u03b2}(s')} Q(s', a') + (1 - \u03bb) \\max_{a' \\sim \u03b2(\\cdot|s')} Q(s', a'))]$\nwhere \u03b2 is the empirical behavior policy in the dataset and $ \\tilde{\u03b2}$ is a mildly generalized policy.\nNote that in typical offline RL algorithms, extrapolation error and value overestimation caused by erroneous generalization are propagated through bootstrapping, and the discount factor of this process is \u03b3. DMG reduces this discount factor to \u03bb\u03b3, mitigating the amplification of value overestimation. On the other hand, in contrast to in-sample methods, DMG allows mild generalization, utilizing the generalization ability of neural networks to seek better performance, as Theorem 1 suggests that value functions are highly likely to generalize well in the mild generalization area.\nTo summarize, the generalization of DMG is mild in two aspects: (i) mild action generalization: based on the mildly generalized policy $ \\tilde{\u03b2}$, which generalizes beyond \u03b2, DMG selects actions in a close neighborhood of the dataset to maximize the Q values in the first part of the Bellman target; and (ii) mild generalization propagation: DMG mitigates the generalization propagation without hindering the propagation of RL learning signals by blending the mildly generalized max with the in-sample max in the Bellman target. This reduces the discount factor through which generalization propagates, mitigating the amplification of value overestimation caused by bootstrapping.\nTo support the above claims, we provide a comprehensive analysis of DMG in both oracle and worst-case generalization scenarios, with particular emphasis on value estimation and performance."}, {"title": "3.3 Oracle Generalization", "content": "This section conducts analyses under the assumption that the learned value functions can achieve oracle generalization in the mild generalization area $ \\tilde{\u03b2}(a|s) > 0$, formally defined as follows.\nAssumption 1 (Oracle generalization). The generalization of learned Q functions in the mild generalization area $\\tilde{\u03b2}(a|s) > 0$ reflects the true value updates according to $T_{DMG}$.\nThe mild generalization area $\\tilde{\u03b2}(a|s) > 0$ may contain some points outside the offline dataset, and $T_{DMG}$ might query Q values of such points. This assumption assumes that the generalization at such points reflects the true value updates according to $T_{DMG}$. The rationale for such an assumption comes from Theorem 1, which characterizes the generalization effect of value functions in the mild generalization area. Now we analyze the dynamic programming properties of the operators $T_{DMG}$ and $T_{in}$, where Tin is the in-sample Q learning operator [37, 88, 21] defined as follows."}, {"title": "3.4 Worst-case Generalization", "content": "This section turns to the analyses in the worst-case generalization scenario, where the learned value functions may exhibit poor generalization in the mild generalization area $ \\tilde{\u03b2}(a|s) > 0$. In other words, this section considers that $T_{DMG}$ is only defined in the in-sample area $\u03b2(a|s) > 0$ and the learned value functions may have any generalization error at other state-action pairs. In this case, we use the notation $\\hat{T}_{DMG}$ to tell the difference.\nWe make continuity assumptions about the learned Q function and the transition dynamics.\nAssumption 2 (Lipschitz Q). The learned Q function is $K_Q$-Lipschitz. \u2200s ~ D, \u2200a1,a2 ~ A,\n$|Q(s, a_1) - Q(s, a_2)| \u2264 K_Q ||a_1 - a_2||$\nAssumption 3 (Lipschitz P). The transition dynamics P is Kp-Lipschitz. \u2200s, s' ~ S, \u2200a1, a2 ~ A,\n$|P(s'|s, a_1) - P(s'|s, a_2)| \u2264 K_P ||a_1 - a_2||$\nFor Assumption 2, a continuous learned Q function is particularly necessary for analyzing value function generalization and can be relatively easily satisfied [24]. Assumption 3 is also a common assumption in theoretical studies of RL [13, 87, 61].\nNow we consider the iteration starting from arbitrary function $Q^0$: $Q_{DMG}^{k+1} = \\hat{T}_{DMG} Q_{DMG}^k$ and $Q_{in}^{k+1} = T_{in} Q_{in}^1, \u2200k \u2208 Z^+$. The possible value of $Q_{DMG}^{k+1}$ is bounded by the following results.\nTheorem 4 (Limited overestimation). Under Assumption 2, the learned Q function of DMG by iterating $ \\hat{T}_{DMG}$ satisfies the following inequality\n$Q_{in}^k (s, a) \u2264 Q_{DMG}^k (s, a) \u2264 Q_{in}^k (s, a) + \\frac{\u03bb\\epsilon_\u03b1 K_Q \\gamma}{(1 - \u03b3)} (1 - \u03b3^k), \u2200s, a \\sim D, \u2200k \u2208 Z^+$.\nSince in-sample training eliminates the extrapolation error [37, 92], $Q_{in}^k$ can be considered a relatively accurate estimate [37]. Therefore, Theorem 4 suggests that DMG exhibits limited value overesti-mation under the worst-case generalization scenario. Moreover, the bound becomes tighter as $ \\epsilon_\u03b1$ decreases (milder action generalization) and \u03bb decreases (milder generalization propagation). This is consistent with our intuitions in Section 3.2.\nFinally, we show in Theorem 5 that even under worst-case generalization, DMG guarantees to output a safe policy with a performance lower bound."}, {"title": "3.5 Practical Algorithm", "content": "This section puts DMG into implementation and presents a simple yet effective practical algorithm. The algorithm comprises the following networks: policy \u03c0\u03c6, target policy \u03c0\u03c6\u2032, Q network Qe, target Qnetwork $Q_\\theta'$, and V network V.\nPolicy learning. Practically, we expect DMG to exhibit a tendency towards mild generalization around good actions in the dataset. To this end, we first consider reshaping the empirical behavior policy \u03b2 to be skewed towards actions with high advantage values $\u03b2^*(a|s) \\propto \u03b2(a|s) exp(A(s, a))$. Then we enforce the proximity between the trained policy and the reshaped behavior policy to constrain the generalization area. We define the generalization set \u03a0G as follows.\n\u03a0G = {\u03c0 | KL(\u03b2*(\u00b7|s)||\u03c0(\u00b7|s)) \u2264 \u03b5}\nNote that forward KL allows \u03c0 to select actions outside the support of \u03b2*, enabling I\u00e7 to generalize beyond the actions in the dataset. With \u03a0G defined, the next step is to compute the maximal Q within \u03a0\u03b1. \u03a4\u03bf accomplish this, we adopt Actor-Critic style training [70] for this part.\n$max_{\\phi} \\mathbb{E}_{s \\sim D, a \\sim \u03c0_{\\phi}(\u00b7|s)}Q_{\\theta}(s, a), s.t. \u03c0_{\\phi} \u2208 \u03a0_G$\nBy treating the constraint term as a penalty, we maximize the following objective.\n$max_{\\phi} \\mathbb{E}_{s \\sim D, a \\sim \u03c0_{\\phi}(\\cdot |s)} Q_{\\theta}(s, a) - v\\mathbb{E}_{s \\sim D}KL(\u03b2^*(\\cdot |s)||\u03c0_{\\phi}(\\cdot |s))$\nThrough straightforward derivations, Eq. (13) is equivalent to the following policy training objective.\n$J_\u03c0(\u00a2) = \\mathbb{E}_{s \\sim D, a \\sim \u03c0_{\\phi}(\\cdot|s)} Q_{\\theta}(s, a) - v\\mathbb{E}_{(s, a) \\sim D} [exp(\u03b1(Q_{\\theta'}(s, a) - V_{\\psi}(s))) log \u03c0_{\\phi}(a|s)]$\nwhere \u03b1 is an inverse temperature and $Q_{\\theta'}(s, a) - V_{\\psi}(s)$ computes the advantage function A(s, a).\nValue learning. Now we turn to the implementation of the TDMG operator for training value functions. By introducing the aforementioned policy, we can substitute $max_{a \\sim \u03b2}$ in TDMG with $\\mathbb{E}_{a \\sim \u03c0}$. Regarding $max_{a \\sim \u03b2}$ in TDMG, any in-sample learning techniques can be employed to compute the in-sample maximum [37, 88, 85, 21]. In particular, based on IQL [37], we perform expectile regression.\n$L_V(\\psi) = \\mathbb{E}_{(s, a) \\sim D} [L_\u03c4^2(Q_{\\theta'}(s, a) - V_{\\psi}(s))]$\nwhere $L_\u03c4^2(u) = |\u03c4 - 1(u < 0)|u^2$ and \u03c4\u2208 (0, 1). For \u03c4 \u2248 1, $V_\u03c8$ can capture the in-sample maximal Q [37]. Finally, we have the following value training loss.\n$L_Q(\\theta) = \\mathbb{E}_{(s, a, s') \\sim D} [(Q_{\\theta}(s, a) - R(s, a) - \u03b3\u03bb\\mathbb{E}_{a' \\sim \u03c0_{\\phi'}} Q_{\\theta'}(s', a') - \u03b3(1 - \u03bb)(1_{\u03c4 \\geq 1} V_{\\psi'}(s'))]^2$\nOverall algorithm. Integrating all components, we present our practical algorithm in Algorithm 1."}, {"title": "4 Discussions and Related Work", "content": "Summary of offline RL work from a generalization perspective. As analyzed above, DMG is featured in both mild action generalization and mild generalization propagation. Within the actor-critic framework upon which most offline RL algorithms are built, these two aspects correspond to the policy and value training phases, respectively. Action generalization concerns whether the policy training intentionally selects actions beyond the dataset to maximize Q values, while generalization propagation involves whether value training propagates generalization through bootstrapping.\nConcerning policy learning, AWR [57], AWAC [55], CRR [80], 10% BC [8], IQL [37], and other works such as [78, 9, 66, 21, 88] extract policies through weighted or filtered behavior cloning, thereby lacking intentional action generalization to maximize Q values beyond the dataset. Typical policy-regularized offline RL methods like TD3BC [17], BRAC [84], BEAR [38], SPOT [83], and others such as [79, 61, 72] introduce regularization terms to Q maximization objectives to regularize the trained policy towards the behavior policy and allows mild action generalization. Online RL algorithms like TD3 [18] and SAC [27] have no constraints and maximize Q values in the entire action space, corresponding to full action generalization. Regarding value training, in-sample learning methods including OneStep RL [7], IQL [37], InAC [85], IAC [92], XQL [21], and SQL [88] completely avoid generalization propagation and accumulation via bootstrapping, whereas typical offline and online RL approaches allow full generalization propagation through bootstrapping. In the proposed approach DMG, generalization is mild in both aspects.\nRecently, Ma et al. [47] have also drawn attention to generalization in offline RL and the issue of over-generalization. They mitigate over-generalization from a representation perspective, differentiating between the representations of in-sample and OOD state-action pairs. Lyu et al. [44] argue that conventional value penalization like CQL [39] tends to harm the generalization of value functions and hinder performance improvement. They propose mild value penalization to mitigate the detrimental effects of value penalization on generalization.\nConnection to heuristic blending approaches. Our approach also relates to the framework of blending heuristics into bootstrapping [10, 81, 71, 28, 82, 22]. In offline RL, HUBL [22] blends Monte-Carlo returns into bootstrapping and acts as a data relabeling step, which reduces the degree of bootstrapping and thereby increases its performance. In contrast, DMG blends the in-sample maximal values into the bootstrapping operator. DMG does not reduce the discount for RL learning but reduces the discount for generalization propagation."}, {"title": "5 Experiments", "content": "In this section, we conduct several experiments to justify the validity of the proposed method DMG. Experimental details and extended results are provided in Appendices C and D, respectively."}, {"title": "5.1 Main Results on Offline RL Benchmarks", "content": "Tasks. We evaluate the proposed approach on Gym-MuJoCo locomotion domains and challenging AntMaze domains in D4RL [16]. The latter involves sparse-reward tasks and necessitates \u201cstitching\u201d fragments of suboptimal trajectories traveling undirectedly to find a path to the goal of the maze.\nBaselines. Our offline RL baselines include both typical bootstrapping methods and in-sample learning approaches. For the former, we compare to BCQ [19], BEAR [38], AWAC [55], TD3BC [17], and CQL [39]. For the latter, we compare to BC [58], OneStep RL [7], IQL [37], XQL [21], and SQL [88]. We also include the sequence-modeling method Decision Transformer (DT) [8].\nComparison with baselines. Aggregated results are displayed in . On the Gym locomotion tasks, DMG outperforms prior methods on most tasks and achieves the highest total score. On the much more challenging AntMaze tasks, DMG outperforms all the baselines by a large margin, especially in the most difficult large mazes. For detailed learning curves, please refer to Appendix D.3. According to [56], we also report the results of DMG over more random seeds in Appendix D.2.\nRuntime. We test the runtime of DMG and other baselines on a GeForce RTX 3090. As shown in Appendix D.1, the runtime of DMG is comparable to that of the fastest offline RL algorithm TD3BC."}, {"title": "5.2 Performance Improvement over In-sample Learning Approaches", "content": "DMG can be combined with various in-sample learning approaches. Besides IQL [37], we also apply DMG to two recent state-of-the-art in-sample algorithms, XQL [21] and SQL [88]. As shown in Table 3 (and Table 2), DMG consistently and substantially improves upon these in-sample methods, particularly on sub-optimal datasets where generalization plays a crucial role in the pursuit of a better policy. This provides compelling empirical evidence that the performance of in-sample methods is largely confined by eschewing generalization beyond the dataset, while DMG effectively exploits generalization, achieving significantly improved performance across tasks."}, {"title": "5.3 Ablation Study for Performance and Value Estimation", "content": "Mixture coefficient \u03bb. The mixture coefficient \u03bb controls the extent of generalization propagation. We fix v = 0.1 and vary \u03bb \u2208 [0, 1], presenting the learned Q values and performance on several tasks in Figure 1. As A increases, DMG enables increased generalization propagation through bootstrapping, and the learned Q values become larger and probably diverge. A moderate \u03bb (mild generalization propagation) is crucial for achieving strong performance across datasets. Under the same degree of action generalization, mild generalization propagation effectively suppresses value overestimation, facilitating more stable policy learning.\nPenalty coefficient v. The penalty coefficient v regulates the degree of action generalization. We fix X = 0.25 and vary v. The results are shown in Figure 2. As v decreases, DMG allows broader action generalization beyond the dataset, which results in higher learned values. Regarding performance, a moderate v (mild action generalization) is also crucial for achieving superior performance."}, {"title": "5.4 Online Fine-tuning after Offline RL", "content": "Benefiting from its flexibility in both generalization aspects, DMG enjoys a seamless transition from offline to online learning. This is accomplished through a gradual enhancement of both action generalization and generalization propagation. Since IQL [37] has demonstrated superior online fine-tuning performance compared to previous methods [55, 39] in its paper, we follow the experimental setup of IQL and compare to IQL. We also train online RL algorithm TD3 [18] from scratch for comparison. We use the challenging AntMaze domains [16], given DMG's already high offline performance in Gym locomotion domains."}, {"title": "6 Conclusion and Limitations", "content": "This work scrutinizes offline RL through the lens of generalization and proposes DMG, comprising mild action generalization and mild generalization propagation, to exploit generalization in offline RL appropriately. We theoretically analyze DMG in oracle and worst-case generalization scenarios, and empirically demonstrate its SOTA performance in offline training and online fine-tuning experiments.\nWhile our work contributes valuable insights, it also has limitations. The DMG principle is shown to be effective across most scenarios. However, when the function approximator employed is highly compatible with a specific task setting, the learned value functions may generalize well in the entire action space. In such case, DMG may underperform full generalization methods due to conservatism."}, {"title": "A Extended Related Work", "content": "Model-free offline RL. In offline RL, a fixed dataset is provided and no further interactions are allowed [40, 42]. As a result, conventional off-policy RL algorithms suffer from the extrapolation error due to OOD actions and exhibit poor performance [19]. To address this challenge, various offline RL algorithms have been developed, primarily categorized into model-free and model-based approaches. In model-free solutions, value regularization methods introduce conservatism in value estimation through direct penalization [39, 36, 46, 86, 11, 64, 51], or via value ensembles [2, 3, 89]. Policy constraint approaches enforce proximity between the trained policy and the behavior policy, either explicitly via divergence penalties [84, 38, 30, 17, 83], implicitly by weighted behavior cloning [9, 57, 55, 80, 49], or directly through specific parameterization of the policy [19, 23, 93]. Some recent efforts focus on learning the optimal policy within the dataset's support (known as in-support or in-sample optimal policy) in a theoretically sound manner [49, 51, 83]. These approaches are less influenced by the the dataset's average quality. Another popular branch of algorithms opts for in-sample learning, which formulates the Bellman target without querying the values of any unseen actions [7, 45, 37, 85, 92, 88, 21]. Among these, OneStep RL [7] evaluates the behavior policy via SARSA [70] and performs only one step of constrained policy improvement without off-policy evaluation. IQL [37] modifies the SARSA update, using expectile regression to approximate an upper expectile of the value distribution and enables multi-step dynamic programming. Following IQL, several recent works such as InAC [85], IAC [92], XQL [21], and SQL [88] have developed different in-sample learning frameworks, further enhancing the performance of in-sample learning approaches. However, this work shows that the performance of in-sample approaches is confined by eschewing generalization beyond the offline dataset. In contrast, the proposed approach DMG utilizes doubly mild generalization to appropriately exploit generalization and achieves significantly stronger performance across tasks.\nModel-based offline RL. Model-based offline RL methods involve training an environmental dynamics model, from which synthetic data is generated to facilitate policy optimization [69, 29, 32]. In the context of offline RL, algorithms such as MOPO [90] and MOREL [33] propose to estimate the uncertainty within the trained model and subsequently impose penalties or constraints on state-action pairs characterized by high uncertainty levels, thus achieving conservatism in the learning process. Some model-based approaches incorporate conservatism in a similar way to those model-free ones. For example, COMBO [91] leverages value penalization, while BREMEN [52] employs behavior regularization. More recently, MOBILE [68] introduces uncertainty quantification via the inconsistency of Bellman estimations within a learned dynamics ensemble. SCAS [50] proposes a generic model-based regularizer that unifies OOD state correction and OOD action suppression in offline RL. However, typical model-based methods often involve heavy computational overhead [29], and their effectiveness hinges on the accuracy of the trained dynamics model [54].\nRecently, Bose et al. [5] explores multi-task offline RL from the perspective of representation learning and introduced a notion of neighborhood occupancy density. The neighborhood occupancy density at a given stata-action pair in the dataset for a source task is defined as the fraction of points in the dataset within a certain distance from that stata-action pair in the representation space. Bose et al. [5] use this concept to bound the representational transfer error in the downstream target task. In contrast, DMG is a wildly compatible idea in offline RL and provides insights into many offline RL methods. DMG balances the need for generalization with the risk of over-generalization in offline RL. Generalization to stata-action pairs in the neighborhood of the dataset corresponds to mild action generalization in the DMG framework."}, {"title": "B Proofs", "content": "In this section, we provide the proofs of all the theories in the paper."}, {"title": "B.1 Proof of Theorem 1", "content": "This section presents the formal theorem for the Theorem 1 in the main paper, along with its proof.\nWe first make several common continuity assumptions for Theorem 1."}, {"title": "B.2 Proofs under Oracle Generalization", "content": "We first restate the several definitions in the main paper"}]}