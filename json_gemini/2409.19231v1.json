{"title": "Double Actor-Critic with TD Error-Driven Regularization in Reinforcement Learning", "authors": ["Haohui Chen", "Zhiyong Chen", "Aoxiang Liu", "Wentuo Fang"], "abstract": "To obtain better value estimation in reinforcement learning, we propose a novel algorithm based on the double actor-critic framework with temporal difference error-driven regularization, abbreviated as TDDR. TDDR employs double actors, with each actor paired with a critic, thereby fully leveraging the advantages of double critics. Additionally, TDDR introduces an innovative critic regularization architecture. Compared to classical deterministic policy gradient-based algorithms that lack a double actor-critic structure, TDDR provides superior estimation. Moreover, unlike existing algorithms with double actor-critic frameworks, TDDR does not introduce any additional hyperparameters, significantly simplifying the design and implementation process. Experiments demonstrate that TDDR exhibits strong competitiveness compared to benchmark algorithms in challenging continuous control tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has gained significant attention as a versatile research area, with applications spanning recommender systems [1], autonomous driving [2], controller design [3], and medical fields [4]. One of the most well-known RL algorithms, deep Q-learning (DQN), has demonstrated success in learning optimal policies [5]. However, DQN struggles in continuous control scenarios, particularly in environments with high-dimensional action spaces [6], [7].\nIn contrast, actor-critic (AC) algorithms, such as trust region policy optimization (TRPO) [8], proximal policy optimization (PPO) [9], and deep deterministic policy gradient (DDPG) [10], are better suited for continuous control tasks. Further insights into AC-based algorithms are discussed in other works [11], [12], [13]. However, DDPG and related algorithms are prone to overestimation bias. To address this, Fujimoto et al. introduced the twin delayed deep deterministic policy gradient (TD3) algorithm [14], which leverages double critics to reduce overestimation. Notably, TD3 still employs a single critic for updates and action selection.\nBuilding on the double actor-critic (DAC) framework, several advanced algorithms, such as double actors regularized critics (DARC) [15], softmax deep double deterministic policy gradients (SD3) [16], and generalized-activated deep double deterministic policy"}, {"title": "II. RELATED WORK", "content": "This section is divided into two parts: the first part reviews related work, and the second part compares the benchmark algorithms with TDDR.\nRelated Work\nDDPG and TD3 serve as benchmark algorithms in this paper, both utilizing TD error for updating critic parameters and are widely adopted within the AC framework. Several advanced algorithms build upon this foundational structure. For example, Wu et al. [18] introduced the triplet-average deep"}, {"title": "III. PRELIMINARIES", "content": "The interaction between an agent and the environment follows a standard RL process, which is modeled as a Markov decision process (MDP). MDP is defined by the tuple $M = (S, A, P, R(s, a), \\gamma)$, where $S$ is the state space, $A$ is the action space, $P$ is the state transition probability, $R(s, a)$ is the reward function, and $\\gamma \\in [0,1]$ is the discount factor. The goal of RL is to find the optimal policy that maximizes cumulative rewards. Value estimation is commonly used to evaluate the effectiveness of policies, with higher value estimations generally indicating better policies [25]. We briefly describe several relevant reinforcement learning algorithms,"}, {"title": "A. Deterministic Policy Gradient", "content": "Silver et al. [26] introduced the deterministic policy gradient (DPG) algorithm, which uses deterministic policies instead of stochastic ones. Let $\\pi_{\\phi}$ refer to the policy with parameters $\\phi$. Specifically, $\\pi_{\\phi}(s)$ denotes the action $a$ output by the policy for the given state $s$. To maximize the cumulative discounted rewards $R(s,a)$, $\\phi$ is updated as $\\phi \\leftarrow \\phi + \\alpha\\nabla_{\\phi}J(\\phi)$, where $\\alpha$ is the learning rate. The DPG is defined as:\n$\\nabla_{\\phi}J(\\phi) = N^{-1}\\sum(\\nabla_aQ_{\\theta}(s,a)|_{a=\\pi_{\\phi}(s)}\\nabla_{\\phi}\\pi_{\\phi}(s))$    (1)\nwhere $J(\\phi)$ is the objective function, typically the expected return we aim to maximize, and $\\nabla_aQ_{\\theta}(s, a)|_{a = \\pi_{\\phi}(s)}$ represents the gradient of the action-value function $Q$ with respect to its parameters $\\theta$, evaluated at $a = \\pi_{\\phi}(s)$. The gradient $\\nabla Q_{\\theta}(s, a)$, for updating $\\theta$, is usually obtained by minimizing $N^{-1}\\sum(y-Q_{\\theta}(s, a))^2$, where $y$ is the TD target, $y-Q_{\\theta}(s,a)$ is the TD error, and $N$ represents the batch size.\nLillicrap et al. [10] introduced deep neural networks into DPG and proposed the DDPG algorithm. DDPG introduces the concept of target networks and utilizes a soft update approach for both policy and value parameters:\n$\\theta' \\leftarrow \\tau\\theta + (1 - \\tau)\\theta', \\phi' \\leftarrow \\tau\\phi + (1 - \\tau)\\phi'        (2)\nwhere $\\theta'$ represents the parameters of the critic target network, and $\\phi'$ represents the parameters of the actor target network. Here, $\\tau \\ll 1$ is a constant. DDPG is an effective RL algorithm for continuous control tasks [27]. The TD target for DDPG, computed using the target networks, is:\ny = r + \\gamma Q_{\\theta'}(s', \\pi_{\\phi'}(s')),                (3)\nwhere $s'$ is the next state, and the output of $\\pi_{\\phi'}(s')$ represents the next action, denoted as $a'$.\nTD3 [14] is an enhanced version of DDPG that adopts a new approach of delayed policy updates to mitigate the adverse effects of update variance and bias. It also incorporates a regularization approach from machine learning by adding noise to the target policy to smooth out value estimations."}, {"title": "B. Double Actor-Critic", "content": "Double critics utilize two independent critic networks that generate separate value estimations without mutual influence. Originating from the concept of double Q-learning, this algorithm employs dual critics to decouple actions, thereby mitigating the overestimation issue. For example, the TD target in TD3 is defined as:\ny = r + \\gamma \\min_{i=1,2}(Q_{\\theta_i}(s', a')),        (4)\nwhere $Q_{\\theta_i}$ and $Q_{\\theta'_i}$ represent the target networks of the double critics. The action $a'$ is defined as:\na' = \\pi_{\\phi'(s')} + \\epsilon        (5)\nincorporating noise $\\epsilon$. The TD target (4) is used to define the TD error, $y - Q_{\\theta_i}(s, a)$ for $i = 1,2$, and update the double critic networks $Q_{\\theta_1}$ and $Q_{\\theta_2}$. TD3 demonstrates superior value estimation compared to DDPG in MuJoCo by OpenAI [28]. Alongside TD3, other benchmark algorithms like DARC, SD3, and GD3 also integrate double critics.\nIn addition to double critics, the use of double actors $\\pi_{\\phi_1}$ and $\\pi_{\\phi_2}$, along with their target networks $\\pi_{\\phi'_1}$ and $\\pi_{\\phi'_2}$, effectively enhances action exploration efficiency and prevents policies from settling into local optima, thereby achieving improved value estimation [15], [16]. The actions $a', i = 1,2$, generated by the target networks are denoted as:\na' = \\pi_{\\phi'_i}(s') + \\epsilon.        (6)\nBenchmark algorithms such as DARC, SD3, and GD3 also incorporate double actors."}, {"title": "C. Regularization", "content": "The TD target for DDPG (3) can be rewritten as\ny = r + \\gamma \\psi,                (7)\nwhere\n$\\psi = Q_{\\theta'}(s', a').               (8)\nWhen double critics are used, the value $\\psi$ for each critic following DDPG (3) is given as\n$\\psi_i = Q_{\\theta_i}(s', a').    (9)\nHowever, this value is not used directly. For example, it is modified in (4) to\n$\\psi = \\min_{i=1,2}Q_{\\theta_i}(s', a').    (10)\nIt is noted that the same $\\psi$, and hence the same TD target $y$, is used for both critics. The difference between (9) and (10) acts as a regularization of the TD error.\nDifferent forms of regularization have been proposed in the literature, varying in the construction of $\\psi$. For TD3, $\\psi$ is defined in (10) where it uses the smaller Q-values from the double critics to compute $\\psi$. For DARC, $\\psi$ is computed as:\n$\\psi =(1 - \\nu) \\max_{j=1,2} \\min_{i=1,2} Q_{\\theta'_i}(s', a'_j)\n+ \\nu \\min_{j=1,2} \\min_{i=1,2} Q_{\\theta'_i}(s', a'_j)           (11)\nwhere $\\nu$ is the weighting coefficient. DARC computes $\\psi$ by combining the Q-values of the double critics and double actors in a convex combination. In addition to the hyperparameter $\\nu$ used to balance overestimation and underestimation, DARC introduces another regularization parameter, $\\lambda$, which restricts the critics from differing too significantly.\nFor SD3 and GD3, $\\psi$ is constructed as:\n$\\psi = \\min_{j,i=1,2}Q_{\\theta'_i}(s', a'_j),   (12)\nwhich is similar to TD3 but incorporates actions from double actors. SD3 introduces two hyperparameters. NNS influences the effect of value estimation but is not highly sensitive. In contrast, the parameter $\\beta$ impacts the effectiveness and bias of value estimation: a lower $\\beta$ decreases variance, while a higher"}, {"title": "IV. THE TDDR ALGORITHM", "content": "The TDDR algorithm adopts DAC with their respective target networks. These eight networks are denoted as $Q_{\\theta_i}, Q_{\\theta'_i}, \\pi_{\\phi_i}, \\pi_{\\phi'_i}$, for $i = 1,2$, as defined earlier. In Fig. 1, these neural networks are simply labeled as $Q_i, Q'_i, A_i, A'_i$, for $i = 1,2$, respectively.\nThe core of the algorithm involves updating the eight net- works. The target networks $Q'_i$ and $A'_i$ are updated following conventional rules, similar to (2), succinctly denoted as:\n$\\theta'_i = f(\\theta_i, \\theta'_i), \\phi'_i = f(\\phi_i, \\phi'_i)$\nfor $i = 1,2$. The update of each actor networks $A_i$ is performed as $\\phi_i \\leftarrow \\phi_i + \\alpha\\nabla_{\\phi_i}J(\\phi_i)$ where the DPG $\\nabla_{\\phi_i} J(\\phi_i)$ is defined in (1) using the corresponding critic network $Q_{\\theta_i}$.\nKey innovations in the proposed TDDR algorithm focus on updating the critic networks $Q_i$, parameterized by $\\theta$. The first innovation involves adopting the clipped double Q- learning (CDQ) approach, which utilizes double target actors to evaluate actions, resulting in CDQ based on double actors (DA-CDQ). Regularizing the TD error plays a crucial role in updating the double critic networks. The second innovation introduces a novel critic regularization architecture (CRA), which is driven by the TD error of their target networks. These two features, along with a comparative analysis against benchmark algorithms, are elaborated upon in the subsequent sections.\nLastly, it is noteworthy that TDDR also utilizes the cross- update architecture employed in DARC, where only one pair, either $(Q_1, A_1)$ or $(Q_2, A_2)$, is updated at each step."}, {"title": "A. Double Actors with CDQ", "content": "The proposed algorithm adopts the CDQ approach, similar to TD3, by incorporating noise during the action sampling process as defined in (6) for each target actor $i = 1,2$. Here, $\\epsilon \\sim clip(\\mathcal{N}(0, \\sigma), -c, c)$ represents Gaussian noise with mean 0 and variance $\\sigma$, clipped to the range of $[-c, c]$ for a positive constant $c$. This clipping effectively limits the noise magnitude, ensuring that actions remain within a reasonable range and preventing excessive perturbations.\nThe DA-CDQ method, inspired by TD3, employs the min operator to clip both value estimations during the update process. In contrast to TD3's single actor, DA-CDQ utilizes double actors, where each actor network independently selects actions. These actions are then evaluated by double critics to assess their value. Importantly, each actor network can autonomously learn and refine its own policy. DA-CDQ en- hances exploration capabilities and effectively prevents actions from becoming trapped in local optima, which can occur with a single actor setup like TD3. This approach facilitates the discovery of potentially optimal policies [15]. Using a single"}, {"title": "B. Critic Regularization Architecture", "content": "Each actor target generates an action $a'_i$ as in (6), which, together with the target networks of double critics, can be used to define the TD error of the critic targets $Q_i$ as follows:\n$\\delta_i = r + \\gamma \\min_{j=1,2} (Q_{\\theta'_j}(s', a'_i)) - \\min_{j=1,2} (Q_{\\theta'_j}(s, a)) (13)\nfor $i = 1,2$. For each action $a'_i$, both critic target networks $Q'_j, j = 1,2$, are used to estimate the values for the next state $s'$. The smaller value is used to define the TD target $r + \\gamma \\min_{j=1,2}(Q_{\\theta'_j}(s', a'_i))$. Correspondingly, the two critic target networks generate two action-values for $(s, a)$, and the smaller value $\\min_{j=1,2}(Q_{\\theta'_j}(s, a))$ is utilized as the actual action-value. The TD target and the actual action-value define the TD error in (13) for each actor target network.\nIt is important to note that these TD errors in (13) are not for the critics $Q_i$ nor are they used to update $\\theta_i$ directly. However, these TD errors drive the calculation of the regularization of the TD errors of $Q_i$. This TD error-driven regularization is a novel feature compared to the existing regularizations introduced earlier. The specific computation of $\\psi$ for TDDR is given as\n$\\psi = \\begin{cases} \\min_{i=1,2}(Q_{\\theta'_i}(s', a'_1)), & \\text{if } |\\delta_1| \\le |\\delta_2| \\\\ \\min_{i=1,2}(Q_{\\theta'_i}(s', a'_2)), & \\text{if } |\\delta_1| > |\\delta_2| \\end{cases}    (14)\nIn the existing calculations of $\\psi$ in (10), (11), and (12), the smaller action-value of the double critic target networks, evaluated either on a single actor target or double actor targets, is directly used. In contrast, in (14), the smaller action-value, evaluated on the selected actor target network that gives the smaller TD error ($\\delta_1$ or $\\delta_2$), is used.\nOnce $\\psi$ is determined in (14), the actual TD error for the critics $Q_i$ is calculated as in (7). The gradient $\\nabla Q_{\\theta_i}(s, a)$, used for updating $\\theta_i$, is obtained by minimizing\n$N^{-1}\\sum(y - Q_{\\theta_i}(s, a))^2.        (15)$\nUsing DAC to compute $\\psi$ as in (14) in TDDR offers several advantages. First, it avoids overestimation by selecting the smaller value based on the comparison between $|\\delta_1|$ and $|\\delta_2|$. This ensures that the TD target is calculated based on the action-value generated by either target policy $A_1$ or $A_2$, reducing the risk of overestimation.\nAdditionally, it balances exploration and exploitation. By using two Q-values to calculate the TD target, the approach encourages a balance between exploring new actions and exploiting known ones. When comparing $|\\delta_1|$ and $|\\delta_2|$, if the current policy is $A_1$, then $A_1$ is exploited while $A_2$ is explored, and vice versa. This dynamic helps in discovering potentially optimal policies while maintaining robustness in the learning process."}, {"title": "C. Comparative Analysis of Algorithms", "content": "The architecture of TDDR is depicted in Fig. 1. Benchmark algorithms can be represented similarly: DDPG consists of one critic and one actor, TD3 incorporates double critics with one actor, and DARC, SD3, and GD3 have DAC architectures like TDDR. Using this figure, the specific differences between TDDR and these benchmark algorithms are analyzed below.\nWhen the eight neural networks reduce to four, with $\\theta_i, \\theta'_i, \\phi_i$, and $\\phi'_i$ for $i = 1,2$ being replaced by $\\theta, \\theta', \\phi$, and $\\phi'$, respectively, TDDR simplifies to DDPG, which is the simplest mode among these six algorithms. With only $\\phi_i$ and $\\phi'_i$ for $i = 1,2$ replaced by $\\phi$ and $\\phi'$, respectively, the remaining six neural networks of TDDR constitute TD3.\nGiven that DARC, SD3, and GD3 share the same network architecture as TDDR, their distinctions are subtle but impact- ful in terms of performance. The first distinction is the use of CDQ in TDDR, which is absent in the other algorithms. The second distinction is in the definition of the TD error- driven $\\psi$ in (14), which differs significantly from those in other algorithms, such as (11) and (12). This difference is highlighted by the \"TDDR only\" block in the figure.\nThe third distinction is that TDDR does not introduce additional hyperparameters beyond those in TD3, whereas DARC, SD3, and GD3 introduce two, two, and four addi- tional hyperparameters, respectively. While hyperparameters can increase the complexity of the algorithms and the tuning workload, TDDR simplifies the design and implementation process by avoiding this additional complexity.\nFor instance, the critics in DARC are regularized to be close according to (11), ensuring they are not isolated. While the mutually regularized double critics in DARC perform well in value function estimation, their performance is significantly influenced by the regularization coefficient $\\nu$. Appropriate regularization coefficients can lead to better value estimation, whereas test results show that unsuitable regularization coeffi- cients can significantly increase the standard deviation of the results, reducing DARC's stability. In contrast, the mutually independent double critics structure in TDDR is not affected by hyperparameter selection. In TDDR, the critics are fully utilized, with each critic responsible for its corresponding counterpart actor, and both critics contributing to policy im- provement."}, {"title": "V. CONVERGENCE ANALYSIS", "content": "Convergence analysis has been studied in double Q-learning [29], which can be formulated as follows:\n$Q_{t+1}^A(s_t, a_t) = Q_t^A(s_t, a_t) + \\alpha_t(s_t, a_t)\\times$\n$(r_t + \\gamma Q_t^B(s_{t+1}, a_1) - Q_t^A(s_t, a_t))$\n$Q_{t+1}^B(s_t, a_t) = Q_t^B(s_t, a_t) + \\alpha_t(s_t, a_t)\\times$\n$(r_t + \\gamma Q_t^A(s_{t+1}, a_2) - Q_t^B(s_t, a_t))                    (16)$\nwhere $Q^A(s_t, a_t)$ and $Q^B(s_t, a_t)$ represent the updates of double Q-values, and $r_t = R(s_t,a_t)$ is the reward for the pair $(s_t, a_t)$. The actions $a_1$ and $a_2$ are defined as:\na_1 = arg \\max_a Q_t^A(s_{t+1}, a), a_2 = arg \\max_a Q_t^B(s_{t+1}, a).\nIt is noted that $r_t + \\gamma Q_t^B(s_{t+1}, a_1)$ is the TD target for $Q^A(s_t, a_t)$, and vice versa for swapping A and B. Treating the critics $Q_{\\theta_1}(s, a)$ and $Q_{\\theta_2}(s, a)$ as $Q^A(s, a)$ and $Q^B(s,a)$, respectively, the TD target in (7) is $r_t+\\gamma\\psi_t$, explicitly depend- ing on t. In the definitions of $\\psi$ in (10), (11), (12), or (14), $\\psi$ cannot be simply expressed as $Q^B(s_{t+1}, a_1)$ or $Q^A(s_{t+1}, a_2)$ as in (16). The first difference is that $\\psi$ is calculated from the critic and actor target networks, while $Q^B(s_{t+1}, a_1)$ or $Q^A(s_{t+1}, a_2)$ are the directly calculated optimal values. The influence of adding target networks to convergence is too complicated to analyze here.\nThe second difference is that $\\psi$ is not $Q^B(s_{t+1}, a_1)$ or $Q^A(s_{t+1}, a_2)$, but rather the smaller value generated by the critics, denoted as $\\min_{i=1,2} Q_{\\theta'_i}$ in all the aforementioned definitions of $\\psi$. To capture this difference in all the aforemen- tioned algorithms, including DARC, SD3, GD3, and TDDR, the formulation in (16) is modified to\n$Q_{t+1}^A(s_t, a_t) = Q_t^A(s_t, a_t) + \\alpha_t(s_t, a_t)(r_t+$\n$\\gamma \\min \\{Q_t^B(s_{t+1}, a_{=1,2}), Q_t^A(s_{t+1}, a_{=1,2})\\} - Q_t^A(s_t, a_t))$\n$Q_{t+1}^B(s_t, a_t) = Q_t^B(s_t, a_t) + \\alpha_t(s_t, a_t)(r_t+$\n$\\gamma \\min \\{Q_t^B(s_{t+1}, a_{=1,2}), Q_t^A(s_{t+1}, a_{=1,2})\\} - Q_t^B(s_t, a_t)).       (17)$\nHere, $a_{=1,2}$ can be either $a_1$ or $a_2$, depending on the specific algorithm. For instance, in TDDR, $a_{=1,2} = a_1$ when $|\\delta_1| \\le |\\delta_2|$, and $a_{=1,2} = a_2$ otherwise, where $\\delta_i$ defined in (13) is modified to\n$\\delta_{i=1,2} =r_t + \\gamma \\min \\{Q_t^B(s_{t+1}, a_{=1,2}), Q_t^A(s_{t+1}, a_{=1,2})\\}$\n$- \\min \\{Q_t^B(s_t, a_t), Q_t^A(s_t, a_t)\\}$.\nIt is worth noting that the convergence analysis of (17) does not necessarily rely on a particular selection of $a_{=1,2}$.\nIn (17), $Q^A(s_t, a_t)$ and $Q^B(s_t, a_t)$ are updated using the same TD target, denoted as\n$\\mathcal{T}_t = r_t + \\gamma \\min \\{Q_t^B(s_{t+1}, a_{=1,2}), Q_t^A(s_{t+1}, a_{=1,2})\\}$,\nboth with $a_1$ or $a_2$. The TD errors are denoted as\n$E_t^A = \\mathcal{T}_t - Q_t^A(s_t, a_t), E_t^B = \\mathcal{T}_t - Q_t^B(s_t, a_t)$.\nHowever, in (16), $Q^A(s_t, a_t)$ and $Q^B(s_t, a_t)$ are updated using different TD targets, one with $a_1$ and the other with $a_2$. This"}, {"title": "VI. EXPERIMENTS", "content": "We conducted extensive experiments on nine continuous control tasks using MuJoCo and Box2d environments from OpenAI Gym [33], [34]. Table II lists the state and action dimensions of these environments. Our benchmark algorithms include DDPG [10], TD3 [14], DARC [15], SD3 [16], and GD3 [17].\nGiven the emphasis on reproducibility [35], [36] and the importance of effective comparisons [37], we conducted our experiments using five random seeds and network initializa- tions to ensure fairness in our comparisons. Additionally, we utilized the default reward function and environment settings without modification to maintain a level playing field. Each of the nine selected environments was run for 1 million steps, with evaluations conducted every 5,000 steps using the average reward from 10 episodes for each evaluation. The batch size is set to 128.\nThe experiment is structured into three parts. The first part compares TDDR with DDPG and TD3, which have different AC architectures. The second part compares TDDR with DARC, SD3, and GD3, all of which share the same DAC architecture. The third part consists of ablation discussion about the impact of different components of TDDR.\nIn the second part, since TDDR does not introduce addi- tional hyperparameters unlike DARC, SD3, and GD3, the per- formance comparison is divided into two scenarios. The first scenario involves configuring the three benchmark algorithms with hyperparameters that yield relatively better performance, while the second scenario involves setting them with hyperpa- rameters that result in relatively poorer performance. The val- ues of the hyperparameters are listed in Table III. This aims to validate the impact of hyperparameters on performance metrics such as standard deviation. It should be noted that, for a fair comparison, the hyperparameters of DARC, SD3, and GD3 were selected based on the results of ablation experiments from their original papers. Here, \u201cadditional hyperparameters\u201d refers to those beyond those of TD3."}, {"title": "A. Comparison with DDPG and TD3", "content": "The experimental results are plotted in Fig. 2, where the solid curves depict the mean across evaluations and the shaded region represents one standard deviation over five runs. A sliding window of five was applied to achieve smoother curves. This same representation is also used in Figs. 3 and 4.\nTable IV displays the average returns from five random seeds for the last ten evaluations, with the maximum value for each task highlighted in bold. This formatting is also applied in Tables V and VI.\nThe comparison here is straightforward. Both Fig. 2 and Table IV clearly demonstrate that TDDR outperforms DDPG and TD3 across all environments."}, {"title": "B. Comparison with DARC, SD3, and GD3", "content": "We have already discussed the additional hyperparameters in DARC, SD3, and GD3, and their effects on these algo-"}, {"title": "C. Ablation Discussion", "content": "TDDR consists of two major feature components: DA- CDQ and CRA, which are are analyzed here using ablation experiments.\nFirst, the DA-CDQ generates actions as in (6). When one actor is removed, either $\\min_{i=1,2}(Q_{\\theta'_i}(s', a'_1))$ or $\\min_{i=1,2}(Q_{\\theta'_i}(s', a'_2))$ would disappear, rendering TDDR in- valid, hence making it impossible to conduct this part of the ablation study.\nAt the same time, it can be observed that when removing an actor, CRA becomes ineffective, meaning that $\\delta_1$ and $\\delta_2$ parts cease to be effective. At this point, TDDR becomes an algorithm based on a single actor, which is equivalent to TD3. Assuming that $\\pi_{\\phi_2}$ is deleted, it can be found that CRA of TDDR contains only $\\delta_1$, which can no longer be compared with $\\delta_2$, thus losing its regularization significance. Hence, the performance evaluation of TDDR compared to TD3 can be regarded as an ablation study with one actor removed.\nSecond, we analyze CRA. Compared to removing an actor, removing either $\\delta_1$ or $\\delta_2$ directly destroys TDDR. First, if it is impossible to compare $\\delta_1$ and $\\delta_2$, then it is not feasible to calculate the TD target, thus (14) are invalid. Second, it would lead to the inability to update the hyperparameters of the critic target network because in TDDR one actor is responsible for one critic. Therefore, CRA is critical to TDDR, and removing CRA would make TDDR ineffective."}, {"title": "VII. CONCLUSION", "content": "In this paper, we combine double actors with double critics to achieve better Q-value estimation and propose the DA- CDQ and CRA based on TD error, leading to the introduction of TDDR. TDDR significantly outperforms both TD3 and DDPG. Notably, when one actor is removed, TD3 and TDDR become equivalent. Another advantage of TDDR is that it does not require additional hyperparameters beyond those in TD3, unlike the recently proposed value estimation methods DARC,"}]}