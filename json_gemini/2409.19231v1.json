{"title": "Double Actor-Critic with TD Error-Driven Regularization in Reinforcement Learning", "authors": ["Haohui Chen", "Zhiyong Chen", "Aoxiang Liu", "Wentuo Fang"], "abstract": "To obtain better value estimation in reinforcement\nlearning, we propose a novel algorithm based on the double\nactor-critic framework with temporal difference error-driven\nregularization, abbreviated as TDDR. TDDR employs double\nactors, with each actor paired with a critic, thereby fully\nleveraging the advantages of double critics. Additionally, TDDR\nintroduces an innovative critic regularization architecture. Com-\npared to classical deterministic policy gradient-based algorithms\nthat lack a double actor-critic structure, TDDR provides superior\nestimation. Moreover, unlike existing algorithms with double\nactor-critic frameworks, TDDR does not introduce any additional\nhyperparameters, significantly simplifying the design and imple-\nmentation process. Experiments demonstrate that TDDR exhibits\nstrong competitiveness compared to benchmark algorithms in\nchallenging continuous control tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement learning (RL) has gained significant\nattention as a versatile research area, with applications\nspanning recommender systems [1], autonomous driving [2],\ncontroller design [3], and medical fields [4]. One of the\nmost well-known RL algorithms, deep Q-learning (DQN),\nhas demonstrated success in learning optimal policies [5].\nHowever, DQN struggles in continuous control scenarios, par-\nticularly in environments with high-dimensional action spaces\n[6], [7].\nIn contrast, actor-critic (AC) algorithms, such as trust region\npolicy optimization (TRPO) [8], proximal policy optimization\n(PPO) [9], and deep deterministic policy gradient (DDPG)\n[10], are better suited for continuous control tasks. Further\ninsights into AC-based algorithms are discussed in other works\n[11], [12], [13]. However, DDPG and related algorithms are\nprone to overestimation bias. To address this, Fujimoto et al.\nintroduced the twin delayed deep deterministic policy gradient\n(TD3) algorithm [14], which leverages double critics to reduce\noverestimation. Notably, TD3 still employs a single critic for\nupdates and action selection.\nBuilding on the double actor-critic (DAC) framework, sev-\neral advanced algorithms, such as double actors regularized\ncritics (DARC) [15], softmax deep double deterministic policy\ngradients (SD3) [16], and generalized-activated deep double\ndeterministic policy gradients (GD3) [17], have introduced dif-\nferent regularization techniques to improve value estimation.\nTo fully leverage the potential of double critics, we propose\nthe temporal difference error-driven regularization (TDDR)\nalgorithm, built upon the DAC framework. Specifically, TDDR\nemploys clipped double Q-learning (CDQ) with double actors\n(DA-CDQ), generating four Q-values and utilizing the TD\nerror from their target networks to guide the selection of the\nappropriate Q-value for critic updates. A more detailed com-\nparison of TDDR with existing algorithms will be provided in\nSection II.\nThe primary contributions of TDDR are as follows.\n1. TDDR demonstrates strong competitiveness in chal-\nlenging continuous control tasks, outperforming benchmark\nalgorithms without introducing additional hyperparameters.\n2. A key innovation is the introduction of TD error-\ndriven regularization within the DAC framework. Furthermore,\nTDDR incorporates the CDQ approach, which utilizes double\ntarget actors for action evaluation, resulting in CDQ based on\ndouble actors.\n3. A convergence proof for TDDR is provided under both\nrandom and simultaneous updates, and its performance is\nvalidated through numerical benchmark comparisons.\nThe remainder of the paper is organized as follows. Sec-\ntion II reviews the related work and compares the benchmark\nalgorithms with the proposed TDDR. Section III covers the\npreliminaries and motivations for our study. In Section IV,\nwe provide a detailed explanation of the TDDR algorithm.\nSection V presents the convergence analysis of the TDDR al-\ngorithm. Section VI includes experimental results on MuJoCo\nand Box2d continuous control tasks, along with a discussion\nof the performance relative to benchmark algorithms. Finally,\nSection VII concludes the paper and outlines potential future\nextensions of this work."}, {"title": "II. RELATED WORK", "content": "This section is divided into two parts: the first part reviews\nrelated work, and the second part compares the benchmark\nalgorithms with TDDR."}, {"title": "A. Related Work", "content": "DDPG and TD3 serve as benchmark algorithms in this\npaper, both utilizing TD error for updating critic parameters\nand are widely adopted within the AC framework. Several\nadvanced algorithms build upon this foundational structure.\nFor example, Wu et al. [18] introduced the triplet-average deep\ndeterministic (TADD) policy gradient algorithm, which incor-\nporates weighted TD error regularization for critic updates.\nHowever, TADD introduces two additional hyperparameters\ncompared to TD3, which are environment-dependent and\ncan increase computational complexity, potentially impairing\nperformance if not selected appropriately.\nCheng et al. [19] proposed the robust actor-critic (RAC)\nalgorithm, which incorporates constraints on the entropy be-\ntween policies. While RAC demonstrates some hyperparam-\neter insensitivity in certain environments, its overall perfor-\nmance remains affected by the additional hyperparameters.\nSimilarly, Li et al. [20] introduced the co-regularization-based\ndeep deterministic (CoD2) policy gradient algorithm, which\nalternates the TD targets between DDPG and TD3; however,\nits performance is highly sensitive to the regularization coef-\nficient.\nConversely, other policy-based regularization algorithms\nexhibit lower sensitivity to hyperparameter settings [21]. Li\net al. [22] introduced the weakly pessimistic value estimation\nand optimistic policy optimization (WPVOP) algorithm, which\nemploys a pessimistic estimation to adjust the TD target.\nHowever, its dependence on hyperparameters varies across\nenvironments, leading to increased computational complexity.\nCetin et al. [23] proposed the generalized pessimism learning\n(GPL) algorithm, which integrates dual TD targets to coun-\nteract bias from pessimistic estimation, resulting in improved\nvalue estimation with reduced computational overhead.\nAdditionally, Li et al. [24] developed the multi-actor mech-\nanism (MAM), which offers multiple action choices within\na single state to optimize policy selection. Despite its higher\ncomputational cost, MAM effectively enhances value estima-\ntion, demonstrating the advantages of multi-actor approaches\n[15].\nWhile these algorithms have shown promising results, lim-\nited research has investigated how DAC can achieve improved\nvalue estimation without introducing additional hyperparame-\nters, a key issue addressed in this paper. Notable algorithms\nin this context, which serve as our benchmark methods built\nupon the DAC framework, include DARC [15], SD3 [16], and\nGD3 [17]."}, {"title": "B. Comparison", "content": "The major differences between the benchmark algorithms\nand the proposed TDDR are summarized in Table I, high-\nlighting their features in key components. A discussion of\nadditional differences follows below, with technical details\nprovided in Sections III and IV.\nThe first distinction is the structure of TDDR compared\nto the benchmark algorithms. TDDR clearly differs from\nDDPG and TD3 in terms of the number of actors and critics.\nWhile the other DAC benchmark algorithms directly apply TD\nerror regularization for critic updates, similar to DDPG and\nTD3, TDDR introduces a novel form of critic regularization\ndriven by the TD error of its target networks, representing a\nsignificant innovation.\nAnother critical difference lies in the hyperparameters used\nin the benchmark algorithms compared to TDDR. Specifically,\nTDDR does not introduce additional hyperparameters beyond\nthose in TD3, while other DAC-based benchmark algorithms\ndo. The introduction of extra hyperparameters can significantly\ndestabilize the learning process if not carefully tuned, often\nleading to increased standard deviation in performance. Conse-\nquently, finding effective hyperparameters presents a challenge\nin reinforcement learning. Moreover, reducing the number\nof additional hyperparameters while ensuring improved value\nestimation is a vital issue that needs to be addressed.\nDARC introduces two regularization coefficients, \\(\\lambda\\) and \\(v\\). The coefficient \\(\\lambda\\) constrains the gap between Q-values during\ncritic updates, while \\(v\\) is used to mitigate the risk of overesti-\nmation. The effectiveness of \\(\\lambda\\) relies on achieving significantly\nimproved value estimation. DARC simplifies computation by\nderiving a single value from the two Q-values produced by\nthe double actors. However, if one Q-value is overestimated,\nthe corrective capability of the other Q-value depends on \\(v\\).\nBoth \\(\\lambda\\) and \\(v\\) can vary across different tasks.\nSD3 adds two additional hyperparameters: the number of\nnoise samples (NNS) and the parameter \\(\\beta\\). NNS controls the\ninfluence of value estimation but shows limited sensitivity in\nSD3. The parameter \\(\\beta\\) is crucial for balancing variance and\nbias in value estimation. GD3 extends this by introducing\nfour hyperparameters, including NNS, \\(\\beta\\), a bias term \\(b\\), and\nactivation functions. While NNS and \\(\\beta\\) function similarly in\nboth SD3 and GD3, the bias term \\(b\\) and the choice of activation\nfunctions are specific to GD3. Both SD3 and GD3 utilize the\nminimum Q-value from the double actors.\nIn summary, DARC, SD3, and GD3 incorporate additional\nhyperparameters to enhance value estimation, which increases\ncomputational complexity and can lead to performance insta-\nbility if these hyperparameters are not finely tuned. In contrast,\nTDDR does not introduce any additional hyperparameters\ncompared to TD3, making it easier to implement."}, {"title": "III. PRELIMINARIES", "content": "The interaction between an agent and the environment\nfollows a standard RL process, which is modeled as a\nMarkov decision process (MDP). MDP is defined by the\ntuple \\(M = (S, A, P, R(s, a), \\gamma)\\), where \\(S\\) is the state space,\n\\(A\\) is the action space, \\(P\\) is the state transition probability,\n\\(R(s, a)\\) is the reward function, and \\(\\gamma \\in [0,1]\\) is the discount\nfactor. The goal of RL is to find the optimal policy that\nmaximizes cumulative rewards. Value estimation is commonly\nused to evaluate the effectiveness of policies, with higher value\nestimations generally indicating better policies [25]. We briefly\ndescribe several relevant reinforcement learning algorithms,\nincluding DDPG, TD3, DARC, SD3, and GD3, which serve\nas benchmarks. Building on these, we propose and compare\nthe new TDDR algorithm."}, {"title": "A. Deterministic Policy Gradient", "content": "Silver et al. [26] introduced the deterministic policy gradient\n(DPG) algorithm, which uses deterministic policies instead of\nstochastic ones. Let \\(\\pi_{\\phi}\\) refer to the policy with parameters \\(\\phi\\).\nSpecifically, \\(\\pi_{\\phi}(s)\\) denotes the action \\(a\\) output by the policy\nfor the given state \\(s\\). To maximize the cumulative discounted\nrewards \\(R(s,a)\\), \\(\\phi\\) is updated as \\(\\phi \\leftarrow \\phi + \\alpha\\nabla_{\\phi}J(\\phi)\\), where\n\\(\\alpha\\) is the learning rate. The DPG is defined as:\n\\(\\nabla_{\\phi}J(\\phi) = N^{-1}\\sum_{s}( \\nabla_a Q_{\\theta}(s,a) \\vert_{a=\\pi_{\\phi}(s)}\\nabla_{\\phi}\\pi_{\\phi}(s) )\\) (1)\nwhere \\(J(\\phi)\\) is the objective function, typically the expected\nreturn we aim to maximize, and \\(\\nabla_a Q_{\\theta}(s, a) \\vert_{a = \\pi_{\\phi}(s)}\\) repre-\nsents the gradient of the action-value function \\(Q\\) with respect\nto its parameters \\(\\theta\\), evaluated at \\(a = \\pi_{\\phi}(s)\\). The gradient\n\\(Q_{\\theta}(s, a)\\), for updating \\(\\theta\\), is usually obtained by minimizing\n\\(N^{-1}\\Sigma(y-Q_{\\theta}(s, a))^2\\), where \\(y\\) is the TD target, \\(y-Q_{\\theta}(s,a)\\)\nis the TD error, and \\(N\\) represents the batch size.\nLillicrap et al. [10] introduced deep neural networks into\nDPG and proposed the DDPG algorithm. DDPG introduces the\nconcept of target networks and utilizes a soft update approach\nfor both policy and value parameters:\n\\(\\theta' \\leftarrow \\tau\\theta + (1 - \\tau)\\theta', \\phi' \\leftarrow \\tau\\phi + (1 - \\tau)\\phi'\\) (2)\nwhere \\(\\theta'\\) represents the parameters of the critic target network,\nand \\(\\phi'\\) represents the parameters of the actor target network.\nHere, \\(\\tau \\ll 1\\) is a constant. DDPG is an effective RL algorithm\nfor continuous control tasks [27]. The TD target for DDPG,\ncomputed using the target networks, is:\n\\(y = r + \\gamma Q_{\\theta'}(s', \\pi_{\\phi'}(s'))\\), (3)\nwhere \\(s'\\) is the next state, and the output of \\(\\pi_{\\phi'}(s')\\) represents\nthe next action, denoted as \\(a'\\).\nTD3 [14] is an enhanced version of DDPG that adopts\na new approach of delayed policy updates to mitigate the\nadverse effects of update variance and bias. It also incorporates\na regularization approach from machine learning by adding\nnoise to the target policy to smooth out value estimations."}, {"title": "B. Double Actor-Critic", "content": "Double critics utilize two independent critic networks that\ngenerate separate value estimations without mutual influence.\nOriginating from the concept of double Q-learning, this al-\ngorithm employs dual critics to decouple actions, thereby\nmitigating the overestimation issue. For example, the TD target\nin TD3 is defined as:\n\\(y = r + \\gamma min_{i=1,2}(Q_{\\theta_i}(s', a'))\\), (4)\nwhere \\(Q_{\\theta_i}\\) and \\(Q_{\\theta'_i}\\) represent the target networks of the double\ncritics. The action \\(a'\\) is defined as:\n\\(a' = \\pi_{\\phi'}(s') + \\epsilon\\) (5)\nincorporating noise \\(\\epsilon\\). The TD target (4) is used to define the\nTD error, \\(y - Q_{\\theta_i}(s, a)\\) for \\(i = 1,2\\), and update the double\ncritic networks \\(Q_{\\theta_1}\\) and \\(Q_{\\theta_2}\\). TD3 demonstrates superior value\nestimation compared to DDPG in MuJoCo by OpenAI [28].\nAlongside TD3, other benchmark algorithms like DARC, SD3,\nand GD3 also integrate double critics.\nIn addition to double critics, the use of double actors\n\\(\\pi_{\\phi_1}\\) and \\(\\pi_{\\phi_2}\\), along with their target networks \\(\\pi_{\\phi'_1}\\) and \\(\\pi_{\\phi'_2}\\),\neffectively enhances action exploration efficiency and prevents\npolicies from settling into local optima, thereby achieving\nimproved value estimation [15], [16]. The actions \\(a', i = 1,2\\),\ngenerated by the target networks are denoted as:\n\\(a' = \\pi_{\\phi'_i}(s') + \\epsilon\\). (6)\nBenchmark algorithms such as DARC, SD3, and GD3 also\nincorporate double actors."}, {"title": "C. Regularization", "content": "The TD target for DDPG (3) can be rewritten as\n\\(y = r + \\gamma\\psi\\) (7)\nwhere\n\\(\\psi = Q_{\\theta'}(s', a')\\) (8)\nWhen double critics are used, the value \\(\\psi\\) for each critic\nfollowing DDPG (3) is given as\n\\(\\psi_i = Q_{\\theta'_i}(s', a').\\) (9)\nHowever, this value is not used directly. For example, it is\nmodified in (4) to\n\\(\\psi = min_{i=1,2} Q_{\\theta'_i}(s', a').\\) (10)\nIt is noted that the same \\(\\psi\\), and hence the same TD target \\(y\\),\nis used for both critics. The difference between (9) and (10)\nacts as a regularization of the TD error.\nDifferent forms of regularization have been proposed in the\nliterature, varying in the construction of \\(\\psi\\). For TD3, \\(\\psi\\) is\ndefined in (10) where it uses the smaller Q-values from the\ndouble critics to compute \\(\\psi\\). For DARC, \\(\\psi\\) is computed as:\n\\(\\psi =(1 \u2013 v) max_{j=1,2} min_{i=1,2} Q_{\\theta'_i}(s', a'_j)\\)\n\\(+ v min_{j=1,2} min_{i=1,2} Q_{\\theta'_i}(s', a'_j)\\) (11)\nwhere \\(v\\) is the weighting coefficient. DARC computes \\(\\psi\\) by\ncombining the Q-values of the double critics and double actors\nin a convex combination. In addition to the hyperparameter \\(v\\)\nused to balance overestimation and underestimation, DARC\nintroduces another regularization parameter, \\(\\lambda\\), which restricts\nthe critics from differing too significantly.\nFor SD3 and GD3, \\(\\psi\\) is constructed as:\n\\(\\psi = min_{j,i=1,2} Q_{\\theta'_i}(s', a'_j),\\) (12)\nwhich is similar to TD3 but incorporates actions from double\nactors. SD3 introduces two hyperparameters. NNS influences\nthe effect of value estimation but is not highly sensitive. In\ncontrast, the parameter \\(\\beta\\) impacts the effectiveness and bias of\nvalue estimation: a lower \\(\\beta\\) decreases variance, while a higher"}, {"title": "IV. THE TDDR ALGORITHM", "content": "The TDDR algorithm adopts DAC with their respective\ntarget networks. These eight networks are denoted as \\(Q_{\\theta_i}, Q_{\\theta'_i},\\ \\pi_{\\phi_i}, \\pi_{\\phi'_i}\\), for \\(i = 1,2\\), as defined earlier. In Fig. 1, these neural\nnetworks are simply labeled as \\(Q_i, Q'_i, A_i, A'_i\\), for \\(i = 1,2\\),\nrespectively.\nThe core of the algorithm involves updating the eight net-\nworks. The target networks \\(Q'\\) and \\(A'\\) are updated following\nconventional rules, similar to (2), succinctly denoted as:\n\\(\\theta' \\leftarrow (\\theta_i, \\theta'_i), \\Phi' \\leftarrow (\\Phi_i, \\Phi'_i)\\)\nfor \\(i = 1,2\\). The update of each actor networks \\(A_i\\) is\nperformed as \\(\\Phi_i \\leftarrow \\Phi_i + \\alpha \\nabla_{\\phi_i}J(\\Phi_i)\\) where the DPG \\(\\nabla_{\\phi_i} J(\\Phi_i)\\)\nis defined in (1) using the corresponding critic network \\(Q_{\\theta_i}\\).\nKey innovations in the proposed TDDR algorithm focus\non updating the critic networks \\(Q_i\\), parameterized by \\(\\theta_i\\).\nThe first innovation involves adopting the clipped double Q-\nlearning (CDQ) approach, which utilizes double target actors\nto evaluate actions, resulting in CDQ based on double actors\n(DA-CDQ). Regularizing the TD error plays a crucial role in\nupdating the double critic networks. The second innovation\nintroduces a novel critic regularization architecture (CRA),\nwhich is driven by the TD error of their target networks.\nThese two features, along with a comparative analysis against\nbenchmark algorithms, are elaborated upon in the subsequent\nsections.\nLastly, it is noteworthy that TDDR also utilizes the cross-\nupdate architecture employed in DARC, where only one pair,\neither \\((Q_1, A_1)\\) or \\((Q_2, A_2)\\), is updated at each step."}, {"title": "A. Double Actors with CDQ", "content": "The proposed algorithm adopts the CDQ approach, similar\nto TD3, by incorporating noise during the action sampling\nprocess as defined in (6) for each target actor \\(i = 1,2\\).\nHere, \\(\\epsilon \\sim clip(N(0, \\sigma), -c, c)\\) represents Gaussian noise with\nmean 0 and variance \\(\\sigma\\), clipped to the range of \\([-c, c]\\) for a\npositive constant \\(c\\). This clipping effectively limits the noise\nmagnitude, ensuring that actions remain within a reasonable\nrange and preventing excessive perturbations.\nThe DA-CDQ method, inspired by TD3, employs the min\noperator to clip both value estimations during the update\nprocess. In contrast to TD3's single actor, DA-CDQ utilizes\ndouble actors, where each actor network independently selects\nactions. These actions are then evaluated by double critics\nto assess their value. Importantly, each actor network can\nautonomously learn and refine its own policy. DA-CDQ en-\nhances exploration capabilities and effectively prevents actions\nfrom becoming trapped in local optima, which can occur with\na single actor setup like TD3. This approach facilitates the\ndiscovery of potentially optimal policies [15]. Using a single\nactor, as in TD3, results in identical training targets for the\ndouble critics, potentially limiting the advantages they offer\n[16]."}, {"title": "B. Critic Regularization Architecture", "content": "Each actor target generates an action \\(a'_i\\) as in (6), which,\ntogether with the target networks of double critics, can be used\nto define the TD error of the critic targets \\(Q_i\\) as follows:\n\\(\\delta_i = r + \\gamma min_{j=1,2}(Q_{\\theta'_j}(s', a'_i)) - min_{j=1,2}(Q_{\\theta'_j}(s, a))\\) (13)\nfor \\(i = 1,2\\). For each action \\(a'_i\\), both critic target networks\n\\(Q'_j\\), \\(j = 1,2\\), are used to estimate the values for the next\nstate \\(s'\\). The smaller value is used to define the TD target\n\\(r + \\gamma min_{j=1,2}(Q_{\\theta'_j}(s', a'_i))\\). Correspondingly, the two critic\ntarget networks generate two action-values for \\((s, a)\\), and the\nsmaller value \\(min_{j=1,2}(Q_{\\theta'_j}(s, a))\\) is utilized as the actual\naction-value. The TD target and the actual action-value define\nthe TD error in (13) for each actor target network.\nIt is important to note that these TD errors in (13) are not for\nthe critics \\(Q_i\\) nor are they used to update \\(\\theta_i\\) directly. However,\nthese TD errors drive the calculation of the regularization\nof the TD errors of \\(Q_i\\). This TD error-driven regularization\nis a novel feature compared to the existing regularizations\nintroduced earlier. The specific computation of \\(\\psi\\) for TDDR\nis given as\n\\(\\psi = \\begin{cases} min_{i=1,2}(Q_{\\theta'_i}(s', a'_1)), & \\text{if } |\\delta_1| \\leq |\\delta_2| \\\\\nmin_{i=1,2}(Q_{\\theta'_i}(s', a'_2)), & \\text{if } |\\delta_1| > |\\delta_2| \\end{cases}\\) (14)\nIn the existing calculations of \\(\\psi\\) in (10), (11), and (12),\nthe smaller action-value of the double critic target networks,\nevaluated either on a single actor target or double actor targets,\nis directly used. In contrast, in (14), the smaller action-value,\nevaluated on the selected actor target network that gives the\nsmaller TD error (\\(\\delta_1\\) or \\(\\delta_2\\)), is used.\nOnce \\(\\psi\\) is determined in (14), the actual TD error for the\ncritics \\(Q_i\\) is calculated as in (7). The gradient \\(\\nabla Q_{\\theta}(s, a)\\),\nused for updating \\(\\theta_i\\), is obtained by minimizing\n\\(N^{-1}\\sum_{S}(y \u2013 Q_{\\theta_i}(s, a))^2\\). (15)\nUsing DAC to compute \\(\\psi\\) as in (14) in TDDR offers\nseveral advantages. First, it avoids overestimation by selecting\nthe smaller value based on the comparison between \\(|\\delta_1|\\) and\n\\(|\\delta_2|\\). This ensures that the TD target is calculated based on\nthe action-value generated by either target policy \\(A_1\\) or \\(A_2\\),\nreducing the risk of overestimation.\nAdditionally, it balances exploration and exploitation. By\nusing two Q-values to calculate the TD target, the approach\nencourages a balance between exploring new actions and\nexploiting known ones. When comparing \\(|\\delta_1|\\) and \\(|\\delta_2|\\), if the\ncurrent policy is \\(A_1\\), then \\(A_1\\) is exploited while \\(A_2\\) is explored,\nand vice versa. This dynamic helps in discovering potentially\noptimal policies while maintaining robustness in the learning\nprocess."}, {"title": "C. Comparative Analysis of Algorithms", "content": "The architecture of TDDR is depicted in Fig. 1. Benchmark\nalgorithms can be represented similarly: DDPG consists of\none critic and one actor, TD3 incorporates double critics with\none actor, and DARC, SD3, and GD3 have DAC architectures\nlike TDDR. Using this figure, the specific differences between\nTDDR and these benchmark algorithms are analyzed below.\nWhen the eight neural networks reduce to four, with \\(\\theta_i\\),\n\\(\\theta'_i, \\phi_i\\), and \\(\\phi'_i\\) for \\(i = 1,2\\) being replaced by \\(\\theta, \\theta', \\phi\\), and \\(\\phi'\\),\nrespectively, TDDR simplifies to DDPG, which is the simplest\nmode among these six algorithms. With only \\(\\Phi_i\\) and \\(\\Phi'_i\\) for\n\\(i = 1,2\\) replaced by \\(\\phi\\) and \\(\\phi'\\), respectively, the remaining six\nneural networks of TDDR constitute TD3.\nGiven that DARC, SD3, and GD3 share the same network\narchitecture as TDDR, their distinctions are subtle but impact-\nful in terms of performance. The first distinction is the use\nof CDQ in TDDR, which is absent in the other algorithms.\nThe second distinction is in the definition of the TD error-\ndriven \\(\\psi\\) in (14), which differs significantly from those in\nother algorithms, such as (11) and (12). This difference is\nhighlighted by the \"TDDR only\" block in the figure.\nThe third distinction is that TDDR does not introduce\nadditional hyperparameters beyond those in TD3, whereas\nDARC, SD3, and GD3 introduce two, two, and four addi-\ntional hyperparameters, respectively. While hyperparameters\ncan increase the complexity of the algorithms and the tuning\nworkload, TDDR simplifies the design and implementation\nprocess by avoiding this additional complexity.\nFor instance, the critics in DARC are regularized to be close\naccording to (11), ensuring they are not isolated. While the\nmutually regularized double critics in DARC perform well in\nvalue function estimation, their performance is significantly\ninfluenced by the regularization coefficient \\(v\\). Appropriate\nregularization coefficients can lead to better value estimation,\nwhereas test results show that unsuitable regularization coeffi-\ncients can significantly increase the standard deviation of the\nresults, reducing DARC's stability. In contrast, the mutually\nindependent double critics structure in TDDR is not affected\nby hyperparameter selection. In TDDR, the critics are fully\nutilized, with each critic responsible for its corresponding\ncounterpart actor, and both critics contributing to policy im-\nprovement."}, {"title": "V. CONVERGENCE ANALYSIS", "content": "Convergence analysis has been studied in double Q-learning\n[29", "follows": "n\\(Q_{t+1"}, {"as": "n\\(a_1 = arg \\underset{a}{max} Q^{A}(s_{t+1}, a), a_2 = arg \\underset{a}{max} Q^{B}(s_{t+1}, a).\\)\nIt is noted that \\(r_t + \\gamma Q^{B}(s_{t+1}, a_{1})\\) is the TD target for\n\\(Q^{A}(s_t, a_t)\\), and vice versa for swapping A and B. Treating\nthe critics \\(Q_{\\theta_1} (s, a)\\) and \\(Q_{\\theta_2} (s, a)\\) as \\(Q^{A}(s, a)\\) and \\(Q^{B} (s,a)\\),\nrespectively, the TD target in (7) is \\(r_t+\\gamma\\psi_t\\), explicitly depend-\ning on \\(t\\). In the definitions of \\(\\psi\\) in (10), (11), (12), or (14), \\(\\psi\\)\ncannot be simply expressed as \\(Q^{B} (s_{t+1}, a_{1})\\) or \\(Q^{A}(s_{t+1}, a_{2})\\)\nas in (16). The first difference is that \\(\\psi\\) is calculated from\nthe critic and actor target networks, while \\(Q^{B}(s_{t+1}, a_{1})\\) or\n\\(Q^{A}(s_{t+1}, a_{2})\\) are the directly calculated optimal values. The\ninfluence of adding target networks to convergence is too\ncomplicated to analyze here.\nThe second difference is that \\(\\psi\\) is not \\(Q^{B}(s_{t+1}, a_{1})\\) or\n\\(Q^{A}(s_{t+1}, a_{2})\\), but rather the smaller value generated by the\ncritics, denoted as \\(min_{i=1,2} Q_{\\theta'}\\) in all the aforementioned\ndefinitions of \\(\\psi\\). To capture this difference in all the aforemen-\ntioned algorithms, including DARC, SD3, GD3, and TDDR,\nthe formulation in (16) is modified to\n\\(Q_{t+1}^{A}(s_t, a_t) = Q_{t}^{A}(s_t, a_t) + \\alpha_{t}(s_t, a_t)(r_t+\\)\n\\(\\gamma min\\{Q_{t}^{B} (s_{t+1}, a_{i=1,2}), Q_{t}^{A} (s_{t+1}, a_{i=1,2})\\} \u2013 Q_{t}^{A}(s_t, a_t))\\)\n\\(Q_{t+1}^{B}(s_t, a_t) = Q_{t}^{B}(s_t, a_t) + \\alpha_{t}(s_t, a_t)(r_t+\\)\n\\(\\gamma min\\{Q_{t}^{B} (s_{t+1}, a_{i=1,2}), Q_{t}^{A} (s_{t+1}, a_{i=1,2})\\} \u2013 Q_{t}^{B}(s_t, a_t)).\\) (17)\nHere, \\(a_{i=1,2}\\) can be either \\(a_1\\) or \\(a_2\\), depending on the specific\nalgorithm. For instance, in TDDR, \\(a_{i=1,2} = a_1\\) when \\(|\\delta_1| \\leq\n|\\delta_2|\\), and \\(a_{i=1,2} = a_2\\) otherwise, where \\(\\delta_i\\) defined in (13) is\nmodified to\n\\(\\delta_{i=1,2} =r_t + \\gamma min\\{Q_{t}^{B} (s_{t+1}, a_{i=1,2}), Q_{t}^{A} (s_{t+1}, a_{i=1"}]}