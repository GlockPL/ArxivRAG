{"title": "Uncertainty-Aware Regularization for Image-to-Image Translation", "authors": ["Anuja Vats", "Ivar Farup", "Marius Pedersen", "Kiran Raja"], "abstract": "The importance of quantifying uncertainty in deep networks has become paramount for reliable real-world applications. In this paper, we propose a method to improve uncertainty estimation in medical Image-to-Image (I2I) translation. Our model integrates aleatoric uncertainty and employs Uncertainty-Aware Regularization (UAR) inspired by simple priors to refine uncertainty estimates and enhance reconstruction quality. We show that by leveraging simple priors on parameters, our approach captures more robust uncertainty maps, effectively refining them to indicate precisely where the network encounters difficulties, while being less affected by noise. Our experiments demonstrate that UAR not only improves translation performance, but also provides better uncertainty estimations, particularly in the presence of noise and artifacts. We validate our approach using two medical imaging datasets, showcasing its effectiveness in maintaining high confidence in familiar regions while accurately identifying areas of uncertainty in novel/ambiguous scenarios.", "sections": [{"title": "1. Introduction", "content": "The significance of quantifying the uncertainty embedded in the learning process of deep neural networks has become increasingly important for identifying the blind spots [20] and biases [13] in models before their application in the real world. Despite the quest for learning from datasets that are larger, more diverse and representative, it has become evident that not all data points will adhere to the assumed distribution, leaving room for potential inaccuracies and biases in model predictions. Model uncertainty as a measure can serve to be a very useful tool for identifying the limitations to our model predictions and accounting for instances where real data may lie outside the learning distribution. Considering critical application domains such as healthcare, military, criminal justice or automated driving for deep learning models, model performance, albeit high in isolation, is being considered grossly insufficient for their adoption in practice [26]. The inability to isolate scenarios where a model isn't confident about its decision and the causes underlying that poses a significant barrier to trust and reliability in these safety-critical domains.\nThis article discusses uncertainty in the context of medical I2I translation. I2I is the problem of transforming an image from one domain into a corresponding image in another domain while maintaining semantic consistency and information preservation during translation. In traditional endoscopy, narrowband imaging (NBI) is used alongside standard imaging for enhanced visualization of abnormalities. However, if a region is not captured with NBI during the procedure, the enhanced information is unavailable post-procedure, limiting its use in retrospective diagnosis. Models that translate standard images to NBI are therefore critical, offering a valuable tool for post-hoc analysis when NBI was not captured. While similar functionality would be highly beneficial in capsule endoscopy, constraints like device size and battery limit real-time acquisition. Virtual chromo-endoscopy (e.g., FICE) can be applied post-capture to enhance abnormalities [25], making I2I translation particularly relevant in endoscopic imaging. Despite advancements, deep learning methods for I2I translation often produce outputs with inherent uncertainties, particularly in ambiguous or unseen scenarios. Moreover, attempts at generalization exacerbates this uncertainty, as variations in datasets or slight shifts in capture modalities can rapidly escalate uncertainty levels. Since, medical image acquisition is often prone to noise and modality-specific artifacts, it is paramount to faithfully quantify and convey model uncertainty to ascertain the extent of generalization achievable. Delineating the model's confidence levels and identifying domain gaps where it struggles, allows to effectively discern where and how to apply the model.\nModel uncertainty is broadly composed to two types, the epistemic or uncertainty regarding the model parameters and aleatoric resulting from noise inherent in the data [9, 17]. The epistemic uncertainty assumes a prior distribution over model parameters and often approximated as the variance in predictions from multiple forward passes through the network with different dropout masks applied for example. The aleatoric uncertainty, on the other hand, assumes a distribution on the models outputs and is approximated using Maximum a Posteriori (MAP) estimation [21]."}, {"title": "2. Related Work", "content": "Medical image-to-image translation has seen significant advancements through the use of generative adversarial networks (GANs) and its variants. Typical application in medical I2I include modality translation [5, 32], image synthesis [35], segmentation [22] and super-resolution [12]. Modality translation using CycleGAN [36] has been particularly influential, enabling unsupervised translation by employing cycle consistency losses to ensure that translated images can be mapped back to the original modality. Similarly, conditional GANs [15] have allowed generation preconditioned on inputs such as anatomical labels [4], modality [8] or priors useful to generation [3]. Another class of models includes diffusion models [14, 16] that utilize parameterized Markov chains to iteratively refine data, optimizing the lower variational bound on the likelihood function [18, 10].\nIn WCE, image translation has been most commonly applied for image super-resolution [2, 27]. Uncertainty quantification in medical I2I has been relatively less explored. In [23] authors argue the usefulness of uncertainty estimation in MR to CT translation for detecting synthesis failures. They use traditional formulations where epistemic uncertainty is estimated by sampling from a variational distribution using dropout, and the aleatoric component is derived from the variance of the predicted distribution. Authors in [6] and [7] utilize variations of test-time augmentation for estimating uncertainty. Ayhan et al. [6] generate augmented examples for each test case to approximate the predictive distribution, whereas Baltruschat et al. leverage predictions from multiple 2D slicing planes instead of augmentations for the same goal. Our work is most closely related to [29, 30, 28] that model predictive distributions using generalized Gaussian distributions. However, unlike [29], which employs multiple sequential GANs to iteratively reduce aleatoric uncertainty, we introduce a lightweight regularization term that achieves this within a single model. As a result, our uncertainty estimates can differentiate between familiar versus newer or significantly larger sources of uncertainty, overcoming the drawback of previous methods that treat all uncertainty sources equally.\nOne of the primary challenges in medical I2I translation problems is the inherent ambiguity associated with image capturing mechanisms and its effect on a model's performance. Consider the case of WCE where images are often captured using low-resolution cameras under myriad distortions [33, 1], requiring significant post-processing before they are suitable for diagnosis. Noise and compression artifacts encountered during transmission further degrade the quality [11]. The cumulative impact of these factors can manifest subtly as deviations from the anticipated model performance, potentially leading to misdiagnoses. As discussed prior, one approach to mitigating this is to quantify the uncertainty associated with model predictions. Measuring the uncertainty allows detecting unaccounted shifts that can be addressed proactively. Despite relevance, uncertainty quantification and refinement is relatively nascent in I2I translation problems."}, {"title": "3. Methodology", "content": "We introduce both the conventional and probabilistic formulations of paired I2I translation, highlighting their limitations. Subsequently, we present the proposed UAR for improving uncertainty estimation and guidance."}, {"title": "3.1. 121 Translation Formulation", "content": "Consider a collection of input images from a domain A denoted as $\\mathcal{X}_A := \\{x_1, x_2, ..., x_n\\}$, and another set of paired images originating from a domain B, expressed as $\\mathcal{X}_B := \\{x'_1, x'_2, ..., x'_n\\}$. The dataset D comprises pairs $(x_i, x'_i)$ drawn from the respective domains A and B. The objective is to learn the underlying conditional distribution $P_{B|A}$ facilitating the translation of images from A \u2192 B.\nAs shown in Fig.1, this can typically be achieved by minimizing the point estimate for per-pixel residual at jk, $d_{jk} = ||x'_{jk}-x_{jk}||^2$ between the reconstructed and ground-truth image from domain B. However, in pixel reconstruction tasks, the solution space is often multimodal, meaning that multiple outputs can yield acceptable solutions. Thus, relying solely on point-wise estimation fails to adequately represent the distribution $P_{B|A}$ over the output space, as well as estimate the uncertainty associated with the reconstruction process. The probabilistic remedy for this is to relax the constraint on the residual by modeling it as a distribution instead of a point estimate, the optimal parameters of which are learned from the data, thus allowing an estimation of the uncertainty. As an example, consider a deep learning model $F(D; \\theta)$ parametrized by $\\theta$ to be trained for translating images from domain A \u2192 B. While one conceivable distribution for $d$ might be an isotropic standard Gaussian, presuming a fixed variance not only imposes an assumption of independence and identical distribution (i.i.d.) on the residuals, which can be easily compromised by slightly out-of-distribution samples [17, 29], but also eliminates the ability to model heteroscedasticity in predictions. Alternatively, the distribution over $d$ can be heteroscedastic Gaussian [29] with zero mean and spatially varying-learnable standard deviation $\\sigma_{jk}$ as in Eq. 1,\n$x_{jk} = \\hat{x}_{jk}+d_{jk}, d_{jk} \\sim \\mathcal{N}(0,\\sigma^2_{jk}); p_{jk} \\sim \\mathcal{N}(x_{jk}, \\sigma^2_{jk})$ (1)\nThe parameters of the network $F(D;\\theta)$ can be optimized by maximizing the likelihood given by:\n$\\mathcal{L}(D; \\theta) := \\prod_{i=1}^{n}P_{B|A}(x'_i; \\{x_i, \\sigma_i\\})$\n$\\theta^* := \\underset{\\theta}{\\operatorname{argmax}} \\mathcal{L}(D; \\theta)$\n$= \\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^{n}P_{B|A}(x'_i; \\{x_i, \\sigma_i\\})$\n$= \\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i}}} e^{-\\frac{(x'_i - x_i)^2}{2\\sigma^2_{i}}}$ (2)\nwhere we omit spatial indices jk for simplicity. The negative log likelihood is,\n$\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\sum_{i=1}^{n} \\{\\frac{(x'_i - x_i)^2}{2\\sigma^2_{i}} + \\frac{1}{2}log(\\sigma^2_{i})\\}.$ (3)\nAssuming that the residuals follow a normal distribution simplifies uncertainty estimation, as the per-pixel variance $\\sigma^2$ itself is the aleatoric uncertainty in prediction. This formulation for modeling aleatoric uncertainty can be improved by assuming a more lenient Generalized Normal Distribution (GND) with zero mean over the residuals [29, 28]. The parameters governing the shape ($\\beta$) and scale ($a$) of the predicted distribution not only accommodate the heteroscedastic variations in residuals, but also enable heavier-tails, which are beneficial for handling outliers.\n$d_{jk} \\sim GND(x; 0, a_{jk}, \\beta_{jk})$ (4)\nAs before, the likelihood can be written as:\n$\\mathcal{L}(D; \\theta) := \\prod_{i=1}^{n} P_{B|A}(x'_i; \\{x_i, a_i, \\beta_i\\})$\n$\\theta^* := \\underset{\\theta}{\\operatorname{argmax}} \\mathcal{L}(D; \\theta)$\n$\\theta^* = \\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^{n} \\frac{\\beta_i}{2a_i \\Gamma(\\frac{1}{\\beta_i})} e^{-(\\frac{|x'_i - x_i|}{a_i})^{\\beta_i}}$ (5)\nTherefore, the negative likelihood is,\n$\\theta^* = \\underset{\\theta}{\\operatorname{argmin}} \\sum_{i=1}^{n} \\{(\\frac{|x'_i - x_i|}{a_i})^{\\beta_i} + \\frac{1}{\\beta_i}log(\\frac{a_i}{\\beta_i}) + log \\Gamma(\\frac{1}{\\beta_i}) \\}$ (6)\nWe refer to this loss as the negative likelihood loss $L_{nll}$, in next sections. The aleatoric uncertainty for $x_i$ can be"}, {"title": "3.2. Uncertainty-Aware Regularization", "content": "We operate under the benign assumption that for good reconstructions pixel-residuals exhibit piece-wise continuity similar to images, implying that since adjacent pixels within one image region show minimal discrepancies their residuals should also be similar, unless influenced by noise. Therefore, large residuals can come from pixels of two types, the pixels that the network actually finds hard to reconstruct to due to lack of knowledge or data drifts, and, the spurious pixels that might not strictly correspond to difficulty in reconstruction but end up having high values. To illustrate this better, we simulate this effect by injecting a small amount of noise in an image (Fig. 3), such that the corruption is visually imperceptible in the image and predict the uncertainty. As expected, the aleatoric uncertainty map is adversely affected, even with comparable reconstructions. Although sensitivity to noise in input data is generally advantageous, excessive sensitivity within the anticipated noise spectrum can result in unreliable and inaccurate uncertainty predictions. We propose to suppress this spurious component for a more accurate estimation of uncertainty by penalizing large differences in the predicted residual distributions for neighboring pixels.\nThis prior assumption can be incorporated by adding a penalty/regularization term that discourages significant deviations between the predicted residual distributions of adjacent pixels, inline with the expectation that neighboring pixels in an image are likely to have similar residuals, unless there is noise or an edge. Further, while enforcing the above constraint, it is crucial to prevent accidentally suppressing those deviations that occur as a result of the network's incapacity/lack of knowledge to reconstruct an input. This is the interesting case when the network is unsure how to reconstruct the output for one or more regions. Thus, we propose to impose a total-variation based penalty on the estimated shape parameter $\\beta$ during the learning process, to smooth out noise in the estimated parameters across neighboring pixels while preserving regions of true uncertainty.\nFor a predicted $\\beta_i$ image corresponding to input $x_i$, the total variation is shown in Eq. 7.\n$TV(\\beta) = \\int_v |\\nabla \\beta(v)| dv$ (7)"}, {"title": "3.3. Training Details and Evaluation Metrics", "content": "We test UAR on two datasets, a new WCE dataset and a public colonoscopy CPC-paired dataset [19]. From the WCE dataset, 5,000 images were utilized for training, and 5,000 for validation. All results are reported on a test-set of another 5,000 image pairs, which is further divided into three subsets for comprehensive evaluation. The training and validation images are sourced from WCE videos of seven patients, while the test images are obtained from three new patients, potentially containing new or different abnormalities. The hyperparameters optimized on the WCE dataset were also effective for the CPC-paired dataset. Consequently, the CPC-paired dataset was split into training and testing subsets (80:20), with results reported on the test set. Both the discriminator and generator utilize the Adam optimizer with an initial learning rate of $10^{-4}$, following a cosine annealing schedule for learning rate adjustment. The outputs of the discriminator are passed through an average pooling layer before applying the MSE loss (equations 10 and 12). We found that results improved when the variation-based regularization (Eq. 8) was activated a little later in the initial learning phase, giving the network a chance to predict unregularized values for $a$ and $\\beta$. Thus, the total variation regularization is activated around epoch 5. Through experimentation, we found that a value of $10^{-12}$ for the regularization weight, $\\lambda$ in Eq. 11 yielded satisfactory results, with room for further optimization and performance improvements (more details in Section 5). Other weights in Eq.11 are $w_{L_1} = 1$, $w_{adv} = 10^{-3}$ and $w_{nll} = 10^{-4}$. All models were trained with an image size of 490 \u00d7 490 and a batch size of 4, using twin-titan RTX GPUs with 48 GB of RAM, achieving a processing speed of approximately 20 images per second. The UAR term can be integrated with minimal computational overhead, as it involves only element-wise operations on the grayscale maps of $\\beta$ resulting in execution speeds comparable to those of $L_1$ loss.\nTo assess the quality of the generated images, we report the results on four metrics, namely Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM) [31], Relative Root Mean Squared Error (RRMSE) and Learned Perceptual Image Patch Similarity (LPIPS) [34]. While, SSIM, PSNR and RRMSE are more common, we use LPIPS additionally as it has shown to correlate better with human visual perception [34] over pixel-wise metrics."}, {"title": "3.4. Dataset", "content": "In this work, we introduce a new paired image-to-image translation dataset for capsule endoscopy. The dataset fa-"}, {"title": "4. Results", "content": "This section presents the qualitative and quantitative evaluation of our method on the two datasets. The approach is tested on three types of commonly occurring noises: Gaussian, Uniform and Impulse (also called salt and pepper noise) at different levels. The baseline corresponds to I2I-translation without regularization as in [29].\nFig.4 shows the qualitative effect of increasing levels of noise on the predicted uncertainty and reconstruction. Comparing the reconstructed image, it is seen that the regularized variant results in a more visually coherent reconstruction even at high noise levels, as compared to the non-regularized method.\nFig.5 shows, the residual errors and the uncertainty maps derived from the two methods, under the impact of noise. As seen in columns 4 and 7 ($\\sigma^2$), UAR generates less noisy uncertainty maps, consistent with the distinctive features within the images, while reducing the residual errors (columns 3 and 6 ($||x - \\hat{x}||^2$)).\nGiven the consistent capture modality and similar patient population, the primary source of noise in the data is the added noise itself. If the predicted uncertainties accurately reflect this, they should be low for familiar image structures and higher in noise-affected regions. Uncertainty maps generated using UAR adhere to this expectation. Conversely, in the absence of regularization, the uncertainties are uniformly high, obscuring relative differences in uncertainty and hindering interpretability. Further, we analyze the quantitative impact of UAR on reconstruction quality in Table 1 and 2. It is seen that the effect of UAR is overall positive on image reconstruction with equivalent or better SSIM and PSNR values, across different noise types and levels. The regularization also consistently improves the LPIPS and RRMSE metrics, across both datasets."}, {"title": "4.1. Impact of Artifacts", "content": "Additionally, we evaluate the performance of uncertainty estimation by systematically introducing more pronounced artifacts into the image. Fig.6 illustrates images with circular artifacts. The UAR variant prominently displays high uncertainty across the entire artifact region, with comparatively lower uncertainties in other areas of the image. In contrast, the baseline method fails to differentiate the network's confidence between these two regions effectively.\nFig.7 replaces the circular artifact with a ring artifact to examine behaviors near the artifact boundaries. Here again, the baseline method significantly underestimates the uncertainty associated with the artifact, whereas UAR accurately delineates uncertainty regions with precise boundaries (notice last row in Fig.7)."}, {"title": "5. Ablation", "content": "Ablation I: Other variation-based losses. Given that the primary goal of the regularization term is to attenuate spurious variances between nearby pixels, other types of variation-based losses are also conceivable. We experiment with two other variations, and analyze their effect on the uncertainty estimation. We hypothesize that, at a minimum, imposing similar penalties should not negatively affect the reconstruction quality for more faithful uncertainty maps.\nOne such penalty could be simply to penalize the squared L2-norm of gradients of the $\\beta$ map. This modifies Eq.9 so that it is differentiable and avoids singularity. However, it comes at the cost of reduced invariance to sharp features, in other words it introduces slight smoothing in the uncertainty map. This variant is referred to as UARL2.\n$R_{\\beta_{l2}} = \\sum_{jk} ((\\beta_{ij+1k} - \\beta_{ijk})^2) + ((\\beta_{ijk+1} - \\beta_{ijk})^2)$ (13)\nNext, we test the regularization of the anisotropic variant of total variation. This is the L1-norm on the gradients of $\\beta_i"}, {"title": "6. Conclusion and Limitations", "content": "In this work, we presented an end-to-end model for I2I translation that integrates an uncertainty-aware regularization. UAR aims at ensuring that the model's confidence levels are clearly delineated and easily interpretable while improving the overall reconstruction quality, thereby facilitating better decision-making in safety-critical applications. Through systematic evaluation and ablation studies, we demonstrated that our approach maintains high fidelity in familiar regions while accurately identifying and quantifying uncertainty in novel situations. This paper employs a basic conditional GAN for I2I translation, but more advanced architectures and improved reconstruction losses could enhance translation quality. Since UAR is model-agnostic, it can be seamlessly integrated with these improvements. Additionally, we plan to involve more clinical experts to assess the quality of uncertainty maps, complementing the current qualitative and quantitative evaluations in the future."}]}