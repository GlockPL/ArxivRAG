{"title": "SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models", "authors": ["Yung-Sung Chuang", "Benjamin Cohen-Wang", "Shannon Zejiang Shen", "Zhaofeng Wu", "Hu Xu", "Xi Victoria Lin", "James Glass", "Shang-Wen Li", "Wen-tau Yih"], "abstract": "We introduce SelfCite, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.", "sections": [{"title": "Introduction", "content": "Assistants built using large language models (LLMs) have become ubiquitous in helping users gather information and acquire knowledge (OpenAI, 2022, 2023). For instance, when asked about recent news, an assistant can read through dozens of relevant articles\u2014potentially more than a user could comb through themselves and use these articles as context to provide a clear, specific answer to the user's query. While this ability can greatly accelerate information gathering, LLMs often produce hallucinations content that sounds plausible but is actually fabricated (Ji et al., 2023). Even when provided with accurate context, models may misinterpret the data or include details that are not supported by the context (Shi et al., 2024; Chuang et al., 2024).\nAlthough completely eliminating hallucinations remains difficult, existing approaches have sought to enhance the reliability of LLMs by providing context attributions-commonly referred to as citations-which are fine-grained references to relevant evidences from the context, alongside generated responses for user verification (Menick et al., 2022; Slobodkin et al., 2024; Zhang et al., 2024). While they have shown promise in generating citations, an outstanding challenge is their reliance on annotated data either from human (Menick et al., 2022; Slobodkin et al., 2024) or costly proprietary APIs (Zhang et al., 2024; Huang et al., 2024) to train models to generate citations. Collecting annotations can be time-consuming and costly, especially with long-context documents.\nTo address this challenge, we introduce SelfCite, a novel alignment approach designed to autonomously enhance the quality of citations generated by LLMs without the need for any annotations in the alignment process. Drawing inspiration from model interpretability techniques (Lei et al., 2016; Cohen-Wang et al., 2024), SelfCite leverages the inherent capabilities of LLMs to provide feedback through context ablation\u2014a process to evaluate the necessity and sufficiency of a citation. If removing the cited text prevents the LLM from assigning high probability to the same response, we can infer that it is necessary for the LLM. Conversely, if the response remains highly probable despite removing all context other than the cited text, this indicates that the citation is sufficient for the LLM to make the claim. This self-evaluation mechanism enables SelfCite to calculate a reward signal without relying on the annotation processes.\nBuilding on this intuition, we design a reward that can be cheaply computed by the LLM itself, composed by"}, {"title": "Method", "content": "In this section, we describe the SelfCite framework. We begin by introducing the task of generating responses with context attributions (2.1), referred to as citations for brevity. We then design a reward for providing feedback on citation quality without human annotations (2.2) as illustrated in Fig. 1. Finally, we discuss two approaches for utilizing this reward to improve citation quality: best-of-N sampling (2.3) and preference optimization (2.4)."}, {"title": "Problem Formulation", "content": "We first formalize the task of generating responses with context attributions and the metrics to self-evaluate context attributions within the SelfCite framework, inspired by previous papers (Zhang et al., 2024; Cohen-Wang et al., 2024) but adapted to our proposed self-supervised reward.\nSetup. Consider employing an autoregressive language model (LM) to generate a response to a specific query given a context of relevant information. Specifically, given an LM PLM, let PLM(ti | t1,..., ti\u22121) denote its output distribution over the next token ti based on a sequence of preceding tokens t1,..., ti\u22121. Next, let C represent the context of relevant information. This context is partitioned into |C| sentences: C1, C2,..., C|C|. Each sentence cj is prepended with a unique identifier (e.g. sentence index j) as a way for the model to reference the sentence when generating citations. The context C is followed by a query Q, a question or instruction for the model. A response R is then sampled from the LM PLM.\nGenerating Responses with Context Attributions. In SelfCite, following prior work on generating responses with context attributions (Zhang et al., 2024), each statement ri in the response R is followed by a citation sequence ei consisting of the identifiers of sentences from the context C. Thus, the entire response sequence R is {r1,e1,r2,e2,...,rs, es}, where S is the total number of generated statements. The citation ei is intended to reference sentences that support the generation of ri. Formally, for each response statement ri, the model outputs a citation sequence ei = {e1^i,e2^i,...,em^i}, where each e \u2208 {1,2,...,|C|} corresponds to a specific"}, {"title": "Self-Supervised Reward via Context Ablation", "content": "We measure the quality of a citation sequence ei by the changes in the LM's probability of generating ri when the cited sentences are either removed from or isolated within the context. To simplify the notation, let all the cited context sentences be Ei = {Ce_1, Ce_2,..., Ce_m}. We define two key metrics: necessity score and sufficiency score, and finally combine them into the final reward, as shown in Fig. 1.\nNecessity Score: Probability Drop. This metric quantifies the decrease in the probability of generating ri when the cited sentences in Ei are all removed from the context (denoted as set minus \\ operator). Formally, it is defined as:\nProb-Drop(ei) = log PLM (ri | C) - log PLM (ri | C \\ Ei).\nTo keep the equation concise, we ignore Q and {r1, e1, ..., ri\u22121, ei\u22121} in the equation, but they are staying in the context history when computing the probabilities. A larger probability drop indicates that the removal of Ei significantly diminishes the likelihood of generating ri, thereby validating the necessity of the cited evidence.\nSufficiency Score: Probability Hold. Conversely, this metric measures if the probability of generating ri is still kept large when only the cited sentences are kept in the context, effectively testing the sufficiency of the citation to support the response statement. Formally:\nProb-Hold(ei) = log PLM (ri | Ei) - log PLM(ri | C).\nA more positive value of probability hold indicates that the cited sentences alone are sufficient to support the generation of ri, while removing all the other irrelevant context. Please note that the values of probability drop or hold can be either positive or negative. For example, if the citation is not relevant to ri or even distracting, it is possible for p(ri | Ei) to be lower than p(ri | C).\nFinal Reward. To comprehensively evaluate the necessity and sufficiency of the generated citations, we add the two metrics together, where the opposing terms cancel out:\nReward(ei) = log PLM (ri | Ei) \u2013 log PLM (ri|C \\ Ei). (1)\nThe combined reward measures if the citations are both necessary and sufficient for generating the response ri."}, {"title": "Best-of-N Sampling", "content": "To leverage the self-supervised reward computed via context ablation, we employ a best-of-N sampling strategy, which is a common way to test the effectiveness of a reward design (Gao et al., 2023a; Lightman et al., 2024) as a performance oracle without any confounders from training. After generating the full response, we locate the position where the citation tags <cite>...</cite> are generated. Within the citation tags, we sample N candidate citation sequences and select the citation set that maximizes the combined reward metric, Eq. (1)."}, {"title": "Preference Optimization", "content": "Best-of-N sampling is a straightforward way to obtain better citations, but at the additional inference time of generating candidates and reranking. Thus, we try to internalize the ability of generating better citations back to the LM itself.\nGiven documents and queries, we can prompt the LM to generate the responses along with the citations R = {r1, e1, ...,rs,es}. By further applying best-of-N sampling, we can obtain new responses of the same statements but with better citations R* = {r_1,e_1,..., r_s, e_s}. Such preference data can be used in direct preference optimization (DPO) (Rafailov et al., 2024) to align the model based on the preference between the original outputs and improved outputs. DPO typically requires more memory usage than SFT due to the need of a reference model. Also, optimizing with preference data pairs inherently makes the per-GPU batch size to be at least 2, limiting the maximum context length that can be used. To address these challenges, we use SimPO (Meng et al., 2024), a variant of DPO that does not require a reference model, freeing up memory for long-context fine-tuning. We further apply Liger-Kernel (Hsu et al., 2024), a collection of efficient Triton kernels, to optimize memory usage and scale the context length to 25.6K, as detailed in Appendix A. Through this self-supervised alignment process, which does not require ground-truth answers or human annotations, the model learns to generate more accurate and contextually grounded citations on its own."}, {"title": "Experiments", "content": "We evaluate the effectiveness of SelfCite by applying the best-of-N sampling and preference optimization methods to existing models that generate responses with citations."}, {"title": "Model Details", "content": "We use the Llama-3.1-8B model (Dubey et al., 2024) fine-tuned on LongCite-45K SFT data, namely the LongCite-8B model (Zhang et al., 2024) as the start point for both best-of-N sampling and preference optimization. We adopt the same text segmentation strategy from Zhang et al. (2024): each document is split into individual sentences using NLTK (Bird, 2006) and Chinese punctuations, and each sentence is prepended with a unique identifier in <C{i}> format. These identifiers serve as the citation indices, enabling the model to cite relevant context right after the statements with the format of <statement> {content ...} <cite>[11 - 12][13 - 14]...</cite></statement>. This format allows the model to cite a single sentence (e.g. i1 = i2) or a span (e.g. i\u2081 < i2) efficiently within several tokens. The responses are generated via top-p sampling (Holtzman et al., 2020) with p=0.7 and temperature=0.95. We set p=0.9 and temperature=1.2"}, {"title": "Preference Optimization", "content": "LongCite-45K. Best-of-N sampling (Section 2.3) requires no training, so no training data is used. For preference optimization with SimPO (Section 2.4), we use 2K document-question pairs from LongCite-45K (Zhang et al., 2024) as the training set but we do not use its ground-truth responses with high-quality citations for SFT. Instead, we generate model responses from the documents and queries, then apply best-of-N to refine citations. We label the original responses as rejected and replace their citations with BoN-refined ones to create the chosen responses, forming preference pairs to build the dataset for SimPO.\nData Construction and Length Balancing Since best-of-N responses tend to have slightly longer citations, directly fine-tuning on them can lead the model to adopt a shortcut-generating longer citations instead of improving citation quality. To prevent this, we introduce length balancing: if an original response has a shorter citation length than the best-of-N response, we insert random citations from nearby sentences. This encourages the model to focus on where to cite rather than simply citing more. Details are provided in Appendix C, with an ablation study in Section 4.2."}, {"title": "Evaluation", "content": "Benchmark. We evaluate our approach on LongBench-Cite (Zhang et al., 2024), a comprehensive benchmark specifically designed for long-context QA with citations (LQAC). Given a long context C and a query Q, the model must produce a multi-statement answer with each statement cites relevant supporting sentences in C. Unlike chunk-level citation schemes (Gao et al., 2023b) which cites short paragraphs, LongBench-Cite adopts sentence-level citations to ensure semantic integrity and finer-grained evidence tracking. LongBench-Cite assesses two main aspects:\nCitation Quality: Whether each statement is fully supported by relevant and only relevant sentences. GPT-40 measures citation recall (extent to which a statement is fully or partially supported by the cited text) and citation precision (whether each cited text truly supports the statement). These are combined into a citation F1 score. Additionally, we track average citation length (tokens per citation) to promote fine-grained citations over unnecessarily long passages.\nCorrectness: How accurately and comprehensively the response answers the query disregarding the citations. This is scored by GPT-40 in a zero-/few-shot fashion based on the query and reference answers.\nThe benchmark contains five datasets, including single-doc QA MultiFieldQA-en/zh (Bai et al., 2023), multi-doc QA HotpotQA (Yang et al., 2018) and DuReader (He et al., 2018), one summarization dataset GovReport (Huang et al., 2021), and LongBench-Chat (Bai et al., 2024) which covers diverse real-world queries with long contexts such as document QA, summarization, and coding.\nBaselines. SelfCite is compared with these baselines.\nPrompting: Zhang et al. (2024) propose the baseline of prompting LLMs with an one-shot example. This can be applied to proprietary models including GPT-40 (OpenAI, 2023), Claude-3-sonnet (Anthropic, 2024), and GLM-4 (GLM et al., 2024), as well as open-source models including GLM-4-9B-chat (GLM et al., 2024), Llama-3.1-{8,70}B-Instruct (Dubey et al., 2024), and Mistral-Large-Instruct (Mistral, 2024).\nContributive context attribution: Contributive context attribution seeks to directly identify the parts of the context that cause the model to generate a particular statement. We consider ContextCite (Cohen-Wang et al., 2024), a contributive context attribution method that performs several random context ablations to model the effect of ablating different parts of the context on a generated statement."}, {"title": "Main Results", "content": "Citation Quality. Table 1 presents our main results. Our best-of-N sampling (BoN) consistently improves both citation recall and citation precision across tasks, increasing the overall F1 score from 73.8 to 77.5. Using SimPO to internalize BoN's gains-eliminating the need for BoN sampling-achieves a similar improvement, with an F1 of 77.9. Applying BoN again to the SimPO fine-tuned model further boosts F1 by 5.3 points to 79.1, the highest across the datasets, suggesting room for further gains. Our results surpass LongCite-8B/9B at similar citation lengths and outperform proprietary model prompting while producing shorter citations. Compared to ContextCite, which also relies on context ablation, our approach is significantly better. A key reason is that ContextCite estimates sentence importance from scratch using linear regression, while we rerank existing LLM-generated citation candidates, leading to more efficient and accurate citation quality estimation. Additionally, we evaluate the latest released Claude Citations API, as shown in Appendix D that SelfCite achieves strong results very close to this commercial-level API that specialized for providing citations.\nFully Self-Supervised Setting. In our main experiment, we start from the Llama-3.1-8B model fine-tuned on the LongCite-45K SFT data, which effectively kick-starts its ability to generate structured citations for best-of-N sampling. The subsequent SimPO alignment stage is entirely self-supervised. We are also curious if it is possible to start from a fully self-supervised SFT model and then apply our self-supervised alignment after that. To begin with, we automatically generate 11K citation SFT data using ContextCite (see Appendix B for details) to replace the LongCite-45K annotations in the training data, as shown in the results at the bottom of Table 1. We can see that SFT on ContextCite can achieve decent initial results (65.7 F1) but still far from LongCite-8B (73.8 F1). BoN helps improving F1 to 67.3. After SimPO training, it achieves 69.9 F1,"}, {"title": "Analysis", "content": "4.1 Ablation Study on Rewards\nTo better understand our final reward design, we explore various reward strategies in the BoN sampling process. Here, all BoN candidates are pre-generated and fixed, the reward is the only factor affecting results. Table 3 presents our ablation results on HotpotQA, while citation lengths are computed across all LongBench-Cite datasets for direct comparison with Table 1. We evaluate four alternative reward designs. BoN by LM log prob re-ranks candidates simply by the probability of the citation string, <cite>[11 - 12][13 - 14]...</cite>, which is similar to the effects of beam search. We observe that this strategy slightly boosts recall while reducing precision, resulting in a minor reduction in F1. BoN by max citation length always selects the candidates with the longest citations, i.e. citing the greatest number of sentences. Although it improves recall, it significantly reduces precision from 77.9 to 73.6 and inflates the citation length from 83.5 to 139.8. By contrast, both BoN by Prob-Drop and BoN by Prob-Hold improve recall without sacrificing precision. Finally, by combining both Prob-Drop and Prob-Hold into our final SelfCite reward, we achieve the best outcome, increasing both recall and precision and a 4-point improvement in F1.\nWe also explored different token-length limits for citations in the bottom of Table 3, as discussed in Section 2.3. By default, we exclude candidates citing more than 384 tokens, unless the citation contains only a single sentence. Lowering the cap to 256 tokens slightly hurts F1, while raising it to 512 tokens has negligible impact. Completely removing length limits inflates citation length to 121.9 tokens and yields worse precision (79.3)"}, {"title": "Citation Length Balance", "content": "As noted in Section 3.2, BoN selects slightly longer citations, making it easy for a model trained directly on BoN-preferred data to adopt the shortcut of generating longer citations without improving quality. To counter this, we apply length balancing, injecting random citations into examples where length bias exists to equalize the number of cited sentences. Table 4 (see w/ vs. w/o length balancing) highlights its critical role in length balancing. Without length balancing, the model overextends citations (average length 152.9), leading to lower precision (62.9) and F1 (60.5). In contrast, enabling length balancing maintains high precision (82.3) and recall (69.4), achieving a better F1 of 71.5 while keeping citation length reasonable (105.7). These results confirm that length balancing prevents shortcut learning, ensuring the model truly learns to cite accurately."}, {"title": "Training Size of SimPO", "content": "In prior study (Zhou et al., 2023), 1K examples are sufficient to align user preferences effectively. Table 4 presents SimPO results with 1K to 8K examples. 1K examples already bring a moderate improvement, raising F1 from 64.1 to 65.7, with gains in precision and recall. Using 2K examples further boosts F1 to 71.5, while"}, {"title": "SimPO vs. SFT on Best-of-N responses", "content": "We also show the effect of applying standard supervised fine-tuning (SFT) on the responses selected by best-of-N sampling, which is a simplified alternative of preference optimization. As the result shown in the last row in Table 4, SFT also improves the F1 score from 64.1 to 68.4, but it still falls behind 71.5 of SimPO. This result confirms that it is necessary to train the model via SimPO with preference data, which enables the model to distinguish between bad and good citations, and thus improve the citation quality."}, {"title": "Off-policy Denoising Perturbed Citations", "content": "We explored a purely off-policy alternative approach. Specifically, given a model-generated response, we randomly shift its citation spans to create perturbed variants. SimPO training pairs were then constructed by preferring the original citation over the perturbed one, encouraging the model to \"denoise\" citations by restoring their original spans. However, as shown at the bottom of Table 4, this approach degrades performance, both when applied to original and best-of-N responses. We attribute this to a mismatch between the training data and the model's natural error distribution since random shifts do not reflect typical citation errors, they fail to provide useful guidance for improvement."}, {"title": "Iterative Preference Optimization", "content": "It has been discussed that an on-policy alignment process can be beneficial to avoid reward exploitation (Bai et al., 2022) and maintains consistency between the generated data and the model's evolving output distribution. We thus experiment with iteratively performing SimPO, similar to the concepts of recent studies (Pang et al., 2024; Yasunaga et al., 2024), to maintain the consistency between the generated data and the model's evolving output distribution. Specifically, after fine-tuning with SimPO, we generate a new dataset via BoN, which is also 2K in size but not overlapped with previous iterations. We continue training the model and repeat the process for three rounds. As shown in Figure 2, while the largest improvement occurs in the first round, improvements continue over three iterations, which further validates the reliability of our reward signal. Iterative SimPO is still not perfect since it remains an off-policy method. Given that our reward can be cheaply computed, we believe that on-policy methods like PPO (Schulman et al., 2017) could further enhance performance. We leave the exploration of such approaches for future work."}, {"title": "Qualitative Study", "content": "Finally, we examine an example that requires citing multiple context sentences to support a complex response. As shown in Table 5, the response integrates information from sentences 302, 303, and 306. Direct sampling (2)"}, {"title": "Related Work", "content": "Citations for Language Models. Recent work has explored various approaches to teaching language models to generate citations, including fine-tuning with direct human feedback or annotations (Nakano et al., 2021; Menick et al., 2022; Slobodkin et al., 2024), annotation/feedback from external models (Huang et al., 2024), and prompting-based methods (Gao et al., 2022, 2023b) to explicitly incorporate relevant retrieved documents. To avoid the human annotation processes, Zhang et al. (2024) introduced CoF (\"Coarse to Fine\"), an automated multi-stage pipeline that simulates human annotations. This approach leverages proprietary LLMs for chunk-level retrieval and sentence-level citation extraction, achieving high citation quality through supervised fine-tuning. However, it depends on two powerful proprietary APIs-GLM-4 for the LLM and Zhipu Embedding-v2 for retrieval\u2014with carefully designed prompting, effectively distilling the capabilities of these powerful proprietary APIs into much smaller models in 8B/9B. In contrast, our SelfCite aims at completely eliminating the reliance on annotations for citation, either from human or proprietary APIs. Instead, our method enables a small 8B model to assess citation quality itself using self-supervised reward signal from context ablation, effectively self-improving without external supervision.\nContributive Context Attribution. Besides being self-supervised, SelfCite also adopts the view that citations should reference the sources from the context that a model actually uses when generating a statement-known as contributive attribution (Worledge et al., 2023)-rather than any sources that merely support the claim. Our reward signal naturally aligns with this attribution framework, as context ablation identifies the sources that"}, {"title": "Conclusion and Limitations", "content": "In this work, we introduced SelfCite, a self-supervised framework that aligns LLMs to generate more accurate, fine-grained citations by directly leveraging their own probabilities for necessity and sufficiency rewards through context ablation. With best-of-N sampling and preference optimization, SelfCite significantly improves citation correctness on the LongBench-Cite benchmark without requiring human annotation, offering a promising self-improving direction towards verifiable and trustworthy LLMs.\nSelfCite also has limitations: 1) While achieving good results with SimPO, integrating other alignment algorithms remains unexplored. 2) While we focus on self-supervision in preference optimization, our attempt to make the SFT stage self-supervised via ContextCite is still preliminary. Better unsupervised ways to kick-start LLMs' ability in generating structured citations can be further explored."}, {"title": "Implementation Details", "content": "For SimPO fine-tuning, we randomly sample 2K document and question pairs from the LongCite-45k data, without using any ground-truth responses and citations. We generate the our own best-of-N responses with our Algorithm 1 to obtain the preference data, and train for one epoch. We sample another 100 examples as development set to pick the best learning rate from {1e-7, 3e-7, 5e-7, 7e-7}. We keep other hyperparameters the same as the original SimPO (Meng et al., 2024). We follow the same prompt format used in Zhang et al. (2024)3 to keep the comparison fair. For the iterative SimPO experiment, in each iteration, we sampled a new, non-overlapping subset of 2K examples to ensure no data repetition across iterations. For self-supervised SFT, we generate 11K citation data unsupervisedly from ContextCite outputs as described in Appendix B, trained with a larger learning rate 7e-6.\nWe use the SimPO source code 4 built from Huggingface Transformers (Wolf et al., 2020) for the finetuning experiments, as well as Liger-Kernel (Hsu et al., 2024)5 to enable memory efficient training for long-context examples in LongCite-45K without tensor parallelization. We run all the finetuning experiments on with 8\u00d7A100 GPUs of 80 GB memory on a single node. The batch size is set to 1 per GPU due to the long context examples. We set our max context length to 25600 to prevent OOM. For the data examples longer than 25600, we perform truncation, starting from removing the sentences that are the most far away from the sentences cited by the ground truth annotation, so as to keep the impact of truncation to be minimum.\nWhen evaluating the citation length, as well as calculating the token length limit of 384 for excluding long BoN candidates, we follow Zhang et al. (2024) to use GLM4-9B's tokenizer to count tokens.\nIn Section 4.5, the denoising citation examples are done by randomly shifting existing citation spans by 3-10 positions in sentence indices."}, {"title": "Obtaining Citations from ContextCite", "content": "In this section, we first describe how the ContextCite method (Cohen-Wang et al., 2024) estimates continuous attribution scores for each sentence in the context. We then explain a simple heuristic for extracting citations (i.e., selecting a subset of context sources) from these scores."}, {"title": "ContextCite", "content": "Given a language model PLM, a context C, a query Q and a generated response R, ContextCite aims to quantify how each source in the context C = {C1, C2, ..., C|C|} contributes to the generated response R (in our case, the sources are sentences). To do so, ContextCite performs several random context ablations. We begin by introducing some notation to describe these ablations. Let v \u2208 {0,1}|| be an ablation vector whose i-th entry toggles whether source ci is included (vi = 1) or excluded (vi = 0). We write ABLATE(C, v) to denote a modified version of the original context C in which sources for which v\u2081 = 0 are omitted. Context Cite seeks to understand how the probability of generating the original generated response,\nf(v) := PLM(R | ABLATE(C, v), Q),\nchanges as a function of the ablation vector v.\nAttribution via Surrogate Modeling. Directly measuring f(v) for all 2|C| ablation vectors is infeasible for large C. Hence, ContextCite seeks to identify a surrogate model f(v) that is easy to understand and approximates"}, {"title": "Heuristic Citation Extraction", "content": "In our setting, we would like a discrete list of cited sentences for each generated statement, rather than a score for every sentence. We will now describe how to convert the attribution scores w into a discrete subset C'CC of citations. Lett be a threshold, p be a cumulative probability mass cutoff, and k be a maximum citation limit.\nThresholding and Merging.\n1. Filtering: Include only those sources ci whose attribution score \u0175i \u2265 t.\n2. Merging Adjacent Sources: If multiple consecutive sources in the original text each exceed t, merge them into a single \"span\" Sj. We assign this merged span the maximum score among its constituents:\nw(Sj) = max w_i.\nc_i E S_j\nHere, adjacency is defined by the original ordering in C. For instance, if c2 and c3 both pass the threshold and appear consecutively, we merge them into a single span Sj.\nSoftmax Normalization. Let {S;} be the set of spans (or single sources) that survived the threshold. We normalize their scores into a probability distribution:\nw'(S_j) = \\frac{exp(w(S_j))}{\\sum_i exp(w(S_i))},\nso that \u2211_j w'(S_j) = 1."}, {"title": "Length Balancing", "content": "To prevent the model from simply generating longer citations rather than focusing on citation correctness, we apply a length balancing procedure to align the total citation length in our two training responses: a chosen prediction and a reject prediction. First, we find the citation string (e.g., [435-437]) enclosed in <cite>...</cite> tags for each statement. We then measure each string's total citation \"coverage\", which means the total number of cited sentences in these intervals.\nIf a reject prediction has a total coverage lower than the corresponding chosen prediction, we insert additional citations around nearby sentence indices to match the chosen coverage. Conversely, if the reject coverage is larger, we randomly remove some of its intervals. We ensure new or inserted citations do not overlap existing intervals and keep them within a small window of 5-10 sentences away from the original citations to maintain realism. Finally, the reject and chosen will have matched coverage. This approach discourages the model from trivially learning to cite more sentences, instead prompting it to learn where and how to cite evidence more accurately. Our ablation in Section 4.2 shows that this length balancing technique significantly improves final citation quality."}, {"title": "Comparison with Claude Citations API", "content": "On January 23rd, 2025, Claude announced an API specialized for providing citations along with responses: Claude Citations. We also try to evaluate this API on the LongBench-Cite benchmark. Since the implementation details and resource requirements (e.g., training data) of Claude Citations are not publicly available yet, and it relies on a significantly larger and more powerful LLM, Claude-3.5-Sonnet, which potentially has over 100 billions of parameters, we consider it as a topline of the benchmark rather than a baseline.\nWhen evaluating it on Chinese examples from LongBench-Cite, we found that the API does not split Chinese text properly. As a result, it cites large passages when processing Chinese examples, leading to an average citation length of approximately 800 tokens per citation.\nTo address this issue, we pre-segment the text ourselves using exactly the same method as our approach following LongCite (Zhang et al., 2024), which uses NLTK and Chinese punctuation segmentation. We then run the Claude Citations API, as it supports both non-segmented and pre-segmented document inputs. The evaluation was conducted using the latest version of claude-3-5-sonnet-20241022.\nAs shown in Table 6, Claude Citations achieves an overall F1 score of 81.3, which is higher than all other models we have tested. However, the performance of Claude Citations is not consistent over all datasets. For example, it is worse than SelfCite on LongBench-Chat and GovReport. The main improvement of Claude is from the DuReader dataset, while the results on other datasets are comparable to the results of SelfCite. Given the fact that SelfCite leverages a much smaller 8B model compared to the Claude-3.5-Sonnet model, the result of SelfCite is very impressive, demonstrating its potential to serve as a strong alternative to proprietary solutions."}]}