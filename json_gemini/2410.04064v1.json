{"title": "Text2Chart31: Instruction Tuning for Chart Generation with Automatic Feedback", "authors": ["Fatemeh Pesaran zadeh", "Juyeon Kim", "Jin-Hwa Kim", "Gunhee Kim"], "abstract": "Large language models (LLMs) have demonstrated strong capabilities across various language tasks, notably through instruction-tuning methods. However, LLMs face challenges in visualizing complex, real-world data through charts and plots. Firstly, existing datasets rarely cover a full range of chart types, such as 3D, volumetric, and gridded charts. Secondly, supervised fine-tuning methods do not fully leverage the intricate relationships within rich datasets, including text, code, and figures. To address these challenges, we propose a hierarchical pipeline and a new dataset for chart generation. Our dataset, Text2Chart31, includes 31 unique plot types referring to the Matplotlib library, with 11.1K tuples of descriptions, code, data tables, and plots. Moreover, we introduce a reinforcement learning-based instruction tuning technique for chart generation tasks without requiring human feedback. Our experiments show that this approach significantly enhances the model performance, enabling smaller models to outperform larger open-source models and be comparable to state-of-the-art proprietary models in data visualization tasks. We make the code and dataset available at https://github.com/fatemehpesaran310/Text2Chart31.", "sections": [{"title": "1 Introduction", "content": "Recently, a range of NLP tasks has been addressed by leveraging the remarkable ability of Large Language Models (LLMs). This advancement has been possible largely through the process of instruction-tuning (Ouyang et al., 2022; Yoo et al., 2024), which fine-tunes LLMs to rely on intuitive natural language instructions and skillfully solve intricate tasks, encompassing fields like question answering (Sanh et al., 2022; Liu and Low, 2023), summarizing (Goyal et al., 2023; Fetahu et al., 2023), and sentiment analysis (Varia et al., 2023). However, available LLMs continue to suffer from the difficult tasks of visualizing complex, fact-based, real-world data through charts and plots, mainly because of two challenges.\nFirstly, the current datasets (Methani et al., 2020; Masry et al., 2022; Kahou et al., 2018; Zhu et al., 2021; Kantharaj et al., 2022; Han et al., 2023) primarily focus on QA in the chart domain rather than chart generation, and they rarely cover a full range of chart types and their varied applications. Several chart forms like 3D, volumetric, gridded, and irregularly gridded remain largely unexplored or insufficiently studied. These forms are important for evaluating the capabilities of LLMs in understanding multidimensional data, spatial data, and vector field data. Developing such instructional datasets typically entails significant expenses due to the complex nature of text-to-chart processes, incorporating various data components such as text, code, and data tables. This complexity, along with the lack of specific online sources containing these plot types, makes their collection difficult and time-consuming. It necessitates human expert intervention to ensure quality, which drives up costs.\nSecondly, existing instruction-tuning methods based on supervised fine-tuning do not fully utilize the potential of rich datasets; for example, chart data include multiple components like text descriptions, code, and figures. Supervised fine-tuning struggles to effectively extract and leverage all the intricate information and relationships within these components, leading to suboptimal performance.\nTo address the first challenge, we propose a novel hierarchical pipeline for chart generation by leveraging the advanced linguistic skills of GPT-3.5-turbo (Ouyang et al., 2022) and code generation and data analysis capabilities of GPT-4-0613 (OpenAI, 2023). We contribute a dataset encompassing 31 unique plot types from the Matplotlib library (Hunter, 2007), featuring 11.1K tuples that combine descriptions, code, data tables, and plots, covering a wide range of use cases. Our pipeline is structured into the following steps: topic generation, description creation, code production, data table and reasoning step formulation, and cycle consistency verification. This approach reduces biases towards common topics or plot types, and ensures consistent and accurate generation of multiple data elements. By minimizing the human supervision in our proposed pipeline, we can generate a high-quality large-scale dataset that includes comprehensive descriptions, codes, data tables, reasoning steps, and illustrated graphs.\nWe further propose a novel reinforcement learning-based instruction tuning technique to address the second challenge. This method is tailored for chart generation tasks without costly human feedback. We propose two different reward functions: the preference reward and alignment reward. For the preference reward, we construct a preference dataset from the supervised fine-tuned model's output and the ground truth code. For the alignment reward, we optimize the model to increase the similarity between ground truth description and regenerated description from the code, exploiting the cycle consistency between code and description. We jointly optimize two sequential policy models using the PPO (Schulman et al., 2017).\nFinally, we make the following contributions:\n\u2022 We develop a novel dataset generation pipeline that populates data samples and filters out the low-quality ones, exploiting the cycle consistency in the task. This approach is scalable to increase the volume of data as needed.\n\u2022 We introduce the Text2Chart31 dataset, comprising 31 plot types with 11.1K tuples that combine descriptions, code, data tables, intermediate reasoning steps, and plots, covering a wide range of use cases.\n\u2022 We introduce an RL-based instruction tuning method that utilizes novel reward functions that leverage automated feedback and cycle consistency. The experiments demonstrate that our fine-tuned models outperform state-of-the-art open and closed-source models on data visualization tasks. To the best of our knowledge, this is the first work to adopt an RL-based instruction tuning approach for the chart generation task."}, {"title": "2 Text2Chart31 Dataset", "content": "Our newly contributed Text2Chart31 dataset supports 31 plot types based on Matplotlib with 11.1K data points. We outline its key characteristics in Table 1 comparing with existing datasets in the data visualization domain. The Text2Chart31 dataset D consists of 11,128 data points, each of which contains a tuple of $(x, c, d, r, y)$: a textual plot description (x), its corresponding code (c), the resulting plots (y). For 8,166 data points, we additionally include a raw data table (d) and intermediate reasoning steps (r) to generate descriptions.\nFor the dataset, we develop a hierarchical plot generation pipeline leveraging GPT-3.5-turbo and GPT-4. Despite their impressive capabilities for text and code generation, collecting high-quality data points is challenging for two primary reasons: (1) GPT-3.5-turbo exhibits bias towards particular topics or narrow plot types that are commonly represented in its training data, and (2) the text-to-chart data involves multiple data elements including descriptions, code, and data tables, making it difficult to generate accurate and consistent data points in a single step. Consequently, we claim that a hierarchical approach is essential for producing higher-quality chart-generation data points. This pipeline is illustrated in Figure 2."}, {"title": "2.1 Task Definition", "content": "Our benchmark is designed to evaluate three tasks. (1) Description-to-Chart: Given a plot description x, an algorithm generates its corresponding code c that creates a chart by the Matplotlib library\u00b9(Hunter, 2007). (2) Raw Data-to-Chart: When provided with only a raw data table d, the algorithm generates intermediate reasoning steps r that analyze the raw data and then generates a description d for the most suitable plot type based on the characteristics of the data. (3) Code-to-Description: Given the code c for a plot, the model generates a detailed description x of the plot."}, {"title": "2.2 Dataset Construction Pipeline", "content": "Our pipeline initiates by generating a topic from which a description x is derived. To ensure both diversity of the topic and alignment with the intended plot type, each topic is filtered before proceeding to the next step. We additionally generate code c, raw data table d and intermediate reasoning step r corresponding to the description. Lastly, we use the cycle-consistency verification to ensure the high quality of the data points. Please refer to Appendix C for the detailed process with examples.\nTopic generation. We generate distinct topic pools for five different plot categories: pairwise, statistical, gridded, irregularly gridded, and 3D/volumetric data. To maintain diversity within each topic pool, we include only topics with low similarity scores compared to those already being presented. To assess similarity, the ROUGE-L metric (Lin, 2004) is employed as a common practice from previous studies (Wang et al., 2023b).\nDescription generation. For each plot type, we start by manually writing 5 to 10 descriptions as seed points that contain all the necessary information for a plot to be illustrated. To generate a description (x), we randomly sample two descriptions and pair them with a topic from the topic pool. This assembled data is prompted into GPT-3.5-turbo, which generates a similar format plot description for the sampled topic. We remove the topic from the pool after a new description is generated to uphold the diversity. Inspired by the studies on the reasoning capabilities of LLMs (Wei et al., 2023; Kojima et al., 2023; Wang et al., 2023a), we instruct GPT-4 to self-evaluate the generated descriptions for quality control. This step is crucial to exclude any incompatible instructions that can lead to the creation of unsuitable plots, thereby avoiding computational waste.\nCode generation. We input descriptions into GPT-4, which is instructed to generate Python code for the Matplotlib library. This code aims to visualize the described plot. We add the generated code (c) to the dataset only if it successfully generates the corresponding plot (y) without a runtime error.\nData table and reasoning step generation. For plots derived from data files in D, GPT-4 is prompted to generate either a raw data table d or Python code that can generate the data table. 3D volumetric, gridded, and irregularly gridded plots often require specific patterns or mathematical relations between variables; therefore, code is created and executed to generate the data table instead of directly generating it. We further generate intermediate reasoning steps r using GPT-4, which is instructed to analyze the characteristics of the data and CSV file, explore possible plot types, determine the most suitable plot type, and consider additional aspects of the description. This process results in data points (x, c, d, r, y).\nCycle-Consistency verification. We argue that given the complex and fact-based nature of text-to-chart datasets, employing human evaluation to check the quality of generated data points is inefficient. To this end, we propose an AI-assisted method using cycle consistency, to assure the quality of the data point. This process involves regenerating an instruction that describes the plot from the generated code and comparing it against the original one. We keep the data only if the regenerated description closely aligns with the original one based on pre-defined criteria, indicating the high quality of the data. We provide further details on the cycle consistency method in Appendix D."}, {"title": "2.3 Analysis of Text2Chart31 Dataset", "content": "As shown in Table 1, we can effectively balance the data points per plot type with equal distribution in the dataset, which is quantified by the Shanon Diversity metric (Friedman and Dieng, 2023). Shannon Diversity is computed through $H = -\\sum_{i=1}^{S} P_i \\log(p_i)$, where S is the total number of classes in the dataset, and $p_i$ is the proportion of instances belonging to the i-th class. Our Text2Chart31 dataset achieve the highest score of 0.981. Figure 6 in Appendix shows a detailed comparison of the distribution per chart type between datasets using pie charts. We further evaluate the content diversity of datasets via Distinct-n score (Li et al., 2016). Our dataset achieves a score of 0.674, indicating that our pipeline effectively reassures the diversity of topics."}, {"title": "3 Instruction Tuning Approach", "content": "We discuss our proposed instruction tuning methods for fine-tuning LLMs to tackle the three data visualization tasks: (1) Description-to-Chart, (2) Raw-Data-to-Chart, and (3) Code-to-Description, using the Text2Chart31 dataset. We respectively denote three specialized models for the three tasks: $\\pi_{\\theta_1}$, $\\pi_{\\theta_2}$, and $\\pi_{\\theta_3}$. We train these models with two phases: supervised fine-tuning (SFT), followed by reinforcement learning (RL) with two types of reward that are specifically tailored to improve chart generation performance. Initially, all three tasks undergo supervised fine-tuning. Afterward, using PPO algorithm (Schulman et al., 2017), we jointly optimize $\\pi_{\\theta_1}$ with the preference reward and $\\pi_{\\theta_3}$ with the alignment reward that ensures cycle consistency and coherence of outputs. Algorithm 1 summarizes the overall procedure."}, {"title": "3.1 Supervised Fine-tuning", "content": "We perform supervised fine-tuning of $\\pi_{\\theta_1}$, $\\pi_{\\theta_2}$, and $\\pi_{\\theta_3}$ using the cross-entropy loss with the Text2Chart31 dataset. For Task 1, the model $\\pi_{\\theta_1}$ maximizes the probability of outputting the ground truth code for a given description by minimizing cross-entropy loss in the Line 3 of Algorithm 1. For Task 2, we design the model $\\pi_{\\theta_2}$ to generate descriptions from raw data in two stages. First, the model generates a reasoning step r from the raw data d, which involves analyzing data characteristics and determining the appropriate plot type. Then, the model is fine-tuned to generate the description x using the data and the reasoning step as in the Line 4. Lastly, we fine-tune the model $\\pi_{\\theta_3}$ for Task 3 to maximize the probability of predicting the ground truth description for a given visualization code as in the Line 5 of Algorithm 1."}, {"title": "3.2 RL via Automatic Feedback", "content": "We design two reward functions, which are the preference reward and the alignment reward, specifically tailored for the chart generation task. It is worth noting that we remove human supervision during these processes and solely rely on automatic feedback.\nPreference reward. We propose an automatic way of designing a preference dataset based on the output of the supervised fine-tuned model $\\pi_{\\theta_1}$. We define preference dataset $D_{pref} = \\{(c^+, c^-)\\}_{i=1}^I$, where a preferred code $c^+$ is the ground truth code, while a less preferred one $c^-$ is a corresponding code output of SFT. Afterward, we train a preference reward model $R_{\\phi}(c)$ following Ouyang et al. (2022) and employ this reward model to train $\\pi_{\\theta_1}$ via proximal policy optimization (PPO) algorithm (Schulman et al., 2017) as follows:\n$\\underset{\\pi_{\\theta_1}}{\\text{maximize }} \\mathbb{E}_{x \\sim D, \\hat{c} \\sim \\pi_{\\theta_1}(\\cdot|x)}\\left[R_{\\phi}(\\hat{c}) - \\beta D_{KL}(\\pi_{\\theta_1} || \\pi_{\\theta_{SFT_1}})\\right].$\nAlignment reward. The alignment reward leverages cycle consistency between a chart's description and code. First, $\\pi_{\\theta_1}$ generates a code from the original description, then $\\pi_{\\theta_3}$ uses this code to produce a regenerated description. The alignment reward is defined as the similarity between the original and regenerated descriptions, measured by BertScore (Zhang et al., 2020; Black et al., 2024). We optimize $\\pi_{\\theta_3}$ via maximizing the alignment reward $R(x, \\hat{x})$ using PPO algorithm as follows:\n$\\underset{\\pi_{\\theta_3}}{\\text{maximize }} \\mathbb{E}_{x \\sim D, \\hat{c} \\sim \\pi_{\\theta_1}(\\cdot|x), \\hat{x} \\sim \\pi_{\\theta_3}(\\cdot|\\hat{c})}\\left[R(x, \\hat{x}) - \\beta D_{KL}(\\pi_{\\theta_3} || \\pi_{\\theta_{SFT_3}})\\right].$"}, {"title": "4 Experiments", "content": "Baselines. For the evaluation of the three target tasks, we compare with the state-of-the-art open-source baseline models as follows: (i) Description-to-Chart: Code Llama Instruct (Rozi\u00e8re et al., 2024), Llama 3 Instruct (Meta AI, 2024), StarCoder (Li et al., 2023), and Instruct CodeGen (Nijkamp et al., 2023), (ii) Raw Data-to-Description: Llama 2 Chat (Touvron et al., 2023) and Llama 3 Instruct model, and (iii) Code-to-Description: Code Llama, Llama 2 Chat, and Llama 3 Instruct models. We also compare with proprietary models including GPT-3.5-turbo (Ouyang et al., 2022), GPT-4-0613, GPT-4-turbo-2024-04-09 (OpenAI, 2023), GPT-4o-2024-05-13 (OpenAI, 2024), and Claude 3 Opus (Anthropic, 2024).\nEvaluation metrics. For the three target tasks, we report the following evaluation measures.\n(i) Description-to-Chart: We report the total error ratio and plot-type error ratio. The total error ratio indicates the percentage of code executions that result in errors. We categorize and report plot-type errors based on Matplotlib classifications. We further evaluate the similarity between the predicted code and the ground truth (GT) code by reporting the METEOR (Banerjee and Lavie, 2005) and CodeBLEU metrics (Ren et al., 2020).\n(ii) Raw Data-to-Description: We report the Jaccard similarity and the Hit Rate. The former measures the intersection ratio between the recommended plot list derived from generated reasoning steps and the GT reasoning steps. The latter is the percentage of recommended lists containing the GT plot type. To evaluate the quality of the generated descriptions, we first use these descriptions to generate code with both the SFT Llama3 Instruct-8B model and the GPT-3.5-turbo, and then calculate the error ratio for the generated codes. Additionally, we report ROUGE-L and BertScore metrics to assess the similarity between the generated descriptions and the GT descriptions.\n(iii) Code-to-Description: We measure ROUGE-1/2/L and BertScore to evaluate the similarity between the generated descriptions and the GTs. Lastly, as done for Task 2, we generate the code by giving the predicted descriptions to the GPT-3.5-turbo and report the error ratio.\nTraining setup. We begin the supervised fine-tuning using LoRA fine-tuning (Hu et al., 2021). When we further fine-tune the model with RL, we merge the original SFT LORA parameters into the base model and fine-tune separate LoRA parameters. For SFT, we utilize a total of 11.1K data points for Task 1, 3, and 7.84K for Task 2. On the other hand, RL fine-tuning is conducted using 0.5K randomly selected data points, representing 4.8% of our $D_{pref}$ dataset. For SFT, we use 2 RTX A6000 GPUs and the training requires 6 to 12 hours, depending on the tasks. For RL, we use 6 RTX A6000 GPUs and the training takes less than 12 hours. Further details of the experiments can be found in Appendix B."}, {"title": "4.1 Results of Description-to-Chart", "content": "Table 2 presents the results for the Description-to-Chart task. We fine-tune Llama 3 Instruct-8B and Code Llama Instruct-13B on our Text2Chart31 dataset for five epochs. We run RL fine-tuning on the Llama 3 Instruct and Code Llama Instruct-13B using preference reward, denoted as $RL_{pref}$. The results show that our fine-tuned models outperform all open-source baselines that we compared. Specifically, the 13B model with SFT and RL achieves even a lower total error ratio than the state-of-the-art closed-source models like GPT-3.5-turbo, GPT-4, GPT-4-turbo, GPT-4o, and Claude 3 Opus. The RL fine-tuning reduces the total error ratio of the Llama 3 Instruct-8B model from 16.09 to 14.55, making it superior to the Claude 3 Opus. Particularly, our models excel in generating underexplored plot types such as gridded, irregularly gridded, and 3D and volumetric plots, compared to open-source models.\nHuman evaluation. We additionally conduct human evaluation to check the correctness of the generated plot and its alignment with the description."}, {"title": "4.2 Results of Raw Data-to-Chart", "content": "Table 3 presents the results of the Raw Data-to-Chart task. We fine-tune Llama 2 Chat-7B and Llama 3 Instruct-8B using our Text2Chart31 dataset. We report the error ratio after visualizing the generated descriptions using our supervised fine-tuned Llama 3 Instruct-8B (w/ SFT) from task 1 and GPT-3.5-turbo (w/ GPT). Notably, our fine-tuned Llama 3 Instruct-8B outperforms all open-source models across all metrics. Furthermore, this model surpasses closed-source models (GPT-3.5-turbo, GPT-4-turbo) in terms of error ratio and generated description similarity."}, {"title": "4.3 Results of Code-to-Description", "content": "Table 4 presents the results on the Code-to-Description task. We fine-tune the Llama 3 Instruct-8B using our dataset and evaluate the description similarity with ROUGE and BertScore. Our fine-tuned model outperforms all open-source and closed-source models across Description similarity. Furthermore, RL fine-tuning with alignment reward consistently increases the description similarity across all metrics. We also provide the generated descriptions to GPT-3.5-turbo and report the error ratio to highlight the quality of the descriptions produced by our fine-tuned models. After RL fine-tuning, the error ratio decreases from 21.36% to 20.31%, and the description similarity consistently improves."}, {"title": "5 Related Work", "content": "Chart datasets. There are several existing chart datasets, including PlotQA (Methani et al., 2020), ChartQA (Masry et al., 2022), FigureQA (Kahou et al., 2018), Unichart (Masry et al., 2023), Autochart (Zhu et al., 2021), Chart-to-Text (Kantharaj et al., 2022). These datasets primarily focus on question and answer (QA) tasks on a limited range of plot types. More recently, ChartLlama (Han et al., 2023) proposes a text-to-chart dataset that includes QA tasks and generates visualization code from provided descriptions. However, these datasets still lack coverage in certain plot categories such as 3D/volumetric plots and vector field plots, and they do not cover the use case of analyzing the raw data and predicting the most suitable plot types. On the other hand, our Text2Chart31 dataset encompasses 31 plot types with 11.1K tuples that combine descriptions, code, data tables, and plots, thereby covering a wide range of use cases.\nInstruction tuning. Employing reinforcement learning with human feedback is a prevalent strategy for enhancing (un)supervised finetuned models, whether by integrating human feedback into the learning loop (Arakawa et al., 2018; Arumugam et al., 2019) or by leveraging preference data generated by human (Ouyang et al., 2022; Glaese et al., 2022; Bai et al., 2022; Stiennon et al., 2022). However, we argue that this methodology might not offer the most practical solution for plot visualization tasks, given the intricate and fact-intensive nature of plot types. Moreover, considering the limitations of human cognition, there is a risk of overlooking crucial small details essential for validating the accuracy of generated plots. To address this, we propose a novel automatic method that constructs a preference dataset using supervised fine-tuned output.\nCycle consistency. Exploiting cycle consistency to enhance the performance of the generative model has been mainly studied in the image domain (Zhu et al., 2020). Recently, DDPO (Black et al., 2024) adopts the LLaVA model (Liu et al., 2023) to increase the alignment between the image and the text. Following this line of research, we propose an alignment reward that exploits cycle consistency between description and code to improve LLM for chart generation tasks. This is made possible because of the rich nature of our Text2Chart31 dataset, which consists of diverse textual modalities, including visualization code and description."}, {"title": "6 Conclusion", "content": "We introduce a novel hierarchical pipeline and a comprehensive dataset for chart generation. The proposed Text2Chart31 dataset, encompassing 31 unique plot types, provides a robust foundation for diverse visualization tasks with its 11.1K tuples of descriptions, code, data tables, and plots. Additionally, we proposed an RL-based instruction tuning technique employing preference and alignment rewards, improving LLMs in data visualization."}, {"title": "Limitations", "content": "There are certain considerations to note. First, our dataset is based on Matplotlib version 3.8. As such, if earlier versions of Matplotlib are used where function names may have changed, the generated code could potentially cause errors. This is a natural consequence of advancements and updates in software libraries. Additionally, the descriptions provided are exclusively in English. This focus ensures clarity and consistency in our current scope but can be expanded to include multiple languages in future iterations. Lastly, our primary focus was on chart generation through large language models (LLMs), rather than on question answering. However, exploring question answering capabilities is a promising direction for future research."}, {"title": "Ethics Statement", "content": "All data points generated in Text2Chart31 were created using large language models (LLMs) and are intended solely for visualization purposes. These data points do not represent real-world facts and should not be referenced as accurate depictions of actual data distributions. Furthermore, they do not contain offensive contents. Matplotlib library is based on PSF license. We have used open source models, libraries, and closed source models for their intended uses, and not use other than research purposes."}]}