{"title": "A Dual-Agent Adversarial Framework for Robust Generalization in Deep Reinforcement Learning", "authors": ["Zhengpeng Xie", "Jiahang Cao", "Yulong Zhang", "Qiang Zhang", "Renjing Xu"], "abstract": "Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) has emerged as a powerful paradigm for solving complex decision-making problems, leveraging an agent's ability to learn from interactions with an environment through trial and error (Sutton, 2018). However, generalization between tasks remains difficult for state-of-the-art deep reinforcement learning algorithms. Although trained agents can solve complex tasks, they often struggle to transfer their experience to new environments. For instance, an agent trained in a specific environment struggles to perform effectively in another, even when the only difference between environments is a subtle alteration, such as the change of colors in the scene (Cobbe et al., 2019; 2020). This limitation underscores the challenges of transferring knowledge across different contexts, emphasizing the importance of developing robust generalization strategies for RL applications in dynamic and variable real-world scenarios (Korkmaz, 2024).\nOne approach to enhancing generalization in RL focuses on data augmentation techniques (Lee et al., 2019; Laskin et al., 2020; Zhang & Guo, 2021), which increase the diversity of training data by modifying input observations or environmental conditions. While this provides a straightforward solution, it can introduce biases that do not align with RL objectives and often neglect the nuances of the RL process, potentially limiting effectiveness. Another approach involves regularizing the learned functions, drawing from traditional techniques used in deep neural networks, such as batch normalization (Liu et al., 2019), contrastive learning (Agarwal et al., 2021), and loss function regularization (Amit et al., 2020). However, these methods can not adequately address the unique challenges of RL, as they often focus on static representations rather than the dynamic nature of agent-environment interactions. Consequently, both data augmentation and traditional regularization methods have limitations that hinder their ability to facilitate effective generalization in RL.\nAdversarial learning (Pinto et al., 2017; Zhang et al., 2020; Oikarinen et al., 2021; Li et al., 2021; Rahman & Xue, 2023) presents a promising direction for enhancing generalization in RL by learning robust representations of irrelevant features through an adversarial process. This framework facilitates the development of agents capable of adapting to new environments by emphasizing the distinction between relevant and irrelevant information. While adversarial learning frameworks integrate the RL process, existing methods often rely on introducing generator and discriminator networks (Goodfellow et al., 2014) or seek to modify fundamental parameters of the simulation environments. Such heterogeneous adversarial processes introduce additional hyperparameters and training costs, necessitating carefully designed architectures. These complexities make it challenging to establish a unified framework for generalization tasks across diverse domains.\nTo address the generalization problem in RL, in this paper, we propose a novel adversarial learning framework, which involves a game process between two homogeneous agents (in Figure 2). This framework offers three key advantages: 1) First, this general framework can integrate well with existing policy learning algorithms such as Proximal Policy Optimization (PPO) (Schulman et al., 2017). 2) Second, the adversarial process allows agents to spontaneously learn the underlying semantics without necessitating additional human prior knowledge, thus fostering robust generalization performance. 3) Lastly, our approach introduces minimal additional hyperparameters, highlighting its potential for widespread applicability across various RL models. Extensive experiments demonstrate that our adversarial framework significantly improves generalization performance in Procgen (Cobbe et al., 2020), particularly in hard-level environments (in Figure 1). This framework marks a significant advancement in addressing generalization challenges in deep reinforcement learning.\nOur contributions are summarized as follows:\n\u2022 We demonstrate that minimizing the policy's robustness to irrelevant features helps improve generalization performance."}, {"title": "2. Preliminaries", "content": "Markov Decision Process and Generalization. We first consider the formalization of generalization in RL. Denote a Markov Decision Process (MDP) as m, defined by the tuple\n$m = (S_m, A, r_m, P_m, \\rho_m, \\gamma)$, (1)\nwhere m is sampled from the distribution $p_M(\\cdot)$, $S_m$ represents the state space, A represents the action space, $r_m: S_m \\times A \\rightarrow R$ is the reward function, $P_m: S_m \\times A \\times S_m \\rightarrow [0, 1]$ is the probability distribution of the state transition function, $\\rho_m: S_m \\rightarrow [0, 1]$ is the probability distribution of the initial state, and $\\gamma \\in (0, 1]$ is the discount factor. Typically, during training, the agent is only allowed to access $M_{train} \\subset M$ and is then tested for its generalization performance by extending to the entire distribution M. The agent generates the following trajectory on $m_t$:\n$\\tau_m = (s_m^0, a_m^0, r_m^0,...,s_m^t, a_m^t, r_m^t,...)$. (2)\nSimilar to standard RL, the state-value function, value function can be defined as\n$Q_m(s^m, a) = E_{s_{t+1},a_{t+1}...} [\\sum_{k=0}^{\\infty} \\gamma^k r_m(s_{t+k}, a_{t+k})]$,\n$V^\\pi_m(s) = E_{a\\sim \\pi(\\cdot|s_m)}[Q^\\pi_m(s^m, a)].$ (3)\nGiven $Q^\\pi_m$ and $V^\\pi_m$, the advantage function can be expressed as $A^\\pi_m(s_m, a_m) = Q^\\pi_m(s_m, a_m) - V^\\pi_m(s_m)$. We now denote $\\zeta(\\pi) = E_{m \\sim p_M(\\cdot), \\tau_m \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_m(s_m, a_m)]$ as the generalization objective given policy $\\pi$, and denote $\\eta(\\pi) = E_{m \\sim p_{M_{train}}(\\cdot), \\tau_m \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_m(s_m, a_m)]$ as the training objective, where the notation $E_{\\tau_m \\sim \\pi}$ indicates the expected return of the trajectory $\\tau_m$ generated by the agent following policy $\\pi$, i.e., $s_m^0 \\sim \\rho_m(\\cdot)$, $a^0_m \\sim \\pi(\\cdot|s_m^0)$, $r_m^0 \\sim r_m(s_m^0, a_m^0)$, $s^1_m \\sim P_m(\\cdot|s_m^0, a_m^0)$, where t \\in N, N is the set of all natural numbers.\nFor the convenience of subsequent theoretical analysis, we decouple the state $s^m$ into u_t and $\\phi_m(\\cdot)$, i.e., $s^m_t = \\phi_m(u_t)$, where $u_t$ is independent of m, while $\\phi_m(\\cdot)$ is completely and only determined by m. For instance, $u_t$ implicitly encompasses significant semantic information, which is crucial for the agent to maximize the expected return. This includes, for example, the relative positional relationship"}, {"title": "3. Theoretical Analysis", "content": "In this section, we derive the lower bounds for the training and generalization performance of the agent. The main conclusion drawn from this is that improving the agent's robustness to irrelevant features will help enhance its generalization performance.\nGiven the probability distribution $p_M$, we first make the following assumption:\nAssumption 3.1. When m is sampled from $M_{train} \\subset M$, i.e., $m \\sim p_{M_{train}}(\\cdot)$, we assume that\n$p_{M_{train}}(m) = \\frac{p_M(m) \\cdot \\mathbb{I}(m \\in M_{train})}{M}$, (4)\nwhere $M = \\int_{M_{train}} p_M(m) dm$ is the normalized coefficient that represents the probability that m, sampled from the entire distribution M, belongs to $M_{train}$, while $\\mathbb{I}(\\cdot)$ is the indicator function.\nIt can be proved that $p_{M_{train}}(m)$ is a probability distribution, please refer to Appendix C.1 for details. Based on Assumption 3.1, we can derive the following generalization theorem:\nTheorem 3.2 (Generalization performance lower bound). Given any policy $\\pi$, the following bound holds:\n$\\zeta(\\pi) \\ge \\eta(\\pi) - \\frac{2 r_{max}}{1 - \\gamma} (1 - M)$, (5)\nwhere $\\zeta(\\pi)$ and $\\eta(\\pi)$ denote the generalization objective and training objective, respectively; $r_{max} = \\max_{m, s, a} |r_m(s, a)|$.\nThe proof is in Appendix C.2. This inspires us that when sampling m from the entire M, with the increase of M (i.e., the probability of the sampled $m \\in M_{train}$), the lower bound of generalization performance is continuously optimized and tends to be consistent with when M = 1.\nAccording to Theorem 3.2, once $M_{train}$ is determined, the value of M is also fixed, at this point, $\\eta$ is the only term that we can optimize in the lower bound. Therefore, we now focus on optimizing $\\eta$. Before that, we present some important theoretical results in the following:\nTheorem 3.3. (Kakade & Langford, 2002) Let $P(s_t = s|\\pi)$ represents the probability of the t-th state equals to s in trajectories generated by the agent following policy $\\pi$, and $\\rho^\\pi(s) = \\sum_{t=0}^{\\infty} \\gamma^t P(s_t = s|\\pi)$ represents the unnormalized discounted visitation frequencies. Given any two policies, $\\pi$ and $\\tilde{\\pi}$, their performance difference can be measured by\n$\\eta(\\pi) = \\eta(\\tilde{\\pi}) + E_{s \\sim \\rho^{\\tilde{\\pi}}(\\cdot), a \\sim \\pi(\\cdot|s)} [A^\\pi(s, a)]$. (6)\nTheorem 3.4. (Schulman, 2015) Given any two policies, $\\pi$ and $\\tilde{\\pi}$, the following bound holds:\n$\\eta(\\pi) \\ge L(\\tilde{\\pi}) - \\frac{4 \\gamma \\max_{s, a} |A^\\pi(s, a)|}{(1 - \\gamma)^2} \\cdot D_{TV}^{max} (\\pi, \\tilde{\\pi})^2$, (7)\nwhere $L(\\tilde{\\pi}) = \\eta(\\tilde{\\pi}) + E_{s \\sim \\rho^{\\pi}(\\cdot), a \\sim \\tilde{\\pi}(\\cdot|s)} [A^\\pi(s, a)]$.\nThe aforementioned theorems only consider standard RL. On this foundation, we further extend them and derive a lower bound for the training objective:\nTheorem 3.5 (Training performance lower bound). Let $P(s_t^m = s|m, \\pi)$ represents the probability of the t-th state equals to s in trajectories generated by the agent following policy $\\pi$ in MDP m, and $\\rho_m^\\pi(s) = \\sum_{t=0}^{\\infty} \\gamma^t P(s_t^m = s|m, \\pi)$ represents the unnormalized discounted visitation frequencies. Given any two policies, $\\pi$ and $\\tilde{\\pi}$, the following bound holds:\n$\\eta(\\pi) \\ge L(\\tilde{\\pi}) - \\frac{4 \\gamma \\tilde{A}}{(1 - \\gamma)^2} (\\sqrt{D_1} + \\sqrt{D_2} + \\sqrt{D_3})^2$, (8)\nwhere $\\tilde{A} = \\max_{m, s, a} |A^\\pi_m(s, a)|$, and\n$\\eta(\\pi) = \\eta(\\tilde{\\pi}) + E_{m \\sim p_{M_{train}}(\\cdot), s \\sim \\rho_m^{\\pi}(\\cdot), a \\sim \\tilde{\\pi}(\\cdot|s)} [A^\\pi_m(s, a)]$,\n$L(\\tilde{\\pi}) = \\eta(\\tilde{\\pi}) + E_{m \\sim p_{M_{train}}(\\cdot), s \\sim \\rho_m^{\\pi}(\\cdot), a \\sim \\tilde{\\pi}(\\cdot|s)} [A^\\pi_m(s, a)]$,\n$D_1 = E_{m \\sim p_{M_{train}}(\\cdot)} \\{ D_{TV}^{max} [\\pi(\\cdot|\\phi_m(u)), \\tilde{\\pi}(\\cdot|\\phi_m(u))] \\}^2$,\n$D_2 = E_{m, m' \\sim p_{M_{train}}(\\cdot)} \\{ D_{TV}^{max} [\\pi(\\cdot|\\phi_m(u)), \\pi(\\cdot|\\phi_{m'}(u))] \\}^2$,\n$D_3 = E_{m, m' \\sim p_{M_{train}}(\\cdot)} \\{ D_{TV}^{max} [\\tilde{\\pi}(\\cdot|\\phi_m(u)), \\tilde{\\pi}(\\cdot|\\phi_{m'}(u))] \\}^2$, (9)\nwhere the notation $D_{TV}^{max}(\\cdot) = \\max_u D_{TV} (\\cdot)$.\n$\\eta(\\pi) - \\eta(\\tilde{\\pi}) \\ge L(\\tilde{\\pi}) - \\eta(\\pi) - C \\cdot (\\sqrt{D_1} + \\sqrt{D_2} + \\sqrt{D_3})^2$, (10)\nwhere $C = \\frac{4 \\gamma \\tilde{A}}{(1 - \\gamma)^2}$. We now denote $M_{\\pi}(\\tilde{\\pi}) = C \\cdot (\\sqrt{D_1} + \\sqrt{D_2} + \\sqrt{D_3})^2$, we can then derive the following monotonic improvement theorem:\nTheorem 3.6 (Monotonic improvement of training performance). Let $\\pi_0, \\pi_1, \\pi_2, ..., \\pi_k$ be the sequence of policies generated by Algorithm 1, then\n$\\eta(\\pi_k) \\ge \\eta(\\pi_{k-1}) \\ge ... \\ge \\eta(\\pi_0)$. (11)\nProof. According to inequality (10) and Algorithm 1, we have\n$\\eta(\\pi_{i+1}) - \\eta(\\pi_i) > L_{\\pi_{i+1}}(\\pi_i) - \\eta(\\pi_i) - M_{\\pi_{i+1}}(\\pi_i) \\ge 0$, (12)\nwhere i = 0,1, . . ., k \u2212 1, so that $\\eta(\\pi_{i+1}) \\ge \\eta(\\pi_i)$, concluding the proof.\nOn the other hand, it is evident that\n$\\eta(\\pi_{i+1}) - \\frac{2 r_{max}}{1 - \\gamma} (1 - M) \\ge \\eta(\\pi_i) - \\frac{2 r_{max}}{1 - \\gamma} (1 - M)$, (13)\nwhich means through the iterative process of Algorithm 1, we optimize the lower bound of generalization performance (5) as well. In fact, if both $D_2$ and $D_3$ are always equal to zero, i.e., given any $m, m' \\in M$ and $u \\in U$, we have $\\pi_i(\\cdot|\\phi_m(u)) = \\pi_i(\\cdot|\\phi_{m'}(u))$, $\\forall i \\in N$. In this case, the agent has complete insight into the underlying semantics without having to rely on m, i.e., the agent is essentially interacting with $m^* = (U, A, r, P, \\rho, \\gamma)$, Theorem 3.5 degenerates into Theorem 3.4.\nHowever, Algorithm 1 is an idealized approach, we have to adopt some heuristic approximations in practical solutions."}, {"title": "4. Methodology", "content": "In the previous section, we derived the lower bound of training performance, which inspires us to optimize the part of the policy that determines robustness. Therefore, in this section, we first analyze the optimization problem of parameterized policies (Section 4.1), then deconstruct what properties a generalization agent should have (Section 4.2), and finally propose a dual-agent adversarial framework to solve the generalization problem (Section 5.2)."}, {"title": "4.1. Optimization of Parameterized Policies", "content": "We first consider the parameterized policies, i.e., $\\pi_\\theta$, and denote the upstream encoder of the policy network as $\\psi_\\omega$, where $\\omega$ and $\\theta$ represent the parameters of the encoder and policy network, respectively.\nFor any given state $s = \\phi_m(u)$, for brevity, we denote $\\xi_m = \\psi_\\omega(\\phi_m(u))$ as the representation input into the policy network $\\pi_\\theta$ after passing through the encoder $\\omega$. Similar to TRPO (Schulman, 2015), the total variational distance and KL divergence satisfy $D_{TV}^{max} [\\pi_{\\theta_{old}}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_m)]^2 < D_{KL} [\\pi_{\\theta_{old}}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_m)]$, where $\\theta_{old}$ represents the policy network parameters before the update, while $\\theta$ represents the current policy network parameters. Through heuristic approximation, the maximum KL divergence $D_{KL}^{max}$ is approximated as the average KL divergence $E[D_{KL}]$, and then Algorithm 1 is approximated as the following constrained optimization problem:\n$\\max_{\\theta} J(\\theta) = L_{\\theta_{old}}(\\theta) - \\eta(\\theta_{old})$, (14)\n$\\text{s.t. } E_{m \\sim p_{M_{train}}(\\cdot)} \\{ D_{KL} [\\pi_{\\theta_{old}}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_m)] \\} \\le \\delta_1, \\\\ E_{m, m' \\sim p_{M_{train}}(\\cdot)} \\{ D_{KL} [\\pi_{\\theta}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_{m'})] \\} \\le \\delta_2,$\nwhere m and m' are MDPs independently sampled from the distribution $p_{M_{train}}$. Then, similar to TRPO, $J(\\theta)$ can be expressed as\n$J(\\theta) = E_{m, s; a \\sim \\pi_{\\theta}(\\cdot|\\xi_m)} [A^\\pi_m(s, a)] = E_{m, s; a \\sim \\pi_{\\theta_{old}}(\\cdot|\\xi_m)} \\frac{\\pi_{\\theta}(a|\\xi_m)}{\\pi_{\\theta_{old}}(a|\\xi_m)} A^\\pi_m(s, a)$, (15)\nwhich is called importance sampling, where $m \\sim p_{M_{train}}(\\cdot)$ and $s \\sim \\rho_m^{\\pi_{\\theta_{old}}}(\\cdot)$. Thus, we can further transform the constrained optimization problem (14) into the following\n$\\max_{\\theta} J(\\theta) = E_{m, s; a \\sim \\pi_{\\theta_{old}}(\\cdot|\\xi_m)} \\frac{\\pi_{\\theta}(a|\\xi_m)}{\\pi_{\\theta_{old}}(a|\\xi_m)} A^\\pi_m(s, a)$, (16)\n$\\text{s.t. } E_{m \\sim p_{M_{train}}(\\cdot)} \\{ D_{KL} [\\pi_{\\theta_{old}}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_m)] \\} \\le \\delta_1, \\\\ E_{m, m' \\sim p_{M_{train}}(\\cdot)} \\{ D_{KL} [\\pi_{\\theta}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_{m'})] \\} \\le \\delta_2,$\nwhere $A(s, a)$ is the estimation of the advantage function, and in this paper, we adopt the GAE (Schulman et al., 2015) technique. The first constraint of (16) measures the difference between the old and new policies, where TRPO (Schulman, 2015) and PPO (Schulman et al., 2017) have already provided corresponding solutions. However, it's important to note that the second constraint in (16) can not be approximated, as it involves different states with the same underlying semantics, and predicting another $\\phi_{m'}(u)$ based on any received state $\\phi_m(u)$ (m \u2260 m') is untraceble.\nHence, understanding different states with the same underlying semantics is the most central challenge in the generalization of deep reinforcement learning. In the following section, we will systematically discuss the characteristics a sufficiently general agent should possess to achieve good generalization performance."}, {"title": "4.2. How to Achieve Good Generalization?", "content": "As discussed previously, it is unable to solve the optimization problem (16) directly, as the expectation $E_{m, m' \\sim p_{M_{train}}(\\cdot)} \\{ D_{KL} [\\pi_{\\theta}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_{m'})] \\}$ cannot be estimated due to the unknown distribution $p_{M_{train}}$ and function $\\phi_m$. In this section, we focus on analyzing the characteristics that a generalization agent should possess.\nAlthough estimating different states with the same semantics during training is challenging, one effective approach to explicitly learn the underlying semantics is to introduce the adversarial method. For instance, Rahman & Xue (2023) aims to maximize expected return while minimizing the interference from adversarial examples generated by its own generator, StarGAN (Choi et al., 2018). This process ultimately facilitates robust policy learning and helps prevent the agent from overfitting irrelevant features in high-dimensional observations, inspiring us to incorporate an adversarial framework into our approach (Section 5.2).\nHowever, StarGAN does not entirely eliminate the biases introduced by human prior knowledge. Specifically, the domain of the original input image is clustered using a Gaussian Mixture Model (GMM), which inherently introduces biases from the GMM. Furthermore, the number of clusters is often determined empirically, adding another layer of human influence.\nTherefore, firstly, a sufficiently general agent should spontaneously learn robust representations for irrelevant features, rather than relying on biases introduced by human prior knowledge. Figure 3 (a) shows the potential impact of introducing biases into the model. Secondly, the entire pipeline for learning generalization must integrate the RL process, as the identification of irrelevant features is closely linked to the objectives of RL, particularly the configuration of the reward function. Figure 3 (b) demonstrates how different reward functions influence the agent's recognition of irrelevant information within a simple maze environment.\nIn summary, we conclude that a sufficiently general agent should possess two characteristics:\n\u2022 The agent is able to spontaneously learn robust representations for high-dimensional input without introducing any bias that benefits from human prior knowledge.\n\u2022 The agent should adaptively adjust its representation of underlying semantics in response to changes in the reward function, demonstrating the ability to identify the semantics corresponding to specific objectives.\nGiven these two points, we will introduce a dual-agent adversarial framework in the following section, which empowers agents with enhanced generalization capabilities."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Settings", "content": "Benchmark. Procgen (Cobbe et al., 2020) is an environment library specifically designed for reinforcement learning research, developed by OpenAI. It provides a diverse and procedurally generated set of platform games, allowing researchers to test the generalization capabilities of agents across different tasks and scenarios.\nBaselines. We verify the performance of our proposed method compared with PPO (Schulman et al., 2017) and DAAC (Raileanu & Fergus, 2021) as the baselines for our comparative experiments.\nTraining Settings. In all experiments, we use the hyperparameters provided in the Appendix B unless otherwise specified. We referred to the original paper for hyperparameters specific to the algorithm. Following the recommendations of Cobbe et al. (2020), we run these methods on hard-level generalization tasks, training on eight environments of 500 levels and evaluating generalization performance on the full distribution of levels. We interact for 50M steps to consider running time. This is sufficient to assess the performance differences between our method and other baselines."}, {"title": "5.2. Adversarial Policy Learning Framework", "content": "In the previous analysis, we summarized the core challenges in the generalization of RL (Section 4.1) and the characteristics a general agent should possess (Section 4.2). However, generating adversarial samples through generative models introduces additional hyperparameters and training costs, and relies on carefully designed model architecture.\nTo address these issues, a viable solution is to attack the agent's encoder instead of directly generating adversarial samples. In this section, we introduce a dual-agent adversarial framework, which involves a game process between two homogeneous agents, as shown in Figure 4.\nIn particular, two symmetric agents are introduced in this framework, both agents have the capability to utilize their respective training data and update the other agent's encoder through backpropagation, which empowers them to perform adversarial attacks on each other. Since the two agents are equivalent in status, we take the perspective of agent 1 as an example. Agent 1 inputs its training data $s_1$ into both its own encoder and the other agent's encoder, obtaining $\\psi_1(s_1)$ and $\\psi_2(s_1)$, resulting in different representations of the same state $s_1$. The adversarial framework consists of two processes:\nAdversarial Attack on Opponent Agent. To prevent the opponent agent from producing good actions, agent 1 attempts to alter the parameters of both encoders to influence agent 2's decision-making, where the KL divergence is used to quantify this distributional perturbation:\n$D_{KL}^{other} = D_{KL} [\\pi_2(\\cdot|\\psi_2(s_1)), \\pi_2(\\cdot|\\psi_1(s_1))]$. (17)\nRobust Defense Against Adversarial Threats. Meanwhile, agent 1 itself attempts to remain robust to this influence, which can be expressed as\n$D_{KL}^{self} = D_{KL} [\\pi_1(\\cdot|\\psi_1(s_1)), \\pi_1(\\cdot|\\psi_2(s_1))]$. (18)\nIt should be noted that when agent 1 is performing adversarial attacks on agent 2's encoder $\\psi_2$, the parameters of agent 2's policy network $\\pi_2$ are frozen during this stage, thus do not participate in gradient updates.\nOverall, the goal of the agent is to maximize the perturbation $D_{KL}^{other}$ while minimizing the self-inference $D_{KL}^{self}$, resulting in the loss function of the form:\n$L_{KL} = D_{KL}^{self} - \\alpha D_{KL}^{other}$, (19)\nSince the adversarial process is coupled with the RL training process, the total loss is defined as\n$L = L_{RL} + \\alpha L_{KL}$, (20)\nwhere $L_{RL}$ is the loss function using a specific RL algorithm, $\\alpha$ is the only additional hyperparameter. As the two agents are equivalent, the training processes for both agents are completely symmetrical. The pseudo-code of the adversarial policy learning process is shown in Algorithm 2.\nThe overall loss comprises two components: the reinforcement learning loss term $L_{RL}$ and the adversarial loss term $L_{KL}$. The adversarial loss facilitates a competitive interaction among agents and functions similarly to a form of regularization, effectively preventing agents from overfitting to irrelevant features in high-dimensional observations. With the alternate updating of the two agents, they will have to consider the truly useful underlying semantics, leading to better generalization performance, or mathematically speaking, a lower $E_{m, m' \\sim p_{M_{train}}(\\cdot)} \\{ D_{KL} [\\pi_{\\theta}(\\cdot|\\xi_m), \\pi_{\\theta}(\\cdot|\\xi_{m'})] \\}$ in constrained optimization problem (16).\nIn summary, our proposed adversarial policy learning framework is well in line with the two characteristics proposed in Section 4.2:\n\u2022 First, the framework does not introduce any additional biases, allowing the agents to learn the underlying semantics spontaneously.\n\u2022 Second, the adversarial process and the reinforcement learning process are highly coupled, which means that the dependency between the reward signal and the corresponding representation can be modeled well."}, {"title": "6. Related Work", "content": "Generalizable RL Methods. The generalization problem of deep reinforcement learning has been well studied, and previous work has pointed out the overfitting problem in deep reinforcement learning (Rajeswaran et al., 2017; Zhang et al., 2018; Justesen et al., 2018; Packer et al., 2018; Song et al., 2019; Cobbe et al., 2019; Grigsby & Qi, 2020; Cobbe et al., 2020; Yuan et al., 2024). Data augmentation methods are considered as effective solutions for enhancing the generalization of agents. Directly integrating existing data augmentation methods with RL algorithms can yield improvements (Laskin et al., 2020; Kostrikov et al., 2020; Zhang & Guo, 2021; Raileanu et al., 2021). Domain randomization techniques (Tobin et al., 2017; Yue et al., 2019; Slaoui et al., 2019; Lee et al., 2019; Mehta et al., 2020; Li et al., 2021) inject random disturbances representing variations in the simulated environment during the training process of RL, effectively enhancing the adaptability of RL agents to unknown environments.\nAdversarial Learning Methods. Adversarial learning has been proven to be a powerful learning framework (Goodfellow et al., 2014; Jiang et al., 2020; Dong et al., 2020). For instance, combining adversarial learning with randomization to enhance the generalization performance of agents (Pinto et al., 2017; Li et al., 2021; Rahman & Xue, 2023). In addition, adversarial attacks are also used to improve the robustness and generalization performance of agents (Gleave et al., 2019; Oikarinen et al., 2021)."}, {"title": "7. Conclusion", "content": "This paper introduces a dual-agent adversarial framework designed to tackle the challenges of generalization in reinforcement learning. By incorporating a competitive process between two agents, our framework leverages adversarial loss to enable both agents to spontaneously learn effective representations of high-dimensional observations, resulting in robust policies that effectively handle irrelevant features. Extensive experimental results demonstrate that this framework significantly enhances both the training and generalization performance of baseline RL algorithms. Our findings indicate that the adversarial approach not only improves the resilience of RL agents but also represents a meaningful advancement in the quest for generalizable reinforcement learning solutions."}, {"title": "C. Proofs", "content": ""}, {"title": "C.1. Proof of Assumption 3.1", "content": "We now prove that $p_{M_{train}}(m)$ is a probability distribution, by integrating it, we obtain\n$\\int_{M_{train}} p_{M_{train}}(m) dm = \\int_{M_{train}} \\frac{p_M(m) \\cdot \\mathbb{I}(m \\in M_{train})}{M} dm = \\frac{1}{M} \\int_{M_{train}} p_M(m) \\cdot \\mathbb{I}(m \\in M_{train}) dm = \\frac{1}{M} \\int_{M_{train}} p_M(m) dm = 1$, (21)\nconcluding the proof."}, {"title": "C.2. Proof of Theorem 3.2", "content": "We are trying to measure the difference between $\\zeta(\\pi)$ and $\\eta(\\pi)$, which is\n$|\\zeta(\\pi) - \\eta(\\pi)| = |E_{m \\sim p_M(\\cdot), \\tau_m \\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r_m(s_m^t, a_m^t)", "a_m^t)": 22}, {"a_m^t)": "E_{m \\sim p_{M_{train}}(\\cdot)} [\\sum_{t=0}^{\\infty} \\gamma^t r_m(s_m^t, a_m^t)"}]}