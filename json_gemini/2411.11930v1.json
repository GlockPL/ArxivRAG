{"title": "AtomThink: A Slow Thinking Framework for Multimodal Mathematical Reasoning", "authors": ["Kun Xiang", "Zhili Liu", "Zihao Jiang", "Yunshuang Nie", "Runhui Huang", "Haoxiang Fan", "Hanhui Li", "Weiran Huang", "Yihan Zeng", "Jianhua Han", "Lanqing Hong", "Hang Xu", "Xiaodan Liang"], "abstract": "In this paper, we address the challenging task of multimodal mathematical reasoning by incorporating the ability of \"slow thinking\u201d into multimodal large language models (MLLMs). Contrary to existing methods that rely on direct or fast thinking, our key idea is to construct long chains of thought (CoT) consisting of atomic actions in a step-by-step manner, guiding MLLMs to perform complex reasoning. To this end, we design a novel AtomThink framework composed of three key modules: (i) a CoT annotation engine that automatically generates high-quality CoT annotations to address the lack of high-quality visual mathematical data; (ii) an atomic step fine-tuning strategy that jointly optimizes an MLLM and a policy reward model (PRM) for step-wise reasoning; and (iii) four different search strategies that can be applied with the PRM to complete reasoning. Additionally, we propose Atom-MATH, a large-scale multimodal dataset of long CoTs, and an atomic capability evaluation metric for mathematical tasks. Extensive experimental results show that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving approximately 50% relative accuracy gains on MathVista and 120% on MathVerse. To support the advancement of multimodal slow-thinking models, we will make our code and dataset publicly available on https://github.com/Quinn777/AtomThink.", "sections": [{"title": "1. Introduction", "content": "Chain-of-thought (CoT) reasoning [34] has provided a novel scheme for large language models (LLMs) to tackle complex reasoning tasks. By utilizing a small number of specially designed instructions, CoT enables LLMs to generate intermediate reasoning steps, significantly enhancing performance on symbolic tasks such as mathematical problems and code writing [44].\nWhile CoT-based methods show clear improvements over direct predictions, they still rely heavily on greedy decoding strategies. More recently, the introduction of OpenAI's o1 [23] marks a substantial advancement in the ability of artificial intelligence systems to perform high-level reasoning. Unlike traditional models, o1 excels in solving complex problems by utilizing extended reasoning chains and adopting test-time scaling, i.e., \"slow thinking\". In addition to o1, several concurrent works have explored methods for incorporating slow thinking capabilities into open-source LLMs, such as Thought Trees [35] and Monte Carlo tree search (MCTS) based tree search techniques [6, 25, 30, 31]. The success of o1 and its variants demonstrate that incorporating slow thinking into LLMs significantly enhances their performance on complex, multi-step tasks, improving their overall problem-solving capabilities.\nHowever, adopting the slow-thinking technique into multimodal large language models (MLLMs) is challenging, due to the increased data and computational resource demand for information modeling in visual tasks [24, 42]. Although many efforts have been conducted to alleviate this issue, such as incorporating interleaved image-text data [1], prompt engineering [20, 26], they are still confined to stimulating the inherent CoT capabilities of MLLMs, without considering the quality of each intermediate step in the reasoning chain. Hence, existing methods are hard to apply test-time scaling laws to guarantee their performance.\nTo validate the importance of the quality of each intermediate step in CoT, we first design a capability evaluation method to perform a fine-grained quality assessment of each atomic step generated by MLLMs. Here we define an atomic step as the minimal prediction node in the slow thinking process. Considering that humans may utilize distinct cognitive abilities for solving mathematical problems, we utilize one of the current most advanced LLMs, i.e., GPT-40 [21] to construct an ability set and estimate scores of atomic steps with outcome supervision. The results shown in Figure 1 indicate that the step quality of existing open-source models is significantly lower than that of GPT-40, particularly in areas such as image recognition, variable definition, and calculation ability. This finding further motivates our focus on the capability gaps in existing models, prompting us to improve performance by enhancing the quality of atomic reasoning steps.\nTherefore, to fully leverage the advantages of CoT and address the aforementioned challenges, we propose a full-process slow-thinking framework called AtomThink. AtomThink introduces a multimodal CoT annotation engine, an atomic step finetuning strategy, and policy searching strategies to generate high-quality atomic steps. It aims to enhance the decoding capabilities of MLLMs through careful training, combined with post-sampling search strategies to identify the optimal prediction nodes. To begin with, the proposed annotation engine is used to create a novel multimodal long CoT dataset including 26k high-level mathematical problems, 157k atomic-granularity steps, and 130k process supervision annotations. The construction of this dataset does not require manual labeling and effectively leverages existing short labels. Secondly, our atomic step finetuning strategy applies step-level masking to the training set, forcing our models to learn multi-turn self-dialogue ability and generate reasoning focused on individual inference actions. Thirdly, we explore different search strategies along both the path and step dimensions during the inference phase to find optimal prediction nodes. To validate the effectiveness of our method, we conduct extensive experiments on public datasets. We improved the accuracy of LLaVA-Llama3-8B on MathVista and MathVerse by 9.6% and 18.8%, respectively. With EMOVA (8B) as the base model, AtomThink achieved the highest accuracy of 40.5% on MathVerse, surpassing the cutting-edge GPT-4V.\nIn summary, our primary contributions are as follows:\n\u2022 We introduce AtomThink, a comprehensive framework that guides MLLMs to focus on atomic step reasoning, which obtains consistent performance improvements across multiple baseline MLLMs.\n\u2022 By designing an atomic capability evaluation based on outcome supervision, we reveal the capability distribution of MLLMs in generating each type of atomic step.\n\u2022 A multimodal long CoT dataset specifically focused on multimodal mathematical tasks, AtomMATH, is first introduced."}, {"title": "2. Related Work", "content": "Slow Thinking in Multimodal Reasoning Tasks Complex reasoning tasks such as mathematical computation and code programming have long been challenging for MLLMs [15, 36, 44]. Some prior work has approached this issue from the perspective of prompt engineering, encouraging models to generate Chain-of-Thought(CoT), which is widely believed to enhance model's reasoning [33, 34]. They carefully modify the input distribution to enable the model to mimic human step-by-step output without finetuning parameters. Other recent studies have explored understanding visual ambiguity by introducing multi-turn chain-of-thoughts [20]. Shao et al. [26] have considered incorporating additional visual tokens into CoTs, such as object regions and precise localization. However, due to the lack of multimodal process supervision data, current works have not explored reward model-based search strategies, which are widely used in LLMs [3, 12, 28, 29, 38].\nLong CoT Annotation for Mathematical Data The introduction of slow thinking relies heavily on the availability of high-quality step-level annotations. In 2023, Lightman et al. [11] constructed a process supervision dataset composed of extensive human annotations, which has been widely used for mathematical reasoning. Recent advancements have focused on automating the data acquisition process, allowing models to generate their own CoT. Techniques like Quiet-STaR [39] have demonstrated how self-generated reasoning can enhance model performance without requiring manually labels. Moreover, some methods based on Monte Carlo estimation have automated the process data collection, but they also introduce additional computational cost [19, 32]. In multimodal domain, MAVIS [41], a dataset consisting of 834k visual math problems annotated with short CoT, has been proposed. Other studies have distilled reasoning processes from short answers [42]. However, these machine-generated annotations are often too brief and challenging to segment semantically."}, {"title": "3. Method", "content": "In this section, we present the details of AtomThink for promoting MLLMs for mathematical reasoning with slow thinking. As shown in Figure 2, AtomThink consists of three key components, including a multimodal CoT annotation engine (Sec. 3.1), atomic step fine-tuning (Sec. 3.2), and policy searching (Sec. 3.3). The annotation engine is designed to efficiently generate long CoTs to address data scarcity. With sufficient data, we fine-tune MLLMS and train a process reward model (PRM) for incorporating slow thinking ability into models. Furthermore, we explore four different path-wise and step-wise strategies for policy searching, allowing the fine-tuned MLLM to ensure that each decision made during its inference contributes to the overall accuracy and consistency of reasoning. Finally, we propose an atomic capability evaluation metric in Sec. 3.4 to measure the reasoning quality of models."}, {"title": "3.1. Multimodal CoT Annotation Engine", "content": "Guiding MLLMs toward deep reasoning requires a substantial amount of high-quality CoT data. However, in the field of visual mathematics, the scarcity of publicly available datasets presents a considerable challenge. To overcome this, we develop an automated data engine capable of generating step-by-step long CoTs, resulting in our own atomic mathematical problem dataset, dubbed Atom-MATH. Specifically, our data engine introduces a dynamic prompting strategy and a semantic-level augmentation strategy to produce multi-step reasoning paths.\nDynamic Prompting Strategy. To overcome the computational cost limitations associated with previous methods that relied on manual annotation or process supervision, we explore the possibility of driving existing models to autonomously generate high-quality reasoning data through simple prompting. Inspired by recent research [9] on using prompting strategies to improve the reasoning capabilities of LLMs, we propose a dynamic prompt strategy for generating atomic inference steps. Specifically, our strategy drives LLMs to iteratively construct state-reasoning paths. Each path node represents a reasoning step and encompasses the previous stage, the current state, and a possible action. The possible action includes continuing reasoning, verifying and drawing conclusion, which is determined by LLM itself. Unlike previous methods such as OmegaPRM [19] and Math-Shepherd [32] that generate a whole reasoning tree at once, our approach implicitly integrates the search over the step dimension into existing reasoning process through prompt engineering. For each problem instance, only a single valid path is explored, eliminating the need for additional process supervision computation.\nShort CoT Augmentation. To fully leverage existing short CoT annotations of VQA datasets, we also employ LLMs to atomize and augment these annotations. An example of short CoT augmentation is provided in the supplemental material. This approach allows us to semantically segment an original reasoning process into multiple discrete steps, and focus on solving a single atomic problem at each stage of the reasoning process, thereby ensuring the clarity and precision of our model."}, {"title": "3.2. Atomic Step Fine-Tuning", "content": "To fully exploit MLLMs for addressing multi-modal mathematical problems, we conduct fine-tuning with atomic step-wise reasoning. Particularly, this process includes fine-tuning the MLLM on our AtomMATH dataset and learning the PRM to estimate reward scores during the inference.\nMLLM Fine-Tuning. To transfer MLLM to step-wise mathematical reasoning, we first fine-tune it within the framework of Markov decision process (MDP) learning. Specifically, we consider the reasoning process of MLLM as an MDP, which can be formulated as M = (V, S, A, R, \u03c0). Here, V denotes the vocabulary, S represents historical reasoning steps, and A corresponds to next atomic step predicted by MLLM. \u03c0(a|s) represents the probability of selecting an action a \u2208 A conditioned on a state s \u2208 S, which is estimated by PRM to guide reasoning process. Hereby we can adopt the visual instruction tuning technique [14] to fine-tune MLLM.\nPRM Training. In a slow thinking process, reasoning is carried out step by step, where each atomic step provides an intermediate conclusion. We train the PRM to implement \u03c0(a|s) and provide feedback for every step, allowing MLLMs to refine and improve its reasoning. Formally, given the description of mathematical problem q, for an arbitrary step t > 1, the PRM predicts a probability pt of selecting an action a given the previous states $s_{1:t-1}$ as follows:\n$p_t(a) = \\text{PRM} ([q, s_{1:t-1}], a) $. (1)\nWe propose to train the PRM by minimizing the following binary cross-entropy loss:\n$L_{PRM} = \\sum_{t=1}^T y_t (a) \\log p_t (a) + (1 - y_t (a)) \\log(1 - p_t(a))$, (2)\nwhere yt(a) denotes the ground-truth CoT annotation that $y_t = 1$ if the action a is selected, otherwise yt(a) = 0. T is the maximum number of steps. Note that we omit to enumerate all possible actions in Eq. (2) for the concise presentation. After selecting the action at the current step at, we concatenate it with the previous states to construct st, i.e., st = s1:t-1 \u222a at.\nIn this subsection, we perform atomic step fine-tuning on the AtomMATH (including A-PRM and A-SFT subsets) and PRM800k dataset [11]. Moreover, we incorporate image captions into the generation of long CoT data, thus we can alleviate the expensive computation burden of image understanding in MLLMs and focus on texts for supervised fine-tuning. Therefore, we post-train an LLM based on Math-psa [31] to evaluate the consistency of atomic texts and supervise fine-tuning."}, {"title": "3.3. Action Search with PRM", "content": "With the fine-tuned MLLM capable of atomic step reasoning and the well-trained PRM providing feedback, we can now begin the reasoning process. As there are many search strategies to generate candidate actions, we categorize the existing strategies into path-wise searching and step-wise searching and explore them in our AtomThink framework.\nPath-wise Search. In path-wise searching, we build upon prior work [28, 31] by parallel sampling multiple paths and aggregating scores to find optimal solutions. We investigate the following two strategies:\n\u2022 Majority Voting: It combines multiple reasoning paths by selecting the most frequent outcome across them. It assumes that the consensus across different paths is more likely to lead to the correct answer.\n\u2022 Best-of-N: Given a generative MLLM, the best-of-N sampling method generates C candidate rollouts simultaneously and selects the solution with the highest score. The evaluation of candidate reasoning processes is determined by the PRM, which employs three aggregation methods to map the dense scores to the overall value of the entire path: 1) The worst action: Compare the worst action among all candidate rollouts. It penalizes solutions with any weak action and is used to search a reasoning that is sensitive to errors. 2) The last action: The score is derived from the prediction of the final answer in inference. 3) Average score: It is calculated by averaging the rewards of all the actions in a chain. The explainability and consistency of intermediate reasoning are emphasized here as important as the outcome.\nStep-wise Search. Searching strategies of this type start with an initial path and incrementally expand the sampling space for each atomic action. Beam search and greedy strategies are applied to prune branches with low quality.\n\u2022 Greedy Algorithm: It focuses on making the locally optimal choice at each step of the reasoning process. It selects the best immediate action (step) based on the current state, without considering future consequences.\n\u2022 Beam Search: It explores multiple branches at each action and maintains a fixed number of top candidates for each stage of reasoning. It balances between exploring different paths and exploiting the most promising ones."}, {"title": "3.4. Atomic Capability Evaluation", "content": "Similar to human problem-solving processes, a CoT may involve the use of multiple reasoning abilities. However, traditional CoT methods do not focus on the quality of individual reasoning steps or provide fine-grained analyses of the underlying abilities. To address this gap, we have developed an atomic capability evaluation strategy, offering a new analytical perspective for slow thinking.\nOur evaluation method aims to assess the mathematical capabilities of a target model from various perspectives, such as understanding, operations, and certifications. To this end, we first need to construct a canonical set of capabilities. As shown in Figure 3, we collect the behavior distribution of GPT-40 on MathVerse [43] and perform clustering, yielding clusters that each of them represents certain abilities utilized by high-level intelligent models in solving mathematical problems. Afterward, we evaluate an atomic action a predicted by the target model based on outcome supervision, namely, we consider that the quality of an action can be reflected by the probability of it leading to a correct answer. This can be formulated as the following soft estimation with K rounds of outcome supervision [11]:\n$C_{soft}(a) = \\frac{\\sum_{k=1}^K [\\hat{a}_k \\text{ is correct}]}{K}$ (3)\nwhere ak denotes the final answer predicted in the k-th round and [] is the Iverson bracket. Consequently, we can evaluate all the actions of the target model and map them back to the capability set, to evaluate the mathematical capabilities of the target model thoroughly."}, {"title": "4. Experiment", "content": "4.1. Setup\nBaselines. Our experiments utilize two open-source MLLMs, including LLaVA-Llama3-8B [14] and EMOVA-8B [4]. We fine-tune only the parameters of their language models and projectors with learning rates of 2e-5 and 2e-6, respectively, and a batch size of 128. We select nine cutting-edge MLLMs for comparison, including OpenAI's o1 [23], 4o [21], and 4v [22], as well as LLava-NeXT-34B [13], InternLM-XComposer2 [41], Qwen-VL-Plus [2], LLaVA-7B [14], G-LLaVA-7B [5], and MAVIS-7B [41].\nDatasets. For LLaVA-Llama3-8B, we use LLaVA-665k [14] for supervised fine-tuning (SFT) as a baseline. Additionally, in LLaVA w/. Formatted and EMOVA w/. Formatted, we transfer the source data of AtomMATH into an aligned CoT format for incremental training, ensuring a fair comparison without introducing bells and whistles. For EMOVA-8B, we downsampled its publicly available SFT data [4] to obtain a basic post-training dataset containing about 200k samples. For models with AtomThink, the AMATH-SFT dataset introduced in Section 3.1, is incorporated to introduce atomic reasoning capabilities.\nEvaluation Setting. We evaluated the performance of our method on MathVista [18], a publicly available benchmark encompassing both general-targeted and mathematics-targeted domains. Additionally, to assess the model's ability to interpret mathematical graphs, we use a more challenging multimodal benchmark, MathVerse [43] for further evaluation. It contains five categories including Text Lite (TL), Text Dominant (TD), Vision Intensive (VI), Vision Dominant (VD), Vision Only (VO).\nOut evaluations include four inference settings, including Direct, CoT, Quick Think, and Slow Think. In the Direct setting, we prompt the model to generate a concise final answer. In CoT, the model is instructed to answer the question through step-by-step reasoning. For the Direct and CoT evaluations, we use prompts from Imms-eval [10, 40]. Our AtomThink-models support two additional settings: Quick Think and Slow Think. In Quick Think, our models follow a single, atomic reasoning path based purely on their learned policies, without employing any supplementary search strategies. In Slow Think, enhanced by the PRM, we utilize beam search with beam width of 2 and temperature of 1.0, encouraging our models to engage in more extensive reasoning."}, {"title": "4.2. Main Results", "content": "Comparison with existing MLLMs. In Table 3, our AtomThink framework is applied to train LLaVA-Llama3-8B and EMOVA-8B, yielding consistent performance improvements over the original models. When combined with PRM, AtomThink-EMOVA achieves a new state-of-the-art on MathVerse, surpassing GPT-40 and narrowing the gap between MLLMs and human performance. On MathVista, it also achieves performance close to that of GPT-40. These results demonstrate the framework's strong generalization capability and practical usability.\nQuick Think with Intuition. Unlike traditional COT methods, Quick Think generates a stepwise reasoning path through multi-turn conversations, bypassing the need for an additional verifier. This approach offers a computational advantage over Slow Think and highlights the model's intuitive reasoning capabilities. For LLaVA-Llama3-8B, our AtomThink framework surpasses the baseline model, achieving approximately a 10% improvement on MathVista [18] and a 19% improvement on MathVerse [43]. For AtomThink-EMOVA, Quick Think achieved a score of 38.3% on MathVerse, outperforming existing open-source MLLMs. These results demonstrate that when a model possesses atomic reasoning capabilities, it can leverage rapid intuition to perform more accurate mathematical reasoning.\nLLM Effectively Supervise Visual Reasoning Processes. Previous work has shown that process supervision reward models are effective in evaluating intermediate reasoning steps, though these methods have been primarily applied within the domain of language models. We fine-tuned an LLM with A-MATH-PRM and applied it for test-time scaling. As shown in the table, AtomThink-EMOVA, when utilizing PRM with beam search, achieved an additional 2% improvement on MathVista [18] compared to Quick Think. In MathVerse [43], it even outperformed the closed-source model GPT-4V by 1%. Additionally, increasing test-time scaling in LLAVA resulted in substantial improvements, positioning it well above its sibling model, LLaVA-1.5-13B.\nWe find that even when the reasoning process heavily relies on visual dominant inputs, our models can avoid taking incorrect paths by improving text decoding. On the one hand, it is attributed to the AtomThink training process, which encourages MLLM to first understand image before reasoning. On the other hand, it also confirms the effectiveness of test-time extension in multimodal tasks.\nTrade-off between General and Math Ability. Similar to the conclusions reported in o1, we observe that MLLMS become weaker on general tasks that rely on world knowledge during deep contemplation, demonstrating a trade-off between higher-level reasoning and direct thinking. For instance, LLaVA-Llama3-8B presents a decline in accuracy of 7% compared to the baseline on the general subset of MathVista, while EMOVA experiences a 17% reduction. However, after applying PRM-based action search, both models are able to narrow this generalization gap and improve accuracies by 4% and 16%, respectively."}, {"title": "4.3. Atomic Ability Analysis", "content": "We first cluster the reasoning behaviors of GPT-40 into a set of capabilities S, including Approximation, Verification, Calculation, Variable Definition, Geometric Reasoning, Conclusion Drawing, Graphs Analysis, Equation Formulation, Image Description, Knowledge Introduction, Information Extraction, and Formula Derivation. Using queries in MathVerse [43], we constructed 500 current states as si with high-quality responses generated by GPT-40. Subsequently, soft estimations of atomic actions are mapped to S for analysis.\nAbility Analysis. Figure 3 illustrates the distribution of atomic behaviors and capability differences between LLAVA-llama3-8b and EMOVA-8B with their AtomThink-versions. The analysis reveals that AtomThink-Model generally outperforms baseline across most abilities, demonstrating higher scores in areas such as Image Description and Verification. It suggests that our model is capable of more accurate analysis of visual information and demonstrates a degree of self-checking and reflective capability."}, {"title": "4.4. Comparison with g1", "content": "In Figure 5, we compare AtomThink with the state-of-the-art open-source inference strategy, g1\u00b9, which employs dynamic prompting to make model focus on single step reflection. In GPT-40, direct application of g1 for multi-turn reasoning yields a greater improvement over Chain-of-Thought, particularly in numeric and geometric tasks. However, due to the reliance on the inherent reasoning capabilities of large-scale language models, its performance significantly degrades on smaller models such as EMOVA-8B and LLaVA-Llama3-8B. In contrast, our AtomThink framework consistently enhances the performance of these MLLMs."}, {"title": "Limitation", "content": "Due to the limitations in computing infrastructure, we are unable to validate our method on larger MLLMs. Additionally, despite undergoing small-scale manual review, our dataset still lacks step-level golden answers, which may introduce noise into training."}, {"title": "5. Conclusion", "content": "This paper introduces atom thinking capabilities to MLLMS for solving visual mathematics problems. We release a high-quality, human-free annotated long-CoT dataset, AtomMATH, consisting of 157k atomic reasoning steps and 130k corresponding annotations. Furthermore, we propose AtomThink, a novel framework that focuses on the quality of atomic steps. The experimental results demonstrate that our method consistently enhances the model's diverse behaviors during the problem-solving process, leading to improved reasoning performance across various multimodal mathematical tasks. This work paves the way for developing generalized slow-thinking models."}]}