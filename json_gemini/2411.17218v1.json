{"title": "GraphSubDetector: Time Series Subsequence Anomaly Detection via Density-Aware Adaptive Graph Neural Network", "authors": ["Weiqi Chen", "Zhiqiang Zhou", "Qingsong Wen", "Liang Sun"], "abstract": "Time series subsequence anomaly detection is an important task in a large variety of real-world applications ranging from health monitoring to AIOps, and is challenging due to the following reasons: 1) how to effectively learn complex dynamics and dependencies in time series; 2) diverse and complicated anomalous subsequences as well as the inherent variance and noise of normal patterns; 3) how to determine the proper subsequence length for effective detection, which is a required parameter for many existing algorithms. In this paper, we present a novel approach to subsequence anomaly detection, namely GraphSubDetector. First, it adaptively learns the appropriate subsequence length with a length selection mechanism that highlights the characteristics of both normal and anomalous patterns. Second, we propose a density-aware adaptive graph neural network (DAGNN), which can generate further robust representations against variance of normal data for anomaly detection by message passing between subsequences. The experimental results demonstrate the effectiveness of the proposed algorithm, which achieves superior performance on multiple time series anomaly benchmark datasets compared to state-of-the-art algorithms.", "sections": [{"title": "1 Introduction", "content": "Detecting anomalies in time series data has a wide variety of practical applications [33], such as tracing patients' biosignals for disease detection [7], monitoring operational data of cloud infrastructure for malfunction location [40], finding risks in IoT sensing time series [9], etc. It has received a great amount of research interest in the literature [4, 12, 14-16, 29, 36]. Most time series anomaly detection (TSAD) algorithms try to locate anomalies at each point of the time series (namely point-wise TSAD). However, this formulation fails to consider temporal relationships of anomalous points, as anomalies can go beyond occurring point by point but tend to exist consecutively over a time interval in many real-world scenarios. For example, some demand patterns of the electric power system change during the holidays. Figure 1 shows a comparison of point-wise anomalies and subsequence anomalies. In this paper, we investigate TSAD from a subsequence perspective by identifying anomalous patterns in a time interval, which is called time series subsequence anomaly detection. Generally speaking, a subsequence anomaly is a sequence of observations that deviates considerably from some concept of normality. The somewhat \"vague\u201d definition itself also hints the challenges of the subsequence anomaly detection problem.\nEarly research on anomaly detection relies mainly on shallow representations [5, 28, 31]. Later, Deep-SVDD [24] enhances the representation learning capability using neural networks. Recently, TSAD methods [6, 29] based on Deep-SVDD are prevailing due to their excellent performance. They introduce a suitable neural architecture for modeling time series and detecting anomalies by computing the distance between a target and its corresponding reference in the latent representation space, where the reference represents normal patterns. The main challenge is that these deep models are difficult to enforce assumptions of anomalies, and typically require large training datasets to achieve accurate results. In contrast, time series discord [14, 16, 20, 38] is another popular category of distance-based TSAD methods. Discords are subsequences that are maximally different from all others in the time series, where the difference is measured via zero-normalized Euclidean distance. The most appealing merit of discords is that anomalies can be discovered by merely examining the test data without a training phase. Despite (or perhaps because of their extremely simple assumptions, discords are competitive with deep learning methods.\nHowever, several important limitations still prevent them from broader applications. First, they fail to detect similar anomalous patterns recurring at least twice, as each occurrence will be the others' nearest neighbor. Second, they rely on an explicit distance measure (z-normalized Euclidean distance), which cannot account for diversified anomalies flexibly and adaptively.\nWe summarize three major challenges in developing an effective and robust subsequence anomaly detector as follows.\nCapturing the temporal dependency from time series data. A distinguishing feature of time series is the complex underlying dynamics and temporal dependencies. A temporal anomaly highly depends on the \"context\", which should incorporate temporal dynamics and dependencies into consideration. Thus, how to learn and utilize temporal dependency is a key challenge in time series subsequence anomaly detection.\nAppropriate subsequence length selection. Another key challenge is how to determine the appropriate subsequence length in subsequence anomaly detection. Here we emphasize the importance of the appropriate length which highlights the normal or anomalous pattern of a subsequence. On the one hand, the duration of anomalies varies. For example, if we use a large window to detect spikes and dips in Figure 1, the anomalies might be \"hidden\" in the normal data. While for a long-term anomaly, a short window cannot depict the full picture of it. On the other hand, even if we have a prior anomaly length, it is still necessary to intelligently infer a suitable length according to data characteristics, as illustrated in Figure 2. This problem becomes even worse when there are multiple abnormal subsequences with different lengths in one series. To the best of our knowledge, most existing methods utilize a predefined window length to detect anomalies and thus cannot intelligently detect anomalous subsequences with different lengths and characteristics.\nRobust representations against inherent normal variance. Different time series data arising from different applications may exhibit different variances and noises. Thus, normal patterns can be flexible and volatile which hinders effectively and robustly distinguishing anomalies from normal data. Even though data transformation or representation learning in different detection algorithms have been proposed and some are resistant to noise to some extent, an inductive bias of variance removal of normal data is still necessary to be incorporated.\nIn this paper, we devote to solving the aforementioned challenges in TSAD with the proposed GraphSubDetector, a graph neural network-based subsequence anomaly detection approach. Here we highlight our main contributions as follows:\n1) In order to intelligently learn subsequence representations with proper length, we introduce a temporal convolution network"}, {"title": "2 Related Work", "content": "Subsequence anomaly detection in time series remains a challenging problem in the time series community. In this section, we briefly introduce some state-of-the-art algorithms for subsequence time series anomaly detection, including discord based methods, one-class based methods, reconstruction based methods, and other methods."}, {"title": "2.1 Anomaly Detection based on Discords", "content": "Subsequence anomaly detection can be formulated by finding discords in time series, where the discords are defined as the subsequences maximally different from other subsequences in the same time series [14]. In [36], it is defined as the subsequence with the highest distance to its nearest neighbors. The distances can be calculated directly on the original signal [36], or the representations like wavelets [11]. Many methods in this category have been proposed, like Matrix Profile [38] and its latest variants [16, 20]. Matrix Profile turns out to be an efficient solution for easy settings due to its efficient distance computation implementation. One limitation of the Matrix Profile is the challenging parameter tuning. For example, its performance degrades significantly when the window size is not properly set. Although it can easily capture the most different anomaly, it fails to detect similar anomalies which recur multiple times as illustrated in previous examples. This limitation can be mitigated to some extent by taking k-th nearest neighbor into consideration [37], but how to adaptively select an appropriate value of k still remains a challenge."}, {"title": "2.2 Anomaly Detection based on One-Class\nClassifier", "content": "One-class approaches are commonly used in anomaly detection since the majority of the data is usually normal in anomaly detection. The One-Class Support-Vector-Machine ([28], OC-SVM) and Support Vector Data Description ([31], SVDD) are two well-known methods in this category, which use a hyperplane and hypersphere to separate the normal data from anomalous data in the kernel-induced feature space, respectively. These methods can also be extended to time series anomaly detection [17]. However, their performances are often not satisfied when facing complex and high-dimensional datasets, due to the curse of dimensionality and non-efficient computational scalability. To overcome these limitations, Deep Support Vector Data Description ([24], DeepSVDD) is proposed to jointly train a deep neural network and optimize a data-enclosing hypersphere in output space. Inspired by Deep SVDD, THOC [29] extends this deep one-class model by considering multiple spheres from all intermediate layers of a dilated recurrent neural network to extract multi-scale temporal features for better performance. Furthermore, Neural Contextual Anomaly Detection ([6], NCAD) incorporates the idea of Hypersphere Classifier (HSC) which improves the Deep SVDD by utilizing known anomalies data under the (semi-)supervised setting [25], as well as a window-based approach specified for time series anomaly detection. Another popular one-class classifier based method is the Deep Autoencoding Gaussian Mixture Model ([42], DAGMM) which integrates a deep autoencoder with the Gaussian mixture model (GMM) to generate a low-dimensional representation to capture the normal data pattern. However, directly applying DAGMM in time series may not lead to improved outlier detection performance as it does not properly model temporal dependencies."}, {"title": "2.3 Anomaly Detection based on Reconstruction", "content": "The reconstruction-based models usually learn a representation of the time series in latent space, and then reconstruct the signal from that representation. The anomalies are determined based on the reconstruction error. The long short-term memory based variational autoencoder ([23], LSTM-VAE) adopts serially connected LSTM and VAE layers to obtain the latent representation, estimates the expected distribution from the representation, and detects an anomaly when the log-likelihood of current observation given the expected distribution is lower than a threshold. OmniAnomaly [30] designs a stochastic recurrent neural network with state space models and normalizing flows for multivariate time series anomaly detection. AnoGAN [27] proposes a deep generative adversarial network (GAN) to model data distribution and estimate their probabilities in latent space for anomaly detection. The Multi-Scale Convolutional Recursive Encoder Decoder ([39], MSCRED) uses a convolutional encoder and convolutional LSTM network to capture the inter-series and temporal patterns, respectively, and adopts a convolutional decoder to reconstruct the input time series for anomaly detection."}, {"title": "2.4 Other Anomaly Detection Methods", "content": "The rest of the anomaly detection methods roughly include density-based, transformer-based, and graph-based schemes. The Local Outlier Factor ([5], LOF) is a classic density-estimation method that assigns each object a degree of being an outlier depending on how isolated the object is with respect to the surrounding neighborhood. Anomaly Transformer [35] proposes a new anomaly-attention mechanism to replace the original attention module and compute the association discrepancy, which can amplify the normal-abnormal distinguishability in time series under a minimax strategy to facilitate anomaly detection. Series2Graph [4] transforms time series subsequences into a lower-dimensional space and constructs a directed cyclic graph, where the graph's edges represent the transitions between groups of subsequences and can be utilized for anomaly detection. MTAD-GAT [41] designs two graph attention layers for learning the dependencies of time series in both temporal and feature dimensions, and then jointly optimizes a forecasting-based model and a reconstruction-based model for better anomaly detection results. While the existing graph-based methods achieve good results in some scenarios, they still have difficulty modeling and adapting to the challenging variable-length subsequences anomalies."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Notations and Problem Definition", "content": "A length $T$ univariate time series is denoted as $x = (x_1, x_2,...,x_T)$, where $x_t$ is a real-valued observation at time step $t$. A subsequence $X_{t,L} = (x_t, x_{t+1},..., x_{t+L-1})$ is a series of continuous observations from time series $x$ starting at time step $t$ with length $L$, where $1<t\\leq T-L+ 1$. The problem of subsequence anomaly detection is to output the anomaly score $s(x_{t,L})$ for each subsequence which should be high for anomalous data and low for the normal. Then one can sort the scores in descending order to detect anomalies.\nUsing the sliding window strategy with a stride $t$, a time series can be split into an ordered set of subsequences $X \\in \\mathbb{R}^{N\\times L}$, where $N$ is the number of subsequences and $L$ is the subsequence length. We denote the $i$-th subsequence $x_{(i-1)t+1,L}$ as $X_i \\in \\mathbb{R}^L$. We assume that the initial maximum subsequence length $L$ is set to be large enough to support detecting various anomalies, which can be typically determined based on domain knowledge. For instance, $L$ can be four times of the period length for some periodic time series, e.g., the weather time series."}, {"title": "3.2 Distance-based Anomaly Detection", "content": "Distance-based anomaly detection methods usually calculate the distance between a target sample (or subsequence) and its reference in explicit data space or latent representation space as the anomaly score. We now provide details on a prevailing part of them.\nTime Series Discords. Time series discords compute anomaly score of $X_i$ as\n$s(X_i) = ||z-norm(X_i) - X_i^{(k-NN)}||_2$, (1)\nwhere $z-norm(x) = (x - mean(x)) /std(x)$ returns zero-normalized subsequence with $mean()$ and $std()$ standing for mean and standard deviation of input subsequence, and the corresponding reference $X_i^{(k-NN)}$ is the k-th nearest neighbor\u00b9 of $X_i$. Despite its effectiveness and wide usage, it has several limitations as we discussed in Section 1.\nDeep Support Vector Data Description. The SVDD and OC-SVM rely on a proper kernel to map data features to a high dimensional space for data separation. And the DeepSVDD algorithm replaces the kernel-induced feature space in the SVDD method with the feature space learned in a deep neural network. Specifically, DeepSVDD [24] is an unsupervised anomaly detection method that solves the following optimization problem\n$\\min \\frac{1}{2} \\sum ||NN(X_i) - c||^2$, (2)\nwhere DeepSVDD calculates the latent distance $||NN(X_i) - c||^2$ as anomaly scores, $NN(\\cdot)$ is a deep neural network, and $c$ is a global reference which is the center of all the training data.\nHypersphere Classifier. The Hypersphere Classifier ([25], HSC) extends DeepSVDD by training the network with the binary cross entropy objective function, which extends the model training with labeled anomalies. In particular, the HSC loss is given by\n$-(1-y_i) \\log l(NN(X_i)) - y_i \\log(1 - l(NN(X_i)))$, (3)\nwhere $y_i \\in \\{0, 1\\}$ with 0 for the normal and 1 for the anomalous, and $l: \\mathbb{R}^d \\rightarrow (0,1)$ maps the representation to a probabilistic prediction. Choosing $l(z) = \\exp(-\\|z\\|^2)$ would reduce Equation (3) to DeepSVDD objective with center $c = 0$ when all labels are 0, i.e., all training samples are normal."}, {"title": "3.3 Graph Neural Networks", "content": "Graph neural networks (GNNs) [34] operate on a graph $G = (V, E, A)$, where $V = [N] := \\{1,..., N\\}$ is the set of node indices, $E$ is the set of edges, and $A \\in \\mathbb{R}^{N\\times N}$ is the weighted adjacency matrix representing the nodes' proximity with $A_{ij} > 0$ denoting a directed edge $(i, j)$ exists and $A_{ij} = 0$ otherwise. Let $H\\in \\mathbb{R}^{N\\times d}$ be node representations, in which the $i$-th row $H_i \\in \\mathbb{R}^d$ is the vectorized representation attached to node $i$. A GNN takes the node representations $H$ along with the graph $G$ as input and returns updated node representations $H' \\in \\mathbb{R}^{N\\times d'}$ using a message passing paradigm. A simplified GNN used in this work is defined as follows:\n$H' = \\rho(\\hat{D}^{-1}A H W_1 + H W_2 + b)$, (4)\nwhere $W_1, W_2 \\in \\mathbb{R}^{d\\times d'}$ are weight matrices that transform neighbor node features and target node features, respectively, $b \\in \\mathbb{R}^{d'}$ is the bias vector, $D \\in \\mathbb{R}^{N\\times N}$ is the diagonal degree matrix of $A$, i.e., $D_{ii} = \\sum_j A_{ij}$, $\\hat{D}^{-1}A$ represents the normalized adjacency matrix, and $\\rho$ is a nonlinear activation function. A layer of GNN can aggregate information of 1-hop neighbors for each node. By stacking multiple layers, the receptive neighborhood range can be expanded."}, {"title": "4 Methodology", "content": "In this section, we describe GraphSubDetector, a graph neural network-based subsequence anomaly detection approach. Figure 3 shows the proposed architecture. A time series is first split into subsequences using sliding window. Then a multi-length feature encoder generates subsequence representations from a multi-length view and a length selection mechanism selects proper subsequence length. Moreover, subsequence proximities are into a prior graph structure using heuristics of time series discords, based on which, the message passing adjacency matrix is learned considering temporal information and distance between subsequences in both data and latent space, and then refine it using local density information. By performing graph neural network on the learned graph, the variance of representations of normal data can be reduced, while the discrepancy between normal and anomalous data is retained. Thus, the final generated representations can facilitate anomaly detection with superior performance."}, {"title": "4.1 Learning Representations of Subsequences\nwith Appropriate Lengths", "content": "While selecting a proper subsequence length that highlights the characteristics of both normal and anomalous patterns is important for TSAD, most of the existing algorithms commonly set the subsequence length through trial and error or by some prior knowledge (e.g., the period length for a periodic time series). A potential modification is to perform anomaly detection with multiple subsequence lengths and vote for anomalies, while it is still not robust since anomaly scores of different lengths are hard to align. Thus, in order to learn expressive subsequence representations with proper length, we propose a temporal convolution network (TCN) [2] based multi-length feature encoder aggregating information at different length resolutions as well as a learnable length selection mechanism to generate representations of proper length."}, {"title": "4.2 Robust Representation Learning via\nDensity-Aware Graph Neural Networks", "content": "As aforementioned, the inherent variance and noise in normal data always prevent detectors from effectively and robustly distinguishing anomalies from normal patterns, hence we propose to alleviate it by performing message passing between subsequences as noise removal. Below, we theoretically verify that, with properly-designed adjacency matrix, the discrepancy between anomalous and normal data can be retained while the variance of normal data is reduced, which makes anomaly prominent in data distribution."}, {"title": "4.3 Model Learning", "content": "Anomaly Injection. As we mentioned above, the labeled anomaly data are rare for training in most situations. Here the anomaly injection method is used to inject several labeled artificial anomalous subsequences into the time series. We consider six different types of anomalies, including spike & dip, resizing, warping, noise injection, left-side right, and upside down. In more detail, spike & dip stands for standard point anomaly with unreasonable high or low value, resizing randomly changes the temporal resolution of subsequence, warping denotes random time warping and noise injection for random noise injected, while the latter two stand for reversement of subsequence from left to right and up to down, respectively. Despite the fact that such artificial anomalies may not coincide with real anomalies, it is beneficial for learning characteristic representations of normal behaviors.\nNearest Neighbour Contextual Anomaly Detection. Following the distance-based anomaly detection methods, our approach identifies anomalies by comparing the target representation with its neighboring representations, and the anomaly score is defined as:\n$s(X_i) = \\frac{1}{|\\mathcal{N}_i|} \\sum_{j \\in \\mathcal{N}_i} ||H_i - H'_j||$, (9)\nwhere $H_i$ and $H'_j$ are node representations generated from DAGNN. $s(X_i)$ returns the mean of Euclidean distances between the target and its neighbors as anomaly scores. By incorporating artificially labeled anomalies, we can derive a loss function $\\mathcal{L}$ following the Hypersphere Classifier (HSC) [25] objective:\n$\\mathcal{L} = 1 -(1- y_i) s(X_i) - y_i \\log(1 - \\exp(-s(X_i)))$, (10)\nwhere $s(X_i)$ is the anomaly score of subsequence $X_i$ (see Equation (9)), and $y_i \\in \\{0, 1\\}$ with 1 for (artificial) anomalous subsequences and 0 for normal and unlabeled subsequences. Furthermore, in the scenarios with both normal and unlabeled data in the training set, we actually use a weighted version of HSC objective, where normal data is assigned with a larger weight than unlabeled data, as if a known normal sample gets a high anomaly score, it needs to be penalized more.\nRegularizations. We introduce two regularizations for stable learning, including auto-encoding regularization and length selection embedding regularization.\nAuto-encoding regularization. We introduce an auxiliary MLP decoder to reconstruct the original subsequence $X_i$ using the output of GNN $H_i$. The auto-encoding regularization is defined as\n$\\mathcal{L}_{dec} = \\frac{1}{N} \\sum_{i=1}^N MSE(X_i, Dec(H_i))$, (11)\nwhere $MSE()$ denotes the mean squared error loss and $Dec()$ is the decoder. Suppose that artificial anomalies are not involved, i.e., $y_i = 0$ for all training samples, the network might corrupt to map all samples to a constant representation and achieve the optimal. This regularization is utilized to restrict the network to preserve the information of input data to avoid trivial solutions. Even with anomaly injection, it can still conduce to the model's robustness.\nLength selection embedding regularization. We impose a restriction that the length selection embeddings of adjacent nodes should be close, which is defined as\n$\\mathcal{L}_{len} = \\frac{1}{|E|} \\sum_{A_{prior_{ij}}=1} ||W_i - W_j||$, (12)\nwhere $W \\in \\mathbb{R}^{N\\times P}$, the row vector $W_i \\in \\mathbb{R}^P$, $\\forall i \\in \\{1,..., N\\}$ is the length selection embedding of each node, and $|E|$ is the cardinality of the edge set, i.e., the total number of edges.\nTraining Procedure. As two components need to be optimized, i.e., model parameters and length selection embedding $W$, we propose a training strategy with two phases operating alternately towards better control of joint optimization\n$\\theta \\leftarrow \\arg \\min_{\\theta} \\mathcal{L} + \\lambda \\mathcal{L}_{dec}, W \\leftarrow \\arg \\min_W \\mathcal{L} + \\mu \\mathcal{L}_{len}$, (13)\nwhere $\\lambda > 0$ and $\\mu > 0$ are hyperparameters. The two phases above alternate during model training."}, {"title": "5 Experiments", "content": "In this section, we compare the performance of our approach with other methods on multiple benchmark datasets, conduct case studies to analyze the model's behavior, evaluate the sensitivity of model parameters, and investigate model variations in ablation studies."}, {"title": "5.1 Datasets and Evaluation Metrics", "content": "Datasets. We adopt the following eight annotated real and synthetic datasets from various scenarios to conduct comprehensive evaluations. 1) UCR2: the well-known subsequence anomaly dataset from \"KDD cup 2021 multi-dataset time series anomaly detection\" competition, consisting of 250 univariate time series from different domains with one subsequence anomaly per series. The lengths of the series vary from 4000 to one million. 2) UCR-Aug\u00b3: since UCR contains one anomaly per time series, we augment UCR by adding various types of subsequence anomalies with variable-length into each time series which is more consistent with most real-world scenarios. 3) SED [1]: from the NASA Rotary Dynamics Laboratory, records disk revolutions measured over several runs (3K rpm speed). 4) IOPS4 [22]: is a dataset with performance indicators that reflect the scale, quality of web services, and health status of a machine. 5) ECG [19]: is a standard electrocardiogram dataset and the anomalies represent ventricular premature contractions. Long series MBA (14046) is split to 47 series. 6) SMAP and 7) MSL5: Soil Moisture Active Passive satellite (SMAP) and Mars Science Laboratory rover (MSL), two datasets published by NASA [13], with 55 and 27 series, respectively. The lengths of the time series vary from 300 to 8500 observations. 8) SMD: Server Machine Dataset [30], a 5 weeks long dataset with 38-dimensional time series each collected from a different machine in large internet companies.\nEvaluation Metrics. For the first five subsequence anomaly datasets, we choose AUC, VUS [21] and Recall@k as evaluation metrics. AUC stands for the area under receiver operating characteristic (ROC) curves. VUS [21] is a recently proposed new metric better for subsequence anomaly detection, which stands for volume under the surface of ROC. Recall@k is the recall rate for the number of anomalous subsequences found in the top-kn anomaly scored ones where n is the total number of anomalous subsequences in one time series, i.e., for each anomaly, we have k reporting opportunities considering the cost of check is limited in reality. For the last three"}, {"title": "5.2 Implementation Details", "content": "We set the length of an indivisible segment $\\Delta = 0.125m$ for periodic time series with period length $m$, and $\\Delta = 10$ for non-periodic time series. We utilize a sliding window with a stride $\\tau = 2\\Delta$ to generate subsequences. For the multi-length view of subsequences, we set the maximum length scale $P = 5$, and thus the length can vary from 0.125 to 4 times of period length for periodic time series, which can stand for most common anomalies.\nHyperparameters. For hyperparameter tuning, we randomly select 8 time series to perform grid search. We use this inferred set of hyperparameters for all datasets. The hyperparameter settings are summarized in Table 1.\nModel Training. We run the model 5 times on each benchmark dataset and report the average score. We train the model with a Tesla V100 32GB GPU. For all datasets, we train GraphSubDetector for 10 epochs with a learning rate of $1 \\times 10^{-4}$ for model parameters and $5 \\times 10^{-4}$ for length selection embeddings."}, {"title": "5.3 Experiment Comparisons", "content": "We first compare GraphSubDetector with anomaly detectors that are specified for subsequence anomaly detection using subsequence anomaly datasets, including Matrix Profile [38] (a time series discord search method), Series2Graph [4], LOF [5], and NCAD [6]. Table 2 reports the performance of different algorithms on UCR and UCR-Aug datasets. It can be seen that our GraphSubDetector outperforms Matrix Profile (the second best method) by a reasonable margin on UCR dataset and a large margin on UCR-Aug dataset. This is mainly because UCR has one anomaly per time series, to which time series discords are naturally applicable. While each time series in UCR-Aug is more challenging with multiple anomalies and some of them are similar in anomalous patterns. Furthermore, we compare GraphSubDetector with more classical anomaly detection baselines from different perspectives, including principal component analysis (PCA), AutoEncoder [26], NORMA [3], IForest and LSTM-AD [18], on total five subsequence anomaly datasets. As shown in Table 7, we use AUC and VUS as they do not rely on a user-defined score threshold to predict normal or anomalous labels, especially VUS would give a more robust evaluation on subsequence anomalies. We observe that GraphSubDetector gives the best performance on most datasets. On the UCR,UCR-Aug, and ECG datasets, GraphSubDetector significantly outperforms all other baselines.\nWe also compare GraphSubDetector against the state-of-the-art deep learning models, including DeepSVDD [24], OmniAnomaly [30], THOC [29], NCAD [6], GTA [8], and GDN [10], on SMAP, MSL, and SMD datasets (mainly containing point-wise anomalies) in Table 4. Note that those deep learning methods are optimized for detecting point-wise anomalies, which is not our main concern. However, GraphSubDetector still remains highly competitive. GraphSubDetector only has a slightly lower F1 score than NCAD on one dataset (MSL), while it outperforms all baselines on the other two datasets (SMAP and SMD)."}, {"title": "5.4 Ablation Studies", "content": "To better understand the effectiveness of each component in GraphSubDetector, we perform ablation studies on UCR and UCR-Aug datasets. We summarize the performance of different model variations in Table 5, where 1) the variation \"w/o graph\" denotes the representations from TCN feature encoder and length selection mechanism are used for anomaly detection without message passing; 2) the variation \"w/o adaptive graph\" denotes the learned adaptive adjacency matrix is not used in message passing, while the $A_{prior}$ is used; 3) the variation \"w/o density information\u201d denotes the learned adaptive adjacency matrix is not refined with density information, and the $A$ is used instead of $\\hat{A}$; 4) the variation \"w/o length selection\" denotes multi-length representations are averaged before input to DAGNN, i.e., different lengths are treated equally; 5) the variation \"fixed length\" denotes that we do not use the multi-length representations but select a fixed subsequence length (which is set to be the period length for periodic time series and a constant 160 for the non-periodic).\nResults in Table 5 demonstrate the advantages of each component in GraphSubDetector. It is important to mention that the performance of the variation \"w/o graph\" which does not perform message passing degrades a lot, suggesting the significance of robust representation learning via graph neural networks."}, {"title": "5.5 Case Studies and Visualization", "content": "Here we conduct typical case studies and visualization to investigate how well GraphSubDetector addresses issues in subsequence anomaly detection problems, including handling recurring anomalies with similar patterns and selecting proper subsequence length. \nFigure 5 illustrates a heartbeat time series in UCR-Aug. There are 5 subsequence anomalies (marked in grey), where Anomaly 3 and 4 share a similar anomalous pattern. For Matrix Profile, we set a fixed period length for its prior input length of subsequence anomaly. Matrix Profile finds 1 of 5 anomalies in its top-5 scored subsequences and it cannot deal with recurring anomalies, while GraphSubDetector finds 4 of 5 anomalies. It can be seen that the GraphSubDetector algorithm detects recurring anomalous subsequences much better than the Matrix Profile algorithm. Furthermore, in GraphSubDetector, we mark the length with the maximum weight learned by the selection mechanism. It shows that this mechanism can adaptively learn different subsequence lengths for better anomaly detection. Therefore, we can see that the existing subsequence anomaly detection algorithm like Matrix Profile needs to set a fixed length prior for subsequence anomaly detection, which can hardly account for anomalies of different types. In contrast, the length selection mechanism of the GraphSubDetector selects appropriate subsequence lengths to achieve better detection performance."}, {"title": "5.6 Model Complexity and Efficiency", "content": "In this subsection, we discuss the complexity of GraphSubDetector. In the architecture of GraphSubDetector, kNN-graph learning is adopted. The graph structure in the learning phase is fixed as we initialize it with time series discord as prior, and only top-k important edges are preserved for each node. As a result, the complexity of learning the adjacency matrix is $O(kN)$ (where $N$ is the number of nodes), which is linear. Meanwhile, the scalability of GraphSubDetector is also evaluated and plotted in Figure 6. Since the time performance of GraphSubDetector is independent of the data quality or any other inputs except the number of subsequences, our experiment uses fixed periodicity and stride parameters, thus the number of subsequences is proportional to the data length. From Figure 6, we can see that the processing time of GraphSubDetector roughly increases linearly with respect to the data length, and the processing time is less than 45 seconds for time series with length 300k. Therefore, GraphSubDetector is efficient and applicable for time series subsequence anomaly detection in most real-world scenarios.\nMoreover, we compare the running time of GraphSubDectector and existing time series anomaly detection algorithms in Table 6. As different methods work on different devices (CPU or GPU), here we unrigorously report the running time of different methods which worked on a time series of length 34.3k. Deep models run on Nvidia Tesla-V100-32G, and non-deep models run on a 2.3GHz@12cores CPU. It can be seen that GraphSubDectector is faster than previous works. We also investigate the different key modules in GraphSubDectector for running time. It can be seen that the two modules only introduce a small amount of computational overhead, as the length selection mechanism only additionally uses multiple pooling and dot product operations and the DAGNN additionally uses vector sum operation. These two modules of GraphSubDectector are important in realistic applications, as they are designed for complicated anomalies with variable types and lengths which are rare but significant."}, {"title": "5.7 Density-Aware GNN", "content": "Figure 7 plots the subsequence representations of GraphSubDetector by t-SNE [32] to demonstrate the benefits of the density-aware GNN (DAGNN) component in GraphSubDetector. Results in Figure 7(a) are without GNN, results in Figure 7(b) use adjacency matrix corresponding to Theorem 4.1 and results in Figure 7(c) correspond to Theorem 4.2. The normal representations of adaptive GNN output have more concentration than the model without GNN, and the proposed DAGNN can generate more compact normal representations which we call noise removal. Besides, the anomalies are relatively distant to the normal samples with DAGNN. The results are consistent with Theorem 4.1 and 4.2. Thus, representations corresponding to normal and abnormal samples using DAGNN in GraphSubDetector are more discriminative, eventually facilitating anomaly detection and improving performance."}, {"title": "5.8 Performance Variance", "content": "In this part, we present standard deviations of GraphSubDetector's results and report the comparison with the best baseline (NormA) as shown in Table 7 of Section 5.3. We repeat the experiments 5 times with different random seeds. It can be seen that GraphSubDetector has low variance with stable performance."}, {"title": "5.9 Hyperparameter Sensitivity", "content": "Our proposed GraphSubDetector involves a few parameters and model specifications. We use ECG dataset to examine the hyperparameter sensitivity, including the impact of hidden dimensions d, the number of neighbors K, the initial maximum subsequence length T, and the number of GNN layers. The results are plotted in Figure 8. As for the hyperparameters $\\lambda$ and $\\mu$ in Eq. 13, we run sensitivity experiments on UCR and SMD datasets for examples and report the numerical results in Table 8.\nIt can"}]}