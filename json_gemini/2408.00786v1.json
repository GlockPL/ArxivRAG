{"title": "Whether to trust: the ML leap of faith", "authors": ["Tory Frame", "George Stothart", "Elizabeth Coulthard", "Julian Padget"], "abstract": "Human trust is critical for trustworthy AI adoption. Trust is commonly understood\nas an attitude, but we cannot accurately measure this, nor manage it. We conflate\ntrust in the overall system, ML, and ML's component parts; so most users do not\nunderstand the leap of faith they take when they trust ML. Current efforts to build\ntrust explain ML's process, which can be hard for non-ML experts to comprehend\nbecause it is complex, and explanations are unrelated to their own (unarticulated)\nmental models. We propose an innovative way of directly building intrinsic trust in\nML, by discerning and measuring the Leap of Faith (LoF) taken when a user trusts\nML. Our LoF matrix identifies where an ML model aligns to a user's own mental\nmodel. This match is rigorously yet practically identified by feeding the user's data\nand objective function both into an ML model and an expert-validated rules-based\nAI model, a verified point of reference that can be tested a priori against a user's\nown mental model. The LoF matrix visually contrasts the models' outputs, so the\nremaining ML-reasoning leap of faith can be discerned. Our proposed trust metrics\nmeasure for the first time whether users demonstrate trust through their actions,\nand we link deserved trust to outcomes. Our contribution is significant because\nit enables empirical assessment and management of ML trust drivers, to support\ntrustworthy ML adoption. Our approach is illustrated with a long-term high-stakes\nfield study: a 3-month pilot of a sleep-improvement system with embedded AI.", "sections": [{"title": "1 Introduction", "content": "Lack of trust is often cited as a barrier to artificial intelligence (AI) adoption, especially in machine\nlearning (ML). Trust has always been a critical enabler of new technology adoption: people tend\nto rely on automation they trust, and shun automation they distrust, in the real world [1] and the\nlaboratory [2]. However, ML faces some unique challenges, which are discussed in the context of the\npsychology, management, and computer-science literatures.\nTrust is commonly understood as a unitary concept but, in practice, it is complex. Lee and Sees'\nhuman-centred definition is most commonly accepted: the attitude that the technology will help us\nachieve our goals [3] in risky circumstances [4], i.e. in an uncertain and vulnerable situation [5].\nInitial trust will vary for a technology because we each have a different general tendency to trust due\nto factors like our culture or personality (\u2018dispositional' trust); and a different ability to deal with the\nsituation and the technology ('situational' trust) [6]. Once we experience the specific technology, we\ncan develop 'learned' trust. Most of the discussion of trust in the literature relates to cognitive or\nrational drivers of which the user is aware; but trust can also be emotional [7] and/ or have automatic\nor unconscious drivers, like impulses from learned associations and innate biases [8]. Lee and Sees'\n'performance'/ 'process'/ 'purpose'framework [3] encapsulates most conscious drivers, but misses\nautomatic ones and does not illuminate emotional ones. They argue potential for trust in technology is\nhighest when: the trustee perceives \u2018performance' as strong; the trustee understands how the 'process'\noperates [9]; and the trustee believes its 'purpose' is aligned to their own goals. This framework is\nnow used to reflect on the trust challenges faced by ML and tools to manage trust.\nML should, in theory, garner higher trust when performance is better than alternatives. For example,\na deep-learning model outperformed a team of 6 radiologists by a statistically-significant 11.5% in a\n27,367-woman UK and US breast-cancer study, measured by relative area under the ROC curve [10].\nHowever, if these technologists tested model output against what they expected given the inputs, they\nwould disagree on outputs where the model outperformed them: false positives and negatives were\n3.9-15.1% (UK-US) higher than the model. Despite this ML model's performance, 4 years later, the\nNHS in the UK does not yet routinely use ML for breast-cancer screening, with trials still ongoing.\nThe Explainable AI (XAI) movement has been motivated by a desire to improve our understanding\nof the 'process' of how ML models' algorithms work [11]. However, because ML works by finding\nthe best fit for mathematical models with large data sets [12], it can be hard to follow its reasoning\nprocess. Tools like feature importance or Shapley causal quasi values [13] reflect the factors that\ncontribute to an ML model's conclusion a posteriori [14], not its reasoning. If we understand a\nreasoning process and it matches our prior knowledge of sensible reasoning, we can directly build\nintrinsic trust; if not, only extrinsic trust is possible: we need external reassurance from an expert or\nan evaluation [4]. XAI's efforts will only increase intrinsic trust if experts can explicitly compare\nthe ML model's reasoning to their own mental model [4]. This is problematic because current\nexplanations do not always fit recipient mental models (their internal representation of how the world\nworks [15]). For example, clinicians can think in terms of evidence-based mechanism of action [16].\nMoreover, each human-in-the-loop may have a unique a priori mental model, which we only start\nto discover when ML outputs do not align with their implicit expectations. When time to resolve\ndiscrepancies is limited, intrinsic trust will not be established. Human systems of trust in high-stakes\nenvironments can rely on a trusted senior colleague pressure-testing a recommendation. Similarly, an\nexpert human-in-the-loop can test whether a rules-based AI model reflects their mental model, and\nbuild intrinsic trust: they can interrogate the logic, testing whether it has been correctly implemented.\nWhen ML outperforms humans, especially on large data sets that are cognitively challenging to\nprocess, there should always be a mismatch between the model's reasoning and expert priors, because\nnew relationships are identified. It is thus not possible to directly build intrinsic trust in ML with\nexperts through a collaborative process [17]. Trust can, at best, be extrinsic, relying on external\nassurances, so even experts require a leap of faith to act: from what they expect to what the model\noutputs. This leap can be challenging for experts with high self-confidence in their own abilities: they\ncan ignore ML advice even though they know doing so lowers their performance [18].\nHowever, understanding an ML model's process requires comprehension of more than the algorithm:\ndata and objective function are also critical. These could be understandable, which may require\nvisualisation and education. An interpretable text feature space is: phrased in everyday language\navoiding codes ('readable'); worded so they are quick and simple to absorb ('human-worded'); reliant\non real-world concepts ('understandable') [19]. However, this approach does not include target\nvariables, objective function, nor data visualisation, which can aid user when data sets are large.\nIf a technology's purpose is clearly communicated, it is generally trusted to do what it was built to\ndo [3], although there could be concerns about how well it might do so. Because humans do not\ndirectly control how an ML algorithm reaches a conclusion, ML represents a fundamental shift in\nagency between humans and technology [20]; especially if outcomes are automated. If intent is\nnot communicated, lack of agency could exacerbate concerns about purpose alignment, as could\nmore general and/ or multi-agent ML: it will be harder to disentangle how its purpose relates to an\nindividual's own goals, or to trust it will make appropriate trade-offs, given other priorities.\nMost of the literature on AI trust-management tools relates to rational drivers, namely performance,\nprocess and purpose. Glikson et al. explore tools with more emotional drivers: embodiment and"}, {"title": "2 Methods", "content": "The study was high stakes because it impacted health [32], and the scale of changes were substantial.\nMost participants were short sleepers \u2013 i.e. they slept less than 7 hours a night \u2013 which is associated\nwith poor health outcomes, e.g. 1.2-1.4x higher Alzheimer's disease risk [33]. Immune response\nsuffers, so short sleepers are 5x more susceptible to infections like the common cold [34]. Reaction\ntimes [35] decline, similar to alcohol intoxication [36]. The main treatment for those who are not\nchronic insomniacs is 'sleep hygiene': up to 16 behavioural and environmental suggestions (e.g.\navoid bright light in the evening). While these interventions have a medium effect in healthy-adult\nsleep trials [37], research has been subjective and failed to take into account individual sensitivity,\neven though it varies greatly: e.g. one person's sensitivity to evening light can be 40x that of another\nperson [38]. Slow-Wave Sleep (SWS) declines as we age [39] and clears beta amyloid plaques, linked\nto Alzheimer's disease, from the brain [40]. There has been limited research into the impact of these\npractices on healthy-adult SWS. The situation was thus uncertain and participants vulnerable, so they\ntook a risk when they followed a recommendation.\nThe sleep-improvement system was built in collaboration with the target audience: 35 healthy 40-\n55-year-olds no chronic insomnia, obstructive sleep apnoea, severe anxiety or depression \u2013 and\ntested in a 10-person pilot. Figure 2 illustrates our socio-technical system [41], i.e. it includes\nboth machines and humans. Smart devices collect data; AI models process data; a user interface\ndelivers a user's recommendations and tracking through their smartphone. A human is responsible\nfor data collection and validation; objective function specification; and recommendation selection and\nsubsequent implementation. The AI is neuro-symbolic, combining ML models with a rules-based\nmodel: Neuro||Symbolic, extending the notation of Kautz [42], signifying that the models work in\nparallel, producing two recommendations for the user. We believe that this parallel design represents\na new form of neuro-symbolics, in addition to the five technical designs identified by Kautz."}, {"title": "3 Proposed architecture", "content": "Model-architecture functional compartmentalisation enables user oversight over data and objective\nfunction, increasing human agency. We use an expert-validated rules-based AI model to create a\nverified point of reference that can be tested a priori by ML model users. Our LoF matrix visually\ncontrasts this reference standard to our ML model's output, using the same data and objective function.\nAreas of agreement can be readily trusted and what is unique to the ML model can be identified and\naddressed. We propose three simple objective metrics to measure demonstrated trust, distinguishing\nintention-setting and follow-through, and whether trust was deserved."}, {"title": "3.1 Enabling human agency and oversight", "content": "Model-architecture functional compartmentalisation is used to separate data and objective function\nfrom model reasoning. Each user was responsible for pre-validating their own data. This exercise\nuses text and data visualisation techniques, extending the text-driven interpretable-feature-space\napproach of Zytek et al. [19] to include target variables and the addition of a visual dimension, to\nmake the data more accessible to users who benefit cognitively from visual processing of data.\nValidation started with data quality \u2013 percentage of each day with complete data for each device\nwhich the system reported daily and weekly against a pre-briefed format and target (70%). Participants\nprogressed to baseline, the next phase of the study, if their one-week trial data was over target.\nParticipants then assessed whether the data collected during their trial week reflected their perception.\nEach feature input was provided in a text and visual format that had been reviewed by sleep experts\nand by 35 participants to ensure each was readable, human-worded and understandable, e.g.: 'You\nconsume 152mg of caffeine a day (0-286mg range)', and the chart on the left-hand side of Figure 3.\nParticipants were educated on how to read each chart, what the units meant, and how they were\ncalculated. Participants were encouraged to challenge data accuracy. Some data required considerable\ninterrogation and education because the data and/ or the units were unfamiliar, especially if they were\nasleep when the data was recorded (e.g. temperature and illumination, as shown in Figure 4).\nParticipants trialed different behaviours for unfamiliar items to see how the charts responded. Some\nchanged how an input was recorded if the issue was a diary-input mistake. These follow-ups continued\nuntil participants were comfortable their data was accurate.\nParticipants then reviewed at least 1 month of complete, quality days of baseline data, to confirm target\nvariables and feature inputs were accurate. This included text and charts with sleep-expert-validated\nlogic from the rules-based model, e.g.: 'You had 330mg of caffeine/ day (blue line). You were over\n230mg, the amount which impacts the average person's sleep, on 82% of days (yellow); over NHS's\n400mg recommended max on 11% of days (red).' and the chart on the right-hand side of Figure 3."}, {"title": "3.2 Discerning the ML leap of faith", "content": "Each participant had unique sleeping challenges (e.g. waking in the night or too early or having\ntrouble sleeping at night) and objectives: some already achieved well over 7 hours' sleep and just\nwanted to improve SWS; some only got 6 hours of sleep and wanted to focus on TST. Sleep objectives\nwere set after the 1-week trial and confirmed once they validated their baseline data and were educated\non what was realistic. They thus set model objective function, increasing their agency.\nParticipants were given the opportunity to remove any baseline data that they did not believe was\naccurate, but none did. This is an example of demonstrating trust in the data component of an\nML model, and of a non-expert being able to build intrinsic trust in part of a complex ML model,\nbecause they are able to understand the data inputs and objective function, albeit with support. Data\nvalidation, with custom question responses, demonstrated personalisation, an immediacy behaviour,\nand increased model trustworthiness by ensuring correct inputs."}, {"title": "3.3 Measuring the ML leap of faith", "content": "Intervention choice, compliance, and sleep outcomes were recorded, enabling three relative-trust\nmetrics to be assessed: DIRTI, DAFTI, and DOTI. We illustrate each using pilot data; see Figure 7\nfor how these metrics relate to the types of trust discussed in the literature.\nDemonstrated Intention Relative Trust Index (DIRTI) measures how users demonstrate trust in ML\nthrough their intentions. Using two study participants on the left-hand side of Figure 8 as an example,\nD chose 1 action with an ML score of above zero on their LoF horizontal axis; all other choices only\nscored highly on the rules axis. D's average rules score was 2 (pale blue) and ML 0.5 (dark blue).\nMost of B's choices scored high on the ML axis; some scored lower on rules, so they averaged 2.5 on\nML and 2.25 on rules. B placed more relative trust in ML than D: B's DIRTI, calculated by dividing\ntheir ML score by their rules score, is 1.1; D's 0.25, as shown on the right-hand side of Figure 8.\nAlthough all participants could have chosen actions with at least equal ML and rules scores, most\nrelied more on rules than on ML when they set intentions; only 2 had DIRTI >1.\nDemonstrating trust requires action, not just intention, which is what Demonstrated Action Follow-\nthrough Index (DAFTI) measures. Some interventions required major changes to participants' daily\nroutines. One commented 'I don't need a sleep coach; I need a life coach' when planning how to\nfollow through on their intentions. For example, as illustrated in Figure 9: B made a big increase\nin their daily exposure in intervention compared to baseline, in red, and shifted a lot of it into the\nafternoon. Figure 10 shows participant follow-through, comparing the last 7 days of intervention to\nbaseline. The last 7 days was used because most participants had to work towards target behaviour in\nstages over the 4 weeks. The proliferation of blue indicates most showed some follow-through. D\nwas very successful on caffeine; B on natural light. Some, in red, failed to follow through: D on 2;\nB's and E's bedroom temperatures increased, but they made other changes to cool their bodies down.\nThe DAFTI metric reflects action follow-through by discarding an intervention score if a participant\nfails to improve on baseline. For example, D acted on their caffeine intention, but failed to implement\na relaxation routine, their only intervention with a positive ML score. As shown in Figure 11, D's\naverage ML score therefore dropped to 0, and their rules score dropped to 1.1 (light blue) because\nthey also failed to follow through on their natural-light intention. Their relative demonstrated trust in\nML (DAFTI), dividing their ML score by their rules score, was 0. B followed through on all but one\naction, and their relative trust increased to 1.2. Three participants now showed more trust in ML than\nrules (DAFTI>1): F did a great job on evening light, a high-scoring ML intervention, and not so well\non re-calibrating natural light levels, a high-scoring rules intervention.\nOur ML models were designed to be trustworthy, so the outstanding deserved-trust question is whether\nthere is a relationship between the models' prioritisation of recommendations and the outcomes\ndelivered, i.e. did participants who implemented high-scoring interventions get better outcomes?\nWas the risk worth it? The Deserving Of Trust index (DOTI) measures whether the trust was worth\nit, reflecting the constraints faced once the participant acted. There will be a placebo effect at play,\nwhich can be considerable, but there is no reason to assume it would not align to demonstrated trust.\nDemonstrated trust is shown on an absolute basis on the left of Figure 12 \u2013 i.e. the correlation (r2)\nbetween each individual's average ML or rules score and their percentage increase in TST and SWS.\nWe compare baseline to the last seven days of intervention because most participants worked towards\ntarget behaviour in stages over four weeks of intervention. In this pilot, the strongest relationship was\nfor ML-interventions at follow-through (dark blue): TST r2 increases from 0.58 at intention to 0.76;\nSWS from 0.22 to 0.54. An increase is consistent with a trustworthy model because recommendations\nonly work if people act on them. Relative DOTI, the ratio of the absolute ML score and the rules score,\nis shown on the right. Trust was more deserved in the ML algorithm for TST than for SWS, which\nmay have been influenced by lower SWS model accuracy. At follow-through, our ML model deserved\nmore trust than the rules-based one; had only intention been considered, the SWS rules-based model\nwould have (incorrectly) been seen to deserve more trust."}, {"title": "4 Discussion", "content": "Contribution The proposed architecture allows the ML leap of faith to be discerned and measured,\nand therefore to be managed over time. Experimental design lessons include:\nMore human agency and control is afforded by data validation and objective-function specification,\nenabled by separating data and objective function from model reasoning and data visualisation. Users\nbuild intrinsic trust by pre-validating data in an accessible format, and deciding the model objective.\nAn explicit reference standard, created through a collectively-developed rules-based model, enables\nexperts to test it against their mental model a priori. They should do so until they intrinsically trust it,\nso it becomes a verified point of reference. Industry or regulatory bodies could maintain the standard\nas critical infrastructure \u2013 e.g. for assurance audits or to train up new experts. Maintenance should\ninclude regularly stress testing it to keep assumptions and use cases up-to-date, and verify ongoing\ntrust. Many domains already use rules-based models, which should evolve into a reference standard.\nA LoF matrix visually and accessibly contrasts the output of the reference model with the ML model's\na posteriori output, using the same data and objective function, thus bringing the ML leap of faith into\nsharp focus. This provides real transparency to model users: they should be inclined to trust where\nmodels agree, and understand a leap of faith is only required where they disagree. Experts can stress\ntest the areas of disagreement, updating the rules-based model with new insights or to capture drift,\nthus narrowing the leap of faith as an aspect of operations management. Different ML algorithms will\nrequire different leaps of faith, because a priori levels of expert insight differ, as do model accuracies.\nBy being explicit about the ML leap of faith, we can assess demonstrated and deserved trust. For the\nfirst time, our metrics measure trust in terms of actions and outcomes. If trust is deserved, so relative\nDOTI increases over time; DIRTI and DAFTI should also increase. These simple metrics allow trust\ncomparisons across models, and flag the need for investigation if they trend down. Industry regulators\ncould track companies' ML-model-trust metrics, especially DOTI, to understand trends. Continuous\naudit processes using these metrics would preserve ML algorithm IP whilst building public assurance.\nLimitations The matrix and metrics may be perceived as insufficiently technical. They are accessi-\nble by design, so they can be used and understood by experts, businesses, and regulators as well as\ncomputer/ data scientists. They need to be applied at scale to reach empirical conclusions about trust\nfor different users/domains/ situations, and what drives it, objectively. The proposed metrics do not\nmeasure attitudinal trust or model trustworthiness. Instead they measure how attitude is reflected\nin decisions, actions, and outcomes, which drive ML adoption. Because our proposed metrics only\nmeasure intentions, actions and outcomes, they could be applied to an untrustworthy system and used\nto encourage adoption. If GDPR (or similar) is not followed, trust metrics could be inappropriately\nsold and/ or used to discriminate against people based on their general or specific trust inclination.\nParticipant time for data validation can be substantial if data does not readily align with perception.\nOur approach may be most suitable for professionals who use model recommendations often, so their\ntime commitment pays off as they focus on where there is a leap of faith to be overcome. The time\nrequired to develop a reference standard from scratch can be considerable, requiring: substantial\nprogrammer subject-matter education; individuals who can translate expert knowledge into rules; and\ntime from experts on each iteration, and to test the resulting model for different use cases. However,\nexperts may resist allocating this time to ML, especially if they have a high opinion of their own\ncapability and/or are concerned about their roles being replaced or diminished. The data-validation\napproach is not universally accessible: our participants had at least one university degree and\nunderstood complex, novel concepts and images. Work is needed to evaluate accessibility for other\nlevels of educational attainment. Prioritisation, unlike direct regression/ classification comparisons,\nrequires calibration if it is to be included in a LoF matrix: different approaches should be tested.\nConclusion The proposed architecture is a more practical, rigorous way of building trust in an ML\nmodel than trying to explain it to people who have a different (probably unarticulated) mental model.\nCritical novel architecture features include: user data validation and objective-function specification;\nan explicit reference standard; a LoF matrix; and demonstrated- and deserved-trust metrics."}]}