{"title": "Whether to trust: the ML leap of faith", "authors": ["Tory Frame", "Elizabeth Coulthard", "George Stothart", "Julian Padget"], "abstract": "Human trust is critical for trustworthy AI adoption. Trust is commonly understood\nas an attitude, but we cannot accurately measure this, nor manage it. We conflate\ntrust in the overall system, ML, and ML's component parts; so most users do not\nunderstand the leap of faith they take when they trust ML. Current efforts to build\ntrust explain ML's process, which can be hard for non-ML experts to comprehend\nbecause it is complex, and explanations are unrelated to their own (unarticulated)\nmental models. We propose an innovative way of directly building intrinsic trust in\nML, by discerning and measuring the Leap of Faith (LoF) taken when a user trusts\nML. Our LoF matrix identifies where an ML model aligns to a user's own mental\nmodel. This match is rigorously yet practically identified by feeding the user's data\nand objective function both into an ML model and an expert-validated rules-based\nAI model, a verified point of reference that can be tested a priori against a user's\nown mental model. The LoF matrix visually contrasts the models' outputs, so the\nremaining ML-reasoning leap of faith can be discerned. Our proposed trust metrics\nmeasure for the first time whether users demonstrate trust through their actions,\nand we link deserved trust to outcomes. Our contribution is significant because\nit enables empirical assessment and management of ML trust drivers, to support\ntrustworthy ML adoption. Our approach is illustrated with a long-term high-stakes\nfield study: a 3-month pilot of a sleep-improvement system with embedded AI.", "sections": [{"title": "1 Introduction", "content": "Lack of trust is often cited as a barrier to artificial intelligence (AI) adoption, especially in machine\nlearning (ML). Trust has always been a critical enabler of new technology adoption: people tend\nto rely on automation they trust, and shun automation they distrust, in the real world [1] and the\nlaboratory [2]. However, ML faces some unique challenges, which are discussed in the context of the\npsychology, management, and computer-science literatures.\nTrust is commonly understood as a unitary concept but, in practice, it is complex. Lee and Sees'\nhuman-centred definition is most commonly accepted: the attitude that the technology will help us\nachieve our goals [3] in risky circumstances [4], i.e. in an uncertain and vulnerable situation [5].\nInitial trust will vary for a technology because we each have a different general tendency to trust due\nto factors like our culture or personality (\u2018dispositional' trust); and a different ability to deal with the\nsituation and the technology ('situational' trust) [6]. Once we experience the specific technology, we\ncan develop 'learned' trust. Most of the discussion of trust in the literature relates to cognitive or\nrational drivers of which the user is aware; but trust can also be emotional [7] and/ or have automatic\nor unconscious drivers, like impulses from learned associations and innate biases [8]. Lee and Sees'\n'performance'/ 'process'/ 'purpose'framework [3] encapsulates most conscious drivers, but misses\nautomatic ones and does not illuminate emotional ones. They argue potential for trust in technology is\nhighest when: the trustee perceives \u2018performance' as strong; the trustee understands how the 'process'\noperates [9]; and the trustee believes its 'purpose' is aligned to their own goals. This framework is\nnow used to reflect on the trust challenges faced by ML and tools to manage trust.\nML should, in theory, garner higher trust when performance is better than alternatives. For example,\na deep-learning model outperformed a team of 6 radiologists by a statistically-significant 11.5% in a\n27,367-woman UK and US breast-cancer study, measured by relative area under the ROC curve [10].\nHowever, if these technologists tested model output against what they expected given the inputs, they\nwould disagree on outputs where the model outperformed them: false positives and negatives were\n3.9-15.1% (UK-US) higher than the model. Despite this ML model's performance, 4 years later, the\nNHS in the UK does not yet routinely use ML for breast-cancer screening, with trials still ongoing.\nThe Explainable AI (XAI) movement has been motivated by a desire to improve our understanding\nof the 'process' of how ML models' algorithms work [11]. However, because ML works by finding\nthe best fit for mathematical models with large data sets [12], it can be hard to follow its reasoning\nprocess. Tools like feature importance or Shapley causal quasi values [13] reflect the factors that\ncontribute to an ML model's conclusion a posteriori [14], not its reasoning. If we understand a\nreasoning process and it matches our prior knowledge of sensible reasoning, we can directly build\nintrinsic trust; if not, only extrinsic trust is possible: we need external reassurance from an expert or\nan evaluation [4]. XAI's efforts will only increase intrinsic trust if experts can explicitly compare\nthe ML model's reasoning to their own mental model [4]. This is problematic because current\nexplanations do not always fit recipient mental models (their internal representation of how the world\nworks [15]). For example, clinicians can think in terms of evidence-based mechanism of action [16].\nMoreover, each human-in-the-loop may have a unique a priori mental model, which we only start\nto discover when ML outputs do not align with their implicit expectations. When time to resolve\ndiscrepancies is limited, intrinsic trust will not be established. Human systems of trust in high-stakes\nenvironments can rely on a trusted senior colleague pressure-testing a recommendation. Similarly, an\nexpert human-in-the-loop can test whether a rules-based AI model reflects their mental model, and\nbuild intrinsic trust: they can interrogate the logic, testing whether it has been correctly implemented.\nWhen ML outperforms humans, especially on large data sets that are cognitively challenging to\nprocess, there should always be a mismatch between the model's reasoning and expert priors, because\nnew relationships are identified. It is thus not possible to directly build intrinsic trust in ML with\nexperts through a collaborative process [17]. Trust can, at best, be extrinsic, relying on external\nassurances, so even experts require a leap of faith to act: from what they expect to what the model\noutputs. This leap can be challenging for experts with high self-confidence in their own abilities: they\ncan ignore ML advice even though they know doing so lowers their performance [18].\nHowever, understanding an ML model's process requires comprehension of more than the algorithm:\ndata and objective function are also critical. These could be understandable, which may require\nvisualisation and education. An interpretable text feature space is: phrased in everyday language\navoiding codes ('readable'); worded so they are quick and simple to absorb ('human-worded'); reliant\non real-world concepts ('understandable') [19]. However, this approach does not include target\nvariables, objective function, nor data visualisation, which can aid user when data sets are large.\nIf a technology's purpose is clearly communicated, it is generally trusted to do what it was built to\ndo [3], although there could be concerns about how well it might do so. Because humans do not\ndirectly control how an ML algorithm reaches a conclusion, ML represents a fundamental shift in\nagency between humans and technology [20]; especially if outcomes are automated. If intent is\nnot communicated, lack of agency could exacerbate concerns about purpose alignment, as could\nmore general and/ or multi-agent ML: it will be harder to disentangle how its purpose relates to an\nindividual's own goals, or to trust it will make appropriate trade-offs, given other priorities.\nMost of the literature on AI trust-management tools relates to rational drivers, namely performance,\nprocess and purpose. Glikson et al. explore tools with more emotional drivers: embodiment and"}, {"title": "2 Methods", "content": "The study was high stakes because it impacted health [32], and the scale of changes were substantial.\nMost participants were short sleepers \u2013 i.e. they slept less than 7 hours a night \u2013 which is associated\nwith poor health outcomes, e.g. 1.2-1.4x higher Alzheimer's disease risk [33]. Immune response\nsuffers, so short sleepers are 5x more susceptible to infections like the common cold [34]. Reaction\ntimes [35] decline, similar to alcohol intoxication [36]. The main treatment for those who are not\nchronic insomniacs is 'sleep hygiene': up to 16 behavioural and environmental suggestions (e.g.\navoid bright light in the evening). While these interventions have a medium effect in healthy-adult\nsleep trials [37], research has been subjective and failed to take into account individual sensitivity,\neven though it varies greatly: e.g. one person's sensitivity to evening light can be 40x that of another\nperson [38]. Slow-Wave Sleep (SWS) declines as we age [39] and clears beta amyloid plaques, linked\nto Alzheimer's disease, from the brain [40]. There has been limited research into the impact of these\npractices on healthy-adult SWS. The situation was thus uncertain and participants vulnerable, so they\ntook a risk when they followed a recommendation.\nThe sleep-improvement system was built in collaboration with the target audience: 35 healthy 40-\n55-year-olds no chronic insomnia, obstructive sleep apnoea, severe anxiety or depression \u2013 and\ntested in a 10-person pilot. Figure 2 illustrates our socio-technical system [41], i.e. it includes\nboth machines and humans. Smart devices collect data; AI models process data; a user interface\ndelivers a user's recommendations and tracking through their smartphone. A human is responsible\nfor data collection and validation; objective function specification; and recommendation selection and\nsubsequent implementation. The AI is neuro-symbolic, combining ML models with a rules-based\nmodel: Neuro||Symbolic, extending the notation of Kautz [42], signifying that the models work in\nparallel, producing two recommendations for the user. We believe that this parallel design represents\na new form of neuro-symbolics, in addition to the five technical designs identified by Kautz."}, {"title": "3 Proposed architecture", "content": "Model-architecture functional compartmentalisation enables user oversight over data and objective\nfunction, increasing human agency. We use an expert-validated rules-based AI model to create a\nverified point of reference that can be tested a priori by ML model users. Our LoF matrix visually\ncontrasts this reference standard to our ML model's output, using the same data and objective function.\nAreas of agreement can be readily trusted and what is unique to the ML model can be identified and\naddressed. We propose three simple objective metrics to measure demonstrated trust, distinguishing\nintention-setting and follow-through, and whether trust was deserved.\n3.1 Enabling human agency and oversight\nModel-architecture functional compartmentalisation is used to separate data and objective function\nfrom model reasoning. Each user was responsible for pre-validating their own data. This exercise\nuses text and data visualisation techniques, extending the text-driven interpretable-feature-space\napproach of Zytek et al. [19] to include target variables and the addition of a visual dimension, to\nmake the data more accessible to users who benefit cognitively from visual processing of data."}, {"title": "3.2 Discerning the ML leap of faith", "content": "The system recommendation was positioned and visually depicted as an intervention 'menu' using a\nLoF matrix which, as shown in Figure 5, discerns the leap of faith required to trust the ML algorithms.\nBy using a separate a priori expert-validated rules-based AI model to create a reference standard,\nwe contrast the ML models' prioritisation of which sleep-hygiene changes would make the most\ndifference to the user's sleep objective, with what an expert would conclude from the same inputs.\nThe user is able to see where the models agree, and where they do not.\nDistance from best practice, assessed by the rules-based model, is used to prioritise opportunities\non the vertical axis; ML-model feature importance is used to prioritise them on the horizontal.\nThe further away an individual was on best practice, the more impact changing that factor could\nhave on the individual's sleep. The rules-based model assigned 1 of 3 levels of opportunity to each\nintervention, or a 4th category: 'angel', meaning they had achieved best practice. The 35 design-phase\nparticipants preferred 3 levels of opportunity, associating them with a 'good, better, best' paradigm,\nand appreciated the self-efficacy boost of knowing they were already achieving best practice.\nFour demarcations were also used to categorise ML opportunities using feature importance, measured\nthrough average gain across all splits where a feature was used. The higher a feature's importance,\nthe higher an individual's sensitivity to that feature. Feature-importance thresholds were set so each\nindividual had at least 4 opportunities in the last 2 columns of the LoF matrix, equivalent to 2- and\n3-star rules-based opportunities. The ML model was more discriminating, e.g. double the number of\n3-star opportunities in Figure 5, and 9 'angels', compared to 1 'angel' in the rules-based model."}, {"title": "3.3 Measuring the ML leap of faith", "content": "Intervention choice, compliance, and sleep outcomes were recorded, enabling three relative-trust\nmetrics to be assessed: DIRTI, DAFTI, and DOTI. We illustrate each using pilot data; see Figure 7\nfor how these metrics relate to the types of trust discussed in the literature.\nDemonstrated Intention Relative Trust Index (DIRTI) measures how users demonstrate trust in ML\nthrough their intentions. Using two study participants on the left-hand side of Figure 8 as an example,\nD chose 1 action with an ML score of above zero on their LoF horizontal axis; all other choices only\nscored highly on the rules axis. D's average rules score was 2 (pale blue) and ML 0.5 (dark blue)."}, {"title": "4 Discussion", "content": "Contribution The proposed architecture allows the ML leap of faith to be discerned and measured,\nand therefore to be managed over time. Experimental design lessons include:\nMore human agency and control is afforded by data validation and objective-function specification,\nenabled by separating data and objective function from model reasoning and data visualisation. Users\nbuild intrinsic trust by pre-validating data in an accessible format, and deciding the model objective.\nAn explicit reference standard, created through a collectively-developed rules-based model, enables\nexperts to test it against their mental model a priori. They should do so until they intrinsically trust it,\nso it becomes a verified point of reference. Industry or regulatory bodies could maintain the standard\nas critical infrastructure \u2013 e.g. for assurance audits or to train up new experts. Maintenance should\ninclude regularly stress testing it to keep assumptions and use cases up-to-date, and verify ongoing\ntrust. Many domains already use rules-based models, which should evolve into a reference standard.\nA LoF matrix visually and accessibly contrasts the output of the reference model with the ML model's\na posteriori output, using the same data and objective function, thus bringing the ML leap of faith into\nsharp focus. This provides real transparency to model users: they should be inclined to trust where\nmodels agree, and understand a leap of faith is only required where they disagree. Experts can stress\ntest the areas of disagreement, updating the rules-based model with new insights or to capture drift,\nthus narrowing the leap of faith as an aspect of operations management. Different ML algorithms will\nrequire different leaps of faith, because a priori levels of expert insight differ, as do model accuracies.\nBy being explicit about the ML leap of faith, we can assess demonstrated and deserved trust. For the\nfirst time, our metrics measure trust in terms of actions and outcomes. If trust is deserved, so relative\nDOTI increases over time; DIRTI and DAFTI should also increase. These simple metrics allow trust\ncomparisons across models, and flag the need for investigation if they trend down. Industry regulators\ncould track companies' ML-model-trust metrics, especially DOTI, to understand trends. Continuous\naudit processes using these metrics would preserve ML algorithm IP whilst building public assurance.\nLimitations The matrix and metrics may be perceived as insufficiently technical. They are accessi-\nble by design, so they can be used and understood by experts, businesses, and regulators as well as\ncomputer/ data scientists. They need to be applied at scale to reach empirical conclusions about trust\nfor different users/domains/ situations, and what drives it, objectively. The proposed metrics do not\nmeasure attitudinal trust or model trustworthiness. Instead they measure how attitude is reflected\nin decisions, actions, and outcomes, which drive ML adoption. Because our proposed metrics only\nmeasure intentions, actions and outcomes, they could be applied to an untrustworthy system and used\nto encourage adoption. If GDPR (or similar) is not followed, trust metrics could be inappropriately\nsold and/ or used to discriminate against people based on their general or specific trust inclination.\nParticipant time for data validation can be substantial if data does not readily align with perception.\nOur approach may be most suitable for professionals who use model recommendations often, so their\ntime commitment pays off as they focus on where there is a leap of faith to be overcome. The time\nrequired to develop a reference standard from scratch can be considerable, requiring: substantial\nprogrammer subject-matter education; individuals who can translate expert knowledge into rules; and\ntime from experts on each iteration, and to test the resulting model for different use cases. However,\nexperts may resist allocating this time to ML, especially if they have a high opinion of their own\ncapability and/or are concerned about their roles being replaced or diminished. The data-validation\napproach is not universally accessible: our participants had at least one university degree and\nunderstood complex, novel concepts and images. Work is needed to evaluate accessibility for other\nlevels of educational attainment. Prioritisation, unlike direct regression/ classification comparisons,\nrequires calibration if it is to be included in a LoF matrix: different approaches should be tested.\nConclusion The proposed architecture is a more practical, rigorous way of building trust in an ML\nmodel than trying to explain it to people who have a different (probably unarticulated) mental model.\nCritical novel architecture features include: user data validation and objective-function specification;\nan explicit reference standard; a LoF matrix; and demonstrated- and deserved-trust metrics."}]}