{"title": "HUMAN-ALIGNED CHESS WITH A BIT OF SEARCH", "authors": ["Yiming Zhang", "Athul Paul Jacob", "Vivian Lai", "Daniel Fried", "Daphne Ippolito"], "abstract": "Chess has long been a testbed for AI's quest to match human intelligence, and in recent years, chess AI systems have surpassed the strongest humans at the game. However, these systems are not human-aligned; they are unable to match the skill levels of all human partners or model human-like behaviors beyond piece movement. In this paper, we introduce ALLIE, a chess-playing AI designed to bridge the gap between artificial and human intelligence in this classic game. ALLIE is trained on log sequences of real chess games to model the behaviors of human chess players across the skill spectrum, including non-move behaviors such as pondering times and resignations In offline evaluations, we find that ALLIE exhibits humanlike behavior: it outperforms the existing state-of-the-art in human chess move prediction and \u201cponders\u201d at critical positions. The model learns to reliably assign reward at each game state, which can be used at inference as a reward function in a novel time-adaptive Monte-Carlo tree search (MCTS) procedure, where the amount of search depends on how long humans would think in the same positions. Adaptive search enables remarkable skill calibration; in a large-scale online evaluation against players with ratings from 1000 to 2600 Elo, our adaptive search method leads to a skill gap of only 49 Elo on average, substantially outperforming search-free and standard MCTS baselines. Against grandmaster-level (2500 Elo) opponents, ALLIE with adaptive search exhibits the strength of a fellow grandmaster, all while learning exclusively from humans.", "sections": [{"title": "1 INTRODUCTION", "content": "Computer chess is among the most studied problems in Artificial Intelligence. In the pursuit of stronger chess engines, decades of hardware and algorithmic improvements since the first computer chess programs (Turing, 1948; Shannon, 1950) have led to the development of increasingly strong chess engines (Campbell et al., 2002). Current state-of-the-art systems, such as Stockfish (Romstad et al., 2008) and AlphaZero (Silver et al., 2017) have reached strength far beyond even the best human players. However, these systems are not calibrated to play at levels matching human strength, and they make moves that are inscrutable even to human experts.\nIn this work, we revisit the classic challenge of computer chess, but with a new objective: developing a skill-calibrated and human-aligned chess AI. By skill-calibrated, we mean an system that is evenly matched (i.e., winning approximately 50% of games) against players across the skill spectrum, while maintaining humanlike play. Skill calibration of AI systems is an open research challenge, and could prove valuable for domains requiring superhuman AI systems to collaborate with and be overseen by imperfect human partners. We define human-aligned as whether the model behaves indistinguishably from a human player. This definition extends beyond just move selection: other key aspects, such as time spent pondering a move and the decision to resign in losing positions, are fundamental to how humans play chess. By incorporating these humanlike behaviors, our chess engine ALLIE aims to serve as an engaging and realistic sparring partner for human players.\nOur approach draws upon recent success in language modeling. Large decoder-only Transformer models, when trained on vast text corpora (Radford et al., 2019; Touvron et al., 2023), learn to generate text that is sometimes indistinguishable to human-written content (Dugan et al., 2023)."}, {"title": "2 RELATED WORK", "content": "Most existing approaches in chess engine development have focused on creating the best possible system. Early successful engines like Deep Blue relied on hand-coded rules and extensive search algorithms (Campbell et al., 2002). In contrast, AlphaZero (Silver et al., 2017) used self-play and Monte-Carlo tree search (MCTS) to learn a probability distribution over actions (policy) and estimate game outcomes with a value function. AlphaZero also employed MCTS at inference time to select winning moves. We explore a variation of this MCTS in Section 3.3, using policy and value functions learned directly from human games, and inference time search budget allocated proportional to human ponder time.\nMore recently, McIlroy-Young et al. (2020) introduced 'MAIA', a neural network trained on human chess games rather than through self-play. In contrast with MAIA, we utilize move history, which proves more effective for predicting human behavior, instead of solely relying on the current game state. Jacob et al. (2022) showed that policy and value functions learned from humans can be combined with MCTS to improve policy strength, and we extend upon their work and demonstrate that adaptive search enables ALLIE to almost perfectly match the strengths of human players up to the grandmaster level. By learning value estimates generated by an oracle search engine, Ruoss et al. (2024) showed that neural networks can achieve grandmaster-level performance without inference-time search. Our approach differs in that our networks are supervised on human data alone.\nOur proposed method is inspired by Toshniwal et al. (2022)'s idea of treating chess like a language modeling task. Feng et al. (2023) fine-tuned a language model on chess games, books and commentary and demonstrated that the model can track pieces throughout games and solve chess puzzles, and Karvonen (2024) demonstrated that a language model trained to predict chess moves exhibits emergent understanding of chess concepts. Zhang et al. (2024) similarly showed that a Transformer model trained on human games can be made to play at a higher skill level than the games in its training data by using a low sampling temperature."}, {"title": "3 BUILDING ALLIE, A HUMAN-ALIGNED CHESS MODEL", "content": "Here, we describe how we represent a chess game, and our training and inference methods."}, {"title": "3.1 REPRESENTING A CHESS GAME SEQUENTIALLY", "content": "Vocabulary To apply language modeling techniques to chess, we need a sequential representation of a chess game. To this end, we view a chess game as a sequence of moves. We encode moves"}, {"title": "3.2 TRAINING ALLIE TO MOVE, PONDER AND EVALUATE", "content": "Using a sequential representation of a chess game, we can naturally apply standard sequence modeling techniques to model how human players make moves and when they decide to resign (we treat \"resignation\" as just another move token the model can assign probability to). ALLIE is built using a decoder-only Transformer model (architecture details in Section 4.2) which inputs the game history as a sequence and has three output heads: (1) a policy head $p_{\\theta}$ that outputs a probability distribution over possible next moves, (2) a pondering-time head $t_{\\theta}$ that outputs the number of seconds a human player would take to come up with this move, and (3) a value assessment head $v_{\\theta}$ that outputs a scalar value representing who is expected to win the game. The pondering-time and value assessment heads are crucial for the human-aligned chess play that we aim to capture. The former allows ALLIE to behave like a human, taking more time to make decisions in complex game states than simple ones,"}, {"title": "3.3 POLICY IMPROVEMENT UNDER TIME CONSTRAINTS WITH ADAPTIVE SEARCH", "content": "Virtually all strong chess engines (Romstad et al., 2008; Pascutto & Linscott, 2019) rely on search, a process of exploring possible future moves to pick the best move. Past work has shown that search is crucial for achieving strong gameplay (Silver et al., 2017; Jones, 2021). Since ALLIE produces both policy and value estimators, planning algorithms such as Monte-Carlo tree search (MCTS) (Coulom, 2007) can be applied off-the-shelf for policy improvement. As shown in Figure 1b, MCTS works by rolling out multiple moves into the future, selecting paths that are most likely to lead to a win.\nState-of-the-art search-based chess engines such as AlphaZero use a constant number of rollout steps for each move, leading to them assessing tens of thousands to millions of positions before playing a move. Such large amounts of search are incompatible with our goal of human-alignment; in blitz games, humans frequently makes moves with <1 second of time usage, and it is practically infeasible to search through such a large number of rollouts on consumer hardware in this timeframe. On the other hand, in critical game states where the model predicts a human would spend more time to ponder, it is plausible that running deeper simulations would allow for better modeling of the elevated depth of human reasoning in such positions and improve policy strength.\nTo this end, we propose a time-adaptive MCTS procedure that aligns MCTS with human reasoning: at each position m, we dynamically set the number of rollouts $N_{sim} = [c \\cdot t_{\\theta}(m)]$, where $t_{\\theta}(m)$ is the predicted human pondering-time at the position m and c a constant. Another alternative implementation of time-adaptive MCTS would be to keep searching until a timeout is reached, but we opted against doing this in order to make our implementation independent of hardware efficiency."}, {"title": "4 EXPERIMENTAL SETUP", "content": "In addition to conducting offline evaluation, we deployed the four configurations of ALLIE described as ALLIE-POLICY was"}, {"title": "4.1 DATASET", "content": "We constructed a raw dataset of chess games using all blitz games played in 2022 on Lichess, a popular online chess platform. To address the data's skew toward low-skill-level games, we downsampled the dataset to have roughly equal numbers of games in bins in increments of 100 Elo. From this downsampled dataset, we use 18 thousand games for testing, and the remaining games for training and validation. In total, the training set contains 91 million games and 6.6 billion tokens.\nOur primary automatic evaluation metric is move-matching accuracy-how often does the model correctly predict the next move in the game. Following McIlroy-Young et al. (2020), when eval-uating accuracy, we discard the first 5 moves of each game, which reduces the impact of opening memorization (there are only so many ways to begin a chess game). We further omit from evaluation any moves made under time pressure (when there is less than 30 seconds on the clock) to avoid the influence of random moves made due to being low on time. This leaves us with 884,049 positions"}, {"title": "4.2 MODEL ARCHITECTURE", "content": "Our model uses a standard decoder-only Transformer architecture (Vaswani et al., 2017) with 355M parameters. We initialize model parameters (excluding embeddings) using weights from the pre-trained GPT-2 medium model (Radford et al., 2019), and embeddings are trained from scratch since the vocabulary is not shared with natural language. It may seem surprising that that learned model weights for language modeling are useful for a non-linguistic task like chess, but this transfer technique is shown effective in other domains (Papadimitriou & Jurafsky, 2020; Shen et al., 2023). The value prediction head is followed by a tanh activation layer that squeezes the value prediction to the range [-1,1], with the extreme values corresponding to wins for each of the two players. The model is trained for 2M steps with a global batch size of 131,072 tokens on our training set. This corresponds to roughly 40 epochs over the training data. Additional training details and hyperparameters are provided in Appendix E.1. In Appendix F, we explore the effect of both dataset size and parameter count on model capability. We find that our setting is mostly data-constrained-model performance is limited by the number of human chess games available on the Internet\u2014and doubling model size has only a small effect on the model's ability of predicting human moves."}, {"title": "4.3 BASELINES", "content": "We compare our ALLIE's learned policy against MAIA (McIlroy-Young et al., 2020), which, like ALLIE, is trained on human-games to make next-move predictions. MAIA is a family of nine individual models, each trained on Lichess games from players with Elo ratings in a given range. We refer to these as MAIA-{1100, 1200, . . ., 1900}. The MAIA network architecture is a residual CNN, and their move prediction objective used during training is similar to our approach, but the input representation is board state without full move history information. To unify the different Maia models into a single strong baseline, we define a MAIA* model by adaptively choosing the Maia model with the closest Elo rating to the players' ratings. For example, a 1480-rated game would be evaluated using the Maia-1500 model.\nThe primary comparative metric we use for automatic evaluation is move-matching accuracy: what fraction of the time does the system correctly predict the move a human would have made. Other aspects of human-aligned chess play (e.g., modeling human moves vs. time usage) require different evaluation metrics, which we detail in Section 5. To the best of our knowledge, there are no existing chess engines that model how humans play chess in terms of pondering and resigning, so we do not have a direct comparison with a baseline system for these behaviors.\nThough large language models (LLMs) such as OpenAI's GPT-3.5(-turbo-instruct) have not (to our knowledge) been explicitly trained to play chess, they have been shown to reliably produce humanlike next moves. This is accomplished by prompting the LLM with a textual representation of the game state using PGN notation. Due to dependency on the textual PGN notation, this approach is not compatible with OpenAI's latest chat-based LLMs (e.g., GPT-4), and we report prompts and implementation details in Appendix B. It is difficult to make a fair comparison between ALLIE and GPT-3.5 because on the one hand, GPT-3.5 has many more parameters and potentially observed much more chess data during pre-training. On the other hand, GPT-3.5 was never intended to play chess, and the fact that it can play chess is somewhat remarkable. We report GPT-3.5 results just to provide context on performance achievable by a frontier large language model."}, {"title": "4.4 LARGE-SCALE HUMAN STUDY", "content": "In addition to conducting offline evaluation, we deployed the four configurations of ALLIE described in Table 1 as well as MAIA*, to play blitz games on the website Lichess. ALLIE-POLICY was conditioned to play adaptively at the opponent's strength, and moves were sampled from the model"}, {"title": "5 RESULTS", "content": "To apply inference-time search to ALLIE, we first need to understand if chess is at all learnable from human-generated data (Section 5.1), and if so, how well ALLIE models human gameplay (Section 5.2). We discuss our main results on adaptive MCTS and skill calibration in a large-scale study against human players in Section 5.3."}, {"title": "5.1 DOES ALLIE LEARN THE RULES OF CHESS?", "content": "First, we ask whether the rules of chess are learnable from human-generated chess data. The model produces a softmax distribution over roughly two thousand possible chess moves, and we can test if the model has indeed learned the rules of chess by checking if model assigns high probability to valid moves, and low probability to invalid moves. While we evaluate the model's behavior on actual human games, it is also important to test if the model can generalize to out-of-distribution positions that are rare in human games but are nevertheless valid: a model that has learned the rules of chess should play legal moves in randomly generated games as well. Beyond testing the model behavior in the aggregate, we further examine the model's behavior when special chess rules restricting valid moves (e.g., check) are in effect."}, {"title": "5.2 How WELL DOES ALLIE MODEL HUMAN GAMEPLAY?", "content": "The ideal human-aligned chess bot should behave indistinguishably from a human chess player. A major aspect of humanlikeness is in the moves played: for a given game state, a humanlike chess bot should play the same move as a human would in the same position. Beyond moves played, we argue that it is important to match the time humans ponder their moves before taking them, and resign when appropriate\u2014these are also essential components of how humans play chess.\nMoves. On the Lichess evaluation set, we compare how often ALLIE, GPT-3.5, and the MAIA models play the same moves as humans. Following McIlroy-Young et al. (2020), we consider the move-matching accuracy metric, defined as the fraction of top-1 moves under the model distribution that matches human moves at the same positions. Over the entire test set, the top move produced by ALLIE matches human moves 55.7% of the time, compared to MAIA*'s 51.6% and GPT-3.5's 53.7% (Table 3). Shown in Figure 2, we find that ALLIE matches human moves more accurately than MAIA and GPT-3.5 models across almost the entire skill spectrum. Notably, ALLIE-ADAPTIVE-SEARCH outperforms ALLIE-POLICY at 2300 Elo and above, providing evidence that search is crucial for modeling the behavior of expert-level human players (Jacob et al., 2022).\nWe further report move-matching accuracy of special moves such as castling, en passant, pawn promotion, and threefold repetition in Table 3. ALLIE reaches higher move-matching accuracy than MAIA* for all four types of special moves, and is competitive with GPT-3.5 overall.\nPondering time and resignation. Additional dimensions of human behavior, including pondering time and resignation, are also key aspects in humanlike gameplay. We find a strong correlation between the model's predicted think time and human think time, with Pearson's r = 0.697. This suggests that ALLIE successfully learns to predict when humans do and do not ponder in a position. Figure 3 shows the distribution of ALLIE's predicted think time for different amounts of time spent by humans. There is a clear monotonic relationship, but interestingly ALLIE tends to predict lower pondering times than humans do. This is probably because of the skew in pondering time distribution: the majority of moves in blitz games is played under 5 seconds, and the model is incentivized to \"hedge\" its prediction and output shorter pondering times.\nWe further evaluate whether ALLIE can resign in losing positions like humans. We define resignation as when a special resignation token <resign> is assigned higher likelihood than all valid moves on"}, {"title": "5.3 EVALUATING SKILL CALIBRATION VIA GAMES WITH HUMANS", "content": "Our offline evaluations suggest that ALLIE predicts human behavior well, but to study whether ALLIE could calibrate to strength of human players, we had ALLIE play against real humans at a variety of skill levels. A chess engine that is perfectly skill-calibrated should win 50% of games against players regardless of their skill level. Inspired by the expected calibration error metric (Naeini et al., 2015; Guo et al., 2017), we define a skill calibration error (SCE) metric. Games between the chess engine and humans are first partitioned into equally spaced bins based on skill level (player Elo). For a bin of games B between the evaluated system and human players, we take the absolute difference between the system's estimated performance on the set of games, and the average Elo of the human players as the calibration error:\n$SCE(B) = |SystemElo(B) - HumanElo(B)|$."}, {"title": "6 DISCUSSION", "content": "In this work, we demonstrate a method for training a state-of-the-art chess AI that models how humans play chess: our system ALLIE exhibits remarkable precision in playing humanlike moves, as well as pondering and resigning like humans. Through a time-adaptive Monte-Carlo tree search algorithm, ALLIE can be evenly matched with players from beginner (1100 Elo) to expert level (2500 Elo) with almost no skill gap, by learning chess exclusively from humans without the need of distilling from a strong chess engine. We believe the techniques developed in this paper have broad applicability for other settings where aligning AI models with imperfect human reasoning is crucial, and we look forward to future explorations in other complex settings, such as the alignment and oversight of superhuman AI systems.\nWhile offline evaluation metrics and quantitative analysis of games with real human players reveal ALLIE'S strengths, especially relative to prior approaches, more progress is still necessary to fully realize our goal of a human-aligned chess engine. In qualitative feedback, many players were positive about ALLIE (see Table 5), but several shortcomings were also repeatedly emphasized. Players especially noted ALLIE's propensity toward late-game blunders and that its pondering times were sometimes long in positions where there is only one reasonable move. However, since players all knew they were playing against a bot, it is hard to disentangle their perspectives from this knowledge. For example, contrasting with the qualitative feedback, we empirically observed that move prediction accuracy actually improves as games progress, especially in the last few turns (see Figure 6). For future work, it would be interesting to conduct a proper Turing test, where players do not know whether they are playing against an AI or a human-player of a similar Elo level.\nOur approach relies on pre-training, which is limited by available data: the vast majority of online chess games are played at fast time controls, and therefore it is more challenging to use data-driven methods to model human behavior in slower games. Future work should explore methods to model human reasoning in slower games, where players have more time to think and make more accurate moves, and test the generalization of our approach to different time controls and game formats."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "All code and model checkpoints for ALLIE, including implementation of the adaptive MCTS algo-rithm are made publicly available on GitHub."}, {"title": "E TRAINING AND INFERENCE", "content": "Our MCTS implementation and hyperparameters follow a variant of AlphaZero (Silver et al., 2017) proposed by Grill et al. (2020). A way to view MCTS is KL-regularized policy optimization (Grill et al., 2020): in the limit, MCTS produces an optimized policy \u03c0 that maximizes search Q values with KL regularization towards the model policy $p_{\\theta}$ learned from humans:\n$\\pi = arg max \\sum_{a}Q(s, a)\\pi(s, a) - \\lambda D_{KL}(\\pi || p_{\\theta}) $.\nThis regularization is key to prevent the search from diverging from the model policy (Jacob et al., 2022), and the KL-regularization strength $\\lambda ~ c/\\sqrt{N_{sim}}$, where c is a hyperparameter. In standard MCTS (ALLIE-SEARCH) with fixed number of rollouts, $N_{sim}$ is fixed, and $\\lambda$ is a constant. In adaptive MCTS (ALLIE-ADAPTIVE-SEARCH), we scale c by the square root of the search budget to achieve the same effect of a constant regularization strength. We refer the interested reader to (Silver et al.,"}, {"title": "F ABLATIONS", "content": "To assess the impact of the training, data, and model decisions on ALLIE's capability to play humanlike chess, we conduct ablation studies with the following scenarios:"}]}