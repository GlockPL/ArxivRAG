{"title": "Neural Networks Generalize on Low Complexity Data", "authors": ["Sourav Chatterjee", "Timothy Sudijono"], "abstract": "We show that feedforward neural networks with ReLU activation generalize on low complexity data, suitably defined. Given i.i.d. data generated from a simple programming language, the minimum description length (MDL) feedforward neural network which interpolates the data generalizes with high probability. We define this simple programming language, along with a notion of description length of such networks. We provide several examples on basic computational tasks, such as checking primality of a natural number, and more. For primality testing, our theorem shows the following. Suppose that we draw an i.i.d. sample of \u0398(N\u00ba ln N) numbers uniformly at random from 1 to N, where \u03b4\u03b5 (0,1). For each number xi, let Yi = 1 if xi is a prime and 0 if it is not. Then with high probability, the MDL network fitted to this data accurately answers (with test error \u2264 O(N\u00af)) whether a newly drawn number between 1 and N is a prime or not. Note that the network is not designed to detect primes; minimum description learning discovers a network which does so.", "sections": [{"title": "Introduction", "content": "Understanding why neural networks generalize well on unseen data is an enduring mystery in the field. For many datasets seen in practice, massively overparametrized neural networks are fit to near-zero training error, yet still generalize on test examples. At the same time, many neural network architectures are capable of fitting pure noise [ZBH+21], yet clearly cannot generalize on these datasets. Classical complexity descriptions from statistical learning theory such as VC dimension [BM03, S+98] cannot explain this phenomenon, as VC dimension is distribution-independent. Given this, it is natural to make structural assumptions about the data. For example, in many real-world datasets for which deep learning is deployed (e.g., computer vision or natural language processing), the data has apparent structure with very low levels of noise. In this paper, we prove generalization guarantees on data of low complexity, with zero noise. We introduce a simple programming language and a notion of description length for neural networks. Using these notions, we show that for data generated from a short program in this language, the MDL feedforward neural network interpolating the data has low test error rate with high probability."}, {"title": "Main Results", "content": "To capture the notion of low complexity data, we define simple neural programs (SNPs). SNPs are simple programs which can define variables and manipulate them with basic operations. They consist of a sequence of statements, and intuitively they may be thought of restricted Python programs."}, {"title": "Defining a Programming Language", "content": "A simple neural program (SNP) P consists of a variable context, specifying all the variables in the program, along with any sequence of statements to be described. Examples of the syntax are described below each of the statements. A variable context for P describes the set of variables to be manipulated in P. All variables in the program must be declared in the variable context. It is comprised of a sequence of statements of two types:\n\u2022 Input statements. These statements define a variable which is taken as input into the program, and do not have a defined value at the beginning of the SNP. The syntax is input <variable name>. All variable names are distinct.\n\u2022 Variable initialization statements. All variables need to be either nonnegative integer valued or boolean valued (i.e., encoded by zero or one). In particular, throughout the runtime of the program, all variables are enforced to be nonnegative integer valued. Variables must be initialized with a fixed value. The syntaxes for the two types are int <variable name> = <value> and bool <variable name> = <value>."}, {"title": "A nested representation of simple neural programs.", "content": "A SNP P with a variable context V can be written as a sequence of statements (S\u2081,\u2026\u2026,SL). Another way to describe the program is to enumerate all the top-level statements, those that are not contained within a for loop."}, {"title": "Top-level representation of P", "content": "Given an SNP P = (S1,...,SL) with variable context V, enumerate all top level for loops and their clauses by (Sni, Ci) for some subsequence {ni} indicating the locations of top-level for loops. The top-level representation of P is the unique sequence (01,...,0k) where each 0\u00a1 is either a top-level statement or a for loop clause pair (Sn;, Cj) and for i < j,0\u00bf appears before 0; in the program."}, {"title": "Encoding SNPs by Feedforward Neural Networks", "content": "The fundamental result for our simple neural programming language is that any atomic program can be converted into a fully-connected feedforward neural network with ReLU nonlinearity. This is perhaps unsurprising given the literature outlined in Section 1.2 on the Turing completeness of neural networks, but the encoding of the program by a neural network is efficiently describable in a way we outline in a Section 4. We consider feedforward neural network architectures Fo which are compositions of affine functions and the ReLU nonlinearity \u03c3(x) = max(x,0),\nFo(x) = g_D \u03bf\u03c3o g_{D\u22121} \u03bf \u03c3\u03bf\u2026 \u03bf g_2 \u03bf\u03c3\u03bf g_1(x)\ng_i(x) = W_i x + b_i,\nwhich may be parametrized by its sequence of layer weights and biases \u03b8 = (\u03b81,...,\u03b8D), where \u03b8k = (Wk,bk).\nWe will construct an encoding of SNPs as such networks. Every variable in the program is stored as a unique node in the neural network, and every statement of the simple neural program corresponds to a sequence of consecutive layers in the network. The ordering of the layers of the neural network reflects the ordering of the statements in the simple neural program. The construction will be inductive on the depth of the program; recall that the depth of a program is maximum number of times that for loops are nested within each other. Consider first the case of depth zero programs."}, {"title": "Base case: depth zero SNP conversion", "content": "Consider a depth-zero SNP P = (S1,\u2026\u2026\u2026,SL) with a variable context V of size V, indexed by x = (x1,...,xv). Let P take inputs in [N]\u00b9. Throughout this section, assume that the maximum possible value of a variable during the program is bounded by B := B(N). Each statement Si in the program will be encoded as a composition of layers f_{S_i} = g_{i,k_i} \u25cb g_{i,k_{i-1}} \u25cb\u2026\u25cbg_{i,2}\u25cbg_{i,1}, where gi,l(y) = \u03c3(W^(i,l)y+\nb^(i,l)), and the non-linearity o acts component-wise. Each sequence of layers fs\u2081 is a map from RV into RV. We will occasionally write fs\u2081,B to emphasize the dependence of the parameters on B. The fs; are strung together to act on x, so that the program P corresponds to the neural network\nf_{S_L} \u00b0 f_{S_{L-1}} \u00b0\u00b0 f_{S_1} (x)."}, {"title": "Statement encodings.", "content": "The variable context V of P defines the input vector x of the neural network. All variable declaration statements such as <var type> var = c initialize the component of x corresponding to the variable var with the value c. Input statements define which components are free variables.\n\u2022 Value assignment. To set the ith variable equal to a fixed constant c \u2265 0, we use W = I - e_i e_i^T and b = cei, where ei denotes the column vector whose ith component is 1 and the rest are 0, and e_i^T is the transpose of ei. For setting the ith variable equal to the jth variable, we use W = I - e_i e_i^T + e_j e_i^T and b = 0.\n\u2022 Basic operations. To sum the jth variable with a constant c and assign the output to variable i, we use W = I-e_i e_i^T + e_j e_i^T and b = cei. Similarly, multiplying the jth variable with a nonnegative constant c and assigning the output to variable i can be encoded with W = I - e_i e_i^T + ce_j e_i^T and b = 0.\n\u2022 Unary operations. Consider first the operation which assigns a variable xi the value 1{xj = c} for another variable xj and a constant c \u2265 0. Encoding this statement and similar statements relies on the identity\n1{x = 0} = \u03c3(x + 1) + \u03c3(x \u2212 1) \u22122\u03c3(x)  (1)\nthat holds for all x \u2208 Z. Because xj is an integer,\n1{xj = c} = \u03c3(xj \u2212 c + 1) + \u03c3(xj \u2212 c \u2212 1) \u2212 2\u03c3(xj \u2212 \u0441)."}, {"title": "Inductive step: For loops", "content": "Now, consider encoding a general depth d SNP P with a feedforward network. Suppose P has top-level representation (01,...,0k). Assume again that the runtime values of variables are bounded by B := B(N). Each object 0\u00bf can be mapped to a sequence of neural network layers fo\u2081,B.\n\u2022 If 0; is an SNP statement, fo\u2081,B is the corresponding layer defined in Section 3.1.\n\u2022 If 0; is a for loop with clause C, fo\u2081,B is the sequence of layers described in the remainder of this section.\nFinally, define FP,N to be the composition\nf_{0_k,B} \u00b0 f_{0_{k-1},B}\u2026 \u00b0 \u00a3_{0_1,B}.\nConsider a for loop with clause C, which increments a counter variable xi\nfrom xs to xe. The start and endpoints of the loop may also be constants; the layer constructions\nbelow adapt straightforwardly. By the inductive hypothesis, C is a depth d-1 SNP with variable\ncontext V, so there exists a neural network Fc encoding the program. The prescription involves the\nfollowing sequence of layers:\n1. The first layer (L1) sets the counter variable xi to the specified start xs, and initializes c \u2190 0,\nwhich is not contained inside the variable context V. Storing the variable c as the (V + 1)th\nvariable, (L1) has weight and bias parameters\nW_{r,.} = e_r, b_r = 0 for r = i,\nW_r, b = \\ W_{r,.} = 0, b_r = 0 for r = V + 1,\nW_{r,.} = e_r, b_r = 0 otherwise.\n2. Repeat the following layers B + 1 times:\n(L2, L3) Assign c \u2190 1{xi \u2264 xe}, a binary operator which requires two layers of (output) widths\nV+3 and V + 1.\n(L4) For each variable x \u2208 V\\{xi}, create a temporary node in the neural network xold storing\nthe current value of x. This layer has output width \u2264 2V.\n(Clause) To the variables in V\\{xi}, apply Fc. Recall that the clause may not add variables or\nmodify the loop counters. To the temporary nodes and c, apply the identity transfor-\nmation. Explicitly, suppose W and bare the parameters of any layer in Fc. Supposing\nthe temporary nodes are indexed by the last V variables, create a similar layer in FP,N\nthat has parameters W and 6, given by\nW = [W] = []\n6\n0\nwhere the I is of order V \u00d7 V and the 0 vector in 6 has length V.\n(L5, L6) Using the if construction, update each variable x in V\\{x} by\ncx + (1 \u2212c)xold,\n(2)\nsimultaneously to all variables in V\\{x}. The if transformation creates two temporary\nvariables for every variable to be updated. Hence the layer has width \u2264 4V."}, {"title": "Proof of encoding.", "content": "The following theorem gives a formal proof that our scheme successfully encodes an SNP by a feedforward neural network.\nTheorem 3.1. Let P be an SNP with variable context V = (x1,...,xv), indexed by the statements (S1,...,SL). Let P take in inputs (x1,...,x1) \u2208 [N] and be B := B(N)-bounded. Then for each N, there is a feedforward neural network FP,N with ReLU nonlinearity, which agrees with the program for all inputs in [N]\u00b9. Further, all parameters of the neural network are bounded by B, and all for loop layers in P repeat B + 1 times."}, {"title": "Maximum width of the neural network.", "content": "The construction of Fp has some additional properties which we record here. Firstly, the width of the neural network is controlled by the length of the SNP. Let Wmax(F) be the maximum width of any feedforward neural network F.\nLemma 3.1 (Bounding the maximum width of the neural network). Consider an SNP P with variable context V of size V, length L, taking inputs [N]\u00b9. Then\nWmax(FP,N) \u2264 4V L.\nProof. We prove this statement again by inducting on the nested depth d of the program. The inductive claim will be Wmax(P) \u2264 4V max(1, d). For the base case, consider d = 0 (so that there are no for loops.) Then Wmax(P) \u2264 V + 3 since there are V variables in the program and all non-for loop SNP operations temporarily increase the width of the neural network by at most 3.\nNow, consider any program P with length L and maximum nested depth d \u2265 1, and write its top-level representation as (01,...,0k). If fo denotes the sequence of neural network layers corresponding to 0; in FP,N, then\nWmax(FP,N) = max_k {Wmax(fo_i)}\nIf 0; is an SNP statement, then Wmax(fo\u2084) \u2264 V + 3. Otherwise, consider when 0\u00bf is a for loop with clause Ci. Notice that Ci is also an SNP, where the maximum nested depth is d-1. By the inductive hypothesis, Wmax(Fc\u2084) \u2264 4V max(d \u2013 1,1). By inspecting the for loop construction, we can bound the widths of the layers. Layers L1, L2, L3 have widths at most V + 3; L4, L5, L6 have widths at most 4V. The layers encoding the clause have widths at most V + Wmax(Fc\u2081), since Fc\u2081 is a mapping from RV \u2192 RV. Finally, L7, L8 have widths at most V + 1. As a result,\nWmax(fo\u2081) \u2264 max(V + 3, 4V, V + Wmax(Fc\u2081)) \u2264 4V + Wmax(Ci) (3)\nBy equation (3) and the inductive claim, we conclude that Wmax(P) \u2264 4V + 4V max(d \u2013 1,1) \u2264 4V max(d, 1). To deduce the original claim, note that the maximum nested depth is at most L."}, {"title": "Compressibility of the neural network.", "content": "Secondly, the sequence of layers of the neural network FP,N are compressible, since for loops are encoded by repetitions of the same layers. To explicitly capture this, consider a B-bounded SNP P with a fixed variable context V. We will define its repetition-compressed representation, which will be a string using exponentiation to capture repetition of parameters. For example, if P has a parameter representation \u03b81\u03b82\u03b83\u03b83\u03b82\u03b83\u03b83, we can express this as\n\u03b81(\u03b82(\u03b83)2)2\nwhere the two representations are equal when interpreted as words of the free algebra generated by all possible parameters.\nTo formally define the repetition-compressed representation of P, first note that any SNP statement Si which is not a for loop maps to a sequence of layers fs\u2081,B = gi,l\u2081 \u25cb\u2026\u25cbgi,1. Each layer gi,j is parametrized by its weight matrix and bias vector di,j = (W^(i,j), b^(i,j)). Denote by (fs,,B) the sequence of parameters of the layers comprising fs\u2081,B:\n\u0398(fsi,B) := \u03b8i,1\u03b8i,2... \u03b8i,li,\ninterpreted as a word in the free algebra generated by all possible parameters. The repetition-compressed representation of P, denoted RC(P), is defined inductively as follows.\n1. Base case. Consider any program P = (S1,..., SL) of nested depth 0, so that there are no for loops. Define its compressed representation as\n\u220f_L \u0398(fsi,B),\ni=1\nthe concatenation of \u0398(fs\u2081,B) for all i. This is also the same as \u0398(FP,N).\n2. Inductive step. Now, consider any SNP of nested depth d \u2265 1. Denote the top-level representation of P by the sequence (01,...,0k). If 0\u00bf represents the for loop statement S; with clause C, extend the map \u0398 by\n\u0398(0) = \u03b8\u03af,1(\u03b8i,2\u03b8i,3\u03b8i,4RC(C)\u03b8i,5\u03b8i,6\u03b8i,7) \u0392+1\u03b8i,8\nwhere \u03b8\u017c,. denote the layer parameters in the for loop construction, and RC(C) is the repetition compressed representation, replacing every parameter 0 = (W,b) with its augmented version 0 := (W,b) as in Eq. (2). Finally, define RC(P) to be the concatenation \u220f_k \u0398(0\u2081)."}, {"title": "The layers of FP,N are efficiently describable", "content": "The layers of FP,N are efficiently describable. Consider an SNP P of length L with variable context V, bounded by B := B(N), with inputs in [N]\u00b9. Denote its neural network encoding by FP,N. Let RC(P) denote the repetition-compressed layer representation of the SNP P.\nThen:\n\u2022 RC(P) is equivalent to the sequence of parameters of FP,N.\n\u2022 The number of unique symbols 0 in RC(P) is \u2264 8L.\n\u2022 The number of parenthesis pairs (...)B+1 in RC(P) is equal to the number of for loops in P.\nThe first claim is evident from the induction and for loop construction in Theorem 3.1. When P is a depth zero program, RC(P) is exactly equal to the parameter sequence of Fp,N. In the general case, consider a program P of depth d > 1 with top-level representation (01,...,0k); then \u220f_k \u0398(0\u2081). If 0; is a for loop with clause Ci, (0\u2081) exactly encodes the elements of the for loop construction: (1) the 8 additional layers in the for loop construction, (2) the repetition of layers B + 1 times, and (3) the augmenting of layers corresponding to Ci."}, {"title": "A Measure of Description Length for Neural Networks", "content": "In this section, we introduce a description length measure for the encoded neural network. The measure roughly corresponds to the number of symbols needed to describe the parameters of the neural network. We will use the following alphabet A of symbols:\n1. A symbol I to represent a node which is an input into the neural network.\n2. , to mark the start of a new number.\n3. 0,1 to describe binary expansions of numbers.\n4. (...)*k to describe k fold repetition of a substring of symbols, with k encoded in binary.\n5. Symbols W, B to demarcate the weight matrix and the bias vector of a layer: following the symbols are the values of the weights and biases.\nThere are a finite number of symbols in A given by {I, \u201c,\u201d,0,1, *,W,B, \u201c(\u201d, \u201c)\u201d}. Every feedforward neural network can be converted to a sequence of symbols, by specifying the weights and biases of every layer using the symbols above. Let bin(n) for ne o be the binary expansion of a number."}, {"title": "Full symbol encoding of a neural network", "content": "Given a neural network F of depth d, let (\u03b81,...,\u03b8d) be the sequence of parameters of the layers, which can be rewritten as\n(W(1), 6(1), W(2), 6(2),..., W(d), b(d)).  (4)\nSuppose the input to F is a vector x \u2208 Rn where some coordinates of x may be fixed, and some \u0442\u0430\u0443 be free variables. Define the string S\u2081 by replacing\n1. each weight matrix W in Eq. (4) by its vectorization in binary, prefixed with the W symbol:\nW, bin(W1,1), bin(W1,2), . . ., bin(Wm,n),\nwhere W\u2208 Rmxn, and\n2. each bias vector by the symbol B and its entries encoded in binary, separated by commas:\nB, bin(b\u2081),...,bin(bm),\nassuming b \u2208 Rm.\nSecondly, define the string S2 by replacing all free variables in the input vector x by the symbol I, and the other coordinates by their binary representations. Then, the A-symbol sequence encoding F is the string obtained by concatenating S2 followed by S1."}, {"title": "Main Theorem: Neural Networks Generalize on Data from Short Programs", "content": "The following result is our main theorem. Roughly, it says that the MDL interpolator generalizes on low complexity data.\nTheorem 5.1. Let P be an SNP of length L which outputs a result P(x) for each input x \u2208 [N]\u00b9, with maximum bound B(N) \u2265 2. Let P have V variables, with V > I. Suppose we observe i.i.d. data (Xi, Yi), i = 1,...,n where n = L3V2N\u00ba In B(N) for some d\u2208 (0,1) and where Xi is uniform from [N]\u00b9 and Y\u2081 = P(Xi). Let fmDL be the minimum-description length neural network interpolating the data. Then with probability \u2265 1 \u2013 B(N)-cL3V2 (where c is a positive universal constant), the error rate of fMDL on a uniformly chosen test point is at most 2cN-d.\nProof. Throughout this proof, Co, C1,... will denote positive universal constants. By our previous results, there exists a neural network Fp of description length < s := coL3V2log2 B(N) which encodes the program P. Letting Ns be the set of all neural networks with description length \u2264 s, Lemma 4.1 states that |N3| \u2264 eC1s < B(N)C2L3V2.\nTake any two networks f1, f2 \u2208 N, which disagree on at least eN of x \u2208 [N]. The chance that f1, f2 agree on the data is \u2264 (1 \u2212 \u0454)\u201d, where recall n is the number of data points. Let A be the event that there exist f1, f2 \u2208 N, which disagree on at least eN points but agree on the data. By the previous point,\nP(A) \u2264 (N_s)(1-6)^n\nNow, consider Fp and fMDL, the minimum description length neural network which interpolates the data. Both of these are in Ns, as fMDL must have description length less than or equal to the description length of Fp, and they both agree on the observed data. On the event A\u00ba, \u00ceMDL and Fp will agree on (1 \u2013 \u20ac)N points, so they will agree on a uniformly chosen test point with probability > 1 - \u20ac. Now, from the previous display, we get\nP(A) \u2264 1/2 e^(-n6^2)/2)\nChoosing\n\u03f5 = 2c_3/(N^\u03b4\nwhere n = N^\u03b4 L^3 V^2 ln(B(N))\nwhere V is uniform from (0,1)\nwe get\nP(A) \u2264 e^(-c^3 L^3 V^2 ln B(N)\ncompleting the proof."}, {"title": "Examples", "content": "In the first two examples below, N is a large number, and our data consists of (xi, Yi), i = 1, ..., n, where n = \u0398( log N) for some \u03b4\u2208 (0,1), x1,...,In are drawn i.i.d. uniformly from [N] := {1,...,n}, and yi = f(xi) for some given function f. In the third example, the x\u012f are vectors are drawn from [N]\u00b3, and yi = f(xi).\nLet us revisit the prime checking program in the introduction. Here, f(x) = 1 if x is prime and 0 if not. The full SNP may be found in Example A.1; the program satisfies L = 11,V = 9,B(N) = N2. By Theorem 5.1, the MDL interpolating neural network has error rate at most c\u2081N-d with probability > 1 \u2212 N-2, where c\u2081 and c\u2082 are positive universal constants. Recall that the density of the primes among the first N natural numbers is (ln N)\u00af\u00b9 via the prime number theorem. Therefore f classifies both primes and non-primes correctly with high accuracy."}, {"title": "Discussion", "content": "Theorem 5.1 provides no practical guidance on how to find the minimum description length neural network interpolating the data, beyond brute-force search. Notice that the architecture may change. [LGCK22] give very interesting empirical results for a type of MDL network different from ours; they show genetic algorithms are useful for finding the MDL network. Our theorem also does not say anything about neural networks trained with gradient-based methods. Motivated by recent results [MRVPL23, MSVP+19, GFRW23] outlined in Section 1.2, proving a result that neural networks optimized through gradient-descent type methods are typically of low complexity could give practical generalization bounds.\nTheorem 5.1 also appears like a result on benign overfitting [BLLT20] due to the appearance of an interpolating neural network, but the primary difference with the literature is that the theorem does not allow for noisy observations of the outcome. Section 2 of [BMR21] sheds some light on the role of noise in benign-overfitting type results, and gives results on minimum complexity interpolants in the noiseless case, for squared loss. However, our focus is on data of low Kolmogorov complexity and neural networks of small description length. Extending Theorem 5.1 to handle noisy observations of the data, or considering other losses such as squared loss, would be of clear interest.\nLimitations. The notion of SNPs is somewhat restricted. Although it accommodates many interesting examples, notice that the number of variables cannot scale with the inputs. Moreover, arrays and accessing arrays with variable locations is not allowed. Other natural expressions are disallowed, such as while loops. Furthermore, all variables must be positive integers, and must be bounded by an absolute constant B := B(N). The way Theorem 5.1 depends on B precludes SNPs that do an exponential amount of computation in N.\nMany of these limitations can be overcome by increasing the expressivity of SNPs as a programming language, while considering more expressive description measures. As long as there is a conversion between short programs and neural networks of low complexity, the generalization idea of Theorem 5.1 carries through. By extending the programming language, other neural network architectures beyond feedforward networks may have to be considered. For example, can generalization guarantees be obtained for convolutional neural network architectures on structured image data? Can similar guarantees be obtained for recurrent architectures on structured sequence data? In particular, there has been much recent interest in the transformer architecture, in an attempt to explain various phenomena in large language models such as in-context learning, out-of-distribution generalization, and length generalization [WWHL24, ABL+24, AM24]. Specializing our argument to transformers and minimum description learning would be of interest."}]}