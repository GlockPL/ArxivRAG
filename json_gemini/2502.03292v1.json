{"title": "ALPET: Active Few-shot Learning for Citation Worthiness Detection in Low-Resource Wikipedia Languages", "authors": ["Aida Halitaj", "Arkaitz Zubiaga"], "abstract": "Citation Worthiness Detection (CWD) determines which sentences, within an article, should be backed up with a citation to validate\nthe information it provides. This study, introduces ALPET, a framework combining Active Learning (AL) and Pattern-Exploiting\nTraining (PET), to enhance CWD for languages with limited data resources. Applied to Catalan, Basque, and Albanian Wikipedia\ndatasets, ALPET outperforms the existing CCW baseline while reducing the amount of labeled data in some cases above 80%.\nALPET's performance plateaus after 300 labeled samples, showing it suitability for low-resource scenarios where large, labeled\ndatasets are not common. While specific active learning query strategies, like those employing K-Means clustering, can offer\nadvantages, their effectiveness is not universal and often yields marginal gains over random sampling, particularly with smaller\ndatasets. This suggests that random sampling, despite its simplicity, remains a strong baseline for CWD in constraint resource\nenvironments. Overall, ALPET's ability to achieve high performance with fewer labeled samples makes it a promising tool for\nenhancing the verifiability of online content in low-resource language settings.", "sections": [{"title": "1. Introduction", "content": "The rise of misinformation in the digital age has made automated fact-checking an essential tool in ensuring\nthe reliability of information [1, 2]. Automated fact-checking are complex systems including tasks that involve the\nidentification of information worthy of checking, linking it with evidence and the subsequent verification step [3].\nWhile substantial progress has been made in fact-checking systems for major languages, low-resource languages\nremain largely underexplored and are recently receiving more attention [4, 5].\nMisinformation has a global impact [6], so contributing towards advancement of fact-checking systems for low-\nresource languages is necessary to maintain information integrity worldwide. However, in doing so, the scarcity of\nlabeled datasets for low-resource languages poses a significant challenge [7]. Many of the existing methods for de-\nveloping automated fact-checking systems rely on extensive labeled data, which is often unavailable for low-resource\nlanguages. Furthermore, in high-resource languages, like English, fact-checking systems have the advantage of focus-\ning on specific domains, such as political discourse, which generates abundant data for verification [8]. In contrast,\nlow-resource languages often lack this advantage, as the volume of digital information available in specialized topics\nis more limited, affecting the ability to build effective systems within a single domain of a language [7, 9]; as a result,\nwe cannot limit ourselves to one specific domain for a low-resource language."}, {"title": "2. Research Objectives and Hypotheses", "content": "The scarcity of labeled data in low-resource languages presents a major challenge for developing automated fact-\nchecking systems. In this research, we aim to address this issue by exploring how the combination of AL and PET\ncan enhance citation check-worthiness detection for low-resource languages, which in our case we investigate with\nAlbanian, Basque, and Catalan.\nThe specific objectives (O) of this research, and the hypotheses (H) we set forth in line with the objectives, are:\n\u2022 01: To investigate whether ALPET can achieve comparable or superior citation worthiness detection perfor-\nmance in low-resource languages (Albanian, Basque, and Catalan) compared to the CCW baseline model, while\nutilising fewer labelled examples. This objective directly addresses hypothesis H1, which posits that ALPET\nwill outperform CCW in data efficiency and F1 score in low-resource languages.\nH1: ALPET outperforms the CCW baseline model in terms of data efficiency (fewer labeled examples) and\nperformance (F1 Score) in low-resource languages while using the same AL query strategies.\n\u2022 02: To determine the optimal number of labelled examples required by ALPET to achieve peak performance in\ncitation worthiness detection for low-resource languages, analysing whether the model's performance plateaus\nafter a certain number of labelled samples. This objective is directly related to hypothesis H2, which suggests\nthat ALPET's performance will plateau after a certain number of samples, highlighting its effectiveness with\nsmall datasets.\nH2: ALPET's performance improves with increasing labeled data but it may reach a plateau at a certain point,\nsuggesting that the method is effective with small sized datasets in low-resource settings.\n\u2022 03: To quantify the reduction in labelled data achieved by ALPET compared to the CCW baseline model\nwhile maintaining comparable citation worthiness detection performance in low-resource languages, assessing\nthe robustness of ALPET's performance with reduced training data. This objective is linked to hypothesis H3,\nwhich predicts that ALPET will achieve competitive performance with an average of 58-72% fewer labelled\nexamples than CCW, demonstrating its robustness in low-resource settings.\nH3: ALPET can match the performance of CCW with far fewer labeled examples in low-resource languages.\nIts performance stays stable even as the number of labeled examples decreases, showing its robustness in these\nsettings.\n\u2022 04: To compare the performance of various active learning query strategies against random sampling in se-\nlecting informative data points for citation worthiness detection in low-resource languages, evaluating their\neffectiveness based on the F1 Score. This objective directly addresses hypothesis H4, which states that active\nlearning query strategies generally achieve higher F1 scores than random sampling in low-resource languages.\nH4: Active learning query strategies generally achieve higher F1 scores than random sampling in low-resource\nlanguage datasets."}, {"title": "3. Background and Related Work", "content": "Citation worthiness detection, often referred to as the \"citation needed\" task in the literature, involves identifying\nwhether a sentence within a given corpus requires a citation [12, 10]. This task is particularly crucial on collaborative\nplatforms like Wikipedia, where maintaining information credibility is essential. By ensuring that unsourced state-\nments are flagged to be properly supported by reliable references, CWD helps prevent the spread of misinformation.\nCWD in Wikipedia simplifies and speeds up the process of verifiability policy\u00b9 by prioritizing unsourced sentences\nto be reviewed by editors. This process is vital for maintaining academic and public trust in Wikipedia, which serves"}, {"title": "3.1. Citation Worthiness Detection (CWD)", "content": "Citation worthiness detection, often referred to as the \"citation needed\" task in the literature, involves identifying\nwhether a sentence within a given corpus requires a citation [12, 10]. This task is particularly crucial on collaborative\nplatforms like Wikipedia, where maintaining information credibility is essential. By ensuring that unsourced state-\nments are flagged to be properly supported by reliable references, CWD helps prevent the spread of misinformation.\nCWD in Wikipedia simplifies and speeds up the process of verifiability policy\u00b9 by prioritizing unsourced sentences\nto be reviewed by editors. This process is vital for maintaining academic and public trust in Wikipedia, which serves\nas a widely-used reference for both educational and general purposes. It is also critical in fact-checking systems to\nevaluate the veracity of claims [13], and in various social media platforms to combat misinformation [14, 4].\nCWD in low-resource languages has several challenges. One major issue is the availability of credible labeled\ndatasets, as these languages often lack the extensive digital content needed to develop such resources. As a result,\nmost NLP tools and pre-trained models, like BERT and GPT, are optimized for high-resource languages, leading to\nsuboptimal performance in low-resource contexts. Specifically, considering that low-resource languages have variety\nof dialects that are underrepresented or not captured at all by PLMs. The scarcity of scientific research focused on\nlow-resource languages further amplifies the problem, as most advancements in the field are designed and tested on\nlarger languages.\nExisting CWD approaches in Wikipedia are usually defined as supervised learning text classification task. The\npioniring work for this task started from the assessment of Wikipedia verifiability policy [10] where they used recurrent\nneural networks (RNN) with GRU cells and GloVe pre-trained word embeddings to identify sentences that needed\ncitation. However, they heavily relied on featured articles \u00b2, citation needed tag \u00b3, and manual annotation efforts to\ncreate the dataset.\nBuilding upon Redi's work, another approach was proposed utilizing positive unlabeled learning where they aimed\nto develop an unified approach to check-worthiness detection tasks including claim detection, rumour detection and\ncitation needed [15]. While aiming to create a unified solution for these three distinct tasks, authors also aimed to\nreduce manual labelling effort by asking annotators to mark only sentences that were clear-cut check-worhty and the\nrest to be handled through positive unlabeled method. They used transfer-learning to transfer the knowledge from one\ntask to another and different from more traditional neural networks used in Redi's approach, in this research they used\npre-trained BERT model.\nBoth studies [10] and [15] focused primarly on English, leveraging featured articles and {citation needed} tags\nadded by active editors. These methods, however, are not applicable to low-resource languages due to the scarcity\nof featured articles and the absence of such tags, resulting from a lack of active editorial communities. To overcome\nthis challenge, a recent study proposed an approach to use quality score of articles to automatically build a credible\ndatasets for low-resource languages [4]. Unlike previous work, which relied solely on the sentence text and its section\nplacement within an article, this study used adjacent sentences as contextual information and employed mBERT for\nthe final classification of sentences needing citations. While their approach advances CWD in low-resource languages,\nit relies on substantial amount of data due to the need for contextual information from adjacent sentences; and the\nautomated large-scale labeling process, although innovative, carries the challenge of potential inaccuracies, as it can-\nnot fully ensure the correctness of every label. Incorporating an oracle in the labeling process through active learning\ncould address these issues by enhancing the credibility of the dataset. Building on this foundation, we propose an\napproach that integrates cold-start pool-based active learning with few-shot learning using Pattern Exploit Training\n(PET), which reduces the data requirements for effective CWD and minimizes dependence on additional contextual\ninformation."}, {"title": "3.2. Active Learning in NLP", "content": "Active learning (AL) is a machine learning approach where the algorithm uses a querying strategy to identify the\nmost informative data points for labeling by an oracle, to improve model's performance [16]. The goal is to overcome\nthe labeling bottleneck of traditional passive learning systems by optimizing the model's performance with a smaller\nset of labeled examples, making the learning process more efficient and cost-effective. This approach is particularly\nvaluable in scenarios with limited labeled data, such as low-resource Natural Language Processing (NLP) tasks.\nAL involves two key concepts: problem scenarios and query strategies. Scenarios define the learning environment,\nincluding how data is presented and how the model interacts with it, while query strategies determine which data\npoints to label within that scenario. Some query strategies can be applied across multiple scenarios, while others are\nscenario-specific. For an AL system to be effective, it is essential to match the appropriate query strategy with the\nright scenario. According to the existing literature some of the main AL scenarios are membership query synthesis\n[17], stream-based selective sampling [18], pool-based sampling [19], batch AL [20], and multi-task AL [21, 22, 23]."}, {"title": "4. Methodology", "content": "The proposed methodology, referred to as ALPET, integrates Active Learning (AL) and Pattern Exploit Training\n(PET) to create an efficient approach for data selection and model training in low-resource setting. It is structured into\nfour key steps:\n1. Data selection: the process begins with applying pool-based active learning strategies to select informative data\npoints from a large pool of unlabeled sentences (see section 4.1).\n2. Data processing: the selected data is processed to remove redundancy when it exists such as duplicates and\nhighly similar sentences, ensuring dataset diversity (see section 4.2).\n3. Multi-round dataset preparation for FSL: multiple rounds of datasets are then created with incremental sample\nsizes for each active learning strategy (see section 4.3).\n4. Model training: the PET model with mBERT is used to train and classify sentences as needing citations (see\nsection 4.4)."}, {"title": "4.1. Data Selection", "content": "The CWD task is framed within a pool-based AL scenario using various query strategies. Central to these strategies\nis the acquisition function, a term commonly used in mathematical definitions to describe the mechanism that assigns\na score to each unlabeled data point based on specific criteria. In the context of AL, this function is often referred\nto as a query framework or query strategy [16]. Traditionally, acquisition functions in AL are based on uncertainty\nor diversity, with more recent approaches incorporating hybrid methods that combine elements of both [91]. In this\nwork, we employ uncertainty, diversity, and hybrid acquisition functions as part of our data subset selection process.\nNext we elaborate these query strategies in more details."}, {"title": "4.1.1. Diversity sampling", "content": "The goal here is to select data points that are as different from each other as possible ensuring a representative\nsample of the overall data distribution. Three diversity sampling methods that we use in this work are geometry-based,\ncorset-based, and cluster samplings."}, {"title": "A. Geometry-based sampling.", "content": "Distance metrics belong in the geometry-based metrics, they play a fundamental\nrole in identifying data points for labeling in the active learning scenarios. Among most widely used distance\nmetrics are cosine and euclidean distance. We have used both of them in ALPET methodology and more details\nare presented below.\n1. Cosine distance is derived from cosine similarity and measures dissimilarity between data points. In\nour task, sentences are represented by embeddings, or high-dimensional vectors, to capture the semantic\ncontent of the sentence. Cosine distance between two vectors A and B is mathematically defined as:\n$\\text{Cosine Distance} = 1 - \\cos(\\theta) = 1 - \\frac{A \\cdot B}{||A|| \\times ||B||}$ \nwhere $A \\cdot B$ is the dot product of the vectors, and $||A||$ and $||B||$ are their magnitudes."}, {"title": "2. Euclidean distance", "content": "measures the straight-line distance between two points in a high-dimensional space.\nIn our task, this metric is used to quantify the absolute difference between sentence embeddings, providing\na direct measure of dissimilarity. The Euclidean distance between two vectors A and B is mathematically\ndefined as:\n$\\text{EuclideanDistance} = ||A - B|| = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}$\nwhere $a_i$ and $b_i$ are the components of vectors A and B in an $n$-dimensional space.\nUsing euclidean distance, we aim to select sentences that are different from those already labeled, thus"}, {"title": "i. Maximum average distance selection:", "content": "In this method, data points are selected based on the maximum\naverage distance from already selected instances. The process begins with a cold start, where the first\nsentence is selected randomly from the unlabeled pool. For the next selection, the average distance of\neach candidate sentence (that has not been selected yet) to the already selected sentence is calculated.\nThe sentence with the maximum average distance to the already selected sentence is chosen next. With\ntwo sentences selected, we recalculate the average distance of the remaining sentences to both selected\nsentences. The candidate with the highest average distance is added to the selection pool. This loop\ncontinues until the desired number of sentences is selected. By focusing on instances that are farthest\nfrom those previously selected, this approach aims to select points that are diverse and representative of\ndifferent regions of the feature space."}, {"title": "ii. Minimum average distance selection:", "content": "This method selects data points with the minimum average dis-\ntance from the already selected instances. The selection process is similar to the one above that consideres\nthe maximum average distance, but instead, here we consider the minimum average distance. The ratio-\nnale here is to focus on data points that are similar to those already chosen, effectively reinforcing the"}, {"title": "iii. Combined maximum and minimum average distance selection:", "content": "This method alternates between se-\nlecting data points based on the maximum and minimum average distances from the already selected\ninstances. The process begins with a cold start, where the first sentence is randomly chosen from the\nunlabeled pool. For the next selection, the average distance of each candidate sentence to the already\nselected sentence is calculated. The sentence with the maximum average distance to the selected instance\nis chosen first. Once this sentence is added to the selection pool, the next sentence is selected based on\nthe minimum average distance from the already selected sentences. With three sentences now in the pool,\nthe average distance of the remaining candidates to the selected sentences is recalculated. The selection\nalternates between choosing the sentence with the maximum average distance and the sentence with the\nminimum average distance, creating a balance between diversity and representativeness. This loop contin-\nues until the desired number of sentences is selected. By combining both maximum and minimum average\ndistances, it ensures that both diverse and representative instances are included in each iteration, covering\na broad spectrum of the data space while also reinforcing existing knowledge."}, {"title": "iv. Maximum, minimum, and random selection:", "content": "In this method, we introduce an element of random-\nness. Each iteration involves selecting one data point with the maximum average distance, one with the\nminimum average distance, and one randomly chosen instance. This approach adds an exploratory com-\nponent, allowing the model to occasionally consider unexpected instances that may not fit neatly into the\nestablished patterns, leading to accidental discoveries."}, {"title": "B. Corset-based strategies:", "content": "In AL a corset is a small, representative subset of the entire dataset that, when used to\ntrain a model, can approximate the performance of a model trained on the full dataset [92]. In our methodology\nwe have used lightweight corset and greedy corset (with cosine and euclidean metrics)."}, {"title": "i. Greedy coreset:", "content": "Originally proposed to address the data labeling bottleneck for deep convolutional neural\nnetworks [92], this approach has been adapted for text data in the small-text library [93]. It constructs a\ngreedy coreset over text embeddings, solving a k-center problem through a greedy approximation. The\nmethod aims for precise coverage and diversity by selecting points that minimize the maximum distance\nbetween any data point and its closest coreset point. While more computationally intensive, it is particu-\nlarly useful when a highly representative subset is crucial, such as in smaller datasets."}, {"title": "ii. Lightweight coreset:", "content": "This strategy selects a representative subset of data points using K-Means clus-\ntering, designed for computational efficiency by approximating the selection process [94]. It works by\nchoosing data points that are farthest from the already selected points, minimizing redundancy. While this\napproach is efficient, it may not capture the full diversity of the data as precisely as more computation-\nally intensive methods like GreedyCoreset [93]. The method's effectiveness depends on the quality of the\nfeature representations, such as text embeddings."}, {"title": "C. Clustering-based strategies:", "content": "This strategy addresses pool-based AL challenges of selecting minority class in\nlarge imbalanced datasets [95]. The term anchor here refers to the chosen class-specific instances from the\nlabelled set. The process begins by selecting anchor points from the labeled dataset, representing different\nclasses of the data space. Each unlabelled instance is then scored based on its average distance from the\nanchors, forming a subpool of the most similar instances. The next step is selecting the instances from\nthe subpool to be labeled by an oracle. Once labeled, instances are added to the labeled dataset and the\nprocess repeats in subsequent AL iterations."}, {"title": "i. Anchor subsampling:", "content": "This strategy addresses pool-based AL challenges of selecting minority class in\nlarge imbalanced datasets [95]. The term anchor here refers to the chosen class-specific instances from the\nlabelled set. The process begins by selecting anchor points from the labeled dataset, representing different\nclasses of the data space. Each unlabelled instance is then scored based on its average distance from the\nanchors, forming a subpool of the most similar instances. The next step is selecting the instances from\nthe subpool to be labeled by an oracle. Once labeled, instances are added to the labeled dataset and the\nprocess repeats in subsequent AL iterations."}, {"title": "4.1.2. Uncertainty sampling", "content": "Uncertainty-based selection focuses on identifying data points where the model is most unsure of its predictions."}, {"title": "i. Prediction entropy:", "content": "This query strategy selects instances with the largest prediction entropy [93]. As a method\nit was initially proposed to reduce the labeling efforts in an object recognition task [96], however, in small-text\nlibrary it was adapted for text-based data. Given a pool of unlabeled data, for each instance the model outputs\na probability distribution over all possible classes. Then the entropy of the predicted probability distribution\nis calculated for each instance which are then ranked by the highest entropy scores indicating cases where the\nmodel is most uncertain about the prediction. The top k instances with the highest entropy scores are selected\nfor labeling. Once labeled, instances are added to the training set, and the model is retrained on the updated\nlabeled dataset."}, {"title": "ii. Least confidence:", "content": "Is one of the earliest uncertainty-based strategies [97]. It selects instances with the least\nprediction confidence (regarding the most likely class) [93]. Specifically, for each instance in the unlabeled\npool, the model assigns a probability distribution over the possible classes. Then it identifies instance where the\nmodel is least confident about the correct class. This instance is then selected for labeling because it represents\na point of high uncertainty, where the model might benefit most from additional labeled data. However, this\nmethod may sometimes overlook instances where the model is uncertain between several classes because it only\nconsiders the confidence of the most likely class."}, {"title": "iii. Breaking ties:", "content": "This function is designed to select data points which have a high uncertainty in classification,\nspecifically those where the margin between the most likely and second most likely predicted class is minimal.\nThe small margin indicates that the model is uncertain about which class the sentence belongs to, making these\ninstances particularly valuable for AL. This strategy was originally proposed in the context of image data [98]\nbut in small-text library it has been adapted to work with text data under the name BreakingTies [93]. The core\nidea to target ambiguous instances remains the same."}, {"title": "4.1.3. Hybrid sampling", "content": "Hybrid acquisition functions were developed to combine the best aspects of both uncertainty and diversity sam-\npling. In this research, we have used Contrastive Active Learning (CAL) and ALPS (Active Learning by Processing\nSurprisal), two state-of-the-art hybrid methods, to maximize the efficiency of the AL process."}, {"title": "i. Contrastive active learning (CAL):", "content": "This function combines uncertainty and diversity sampling for warm-start\nAL. Using ContrastiveActiveLearning from small-text library [93], we implement CAL [91], which fine-tunes\nBERT with an initial labeled dataset and applies a KNN algorithm to identify the closest labeled examples\nfor each data point in the unlabeled pool. A contrastive score, based on Kullback-Leibler (KL) divergence\nbetween predicted probabilities of the unlabeled candidate and its labeled neighbors, is calculated to select\nhigh-divergence examples for labeling by a proxy. The labeled batch is then removed from unlabeled pool and\nadded to the training (labeled) dataset. This loop repeats until all unlabaled data have been labeled. This method\naims to effectively identify sentences with similar vocabulary but differing predictions, enhancing the selection\nof informative examples."}, {"title": "ii. Active learning by processing surprisal (ALPS):", "content": "This method combines uncertainty and diversity sampling\nfor cold-start AL. Implemented as EmbeddingKMeans in small-text library [93], ALPS [99] uses surprisal\nembeddings derived from the masked language modeling loss in PLMs like BERT to estimate uncertainty,\nbypassing the need for unreliable model confidence scores in the cold-start scenario. After computing surprisal\nembeddings for each sentence in the unlabeled pool, K-Means is applied to cluster these embeddings and the\nsentence closest to each cluster center is selected. Thus ALPS identifies data points that are both surprising\n(indicating high uncertainty) and representative of diverse, underexplored areas in the data space, making it\nparticularly effective in early stages where labeled data is scarce."}, {"title": "4.2. Data Processing", "content": ""}, {"title": "4.2.1. Duplicate and similarity removal", "content": "In cold-start AL iterations, after each data subset selection, we observed duplicate sentences or sentences with\nhigh structural similarity. To mitigate redundancy in the dataset, we applied a cosine similarity with TF-IDF weights.\nSentences with a cosine similarity score above 0.8 were considered too similar and were excluded from the final\ndataset. This threshold was selected based on our empirical analysis of the dataset, where we observed that sentences\nwith a similarity score above 0.8 were almost identical except for minor variations, such as differing in only the last\none or two words. For our CWD task, we aim to capture diverse sentence structures and contexts, so such highly\nsimilar sentences were unnecessary."}, {"title": "4.2.2. Data balancing", "content": "This research was conducted using Wikipedia articles where each was split into individual sentences, forming\nthe unlabeled pool of data for AL. Although the sentences were pre-labeled, we temporarily removed the labels to\nsimulate an active learning environment where labels are obtained iteratively from an oracle.\nEach languages' dataset (ca, eu, and sq) is imbalanced, with higher proportion of sentences that do not contain\ncitations compared to those that do. In pool-based AL scenarios, imbalance can become more pronounced due to the\nmodel's tendency to favor majority class examples, leading to a loop of oversampling the majority class. To address\nthis, we applied random undersampling to the majority class post-selection, reducing its size to match the minority\nclass. In this way the minority class remained sufficiently represented throughout the learning process which is critical\nfor our task."}, {"title": "4.3. Multi-Round Dataset Preparation for Few-Shot Learning", "content": "After data processing steps, each query strategy yielded a dataset with 3,000 data points per class. We then\nconstructed six distinct rounds of datasets. Each round comprises ten subsets, with the number of data points per class\nvarying from 50 to 500, increasing in increments of 50. The dataset preparation process involved the following steps:\n1. Round-based data partitioning: We randomly separated the 3,000 data points per class into six distinct\ngroups, each containing 1,000 total data points (500 per class), to allow for multiple experimental iterations. The\nsix rounds were chosen in line with standard practices in machine learning experiments, where 5 to 10 rounds are\ntypically used to ensure reliable and generalizable results.\n2. Incremental sample sizes: For each group created in the first step, we generated ten cumulative subsets,\nwith sample sizes ranging from 50 to 500 data points per class with increments of 50. This incremental approach is\ncommonly used in few-shot learning experiments, as it allows for a fine-grained analysis of the model's performance\nacross different levels of data availability."}, {"title": "4.4. Model Training with Pattern Exploit Training (PET)", "content": "Pattern-Exploiting Training (PET) [100] is a semi-supervised method for few-shot learning in NLP tasks like\ntext classification and natural language inference. The core idea behind PET is to reformulate input examples as\ncloze-style questions (fill-in-the-blank), that help PLMs better understand the task. This method uses a concept called\nPattern-Verbalizer Pair (PVP) which includes two elements:\n1. Pattern where the input is transformed into a fill-in-the-blank questions. This is done by inserting a masked\ntoken into the input text, which the model will later try to predict.\n2. Verbalizer maps task labels to actual words in the language model's vocabulary. These words are what the\nmodel predicts to fill in the blank created by the pattern.\nExample:\n\u2022 Input: \"This movie was amazing.\"\n\u2022 Pattern: The input is transformed into \"This movie was [mask].\""}, {"title": "5. Experimental Settings", "content": ""}, {"title": "5.1. Datasets", "content": "To validate our hypothesis, we use real-world data sourced from Wikipedia articles. Specifically, we employ three\ndatasets in different languages: ca-citation-needed, eu-citation-needed, and sq-citation-needed [4] in Catalan, Basque\nand Albanian, respectively. These datasets contain contextual information beyond individual sentences and labels.\nFor this study, however, we focused exclusively on two components: the text of sentences from Wikipedia articles and\ntheir labels indicating the presence of inline citations. The sentence text was used for data selection through various\nAL query strategies, while the labels served primarily to simulate the annotation process with oracles when necessary.\nFor each dataset ca-citation-needed, eu-citation-needed, and sq-citation-needed we applied a consistent\nlabeling budget and data split, dividing the data into training, development, and testing sets.\nIn the context of FSL, where models are trained on very limited data, even slight changes in the test or development\nset sizes can sometimes affect performance [11]. Thus, in our experiments, we tested different sizes of the development\nand test sets, even though improvements in learning performance were not necessarily expected. The primary purpose\nof this approach was to assess how varying the size of these sets might influence the stability of our model's evaluation\nmetrics. Although the stability and performance remained largely unchanged, we observed an increased in time and\nresource consumption when the test and development sets were used at their maximum capacity. Therefore, in our\nresults, we report only the experiments where the number of shots in the test and development sets were limited. The\ndata selection for these reduced sets was done randomly. Table 1 presents the details of the datasets used for the three\nlanguages, including their splits into training, development, and test sets. As described in section 4.3 for training PET\nmodels of each language we have created 6 distinct training datasets, each used to train a separate model. But we have\nevaluated each model using the same development and test datasets. The results of each language for all models per\nspecific shots are then averaged and reported."}, {"title": "5.2. PET with Active Samples", "content": "Models that can be used with PET tasks are PLMs and in our CWD task we employeed a multilingual BERT\n(mBERT) to calculate probabilities of candidate tokens that could replace [mask] in predefined patterns for each of\nthe datasets we used. Since we are working with datasets of threee languages, we had to maually pre-define patterns\nfor each language. In order to avoid introducing any bias in any of the languages we decided to use the same pattern\nstructure for three languages but we translated them accordingly to match the language. Even though the goal of\nthis research is not to find the most optimized patterns and verbalizer for PET, we experimented with a couple of\npatterns and we choose the best performing ones to report the final results on. It is worth mentioning that we started"}, {"title": "5.3. Baseline Model", "content": "This study seeks to evaluate the efficiency of citation worthiness detection (CWD) within few-shot learning settings\nto accommodate languages with low-resources using the ALPET method.\nWe benchmark our work against the Contextualized Citation Worthiness (CCW) model [4] due to its SOTA ap-\nproach in addressing CWD in the Wikipedia domain. The baseline incorporates a transformer-based architecture,\nutilizing contextual embeddings from mBERT to capture sentence-level features. To ensure methodological consis-\ntency and comparability, we adapted CCW by adding an AL step for data selection. This modification aligns with\nour ALPET approach, and it allow for a more direct performance comparison between the models in terms of both\naccuracy and efficiency across languages like Albanian, Basque, and Catalan."}, {"title": "5.4. Evaluation Metrics", "content": "To assess and compare the performance of the proposed ALPET method against the baseline CCW, we employ\nthe macro F1 score as the primary evaluation metric. Given the balanced nature of our dataset and the equal im-\nportance of both classes-positive (sentences requiring inline citations) and negative (sentences not requiring inline\ncitations) the macro F1 score is well-suited for this task as it ensures that the performance across both classes is\nequally represented."}, {"title": "5.5. Training Details", "content": "Hyperparameters. The ALPET method was trained using the following hyperparameters. We utilized the multi-\nlingual BERT with a maximum sequence length of 256 tokens and a batch size of 4 for training and 8 for evaluation.\nThe model was trained for 3 epochs, with a learning rate of le-5 and a weight decay of 0.01. Optimization was han-\ndled by the Adam optimizer with an epsilon value of 1e-8 and a maximum gradient norm of 1.0 to prevent gradient"}, {"title": "6. Experiment Results", "content": "In this section we present the evaluation of our ALPET model alongside the CCW baseline model on three datasets\nCA-citation-needed, EU-citation-needed, and SQ-citation-needed. Results are organised as answers to hypotheses."}, {"title": "6.1. ALPET Outperforms Baseline Models in Low-Resource Languages (H1)", "content": "This section aims to evaluate Hypothesis H1. We hypothesized that ALPET would outperform the baseline CCW\nmodel in terms of data efficiency (i.e., achieving comparable performance with fewer labeled examples) and predictive\nperformance (F1 Score) in low-resource languages, while utilizing the same AL query strategies."}, {"title": "6.2. ALPET's Performance Plateau and Data Efficiency (H2)", "content": "This section aims to evaluate Hypothesis H2. We hypothesized that ALPET's performance improves with increas-\ning"}]}