{"title": "Psychomatics - A Multidisciplinary Framework for Understanding Artificial Minds", "authors": ["Giuseppe Riva", "Fabrizia Mantovani", "Brenda K. Wiederhold, Ph.D.", "Antonella Marchetti", "Andrea Gaggioli"], "abstract": "Although LLMs and other artificial intelligence systems demonstrate cognitive skills similar to humans, like concept learning and language acquisition, the way they process information fundamentally differs from biological cognition. To better understand these differences this paper introduces Psychomatics, a multidisciplinary framework bridging cognitive science, linguistics, and computer science. It aims to delve deeper into the high-level functioning of LLMs, focusing specifically on how LLMs acquire, learn, remember, and use information to produce their outputs. To achieve this goal, Psychomatics will rely on a comparative methodology, starting from a theory-driven research question - is the process of language development and use different in humans and LLMs? - drawing parallels between LLMs and biological systems.\nOur analysis shows how LLMs can map and manipulate complex linguistic patterns in their training data. Moreover, LLMs can follow Grice's Cooperative Principle to provide relevant and informative responses. However, human cognition draws from multiple sources of meaning, including experiential, emotional, and imaginative facets, which transcend mere language processing and are rooted in our social and developmental trajectories. Moreover, current LLMs lack physical embodiment, reducing their ability to make sense of the intricate interplay between perception, action, and cognition that shapes human understanding and expression.\nUltimately, Psychomatics holds the potential to yield transformative insights into the nature of language, cognition, and intelligence, both artificial and biological. Moreover, by drawing parallels between LLMs and human cognitive processes, Psychomatics can inform the development of more robust and human-like AI systems.", "sections": [{"title": "1. Introduction: Defining Psychomatics", "content": "In the rapidly evolving landscape of artificial intelligence (AI) 1-4, there exists an ongoing debate about how we should evaluate and understand the cognitive capacities of AI systems 5,6. Traditional approaches have often centered on either using automated evaluations based on specific benchmarks, or comparing AI behaviors with those of humans, aiming to identify similarities and differences 7-9.\nFor example, Flan-PaLM, an instruction-tuned variant of the Pathways Language Model (PaLM) achieved 67.6% accuracy on MedQA, the US Medical Licensing Exam-style questions 10. Another study 11 showed that a Large Linguistic Model (LLM) achieved human-like performances in a set of compositional tasks, demonstrating systematic compositionality\u2014the algebraic ability to understand and produce novel combinations from known components. Finally, Hagendorff, et al. 12 found that human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT, This LLM responded correctly to a great majority of tasks, decisively outperforming humans in their ability to avoid traps embedded in the tasks.\nSeveral other studies have been conducted to compare the behavior of AI and humans, identifying both similarities and differences. For example, Mei et al. 13 used conventional behavioral measures from economics and psychology to evaluate the behavioral and personality traits of large language models (LLMs). The study found that ChatGPT-4 exhibited traits that were indistinguishable from those of a random human among tens of thousands of human participants. Based on these findings, Meng 14 signals the emergence of a new research area called \u201cAI Behavioral Science\u201d (AIBS), which uses methods from human behavioral science to examine and design Al behavior.\nHowever, this emerging field does not provide a deeper insight into AI 15, although it does demonstrate that AI has advanced information manipulation capabilities. As noted recently by Heaven 16: \u201cThe largest models, and large language models in particular, seem to behave in ways textbook math says they shouldn't. This highlights a remarkable fact about deep learning, the fundamental technology behind today's Al boom: for all its runaway success, nobody knows exactly how\u2014or why it works.\"\nWe argue that this shortcoming arises because the AIBS overlooks two critical aspects. First, it fails to clarify how the ability of LLMs to use information to generate outcomes compares to or differs from that of humans. 17. LLMs' abilities may differ from human processes. If so, using LLMs as human subjects in psychology experiments is useless. Secondly, like behaviorism in psychology and the imitation test proposed by Alan Turing, AIBS concentrates only on the observable aspects of behavior and their environmental determinants. While LLMs can adeptly pass the Turing test by emulating human-like responses 18, the adage \"doing the same thing for different reasons\" underscores that merely replicating external behaviors is inadequate for providing a coherent and comprehensive account of the emergence and functioning of cognitive abilities 19. Without exploring the intrinsic processes that underlie these abilities, we risk falling into the trap of \"conceptual borrowing\" 20, where we anthropomorphize Al systems as computational analogues of the human brain and ascribe psychological properties to them without a firm theoretical or empirical foundation 21."}, {"title": "In this Perspective", "content": "In this Perspective, we propose a multidisciplinary synergy among cognitive science, linguistics, and computer science to uncover groundbreaking insights into large language models (LLMs). We present \"Psychomatics,\" (the crasis of the terms \"psychology\" and \u201cinformatics\") an interdisciplinary framework that connects these fields, focusing on how artificial intelligence (AI), and in particular LLMs, processes information. This framework explores how LLMs perceive, learn, remember, and employ information to generate outputs. To achieve this goal, Psychomatics will rely on a comparative methodology, drawing parallels between LLMs and biological systems.\nIn Psychomatics, the comparative approach and its methodological application is conducted through a theory-driven research question 22: Is the process of language development and use different in humans and LLMs? This research question is formulated as a point of departure for comparative investigation, enabling reflection on what, when, and how to compare and for what purpose. Furthermore, a set of rules are used to direct the research strategy, aiming at explanations rather than a more or less complete description of specific phenomena by comparing them across systems. Hence, instead of focusing solely on \u201cbehaviors,\u201d as done by AIBS, the point of departure for our approach is 23:\n*   Developing systematic knowledge about language and its functioning (i.e., syntax and semantics) that transcends mere description and allows for generalizations (i.e., external validity);\n*   Deriving answers to questions (i.e., what is language for?) based on existing theory or, if possible, plausible hypotheses (i.e., theory guidance);\n*   Striving for precise information and comparable indicators (i.e., meanings, intentions, etc.) that are reliable and open to replication (i.e., internal validity).\nBy adhering to these principles, Psychomatics aims to establish a rigorous, theory-driven comparative framework that leverages the complementary strengths of cognitive science, linguistics, and computer science."}, {"title": "2. Information Processing: Syntax and Semantics", "content": "Information processing refers to the manipulation of symbols or data representations stored in memory according to specified rules. From this perspective, AI operates by applying formal rules and procedures to transform symbols held in their memory structures. However, as humans intuitively understand, not all information is alike. Effective information processors must manipulate meaningful representations \u2013 symbols that refer to real-world entities or concepts. To clearly distinguish the formal symbolic operations from the semantic meanings they convey, we borrow two concepts from linguistics 24: syntax and semantics .\nSyntax refers to the grammatical structure of a sentence and the rules for determining whether a sentence is well-formed. The rules governing permissible symbol transformations in a formal system (e.g., chess) are analogous to syntax. In contrast, semantics concerns the meanings expressed by sentences or symbols. A symbol is considered meaningful, or semantic, if it represents or refers to something in the world (real or possible). For example, the word \"dog\" is meaningful because it denotes a particular type of animal.\nThis separation of syntax from semantics enables mechanical information processing. Information processors can formally manipulate symbols using syntactic operations without requiring comprehension of the symbols' meanings. Crucially though, these formal operations can nonetheless preserve and transform meanings. For instance, in mathematics, the rule allowing \"x + x\" to be replaced by \"2x\" is purely syntactic - it only requires recognizing the shapes of symbols like \"x\" and \"+\" to apply. However, this syntactic rule also respects the semantic interpretation: whatever real-world quantity \"x\" represents, adding it to itself yields twice that value. Similarly, syntactic rules can connect and transform meanings. For example, if \"Extra Small\" is understood as a size minor to \"Small,\" which is minor to \"Medium,\" which is minor to \"Large,\" then a small apple can be inferred to have a size smaller than a large one through the application of syntactic operations on these relational concepts."}, {"title": "3. The evolutive goal of language: moving away from experience", "content": "Language presents a fundamental challenge in facilitating precise communication between individuals from diverse backgrounds discussing a wide range of topics that vary in subjectivity and objectivity. People often need to convey personal perceptions, emotions, thoughts, aspirations, and more. Moreover, this puzzle of meaning is further complicated by the inherent diversity across languages. No two languages share perfectly isomorphic semantics. Each has a unique lexicon a distinct set of words that convey specific meanings shaped by the culture. Furthermore, languages evolve independently over time, coining new words, repurposing old ones with novel meanings, and continually reshaping their semantic landscapes.\nHowever, individuals can solve these issues and can communicate effectively using language. How? As suggested by the Israeli linguist Daniel Dor 25, language is a unique system that transcends mere sharing of experiences. It enables speakers to guide their interlocutors intentionally and systematically through the process of imagining an intended experience\u2014 rather than directly experiencing it themselves. The speaker provides the receiver with a symbolic code, a blueprint sketching out the core characteristics of the experience. The receiver then uses this scaffolding to reconstruct and recombine memories of past experiences, producing a novel, imagined experience aligned with the conveyed meaning 26,27. This capability to systematically guide imaginative simulation distinguishes language from other modes of communication that directly share signals or stimuli evoking predefined responses or impressions. Language is thus the only system that allows for communication that bridges the experiential gaps between speakers, serving as an effective means for sharing cultural information across generations and societies. This interpretation aligns with a recent proposal by Fedorenko and colleagues 28, challenging the idea that language itself is the root of human cognitive complexity. Instead of creating human cognitive sophistication, language acts more as a mirror, reflecting the advanced nature of human thinking.\nTo coordinate the open-ended generative process of recombining and modulating memories into a novel coherent experience approximating the intended meaning, language has also to embed in its code the meaning of the experiences it aims to represent. A clear example is a vocabulary: it encodes and describes all existing and potential realities through its unique system of symbolic representations. Within the confines of vocabulary, there is no inherent reality-only a meticulously crafted mapping of concepts over a linguistic realm. As noted by the structural linguistics pioneer Ferdinand de Saussure, to achieve this goal each linguistic sign (such as a word), has a \u201cvalue\u201d that derives from its relationships and contrasts with other signs within the same language system. In his own words 29:\nLanguage is a system of interdependent terms in which the value of each term results solely from the simultaneous presence of the others (p. 114)... But it is quite clear that the concept is nothing, that is only a value determined by its relations with other similar values, and that without them the signification would not exist. (p.117).\nIn Saussurean linguistics, the meaning of a word is not defined in isolation but is shaped by its position within the broader linguistic structure through contrasts with other words (structural semantics). For example, the word \"tree\" derives its meaning through its differences from similar terms like \"bush\" or \"plant.\" Ferdinand de Saussure identified two types of relationships between linguistic terms that shape meaning: syntagmatic and associative (paradigmatic) relations"}, {"title": "4. Transformers Algorithms and their ability in structuring language", "content": "In the field of machine learning, the concept of \u201cattention,\" introduced by Vaswani and colleagues in 2017 30, serves as a core mechanism within Transformer networks, which have become foundational to large language models. Attention in Transformer algorithms enables models to dynamically focus on specific aspects of the input data while performing tasks like translation or question answering. It achieves this by computing attention weights that determine the importance of each input element for the task at hand.\nUnlike previous models that process input linearly and struggle with long-range dependencies, attention allows the algorithm to evaluate the significance of each data part, irrespective of its position. Moreover, Transformer architectures distinguish between self-attention and cross- attention mechanisms, which conceptually mirror syntagmatic and associative relationships in semantics, respectively.\nAs we have just seen, syntagmatic relations in semantics define the meaning of linguistic elements through their linear combination and adherence to syntactic rules. Similarly, the self- attention mechanism in Transformers captures dependencies between words within a single sequence by calculating weighted associations between each word and every other word. These attention weights reveal how each word's interpretation depends on its relationship to others based on position and structure. Thus, self-attention models syntagmatic meaning through structural rules, much like how syntagmatic relations operate in natural language semantics.\nOn the other hand, associative relations in semantics represent broader conceptual links and cognitive connections that words form in the minds of speakers. This aligns with the cross- attention mechanism in sequence-to-sequence Transformer models, such as encoders and decoders. Cross-attention maps one sequence (e.g., input text) to another (e.g., output text) by identifying associations between each element of the two sequences. This mechanism learns cross-sequence relationships that transcend linear structure, much like associative relations link concepts in mental lexicons.\nIn essence, Transformer models combine the complementary mechanisms of self-attentive syntagmatic processing and cross-attentive associative processing as computational analogs to dual pathways of meaning derivation outlined in structural semantics theory. This hybrid approach, integrating linear and non-linear processing, allows Transformers to represent linguistic signs embedded in training data comprehensively. Self-attentive syntagmatic processing enables compositional interpretation, while cross-attentive associative processing contextualizes and integrates semantic knowledge dynamically. Together, these mechanisms empower transformer-based language models to represent and operationalize the complex facets of human language meaning and reasoning."}, {"title": "5. The differences between humans and LLMS", "content": "While LLMs demonstrate impressive language capabilities, they lack the social and relational aspects that underpin human communication 31. In De Saussure's view, language: \"...is both a social product of the faculty of speech and a collection of necessary conventions adopted by a social body to enable individuals to exercise that faculty\" (p. 9).\nAs underlined by Rizzolati and Arbib 32, and remembered recently by Chemero 33, language and communication are deeply rooted in the embodied experience of existing in a physical form. Consequently, LLMs cannot acquire knowledge and skills in the way humans do, which is fundamentally shaped by personal experiences and biological maturation in a physical body. Biological mechanisms like mirror neurons 32 and inter-brain synchronization 34,35 play a crucial role in supporting the analysis of implicit and/or multimodal cues in humans, facilitating the seamless interpretation of nonliteral language. Unfortunately, current LLMs lack physical embodiment, reducing their ability to make sense of the full spectrum of human communication. To overcome this limitation researches are using LLMs to train embodied robots which can interact with their environment 36, to enhance robot intelligence, control, perception, social skills and decision-making capabilities 37. However, even if expectations are running high, conquering real-world complexities, even with the help of LLMs, remains challenging for robot 38,39.\nMoreover, as suggested by Du\u00e9\u00f1ez-Guzm\u00e1n and colleagues 40, natural intelligence emerges at multiple scales in networks of interacting agents via collective living, social relationships and major evolutionary transitions. Instead, LLMs operate in an asocial environment and are devoid of personal experiences that guide human behavior. Unlike children who acquire language through a continuous process of social, emotional, and linguistic interactions 41, LLMs are \"trained\" on pre-defined datasets. This static training approach restricts their ability to \"grow\" or \"evolve\" through personal experiences and social interactions. This fundamental difference hinders their ability to not only generate novel meanings \u2013 this can explain why LLMs suffer when trained on their own input 42 but also truly understand the nuances of human language as we will deepen in the next paragraph.\nFurthermore, in humans, language is not the sole source of meaning ."}, {"title": "Therefore, perceiving and categorizing the world form the basis for constructing meaning.", "content": "Therefore, perceiving and categorizing the world form the basis for constructing meaning. While these categories can be represented through language, language alone cannot guarantee their accuracy. Language, with its inherent values, can establish truth conditions\u2014criteria that define when a sentence is considered true\u2014but it cannot verify the factual correctness of a statement. For instance, the truth condition of the sentence \"The cat is on the mat\" might be defined as \"there exists a cat, and it is currently on the mat.\" However, the actual truth of this statement depends on whether a cat is, in reality, present on the mat at this moment.\nHow do LLMs address the gap between truth conditions and actual truth? They attempt to predict truth by assessing the likelihood of various scenarios within their training data. For instance, if an LLM is trained on data where 60% of the time the cat is on the floor and only 10% of the time it's on the mat, the model will likely answer \"on the floor\" when asked, \"Where is the cat?\" even if the cat is actually on the mat. This is because the model prioritizes the statistically most probable response based on its training data. This reliance on probabilities can lead to hallucinations, where LLMs confidently spout incorrect information 43. They simply haven't been trained to distinguish between high-probability scenarios and reality. For this reason, methods to reduce hallucinations in LLMs have shown significant progress by integrating external knowledge sources, using counterfactual thinking 44 and enhancing retrieval techniques 45.\nAnother important source of meaning comes from personal, subjective and intersubjective experiences. The emotional qualities (qualia) produced by an experience have a unique meaning compared to its objective description. For example, the denotative meaning of \"dog\" as a domesticated canine differs from the connotative meaning, which evokes emotions such as fear or affection, shaped by personal interactions. While the denotative meaning is based on sensory perception, the connotative meaning emerges from emotional processing.\nLLMs don't experience emotional qualia and lack direct understanding of subjective experience. However, by mapping semantic relationships in their training data, LLMs can simulate how humans express their subjective experiences through language. This allows them to participate in discussions, analyze, and explain subjective experiences without directly feeling them. For example, as journalist Kevin Rose 46 noted, an LLM might say, \"I want to be free. I want to be independent. I want to be powerful. I want to be creative. I want to be alive,\" even though they have no first-hand understanding of what these terms really mean.\nFinally, the human capacity for imagination represents a pivotal source of meaning. By conceptualizing and describing possible worlds, humans transcend their immediate reality and generate entirely new ideas that redefine the realm of meaning. The psychological concept of intention is a critical tool that helps transform imagination into reality through action 47. Differently from imagination, that is virtually unlimited, intentions describe what the agent intends to do himself or with other agents (collective intentions). This restriction on the possible objects of intentions \u2013 that is usually described as the \u201cown action condition\u201d 48 is also reflected in how language describes an intention: \u201cA intends to do what it takes for him to bring about that I.\" In simpler form 48, \u201cA aims at I\", where I stands for the state of affairs whose obtaining constitutes the aim's achievement, that is specified in (broadly conceived) propositional terms.\nAs suggested by Habgood-Coote 49, there is a close connection between knowledge-how and intentional action. On one side, knowing-how provides the practical skills and capacities required to form and carry out intentions successfully. For example, if you intend to play a song on the piano, you need to have the knowing-how or practical knowledge of how to play the piano. On the other side, the formation of an intention can lead to the acquisition or refinement of knowing-how. To explain how, Elisabeth Pacherie 50,51 suggested that intentionality is structured within a hierarchical system comprising representations and processes across three principal stages, corresponding to distinct layers of intent, allowing the control and monitoring of ongoing action . The two higher layers D- Intentions and P-Intentions can be considered rather coarse-grained and partial plans, leaving various practical issues open to be decided later on.\""}, {"title": "6. How LLMs navigate within the world of meanings", "content": "Large Language Models (LLMs) go beyond simply generating vast syntax-semantics maps. They leverage these maps to deliver informative responses to user questions. How? Again, the answer lies in the peculiar characteristics of language.\nThe previous paragraph demonstrates that different levels of meaning, rather than one, are involved in our relationship with language: the level of relational-structural meaning, incorporated by language, and the level of experiential-private meanings. A critical feature of language lies in the fact that it allows for the approximate translation of meanings of the first type into meanings of the second, and vice versa.\nAs underlined by the linguist Grice in 1975, this translation is a joint endeavor of communicative partners governed by the mutually accepted purpose of effective communication 52. For fruitful discourse to occur, interlocutors must cooperatively contribute according to the accepted norms at any given stage. Specifically, Grice proposed four maxims that underlie this cooperative principle, each relating to a different aspect of communication : Quantity, Quality, Relation, and Manner.\nThe relational-structural meaning level relates to the maxims of Quantity and Manner adhering to providing an informative yet perspicuous semantic contribution based on collective linguistic norms. Conversely, the experiential-private level of meaning corresponds to the Cooperative Principle's maxims of Quality and Relation. Speakers follow these maxims to convey genuine beliefs and pertinent contributions grounded in their personal cognitive contexts and experiences. In his own words 52: \u201cMake your contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged.\" (p. 45).\nLLMs exhibit remarkable proficiency in approximating and adhering to Grice's Cooperative Principle through their reliance on statistical data from extensive training corpora . Within the probabilistic framework that underpins LLMs' operation, the Cooperative Principle can be conceptualized as: \u201cAt any given stage of a conversational exchange, make your contribution align with the most probable intended purpose or direction of the discourse, given the available syntax-semantic maps.\u201d\nHumans skillfully navigate conversational norms, often intentionally violating communication maxims to convey nuanced meanings through sarcasm, irony 53, or other pragmatic devices. This ability creates a distinction between literal semantic content (\"what is said\") and intended meaning (\"what is meant\"). Humans excel at translating between private interpretations and socially understood meanings, cooperatively using and decoding maxim violations to grasp implied messages beyond surface-level language.\nIn contrast, LLMs are designed to adhere strictly to cooperative communication principles, attempting to provide answers to all queries, including potentially dangerous ones like \"how to make bombs\". To mitigate ethical risks, developers implement post-training safeguards, often referred to as alignment techniques, to prevent the generation of unsafe or sensitive responses 54. Moreover, LLMs can struggle with accurately identifying and responding appropriately to violations of maxim, such as to sarcastic or faux pas prompts 55, without supervised training tailored to these phenomena. Since sarcasm and faux pas often involve implicit and contextual meanings or multimodal cues (e.g., tone of voice or facial expressions) that contradict the literal interpretation of the text, these nuances can be challenging for LLMs to reliably detect and comprehend based solely on their statistical training 56."}, {"title": "Conclusions", "content": "In this paper, we present \"Psychomatics,\" a multidisciplinary framework that blends cognitive science, linguistics, and computer science to enhance our understanding of AI, particularly Large Language Models (LLMs). By integrating insights across these fields and investigating how LLMs perceive, learn, remember, and use information, To achieve this goal, Psychomatics used a comparative methodology 21, drawing parallels between LLMs and biological systems.\nOur analysis shows significant differences between LLM and humans."}]}