{"title": "Everyday AR through AI-in-the-Loop", "authors": ["Ryo Suzuki", "Mar Gonzalez-Franco", "Misha Sra", "David Lindlbauer"], "abstract": "This workshop brings together experts and practitioners from augmented reality (AR) and artificial intelligence (AI) to shape the future of AI-in-the-loop everyday AR experiences. With recent advancements in both AR hardware and AI capabilities, we envision that everyday AR-always-available and seamlessly integrated into users' daily environments-is becoming increasingly feasible. This workshop will explore how AI can drive such everyday AR experiences. We discuss a range of topics, including adaptive and context-aware AR, generative AR content creation, always-on AI assistants, AI-driven accessible design, and real-world-oriented AI agents. Our goal is to identify the opportunities and challenges in AI-enabled AR, focusing on creating novel AR experiences that seamlessly blend the digital and physical worlds. Through the workshop, we aim to foster collaboration, inspire future research, and build a community to advance the research field of AI-enhanced AR.", "sections": [{"title": "1 MOTIVATION", "content": "Augmented Reality (AR) technologies are continuously advancing, both in terms of hardware and software. Smaller form factors, extended battery life, and constant connectivity make it possible to results in a paradigm shift in how we think about AR: while current AR scenarios are focused on specialized applications such as productivity or maintenance [11, 12], we believe that everyday AR is increasingly feasible. With everyday AR we refer to the making AR always available to users, enabling seamless interactions with the digital world, and potentially replacing or augmenting currently predominant technologies such as smartphone or desktop computing. Users will be able to ubiquitously query and interact with digital information, communicate with other users and virtual agents [4], and leverage AR for a majority of their interactions.\nThis rise of everyday AR is fueled not just by improvements in hardware and software infrastructure, but is also heavily tied to recent advances in Artificial Intelligence (AI) and Machine Learning (ML). Generative AI techniques [13], for example, enable the creation of multi-modal digital content on-the-fly; and Large Language Models [5] lead to advancements in various applications domains and make it feasible for users to interact with virtual agents through text and other inputs in a \"natural\" way. Leveraging advances in those areas will enable researchers and practitioners to develop new and refine existing interactions for AR. We believe that in order to make everyday AR feasible, we need to adopt an AI-in-the-loop approach, where the digital interactions and content continuously anticipate and adapt to users every-changing needs and context. Thus, we believe the combination of AR and AI will enable us to move beyond monolithic applications in AR towards a truly user-centered and adaptive experience where both scene understanding and content generation becomes dynamic.\nThe goal of this workshop, which is an evolution of our previous XR and AI Workshop at ACM UIST 2023 [24], is to bridge the fields of AR and AI and enable discussions around the feasibility and requirements for AI-enabled everyday AR. Our previous workshop was broadly centered around the topic of what is possible when AR and AI are combined, the second iteration focuses on everyday AR interactions. We plan to explore usage scenarios of this new paradigm, explore requirements and limitations of current and future AR with AI-in-the-loop, and identify and address common challenges of AR (e.g., accessibility, privacy) and AI (e.g., explainability, sustainability). In that sense, we consider this workshop as a natural evolution, not only we meet but also define a way forwards for the community. This workshop seeks to further refine the thinking of the community towards a shared vision, and should lay the foundation for future workshops at conferences, longer seminars, and work towards a reference paper to consolidate the research efforts. As such we propose a workshop very oriented to shared vision, and"}, {"title": "1.1 Perspectives of Interest", "content": "This workshop welcomes HCI researchers and practitioners in AR, AI, machine learning, and computational interaction domains to share diverse perspectives and expertise. There are several domains that are not fully explored yet in the literature of XR and AI [18]. We plan to discuss the topics that include but are not limited to the following areas:\nAdaptive and Context-Aware AR. Unlike traditional user interfaces on smartphones or computers, AR can embed virtual elements into the user's physical world. However, without careful design, such interfaces can become overwhelming and distract users unnecessarily. Context-aware AR aims to seamlessly blend these virtual elements by automatically adapting them based on users' needs, context, and environment [14, 22, 23]. Recent research has explored various aspects of context-aware AR, such as using computational optimization methods to blend AR interfaces into physical objects [16], leveraging everyday objects to provide affordances for virtual interactions [17, 19], and applying adaptive AR for specific use cases like improving accessibility for people with low vision [20]. We are interested in exploring how to expand current research into broader everyday AR applications by leveraging advanced Al capabilities. These capabilities can help AR systems better understand various types of contextual information, such as room geometry, the affordances of physical objects, and user activities, enabling more seamless and adaptive experiences.\nAlways-on Al Assistant by Integrating LLMs and AR. Large Language Models (LLMs) have significant potential to augment AR experiences. Existing LLM interfaces, like ChatGPT, require explicit user interaction, but integrating LLMs into everyday AR interfaces allows for always-on Al assistants that can implicitly support user activities and integrate naturally into the environment. For example, rather than requiring users to type questions on a screen, the integration of LLMs and AR can facilitate seamless interactions with digital content, making AR more intuitive. We are interested in exploring how the integration of LLMs can extend beyond simple screen-based interactions to enable new, multimodal applications within AR environments. Recent work has demonstrated how augmented object intelligence can make the physical world interactable in AR [9], on-demand mixed reality document enhancement [15], gaze and gesture-based question answering [21], and everyday assistance based on current activities through wearable devices [3]. We are interested in exploring how this integration can be advanced with the multimodal capabilities of LLMs. Moreover, we believe there are significant opportunities beyond just textual output modalities, utilizing embedded visual, tangible, and spatial modalities that are unique to AR interfaces.\nAl-Assisted Task Guidance in AR. AI-assisted task guidance offers significant advantages over traditional video-based instruction [6]. While videos provide static, one-way demonstrations, AI systems can analyze a user's movements and activities in real-time, offering personalized feedback and corrections. We are interested"}, {"title": "2 ORGANIZERS", "content": "Ryo Suzuki is an Assistant Professor in the ATLAS Institute and Department of Computer Science at the University of Colorado Boulder. His research interests focus on augmented reality and tangible user interfaces. His research aims to transform everyday environments into dynamic physical and spatial media with the power of AI and AR. His website is https://ryosuzuki.org/.\nMar Gonzalez-Franco is a Research Manager at Google, where she leads Blended Interaction Research and Devices (BIRD) group. She has pioneering research in the field of extended reality in various domains such as human perception of virtual avatar, haptic interfaces for VR, multi-device interactions for XR, and ML-enabled novel experiences. Her website is https://margonzalezfranco.github.io/.\nMisha Sra is an Assistant Professor in the Department of Computer Science at the University of California, Santa Barbara, where she leads the Human-AI Integration Lab (HAL). Her research focuses on the design of Human-AI systems in XR to augment human potential which includes understanding human psychology, building specialized hardware for data collection, designing new Al models, creating interactive human-AI systems, and conducting large scale evaluations of novel applications. Her website is https://sites.cs.ucsb.edu/~sra/.\nDavid Lindlbauer is an Assistant Professor at the Human-Computer Interaction Institute at Carnegie Mellon University where he leads the Augmented Perception Lab. His research focuses on creating and studying enabling technologies and computational approaches for adaptive user interfaces to increase the usability of AR and VR interfaces, with applications in casual interaction, productivity, health and robotics. His website is https://davidlindlbauer.com/"}, {"title": "3 WORKSHOP FORMAT AND PLANS", "content": "3.1 In-Person Workshop Structure\nThe workshop will be a one-day, in-person event featuring a variety of activities, including small-group discussions, paper presentations, keynote talks from invited experts, and hands-on design activities to explore the future of AI-enabled everyday AR. One of the primary goals is to foster a strong community around this emerging topic, leveraging the networking opportunities provided by face-to-face interactions, such as a group lunch and dinner. The design sprint activities will particularly benefit from the collaborative nature of in-person participation, allowing teams to work flexibly with shared digital and physical materials for prototyping. To ensure an intimate and engaging workshop experience, we plan to cap attendance at 30 participants. This small group size will help create an environment that encourages meaningful interactions and effective collaboration."}, {"title": "3.2 Workshop Activities", "content": "The workshop will include a series of interactive and talk-based activities, similar to our successful workshop at ACM UIST 2023, as shown in Figure 1. In the previous UIST workshop, we had a total of 40 participants from both academia (32) and industry (8), representing diverse countries, genders, fields, backgrounds, and levels of experience. They actively engaged in lightning talks, prototyping sessions, discussions, and design activities. For the upcoming iteration, we will retain these participant-centered activities while also expanding the workshop to include position paper presentations from attendees and a keynote talk to inspire a vision for the future of AI and AR in everyday contexts. Below, we outline our current workshop format and plans.\nIntroductions and Lightning Talks. The organizers will kick off the workshop with introductions and each participant's lighting talks highlighting their related research experience. To start community building, we will facilitate pre-workshop engagement to cultivate and propose an initial set of themes through the creation of a shared slide deck that highlights each participant's background and interests. We will invite two keynote speakers who will deliver presentations to provoke further discussions.\nPosition Paper Presentations. In this workshop, we will announce a public call for position papers from the CHI community, inviting researchers and practitioners to contribute their insights. The purpose of these talks is to provide an opportunity for attendees to share their opinions and positions on their recent and ongoing work related to AR and AI. We aim to foster a collaborative environment where participants can present radical ideas, exchange feedback, and inspire new directions for research. For each accepted position paper, we plan to allocate 15-20 minutes for presentations. Additionally, with the consent of the participants, we plan to publish these position papers after the workshop to make the insights available to a broader audience and promote further discussion.\nKeynote Talks. We will also invite distinguished speakers to present keynote talks, with the goal of sharing forward-thinking visions and ideas with the audience. These talks are intended to inspire and set future directions for the field, similar to the UIST Vision Talks. Each keynote session will be approximately one hour long, providing ample time for in-depth exploration of ideas. We also plan to publish the title, speaker bio, and abstract on the workshop website to provide attendees with context and background information ahead of the event.\nParticipant Activities. Our workshop aims to provide a balanced mix of presentations and interactive participant activities to foster engagement and collaboration among attendees. To achieve this, we have organized several activities designed to encourage hands-on exploration and creative problem-solving. These include:\n1) Hackathon and Rapid Prototyping: Teams use low-fidelity materials (paper, cardboard, markers) to mock up an XR interface or device. Teams incorporate how AI might enhance the user experience. Teams present prototypes in a \"science fair\" style showcase.\n2) Design Sprint: We will challenge participants to redesign a common XR experience for users with specific disabilities. Users may integrate AI to propose adaptive interfaces or alternative input"}, {"title": "Theme Organization and Discussion", "content": "Participants will collectively extrapolate themes from the presentations and shared readings. The discussed themes will be combined with the initial set of topics developed prior to the workshop to serve as guiding directions for the first round of discussions. Once a preliminary list of discussion topics has been defined, each topic will be assigned a 'table. During the session, participants will rotate between tables to engage in focused discussions of topics of their choice. One participant at each table will be designated as the discussion mediator, whose responsibilities will involve guiding and documenting the discussion. The second session on theme organization will begin with lightning talks by the discussion mediators summarizing earlier conversations. Participants will then collectively revisit the discussion topics, reorganizing accordingly based on the results of the first session and expert perspectives. With the refined list of topics, the remainder of the session will follow the same format as the first round of discussions."}, {"title": "Defining Future Challenges and Research Directions", "content": "In this final discussion session, we will begin by regrouping and refining the list of discussion topics based on the results of the discussion sessions. The final workshop session will focus on summarizing the workshop findings and defining next steps. First, the organizers will provide a recap of the workshop activities, including the defined themes from the pre-workshop activities and a summary of the morning and afternoon discussion outcomes. The floor will then be opened for participants to contribute their reflections on the workshop discussion. A final discussion will be held around potential future directions, such as follow-up workshops and publications."}, {"title": "4 CALL FOR PARTICIPATION", "content": "This one-day, in-person CHI 2025 workshop invites researchers, practitioners, and students interested in exploring the future of AI-enabled augmented reality (AR) to join us in shaping a vision for \"Everyday AR.\" With recent advancements in AI and hardware technologies, the idea of always-available, seamlessly integrated AR is becoming increasingly feasible. Our workshop will focus on how AI can enable adaptive, user-centered AR experiences that seamlessly integrate into everyday life. Participants in the workshop will discuss with a variety of topics, including adaptive and context-aware AR, generative AR content creation, LLM-powered always-on Al assistants, AI-driven accessibility, and the development of real-world-oriented Al agents. Through a combination of keynote talks, participant-led presentations, and hands-on activities, attendees will have the opportunity to co-create visions of the future, exchange ideas, and contribute to the development of truly adaptive and AI-enhanced AR. Participants will also learn about the state-of-the-art in this space, hear perspectives from experts in AR and AI, and participate in collaborative prototyping and design activities. We aim to create a shared vocabulary and research agenda for Al-driven AR, inspiring new directions and fostering future collaborations. To apply for participation, interested individuals should complete an application form indicating their background, experience, and interests in AR and AI by February 15th. Applicants are encouraged to optionally submit a position paper of up to 2 pages (without references) in the ACM double-column format. Participants will be selected based on their application materials, with a focus on the relevance of their research or disciplinary background to the workshop topic, and a goal of creating a diverse and academically engaging environment."}, {"title": "4.1 Registration Requirements", "content": "Note that per CHI 2025 policies, workshop participants (including at least one author of each position paper, for applications that optionally include position papers) must attend the workshop in Yokohama, and all participants must register for both the workshop and for at least one day of the conference."}, {"title": "4.2 Website and Participant Recruitment", "content": "The website of the workshop can be found at https://xr-and-ai.github.io/. We will distribute a Call for Participation on the website through our research groups, professional networks, social media platforms, previous attendees of the UIST workshop, and HCI-related mailing lists. Prospective attendees will be invited to complete a short form describing their background, experience, and interests. Workshop organizers and invited external experts will collectively curate an inclusive selection of participating researchers based on the diversity of career levels, research interests, world locations, and genders, while endeavoring to match expertise in experience in XR and AI research."}, {"title": "4.3 Plans to Publish Workshop Proceedings", "content": "We plan to publish the content submitted by participants, including position papers, as workshop proceedings to ensure accessibility and foster ongoing collaboration. These materials will be hosted on our website, and we will encourage participants to also publish their work on open-access platforms such as CEUR-WS or arXiv."}]}