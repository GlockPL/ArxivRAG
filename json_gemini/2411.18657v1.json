{"title": "Scale Viz: Scaling Visualization Recommendation Models on Large Data", "authors": ["Ghazi Shazan Ahmad", "Shubham Agarwal", "Subrata Mitra", "Ryan Rossi", "Manav Doshi", "Vibhor Porwal", "Syam Manoj Kumar Paila"], "abstract": "Automated visualization recommendation (Vis-Rec) models help users to derive crucial insights from new datasets. Typically, such automated Vis-Rec models first calculate a large number of statistics from the datasets and then use machine-learning models to score or classify multiple visualizations choices to recommend the most effective ones, as per the statistics. However, state-of-the-art models rely on a very large number of expensive statistics and therefore using such models on large datasets becomes infeasible due to prohibitively large computational time, limiting the effectiveness of such techniques to most large real-world datasets. In this paper, we propose a novel reinforcement-learning (RL) based framework that takes a given Vis-Rec model and a time budget from the user and identifies the best set of input statistics, specifically for a target dataset, that would be most effective while generating accurate enough visual insights. We show the effectiveness of our technique as it enables two state of the art Vis-Rec models to achieve up to 10X speedup in time-to-visualize on four large real-world datasets.", "sections": [{"title": "Introduction", "content": "As more and more data is being collected from various sources, users often encounter data that they are not familiar with. A dataset can contain numerous columns, both numerical and categorical, with multiple categories. It is a daunting task to even decide how to dissect and plot such data to reveal any interesting insights. Visualization recommendation (Vis-Rec) techniques [6,13] help to automatically generate, score, and recommend the most relevant visualizations for a dataset and can improve productivity by reducing the time required by analysts to first find interesting insights and then visualize them.\nAutomated Vis-Rec techniques typically work through the following steps. First, they calculate various statistics, as much as up to 1006 number of statistics as used by Qian et al. in [13] per column from the data to capture overall statistical landscape of the data. Second, these statistics are used as features to score prospective visualization configurations (i.e. combination of columns, aggregates, plot types etc.) in a supervised learning setup. Finally, queries are issued against the actual data to populate the top recommended visualization charts. A significant number of prior works [4-7, 15] focused on perfecting the visualization recommendation technique, which evolved from initial algorithmic approaches to most recent deep-learning based approaches [6, 11, 13]. Further, Qian et al. [13] extended these techniques to address the problem of how to generalize these models on unseen datasets having completely different schema structure and data distributions. The way such generalization work is that the neural network learns the importance of different visualizations at much abstract level by extracting a large number of higher order statistical features extracted from the data. However, prior works did not address another very important problem, which is the scalability of these algorithms on datasets with large number of columns and/or rows. Real world datasets can be huge, having several hundreds of millions or even several hundreds of billions of rows. Calculating large number of statistical features on such large datasets is intractable by the state-of-the-art (SOTA) visualization recommendation algorithms. For example, in Qian et al. [13] collects 1006 various higher-order statistics per column of the dataset, which itself can have a large number of columns. On top of that, they calculate multi-column statistics to capture dependency, correlation and other such properties. Now, there can be two ways to overcome this problem:\nFirst option could be to drop certain statistics to reduce the computation. But which ones? These statistics are basically the features to the core visualization recommendation model. Which statistics are important and carry important signals that would make a particular combination of columns and visualization style interesting - is very dataset dependent. A statistics that is very computationally intensive might carry significant importance for one dataset and might not be relevant for another dataset. Indiscriminate dropping of certain statistics or identifying the important statistics based on few datasets and extending that decision to other datasets, can lead to poor quality output.\nSecond option could be to take a small sample of the data, on which calculation of large number statistics is tractable, and then generate the visualization recommendations based on that sample. However, for massive amounts of data, such sample has to be a tiny fraction for the existing Vis-Rec pipeline to work and such a tiny sample may not be representative of the complete data. Therefore, the visualization recommendations generated on the sample can be completely misleading or inaccurate.\nTo overcome these drawbacks of naive ways to speed-up visualization recommendation generation, in this paper we present a framework, called SCALEVIZ (code), that takes such a generalized Vis-Rec model and customizes it for a given dataset so that we can produce visualization recommendations at large"}, {"title": "Problem Formulation", "content": "In this section, we first formally define the problem of budget-aware visualization recommendation generation.\nLet P be a target Vis-Rec model that a user wants to apply on a large tabular dataset D. Let D consists m columns and r rows. Let F be the feature space for dataset D based on statistical features used in the model P. As Vis-Rec models calculate a large number of different statistics from each column, let us denote the number of statistical features computed from each column be n. Let $f_{ij}$ denote the j-th feature for i-th column, where $i \\in \\{1, ..., m\\}$ and $j \\in \\{1, ..., n\\}$.\nWe introduce the cost function $c_k : F \\rightarrow R^{m \\times n}$, quantifying the computational time required to calculate each of the features based on a k fraction from D (i.e. such fraction will consist of 1/k rows of D). Notably, $c^1$ serves as the cost function for the entire dataset D, and for brevity, we use $c^1$ denoted as c throughout the paper.\nTo formalize the problem, we frame the statistical feature selection as an optimization task. Let $\\theta: F \\rightarrow \\{0,1\\}^{m \\times n}$ be a function mapping features to binary acquisition decisions. $\\theta(f) \\odot f$ gives a subset of features by ignoring the masked features, where $f \\in F$ and $\\odot$ is the Hadamard operator which calculates the product of two matrices of the same dimension. L is a loss function which compares the output of the model on two different input feature set. The objective is to find the feature mask minimizing the error in the model's prediction while ensuring the total cost of selected features adheres to the budget B:\n$\\min L[P(\\theta(f) \\odot f) \u2013 P(f)]$, subject to: $\\sum_{i,j} \\theta(f) \\odot c(f) \\leq B$ (1)\nHere, the budget B is constrained by the total computational cost of features calculated on the complete dataset:\n$B\\leq\\sum_{i,j}c(f)$ (2)\nNote, in this formulation, we use B, that is time-to-compute visualization recommendations as the constraint, because it is intuitive for users to specify a time-budget. Alternatively, we could also make this constraint relative to the size of D. In that case, $B \\leq r \\times \\sum_{i,j} c(f)$ where r is a particular user-specified fraction of the statistical feature computation time for the base Vis-Rec model."}, {"title": "Proposed Solution", "content": "We approach the above problem as a scenario where decisions are made sequentially over time and model the problem as a reinforcement learning problem.\nThe overall pipeline, as shown in Fig. 2, consists of a Cost profiler, which employs polynomial regression to estimate the computational cost of computing statistics across varying dataset sizes. This estimation is crucial for predicting costs without actually computing them. Subsequently, the RL agent training module teaches the agent to acquire features under budget constraints across increasing data samples. Once trained, the Inference pipeline utilizes the RL agent to select features for the given budget, computing only the learned subset of features on the entire dataset to obtain model predictions. We provide a detailed description of the two main components and also describe the RL agent training algorithm."}, {"title": "Cost Profiling", "content": "The Cost Profiler module profiles the computation time (cost) of each statistical feature across varying dataset sizes. It collects data points to estimate the computation cost for each feature on larger datasets without actual computation.\nGiven the dataset D, the cost function ck is obtained for k fractions of the dataset, denoted as {ck1,k2, ...,ck||}. For each feature fij, the goal is to predict its cost cij on the full dataset. Some features, such as column types, number of categories in a column, max-min value in a column, exhibit zero-cost,"}, {"title": "RL agent", "content": "We use an RL agent based framework to learn feature acquisition under budget constraints. Each episode consists of the agent choosing the important subset of features for a sample Sk. We define the state of the agent for an episode k as the feature set acquired by it so far in an episode (i.e $x^{k,t} = \\theta^{k,t}(f) \\odot f)$, where $\\theta^{k,t}$ is the mask of the features at time t. The action $a_{k,t}$ of the agent is to select a feature which has not been masked in the feature set (i.e $x^{k,t}$).\nAt every step t, the agent selects a feature $i_j$ and masks that feature as selected. The agent moves to the next state, which is $x^{k,t+1} = (\\theta^{k,t+1}(f) \\odot f)$. A cost of $c_{ij}$ is deducted from the remaining budget for choosing the feature. The reward for an action $a_{k,t}$ is calculated as the absolute change in the score before and after acquiring the feature, $i_j$ with a penalty of c.\n$r_t = \\frac{||P (x^{k,t}) \u2013 P (x^{k,t+1}) ||}{c_{ij}}$ (3)\nWe use the technique of double deep Q-learning with experience replay buffer [12] to train the RL agent. The agent explores the feature space with a e-greedy approach, with the probability of exploration decaying exponentially. The architecture of the Q-networks is a feed-forward neural network with three layers of sizes [512, 128, 64].\nAlgorithm 1 describes the training procedure for the RL agent, designed for cost-effective feature acquisition. The process initiates with the agent receiving a dataset D, a pre-defined budget B and a Vis-Rec model P. The dataset is sequentially explored through a series of samples. The algorithm initializes by setting an initial exploration probability $P_rrand$ and a termination threshold e. In each episode, the agent learns the important subset of features for a particular sample Sk. Every episode starts with the same budget, B and the size of the samples keeps increasing with the number of episodes. The RL agent starts with a zero-cost feature set and keeps acquiring features till it runs out of budget. At every step of an episode, the agent chooses to explore randomly or exploit the current knowledge by selecting the feature with the maximum Q-value. The tuple (state, action, next state, reward) is pushed into the experience replay buffer. The Q and the target-Q networks are periodically updated using the tuples from the buffer. The process is terminated when the loss for an episode falls below the threshold e. The increasing size of the samples across episodes helps the agent to exploit the learned behavior of the model on a larger sample. This is particularly important because, we ultimately want the agent to predict the important features on the full dataset which it has not been trained on.\nThe RL agent ultimately selects the important and highly sensitive statistical features for the target base Vis-Rec model P from a given dataset D."}, {"title": "Evaluations", "content": "We first evaluate the speed-up achieved by SCALEVIZ compared to the baselines approaches when we ensure that the resulting error due to use of less features (for SCALEVIZ, Random, and Greedy) or less data (for Sample) is less than 0.0002, 3.43e-05 for VizML and MLVR respectively. As can be observed that SCALEVIZ helps both the models to choose most effective features, tailored for each datasets, leading to generation of visual recommendtion generation upto 10.3 times faster, which is much higher than the baseline models."}, {"title": "Budget vs. Error Trade-off", "content": "We assess the recommendation errors of SCALEVIZ across various budget percentages of the total time on four distinct datasets for two Vis-Rec models. Notably, SCALEVIZ consistently outperforms baselines, showcasing significantly lower errors in visualization recommendations. This effect is particularly prominent at lower budget ranges, highlighting SCALEVIZ's capability to identify the set of most important statistical features that can be computed under a given time-budget constraint while minimizing respective errors for the corresponding base Vis-Rec models."}, {"title": "Need for Dataset-Specific Feature Selection", "content": "We now analyze if there is indeed a need for dataset specific feature-selection. For this, we investigate how much overlap there is in terms of the selected statistical features from different datasets after the runtime feature selection by SCALEVIZ'S RL-agent converges to a negligible error with respect to the baseline Vis-Rec models. It can be observed that IoU values ranges from 10% to a maximum of 22% for VizML. Similarly, for MLVR, the overlap varies from 3% to a maximum of 14%. This emphasizes the design choice of SCALEVIZ highlighting the fact that feature selection is highly dependent on both the choice of Vis-Rec model (P) and the target dataset (D) and a dataset agnostic pruning of features (even when done in a computation cost-aware manner) would remain suboptimal."}, {"title": "Scalability with Increasing Data Size", "content": "We now show how SCALEVIZ's benefit keeps on increasing as the size of the dataset (in terms of number of rows) increases. We define a saturation budget B' as the computation time taken by the selected features by SCALEVIZ where the resulting visualization recommendations has insignificant error ( \u2264 \u0454 ) compared to the base Vis-Rec model. We us $B_{MAX}$ to denote the time taken by the base Vis-Rec model to produce the visualizations. As can be observed SCALEVIZ saturated at around half the budget for a 1k dataset, saturated at around one-fifth of the budget for 100k, and its efficiency scales even more impressively with larger datasets, reaching about one-tenth of the budget for a dataset size of 1M. This scalability advantage positions SCALEVIZ as an efficient and cost-effective solution to boost Vis-Rec models for large datasets."}, {"title": "Conclusion", "content": "In this paper, we identify an important drawback of the state-of-the-art visualization recommendation (Vis-Rec) models that these models sacrificed the scalability in order to make them generalize over unknown datasets. Such models compute a very large number of statistics from the target dataset, which becomes infeasible at larger dataset sizes. In this paper, we propose SCALEVIZ- a scalable and time-budget-aware framework for visualization recommendations on large datasets. Our approach can be used with existing Vis-Rec models to tailor them for a target dataset, such that visual insights can be generated in a timely manner with insignificant error compared to alternate baseline approaches."}]}