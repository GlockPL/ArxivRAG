{"title": "SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models", "authors": ["Jianyi Zhang", "Da-Cheng Juan", "Cyrus Rashtchian", "Chun-Sung Ferng", "Heinrich Jiang", "Yiran Chen"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have achieved remarkable breakthroughs in recent years, demonstrating exceptional performance across various domains . However, a significant challenge associated with LLMs is their tendency to hallucinate or distort the truth, resulting in outputs that are not factual . This issue of hallucination undermines the reliability and trustworthiness of LLMs in practical applications. A popular strategy for improving the LLM factuality involves refining the decoding process . Decoding focuses on how the model selects the next token during the generation process, which can significantly influence the factual accuracy of the output. The decoding methods can be cost-effective since (a) they do not rely on external knowledge and (b) no additional training is required. Furthermore, decoding methods can be synergistically combined with other techniques aimed at improving the LLM factuality, such as retrieving information from external knowledge bases , various fine-tuning strategies for better alignment , or ensemble learning methods ."}, {"title": "2 Self Logits Evolution Decoding", "content": "A large language model, equipped with N layers and a vocabulary V = [v_1, v_2, ..., v_d], typically generates text in the next-token prediction fashion. For each given prefix, the model computes the logits at the final (N-th) layer, $\\text{logits}^N \\equiv (l_{(1, N)}, l_{(2, N)},..., l_{(d, N)})$, which are obtained by applying a linear transformation to the hidden states of the final layer, projecting the high-dimensional hidden state vectors onto the space of the vocabulary size. Subsequently, the output distribution $P_{\\text{logits}}^N$ at the final (N-th) layer for the next token is derived by applying softmax function on the logits,\n$P_{\\text{logits}} \\equiv (P_{(1,N)},...,P_{(d,N)}) = \\text{softmax}(\\text{logits}^N/\\tau)$, \nwhere $\\tau$ is the temperature parameter. Therefore, for each $p_{(i,N)}$ ($1 \\le i \\le d$), we have\n$P_{(i,N)} = \\text{exp}(l_{(i,N)}/\\tau)/S$, where $S = \\sum_{j=1}^d \\text{exp}(l_{(j,N)}/\\tau)$.\nSimilarly, we can also derive the logits from early layers by applying the same linear transformation mentioned above to their hidden states. For any early layer n (n < N), we denote its logits as $\\text{logits}^n \\equiv (l_{(1,n)},..., l_{(d,n)})$ and the corresponding distribution as $P_{\\text{logits}}^n \\equiv (P_{(1,n)},\u2026\u2026\u2026,P_{(d,n)})$."}, {"title": "2.1 Logits Evolution", "content": "To improve factual accuracy, it is crucial that the correct token $v_i$ receives a higher value of $\\text{logits}^N$ to ensure a higher probability value $P_{(i,N)}$ in the output distribution $P_{\\text{logits}}^N$. From a mathematical perspective, this means aligning the model's output distribution $P_{\\text{logits}}^N$ closely with the real-world factuality distribution $P_{\\text{real}}$. Specifically, we can formulate this goal as optimizing the following loss function $\\mathcal{L}$ regarding the logits:\n$\\mathcal{L}(\\text{logits}) \\equiv KL(P_{\\text{real}}, P_{\\text{logits}})$, where $\\text{logits} = (l_1, ..., l_d)$, $P_{\\text{logits}} = \\text{softmax}(\\text{logits}/\\tau)$   (1)\nWe describe the above optimization as Logits Evolution. Interestingly, the training of LLMs also aims at minimizing the divergence (typically the KL divergence, as the training loss function is often the cross-entropy loss) between the ground truth $P_{\\text{real}}$ and the output distribution $P_{\\text{logits}}^N$. During the training phase, the logits evolution is driven externally by the real-world distribution $P_{\\text{real}}$ presented in the training dataset, and the corresponding solution is $\\text{logits} = \\text{logits}^N$. However, $P_{\\text{real}}$ is not accessible during the inference phase. To address this challenge, SLED utilizes the model's latent knowledge to estimate $P_{\\text{real}}$ and enables \"self-evolution\" of the logits. We denote the estimation as $P_{\\text{latent}}$ and the self logits evolution can be achieved by the following gradient-descent operation:\n$\\text{logits}^N = \\text{logits}^N - \\alpha \\cdot \\nabla_{\\text{logits}^N} KL(P_{\\text{latent}}, P_{\\text{logits}}^N)$.  (2)\nThe parameter $\\alpha$, termed the Evolution Rate, governs the magnitude of adjustments applied to $\\text{logits}^N$ in the direction of the gradient $\\nabla_{\\text{logits}^N} KL(P_{\\text{latent}}, P_{\\text{logits}}^N)$. In the following Section 2.2 and 2.3, we discuss how we derive the $P_{\\text{latent}}$ as the estimation of the real-world distribution $P_{\\text{real}}$."}, {"title": "2.2 Estimate P_real by Tracking the Logits Evolution Direction throughout Layers", "content": "The core principle of our method involves leveraging the difference between each early layer's logits and the final layer's logit, $\\text{logits}^n - \\text{logits}^N$ to approximate the gradient of $KL(P_{\\text{real}}, P_{\\text{logits}})$ at $\\text{logits} = \\text{logits}^n$. Then we estimate $P_{\\text{real}}$ based on this approximation.\nThis is inspired by a new perspective of interpreting the training phase of LLMs as the evolution of logits described in Problem 1. As mentioned above, the solution derived by the training phase is the final layer's logits $\\text{logits} = \\text{logits}^N$, since the final layer's logits directly engage with the real-world distribution $P_{\\text{real}}$ through the loss function in training. This implies that we can generally consider the final logits $\\text{logits}^N$ to be a better solution than the logits from an early layer $\\text{logits}^n$, with $KL(P_{\\text{real}}, P_{\\text{logits}^N}) < KL(P_{\\text{real}}, P_{\\text{logits}^n})$. Based on this discussion, if we contrast the final layer's logits with the early layer's logits, we can consider the direction (orientation) of $\\text{logits}^n - \\text{logits}^N$ can approximately align with the direction of the gradient $\\nabla_{\\text{logits}} KL(P_{\\text{real}},P_{\\text{logits}})|_{\\text{logits}=\\text{logits}^n}$. To further verify this motivation, we calculate the cosine similarity between $\\text{logits}^n - \\text{logits}^N$ and $\\nabla_{\\text{logits}} KL(P_{\\text{real}}, P_{\\text{logits}})$ for values are positive, which means that the directions of these two vectors are close.\nHence, for each early layer n, we propose to maximize the following function of cosine similarity and derive the $P_{\\text{latent}}^{(n)}$ to estimate the $P_{\\text{real}}$.\n$P_{\\text{latent}}^{(n)} = \\text{arg max}_P \\text{CosSim}(\\text{logits}^n - \\text{logits}^N, \\nabla_{\\text{logits}} KL(P, P_{\\text{logits}}))$ (3)"}, {"title": "2.3 Achieving the Self Logits Evolution in Three Phases", "content": "Based on the above analysis, we can introduce the procedures of SLED: First we estimate $P_{\\text{latent}}^{(n)}$ for each early layer n using the gradient approximation in Section 2.2. Subsequently, we apply a weighted average on {$P_{\\text{latent}}^{(n)}$} across all early layers n < N to derive $P_{\\text{latent}}$, which serves as the final estimation of the real-world distribution. Finally, we apply $P_{\\text{latent}}$ in Equation 2 to facilitate the self-evolution of $\\text{logits}^N$, thereby derive the updated logits, $\\text{logits}^N$.\nPhase 1: An exhaustive search for an exact solution to the complex optimization problem (Equation 3) is computationally impractical. We can reduce the solution space by the following. Suppose that the real-world factuality distribution dictates that the next word to be generated is the i-th token $v_i$ from the vocabulary V. Thus $P_{\\text{real}} = P_{e_i}$, where $P_{e_i}$ represents a standard basis vector (one-hot vector) with the i-th component set to 1 and all other components set to 0. Then, we can simplify the aforementioned optimization problem by limiting the solution space to {$P_{e_i}$}=0 and decide which token i should be selected. The corresponding gradient when $P = P_{e_i}$ has the following formulation.\nProposition 1. The gradient of $KL(P_{e_i}, P_{\\text{logits}})$ at $\\text{logits} = \\text{logits}^n$ is:\n$\\nabla_{\\text{logits}} KL(P_{e_i}, P_{\\text{logits}}) = (P_{\\text{logits}} - P_{e_i})/\\tau = (P_{(1,n)},..., P_{(i,n)} - 1,...,P_{(d,n)}) / \\tau$  (4)\nWe calculate the cosine similarity between the gradient $\\nabla_{\\text{logits}} KL(P_{e_i}, P_{\\text{logits}})$ and the difference $\\text{logits}^n - \\text{logits}^N$ for each token in the vocabulary V. Then we select the $P_{e_{i^*}}$ of which the gradient is closest to $\\text{logits}^n - \\text{logits}^N$ as the estimation $P_{\\text{latent}}^{(n)}$. Mathematically, this involves selecting i* according to the following criterion\ni^* = \\text{arg max}_{1<i<d} m_i^{(n)}, \\text{where } m_i^{(n)} = \\text{max}( \\text{CosSim}(\\text{logits}^n - \\text{logits}^N, P_{\\text{logits}} - P_{e_i}), 0),\nand adopting $P_{\\text{latent}} = P_{e_{i^*}}$ as the \"hard estimation\" of $P_{\\text{real}}$. Drawing from the concept of hard and soft targets in label smoothing and knowledge distillation, we further extend it to the \"soft estimation\",\n$P_{\\text{latent}}^{(n)} = (\\frac{m_1^{(n)}}{\\sum_{i=1}^d m_i^{(n)}},...,\\frac{m_i^{(n)}}{\\sum_{i=1}^d m_i^{(n)}},...,\\frac{m_d^{(n)}}{\\sum_{i=1}^d m_i^{(n)}})$, where $m^{(n)} = \\sum_{i=1}^d m_i^{(n)}$ is a normalization factor.\nPrior studies have shown that soft targets usually offer stronger generalization capabilities, carry more information, and are more robust to noise than hard targets . Hence, we also adopt the soft estimation in lieu of the hard estimation."}, {"title": "2.4 Computational Complexity and Design Decisions", "content": "For each layer, computing $\\text{CosSim}(\\text{logits}^n - \\text{logits}^N, P_{\\text{logits}}^n - P_{e_i})$ for every token $v_i$ in the vocabulary V needs O(d2) operations. To reduce the computational complexity, we select only a subset $V_I$, where the token $v_i \\in V_I$ it has the top-k highest logits in the final layer. In this scenario, we only initiate the self-evolution in Equation 2 of the logits corresponding to these top-k tokens. For the remaining tokens, which have lower probabilities, their logits are adjusted to a very lower numerical value, e.g., -1000. This strategy significantly reduces the computational complexity to O(k2) where k << d, while maintaining focus on the most relevant tokens. We name the parameter k, as Evolution Scale, since it determines the number of top-probability tokens active for self-evolution."}]}