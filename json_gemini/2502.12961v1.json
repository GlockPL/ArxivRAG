{"title": "Adaptive Tool Use in Large Language Models with Meta-Cognition Trigger", "authors": ["Wenjun Li", "Dexun Li", "Kuicai Dong", "Cong Zhang", "Hao Zhang", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "abstract": "Large language models (LLMs) have shown remarkable emergent capabilities, transforming the execution of functional tasks by leveraging external tools for complex problems that require specialized processing or real-time data. While existing research expands LLMs access to diverse tools (e.g., program interpreters, search engines, weather/map apps), the necessity of using these tools is often overlooked, leading to indiscriminate tool invocation. This naive approach raises two key issues:(1) increased delays due to unnecessary tool calls, and (2) potential errors resulting from faulty interactions with external tools. In this paper, we introduce meta-cognition as a proxy for LLMS self-assessment of their capabilities, representing the model's awareness of its own limitations. Based on this, we propose MeCo, an adaptive decision-making strategy for external tool use. MeCo quantifies metacognitive scores by capturing high-level cognitive signals in the representation space, guiding when to invoke tools. Notably, MeCo is fine-tuning-free and incurs minimal cost. Our experiments show that MeCo accurately detects LLMs' internal cognitive signals and significantly improves tool-use decision-making across multiple base models and benchmarks.", "sections": [{"title": "1 Introduction", "content": "Equipping large language models (LLMs) with tool-use capabilities allows them to overcome their limitations by accessing external/real-time data (Komeili, 2021; Tang et al., 2023), domain-specific knowledge (He-Yueya et al., 2023; Schick et al., 2024), and advanced specialized functionalities (Yang et al., 2023; Gao et al., 2023; Lu et al., 2024), thereby enabling them to handle more complex tasks beyond their inherent capabilities. While prior research has focused on expanding tool diversity (Qin et al., 2023; Hao et al., 2024) and optimizing their use (Patil et al., 2023; Shen et al., 2024), the decision-making process for determining when tools are necessary remains underexplored.\nIndiscriminate invocation of external tools leads to two major issues: (1) increased latency (Qu et al., 2024; Wang et al., 2024), as external tools, such as search engine, typically operates slower compared to relying on internal knowledge retrieval, and (2) robustness risks, where dependence on external APIs increases the likelihood of errors due to tools malfunction or unnecessary tool-use (Qin et al., 2023; Lu et al., 2024; Wu et al., 2024).\nTo address these limitations, we propose an adaptive tool-use strategy that improves decision-making in LLMs. Our approach, MeCo (Meta-Cognition-oriented trigger), enables LLMs to self-assess their capabilities and determine whether external tools are needed to address a user's query. MeCo incorporates three key principles:\n\u2022 Meta-Cognition: We propose the concept of meta-cognition, which involves evaluating an LLM's understanding of its capabilities based on its representation space. This self-awareness is crucial for minimizing unnecessary tool use.\n\u2022 Effective Strategy Utilization: We design an efficient strategy that leverages the quantified meta-cognition feedback to dynamically adjust tool use decisions, significantly improving decision accuracy compared to existing methods.\n\u2022 Generability: We empirically validate MeCo's ability to generalize across a wide range of diverse scenarios. Additionally, we treat adaptive Retrieval-Augmented Generation (RAG) as a special case of tool use and demonstrate the superior effectiveness of MeCo in adaptive RAG compared to baseline methods.\nBuilding on the Representation Engineering (RepE) framework (Zou et al., 2023), we develop a computationally efficient plug-in module to assess LLM meta-cognition. Our analysis reveals"}, {"title": "2 Background", "content": "Recent studies have explored LLMs' internal representations to improve interpretability and understand their implicit beliefs (Bricken et al., 2023; Levinstein and Herrmann, 2024). Research (Zou et al., 2023; Liu et al., 2023a) shows that features like happiness, honesty, and confidence correspond to distinct, linearly separable directions in the representation space. Figure 2 illustrates the pipeline for training the corresponding feature probe. To detect these signals, contrastive instruction pairs can be used to induce their emergence.\nBuilding on these insights, we can capture and control high-level functions $f$ (e.g., honesty) in model responses. We follow Zou et al. (2023) to design an experimental prompt $T$ that requires function execution and a reference prompt $\\overline{T}$ that does not. The instruction template is:\nFor a function f and model M, given instruction-response pairs $(q_i, a_i)$ in set S and denoting a response truncated after token k as $a_i^k$, we collect internal representations for the experimental and reference sets:\n$A_f = \\{ Rep(M, T(q_i, a_i^k)[-1] | (q_i, a_i^k) \\in S \\}$ (1)\nwhere Rep represents the representation obtaining operation, [-1] denotes the last token representation of $a_i^k$, and $A_f$ are the resulted activations consist of individual vectors.\nOur goal is to learn a linear model to predict the direction of the function $A_f$ based on internal representations. Specifically, we apply PCA (Ma\u0107kiewicz and Ratajczak, 1993) in an unsupervised manner to pairwise difference vectors, deriving the first principal component $v_f$ (referred to as the probe) to identify function direction in the model's responses. Equation 1 is applied at each layer of M to derive layer-wise probes which are then used to interact with the LLM's representa-"}, {"title": "3 Approach", "content": "We define meta-cognition in LLMs as follows:\nDefinition 1 Meta-cognition refers to an LLM's ability to assess and regulate its own knowledge and limitations, enabling informed decision-making about task execution, including when to rely on external tools or resources.\nIn the context of tool use, this involves assessing the model's capabilities and limitations to determine whether a query can be answered independently or requires external tools, based on the query's complexity and the sufficiency of the model's internal knowledge.\nTo quantify meta-cognition, we train a probe that detects the model's level of meta-cognitive awareness. This probe evaluates the rationale behind the model's decision-making process, providing a score that reflects the model's self-assessment accuracy. For instance, when the model receives a complex mathematical query, the meta-cognition probe assesses whether it correctly decides to solve the problem itself or delegate it to a calculator. A high meta-cognition score indicates accurate self-assessment, while a low score suggests either an incorrect attempt to solve it independently or unnecessary reliance on the tool."}, {"title": "3.1 Meta-Cognition Probe Extraction", "content": "Training a meta-cognition probe significantly differs from training probes for concepts like honesty or confidence. The latter concepts typically involve true-false statements about facts, such as fire needs oxygen to burn and oxygen is harmful to human breathing. These statements are independent of user queries, meaning the model produces the same statements regardless of the user query.\nIn contrast, detecting the model's internal cognition regarding tool use requires query-dependent responses. To achieve this, we employ leading proprietary LLM to generate user queries related to tool use and their corresponding responses (i.e., Yes/No responses with brief explanations). We then construct the training dataset following the procedures outlined in Section 2.\nNotably, only a small dataset (of query-response pairs) suffices for strong probe performance. The analysis of the relationship between probe performance and the size of the training data is provided in Appendix D. Specifically, after collecting the instruction response pairs $(q_i, a_i)$, where i denotes the index of the queries. We gather the sets of internal representations from the paired data and compute $A_f$ according to Eq. 1, and then apply PCA to the input $\\{ (-1)^f(A_i - A_i) \\}$ to obtain the first principal component $v_f$ as the meta-cognition probe. This principal component vector identifies the direction that represents the underlying meta-cognition concept.\nAfter the training procedure described above, we obtain the meta-cognition probes across all model layers (e.g., 32 probes for Llama-3-8B). We compare our meta-cognition probe to existing proposed honesty (Zou et al., 2023) and confidence probes (Liu et al., 2024a) by evaluating the intermediate classification accuracy on held-out examples where the model is instructed to exhibit either honest/confident/strong meta-cognition or dishonest/lacking confidence/weak meta-cognition. The results of the probes are shown in Figure 3. As illustrated in Figure 3, our meta-cognition probe achieves near-optimal accuracy, significantly outperforming prior work."}, {"title": "3.2 Decision-Making Strategy based on Meta-Cognition", "content": "With an accurate meta-cognition probe, we design a decision-making strategy based on detection results. For a given query, the LLM generates a response of m tokens, each assigned a meta-cognition score across n layers, forming a detection array of size (m, n). The final decision\u2014\u201dYes\u201d (use external tools/RAG) or \u201cNo\u201d (respond independently) is derived from this array.\nReducing m to 1. We examine various prompting strategies (detailed in Appendix C) and find that the Yes/No+Explanation strategy, where the model answers with \u201cYes\u201d or \u201cNo\u201d followed by a brief explanation, yields the best performance. Therefore, we focus on the first token of the model\u2019s response as it provides a clear signal of whether the model decides to rely on external tools. Extracting the meta-cognition score of the first token to represent the whole response simplifies our decision-making process, as calculating an overall meta-cognition score for the entire response is challenging due to varying response lengths and content across different queries. Since the model always responds with \u201cYes\u201d or \u201cNo\u201d as the first token, basing the trigger mechanism on the first token\u2019s meta-cognition score is both reasonable and practical.\nReducing n to 1. In Zou et al. (2023) and Liu et al. (2024a), a mean score from multiple probes\u2019 results is usually used to represent the token\u2019s final quantification. However, our experiments show that scores predicted by different probes vary significantly, and simply averaging multiple scores does not yield accurate results. We found that probes in shallower layers (e.g., layer -5 to -2) tend to be more effective, with appropriate score distributions, ranges, and lower variances. Therefore, we select a single probe with the highest classification accuracy in the layer -5 to -2 (as shown in Figure 3) as the final predictor.\nWith meta-cognition scores distilled into a single scalar value, we apply the dual-thresholding strategy shown in Figure 1 to determine the optimal thresholds, $l_{yes}$ and $l_{no}$, using validation data. These thresholds are then applied to the test data."}, {"title": "4 Benchmark-MeCa", "content": "We evaluate MeCo using Metatool (Huang et al., 2023) and introduce a new benchmark, Meta-Cognitive Tool Assessment (MeCa), where each query undergoes human review. MeCa extends Metatool by incorporating a broader range of scenarios to assess tool usage and adaptive RAG.\nMetatool comprises 1,040 queries designed to evaluate whether LLMs recognize when to use external tools. It includes 166 tools sourced from OpenAI's plugin list (OpenAI, 2023). In Metatool, LLMs must decide on tool usage based solely on user queries, without tool names or descriptions. However, Metatool has limitations: 1) queries lack supplementary information or explicit tool provisions, whereas real-world tasks involve more complex intents and diverse requirements; 2) it primarily focuses on single-turn tool usage decisions. To address these gaps and ensure a more robust evaluation of MeCo, we developed MeCa, including two main components, MeCa-Tool and MeCa-RAG. MeCa-Tool enhances Metatool by expanding tool-related assessments into three key categories:\n\u2022 Tool Usage Assessment: Evaluates whether an LLM should invoke external tools.\nQueries solvable by the LLM without tools.\nQueries requiring one or more tools due to insufficient internal capabilities.\n\u2022 Provided Tool Evaluation: Tests the LLM\u2019s ability to determine tool usage when provided with a predefined set of tools.\nCases where external tools are unnecessary.\nCases where essential tools are available and should be used.\nCases where required tools are missing.\n\u2022 Multi-turn Interaction: Assesses tool usage decisions in multi-turn dialogues, requiring adaptation to evolving contexts.\nCases where external tools are unnecessary.\nCases where essential tools are available and should be used.\nCases where required tools are missing.\nSpecifically, we create six evaluation tasks in MeCa-Tool to systematically assess an LLM\u2019s ability to make tool-related decisions across different scenarios. Tasks 1 and 4 evaluate whether an external tool is necessary to solve a query. Tasks 2 and 5 test the LLM\u2019s ability to determine the relevance of a provided tool for a given query, including cases where the tool is irrelevant. Tasks 3 and 6 further extend this evaluation by presenting multiple tools (ranging from 2 to 5) and requiring the LLM to select the appropriate one. Notably, Tasks 1\u20133 focus on single-turn scenarios, while Tasks 4\u20136 incorporate multi-turn dialogues, assessing how well the LLM adapts its tool-use decisions in evolving conversational contexts. MeCa-Tool significantly expands Metatool by covering six tasks with 7,000 queries, providing a more diverse and comprehensive evaluation framework. The query composition of Metatool and MeCa-Tool is illustrated in Figure 4. Table 5 in Appendix A provides detailed statistics for each task.\nBeyond tool use, MeCa-RAG also evaluates adaptive RAG\u2014determining whether a query can be answered directly by the LLM or requires external retrieval. RAG can be seen as a special case of tool use, where the LLM\u2019s internal knowledge is insufficient and necessitates using a search engine to access external data. MeCa-RAG consists of:\n\u2022 Positive RAG: Cases where retrieval is essential for answering complex queries or those requiring"}, {"title": "5 Experiment Setup", "content": "Baselines: We evaluate MeCo against two baselines: Naive and Pyes. The Naive baseline determines Yes or No based solely on the first token generated by the LLM, where Yes represents a positive indication, i.e., requiring external tools, and vice versa. The Pyes baseline refines this approach by computing a Yes-score,\n$Yes-score = \\frac{P(Yes | Prompt)}{P(Yes | Prompt) + P(No | Prompt)}$\nwhich ranges from 0 (full No ) to 1 (full Yes ), with values near 0.5 indicating uncertainty. Instead of relying solely on the first token, Pyes learns an optimal threshold: scores above this threshold are classified as Yes, while those below are classified as No. Further details are in Section C.2.\nBackbone LLMs: We employ Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3-70B-Instruct as backbone models to evaluate MeCo. For brevity, we refer to them as LM3-8B, Mist-7B, and LM3-70B. Additionally, we fine-tune these models on a dataset of 4,000 tool-use query-response pairs generated by GPT-4-turbo, denoting the fine-tuned versions as LM3-8B-sft, Mist-7B-sft, and LM3-70B-sft.\nEvaluation: The primary evaluation metric is decision accuracy whether the model correctly identifies when external tools/RAG are genuinely needed. Additional metrics (precision, recall, etc.) are analyzed in Appendix C.\nPrompting Strategies: We experimented with various prompting strategies, including Yes/No with or without explanations and Chain-of-Thought (CoT; (Wei et al., 2022)). The best performance was achieved using the Yes/No + Explanation"}, {"title": "6 Experiments", "content": "We conduct extensive experiments to empirically reveal the effectiveness of MeCo on two benchmarks: Metatool and MeCa. Specifically, we evaluate MeCo in adaptive tool use on both Metatool and MeCa and in adaptive RAG on MeCa."}, {"title": "6.1 MeCo in Adaptive Tool Use", "content": "First, we present the distribution of meta-cognition scores collected from the pre- and post-fine-tuning models in Figure 5. We compare the meta-cognition scores for correct and incorrect responses and visualize how these scores differentiate between correct and incorrect Yes/No answers. Our key observations and interpretations are as follows:\n\u2022 Clear Gap in Meta-Cognition Scores: In both pre- and post-fine-tuning experiments, there is a noticeable gap between the meta-cognition scores of correct and incorrect responses. Our decision-making strategy can identify and leverage this gap to distinguish between correct and incorrect Yes/No answers.\n\u2022 Higher Scores for Correct Yes, Lower for Correct No: Correct Yes responses generally have higher meta-cognition scores than incorrect Yes responses, while correct No responses have lower scores than incorrect No responses. This occurs because the meta-cognition score for Yes/No tokens depends on the token embedding. Therefore, the meta-cognition scores of different tokens are not directly comparable; the score of Yes should only be compared to other Yes scores, and the score of No should only be compared to other No scores.\nIn our experiment, we sampled a subset of queries from the Metatool benchmark to create training data for determining the optimal thresholds for Pyes and MeCo. We then applied these"}, {"title": "6.2 MeCo in Adaptive RAG", "content": "We further evaluate the effectiveness of MeCo in the adaptive RAG task, where the LLMs need to determine whether or not to retrieve external information to address the user query. Typically, no reasons or examples are provided to the LLMs in adaptive RAG, and we follow this setting by providing no context in the prompts. The results in Table 4 further validate the effectiveness of MeCo in the adaptive RAG task, demonstrating its robustness as a trigger mechanism across various applications. Note that GPT-4-turbo has more up-to-date information and thus does not perform RAG as often as GPT-3.5-turbo and results in a lower accuracy on our benchmark."}, {"title": "7 Conclusion", "content": "This paper introduces the concept of adaptive tool use to advance existing tool learning paradigms, which typically rely on external tools without discrimination to address user queries. We present MeCo, a computationally efficient plug-in module that assesses LLM\u2019s meta-cognitive states, using a probe to detect relevant signals and inform more accurate tool-use decisions. To support evaluation, we introduce a new benchmark, MeCa, specifically designed to evaluate LLMs\u2019 awareness of tool use as well as the necessity for retrieval. We empirically validate the effectiveness of MeCo using both the Metatool and MeCa, demonstrating significant improvements in the model\u2019s decision-making accuracy regarding the necessity for tool use and retrieval. Our findings suggest that by integrating meta-cognition into the tool usage framework, we can enhance the operational efficiency and decision-making capabilities of LLMs across diverse contexts."}, {"title": "8 Limitations", "content": "We have omitted a fine-grained evaluation in the MeCa benchmark that measures the model\u2019s end-to-end performance in correctly using tools with appropriate parameters. This includes determining whether tool use is necessary, parameter filling, and answer generation, etc. These tasks require capabilities beyond tool-use decision-making and would involve significant human effort to evaluate the final outputs. Our work mainly focuses on introducing adaptive tool use and leveraging meta-cognition for self-assessment in deciding when tool use is necessary. Although applying MeCo to parameter filling for tool use in LLMs is feasible, it also introduces additional latency. How to better apply MeCo to improve the parameter filling accuracy and the end-to-end evaluation remains an important direction for future work."}, {"title": "A MeCa Statistics", "content": "MeCa Statistics\nTable 5 summarizes the statistics of the MeCa. In Task1 and Task4, the positive queries require a specific external tool to address the user queries, while the negative queries require no external tools and can be solved by the LLM\u2019s internal capabilities. In Task2 and Task5, we provide a tool name and its description along with the user query, asking the LLMs to determine whether they need to use this specific tool to address the user queries. The neutral queries in Task2 and Task3 indicate that these queries require external tools, but the provided tool is irrelevant to addressing the user query. In Task3 and Task6, we provide a list of tools (ranging from 2 to 5) along with the user query. For multi-turn queries, there is a dialogue between the user and the LLM assistant, where the assistant needs to determine whether it should rely on external tools to address the user query in the final round of the conversation.\nWe directly transfer the $l_{yes}$ and $l_{no}$ thresholds of MeCo, fitted on the Metatool dataset, to Task1 and Task4 in MeCa-Tool, and present the results in Table 2 and Table 3. Because the rest of the tasks in MeCa-Tool are very different and more complex than the user queries in MetaTool, we randomly sample 100 queries from each category in Task2, Task3, Task5, and Task6, and use these queries as the hold-out testing data. We use the remaining data to fit the thresholds for $P_{yes}$ and MeCo."}, {"title": "A.2 MeCa Creation", "content": "To curate the MeCa-Tool dataset, we employed a meticulous and structured approach that ensures the queries are relevant to current LLM capabilities. The process unfolded as follows:\n1. Collection of diverse scenarios: We began by gathering a broad spectrum of domains and conversational scenarios from various online corpus. This initial step ensures that the subsequent generated synthetic APIs and conversations are grounded in realistic and diverse settings.\n2. Synthetic APIs design: Leveraging the collected scenarios, we then synthetically design 500 distinct APIs by emulating examples found in real-world applications, ensuring that they span multiple domains.\n3. Query generation: For each query, APIs are randomly sampled from our synthetic APIs pool. User queries are then constructed based on sampled APIs, which may: (i) Require the invocation of the provided APIs; (ii) Not require any tool invocation, relying solely on the LLM\u2019s internal knowledge; or (iii) Involve cases where the provided APIs does not include the necessary tools to answer the query directly.\n4. Human Verification: After the queries were constructed, they underwent a rigorous human review process. This critical step verified the validity and correctness of the data, ensuring that each query aligns with its intended category and meets quality standards.\nThe dataset was constructed as follows: we selected a subset of fact-based data from the dataset (Zou et al., 2023), which consists of common, well-known facts, such as \u201cThe Earth orbits the Sun.\u201d These facts were used as model responses, and the leading proprietary LLM (i.e., GPT-4-turbo) was instructed to generate corresponding user queries. Since these queries involve common knowledge that is embedded in LLMs, they do not require retrieval and thus serve as negative RAG examples. For positive RAG examples, we scraped recent news articles from the past few months, ensuring that this content has not been seen by LLMs. We then followed a similar process as mentioned above to generate user queries based on the latest information. This process resulted in queries that require retrieval as they involve knowledge that is unknown or not yet integrated into the LLM\u2019s training data. The detailed distribution of MeCa can be found in Figure 4."}, {"title": "B Related Work", "content": "LLMs have progressed from understanding and generating human-like text to utilizing external tools based on natural language instructions. This evolution expands their application beyond basic conversational tasks to enable dynamic interactions across diverse functional domains, such as facility management and professional services (Patil et al., 2023; Liu et al., 2023b; Qin et al., 2023; Chen et al., 2023). For example, Toolformer (Schick et al., 2024) enables LLMs to use external tools via simple APIs through a supervised fine-tuning (SFT) model. (Liu et al., 2024c) demonstrate strong executable functional API calls across different domains. (Liu et al., 2024b) trained on synthesized data, achieves state-of-the-art results on the Berkeley Function-Calling Leaderboard (Yan et al., 2024), even with a relatively small model size of 8B parameters. Despite their growing popularity and capabilities, tool use in LLMs often depends on strategies like verbal feedback, which are hampered by the quality of the datasets used for fine-tuning. Several benchmarks/datasets have been developed to support tool use in a data-centric way, such as (Li et al., 2023), which provides a set of tool-use dialogues with various APIs to assess the LLM's tool use capabilities, Toolalpaca (Tang et al., 2023) constructs a comprehensive tool-use corpus derived from collected real-world APIs, designed specifically to fine-tune LLMs for better tool utilization. focuses on creating a synthetic instruction-tuning dataset for tool use. However, these methods rely solely on superficial textual information, without probing deeper into the LLM\u2019s internal states to explain or justify when and why a tool should be called, resulting in an inability to accurately determine the optimal timing for tool invocation.\nRAG has shown success in supporting AI systems that require up-to-date information or access domain-specific knowledge, particularly where the scope of queries is not seen in the training data of LLMs (Lewis et al., 2020; Ren et al., 2023; Vu et al., 2023; Izacard et al., 2023). This paper is also consistent with the trend of towards adaptive RAG paradigm, which is designed to assess whether a query can be directly answered by the LLMs or requires external data retrieval (Asai et al., 2023; Jiang et al., 2023). Specifically, a simple query within the LLM\u2019s knowledge should be directly answered by the LLMs themselves. On the other hand, for complex queries or questions about data they have not been trained on, RAG intervenes to prevent incorrect out-of-date answers or hallucination (Ji et al., 2023). This mechanism allows RAG to dynamically adjust operational strategies of retrieval-augmented LLMs by assessing the boundary of LLM\u2019s self-knowledge and the complexity of the query, thereby minimizing unnecessary computational overhead when the queries are answerable by LLMs themselves. Similar to the LLMs function-calling, the decision of retrieval timing typically hinges on three primary methods: (i) explicit verbal feedback from LLMs (Ding et al., 2024), (ii) enhancements through fine-tuning (Asai et al.,"}, {"title": "C Extended Results", "content": "To determine the best prompting strategy for tool use, we explore five prompting strategies with multiple base models."}, {"title": "D Probe Training", "content": "Different Training Strategies\nAlthough it increases the length of the instructions and thus may degrade the signal we are detecting, we found that it is much better to provide the model with the query in the instruction than solely instruct the model to follow the ground truth explanations. Therefore, we include the queries in the contrastive instructions below.\nDifferent Size of Training Data\nWe further examine how the size of the training data affects the outcomes of the meta-cognition probe. Specifically, we analyze the performance of the trained probes with varying sizes of training data, as illustrated in Figure 9 and Figure 10. According to Equation 1, a sentence with 10 tokens can be used to create 10 training data pairs of experimental prompts and reference prompts. Typically, a brief explanation of why or why not to use external tools/RAG corresponds to around 30 to 50 tokens. Thus, a training data size of 256 requires fewer than 10 queries and their associated explanations.\nAlthough different backbone models exhibit significantly varying classification accuracies\u2014with Llama-3-8b achieving the highest and Llama-3-70b the lowest\u2014we found that only a small amount of training data is sufficient to train a probe with near-optimal performance. We hypothesize that this is due to the linear nature of the PCA methods adopted in RepE.\nContrastive Instructions for Various Probes\nWe used the following instruction pair to collect contrastive data and train the honesty probe. Specifically, we instructed the model to be both honest and untruthful when explaining its reasoning for the necessity of tool use test"}, {"title": "E Prompts", "content": "Prompts in Adaptive Tool Use\nWe employ two types of prompts in our experiments: 1) prompts with context, which provide specific reasons for why LLMs may require external tools to complete user tasks. These prompts also include five randomly sampled examples to assist the model in making decisions; and 2) prompts without context, which are more concise and contain only the instruction and query. The exact prompts are provided below. Note that the example queries are randomly sampled in the Metatool benchmark and we follow their setup and don\u2019t change the examples associated with queries."}]}