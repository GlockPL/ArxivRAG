{"title": "Nd-BiMamba2: A Unified Bidirectional Architecture for Multi-Dimensional Data Processing", "authors": ["Hao Liu"], "abstract": "Deep learning models often require specially designed architectures to process data of different dimensions, such as 1D time series, 2D images, and 3D volumetric data. Existing bidirectional models mainly focus on sequential data, making it difficult to scale effectively to higher dimensions. To address this issue, we propose a novel multi-dimensional bidirectional neural network architecture, named Nd-BiMamba2, which efficiently handles 1D, 2D, and 3D data. Nd-BiMamba2 is based on the Mamba2 module and introduces innovative bidirectional processing mechanisms and adaptive padding strategies to capture bidirectional information in multi-dimensional data while maintaining computational efficiency. Unlike existing methods that require designing specific architectures for different dimensional data, Nd-BiMamba2 adopts a unified architecture with a modular design, simplifying development and maintenance costs. To verify the portability and flexibility of Nd-BiMamba2, we successfully exported it to ONNX and TorchScript and tested it on different hardware platforms (e.g., CPU, GPU, and mobile devices). Experimental results show that Nd-BiMamba2 runs efficiently on multiple platforms, demonstrating its potential in practical applications. The code is open-source: https://github.com/Human9000/nd-Mamba2-torch.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has made significant progress in many fields, but data of different dimensions (e.g., 1D time series, 2D images, and 3D volumetric data) often require specially designed model architectures. For instance, convolutional neural networks (CNNs) [1] excel at processing image data, recurrent neural networks (RNNs) [2] are suitable for sequential data, while 3D CNNs are used for volumetric data. This domain-specific model design paradigm leads to increased development and maintenance costs and limits the generalization ability of models.\nAlthough bidirectional models, such as bidirectional RNNs (BiRNNs) [3], have been successful in sequential data modeling, they struggle to scale effectively to higher-dimensional data and face challenges in cross-platform deployment. The sequential processing nature of BiRNNs limits their parallelization capabilities, making them inefficient for long sequences and high-dimensional data, and they are prone to gradient vanishing issues. Moreover, the recurrent structure of RNNS makes it difficult to convert them into formats like ONNX or TorchScript for cross-platform deployment. On the other hand, self-attention mechanisms like Transformers [4] can capture long-range dependencies, but their computational complexity becomes prohibitive and memory consumption increases when processing high-dimensional data, complicating deployment.\nWhile the existing Mamba [5] model strikes a balance between efficiency and performance, most are limited to unidirectional processing or data of specific dimensions. To overcome these limitations, this paper proposes a novel multi-dimensional bidirectional neural network architecture, Nd-BiMamba2. The core innovations of Nd-BiMamba2 include: 1) extending the Mamba2 module to support efficient bidirectional processing that can be applied effectively to 1D, 2D, and 3D data; 2) introducing an adaptive padding strategy that adjusts padding size based on input data dimensions, improving computational efficiency and reducing memory consumption.\nThe main contributions of this paper are as follows:\n\u2022 We propose Nd-BiMamba2, a unified bidirectional network architecture that can efficiently process multi-dimensional data.\n\u2022 We design an innovative bidirectional processing mechanism that effectively captures bidirectional information in high-dimensional data.\n\u2022 We introduce an adaptive padding strategy to improve computational efficiency and reduce memory consumption.\n\u2022 We validate the portability and deployment capability of Nd-BiMamba2 across different hardware platforms.\nThe following sections will provide detailed descriptions of the network structure and implementation details of Nd-BiMamba2, experimental results, its performance on multi-dimensional tasks, and discuss the model's advantages and potential applications."}, {"title": "II. RELATED WORK", "content": "Multi-dimensional data modeling is a key research direction in deep learning, encompassing various scenarios such as 1D time series, 2D images, and 3D volumetric data. To efficiently model multi-dimensional data, researchers have proposed various methods, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), self-attention mechanisms, and recently emerging modular architectures such as Mamba. However, these methods have limitations to varying degrees and struggle to balance the efficiency and generalizability required for multi-dimensional feature modeling."}, {"title": "A. Convolutional Neural Networks (CNN)", "content": "CNNs, as classical deep learning methods, have achieved outstanding performance in image processing tasks. Typical models such as LeNet [6], ResNet [7], and U-Net [8] extract local features through multiple layers of convolutions and progressively expand the receptive field. However, CNNs face the following limitations in multi-dimensional data modeling:\n- Inadequate long-range dependency modeling: CNNs struggle to capture global context information when processing long sequences or high-resolution images. - High computational cost for high-dimensional extension: 3D CNNs are effective for spatial feature extraction but significantly increase the parameter scale and computational complexity, limiting their practical applications."}, {"title": "B. Recurrent Neural Networks (RNN)", "content": "RNNs and their variants (such as LSTM [9] and GRU [10]) perform excellently in sequence modeling, especially in capturing long-term and short-term dependencies in time series. For example, bidirectional LSTMs (BiLSTMs) [11] enhance context modeling in natural language processing tasks by fusing bidirectional information. WaveNet [12] introduces a novel deep neural network architecture based on dilated causal convolutions, capable of directly generating high-quality raw audio waveforms and effectively capturing long-range dependencies in audio signals. However, RNNs have the following limitations:\n1) Difficulty in parallelization: The sequential processing nature of RNNs makes them inefficient when handling long sequences. 2) Challenges in scaling to high-dimensional data: The recurrent structure does not adapt well to 2D images or 3D volumetric data, leading to increased memory consumption and computational complexity. 3) Training stability issues: RNNs still face gradient vanishing and gradient explosion problems, impacting model performance."}, {"title": "C. Self-Attention Mechanisms (SA)", "content": "Self-attention mechanisms, with their global modeling ability, have been widely applied to natural language processing and computer vision tasks. The Transformer [4] is a representative model, and its extensions such as BERT [13] and ViT [14] have made significant progress in various fields. However, in multi-dimensional data modeling, self-attention mechanisms still face the following challenges:\n1) High computational complexity: Although numerous Swin-based attention methods [15], [16] have been proposed to reduce computational complexity in 2D, the quadratic complexity of attention mechanisms leads to a significant increase in memory and computational resource requirements when dealing with high-dimensional data. 2) Poor adaptability to high-dimensional scenarios: While low-rank decomposition methods (such as Linformer [17],Rethinking [18]) reduce complexity, they still do not fully solve the memory bottleneck in high-dimensional data processing."}, {"title": "D. Mamba Modules", "content": "The Mamba module is a lightweight architecture that combines the advantages of convolution and attention mechanisms, which has recently gained prominence in multi-dimensional data modeling. For example, the latest Mamba2 [19] and vssd [20] etc [21], [22]modules significantly improve image classification performance by combining local feature extraction with global information modeling. However, existing Mamba modules primarily focus on unidirectional feature modeling and have the following limitations:\n1) Lack of bidirectional feature modeling: The inability to effectively capture bidirectional information in multi-dimensional data limits its generalization capability. 2) Insufficient adaptation to multi-dimensional data: Current designs mainly target 1D or 2D image data individually, making it challenging to efficiently extend to 3D scenarios."}, {"title": "E. Summary and Limitations", "content": "Existing methods each have their advantages, but still face shortcomings in efficient and multi-dimensional feature modeling:\n1) CNNs excel at local feature extraction but struggle to capture global context.\n2) RNNs are strong in modeling sequential data but suffer from low computational efficiency and poor scalability.\n3) Self-attention mechanisms offer global modeling capabilities but come with high computational complexity.\n4) The Mamba module, while excelling in lightweight design, lacks a unified modeling capability for multi-dimensional data."}, {"title": "F. Innovations of Nd-BiMamba2", "content": "To address the above issues, we propose a unified bidirectional modeling architecture, Nd-BiMamba2. By extending the Mamba2 module, it supports efficient modeling of 1D, 2D, and 3D data. The bidirectional processing mechanism fully explores directional information in multi-dimensional data. Dynamic padding adjustment based on input data dimensions improves computational efficiency and reduces memory consumption. We validated the model's efficiency on CPU, GPU, and mobile devices, enhancing its practical application potential.\nIn conclusion, Nd-BiMamba2 provides a general and efficient solution, opening new directions for multi-dimensional data modeling and cross-platform deployment."}, {"title": "III. ALGORITHM DESIGN", "content": null}, {"title": "IV. ALGORITHM DESIGN OPTIMIZATION", "content": null}, {"title": "A. Design Objectives and Challenges", "content": "To handle data of different dimensions (1D, 2D, 3D) and optimize model performance, the design objectives of Nd-BiMamba2 include the following:\n\u2022 Generality: The algorithm needs to provide a unified processing framework to accommodate multi-dimensional data.\n\u2022 Efficiency: To reduce computational redundancy on high-dimensional data, convolution operations need to be designed for adaptation.\n\u2022 Boundary Handling: To avoid boundary effects in multi-dimensional scenarios, tailored padding strategies must be designed."}, {"title": "B. Core Algorithm Design", "content": "1) Input Representation: To simplify the processing of data with different dimensions, the input tensor is uniformly represented as:\n$X \u2208 R^{B \\times C \\times D_1 \\times D_2 \\times D_3}$ \nwhere B is the batch size, C is the number of channels, and $D_1, D_2, D_3$ represent the sizes of the three dimensions. For 1D and 2D data, this is maintained consistently by setting $D_3 = 1$ or $D_2 = D_3 = 1$.\nThis unified representation reduces the complexity of handling logical branches between different dimensional data, allowing subsequent convolution and activation operations to reuse the same logic.\n2) Core Convolution Calculation Formula:\na) Design Philosophy:: To capture local features, dimension-adaptive convolution operations are employed. The core calculation formulas are as follows:\n$F(X) = \\sigma(W_f * X + b_f)$ \n$B(X) = \\sigma(W_b * X + b_b)$ \nwhere $W_f, W_b$ are the convolution kernels for the forward and backward paths, $b_f, b_b$ are the biases, * denotes dimension-adaptive convolution, and o is the activation function.\nTo enhance the model's ability to handle directional information, separate forward and backward paths are designed. Additionally, an activation function o is included to improve the model's nonlinear modeling capabilities.\nb) Dimensional Differences and Optimizations::\n1) 1D Data: For processing sequential data, the convolution kernel is designed with shape (k, 1, 1), sliding only along the $D_1$ direction:\n$Y[i] = \\sum_{j=0}^{k-1}W[i, j] \\cdot X[i \\cdot s + j] + b[i]$ \nwhere k is the kernel size, and s is the stride. To reduce computational redundancy in other dimensions, the convolution operation slides only along the $D_1$ direction, improving computational efficiency.\n2) 2D Data: For processing image data, the convolution kernel is designed with shape ($k_1, k_2, 1$), sliding along both $D_1$ and $D_2$:\n$Y[i, j] = \\sum_{m=0}^{k_1-1} \\sum_{n=0}^{k_2-1}[i, j, m, n] \\cdot X[i\\cdot s_1 + m, j\\cdot s_2 + n] + b[i, j].$ \nTo effectively capture local pattern information, the convolution operation slides simultaneously along $D_1$ and $D_2$, which is suitable for extracting image features.\n3) 3D Data: For processing high-dimensional spatial data, the convolution kernel is designed with shape ($k_1, k_2, k_3$), sliding along $D_1, D_2, D_3$ simultaneously:\n$Y [i,j,k] = \\sum_{m=0}^{k_1-1}\\sum_{n=0}^{k_2-1}\\sum_{l=0}^{k_3-1} W[i, j, k, m, n, l] \\cdot X[i\\cdot s_1+ m, j \\cdot s_2 + n, k \\cdot s_3 + l]+b[i, j, k].$ \nTo capture the complex features in 3D space, convolution operations are evenly distributed across the three dimensions, improving feature extraction capabilities.\n3) Padding Strategy:\na) Formula Definition:: To handle boundary effects, the padding size $p_i$ in the i-th dimension is calculated as:\n$p_i = max(0, \\lceil\\frac{D_i - 1 \\cdot s_i + k_i - 1}{2}\\rceil)$ \nwhere $k_i$ and $s_i$ are the kernel size and stride for the i-th dimension, respectively.\nb) Dimensional Differences and Advantages::\n1) 1D Data: To preserve the original data characteristics, padding is minimized only along the $D_1$ direction.\n2) 2D Data: To enhance the effectiveness of the boundary regions, a mirroring padding strategy is applied along both $D_1$ and $D_2$.\n3) 3D Data: To balance boundary handling with computational complexity in high-dimensional scenarios, padding is uniformly distributed across $D_1, D_2, D_3$.\n4) Activation Function Selection: To improve the model's nonlinear expression capabilities, Nd-BiMamba2 uses the GELU (Gaussian Error Linear Unit) activation function, defined as:\n$\\sigma(x) = x \\cdot \\Phi(x)$ \nwhere $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution.\nThe GELU activation function was selected to more smoothly handle the distribution of input values, especially exhibiting stronger feature extraction abilities in high-dimensional data.\nOverall, Nd-BiMamba2 retains the advantages of Bi-Mamba2 when processing sequential and image data, and by incorporating support for three-dimensional data, along with more refined partitioning and feature fusion techniques, it extends the application scope. This improvement enables Nd-BiMamba2 to provide more efficient and accurate modeling capabilities when dealing with more complex input data.\nNd-BiMamba2's modules and functional layers are shown in Table I:"}, {"title": "C. Comparative Analysis", "content": "To highlight the advantages of Nd-BiMamba2, the following Table II summarizes its comparison with other models:"}, {"title": "D. Summary", "content": "By optimizing strategies for padding, dimension rearrangement, channel adjustment, and feature fusion across different dimensions, the model can efficiently extract features from 1D, 2D, and 3D data while maintaining consistency in the output dimension with the input data. These steps are clearly described through mathematical symbols to ensure the correctness and efficiency of multi-dimensional optimization."}, {"title": "E. Model Export and Deployment", "content": "To enhance model portability and deployment capabilities, Nd-BiMamba2 supports multiple export formats:\n\u2022 ONNX Export: Supports converting the model to ONNX format for running on various hardware platforms.\n\u2022 TorchScript Export: Supports converting the model to TorchScript format to ensure efficient inference in production environments.\nThrough this modular design and multi-dimensional optimization, Nd-BiMamba2 achieves efficient and unified modeling for 1D, 2D, and 3D data, providing powerful support for multi-modal data processing."}, {"title": "V. EXPERIMENTS", "content": null}, {"title": "A. Experimental Setup", "content": "All experiments were conducted on the following hardware platform:\n\u2022 Processor (CPU): Intel Core i9-11900K, 8 cores, 16 threads, 3.5 GHz base frequency. The high clock speed and multi-core design of this processor allow it to efficiently handle parallel computing tasks, particularly for processing large amounts of data and task scheduling, significantly enhancing overall computational performance.\n\u2022 Graphics Processing Unit (GPU): NVIDIA RTX 4090D, 24GB GDDR6X VRAM. As one of the latest high-performance GPUs, the RTX 4090D provides powerful parallel computing capabilities for deep learning model training and inference, especially for large-scale datasets and complex models. The 24GB of VRAM ensures the processing of large models and high-resolution data, effectively mitigating memory bottlenecks.\n\u2022 Memory (RAM): 64GB DDR4 3200 MHz. The ample memory capacity ensures efficient data reading and caching during model training, preventing computational bottlenecks due to memory limitations. This is especially important when handling large-scale data, maintaining high data throughput.\n\u2022 Storage: 1TB NVMe SSD (used for data storage and intermediate result caching). The high-speed SSD improves data read/write speed, significantly reducing I/O latency, especially when training involves large amounts of data input and output, ensuring efficient operation during the training process."}, {"title": "B. Feature Representation Ability of Nd-BiMamba2", "content": "The bidirectional modeling module of the Nd-BiMamba2 model enhances its feature perception ability by incorporating both forward and backward information flows. In traditional unidirectional modeling, the model can only rely on information from one direction of the input sequence for inference. In contrast, bidirectional modeling considers both forward and backward information flows, allowing for the capture of more comprehensive features. The advantages of bidirectional modeling are particularly evident in various data dimensions (1D, 2D, and 3D), especially in capturing long-range dependencies and local features.\nThrough comparative experiments across different data dimensions (1D, 2D, and 3D), we have validated the improvement in feature representation by bidirectional modeling, demonstrating that this approach is more efficient than traditional unidirectional modeling when dealing with complex data. The experimental model configuration parameters were set as follows: Cin = 64, Cout = 64, dmodel = 128, ensuring the ability to handle high-dimensional data and perform sufficient feature extraction."}, {"title": "C. Flexibility and Adaptability from Modular Design", "content": "The modular design of Nd-BiMamba2 provides strong support for model flexibility and adaptability. Through experiments on 1D, 2D, and 3D data, the model can adaptively adjust padding strategies according to different data dimensions, ensuring computational efficiency and flexibility. With this design, Nd-BiMamba2 can dynamically adjust input size and padding strategy, achieving good computational efficiency and performance across different data dimensions.\nTo observe the model's performance with adaptive padding strategies across different data dimensions (1D, 2D, 3D), we conducted comparative experiments on multi-dimensional adaptive padding strategies. This verified that the strategy automatically adjusts padding methods for various input sizes to ensure dimensional consistency and efficient computation.\nAs seen in Table IV, the model demonstrates excellent flexibility under the adaptive padding strategy. Especially in 2D and 3D data processing, the adaptive padding proves particularly important. It effectively improves computational efficiency while maintaining high accuracy across different input sizes. This shows that Nd-BiMamba2 has strong adaptability in processing multi-dimensional data, adjusting itself according to the different characteristics of the data."}, {"title": "D. Conclusion", "content": "Through the analysis and experiments on the nd-BiMamba2 model, several significant advantages have been identified:\n\u2022 Bidirectional Modeling: Bidirectional modeling significantly enhances the model's ability to perceive features, especially in capturing long-range dependencies and local characteristics.\n\u2022 Modular Design: The modular design provides flexibility and adaptability, allowing the model to automatically adjust input sizes and padding strategies based on different data dimensions, ensuring computational efficiency and model flexibility.\n\u2022 Efficient Performance: Despite the increased computational overhead from bidirectional modeling and adaptive padding, the model still performs excellently across multiple data dimensions, demonstrating its advantage in processing complex data.\nOverall, nd-BiMamba2 exhibits strong performance in high-dimensional data processing, feature extraction accuracy, and computational efficiency, proving its effectiveness in complex data analysis, long-range dependency modeling, and large-scale data handling."}, {"title": "APPENDIX", "content": "Algorithm 1 Nd-BiMamba2 Algorithm\nInput: X \u2208 $R^{C \\times d_1 \\times d_2 \\times \\dots \\times d_n}$ \nOutput: Houtput \u2208 $R^{c' \\times d_1 \\times d_2 \\times \\dots \\times d_n}$ \nStep 1: Data Preprocessing\n\u2022 Xpadded \u2190 Pad(X) Padding input data\n\u2022 Xreshaped \u2190 Reshape(Xpadded) Adjusting dimensions\n\u2022 Xmapped \u2190 FCin(Xreshaped) Mapping the channel count\nStep 2: Bidirectional Modeling\n\u2022 Hforward \u2190 Mamba2for(Xmapped) Forward feature extraction\n\u2022 Hbackward \u2190 Flip(Mamba2back(Flip(Xmapped))) Back-ward feature extraction\n\u2022 Hfused \u2190Hforward + Hbackward Fusing features\nStep 3: Output Generation\n\u2022 Hfc_out \u2190 FCout (Hfused) Restoring the channel count\n\u2022 Houtput \u2190 Trim(Hfc_out) Removing padded parts\nReturn: Houtput"}]}