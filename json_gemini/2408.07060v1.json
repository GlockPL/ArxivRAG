{"title": "DIVERSITY EMPOWERS INTELLIGENCE: INTEGRAT-ING EXPERTISE OF SOFTWARE ENGINEERING AGENTS", "authors": ["Kexun Zhang", "Weiran Yao", "Zuxin Liu", "Yihao Feng", "Zhiwei Liu", "Rithesh Murthy", "Tian Lan", "Lei Li", "Renze Lou", "Jiacheng Xu", "Bo Pang", "Yingbo Zhou", "Shelby Heinecke", "Silvio Savarese", "Huan Wang", "Caiming Xiong"], "abstract": "Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise. DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin. For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions. Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have transformed software engineering (SWE) and other domains. Originally developed as chatbots (Schulman et al., 2022; Team, 2024), LLMs have evolved into the core of AI agents, capable of understanding and generating human-like conversations, as well as autonomously executing actions in both real-world and digital environments. SWE agents, a specialized subset of these AI agents, integrate these capabilities with software engineering tools and techniques for tasks like code generation, automated testing, and project management, aiming to identify and resolve practical software issues (Zhang et al., 2024).\nIn this paper, we study one specific task of SWE agents \u2013 resolving real-world GitHub issues based on their descriptions. Automatically fixing a bug in a code repository is an extremely challenging task that involves navigating extensive codebases, understanding complex function interactions, detecting subtle errors, and generating the correct fix patch. The large action space of SWE agents, together with long trajectories, inevitably result in the diversity of Github issue solutions, as shown in Figure 1. We have observed that different SWE agents resolve very different sets of issues (the colored girds in Figure 1a), despite having similar resolve rates (Figure 1b). This is probably due to different skill sets of SWE agents. For instance, OpenDevin (Wang et al., 2024c) explicitly instructs the LLM to first replicate the bug in an issue and executes its replication in a development workspace to provide feedback for its generated patches, but other agents like Moatless Tools (\u00d6rwall, 2024) and Agentless (\u00d6rwall, 2024) do not actually execute code in the issue-specific repository.\nA garden's beauty never lies in one flower. Diversity in all its forms is the path to greatness."}, {"title": "2 RELATED WORK", "content": "We review the work in fundamental language agent architecture, recent developments for SWE agents, and multi-agent or ensemble methods in this section."}, {"title": "Fundamental Language Agent", "content": "Pioneering AI agent methods along this line of work include ReAct (Yao et al., 2023), Reflexion (Shinn et al., 2023), CodeAct (Wang et al., 2024b), etc., in which ReAct interprets the user query, generates functional API calls, and gets the tool outputs in real time; Reflexion further appends failed trial experience into the memory, enabling effective retrials to prevent repetitive errors. CodeAct (Wang et al., 2024b), instead of generating function calls, uses code generation to consolidate AI agents' actions into a unified action space."}, {"title": "Software Engineering Agent", "content": "We present the SWE agents which have disclosed the technical details on the SWE-bench Lite leaderboard. Alibaba Lingma Agent (Ma et al., 2024) constructs a repository knowledge graph to represent code and dependencies, using a Monte Carlo tree search-based strategy for repository exploration, and generates patches to address real-world GitHub issues. AutoCodeRover (Zhang et al., 2024) adds advanced code search tools, such as abstract syntax trees, and spectrum-based fault localization to the agent for enhancing context understanding and issue resolution. Code-R (Chen et al., 2024) chooses a multi-agent framework with pre-defined task graphs to resolve Github issues. Agentless (Xia et al., 2024), is a simplified two-phase approach for solving software development problems. It focused on localization and repair without relying on LLMs to make decisions, highlighting the potential of straightforward techniques in autonomous software development. OpenDevin (Wang et al., 2024c) is a hub of community-contributed agents including CodeAct (Wang et al., 2024b), browser agent, GPTSwarm (Zhuge et al., 2024), and task-specific micro agents. Finally, SWE-agent (Yang et al., 2024) developed agent-computer interface that consists of LM-friendly commands and environment feedback to enable LM agents to autonomously use computers to solve software engineering tasks."}, {"title": "Multi and Ensemble Agents", "content": "Recent works observe that organizing multiple specialized AI agents (Hong et al., 2024; Li et al., 2023; Liu et al., 2024) enable the task decomposition ability of an agent system, which improves the task-resolving performance. Current multi-agent frameworks are categorized into three types based on their execution patterns. Firstly, static agent working flow (Wu et al., 2024; Github, 2023), which pre-defines the agent execution flows and ignites agent transitions via specified conditions. Controlling a multi-agent system with pre-determined states is robust, though losing flexibility in terms of unseen states or conditions. Secondly, ensemble via group chatting (Wu et al., 2023; Hong et al., 2024; Wang et al., 2024a; Chen et al., 2023). This is built upon an environment where multiple agents send messages to each other in a group channel such that their thoughts are ensembled. Variants of group chatting includes debating (Liang et al., 2023; Chan et al., 2023) and model-wise ensembling (Wang et al., 2024a). Last but not least, hierarchical task assignment (Liu et al., 2024; 2023). Organizing multi-agent in a hierarchical structure benefits the top-down task decomposition and thus enables efficient multi-agent collaboration."}, {"title": "3 INTEGRATING EXPERTISE OF SWE AGENTS", "content": ""}, {"title": "3.1 BACKGROUND", "content": "Resolving issues in SWE-Bench. One important task in software engineering is to resolve issues raised by developers. SWE-Bench curates instances of this task by collecting successfully resolved issues from open-source repositories on Github. Each instance in SWE-Bench consists of a textual issue description, a version of the repo just before the issue was resolved, and (hidden) unit tests that went from fail to pass after the human-written patch. To resolve an instance, the model is required to generate a patch that can pass these unit tests.\nSWE Agents. In this paper, we use the term \u201cSWE agents\u201d\u00b9 to refer to any LLM-based system that generates patches to solve issues in a code base, e.g., an instance in SWE-Bench. While the specific implementation varies, a typical SWE agent usually gives their underlying LLM several tools in the form of callable functions to navigate through the code base, find relevant context, edit files, and run tests. The workflow of SWE agents often involves multiple LLM calls, each taking some or all outputs from previous steps as input."}, {"title": "3.2 DIVERSITY OF SWE AGENTS", "content": "We consider two types of diversity: intra-agent diversity and inter-agent diversity.\nIntra-agent diversity is defined as the degree to which different runs of the same agent solve different problem instances. It is most likely from the non-determinism of the underlying LLM due to sampling in decoding and mixture-of-experts architecture (Chann, 2023). Since the workflow of SWE agents involves multiple steps and LLM calls, a slight difference in an earlier step can easily propagate and result in significant differences in the final outcome.\nInter-agent diversity is defined as the degree to which different agents solve different problem instances. Besides sharing the potential causes of intra-agent diversity, inter-agent diversity is also largely because of differences in agent design, including different tools, workflows, and prompts."}, {"title": "3.3 APPROACH", "content": ""}, {"title": "3.3.1 SWE AGENT PROBLEM FORMULATION", "content": "We formulate the SWE agent problem under the contextual Markov decision process (CMDP) framework (Hallak et al., 2015), represented by the tuple M = (S, C, A, R, P, Po, p). Here, S denotes the state space, which encompasses all possible states the agent could encounter, such as the current status of files. The context space, C, includes relevant repository information and issue descriptions. The action space, A, represents all potential actions or tools the SWE agent can utilize, such as search or editing. The context-dependent reward function, R : S\u00d7A\u00d7 C \u2192 R, assigns scores based on the actions taken by the agent. For instance, the reward is high if the agent successfully addresses an issue, while it is low if the action results in new bugs in the repository. The context-dependent transition function, P : S \u00d7 A \u00d7 C \u2192 \u2206(S), defines how the state of the repository or information changes following a specific action. The context-dependent initial state distribution is denoted by po : C \u2192 \u2206(S), and \u03c1 \u2208 \u0394(C) represents the context distribution.\nGiven the initial context c ~ p and initial state so ~ po( | c), at each time step t, the agent follows a policy \u03c0 : S\u00d7C \u2192 A(A) to select an action at ~ \u03c0(st, c) and receives a reward R(st, at, c). The environment then transitions to the next state st+1 ~ P(\u00b7 | St, at, c), providing the agent with a new state observation. As the iteration progresses to time T, a sampled trajectory r := {st, at, rt}=0 is obtained. The objective of an SWE agent is to maximize the cumulative reward along the trajectory, which is captured by the value function:\n$\\max V^{\\pi} (\\rho) = \\max_{\\pi} E_{c \\sim \\rho ; \\tau \\sim \\pi } [\\sum_{t=0}^{T} R(s_t, a_t, c) ]$ (1)"}, {"title": "3.3.2 OUR FRAMEWORK: DIVERSITY EMPOWERED INTELLIGENCE (DEI)", "content": "Many efforts have been made to implement sophisticated agent systems that aim to achieve the objective described in Eq. 1. However, as discussed in Section 1, these systems often exhibit varying levels of effectiveness across different contexts. It is challenging to devise a single agent that can consistently perform well across all possible contexts.\nFormally, suppose there are N agent policies, denoted as {\u03c01, \u03c02,...,\u03c0\u03c1}, where each policy is tailored to address a specific context {p1, p2, ...,pn}. The union of these contexts is a subset of the entire context space, i.e., P1 U P2 U\u06f0\u06f0\u06f0 UPN \u2286 \u03c1. For each agent policy \u03c0\u2081, the objective is:\n$\\pi_i = \\max_{\\pi_i} E_{c \\sim \\rho_i ; \\tau \\sim \\pi } [\\sum_{t=0}^{T} R(s_t, a_t, c) ]$ (2)\nHowever, an agent policy \u03c0\u2081 may perform poorly in a different context pj (where j \u2260 i). \u03a4\u03bf address this limitation, we propose our framework: Diversity Empowered Intelligence (DEI). The DEI framework leverages the strengths of each agent in their respective contexts to enhance overall performance across all contexts."}, {"title": "3.3.3 DEIBASE: A SIMPLE YET EFFECTIVE IMPLEMENTATION", "content": "We introduce a meta-policy, denoted as DEI, which aims to optimally select among the available agent policies based on the context. The goal of TDEI is defined as:\n$\\pi_{DEI} = \\max_{\\pi} E_{c \\sim \\rho} E_{\\tau \\sim \\pi(c)} [\\sum_{t=0}^{T} R(s_t, a_t, c) ]$, (3)\nwhere \u03c0(c) denotes the selection of the optimal agent policy from {\u03c01, \u03c02, ..., \u03c0} based on the observed context c. By dynamically choosing the most suitable agent policy for each context, the DEI framework seeks to maximize the expected cumulative reward across all possible contexts.\nWe present DEIBASE, a simple yet powerful implementation of the DEI framework, tailored for SWE-Bench like problems. The context in the setup includes the repository, along with relevant files and issue descriptions. The meta-policy's action space consists of the final patches generated by different agent frameworks, each specialized in addressing various aspects of the problem.\nDEIBASE utilizes a Large Language Model (LLM) as a code review committee. The LLM evaluates candidate patches by analyzing the state of the code base before and after the proposed changes, in conjunction with the contextual information from the issue descriptions. It produces detailed explanations for each patch, justifying the modifications based on the identified issues, the context, and the specific changes made.\nWhile other methods of code review and scoring, such as rule-based approaches, can be incorporated into our framework, the use of an LLM-based committee offers a unique advantage. LLMs often excel at evaluating solutions when evaluation is easier than generation. DEIBASE thus serves as an effective baseline for LLM-based SWE evaluation, highlighting potential performance variations among diverse SWE agents and showcasing the capabilities of our method."}, {"title": "Step 1: Input Construction", "content": "Four inputs are given to DEIBASE for each patch: the issue description itself, relevant context (code snippets identified by an SWE agent as relevant to the issue), code before the patch, and code after the patch. The form of inputs reflects two design choices. First, the entire repository is often too large to fit directly in the context limit of LLMs, so we use the relevant context instead to save token costs and help the model focus. Second, the format of a patch is not the easiest for an LLM to read as it switches back and forth between the pre-change code and"}, {"title": "Step 2: Explanation Generation", "content": "To help the model better \u201cunderstand\u201d the patch before scoring, we instruct it to generate various explanations regarding the patch in a specified order. The order is decided so that the earlier explanations can also help the later ones. We describe each explanation in the order they are generated here: 1) Issue explanation explains what the issue is about and what problem it may be causing. 2) Context explanation explains how and why each relevant code span (there might be many of these) is relevant to the issue. 3) Location explanation explains if and why the patch is modifying the correct part of the code that's faulty. 4) Patch explanation explains if and how the patch is fixing the issue. 5) Conflict detection is about checking whether the patch conflicts with other relevant code snippets. We explicitly instruct the model to refer back to the earlier explanations while generating the later ones."}, {"title": "Step 3: Patch Scoring", "content": "Based on its own explanations, the model is asked to give the candidate patch a score of 1 to 10. We give the model detailed rubrics of what violations/mistakes lead to higher score deductions and what should only be considered minor violations. For example, if the model finds the modification location to be wrong, it is considered a serious mistake."}, {"title": "4 EXPERIMENTS", "content": "We aim to answer two research questions with our experiments: 1) How diverse are LLM-based SWE agents in terms of intra- and inter-agent diversity? 2) To what extent can DEI harness the diversity and increase the performances of these SWE agents?"}, {"title": "4.1 EXPERIMENT SETUP", "content": ""}, {"title": "4.1.1 BENCHMARK AND AGENTS", "content": "Benchmark. We conduct our experiments on SWE-Bench Lite, a 300-instance subset sampled from the full SWE-Bench for providing a more self-contained evaluation of functional bug fixes (Jimenez et al., 2024). Compared to the full SWE-Bench, SWE-Bench Lite has significantly more submissions on the leaderboard for us to conduct a more comprehensive analysis of inter-agent diversity.\nAgents. For intra-agent diversity, we consider three well-performing open-source agents on the SWE-Bench Lite leaderboard: Agentless (Xia et al., 2024), Moatless (Orwall, 2024), and Aider (Gauthier, 2024) by running them 10 times with the same parameters. For inter-agent diversity, we consider 10 agents (open-source or not) that have similar resolve rates, all between 26.0% and 31.0% on the leaderboard by directly using their submitted patches to the SWE-Bench issues. For the evaluation of DEIBASE on different agents, we consider 3 groups of agents that form different DEI Committees, including one group consisting of only open-source agents. For the evaluation of DEIBASE on multiple runs of a single agent, we use the generations of the three aforementioned agents - Agentless, Moatless Tools, and Aider. More details can be found in Appendix A.1."}, {"title": "4.1.2 EVALUATION METRICS", "content": "We use the same set of metrics for both intra- and inter-agent diversity as these metrics are defined for multiple candidate solutions without requiring them to come from the same candidate:\nResolve rate measures how good a SWE agent is. It is defined as the percentage of issues resolved by the agent. We measure both single SWE agents and DEI with it to see how much DEI helps.\nUnion@k measures the best case performance had the agents been perfectly consistent by counting the number of problems solved by any of the k solutions. In the ideal case where the agents are perfectly consistent, Union@k should be the same as Union@1. Union@k can also be considered as the case where we have an oracle reward function Roracle that always selects the correct candidate."}, {"title": "Intersect@k", "content": "measures the worst case performance by computing the number of problems solved by all k solutions. The assumption is that a problem is only consistently solved if it's always solved. Yao et al. (2024) calls this metric pass^k. Intersect@k can also be considered as the case where we have an adversarial reward function Radv that tries to pick an incorrect candidate if there is one.\nAverage@k measures the average case performance by computing the average number of problems solved. It corresponds to the case of a random reward function Rrandom that uniformly samples a candidate solution for each problem.\nn@k measures the performance of any reranking mechanism by computing the number of problems solved by n chosen submissions from a total of k samples. The better a reranking mechanism is at telling good solutions from bad ones, the higher n@kis. Note that for an oracle that always picks the correct solution over incorrect ones, n@k is the same as Union@k. For a random reranker that picks a random solution uniformly, n@k is the same as Union@n. In our case, we evaluate n = 1.\nOur research questions can be answered by the gaps between these metrics. Union@k - Intersect@ measures how diverse the agents are, while n@k - Average@k measures how much DEI helps in selecting the correct candidate from these agents. Note that the order \u2013 in which different runs are added matters as k gets larger, especially when the k candidate solutions come from k different agents. In our experiments, we add candidate solutions from the single agent according to the order they are generated, while we add solutions from different agents in a fixed order (see Appendix A.1)."}, {"title": "4.2 MAIN RESULTS", "content": ""}, {"title": "4.2.1 RESEARCH QUESTION 1: HOW DIVERSE ARE LLM-BASED SWE AGENTS?", "content": "To answer this question, we report the \u201c@k\u201d metrics of 10 different agents and 10 runs of single agents in Figure 3. The detailed values of these metrics can also be found in Table 2."}, {"title": "4.2.2 RESEARCH QUESTION 2: HOW MUCH DOES DEI HELP?", "content": "We apply DEIBASE to the candidates in Figure 3 as they are added to the group. Our findings are:\nDEIBASE helps in most cases. For most values of k in all subfigures, we observe a significant improvement of n@k over Average@k, indicating that DEIBASE selects correct candidates much better than a random baseline.\nDEIBASE helps more when the candidates come from different agents. This finding resonates with a similar finding from research question one: Since candidates from multiple agents have a larger potential for improvement (Union@k - Average@k), the actual improvements created by DEIBASE (n@k - Average@k) are also larger. This suggests that given a limited budget of candidates, it would be better to choose a diversity of agents over multiple runs of the same agent.\nAs k gets larger, DEIBASE's improvement first increases and then plateaus. While larger k generally indicates higher n@k, the margin gets smaller and there are cases when an increase in k results in a slight drop in performance. This suggests that the current DEIBASE is not ideal for a large group of agents and there is still room for a better reranking mechanism.\nBased on the lessons above, we propose three DEIBASE groups in which each candidate is from a different agent and no more than 5 candidates exist for each instance. The members of these DEIBASE groups and their performance are reported in Table 1. DEIBASE-1 consists of the top 2"}, {"title": "4.3 ABLATION AND ANALYSES", "content": "In this subsection, we demonstrate some ablation studies to investigate the effectiveness of different components in the framework, in order to answer the following questions. To advocate for open science, all the ablation experiments are conducted on either our own reproduction of open-source SWE agents or their official generations.\nQuestion 1: Does DEI get better with more votes?"}, {"title": "Answer 1:", "content": "Yes. Arguably, DEI itself has the same potential characteristics as SWE agents that may cause diverse outputs. So it is important for us to harness the diverse outputs of DEI as well. However, unlike SWE agents whose outputs are code patches, DEI's output is an integer score, which can easily be aggregated and averaged. This is why we give DEI more votes and rerank the candidates according to the average of scores. In most DEIBASE experiments, we allow 10 votes for each candidate patch. To investigate whether more votes lead to better patch reviewing, we directly take the scores generated for DEIBASE-Open, DEIBASE-Agentless, DEIBASE-Aider, and DEIBASE-Moatless, and check for various values of m, how the first m scores can help us find the best patch.\nAs demonstrated in Figure 4, more votes generally lead to better resolve rates. Another finding is that for 3 out of the 4 evaluation settings, DEIBASE was able to get much better performance than the average candidate with only one vote. Even when DEIBASE wasn't able to get better than average with one vote, it managed to get an improvement with only three votes. These results suggest that DEIBASE itself also produces diverse outputs, but it is easier to aggregate them via score averaging."}, {"title": "Question 2: Are the explanations necessary?", "content": "Answer 2: Yes. We remove the part about asking for explanations from the prompt and compare DEIBASE-Open, DEIBASE-Agentless, DEIBASE-Aider, and DEIBASE-Moatless under the same evaluation setting with and without explanations. We report their resolve rates in Table 3. For all 4 settings we evaluated, DEIBASE with explanations performs slightly better than DEIBASE without explanations."}, {"title": "5 CONCLUSION", "content": "In this paper, we present Diversity Empowered Intelligence (DEI), a meta-policy module designed to integrate with any existing SWE agent frameworks to enable scalable management and collaboration among specialized agents, thereby fostering a more powerful software engineering organization. Through extensive evaluations, we find that different agents show a great level of diversity in the issues they resolve: a group of agents with an average resolve rate of 26.6% can actually solve 54.3% of the issues if we have an oracle that selects the correct candidate. DEI, as our first step towards harnessing such diversity, can improve the group's resolve rate to 34.3% (+7%), suggesting that LLMs are great code reviewers. These findings mirror the benefits of diversity in the tech industry, where diverse perspectives and skills lead to greater innovation and problem-solving capabilities.\nBroader Impacts. DEI represents our initial step toward realizing a fully automated organizational AI. We believe that the full potential of multi-agent AI systems extends beyond enhancing task completion accuracy with agentic workflows, which is the current focus of most industry practices. Instead, DEI offers a horizontal, scaling-out approach that facilitates the collaboration and integration of existing diverse agents without necessitating refactoring of engineering work. This capability not only optimizes and speeds up immediate software development tasks but also sets the groundwork for future innovations in AI-driven organizational management."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 AGENTS EVALUATED", "content": "We add the following agents to the DEI Committee (the one in Figure 3) in the order they appear (the order is generated by randomly shuffling their chronological order using python's random shuffle function with a random seed of 42):\n1. 20240612 IBM Research Agent101\n2. 20240612 MASAI gpt 40\n3. 20240604 CodeR\n4. 20240523 aider\n5. 20240630 agentless gpt4o\n6. 20240617 moatless gpt4o\n7. 20240725 opendevin codeact v1.8 claude35sonnet\n8. 20240706 sima gpt 40\n9. 20240621 autocoderover-v20240620\n10. 20240509 amazon-q-developer-agent-20240430-dev"}]}