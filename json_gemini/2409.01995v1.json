{"title": "vec2wav 2.0: Advancing Voice Conversion via Discrete Token Vocoders", "authors": ["Yiwei Guo", "Zhihan Li", "Junjie Li", "Chenpeng Du", "Hankun Wang", "Shuai Wang", "Xie Chen", "Kai Yu"], "abstract": "We propose a new speech discrete token vocoder, vec2wav 2.0, which advances voice conversion (VC). We use discrete tokens from speech self-supervised models as the content features of source speech, and treat VC as a prompted vocoding task. To amend the loss of speaker timbre in the content tokens, vec2wav 2.0 utilizes the WavLM features to provide strong timbre-dependent information. A novel adaptive Snake activation function is proposed to better incorporate timbre into the waveform reconstruction process. In this way, vec2wav 2.0 learns to alter the speaker timbre appropriately given different reference prompts. Also, no supervised data is required for vec2wav 2.0 to be effectively trained. Experimental results demonstrate that vec2wav 2.0 outperforms all other baselines to a considerable margin in terms of audio quality and speaker similarity in any-to-any VC. Ablation studies verify the effects made by the proposed techniques. Moreover, vec2wav 2.0 achieves competitive cross-lingual VC even only trained on monolingual corpus. Thus, vec2wav 2.0 shows timbre can potentially be manipulated only by speech token vocoders, pushing the frontiers of VC and speech synthesis.", "sections": [{"title": "I. INTRODUCTION", "content": "Discretizing speech into \"tokens\" has prevailed in speech generative tasks, such as text-to-speech (TTS) [1]\u2013[4], in the era of large language models (LLMs). However, the potential of discrete speech tokens in voice conversion (VC) has not been fully mined, which typically aims to convert source speech into target timbre from reference speech. Speech discrete tokens can be roughly divided into acoustic tokens and semantic tokens [5]. Although acoustic tokens [6], [7] reconstruct speech signals well, they lack the ability of VC because all aspects of information in speech are mixed and retained together. Semantic tokens usually come from speech self-supervised (SSL) models [8]\u2013[11] that emphasize on content-related information. Whether timbre is intentionally or unintentionally removed in these tokens, they can act as content representations and thus be utilized in the recognition-synthesis VC paradigm [12].\nThroughout the history, VC methods with a continuous feature space have been researched with depth. The AutoVC series of work [13]\u2013[15] attempt to decouple speech attributes via autoencoder bottlenecks. VC with advanced generative algorithms has then achieved remarkable results, such as normalizing flows [16]\u2013[18] and diffusion models [19]\u2013[21]. After the rise of speech SSL methods, researchers begin to apply SSL features in VC [12], [22]\u2013[24] where the rich phonetic content information from SSL features are utilized. But continuous features make it hard to integrate VC with LLMs and thus an isolated step with other speech-related tasks. Discrete speech tokens can serve as content representations, so VC can be treated as a speech re-synthesis task then [25]. Starting from [25], discrete SSL features are increasingly investigated in VC for keeping phonetic content while discarding most of the acoustic details, such as UUVC [26] and Vec-Tok-VC+ [27]. ContentVec [28] introduces speaker disentanglement to SSL features that benefit VC. There also exist researches on decoupled speech codecs that also facilitate VC, such as SSVC [29] and FACodec [4]. Nevertheless, the performance of those VC methods is still limited compared to continuous state-of-the-arts. Also, excessive design of speaker disentanglement in the discrete tokens may lead to a negative impact on other paralinguistic information that needs to be preserved, such as prosody.\nInstead of pursuing perfect disentanglement in tokens, a different approach is to enhance the timbre controllability in discrete token vocoders. A typical instance is the idea of \"prompted vocoders\" proposed by CTX-vec2wav [3] which is later verified in VC [30]. In CTX-vec2wav, timbre information is injected using a reference prompt. By its position-agnostic cross-attention mechanism, timbre in the mel-spectrogram prompts can be effectively incorporated into the process of speech re-synthesis than only using a time-invariant speaker embedding vector [30]. This indicates the larger potential of performing VC through discrete token vocoders.\nIn this study, we make key improvements upon this framework that significantly boost the effect of acoustic prompts as the source of timbre information. Advanced SSL features are utilized for providing discriminative timbre representation. Most notably, we propose a novel adaptive Snake activation function where the magnitude and frequency of the sinusoidal functions are both controlled by the target speaker's timbre features. This makes the intrinsic periodical properties in the generated signal highly sensitive to the provided timbre features. The resulting model, vec2wav 2.0, is then a discrete token vocoder with strong timbre controlling abilities while retaining the content and styles from the content discrete tokens. In general, vec2wav 2.0 has the following advantages:\n\u2022 Unity. vec2wav 2.0 unifies speech discrete token re-synthesis and VC into the same framework of prompted vocoders.\n\u2022 Simplicity. vec2wav 2.0 does not need any labeled data to train. The only data assumption is utterances are segmented into single-speaker ones. The training criterion is also simple enough, without additional losses for decoupling.\n\u2022 Competitiveness. vec2wav 2.0 achieves superior any-to-any VC performance even compared to continuous VC methods. Furthermore, though only trained on English corpus, vec2wav 2.0 shows remarkable cross-lingual VC performance.\n\u2022 New Paradigm. vec2wav 2.0 proves that speaker timbre can be almost manipulated solely by vocoders even if the speech tokens are not perfectly speaker-decoupled, which may simplify the paradigm in the LLM-based zero-shot TTS world nowadays."}, {"title": "II. VEC2WAV 2.0: PROMPTED TOKEN VOCODER", "content": "We design vec2wav 2.0 to be a prompted discrete token vocoder as shown in Fig.1. The overall architecture inherits the frontend-generator framework of CTX-vec2wav [3], where the input discrete speech tokens are first fed to a Conformer-based frontend module to soften the discreteness, before a vocoder generator that finally outputs the realistic waveforms. The acoustic prompt brings sufficient timbre information into the process of speech re-synthesis. We first extract prompt embeddings through a pretrained WavLM model, then use a CNN pre-net to process the hidden embeddings. In the frontend module, the prompt embeddings are utilized by the position-agnostic cross-attention mechanism [3], [30], which does not apply positional encoding to the query sequence. This special cross attention mechanism simulates shuffling the query sequence and inherently breaks the local patterns in the reference prompt, e.g. linguistic and prosodic features, which enables more accurate learning of target timbre as some global information.\nAfter timbre is preliminarily merged into the frontend, we design an adaptive BigVGAN [31] generator to further incorporate the timbre embedding in waveform generation. The core component of this adaptive generator is a novel adaptive Snake activation function, which will be illustrated in Section II-B."}, {"title": "B. Adaptive Snake Activation", "content": "The Snake activation function is proposed in [32] for modeling periodical inductive bias, which is then adopted in the BigVGAN vocoder to achieve state-of-the-art performance. This activation function can be represented as $f_{\\theta}(x) = x + \\frac{1}{\\beta}sin^2(\\alpha x)$. The learnable parameters $\\theta = {\\alpha, \\beta}$ are designed to control the frequency and magnitude respectively, and $f_{\\theta}$ can operate on each input channel independently, i.e. different $\\theta$ for each input channel.\nAs this Snake activation can subtly capture the periodical pattern in the speech signals, we propose to inject more information from the target speaker timbre. Let $s \\in \\mathbb{R}^d$ be some representative speaker embedding extracted from the target speaker, we design an adaptive Snake activation where the frequency and magnitude of sinusoidal function are both affected by $s$:\n$$T(s) = \\tanh(W s + b)$$\n$$f_{\\theta}(x, s) = x + \\frac{1}{\\beta + T(s)} sin^2 \\left[ (\\alpha + T(s))x \\right]$$\nwhere $T$ is a linear transform followed by $\\tanh$ activation, and operations in (2) are all element-wise. $T(s)$ is discounted by 1/2 on the magnitude part for numerical stability. To save parameters, we apply the same $T$ transformation to both magnitude and frequency. In this way, the learnable parameter for each adaptive Snake is $\\theta = {\\alpha, \\beta, W, b}$, and the target timbre information can be effectively injected in every layer of the vocoder via adaptive activations, with strengthens the timbre controllability to a considerable extent.\nHere in vec2wav 2.0, the prompt embeddings are first mean-pooled to form a single vector that averages out linguistic details and preserves global timbre, then inserted to every adaptive activation layer in the BigVGAN generator. Fig.2 illustrates the detailed architecture of adaptive BigVGAN generator. The input hidden states are iteratively upsampled by transposed convolutions and transformed by anti-aliased multi-periodicity composition (AMP) blocks. Each AMP block receives an additional prompt embedding that is fed to the adaptive Snake activation layer for timbre control. Low-pass (LP) filters are applied after each upsampling and downsampling operation to prevent aliasing [31]. The hidden states are recovered to sampling points after a final adaptive Snake and convolution block."}, {"title": "C. Content and Prompt Features", "content": "Both the content and prompt inputs to vec2wav 2.0 are SSL features with different goals. For input discrete tokens, there should be as less timbre as possible; for prompt features, there should be enough and apparent timbre information to help reconstruction.\nContent Features We use the off-the-shelf vq-wav2vec [8] SSL model for extracting the discrete content representation to be re-synthesized. The discrete tokens are extracted from the quantizer output before the feature aggregator, which is a two-group integer index sequence. We favor this representation because a lot of speaker timbre information is removed due to the information bottleneck, while most of the phonetic pronunciation and prosody are retained [33]. Also, compared to HuBERT [9]-style Transformer SSL models, vq-wav2vec is free of manual clustering and is also fully convolutional with a certain receptive field. This produces a representation that is unaware of the total sequence length, keeping consistent results for a given window. This consistency also shows potential for cross-lingual conversion, as its language-agnostic property has been successfully applied in multilingual TTS [34]. Although there exists measurable speaker timbre leakage in the discrete tokens [33], [35], [36], the vec2wav 2.0 vocoder exhibits strong timbre controllability, so that competitive VC can still be achieved.\nPrompt Features Following CTX-vec2wav, the reference prompt segment is randomly cut from the current utterance, to maintain the same speaker identity without labeled data. Instead of using mel-spectrogram to provide timbre information from the reference prompt, we use a pretrained WavLM [11] model as a timbre feature extractor owing to its widely-verified advantage on speaker verification [35], [37]. We freeze the WavLM model in training and only use the output feature at a certain location of its Transformer blocks. In practice, we use the 6th layer of WavLM-Large model as early layers are proven to contain rich timbre information [23]."}, {"title": "D. Discriminators and Training Criterion", "content": "We inherit the multi-scale discriminators (MSD) and multi-period discriminators (MPD) from HifiGAN [38]. These discriminators are jointly trained with the generator to distinguish fake signals from real ones in multiple scales and periods. With the generator adversarially trained to fool the discriminators, we achieve high-fidelity speech re-synthesis and VC results. Different from some current VC models that often suffer from audio quality issues, vec2wav 2.0 ensures the audio quality of speech signals by GAN training.\nThe training criteria include the auxiliary mel prediction loss and all the other GAN losses from HifiGAN. The auxiliary mel prediction loss is an L1 loss between the ground truth mel-spectrograms and predicted ones that come from linear projections after the Conformer frontend, to warm up the whole model. This loss is added with a certain coefficient, and following vec2wav [1] and CTX-vec2wav [3], we cancel it after warming up."}, {"title": "E. Any-to-Any Voice Conversion", "content": "Although not directly optimized for VC, vec2wav 2.0 still has strong conversion ability due to its effectiveness on incorporating target speaker timbre. The content features retain most of the phonetic and prosodic information while losing much speaker identity, while the speaker timbre is controlled by the reference prompt. Therefore, we can achieve VC simply by using the target speaker's reference speech as the prompt input. This method naturally supports any-to-any VC because the content and prompt features are both acquired by SSL models trained on data with enough speaker variations.\nMoreover, as both the cross attention mechanism and the adaptive Snake activation are position agnostic, the ordering of the prompt features plays minimal role in timbre control. This allows cross-lingual VC where target speakers may come from unseen languages, since almost all linguistic-relevant patterns are broken by these position-agnostic operations. As long as the global traits are apparent enough in the WavLM features, speaker timbre can be successfully transferred, even if the model is not trained on multilingual data."}, {"title": "III. EXPERIMENTS", "content": "We use all the train splits of LibriTTS [39], an English corpus with 585 hours of 24kHz speech data spoken by around 2500 speakers, to train vec2wav 2.0. We only keep utterances from 6s to 30s to ensure proper prompt lengths. The resulting training set has around 360 hours. The prompt segment is cut starting from a random point within 1 second of either the beginning or the end of an utterance, extending inward towards the middle, with its length randomly sampled between one third and one half of the original utterance's duration. In this way, a reasonable range of prompt lengths is covered in training, and vec2wav 2.0 learns to handle short reference lengths well.\nWe use the k-means version of official vq-wav2vec model to extract content tokens from source speech. As this model adopts grouped vector quantization, we concatenate the code-vectors corresponding to each group before feeding the Conformer frontend. The input to the frontend is thus a 512-dimensional sequence in 10ms strides. The prompt embeddings are extracted from official WavLM-Large at the 6th layer.\nThe Conformer frontend of vec2wav 2.0 contains 2 Conformer blocks, where each of the self and cross attention modules has 2 heads and 184 attention dimensions. The prompt prenet has four CNN blocks with scaled residual connections, where the hidden dimensions are 128, 256 and 512 before being fed to cross attentions. The resulting generator model has 40.3M parameters.\nThe whole model is trained for 1 million steps on 4 NVIDIA A10 GPUs with a maximum batch capacity of 36s speech data per device. Other hyper-parameters follow CTX-vec2wav [3]."}, {"title": "B. English Any-to-Any VC", "content": "We conduct English any-to-any VC comparisons using the unseen speakers in the LibriTTS test-clean split. We randomly select 10 speakers, from each of whom 2 utterances are chosen to be the source utterances. Another 10 speakers are selected as target speakers with one 3-second reference utterance for each. This yields a test set of 200 any-to-any VC cases.\nTo measure the performance of VC systems, we adopt multi-faceted metrics that contain both objective and subjective ones:\n1) Quality and intelligibility: We use the subjective naturalness MOS (NMOS) and word error rate (WER percentage) between recognized hypotheses and ground truth texts. The NMOS tests require listeners to rate the utterances by quality and naturalness ranging from 1 to 5. WERs are computed using NeMo ASR.\n2) Speaker similarity: We conduct similarity MOS (SMOS) tests and compute speaker embedding cosine similarity (SECS). Listeners in SMOS tests are asked to rate timbre similarity between reference and synthesized items in 1-5 scale. SECS is computed via Resem-blyzer where speaker embeddings are extracted by a verification model for computing cosine similarity in range of -1 to 1.\n3) Prosody preservation: We additionally measure the correlation coefficient of pitch contours (P.Corr) between the source speech and converted speech. This is also an important metric in VC because ideal VC systems should preserve prosodic variations in source speech while transferring timbre attributes. The value range is -1 to 1, with higher values indicating better preservation.\nWe compare vec2wav 2.0 with some famous VC models. YourTTS [16] is a famous flow-based end-to-end VC model. DiffVC [19] and Diff-HierVC [20] promote convertibility via diffusion models. UUVC [26] also performs VC by discrete token recon-struction, but incorporates HuBERT tokens and additional prosody predictions. FACodec [4] is a state-of-the-art speech codec based on supervised decoupling of content, prosody, timbre and detail information. FACodec is capable of converting voices by simply replacing the speaker embedding into the target speaker and then decoding into waveform. We discard the detail tokens in FACodec for VC since we find these tokens still contain considerable speaker information that harms VC performance. We use the official implementation and checkpoints for all baselines. Note that the training data in"}, {"title": "C. Cross-Lingual Any-to-Any VC", "content": "To verify the cross-lingual VC ability of vec2wav 2.0, we use the same set of English source utterances in Table I, but convert to target speakers in other languages. We collect reference utterances from five languages in MLS [40]. The test set is the full combination of source and target utterances. For each of those languages, one male and one female speaker are randomly chosen as target speakers, and one reference utterance for each target speaker is sampled. We compare vec2wav 2.0 with the famous cross-lingual VC model YourTTS that is trained on multilingual data, and also Diff-HierVC which appears to be the most competitive in Table I. We conduct subjective and objective evaluations in the same way as Section III-B.\nTable II shows the results. Although not trained on multilingual data, vec2wav 2.0 consistently outperforms YourTTS and Diff-HierVC in speaker similarity and quality with a significant margin. The WER and P.Corr comparisons show a similar conclusion with Table I that vec2wav 2.0 possesses a decent level of intelligibility and prosody preservation, although not the best. Therefore, it is demonstrated that vec2wav 2.0 performs competitive conversions, regardless of the languages of references."}, {"title": "D. Ablation Study", "content": "We also conduct ablation studies on different input SSL discrete tokens and vocoder architectures. Apart from vq-wav2vec, we train CTX-vec2wav (our predecessor) and vec2wav 2.0 on HuBERT tokens and wav2vec 2.0 [10] tokens. The HuBERT tokens are obtained by 2048-centroid clustering on the output of the last layer. The wav2vec 2.0 tokens are considered the quantizer output before the Transformer, with 2 codebook groups each with 320 codes.\nTo compare architectures, we additionally train two variants of vec2wav 2.0 on vq-wav2vec inputs: vec2wav 2.0-ab1 that replaces the adaptive Snake activations in BigVGAN by the original Snakes; and vec2wav 2.0-ab2 that further replaces BigVGAN with HifiGAN. Thus the comparison between vec2wav2.0 and \"abl\" indicates the effect of adaptive Snake activation, while that between CTX-vec2wav and \"ab2\" shows the difference made by prompt feature and modules. We present the ablation studies in terms of SECS and P.Corr in Fig.3, together with the baselines in Section III-B. It can be found that"}, {"title": "E. Analysis of Pitch Contours: Case Study", "content": "Pitch plays an important role in both the perception of global timbre and local prosody variations. An ideal VC system should retain pitch variations but shift the global ranges according to target speakers. Fig.4 shows the pitch contours of a sample converted by vec2wav 2.0. It can be seen that the average pitch of the converted utterance closely matches that of the reference, while the source and converted ones share the similar variations. This intuitively shows vec2wav 2.0 properly manages the global pitch range while keeping the local traits."}, {"title": "IV. CONCLUSION", "content": "We present a novel VC method, vec2wav 2.0, based on the re-synthesis of speech discrete tokens. It takes advantage of SSL features in both content and timbre representations and enhances CTX-vec2wav in architectural designs. The adaptive Snake activation technique is proposed to better incorporate timbre into waveform reconstruction. The resulting model achieves remarkable performance on intra and cross-lingual VC tasks. We believe vec2wav 2.0 has promising impacts on the future LLM-based speech generation paradigm. Future efforts are needed in improving the intelligibiltiy and prosody preservation of the proposed method."}]}