{"title": "Balancing Content Size in RAG-Text2SQL System", "authors": ["Prakhar Gurawa", "Anjali Dharmik"], "abstract": "Large Language Models (LLMs) have emerged as a promising solution for converting natural language queries into SQL commands, enabling seamless database interaction. However, these Text-to-SQL (Text2SQL) systems face inherent limitations, hallucinations, outdated knowledge, and untraceable reasoning. To address these challenges, the integration of retrieval-augmented generation (RAG) with Text2SQL models has gained traction. RAG serves as a retrieval mechanism, providing essential contextual information, such as table schemas and metadata, to enhance the query generation process. Despite their potential, RAG + Text2SQL systems are susceptible to the quality and size of retrieved documents. While richer document content can improve schema relevance and retrieval accuracy, it also introduces noise, increasing the risk of hallucinations and reducing query fidelity as the prompt size of the Text2SQL model increases. This research investigates the nuanced trade-off between document size and quality, aiming to strike a balance that optimizes system performance. Key thresholds are identified where performance degradation occurs, along with actionable strategies to mitigate these challenges. Additionally, we explore the phenomenon of hallucinations in Text2SQL models, emphasizing the critical role of curated document presentation in minimizing errors. Our findings provide a roadmap for enhancing the robustness of RAG + Text2SQL systems, offering practical insights for real-world applications.", "sections": [{"title": "1 Introduction", "content": "The rapid growth of data in relational databases has made accessing and analyzing this information increasingly challenging, especially for non-technical users who lack expertise in writing SQL queries. Translating natural-language queries into SQL commands is critical to enabling seamless interaction with complex databases. Text-to-SQL models address this need by leveraging natural language processing techniques to convert natural language into executable SQL queries, thus democratizing database access (Shi et al. (2024)).\nLarge Language Models (LLMs) underpin many Text2SQL systems, offering impressive capabilities in understanding and generating natural language. However, they have limitations, such as hallucinations, outdated knowledge, and non-transparent reasoning processes (Gao et al. (2024)). \u03a4\u03bf overcome these challenges, Retrieval-Augmented Generation (RAG) has emerged as a transformative approach (Lewis et al. (2020);Izacard et al. (2022);Guu et al. (2020);Borgeaud et al. (2022)). By incorporating external knowledge from databases, RAG enhances the accuracy and reliability of LLMs for knowledge-intensive tasks. It enables continuous updates of knowledge and the integration of domain-specific information, integrating the intrinsic capabilities of LLMs with dynamic external data repositories (Gao et al. (2024);Wang et al. (2024)). The information retrieved from the RAG system serves as input for the LLM or Text2SQL model, forming a critical prompt component. The information retrieved from the RAG system is incorporated into the prompt provided to the LLM or Text2SQL model, playing a pivotal role in its ability to generate accurate SQL queries. The quality and quantity of this retrieved information not only determine the size of the prompt but also significantly impact the overall performance of the model. Thus, optimizing what and how much information is captured from the RAG system is crucial to achieving a balanced and efficient query generation process. In the broader domain of artificial intelligence, prompt engineering has emerged as a transformative discipline, harnessing the full potential of LLMs by tailoring input prompts to maximize their effectiveness and reliability (Sahoo et al. (2024);Chang et al. (2024)).\nDespite its advantages, RAG introduces its own set of challenges. The \"Garbage In, Garbage Out\" (GIGO) principle is highly applicable to the Retrieval-Augmented Generation (RAG) system, just as it is in other machine learning and data processing contexts. Incorporating excessive or irrelevant information can degrade the performance of LLM models, introduce noise into the prompts, and increase the risk of hallucinations (Liu et al. (2023);Maynez et al. (2020)). When provided with incorrect or incomplete information, LLMs may produce inaccurate responses or fail to comprehend the input query (Bian et al. (2023);Adlakha et al. (2024)). This makes the quality of retrieved content a critical factor in the overall performance of RAG-enabled systems. In recent years, there has been a surge in the application of LLMs for Text2SQL tasks, driven by their enhanced performance, adaptability, and potential for future improvements (Shi et al. (2024)). Although RAG + Text2SQL systems themselves are not enough to solve this problem, and in the real world, we might need other powerful additions in systems like TAG (Biswal et al. (2024)) and many other additions, but we will keep our scope limited to RAG + Text2SQL. Real-world deployment of RAG + Text2SQL systems often demands careful optimization of document content used for retrieval. Larger documents may improve retrieval precision but simultaneously increase the risk of hallucinated SQL queries as prompts become longer and noisier. Balancing these factors is crucial to achieving reliable system performance. This paper focuses on the following core challenges:\n1.  The performance of RAG + Text2SQL systems is highly sensitive to the quality and size of the documents used for retrieval.\n2.  Larger and detailed documents can improve the accuracy of RAG but exacerbate hallucination issues in Text2SQL models.\nImproving the RAG + Text2SQL system holds significant potential for enhancing efficiency in everyday tasks within the software industry."}, {"title": "1.1 Why This Problem Matters", "content": "In practical applications, such as business intelligence, automated reporting, and natural language interfaces for enterprise data, even a hallucinated SQL query can lead to incorrect insights and significant decision-making errors. Therefore, ensuring a balance between document size and quality is essential to maintaining the reliability of query results. Enhancing the RAG + Text2SQL system holds significant potential for increasing efficiency in everyday tasks within the software industry. This, in turn, will contribute to the development of better software solutions, ultimately improving various aspects of our day-to-day lives."}, {"title": "1.2 Objectives of the Paper", "content": "This research addresses the trade-off between document size and quality in RAG + Text2SQL systems. Specifically, we:\n1.  Analyse how varying document content impacts retrieval accuracy, hallucination rates, and system reliability.\n2.  Explore the performance differences in RAG systems and RAG + Text2SQL systems across a range of document configurations.\n3.  Propose a framework for designing concise, high-quality documents to achieve an optimal balance, minimizing hallucinations while maximizing retrieval effectiveness.\nBy conducting extensive experiments with multiple document sets featuring varying levels and amounts of information, we provide actionable insights into optimizing RAG + Text2SQL frameworks for real-world applications."}, {"title": "2 Related Work", "content": "The field of Large Language Models (LLMs) is advancing at an unprecedented pace, with numerous innovations in architectures and methodologies to enhance their performance. One such approach is Retrieval-Augmented Generation (RAG), which has gained significant attention for improving the utility of LLMs by integrating domain-specific external retrieval mechanisms."}, {"title": "2.1 Text-to-SQL Models", "content": "Text-to-SQL models have evolved considerably over time, focusing on bridging the gap between natural language understanding and database querying. The survey by (Shi et al. (2024)) explores the use of LLMs for Text-to-SQL tasks, highlighting the increasing importance of efficient querying in the face of growing data volumes. As relational databases rely on SQL for interaction, the need to democratize access for non-expert users has driven advancements in Text-to-SQL technologies. One notable work, Seq2SQL (Zhong et al. (2017)), introduced a deep neural network architecture to translate natural language questions into SQL queries, addressing the accessibility challenge for users without SQL expertise."}, {"title": "2.2 Impact of Document Characteristics on RAG Systems", "content": "Recent studies have examined how document characteristics, including structure and content length, influence RAG system performance. The work by (on Building RAG Systems for Technical Documents (2024)) provides insights into factors such as chunk size, embedding reliability, sentence- versus paragraph-based retrieval, keyword placement, and context order. These findings emphasize the importance of tailoring retrieval strategies to document properties, particularly for technical domains. Similarly, (Zhao et al. (2024)) investigates the impact of retrieved document characteristics and prompt strategies on RAG system performance, highlighting how document quality, size, and content type significantly affect the accuracy and reliability of responses. This study further explores document selection methods and prompting strategies, underscoring their role in RAG + LLM frameworks."}, {"title": "2.3 Prompt Design and Its Influence on LLM Performance", "content": "Prompt engineering has emerged as a key factor in determining the performance of LLMs. Research such as (He et al. (2024)) reveals that prompt formatting significantly impacts GPT-based models (Brown et al. (2020)), with no single format proving universally superior. This underscores the need for diverse prompt formats to enhance future LLM testing and performance optimization. Additionally, (Kojima et al. (2023)) introduces Zero-shot-CoT, a zero-shot prompt designed to elicit chain-of-thought reasoning in LLMs, contrasting with previous few-shot approaches that require handcrafted examples. The work by (Yugeswardeenoo et al. (2024)) explores question analysis prompting, demonstrating its potential to improve accuracy across diverse reasoning tasks, including mathematics and commonsense queries. These studies underline the critical role of prompt strategies in achieving robust and accurate model outputs, particularly in scenarios where document content directly influences prompt design, as in RAG + Text2SQL systems."}, {"title": "2.4 Hallucination in LLMs and RAG Systems", "content": "Hallucination remains a persistent challenge in generative AI and LLMs. Multiple studies (Peng et al. (2023);Dziri et al. (2021);Feldman et al. (2023);Varshney et al. (2023);Dhuliawala et al. (2023)) have investigated hallucination phenomena, analysed their origins and proposed mitigation strategies. A notable contribution comes from (Jesson et al. (2024)), which presents a technique for estimating hallucination rates in conditional generative models within in-context learning frameworks. These findings are crucial for our research, as document quality and retrieved information significantly impact hallucination rates in RAG + Text2SQL systems.\nBuilding upon the existing body of work, our research focuses on the interplay between document size and quality in RAG + Text2SQL systems. By analysing how these factors affect retrieval accuracy, prompt design, and hallucination rates, we aim to provide actionable insights for optimizing these systems in real-world applications."}, {"title": "3 Problem Definition", "content": "When integrated with text-to-SQL (Text2SQL) models, the effectiveness of retrieval-augmented generation (RAG) systems heavily depends on the quality and size of the documents used for retrieval. This dependency arises from these documents' role in providing schema and context for accurate SQL query generation.\nOur study used a subset of the SPIDER data set (Yu et al. (2019)), a widely recognized benchmark for Text2SQL systems, as the basis for experimentation. By sampling various SQL table schemas from the SPIDER data set, we constructed multiple document representations to systematically evaluate their influence on the combined RAG + Text2SQL system.\nEach document in our study corresponds to a single table and includes:\n1.  The table's schema includes column names and other relationships among columns.\n2.  Additional metadata that may aid the RAG system's understanding and retrieval accuracy.\nTo explore the impact of document content, we created multiple iterations of data sets based on the SPIDER data set. These iterations varied in document size, level of detail, and quality, enabling us to analyze their effect on the RAG system and the RAG + Text2SQL system as a unified framework. The primary goal of this study is to determine the optimal document structure that balances two competing objectives:\n1.  Minimizing hallucination risks in Text2SQL outputs caused by noisy or excessive document content.\n2.  Maximizing retrieval relevance and accuracy in SQL query generation by ensuring sufficient and precise document information.\nBy conducting controlled experiments across these varied document representations, we aim to provide actionable insights into document design strategies for RAG + Text2SQL systems. This research highlights how document quality and content size directly influence system performance, paving the way for more robust and reliable implementations of such frameworks in real-world applications."}, {"title": "4 Experimental Setup", "content": "In this section, we present the key components of our experimental setup 1. We begin by introducing the data set employed to evaluate the performance of the RAG+Text2SQL system. Next, we outline the document creation process tailored for the RAG system, followed by a detailed discussion of the RAG system architecture and its configuration parameters. Finally, we describe the Text2SQL model, an advanced LLM-based system, elaborating on its prompt design and implementation strategy."}, {"title": "4.1 About Data Set: Spider", "content": "The SPIDER data set 2, introduced in \"Spider: A Large-Scale Human-Labeled data set for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task\" (Yu et al. (2019)) is a benchmark designed to evaluate the ability of models to generate SQL queries from natural language queries across diverse domains. It consists of 10,181 questions and 5,693 unique complex SQL queries, spanning 200 databases with multiple tables and representing 138 different domains. The data set's complexity and diversity make it a valuable resource for advancing research in semantic parsing and text-to-SQL systems. The data set is highly versatile and includes SQL queries across multiple levels of complexity, encompassing all major SQL query components. It features queries with SELECT clauses involving multiple columns and aggregations, along with WHERE, GROUP BY, HAVING, ORDER BY, and LIMIT clauses. Advanced operations such as JOIN, INTERSECT, EXCEPT, UNION, NOT IN, OR, AND, EXISTS, and LIKE are well-represented. Additionally, the data set includes nested queries and subquery structures, reflecting real-world query scenarios. To ensure comprehensive coverage, the annotators have carefully ensured that every table in the database is referenced in at least one query (Yu et al. (2019)).\nThe data set is structured into training, validation, and test subsets to facilitate systematic evaluation. We utilized a curated section of the data set for our experiments, focusing on 15 domains. This subset comprises 719 queries and includes 54 tables, enabling us to conduct domain-specific analysis while maintaining a manageable scale for experimentation. Table 1 presents an overview of the 15 distinct domains covered in the data set, along with the corresponding SQL queries. The data set comprises 719 SQL queries, highlighting its diverse and comprehensive coverage across various database schemas."}, {"title": "4.2 Document Variations", "content": "RAG systems retrieve information by analysing the similarity between the user query and the available documents. Based on this similarity, the system identifies and selects the top k most relevant documents, accompanied by their respective similarity scores, ensuring that the retrieved content is closely aligned with the user's query. In our experiment, each document represents a single SQL table, encapsulating information about the table schema and, in some cases, additional metadata such as textual descriptions or example insert queries. The documents were created by sampling table schemas from the SPIDER data set, generating variations based on schema representation and adding contextual details, as outlined in the previous section.\nWhen a user provides a natural language query, it is first passed through the RAG system, which retrieves the top three relevant documents from the collection. These documents and RAG are expected to represent the tables most pertinent to answering the query. The retrieved documents are combined with the original user query and specific Text2SQL instructions to form a prompt. This prompt is provided to the Text2SQL model, which processes the input and generates the desired SQL query. This setup ensures that the system leverages both the retrieval capabilities of RAG and the reasoning power of the Text2SQL model. By evaluating the performance across various document representations, we aim to understand how changes in document quality and content size influence retrieval accuracy, prompt construction, and the final SQL query generation.\nTo explore the effect of document quality and content on RAG + Text2SQL performance, we created several variations of table schema documents:\n1.  Spider-Data-1: Includes only the original CREATE TABLE statements from the SPIDER data set.\n2.  Spider-Data-2: Features a uniform representation of the CREATE TABLE statements across all tables to ensure consistency.\n3.  Spider-Data-3: Contains the modified CREATE TABLE statements along with one example INSERT query to illustrate data usage.\n4.  Spider-Data-4: Extends Spider-Data-3 by including two example INSERT queries to provide richer context.\n5.  Spider-Data-5: Combines the modified CREATE TABLE statements with a textual description of the table and its columns.\n6.  Spider-Data-6: Builds upon Spider-Data-5 by adding one example INSERT query to the document.\n7.  Spider-Data-7: Extends Spider-Data-6 by including two example INSERT queries for even greater detail."}, {"title": "4.2.1 DOCUMENT QUALITY VS. SIZE TRADE-OFF", "content": "The variations in document content among the different document set reflect a trade-off between document size and quality, a critical factor in the RAG + Text2SQL pipeline. It will impact the overall system in the following ways:\n1.  Effect on Retrieval: Larger documents, enriched with more contextual and descriptive information, enhance the retrieval step by providing more relevant matches to the user query. For instance, adding textual descriptions or sample data inserts provides the RAG system with richer signals to determine relevance.\n2.  Effect on Prompt Size: The retrieved documents, combined with the user query and predefined instructions, are formatted into prompts for the Text2SQL model. Large documents can increase prompt size, which, while beneficial for retrieval, risks overloading the Text2SQL model. This can lead to inefficiencies in token usage and increased chances of hallucination, where the model generates queries based on incorrect or fabricated interpretations of the input.\n3.  Balancing Trade-offs: This study hypothesizes that balancing document size and quality is essential for optimizing overall system performance. Documents should contain enough information to support accurate retrieval without overburdening the Text2SQL model's processing capabilities."}, {"title": "4.3 Description of RAG system", "content": "Retrieval-Augmented Generation (RAG) introduces an information retrieval process that enhances the generative model's accuracy and robustness by fetching relevant objects from external data stores. This integration allows RAG systems to dynamically incorporate up-to-date and domain-specific knowledge, significantly improving their performance, particularly in knowledge-intensive tasks (Zhao et al. (2024)).\nA retrieval mechanism is embedded into the model pipeline in an RAG system. It fetches contextually relevant information from an external knowledge base or document corpus based on the user query. The retrieved content is then combined with the original query and passed to a generative model, which uses this enriched context to produce its output. This approach represents a transformative shift in Generative AI, creating more transparent (\"glass-box\") models that excel in accuracy and reliability, especially in domains requiring precise information (Khan et al. (2024)). RAG systems also mitigate the need for frequent retraining of large models, reducing both computational and financial costs. This adaptability makes RAG particularly appealing for enterprise applications, where maintaining up-to-date models is essential.\nWe designed our RAG system using the following components:\n1.  Framework: We utilized LangChain (Chase (2022)), a robust framework that simplifies the development of advanced applications integrating language models. Its modular design allows seamless integration of RAG components.\n2.  Embeddings: Semantic search in RAG systems relies on vector embeddings. For our implementation, we used all-MiniLM-L12-v2, a sentence-transformer model capable of converting textual data into fixed-size embeddings. This model is ideal for clustering and semantic search tasks and has demonstrated superior performance among open-source embedding models (Aperdannier et al. (2024)).\n3.  Vector Store: Efficient storage and querying of embeddings are essential for the RAG pipeline. We employed FAISS (Facebook AI Similarity Search) (Douze et al. (2024)), an open-source library optimized for fast and lightweight similarity searches. FAISS retrieves relevant document chunks during query processing with high precision.\n4.  k Value (Number of Retrieved Documents): The k value in RAG systems specifies the number of documents retrieved from the external knowledge source for a given query. These documents provide context to the generative model for response generation. In our setup, we set k = 3. This choice aligns with our task of SQL query generation, where a single SQL query typically involves no more than three tables, each represented as a document in our system. Selecting an appropriate k value is crucial as it directly impacts system performance. A higher k value offers a more comprehensive context, potentially improving response quality. However, it can also introduce noise or irrelevant information, increasing processing complexity and a higher risk of hallucinations or incorrect outputs. Conversely, a smaller k value may provide insufficient context, resulting in incomplete or inaccurate responses. Determining the optimal k value requires empirical evaluation to balance sufficient context with minimizing irrelevant information.\nThis carefully constructed RAG system serves as the foundation for our experiments, enabling us to evaluate the impact of document size and quality on performance in RAG + Text2SQL settings."}, {"title": "4.4 About Text2SQL Model", "content": "Language models are transforming data management by enabling users to query databases using natural language, eliminating the need for specialized SQL knowledge. This innovation has spurred extensive research in both Text2SQL and Retrieval-Augmented Generation (RAG) methods (Biswal et al. (2024)). For our Text2SQL tasks, we utilized the LLama-3-based SQLCoder-8B (Defog (2024)), a state-of-the-art AI model designed to translate natural language queries into SQL. SQLCoder-8B addresses the limitations of traditional Text2SQL models, setting new benchmarks for accuracy, adaptability, and ease of use. With an impressive accuracy rate exceeding 90% in zero-shot scenarios, the model demonstrates superior performance across diverse query contexts, making it a cornerstone of our study. To ensure optimal performance, we carefully tuned several hyperparameters during experimentation:\n1.  Temperature: Set to 0.01 to minimize output randomness, enhancing generated queries' consistency.\n2.  Top-p: Configured to 0.7 to control the diversity of generated responses while maintaining relevance.\n3.  Max New Tokens: Limited to 1024, ensuring the model generates complete and syntactically correct SQL queries without unnecessary verbosity.\n4.  Return Full Text: Set to False to streamline outputs for integration into downstream processes.\nThese optimizations were critical in enabling SQLCoder-8B to function effectively within the RAG + Text2SQL system, providing accurate and efficient query generation. The model's robustness and adaptability make it an ideal choice for exploring the impact of document size and quality on system performance.\nFigure 1 illustrates the complete workflow of our Retrieval-Augmented Generation (RAG) integrated Text2SQL system. It showcases the interaction between the user query, the retrieval process, and the generative model, highlighting how relevant context from external documents is seamlessly incorporated into the SQL generation process."}, {"title": "4.4.1 PROMPT DESIGN FOR THE TEXT2SQL MODEL", "content": "One of the key factors influencing the performance of large language models (LLMs) is the structure and content of the input prompt. Each model requires a carefully crafted prompt format to ensure optimal functioning. In the case of our Text2SQL model, the prompt is designed to integrate essential components that guide the model in generating accurate SQL queries. The prompt consists of the following three core elements:\n1.  User Question: This is the user's natural language query, which needs to be converted into a corresponding SQL statement.\n2.  Instructions: These are specific directives tailored to instruct the Text2SQL model on how to process the input and generate the desired SQL query. Clear and precise instructions play a pivotal role in ensuring the model adheres to the intended logic and query format.\n3.  Information from the RAG System: In our system, the Retrieval-Augmented Generation (RAG) mechanism provides critical context by retrieving schema details and relevant knowledge from the top 3 most relevant documents or tables. This retrieved information is incorporated into the prompt, enabling the Text2SQL model to understand the database structure and generate accurate SQL queries.\nBelow, we present an example of the prompt structure utilized in our Text2SQL model. The example illustrates how instructions are provided to guide the model in generating an SQL query, along with the user's question and the corresponding table schema.\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nGenerate a SQL query to answer this question: \u2018{question}'\n### Instructions\n\u2013 Given an input question, create a syntactically correct query to run, then look at the results of the query and return the answer.\n\u2013 Never query for all the columns from a specific table; only ask for the relevant columns given the question.\n\u2013 Only return the columns the user asks for; do not give any additional ID column that the user does not ask for explicitly.\n\u2013 Do not add ORDER BY in the query if the user has not explicitly asked to order it.\n\u2013 If you cannot answer the question with the available database schema, return 'I do not know.'\n\u2013 Make sure that you never return two columns with the same name, especially after joining two tables. You can differentiate the same column name by applying column_name + table_name.\n\u2013 DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP, etc.) to the database.\n\u2013 If you are fetching data from a table only then use its columns to filter out the data.\n\u2013 You MUST double-check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\nDDL statements:\nCREATE TABLE \"farm\" (\n\"Farm_ID\" int,\n\"Year\" int,\n\"Total_Horses\" real,\n\"Working_Horses\" real,\n\"Total_Cattle\" real,\n\"Oxen\" real,\n\"Bulls\" real,\n\"Cows\" real,\n\"Pigs\" real,\n\"Sheep_and_Goats\" real,\nPRIMARY KEY (\"Farm_ID\")\n);\nCREATE TABLE \"farm_competition\" (\n\"Competition_ID\" int,\n\"Year\" int,\n\"Theme\" text,\n\"Host_city_ID\" int,\n\"Hosts\" text,\nPRIMARY KEY (\"Competition_ID\"),\nFOREIGN KEY (\u2018Host_city_ID\u2018) REFERENCES \u2018city\u2018(\u2018City_ID\u2018)\n);\nCREATE TABLE \"competition_record\" (\n\"Competition_ID\" int,\n\"Farm_ID\" int,\n\"Rank\" int,\nPRIMARY KEY (\"Competition_ID\",\"Farm_ID\"),\nFOREIGN KEY (\u2018Competition_ID\u2018) REFERENCES \u2018farm_competition\u2018(\u2018Competition_ID\u2018),\nFOREIGN KEY (\u2018Farm_ID\u2018) REFERENCES \u2018farm\u2018('Farm_ID')\n);<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nThe following SQL query best answers the question \u2018{question}\u2018:\n'''sql"}, {"title": "5 Evaluation Metrics", "content": "To evaluate the performance of the RAG system and the combined RAG + Text2SQL system, we have created multiple sets of documents. Our goal is to assess how document content and quality variations impact both systems' performance. By analyzing these variations, we aim to understand how different document characteristics influence the overall performance of the RAG-based retrieval process and the subsequent Text2SQL query generation."}, {"title": "5.1 Evaluation Metric for the RAG System", "content": "The RAG system evaluates a user query by retrieving the top-k relevant documents and their corresponding relevance scores, where a lower score indicates higher relevance. To assess the RAG system's performance on the different data sets created from the SPIDER data set, we measure and compare its ability to effectively distinguish relevant tables from non-relevant ones.\nA superior RAG system is characterized by its ability to assign a broader range of relevance scores, demonstrating a clear differentiation between valid and non-relevant documents. Conversely, if the scores for the top-k documents are closely clustered, it indicates difficulty in discrimination and potential confusion in the system. We executed all user queries through the RAG system for each data set variation and recorded the top-k retrieved documents along with their associated relevance scores. The distribution of these scores was analyzed using the following metrics:\n1. Discounted Cumulative Gain (DCG): Evaluates the relevance of retrieved documents, emphasizing higher-ranked documents.\n$DCG_p = \\sum_{i=1}^{p} \\frac{rel(i)}{log_2(i + 1)}$\nwhere:\n\u2022 rel(i) is the relevance score of the document at position i.\n\u2022 p is the rank position (top-k in your case).\n2. Standard Deviation (Std Dev): Measures the spread of relevance scores to understand the variability in the system's differentiation capabilities.\n$StdDev = \\sqrt{\\frac{1}{k} \\sum_{i=1}^{k} (rel(i) \u2013 \\overline{rel})^2}$\nwhere:\n\u2022k is the number of top documents retrieved.\n\u2022 rel(i) is the relevance score of the document at position i.\n\u2022 $\\overline{rel}$ is the mean relevance score of the top-k documents:\n$\\overline{rel} = \\frac{1}{k} \\sum_{i=1}^{k} rel(i)$\n3. Range: Captures the difference between the highest and lowest scores among the top-k documents to indicate the breadth of the system's scoring distribution.\n$Range = max(rel(i)) \u2013 min(rel(i))$\nwhere:\n\u2022 max(rel(i)) is the highest relevance score among the top-k documents.\n\u2022 min(rel(i)) is the lowest relevance score among the top-k documents.\nFor each data set, we computed the average values of these metrics across all user queries. By comparing the aggregated metrics, we aim to evaluate and rank the performance of different RAG system configurations and document representations. This approach ensures a comprehensive understanding of the RAG system's ability to retrieve and rank relevant tables effectively across varying data set structures."}, {"title": "5.2 Evaluation Metrics for the RAG + Text2SQL System", "content": "In this research, our primary focus is not on measuring the performance of the Text2SQL models in isolation, as that would require experiments with different hyperparameters and model configurations, which are beyond the scope of this study. Instead, our objective is to evaluate the performance of various document data sets on the RAG + Text2SQL system. To achieve this, we assess the quality of the SQL queries generated by the whole RAG + Text2SQL system.\nThe evaluation involves measuring instances of hallucination and conducting SQL query similarity checks. For this purpose, we compare the generated SQL queries with the corresponding correct queries provided in the SPIDER data set. The following metrics were utilized to measure query similarity and identify discrepancies:\n1.  Normalized Edit Distance: The process involves following steps, calculate the edit distance between the generated and correct SQL queries. Then, normalize the distance by dividing it by the length of the queries. If the normalized edit distance is below a threshold of 0.5 (determined empirically), the queries are considered similar; otherwise, they are not.\n2.  Embedding Matching: The steps are converting SQL queries into textual embeddings using the all-MiniLM-L12-v2 model. Then, the cosine similarity between the embeddings is measured. Finally, queries with a similarity score less than or equal to 0.85 are classified as mismatched.\n3.  Fuzzy Matching: Involves using fuzzy string-matching algorithms to calculate the similarity score between the queries. A threshold of 75 was set, so queries with a similarity score equal to or exceeding this value are considered similar.\n4.  SQL Component Matching: It involves performing a component-wise comparison between the generated and correct SQL queries, evaluating:\n\u2022 Table Selection: Matches the tables involved in the query.\n\u2022 Column Selection: Matches the columns being queried.\n\u2022 Operation Selection: Matches the operations (e.g., JOIN, WHERE, GROUP BY) used in the query.\nAny mismatches in these components indicate that the generated query does not align with the expected query, highlighting instances of hallucination in query generation.\n5.  Database Execution Comparison: In this step, execute the generated and correct SQL queries on a PostgreSQL database and compare the outputs. Then, it utilizes exact matching and rule-based matching of results. Mismatches in the outputs suggest that the generated SQL query is either incorrect or unsuitable for the given query.\nFinally, we measure the percentage of SQL queries showing hallucination as the above metrics. By applying these metrics, we systematically evaluate the impact of document variations on the RAG + Text2SQL system and provide insights into the quality and reliability of the generated SQL queries.\n$\\text{AvgDCG} = \\frac{1}{Q} \\sum_{q=1}^{Q} DCG_q$\n$\\text{AvgStdDev} = \\frac{1}{Q} \\sum_{q=1}^{Q} \\text{StdDev}_q$\n$\\text{AvgRange} = \\frac{1}{Q} \\sum_{q=1}^{Q} \\text{Range}_q$\nWhere $Q$ is the total number of user queries."}, {"title": "6 Results and Analysis", "content": "To evaluate the impact of varying document content and quality on RAG + Text2SQL systems, we utilized seven document sets derived from the SPIDER data set. The data sets ranged from Spider-Data-1, containing only the original CREATE TABLE statements, to Spider-Data-7, which incorporated modified CREATE TABLE statements, textual table descriptions, and two example INSERT queries to provide comprehensive context. The progression from Spider-Data-1 to Spider-Data-7 systematically increased the amount of data and contextual richness available to the RAG system. The primary objective was to analyze how the performance of both the RAG system and the combined RAG + Text2SQL system changed as the content in the documents increased. While all additional information across data sets, such as textual descriptions and example INSERT queries, was relevant, we aimed to determine the effect of such content augmentation on retrieval relevance and SQL query generation. The added information aimed to improve the Text2SQL model's understanding of tables, schemas, and data types."}, {"title": "6.1 RAG Retrieval Performance", "content": "To assess the RAG system's performance", "performance": "n1.  Range of Relevance Scores: The difference between the highest and lowest relevance scores for the top-k documents.\n2.  Standard Deviation of Relevance Scores: Indicates the variability in scores and the system's differentiation capability.\n3.  Discounted Cumulative Gain (DCG): Measures the ranking quality of retrieved documents", "system": "n1.  Range of Scores: The range increased with richer document content", "Deviation": "A higher standard deviation was observed for data sets with richer content", "DCG": "The DCG metric demonstrated a downward trend as more information was added, indicating that the top-ranked documents retrieved by the RAG system became increasingly relevant in the DCG formula; we used the RAG score, which the lower they are, the more relevant the documents are.\n4.  The plots illustrate a significant improvement in"}]}