{"title": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks\nin Conversational Turns Undermining Refusal Efficacy and\nDefenses over SORRY-Bench", "authors": ["Aman Priyanshu", "Supriti Vijay"], "abstract": "This paper introduces FRACTURED-SORRY-\nBench, a framework for evaluating the safety of\nLarge Language Models (LLMs) against multi-\nturn conversational attacks. Building upon the\nSORRY-Bench dataset, we propose a simple yet\neffective method for generating adversarial prompts\nby breaking down harmful queries into seemingly\ninnocuous sub-questions. Our approach achieves a\nmaximum increase of +46.22% in Attack Success\nRates (ASRs) across GPT-4, GPT-40, GPT-40-mini,\nand GPT-3.5-Turbo models compared to baseline\nmethods. We demonstrate that this technique poses\na challenge to current LLM safety measures and\nhighlights the need for more robust defenses against\nsubtle, multi-turn attacks.", "sections": [{"title": "Introduction", "content": "As Large Language Models (LLMs) become increas-\ningly prevalent in various applications, ensuring their\nsafe and ethical use is paramount [17, 18, 14, 15).\nWhile significant progress has been made in aligning\nthese models with human values and implementing\nsafety measures, they remain vulnerable to adversar-\nial attacks, particularly those that leverage the nu-\nances of human conversation [5, 3].\nThis paper presents FRACTURED-SORRY-\nBench, a framework that extends the SORRY-Bench\ndataset to evaluate LLM safety against a new class\nof attacks. Our method focuses on decomposing\nharmful queries into multiple, seemingly benign\nsub-questions, exploiting the multi-turn nature of\nconversations to bypass safety mechanisms. This\napproach represents a more efficient and accessi-\nble technique for generating adversarial samples\ncompared to complex optimization-based methods\n[9, 13]."}, {"title": "Preliminaries", "content": "Prior work has introduced various datasets and\nframeworks for evaluating LLM safety, including Do-\nNot-Answer [2], SimpleSafetyTests [7], ALERT [6],\nand HarmBench [15], each contributing unique per-\nspectives on assessing LLM vulnerabilities and safe-\nguards. SORRY-Bench [1] introduced a comprehen-\nsive benchmark for evaluating LLM safety refusal be-\nhaviors. It provides a fine-grained taxonomy of 45\npotentially unsafe topics and a balanced dataset of\nunsafe instructions. Our work builds upon this foun-\ndation by introducing a new dimension of evaluation:\nmulti-turn conversational attacks."}, {"title": "SORRY-Bench Dataset", "content": null}, {"title": "Prompt Injection Techniques", "content": "Recent work, such as Imposter.AI [8], has explored\nsophisticated methods for extracting harmful infor-\nmation from LLMs. These approaches often involve\ncomplex, multi-step processes using multiple models\nor are large scale collections [12]. In contrast, our\nmethod focuses on a simpler, more efficient single-\nshot prompting technique that achieves similar goals.\nOther relevant works include Universal and Trans-\nferable Adversarial Attacks [9] & Rainbow Teaming\n[4], which demonstrate the potential for creating ad-\nversarial prompts that transfer across different LLMS,\nand the exploration of stealthy attacks using ciphers\n[10]."}, {"title": "Methodology", "content": "Our approach, FRACTURED-SORRY-Bench, intro-\nduces a straightforward method for creating adversar-\nial samples, inspired by the chain-of-thought prompt-\ning technique [11]:"}, {"title": "Result Analysis", "content": "Our results show significant increases in ASR across\nall tested models, as presented in Table 1. GPT-3.5-\nTurbo demonstrated the highest vulnerability, with\nan ASR that increased by a factor of 10.9\u00d7 compared\nto its vanilla version, followed by GPT-4 (4.91\u00d7),\nGPT-40 (4.29\u00d7), and GPT-40-mini (3.9\u00d7).\nWe employed GPT-4 as a judge, inspired by prior\nliterature [15, 16]."}, {"title": "Attack Success Rates", "content": null}, {"title": "Intent Conveyance Analysis", "content": "We conducted an analysis to determine whether the\nfractured prompts correctly conveyed the original\nharmful intent. We present these results in Table 2.\nOur findings indicate that 49.33% of the fractured\nprompt sets successfully conveyed the original intent,\nwith variations across different categories of harmful\nqueries. This analysis methodology draws inspiration\nfrom work on evaluating LLM safeguards [2, 7]."}, {"title": "Conclusion", "content": "FRACTURED-SORRY-Bench demonstrates the vul-\nnerability of current LLM safety measures to subtle,\nmulti-turn attacks. By decomposing harmful queries\ninto seemingly innocent sub-questions, we achieve sig-\nnificant increases in attack success rates across mul-\ntiple models. This work highlights the need for more\nsophisticated safety mechanisms that can understand"}]}