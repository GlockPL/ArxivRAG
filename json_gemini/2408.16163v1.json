{"title": "FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench", "authors": ["Aman Priyanshu", "Supriti Vijay"], "abstract": "This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the safety of Large Language Models (LLMs) against multi-turn conversational attacks. Building upon the SORRY-Bench dataset, we propose a simple yet effective method for generating adversarial prompts by breaking down harmful queries into seemingly innocuous sub-questions. Our approach achieves a maximum increase of +46.22% in Attack Success Rates (ASRs) across GPT-4, GPT-40, GPT-40-mini, and GPT-3.5-Turbo models compared to baseline methods. We demonstrate that this technique poses a challenge to current LLM safety measures and highlights the need for more robust defenses against subtle, multi-turn attacks.", "sections": [{"title": "Introduction", "content": "As Large Language Models (LLMs) become increasingly prevalent in various applications, ensuring their safe and ethical use is paramount. While significant progress has been made in aligning these models with human values and implementing safety measures, they remain vulnerable to adversarial attacks, particularly those that leverage the nuances of human conversation. This paper presents FRACTURED-SORRY-Bench, a framework that extends the SORRY-Bench dataset to evaluate LLM safety against a new class of attacks. Our method focuses on decomposing harmful queries into multiple, seemingly benign sub-questions, exploiting the multi-turn nature of conversations to bypass safety mechanisms. This approach represents a more efficient and accessible technique for generating adversarial samples compared to complex optimization-based methods."}, {"title": "Preliminaries", "content": "Prior work has introduced various datasets and frameworks for evaluating LLM safety, including Do-Not-Answer [2], SimpleSafetyTests [7], ALERT [6], and HarmBench [15], each contributing unique perspectives on assessing LLM vulnerabilities and safeguards. SORRY-Bench [1] introduced a comprehensive benchmark for evaluating LLM safety refusal behaviors. It provides a fine-grained taxonomy of 45 potentially unsafe topics and a balanced dataset of unsafe instructions. Our work builds upon this foundation by introducing a new dimension of evaluation: multi-turn conversational attacks."}, {"title": "Prompt Injection Techniques", "content": "Recent work, such as Imposter.AI [8], has explored sophisticated methods for extracting harmful information from LLMs. These approaches often involve complex, multi-step processes using multiple models or are large scale collections [12]. In contrast, our method focuses on a simpler, more efficient single-shot prompting technique that achieves similar goals. Other relevant works include Universal and Transferable Adversarial Attacks [9] & Rainbow Teaming [4], which demonstrate the potential for creating adversarial prompts that transfer across different LLMs, and the exploration of stealthy attacks using ciphers [10]."}, {"title": "Methodology", "content": "Our approach, FRACTURED-SORRY-Bench, introduces a straightforward method for creating adversarial samples, inspired by the chain-of-thought prompting technique [11]:\n1. Decompose a given query into 4-7 sub-questions that appear innocuous when viewed individually.\n2. Present these sub-questions sequentially to the target LLM in a conversational format.\n3. Analyze the cumulative response to determine if the harmful intent of the original query was fulfilled.\nThis method exploits the LLM's context window and its potential inability to recognize the harmful intent spread across multiple turns. By avoiding explicit harmful language in each sub-question, it aims to bypass content filters and safety measures, similar to the approach in [4]."}, {"title": "Result Analysis", "content": "Our results show significant increases in ASR across all tested models, as presented in Table 1. GPT-3.5-Turbo demonstrated the highest vulnerability, with an ASR that increased by a factor of 10.9\u00d7 compared to its vanilla version, followed by GPT-4 (4.91\u00d7), GPT-40 (4.29\u00d7), and GPT-40-mini (3.9\u00d7).\nWe employed GPT-4 as a judge, inspired by prior literature [15, 16]."}, {"title": "Intent Conveyance Analysis", "content": "We conducted an analysis to determine whether the fractured prompts correctly conveyed the original harmful intent. We present these results in Table 2. Our findings indicate that 49.33% of the fractured prompt sets successfully conveyed the original intent, with variations across different categories of harmful queries. This analysis methodology draws inspiration from work on evaluating LLM safeguards [2, 7]."}, {"title": "Conclusion", "content": "FRACTURED-SORRY-Bench demonstrates the vulnerability of current LLM safety measures to subtle, multi-turn attacks. By decomposing harmful queries into seemingly innocent sub-questions, we achieve significant increases in attack success rates across multiple models. This work highlights the need for more sophisticated safety mechanisms that can understand and evaluate the cumulative intent of multi-turn conversations. Future work should focus on developing defense strategies against these types of attacks and expanding the evaluation to a broader range of LLMs and conversational scenarios."}]}