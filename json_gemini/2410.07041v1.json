{"title": "EMERGENT PROPERTIES WITH REPEATED EXAMPLES", "authors": ["Fran\u00e7ois Charton", "Julia Kempe"], "abstract": "We study the performance of transformers as a function of the number of rep-\netitions of training examples with algorithmically generated datasets. On three\nproblems of mathematics: the greatest common divisor, modular multiplication,\nand matrix eigenvalues, we show that for a fixed number of training steps, models\ntrained on smaller sets of repeated examples outperform models trained on larger\nsets of single-use examples. We also demonstrate that two-set training - repeated\nuse of a small random subset of examples, along normal sampling on the rest of\nthe training set - provides for faster learning and better performance. This high-\nlights that the benefits of repetition can outweigh those of data diversity. These\ndatasets and problems provide a controlled setting to shed light on the still poorly\nunderstood interplay between generalization and memorization in deep learning.", "sections": [{"title": "INTRODUCTION", "content": "When training neural networks, it has become customary to use the largest and most diverse datasets\navailable, and to limit example reuse as much as possible. This tendency is manifest in large lan-\nguage models. GPT (Radford & Narasimhan, 2018) was trained for 100 epochs (each example was\nseen 100 times on average), BERT (Devlin et al., 2019) on 40 and GPT-2 (Radford et al., 2019) on\n20. In recent models, most examples in the pre-training corpus are seen only once, a few specialized\ndatasets are iterated 2 or 3 times, and fine-tuning examples are seen once or twice. Meanwhile, data\nbudgets are on the increase: GPT-2 was trained on less than 10 billion tokens, GPT-3 (Brown et al.,\n2020) was pre-trained on 300 billion, Chinchilla (Hoffmann et al., 2022) and Llama (Touvron et al.,\n2023) on 1.4 trillion, Llama2 (Touvron & et al., 2023) on 2 trillion, and Llama3 (Dubey & et al.,\n2024) on 15.6 trillion. Whereas the use of large train sets is grounded in theory (Vapnik & Kotz,\n2006), the practice of not repeating training examples is less motivated. It reflects the belief that,\nwhen availability permits fresh data is superior to repeated use of a corpus (Komatsuzaki, 2019;\nRaviv et al., 2022; Hernandez et al., 2022; Muennighoff et al., 2023). This belief is grounded in the\nidea that memorization of repeated examples hinders generalization (Zhang et al., 2017). From a\nhuman learner point of view, this is counter-intuitive. When faced with a situation we never experi-\nenced, we recall similar instances (Proust, 1919), and use them as anchors to navigate the unknown.\nIf memorization benefits human learners (Ambridge et al., 2015), why should it hinder machines?\nIn this paper we challenge the view that the repetition of training examples is undesirable, and that\nfor a given training budget (TB, the total number of training examples), one should maximize the\ndata budget (DB, the number of distinct training examples). We explore the impact of repeated\nsamples in three controlled settings using generated data: computing the greatest common divisor\n(GCD) of two integers (Charton, 2024), modular multiplication of two integers, and calculating the\neigenvalues of symmetric real matrices (Charton, 2022). These settings allow for perfect control over\nthe distribution of repeated examples, unlike natural datasets (e.g. text from the web) which may\nfeature unintended duplication and redundancy. Our experiments uncover two striking phenomena:\n1. Repetition Helps: For fixed training budgets (300M to 1B examples), models trained from small\ndata budgets (25 to 50M examples) outperform models trained on large DB. This sometimes\ngives rise to \"emergent\" phenomena: properties only learned by models trained on small DB.\n2. Two-Set Training: For fixed data budgets, learning speed and performance are significantly\nenhanced by randomly selecting a subset of training examples, and repeating them more often\nduring training. The \"two-set effect\" is all the more surprising as the repeated examples are not\ncurated, and only differ from the rest of the training data by their frequency of use."}, {"title": "BACKGROUND AND RELATED WORK", "content": "In this paper, we focus on relatively small transformer models performing mathematical tasks, plac-\ning it into a long established corpus of works that study interesting phenomena in a controlled setting,\nand advance our understanding of the underlying mechanisms in larger models in the wild, see e.g.\nPower et al. (2022); Garg et al. (2022); Charton (2024); Dohmatob et al. (2024).\nOne such example is the study of \"grokking\", first observed with modular arithmetic a phe-\nnomenon where models generalize long after achieving 100% accuracy on their (small) training\nset (Power et al., 2022; Liu et al., 2022b; 2023). On the surface, grokking shares similarities with"}, {"title": "EXPERIMENTAL SETTINGS AND BASELINES", "content": "We focus on three problems of mathematics: computing the greatest common divisor, multiplication\nmodulo 67, and computing the eigenvalues of real symmetric matrices. The GCD and eigenvalues\nwere studied in prior work (Charton, 2022; 2024; Dohmatob et al., 2024; Feng et al., 2024).\nGreatest common divisor. The model is tasked to predict the GCD of two integers uniformly\ndistributed between 1 and 1 million, encoded in base 1000. Following Charton (2024), who observes\nthat throughout training almost all pairs of integers with the same GCD are predicted the same, we\nevaluate model performance by the number of GCD below 100 predicted correctly, measured on a\nrandom test sample of 100,000 pairs: 1000 pairs for each GCD from 1 to 100. Charton reports a\nbest performance of 22 correct GCD for a model trained on uniformly distributed inputs.\nNote. We prefer this test metric over a more standard accuracy on random input pairs, because the\nGCD are distributed according to an inverse square law. In particular the probability that a GCD is\n1 is about 62%. As a result, the accuracy metric results in overly optimistic model performances.\nModular multiplication. Modular arithmetic plays an important role in many public key cryptog-\nraphy algorithms (Diffie & Hellman, 1976; Regev, 2005), and is known to be a hard problem for\nneural networks (Palamas, 2017). Modular addition was studied in several previous works, in the\ncontext of grokking (Power et al., 2022; Liu et al., 2022a) and mechanistic interpretability (Zhong"}, {"title": "REPETITION HELPS", "content": "We now embark on a systematic study of the impact of data budget on performance, for various\ntraining budgets. In other words, we compare the performance of models trained on datasets with a\nfixed number of examples (data budget), for increasing amounts of time (training budget).\nOn the GCD problem, we consider data budgets of 1, 5, 10, 25, 50 and 100M distinct examples, and\nan \"unlimited data\" setting, where new examples are generated on the fly and DB\u2248 TB2. For each\ndata budget, we train 5 models with a training budget of over 1 billion examples, and report their\naverage performance (number of correctly predicted GCD), as the TB increases (Figure 2 Left).\nFor a modest training budget of 30 million, the models with the smallest DB (1 and 5 million, 1M\nand 5M-models henceforth) achieve the best performance (20 GCD vs 13 for all other DB). As\nTB increases, the 1M-models start overfitting, as shown by the increasing test losses in Figure 2\n(Right), and their performance saturates at 21 correct GCD. The performance of the 5M models\nkeeps improving to 36 GCD, for a TB of 150 million examples, then saturates around 38 GCD\nas the models overfit. For TB of 150 and 300 million examples, the best performing models are\nthe 10M. As training proceeds, they are outperformed by the 25M models, which achieve the best\nperformance for TB from 450 million to 1.05 billion examples (with the 50M-model a close second\nat 1 billion). Throughout training, the models trained on small data budgets learn faster. However,\npast a certain TB, they overfit their training data, and their performance saturates.\nNote. Overfitting is an overloaded term. In this paper, we define it by its empirical consequences:\na model overfits when its test loss starts increasing, while the train loss continues to decrease. The\nrelation between learning and overfitting is further studied in Appendix A.\nPower et al. (2022) also study modular division, equivalent to modular multiplication.\n2For GCD and modular multiplication, input pairs are uniformly sampled integers from 1 to 1 million. In\nthe unlimited data case, this gives rise to infrequent repetitions: over ~ 1 billion input pairs, our largest data\nbudget, no elements are repeated 3 or more times, and about 500 thousand are repeated twice."}, {"title": "TWO-SET TRAINING", "content": "The previous experiments demonstrate that for a fixed training budget, the optimal data budget is\nnot the largest possible, as commonly practiced. On all three tasks, training from a set of distinct\nexamples an order of magnitude smaller than the training budget, repeated many times, improves\nperformance. We now turn to a different but related problem: how to best use a given data budget?\nAs we have seen, repeated examples help the model learn. Training from a small subset of the\navailable data should therefore be beneficial, since it would increase repetition. However, models\ntrained from very small datasets will eventually overfit their data, causing their accuracy to saturate.\nYet, this can be prevented by increasing the size of the training set. To address these contradictory\nrequirements a small train set to increase repetition vs a large train set to avoid overfitting \u2013 we\npropose two-set training. We randomly split the training sample into a small set of examples that\nwill be repeated many times during training, and a large set of examples that will be seen a few times\nonly. By doing so, we hope that the small set fosters learning, while the large set prevents overfit.\nSpecifically, for a data budget of N distinct examples, we randomly select S < N examples that\nwill form the repeated set \u2013 in practice, we shuffle the training set, and assign the S first examples to\nthe repeated set. During training, examples are selected from the repeated set with probability p, and\nfrom the N - S others with probability (1 \u2013 p). As a result, a model trained with a training budget\nof T will see pT examples from the repeated set, repeated pT/S times on average, while the N \u2013S\nremaining examples will be repeated (1 \u2013 p)T/(N \u2013 S) times on average. The repetition levels in\nboth samples can be adjusted by choosing the values of S and p. Note that the limiting cases p = 0\nand p = 1 correspond to one-set training, with a data budget of N \u2013 S and S examples respectively.\nOn the GCD problem, models trained on a single set, with a data budget of 100 million examples\nand a training budget of 600 million, predict 27 GCD on average (Figure 2 (Left)). Experimenting\nwith two-set training for different values of S and p, we observe that models trained on a repeated\nset of 250, 000 examples or less, with a probability p of 0.25 or 0.5, predict more than 62 GCD on\naverage, a much better performance than their one-set counterparts. For S = 50,000 and p = 0.25,\nmodels predict 69 GCD on average, a better performance than the best models trained on a single\nset, with a larger training budget of 1 billion examples. For these parameters, the 50k examples in\nthe small set are repeated 3,000 times on average, and the rest of the training examples 4.5 times on\naverage. On a 100M data budget, two-set training clearly outperforms single set training."}, {"title": "ABLATIONS AND VARIATIONS", "content": "In this section, we discuss possible improvements to two-set training. Detailed ablation results can\nbe found in AppendixC.\nCurating the repeated sample. In two-set training, repeated examples are randomly sampled\nfrom the available training data. We now experiment with a possible improvement: selecting the\nrepeated examples. Perhaps what really matters is the repetition of a particular class of \u201cinforma-\ntive\" examples, as in curriculum learning. The GCD problem is particularly well suited for this type\nof investigation. Charton (2024) showed that increasing the proportion of small integers, or over-\nsampling the tails of the distribution of GCD in the training set (Prob(GCD = k) ~ ), greatly\nimproved model performance.\nWe experimented with three curation strategies for the repeated set: log-uniform and uniform distri-\nbutions of operands and input, shown to be beneficial by Charton, \u201ceasy sets\" featuring small input\nand outcomes, and \"heavy tail sets\" featuring large GCD. For each setting, we trained 5 models with\nfour \"good choices\" of S and p (Table 4), a data budget of 100M and training budget of 600M."}, {"title": "BATCHING IN TWO-SET TRAINING: MIXED BATCHES ARE NEEDED", "content": "In all experiments, during training, the model computes gradients over minibatches of 64 examples.\nIn two-set training, minibatches mix examples from the small and large set. We experimented with\nusing \"mono-batches\" that use samples from one set at a time. For instance, when training with\np = 0.25, 25% of minibatches would use examples from the small set (of size S) only, and 75%\nwould only use those from its complement.\nOn the GCD problem, we rerun the most successful two-set experiments (Section 5) with \u201cmono-\nbatches\" for S = 50K, 100K and 250K, and p = 0.25 and 0.5. For training budgets of 600M\nand data budget of 100M examples, the models trained on mixed batches predicted 62 to 69 GCD\n(Section 5). With \"mono-batches\", the number of correctly predicted GCD never rises above 15.\nFor modular multiplication, we experimented with the following (S,p) pairs (S in millions):\n(0.5, 0.1), (2.5, 0.25) and (10,0.5) with data budget 100M and training budget 600M. With these\nsettings, mixed-batch models achieve an average accuracy of 67% or more (Section 5). With \u201cmono-\nbatches\", none of the models manages to learn (accuracy around 4%). This indicates that mixed\nbatching of samples from each of the two sets plays a central role for the two-set effect."}, {"title": "SHIFTING THE SMALL SET", "content": "In these experiments, we study, in two-set training, the possible impact of overfitting on the small\nset, by refreshing the small set with fresh examples periodically. This mimics certain aspects of\ncurriculum learning, where the training set is changed over time. On the GCD experiments, with\na data budget of 100 million, a training budget of 600 million, we shift the small set as training\nproceeds, so that examples in the small set are seen k times on average. At the beginning of training,\nthe small set is the S first elements in the train set. After training on kS/p examples, examples in\nthe small set have been seen k times, and the small set is shifted to elements S + 1 to 2S of the\ntraining set.\nTable 9 provides performances for two-set training with shift, for different values of p, S and k, for a\ndata budget of 100 million, and a training budget of 600 million. It is interesting to note that shifting\nbrings no improvement to 2-set training."}, {"title": "FROM TWO-SET TO MANY-SET TRAINING", "content": "Two-set training with a small randomly selected subset S amounts to assigning different probabilities\nto elements in the training set. For a randomly shuffled training set of size N, two-set training\namounts to selecting the first S elements with probability p/S (with replacement) and the N \u2013 S last\nwith probability (1 \u2013 p)/(N \u2013 S), a step-function distribution over {1, . . ., N }. We now generalize\nthis approach by introducing a probability law P such that P(i) is the probability of selecting the\ni-th example in the training set. Our motivation is to obtain a smooth, possibly more principled,\ndistribution than the step-function induced by the two-set approach. Pragmatically, a one-parameter\nfamily of smooth distributions eliminates the need to tune both S and p. Lastly, we can study\nwhether a smooth decay in frequency might be even more beneficial than a non-continuous two-set\npartition.\nIn this section, we consider a discrete exponential distribution:\nP(i) ~ \u03b2e-\u03b2\u03af/N,"}, {"title": "VARYING THE OPTIMIZER", "content": "Some effects observed in deep learning depend on the optimizer, with grokking being a prominent\nexample (Power et al., 2022). Here we provide experimental evidence to show that our findings hold\nfor a variety of optimizers and are thus robust and universal. We rerun models used for the GCD\nproblem with different optimizers. Specifically, we trained models to predict GCD, with a training\nbudget of 600 million examples, single and two-set training (with |S| = 50,000 and p = 0.25), and\ndata budgets of 25 million, 50 million and unlimited. We considered four optimizer settings:\n\u2022 Adam without dropout or weight decay,\n\u2022 Adam with weight decay 0.01,\n\u2022 Adam with dropout (0.1) in the feed-forward networks of the transformer,\n\u2022 AdamW with weight decay 0.01.\nTable 11 presents the best performance of 5 models for each configuration. On average, dropout has\nan adverse effect on learning, but there is no clear benefit of using weight decay, or AdamW over\nAdam. Importantly, the separation in performance between single-epoch unlimited training, training\non smaller data budgets with more repetitions and two-set training persists across optimizers: the\neffects we present are robust.\nThe normalization factor is (1 \u2013 e-B)-1. In our calculations we will approximate it by 1 to simplify\ncomputing Seff. For the range of \u1e9e we consider, the resulting approximation error is negligible. In general,\nfor fixed p, to compute the size of the set S(p) of first elements that carry probability mass p, we can use\n\u03b2 \u2248 - ln (1-p)N/|S(p)|."}]}