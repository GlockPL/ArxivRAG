{"title": "Building Efficient Lightweight CNN Models", "authors": ["Isong Nathan"], "abstract": "Convolutional Neural Networks (CNNs) are pivotal in image classification tasks due to their robust feature extraction capabilities. However, their high computational and memory requirements pose challenges for deployment in resource-constrained environments. This paper introduces a methodology to construct lightweight CNNs while maintaining competitive accuracy. The approach integrates two stages of training; dual-input-output model and transfer learning with progressive unfreezing. The dual-input-output model train on original and augmented datasets, enhancing robustness. Progressive unfreezing is applied to the unified model to optimize pre-learned features during fine-tuning, enabling faster convergence and improved model accuracy.\nThe methodology was evaluated on three benchmark datasetshandwritten digit MNIST, fashion MNIST, and CIFAR-10. The proposed model achieved a state-of-the-art accuracy of 99% on the handwritten digit MNIST and 89% on fashion MNIST, with only 14,862 parameters and a model size of 0.17 MB. While performance on CIFAR-10 was comparatively lower (65% with less than 20,00 parameters), the results highlight the scalability of this method. The final model demonstrated fast inference times and low latency, making it suitable for real-time applications.\nFuture directions include exploring advanced augmentation techniques, improving architectural scalability for complex datasets, and extending the methodology to tasks beyond classification. This research underscores the potential for creating efficient, scalable, and task-specific CNNs for diverse applications.", "sections": [{"title": "1 Introduction", "content": "Convolutional Neural Networks (CNNs) are a cornerstone of image classification tasks, enabling robust feature extraction from visual data, as demonstrated in various works like Szegedy et al. (2015); Huang et al. (2018). Despite their success, CNNs are often characterized by high computational demands and significant memory requirements, making their training difficult or requiring costly devices and time for training. Moreover, their deployment is equally challenging in resource-constrained environments such as mobile devices and embedded systems due to their sizes and latencies (Liu et al., 2022). This limitation has driven the development of several methods such as pruning (Han et al., 2015), quantization (Jacob et al., 2018), and and filter decomposition (Denton et al., 2014), aiming to reduce CNN models complexity while trying to maintain competitive accuracy.\nThese traditional techniques for reducing model complexity, primarily optimize pre-trained models. While effective, these methods often require additional retraining cycles or"}, {"title": "2 Related Work", "content": "The technique of pruning (Han et al., 2015) removes unnecessary weights and connections from CNNs, thus improving both speed and memory usage. Despite its benefits, pruning is computationally expensive, requiring long iterative pruning and retraining cycles, which makes it time consuming and unreliable for faster development applications. The pruning process may also fail to provide substantial improvements in runtime speed, especially for deeper networks, and it can be challenging to implement directly in complex architectures.\nQuantization (Jacob et al., 2018) offers a way to compress CNNs by reducing the pre-cision of model weights and activations, thus lowering memory and computational require-ments. While effective in reducing the model size, quantization does not address structural inefficiencies and may introduce errors, particularly in models that rely on higher precision computations. The performance of quantized models can be degraded, particularly in tasks requiring high-accuracy predictions.\nThe 1D-FALCON scheme (Maji and Mullins, 2018) proposes a filter decomposition method coupled with the Toom-Cook algorithm to accelerate convolution operations in CNNs. This technique significantly reduces the computational load during convolution, yielding faster inference times. However, the fixed nature of the algorithm may limit its applicability across various architectures and tasks, especially in scenarios where the network structure is highly dynamic or complex.\nGiray et al. (2024) introduced the pyramid training methodology, which reduces net-work complexity by progressively combining features across multiple sub-networks. While the approach achieves up to a 70% reduction in model size, it faces challenges in effectively handling feature map combination, which can add additional computational costs. Addi-tionally, the training of smaller sub-networks introduces overhead, potentially hindering its utility for real-time applications that require fast processing.\nThe Layer-Wise Complexity Reduction Method (LCRM) (Zhang et al., 2023) reduces the computational demands of CNNs by replacing standard convolutions with depthwise separable and pointwise convolutions. This strategy has shown promise in improving effi-ciency for models like AlexNet (Krizhevsky et al., 2012) and VGG-9 (Simonyan and Zisser-man, 2014). However, its application to more complex architectures is limited, and while it offers computational savings, it often results in a reduction in model performance, as the simplifications do not always preserve the discriminative power of the original network. Additionally, the method's reliance on manual layer adjustments limits its flexibility in handling architectures outside of predefined models."}, {"title": "3 Methodology", "content": "The methods describe in this paper consists of two main stages as shown in figure 1: the first being the training of a dual-input-output model created and trained on the original and augmented version of the input dataset, the second involves the use of transfer learning method to fine tune the pre-trained model and and produce the final model with one input and one output."}, {"title": "3.1 Datasets", "content": "Three publicly available datasets were used in this experiment:\n1. MNIST handwriting dataset: The training dataset contained 60,000 images, split into 50,000 for training and 10,000 for validation, while the test dataset consisted of 10,000 images.\n2. fashion MNIST dataset: Same as the handwritten MNIST dataset.\n3. CIFAR-10 dataset: The training dataset contained 50,000 images, split into 40,000 for training and 10,000 for validation, while the test dataset consisted of 10,000 images.\nAll datasets were obtained directly from TensorFlow's tf.keras.datasets library. The datasets were converted to tensorflow.data. Dataset for better data pipeline streaming during training.\nNote that for the two MNIST datasets, having shapes [28x28], a third dimension was added to make them [28, 28, 1], to satisfy CNN input requirements of height, width, channel], while the CIFAR-10 dataset was left unchanged ([32x32x3])."}, {"title": "3.2 Data Preparation and Transformation", "content": "According to the model shown in Figure 3, the first sub-model was given the original data, while the second sub-model received the augmented version of the data using the tf.keras.Sequential pipeline. The dataset was augmented using the following techniques:"}, {"title": "3.3 Model Architecture", "content": "The CNN model architecture, which was implemented using TensorFlow's keras framework, consists of concatenated sub-model layers and a dense layer.\nThe model was initially created to contain two identical models with separate inputs and outputs. The inputs were fed two different sets of data: the first received the original dataset, and the second received the augmented version of the dataset. The concatenated models each had the following structure:\n1. Input Layer: The input shape is implicitly defined as [28, 28, 1] for the MNIST datasets and [32, 32, 3] for the CIFAR-10 dataset, corresponding to grayscale im-ages of size 28x28x1 pixels for MNIST and color images of size 32x32x3 pixels for CIFAR-10.\n2. First Convolutional Block: A 2D convolutional layer (Conv2D) with:"}, {"title": "3.4 Model Compilation and Callbacks", "content": "The model was compiled using the Nadam optimizer, with the evaluation metric set to accuracy and the loss function set to sparse_categorical_crossentropy. All other pa-rameters were left at their default values. The choice of the Nadam optimizer, as supported by Depuru et al. (2019), is due to its ability to improve convergence speed and generalization in Convolutional Neural Networks (CNNs).\nTwo separate ModelCheckpoint callbacks were defined to monitor and save the best model weights: one for the first sub-model based on val_model1_accuracy and one for the second sub-model based on val_model2_accuracy. Both callbacks were set to save only the best-performing weights.\nThe EarlyStopping callback was employed to monitor the model's validation perfor-mance and halt training if no improvement was observed over a set number of epochs which is very effective in preventing model overfitting (Bengio, 2012). The early stopping mechanism was implemented as a functional callback with the following default parameter values:"}, {"title": "3.5 Model Training", "content": "The model training was conducted on Kaggle using the NVIDIA Tesla P100-PCIE-16GB GPU resources and an Intel(R) Xeon(R) CPU @ 2.00GHz (Kaggle, 2025; Yee, 2025). The GPU was utilized for training the models, while the CPU was used for data preprocessing tasks. All processes were consistent across all three datasets and their corresponding models; any modifications are explicitly highlighted in this paper."}, {"title": "3.5.1 FIRST TRAINING STAGE", "content": "In the first training stage, the epoch was set to 20, but the models trained for lesser epochs before stopping. During the training, the performance of the two sub-models were monitored on the validation set. The models performance metrics are shown in Figure 4-6. For each model, as shown in the figure, the epoch at which the highest accuracy was achieved is marked by a dotted vertical line. The corresponding accuracy value is displayed next to the line, indicating the best model saved by the ModelCheckpoint callback. Note that in all the figures, submodel modell receives the original data and submodel, model2, receives the augmented data version."}, {"title": "3.5.2 CREATING THE FINAL MODEL", "content": "To create the final model, we first defined a new input layer. The previously trained model was loaded, and its layers were extracted. The original dense layers from each sub-model were removed and replaced with Conv2D. These convolutional layers were set with a kernel size of (1,1), a stride of (1,1), and 'same\u2019padding, ensuring that the convolution operation would mimic the fully connected behavior of the dense layer while maintaining spatial locality.\nThe number of filters in the Conv2D layers was set to match the number of units (10) in the original dense layers, which allows the convolutional layers to capture similar learned features. Additionally, the weights of the dense layers were used to initialize the correspond-ing convolutional layers. This was done by reshaping the dense layer's weight matrix into the appropriate format for the convolutional kernels, ensuring that the final model retained the learned features from the initial training. This describes the techniques involved in the conversion of a dense layer to a CNN layer as discussed by Long et al. (2015).\nAfter concatenating the outputs of the two convolutional layers, the result was flattened and passed through a dense layer with 32 neurons, 50% dropout rate was added to prevents overfitting (Srivastava et al., 2014), and a ReLU activation function. This dense layer was designed to capture the combined features from both sub-models. Finally, a softmax output layer was added to produce predictions for the 10 classes. It is worth noting that from this stage onward, further trainings made used of the original dataset and not the augmented version. This has no technical reasons than that the aim of the augmentation was to have a form of a submodel that could capture more variability that might not have been in the original dataset. Hence after the first training phase it was never used anymore, of course future work can look into this in details."}, {"title": "3.5.3 SECOND (FINAL) TRAINING STAGE", "content": "The second training stage which is also the final stage consists of the following stages:\n\u2022 Stage 1: In the first stage, the model's layers were frozen except for the last two layers (the newly added layers). The model was compiled using the same parameters as before. Early stopping was applied during training to prevent overfitting, and the model was trained for 20 epochs with early stopping based on validation accuracy. The training metrics are shown in Figure 8- 10.\n\u2022 Stage 2: After completing the first stage, all the layers of the model were unfrozen, and the model was recompiled using the Stochastic Gradient Descent (SGD) optimizer. The optimizer was configured with the following parameters:\nLearning rate: 0.001\nMomentum: 0.9\nWeight decay: $1 \\times 10^{-4}$\nNesterov momentum: True\nThe use of SGD was in order to provide finer control over weight updates and reduce the risk of overfitting as Kumar et al. (2022) highlight; SGD is particularly effective in"}, {"title": "4 Results and Discussion", "content": "The results of these experiments are not directly compared to other models, instead it is demonstrated in different aspects how effective the model is. Seeing that the model is very light-weighted, the focus is in what the model had achieved. Moreover, to the best of my knowledge, even LeNet (LeCun et al., 1998) which was designed specifically for handwritten digit MNIST and was later applied to fashion MNIST had about 60,000 parameters and reached the same accuracy as the model with less than 14,900 parameters proposed in this paper."}, {"title": "4.1 Training Performance", "content": "As stated earlier, a consistent architecture and training process were maintained across all three datasets, with the exception of adjusting the model input shape: [32x32x3] for CIFAR-10 and [28x28x1] for the two MNIST datasets.\nThe model performance for the datasets is summarized in Table 1. Across the datasets, the lightweight model demonstrated strong accuracy, with a consistent trend in both training and test set accuracy.\nFrom table 1, it can be seen that that the model performed much better on the hand-written digit MNIST and the fashion MNIST datasets; The consistent results between these two datasets suggest that the model is well balanced in simplicity and accuracy for both datasets. However, the accuracy dropped much more with the cifar10 dataset, a more com-plex dataset with color images and higher variability, indicating that the model might be too simple for the dataset and could benefit from slight modifications to improve performance as discussed in Future Work.\nThe confusion matrix for the datasets, evaluated on the test sets, are shown in Figure 14 to 16.\nAdditionally, the classification report for each dataset, evaluated on their corresponding trained models, is shown in tables 2 to 4."}, {"title": "4.2 Model Lightweightness", "content": "The primary aim of this paper is to create a lightweight CNN model that maintains very high accuracy fast inference time (Breck et al., 2020). The lightweight nature of the model is demonstrated by measuring its latency, throughput, and size. Notably, the final models were not subjected to any post-training quantization or modifications to reduce their size or improve runtime performance.\nAs mentioned earlier, the training of these models were carried out on the kaggle platform using the NVIDIA Tesla P100-PCIE-16GB GPU resources and an Intel(R) Xeon(R) CPU @ 2.00GHz. Hence the following evaluations would be based on these hardware."}, {"title": "4.2.1 MODEL LATENCY", "content": "Table 5 shows the latency of each model evaluated over a single instance and averaged over 100 repetitions. the standard deviation is also shown for both of the GPU and the CPU."}, {"title": "4.2.3 MODEL SIZE", "content": "Model sizes and parameter counts are shown in table 6"}, {"title": "4.3 Discussion", "content": "The results from the three datasets demonstrate the effectiveness of the proposed lightweight model architecture and training methodology in tackling diverse image classification tasks. Each dataset was used to train the same model architecture independently, employing iden-tical processes, and the outcomes were evaluated separately.\nThe consistent use of the same architecture across all datasets without any structural modifications underscores the flexibility and robustness of the model. For smaller, grayscale datasets such as handwritten digit MNIST and fashion MNIST, the lightweight architecture performed exceptionally well reaching a a the value of 99% and 89% for all of the metrics respectively as shown on table 2 and 3. However, CIFAR-10 had a much lower metrics scores of 65% except fo its F1-Score which has a value of 64% as in table 4."}, {"title": "5 Conclusion", "content": "This paper proposes a unique combination of methods for building lightweight Convolutional Neural Networks (CNNs) that achieve high accuracy while maintaining reduced complexity. The approach leverages two key strategies:"}, {"title": "Future Work", "content": "For the improvement of the results and strategies used in this paper, future work can take care of the following limitations or proposals:\n\u2022 Explore the applicability of this method to various image recognition tasks beyond MNIST, fashion MNIST and the cifar10 dataset.\n\u2022 Investigate the impact of different hyperparameter settings (e.g., number of filters, learning rate) on the performance of the model, but note that the aim is to keep the model simple, hence the initialization should also be relatively simple.\n\u2022 Formulate rules to follow in initializing hyper-parameters such as number of filters, number of convolutional layers, number of dense layers as well as the number of neu-rons, so as to keep the overall process simple during model creation and initialization and not against the essence of the paper which seeks to keep models light-weighted.\n\u2022 Explore a much better data augmentation technique and the unfreezing order that would foster better performance of the model (Shorten and Khoshgoftaar, 2019; Howard and Ruder, 2018).\n\u2022 Analyze a better model stoppage techniques other than the early stopping used in this project."}]}