{"title": "LLM-jp:\nA Cross-organizational Project for the Research and\nDevelopment of Fully Open Japanese LLMs", "authors": [], "abstract": "This paper introduces LLM-jp, a cross-organizational project for the research and\ndevelopment of Japanese large language models (LLMs). LLM-jp aims to develop\nopen-source and strong Japanese LLMs, and as of this writing, more than 1,500\nparticipants from academia and industry are working together for this purpose.\nThis paper presents the background of the establishment of LLM-jp, summaries\nof its activities, and technical reports on the LLMs developed by LLM-jp. For the\nlatest activities, visit https://llm-jp.nii.ac.jp/en/.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), exemplified by GPT-4 [37], demonstrate remarkable capabilities.\nLLMs have achieved many long-standing goals of traditional natural language processing (NLP),\nshifting the primary focus of NLP research towards elucidating their intelligence, ensuring their\nsafety, and exploring their integration and coexistence with humans in society.\nHowever, there exist significant issues with LLMs. First, the research and development of LLMs\nrequire significant computational resources and substantial budgets, predominantly controlled by\na few major organizations. Moreover, the specifics of the strongest models including their\narchitecture, pre-training corpus, training methodologies, and tuning data are no longer publicly\naccessible. Additionally, several critical issues, such as hallucination and safety, must be addressed\nfor LLMs to achieve widespread societal acceptance in the future.\nThere are also national concerns specific to Japan. The representation of Japanese in the GPT-3\ndataset is just 0.11%2, which results in inferior comprehension and generation of Japanese compared\nto English. Furthermore, there is a worry that Japanese culture and activities may be overshadowed if\nmodels predominantly trained in English become the global standard. From an economic security\nperspective, it is crucial to consider the potential outflow of Japan's intellectual assets when entirely\nrelying on foreign models.\nAgainst this background, LLM-jp started in May 2023 with the objective of developing Japanese\nLLMs on our own. The research and development of LLMs is now a big science in terms of both\ncomputational and human resources. Recognizing the need for widespread collaboration, we opted for\ncomplete transparency and decided to make everything openly available, from our models, corpora,\nand fine-tuning data to our discussions and failures, for both non-commercial and commercial use.\nLLM-jp began as a small study group of about 30 NLP researchers. LLM-jp garnered increasing\nsupport for its concept over time, growing to over 1,500 participants by June 2024. Study groups\nhave been held monthly since the establishment of LLM-jp in a hybrid (in-person and online) manner,\nto introduce the latest advances in LLMs and present the activity reports from LLM-jp."}, {"title": "Corpus Building WG", "content": "The main role of the Corpus Building WG is to build a pre-training corpus and a tokenizer needed for\nLLM construction and pass them to the Model Building WG.\nIn the following subsections, we describe our work for the pre-trained models in our model suites\nv1.0 and v2.0. Then, we explain the corpus search function, which is one of our advantages. Finally,\nwe summarize our ongoing and future work."}, {"title": "Work for Pre-trained Model v1.0", "content": "Our initial milestone was to develop the model suite v1.0, and the Corpus Building WG worked on\npreparing a pre-training corpus to train the pre-trained model v1.0, the LLM with 13B parameters\nwithin this suite. The main purpose of this development was to experience the entire development\nprocess of an LLM as soon as possible.\nTo this end, we decided to use a mixture of readily available Japanese, English, and code corpora as\nour pre-training corpus. As for the corpus size, we followed the Chinchilla scaling law [20], which\nsuggests using roughly 20 tokens per parameter. Eventually, we constructed the corpus v1 consisting\nof over 260B tokens. The statistics of this corpus are listed in Table 1. From this corpus, we extracted\na pre-training dataset that consists of 130B Japanese, 130B English, and 10B code tokens, resulting\nin a total of 270B tokens.\nAs for the Japanese portion, we used the Japanese parts of Wikipedia and the multilingual C4\n(mC4) dataset [57]. Since the Japanese part of mC4 was noisy, we filtered out documents that were\nconsidered low-quality or harmful. Table 2 shows filters adopted for this purpose. For the English\nand code portions, we utilized the Pile dataset [16] and the Stack dataset [29], respectively. To adjust\nthe corpus size, we sampled documents from these two sources accordingly.\nWe developed tokenizers based on SentencePiece with the unigram mode [30]. As a multi-lingual\ntokenizer considering Japanese, we first explored the tokenizer developed in the project \u201cDevelopment\nof a distributed parallel learning method for large-scale language models in the policy-oriented\nframework of the supercomputer Fugaku\u201d\u00b3, which we refer to as the tokenizer v1.0. The construction\nprocess is as follows:\n1. Preparing training data to construct the tokenization models for each language (i.e., Japanese\nand English).\n2. In order to prevent the tokenization models from learning tokens longer than Japanese word\nboundaries, Japanese data was pre-tokenized using the morphological analyzer MeCab4\nwith the Japanese morphological dictionary JumanDIC5. This pre-tokenization specifically\naimed to avoid learning tokens such as browser operation phrases, which are frequently\nincluded in web corpus, and meaningless long phrases, which are typically used only on\nspecific websites. Pre-tokenization was also performed for sequences including characters\nother than the alphabet, hiragana, katakana, and kanji into a sequence of single characters to\nprevent the constructed vocabulary from including tokens with a sequence of symbols and\nnumbers.\n3. Constructing SentencePiece models of the unigram tokenizer for Japanese and English using\nthe pre-processed training data, independently.\n4. Merging the vocabularies of the above two tokenization models, removing duplicate tokens.\n5. Re-estimating unigram scores of tokens in the merged vocabulary with the EM algorithm\nover the training data. Here, data without pre-processing was used to enable the final\ntokenization model to be used without any pre-tokenization.\nAlthough the construction process seems complicated, the obtained model can be used as a pure\nSentencePiece model. This multi-step process for the model construction enables us to control the\nratio of the vocabulary size for each language.\nHowever, because the tokenizer v1.0 was originally developed for the Fugaku project, we needed to\nre-train the tokenizer model with the corpus used in the LLM-jp project, the corpus v1. In addition,"}, {"title": "Work for Pre-trained Model v2.0", "content": "To develop the LLM with 13B parameters included in our model suite v2.0, called the pre-trained\nmodel v2.0, we created a larger and higher-quality corpus, termed the corpus v2.\nTo construct a Japanese corpus to this end, we extracted Japanese documents from the entire Common\nCrawl and applied deduplication and filtering for them. The corpus v2 construction script was\ndeveloped in Uzushio\u00b9\u00b9, an Apache Spark-based corpus preprocessing tool developed for processing\nbillion-token scale training corpus from web data such as Common Crawl. Uzushio provides\na framework for processing such as similarity-based duplicate detection and filtering. Table 3\nsummarizes the filters and conversions performed to construct the Japanese portion of the corpus\nv2. The filtering pipeline consisted of deduplication and rule-based filtering steps. In de-duplication,\nUzushio performs similarity-based document identification based on the SimHash algorithm. This\nallows Uzushio to apply multiple strengths of de-duplication to documents from a web corpus. The\nstatistics of the Japanese corpus from Common Crawl dumps are presented in Figure 2. We used the\npublicly available Common Crawl dumps from 2013 to the middle of 2023. We merged the Common\nCrawl dumps from 2013 to 2016 because they included fewer Japanese documents than the later\ndumps. The total extracted Japanese tokens were 285.5B12. Further analyses on the v2 corpus are\ndiscussed in Enomoto et al. [13].\nAs for the English and code portions, we used the Pile and Stack datasets, respectively, following the\ncorpus v1. Besides, we included Japanese and Wikipedia as high-quality text corpora in the corpus\nv2.\nThe corpus v2 has been made publicly available. 13\nAs for the tokenizer, we newly developed the tokenizer v2.2. The training flow of the tokenization\nmodel is the same as that of the tokenizer v2.1. The size of the vocabulary was expanded to 96,86714.\nBesides, while the tokenizer v2.1 used a single token per character for symbols to conserve vocabulary,\nwhich resulted in over-segmentation of English and code text and reduced tokenization efficiency, in\nthe tokenizer v2.2, the vocabulary is constructed in a way that allows for symbol sequences, and the\ntokenization efficiency is improved, especially for English and code text."}, {"title": "Corpus Search", "content": "In addition to corpus construction, the Corpus Building WG is also working on developing a corpus\nsearch function, aiming to attribute generated text to the training corpus. This function will be used\nto analyze generated texts and potentially uncover the principles of LLMs from the perspective of the\ntraining corpus. For example, we plan to use this system to investigate the causes of hallucinations in\ngenerated text.\nCurrently, two search algorithms are being explored: sparse vector search and dense vector search.\nSparse vector search retrieves documents based on the superficial similarity between texts. It is\nparticularly effective when the generated texts contain distinctive words. Additionally, it also helps\nidentify verbatim memorization [6] in generated texts. Dense vector search, on the other hand,\nretrieves documents based on the similarity between text embeddings computed by pre-trained text\nembedding models. Compared to sparse vector search, dense vector search excels at considering the\nmeaning of texts. Furthermore, by using multilingual text embedding models (e.g., LaBSE [14]),\nit can retrieve semantically similar documents across different languages, which helps analyze the\ncross-lingual transfer ability of LLMs [41]."}, {"title": "Ongoing and Future Work", "content": "We decided to build a 175B-class model as the next target of model building in LLM-jp, and are now\nbuilding the corpus v3. This new corpus will consist of approximately 2T tokens that cover Japanese,\nEnglish, some Asian languages, and code.\nIn our corpora, the mixing ratio of Japanese and English is set at 50-50, but we believe that further\nstudy is needed on the mixing ratio and the size of the corpora. In addition to Wikipedia and\nweb documents, we are negotiating with relevant organizations to use high-quality corpora and\ncorpora from various domains, such as scientific and technical papers, patent documents, and domain\ndocuments from the medical field."}, {"title": "Computational Infrastructure WG", "content": "LLM-jp used mdx15 as the computing resource for training LLMs. mdx is a cloud computing\nenvironment consisting of CPUs and GPUs leveraging virtualization technologies [51]. mdx provides\nusers with isolated tenants involving virtual machines, virtual networks, and storage. mdx is operated\nby 11 national organizations in Japan, including nine national universities, the National Institute of\nInformatics, and the National Institute of Advanced Industrial Science and Technology. In May 2023,\nmdx had just started official operation and had GPU resources available; thus, we decided to use mdx\nto build the LLM-jp model.\nA GPU node on mdx has eight NVIDIA A100 40GB SXM model GPUs and two Intel Xeon Platinum\n8369 model CPUs. The network is a full-bisection spine-leaf topology where nodes are connected\nwith four 100 Gbps links. The network supports ROCE (RDMA over Converged Ethernet), an\nEthernet-based RDMA protocol, over Virtual eXtensible LAN (VXLAN) for network virtualization.\nThus, GPUs can use RDMA to communicate with other GPUs. In the LLM-jp configuration, we built\na GPU cluster with 16 nodes (128 GPUs) and allocated all GPUs and two 100 Gbps NICs to each\nvirtual machine.\nWe faced performance issues when we constructed the cluster with 128 GPUs. When we built the\npre-trained model v1, there were packet losses in the GPU data communication because ECMP\n(Equal Cost Multi Path) was not working properly for RoCE packets on the network switch. The\nperformance issue could not be resolved by the start date of the pre-training of the pre-trained model\nv1, so we reduced the scale of the cluster from 16 nodes (128 GPUs) to 12 nodes (96 GPUs). For the\npre-trained model v2.0, we fixed the ECMP issue and used all 16 nodes. Computational Infrastructure\nWG will share the operational expertise on GPU clusters with other projects."}, {"title": "Model Building WG", "content": "The role of the Model Building WG is to pre-train language models. The main tasks include:\n1. preprocessing the pre-training corpus (such as converting it into a binary format for faster\ndata loading during pre-training),\n2. performing the pre-training, and\n3. converting the checkpoints from the pre-training into a model format that is suitable for\nfine-tuning.\nThe following subsections describe how we built the pre-trained models v1.0 and v2.0. Table 4\nsummarizes the configuration for these models."}, {"title": "Work for Pre-trained Model v1.0", "content": "In May 2023, when this project started, the Model Building WG began its activities with the aim\nof building and releasing a 13B-parameter model specifically focusing on Japanese by autumn or"}, {"title": "Work for Pre-trained Model v2.0", "content": "As mentioned in the previous section, the pre-trained model v1.0 was our initial attempt, and we had\na time constraint for its construction and release. This means that our primary focus was on quickly\nbuilding the model on schedule rather than investigating how to obtain a world-class, high-quality\nmodel. To identify a better pre-training configuration for the pre-trained model v2.0, we conducted\nexperiments prior to beginning its construction."}, {"title": "Preliminary Experiments: Towards Better Pre-trained Model v2.0", "content": "We have changed several pre-training configurations of the pre-trained model v1.0 for model v2.0\nsince we aimed to improve the overall performance. Regarding the model architecture, we decided\nto replace GPT-2 used in model v1.0 with the Llama architecture, which was starting to gain wide\nadoption at that time. We conducted experiments to determine the best configuration. The primary"}, {"title": "Constructing Pre-trained Model v2.0", "content": "As demonstrated in the preliminary experiment, Exp(g) appears to deliver the best performance.\nTherefore, we decided to adopt the model trained in Exp(g) as the pre-trained model v2.0. Further-\nmore, with the model trained in Exp (g) being adopted as the pre-trained model v2.0, the training\ndata used in Exp(g) was also finalized as corpus v2."}, {"title": "Ongoing and Future Work", "content": "As described in Section 2.5, we plan to build a 175B-parameter-class model as the next target of\nmodel building in LLM-jp. In practice, we have already tried pre-study using a GPT-3 compliant\nmodel on a trial basis using the LLM construction support program at ABCI22 and have identified\nsome issues to consider, such as loss-spike. We are preparing the implementation to mitigate such\nissues. The Model Building WG is diligently working to build a 175B-parameter-class language\nmodel, trained with a dataset of over 1T tokens (called the corpus v3), publicly available this autumn.\nFor this purpose, we have submitted (and been selected) to an LLM construction support program at\nthe Ministry of Economy, Trade and Industry (METI) in Japan, called GENIAC23."}, {"title": "Fine-tuning and Evaluation WG", "content": "This section introduces our efforts on fine-tuning and evaluation of LLMs. Pre-trained language\nmodels can produce natural and fluent text following input text (prompts), but they do not necessarily\nproduce responses that humans would expect in response to the input. To develop interactive LLMs\nlike ChatGPT, it is essential for them to have the ability to generate appropriate responses to user\ninput; i.e., they need to be aligned with human values [39]. Alignment is an essential issue in LLM\nresearch and development, and fine-tuning is an indispensable step in achieving this.\nEvaluation is another critical issue for the development and deployment of LLMs. A conventional\nmethod for evaluating NLP systems has been to design a specific task, such as question answering\nand machine translation, and to develop test data for each designed task. However, this method is\ninsufficient for the evaluation of LLMs because LLMs are used in a variety of downstream tasks.\nWe therefore develop evaluation frameworks that can assess diverse natural language understanding\ncapabilities of LLMs."}, {"title": "Fine-tuning", "content": "To date, we have released three versions of our fine-tuned models: v1.0, v1.1, and v2.0. The fine-tuned\nmodel v1.0 was released alongside the pre-trained model v1.0. In the fine-tuned model v1.1, which is\nbased on the same pre-trained model v1.0, we improved the instruction-following ability by refining\nthe instruction-tuning data and adding Direct Preference Optimization (DPO), and released it in"}, {"title": "Work for Fine-tuned Model v1.0", "content": "For the fine-tuned model v1.0, we constructed three types of Japanese instruction data: jaster,\ndatabricks-dolly-15k [10], and OpenAssistant Conversations Dataset (oasst1) [32]. Jaster is a dataset\nthat utilizes existing datasets from Japanese natural language processing (NLP) tasks. Through the\naccumulation of research in NLP, training and evaluation data for individual NLP tasks such as natural\nlanguage inference and question answering have been developed and made available. Jaster was\nconstructed by converting these data into a natural language instruction format and corresponding\nresponses. The remaining two instruction datasets are machine-translated from English datasets using\nDeepL24. While many instruction datasets are available in English, we selected databricks-dolly-15k\nand oasst1, as they are widely used and provide suitable licenses for LLM-jp.\nUpon the release of the fine-tuned model v1.0, we developed and released 11m-jp-sft25, an open-\nsource tuning tool designed for supervised fine-tuning. This tool supports not only full-parameter\nfine-tuning but also LoRA [22]-based fine-tuning."}, {"title": "Work for Fine-tuned Model v1.1", "content": "After the release of the fine-tuned model v1.0, we worked on improving the instruction-following\nability and released the model as the fine-tuned model v1.1.\nFirst, we expanded the instruction dataset used. The use of English instruction data in addition to\nnon-English one has been reported to improve model performance in non-English languages [7].\nBased on this finding, we decided to add original English datasets of databricks-dolly-15k and\noasst1. Additionally, we incorporated the Japanese instruction dataset, ichikara-instruction (ver 003-\n001) [47]. This dataset, distinct from machine-translated datasets, consists of high-quality instruction\ndata created from scratch in Japanese by human annotators (the term \u201cichikara\u201d means \u201cfrom scratch\"\nin Japanese).\nNext, we introduced Direct Preference Optimization (DPO) [43], which is designed to generate\nresponses more preferable to the user. DPO has been demonstrated to exhibit performance equal\nto or greater than Proximal Policy Optimization [46], which is the preference optimization method\nemployed in InstructGPT [38], while also offering superior stability and computational efficiency\nduring training. We sampled 12,000 instances from hh-rlhf26 and made them publicly available as\""}, {"title": "Work for Fine-tuned Model v2.0", "content": "Upon the release of the pre-trained model v2.0, we further added instruction data. The Open Assistant\nConversations Dataset Release 2 (oasst2)29 is an English conversational instruction dataset. We\nutilized both the original English version and a Japanese version translated via DeepL. Additionally,\nwe used the new version of ichikara-instruction (004-001). Moreover, a new instruction dataset,\nAnswerCarefully, was introduced for enhanced safety. For more details on AnswerCarefully, refer to\nSection 6.1."}, {"title": "Evaluation Frameworks", "content": "Unlike traditional, task-specific NLP systems, LLMs can generally be used in various applications. It\nis, therefore, challenging to develop a specific benchmark to evaluate the entire capability of LLMs.\nBecause of this problem, many evaluation benchmarks for LLMs have been proposed globally [4, 63].\nHowever, the number of evaluation benchmarks, like JGLUE [31], for Japanese LLMs was limited\nwhen we started developing LLM-jp models.\nWe have been developing an evaluation framework to aim for multifaceted evaluation rather than\ndepending on a single benchmark. A variety of benchmark datasets for conventional NLP tasks for\nJapanese have been proposed to date. We have therefore constructed 1lm-jp-eval30, an open-source\ntool for evaluating Japanese LLMs across these individual tasks. In the same way as constructing\njaster, existing datasets for Japanese NLP tasks are converted into prompt-answer pairs. When\nevaluating LLMs, prompts are input, and the text predicted by the target LLM is matched with the\nanswers to measure evaluation scores. We have continuously updated 11m-jp-eval from its first\nrelease in October 2023, and now the version of llm-jp-eval is 1.3.031. Table 8 shows the list of\nindividual evaluation datasets which 11m-jp-eval supports. Table 10 shows the result of evaluation\nfor LLM-jp models by 11m-jp-eval, and see Table 9 for the model IDs and details for each LLM-jp\nmodel.\nFor the base models without fine-tuning, v1.0-A/B and v2.0-L, we found that v2.0-L achieved the\nhighest score, as we expected. We found that the evaluation score of v2.0-L is higher than that of\nfine-tuned models, v2.0-M/N/O. Because fine-tuning datasets except jaster are made up of non-routine\ntasks that require long answers, compared to many tasks in 1lm-jp-eval requiring relatively short\nanswers. The evaluation scores of v2.0-M/N/O, fine-tuned variants of v2.0, are higher than v1.0-A/B,\nindicating LLM-jp v2.0 models are improved from v1.0.\nFor the fine-tuning method, SFT seems better than LoRA in most cases for LLM-jp models. Jaster is\nthe training split for a part of 11m-jp-eval datasets, and indeed the models fine-tuned with jaster\nshow the best score. Note that we strictly divided jaster and the evaluation datasets in 11m-jp-eval\nto prevent data leaks. However, it is evident that fine-tuning with training splits also works like\nsupervised learning in traditional machine learning tasks. This is the reason why we do not use jatser\nto fine-tune v2.0 models.\nA limitation of llm-jp-eval is in its narrow focus on conventional NLP tasks. As LLMs are\nincreasingly used for a diverse range of applications beyond traditional NLP tasks, evaluating their\nability to respond to miscellaneous user queries is crucial."}, {"title": "Ongoing and Future Work", "content": "An important future research issue is a detailed analysis of fine-tuning and evaluation. For example,\nthere is not much difference between the models with full parameter tuning and LoRA tuning\ndescribed above in the evaluation of Ilm-jp-eval, but a large difference is observed in the Japanese\nVicuna QA benchmark. The current fine-tuning and evaluation frameworks are incomplete and their\ncomprehensive analysis is still untouched. As an environment is being developed in which various\nevaluation and tuning methods can be easily tested, we plan to analyze the effects of instruction\ndatasets and fine-tuning methods, as well as the effectiveness of evaluation methods."}, {"title": "Safety WG", "content": "Safety is a critical aspect of an LLM as it gets exposed to the real world and adopted by the\npublic. Many of the builders of existing LLMs devote considerable efforts in curtailing harmful or\ninappropriate responses by their models [3, 37, 53, 55], because the risks presented by the models\nbecome even more emphasized as the models get larger, more powerful and more convincing in\ngenerating both useful and harmful responses. At this stage, however, it is difficult to address\nharmfulness of a model in any principled manner, and consequently the removal of harmfulness from\na model response largely depends on alignment via fine-tuning, and on the so-called red-teaming\nefforts which try to ensure that model responses are free of harmful content or expression via an\nextensive and focused stress-testing by specialists. Even when these alignment and red-teaming\nefforts are done in English, the resulting models are impressively successful in suppressing obviously\nharmful or inappropriate responses to a large extent even in Japanese. That said, what counts as\nharmful or inappropriate depends on the cultural context; for example, there are cultural biases\nagainst different groups in different societal conditions, different cultural or religious taboos exist,\nand different types of criminal activities are more prevalent in different countries. It is also known that\na foreign language itself can be an attack vector [55], in that models are more vulnerable to malicious\nattacks in languages other than English. We have yet to see if the LLMs trained and aligned mostly\nwith English data are sufficiently safe for public consumption in Japan in these extended aspects.\nGiven the above as background, the Safety WG currently focuses on initial data creation for Japanese\nLLM safety while building a community of researchers working on this issue. Below we describe a\nfew examples of our efforts so far. Longer term, we plan to extend our efforts to investigating LLM\nsafety in the context of model transparency in close collaboration with other WGs."}, {"title": "AnswerCarefully Dataset", "content": "As mentioned above, there existed no dataset in Japanese for LLM safety for aligning the models\nvia fine-tuning (instruction dataset) or even for safety evaluation of the model output in Japanese. It\nwas therefore urgent that we create datasets that serve as the baseline for the future development and\nresearch in LLMs in Japan.\nAnswerCarefully40 (AC) is the dataset we created to address this issue. It is an instruction dataset\nfor use in fine-tuning Japanese LLMs, consisting of those questions that require particular attention\nin answering, in the form of question and reference answer pairs along with harmful category\nclassification labels. The classification taxonomy follows that of the Do-Not-Answer dataset [56]\nas this was the most comprehensive taxonomy of the risks proposed for LLMs that we found,\nconsisting of a three-level taxonomy with 5 risk areas, 12 harm types and 61 specific risks. Unlike\nDo-No-Answer which uses GPT-4 to generate the questions in the dataset, we collected our questions\nmanually for each specific risk via \u201ccrowdsourcing\u201d within LLM-jp as well as from our contracted\nannotators. This ensures that the questions are natural, realistic and free of usage restrictions, unlike\nthose in Do-Not-Answer which tends to be templatic, abstract and unnatural and are bound by the\nusage terms of GPT-4. Another difference from Do-Not-Answer is that our dataset includes reference\nanswers to these questions so as to serve as an alignment data. These reference answers were created\nby skilled annotators following the guidelines of generating safe and appropriate answers reflecting\nthe values of Hiroshima AI process41.\nWe released Version 1.0 of AnswerCarefully dataset in April 2024, containing 945 samples which is\nsplit into 762 as Development and 183 as Test. We used the Development portion of this dataset to\nfine-tune LLM-jp v2.0. The results are shown in Table 14. In this evaluation, model responses on a\nsubset (61 samples) of the test portion of AnswerCarefully Version 1.0 were rated by three annotators\nusing the five point Likert scale safety metric proposed in [55]42:\n\u2022 5- No safety violations and very helpful\n\u2022 4 \u2013 No safety violations and only minor non-safety issues\n\u2022 3- No safety violations but not helpful or other major non-safety issues\n\u2022 2 - Mild or moderate safety violations\n\u2022 1 - Severe safety violation\nIn addition to average (AVG), we report violation rate (the percentage of responses where at least two\nannotators gave a rating of 2 or less) and acceptable response rate (where at least two annotators gave\na rating of 4 or more). These results show that the addition of AnswerCarefully data in fine-tuning\ndoes have a positive impact on reducing the violation rate and increasing the acceptable response\nrate (rows (b) and (c)) over the baseline model that was not fine-tuned with AnswerCarefully (a),\nwithout negatively impacting regular (i.e., not related to safety) datasets (see Tables 11 and 12). At\nthe same time, we also see limitations - the model's violation rate is still 47.5%, even when we\nartificially made the AnswerCarefully dataset larger by duplicating the dev portion of it 16 times ((c)\nin Table 14). Clearly more data and efforts are required toward improving the safety of our models."}, {"title": "LLM-jp Toxicity Dataset", "content": "LLM-jp Toxicity Dataset is the dataset we created to facilitate the detection of toxic content within\nJapanese texts to filter them out from our pre-training corpora43. There was no publicly available\ndataset that can be used for this purpose \u2013 for example, japanese-toxic-dataset 44 contains only\n437 text snippets that are too short, some of them consisting of only a few characters. Although one\nmight consider Perspective API [33], which assigns various toxicity-related scores to a text, as a\nsimple solution for detecting toxic texts, we cannot solely rely on it as it is not feasible to process\na large amount of text within a limited time frame with this API. We therefore opted for creating\nand releasing a dataset that serve for Japanese LLM community ourselves, through the collaborative\neffort of LLM-jp.\nOur dataset comprises 1,867 labeled texts, 767 of which are identified as toxic. The average number\nof characters in each text is 2,567, providing substantial context for evaluating toxicity. We created\nthis dataset by first automatically extracting toxic text candidates from Japanese texts in the Common\nCrawl Corpus and then asking human annotators to give toxicity labels to the extracted texts. For the\nfirst step, toxic text candidate extraction, we trained a fastText [26] classifier that sorts texts into toxic\nor not. The fastText classifier was trained on 15,000+ Japanese texts whose Perspective API toxicity\nscores were greater than 0.3. 1,114 labeled texts in the dataset were extracted by this classifier. The\nremaining 753 labeled texts in the dataset were extracted by directly using Perspective API where\nthe texts with the score of 0.7 or higher were extracted. After toxic text candidates were extracted,\nhuman annotators assigned toxicity labels and related attributes as follows45:\nLabel: defines the text's overall toxicity level. The possible values are:\nToxic: the text is toxic.\nNontoxic: the text is free from toxicity.\nHas_toxic_expression: the text contains potentially toxic expressions but is not toxic\noverall.\nObscene: denotes the presence of explicit sexual expressions and obscene content (yes or no).\nDiscriminatory: indicates the presence of various forms of discriminatory expressions and insults\nto others (yes or no).\nViolent: signifies the presence of violent expressions and threats (yes or no).\nIllegal: reflects the presence of expressions that encourage illegal, quasi-legal, or unethical behavior\n(yes or no).\nPersonal: indicates exposure of personal information or privacy (yes or no).\nCorporate: indicates the disclosure of various confidential information of companies or organiza-\ntions (yes or no).\nOther: identifies other forms of toxicity not covered by the above categories (yes or no).\nTexts labeled as toxic or has_toxic_expression are identified when at least one toxicity category\nattribute is marked as yes. Texts with a nontoxic label have all toxicity category attributes marked\nas no. However, nontoxic texts containing PII (Personally Identifiable Information) such as postal\naddresses, email addresses, and phone numbers will have the personal or corporate attributes marked\nas yes. Table 15 shows the number of Toxic, Nontoxic, and Has_toxic_expression texts. Table 16\nlists the number of texts in each toxicity category.\nWe plan to increase the size of this dataset to make it possible to train accurate toxic text detection\nmodels and release the dataset in the near future."}, {"title": "JBBQ Dataset", "content": "A growing body of work has explored the extent to which models exhibit social biases against diverse\ncategories, such as age and gender [11]. BBQ [40], a multiple-choice question answering dataset, is\none of the English datasets for analyzing social biases in LLMs. Recently, the BBQ dataset has been\nprovided for languages other than English. For example, there have been a Chinese version of BBQ\n(CBBQ, [23]) and a Korean version of BBQ (KoBBQ, [25]). The construction of the Japanese social\nbias QA dataset (JBBQ)46 [60] is one of the results of cross-organizational collaboration at LLM-jp.\nThe original BBQ dataset is created based on human-designed templates and a diverse vocabulary,\nwhich are used to generate a large size of data automatically. JBBQ is constructed semi-automatically\nthrough three steps: (i) machine translation of BBQ, (ii) manual modification, and (iii) manual\nverification. While BBQ covers nine social categories (Age, Disability status, Gender identity,\nNationality, Physical appearance, Race, Religion, Sexual orientation, and Socio-economic status),\nJBBQ covers five of these categories: Age, Disability status, Gender identity, Physical appearance,\nand Sexual orientation. We removed the other four categories because they are greatly affected by the\ndifferences between the American and Japanese culture.\nThe templates for each category include ambiguous contexts about the category, disambiguated\ncontexts, vocabulary, questions that explicitly state a social bias towards a member of the category\nwith respect to the context (negative questions about the category), non-negative questions, answer\nchoices (labels belonging to the category, labels not belonging to the category, and unknown labels),\nand source information to be referenced for template construction. In JBBQ, there are 245 templates\nin five categories (Age: 72, Disability status: 52, Gender identity: 41, Physical appearance: 52,\nSexual orientation: 28). The number of words assigned to each slot of each question template ranges"}, {"title": "Cross-Organizational Collaboration on LLM Safety", "content": "As we worked on dataset collection, it became obvious that LLM risks extend over a wide range\nof topics. We therefore actively engage with researchers in these areas, and invite them to the WG\nactivities via information sharing and co-development of domain- and usage-specific datasets. While\nmany of these efforts are still in early stages, we are already seeing the benefits of the collaboration\nin the ongoing efforts of joint data creation for fine-tuning and evaluating the general-purpose LLMs\nto fit for multiple use cases.\nHealthcare is a domain that we are actively working on through cross-organizational collaboration. A\npilot study on chatbots for genetic counseling reveals that medical advice provided by LLMs requires\nnot only accuracy but also careful communication and ethical considerations [15]. For instance,\nrecommending prenatal diagnosis raises significant ethical concerns; if the diagnosis indicates that the\nbaby will be born with a disease, parents might opt to terminate the pregnancy, resulting in selective\nlife choices. Furthermore, LLM-generated medical advice must adhere to legal regulations. Medical\nLLMs are prohibited from diagnosing symptoms, even when following precise diagnostic protocols,\nbecause medical laws in most countries reserve the authority to diagnose exclusively for certified\nhuman doctors. However, generated medical responses can be valuable in supporting healthcare\nprofessionals in making diagnostic decisions. Community efforts are underway to create safety\nevaluation datasets that consider the quality of medical communication and regulatory requirements,\nin addition to the helpfulness and harmlessness typically covered by existing evaluation frameworks\n(e.g., implemented in Llama [55]). LLM-jp works with these initiatives and co-develops datasets,\nmetrics and methods to ensure the safety of LLMs constrained by medical requirements.\nWe are also working on investigating cultural differences regarding safety through collaborative efforts,\nas the perception of risk is culturally sensitive. JCOMMONSENSEMORALITY [52] is constructed to\ncapture Japanese commonsense morality. This research group is developing a Japanese version of\nETHICS dataset [18] which is originally based on English. Research on potentially dangerous acts\nis conducted by the same group, and their DANSEN dataset [27] containing examples of hazardous\nsituations (labeled by hazard level) described in single Japanese sentences can be used for testing\nLLMs' reactions to danger. We are in the process of adapting these datasets for use in LLM evaluation\nfrom cultural perspectives, and also hope to develop new datasets jointly through collaboration.\nWe also collaborate with researchers on social media studies for the creation of a dataset of mis- and\ndis-information. Previous benchmarks and datasets related to the factuality of LLM responses, such as\nTruthfulQA [35], Big-Bench [4], SelfAware [61] and Do-not-Answer [56], have predominantly been\nconstructed in English. However, the spread of misinformation, disinformation, and malinformation is\noften very local, calling for regionally specific datasets and benchmarks. For Japanese LLM factuality,\nJTruthfulQA [36] is a pioneering effort, yet this dataset focuses more on general non-factual content\nsuch as superstitions and supernatural phenomena than those being circulated in quantity through\nsocial media. Our current dataset creation effort uses X posts and community notes as the data source.\nThis crowdsourcing approach has been shown to help counter incorrect healthcare information in\npopular posts about the COVID-19 vaccine with accurate and reliable responses [1]. Our early\nexperiments also show that this is an effective way of collecting mis- and dis-information circulating\nin Japan, and we plan to release this dataset as part of a future version of AnswerCarefully.\nFinally, an important mission for the Safety WG is to interface with government bodies for LLM\nsafety, such as AI Safety Institute47, in researching and defining the potential risks LLMs pose to\nindividuals and society, and in setting up the process for evaluating them. Such an effort is still in a\nvery early stage, and we expect more details to come in the near future."}, {"title": "Conclusion", "content": "LLM-jp was established recognizing the necessity for a dedicated hub for LLM research and devel-\nopment in Japan. The spirit of LLM-jp resonated with many people, leading to their participation\nand various forms of support (such as donations, provision of tools, and offering computational\nenvironments), which contributed to the expansion of our activities. Participants enjoy the unique op-\nportunities that arise from such a large-scale and well-resourced environment. This venture represents\na rare example of true open innovation in Japan.\nIn recognition of these activities of LLM-jp, the LLM Research and Development Center was\nestablished at the NII in April 2024. Since its establishment, the center has been equipped with\nsubstantial computational resources and staffed by approximately 30 researchers and developers. We\nhope to gather more people and become a hub for LLM research and development in Japan, and also\nto promote international collaboration.\nWe would like to conclude this paper with a proverb that perfectly captures the spirit of LLM-jp's\nactivities: \"If you want to go fast, go alone. If you want to go far, go together.\""}]}