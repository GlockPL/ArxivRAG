{"title": "LLM-jp:\nA Cross-organizational Project for the Research and\nDevelopment of Fully Open Japanese LLMs", "authors": ["LLM-jp"], "abstract": "This paper introduces LLM-jp, a cross-organizational project for the research and\ndevelopment of Japanese large language models (LLMs). LLM-jp aims to develop\nopen-source and strong Japanese LLMs, and as of this writing, more than 1,500\nparticipants from academia and industry are working together for this purpose.\nThis paper presents the background of the establishment of LLM-jp, summaries\nof its activities, and technical reports on the LLMs developed by LLM-jp. For the\nlatest activities, visit https://llm-jp.nii.ac.jp/en/.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs), exemplified by GPT-4 [37], demonstrate remarkable capabilities.\nLLMs have achieved many long-standing goals of traditional natural language processing (NLP),\nshifting the primary focus of NLP research towards elucidating their intelligence, ensuring their\nsafety, and exploring their integration and coexistence with humans in society.\nHowever, there exist significant issues with LLMs. First, the research and development of LLMs\nrequire significant computational resources and substantial budgets, predominantly controlled by\na few major organizations. Moreover, the specifics of the strongest models including their\narchitecture, pre-training corpus, training methodologies, and tuning data are no longer publicly\naccessible. Additionally, several critical issues, such as hallucination and safety, must be addressed\nfor LLMs to achieve widespread societal acceptance in the future.\nThere are also national concerns specific to Japan. The representation of Japanese in the GPT-3\ndataset is just 0.11%2, which results in inferior comprehension and generation of Japanese compared\nto English. Furthermore, there is a worry that Japanese culture and activities may be overshadowed if\nmodels predominantly trained in English become the global standard. From an economic security\nperspective, it is crucial to consider the potential outflow of Japan's intellectual assets when entirely\nrelying on foreign models.\nAgainst this background, LLM-jp started in May 2023 with the objective of developing Japanese\nLLMs on our own. The research and development of LLMs is now a big science in terms of both\ncomputational and human resources. Recognizing the need for widespread collaboration, we opted for\ncomplete transparency and decided to make everything openly available, from our models, corpora,\nand fine-tuning data to our discussions and failures, for both non-commercial and commercial use.\nLLM-jp began as a small study group of about 30 NLP researchers. LLM-jp garnered increasing\nsupport for its concept over time, growing to over 1,500 participants by June 2024. Study groups\nhave been held monthly since the establishment of LLM-jp in a hybrid (in-person and online) manner,\nto introduce the latest advances in LLMs and present the activity reports from LLM-jp."}, {"title": "Corpus Building WG", "content": "The main role of the Corpus Building WG is to build a pre-training corpus and a tokenizer needed for\nLLM construction and pass them to the Model Building WG.\nIn the following subsections, we describe our work for the pre-trained models in our model suites\nv1.0 and v2.0. Then, we explain the corpus search function, which is one of our advantages. Finally,\nwe summarize our ongoing and future work."}, {"title": "Work for Pre-trained Model v1.0", "content": "Our initial milestone was to develop the model suite v1.0, and the Corpus Building WG worked on\npreparing a pre-training corpus to train the pre-trained model v1.0, the LLM with 13B parameters\nwithin this suite. The main purpose of this development was to experience the entire development\nprocess of an LLM as soon as possible.\nTo this end, we decided to use a mixture of readily available Japanese, English, and code corpora as\nour pre-training corpus. As for the corpus size, we followed the Chinchilla scaling law [20], which\nsuggests using roughly 20 tokens per parameter. Eventually, we constructed the corpus v1 consisting\nof over 260B tokens. From this corpus, we extracted\na pre-training dataset that consists of 130B Japanese, 130B English, and 10B code tokens, resulting\nin a total of 270B tokens.\nAs for the Japanese portion, we used the Japanese parts of Wikipedia and the multilingual C4\n(mC4) dataset [57]. Since the Japanese part of mC4 was noisy, we filtered out documents that were\nconsidered low-quality or harmful. For the English"}, {"title": "Work for Pre-trained Model v2.0", "content": "To develop the LLM with 13B parameters included in our model suite v2.0, called the pre-trained\nmodel v2.0, we created a larger and higher-quality corpus, termed the corpus v2.\nTo construct a Japanese corpus to this end, we extracted Japanese documents from the entire Common\nCrawl and applied deduplication and filtering for them. Uzushio\nprovides\na framework for processing such as similarity-based duplicate detection and filtering.\nv2. The filtering pipeline consisted of deduplication and rule-based filtering steps. In de-duplication,\nUzushio performs similarity-based document identification based on the SimHash algorithm. This\nallows Uzushio to apply multiple strengths of de-duplication to documents from a web corpus. The\nstatistics of the Japanese corpus from Common Crawl dumps are presented in Figure 2. We used the\npublicly available Common Crawl dumps from 2013 to the middle of 2023. We merged the Common\nCrawl dumps from 2013 to 2016 because they included fewer Japanese documents than the later\ndumps. Further analyses on the v2 corpus are\ndiscussed in Enomoto et al. [13].\nAs for the English and code portions, we used the Pile and Stack datasets, respectively, following the\ncorpus v1. Besides, we included Japanese and Wikipedia as high-quality text corpora in the corpus\nv2.\nAs for the tokenizer, we newly developed the tokenizer v2.2. The training flow of the tokenization\nmodel is the same as that of the tokenizer v2.1. The size of the vocabulary was expanded to 96,86714.\nBesides, while the tokenizer v2.1 used a single token per character for symbols to conserve vocabulary,\nwhich resulted in over-segmentation of English and code text and reduced tokenization efficiency, in\nthe tokenizer v2.2, the vocabulary is constructed in a way that allows for symbol sequences, and the\ntokenization efficiency is improved, especially for English and code text."}, {"title": "Corpus Search", "content": "In addition to corpus construction, the Corpus Building WG is also working on developing a corpus\nsearch function, aiming to attribute generated text to the training corpus. This function will be used\nto analyze generated texts and potentially uncover the principles of LLMs from the perspective of the\ntraining corpus. For example, we plan to use this system to investigate the causes of hallucinations in\ngenerated text.\nCurrently, two search algorithms are being explored: sparse vector search and dense vector search.\nSparse vector search retrieves documents based on the superficial similarity between texts. It is\nparticularly effective when the generated texts contain distinctive words. Additionally, it also helps\nidentify verbatim memorization [6] in generated texts. Dense vector search, on the other hand,\nretrieves documents based on the similarity between text embeddings computed by pre-trained text\nembedding models. Compared to sparse vector search, dense vector search excels at considering the\nmeaning of texts. Furthermore, by using multilingual text embedding models (e.g., LaBSE [14]),\nit can retrieve semantically similar documents across different languages, which helps analyze the\ncross-lingual transfer ability of LLMs [41]."}, {"title": "Ongoing and Future Work", "content": "We decided to build a 175B-class model as the next target of model building in LLM-jp, and are now\nbuilding the corpus v3. This new corpus will consist of approximately 2T tokens that cover Japanese,\nEnglish, some Asian languages, and code.\nIn our corpora, the mixing ratio of Japanese and English is set at 50-50, but we believe that further\nstudy is needed on the mixing ratio and the size of the corpora. In addition to Wikipedia and\nweb documents, we are negotiating with relevant organizations to use high-quality corpora and\ncorpora from various domains, such as scientific and technical papers, patent documents, and domain\ndocuments from the medical field."}, {"title": "Computational Infrastructure WG", "content": "LLM-jp used mdx15 as the computing resource for training LLMs. mdx is a cloud computing\nenvironment consisting of CPUs and GPUs leveraging virtualization technologies [51]. mdx provides\nusers with isolated tenants involving virtual machines, virtual networks, and storage. mdx is operated\nby 11 national organizations in Japan, including nine national universities, the National Institute of\nInformatics, and the National Institute of Advanced Industrial Science and Technology. In May 2023,\nmdx had just started official operation and had GPU resources available; thus, we decided to use mdx\nto build the LLM-jp model.\nA GPU node on mdx has eight NVIDIA A100 40GB SXM model GPUs and two Intel Xeon Platinum\n8369 model CPUs. The network is a full-bisection spine-leaf topology where nodes are connected\nwith four 100 Gbps links. The network supports ROCE (RDMA over Converged Ethernet), an\nEthernet-based RDMA protocol, over Virtual eXtensible LAN (VXLAN) for network virtualization.\nThus, GPUs can use RDMA to communicate with other GPUs. In the LLM-jp configuration, we built\na GPU cluster with 16 nodes (128 GPUs) and allocated all GPUs and two 100 Gbps NICs to each\nvirtual machine.\nWe faced performance issues when we constructed the cluster with 128 GPUs. When we built the\npre-trained model v1, there were packet losses in the GPU data communication because ECMP\n(Equal Cost Multi Path) was not working properly for RoCE packets on the network switch. The\nperformance issue could not be resolved by the start date of the pre-training of the pre-trained model\nv1, so we reduced the scale of the cluster from 16 nodes (128 GPUs) to 12 nodes (96 GPUs). For the\npre-trained model v2.0, we fixed the ECMP issue and used all 16 nodes. Computational Infrastructure\nWG will share the operational expertise on GPU clusters with other projects."}, {"title": "Model Building WG", "content": "The role of the Model Building WG is to pre-train language models. The main tasks include:\n1. preprocessing the pre-training corpus (such as converting it into a binary format for faster\ndata loading during pre-training),\n2. performing the pre-training, and\n3. converting the checkpoints from the pre-training into a model format that is suitable for\nfine-tuning.\nThe following subsections describe how we built the pre-trained models v1.0 and v2.0."}, {"title": "Work for Pre-trained Model v1.0", "content": "In May 2023, when this project started, the Model Building WG began its activities with the aim\nof building and releasing a 13B-parameter model specifically focusing on Japanese by autumn or"}, {"title": "Preliminary Experiments: Towards Better Pre-trained Model v2.0", "content": "We have changed several pre-training configurations of the pre-trained model v1.0 for model v2.0\nsince we aimed to improve the overall performance. Regarding the model architecture, we decided\nto replace GPT-2 used in model v1.0 with the Llama architecture, which was starting to gain wide\nadoption at that time. We conducted experiments to determine the best configuration. The primary"}, {"title": "Constructing Pre-trained Model v2.0", "content": "As demonstrated in the preliminary experiment, Exp(g) appears to deliver the best performance.\nTherefore, we decided to adopt the model trained in Exp(g) as the pre-trained model v2.0. Further-\nmore, with the model trained in Exp (g) being adopted as the pre-trained model v2.0, the training\ndata used in Exp(g) was also finalized as corpus v2."}, {"title": "Ongoing and Future Work", "content": "As described in Section 2.5, we plan to build a 175B-parameter-class model as the next target of\nmodel building in LLM-jp. In practice, we have already tried pre-study using a GPT-3 compliant\nmodel on a trial basis using the LLM construction support program at ABCI22 and have identified\nsome issues to consider, such as loss-spike. We are preparing the implementation to mitigate such\nissues. The Model Building WG is diligently working to build a 175B-parameter-class language\nmodel, trained with a dataset of over 1T tokens (called the corpus v3), publicly available this autumn.\nFor this purpose, we have submitted (and been selected) to an LLM construction support program at\nthe Ministry of Economy, Trade and Industry (METI) in Japan, called GENIAC23."}, {"title": "Fine-tuning and Evaluation WG", "content": "This section introduces our efforts on fine-tuning and evaluation of LLMs. Pre-trained language\nmodels can produce natural and fluent text following input text (prompts), but they do not necessarily\nproduce responses that humans would expect in response to the input. To develop interactive LLMs\nlike ChatGPT, it is essential for them to have the ability to generate appropriate responses to user\ninput; i.e., they need to be aligned with human values [39]. Alignment is an essential issue in LLM\nresearch and development, and fine-tuning is an indispensable step in achieving this.\nEvaluation is another critical issue for the development and deployment of LLMs. A conventional\nmethod for evaluating NLP systems has been to design a specific task, such as question answering\nand machine translation, and to develop test data for each designed task. However, this method is\ninsufficient for the evaluation of LLMs because LLMs are used in a variety of downstream tasks.\nWe therefore develop evaluation frameworks that can assess diverse natural language understanding\ncapabilities of LLMs."}, {"title": "Fine-tuning", "content": "To date, we have released three versions of our fine-tuned models: v1.0, v1.1, and v2.0. The fine-tuned\nmodel v1.0 was released alongside the pre-trained model v1.0. In the fine-tuned model v1.1, which is\nbased on the same pre-trained model v1.0, we improved the instruction-following ability by refining\nthe instruction-tuning data and adding Direct Preference Optimization (DPO), and released it in"}, {"title": "Work for Fine-tuned Model v1.0", "content": "For the fine-tuned model v1.0, we constructed three types of Japanese instruction data: jaster,\ndatabricks-dolly-15k [10], and OpenAssistant Conversations Dataset (oasst1) [32]. Jaster is a dataset\nthat utilizes existing datasets from Japanese natural language processing (NLP) tasks. Through the\naccumulation of research in NLP, training and evaluation data for individual NLP tasks such as natural\nlanguage inference and question answering have been developed and made available. Jaster was\nconstructed by converting these data into a natural language instruction format and corresponding\nresponses. The remaining two instruction datasets are machine-translated from English datasets using\nDeepL24. While many instruction datasets are available in English, we selected databricks-dolly-15k\nand oasst1, as they are widely used and provide suitable licenses for LLM-jp.\nUpon the release of the fine-tuned model v1.0, we developed and released 11m-jp-sft25, an open-\nsource tuning tool designed for supervised fine-tuning. This tool supports not only full-parameter\nfine-tuning but also LoRA [22]-based fine-tuning."}, {"title": "Work for Fine-tuned Model v1.1", "content": "After the release of the fine-tuned model v1.0, we worked on improving the instruction-following\nability and released the model as the fine-tuned model v1.1.\nFirst, we expanded the instruction dataset used. The use of English instruction data in addition to\nnon-English one has been reported to improve model performance in non-English languages [7].\nBased on this finding, we decided to add original English datasets of databricks-dolly-15k and\noasst1. Additionally, we incorporated the Japanese instruction dataset, ichikara-instruction (ver 003-\n001) [47]. This dataset, distinct from machine-translated datasets, consists of high-quality instruction\ndata created from scratch in Japanese by human annotators (the term \u201cichikara\u201d means \u201cfrom scratch"}]}