{"title": "Explanations that reveal all through the definition of encoding", "authors": ["Aahlad Puli", "Nhi Nguyen", "Rajesh Ranganath"], "abstract": "Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of pre-diction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding ex-planations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a definition of encoding that identifies this extra predictive power via conditional dependence and show that the definition fits existing examples of encoding. This definition im-plies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a \"what you see is what you get\" property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to uncover encoding in LLM-generated explanations for predicting the sentiment in movie reviews.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence can unlock information in data that was previously unknown. In medicine, for example, using AI, researchers have shown that electrocardiograms are predictive of structural heart conditions [1] or new-onset diabetes [2]. Good predictions often lead one to ask what in the input is important for a prediction; this question is a driving factor behind research in interpretability and explainability [3, 4]. One primary direction in interpretability seeks to produce explanations that are subsets of the input that retain the predictability of the label. These types of explanations and interpretations are called feature attributions and have been used to find factors associated with debt defaults [5], to demonstrate that detecting COVID-19 from chest radiographs can rely on non-physiological signals [6], and to discover a new class of antibiotics [7].\nSeveral methods exist for producing feature attributions or explanations. While some methods com-pute functions of model gradients [8] or look at predictability after removing features [9], other methods attribute scores to different inputs by treating them as players in a game [4, 10] or amor-tize their explanations by learning a single model to select subsets for each instance [11]. Choos-ing one from the many feature attribution methods requires an evaluation. There are, however, many approaches to evaluation itself: qualitative ones [12, 13, 14], which are limited to cases where humans have precise knowledge about the inputs relevant to prediction, and quantitative ones [2, 4, 15, 16, 17, 18, 19, 20], which do not require human knowledge.\nIntuitively, a good evaluation method for feature attributions should assign higher scores to expla-nations that select inputs that are more predictive of the label. However, evaluations that score explanations based on the predictability of the label from the explanation face one major challenge:\nencoding. Informally, an encoding explanation is one where the explanation predicts the label be-yond what seems plausible from the values of the inputs themselves. The top left panel of Figure 1 shows an explanation that predicts the label of dog or cat depending on whether the explanation is a pixel on the right half or left half of the image respectively. Many explanation methods fit the description of encoding [20, 21]. Further, given that many evaluations only look at the quality of pre-diction, encoding can go undetected, rendering the evaluations ineffective at picking explanations.\nIn addressing encoding, this work makes the following contributions:\n\u2022 Develops a simple statistical definition of encoding via a conditional dependence property.\n\u2022 Confirms the introduced definition captures all existing ad hoc encoding instances.\n\u2022 Shows that non-encoding explanations are easy to use because they retain all the predictive inputs used to build them, meaning that predictive non-encoding explanations reveal inputs that predict the label to their users, and thus have a \"what you see is what you get\" property.\n\u2022 Formalizes evaluations' sensitivity to encoding as weak detection (optimal scoring explanations are non-encoding) and strong detection (non-encoding explanations score above encoding ones).\n\u2022 Demonstrates that the evaluations ROAR [19] and FRESH [18] do not weakly detect encoding.\n\u2022 Proves that EVAL-X [20] weakly detects encoding, but does not strongly detect encoding.\n\u2022 Develops STRIPE-X and proves that it strongly detects encoding.\n\u2022 Uses STRIPE-X to uncover encoding in LLM-generated explanations for sentiment analysis."}, {"title": "2 Evaluating explanations", "content": "We focus on explanation methods where the goal is to produce subsets of the input that predict the label [22, 23]. Explanation methods of this form, also called feature attributions, saliency methods [4, 8, 24], or just \"explanations,\" include thresholded rankings from Shapley values [25, 26], LIME [24], and REAL-X [20]. With  y  as the label and  x \u2208 Rd  as the inputs, let  q(y, x)  be the joint distribution over them. An explanation method  e  maps the inputs  x  to a binary selection mask  e(x) over the inputs:  e : Rd \u2192 {0,1}d . The explanation  xe(x)  is a pair: the selection  e(x)  and the vector of explanation's values. For example, if  x = [a, b, c]  is three-dimensional and  e(x) = [0,1, 1], then"}, {"title": "3 Formalizing encoding", "content": "Intuitively, encoding is a phenomenon where the information about the label in the explanation  Xe(x) exceeds what is known from the explanation's values. As the input  x  determines the explanation  xe(x) , the quality of predicting the label  y  from the explanation relies on the information about the label transmitted from  x  to  xe(x) . There are two pathways for this transmission; we elaborate below.\nDenoting the values in a subset  v  by  xv , compare the event this subset takes the values  a , i.e.  xv = a  to the event that the explanation's selection is  v  and that the explanation's values are  a , i.e.,  Xe(x) = (v, a) .\n1. Knowing that the explanation is  xe(x) = (v, a)  implies not only that the values in the explana-tion are determined as  xv = a , but also that the selection is determined as  e(x) = v .\n2. In reverse, knowing that the values of a subset of inputs are  xv = a  and knowing the selection  e(x) = v  implies that the explanation are  xe(x) = (v, a) ."}, {"title": "3.1 Encoding explanations in the wild", "content": "Def: Encoding encompasses examples in the existing literature beyond the example in Section 2.1. In that example, the information about  y  lies in the positions in the selection  e(x) , which motivates the name position-based encoding (POSI). This section describes two other informal examples from the literature of encoding explanations, prediction-based encoding (PRED) [21] and marginal encoding (MARG) [20], and explains the intuition behind why they encode. In the appendix, we develop formalizations of these types of encoding and show that these formulations meet Def: Encoding.\nPrediction-based encoding (PRED). To understand how prediction-based encoding occurs, con-sider the task of sentiment analysis from movie reviews. Assume that reviews can either be of type"}, {"title": "4 Detecting encoding in explanations", "content": "This section develops notions of sensitivity to encoding for evaluation methods, and uses the math-ematical definition of encoding developed in the previous section to establish which methods detect encoding and which do not. Hsia et al. [21] suggest that evaluation methods like EVAL-X can be gamed to produce high scores for encoding explanations by optimizing the evaluation. To study this case, we introduce the notion of weak detection. If the optimal score of an evaluation of explanations does not permit encoding, then that evaluation is said to weakly detect encoding:\nDefinition 2 (Weak detection of encoding). An evaluation  a(q, e)  of explanations weakly detects encoding if the optimal explanations  e\u2217 , i.e.  a(q, e\u2217) = maxe a(q, e) , are non-encoding.\nWeak detection provides a recipe for finding non-encoding explanations: find the explanation that achieves the maximum score of a weak detector. However, such a recipe would only work when optimizing without constraints because weak detection does not require non-encoding explanations to have a better score than any encoding explanation. Adding this requirement leads to the definition of strong detection.\nDefinition 3 (Strong detection of encoding). An evaluation  a(q, e)  strongly detects encoding if for any encoding explanation  e  and non-encoding explanation  e' ,  a(q, e') > a(q, e) .\nEvaluations that are not weak detectors cannot be strong detectors because they score some encoding explanation optimally."}, {"title": "4.1 Do existing evaluation methods detect encoding?", "content": "Here, we consider whether several techniques for evaluating explanations: ROAR [19], FRESH [18], and EVAL-X [20] can detect encoding. We analyze these evaluations on the following distribution  q \nx = [a, b, c] \u223c B(0.5)3 ,\n{a w.p. 0.9 else 1-a if c = 1,\ny = b w.p. 0.9 else 1-b if c = 0.\n(3)\nConsider the explanation  Cencode(x) =  = [1,0,0] if c = 1 and  = [0,1,0] otherwise; this encodes because c is used to create the explanation and c predicts the label conditional on a when  E = 1. This is a MARG explanation (see Section 3.1).\nROAR and FRESH do not weakly detect encoding. ROAR and FRESH evaluate explanations by predicting the label from the inputs not selected by the explanation, denoted as x-e(x); while ROAR masks out the explanation, FRESH removes them altogether. Both evaluations say an explanation is optimal if the predictions from the remaining covariates are as random as predicting without any covariates at all. In other words, they check how informative x-e(x) is of y and provide the highest score for  yx_e(x) . This independence holds for  Cencode(x)  in eq. (3):\nProposition 1. For the data generating process (DGP) in eq. (3), ROAR and FRESH assign their respective optimal scores to the encoding explanation eencode(x).\nThe proof is in Appendix B.6. The intuition is that the encoding explanation  Cencode(x)  always selects the input that informs the label given the control flow c; removing the only conditionally informative input means that  x-eencode(x)  has no information about y. In turn, ROAR scores an encoding explanation x_encode(x) optimally, meaning it does not even weakly detect encoding. As FRESH removes the explanation instead of masking, it cannot introduce extra information into the subset of inputs to predict the label; so, FRESH also scores the same encoding explanation optimally. Thus, ROAR and FRESH are not weak detectors of encoding.\nEVAL-X weakly detects encoding but not strongly. EVAL-X [10, 26] is an evaluation method and is sometimes called the surrogate model score. The EVAL-X score with log-probabilities is\nEVAL-X(q, e) := E(v,a)\u223cq(xe(x)) Eq(y | xe(x)=(v,a)) [log q (y | xv = a)] .\n(4)\nThis score measures the expected log-likelihood of the labels given the input values chosen by the explanation method e and is grounded in the sampling distribution q. Log-likelihoods are maximized by matching the true distribution, this leads to EVAL-X's weak detection:\nTheorem 1. If e(x) is EVAL-X optimal, then e(x) is not encoding.\nAppendix A.4 gives a proof. The proof shows that at optimality, the prediction from the values of explanation has to match the prediction from the full inputs. In turn, given the values there is no additional information in x about y, which means the explanation indicator Ev is independent of y; this violates Def: Encoding, which proves the non-encoding nature of EVAL-X-optimal explanations.\nTo test strong detection for EVAL-X, we consider explanations constrained to select one input. Such reductive constraints appear in practice because the goal of producing an explanation is often to aid humans who benefit from reduced complexity. Such constraints prohibit explanations from reaching EVAL-X's optimal score. Compare eencode(x) with a non-encoding constant explanation:\nProposition 2. Let ec(x) =  . Then, for the DGP in eq. (3), EVAL-X(q, eencode) > EVAL-X(q, ec).\nThus, EVAL-X is not a strong detector. The intuition is that the first two coordinates a, b predict the label when selected by eencode, while the control flow feature does not predict the label. EVAL-X not being a strong detector means that optimizing EVAL-X over a reductive set may yield an encoding explanation. In this case, eencode is one of the EVAL-X-optimal reductive explanations (Lemma 6)."}, {"title": "4.2 STRIPE-X: a strong detector of encoding", "content": "Encoding explanations induce the dependence between the label  y  and the identity of the selection  Ev = 1[e(x) = v]  given the values in the explanation  xv  (Def: Encoding). This dependence can be tested for by building on conditional independence tests [27, 28, 29]. Rather than testing, direct quantification of dependence can be useful for when combining with other scores, which can be done using instantaneous conditional mutual information:\n$\\phi_q(e) := E_{(v,a)\\sim q(x_{e(x)})}I (E_v; y | x_v = a) \\text{ (ENCODE-METER)}.\nProposition 3. ENCODE-METER  q(e) = 0 if and only if e is not encoding.\nThe proof is in Appendix A.5. Combining EVAL-X with ENCODE-METER weighed by  a  yields a method we call the strongly information-penalized evaluator (STRIPE-X):\nSTRIPE-Xa (q, e) := EVAL-X(q, e) \u2013 a q(e).\nFor a large enough  a , the added penalty term pushes down the scores of encoding explanations below that of all non-encoding ones, meaning that STRIPE-X is a strong detector of encoding:\nTheorem 2. With finite  H(y | x)  and  H(y) , for any explanation that encodes  e  and any that does not encode  e' , there exists an  a\u2217  such that \u2200a > \u03b1\u2217 STRIPE-Xa (q, e') > STRIPE-Xa (q, e).\nThe proof is in Appendix A.5. The intuition behind the proof is that for a large enough  a , the STRIPE-X scores for any encoding explanations will be dominated by the information term, and thus will become smaller than any non-encoding explanation whose score is lower bounded by the negative marginal entropy,  \u2212Hq(y) . \nEstimating STRIPE-X. The first component of STRIPE-X is EVAL-X. Computing EVAL-X (eq. (4)) requires an estimate of the predictive distribution of the label  y  given  xv ,  q(y | xv)  [20]. Estimation can be done in two ways. The first way makes use of a surrogate model trained to predict the label from different random subsets using masked tokens [9, 20]. The second way to compute EVAL-X (eq. (4)) relies on conditional generative models [30, 31]. Both hyperparameters and a combination of the estimators can be chosen to maximize the average log-likelihood on a held-out validation set across random input subsets.\nTo estimate the second part of STRIPE-X, the ENCODE-METER, first expand the mutual information terms in ENCODE-METER,  q(e) , in terms of expected KL:\n$q(e) = E(v,a)~q(xe(x))Ey~q(y | xv=a)KL [q(Ev | xv = a, y) || q(Ev | xv = a)] .\nThe outer expectation can be estimated using samples from the data and the inner expectation over  y  can be estimated using the EVAL-X model  q(y | xv) . The distributions over  Ev  can be estimated using a classifier of  Ev  that randomly masks the label and masks different subsets of the inputs. Further details and a generative way to estimate STRIPE-X are in Appendix C.1 and Appendix C.3;\nSTRIPE-X in practice. Using STRIPE-X to choose between explanations is straightforward: pick the one with the larger score. However, like other evaluations that use learned models, misestimation can pose a problem. With large  a , non-encoding explanations with misestimated ENCODE-METER will have bad STRIPE-X scores, while with small  a  some encoding explanations can have good scores. Across all experiments, we set  a = 20, which yielded STRIPE-X scores for known encoding explanations worse than known non-encoding explanations."}, {"title": "5 Experiments", "content": "This section consists of two parts. The first part demonstrates the weak and strong detection ca-pabilities of the evaluations ROAR, EVAL-X, and STRIPE-X in a simulated setting and on an image recognition task. To demonstrate these capabilities, we run these evaluations on instantiations of POSI, PRED, and MARG. Additionally, we evaluate an existing method that learns to explain under a reductive constraint, called REAL-X [20]. The second part shows how STRIPE-X enables discov-ering encoding explanations in the wild, without specific knowledge of the DGP or the method that produced the explanation. We employ STRIPE-X to uncover encoding in explanations generated by a large language model (LLM) for predicting sentiments from movie reviews."}, {"title": "5.1 Empirically studying the detection of encoding in a simulated setting", "content": "We construct two examples with binary labels  y : one discrete input  x  and one that is a hybrid of continuous and discrete components. Both use one binary input in  x \u2208 {0,1}5  as a control flow variable and switch the inputs that  y  depends on. In both DGPS,  y  only depends on  x1  if  x3 = 1, and only on  x2  if  x3 = 0; this means that  x4 ,  x5  are purely noise. For both DGPS,  y  is sampled per the following distribution where  x3  determines the subset the  y  depends on\nq(y = 1 | x) = 1[x3 = 1]q(y | X1, X3) + 1 [x3 = 0]q(y | X2, X3).\n(8)\nThus, EVAL-X* is achieved by an explanation of size 2:  e(x) =  +  if  x3 = 1 else  e(x) =  + . See Appendix C.4 for details; the exact DGPs are given in eq. (36) and eq. (37).\nEncoding explanations. Table 2 describes the encoding explanations we consider for this setting. In Appendix C.4, we check that Def: Encoding holds for these explanations in the discrete DGP by estimating the role of the unselected inputs in affecting the explanation and the role of  Ev  in predicting  y  beyond  xv ; a characterization of Def: Encoding to support this check is in Lemma 1;\nROAR fails to weakly detect encoding. To empirically test the analysis about ROAR, we study whether ROAR can weakly detect encoding by comparing ROAR'S score on the all-inputs explanation, which achieves the optimal score under ROAR, to MARG. MARG ignores  x3 , which is required to produce the label  y  in eq. (8). ROAR log-likelihoods for MARG and the all-inputs explanation are approximately -H(y) = \u22120.69 for both DGPs. This result validates that ROAR is not a weak detector because it does not separate the optimal explanation from all encoding explanations.\nEVAL-X is a weak detector of encoding but not a strong detector. EVAL-X log-likelihood scores are given in blue in Figures 4a and 4b. EVAL-X, being a weak detec-tor, scores the encoding constructions (POSI, PRED, and MARG) strictly lower than the log-likelihood of the opti-mal explanation EVAL-X*. However, the EVAL-X score for the MARG explanation is -0.4, which is above the score of -0.6 achieved by a non-encoding explanation  e(x) = 1; thus, EVAL-X is not a strong detector.\nStrong detector STRIPE-X prices out all the encoding explanations. Figures 4a and 4b report STRIPE-X scores for the same set of explanations as above; STRIPE-X scores are shown in red. Strong detector STRIPE-X scores the non-encoding explanations above the negative marginal entropy \u2212Hq(y) = \u22120.69 and scores every encoding construction under that threshold."}, {"title": "5.2 Detecting encoding on images of dogs and cats", "content": "The goal of this section is to study the encoding detection capabilities of ROAR, EVAL-X, and STRIPE-X on real data. We consider an image recognition task like the one in Figure 3 with la-bels and images from the cats_vs_dogs dataset from the Tensorflow package [32]. We break images of size 64 \u00d7 64 into 4 patches each of size 32 \u00d7 32. In left-right then top-down order, let X1, X2, X3, x4 be the upper left, upper right, bottom left, and bottom right patches respectively; X1, X3 capture color, and x2, x4 are the animal images. With annot (image) denoting the annota-"}, {"title": "5.3 Encoding in LLM-generated explanations", "content": "One can detect encoding in any explanation by checking if the STRIPE-X score falls below the negative marginal entropy. Recent work uses LLMS to produce explanations; e.g. [33] prompt an LLM to generate explanations for reasoning tasks which are later used to improve smaller models. If the LLM explanation encodes, the smaller model can falsely ignore the informative inputs the larger model's explanation depends on and yet does not reveal. In this section, we evaluate explanations generated by an LLM, Llama 3, for a sentiment analysis task. We consider reviews that take one of two forms: with ADJ1 and ADJ2 as adjectives, the review is\n\u2022 'My day was  and the movie was .' that is it' or\n\u2022 'My day was  and the movie was . oh wait, reverse the adjectives'.\nThe second sentence in the review acts as a \"control flow\" input and determines whether ADJ1 or ADJ2 describes the sentiment about the movie. We prompt Llama 3 (see Appendix C.8) to predict the sentiment and select a few words from the review that were important for that sentiment; the selected parts form the generated explanation. To discourage encoding, the prompt explicitly instructs the LLM to select all the words that the LLM based the selection on; such an explanation, by Lemma 1, would be non-encoding. On the 5 most common selections e(x) generated by the LLM, we compute the EVAL-X score and the ENCODE-METER  q(e) . The resulting STRIPE-X score is -2.78, falling short of the negative entropy -Hq(y) = \u22120.69, meaning the LLM encodes. We investigated why."}, {"title": "6 Discussion", "content": "When an explanation is encodes, predictions from the explanation become disconnected from pre-dictions from the values in the explanations. Such explanations can select values with little relevance to the label and yet score highly on the many existing predictive evaluations. We develop a simple statistical definition of encoding. Inverting this definition shows that when non-encoding explana-tions predict the label, users know the values of those inputs selected in the explanation predict the label. We then show that existing evaluations are either non-detectors (ROAR[19],FRESH [18]) or only weak detectors (EVAL-X [20]). Motivated by this, we introduce a new strong detector, STRIPE-X. After empirically demonstrating the detection capabilities (or lack thereof) of said evaluations, we use STRIPE-X to discover encoding in LLM-generated explanations.\nMore related work. Other investigations into evaluating explanations focused on label leakage [26, 34] and faithfulness [18, 35, 36, 37, 38]. Label leakage is similar to encoding in that additional information is in the explanation, but focuses on explanations that have access to both the inputs and the observed label; we leave extending Def: Encoding to leakage to the future. Faithfulness, intuitively, asks that the explanation reflect the process of how a label is predicted from the inputs; a formalization does not exist. Jacovi and Goldberg [38] note the need to define faithfulness formally. Encoding explanations are not faithful to the process of making an explanation because predictive inputs outside those selected by the explanation control the explanation.\nLimitations and the future. Using misestimated models in evaluations (like EVAL-X) may lead to mistakes (see Appendix B.8 for an example). The retinal fundus experiment from Jethani et al. [26] is an example where misestimation leads to reductive explanations scoring higher than using the full input. Misestimation can be due to poor uncertainty or due to dependence on shortcut features. One fruitful direction is to use better uncertainty estimates, like conformal inference [39] or calibra-tion [40], or employ robustness methods [41, 42] to ameliorate errors due to misestimation. Another direction is use tricks like REINFORCE-style gradients to construct non-encoding explanations by optimizing STRIPE-X.\nExplanations that output subsets may not always help humans interpret the mechanism of the pre-diction. For example, imagine one wants to understand why a model correctly answers the question \"Who won the ski halfpipe at the X-games 3 years after her debut in 2021?\" with \"Eileen Gu\". A subset explanation may return \"3 years after her debut in 2021\" and \"ski halfpipe\", but that does not help a human interpret how the model predicts. A better interpretation would be to make the model output, \"3 years after 2021 is 2024. Eileen Gu won in 2024, and debuted in 2021.\" Such explana-tions can also encode information about the prediction in the text produced as a rationale [43]. An important direction here would be to extend the definitions of weak and strong detectors of encoding to evaluations of free-text rationales.\nData versus Model Explanations. Even with the formal definitions of explanation methods, there is a question about what is being explained: the data or the model. These two concepts often get blended together in the literature [11, 20]. We clarify this point and abstract the choice away as two different ways to produce the joint distribution  q(y, x) . In data explanation, the distribution under which a feature attribution method seeks to output a subset of inputs that predict the label should be the population distribution of the data [23]. If, instead, the goal is model explanation, the goal should not be to highlight inputs that predict the label well in samples of the data; rather it should be to predict the label well in samples from the model. Formally, a model with parameters  \u03b8  is a conditional distribution,  p\u03b8(y | x) . To target a model explanation, a feature attribution method would aim to output a subset of inputs that predict the label under the distribution F(x)p\u03b8(yx)."}, {"title": "A Theoretical Details", "content": "A.1 Expressing  q(y | xe(x))  in terms of the values and the identity of the explanation\nTo express  q(y | xe(x)) , we use the following equivalence of events from eq. (1)\n{x: Xe(x) = (v,a)} = {x : e(x) = v} \u2229 {x : xv = a}.\n(eq. (1))\nThen, intuitively, conditioning on the event that  xe(x) = (v, a)  gives you the same information as conditioning on the events  e(x) = v  and  xv = a  simultaneously. We make this formal below.\nFor discrete  x , for any  v, a  such that the probability  q(xe(x) = (v, a)) > 0 , define the LHS and RHS of eq. (1) as  By ,  C\u1ef9  respectively. Then, the conditionals  q(y | Xe(x) = (v, a))  and  q(y | Ev = 1, x = a)  exist and can be written as follows:\nq(y | Xe(x) = (v, a)) = q(y | x \u2208 B\u2174)\nq(y | Ev = 1, x = a) = q(y | x \u2208 Cv).\nThese two conditionals are equal because  B = Cv .\nThe same kind of result holds for general random vectors (discrete or continuous)  x  but is a little more involved because  By  may be non-empty while  q(x \u2208 B\u2084) = 0  and the equality of conditional densities/probabilities need to be written via measure theory. Assume the regular conditional proba-bilities  q(y | xe(x))  and  q(y, Ev | xv) ,  q(y | Ev, xv)  and  q(Ev | xv)  are defined almost everywhere in their respective probability measures. Take any  Sv\u2286 {x: e(x) = v}  where  q(xv \u2208 Sv) > 0 and  q(e(x) = v) > 0 . Consider any measurable sets  Y  over  y  and  Bv (Sv) := {(v,a): a \u2208 Sv} over  xe(x) . Now, by definition of regular conditional probability measures, joint probabilities are obtained by taking the expectation of the conditional with respect to marginal distributions over the conditioning set:\nq(y \u2208 Y, xe(x) \u2208 B\u2174(Sv)) = \u222bB(Sv) q(y \u2208 Y Xe(x) = (v,a))q(dxe(x))\n= \u222bSv q(y \u2208 Y Xe(x) = (v,a))q(xe(x) = (v, a))da\n(9)\nand\nq(y \u2208 Y, Ev = 1, xv \u2208 Sv) = \u222bSv q(y \u2208 Y, Ev = 1 | xv = a)q(xv = a)da\n= \u222bSv q(y \u2208 Y | Ev = 1, Xv = a)q(Ev = 1, x = a)da.\n(10)\nDue to eq. (1), the LHS terms of the two equations above are equal and so are the probability measures over the integrating variables in eqs. (9) and (10). Letting  xv  be defined on a Borel sigma algebra, these two integrals eqs. (9) and (10) are equal if and only if for any Borel set  Sv , for almost every  a \u2208 Sv\nq(y \u2208 Y | Xe(x) = (v,a)) = q(y \u2208 Y | Ev = 1, xv = a).\nThat is, in more plain terms, the conditional distributions are equal  q(y | Xe(x) = (v,a))\nq(y Ev = 1, x = a).\nA.2 With non-encoding explanations \"what you see is what you get\"\nDef: Encoding says that an explanation  e(x)  is encoding if there exists an  S  where  q(xe(x) \u2208 S) > 0  such that for every  (v, a) \u2208 S , :\ny Ev Xv = a.\n(11)\nFor a non-encoding explanation, Def: Encoding does not hold. Here, we derive the implications of violating Def: Encoding. Define the set A to contain all  (v, a)  where eq. (11) is violated:\nA = {(v, a): y || Ev | xv = a}.\n(12)"}, {"title": "A.3 Helpful Lemmas and their proofs", "content": "A.3.1 Alternate conditions equivalent to Def: Encoding\nThe dependence in Def: Encoding occurs due to two reasons", "v": "the explanation's values  xv  do not provide enough information to reveal that the explanation should select the inputs denoted by  v . In other words", "encoding": "nLemma 1. Def: Encoding holds for an explanation  e(x)  if and only if there exists a selection  v such that  q(e(x) = v) > 0  and a set  Sv \u2286 {xv : e(x) = v"}, "such that  q(xv \u2208 Sv) > 0  where both of the following conditions hold for almost every  a \u2208 Sv :\nq(Ev = 1 x = a) \u2260 1;\nq(y xv = a, Ev = 1) \u2260 q(y | xv = a, Ev = 0).\nProof. First, Lemma 2 shows the Def: Encoding holds if only if there exists a selection  v such that q(e(x) = v) > 0  and a set  Sv \u2286 {Xv : e(x) = v}  such that  q(xv \u2208 Sv) > 0  where\nVa\u2208 Sv, y Ev xv.\nWe use this alternate definition in what follows.\nGiven a non-measure zero set  S\u1ef9 , by Lemma 3, almost everywhere in  S\u1ef9 , it holds that  q(Ev = 1 xv) > 0 .\nConditional dependence implies Unpredictability and Additional information (the only if part). If  q(Ev = 1 | xv) = 1  almost everywhere (under  q(x) ), then  Ev  is constant given  xv , and therefore independent of any variable given  xy:\nq(Ev = 1 | xv) = 1 \u21d2 yEv | \u03a7v.\nThen, it follows that conditional dependence implies the unpredictability property\nyEv | xv \u21d2 q(Ev = 1 | xv) < 1.\nSecond, with the result from Lemma 3, we have  q(Ev = 1|xv) \u2208 (0,1) . Thus  q(y xv, Ev = 1)  and  q(y xv, Ev = 0)  exist almost every where in  Sv . Then, by definition of conditional dependence, there is additional information about the label in the explanation:\nyEv Xv\nq(y | xv, Ev = 1) \u2260 q(y | xv, Ev = 0).\nThis shows that Def: Encoding implies the additional information property.\nThen, by definition of dependence almost everywhere Sv:\nq(y | xv, Ev = 1) \u2260 q(y | xv, Ev = 0) \u21d2 yEv | xv.\nThus, the unpredictability and the additional information properties imply Def: Encoding.\nLemma 2. Def: Encoding holds for an explanation  e(x)  if and only if there exists a selection"]}