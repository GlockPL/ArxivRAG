{"title": "AIR: Complex Instruction Generation via Automatic Iterative Refinement", "authors": ["Wei Liu", "Yancheng He", "Hui Huang", "Chengwei Hu", "Jiaheng Liu", "Shilong Li", "Wenbo Su", "Bo Zheng"], "abstract": "With the development of large language models, their ability to follow simple instructions has significantly improved. However, adhering to complex instructions remains a major challenge. Current approaches to generating complex instructions are often irrelevant to the current instruction requirements or suffer from limited scalability and diversity. Moreover, methods such as back-translation, while effective for simple instruction generation, fail to leverage the rich knowledge and formatting in human written documents. In this paper, we propose a novel Automatic Iterative Refinement (AIR) framework to generate complex instructions with constraints, which not only better reflects the requirements of real scenarios but also significantly enhances LLMs' ability to follow complex instructions. The AIR framework consists of two stages: 1) Generate an initial instruction from a document; 2) Iteratively refine instructions with LLM-as-judge guidance by comparing the model's output with the document to incorporate valuable constraints. Finally, we construct the AIR-10K dataset with 10K complex instructions and demonstrate that instructions generated with our approach significantly improve the model's ability to follow complex instructions, outperforming existing methods for instruction generation.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Models (LLMs) have shown impressive performance across a wide range of tasks (Zhao et al., 2023; Li et al., 2024a; He et al., 2024b). Driven by vast amounts of data and efficient training, most current LLMs are capable of effectively following user instructions and aligning to a certain extent with human preferences (Ouyang et al., 2022; Li et al., 2024b; Huang et al., 2025). However, despite these successes, they still face significant challenges when it comes to following complex instructions (Jiang et al., 2023; Wen et al., 2024).\nExisting complex instructions datasets primarily originate from two sources: 1) Curated data from open-source datasets or human annotations (Zhou et al., 2024; Zhang et al., 2024), which are resource-intensive and lack scalability, and 2) Transforming simple instructions into complex ones automatically using proprietary LLMs (Xu et al., 2023; Sun et al., 2024). While the automatic transformation improves scalability, the generated constraints are often recombinations of few-shot examples, resulting in limited diversity. Moreover, these constraints may have low relevance with the target output, failing to reflect real-world scenarios.\nRecently, back-translation, which involves translating text from the target side back into the source side, has been proposed to generate scalable and diverse instructions from human-written corpora (Sennrich, 2015; Hoang et al., 2018; Zheng et al., 2024a; Li et al., 2023). However, these methods typically focus on generating simple instructions and have not fully explored the rich knowledge contained in the human corpus.\nIn this paper, we propose an Automatic Iterative Refinement (AIR) framework for generating high-quality complex instructions. Specifically, our approach is based on two key observations. First, human-written documents contain massive human preferences that can be converted to specific constraints, such as formatting conventions in legal documents. Second, human often refine complex instructions iteratively based on feedback from model outputs. As illustrated in Figure 1, simple instructions are progressively adjusted and enriched to better align with human preferences. This iterative process plays a critical role in crafting effective complex instructions.\nTherefore, our AIR framework incorporates document-based knowledge and LLM-as-judge to iteratively construct complex instructions. The framework consists of two key steps: 1) Initial Instruction Generation, where the model generates initial instructions based on the document content; 2) Iterative Instruction Refinement, where instructions are iteratively refined with LLM-as-judge guidance by comparing model outputs with the document, to identify and incorporate valuable constraints. This process enables the framework to generate more challenging instructions that align more closely with real-world scenarios.\nIn summary, our contributions are as follows:\n\u2022 To better align with real-world scenarios, we propose the AIR framework, which iteratively refines complex instructions with LLM-as-judge guidance by comparing with the document.\n\u2022 We introduce a novel instruction dataset, AIR-10K, generated using our framework. Experimental results demonstrate that our fine-tuned model significantly outperforms existing methods on instruction-following benchmarks.\n\u2022 We provide a comprehensive experimental analysis to evaluate the individual components of our framework, validating the contribution of each step to the overall improvement."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Instruction Generation", "content": "Instruction tuning is essential for aligning Large Language Models (LLMs) with user inten-"}, {"title": "2.2 Back Translation", "content": "Back-translation, a process of translating text from the target language back into the source language, is mainly used for data augmentation in tasks like machine translation (Sennrich, 2015; Hoang et al., 2018). Li et al. (2023) first applied this to large-scale instruction generation using unlabeled data, with Suri (Pham et al., 2024) and Kun (Zheng et al., 2024a) extending it to long-form and Chinese instructions, respectively. Nguyen et al. (2024) enhanced this method by adding quality assessment to filter and revise data. Building on this, we further investigated methods to generate high-quality complex instruction dataset using back-translation."}, {"title": "3 Approach", "content": "Our approach mainly consists of two steps: 1) Initial Instruction Generation; 2) Iterative Instruction Refinement, as shown in Figure 2. In this section, we will introduce the two steps in detail."}, {"title": "3.1 Initial Instruction Generation (IIG)", "content": "Document Collection. Traditional instruction generation methods such as Self-Instruct (Wang et al., 2022) often suffer from limited diversity, as the generated instructions are generally recombinations of the provided few-shot examples. Inspired by the work by Li et al. (2023), we generate initial instructions using back translation based on human-written documents.\nTo further enhance the diversity of the generated instructions, we implement a density-based sampling mechanism for documents, as shown in Algorithm 1. Specifically, we convert documents into vector representations based on Sentence-Transformers, and perform sampling to maximize the density of samples in the representation space. In this way, we effectively eliminate redundant documents, enhancing the diversity of instructions. Moreover, this approach ensures that the knowledge introduced during instruction fine-tuning is evenly distributed across various domains. This not only prevents the model from overfitting to a specific domain but also mitigates the risk of catastrophic forgetting of fundamental capabilities.\nMoreover, to further ensure the quality of the document collection, we filter out documents based on the following criteria: 1) Length: Documents with fewer than 50 words or exceeding 2,048 words are removed. 2) Symbol-to-text ratio: Documents where the proportion of symbols exceeds that of textual content are excluded. 3) Redundancy: Documents containing repetitive paragraphs or excessive symbol repetitions are eliminated.\nInstruction Back-translation Based on the sampled documents, we employ the back-translation method to construct initial instructions. Specifically, we utilize a guidance model to predict an instruction which can be accurately answered by (a portion of) the document. This enables the generation of new instructions without relying on few-shot examples or pre-designed rules. Moreover, we can further ensure the diversity of the generated instructions by diversifying the documents.\nHowever, despite being constructed from the document, the instruction do not always align well with the document in two key aspects (Nguyen et al., 2024). First, the document is unstructured and does not follow the AI-assistant format. Second, it may contain content irrelevant to the instruction. Therefore, we introduce an additional refinement step to transform the document into response format and remove irrelevant content.\nTo further ensure the quality of the instructions, we introduce a scoring step to filter out low-quality data. Each instruction is assigned a score on a scale of 1 to 5 by the guidance model, with each point corresponding to a specific aspect. Only instructions with a score greater than (or equal to) 4 are retained for the next step."}, {"title": "3.2 Iterative Instruction Refinement (IIR)", "content": "To enhance a model's ability to follow complex instructions, it is crucial to construct complex instruction data that incorporates multiple constraints. Previous methods typically start with simple instructions and generate complex ones through rewriting or recombination (Xu et al., 2023). However, the constraints generated in this way often do not meet actual needs or lack diversity.\nAn effective sample for complex instruction fine-tuning should adhere to two key principles:\n1. Whether the model's response originally misaligns with constraint before it is added;\n2. Whether the model's response still misaligns with the constraint after it is added.\nThese constraints highlight the model's weaknesses in handling complex instructions and require further improvement. Conversely, if a constraint does not meet these principles, it indicates that the constraint falls within the model's current capabilities and does not require additional learning.\nTherefore, we introduce constraint generation with LLM-as-judge guidance (Zheng et al., 2023), which mimics the human process of iteratively refining prompts to form complex instructions. As shown in Algorithm 2, during the process of iteration, we obtain the constraints that the model fails to satisfy, which require further fine-tuning.\nThroughout this process, as the number of constraints increases, the model's response also improves, making the identification of new constraints more challenging. To uncover constraints that better reflect human preferences, we use the refined document as the reference answer for the judgment process. Human-written documents inherently contain vast amounts of knowledge and formatting conventions that reflect human preferences. Therefore, the derived constraints will also align more closely with human preferences.\nFinally, the constraint set is merged into a new complex instruction. Notice two constraint sets are derived: the first set Cn satisfies Principle 1, while the second set C , which includes an additional checking step, satisfies both Principle 1 and 2.\nWhile we leverage the refined document as the reference for the judgment process, it should not be used as the target for fine-tuning as in Nguyen et al. (2024), as the document is not refined with the constraints presented explicitly. Therefore, we leverage the guidance model to re-generate the response based on the combined instruction."}, {"title": "3.3 Data Statistics of AIR-10K", "content": "With our proposed framework, we constructed a high-quality complex instruction dataset, AIR-10K, based on openly available documents. We present the real-life scenario-specific domain distribution of AIR-10K in Figure 3(a). As can be seen, our dataset encompasses nearly 20 different domains in total, demonstrating a high degree of balance across diverse fields. Furthermore, we present the distribution of constraint types during iteration 1 and 5 in Figure 3(b). It is evident that in iteration 1, Inclusion and Document Structure constraints dominate. However, after four rounds of constraint additions, by iteration 5, the proportions of each constraint type become more uniform.\nWe also analyze the length distributions of both instructions and responses. As shown in Figure 4(a) and 4(b), our instructions are of substantial information for capturing complex tasks."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Set-up", "content": "Data. Following Nguyen et al. (2024), we utilize a subset of Dolma v1.7 (Soldaini et al., 2024) as the document source, which is derived from a collection of web pages and has undergone rigorous quality and content filtering to ensure data quality.\nModels. We apply our method on two models, Llama-3-8B and Qwen2.5-7B, and we apply preliminary supervised fine-tuning for both models. The preliminary fine-tuning process is conducted on two general instruction datas, namely ultrachat-200k (Ding et al., 2023) and tulu-330k (Lambert et al., 2024), respectively. For the guidance model to construct the data, we rely on a larger model with the same group to ensure data quality, namely Qwen-2.5-72B-Instruct for Qwen-2.5-7B, and Llama-3-70B-Instruct for Llama-3-8B. We set the maximum number of iterations to 5.\nEvaluation. We mainly conduct evaluation on two complex instruction-following benchmarks, CFBench and Follow-Bench , where instructions consist of multiple constraints. We also conduct evaluations on a general instruction benchmark of AlpacaEval2 (Dubois et al., 2024). Note that all benchmarks require GPT-4 for judgment, and we use GPT-40-0806 as the evaluator for all of them. We also conduct evaluation on fundamental capability benchmarks, including math, code, and knowledge tasks, and the results are presented in Appendix A due to space limitation.\nBaselines. We mainly compare our method with four groups of methods as follows:\n1. Human crafted instruction data: This includes ShareGPT, which is a collections of real human-AI conversations.\n2. Automatic crafted general instruction data: This includes Self-Instruct (Wang et al., 2022), which leverages few-shot examples to self-generate simple instruction samples.\n3. Automatic rewritten complex instruction data: This includes Evol-Instruct (Xu et al., 2023), ISHEEP (Liang et al., 2024), Muffin (Lou et al., 2023) and Conifer (Sun et al., 2024), which initiate with simple instructions and progressively construct more complex ones through rewriting or recombination.\n4. Automatic back-translated complex instruction data: This includes Suri (Pham et al., 2024) and Crab (Qi et al., 2024), which curate the complex instructions and constraints by back-translating the pre-existing response. These methods are the most closest to our work.\nAdditionally, we also compare with the original back-translation (Cao et al., 2023) and back-and-forth (Nguyen et al., 2024), where IIR is skipped and initial instructions are directly used.\nNote that for all constructed datasets, we sample 10k instruction-response pairs for supervised fine-tuning under the same hyper-parameters."}, {"title": "4.2 Main Results", "content": "As shown in Tables 1 and 2, our proposed method achieves the best performance on both complex and general instruction-following benchmarks, demonstrating its effectiveness. In contrast, automatically crafted general instruction data significantly underperform, highlighting the importance of multiple constraints in effective instruction fine-tuning. Automatic rewritten instructions also underperform, as their constructed constraints do not align with real-world practice. Additionally, automatically back-translated instructions underperform as well. Despite the constraints being derived from documents, the documents (even after refinement) suffer from misalignment and should not be direct used as the target for fine-tuning."}, {"title": "4.3 Data Quality Evaluation", "content": "To evaluate our dataset's quality, we employed the Deita scorer (Liu et al., 2024), which utilizes LLM to assess complexity score for instructions and quality score for both instructions and responses. As shown in Figure 5, our approach significantly outperforms human crafted instructions, automatically crafted general instructions, and automatically rewritten complex instructions in terms of both complexity and quality scores. Notably, our method shows marginal improvements over automatic back-translation approaches like Suri and Crab, despite their use of high-quality seed datasets (e.g., Alpaca GPT4 for Crab) and advanced models (e.g., GPT-4-turbo for Suri). These results validate the effectiveness of our data generation strategy.\nTo investigate the effect of iterative refinement, we analyze the variation of average unique trigrams and token lengths across iterations in Figure 6(a). The results demonstrate consistent increases in both instruction length and unique trigrams, indicating that newly added constraints is diverse rather than mere repetition. Furthermore, Figure 6(b) displays the evolution of complexity and quality scores throughout the iterations, showing steady improvement of data quality as the iterations progress."}, {"title": "4.4 Judgment Strategy for Better Constraint", "content": "In this section, we investigate the optimal judgment strategy for constraint generation. When humans adjust prompts based on the output, they typically have a pre-expected response as the reference in mind, and constraints are issued to guide the response closer to the reference. Therefore, we compare three judgment settings: 1) No judgment, directly curate constraints; 2) Judge without document as the reference. Instead, use the guidance models' response as the reference; 3) Judge with the refined document as the reference.\nAs shown in Table 3, the judgment process is essential for uncovering valuable constraints to improve the complex instruction following ability. LLM-judge can curate constraints that reflects the insufficiency of the model which requires further tuning. Moreover, using document as reference is also essential due to the limited judgment ability of the model, and human-written references aid in more targeted constraint construction.\nOn the other hand, the additional checking step does not improve complex instruction-following ability, as the checking step would result in fewer constraints. However, we observe improved performance on general-instruction following, indicating there exists a trade-off between general and complex instruction following abilities."}, {"title": "4.5 Influnce of Iterative Judge", "content": "In this section, we investigate the effectiveness of iterative judge by examining model performance across different iterations. As shown in Table 4, the iterative judge process demonstrates clear benefits compared to both the baseline and IIG step.\nSpecifically, we observe consistent improvements on FollowBench and AlpacaEval2 through the first two iterations. This suggests that the iterative judging process effectively identifies and incorporates increasingly sophisticated constraints that are valuable for complex instruction following. However, improvements tend to plateau after the third iteration. This could be attributed to the fact that the most critical and fundamental constraints have already been discovered in earlier iterations."}, {"title": "4.6 Influence of Data Quantity", "content": "In this section, we investigate the impact of data quantity on AIR's performance. We present the results of models trained with varying amounts of data in Figure 7. As shown, performance on both general and complex instruction tasks improves with increasing data quantity. On the other hand, the model can achieve superior performance with only 1k training samples, and the performance gains become marginal as more data is added. Therefore, in practical applications, the optimal amount of fine-tuning data can be determined based on available computational resources."}, {"title": "4.7 Influence of Guidance Model Size", "content": "In Table 5, we investigate the impact of guidance model size on AIR's performance. We performed experiments with Qwen-2.5-7B-UltraChat as the base model, while varying the guidance model size from 14B to 72B parameters. As shown, all guidance models significantly improve instruction-following ability compared to the baseline, while larger models generally present more improvement. On the other hand, even the 14B guidance model demonstrates remarkable improvement. This scalability across different model sizes highlights the robustness and efficiency of our approach."}, {"title": "5 Conclusion", "content": "This paper introduces the Automatic Iterative Refinement (AIR) framework, a novel approach for generating complex instructions that better align with real-world scenarios. The framework employs an iterative refinement process guided by LLM-as-judge to generate high-quality complex constraints. We also construct a complex instruction dataset, AIR-10K, to facilitate the research and application of complex instruction following.\nWhile previous methods for complex instruction following often introduce constraints without clear justification, it is crucial to understand what authentic complex instruction entails. In the future, we will conduct further research on the effectiveness and efficiency of complex instruction data."}, {"title": "Limitations", "content": "Our work has several limitations. 1) Although our evaluation includes multiple established bench-"}, {"title": "Ethical Considerations", "content": "Our data construction framework primarily leverages proprietary models such as Llama-3-70B-Instruct, which have undergone extensive preference optimization to minimize the likelihood of generating instructions that raise ethical concerns. However, large-scale web corpora our primary data sources are uncensored and may contain harmful or toxic content. To address this, we recommend implementing more rigorous and meticulous filtering mechanisms to proactively identify and remove such instances if possible.\nWhile the AIR framework mainly aims to enhance models' ability to follow complex instructions, it is important to note that some user constraints may conflict with system constraints set by developers. For example, users may request the generation of harmful or toxic content. Although our study does not specifically investigate conflicting constraints, there is a potential risk that the pipeline could prioritize user requests over developer-defined safeguards."}, {"title": "A Impact on Fundamental Capabilities", "content": "Previous methods have shown LLMs may suffer from capability degradation during alignment (Ouyang et al., 2022). To evaluate this concern, we tested our AIR method on MMLU (Hendrycks et al., 2021), CommonsenseQA (CQA) (Talmor et al., 2019), Natural Questions (NQ) (Kwiatkowski et al., 2019), HumanEval (Chen et al., 2021), and GSM8K (Cobbe et al., 2021). In Table 6, our method does not have a negative impact on fundamental capabilities. For Qwen-2.5-7B-UltraChat and Llama-3-8B-Tulu, our method even improves the average performance by 1.19 and 0.44 points, respectively. This indicates that instruction constructed from documents with evenly sampled distributions also present even distribution, which would not lead to catastrophic forgetting of fundamental capabilities."}, {"title": "B Case Study for Complete Pipeline", "content": "This section presents a detailed end-to-end demonstration of our pipeline in Figure 8. The case study provides a thorough walkthrough of each stage in our instruction generation and refinement process."}, {"title": "C Constraint Type Taxonomy and Distribution Analysis", "content": "This section provides a detailed classification of constraint types, as defined in Table 7. Additionally, we present a comprehensive analysis of constraint type distribution patterns observed across five iterative refinement rounds, as visualized in Figure 9."}, {"title": "D Model Training Hyper-parameters", "content": "This section details our model training configuration based on the LlamaFactory (Zheng et al., 2024b) framework. We employed Supervised Fine-Tuning (SFT) with hype-rparameters as outlined in Table 8."}, {"title": "E Prompts for Initial Instruction Generation", "content": "This section presents the prompts used in our data generation pipeline in Initial Instruction Generation step. These prompts serve different purposes in our methodology, from initial instruction generation through back-translation (Figure 10) to document refining (Figure 11) and instruction scoring (Figure 12)."}, {"title": "F Prompts for Iterative Instruction Refinement", "content": "This section presents the prompts used in our data generation pipeline in Iterative Instruction Refinement step. These prompts serve different purposes in our methodology, from constraint generation (Figure 13), constraint verification (Figure 14), and finally combines all elements into refined instructions (Figure 15)."}, {"title": "G Instruction Score Examples", "content": "This section presents a comprehensive analysis of instruction quality through representative examples. As illustrated in Figure 16, we provide a diverse set of instructions spanning the entire quality spectrum (scores 1-5). Each score category is exemplified by five carefully selected cases, where score 1 represents basic quality and score 5 demonstrates exceptional quality."}]}