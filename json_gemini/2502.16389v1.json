{"title": "An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving", "authors": ["Tianchen Ji", "Neeloy Chakraborty", "Andre Schreiber", "Katherine Driggs-Campbell"], "abstract": "As automated vehicles enter public roads, safety in a near-infinite number of driving scenarios becomes one of the major concerns for the widespread adoption of fully autonomous driving. The ability to detect anomalous situations outside of the operational design domain is a key component in self-driving cars, enabling us to mitigate the impact of abnormal ego behaviors and to realize trustworthy driving systems. On-road anomaly detection in egocentric videos remains a challenging problem due to the difficulties introduced by complex and interactive scenarios. We conduct a holistic analysis of common on-road anomaly patterns, from which we propose three unsupervised anomaly detection experts: a scene expert that focuses on frame-level appearances to detect abnormal scenes and unexpected scene motions; an interaction expert that models normal relative motions between two road participants and raises alarms whenever anomalous interactions emerge; and a behavior expert which monitors abnormal behaviors of individual objects by future trajectory prediction. To combine the strengths of all the modules, we propose an expert ensemble (Xen) using a Kalman filter, in which the final anomaly score is absorbed as one of the states and the observations are generated by the experts. Our experiments employ a novel evaluation protocol for realistic model performance, demonstrate superior anomaly detection performance than previous methods, and show that our framework has potential in classifying anomaly types using unsupervised learning on a large-scale on-road anomaly dataset.", "sections": [{"title": "1 Introduction", "content": "Autonomous driving is at a critical stage in revolutionizing transportation systems and reshaping societal norms. More than 1,400 self-driving cars, trucks, and other vehicles are currently in operation or testing in the U.S. (Etherington 2019), and 4.5 million autonomous vehicles are expected to run on U.S. roads by 2030 (Meyer 2023). While autonomous driving is promising in improving traffic efficiency and personal mobility, safety is a prerequisite of all possible achievements and is becoming the first priority in practice (Du et al. 2020). In October 2023, Cruise, one of the leading autonomous driving companies, was ordered by California to stop operations of driverless cars in the state after one of Cruise's cars struck a pedestrian in San Francisco (Kerr 2023). The rare incident involved a woman who was first hit by a human driver and then thrown onto the road in front of a Cruise vehicle. The Cruise vehicle then rolled over the pedestrian and finally stopped on top of her, causing serious injuries. Such an accident reflects one of the greatest challenges in autonomous driving: the safety of an autonomous car is largely determined by the ability to detect and react to rare scenarios rather than common normal situations, which have been well considered during development. Although rare in a long-tailed distribution, unusual driving scenarios do happen and can have large impact on driving safety.\nTo mitigate the impact of abnormal ego behaviors when outside the design domains, a detection system for anomalous driving scenarios is necessary, the output of which can be potentially used as a high-level decision for motion planning. Recently, deep-learning based anomaly detection (AD) algorithms have been widely adopted in robotic applications where rare events are closely"}, {"title": "2 Related work", "content": "Anomaly detection, also known as outlier detection or novelty detection, is an important problem that has been studied within diverse research areas and application domains (Chandola et al. 2009; Chalapathy and Chawla 2019). The problem of traffic AD bares similarities with the disciplines of robot AD and AD for surveillance cameras. In this section, we briefly review the related research and introduce common techniques in ensemble deep learning.\nRecent research efforts have made noteworthy progress in developing learning-based AD algorithms for robots and mechanical systems. Malhotra et al. (2016) introduces an LSTM-based encoder-decoder scheme for multi-sensor AD (EncDec-AD) that learns to reconstruct normal data and uses reconstruction error to detect anomalies. Park et al. (2018) proposes an LSTM-based variational autoencoder (VAE) that fuses sensory signals and reconstructs their expected distribution. The detector then reports an anomaly when a reconstruction-based anomaly score is higher than a state-based threshold. Feng et al. (2023) attacks multimodal AD with missing sources at any modality. A group of autoencoders (AEs) first restore missing sources to construct complete modalities, and then a skip-connected AE reconstructs the complete signal. Although similar in ideas, these approaches were proposed for low-dimensional signals (e.g., accelerations and pressures) and have not shown effective on high-dimensional data (e.g., images).\nAD for robot navigation often involves complex perception signals from cameras and LiDARs in order to understand the environment. Ji et al. (2021) proposes a supervised VAE (SVAE) model, which utilizes the representational power of VAE for supervised learning tasks, to identify anomalous patterns in 2D LiDAR point clouds during robot navigation. The predictive model proposed in LaND (Kahn et al. 2021) takes as input an image and a sequence of future control actions to predict probabilities of collision for each time step within the prediction horizon. Schreiber et al. (2023) further enhances the robot perception capability with the fusion of RGB images and LiDAR point clouds using an attention-based recurrent neural network, achieving improved AD performance on field robots. Different from these supervised-learning-based methods, Wellhausen et al. (2020) uses normalizing flow models to learn distributions of normal samples of multimodal images, in order to realize safe robot navigation in novel environments. However, driving scenarios have additional complexities than field environments. While road environments are more structured than field environments, additional hazards arise from the presence of and interactions between dynamic road participants, which pose extra challenges on AD algorithms.\nAnother widely explored research area that is relevant to our work is AD for surveillance cameras, which mainly focuses on detecting the start and end time of anomalous events within a video. Under the category of frame-level methods, Hasan et al. (2016) proposes a convolutional autoencoder to detect anomalous events by reconstructing stacked images. Chong and Tay (2017) and Luo et al. (2017b) extend such an idea by learning spatial features and the temporal evolution of the spatial features separately using convolution layers and ConvLSTM layers (Shi et al. 2015), respectively. Instead of reconstructing frames, Liu et al. (2018) trains a fully convolutional network to predict future frames based on past observations and uses the Peak Signal to Noise Ratio of the predicted frame as the anomaly score. Gong et al. (2019) develops an autoencoder with a memory module, called memory-augmented autoencoder, to limit the generalization capability of the network on reconstructing anomalies. To focus more on small anomalous regions, patch-level methods generate the anomaly score of a frame as the max pooling of patch errors in the image rather than the averaged pixel error used in frame-level methods (Wang et al. 2023). In addition, object-level approaches have also been explored, which often focus on modeling normal object motions either through extracted features (e.g., human skeletons) (Morais et al. 2019) or raw pixel values within bounding boxes (Liu et al."}, {"title": "3 Problem overview", "content": "Our goal is to develop an AD module that enables an autonomous car to detect anomalous events online in diverse driving scenarios.\nTo guide our anomaly detector design, an analysis of common anomaly patterns in driving scenarios is presented in Figure 2a. In most of the anomalous events on road, there exist three main players, namely the ego car, other road participants (e.g., cars, motorcycles, pedestrians), and the environment (e.g., guardrails, traffic signs, slippery roads), which are represented as the graph nodes. Any two of the nodes can produce a type of anomaly: the edge between the ego car and other road participants includes abnormal cases such as a collision between the ego car and a pedestrian; the edge between the ego car and the environment encompasses accidents such as the ego car being out of control due to slippery roads after raining; the edge connecting other road participants and the environment stands for non-ego involved individual anomalies where a single agent behaves abnormally, such as a car colliding with a guardrail. In particular, there is a self-loop around the node of other road participants, which represents non-ego involved interactive anomalies where anomalous interactions happen between other agents, such as two vehicles colliding with each other. We further summarize the two edges with one of the ends being the ego car as ego involved anomalies.\nNote that the presented graph is general enough to encompass rare moments with more than one anomaly present by activating multiple edges simultaneously. For example, in an event where two cars collide with each other and another car swerves due to loss of control, both non-ego involved interactive anomalies and non-ego involved individual anomalies edge will be activated. Moreover, two non-ego involved individual anomalies edges will be activated if two non-ego cars lose control independently in the same time. The AD problem can now be converted into activating relevant graph edges properly and promptly in abnormal events. Note that our AD problem formulation is not limited to automated cars and can be generalized to other robotic applications with proper modifications.\nWe approach the AD task in autonomous driving using unsupervised learning to avoid the difficulty and cost of collecting large-scale labeled anomaly data for training. Different normal patterns in non-anomalous driving videos have been modeled in prior works to detect out-of-distribution samples during test time for AD. However,"}, {"title": "4 Anomaly detection experts in autonomous driving", "content": "In this section, we present the model architecture, training objective, and anomaly score generation of each expert. Specifically, scene, interaction, and behavior experts are introduced for ego involved, non-ego involved interactive, and non-ego involved individual anomalies, respectively, as identified in Section 3. Targeting online AD, our overall design philosophy is to keep each module parallelizable to one another and easy to implement. All the experts are trained independently with a large-scale dataset of normal driving videos without any anomalies."}, {"title": "4.1 Scene expert", "content": "Some of the on-road anomalies are often accompanied with large unexpected scene changes in egocentric videos, especially for ego involved anomalies. In events where the ego dynamics deviate significantly from the nominal one (e.g., when the ego car swerves, rotates suddenly due to a side collision, is rear-ended), future frames become unpredictable from the perspective of the ego camera, leading to a large prediction error in video frames. However, false negatives can still happen if we only rely on detecting large inconsistency across frames. For example, when the ego car slips forward slowly and hits another car in the back, future frames can still be predicted accurately due to the slow motion of the car, making the system refuse to raise an alert. A useful hint for anomaly detection in such a case is the rarity of the scene itself: the preceding car would never be this close to the ego camera (spatial features) and keep getting closer (temporal features) in normal scenarios.\nIn order to detect ego involved anomalies in both cases of significant dynamics change and slow motions, we assemble the frame-level scene expert with two submodules: future frame prediction (FFP) and spatial-temporal reconstruction (STR).\nFFP aims to detect anomalies by monitoring large differences between the predicted future frame and the actual future frame. The preceding $T_{ffp}$ frames are used for prediction.\nIt is known that optical flow information plays an important role in predicting future images: given the image $I_t$ and the optical flow $f_t$ between $I_t$ and $I_{t+1}$, the future image $I_{t+1}$ can be predicted accurately by shifting pixels (Fang et al. 2022). However, $f_t$ is unknown at time t. A common solution is to utilize the optical flow information retrospectively during training: the predicted image $\\hat{I}_{t+1}$ is first generated, then the difference between the induced optical flow and the ground truth optical flow at time t is incorporated into the loss function for optimization (Liu et al. 2018; Ye et al. 2019). Alternatively, we explicitly predict the scene motion $f_t$, which will also be utilized in STR, before frame prediction. The preceding $T_{ifp}$ RGB images $I_{t-T_{ifp}+1:t} := (I_{t-T_{ifp}+1}, I_{t-T_{ifp}+2},\u00b7\u00b7\u00b7, I_t)$ are first processed by pretrained RAFT (Teed and Deng 2020), an optical flow estimation network, to generate scene motions $f_{t-T_{ifp}+1:t\u22121} := (f_{t-T_{ifp}+1}, f_{t-T_{ifp}+1},\u2026\u2026, f_{t-1})$ in the past $T_{ifp}-1$ frames. We perform RAFT inference at a relatively high resolution of 384 \u00d7 672 to ensure the quality of the estimated optical flow, which is then downsampled to 256 \u00d7 256 for fast training\u00b9. The resulting optical flows $f_{t-T_{ifp}+1:t-1}$ in pixel units are then concatenated temporally and fed as the input to subsequent layers of OFP, which are based on u-net (Ronneberger et al. 2015), to predict"}, {"title": "4.1.2 Spatial temporal reconstruction", "content": "Reconstructing sensor signals is another pervasive approach in unsupervised AD as unseen scenarios can be directly monitored and detected. Frame reconstruction can complement frame prediction in cases where the anomaly is not accompanied with large inconsistency across frames, such as a slight rear-end collision due to slipping or scratches with guardrails due to gradual deviation from the lane center.\nOne of the key design choices in scene reconstruction is the input space. A natural idea is to represent a scene in RGB or grayscale values (Luo et al. 2017b; Hasan et al. 2016). However, in complex driving scenarios, the intra-class variance of samples using such a representation is high, thus leading to frequent false alarms. For example, if there is a colorful car for some special events driving in front of the ego car, the scene is likely to be categorized as an anomaly as such a car painting has never been observed in normal training data, although the scene should belong to normal cases as all the driving-related events appear normal. As a result, we express driving scenes in depth and motion space, delivering spatial and temporal features of a scene, respectively. Such an input space can selectively ignore novel appearances of normal objects and focus more on cues that indeed lead to anomalous events.\nSTR is designed to detect anomalies by observing the reconstruction error of the spatiotemporal features of a scene. Our network structure based on Conv-AE (Hasan et al. 2016) is shown in Figure 5. Although the scene motion $f_t$ is unknown at time t, an estimation $f_t$ can be predicted from $I_{t-T_{ifp}+1:t}$ by OFP. We further generate a disparity map $d_t$ (i.e., inverse depth map up to scale) of the scene $I_t$ using pretrained MiDaS (Ranftl et al. 2022). Due to the scale ambiguity in monocular depth estimation, a min-max normalization on disparity values within an image is often performed (Ranftl et al. 2022; Godard et al. 2019, 2017). However, such a relative depth tends to eliminate informative distinctions between frames. For example, consider a scene with only a car close to the ego car and another scene with only a distant car. The disparity values of these two cars will be identical after normalization even though the metric depths are different. Therefore, we remove the min-max normalization to preserve global distinctions and assume that the estimated disparity and the actual metric depth are correlated. Disparity values are then clipped and divided by an upper bound globally on the entire dataset for a normalization to [0, 1].\nThe concatenation of the predicted optical flow and the disparity map is then processed by a convolutional autoencoder. The bottleneck in the middle of the network, with the fewest channels and lowest resolution among all the feature maps, ensures that only representative common features of normal scenes are extracted and that the reconstruction will be of low quality if such normal patterns are absent. Although memory-augmented AE has been shown effective in surveillance (Gong et al. 2019; Park et al. 2020), we choose a simple architecture since the diversity in egocentric driving videos can require a significant amount"}, {"title": "4.2 Interaction expert", "content": "Although the scene expert is good at detecting ego-involved anomalies by learning frame-level normal patterns, non-ego involved anomalies can be easily missed due to the relatively small scales of anomaly participants in frames. Some object-level anomalies are often accompanied with abnormal interactions, which we define as the relative motion between two objects. For example, a collision between two cars is often preceded by a shrinking relative distance without sufficient deceleration, which is not a common interaction pattern in normal driving scenarios. An anomaly detector modeling interactions can also potentially raise alarms earlier than that modeling individual behaviors as anomalous interactions (e.g., approaching at high relative velocity) appear earlier than anomalous individual behaviors (e.g., sudden stop of a car after a collision).\nOur interaction expert models normal interactions by reconstructing motions of two objects, and an anomaly is detected whenever a high reconstruction error is observed. We denote a detected object's bounding box as $X_t = [c_x^t, c_y^t, w_t, h_t]$, where $[c_x^t, c_y^t]$ is the location of the center of the box and $w_t$ and $h_t$ are the width and height of the box, respectively. Given a pair of bounding box trajectories in the image plane $\\{X_{i,-T_{int}+1:t}, X_{j,-T_{int}+1:t}\\}$, where $T_{int}$ is the history horizon of the interaction expert and $i, j$ denote the IDs of the two objects, our time series model reconstructs the two trajectories simultaneously. We use pretrained Mask R-CNN (He et al. 2017), following prior works (Yao et al. 2023, 2020; Fang et al. 2022), and StrongSORT (Du et al. 2023) to acquire and associate object bounding boxes across frames at the original resolution, respectively. Each bounding box is normalized with respect to the dimensions of the images to facilitate efficient training.\nOur gated recurrent unit based autoencoder (GRU-AE) for trajectory pair reconstruction is inspired by the work on driver trait modeling (Liu et al. 2022). For brevity, we denote each pair of trajectories fed as input to GRU-AE at time t as a one-indexed sequence of length Tint. Each pair of bounding boxes at time k within a trajectory pair is flattened into a 1D vector to represent the joint localization of the two objects. The encoder GRU then applies a non-linear embedding function $\\phi_{encoder}$ to each flattened pair of bounding boxes and feeds the embedded features to the GRU cell:\n$h_k = GRU(h_{k-1}, \\phi_{encoder}([X_i^k, X_j^k]))$,\nwhere $h_k$ is the hidden state of the encoder GRU at time $k \\in (1,..., T_{int})$. After the entire trajectory pair is processed, we take the last hidden state $h_{T_{int}}$ as the feature of the interaction and create a bottleneck through another fully connected layer $\\phi_z$ to distill representative normal patterns:\nz = \\phi_z(h_{T_{int}}).\nWe choose an AE over a variational autoencoder (VAE) since we observe that any regularization of the latent space significantly degrades the reconstruction performance of the network in our experiments, possibly due to the highly diverse interaction patterns in normal driving videos.\nGRU-AE is tasked with reconstructing the original bounding box trajectories. However, directly outputting the absolute coordinates of bounding boxes from the network is inefficient to achieve our goal: we care more about object motions than object positions. In other words, it is the motion that the two cars are approaching each other, not where the two cars collide in the frame, that leads to"}, {"title": "4.3 Behavior expert", "content": "In cases where a non-ego involved anomaly only contains a single agent, the interaction expert can fail easily. For example, if there is only one vehicle visible in the field of view and starts swerving due to the snow, the interaction expert will not be activated as no interactions between road participants exist. A useful hint for anomaly in such a case is the observed unusual behavior: a car would never zigzag aggressively in normal driving scenarios.\nOur behavior expert models normal behaviors by predict-ing future locations of a road participant, and an anomaly is detected whenever an object's observed trajectory deviates significantly from the predicted trajectory. In addition to the past bounding boxes of an object, prior works also include ego motions as one of the inputs to compensate for the object location change due to such motions (Yao et al. 2019, 2023). However, Monocular SLAM algorithms (Mur-Artal et al. 2015; Mur-Artal and Tard\u00f3s 2017; Campos et al. 2021), used to estimate ego motions, require known camera intrinsics, which is often unavailable in a large-scale dataset with videos from multiple sources. Moreover, the tracking is often lost in featureless scenes, such as a highway in a rural area in a plain. Therefore, we remove the ego motions from our inputs to facilitate a more robust system.\nAnother useful hint for the prediction of future object localization is the global information contained in an image. For example, a normal trajectory of a vehicle is often constrained by obstacle-free drivable areas, within which the predicted future locations should reside. As a result, we made our behavior expert aware of the environment by incorporating the latest full RGB image into the inputs. Furthermore, we include the past optical flow features of each object as a final part of our inputs to enhance the perception on object motions, similar to prior works (Fang et al. 2022; Yao et al. 2023).\nFor each object, given the past bounding box tra-jectory $(X_1, X_2,\u2026\u2026, X_t)$, the past optical flow fea-tures $(f^{obj}_1, f^{obj}_2,\u2026\u2026, f^{obj}_t)$, and the original image $I_t$, our future object localization model predicts a trajectory $(X_{t+1|t}, X_{t+2}, ..., X_{t+\\delta|t})$ based on the information up until time t, where $\\delta > 0$ is the prediction horizon. Each bounding box is normalized with respect to the dimensions of the images as in the interaction expert, and the object's optical flow features are extracted by a 5 \u00d7 5 RoIAlign (He et al. 2017) operation from the frame-level optical flow fields. Our time series model, shown in Figure 7, follows an encoder-decoder structure, similar to GRU-AE proposed in Section 4.2 but without a bottleneck. One major difference between the behavior and interaction expert is that the behavior expert only needs to process the latest data (i.e., $X_t, f^{obj}_t,$ and $I_t$) at a time, as the historical information of the bounding boxes and optical flow features is propagated through the hidden states of the two GRUs since the first appearance of the object. Specifically, the bounding box $X_t$ and the flattened optical flow features $f^{obj}_t$ are processed at time t + 1 to update the hidden state of the two GRUs:\n$h^{box}_t = GRU(h^{box}_{t-1}, \\phi^{box}_{encoder}(X_t)),$\n$h^{flow}_t = GRU(h^{flow}_{t-1}, \\phi^{flow}_{encoder}(f^{obj}_t)),$"}, {"title": "5 Expert ensemble", "content": "The scene, interaction, and behavior expert are designed for different types of anomalies, as identified in Figure 2a. An efficient ensemble method is required to build a robust AD system as a driving scenario can be accompanied with any kinds of anomalies. In this work, we adopt a result-level fusion by combining the anomaly scores from all the experts to generate a final anomaly score at time t:\n$s_t = e(g_s(o_t), g_i(o_t), g_b(o_t))$,\nwhere e is the ensemble function. We now present two necessary steps towards the final score during evaluation."}, {"title": "5.1 Expert score normalization", "content": "The anomaly scores produced by different experts can differ in orders of magnitude due to the completely different mechanisms of score generation. To balance the effect of each score on the final anomaly score, we perform a score normalization step for each expert during evaluation:\n$s^{exp}_{t} = \\frac{s^{exp}_{t} - \\mu^{exp}}{\\sigma^{exp}}$,\nwhere $s^{exp}_{t}$ is the normalized expert score, $s^{exp}_{t} \\in \\{s^{ffp}_{t}, s^{str}_{t}, s^{int}_{t}, s^{beh}_{t}\\}$, and $\\mu$ and $\\sigma$ are the mean and standard deviation of the expert score on the training data without any anomalies, respectively. The normalization (26) differs slightly from the z-score normalization in that $\\mu$ and $\\sigma$ are computed on the training dataset while $s^{exp}_{t}$ comes from a mutually different evaluation dataset, which includes both normal and anomalous data. As a result, only $s^{exp}_{t}$'s obtained from the normal data are expected to have a mean around 0 and a standard deviation around 1, while those obtained from the anomalous scenarios are not.\nThe amount of training data may be limited. Therefore, to estimate a more realistic mean and standard deviation in (26), we employ a kernel density estimation (Weglarczyk 2018) to fit the probability density function (pdf) for the normal expert scores. We use a Gaussian kernel and apply the transformation trick (Shalizi 2013) to make sure that the estimated pdfs have support on [0, +\u221e], as all the expert scores are guaranteed to be positive. The mean and the standard deviation are then computed from the numerical integration on the estimated pdf. We note that only the information derived from the training data is required and thus the computation can be done before model deployment.\nIn the meantime, we set a threshold for AD for each expert. The threshold is determined by $\\tau^{exp} = U(\\alpha)$, which"}, {"title": "5.2 Kalman filter based score fusion", "content": "After the score normalization, all the expert scores are brought to a similar range and are ready for the final fusion. However, although informative, the outputs from the experts can be noisy in terms of conveying how likely a type of anomaly is present. In this work, we view each normalized expert score $s^{exp}_{t}$ as a noise-corrupted observation of the system state $x^{exp}_{t}$, which reflects the ground truth likelihood of encountering a specific type of anomaly. As mentioned in Section 3, it is possible that each point in time contains more than one anomaly. Therefore, we define the final anomaly score $s_t$ as the addition of the underlying system states for each type of anomaly. Our state vector then becomes $x_t = [x^{ffp}_t, x^{str}_t, x^{int}_t, x^{beh}_t, s_t]^T \\in R^5$, and the system model has the form of:\n$x_{t+1} = Ax_t + w_t$\n$y_t = Hx_t + v_t,$\nwhere the observation $y_t = [s^{ffp}, s^{str}, s^{int}, s^{beh}]$ and the state-transition matrix A and the observation matrix H are set respectively as:\nA = \n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 1\n\\end{bmatrix}\n, H = \n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{bmatrix}\n,\nThe process noise $w_t$ and the observation noise $v_t$ are drawn from two zero mean normal distributions with covariance Q and R, respectively. Although the coefficients for the weighted sum in the last row of A can be different, we use identical values for simplicity.\nTo estimate the final anomaly score, we employ Kalman filter, which has been widely used for estimating the internal state of a system in various application domains (Bewley et al. 2016; Wojke et al. 2017; Sun et al. 2021). The state dynamics of our Kalman filter follows the standard framework, such as the one in Patel and Thakore (2013), and is omitted here for brevity. We set the covariance of the observation noise R to be an identity matrix as a result of the expert score normalization. Using 1-based indexing, the initial a posteriori estimate covariance matrix $P_{1|1}$ in Kalman filter and the covariance of the process noise Q are both set to be a diagonal matrix filled with 0.1 for"}, {"title": "6 Experimental results", "content": "In our experiments, we evaluate the anomaly detection performance of Xen on the largest on-road video anomaly dataset, named the Detection of Traffic Anomaly (DoTA) dataset (Yao et al. 2023). DoTA is comprised of diverse video clips collected from dashboard cameras in different areas (e.g., East Asia, North America, Europe) under different weather (e.g., sunny, cloudy, raining, snowing) and lighting conditions (day and night). Each video in the dataset is recorded at 10 fps and contains one anomalous event. Due to different video sources, camera intrinsics and poses are different across videos.\nWe use the same training and test split of DoTA as those in Yao et al. (2023) and further augment the test split with 88 videos, which are provided in the dataset but unlabeled, by following the annotation principles in the original paper. The final training set consists of 3275 videos and the test set contains 1490 videos. Each video is annotated with anomaly start and end time, based on which a frame is labeled as 0 or 1 for a normal or anomalous time step, respectively, for evaluation purposes only. Notably, the anomaly start time in DoTA is defined as the time when the anomaly is inevitable, which is often prior to an actual accident (e.g., a car crash). Therefore, the evaluation reflects the model performance of early anomaly detection.\nThere exist 9 categories of on-road anomaly in DoTA, and each category is further split into ego involved and non-ego involved cases, resulting in 18 categories in total. For quick reference, we list the 9 ego involved categories in Table 1. To represent non-ego involved counterparts, we append a letter of \"N\" to each label in Table 1. For example, ST-N stands for non-ego involved collisions with another vehicle that starts, stops, or is stationary. We refer to Yao et al. (2023) for data samples in DoTA.\nBased on the analysis on common anomaly patterns in Section 3, we classify all the anomaly categories in Table 1 as ego involved anomalies, VO-N, OO-N, UK-N as non-ego involved individual anomalies, and the rest 6 categories as non-ego involved interactive anomalies. Unsupervised AD models can only be trained with normal data, so we use the frames before the anomaly start time in each video in the"}, {"title": "6.1 Baselines and metrics", "content": "We evaluate the performance of the proposed method on the test set, along with the following baseline methods:\n\u2022 Conv-AE (Hasan et al. 2016): An AE-based model which encodes temporally stacked RGB images with convolution layers and decodes the resulting features with deconvolution layers to reconstruct the input. The MSE over pixels in a frame is computed as the anomaly score.\n\u2022 AnoPred (Liu et al. 2018): A unet-based model which takes four preceding RGB frames as input to directly predict a future frame. The negative PSNR of the predicted frame is used as the anomaly score.\n\u2022 FOL (Yao et al. 2019): A recurrent encoder-decoder network which sequentially processes the past ego motions, object's bounding boxes, and optical flow features to predict future locations of the object over a short horizon. Among the three strategies of computing anomaly scores proposed in the original paper, we use the best-performing prediction consistency (22) as our baseline.\n\u2022 FOL-Ensemble (Yao et al. 2023): An ensemble method which achieves the state-of-the-art results on the DoTA dataset. The object anomaly score, generated by FOL, is first mapped to per-pixel scores by putting a Gaussian function at the center of each object. Such a pseudo anomaly score map is then averaged with the error map generated by AnoPred to produce the fused anomaly score map. The averaged score over all the pixels in the fused score map serves as the final anomaly score.\nAll the baselines are adapted to DoTA based on the corresponding released code and original paper. We also perform an expert-level ablation study by sequentially removing the experts from Xen to investigate the necessity of each module. In the rest of this section, we use Xen-S to denote the ensemble of FFP and STR in the scene expert, Xen-SI the ensemble of the scene and interaction"}, {"title": "6.2 Overall results", "content": "The overall results are presented in Table 3. As shown, Xen-SIB achieves the best AUC and highest F1-score with a large margin over the four baselines. Although we were able to reproduce the performance of FOL in the original paper (Yao et al. 2023) with 0.1 difference in AUC, the AD metrics drop significantly after evaluating on all test videos and removing the video-wise min-max normalization. We argue that this is partially due to the fact that object-centric methods can completely fail in"}, {"title": "6.3 Per-class results", "content": "Table 4 shows the results of different methods broken out into ego involved and non-ego involved cases. In general, non-ego involved anomalies are more difficult to detect than ego involved ones, possibly because non-ego anomalies are often accompanied with small anomalous regions and low camera visibility of objects. By comparing Xen-SI with frame-level methods (e.g., AnoPred and Xen-S), we found that the improvement in non-ego involved cases is much more significant than that in ego involved cases, which is expected since the interaction expert mainly enhances the object-level AD performance. A similar improvement is observed when further including the behavior expert in Xen-SIB, which achieves the best AUC and F1-score in both ego involved and non-ego involved cases. By contrast, the object-centric method FOL performs relatively well in non-ego involved cases but struggles to excel in ego involved cases, resulting in overall inferior performance compared to Xen variants. Despite the fusion of frame-level and object-centric method, FOL-Ensemble fails to provide as large improvements as Xen-SIB does over each component (e.g., AnoPred) in the ensemble, which highlights the importance of effective fusion mechanism for AD in complex driving scenarios. Last but not least, in both ego involved and non-ego involved cases, Xen variants with immediate filtering since time 1 still consistently outperform those with filtering since the time when all experts start to produce anomaly scores, which agrees with the findings in Table 3.\nMore fine-grained results on the AD performance for each type of anomaly are presented in Table 5. We observe that Xen-SI and Xen-SIB exhibit the highest AUC and F1-score in most columns"}]}