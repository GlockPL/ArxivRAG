{"title": "Beyond Monte Carlo: Harnessing Diffusion Models to Simulate Financial Market Dynamics", "authors": ["Andrew Lesniewski", "Giulio Trigila"], "abstract": "We propose a highly efficient and accurate methodology for generating synthetic financial market data using a diffusion model approach. The synthetic data produced by our methodology align closely with observed market data in several key aspects: (i) they pass the two-sample Cramer von Mises test for portfolios of assets, and (ii) Q - Q plots demonstrate consistency across quantiles, including in the tails, between observed and generated market data. Moreover, the covariance matrices derived from a large set of synthetic market data exhibit significantly lower condition numbers compared to the estimated covariance matrices of the observed data. This property makes them suitable for use as regularized versions of the latter. For model training, we develop an efficient and fast algorithm based on numerical integration rather than Monte Carlo simulations. The methodology is tested on a large set of equity data.", "sections": [{"title": "Introduction", "content": "In this paper, we present an efficient methodology for generating synthetic financial market data, based on the diffusion model approach. Diffusion models [19], [20], [6], [21], [22], a class of deep generative models, are mathematical models designed to generate synthetic data by Monte Carlo sim- ulating a reverse-time stochastic process, which is specified as an Ito stochastic differential equation (diffusion process). The diffusion model strategy to synthetic data generation and is a two stage pro- cess: encoding and decoding. This process employs the use of linear stochastic differential equations. These models have demonstrated impressive results across various applications, including com- puter vision, natural language processing, time series modeling, multimodal learning, waveform sig- nal processing, robust learning, molecular graph modeling, materials design, and inverse problem solving [25]. Despite their successes, certain aspects of diffusion models, particularly those related to the learning mechanism, require further refinement and development. Ongoing research efforts focus on addressing these performance-related challenges and enhancing the overall capabilities of diffusion modeling methodologies.\nThe significance of synthetic data across various domains, including addressing challenges related to data privacy, regulatory compliance, and fraud detection, is highlighted in [10] and the references therein. In the context of the finance industry, the ability to generate high-quality synthetic market data plays a crucial role in applications such as portfolio allocation, portfolio and enterprise risk quantification, the design and back-testing of trading and market-making strategies, what-if analysis, and more.\nIn financial practice, accurately estimating various statistics of asset returns is critical. A key example is the covariance matrix, which plays a central role in quantitative metrics such as portfolio asset allocation and value at risk. These methodologies often assume, either explicitly or implicitly, that returns are drawn from a multidimensional Gaussian distribution or, at a minimum, an ellipti- cal distribution. Various estimation techniques have been proposed, including: (i) standard sample (or maximum likelihood) estimators, (ii) shrinkage estimators [15], (iii) factor-based estimators [4], and others [3]. However, standard sample estimators often produce poorly conditioned covariance matrices with unstable inverses. This issue is commonly attributed to two primary factors:\n(i) First, the dimensionality of the problem poses challenges: a covariance matrix of dimension d has d(d+1)/2 parameters, which are estimated from a time series of n observations. For typical portfolio sizes, this leads to a severely underdetermined problem. The alternative methods listed above address this issue by regularizing the sample estimator using exogenous mechanisms.\n(ii) Second, it is often believed that the estimation process itself is somehow prone to \"error maxi- mization\", rendering it unreliable for practical use.\nA natural approach to regularizing the estimated covariance matrix is to reliably generate a large number of samples from the unknown, non-parametric probability distribution underlying the asset returns. This would allow for the estimation of statistics, including the covariance matrix, with im- proved conditioning properties.\nSpecifically, we consider a portfolio of d assets A\u00b9,..., Ad (e.g. equities), whose prices are observed at discrete times. By X\u00b9, . . ., Xd we denote the returns on these assets. While the frequency of price observations is irrelevant to our analysis, for clarity, we assume daily observations. Denote the d-dimensional vectors of their returns\u00b9 at each time ti, i = 1, ..., n, where t\u2081 < t2 < ... < tn, by X = {xi, i = 1, ..., n}. Notably, we impose no specific assumptions on the distribution of these returns. The proposed method of generating synthetic data consistent with X consists in the following steps:\n1. We view the market data X as a sample drawn from an unspecified probability distribution Po. To each data point in X random noise is added incrementally over a time interval until the data is completely deformed into white (Gaussian) noise. This random noise is modeled as a suitable diffusion process.\n2. Specifically, we generate random noise by a multi-dimensional stochastic differential equation (SDE) of dimension d, whose initial value at time t = 0 is an element of X. This SDE is referred to as the denoising SDE (DSDE). The diffusion process is carefully chosen to smoothly transform the distribution Po into d-dimensional white noise as the time variable approaches the terminal time t = 12. The DSDE is parameterized by coefficient functions that control how quickly the process transitions to white noise."}, {"title": "Model learning via score matching", "content": "We begin by reviewing the score matching method and its refinement to denoising score matching. These methods are the foundations of the diffusion model approach."}, {"title": "Explicit score matching", "content": "The score matching methodology for model learning was introduced by Hyv\u00e4rinen in [7] as a practical alternative to the maximum likelihood estimation (MLE) of the parameters of a probability distribu- tion. The advantage of score matching over MLE is that it is directly applicable even if the probability density function (PDF) is known only up to a normalization factor. The method consists in the fol- lowing.\nLet p(x) be the PDF of a continuous probability distribution defined for x \u2208 Rd. Its (Stein) score s(x) defined as\n$$s(x) = \u2207 log p(x).$$", "equations": ["s(x) = \u2207 log p(x)."]}, {"title": "Denoising score matching", "content": "A practically important variant of score matching, replacing the empirical distribution (7) with a smooth probability distribution, was developed by Vincent [23]. The method consists in regularizing the empirical distribution px(x) by its Parzen-Rosenblatt estimate,\n$$Px,h(x) = \\frac{1}{n} \\sum_{i=1}^{n}Ph(x|x_i),$$"}, {"title": "Diffusion models", "content": "The incorporation of noise into the system will be achieved by generating sample paths of a suitable diffusion (Ito) process, which is characterized as denoising stochastic differential equations. This method facilitates the simulation of noise effects within the model."}, {"title": "Diffusion processes", "content": ""}, {"title": "Stochastic differential equations", "content": "A diffusion process Xt \u2208 Rd over the time interval t \u2208 [0, 1] is the solution to a stochastic differential equation (SDE) [17], [8]\n$$dXt = a(t, Xt) dt + \u03c3(t, Xt) dWt,$$"}, {"title": "Linear SDES", "content": "Specifically, a d-dimensional linear SDE driven by a d-dimensional Brownian motion (i.e. p = d in the notation of Section 14) is a diffusion process of the form\n$$dXt = a(t, Xt)dt + \u03c3(t)dWt,$$"}, {"title": "Reverse-time diffusion process", "content": "The key input to a diffusion model is the reverse-time diffusion process, which corresponds to the (forward-time) diffusion process defined above. This process constitutes the decoding phase of the model, and the generated sample paths represent the synthetic scenarios. The reverse-time diffusion processes corresponding to a forward-time diffusion (14) is an SDE which runs in reverse time and which generates the same sample paths as the original process. The general concept of reverse-time diffusion processes was introduced and studied by Anderson in [1], who extended earlier work on reverse-time linear diffusions.\nThe theory developed in [1] can be summarized as follows. Consider the diffusion process defined by (14) with a(t, x) and o(t, x) sufficiently regular, so that the probability density function p(t, x), i.e. the solution to the Kolmogorov equation (19), exists and is unique. In the case of linear diffusions, these assumptions are satisfied. We define the following p-dimensional process Wt:\n$$dWt = dWt+ \\frac{1}{p(t, Xt)}\u25bd(\u03c3(t, Xt)p(t, Xt))dt,$$"}, {"title": "Denoising SDEs and their time reversals", "content": "A denoising SDE (DSDE) is a linear SDE with suitable mean-reversion properties, designed to model noise within the context of a diffusion model. The following three examples are commonly used as standard realizations of DSDEs [22]:\n1. Variance preserving (VP) SDE.\n2. Sub-variance preserving (sub-VP) SDE.\n3. Variance exploding (VE) SDE.\nIn the following sections, we will focus on the VP SDE. However, for completeness, we also provide brief discussions of the sub-VP and VE SDEs."}, {"title": "Variance preserving SDE", "content": ""}, {"title": "Forward time VP SDE", "content": "Let \u1e9e(t) \u2208 Rd be a vector of strictly positive, deterministic functions Bi(t), for 0 < t < 1 and i = 1,...,d, such that fo Bi(s)ds \u2192 \u221e, as t \u2192 1. A convenient choices of Bi(t) is the power function\n$$Bi(t) = bi(1-t)^{-(1+a)},$$"}, {"title": "Reverse-time VP SDE", "content": "According to (35), the reverse-time SDE for Xt reads\n$$dXt = -8(t) (\\frac{1}{2}Xt+st, X\u2081))dt + \u221a\u1e9e(t)dWt,$$"}, {"title": "Sub-variance preserving SDE", "content": ""}, {"title": "Forward-time VE SDE", "content": "The sub-variance preserving (sub-VP) SDE, introduced by [22], modifies the VP SDE as follows:\n$$dXt = \\frac{1}{2}B(t)X+dt + \u221a(t) (1-e^{-2 \\int B(s)ds}) dWt.$$"}, {"title": "Reverse-time sub-VP SDE", "content": "According to (31), the reverse-time SDE for Xt reads\n$$dXt = -\u03b2(t)(\\frac{1}{2} Xt+ (1-e^{-2 \\int_0^{t}B(s)ds})s(t, Xt)) dt + \u221a(t) (1 \u2013 e^{-2 \\int_0^{t}B(s)ds}) dwt,$$"}, {"title": "Variance exploding SDE", "content": ""}, {"title": "Forward-time VE SDE", "content": "Let v(t) \u2208 Rd be a vector of positive, strictly increasing differentiable functions. A convenient choice is\n$$vi(t) = biat,$$"}, {"title": "Reverse-time VE SDE", "content": "According to (31), the reverse-time SDE for Xt reads\n$$dXt = -s(t, Xt) \\frac{d}{dt} v(t) dt + \\sqrt{\\frac{d}{dt} v(t)} dWt,$$"}, {"title": "Model training via score matching", "content": "The parameters 0 are estimated from the objective function [22]\n$$CSM (0) = \\frac{1}{2} \\int_0^1 \\int_{\\mathbb{R}^d}  x(t)||s(t, x; 0) - \\log p(t, x) ||\u00b2p(t, x) dx dt,$$"}, {"title": "Parameterization of the DSM objective function", "content": "According to (2), the model score function s(t, x) is given by\n$$s(t, x) = - \\frac{1}{C(t)}(x \u2013 \u03bc(t, xo)),$$"}, {"title": "Evaluation of the DSM objective function", "content": "The high dimensional integrals is in (38) require an efficient numerical evaluation method. Integration over t is carried out accurately using Simpson's rule. In fact, a small number of subdivisions of the integral over [0,1] (such as 10 or even less) is generally sufficient to get an accurate value of the integral. Integration over x can, in principle, be carried out via Monte Carlo simulations. This method requires very significant computing resources even if the dimensionality d of the problem is moderate, and is impractical for large values of d (such as d > 100). However, owing to the specific form of the objective function, the d-dimensional integration can be reduced to one- and two-dimensional Gaussian integrals which can be efficiently computed using Gauss - Hermite quadrature.\nWe start by expressing LDSM (0) as a sum:"}, {"title": "Synthetic markets", "content": "The process of generating synthetic markets involves several tasks:\n(i) Training of the fitted score function, as discussed in Section 5.\n(ii) Encoding and decoding of the training (historically observed) data.\n(iii) Testing whether the generate data follow the same probability distribution as the training data. This step is crucial, as financial data are often difficult to visualize.\nIn this section we focus on tasks (ii) and (iii)."}, {"title": "Euler-Maruyama scheme for the forward and reverse-time DSDE", "content": "The process of generating a synthetic market scenario involves two runs of the denoising SDE: a for- ward run and a reverse-time run. Both runs effectively draw a sample from the probability distribution associated with a DSDE. This is achieved using the Euler-Maruyama numerical scheme [11], which simulates approximate sample paths of the SDE.\nForward path generation. To generate a forward path for the linear SDE (20) starting at Xinit, we divide the interval [0, 1] into K + 1 equal subintervals of length d = 1/K using the points tj = j/K, where j = 0, . . ., K, and denote by Xt, the (approximate) value of Xt at t = tj."}, {"title": "Generating synthetic market scenarios", "content": "Given a training set of market data X representing n observations of returns on d assets, we aim at generating m synthetic returns on these assets, where the choice of m is unrelated to n. We proceed as follows:\nA. Train the fitted score function (71) on the dataset X, as explained in Section 5.\nB. Generate synthetic samples. For k = 1, . . ., m perform the following steps:\n(i) Randomly select an index ik \u2208 {1, ..., n}.\n(ii) Set Xinit = Xik, and generate a forward path starting at Xinit. Let Xk denote the terminal value of this path.\n(iii) Set Xterm = Xk, and generate a reverse-time path starting at Xterm. Let k denote the initial (t = 0) value of this path.\nC. The set x = {1, ..., \u00c2m} is the desired set of m synthetic market scenarios."}, {"title": "Experiments", "content": "For our experiments, we downloaded daily closing prices from Bloomberg for 33 large U.S. eq- uities over the five-year period from November 13, 2019, to November 13, 2024.. This period notably encompasses the economic distress caused by the COVID-19 pandemic, which introduced significant volatility to equity markets. Testing the model's ability to accurately simulate under these non-stationary conditions serves as a critical assessment of its validity.\nWe selected various data windows, each n = 256 days in length - approximately equivalent to the number of trading days in a year. For each window, we trained the diffusion model on the corresponding data and generated m = 1,024 synthetic market scenarios.\nThe model was configured as follows: the VP DSDE was selected with instantaneous volatility functions defined by (36), using parameters a = 0 and bi = 0.1 for all i. Gauss-Hermite integration was performed with an order of D = 4, and Simpson integration was performed using 8 subintervals.\nThe function (t) in (76) was set to 10(t) = 1, and the number of hidden neurons was configured as h = 16. Training was conducted using the Adam optimizer.\nTo evaluate the model's performance for each run, we recorded the following metrics:\n(i) The p-value PCvM of the CvM statistic of the equally weighted portfolio of all selected assets."}]}