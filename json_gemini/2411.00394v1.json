{"title": "Right this way: Can VLMs Guide Us to See More to Answer Questions?", "authors": ["Li Liu", "Diji Yang", "Sijia Zhong", "Kalyana Suma Sree Tholeti", "Lei Ding", "Yi Zhang", "Leilani H. Gilpin"], "abstract": "In question-answering scenarios, humans can assess whether the available in-\nformation is sufficient and seek additional information if necessary, rather than\nproviding a forced answer. In contrast, Vision Language Models (VLMs) typi-\ncally generate direct, one-shot responses without evaluating the sufficiency of the\ninformation. To investigate this gap, we identify a critical and challenging task\nin the Visual Question Answering (VQA) scenario: can VLMs indicate how to\nadjust an image when the visual information is insufficient to answer a question?\nThis capability is especially valuable for assisting visually impaired individuals\nwho often need guidance to capture images correctly. To evaluate this capa-\nbility of current VLMs, we introduce a human-labeled dataset as a benchmark\nfor this task. Additionally, we present an automated framework that generates\nsynthetic training data by simulating \"where to know\" scenarios. Our empir-\nical results show significant performance improvements in mainstream VLMs\nwhen fine-tuned with this synthetic data. This study demonstrates the potential to\nnarrow the gap between information assessment and acquisition in VLMs, bring-\ning their performance closer to humans. Our dataset and code are available at:\nhttps://github.com/LeoLee7/Directional_guidance.", "sections": [{"title": "1 Introduction", "content": "In recent years, Vision-Language Models (VLMs) have made significant strides in general multimodal\ntasks such as visual recognition and Visual Question Answering (VQA) [1, 49]. This progress has\nopened up a vast potential for various applications, including enhancing visual accessibility for\nvisually impaired individuals [4, 20], supporting decision-making in autonomous systems [50, 32],\nenabling interactive technologies [39], etc. Despite these advances, VLMs still fall short of human\ncapabilities. Humans can intuitively assess whether the available information is sufficient to answer a\nquestion and seek additional details when necessary [36, 11]. In contrast, VLMs typically tend to\nprovide direct, single-response outputs even when information is insufficient to answer the question\naccurately. This limitation reduces their effectiveness in real-world applications [13]. To address this\nissue, recent studies have explored ways to teach VLMs to assess information sufficiency [42]. These\nstudies aim to have VLMs either provide concrete answers or label questions as unanswerable, using\nbenchmark datasets from real user questions like VizWiz [20].\nHowever, a significant gap remains in handling unanswerable cases: deciding what actions to take\nwhen VLMs identify a question to be unanswerable. Humans naturally possesses the ability to\nseek additional details when faced with unanswerable questions a challenge often encountered in\nreal-world VQA tasks due to poor image quality, ambiguous questions, or loss of context [3, 9]. \u03a4\u03bf"}, {"title": "2 Related Work", "content": "Directional Visual Understanding. Many studies have identified that current VLMs struggle to\ninterpret and understand spatial relationships within an input image, especially on fundamental visual\nconcepts like relative directions [15, 40]. This ability is important for interactive VQA applications"}, {"title": "3 The Cognitive Question: From What's Unknown and Where to Know", "content": "To understand how a statistical model conceptualizes the world, one effective approach is to draw an\nanalogy to human cognition. In the meta task in NLP (i.e., Question-Answering as other core NLP\ntasks can be transformed into QA), human cognitive processes in problem-solving and learning are\nmultifaceted, involving not just the retrieval of stored information but also the recognition of one's\nknowledge boundaries and the strategic acquisition of new knowledge. To simulate these processes,\nwe propose a hierarchical cognitive process pattern comprising three levels:\n1. Response Generation (knowing what's known): At the foundational level, the model\nutilizes its existing knowledge base and basic analysis capabilities to generate responses to\nqueries. This process mirrors the human cognitive function of retrieving known information\nfrom memory, akin to recall or recognition tasks in cognitive psychology [28, 35]. It reflects\nthe model's ability to combine available information into coherent answers.\n2. Awareness of Knowledge Limits (knowing what's unknown): The second level reflects\nthe model's metacognitive ability to evaluate its own knowledge state, recognizing when it\nlacks sufficient information to answer a question accurately [14, 36, 37]. This awareness is\ncrucial for intellectual honesty and mirrors the human cognitive process of monitoring and\nevaluating one's understanding and capabilities, a key aspect of metacognition [51, 38, 11].\n3. Knowledge Acquisition Direction (knowing where to know the unknown): At the most\nadvanced level, the model identifies pathways for acquiring new knowledge when existing"}, {"title": "4 Method", "content": "As mentioned in Section 1, most existing works on VLMs cognitive questions are focused on the first\ntwo levels [42, 15, 5], and the third level is mostly under-explored. We argue that the challenges lie in\nthe difficulty of collecting suitable data for benchmark and training data: there are few VQA samples\nthat exhibit both awareness of knowledge limits and knowledge acquisition direction. Therefore, in\nour study, we focus on benchmark dataset curation and training data generation."}, {"title": "4.1 Directional Guidance Task", "content": "We define our Directional Guidance task as follows: in the context of VQA, given an image-question\npair < I, Q >, the model M should determine whether the image needs to be\nreframed. To be specific, if the target object is only partially visible and not sufficient to answer the question, the\nmodel should give clear guidance for the reframing direction (left, right, up, or down). Otherwise,\nthe model should inform whether the question is already answerable (no need to change) or remains\nunanswerable even with potential reframing (none of the other options). This task mirrors real-world\nscenarios where visually impaired individuals need guidance to position their cameras correctly\nthrough many attempts. Although the target object might be only partially visible on each attempt,\nwith continuous adjustments under guidance, the user can always capture a better view and finally\nhave a better chance to get the question answered. This task goes beyond simply detecting the\nill-framing issue of the image: it assesses whether the framing issue impacts the model's ability to\nanswer the specific question posed. For example, reframing may not be necessary if the question can\nbe directly answered with the available visual information even if the image is ill-framed. We regard\nthese guide responses as an additional output that complements the original VQA answering process.\nThis task exemplifies three levels of the hierarchical cognitive pattern discussed in Section 3. Instead\nof a binary classification of answerable/unanswerable as proposed in [9], this task emphasizes the\nmodel's ability to effectively utilize available visual information. It requires the model to assess\nwhat is known and determine where to acquire extra information, standing in the transition from\nunanswerable to answerable."}, {"title": "4.2 Directional Guidance Dataset", "content": "Benchmark dataset. To evaluate model performance in our task setting, we created a benchmark\ndataset derived from VizWiz dataset families [20, 7]. The VizWiz dataset consists of real VQA queries\ncollected from visually impaired individuals [20]. From this dataset, we used all the unanswerable\nsamples (1.4k) from the validation set as the training set may have potential leakage issues during\nthe pre-training process of VLMs. We invited 26 human annotators to identify ill-framed photos\nand label the most promising direction to move the camera, by which the reframing action could\npotentially help to answer the question (more details are available in Appendix A.1). After cleaning\nand re-evaluation, we collected 291 samples where reframing could potentially lead to an answer, and\n230 samples unlikely to be answered even with reframing. The rest samples are where the human\nannotators have disagreements. The details of the data collection are presented in the Supplementary\nMaterials. To ensure a balanced distribution in the test set, we randomly selected 300 samples\nfrom the VizWiz-grounding test set and simulated the case where the current image already has\nsufficient information to answer the question, under the assumption that the visual evidence could be\ntheoretically grounded in the image. Combining those three groups, we get a high-quality Directional\nGuidance benchmark dataset including 821 samples. Despite the size of the dataset being relatively\nsmall, this reflects the inherent challenge of the task, where ill-framed images are rare in standard\nVQA datasets but commonly seen in real-world scenarios. Furthermore, our dataset's diversity and\ncomprehensiveness make it suitable for evaluating model performance on the target task, providing a\nvaluable foundation for future studies."}, {"title": "Training dataset.", "content": "We propose a data augmentation process to simulate the ill-framed samples,\ninstead of collecting ad-hoc images that suit the task. Initially, we take all the training samples from a\ndataset pool - the validation set of VizWiz-grounding dataset [7] as it includes the visual groundings\nfor each answerable VQA query. With that visual grounding information, manual perturbations have\nbeen applied to simulate ill-framing. Specifically, we identify the bounding box surrounding the\ntarget object and divide it into 10 zones, horizontally and vertically. We then choose a specific zone\nfor cropping, resulting in an image that has some missing information while retaining a part of the\ntarget object. With a series of perturbations, we observe the consistency of the model's response to the\ninitial VQA query and capture the cases where an ill-framing issue impacts the question-answering.\nAs the VizWiz is an open-ended task, we use precision as the evaluation:\nPrecision = \\frac{\\sum_w |P(w) \\cap T(w)|}{\\sum_w |P(w)|} \t\t\t(1)"}, {"content": "P(w) and T(w) denote a word from the model prediction and from the ground-true answer. Precision\ncalculates how many words in the predictions also appear in the ground-truth answer, and we set a\nthreshold e to identify the correctness. Following [19], only non-stop words have been taken into\nconsideration. Figure 2 and algorithm 1 outline the process of generating training data with guidance\nlabels.\nAnother crucial case in the benchmark test set involves samples that remain unanswered even after\nadjusting the camera. One more data argumentation technique has been placed: we mismatch\nthe questions and images from the same dataset pool to create new pairs with different semantic\ninformation. Most questions in the original dataset pool are generic, as a highly frequent question is\n\"What is this?\" without semantic information. Correspondingly, our GPT-4 enabled argumentation\nhelped rephrase the paired question and answer. For example, given an image I\u2081 with the question\nQi \"What is this?\u201d and an answer A\u00bf \u201claptop,\u201d the new question Q will be rephrased to \u201cWhat's\nthe color of this laptop?.\u201d Then, we mismatch the Q with another irrelevant image I; to form a new\npair. This augmentation generates complex, real-world queries where straightforward answers are\ninfeasible, compelling models to learn deeper semantic information."}, {"title": "4.3 Experiment settings", "content": "Model Selection. To verify the feasibility and effectiveness of our approach for different model\narchitectures and sizes, we analyze the experiments of four mainstream open-source large models\nwith different sizes, including: LLaVA-1.5 [27], InstructBlip [10], GPT-40 [31], and CLIP [33]. First,\nwe benchmark the test set on the LLaVA-1.5, InstructBlip, and GPT-40 on the zero-shot setting. A\nseries of prompts has been designed to test their zero-shot performances, serving as our baselines.\nNext, we generate a training dataset using algorithm 1 and apply LoRA [21] fine-tuning on the\nopen-sourced models. We anticipate the effectiveness of our proposed training framework will be\nreflected by the improvement of model performance compared with the zero-shot baseline.\nTask format. To quantitatively analyze the model's ability to provide guidance, we format the task\nwith a basic VQA multiple choice template: \u201c<image>{Original_Question} To improve the\nimage and answer the question, how should the camera be moved? A.Leave it\nunchanged. B.Left. C.Right. D.Up. E.Down. F. None of the other options.\u201d\nEach option reflects the model's decision of Directional Guidance: The leave it unchanged\noption indicates that the current image contains all the necessary information to answer the question.\nThe four directional options suggest that the relevant object is only partially visible, and further\nimage adjustment is needed. The None of the other options implies that moving the camera\nwill not help because the question is inherently unanswerable, i.e. due to the ambiguity, or the\nrelevant object is absent from the current image. We use the F1 score and accuracy as the evaluation\nmetrics and also analyze the confusion matrix of the different options.\nZero-shot prompt setting. For the zero-shot baseline, we enhance the basic template with addi-\ntional instructions and explanations tailored for each model. We designed two prompt settings to\naccommodate their varying capabilities. The first setting is a single-round query where the model\nmakes predictions from six options directly. The second setting is a two-round prompt, following the\nChain-of-Thought [43] process. This two-round prompt decomposes the tasks and works as follows:\nInitially, we prompt the model to determine if the target object is fully present in the image, partially\nvisible, or if the question is unanswerable. The corresponding options are: leave it unchanged,\nreframe, and none of the other options. If the model indicates that the target object is only\npartially visible, we then ask it to decide a specific direction for movement: left, right, up, or\ndown. To ensure reproducibility, we include all prompts we used in Supplementary Materials A.5.\nFine-tune setting. In our training framework, we utilize data augmentation to generate potential\nsamples with guidance labels. We assess the consistency of the model's predictions before and after\nperturbations and categorize the samples into two groups. Samples where the model fails to predict\npost-perturbation are considered positive, and their Directional Guidance labels are assigned one of\nfour directions: left, right, up, or down. Conversely, samples where the model maintains correct\npredictions are labeled as negative, with the Directional Guidance label set to leave it unchanged.\nUpon analyzing these groups, we observed that negative samples predominated the generated training\nset. To ensure a balanced distribution within the training dataset, we under-sampled the negative\nsamples to align with the average count of the four directional categories. We also adjusted the\nnumber of None of the other options samples to achieve an even distribution across the entire\ntraining set.\nAfter generating the training set, we format the new pairs into a standardized instruction fine-tuning\nlayout: each sample, comprising < I', question >, is supplemented with option choices and\ninstructions. Since the task requires models to respond with a single letter, the prediction process\nis equivalent to a classification task. Following the settings in [27], the loss is only computed on\nthe token for the chosen letter and the < eos > (end of the sentence) token. Also, to prevent the\nmodel from memorizing the letter distribution, we randomly shuffle the association between letters\nand options, ensuring each letter (from A to F) is paired with an option randomly in each training\nsample. When fine-tuning each model, most training configurations follow the officially suggested\nsettings, and more training details are presented in Supplementary Materials A.2.\nNone-generative Models. Since the task has been simplified to a classification problem, we also\ninvestigate whether a non-generative model with a simpler architecture could suffice. Accordingly, we\nadd a linear probe layer onto CLIP and perform a classification head, using a vision encoder (CLIP-\nViT-L-336px) aligned with LLaVA-1.5 and a text encoder in the default setting. We concatenate the\nimage feature and the text feature as the input for the classification head. Since the CLIP model can\nnot generate open-ended answers, we use the synthetic training dataset generated by LLaVA-1.5 13b."}, {"title": "5 Results", "content": "5.1 Directional Guidance benchmark dataset and baseline performance\nFig. 3 (a) shows the distribution of four directions in our benchmark dataset. The horizontal directions\nare the most common, with left at 38.5% and right at 29.6%. The figure displays four typical samples\nfrom each direction. We also identified a frequent scenario where users need to take another photo\nand attempt a different question, as shown in Fig. 3 (e). This pattern reveals a common challenge for\nvisually impaired individuals: without continuous guidance, the user and assistant can easily lose the\ncontext of the original VQA. These findings emphasize the importance of providing clear, sequential\ndialogue-based guidance for effectively adjusting the camera position.\nAs mentioned in Section 4.3, we use different prompt settings for each group of models that suit\ntheir capabilities. For the 7b models, we use the two-round prompt because these models benefit\nfrom a more structured, step-by-step approach, which helps them handle the task more effectively.\nIn contrast, we tested the LLaVA-1.5 13b and GPT-40 model with a single-round of prompting to\nsee if they were capable of this task. The prediction results are presented in Fig. 4, from (a1) - (a4).\nWe observe that three open-sourced models (LLaVA-1.5 7b/13b, and Instructblip 7b) show similar\nbehaviors: these models tend to avoid predicting the reframing cases and mistakenly categorize them"}, {"title": "5.2 Model's performance after fine-tuning", "content": "We present the heat maps of the fine-tuned model predictions in Fig. 4. By comparing the prediction\nresults between the zero-shot baselines and the fine-tuned models, we observe significant and consis-\ntent improvements in prediction performance, demonstrating the effectiveness and generalizability\nof our proposed method. Although there is considerable potential to improve the overall accuracy,\nthe fine-tuned models reduce confusion between reframing, leaving it unchanged(O), and\nnone of the other options(X). The fine-tuned models are more likely to provide directional\nguidance on the reframing cases. Moreover, the predictions within reframing cases show noticeable\nimprovement, as indicated by the clear diagonal line in the heat map. Another interesting finding is\nthe substantial reduction in wrong predictions with opposite directions (e.g., predicting an up case as\ndown). This clarity is meaningful, as it lowers the chance of users receiving conflicting guidance,\nthereby enhancing safety and efficiency in real-world applications. Overall, the fine-tuned models\nreduce errors across all options, showing significant improvement in both cross-category and within\nreframing predictions.\nTo evaluate our training framework's sensitivity to different settings, we conducted groups of com-\nprehensive experiments. We used three metrics to quantitatively assess the model's performance:\noverall F1 score, overall Accuracy, and Accuracy on the reframing cases denoted as ACC(F). The\nmetrics for the baseline models are presented in Table 1. In some baseline experiments, we found that\nthe zero-shot setting did not always ensure a standard output format. In such cases, we performed\npost-processing and excluded samples with predictions that did not fall within our options. The total\nnumber of excluded samples was fewer than 10, and this only occurred in the zero-shot baseline\nmodels. The results, as shown in Table 1, include a combination of different settings from two\naspects: varying perturbation ranges and the impact of shuffling letters and options in the training\ndata. Regarding the choice to shuffle, we observed that randomly mixing letters and options does not\nconsistently enhance performance. For instance, with a perturbation range of 0.1-0.9, the unshuffled\napproach often outperformed the shuffled version, while with a perturbation range of 0.3-0.7, shuffling\ngenerally resulted in inferior performance."}, {"title": "6 Discussion", "content": "In this section, we analyze the effect of different settings, including perturbation range and shuffling\noperations, on the generation of training data. A detailed heatmap of the model's predictions is\npresented in Supplementary Materials with Figure 6. The perturbation range determines the crop"}, {"title": "7 Limitation and Future Work", "content": "In this study, we focused specifically on guiding image reframing directions as a proof of concept.\nAlthough reframing is one of the most common needs when assisting visually impaired individuals,\nsome other aspects that impact the VQA process could also be explored, such as orientation, exposure,\nand focus. Our data augmentation framework can be extended to these aspects and generate training\ndata in a similar way, and we plan to explore this in future work. Second, the directional guidance\nhas been simplified to a classification task on directions, which may not fully capture the complexity\nof real-world scenarios. For example, effective reframing might require combining multiple direc-\ntions-such as moving both up and left\u2014or even zooming out. A more informative guidance in\npractice would also consider additional parameters like the magnitude of the reframing action. Those\ncomplexities can confuse the model, leading to inaccurate evaluations. To enhance clarity and reduce\nambiguity in our benchmark dataset, we included only cases that received consistent annotations\nfrom multiple annotators, which resulted in a limited size in our benchmark test set. Additionally,\nour preliminary experiments are designed to validate the effectiveness of our proposed framework\nrather than to maximize the model's performance. Consequently, the current method cannot fully\nguarantee the reliability of the model's prediction, and it still requires cautious deployment in high-\nrisk scenarios. Moving forward, we aim to refine the task design and data generation framework,\nadapting more effectively to complex, real-world applications. In addition, theoretically, our guidance\nframework can be extended to more general and quantitative scenarios. By simulating spatial drift,\nwe can customize the ratio of drift and produce synthetic training data with quantitative values. This\npotential extension would allow models to identify not just the direction but also the extent of camera\nmovement required. This makes such quantitative guidance particularly meaningful in applications\nsuch as robotics, where precise tracking of target objects is crucial, for example, in calculating the\nground truth for the extent of movement needed [16]. Furthermore, we expect our unsupervised data\ngeneration framework to alleviate the pressing data needs of studies exploring LLM or VLM for\nspatial or temporal reasoning tasks [6, 45, 23]."}, {"title": "8 Conclusion", "content": "In this paper, we introduced a novel task and benchmark dataset within the context of Visual Question\nAnswering (VQA) aimed at improving the self-knowledge of Vision-Language Models (VLMs).\nOur task specifically evaluates how well VLMs can assess the sufficiency of visual information and\ndetermine the necessary actions to reframe an image to obtain additional information. To address\nthe challenge of limited training data, we proposed an automated framework that generates synthetic\ndata by simulating unanswerable scenarios through perturbations applied to answerable cases. Our\nresults show that current high-performing VLMs, including LLaVA and GPT-40, struggle with this\ntask, revealing a gap in their ability to handle incomplete or ambiguous visual inputs. However, when\nf"}]}