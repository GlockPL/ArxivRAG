{"title": "COMMUNITY RESEARCH EARTH DIGITAL INTELLIGENCE TWIN\n(CREDIT)", "authors": ["John S. Schreck", "Yingkai Sha", "William Chapman", "Dhamma Kimpara", "Judith Berners", "Seth McGinnis", "Arnold Kazadi", "Negin Sobhani", "Ben Kirk", "David John Gagne II"], "abstract": "Recent advancements in artificial intelligence (AI) numerical weather prediction (NWP) have significantly transformed atmospheric modeling. AI NWP models outperform physics-based systems like the IFS on several global metrics while requiring far fewer computational resources. Despite these successes, existing AI NWP models face limitations related to their training datasets and timestep choices, often leading to artifacts that hinder model performance. To begin to address these challenges, we introduce the Community Research Earth Digital Intelligence Twin (CREDIT) framework, developed at NSF NCAR. CREDIT provides a flexible, scalable, and user-friendly platform for training and deploying AI-based atmospheric models on high-performance computing systems, offering an end-to-end pipeline for data preprocessing, model training, and evaluation that democratizes access to advanced AI NWP. We showcase CREDIT's capabilities on a new AI NWP model: WXFormer, a novel deterministic vision transformer designed to autoregressively predict atmospheric states while mitigating common AI NWP model pitfalls, such as compounding error growth, through techniques like spectral normalization, padding, and extensive multi-step training. To show the flexibility of CREDIT and to have a state-of-the-art model comparison we train the FUXI architecture within the CREDIT framework. Our results demonstrate that both FuXi and WXFormer, when trained on 6-hourly hybrid sigma-pressure level ERA5, generally outperform IFS HRES on 10-day forecasts, potentially offering significant improvements in efficiency and forecast accuracy. The modular nature of the CREDIT platform enables researchers to experiment with various models, datasets, and scaled training options, fostering collaboration and innovation in the scientific community.", "sections": [{"title": "Introduction", "content": "Rapid advancements in artificial intelligence (AI) numerical weather prediction (NWP) have shaken the foundations of\nthe meteorological community. Spurred by the release of the WeatherBench framework [1] on the ERA5 reanalysis\ndataset [2], multiple teams spanning motivated individuals [3], universities [4, 5], tech companies [6, 7, 8], non-profits\n[9], and government agencies [10] have developed a variety of AI NWP models that have quickly advanced and\nsurpassed the headline global verification scores of the ECMWF integrated forecast system (IFS) global model. In\naddition to improved verification scores, the AI NWP models require orders of magnitude fewer computational resources\nto run than conventional NWP. The combination of improved forecast performance at minimal cost opens the door for a\nflurry of new possibilities in how we can interact with NWP models, including much larger ensembles, more rapid\nupdates, and potentially improved forecast performance relative to traditional NWP. Unfortunately, these seemingly\nmajor advancements come with some caveats that have become more apparent once the meteorological community\nstarted meticulously investigating these AI models.\nDespite these significant advancements, however, a deeper look into the published AI NWP models reveals common\nlimitations, particularly regarding the data used for training. Most of these models, including the ECMWF IFS NWP\nmodel, rely on just five state variables (temperature, u-wind, v-wind, water vapor mixing ratio, and surface pressure),\nwhile all other variables are diagnosed from those or are updated solely within parameterizations. Additionally, the IFS\nvertical coordinate system uses hybrid sigma-pressure levels (i.e., model levels), which follow terrain near the surface\nand relax to pressure levels aloft. The WeatherBench ERA5 dataset uses pressure level data instead, which works well\naloft but intersects with terrain near the surface. ERA5 persists the surface values at pressure levels that are below\nterrain height with the exceptions of temperature and geopotential height, which are extrapolated [11].\nThe existing AI NWP models are still able to produce successful forecasts, but these data choices may be causing\nartifacts in the predictions that then have to be addressed by complex choices in terms of architecture, training procedure,\ntime-stepping, and post-processing. Additionally, most of the published AI NWP models use a 6-hour timestep, or\nin the case of Pangu-Weather [8], a mixture of models each with a different timestep to delay the accumulation of\nregression artifacts. Stepping forward the 1-hour Pangu-Weather model results in dramatic error growth after roughly\n12 hours, and the error curve with time does not follow the chaotic error growth pattern expected of physics-based\nmodels.\nOur approach aims to address some of the deficiencies of current AI NWP models through several key improvements.\nWe utilize a more fit-for-purpose training dataset that better represents the complexities of atmospheric dynamics.\nThis is complemented by a carefully selected set of input variables that capture essential meteorological processes.\nAdditionally, we employ a computationally efficient and scalable neural network architecture, adapted to handle the\nintricacies of weather and climate prediction across various temporal and spatial scales.\nCentral to our work is the Community Research Earth Digital Intelligence Twin (CREDIT) framework, developed by the\nMachine Intelligence Learning for Earth Systems (MILES) group at the NSF National Center for Atmospheric Research\n(NCAR). While other groups have released model weights and necessary code to run these models, CREDIT aims to\nprovide a comprehensive, end-to-end pipeline. It is designed to scale on standard HPC systems while remaining easily\naccessible and fully supported. It is fully supported on NSF NCAR's Derecho supercomputer, a globally recognized\nresource for atmospheric and climate research. This framework represents a significant step towards democratizing\naccess to weather and climate emulation technologies. By providing a robust, user-friendly platform that encompasses\nthe entire modeling process from data preprocessing to model training and evaluation\u2014CREDIT enables the broader\nscientific community, including researchers, educators, and enthusiasts, to engage with and contribute to advanced\natmospheric modeling without the typical barriers to entry.\nTo demonstrate the versatility and power of the CREDIT platform, we present two key components in this paper. Firstly,\nwe showcase CREDIT's ability to support and modify existing models from the literature, such as the FuXi model [5].\nThis capability allows researchers to build upon and refine established approaches within a standardized framework.\nSecondly, we introduce WXFormer, a new vision transformer model developed within the CREDIT framework.\nWXFormer is the latest advancement in deterministic AI-driven atmospheric modeling, specifically designed to\nautoregressively predict the state of the atmosphere at a selected time resolution. We present 6-hour intervals in this\nmanuscript as many other models currently do and comment on challenges involved with smaller time steps. WXFormer\nimplements several improvements to mitigate compounding error growth, such as spectral normalization of neural\nnetwork layers, and integrates physical knowledge into its datasets through static variables like solar radiation at the top\nof the atmosphere.\nWXFormer incorporates padding techniques to handle the spherical nature of Earth in its global weather simulations.\nThe model employs boundary padding along the map boundaries of [0\u00b0-360\u00b0] longitude and [-90\u00b0-90\u00b0] latitude,\naddressing the challenges posed by polar regions and the dateline. Additionally, spectral normalization is utilized\nto enhance model stability during training and inference. These design choices enable WXFormer to maintain data\ncontinuity and physical consistency across the entire globe, allowing for extended simulation periods.\nTo effectively handle the spherical nature of the Earth in its simulations, WXFormer employs circular padding along\nthe 0-360\u00b0 longitude line, wrapping data from one edge to the opposite edge to simulate periodic boundaries. This\nensures seamless transitions across the dateline. Additionally, a 180-degree shift is applied to align the data correctly\nat the poles before padding. The top rows from the North Pole are flipped upside down and added above the original\ndata, while the bottom rows from the South Pole are also flipped and added below. This method guarantees smooth"}, {"title": "What is CREDIT?", "content": "The CREDIT framework aims to provide a comprehensive research platform for developing and deploying AI-driven\nmodels for the earth system, here focused on the atmosphere. It is built on three core components: access to state-of-\nthe-art datasets, a library of advanced models, and an infrastructure designed for scalable training.\nCREDIT's approach to data management is a cornerstone of its functionality. The framework currently provides\nresearchers with curated, high-quality datasets crucial for training accurate atmospheric models, ERA5 and CONUS404\nas of this writing, with more additions planned. These datasets are preprocessed and formatted to be readily usable. This\nfeature significantly lowers the barrier to entry for researchers new to the field or those without extensive resources for\ndata acquisition and processing. Advanced users can customize a preprocessing framework within CREDIT. Moreover,\nCREDIT's data pipeline is designed to be extensible, allowing for the integration of new datasets as they become\navailable.\nThe framework's model library offers a diverse and growing collection of model architectures. This includes simpler\nfully convolutional-based models like U-Net and its derivatives, as well as state-of-the-art models such as FuXi, the\nSwin weather model [13], the Spherical Fourier Neural Operator (SFNO) [14], with its application in the AI2 Climate\nEmulator (ACE) model [15], among others.\nCREDIT provides a scalable training infrastructure that leverages a standard High-Performance Computing (HPC)\nsystem, Derecho, enabling researchers to take full advantage of available computational resources for training large-scale\nAI models. This framework facilitates end-to-end training software, providing users a template for efficiently training\ncomplex models across multiple GPUs without necessarily needing extensive expertise in parallel computing or HPC\nenvironments. By managing many of the intricacies of distributed training, CREDIT will enable scientists to focus on\ntheir research goals rather than technical details. Additionally, it supports the creation of customized training recipes,\nenhancing flexibility and adaptability in model training.\nThe framework offers a user-friendly interface and documentation aimed at making it accessible to a wide range of\nusers, from experienced climate scientists to students beginning their research journey. As we continue to develop\nand refine these resources, our long-term vision focuses on fostering community-driven development of CREDIT. We\naim to create an open and collaborative environment where researchers at various levels of expertise can contribute\nto the framework's evolution. This approach seeks to leverage collective expertise to enhance CREDIT's capabilities\nand ensure it addresses the diverse needs of the atmospheric science research community. As the user base grows, we\nanticipate that community feedback and contributions will play a crucial role in shaping its features, usability, and\noverall direction.\nBeyond its role as a software framework, CREDIT aspires to be an ecosystem that empowers researchers, educators,\nand enthusiasts to explore new frontiers in atmospheric, ocean, and land-surface modeling. By providing access to\ntop-tier resources, simplifying technical complexities, and lowering barriers to entry, it aims to accelerate research in\nAI-driven atmospheric science. This approach has the potential to contribute to advancements in our understanding of\nweather patterns and climate change, possibly playing a role in deepening our collective knowledge of atmospheric\nprocesses on both weather and climate scales."}, {"title": "Methods", "content": "WXFormer is a hybrid architecture developed at NCAR that consists of encoding stages using a CrossFormer backbone\n[12] and decoding stages with hierarchical transpose convolutional layers. Skip connections were assigned from\nencoders to decoders similar to that of U-net [17] having overall a pyramid structure. Its overall design is illustrated\nin Figure 1(a). WXFormer leverages the multi-scale feature processing and long-range dependency modeling of\nCrossFormer backbones, while incorporating efficient feature processing and detail preservation from the pyramid\nstructure. CrossFormer, the transformer basis of WXFormer, has demonstrated similar performance compared to other\ncomparable vision transformers such as Swin-Transformers, already used in AI-NWP approaches [5, 13] where it forms\nthe transformer foundation of the FuXi weather model [5]. Similar to models incorporating Swin, this architectural\nchoice potentially offers advantages in terms of feature representation and model efficiency. The model takes as input\nthe state of the atmosphere at times i and is tasked with predicting the state at i + 1 at a one-hour time step.\nThe Cross-scale Embedding Layer (CEL), depicted in Figure 1(b), forms the foundation of CrossFormer and hence\nWXFormer. CEL employs a multi-kernel approach, utilizing four convolutional kernels (4-by-4, 8-by-8, 16-by-16, and\n32-by-32) in the initial stage, all with a 4-by-4 stride [12]. This multi-scale sampling captures both fine-grained and\ncoarse-grained image features. To optimize computational efficiency, CEL implements a dimension allocation strategy,\nassigning fewer dimensions to larger kernels and more to smaller ones. For a 128-dimensional embedding, this might\ntranslate to 64 dimensions for 4-by-4, 32 for 8-by-8, and 16 each for 16-by-16 and 32-by-32 kernels.\nThe Long Short Distance Attention (LSDA) mechanism, crucial to WXFormer, is divided into Short Distance Attention\n(SDA) and Long Distance Attention (LDA), as shown in Figure 1(c). SDA operates on local GxG neighborhoods, while\nLDA facilitates long-range interactions by sampling embeddings at a fixed interval I. This approach bears similarities to\naxial attention, particularly in its global attention scheme [18]. The global attention is performed across the windowing\ndimension, reducing computational complexity in a manner analogous to axial attention. This dual attention mechanism\nenables efficient processing of both local and global contextual information, reducing computational complexity from\nO(S4) to O(S2G2), where S is input size and G << S.\nTo handle variable-sized inputs, WXFormer incorporates a Dynamic Position Bias (DPB) module. The DPB is a\nsignificant improvement over traditional fixed-size relative position bias matrices. It generates relative position bias\ndynamically using an MLP-based structure comprising three fully-connected layers. The input to this module is a\ntwo-dimensional vector (Ax, Ay) representing the coordinate distance between two embeddings. The first two layers\nof the DPB have a dimension of D/4, where D is the embedding dimension, and employ Layer Normalization followed\nby ReLU activation. The final layer reduces the output to a single scalar value, which serves as the position bias. This\ndynamic approach allows WXFormer to adapt to different image and group sizes without the limitations of fixed-size\nbias matrices. Moreover, the DPB is trainable and optimized end-to-end with the rest of the model, enabling it to learn\ncomplex positional relationships that may vary across different scales and regions of the input.\nThe principle decoder component of WXFormer, shown in Figure 1(d), incorporates convolutional upsampling blocks\nwith skip connections from corresponding transformer layers. Each block comprises a ConvTranspose2D layer followed\nby a Conv2D layer, with a residual connection between the ConvTranspose2D output and the final Conv2D output.\nThe ConvTranspose2D layer increases spatial dimensions of feature maps, while the Conv2D layer refines upsampled\nfeatures and adjusts channel counts. This architecture block progressively increases feature map resolution while\nmaintaining fine-grained spatial information, enabling accurate reconstruction of detailed outputs."}, {"title": "Baseline method: FuXi", "content": "FuXi, a state-of-the-art AI NWP model, was selected as the AI NWP model baseline. The implementation of FuXi\nbaseline follows its original design as in [5] but with reduced model sizes (hereafter, \u201cFuXi baseline\u201d or \u201cFuXi\u201d).\nThe modification of the FuXi baseline was driven by several considerations. First, the WXFormer design prioritized the\nbalance between model size and performance, resulting in a relatively small model. To ensure a fair comparison, it\nwas reasonable to re-scale FuXi to a size that is comparable to WXFormer. Second, using N320 regular Gaussian grid\nreduced the input tensor sizes, making it feasible to reduce the FuXi model sizes without downgrading its performance.\nDifferent from [5], which developed three cascaded FuXi models for 0-15 days of forecasts, this study develops only\none FuXi baseline for 0-10 days.\nThe re-scaling of the FuXi model was guided by [13], which explored the effectiveness of hyperparameters on the\ndesign of a Swin-Transformer-based AI NWP model. Based on [13], window size and patch size are the most important\nhyperparameters that can impact model performance; larger embedding dimensions are generally beneficial, and model\ndepth has the least impact. With these findings, we preserved the original window and patch sizes from FuXi, slightly\nreduced its embedding dimensions from 1536 to 1024, and largely reduced its depth from 48 to 16 Swin-Transformer\nblocks. Spectral normalization, boundary padding, and non-negative correction on specific humidity, as in WXFormer,\nwere added to the re-scaled FuXi baseline to improve its stability and performance.\nThe FuXi baseline serves as a proof of methodology. By benchmarking WXFormer against a successful AI NWP model,\nits overall level of effectiveness can be evaluated. In addition, the results and comparisons with the FuXi baseline will\ndemonstrate the efficacy of the CREDIT framework, which supports flexible re-scaling and training of state-of-the-art\nAI NWP models based on user demand."}, {"title": "Model training", "content": "The training of AI NWP models was carried out in two stages. The first stage is single-step pre-training, where models\nwere trained to predict the state of the atmosphere for the next time step. The second stage is multi-step fine-tuning,\nwhere the models were trained to predict multiple consecutive future states autoregressively, and the loss from each\nforecasted state was accumulated and used to optimize the model weights. Single-step pre-training provides the basic\ninformation for AI NWP models to understand the evolution of atmospheric variables, whereas multi-step fine-tuning\naims to enhance model performance and stability for longer forecast lead times, but may also introduce problems such\nas overly smoothed outputs and decreased ensemble spread (e.g., [21, 22]). The combination of the two stages has\nproven effective in many AI NWP model studies (e.g., [5, 7, 13, 23]).\nFor the single-step pre-training, cosine-annealing schedules were applied with a set of initial learning rates:\n{1E-3, 1E-4, 1E-5}. When the higher initial learning rate training is early-stopped based on the validation\nset performance, the next lower learning rate training will start until the 1E-5 scheduler is early-stopped or a total\nnumber of 70 epochs reached.\nThe multi-step fine-tuning was conducted on 06-48 forecast lead times (i.e., fixed 8 autoregressive steps for 6-hourly\nmodels, and fixed 24 steps for the hourly WXFormer) were accumulated to update the weights. The same cosine-\nannealing schedule was used with initial learning rates of {1E \u2013 4, 1\u0415 \u2013 5, 1\u0415 \u2013 6}. For 6-hourly models, a total\nnumber of 15 epochs was trained. This approach is similar to that of [13]. For 1-hourly WXFormer, due to resource\nconstraints, its 24-step fine-tuning was not conducted on full epochs, but 35 epochs with shuffled 100 batches per epoch.\nBoth stages used latitude-weighted mean squared error as loss functions, the AdamW optimizer [24], and batch sizes of\n32. The training was conducted on 32 NVIDIA A100 GPUs, each with 40 GB memory, using Pytorch [25]. Several\ntechnical strategies, including fully sharded data parallel [26] specifically focused on the attention layers in the models,"}, {"title": "Baseline method: IFS-HRES", "content": "The ECMWF Integrated Forecast System (IFS) features a high-resolution (HRES) configuration with 0.1\u00b0 grid spacing.\nIFS-HRES is recognized as the best global, operational, medium-range weather forecast. Its initial conditions were\nestimated every 6 hours using an ensemble 4D-Var system with forecast inputs from the previous assimilation cycle and\nobservations within a 3-hour time window. The IFS-HRES forecasts are operated four times per day. Its 00 and 12Z\ninitialization would go up to day 10, whereas the 06 and 18Z initializations are available for up to day 4.\nIn this study, the 00 and 12Z IFS HRES baselines were obtained from the Weatherbench 2 data archive [27], and\ninterpolated to the N320 regular Gaussian grid for verifications. IFS-HRES provides a comparable NWP baseline for\nAI NWP models that were initialized from and evaluated against ERA5. However, it is important to note that due to\nthe additional interpolation step and the ongoing improvements of IFS, the operational model is expected to be more\nskillful. Therefore, the IFS HRES verification results of this study should be interpreted with caution."}, {"title": "Verfication methods", "content": "Pre-trained 6-hourly models and hourly WXFormer are verified in a three-year period of 2019-2022 with ERA5\ninitializations 00 and 12 UTC daily. The 6-hourly models produced iterative forecasts for up to day-10 (i.e., 40 forecast\nsteps), whereas the hourly WXFormer produced iterative forecasts for up to day-5 (i.e., 120 forecast steps).\nVerification scores of Root Mean Square Error (RMSE) and Anomaly Correlation Coefficient (ACC) were calculated as\nin [27] and for every 6 hours, for both hourly WXFormer and 6-hourly models, up to day 10. Note that all verified\nvariables are instantaneous, which means only the 6-hourly ERA5 dataset and 6-hourly climatology are used.\nRMSE measures the average magnitude of the forecast errors. Given forecast F and the ERA5 verification target O,\nRMSE as functions of initialization time (ti) and forecast lead time (ti) is defined as follows:\n$RMSE(t_i, t_l) = \\sqrt{\\dfrac{1}{N_{\\phi}N_{\\lambda}}\\sum_{i=1}^{N_{\\phi}}\\sum_{j=1}^{N_{\\lambda}} {w(i) [F(t_i,t_l,i,j) \u2013 O(t_i + t_l, i, j)]^2}}$\nWhere N and N are the number of latitude ($) and longitude (\u5165) grid cells, respectively. w (i) = cos(i) is the\nlatitude weighting coefficient. ti + ti is the derivation of observational time based on initialization and forecast lead\ntimes. RMSE (ti, ti) was computed on target variables and bootstrapped on the ti dimension to produce verification\nresults as functions of t\u2081.\nACC is the Pearson correlation coefficient between the anomalies of F and O:\n$ACC(t_i, t_l) = \\dfrac{Cov [F' (t_i, t_l, i, j), O'(t_i + t_l, i, j)]}{\\sqrt{Var [F'(t_i,t_l, i, j)]\\sqrt{Var [O'(t_i + t_l, i, j)]}}$\nWhere F' = F \u2013 C and O' = O \u2013 C are anomalies computed from the ERA5 climatology (C). Cov and Var are the\nlatitude-weighted average of covariance and variance:"}, {"title": "Global energy spectrum", "content": "The verification of the global energy spectrum is computed using spherical harmonic transforms. For a given forecasted\nor analyzed field F ($, A), it can be represented using spherical harmonic functions Y (\u03c6, \u03bb) as orthonormal basis and\nspherical harmonic coefficients (a):\n$F (\\phi, \\lambda) = \\sum_{l=0}^{I_{max}}\\sum_{m=-l}^{l}a_l^m Y_l^m(\\phi, \\lambda)$\nWhere degree l represents the total angular frequency of Y. m is the zonal wave number. The energy spectrum of F at\na given m is the sum of magnitudes of a in all degrees with l > m:\n$P (m) = \\sum_{l>m} ||a_l^m||^2$\nThe kinetic energy (m\u00b2 \u00b7 s\u22122) and potential temperature energy (K2) spectrum on 500 hPa pressure level were computed\nand as functions of m, ti, and t\u0131. The result is averaged on t\u2081. Comparing P (m) on forecasts and the ERA5 target,\nthe ability of weather prediction models to represent the energy transfer across scales can be verified. In addition, the\nenergy spectrum provides a measure of the effective resolution of AI NWP models, which helps identify the smoothing\neffect of neural-network-based computations and model training."}, {"title": "Spatial correlation", "content": "Spatial correlation coefficients were computed for ERA5 and AI NWP model outputs on upper-air variables (Table 3.1)\nat all model levels.\nFor each ti and ti (or ti + ti for the ERA5) and model level, upper-air variables were flattened to one-dimensional\nvectors. Pearson correlation coefficients were then computed between variables on their flattened spatial locations.\nSpatial correlation coefficients on each ti were computed separately and averaged over the entire verification period to\nexamine the ability of AI NWP models to preserve the relationships between variables across levels and locations."}, {"title": "Results", "content": "In Figure 2, panels (a) through (f), illustrate the RMSE and ACC for various atmospheric variables for a 10-day forecast.\nWXFormer (orange) and FuXi (red) are compared against IFS (blue). WXFormer and FuXi demonstrate competitive or\nbetter performance compared to IFS at extended lead times, particularly for variables like 500 hPa u- and v-wind (a and\nb) and specific humidity (d), where they exhibit lower RMSE and comparable or better ACC, especially from 144 to\n240 hours. The only standout is geopotential height in which IFS does slightly better than both models until about the\nfive day mark when the trend reverses. Notably, while FuXi was not originally developed by us, we introduced spectral\nnormalization, boundary padding, and non-negative correction on specific humidity to it, resulting in a more efficient\nversion that does not require the cascading approach used in the original paper. These enhancements, along with the\nresidual loss mechanism in both WXFormer and FuXi, contribute to a stable and robust performance across lead times,\nallowing the models to handle complex atmospheric dynamics effectively."}, {"title": "Verification of energy spectrum", "content": "Figure 3 shows the model spectral energy over different lead times for kinetic energy (a-c) and potential temperature\nenergy (d-f) at 500 hPa. The energy spectra are plotted for forecast intervals of 18-24 hours, 114-120 hours, and 234-240\nhours, revealing how the models evolve during the forecast. Both WXFormer and FuXi show a loss of spectral resolution\nas lead time increases, particularly evident in (c) and (f). The spectral slopes smooth with increased forecast lead-time,\nindicating increased energy dissipation and a diminished ability to capture small-scale features. In contrast, IFS exhibits\na retains spectral energy consistent with the ERA5 product at smaller scales over longer forecasts, suggesting a more\neffective preservation of fine-scale atmospheric features throughout the forecast period."}, {"title": "Spatial correlation analysis", "content": "Figure 4 illustrates the spatial correlation between upper-air diagnostic variables in WXFormer, FuXi, and ERA5 for a\n120-hour forecast lead time. This analysis reveals that the AI-NWP models (WXFormer and FuXi) successfully retain\nthe correct and coherent spatial relationships between key atmospheric variables even five days into the forecast, despite\nthis coherence not being explicitly part of their training objectives.\nThe spatial correlation coefficients for both models generally exhibit the correct sign and magnitude in relation to\nconcurrent upper-air variables, with deviations from ERA5 typically below 5%. In panels (b) and (c), we observe that\nboth WXFormer and FuXi slightly overestimate the positive correlation in specific humidity within the mid and lower\ntroposphere, as well as the correlation between air temperature and specific humidity in the upper troposphere.\nHowever, some challenges remain. The models struggle to capture the spatial coherence of meridional winds in the mid\nand upper troposphere, and correlations between surface-level temperature and humidity with upper-level variables are\nless accurately represented. Notably, FuXi appears to capture the relationships at the tropopause level more accurately\nthan WXFormer, indicating slight model-specific variations in performance. The one-hourly model (e) at 120 struggles\nmore than the 6-hourly models. This should be expected as the relative forecast skill at this period is relatively lower\n(Figure A2). Across most variables we see a growing incoherence between forecast fields, though some fields like the\ntropospheric specific humidity are much better represented."}, {"title": "Case Study: Hurricane Laura (2020)", "content": "In addition to bulk verification statistics, CREDIT models are compared for the case of Hurricane Laura in 2020, which\nstruck western Louisiana as a category 4 storm. In Fig. 5, the 1000 hPa surface pressure contour is used as a proxy for\nboth track and intensity prediction with a larger contour indicating a more intense storm in each model. The 6-hour\nWXFormer model has the longest lead time on predicting the area of the 1000 hPa contour just before landfall with\ncorrect indications out to 120 hours (5 days). FuXi also had indications of the storm out to 5 days but with a weaker\nstorm at day 5. Both had similar track errors and placed the storm too far south until 48 hours prior to landfall. The 1\nhour WXFormer model had much lower track errors but had the storm much weaker prior to landfall. The IFS had track\nerrors in the opposite direction of the AI models."}, {"title": "Discussion", "content": "The experimental results showcase the CREDIT framework's capability as a platform for AI NWP models. By\nsuccessfully testing both an existing model (FuXi) and a new architecture (WXFormer), CREDIT demonstrates its\nability to accommodate proven models while facilitating the development of innovative approaches. This flexibility is\nvaluable in the field of weather prediction, where both building on established methods and exploring new techniques\ncontribute to performance advancements.\nThe modification of FuXi within CREDIT highlights the potential advantages of working within a standardized\nframework. The addition of spectral normalization, padding (see supplemental), and non-negative correction on specific\nhumidity simplified the original cascading architecture while maintaining or improving performance. This demonstrates\nhow a well-designed framework can facilitate architectural improvements that might otherwise be challenging to\nimplement and validate. The ability to rapidly test such modifications while maintaining reproducibility could prove\nbeneficial for the research community.\nSpectral analysis reveals that both AI models produce smoother forecasts over time, with reduced energy at smaller\nscales compared to IFS, while still maintaining competitive or better performance metrics. This finding suggests that"}, {"title": "Insights from 6-hour model performance", "content": "The trade-off between spectral fidelity and forecast accuracy revealed in this study is particularly intriguing. WXFormer\nand FuXi's loss of spectral power at longer lead times, resulting in smoother fields, may help prevent the propagation\nof small-scale errors, ultimately allowing for a more accurate representation of large-scale atmospheric features.\nConversely, IFS, while producing more detailed forecasts, may include fine-scale inaccuracies that diverge from the\ntrue state of the atmosphere as forecast time progresses. This observation indicates that maintaining spectral fidelity\nis not always beneficial for long-term forecast accuracy, and highlights the need for a balanced approach in model\ndevelopment.\nThe impact of model architecture choices becomes apparent in the treatment of boundary conditions. The padding\napproach helped to address some of the artifact generation found at the polar and date-line regions, which are areas where\nmany previous AI NWP models have faced difficulties. For models trained on a rectangular lat-lon grid, this approach\noffers a potential solution to the challenge of handling these geographic boundaries in global weather prediction. The\nimplementation of this approach within CREDIT suggests that standardized solutions to common technical challenges\ncould benefit researchers working in this field."}, {"title": "Challenges of hourly AI forecasts", "content": "In this study, we developed and trained the hourly WXFormer model using ERA5 data. However, its deterministic\nverification results did not reach the performance level of the 6-hourly models. Through our analysis, we identified\nseveral factors that may have contributed to the reduced accuracy in the hourly forecasts. One major takeaway from this\nwork is the significant technical and resource challenges involved in moving from 6-hour to 1-hour timesteps.\nHigh-frequency wave patterns emerged near the tropics and Southern Ocean in our day-2 forecasts and persisted\nthroughout the 10-day forecast period (see Figure A3 for an example). Additional anomalies were observed closer to\nthe poles, including nonphysical spatial variations in some variables. The wave patterns were particularly pronounced\nin near-surface variables, such as 2-m air temperature and surface pressure, while their impact on 500 hPa geopotential\nheight was muted.\nWe intuit that the high-frequency waves and the small relative motion of the propagating patterns lead to resonance\nfrequencies that are easily amplified by the model's learned dynamics. In contrast, the 6-hourly models benefit from\ninherent smoothing due to temporal averaging, which naturally mutes these waves. One attempted solution was to train\nthe hourly model over a longer forecast roll-out period; however, this approach is computationally costly, introduces\nspatial smoothing (see Supplemental Figure A2), and negates some of the purpose of producing high-frequency output.\nInitial experiments with running hourly predictions using 6-hourly pre-trained model weights showed promising results,\nwith no high-frequency wave or other obvious anomalous patterns in the outputs. This suggests that the error pattern is\nunique to hourly models and appears unrelated to the model design. Transfer learning from 6-hourly models could\npotentially be a viable solution.\nWe also experimented with different multi-step fine-tuning strategies, finding that while many of the models exhibited\nthis high-frequency pattern, one model trained out to 120 hours with a modified multi-step loss to speed up the training,\nshowed fewer wave patterns upon visual inspection. However, this model remained extremely cumbersome to train and\ndid not achieve better overall performance than our primary hourly model shown here. While this suggests that the\nwave patterns might be addressable through training strategies, the computational costs and performance trade-offs\nrequire further investigation.\nLastly, we identified several technical operations that helped reduce the impact of high-frequency waves. Pole filtering\nand 2nd-order Laplacian-based diffusion filtering helps to dampen high-frequency waves by smoothing the model\nfield. However, choosing when to apply the filter and to which variables remains an open question. Additionally,\nwhile boundary padding and non-negative corrections did not directly reduce high-frequency waves, they nonetheless\ncontributed to improved deterministic verification scores for the hourly WXFormer model.\nFuture studies will be conducted to address the challenges above. The authors believe that CREDIT can be a powerful\nframework for investigating the error characteristics and forming systematic solutions of hourly AI forecasts."}, {"title": ""}]}