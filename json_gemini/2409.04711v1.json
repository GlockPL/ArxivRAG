{"title": "ALGORITHMIC SCENARIO GENERATION\nAS QUALITY DIVERSITY OPTIMIZATION", "authors": ["Stefanos Nikolaidis"], "abstract": "Abstract-The increasing complexity of robots and au-\ntonomous agents that interact with people highlights the\ncritical need for approaches that systematically test them\nbefore deployment. This review paper presents a general\nframework for solving this problem, describes the insights\nthat we have gained from working on each component of\nthe framework, and shows how integrating these components\nleads to the discovery of a diverse range of realistic and\nchallenging scenarios that reveal previously unknown failures\nin deployed robotic systems interacting with people.", "sections": [{"title": "I. INTRODUCTION", "content": "Consider a robot arm that collaborates with people\nin meal preparation tasks, such as steaming vegetables.\nAfter steaming, the user empties the boiling water into\nthe nearby sink. The robot then places the pot back on\nthe stove. We test the system with multiple participants\nin a user study and confirm that the human-robot team\nperformed the task consistently well. We then deploy the\nrobot at home, where the stove is far away from the sink.\nAt one point during long-term deployment, the user gets\ndistracted and forgets to empty the water (Fig. 1). Unaware\nof that, the robot picks up the pot with the boiling water,\nbut as it extends to reach the stove, it reaches its torque\nlimits and drops the pot.\nThe complexity of interactions like this highlights the\nneed for methods to systematically test novel algorithms\nand applications. Testing autonomous agents and robotic\nsystems with actual users has limitations in the number\nof environments and user behaviors that can be covered.\nExhaustive testing of every possible scenario in simula-\ntion is also computationally prohibitive. This highlights a\ncritical need for generating scenarios in simulation that\nreveal undesirable behaviors.\nThis is a very challenging problem. It requires us to\niteratively search the vast, continuous space of scenarios,\ngenerate realistic scenarios that are hard to find but may\nactually occur in the real world, and evaluate them by\nsimulating agent and human behaviors.\nWe address this research question by formulating the\nalgorithmic scenario generation problem as a quality\ndiversity optimization problem and proposing a general\nframework to solve it. This review paper highlights the\ninsights that we have gained from our work on each com-\nponent of the framework, building on previous advances\non evolution strategies, generative modeling, surrogate\nmodels, and randomized algorithms. We demonstrate how\nintegrating these components leads to the discovery of\ndiverse, realistic and challenging edge-case failures in\nhuman-robot interaction. Finally, we describe open sci-\nentific challenges that pave the way for future research."}, {"title": "II. PROBLEM FORMULATION", "content": "Quality Diversity Optimization. Quality diversity (QD)\nalgorithms [1] differ from pure optimization methods,\nin that they do not attempt to find a single optimal\nsolution, but a collection of good solutions that differ\nacross specified dimensions of interest, using existing\nsolutions as \"stepping stones\" to generate higher quality\nand more diverse solutions.\nFormally, the QD problem consists of an objective\nfunction $f: R^n \\rightarrow R^+$ that maps n-dimensional solution\nparameters to the scalar value representing the quality\nof the solution and k measure functions, also commonly\nreferred to as behavior descriptors, $m_i: R^n \\rightarrow R$ or, as a\nvector function, $m: R^n \\rightarrow R^k$ that quantify the behavior\nor attributes of each solution. The range of m forms a\nmeasure space $S = m(R^n)$. The QD objective is to find a\nset of solutions $\\theta \\in R^n$, such that $m(\\theta) = s$ for each $s$ in $S$\nand $f(\\theta)$ is maximized.\nThe measure space S is continuous, but solving algo-\nrithms need to produce a finite collection of solutions.\nTherefore, QD algorithms in the MAP-Elites [2], [3] family\nrelax the QD objective by tessellating the space S into M\ncells that form an archive A. Each solution $\\theta_i$ is mapped\nto a cell in the archive based on its measure values $m(\\theta_i)$.\nThe QD objective becomes to find a solution $\\theta_i$ for each\nof the i\u2208 {1,..., M} cells and to maximize the objective\nvalue $f(\\theta_i)$ of all cells:\n$\\max J = \\sum_{i=1}^M f(\\theta_i)$\nAlgorithmic Scenario Generation. In algorithmic scenario\ngeneration, we wish to test and analyze the performance\nof one or more autonomous agents performing a task\nby executing a fixed policy. The agents interact with the\nenvironment and possibly with other agents or humans.\nThese interactions are governed by a set of scenario\nparameters $\\theta$ that characterize the environment and the\npolicies of the other agents or humans.\nWe simulate the actions of all agents until task comple-\ntion or a time limit. The evaluation returns measures $m(\\theta)$\nand a quality metric $f(\\theta)$. Our objective is to find solutions\n$\\theta$ that fill in as many cells of the archive as possible\nwith policies of high quality f. The measures m represent\naspects of scenarios for which we wish to have diversity,\nsuch as scene clutter and human rationality. The objective\nf represents the quality of the generated scenario. For\ninstance, it can be the negative of a task performance\nmetric when the goal is to identify failure scenarios, or\nthe task performance metric itself when the aim is to find\nscenarios where the agents perform optimally.\nUsing the scenario of Fig. 1 as example, the robot\nexecutes a fixed policy and the scenario parameters $\\theta$\ninclude the object positions in the environment and the\npolicy parameters of the simulated human. We assume as\nmeasure $m_1$ the user's observed rationality level, as $m_2$ the\ndistance between the pot and the stove in the scene, and\nas objective f the negative of task performance. Evaluating\nthe scenario results in low $m_1$ and high $m_2$ values. It\nalso results in a high f value, because the team failed to\nsuccessfully complete the task. The scenario will occupy\na cell in the archive A with coordinates ($m_1$, $m_2$)."}, {"title": "III. GENERAL FRAMEWORK", "content": "Our objective, as described in section II, is to populate\nan archive with high-quality scenarios that are diverse\nwith respect to the specified measures of interest. Our\nframework consists of the following steps (Fig. 3):\n\u2022 Sample scenario parameters.\nGenerate scenarios in a simulator\nEvaluate the generated scenarios.\n\u2022 Update the archive with the new scenarios.\nWe iterate the steps above for a maximum number of\niterations or until we achieve a desired archive quality and\ncoverage.\nA naive approach would be to uniformly sample sce-\nnario parameters. In our running example of Fig. 1, these\nwould include the positions and the types of all objects\nat the scene and the parameters of the human policy.\nThe next step would involve generating the scenario in\na robotic simulator, executing a human and robot policy\nrollout and adding the new scenario to the archive. Un-\nfortunately, this approach presents multiple challenges.\nMeasure Space Coverage. First, uniformly sampling the\nhigh dimensional space of scenario parameters is unlikely\nto result in coverage of the lower-dimensional measure\nspace. To provide some intuition, assume as measure"}, {"title": "IV. SEARCH SCENARIO PARAMETERS", "content": "We describe how to efficiently search the continuous,\nmulti-dimensional space of scenario parameters using QD\noptimization.\nMAP-Elites. A popular QD algorithm, MAP-Elites [2],\n[3], generates an archive A of high-performing solutions\n(scenario parameters) by retaining the best performing\nsolution in each cell of the measure space. MAP-Elites\nfirst initializes the archive by sampling solutions from a\nGaussian distribution. It then uniformly selects solutions\n$\\theta$ from the archive of solutions and perturbs them with\nisotropic Gaussian noise:\n$\\theta' = \\theta_i + \\sigma N(0, I)$\nMAP-Elites evaluates each new solution and returns an\nobjective value $f(\\theta')$ and measures $m(\\theta')$. The algorithm\nthen interacts with the archive by first mapping the\nsolution $\\theta'$ to a cell e in the archive A (Fig. 4). It then\ncompares the objective value $f(\\theta')$ of the new solution\nto a function $f_A(\\theta')$ equal to the objective value of the\nincumbent solution in that cell, i.e., $f_A(\\theta') = f(\\theta_e)$. If\n$\\theta'$ has higher quality than $\\theta_e$, i.e., $f(\\theta') > f_A(\\theta')$, MAP-\nElites replaces $\\theta_e$ with $\\theta'$. If cell e is empty, we assume\n$f_A(\\theta') = 0$ and $\\theta'$ is added to the archive. This way, MAP-\nElites retains an elitist archive with each cell containing\nthe highest performing solution found so far for that cell.\nThe intuition behind MAP-Elites' iterative process is that\nthe elites act as stepping stones to generate new, high-\nperforming solutions in other parts of the archive."}, {"title": "V. GENERATE SCENARIOS", "content": "Generating scenarios based on the sampled parameters\noften results in scenarios that are invalid or unrealistic. In\nour running example of Fig. 1, all sampled positions of\nthe pot should be reachable by the robot. Furthermore,\nthere should be no decorative objects placed on top of or\nunder the pot.\nLatent Space Illumination. The use of generative mod-\nels [9] has enabled the efficient generation of realistic\ncontent that matches the style of a training distribution.\nFurthermore, navigating the latent space of these models\nallows us to \"steer\" the output in a given direction. Our\ninsight is that we can search with QD the latent space of\ngenerative models trained with human-authored examples\nto generate diverse scenarios that match the style of\nreal-world examples. We call this approach latent space\nillumination [10]. Fig. 5 shows an example application,\nwhere we search with CMA-ME the latent space of a GAN"}, {"title": "VI. EVALUATE SCENARIOS", "content": "Once the scenarios are generated, we evaluate them by\nexecuting the agent policies and computing the objective\nf and measures m. Given the stochasticity of the policies\nand of the environment in the general case, evaluation\nrequires multiple trials, which in a game engine or a\nrobotics simulator can be time-consuming.\nHowever, in the beginning of the QD search we often\ndo not need exact computations of the objective and\nmeasures, since the first solutions typically act as stepping\nstones, to be replaced by better solutions later on. If\nwe could instead approximate the objective and measure\nvalues in the beginning of the search, we would avoid the\nexpensive ground-truth evaluations.\nOne idea would be to train deep neural networks\nin a supervised learning manner so that they act as"}, {"title": "VII. UPDATE ARCHIVE", "content": "The final component of the framework is to add the new\nscenarios to the archive. A key observation is that how\nsolutions are added to the archive has a significant effect\non the performance of the search, since QD algorithms\ninteract with the evolving archive throughout the search.\nWe recall that CMA-ME (section IV) determines a search\ndirection by sampling solutions from a multi-variate Gaus-\nsian and ranking the sampled solutions based on archive\nimprovement. The archive improvement of a sampled\nsolution $\\theta'$ is computed as $\\Delta = f(\\theta') - f_A(\\theta')$. As we\ndescribe in section IV, in elitist archives $f_A (\\theta') = f(\\theta_e)$ with\n$\\theta_e$ the best performing solution found at cell e. CMA-ME\nthen moves in the direction of higher-ranked solutions.\nElitist Archive. We provide an example of how CMA-\nME can behave when interacting with an elitist archive.\nAssume that CMA-ME samples a new solution $\\theta'$ with a\nhigh objective value of $f(\\theta') = 99$. If the current occupant\n$\\theta_e$ of the corresponding cell has a low objective value\nof $f(\\theta) = 0.3$, then the improvement in the archive $\\Delta =$\n$f(\\theta') - f(\\theta) = 98.7$ is high and the search will move\ntowards $\\theta'$. Now, assume that in the next iteration the\nalgorithm discovers a new solution $\\theta''$ with objective value\n$f (\\theta'') = 100$ that maps to the same cell as $\\theta'$. The improve-\nment then is $\\Delta = f(\\theta'') - f(\\theta') = 1$ as $\\theta'$ replaced $\\theta_e$ in\nthe archive in the previous iteration. CMA-ME would likely\nmove away from $\\theta''$ because the solution resulted in low\nimprovement. In contrast, the single-objective optimizer\nCMA-ES would move towards $\\theta''$ because it ranks only by\nthe objective f, ignoring previously discovered solutions\nwith similar measure values.\nIn the above example, CMA-ME moves away from\na high performing region in order to maximize how\nthe archive changes. However, in domains with hard-to-\noptimize objective functions, it is beneficial to perform\nmore optimization steps towards the objective f before\nleaving each high-performing region.\nSoft Archive. We present a new algorithm, CMA-MAE [17],\nthat addresses this limitation. Like CMA-ME, CMA-MAE\nmaintains a discount function $f_A(\\theta')$ and ranks solutions\nby improvement $f(\\theta') - f_A(\\theta')$. However, instead of main-\ntaining an elitist archive by setting $f_A(\\theta')$ equal to $f(\\theta_e)$,\nwe maintain a soft archive by setting $f_A(\\theta')$ equal to $t_e$,\nwhere $t_e$ is an acceptance threshold maintained for each\ncell in the archive (Fig. 9). When adding a candidate\nsolution $\\theta'$ to the archive, we control the rate that $t_e$\nchanges by the archive learning rate $\\alpha$ as follows: $t_e$\n$\\leftarrow (1 - \\alpha) t_e + \\alpha f (\\theta').$\nThe archive learning rate $\\alpha$ in CMA-MAE allows us\nto control how quickly we leave a high-performing re-\ngion of measure space. For example, consider discovering\nsolutions in the same cell with objective value 100 in\n5 consecutive iterations. The improvement values com-\nputed by CMA-ME against the elitist archive would be\n100,0,0,0,0, thus CMA-ME would move rapidly away from"}, {"title": "VIII. CASE STUDY", "content": "The previous sections have discussed how to efficiently\nsearch the continuous space of scenario parameters (sec-\ntion IV), how to generate realistic and valid scenarios\n(section V), how to efficiently evaluate the generated\nscenarios (section VI) and how to update the archive of\nscenarios to enable the search algorithm to focus on high-\nperforming solutions (section VII). Here we discuss how\nwe can leverage our contributions in all the components\nof the framework to find diverse failure scenarios in\nhuman-robot interaction.\nWe consider two different domains: (1) A shared\nworkspace collaboration task, where a user attaches a\nlabel on one of three packages while the robot presses a\nstamp (Fig. 11). The robot infers the human's current goal\nand moves to a different goal along the shortest path. (2) A\nshared control teleoperation task, where the user provides\ninputs to the robot through a joystick interface. The robot\nuses the inputs to infer the human goal and moves along\nthe shortest path to that goal. In both domains, the team\nattempts to complete the task as quickly as possible.\nWe parameterize the scenario with the coordinates of\nthe goal objects and the parameters of the human model,\nsuch as the type of each human action and the speed of\nthe human movements. Our objective is to find scenarios\nthat minimize the performance of the team. We select\nas measures of diversity factors that affect the type of\nscenarios that we wish to explore; for instance, by select-\ning as measure the maximum probability assigned by the"}, {"title": "IX. LOOKING AHEAD", "content": "We have presented a general framework for algorithmic\nscenario generation. We have shown how combining the\narchiving properties of MAP-Elites with the ranking and\nselection properties of CMA-ES enables us to efficiently\nexplore the vast, continuous space of scenarios. Integrat-\ning our QD algorithms with generative models trained\nwith human-authored examples allows us to generate\nvalid, realistic scenarios. Using deep neural networks as\nsurrogate models for evaluation improves sample effi-\nciency. Finally, introducing an acceptance threshold into\neach cell of the archive guides the search towards high-\nperforming solutions and helps it escape flat objective\nregions. Our framework can find rare, previously unknown\nfailures in human-robot interaction domains.\nOur work has a number of limitations that suggest\npotential future directions.\nScenario Complexity. As we increase the complexity of\nthe scenes and the number of agents acting in the\nenvironment, the number of scenario parameters will\nalso increase. Our QD algorithms are based on CMA-ES,\nwhich has quadratic time complexity to the number of\nparameters. CMA-ES's complexity arises from its modeling\nof the search distribution with a Gaussian that has a full\nrank covariance matrix. This naturally limits the maximum\nnumber of scenario parameters.\nWe have explored CMA-MAE variants that approximate\nthe covariance matrix with a low-dimensional represen-\ntation or with a diagonal matrix [19]. We have found\nthat in many domains these variants achieve performance\nclose to that of CMA-MAE, with orders of magnitude\nimprovements in computational complexity.\nAlternatively, if the objective and measure gradients\nare available or can be approximated, DQD algorithms\nsuch as CMA-MAEGA retain a covariance matrix in the\nmuch lower-dimensional space of the objective-measure\ngradient coefficients (Eq. 4), instead of the space of sce-\nnario parameters, and do not suffer from scenario space\nscalability limitations. We are excited about future work\nthat leverages these insights to improve the complexity of\nthe generated scenarios.\nArchive Size. We have discussed that tessellating the\nmeasure space results in an archive of solutions, which\nconsists of uniformly spaced grid cells. The number of\ncells thus increases exponentially to the number of mea-\nsure functions, leading to a \"curse of dimensionality.\"\nOne approach would be to specify first the number of\ndesired cells and then maximally spread them in the mea-\nsure space, e.g., by using a centroidal Voronoi tessellation\n(CVT) of that space [20]. However, this would significantly\nincrease the volume of each cell with the number of\ndimensions. In turn, this can cause the solutions sampled\nby the QD algorithm to all land in the same cell, heavily\nimpacting its ability to explore new cells.\nRather than relying on measure space tessellations, we\nhave proposed using continuous density estimation of\nthe measure space [21]. In the special case where we\nfocus only on diversity (diversity optimization) and ignore\nsolution quality, we have observed notable performance\nin high-dimensional measure spaces. We look forward to\nfuture work that uses continuous density estimation to\nsolve the quality diversity problem.\nScene Realism. While we have applied latent space illu-\nmination on GANs to generate diverse, realistic scenes,\ndiffusion models [22] have emerged as a current state-of-\nthe-art approach in generative modeling, considering their\ncapability to generate high-resolution, complex scenes\nthat capture the natural variations found in real-world\nscenes. A recent tutorial [23] that was introduced in\nour open-source QD library pyribs [24] describes the\nintegration of QD algorithms with diffusion models. As\nmore powerful generative models continue to emerge, we\nanticipate illuminating the latent space of these models to\nfurther enhance the realism of the generated scenarios.\nHuman Model Realism. We have shown how integrat-\ning QD algorithms with generative models of environ-\nments can result in realistic environments. We expect\nthese insights to transfer to the simulation of realistic\nhuman behaviors. For instance, in our past work [25],\nwe have modeled human decision making using learned\nlow-dimensional representations of human strategies and\nTheory-of-Mind models. We believe that learning compact\nrepresentations of human states and actions and search-\ning with QD these representations holds much promise.\nRobustness. Ultimately, the purpose of generating failure\nscenarios is to enhance the robustness of deployed robotic\nsystems. We have ourselves leveraged the generated failure\nscenarios in the shared control teleoperation example\n(Fig. 13) to refine the design of the robot's cost function\nand prevent it from failing [26].\nBeyond manual debugging, the generated archive of\nscenarios serves as a diverse, high-quality dataset that\ncould provide a foundation for training learning algo-\nrithms. We hypothesize that leveraging the structure of the\ndataset can significantly improve the learning efficiency.\nConclusion. While we have presented single and multi-\nagent games, as well as human-robot collaborative tasks\nas application domains, our algorithmic scenario gener-\nation framework is general; for instance, recent works\nhave algorithmically generated diverse prompts for red-\nteaming LLMs [27] and diverse safety-critical scenarios\nfor autonomous cars [28]. We expect our findings to have\nsignificant impact in other domains as well.\nOverall, the presented work reflects our long-term vision\nthat deployed autonomous agents and robotic systems can\ncontinuously improve their robustness through simulated\nand real-world experiences. We look forward to continue\naddressing the exciting scientific challenges in this area."}]}