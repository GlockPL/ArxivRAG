{"title": "Leveraging Large Language Models for Enhancing the Understandability of Generated Unit Tests", "authors": ["Amirhossein Deljouyi", "Roham Koohestani", "Maliheh Izadi", "Andy Zaidman"], "abstract": "Automated unit test generators, particularly search-based software testing tools like EvoSuite, are capable of generating tests with high coverage. Although these generators alleviate the burden of writing unit tests, they often pose challenges for software engineers in terms of understanding the generated tests. To address this, we introduce UTGen, which combines search-based software testing and large language models to enhance the understandability of automatically generated test cases. We achieve this enhancement through contextualizing test data, improving identifier naming, and adding descriptive comments. Through a controlled experiment with 32 participants from both academia and industry, we investigate how the understandability of unit tests affects a software engineer's ability to perform bug-fixing tasks. We selected bug-fixing to simulate a real-world scenario that emphasizes the importance of understandable test cases. We observe that participants working on assignments with UTGen test cases fix up to 33% more bugs and use up to 20% less time when compared to baseline test cases. From the post-test questionnaire, we gathered that participants found that enhanced test names, test data, and variable names improved their bug-fixing process.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's software-dominated world, software reliability and correctness are very important [1]. Consequently, auto-mated testing in the form of unit tests has become a crucial element for software engineers in ensuring high-quality software [2]\u2013[4]. Despite the widely acknowledged importance of testing, writing tests is tedious and time-consuming [5]\u2013[8]. To alleviate this burden on developers and testers, the research community has devoted considerable effort on investigating automatic test generation approaches [9]\u2013[14]. Among the notable test generators are Randoop [15] and EvoSuite [11]. EvoSuite, for example, is a search-based test generator that employs genetic algorithms to construct a test suite [16] and has demonstrated good results in terms of coverage [17], [18]. However, based on insights obtained through industrial case studies, there are limitations in terms of the quality of the generated test cases [19]\u2013[25]. One critical limitation revolves around the understandability of generated test cases, which involves various aspects such as meaningful test data, proper assertions, well-defined mock objects, descriptive identifiers and test names, as well as informative comments. Additionally, the difficulty in following the scenario depicted in the test case and the ambiguity surrounding test data significantly hamper clarity [25], [26].\nTo address these issues, we aim to enhance automatically generated test cases by focusing on contextual test data, clear test method and identifier names, and adding descriptive comments. In this study, we investigate the synergy of Search-Based Software Testing (SBST) and Large Language Models (LLMs). While Natural Language Processing (NLP) techniques have shown promise in text generation and optimization [27], [28], and LLMs have advanced text-based capabilities [29]\u2013[33], their impact in generating high-coverage test cases for complex systems remains limited [34], [35]. Conversely, SBST, while effective in coverage, often falls short in test case understandability.\nOur approach, UTGen, integrates an LLM into the SBST test generation process. We hypothesize that this combined approach can leverage the strengths of both techniques to generate effective and understandable test cases. Our study is steered by three Research Questions (RQs) that consider the effectiveness of the UTGen approach, and the understand-ability of the generated test cases.\nRQ1 Does UTGen have the capability to generate effective unit tests by utilizing a combination of LLMs and SBST? The investigation into the effectiveness of the approach seeks to establish whether the non-determinism of both the SBST and LLM components impact the ability to generate compilable and high-coverage unit tests.\nRQ2 What is the impact of LLM-improved unit tests' understandability on the efficiency of bug fixing by developers? When it comes to the understandability of generated test cases, we intend to measure understandability through the ease by which software engineers can perform bug-fixing tasks involving failing test cases, a setup previously used by Panichella et al. [36]."}, {"title": "II. BACKGROUND", "content": "Automated test generation approaches have been developed in order to reduce testing effort. Tools like EvoSuite [11] and Randoop [15] generate a test suite starting from Java source code using a search-based or random approach [17], [18]. Several studies have uncovered challenges involving automat-ically generated tests [20]\u2013[25], an important one being that generated tests are typically less readable than their human-written counterparts [38]. In this context, Almasi et al. [25] have observed that developers 1) find the test case scenario difficult to follow, 2) find the test data unclear, and 3) have difficulties with the meaningfulness of generated assertions.\nLarge Language Models are a subset of AI systems predom-inantly based upon the transformer architecture [39]. These LLMs are trained on vast amounts of data, through which, they learn the underlying patterns inherent in texts, code, dialogue, etc., and are therefore capable of generating a (somewhat) relevant response given a prompt by the user [40]. LLMs operate based on predicting the subsequent tokens in a sequence and reusing the extended sequence by running it through the model once again to predict the tokens to follow (referred to as autoregression). This process is continued up until a point in which either the maximum amount of required tokens is reached or a termination character is generated."}, {"title": "III. THE UTGEN APPROACH", "content": "Figure 2 provides an overview of our approach, named UTGen. The core of our framework is a search-based ap-proach in which we integrated an LLM in various stages of the test generation process (highlighted in green). We use EvoSuite [11] as the search-based test generation framework of choice, and we have developed additional functionalities that facilitate the integration of EvoSuite with LLMs (highlighted in blue).\nThe aim of our approach, UTGen, is to enhance the under-standability of test cases by improving four key elements of generated tests: 1) providing context-rich test data, 2) incor-porating informative comments, 3) using descriptive variable names, and 4) picking meaningful test names. These goals define the stages in our approach.\nAs a first step, after the genetic algorithm has ended and the test cases mature in the search-based process, our approach focuses on refining test data (\u2460 in Figure 2). UTGen uses an LLM to generate contextually relevant test data, unlike traditional search-based methods that often rely on random values. Following this refinement, the search process ends, and we transition to post-processing tasks. Here, EvoSuite minimizes the number of test cases in the test suite, shortens the length of individual tests, and adds assertions.\nOnce the test cases are fully formed, at stage \u2461, UTGen leverages an LLM to add descriptive comments and enhance variable names. In stage \u2462, UTGen uses an LLM to suggest suitable names for the tests, reflecting the assertions and logic within. Finally, to ensure that test cases are compilable and stable after these enhancements, UTGen compiles them (stage \u2463), and in case of compilation issues, the process iteratively revisits stage 2 for adjustments.\nWe first explain the prompt engineering component and then describe our test generation process per stage."}, {"title": "A. Prompt Generation", "content": "The prompt component of UTGen uses the code-llama:7b-instruct model from Meta [44] as provided by Ollama\u00b9. We have designed UTGen in such a way, that the Code-llama can easily be exchanged for another LLM. There are three stages within the UTGen approach uses the prompt component: 1) the refinement of test data, 2) the post-processing of tests, and 3) the naming of tests. The general prompt component contains two distinct parts, namely \u03b1 which is responsible for generating the prompts provided to the LLM, and \u03b2 which manages the request and ensures the correctness of the returned response.\nFor each stage, we devised specialized prompts following guidelines from recent prompt engineering research [50]\u2013[52]. As shown in Figure 3, these guidelines emphasize the follow-ing: writing clear instructions with action words (as in \u2461), adopting a persona for the model (as in \u2460), allowing sufficient processing time through techniques like Chain of Thought (CoT) (as in \u2462), standardizing input and output formats, and framing requests in a positive manner (as in \u2463). The starting point for each prompt resembles the one presented in Figure 3. As each model has its complexities, pitfalls, and preferred input format, no one-size-fits-all solution exists to prompt engineering, however, the guidelines set out above have guided us. We have followed an iterative prompt engineering process in which each adjustment of the prompt was deliberated upon, before being accepted or rejected by the authors based on potential improvements in the results. An emerging pattern that we initially observed is that LLMs are incapable of always adhering to the output format described for them. Therefore, we put guidelines in place to deal with such mismatches; as an example, we had to deal with cases where plain text was placed inside the code blocks, or when the intended delineation was not used by the LLM. Our replication package contains the final versions of the prompts that we engineered, in addition to other measures that were taken [37]."}, {"title": "B. Stage 1: Test Data Refinement", "content": "In this stage, we focus on requesting contextualized test data from the LLM to increase the domain relevance of test data for a test scenario. We designed a parser that converts the LLM's responses into the structured format required by EvoSuite.\nThe test data refinement stage should be considered as another iteration in the search process in which both new and original test cases coexist in the test population. The refined test cases are capable of changing the logic of the original test, and they cover different parts of the method under test.\nHowever, it is important to acknowledge certain limitations in the LLM's responses. Occasionally, the LLM may hallucinate [54], e.g., generate lines that deviate from the original test case, or alter the number of parameters in method invocations. To mitigate these inconsistencies, we designed our parser to substitute the erroneous line with the corresponding line from the original test case if a corresponding line exists for it in the original test case. In the absence of a corresponding line in the original test case, the parser will skip parsing these erroneous lines and continue parsing the remaining portions of the LLM-generated test cases. This increases the chance that even test cases with omissions are valid for compilation. For instance, if the LLM's response adds a non-existent statement like weaponGameData0.increaseDmg(10), the parser skips this line and continues processing. Similarly, if the LLM alters a method's parameter count, like changing weaponGameData0.getDmgBonus() to weaponGameData0.getDmgBonus(10), the parser uses the original method call with zero parameters. These strategies ensure the parser extracts the maximum number of statements from the LLM responses, minimizing the need for re-prompting.\nIn post-refinement, EvoSuite optimizes the test case popu-lation and adds assertions to them. The optimization includes shortening test cases, and eliminating duplicated test cases from the population. The selection of which duplicate test case to keep and which to eliminate is directed by a secondary ob-jective, which prioritizes selecting the test case that minimizes the total length of all test cases within the set of duplicates."}, {"title": "C. Stage 2: Post-Processing", "content": "In this stage, we make the final chosen test as understand-able as possible by making various aspects of the code more understandable. UTGen achieves this by adding descriptive comments, and making variable names more clear.\nAfter the post-refinement has finished, assertions are added to the test cases, and the test cases have reached maturity in terms of coverage, they are given to the LLM for improvement.\nThe LLM is instructed to add comments (using the Given, When, Then convention seen as more understandable [55]) and to exclusively change the naming of the variables but to let the data and logic untouched given that this could impact the intended behavior of a certain test.\nTo ensure maximal logical similarity between original and enhanced test cases, we use the CodeBLEU metric which effectively assesses syntactic and semantic similarities between two sequences [56]. We choose to control for similarity to increase the cohesion between generated and improved test cases as well as minimize the impact of LLM hallucinations. A CodeBLEU score below 0.5 triggers a re-prompting process.\nWe cap re-prompting at three iterations, as our findings suggest that this limit preserves logical coherence and still facilitates the improvement of tests. If the LLM does not meet the threshold after three attempts, the prompt simplifies, removing comment structure constraints, and thus allowing deviation from the initial format. Should the LLM's response still not reach a satisfactory level after a total of six attempts, the original test case is retained. The value of three attempts per prompting strategy is also chosen to balance effectiveness and execution cost.\nAdditionally, as previous literature has pointed out, results from LLMs can be non-deterministic given a non-modified temperature of the model being used, this can in turn lead to results diverging from the original tests, or tests that do not have correct syntax. To ensure consistency and reliability in the returned results, we employ a set of heuristic safeguards to facilitate the process of controlling for such anomalies. With each response from the LLM, we 1) try to identify and remove common mistakes made by the LLM in the code, e.g., comments placed inside the code as plain text and not as comments, 2) attempt to correct any missing closing brackets in a piece of code, 3) validate code using CodeBLEU as previously described, and 4) check the syntactic correctness of the returned results with the parser generator tool ANTLR\u00b2.\nFurthermore, we re-prompt the LLM in the case when any of the previously described safeguards fail to improve the response, fail to achieve syntactic correctness, or have lower-than-threshold values for CodeBLEU. We limit the amount of recursive calls that are made to not have a single improvement request stall the entire process. All the processes explained above relate to the component marked with in Figure 2.\nAn example of this step is shown in \u2461 in Figure 4: the comments in Given-when-then format are added, and the variable names are changed from weaponGameData0 and weaponGameDatal to the defaultWeapon and cus-tomWeapon, matching the logic of the test case. Also, the assertion message is added from the LLM response."}, {"title": "D. Stage 3: Test Method Name Suggestion", "content": "In this stage, UTGen gives the LLM the completed method body of the test, and it is asked to deduce a descriptive name. We chose to put this stage after the post-processing of the test method body because then the test case includes comments that increase the context for the LLM to generate a descriptive test method name. If another test case already has a similar test name, we re-prompt until it has a unique name.\nFor instance, in \u2462 in Figure 4, the LLM suggests testEqualsWithDifferentMinDmgValues(). This name reflects the test's functionality of examining the equals method across varying minimum damage values. In comparison, EvoSuite named this test testCreatesWeaponGameDataTaking6ArgumentsAnd-CallsEquals3."}, {"title": "E. Stage 4: Compile and Verify", "content": "After successfully navigating through the safeguards, it is still possible for a test case to fail to compile. Therefore, we compile all test cases, and a non-compiling test case undergoes a repeated cycle of post-processing and test method name suggestion, with a default post-processing budget of 2 iterations.\nCompiling test cases are then assessed for their stability. A test case is considered unstable if it fails due to an exception unrelated to a JUnit assertion. All test cases that are both compilable and stable are saved."}, {"title": "IV. EXPERIMENT SETUP", "content": "In this section, we describe the methodology of evaluation of our approach. We investigate the following RQs:\nRQ1 Does UTGen have the capability to generate effective unit tests by utilizing a combination of LLMs and SBST?\nRQ2 What is the impact of LLM-improved unit tests' under-standability on the efficiency of bug fixing by developers?\nRQ3 Which elements of UTGen affect the understandability of the generated unit tests?\nWe now discuss the evaluation strategies for RQ1 to RQ3."}, {"title": "A. Effectiveness Evaluation Setup (RQ1)", "content": "We explore the effectiveness of UTGen on two axes: the compilability rate of LLM-improved test cases, and a compar-ison in coverage of baseline and UTGen test cases."}, {"title": "B. Controlled Experiment (RQ2)", "content": "We conducted a controlled experiment to assess the un-derstandability of test cases in a real-world scenario, namely bug fixing [59]. This extends the work of Panichella et al., who investigated the impact of generating documentation for automatically generated tests in the context of bug fixing [36]. The experiment involved 32 participants. The experimental group worked with UTGen test cases, while the control group was given EvoSuite test cases. We configured EvoSuite with coverage-based test naming, which generates more readable test names than the default setting [60].\nWe examined two dependent variables in the experiment: 1) the number of fixed bugs, and 2) time efficiency, measured as the time taken to fix the bugs."}, {"title": "C. Post-Test Questionnaire (RQ3)", "content": "We used the post-test questionnaire to obtain feedback from the participants of the controlled experiment on which aspects of UTGen affect the understandability of test cases (see Ta-ble IV). We focused on gauging three aspects: 1) participants' views on how the understandability of test cases impacts their bug-fixing effectiveness, 2) their opinion on what factors in test code contribute to the understandability of generated test cases, and 3) their ratings of the quality of these factors in test cases with and without the LLM-improved enhancements.\n1) Questionnaire: In Q1, we ask participants to identify factors they believe to affect bug fixing effectiveness. Im-portantly, at this stage, the participants are unaware that the experiment focuses on the understandability of generated test cases, ensuring that their responses genuinely reflect their initial thoughts on bug fixing. In Q2, we query whether the participants think the clarity of generated test cases influences bug fixing. Q3 and Q4 gauge which factors impact under-standability most. In Q5, we ask the participants to rate the understandability of the two tasks, using a Likert scale along with open-ended feedback. Finally, in Q6 and Q7, we ask participants to rate specific elements such as comments, test data, test names, and variable naming in the test cases of both tasks in terms of completeness, conciseness, clarity, and naturalness, thus aiming for a detailed evaluation of different aspects of test case quality [28], [36], [66]."}, {"title": "V. RESULTS", "content": "We define effectiveness as the capability of UTGen to generate unit tests that are compilable and execute reliably, along with their ability to cover the classes under test. The success rate, defined as the proportion of generated tests that pass upon execution, reflects functional correctness. It is important to note that while all generated tests compile, the success rate pertains solely to their execution outcome.\nUTGen successfully generates a total of 8430 tests, with a pass rate of 73.27%, while EvoSuite produces 8315 tests at a slightly higher pass rate of 79.01%. The heuristic safeguard described in Section III-C ensures the syntactic correctness and compilability of test cases, but also leads to 27.52% of the tests were categorized as \u201cenhancement stagnation\", i.e., the LLM could not improve the test case, or \u201creverted\u201d, i.e., we went back to the EvoSuite base test case, as the test case failed to compile. As such, these 27.52% of test cases compile, but are not meaningfully affected by UTGen.\nThe origin of certain test cases not being meaningfully affected by UTGen lies in the non-deterministic nature of LLMs. As we have no guarantee that tests given to the LLM will compile upon improvement due to the possible hallucinations by the LLM, we employ several safeguards. While the safeguards explained in Section III-C do manage to catch a great portion of the tests that would not compile, some do fall through. Therefore, we perform a compilation check (\u2463 in Figure 2). If any (improved) test fails to compile, we revert back to an EvoSuite-generated test case.\nOut of the total 8430 tests generated by UTGen, 11.77% are non-compiling and are thus reverted to the initial test case generated by EvoSuite. The remaining 15.75% of tests are due to the stagnation of the enhancement process and the inability of the LLM to make a significant contribution.\nFinally, from Table V we observe that EvoSuite reaches slightly higher coverage compared to UTGen: instruction cov-erage is 25.03% compared to 24.43%, while branch coverage is 18.68% compared to 17.87%. In a further investigation into the reason for this delta in coverage, we find that small changes in the post-processing step, e.g., changes in values of parameters, affect the overall coverage achieved."}, {"title": "B. RQ2: The Impact on Bug Fixing", "content": "Figure 5 presents the results of the controlled experiment in terms of two dependent variables: 1) the number of bugs fixed, and 2) time efficiency, measured by the duration required to complete the tasks. The results are reported for respectively the entire population, the academic participants, and the industry participants.\nFor both objects, participants fixed more bugs in the task with the LLM-improved test cases compared to the baseline test cases. According to the tests of fixed effects presented in Table VI, we observe in the fixed bugs column that both technique (p = 0.024) and object (p = 0.025) significantly influence the number of fixed bugs. This implies that using the LLM-Improved test cases significantly increases the likelihood of fixing more bugs. Similarly, when the object is JSWeaponData, the probability of fixing more bugs is also significantly higher. The result of Cohen's d effect size for the treatment is medium at 0.59.\nRegarding time efficiency, participants using LLM-improved test cases generally took less time to fix all bugs for both classes.\nHowever, the differences in timing are not statistically significant for the technique (p = 0.063), with significance observed only for the object (p = 0.031). The difference is more apparent in the JSWeaponData class, where the average time to fix all bugs was 18:22 for LLM-improved versus 22:06 for baseline test cases (20% less time). For the Budget class, the averages are closer: 27:06 for LLM-improved and 27:51 for baseline test cases. This is mainly due to a 30-minute cutoff, which limited the observable difference.\nAdditionally, a post hoc analysis of Estimated Marginal Means involving a pairwise comparison of different technique levels for each specific object level indicates that in the Budget class, the treatment (LLM-Improved test cases) is significant (p = 0.024), whereas it is not significant in the JSWeaponData class (p = 0.319). The Cohen's d effect size is large of 0.92 for the treatment in the Budget class. We hypothesize that the statistically significant improvement in the number of bugs fixed in the Budget assignment, compared to JSWeaponData, is due to the greater complexity of scenarios and bugs in the Budget class. This complexity likely increases the demand for clearer and more understandable test cases.\nFurthermore, neither the period (p = 0.176 and p = 0.068) nor the order (p = 0.138 and p = 0.517) significantly impact the number of fixed bugs and time efficiency. This indicates that there is no carryover effect between treatments. The interaction between technique and object is not significant, suggesting that the effect of the technique on the number of bugs fixed and time efficiency does not depend on the object. Additionally, our analysis found no significant interaction between the technique and co-factors such as participants' backgrounds, experience in Java and testing, or whether they attended sessions remotely or in person (p >> 0.05).\nFinally, in terms of the influence of the background of our participants, we observe that both population groups show better performance when using LLM-improved test cases com-pared to baseline test cases in terms of both number of bugs fixed and time taken to fix bugs. We observe that academic participants seem to benefit more from the LLM-improved test cases in aiding bug fixing. For industrial participants on the other hand, the time-saving gain is more pronounced."}, {"title": "C. RQ3: The effects of different elements of UTGen on under-standability", "content": "The results of the post-test questionnaire show three aspects: 1) the participants' views on how the understandability of test cases impacts their bug-fixing effectiveness, 2) their opinion on what factors in test code contribute to the understandability and 3) their ratings of the quality of elements in test cases with and without the LLM-improved enhancements.\nAspect 1: How understandability of test cases impacts bug-fixing: We answer the first aspect through the responses to Questions 1 and 2 in the survey. We have observed that participants find a well-written test suite important for bug fixing: they frequently highlighted (14 mentions) the impor-tance of descriptive and clear test names, appropriate use of assertions, and well-chosen test data in test suites. This aspect was prioritized over other factors like high-quality production code (10 mentions). We also take note of the overall (strong) agreement that test case understandability is important in in the context of bug fixing, as indicated by a median score of 4 out 5 (Q2 in Table VII).\nAspect 2: What factors in test code contribute to under-standability: We have analyzed the participants' responses to Questions 3 and 4, where they ranked and scored the importance of elements. From Table VII we observe that par-ticipants give more importance to comments and test names, than to variable naming and test data. Specifically, 34.3% of the participants ranked comments as most important, while 40.6% gave priority to test names in Question 3.\nAspect 3: The quality of factors in test cases with and without LLM enhancements: In Q5 of Table VII, we see that participants rate the understandability of LLM-improved tests somewhat better when compared to the baseline test cases.\nWe asked participants in Q6 and Q7 to evaluate an LLM-improved and baseline test case of the assignments on different criteria and specifically per test element. These criteria com-prised completeness, conciseness, clarity, and naturalness.\nFigure 6 shows the results of Q6 and Q7. The results indicate that LLM-improved test cases are consistently rated higher compared to baseline test cases for each of the cri-teria (first row). The Wilcoxon test confirms this statistically significant difference (p-value < 0.05) for all criteria. The effect size for conciseness was small, while it was large for completeness, naturalness, and clarity. Notably, in the open-ended responses, some participants mentioned that some comments in LLM-improved test cases were too general and added little value. The respondents did appreciate the Given-When-Then-structured comments.\nWhen we zoom into the test elements, we see improvements in all areas for LLM-improved test cases: comments, test data, test name, and variable naming. The Wilcoxon test for all of these elements is statistically significant with a p-value << 0.05. The effect size for comments, test data, and test names is medium, while very large for variable naming (d > 1.2). Through the analysis of the open-ended responses to Ques-tions 5\u20137, we found that the complexity of a test case has an impact on the necessity of comments. For simpler test cases, using a Given-When-Then (Arrange/Act/Assert) structure is often sufficient. However, for more complex cases, more de-tailed comments are needed to ensure optimal comprehension. Overall, participants mentioned this point 18 times, with one participant stating \"The test code lines are straightforward, so comments are unnecessary.\". Similarly, for simpler test cases, the quality of variable naming was less of a concern: participants mentioned this factor only 4 times when rating a short baseline test case."}, {"title": "RQ3 Comments, test names, variable names, and test data are improved compared to the baseline. Specifically, participants highlighted improved conciseness, clarity, and naturalness in these test elements.", "content": "In this section, we discuss our results, their implications, and threats to the validity of our study."}, {"title": "A. Revisiting the Research Questions", "content": "RQ1: Does UTGen have the capability to generate effective unit tests by utilizing a combination of LLMs and SBST? When we compare the effectiveness of our LLM-inspired UTGen approach and EvoSuite, we observe that UTGen generates test cases that have relatively similar structural coverage. However, we also noticed a phenomenon that we term enhancement stagnation, which occurs when the LLM is not able to im-prove the test case, even when re-prompting multiple times. We analyzed this situation and found indications that this stagnation is correlated with high complexity. In this context, we define complexity at the level of the class under test to be: 1) methods having a high number of parameters, and 2) methods being tightly coupled, i.e., many method calls between objects or within an object. While generally adding more relevant context can help an LLM, highly complex projects can overwhelm LLMs due to lengthy input codes and insufficient contextual information, thus hindering the en-hancement process during post-processing. To overcome this, we propose to incorporate Retrieval Augmented Generation (RAG) techniques. We hypothesize that these enhancements can reduce occurrences of Enhancement Stagnation as it has resolved similar stagnation issues in other domains [67], [68]. RAG involves enhancing LLMs by dynamically integrating knowledge from databases, knowledge graphs, or the internet in real time into the generation process to provide contextually richer and more accurate responses.\nRQ2: What is the impact of LLM-improved unit tests' under-standability on the efficiency of bug fixing by developers? From the results of the controlled experiment, we see indications that the LLM-based enhancements brought to the generated unit tests improve their understandability in the bug-fixing scenario. Specifically, the experimental group outperformed the control group by fixing up to 33% more bugs and completing tasks up to 20% faster. Our experiment consisted of two assignments in-volving respectively the Budget and JSWeaponData classes.\nWhile we observed statistically significant improvements for the Budget assignment, the other assignment did not reach statistical significance. Since the Budget class is comprised of more complex scenarios and bugs, we hypothesize that the complexity of a test scenario increases the need for understandable test cases. This hypothesis was anecdotally confirmed by participants in the post-test questionnaire.\nRQ3: Which elements of UTGen affect the understandability of the generated unit tests? Through the post-test questionnaire, we captured that participants think that LLM-improved test cases are showing improvements in terms of comments, test names, test data, and variable names when compared to baseline test cases. At a higher level, participants also rated completeness, conciseness, clarity, and naturalness as better. However, feedback from open-ended questions highlights that comments should be more precise and informative. Similarly, some participants also highlighted that simple test methods might not require (extensive) comments. Upon reflecting on this feedback, we hypothesize that generically trained LLMs, while generally robust, might lack task-specific data to effec-tively assist in creating comments."}, {"title": "B. Implications", "content": "Our study's results have an important implication for re-searchers and tool builders. In particular, our study indicates that a generally trained LLM can already instigate a consid-erable improvement in the understandability of search-based generated test cases. However, our results also show that test case comments should be more detailed in some cases, while seeming superfluous in other situations. Therefore, we see potential in creating specifically-trained LLMs for particular software engineering tasks, but equally in customizing LLM responses to individual software engineers."}, {"title": "C. Threats to Validity", "content": "Construct Validity. Threats to construct validity relate to the setup of our study. We conducted the study either in person or remotely, with an examiner present. To control for factors other than the codebase, we ensured a consistent setup for all participants and limited the choice of IDE to IntelliJ, providing uniform capabilities. However, this approach may disadvantage participants having experience with other IDEs, potentially affecting their performance.\nInternal Validity. To mitigate threats to internal validity, we did not reveal the tool names in our experiment and questionnaire. To prevent bias in selecting classes for the assignments, we followed a systematic selection process to strengthen the methodological integrity. To compensate for a learning effect, we created four different sequences of the experimental design. Using mixed models, we found that period and carryover effects were not statistically significant, indicating they do not pose major threats to the study's validity.\nExternal Validity. The classes that we use to determine the efficacy of test generation in RQ1 are a potential threat to the generalization of our results. To address this, we used a dataset of 346 classes from 117 open-source Java projects that form a representative sample and were previously used in software testing studies [18], [69]. We limited the controlled experiment in RQ2 to two Java classes. To ensure their representativeness, we carefully selected them from the SF110 dataset containing real-world classes, and taking the average LOC of that entire dataset into consideration to select \"average classes\". Future work will explore more complex classes. In order to mitigate potential imbalance between the experimental and control groups, we carefully balanced participants over both groups in terms of experience and background."}, {"title": "VII. RELATED WORK", "content": "Panichella et al. [36] introduced TestDescriber, which gener-ates test case summaries that describe the intent of a generated unit test; they established that these summaries enable software engineers to resolve bugs more quickly. Similarly, Roy et al. [28] developed DeepTC-Enhancer, leveraging deep learning to produce method-level summaries for test cases. Both efforts highlight the value of summarizing test cases. In contrast, UTGen generates detailed comments within the test cases themselves and provides a narrative of the test scenario.\nZhang et al. [27] introduced an NLP technique for automat-ically generating descriptive unit test names. Daka et al. [60] applied coverage criteria for naming automatically generated unit tests, while Roy et al. [28] created DeepTC-Enhancer by employing deep learning to rename identifiers in test cases to improve readability. Unlike these methods that rely on traditional NLP techniques, UTGen utilizes LLMs to suggest identifiers that fit the test scenario's context.\nAfshan et al. [70] enhanced the readability of inputs by combining natural language models with search-based test generation. Deljouyi et al. [66] proposed an approach that gen-erates understandable test cases with meaningful data through end-to-end test scenario carving. Baudry et al. [71] developed a test data generator using LLMs to produce realistic, domain-specific constraints. Our method is similar to Baudry et al.'s, but we focus on search-based unit test generation."}, {"title": "B. Generating Test Cases by LLM", "content": "Despite the progress in LLM-based test generation, to the best of our knowledge, no study has focused on enhancing unit test case understandability through the integration of search-based methods and LLMs. Research in this field shows considerable variability in methods and outcomes. Siddiq et al. generated tests using LLMs and reported 2% coverage on the SF110 dataset [35", "29": "TestPi-lot for JavaScript achieved 70% statement-level"}]}