{"title": "Leveraging Large Language Models for Enhancing the Understandability of Generated Unit Tests", "authors": ["Amirhossein Deljouyi", "Roham Koohestani", "Maliheh Izadi", "Andy Zaidman"], "abstract": "Automated unit test generators, particularly search-based software testing tools like EvoSuite, are capable of generating tests with high coverage. Although these generators alleviate the burden of writing unit tests, they often pose challenges for software engineers in terms of understanding the generated tests. To address this, we introduce UTGen, which combines search-based software testing and large language models to enhance the understandability of automatically generated test cases. We achieve this enhancement through contextualizing test data, improving identifier naming, and adding descriptive comments. Through a controlled experiment with 32 participants from both academia and industry, we investigate how the understandability of unit tests affects a software engineer's ability to perform bug-fixing tasks. We selected bug-fixing to simulate a real-world scenario that emphasizes the importance of understandable test cases. We observe that participants working on assignments with UTGen test cases fix up to 33% more bugs and use up to 20% less time when compared to baseline test cases. From the post-test questionnaire, we gathered that participants found that enhanced test names, test data, and variable names improved their bug-fixing process.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's software-dominated world, software reliability and correctness are very important [1]. Consequently, automated testing in the form of unit tests has become a crucial element for software engineers in ensuring high-quality software [2]\u2013[4]. Despite the widely acknowledged importance of testing, writing tests is tedious and time-consuming [5]\u2013[8]. To alleviate this burden on developers and testers, the research community has devoted considerable effort on investigating automatic test generation approaches [9]\u2013[14]. Among the notable test generators are Randoop [15] and EvoSuite [11]. EvoSuite, for example, is a search-based test generator that employs genetic algorithms to construct a test suite [16] and has demonstrated good results in terms of coverage [17], [18]. However, based on insights obtained through industrial case studies, there are limitations in terms of the quality of the generated test cases [19]\u2013[25]. One critical limitation revolves around the understandability of generated test cases, which involves various aspects such as meaningful test data, proper assertions, well-defined mock objects, descriptive identifiers and test names, as well as informative comments. Additionally, the difficulty in following the scenario depicted in the test case and the ambiguity surrounding test data significantly hamper clarity [25], [26].\nFigure 1 provides an example of an EvoSuite-generated test case. This test case checks the equals method with two objects of weaponGameData with different minimum damage values. Here, we see several comprehension challenges: 1) the purpose and functionality of a test method named with five arguments and \u201ccallsEquals3\" is obscure, 2) the rationale behind the chosen test data remains unclear, 3) the identifiers are not providing any additional information, and 4) the absence of comments leaves the test case without essential explanatory context.\nTo address these issues, we aim to enhance automatically generated test cases by focusing on contextual test data, clear test method and identifier names, and adding descriptive comments. In this study, we investigate the synergy of Search-Based Software Testing (SBST) and Large Language Models (LLMs). While Natural Language Processing (NLP) techniques have shown promise in text generation and optimization [27], [28], and LLMs have advanced text-based capabilities [29]\u2013[33], their impact in generating high-coverage test cases for complex systems remains limited [34], [35]. Conversely, SBST, while effective in coverage, often falls short in test case understandability.\nOur approach, UTGen, integrates an LLM into the SBST test generation process. We hypothesize that this combined approach can leverage the strengths of both techniques to generate effective and understandable test cases. Our study is steered by three Research Questions (RQs) that consider the effectiveness of the UTGen approach, and the understand-ability of the generated test cases.\nRQ1 Does UTGen have the capability to generate effective unit tests by utilizing a combination of LLMs and SBST? The investigation into the effectiveness of the approach seeks to establish whether the non-determinism of both the SBST and LLM components impact the ability to generate compil-able and high-coverage unit tests.\nRQ2 What is the impact of LLM-improved unit tests' under-standability on the efficiency of bug fixing by developers? When it comes to the understandability of generated test cases, we intend to measure understandability through the ease by which software engineers can perform bug-fixing tasks involving failing test cases, a setup previously used by Panichella et al. [36]."}, {"title": "II. BACKGROUND", "content": "A. Search-Based Software Testing\nAutomated test generation approaches have been developed in order to reduce testing effort. Tools like EvoSuite [11] and Randoop [15] generate a test suite starting from Java source code using a search-based or random approach [17], [18]. Several studies have uncovered challenges involving automat-ically generated tests [20]\u2013[25], an important one being that generated tests are typically less readable than their human-written counterparts [38]. In this context, Almasi et al. [25] have observed that developers 1) find the test case scenario difficult to follow, 2) find the test data unclear, and 3) have difficulties with the meaningfulness of generated assertions.\nB. Large Language Models\nLarge Language Models are a subset of AI systems predom-inantly based upon the transformer architecture [39]. These LLMs are trained on vast amounts of data, through which, they learn the underlying patterns inherent in texts, code, dialogue, etc., and are therefore capable of generating a (somewhat) relevant response given a prompt by the user [40]. LLMs operate based on predicting the subsequent tokens in a sequence and reusing the extended sequence by running it through the model once again to predict the tokens to follow (referred to as autoregression). This process is continued up until a point in which either the maximum amount of required tokens is reached or a termination character is generated."}, {"title": "III. THE UTGEN APPROACH", "content": "Figure 2 provides an overview of our approach, named UTGen. The core of our framework is a search-based ap-proach in which we integrated an LLM in various stages of the test generation process (highlighted in green). We use EvoSuite [11] as the search-based test generation framework of choice, and we have developed additional functionalities that facilitate the integration of EvoSuite with LLMs (highlighted in blue).\nThe aim of our approach, UTGen, is to enhance the under-standability of test cases by improving four key elements of generated tests: 1) providing context-rich test data, 2) incorporating informative comments, 3) using descriptive variable names, and 4) picking meaningful test names. These goals define the stages in our approach.\nAs a first step, after the genetic algorithm has ended and the test cases mature in the search-based process, our approach focuses on refining test data (\u2460 in Figure 2). UTGen uses an LLM to generate contextually relevant test data, unlike traditional search-based methods that often rely on random values. Following this refinement, the search process ends, and we transition to post-processing tasks. Here, EvoSuite minimizes the number of test cases in the test suite, shortens the length of individual tests, and adds assertions.\nOnce the test cases are fully formed, at stage \u2461, UTGen leverages an LLM to add descriptive comments and enhance variable names. In stage \u2462, UTGen uses an LLM to suggest suitable names for the tests, reflecting the assertions and logic within. Finally, to ensure that test cases are compilable and stable after these enhancements, UTGen compiles them (stage \u2463), and in case of compilation issues, the process iteratively revisits stage \u2461 for adjustments.\nWe first explain the prompt engineering component and then describe our test generation process per stage.\nA. Prompt Generation\nThe prompt component of UTGen uses the code-llama:7b-instruct model from Meta [44] as provided by Ollama\u00b9. We have designed UTGen in such a way, that the Code-llama can easily be exchanged for another LLM. There are three stages within the UTGen approach uses the prompt component: 1) the refinement of test data, 2) the post-processing of tests, and 3) the naming of tests. The general prompt component contains two distinct parts, namely  which is responsible for generating the prompts provided to the LLM, and (\u03b2 which manages the request and ensures the correctness of the returned response.\nFor each stage, we devised specialized prompts following guidelines from recent prompt engineering research [50]\u2013[52]. As shown in Figure 3, these guidelines emphasize the follow-ing: writing clear instructions with action words (as in \u2461), adopting a persona for the model (as in \u2460), allowing sufficient processing time through techniques like Chain of Thought (CoT) (as in 3), standardizing input and output formats, and framing requests in a positive manner (as in 4). The starting point for each prompt resembles the one presented in Figure 3. As each model has its complexities, pitfalls, and preferred input format, no one-size-fits-all solution exists to prompt engineering, however, the guidelines set out above have guided us. We have followed an iterative prompt engineering process in which each adjustment of the prompt was deliberated upon, before being accepted or rejected by the authors based on potential improvements in the results. An emerging pattern that we initially observed is that LLMs are incapable of always adhering to the output format described for them. Therefore, we put guidelines in place to deal with such mismatches; as an example, we had to deal with cases where plain text was placed inside the code blocks, or when the intended delineation was not used by the LLM. Our replication package contains the final versions of the prompts that we engineered, in addition to other measures that were taken [37].\nB. Stage 1: Test Data Refinement\nIn this stage, we focus on requesting contextualized test data from the LLM to increase the domain relevance of test data for a test scenario. We designed a parser that converts the LLM's responses into the structured format required by EvoSuite.\nThe test data refinement stage should be considered as another iteration in the search process in which both new and original test cases coexist in the test population. The refined test cases are capable of changing the logic of the original test, and they cover different parts of the method under test.\nAn example of the refinement stage can be seen in stage 1 of Figure 4, with the original and enhanced test data shown side by side. Based on the context, the LLM changes the fourth argument of the WeaponGameData constructor call from \"N&zMn$@6gffi<\" into \"Ninja Sword\", which is more meaningful in the context of WeaponGameData.\nHowever, it is important to acknowledge certain limitations in the LLM's responses. Occasionally, the LLM may hallucinate [54], e.g., generate lines that deviate from the original test case, or alter the number of parameters in method invocations. To mitigate these inconsistencies, we designed our parser to substitute the erroneous line with the corresponding line from the original test case if a corresponding line exists for it in the original test case. In the absence of a corresponding line in the original test case, the parser will skip parsing these erroneous lines and continue parsing the remaining portions of the LLM-generated test cases. This increases the chance that even test cases with omissions are valid for compilation. For instance, if the LLM's response adds a non-existent statement like weaponGameData0.increaseDmg(10), the parser skips this line and continues processing. Similarly, if the LLM alters a method's parameter count, like changing weaponGameData0.getDmgBonus() to weaponGameData0.getDmgBonus(10), the parser uses the original method call with zero parameters. These strategies ensure the parser extracts the maximum number of statements from the LLM responses, minimizing the need for re-prompting.\nIn post-refinement, EvoSuite optimizes the test case popu-lation and adds assertions to them. The optimization includes shortening test cases, and eliminating duplicated test cases from the population. The selection of which duplicate test case to keep and which to eliminate is directed by a secondary ob-jective, which prioritizes selecting the test case that minimizes the total length of all test cases within the set of duplicates.\nC. Stage 2: Post-Processing\nIn this stage, we make the final chosen test as understand-able as possible by making various aspects of the code more understandable. UTGen achieves this by adding descriptive comments, and making variable names more clear.\nAfter the post-refinement has finished, assertions are added to the test cases, and the test cases have reached maturity in terms of coverage, they are given to the LLM for improvement.\nThe LLM is instructed to add comments (using the Given, When, Then convention seen as more understandable [55]) and to exclusively change the naming of the variables but to let the data and logic untouched given that this could impact the intended behavior of a certain test.\nTo ensure maximal logical similarity between original and enhanced test cases, we use the CodeBLEU metric which effectively assesses syntactic and semantic similarities between two sequences [56]. We choose to control for similarity to increase the cohesion between generated and improved test cases as well as minimize the impact of LLM hallucinations. A CodeBLEU score below 0.5 triggers a re-prompting process.\nWe cap re-prompting at three iterations, as our findings suggest that this limit preserves logical coherence and still facilitates the improvement of tests. If the LLM does not meet the threshold after three attempts, the prompt simplifies, removing comment structure constraints, and thus allowing deviation from the initial format. Should the LLM's response still not reach a satisfactory level after a total of six attempts, the original test case is retained. The value of three attempts per prompting strategy is also chosen to balance effectiveness and execution cost.\nAdditionally, as previous literature has pointed out, results from LLMs can be non-deterministic given a non-modified temperature of the model being used, this can in turn lead to results diverging from the original tests, or tests that do not have correct syntax. To ensure consistency and reliability in the returned results, we employ a set of heuristic safeguards to facilitate the process of controlling for such anomalies. With each response from the LLM, we 1) try to identify and remove common mistakes made by the LLM in the code, e.g., comments placed inside the code as plain text and not as comments, 2) attempt to correct any missing closing brackets in a piece of code, 3) validate code using CodeBLEU as previously described, and 4) check the syntactic correctness of the returned results with the parser generator tool ANTLR\u00b2.\nFurthermore, we re-prompt the LLM in the case when any of the previously described safeguards fail to improve the response, fail to achieve syntactic correctness, or have lower-than-threshold values for CodeBLEU. We limit the amount of recursive calls that are made to not have a single improvement request stall the entire process. All the processes explained above relate to the component marked with in Figure 2.\nAn example of this step is shown in 2 in Figure 4: the comments in Given-when-then format are added, and the variable names are changed from weaponGameData0 and weaponGameDatal to the defaultWeapon and customWeapon, matching the logic of the test case. Also, the assertion message is added from the LLM response.\nD. Stage 3: Test Method Name Suggestion\nIn this stage, UTGen gives the LLM the completed method body of the test, and it is asked to deduce a descriptive name. We chose to put this stage after the post-processing of the test"}, {"title": "IV. EXPERIMENT SETUP", "content": "In this section, we describe the methodology of evaluation of our approach. We investigate the following RQs:\nRQ1 Does UTGen have the capability to generate effective unit tests by utilizing a combination of LLMs and SBST?\nRQ2 What is the impact of LLM-improved unit tests' under-standability on the efficiency of bug fixing by developers?\nRQ3 Which elements of UTGen affect the understandability of the generated unit tests?\nWe now discuss the evaluation strategies for RQ1 to RQ3.\nA. Effectiveness Evaluation Setup (RQ1)\nWe explore the effectiveness of UTGen on two axes: the compilability rate of LLM-improved test cases, and a compar-ison in coverage of baseline and UTGen test cases.\n1) Dataset: We utilize the DynaMOSA dataset composed of 346 non-trivial Java classes from 117 open-source projects for RQ1 [18]. The classes are selected from four different benchmarks, with the primary source being the 204 non-trivial classes of SF110 [57].\n2) Evaluation: We evaluated UTGen using the EvoSuite framework as a baseline. We applied UTGen on a dataset and generated two types of test cases: original EvoSuite test cases and LLM-improved test cases. We then compare these two types of test cases by measuring 1) the number of LLM-im-proved test cases that compiled successfully, 2) branch and instruction test coverage, and 3) pass/fail rates.\n3) Parameter Configuration: We decided to use the default configuration parameters for EvoSuite, which have been em-pirically shown to provide good results [58]. We did increase the test budget (max_time) from 60 to 200 seconds, to ensure that the search algorithm has enough time to generate a test population that achieves reasonable coverage levels.\nB. Controlled Experiment (RQ2)\nWe conducted a controlled experiment to assess the un-derstandability of test cases in a real-world scenario, namely bug fixing [59]. This extends the work of Panichella et al., who investigated the impact of generating documentation for automatically generated tests in the context of bug fixing [36].\nThe experiment involved 32 participants. The experimental group worked with UTGen test cases, while the control group was given EvoSuite test cases. We configured EvoSuite with coverage-based test naming, which generates more readable test names than the default setting [60].\nWe examined two dependent variables in the experiment: 1) the number of fixed bugs, and 2) time efficiency, measured as the time taken to fix the bugs.\n1) Participants: We recruited participants with academic and industrial backgrounds. Table I presents their demograph-ics. To engage academic participants, the experiment was advertised via the university's communication channels. Ad-ditionally, developers from an industrial partner were enlisted. Furthermore, all authors reached out to their professional networks of software engineers. We made sure to extend the invitation to individuals with experience in Java and testing.\n2) Objects: To design the bug-fixing assignments and com-pare experimental and control groups, it was essential to choose two projects that would offer a solid foundation for understanding the context of bug fixing. To do so, we analyzed all classes within the SF110 dataset, gathered insights into the distribution of Lines of Code (LOC), which serves as an indicator of complexity [61]. Using this data, we calculated the mean ($\\mu$) and standard deviation ($\\sigma$) for each distribution. We then identified all classes falling within the range of $\\mu \\pm 0.1\\sigma$"}, {"title": "V. RESULTS", "content": "A. RQ1: Effectiveness of Integrating LLMs and Search-Based Methods for Generating Unit Tests\nWe define effectiveness as the capability of UTGen to generate unit tests that are compilable and execute reliably, along with their ability to cover the classes under test. The success rate, defined as the proportion of generated tests that pass upon execution, reflects functional correctness. It is important to note that while all generated tests compile, the success rate pertains solely to their execution outcome.\nUTGen successfully generates a total of 8430 tests, with a pass rate of 73.27%, while EvoSuite produces 8315 tests at a slightly higher pass rate of 79.01%. The heuristic safeguard described in Section III-C ensures the syntactic correctness and compilability of test cases, but also leads to 27.52% of the tests were categorized as \u201cenhancement stagnation\", i.e., the LLM could not improve the test case, or \u201creverted\", i.e., we went back to the EvoSuite base test case, as the test case failed to compile. As such, these 27.52% of test cases compile, but are not meaningfully affected by UTGen.\nThe origin of certain test cases not being meaningfully affected by UTGen lies in the non-deterministic nature of LLMs. As we have no guarantee that tests given to the LLM will compile upon improvement due to the possible hallucinations by the LLM, we employ several safeguards. While the safeguards explained in Section III-C do manage to catch a great portion of the tests that would not compile, some do fall through. Therefore, we perform a compilation check (4) in Figure 2). If any (improved) test fails to compile, we revert back to an EvoSuite-generated test case.\nOut of the total 8430 tests generated by UTGen, 11.77% are non-compiling and are thus reverted to the initial test case generated by EvoSuite. The remaining 15.75% of tests are due to the stagnation of the enhancement process and the inability of the LLM to make a significant contribution.\nFinally, from Table V we observe that EvoSuite reaches slightly higher coverage compared to UTGen: instruction cov-erage is 25.03% compared to 24.43%, while branch coverage is 18.68% compared to 17.87%. In a further investigation into the reason for this delta in coverage, we find that small changes in the post-processing step, e.g., changes in values of parameters, affect the overall coverage achieved.\nRQ2 In our experiment, using LLM-Improved tests significantly increases the likelihood of fixing more bugs.\nC. RQ3: The effects of different elements of UTGen on under-standability\nThe results of the post-test questionnaire show three aspects: 1) the participants' views on how the understandability of test cases impacts their bug-fixing effectiveness, 2) their opinion on what factors in test code contribute to the understandability and 3) their ratings of the quality of elements in test cases with and without the LLM-improved enhancements.\nAspect 1: How understandability of test cases impacts bug-fixing: We answer the first aspect through the responses to Questions 1 and 2 in the survey. We have observed that participants find a well-written test suite important for bug fixing: they frequently highlighted (14 mentions) the impor-tance of descriptive and clear test names, appropriate use of assertions, and well-chosen test data in test suites. This aspect was prioritized over other factors like high-quality production code (10 mentions). We also take note of the overall (strong) agreement that test case understandability is important in in the context of bug fixing, as indicated by a median score of 4 out 5 (Q2 in Table VII).\nAspect 2: What factors in test code contribute to under-standability: We have analyzed the participants' responses to Questions 3 and 4, where they ranked and scored the importance of elements. From Table VII we observe that par-ticipants give more importance to comments and test names, than to variable naming and test data. Specifically, 34.3% of the participants ranked comments as most important, while 40.6% gave priority to test names in Question 3.\nAspect 3: The quality of factors in test cases with and without LLM enhancements: In Q5 of Table VII, we see that participants rate the understandability of LLM-improved tests somewhat better when compared to the baseline test cases.\nWe asked participants in Q6 and Q7 to evaluate an LLM-improved and baseline test case of the assignments on different criteria and specifically per test element. These criteria com-prised completeness, conciseness, clarity, and naturalness.\nRQ3 Comments, test names, variable names, and test data are improved compared to the baseline. Specifically, participants highlighted improved conciseness, clarity, and naturalness in these test elements."}, {"title": "VI. DISCUSSION", "content": "In this section, we discuss our results, their implications, and threats to the validity of our study.\nA. Revisiting the Research Questions\nRQ1: Does UTGen have the capability to generate effective unit tests by utilizing a combination of LLMs and SBST? When we compare the effectiveness of our LLM-inspired UTGen approach and EvoSuite, we observe that UTGen generates test cases that have relatively similar structural coverage. However, we also noticed a phenomenon that we term enhancement stagnation, which occurs when the LLM is not able to im-prove the test case, even when re-prompting multiple times. We analyzed this situation and found indications that this stagnation is correlated with high complexity. In this context, we define complexity at the level of the class under test to be: 1) methods having a high number of parameters, and 2) methods being tightly coupled, i.e., many method calls between objects or within an object. While generally adding more relevant context can help an LLM, highly complex projects can overwhelm LLMs due to lengthy input codes and insufficient contextual information, thus hindering the en-hancement process during post-processing. To overcome this, we propose to incorporate Retrieval Augmented Generation (RAG) techniques. We hypothesize that these enhancements can reduce occurrences of Enhancement Stagnation as it has resolved similar stagnation issues in other domains [67], [68]. RAG involves enhancing LLMs by dynamically integrating knowledge from databases, knowledge graphs, or the internet in real time into the generation process to provide contextually richer and more accurate responses.\nRQ2: What is the impact of LLM-improved unit tests' under-standability on the efficiency of bug fixing by developers? From the results of the controlled experiment, we see indications that the LLM-based enhancements brought to the generated unit tests improve their understandability in the bug-fixing scenario. Specifically, the experimental group outperformed the control group by fixing up to 33% more bugs and completing tasks up to 20% faster. Our experiment consisted of two assignments in-volving respectively the Budget and JSWeaponData classes.\nWhile we observed statistically significant improvements for the Budget assignment, the other assignment did not reach statistical significance. Since the Budget class is comprised of more complex scenarios and bugs, we hypothesize that the complexity of a test scenario increases the need for understandable test cases. This hypothesis was anecdotally confirmed by participants in the post-test questionnaire.\nRQ3: Which elements of UTGen affect the understandability of the generated unit tests? Through the post-test questionnaire, we captured that participants think that LLM-improved test cases are showing improvements in terms of comments, test names, test data, and variable names when compared to baseline test cases. At a higher level, participants also rated completeness, conciseness, clarity, and naturalness as better. However, feedback from open-ended questions highlights that comments should be more precise and informative. Similarly, some participants also highlighted that simple test methods might not require (extensive) comments. Upon reflecting on this feedback, we hypothesize that generically trained LLMs, while generally robust, might lack task-specific data to effec-tively assist in creating comments.\nB. Implications\nOur study's results have an important implication for re-searchers and tool builders. In particular, our study indicates that a generally trained LLM can already instigate a consid-erable improvement in the understandability of search-based generated test cases. However, our results also show that test case comments should be more detailed in some cases, while seeming superfluous in other situations. Therefore, we see potential in creating specifically-trained LLMs for particular software engineering tasks, but equally in customizing LLM responses to individual software engineers.\nC. Threats to Validity\nConstruct Validity. Threats to construct validity relate to the setup of our study. We conducted the study either in"}, {"title": "VII. RELATED WORK", "content": "A. Improving the Understandability of Test Cases\nPanichella et al. [36] introduced TestDescriber, which gener-ates test case summaries that describe the intent of a generated unit test; they established that these summaries enable software engineers to resolve bugs more quickly. Similarly, Roy et al. [28] developed DeepTC-Enhancer, leveraging deep learning to produce method-level summaries for test cases. Both efforts highlight the value of summarizing test cases. In contrast, UTGen generates detailed comments within the test cases themselves and provides a narrative of the test scenario.\nZhang et al. [27] introduced an NLP technique for automat-ically generating descriptive unit test names. Daka et al. [60] applied coverage criteria for naming automatically generated unit tests, while Roy et al. [28] created DeepTC-Enhancer by employing deep learning to rename identifiers in test cases to improve readability. Unlike these methods that rely on traditional NLP techniques, UTGen utilizes LLMs to suggest identifiers that fit the test scenario's context.\nAfshan et al. [70] enhanced the readability of inputs by combining natural language models with search-based test generation. Deljouyi et al. [66] proposed an approach that gen-erates understandable test cases with meaningful data through end-to-end test scenario carving. Baudry et al. [71] developed a test data generator using LLMs to produce realistic, domain-specific constraints. Our method is similar to Baudry et al.'s, but we focus on search-based unit test generation.\nB. Generating Test Cases by LLM\nDespite the progress in LLM-based test generation, to the best of our knowledge, no study has focused on enhancing unit test case understandability through the integration of search-based methods and LLMs. Research in this field shows considerable variability in methods and outcomes. Siddiq et al. generated tests using LLMs and reported 2% coverage on the SF110 dataset [35]. In contrast, Sch\u00e4fer et al.'s [29] TestPi-lot for JavaScript achieved 70% statement-level coverage on relatively small systems. Alshahwan et al. aimed to improve human-written tests by LLMs and submit them for human review [72]. Meanwhile, Lemieux et al. explored overcoming coverage stalls in SBST with LLMs [48], and Moradi et al. investigated mutation testing with LLMs [73]. Steenhoek et al. improved test generation by minimizing test smells through reinforcement learning [74]. Unlike the aforementioned stud-ies, UTGen focuses on enhancing understandability through integrating LLMs in the SBST process. Notably, UTGen achieved 17.87% branch coverage, surpassing the pure LLM approach by Siddiq et al. [35]."}, {"title": "VIII. CONCLUSION", "content": "Recent research has suggested that the understandability of test cases is a key factor to optimize in the context of automated test generation [25]. Therefore, in this paper, we introduce the UTGen approach that incorporates a Large Lan-guage Model (LLM) into the Search-Based Software Testing (SBST) process. In doing so, UTGen aims to improve the un-derstandability by providing context-rich test data, informative comments, descriptive variables, and meaningful test names.\nWe first evaluated UTGen's test generation effectiveness on 346 non-trivial Java classes, observing that UTGen success-fully enhanced 72.48% of the test cases, and slightly decreased coverage compared to EvoSuite-generated tests (RQ1). We then performed a controlled experiment with 32 participants from industry and academia; we observed that test cases generated by UTGen facilitated easier bug-fixing with par-ticipants fixing up to 33% more bugs and doing so up to 20% faster (RQ2). Feedback from participants in the post-test questionnaire indicated a significant improvement in test case completeness, conciseness, clarity, and naturalness (RQ3).\nIn future work, we aim to explore optimization strategies, such as Retrieval Augmented Generation (RAG), to enhance prompt efficiency and minimize the need for re-prompting. Furthermore, we plan to refine our approach by creating customized fine-tuned LLMs specifically for test generation. These customized LLMs would replace the publicly-available pre-trained LLM that we currently use."}]}