{"title": "From Silos to Systems: Process-Oriented Hazard Analysis for AI Systems", "authors": ["Shalaleh Rismani", "Roel Dobbe", "AJung Moon"], "abstract": "To effectively address potential harms from Artificial Intelligence (AI) systems, it is essential to identify and mitigate system-level hazards. Current analysis approaches focus on individual components of an AI system, like training data or models, in isolation, overlooking hazards from component interactions or how they are situated within a company's development process. To this end, we draw from the established field of system safety, which considers safety as an emergent property of the entire system, not just its components. In this work, we translate System Theoretic Process Analysis (STPA) - a recognized system safety framework-for analyzing AI operation and development processes. We focus on systems that rely on machine learning algorithms and conducted STPA on three case studies involving linear regression, reinforcement learning, and transformer-based generative models. Our analysis explored how STPA's control and system-theoretic perspectives apply to Al systems and whether unique AI traits such as model opacity, capability uncertainty, and output complexity-necessitate significant modifications to the framework. We find that the key concepts and steps of conducting an STPA readily apply, albeit with a few adaptations tailored for AI systems. We present the Process-oriented Hazard Analysis for AI Systems (PHASE) as a guideline that adapts STPA concepts for AI, making STPA-based hazard analysis more accessible. PHASE enables four key affordances for analysts responsible for managing Al system harms: 1) detection of hazards at the systems level, including those from accumulation of disparate issues; 2) explicit acknowledgment of social factors contributing to experiences of algorithmic harms; 3) creation of traceable accountability chains between harms and those who can mitigate the harm; and 4) ongoing monitoring and mitigation of new hazards.", "sections": [{"title": "Introduction", "content": "Analyzing hazards - potential sources of harm - that render AI systems to have adverse impacts for individuals and society is a daunting task owing to several factors. First, the inner workings of AI systems are opaque and complex, making it difficult for even developers and auditors to identify potential harms (Burrell 2016). Unlike static physical products, some AI components, like predictive models re-trained on incoming data, continuously evolve, complicating the assessment of their limitations. The wide range of possible outputs, especially in large generative models, further complicates predicting negative impacts. Additionally, the decentralized and fragmented nature of AI development, split between open-source and proprietary environments, hinders harm identification. Finally, AI can cause not only traditional harms but also less tangible ones, such as psychological harm, requiring an approach that considers social dynamics. In response to these challenges, we advocate for a shift from merely evaluating AI system components (i.e., models, datasets) to frameworks that prioritize analyzing hazards by examining the processes and interactions involved in creating and deploying AI systems-an approach we believe is essential for effective hazard management, especially given the rapidly evolving nature of AI technology.\nExisting hazard analysis approaches for AI systems take a component-centric approach and afford developers and analysts to evaluate specific elements of an Al system, such as identification of harmful content in training datasets, examination of how AI algorithms prioritize utility over privacy or fairness, and detection of imbalances in model outputs across protected attributes. Although an essential first step, examining these components in silos ignores the impact of integrating them with each other, such as the effects of combining a harmful training dataset with a \"fair\" classifier that satisfies a specific fairness constraint (Biswas and Rajan 2021). Additionally, the component-level analysis overlooks how they are situated socially, including the roles of decision-makers who develop or deploy them and the experiences of users interacting with the AI system, which may lead to emergent forms of harm that are not possible to anticipate at the level of model parameters.\nExamining the interactions between different components of an AI system and how they are situated in a social system of decision makers, developers and users are necessary for comprehensive analysis of potential sources of harms from Al systems. Recent research underscores the importance of viewing Al systems as integrated wholes, not just isolated parts, when documenting them (Research; Gilbert et al. 2022). Furthermore, current research highlights how decisions on governance, design, and deployment along the AI supply chain are critical moments where potential harms can emerge (Widder and Nafus 2022). In addition, researchers are now using methods from HCI and community-based research to understand the harms arising from the interaction"}, {"title": "Background", "content": "In this section, we situate our work in today's AI safety discourse, Responsible AI (RAI), and system safety literature. We then briefly describe the key tenants and the steps of the STPA framework in Section 2.4.\nNote that in this work, we use the Organisation for Economic Co-operation and Development (OECD) definition of an AI system: \u201ca machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.\u201d (OECD 2024). Rather than other existing definitions of AI that sometimes emphasize certain techniques (e.g., ML), our intention in adopting this definition is to encompasses a wide variety of technologies that can be considered as AI."}, {"title": "Algorithmic harm and approaches to responsible and safe AI", "content": "Alongside the increased public awareness of how AI systems can introduce to societal disruptions and harm to individuals, RAI scholars have been actively studying the various types of algorithmic harms and means of harm mitigation. Shelby et al. (2023), for example, presents at least 170 articles related to sociotechnical harms of algorithmic systems - defined as \u201cadverse lived experiences of individuals and communities.\" The rapid advancements and adoption of large-scale, multi-modal, and general-purpose ML models in the last five years have further intensified the discussions about AI-related harms (Bender et al. 2021; Birhane, Prabhu, and Kahembwe 2021; Solaiman 2023). The emerging epistemic community of AI safety is also drawing more attention to concerns about AI-driven existential and catastrophic risks to humanity (Ahmed et al. 2023; Bucknall and Dori-Hacohen 2022; Russell 2022). To date, the types of harm that can be expected from current and near-future AI systems remains a subject of heated debate.\nAlongside these discussions are multitudes of theoretical frameworks, tools, and processes proposed to analyze and mitigate these harms. For instance, there are qualitative frameworks that help assess the societal and normative impact of AI systems (Watkins et al. 2021; Mantelero and Esposito 2021; Reisman et al. 2018); Technical tools exist in the form of various software libraries and code bases that can be used to assess and manipulate datasets and models (Barocas et al. 2021; ODSC-Open Data Science 2022); Safety analysis tools \u2013 inspired by processes such as red teaming (Ganguli et al. 2022) or Failure Mode and Effects Analysis (FMEA) \u2013 help identify potential points of failures of an Al model (Rismani and Moon 2021; Raji et al. 2020; Li and Chignell 2022); More recently, participatory and community-based research methods are being adopted to examine the impact of interactions between AI systems, individuals, and communities (Lima, Grgi\u0107-Hla\u010da, and Cha 2023; Blodgett et al. 2022; Wenzel et al. 2023; Bai et al. 2022).\nHowever, existing methods are designed to be effective and powerful in specific contexts, focusing mostly on analyzing components of an Al system (e.g., dataset, model) and its processes (e.g., human-AI interaction) in silos without examining their connections (Wong, Madaio, and Merrill 2023; Vera Liao and Xiao 2023). Moreover, implementation of these tools, frameworks, and processes remain ad-hoc in practice where harms identification, analysis, and mitigation are done independently by different actors even within the same organization (Rakova et al. 2021; Rismani et al. 2023; Madaio et al. 2022; Hopkins and Booth 2021).\nYet, harms from AI system are sociotechnical and emergent. That is, harms from a system they can stem from accumulation of issues considered to be minor by different individuals at component-level considerations and changes to one part of the system can cascade into different forms of failures in another component. As such, a comprehensive accounting of an AI system's algorithmic harms can only be understood and addressed across the interactions of technical artifacts, social agents (individuals, teams, human processes), and institutional mechanisms (rules, standards, protocols, cultural norms) (Selbst et al. 2019; Emery 2016; Lazar and Nelson 2023)."}, {"title": "System Safety", "content": "Safety engineering frameworks have been used to establish safety of technical systems as early as 1940s with the use of FMEA and Fault Tree Analysis (FTA) in military-based technology and the Apollo mission (Carlson 2012; Swuste 2022; Calder 1899). These early frameworks took on an analytical reductive approach where component failures and human errors can be analyzed as a chain of events that lead to an incident.\nWith the increase in complexity of technological systems and specifically the introduction of software, the analytic reductive approach deemed ineffective in capturing new types of hazards. In the early 2000s, Nancy Leveson proposed an alternative accident model called Systems-Theoretic Accident Model and Processes (STAMP) (Leveson 2004). Consistent with system theory (Von Bertalanffy 1972), STAMP frames safety as an emergent property of a system, where the different constraints imposed on the system's processes control these properties (Leveson 2011). This model of safety motivated the system safety community to look beyond the analysis of units and components, and consider implementing proper constraints when multiple software programs need to be integrated to work as a whole (Leveson 2017, 1986; Knight 2002).\nSubsequently, Leveson and Thomas developed STPA as a hazard analysis method based on STAMP (Leveson and Thomas 2018). Currently, STPA is applied by system and safety experts across safety-critical application domains (Park et al. 2022; Chen et al. 2020; Bas 2020)."}, {"title": "System Safety for AI Systems", "content": "The system theoretic perspective recognizes that AI models do not exist in silos in practical product environments. They are often integrated in a complex network of API calls, other Al models, a range of datasets and model outputs, and various user interfaces (Nabavi and Browne 2023; Widder and Nafus 2022; Crawford 2022). In addition, an Al system design is influenced by the decisions of a wide range of actors, including the data curation team, model development team, responsible AI analysts, users, company executives, product managers, and regulators (Sloane and Zakrzewski 2022; Figueras, Verhagen, and Pargman 2022; M\u00e4ntym\u00e4ki et al. 2022).\nIdentifying and mitigating harms from Al systems requires a systematic approach that explores and maps out the distribution of existing product development practices beyond an ML model and across individuals, automated systems, collectives and institutional mechanisms (Dobbe 2022; Lazar and Nelson 2023; Jatho et al. 2023; Kroll 2021). Narrowly examining and designing for safety at the ML model level is important but not sufficient for ensuring the safety of products that only partially rely on these models (Brundage et al. 2020; Khlaaf 2023; Mantelero and Esposito 2021).\nWe therefore define unsafe Al system to be an AI system that displays or is subjected to inadequate control which adversely affects people and the environment. Seen through a control theory perspective, an AI can not be only modeled"}, {"title": "System Theoretic Process Analysis (STPA)", "content": "Below, we provide a brief description of the key steps for conducting STPA outlined in the STPA handbook (Leveson and Thomas 2018). These steps are meant to be performed iteratively and cyclically across the lifecycle of technology development.\nIdentify purpose of the analysis. The STPA process starts by identifying losses and hazards. By definition, a loss occurs when a stakeholder's value is disregarded or violated. Hazards are a system state or conditions that could be a potential source of harm. Hazards are defined per system boundary, which outlines parts of the system where designers, developers, engineers, or automated agents could exercise some level of control. To provide a simplistic example, a construction company repairing a roof of a house may point to safety considerations (loss of human life) to be important for the project; identify the possibility of workers falling from the steep and bare edges of the rooftop as a fall hazard; and is only able to install safety equipment such as fences within the boundaries of the roof being renovated and not that of the neighbouring house (system boundary).\nCreate control diagrams. The next step involves creating control diagrams to visually capture the model of the system within its system boundary. It represents the elements and interactions between different elements within the system in the form of feedback loop called control feedback loop.\nThe two key elements of a control feedback loop are the controller \u2013 an algorithm, a person, or an organization that can make decisions or perform actions to control something \u2013 and the controlled process \u2013 the component, process, or another controller being controlled by the controller. A controller is represented with a control algorithm \u2013 a description of how the controller makes decisions \u2013 and process model \u2013 the controller's internal/mental model the environment used to make decisions.\nThe relationship between controllers and controlled processes are represented with two types of interactions: control action which is a constraint that the controller puts on the controlled process, and feedback which is the response/feedback the controlled process provides to the controller. Typically, the various controller, controlled processes and"}, {"title": "Methodology", "content": "Figure 1 presents an overview of our research method. As Njie and Asimirian outline, case studies as a research method enables in-depth and first-hand investigation of events and processes (Njie and Asimiran 2014). Therefore, we used case studies to investigate how STPA needs to be interpreted when it is used to analyze AI systems. The insights we gathered from the case study led to the development of PHASE, a guideline for conducting an STPA of AI. Subsequently, we conducted an iterative reflexive analysis to identify key affordances that PHASE provides. We improved the clarity of the language used in the guideline upon receiving feedback from four individuals who pilot tested an early draft. Here, we describe the case studies, the details of applying the STPA framework, and the process of developing reflexively analyzing PHASE."}, {"title": "Case studies", "content": "To illustrate the applicability of STPA we chose the following real-world case studies based on the diverse range of ML model they represent, application context, and the nature of interaction between the AI system and a human user.\n\u2022 Case 1 (C1) - Early warning system: A linear regression algorithm is being designed in the Netherlands to predict the likelihood of late-onset sepsis (LOS) in preterm infants before a clinical suspicion (van den Berg et al. 2023). It is to be used as a proactive warning system alerting clinicians to start early diagnostic testing for LOS and, ultimately, provide timely treatment. As a human-in-the-loop system, a clinician makes final, clinical decisions.\n\u2022 Case 2 (C2) Insulin Injection: An Reinforcement Learning (RL)-based, automated and personalized insulin injection system is being developed in Canada for patients with diabetes (Basu et al. 2023). Some diabetic patients use artificial pancreas that facilitates automatic insulin injection. The RL algorithm is being developed for use in the artificial pancreas to allow tailored insulin doses for each individual in a human-out-of-the-loop manner.\n\u2022 Case 3 (C3) - Story boarding: Some ML artists have adapted Text-to-image (T2I) models as part of their creative practice such as storyboarding for video creation (Yu et al. 2022; Franceschelli and Musolesi 2021; Saharia et al. 2022; Li and Chignell 2022). In this case study, we focus on T2I demo platforms generally. While we do not focus on a specific platform, many such platforms are produced by companies in the United States, and rely on some combination of transformer-based or diffusion architecture T2I generative models."}, {"title": "Applying the STPA framework", "content": "To apply STPA, we started by gathering information about the AI system, its stakeholders, and how the system was developed and integrated in the given context. To do so, we connected with the leads and developers of each of the projects and conducted secondary research.\nConsidering time limitations, data availability, and access to subject-matter experts, additional information-gathering process was customized for each case. For C1, the lead author held a preliminary meeting with the ML developer and data scientist to learn more about the objectives of the project and used a peer-reviewed academic publication on the project as the main reference point for the information used for the analysis. For C2, the lead author held monthly meetings with two RL researchers one of whom developed RL algorithms for insulin injection. For C3, the lead author conducted eight interviews and one workshop (15 participants) involving experts along the T2I model development process and ML artists who use these models in their creative practice.\nAfter information gathering, the lead author followed the STPA steps outlined earlier (Section 2.4) and interpreted them for each case study. We then cross-checked the findings with existing literature and subject-matter experts to validate"}, {"title": "Developing the PHASE guideline", "content": "After interpreting and applying the STPA process to all three cases, the lead author generated a step-by-step translation of STPA for conducting hazard analysis of algorithmic harm from AI systems. These translations were converted into a guideline that could be used by practitioners who do not have previous training in system safety. Having undergone multiple rounds of iterative feedback from the co-authors, we produced the final guideline, Process-oriented Hazard Analysis for AI Systems (PHASE)."}, {"title": "PHASE Guideline: Interpreting STPA for AI System Case Studies", "content": "In this section, we describe how each of the four steps of STPA was interpreted across the three case studies (Section 4). The guideline encapsulating the final translations of STPA concepts and steps along with an illustrative analysis of an example case study are provided in the supplemental material."}, {"title": "Step 1: Identify purpose of analysis", "content": "Much of the first step of STPA (Section 2.4 - identify purpose of analysis) could be applied to our case studies without difficulty. However, we found that the types of losses and hazards considered for an STPA of an Al system are not generalizable across all systems and are heavily informed by the sociotechnical context within which the technology is developed and used.\nConsidering the growing body of research in algorithmic harms and our findings from the case studies, we found it much more appropriate to use existing taxonomies (Weidinger et al. 2022; Shelby et al. 2023; Bird, Ungless, and Kasirzadeh 2023), industry-specific regulatory requirements (euA 2023), and community-based and participatory research methods (DeVos et al. 2022; Wang et al. 2022b) for loss identification of AI systems. Furthermore, in the guideline, we outline a list of potential system boundaries for AI system and provide examples of hazards that capture a range of problematic contextual factors that lead to the aforementioned types of losses.\nIdentifying losses The losses identified for each case study go beyond the traditional definition of safety (e.g., loss of life, damage to property or environment). As shown in Table 1, the identified losses consider a broader set of emergent harms from AI systems. Our reflexive analysis reveals that there are three different categories of losses pertinent in an STPA of an ML system: 1) traditional safety-critical losses, 2) performance-related losses, 3) sociotechnical losses. Safety-critical losses represent losses in the traditional definition of safety such as loss of human life, property damage, and the environment (C1.L1). Performance-related losses, on the other hand, emerge when the AI system does not meet performance expectations (C1.L2) or"}, {"title": "Step 2: Create a control diagram", "content": "As described in Section 2.4, drawing a control diagram for each system boundary facilitates an understanding of the dynamics between the actors implicated in the system. This includes their levels of control over components of the system. We were able to translate the key concepts from the STPA framework and map the elements and interactions within our chosen system boundaries. From the case study analysis, we interpret what controllers, controlled processes, control actions, feedback, inputs, outputs, and control hierarchy could look like for AI system boundaries.\nFollowing the STPA process, we created a control diagram for each of the system boundaries, leading to a total of nine diagrams. Figure 3 shows one control diagram from each of the case studies. The sub-systems captured in the diagram as controllers or controlled processes ranged from individuals, teams, and technical artifacts. Across the case studies, many of the controllers were individuals or teams responsible for decision-making during the use or development of the process. For example for C1-SDI, all the controllers are individuals or teams such as the data collection team, data processing team, and hospital database manager. This would mean that technical artifacts (i.e., collected dataset) were controlled processes. Another example of technical artifacts as a controlled process is the input and output safety filters and T2I models in C3.SD2. However, individuals and teams were not always the controllers. For example, the controllers for C2-SD3 include technical artifacts such as the RL agent and the insulin pump. Moreover, depending on how the system is meant to be used, the AI system could have control over an individual or a team. For instance, if the use of an early alert system is mandated for physicians in C1-SD3, then the system constrains the decisions of the physician.\nThe top-to-bottom representation of the control hierarchy (highest level of control on top) adequately reflected the level of control one has over another. For many of the control diagrams, individuals or groups were higher in this hierarchy. However, as shown in the control diagram of C1-SD3 (Figure 3), the ordering of the control hierarchy is context-dependent.\nIn the case studies, control actions range from decisions that the AI developers make about dataset processing and model design, prompts that ML artists enter when using a T2I demo, to technical constraints such as setting a threshold for a filter. Feedback took the form of evaluation metrics, sensor outputs, user feedback, or other human communication forms. The input and output from the controlled and control processes relate to technical artifacts such as training dataset that is often input to the model development process. Moreover, input and output capture collaborations or com-"}, {"title": "Step 3: Identify unsafe control actions", "content": "The process of identifying UCA guided us to think through each one of the control actions with respected to the system-level hazards identified for a system boundary and its corresponding context (e.g., when, where, and how that control is executed). The context could include information about a particular dataset that is collected, how the team and their workflow are structured, or the preferences of the user.\nFrom our analysis, we identified three main types of UCAs applicable to AI systems.\nFunctional UCAS The most common type was functional UCAs where the inadequate control was a result of a malfunction/dysfunction of the controller. An example of this UCA is when the insulin pump in the artificial pancreas is not able to release the necessary amount of insulin at a given time (C2.SD2). An example from C1.SD3 include a wrong alarm from the system that could negatively affect the physician's decisions. These UCA could lead to all three types of system-level hazards and losses outlined in Section 4.1, although UCAs for C1 and C2 mostly connected to performance-based or safety-critical losses.\nUCAs from poor design decision-making or misuse Another type of UCA included cases of poor design decision-making or misuse of an Al system by expert(s), system developers or user(s). For example, in C3.SD2, the model developers could select an improper filter threshold which could lead to the inclusion of certain harmful prompts as the input or generation of harmful images. In C1.SD1, the data processing team could select incorrect clinical variables as feature variables that have information about clinical suspicions of late-onset sepsis.\nUCAs from communication/coordination The last type of UCAs from our analysis are cases including inadequate organizational coordination and lack of or miscommunications. For instance, the model developers might have received the wrong set of safety requirements for a model or they do not have sufficient time to address them before their launch timeline (C3.SD2)."}, {"title": "Step 4: Identify loss scenarios", "content": "The original STPA process was highly effective in the process of understanding loss scenarios for AI systems. We brainstormed a list of loss scenarios by thinking through all the elements of the control diagram. A diverse range of loss scenarios were identified, which included organizational/institutional causal scenarios (i.e., poor understanding or communication), interaction issues (e.g., incorrect mental model) and technical challenges (e.g., missing metrics/evaluations of a system). A common institutional causal scenario across our case studies involved missing feedback loops between different teams that are in charge of various parts of the development process. For instance, often model development teams use pre-existing datasets that have been col-"}, {"title": "Affordances of System Theoretic Perspective for algorithmic harm from AI systems", "content": "We identified four key affordances that STPA-based analysis of AI systems provide. Some of these affordances map directly to the affordances of traditional applications of STPA. For instance, the most obvious is that \u2013 unlike many other assessment frameworks that focus only on the model, dataset, or user interface \u2013 the framework allows one to analyze an AI system at the systems level, considering interaction and processes surrounding the various technical artifacts.\nOthers are newly enabled by PHASE. We find that the framework affords an analyst to a) account for social factors and sociotechnical harms; b) identify accountability chain by establishing traceability; c) monitor hazards stemming from the evolution and dynamics of AI systems, social actors, and their interactions over time."}, {"title": "Affordance 1: Analyze AI at the systems level", "content": "Since STPA is designed to analyze a process or an artifact at the systems level, PHASE inherits this affordance. Prior work suggests that algorithmic harms from AI arise from system-wide issues (Smuha 2021; Selbst 2021; Thomasen 2023). For instance, representational harms are system-level issues that \"occur when algorithmic systems reinforce the subordination of social groups along the lines of identity, such as disability, gender, race and ethnicity, religion, and sexuality\" (Shelby et al. 2023; Karizat et al. 2021). They arise from a combination of ad-hoc factors such as inaccurate ML model performance for minority groups, misuse of the system in a given application, or exclusionary social norms. Instead of narrowly focusing on \"fixing\" an unbalanced training dataset, system-level analysis prompts us to examine how this dataset will interact with other parts of the system, such dataset analysts, other ML models, or the user interface (Barocas et al. 2021; Wang et al. 2022a).\nSTPA is a systematic method that establishes possible causal chains between undesirable outcomes of the system and inadequate control. As such, the losses and harms identified in the first step of PHASE (Section 4.1) define the negative impact of the system an analyst aims to prevent or mitigate. Subsequently, UCAs (Section 4.3) explicitly define all possible cases of inadequate control in the chosen system boundary that lead to a hazard. Loss scenarios (Section 4.4) finally establish the causal link connecting the undesirable outcomes to specific unsafe states of the system. For example, we identified the hazard \u201cH: Clinician misdiagnoses the patient.\" to stem from inadequate control over the proper use of the early alarm system and timing for ordering a blood test, which can lead to L: loss of life. In this case, the control action of \u201cclinical use of the early alert system\" could be unsafe and in several loss scenarios, such as when the clinician does not understand how to use the early alert system."}, {"title": "Affordance 2: Accounting for social factors and sociotechnical harms", "content": "Formalizing losses at the beginning of STPA mobilizes a discussion and instantiates the need for consensus amongst stakeholders about harms that must be prioritized. Amongst practitioners and scholars, there is a divergence of perspectives on the types of harm that need to be assessed and mitigated for AI systems (S\u00e6tra and Danaher 2023; Lazar and Nelson 2023; Hanna and Bender 2023). Currently, company policies and ad-hoc conversations amongst limited, invited individuals shape practitioners' choices on what harms need to be minimized or fully prevented (ope 2023). However, following PHASE requires analysts to consider a broad set of sociotechnical harms. Furthermore, in defining the system boundary, PHASE connects the losses to a network of individuals, teams, technical artifacts, automated systems, and institutional mechanisms which form a system boundary along the AI development cycle as opposed to just examining a loss by focusing on one of these elements (i.e. blaming a developer or assuming ML models cannot be explained). Lastly, the control diagram and its components in PHASE provide the conceptual flexibility to map social and technical elements within a system boundary. For example, a controller could be a human (e.g., artist) who has a specific mental model of how to use a T2I platform. It could also be an RL agent with a map of the environment (process model) and a reward model (control algorithm) that guides the suggested dosage of insulin (control actions). This conceptual flexibility allows for the integration and prioritization of sociotechnical factors in AI harms analysis."}, {"title": "Affordance 3: Identify accountability chain by establishing traceability", "content": "Traceability in harm reduction is a pressing concern in the AI community. Recent studies indicate that responsible AI practitioners are frustrated by the lack of systematic methods that establish a connection between a specific set of harms and the means to mitigate the harm (Madaio et al. 2022; Rakova et al. 2021; Sloane and Zakrzewski 2022). This problem is exacerbated by the fact that many systems are developed by a multitude of individuals and groups across institutions that hardly interact directly with one another (Widder and Nafus 2022; Nabavi and Browne 2023). That is, many unsafe system states can accumulate between individual decision-makers, the technical system, and the development process without the situation being detected or managed (Widder and Nafus 2022; Kroll 2021). As described earlier, the STPA framework allows us to connect harms to inadequate control within a system. Furthermore, it allows an analyst to make explicit who has control over what, and make the harms and hazardous states of a system traceable for everyone. Specifically, the concept of hierarchical control structure as interpreted in PHASE engages the analyst to map who or what has control in relation to other actors or artifacts by mapping their interaction using control action and feedback. For instance, by drawing the control structure, we needed to examine whether the clinician had control over the early alert system meaning that they could choose to"}, {"title": "Affordance 4: Monitor and address emergent hazards over time", "content": "STPA framework enables an analyst to monitor and account for changes in individuals, teams, technical artifacts, and institutional mechanisms over time. As C2 illustrates, this includes the changes that stem from the adaptivity of AI systems such as RL. These adaptations can be reflected in various components of a control diagram, such as control actions, feedback loops, and elements of a controller (e.g., decision-making protocol, process/mental model of the world). For instance, the change in the output of an AI model could be captured in a control action as constraints/information a clinician or an artist receives. Furthermore, changes in an AI model's latent representation could be captured in the process model of the controller or controlled process (depending on whether a model is acting as a controller or is a controlled process). Changes in how individuals, teams, and organizations perceive and operate such as changes in how an AI model is evaluated could be captured as part of the feedback loop between the model and the developer team. Such concrete means of accounting for changes in the system allow analysts to monitor the dynamics systematically and iteratively, prompting them to examine emergent hazards as AI systems change in how they are integrated or interact with their sociotechnical context. This is a critical affordance given the rapid pace of AI development and increased interest in adaptive and agentic AI systems (Shavit et al. 2023; Weidinger et al. 2023)."}, {"title": "Discussion", "content": "In this paper, we employed a case study approach to translate STPA for the analysis of AI systems. We illustrated that STPA framework can not only be translated to analyze AI systems and identify potential sources of algorithmic harm, but that the PHASE implementation of the analysis enables four affordances critically missing in existing AI assessment frameworks. STPA framework allows analysts to map out sociotechnical system dynamics, and instantiate accountability by examining harms at the system level and tracing them to control methods for harm reduction. Moreover, the framework allows for accounting of changes in individuals, collectives, AI models and artifacts, and institutional processes that could happen over time. Here, we discuss the broader implications of adopting a system theoretic perspective for AI governance and building of responsible AI culture in organizations. We also elaborate on limitations and avenues for future work."}, {"title": "Implications for policy and AI governance", "content": "From self-governance mechanisms in corporations to national regulations on AI, there has been a growing effort to practice due diligence and establish accountability mechanisms across the AI industry (The White House 2023; euA 2023). As illustrated in our findings (Section 5.3), establishing traceability between harms and causal scenarios can be a powerful tool toward this end. It can enable an analyst to define a division of accountability where specific roles/groups are held responsible for maintaining adequate/safe control of certain sub-systems (Leveson 2011). Although our system boundaries did not explicitly focus on internal or external governance mechanisms, some of the controls added by internal governance mechanisms were observed in our case studies. For example, for C3-SD1 as captured in Figure 3, responsible AI analyst provides input on safety requirements to product managers about datasets which then is passed onto the data scientists. These safety requirements would typically be developed based on existing regulations or internal company policies (ope 2023). Given that STPA has already been used in other domains to examine and improve governance mechanisms, we expect PHASE to be able to afford the same function for AI system (Leveson 2004).\nFor example, such a process could point out the missing feedback loop between the data scientist and the responsible AI analyst in C3-SD1, which hints at the lack of quality control of the training dataset and the potential need for a data quality approval mechanism. In addition to improving internal accountability protocols for a company, external actors such as standard organizations could leverage these affordances to establish best practices for conducting hazard analysis, and mobilize policymakers to refer to these standards in the regulation for certain AI products (Suo et al. 2017)."}, {"title": "Building a safety culture: systems lens for responsible and safe AI", "content": "Given the focus of our three case studies in the product development context, we also observed how existing culture within the organization and management practices can contribute to sociotechnical harm. For example, in C3-SD1, the loss scenarios for the UCA of poor safety filter design could stem from a responsible AI analyst not having enough time to review the dataset and provide safety requirements to the product manager before the product needs to be launched.\nThe presence of toxic or profit-driven culture"}]}