{"title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks", "authors": ["Yunfan Gao", "Yun Xiong", "Meng Wang", "Haofen Wang"], "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of \"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.", "sections": [{"title": "I. INTRODUCTION", "content": "ARGE Language Models (LLMs) have demonstrated remarkable capabilities, yet they still face numerous challenges, such as hallucination and the lag in information updates [1]. Retrieval-augmented Generation (RAG), by accessing external knowledge bases, provides LLMs with important contextual information, significantly enhancing their performance on knowledge-intensive tasks [2]. Currently, RAG, as an enhancement method, has been widely applied in various practical application scenarios, including knowledge question answering, recommendation systems, customer service, and personal assistants. [3]-[6]\nDuring the nascent stages of RAG, its core framework is constituted by indexing, retrieval, and generation, a paradigm referred to as Naive RAG [7]. However, as the complexity of tasks and the demands of applications have escalated, the limitations of Naive RAG have become increasingly apparent. As depicted in Figure 1, it predominantly hinges on the straightforward similarity of chunks, result in poor performance when confronted with complex queries and chunks with substantial variability. The primary challenges of Naive RAG include: 1) Shallow Understanding of Queries. The semantic similarity between a query and document chunk is not always highly consistent. Relying solely on similarity calculations for retrieval lacks an in-depth exploration of the relationship between the query and the document [8]. 2) Retrieval Redundancy and Noise. Feeding all retrieved chunks directly into LLMs is not always beneficial. Research indicates that an excess of redundant and noisy information may interfere with the LLM's identification of key information, thereby increasing the risk of generating erroneous and hallucinated responses. [9]\nTo overcome the aforementioned limitations, Advanced RAG paradigm focuses on optimizing the retrieval phase, aiming to enhance retrieval efficiency and strengthen the utilization of retrieved chunks. As shown in Figure 1, typical strategies involve pre-retrieval processing and post-retrieval processing. For instance, query rewriting is used to make the queries more clear and specific, thereby increasing the accuracy of retrieval [10], and the reranking of retrieval results is employed to enhance the LLM's ability to identify and utilize key information [11].\nDespite the improvements in the practicality of Advanced RAG, there remains a gap between its capabilities and real-world application requirements. On one hand, as RAG technology advances, user expectations rise, demands continue to evolve, and application settings become more complex. For instance, the integration of heterogeneous data and the new demands for system transparency, control, and maintainability. On the other hand, the growth in application demands has further propelled the evolution of RAG technology.\nAs shown in Figure 2, to achieve more accurate and efficient task execution, modern RAG systems are progressively integrating more sophisticated function, such as organizing more refined index base in the form of knowledge graphs, integrating structured data through query construction methods, and employing fine-tuning techniques to enable encoders to better adapt to domain-specific documents.\nIn terms of process design, the current RAG system has surpassed the traditional linear retrieval-generation paradigm. Researchers use iterative retrieval [12] to obtain richer context, recursive retrieval [13] to handle complex queries, and adaptive retrieval [14] to provide overall autonomy and flexibility. This flexibility in the process significantly enhances the expressive power and adaptability of RAG systems, enabling them to better adapt to various application scenarios. However, this also makes the orchestration and scheduling of workflows more complex, posing greater challenges to system design. Specifically, RAG currently faces the following new challenges:\nComplex data sources integration. RAG are no longer confined to a single type of unstructured text data source but have expanded to include various data types, such as semi-structured data like tables and structured data like knowledge graphs [15]. Access to heterogeneous data from multiple sources can provide the system with a richer knowledge background, and more reliable knowledge verification capabilities [16].\nNew demands for system interpretability, controllability, and maintainability. With the increasing complexity of systems, system maintenance and debugging have become more challenging. Additionally, when issues arise, it is essential to quickly pinpoint the specific components that require optimization.\nComponent selection and optimization. More neural networks are involved in the RAG system, necessitating the selection of appropriate components to meet the needs of specific tasks and resource configurations. Moreover, additional components enhance the effectiveness of RAG but also bring new collaborative work requirements [17]. Ensuring that these models perform as intended and work efficiently together to enhance the overall system performance is crucial.\nWorkflow orchestration and scheduling. Components may need to be executed in a specific order, processed in parallel under certain conditions, or even judged by the LLM based on different outputs. Reasonable planning of the workflow is essential for improving system efficiency and achieving the desired outcomes [18].\nTo address the design, management, and maintenance challenges posed by the increasing complexity of RAG systems, and to meet the ever-growing and diverse demands and expectations, this paper proposes Modular RAG architecture. In modern computing systems, modularization is becoming a trend. It can enhance the system's scalability and maintainability and achieve efficient task execution through process control.\nThe Modular RAG system consists of multiple independent yet tightly coordinated modules, each responsible for handling specific functions or tasks. This architecture is divided into three levels: the top level focuses on the critical stages of RAG, where each stage is treated as an independent module. This level not only inherits the main processes from the Advanced RAG paradigm but also introduces an orchestration module to control the coordination of RAG processes. The middle level is composed of sub-modules within each module, further refining and optimizing the functions. The bottom level consists of basic units of operation-operators. Within the Modular RAG framework, RAG systems can be represented in the form of computational graphs, where nodes represent specific operators. The comparison of the three paradigms is shown in the Figure 3. Modular RAG evolves based on the previous development of RAG. The relationships among these three paradigms are ones of inheritance and development. Advanced RAG is a special case of Modular RAG, while Naive RAG is a special case of Advanced RAG.\nThe advantages of Modular RAG are significant, as it enhances the flexibility and scalability of RAG systems. Users can flexibly combine different modules and operators according to the requirements of data sources and task scenarios. In summary, the contributions of this paper are as follows:\n\u2022\tThis paper proposes a new paradigm called modular RAG, which employs a three-tier architectural design comprising modules, sub-modules, and operators to define the RAG system in a unified and structured manner. This design not only enhances the system's flexibility and scalability but also, through the independent design of operators, strengthens the system's maintainability and comprehensibility.\n\u2022\tUnder the framework of Modular RAG, the orchestration of modules and operators forms the RAG Flow, which can flexibly express current RAG methods. This paper has further summarized six typical flow patterns and specific methods have been analyzed to reveal the universality of modular RAG in practical scenarios.\n\u2022\tThe Modular RAG framework offers exceptional flexibility and extensibility. This paper delves into the new opportunities brought by Modular RAG and provides a thorough discussion on the adaptation and expansion of new methods in different application scenarios, offering guidance for future research directions and practical exploration."}, {"title": "II. RELATED WORK", "content": "The development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multi-step retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28].\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications. They emphasize the importance of considering robustness, accuracy, and relevance when evaluating RALMs and propose several evaluation methods. Ding et al., [31] provide a comprehensive review from the perspectives of architecture, training strategies, and applications. They specifically discuss four training methods of RALMs: training-free methods, independent training methods, sequence training methods, and joint training methods, and compare their advantages and disadvantages. Zhao et al., [32]analyze the applications of RAG technology in various fields such as text generation, code generation, image generation, and video generation from the perspective of augmented intelligence with generative capabilities.\nThe current collation of RAG systems primarily focuses on methods with a fixed process, mainly concerned with optimizing the retrieval and generation stages. However, it has not turned its attention to the new characteristics that RAG research is continuously evolving, namely the characteristics of process scheduling and functional componentization. There is currently a lack of comprehensive analysis of the overall RAG system, which has led to research on paradigms lagging behind the development of RAG technology."}, {"title": "III. FRAMEWORK AND NOTATION", "content": "For query $Q = {q_i}$, a typical RAG system mainly consists of three key components. 1) Indexing. Given documents $D = {d_1,d_2,...,d_n}$, where $d_i$ represents the document chunk. Indexing is the process of converting $d_i$ into vectors through an embedding model $f_e(\u00b7)$, and then store vectors in vector database.\n$I = {e_1, e_2, ...,e_n}$ and $e_i= f_e(d_i) \u2208 R^d$    (1)\n2) Retrieval. Transform the query into a vector using the same encoding model, and then filter out the top k document chunks that are most similar based on vector similarity.\n$R: topk Sim(q, d_i) \u2192 D_q$    $d_i ED$    (2)\n$D_q = {d_1,d_2,...,d_k}$ represents the relevant documents for question q. The similarity function $Sim(\u00b7)$ commonly used are dot product or cosine similarity.\n$Sim(q, d_i) = \\frac{e_q \\cdot e_{di}}{||e_q||||e_{di} ||} or  e_q \\cdot e_{di}$    (3)\n3) Generation. After getting the relevant documents. The query q and the retrieved document $D^q$ chunks are inputted together to the LLM to generate the final answer, where $[\u00b7,\u00b7]$ stands for concatenation.\n$y = LLM([D^q, q])$   (4)"}, {"title": "IV. MODULE AND OPERATOR", "content": "This chapter will specifically introduce modules and operators under the Modular RAG framework. Based on the current stage of RAG development, we have established six main modules: Indexing, Pre-retrieval, Retrieval, Post-retrieval, Generation, and Orchestration.\nA. Indexing\nIndexing is the process of split document into manageable chunks and it is a key step in organizing a system. Indexing faces three main challenges. 1) Incomplete content representation. The semantic information of chunks is influenced by the segmentation method, resulting in the loss or submergence of important information within longer contexts. 2) Inaccurate chunk similarity search. As data volume increases, noise in retrieval grows, leading to frequent matching with erroneous data, making the retrieval system fragile and unreliable. 3) Unclear reference trajectory. The retrieved chunks may originate from any document, devoid of citation trails, potentially resulting in the presence of chunks from multiple different documents that, despite being semantically similar, contain content on entirely different topics.\n1) Chunk Optimization: The size of the chunks and the overlap between the chunks play a crucial role in the overall effectiveness of the RAG system. Given a chunk $d_i$, its chunk size is denoted as $L_i = |d_i|$, and the overlap is denoted as $L_i = |d_i \u2229 d_{i+1}|$. Larger chunks can capture more context, but they also generate more noise, requiring longer processing time and higher costs. While smaller chunks may not fully convey the necessary context, they do have less noise [17].\nSliding Window using overlapping chunks in a sliding window enhances semantic transitions. However, it has limitations such as imprecise context size control, potential truncation of words or sentences, and lacking semantic considerations.\nMetadata Attachment. Chunks can be enriched with metadata like page number, file name, author, timestamp, summary, or relevant questions. This metadata allows for filtered retrieval, narrowing the search scope.\nSmall-to-Big [33] separate the chunks used for retrieval from those used for synthesis. Smaller chunks enhance retrieval accuracy, while larger chunks provide more context. One approach is to retrieve smaller summarized chunks and reference their parent larger chunks. Alternatively, individual sentences could be retrieved along with their surrounding text.\n2) Structure Organization: One effective method for enhancing information retrieval is to establish a hierarchical structure for the documents. By constructing chunks structure, RAG system can expedite the retrieval and processing of pertinent data.\nHierarchical Index. In the hierarchical structure of documents, nodes are arranged in parent-child relationships, with chunks linked to them. Data summaries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract. This approach can also mitigate the illusion caused by chunk extraction issues. The methods for constructing a structured index primarily include: 1) Structural awareness based on paragraph and sentence segmentation in docs. 2) Content awareness based on inherent structure in PDF, HTML, and Latex. 3) Semantic awareness based on semantic recognition and segmentation of text.\nKG Index [34]. Using Knowledge Graphs (KGs) to structure documents helps maintain consistency by clarifying connections between concepts and entities, reducing the risk of mismatch errors. KGs also transform information retrieval into instructions intelligible to language models, improving retrieval accuracy and enabling contextually coherent responses. This enhances the overall efficiency of the RAG system. For example, organizing a corpus in the format of graph $G = {V, E, X}$, where node $V = {v_i}_{i=1}^n$ represent document structures (e.g.passage, pages, table), edge $E \u2282 V \u00d7 V$ represent semantic or lexical similarity and belonging relations, and node features $X = {X_i}_{i=1}^n$ represent text or markdown content for passage.\nB. Pre-retrieval\nOne of the primary challenges with Naive RAG is its direct reliance on the user's original query as the basis for retrieval. Formulating a precise and clear question is difficult, and imprudent queries result in subpar retrieval effectiveness. The primary challenges in this module include: 1) Poorly worded queries. The question itself is complex, and the language is not well-organized. 2) Language complexity and ambiguity. Language models often struggle when dealing with specialized vocabulary or ambiguous abbreviations with multiple meanings. For instance, they may not discern whether LLM refers to Large Language Model or a Master of Laws in a legal context.\n1) Query Expansion: Expanding a single query into multiple queries enriches the content of the query, providing further context to address any lack of specific nuances, thereby ensuring the optimal relevance of the generated answers.\n$f_{qe}(q) = {q_1,q_2,\u2026\u2026\u2026,q_n} \u2200q_i\u2208 {q_1, q_2,..., q_n}, q_i \u2209 Q$   (7)\nMulti-Query uses prompt engineering to expand queries via LLMs, allowing for parallel execution. These expansions are meticulously designed to ensure diversity and coverage. However, this approach can dilute the user's original intent. To mitigate this, the model can be instructed to assign greater weight to the original query.\nSub-Query. By decomposing and planning for complex problems, multiple sub-problems are generated. Specifically, least-to-most prompting [35] can be employed to decompose the complex problem into a series of simpler sub-problems. Depending on the structure of the original problem, the generated sub-problems can be executed in parallel or sequentially. Another approach involves the use of the Chain-of-Verification (CoVe) [36]. The expanded queries undergo validation by LLM to achieve the effect of reducing hallucinations.\n2) Query Transformation: Retrieve and generate based on a transformed query instead of the user's original query.\n$f_{qt}(q) = q'$   (8)\nRewrite. Original queries often fall short for retrieval in real-world scenarios. To address this, LLMs can be prompted to rewrite. Specialized smaller models can also be employed for this purpose [24]. The implementation of the query rewrite method in Taobao has significantly improved recall effectiveness for long-tail queries, leading to an increase in GMV [10].\nHyDE [37]. In order to bridge the semantic gap between questions and answers, it constructs hypothetical documents (assumed answers) when responding to queries instead of directly searching the query. It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. In addition, it also includes reverse HyDE, which generate hypothetical query for each chunks and focuses on retrieval from query to query.\nStep-back Prompting [38]. The original query is abstracted into a high-level concept question (step-back question). In the RAG system, both the step-back question and the original query are used for retrieval, and their results are combined to generate the language model's answer.\n3) Query Construction: In addition to text data, an increasing amount of structured data, such as tables and graph data, is being integrated into RAG systems. To accommodate various data types, it is necessary to restructure the user's query. This involve converting the query into another query language to access alternative data sources, with common methods including Text-to-SQL or Text-to-Cypher . In many scenarios, structured query languages (e.g., SQL, Cypher) are often used in conjunction with semantic information and metadata to construct more complex queries.\n$f_{qc}(q) = q^*, q^* \u2208 Q^* = {SQL,Cypher, . . . }$   (9)\nC. Retrieval\nThe retrieval process is pivotal in RAG systems. By leveraging powerful embedding models, queries and text can be efficiently represented in latent spaces, which facilitates the establishment of semantic similarity between questions and documents, thereby enhancing retrieval. Three main considerations that need to be addressed include retrieval efficiency, quality, and the alignment of tasks, data and models.\n1) Retriever Selection: With the widespread adoption of RAG technology, the development of embedding models has been in full swing. In addition to traditional models based on statistics and pre-trained models based on the encoder structure, embedding models fine-tuned on LLMs have also demonstrated powerful capabilities [39]. However, they often come with more parameters, leading to weaker inference and retrieval efficiency. Therefore, it is crucial to select the appropriate retriever based on different task scenarios.\nSparse Retriever uses statistical methods to convert queries and documents into sparse vectors. Its advantage lies in its efficiency in handling large datasets, focusing only on non-zero elements. However, it may be less effective than dense vectors in capturing complex semantics. Common methods include TF-IDF and BM25.\nDense Retriever employs pre-trained language models (PLMs) to provide dense representations of queries and documents. Despite higher computational and storage costs, it offers more complex semantic representations. Typical models include BERT structure PLMs, like ColBERT, and multi-task fine-tuned models like BGE [40] and GTE [41].\nHybrid Retriever is to use both sparse and dense retrievers simultaneously. Two embedding techniques complement each other to enhance retrieval effectiveness. Sparse retriever can provide initial screening results. Additionally, sparse models enhance the zero-shot retrieval capabilities of dense models, particularly in handling queries with rare entities, thereby increasing system robustness.\n2) Retriever Fine-tuning: In cases where the context may diverge from pre-trained corpus, particularly in highly specialized fields like healthcare, law, and other domains abundant in proprietary terminology. While this adjustment demands additional effort, it can substantially enhance retrieval efficiency and domain alignment.\nSupervised Fine-Tuning (SFT). Fine-tuning a retrieval model based on labeled domain data is typically done using contrastive learning. This involves reducing the distance between positive samples while increasing the distance between negative samples. The commonly used loss calculation is shown in the following:\n$L(D_R) = - \\frac{1}{T} \\sum_{i=1}^T log(\\frac{e^{(sim(q_i,d_i^+))}}{e^{(sim(q_i,d_i^+))} + \\sum_{j=1}^N e^{(sim(q_i,d_i^-))}})$   (10)\nwhere $d_i^+$ is the positive sample document corresponding to the i-th query, $d_i^-$ is several negative sample, T is the total number of queries, N is the number of negative samples, and $D_R$ is the fine-tuning dataset.\nLM-supervised Retriever (LSR). In contrast to directly constructing a fine-tuning dataset from the dataset, LSR utilizes the LM-generated results as supervisory signals to fine-tune the embedding model during the RAG process.\n$P_{LSR}(d|q,y) = \\frac{e^{P_{LM}(y|d,q)/\u03b2}}{\\sum_{d'\u2208D} e^{P_{LM}(y|d,q)/\u03b2}}$   (11)\n$P_{LM}(y|d, q)$ is LM probability of the ground truth output y given the input context d and query q, and $\u03b2$ is a hyper-paramter.\nAdapter. At times, fine-tuning a large retriever can be costly, especially when dealing with retrievers based on LLMs like gte-Qwen. In such cases, it can mitigate this by incorporating an adapter module and conducting fine-tuning. Another benefit of adding an adapter is the ability to achieve better alignment with specific downstream tasks [42].\nD. Post-retrieval\nFeeding all retrieved chunks directly into the LLM is not an optimal choice. Post-processing the chunks can aid in better leveraging the contextual information. The primary challenges include: 1) Lost in the middle. Like humans, LLM tends to remember only the beginning or the end of long texts, while forgetting the middle portion [43]. 2) Noise/anti-fact chunks. Retrieved noisy or factually contradictory documents can impact the final retrieval generation [44]. 3) Context Window. Despite retrieving a substantial amount of relevant content, the limitation on the length of contextual information in large models prevents the inclusion of all this content.\n1) Rerank: Rerank the retrieved chunks without altering their content or length, to enhance the visibility of the more crucial document chunks. Given the retrieved set $D^q$ and a re-ranking method $f_{rerank}$ to obtain the re-ranked set:\n$D' = f_{rerank}(q, D^q) = {d'_1, d'_2, ...,d'_k}$  where $f(d'_1) \u2265 f(d'_2) \u2265 ... \u2265 f(d'_k)$   (12)\nRule-base rerank. Metrics are calculated to rerank chunks according to certain rules. Common metrics include: diversity, relevance and MRR (Maximal Marginal Relevance) [45]. The idea is to reduce redundancy and increase result diversity. MMR selects phrases for the final key phrase list based on a combined criterion of query relevance and information novelty.\nModel-base rerank. Utilize a language model to reorder the document chunks, commonly based on the relevance between the chunks and the query. Rerank models have become an important component of RAG systems, and relevant model technologies are also being iteratively upgraded. The scope reordering has also been extended to multimodal data such as tables and images [46].\n2) Compression: A common misconception in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM's perception of key information. A common approach to address this is to compress and select the retrieved content.\n$D' = f_{comp}(q, D^q)$, where|$d'_i$| < |$d$| \u2200d \u2208 $D^q$  (13)\n(Long)LLMLingua [47]. By utilizing aligned and trained small language models, such as GPT-2 Small or LLaMA-7B, the detection and removal of unimportant tokens from the prompt is achieved, transforming it into a form that is challenging for humans to comprehend but well understood by LLMs. This approach presents a direct and practical method for prompt compression, eliminating the need for additional training of LLMs while balancing language integrity and compression ratio.\n3) Selection: Unlike compressing the content of document chunks, Selection directly removes irrelevant chunks.\n$D' = f_{sel}(D_q) = {d'_i \u2208 D | P(d_i)}   (14)\nWhere $f_{sel}$ is the function for deletion operation and $P(d_i)$ is a conditional predicate indicating that document ($d_i$) satisfies a certain condition. If document ($d_i$) satisfies ($P(d_i)$), it will be deleted. Conversely, documents for which ($\u00acP(d_i)$) is true will be retained.\nSelective Context. By identifying and removing redundant content in the input context, the input is refined, thus improving the language model's reasoning efficiency. In practice, selective context assesses the information content of lexical units based on the self-information computed by the base language model. By retaining content with higher self-information, this method offers a more concise and efficient textual representation, without compromising their performance across diverse applications. However, it overlooks the interdependence between compressed content and the alignment between the targeted language model and the small language model utilized for prompting compression [48].\nLLM-Critique. Another straightforward and effective approach involves having the LLM evaluate the retrieved content before generating the final answer. This allows the LLM to filter out documents with poor relevance through LLM critique. For instance, in Chatlaw [49], the LLM is prompted to self-suggestion on the referenced legal provisions to assess their relevance.\nE. Generation\nUtilize the LLM to generate answers based on the user's query and the retrieved contextual information. Select an appropriate model based on the task requirements, considering factors such as the need for fine-tuning, inference efficiency, and privacy protection.\n1) Generator Fine-tuning: In addition to direct LLM usage, targeted fine-tuning based on the scenario and data characteristics can yield better results. This is also one of the greatest advantages of using an on-premise setup LLMs.\nInstruct-Tuning. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. General fine-tuning dataset can also be used as an initial step. Another benefit of fine-tuning is the ability to adjust the model's input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed [50].\nReinforcement learning. Aligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach [51]. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers.\nDual Fine-tuing Fine-tuning both generator and retriever simultaneously to align their preferences. A typical approach, such as RA-DIT [27], aligns the scoring functions between retriever and generator using KL divergence. Retrieval likelihood of each retrieved document d is calculated as :\n$P_R(d|q) = \\frac{e^{(sim(d,q)/\u03b3}}{\\sum_{d\u2208D^q} e^{(sim(d,q)/\u03b3}}$  (15)\n$P_{LM} (y|d, q)$ is the LM probability of the ground truth output y given the input context d, question q, and $\u03b3$ is a hyperparamter. The overall loss is calculated as:\n$L = \\frac{1}{T} \\sum_{i=1}^T KL(P_R(d|q)||P_{LSR}(d|q,y|))$   (16)\n2) Verification: Although RAG enhances the reliability of LLM-generated answers, in many scenarios, it requires to minimize the probability of hallucinations. Therefore, it can filter out responses that do not meet the required standards through additional verification module. Common verification methods include knowledge-base and model-base.\n$y_k = f_{verify}(q, D^q, y)$   (17)\nKnowledge-base verification refers to directly validating the responses generated by LLMs through external knowledge. Generally, it extracts specific statements or triplets from response first. Then, relevant evidence is retrieved from verified knowledge base such as Wikipedia or specific knowledge graphs. Finally, each statement is incrementally compared with the evidence to determine whether the statement is supported, refuted, or if there is insufficient information [52].\nModel-based verification refers to using a small language model to verify the responses generated by LLMs [53]. Given the input question, the retrieved knowledge, and the generated answer, a small language model is trained to determine whether the generated answer correctly reflects the retrieved knowledge. This process is framed as a multiple-choice question, where the verifier needs to judge whether the answer reflects correct answer. If the generated answer does not correctly reflect the retrieved knowledge, the answer can be iteratively regenerated until the verifier confirms that the answer is correct.\nF. Orchestration\nOrchestration pertains to the control modules that govern the RAG process. Unlike the traditional, rigid approach of a fixed process, RAG now incorporates decision-making at pivotal junctures and dynamically selects subsequent steps contingent upon the previous outcomes. This adaptive and modular capability is a hallmark of modular RAG, distinguishing it from the more simplistic Naive and Advance RAG paradigm."}, {"title": "1) Routing:", "content": "In response to diverse queries, the RAG system routes to specific pipelines tailored for different scenario, a feature essential for a versatile RAG architecture designed to handle a wide array of situations. A decision-making mechanism is necessary to ascertain which modules will be engaged, based on the input from the model or supplementary metadata. Different routes are employed for distinct prompts or components. This routing mechanism is executed through a function, denoted as $f_r(\u00b7)$, which assigns a score $\u03b1_i$ to each module. These scores dictate the selection of the active subset of modules. Mathematically, the routing function is represented as:\n$f_r: Q\u2192F$   (18)\nwhere $f_r(\u00b7)$ maps the identified query to its corresponding RAG flow.\nMetadata routing involves extracting key terms, or entities, from the query, applying a filtration process that uses these keywords and associated metadata within the chunks to refine the routing parameters. For a specific RAG flow, denoted as $F_i$, the pre-defined routing keywords are represented as the set $K_i = {k_{i1}, k_{i2},..., k_{in}}$. The keyword identified within the query $q_i$ is designated as K. The matching process for the query q is quantified by the key score equation:\n$score_{key}(q_i, F_j) = \\frac{|K_i \u2229 K|}{|K|}$   (19)\nThis equation calculates the overlap between the pre-defined keywords and those identified in the query, normalized by the count of keywords in K. The final step is to determine the most relevant flow for the query q:\n$F_i(q) = argmax_{F_j\u2208F} score(q, F_j)$   (20)\nSemantic routing routes to different modules based on the semantic information of the query. Given a pre-defined intent $\u0398 = {\u03b8_1, \u03b8_2, ..., \u03b8_n}$, the possibility of intent for query q is $P_\u03b8(\u03b8|q) = \\frac{e^{P_{LM}(\u03b8|q)}}{\\sum_{\u03b8'\u2208\u0398} e^{P_{LM}(\u03b8|q)}}$. Routing to specific RAG flow is determined by the semantic score:\n$socre_{semantic}(q, F_j) = argmax_{\u03b8_j \u2208\u0398} P(\u0398)$   (21)\nThe function $\u03b4(\u00b7)$ serves as a mapping function that assigns an intent to a distinct RAG flow $F_i = \u03b4(\u0398)$\nHybrid Routing can be implemented to improve query routing by integrating both semantic analysis and metadata-based approaches, which can be defined as follows:\n$\u03b1_i = \u03b1\u00b7score_{key}(q, F_j)+(1-\u03b1)\u00b7max_{\u03b8_j \u2208\u0398} socre_{semantic}(q, F_j)$  (22)\n\u03b1 is a weighting factor that balances the contribution of the key-based score and the semantic score."}, {"title": "2) Scheduling:", "content": "The RAG system evolves in complexity and adaptability", "follow": "n$Y_t = \\begin{cases}   \u015d_t & \\text{if all tokens of } \u015d_t \\text{ have probs > \u03c4} \\\\   S_t = LM([D_{q_t}, X, Y_{<t}"}]}