{"title": "Doubly Robust Monte Carlo Tree Search", "authors": ["Manqing Liu", "Andrew L. Beam"], "abstract": "We present Doubly Robust Monte Carlo Tree\nSearch (DR-MCTS), a novel algorithm that in-\ntegates Doubly Robust (DR) off-policy estima-\ntion into Monte Carlo Tree Search (MCTS) to\nenhance sample efficiency and decision quality\nin complex environments. Our approach intro-\nduces a hybrid estimator that combines MCTS\nrollouts with DR estimation, offering theoreti-\ncal guarantees of unbiasedness and variance re-\nduction under specified conditions. Empirical\nevaluations in Tic-Tac-Toe and the partially ob-\nservable VirtualHome environment demonstrate\nDR-MCTS's superior performance over standard\nMCTS. In Tic-Tac-Toe, DR-MCTS achieves an\n88% win rate compared to a 10% win rate for\nstandard MCTS. In compound VirtualHome tasks,\nDR-MCTS attains a 20.7% success rate versus\n10.3% for standard MCTS. Our scaling analysis\nreveals that DR-MCTS exhibits better sample ef-\nficiency, notably outperforming standard MCTS\nwith larger language models while using a smaller\nmodel. These results underscore DR-MCTS's po-\ntential for efficient decision-making in complex,\nreal-world scenarios where sample efficiency is\nparamount.", "sections": [{"title": "1. Introduction", "content": "Decision-making in complex, partially observable environ-ments remains a fundamental challenge in artificial intelli-gence. Monte Carlo Tree Search (MCTS) has emerged as\na powerful approach to addressing this challenge, demon-strating remarkable success in domains ranging from game\nplaying to robotics (Browne et al., 2012). Recently, MCTS\nhas found applications in enhancing the reasoning capabili-ties of Large Language Models (LLMs) (Yao et al., 2023;\nZhou et al., 2024), enabling more structured and coherenttext generation. However, as environments become increas-ingly complex and the cost of sampling increases, particu-larly in the context of LLMs where each node expansion\nmay involve expensive model queries (Yao et al., 2023),\nthere is a growing need for more sample-efficient methods\nthat can make better decisions with fewer simulations.\nTo address this challenge, we introduce DR-MCTS, a novel\nalgorithm that integrates Doubly Robust (DR) off-policy\nestimation into the MCTS framework. By leveraging the\nstrengths of both MCTS and DR estimation, our approach\naims to improve sample efficiency and decision quality in\ncomplex environments. DR-MCTS represents a significant\nadvancement in planning algorithms, particularly for sce-narios where sample efficiency is crucial, such as in appli-cations involving large-scale language models (Patterson\net al., 2021).\nOur work makes the following key contributions:\n1.  We introduce DR-MCTS, a novel algorithm that incor-porates Doubly Robust off-policy estimation into the\nMCTS framework.\n2.  We formulate a hybrid estimator that combines tradi-tional MCTS rollouts with DR estimation, and provide\ntheoretical guarantees for unbiasedness and demon-strate variance reduction properties.\n3.  We conduct extensive empirical evaluations in both\nthe fully observable domain of Tic-Tac-Toe and the\npartially observable VirtualHome environment, demon-strating the superior performance of DR-MCTS com-pared to standard MCTS and MCTS with Importance\nSampling.\n4.  We perform a scaling analysis to show that DR-MCTS\nachieves better sample efficiency, particularly in sce-narios with large language models."}, {"title": "1.1. Related Work", "content": "Our work builds upon and extends research in several areas,\nincluding Monte Carlo Tree Search and off-policy evalua-tion in reinforcement learning.\nMonte Carlo Tree Search (MCTS) (Coulom, 2006) has be-come a cornerstone in reinforcement learning, leading to"}, {"title": "2. Methods", "content": "2.1. Preliminaries\nThis section provides an overview of the key concepts under-lying our work: Monte Carlo Tree Search (MCTS), Impor-tance Sampling (IS), and Doubly Robust (DR) estimation."}, {"title": "2.1.1. \u039c\u039fNTE CARLO TREE SEARCH", "content": "Monte Carlo Tree Search (MCTS) is a heuristic search al-gorithm for decision processes, particularly effective in do-mains with large state spaces (Browne et al., 2012). MCTS\nbuilds and updates a search tree iteratively, with each itera-tion consisting of four steps: selection, expansion, simula-tion, and backpropagation.\nIn the selection step, the algorithm traverses the tree from\nthe root to a leaf node using a tree policy. While tradi-tional MCTS often uses the Upper Confidence Bounds for\nTrees (UCT) algorithm, in our implementation, we employ\nthe Polynomial Upper Confidence Trees (PUCT) algorithm\n(Silver et al., 2017), which is defined as:\na* = argmax_{a} Q(h, a) + c \u03c0_b(a|h) \\sqrt{N(h)} / {1+ N(h, a)} (1)\nwhere Q(h, a) is the estimated value of action a given his-tory h, \u03c0_b(a|h) is the behavior policy, N(h) is the number\nof visits to the node with history h, N(h, a) is the number of\ntimes action a was selected in the node with history h, and\nc is an exploration constant. In environments with large ac-tion spaces, PUCT's use of the behavior policy helps focus\nthe search on promising actions more quickly than UCT's\npurely visit-count-based exploration. This often results in\nimproved sample efficiency, allowing PUCT to find good\nsolutions with fewer simulations than UCT, which is particu-larly beneficial in computationally expensive environments.\nThe expansion step adds one or more child nodes to the\nselected leaf node. In the simulation step, a rollout is per-formed from the new node(s) to estimate the value of the\nstate. The value estimate V_{MCTS}(h) for a given history h is\ntypically calculated as the average of the rewards obtained\nfrom all simulations that pass through the node with history\nh:"}, {"title": null, "content": "V_{MCTS} (h) = 1/N(h) \\sum_{i=1}^{N(h)} R_i(h)  (2)\nwhere R_i (h) is the cumulative reward of the i-th simulation\nthat passes through the node with history h, and N (h) is the\ntotal number of such simulations.\nFinally, the backpropagation step updates the statistics of\nthe nodes in the path from the expanded node to the root,\nincluding the visit counts N(h) and N(h, a), and the value\nestimates Q(h, a)."}, {"title": "2.2. Importance Sampling", "content": "Importance Sampling (IS) is a technique used in off-policy\nevaluation to estimate the expected return of a target policy\nusing data collected from a different behavior policy (Precup\net al., 2000). We present two forms of the IS estimator: the\nbasic trajectory-wise IS estimator and an improved step-wise version (Jiang & Li, 2016).\nLet \u03c1_t := \u03c0_e(a_t|h_t) / \u03c0_b(a_t|h_t) be the per-step importance ratio, where\n\u03c0_e is the target (evaluation) policy and \u03c0_b is the behavior\npolicy. Define the cumulative importance ratio as \u03c1_{1:t} :=\n\\prod_{k=1}^t \u03c1_k. Then, for a trajectory (h_0, a_0, r_0,..., h_t), the\nestimators are defined as follows:\nV_{IS}(h) := \\sum_{t=0}^{H-1} \u03c1_{1:H}r_t  (3)\nV_{step-IS}(h) :=  \\sum_{t=0}^{H-1} \u03c1_{1:t}r_t (4)\nThe step-wise IS estimator (V_{step-IS}) often achieves lower\nvariance than the basic IS estimator (V_{IS}), especially for\nlonger trajectories. This is because it allows for partial\nusage of a trajectory: the cumulative importance ratio \u03c1_{1:t}\nonly accounts for the portion of the trajectory up to time t."}, {"title": "2.3. Doubly Robust Estimation", "content": "Doubly Robust (DR) estimation combines direct method\n(DM) estimation with importance sampling to provide\nan estimator that is unbiased and potentially has lower variance\nthan pure IS (Jiang & Li, 2016). The DR estimator is defined\nas:\nV_{DR}(h) =  \\hat{V}(h) + \\sum_{t=0}^{H-1} \u03b3^t \u03c1_{1:t}(r_t + \u03b3 \\hat{V}(h_{t+1}) - Q(h_t, a_t)) (5)"}, {"title": null, "content": "where \u03c1_{1:t} = \\prod_{k=1}^t \u03c0_e(a_k|h_k) / \u03c0_b(a_k|h_k) is the cumulative impor-tance ratio, \\hat{V}(h) is an estimate of the value function, and\nQ(h_t, a_t) is an estimate of the action-value function.\nThe DR estimator has the desirable property of being unbi-ased if either the importance sampling weights are correct\nor if the value function estimates are accurate. This \"dou-ble\" protection against misspecification is what gives the\nestimator its name and makes it particularly useful in practi-cal applications where perfect models are rare (Thomas &\nBrunskill, 2016).\nA key aspect of DR estimation, as established in the seminal\nwork of Jiang & Li (2016), is the use of cross-validation\nfor estimating Q(h_t, a_t). It helps reduce bias by mitigat-ing overfitting, as using the same data to estimate Q and\nevaluate the DR estimator can lead to biased estimates (Cher-nozhukov et al., 2018). In reinforcement learning settings\nwhere data can be scarce and expensive to obtain, cross-validation allows for more efficient use of the available data\nby reusing it for both model fitting and evaluation (Arlot &\nCelisse, 2010)."}, {"title": "2.4. Doubly Robust MCTS", "content": "We propose a hybrid estimator that combines the standard\nMCTS rollout estimate with the Doubly Robust (DR) esti-mate, leveraging the strengths of both MCTS and off-policy\nevaluation techniques. Our hybrid estimator is defined as:\nV_{hybrid}(h) = \u03b2V_{MCTS}(h) + (1 \u2212 \u03b2)V_{DR}(h) (6)\nwhere \u03b2\u2208 [0, 1] is a weighting parameter, V_{MCTS}(h) is the\nstandard MCTS rollout estimate, and V_{DR}(h) is the DR\nestimate.\nA crucial aspect of our DR-MCTS algorithm is the estima-tion of target and behavior policies. The target policy \u03c0_e is\ncomputed using a softmax function over the Q-values of the\nchildren of a given state node:\n\u03c0_e(a/h) = exp(Q(h, a)/\u03c4) / {\\sum_{a'} exp(Q(h, a')/\u03c4) } (7)\nwhere Q(h, a) is the Q-value of action a in history h, and\n\u03c4 is a temperature parameter controlling the exploration-exploitation trade-off. The behavior policy \u03c0_b varies depend-ing on the specific environment, as detailed in Appendix\nB.\nIn our DR-MCTS implementation, we estimate \\hat{V}(h) using\na weighted average of rewards from child nodes, where\nweights are derived from the target policy:"}, {"title": null, "content": "\\hat{V}(h) = \\sum_{a} \u03c0_e(a|h) N(h, a) / N(h) \\sum_{i=1}^{N(h, a)} R_i(h, a) (8)\nwhere N(h, a) is the number of times action a was taken\nin history h, and R_i(h, a) is the i-th observed reward for\ntaking action a in history h.\nWe estimate Q(h_t, a_t) using k-fold cross-validation on the\nrewards collected for each action:\nQ(h_t, a_t) = 1/K \\sum_{k=1}^K 1/|D_k| \\sum_{i \u2208 D_k} R_i(h_t, a_t) (9)\nwhere K is the number of folds, and D_k is the set of in-dices for the k-th fold. When insufficient data is available\nfor k-fold cross-validation, we use the mean of all avail-able rewards for the action. This approach helps to reduce\noverfitting and provides more robust estimates, which is\nparticularly important in MCTS where the number of visits\nto each state-action pair can vary significantly.\nThe weighting parameter \u03b2 in our hybrid estimator plays\na crucial role in balancing the contributions of the MCTS\nrollout estimate and the DR estimate. We explored a range\nof \u03b2 values from [0, 0.25, 0.35, 0.5], and chose the one with\nbest performance.\nTo establish the theoretical foundations of our hybrid esti-mator (Equation 6), we present two key theorems. These\ntheorems demonstrate that our proposed hybrid approach\npreserves the desirable properties of the Doubly Robust\n(DR) estimator (Equation 5) while potentially offering addi-tional benefits. Specifically, we prove the unbiasedness of\nthe hybrid estimator and establish conditions under which it\nachieves variance reduction compared to standard MCTS.\nTheorem 2.1 (Unbiasedness of Hybrid Estimator). The\nhybrid estimator is unbiased for estimating the value of the\ntarget policy \u03c0_e.\nProof. See Appendix A.1 for the detailed proof.\nImplication: Theorem 2.1 ensures that our hybrid estimator\nmaintains the crucial property of unbiasedness, guaranteeing\nthat the value estimates produced by our method will, on\naverage, correctly represent the true values of states and\nactions.\nTheorem 2.2 (Variance Reduction Condition for Hybrid Es-timator). Let V_{hybrid}(h) be the hybrid estimator as defined in\nEquation 6, and V_{UCTS}(h) be the standard MCTS estimator.\nThe hybrid estimator has lower variance than the standard\nMCTS estimator when:"}, {"title": null, "content": "E[\\sum_{t=0}^{H-1} \u03b3^{2t}\u03c1_{1:t}(Q(h_t, a_t) - \\hat{Q}(h_t, a_t))^2] = o(Var(V_{MCTS}(h)) / (1 - \u03b2)^2) (10)\nwhere \u03b2\u2208 (0,1) is the weighting parameter in the hybrid\nestimator, \u03b3 is the discount factor, \u03c1_{1:t} is the cumulative\nimportance ratio, and Q(h_t, a_t) and \\hat{Q}(h_t, a_t) are the true\nand estimated action-value functions, respectively.\nProof. See Appendix A.2 for the detailed proof.\nImplication: Theorem 2.2 provides a precise condition for\nvariance reduction in our hybrid estimator compared to stan-dard MCTS. The variance reduction is maximized when\n\\hat{Q} closely approximates Q, while still allowing for some\ndiscrepancy. However, our hybrid estimator is robust and\ncan provide benefits even with imperfect Q-value estimates.\nAs long as the Q-value estimation errors are sufficiently\nsmall relative to MCTS variance, our method ensures more\nreliable value estimates across various scenarios. This prop-erty potentially leads to faster convergence, more robust\ndecision-making, and improved sample efficiency, even in\nchallenging environments where perfect Q-value estimation\nis infeasible.\nWe now introduced the algorithm we used for DR-MCTS\n(Algorithm 1). Our algorithm initializes a search tree with"}, {"title": "3. Experiments", "content": "3.1. Experimental Setup\nTo evaluate the performance of our DR-MCTS algorithm,\nwe conducted experiments in two distinct environments: the\nclassic game of Tic-Tac-Toe and the more complex Virtual-Home environment. These environments were chosen to test\nthe algorithm's effectiveness in both simple, fully observ-able domains and complex, partially observable scenarios."}, {"title": "3.1.1. TIC-TAC-TOE", "content": "Tic-Tac-Toe. To evaluate our DR-MCTS algorithm, we first\nconducted experiments in the classic game of Tic-Tac-Toe.\nTic-Tac-Toe is played on a 3x3 grid where two players alter-nately place their symbols (X and O). The game concludes\nwhen one player forms a line of three symbols horizontally,vertically, or diagonally, or when all cells are filled, resulting\nin a draw.\nData. We implemented a Tic-Tac-Toe environment in\nPython to generate game data for our experiments. The\ngame state is represented by a list of 9 elements, each corre-sponding to a cell on the board, which can be empty, 'X', or\n'O'. Our environment tracks the current player and provides\nmethods for making moves, checking for valid moves, de-termining game outcomes, and cloning the game state. The\nreward structure assigns 1.0 for a win, 0.5 for a draw, and\n0.0 for non-terminal moves. Invalid moves, while accounted\nfor in the implementation with a -1.0 reward, do not occur\nin actual gameplay due to the constraints imposed by the\naction selection process.\nEvaluation. Our evaluation framework compared three\nalgorithms: standard MCTS, MCTS with Importance Sam-pling (IS-MCTS), and our proposed DR-MCTS. We con-ducted experiments across a range of MCTS rollouts (20,\n40, 60, 80, and 100) to assess performance scaling with\nincreased computational resources. For each algorithm pair\nand rollout count, we simulated 100 games, alternating the\nroles of X and O to ensure fairness. The primary perfor-mance metric was the win rate of each algorithm against its\nopponent, with draws recorded separately.\nBaselines. We used two baseline models for comparison.\nThe first was standard MCTS, which uses pure Monte Carlo\nrollouts to estimate state values. The second was IS-MCTS,\nan intermediate comparison point that incorporates the step-wise importance sampling estimator. That is, we replace\nV_{DR} in Algorithm 1 with equation 4. Both baselines use the\nsame tree structure and selection strategy as our DR-MCTS\nalgorithm, differing only in their value estimation methods."}, {"title": "3.1.2. VIRTUAL HOME", "content": "Virtual Home. Virtual Home is a complex, partially ob-servable 3D environment simulating a household setting\n(Puig et al., 2018). It contains multiple rooms (e.g., kitchen,\nliving room, bedroom) with hundreds of interactive items\nand containers. The state space is high-dimensional and par-tially observable, while the action space is large, including\nnavigation and object interaction actions. Following Zhao\net al. 2023 (Zhao et al., 2023), we utilize Large Language\nModels (LLMs) in two crucial roles: as a world model and\nas a policy model. The LLM-based world model informs\nthe planning process by providing commonsense knowledge\nabout the environment, which is used to estimate Q-values.\nAdditionally, an LLM-based policy model is employed to\ngenerate a heuristic policy, guiding action selection during\nthe search process. This dual use of LLMs enables more\nefficient exploration of the vast state-action space in Virtual\nHome by leveraging the models' understanding of typical\nhousehold layouts and object interactions.\nData. We create tasks with randomly initialized scenes\nand expert trajectories. We evaluate on four datasets, each\ntesting different aspects of generalization:\n1)  Novel Simple: Rearranging familiar items in new ways,\ne.g., \"put one plate inside the fridge\u201d when training only\nincluded \"put one plate on the kitchen table\".\n2)  Novel Objects: Tasks with unfamiliar objects, e.g., \"place\nthe blender on the kitchen counter\" when blenders were not\nin training data.\n3)  Novel Comp.: New combinations of familiar tasks,\ne.g., \"put the fork on the dining table and the milk in the\nfridge\" when training only had these actions separately.\n4)  Novel Comp. + Objects: Combining new task composi-tions with unfamiliar objects, e.g., \"place the juicer on the\nkitchen counter and the kiwi in the fruit bowl,\" where neither\nitem appeared in training data.\nEvaluation. We evaluate DR-MCTS against standard\nMCTS and IS-MCTS in two parts: success rate comparison\nand inference time scaling. For success rates, we use LLMs\n(GPT-40-mini and GPT-40) as world and policy models\nacross all four tasks. Success is defined as task completion\nwithin 10 steps. For scaling analysis, we compare two con-"}, {"title": "3.2. Results", "content": "Tic-Tac-Toe Figure 1 and Table 1 show the win rates of\nDR-MCTS and IS-MCTS compared to standard MCTS in\nTic-Tac-Toe, across different rollout numbers. Both DR-MCTS and IS-MCTS consistently outperform MCTS, with\nDR-MCTS showing a more pronounced advantage. DR-MCTS achieves win rates ranging from 63% to 88%, while\nIS-MCTS wins between 57% and 82% of games. The per-formance gap widens as the number of rollouts increases,\nwith DR-MCTS reaching an 88% win rate at 100 rollouts\ncompared to MCTS's 10%. These results demonstrate the\nsuperior performance of both DR-MCTS and IS-MCTS over\nstandard MCTS in Tic-Tac-Toe, with DR-MCTS exhibiting\nthe strongest overall performance.\nVirtual Home Table 2 and Figure 2 presents the perfor-mance of MCTS, IS-MCTS, and DR-MCTS across different\ntask types and LLM models. DR-MCTS consistently out-performs the other methods in terms of success rate across\nall task types. For Novel Simple tasks, DR-MCTS achieves\na 50.4% success rate with GPT-40-mini and 91.1% with\nGPT-40, compared to 32.4% and 85.4% for standard MCTS,\nrespectively. This performance advantage is maintained\nfor more complex tasks, with DR-MCTS achieving 44.1%\nsuccess on Novel Objects tasks and 22.7% on Novel Comp.\ntasks, compared to 23.5% and 19.0% for MCTS, respec-"}, {"title": null, "content": "tively.\nInterestingly, DR-MCTS exhibits a task-dependent pattern\nin resource utilization. For simpler tasks, it generates fewer\ntokens and requires less computation time while achiev-ing higher success rates. For instance, in Novel Simple\ntasks with GPT-40, DR-MCTS uses 16.053M input tokens\nand takes 7:24:42, compared to 16.402M input tokens and\n8:43:53 for MCTS, while achieving a higher success rate\n(91.1% vs 85.4%). As task complexity increases, DR-MCTS tends to generate more tokens to achieve its superior\nperformance. In Novel Comp. + Objects tasks, DR-MCTS\nuses 7.501M input tokens compared to 6.279M for MCTS\nto achieve a higher success rate (20.68% vs. 10.34%). This\nsuggests that DR-MCTS adapts its resource allocation based\non task difficulty, investing computational effort more ef-fectively to maintain high success rates across varying task\ncomplexities.\nFigure 3 illustrates the scaling behavior of different algo-rithms as the number of MCTS search steps increases. DR-MCTS demonstrates superior performance across all search\ndepths, with its advantage being particularly pronounced\nin the early stages and for complex tasks. This highlights"}, {"title": null, "content": "DR-MCTS's efficiency in quickly identifying promising\nactions in challenging environments. Notably, DR-MCTS\nwith GPT-40-mini as the world model often outperforms\nstandard MCTS with the larger GPT-40 model, especially\nat lower number of MCTS iterations. These findings indi-cate that DR-MCTS can achieve competitive performance\nwith fewer computational resources and smaller models,offering crucial advantages in time-sensitive or resource-constrained scenarios and potentially leading to substantial\ncomputational savings in practical applications."}, {"title": "4. Discussion", "content": "Our experimental results demonstrate the significant advan-tages of DR-MCTS over traditional MCTS across differentdomains, from simple Tic-Tac-Toe to complex, partially"}, {"title": null, "content": "observable VirtualHome environments. In Tic-Tac-Toe, DR-MCTS consistently outperformed standard MCTS, with win\nrates ranging from 63% to 88%, compared to MCTS's 10%\nto 37%. This performance gap was maintained in the morechallenging VirtualHome tasks, highlighting DR-MCTS's\nability to handle increased complexity and partial observ-ability.\nThe superior performance of DR-MCTS can be un-derstood through our theoretical findings on vari-ance reduction. Our analysis shows that the hy-brid estimator achieves lower variance than standard\nMCTS when \u0395[\\sum_{t=0}^{H-1} \u03b3^{2t}\u03c1_{1:t}(Q(h_t, a_t) - \\hat{Q}(h_t, a_t))^2] =o(Var(V_{MCTS}(h))/(1 \u2013 \u03b2)^2), a condition likely met in many\npractical scenarios, especially in complex environments\nwhere MCTS variance is typically high. Importantly, our\ntheory suggests that variance reduction is maximized when\n\\hat{Q} closely approximates Q, while still allowing for some\ndiscrepancy. This insight aligns with our experimental ob-servations, where DR-MCTS showed improvements across\ndifferent environments, with particularly notable gains incomplex, partially observable environments like Virtual-Home.\nThe observed task-dependent resource utilization pattern\nfurther supports our theoretical insights. In simpler tasks,\nwhere Q-value estimation errors might be small, DR-MCTSachieves superior performance with minimal additional re-sources. In more complex tasks, it allocates more resourcesto capitalize on the greater potential for variance reduction.This adaptive behavior demonstrates how DR-MCTS bal-ances the trade-off between Q-value estimation accuracyand MCTS exploration, as implied by our theoretical find-"}, {"title": null, "content": "ings. Moreover, DR-MCTS's ability to achieve superiorperformance using smaller language models as world mod-els aligns with our variance reduction theorem. The hybridestimator's tolerance for imperfect Q-value estimates allowsit to leverage less precise but more computationally efficientmodels effectively.\nWhile DR-MCTS has demonstrated effectiveness, it facesseveral limitations that warrant further investigation. Thefine-tuning process for the \u03b2 parameter in the hybrid es-timator can be computationally expensive, particularly inLLM-based environments where API calls are costly. Addi-tionally, our current method for estimating \\hat{V} and Q relieson empirical averages rather than a strictly model-based ap-proach. While effective in our experiments, this techniquemay not fully capture the intricacies of environment dynam-ics, especially in highly stochastic or partially observablesettings.\nTo address these limitations and extend the capabilities ofDR-MCTS, we propose several promising future researchdirections. First, implementing a dynamic \u03b2 function thatadapts based on the relative magnitudes of Q-value estima-tion errors and MCTS variance could potentially reduce theneed for extensive parameter tuning and mitigate computa-tional costs. Second, incorporating more rigorous, model-based estimation methods for \\hat{V} and Q could lead to im-proved performance. For instance, using neural networksto learn state-value functions or employing Gaussian pro-cesses for value function approximation might better handlesparse rewards and provide stronger theoretical convergenceguarantees, particularly in complex environments requiringlong-term planning. These enhancements could further op-timize the balance between Q-value estimation accuracyand MCTS exploration, potentially leading to even greaterperformance gains in challenging domains."}, {"title": "5. Conclusion", "content": "DR-MCTS represents a significant advancement in MonteCarlo Tree Search methods, demonstrating consistent im-provements over traditional approaches across various do-mains and complexity levels. By leveraging doubly robustestimation, MCTS-DR achieves higher success rates, bettersample efficiency, and improved scalability, particularly inchallenging scenarios and under computational constraints.The ability of DR-MCTS to maintain high performance withsmaller large language models opens up new possibilitiesfor deploying sophisticated planning algorithms in resource-limited settings. As AI systems are increasingly applied tocomplex real-world problems, techniques like DR-MCTSthat can effectively balance performance, efficiency, andadaptability will become increasingly valuable."}, {"title": "Impact Statement", "content": "This paper presents advancements in Monte Carlo TreeSearch (MCTS) algorithms, aiming to improve decision-making processes in complex, partially observable environ-ments. While our primary goal is to contribute to the field ofMachine Learning, we acknowledge that this work may havebroader societal implications. Our DR-MCTS algorithmcould potentially enhance planning and decision-makingin various real-world applications, such as robotics, auto-mated planning, and resource management. The improvedsample efficiency and performance with smaller modelsmay lead to more energy-efficient AI systems, contributingto reduced computational costs and environmental impact.However, as with any advanced AI technology, there is arisk of unintended consequences if deployed in sensitivesdomains without proper oversight. The improved decision-making capabilities could be misused if applied in ethicallyquestionable contexts. Additionally, while our algorithmains to make better decisions, it inherits any biases presentin the underlying models or training data. Users should beaware of potential biases and take appropriate measures toensure fair and ethical application. We encourage furtherresearch into the ethical implications of advanced planningalgorithms and advocate for responsible development anddeployment of such technologies."}, {"title": "A. Theoretical Analysis of DR-MCTS", "content": "A.1. Unbiasedness of the Hybrid Estimator\nTheorem A.1 (Unbiasedness of Hybrid Estimator). The hybrid estimator is unbiased for estimating the value of the target\npolicy \u03c0_e.\nProof. We know that V_{UCTS}(h) is unbiased due to the properties of Monte Carlo estimation. For V_{DR}(h), we can express it\nas:"}, {"title": null, "content": "V_{DR}(h) = \\hat{V}(h) + \\sum_{t=0}^{H-1} \u03b3^t \u03c1_{1:t}(r_t + \u03b3 \\hat{V}(h_{t+1}) - Q(h_t, a_t)) (11)\nwhere \u03c1_{1:t} = \\prod_{k=1}^t \u03c0_e(a_k|h_k) / \u03c0_b(a_k|h_k) is the cumulative importance ratio.\nTaking the expectation with respect to the behavior policy \u03c0_b:\nE_{\u03c0_b}[V_{DR}(h)] = E_{\u03c0_b}[\\hat{V}(h)] + E_{\u03c0_b}[\\sum_{t=0}^{H-1} \u03b3^t \u03c1_{1:t}(r_t + \u03b3 \\hat{V}(h_{t+1}) - Q(h_t, a_t))] (12)\n= \\hat{V}(h) + \\sum_{t=0}^{H-1} \u03b3^t E_{\u03c0_b}[\u03c1_{1:t}(r_t + \u03b3 \\hat{V}(h_{t+1}) - Q(h_t, a_t))] (13)\n= \\hat{V}(h) + \\sum_{t=0}^{H-1} \u03b3^t (Q(h_t, a_t) - E_{\u03c0_e}[Q(h_t, a_t)]) (14)\n= \\hat{V}(h) (15)\nThe last step follows from the fact that E_{\u03c0_b}[Q(h_t, a_t)] = Q(h_t, a_t) when Q is an unbiased estimator of Q.\nTherefore, both V_{MCTS}(h) and V_{DR}(h) are unbiased estimators of \\hat{V}(h). Since V_{hybrid}(h) is a linear combination of these\nunbiased estimators, it is also unbiased:\nE[V_{hybrid}(h)] = E[\u03b2V_{MCTS}(h) + (1 \u2212 \u03b2)V_{DR}(h)] (16)\n= \u03b2E[V_{MCTS}(h)] + (1 \u2212 \u03b2)E[V_{DR}(h)] (17)\n= \u03b2\\hat{V}(h) + (1 \u2212 \u03b2)\\hat{V}(h) (18)\n= \\hat{V}(h) (19)\nThus, the hybrid estimator remains unbiased in the DR-MCTS context."}, {"title": "A.2. Variance Reduction Condition for Hybrid Estimator", "content": "Theorem A.2 (Variance Reduction Condition for Hybrid Estimator). Let V_{hybrid}(h) be the hybrid estimator as defined in\nEquation 6, and V_{UCTS}(h) be the standard MCTS estimator. The hybrid estimator has lower variance than the standard\nMCTS estimator when:\nE[\\sum_{t=0}^{H-1} \u03b3^{2t}\u03c1_{1:t}(Q(h_t, a_t) - \\hat{Q}(h_t, a_t))^2] = o(Var(V_{MCTS}(h)) / (1 - \u03b2)^2) (20)\nwhere \u03b2\u2208 (0,1) is the weighting parameter in the hybrid estimator, \u03b3 is the discount factor, \u03c1_{1:t} is the cumulative\nimportance ratio, and Q(h_t, a_t) and \\hat{Q}(h_t, a_t) are the true and estimated action-value functions, respectively."}, {"title": null}]}