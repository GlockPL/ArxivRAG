{"title": "How Can LLMs and Knowledge Graphs Contribute to Robot Safety? A Few-Shot Learning Approach", "authors": ["Abdulrahman Althobaiti", "Angel Ayala", "JingYing Gao", "Ali Almutairi", "Mohammad Deghat", "Imran Razzak", "Francisco Cruz"], "abstract": "Large Language Models (LLMs) are transforming the robotics domain by enabling robots to comprehend and execute natural language instructions. The cornerstone benefits of LLM include processing textual data from technical manuals, instructions, academic papers, and user queries based on the knowledge provided. However, deploying LLM-generated code in robotic systems without safety verification poses significant risks. This paper outlines a safety layer that verifies the code generated by ChatGPT before executing it to control a drone in a simulated environment. The safety layer consists of a fine-tuned GPT-40 model using Few-Shot learning, supported by knowledge graph prompting (KGP). Our approach improves the safety and compliance of robotic actions, ensuring that they adhere to the regulations of drone operations.", "sections": [{"title": "1 Introduction", "content": "The increasing complexity of robotic systems has increased the demand for efficient programming tools that can simplify the development process. Traditional robot programming methods require a deep understanding of both hardware and software, creating a barrier for those without specialized expertise. To address these challenges, recent advances in Natural Language Processing (NLP) specifically, with Large Language Models (LLMs) such as ChatGPT driving significant advances [Wei et al., 2022], have led to the development of systems capable of generating code directly from human-readable instructions.\nRobotics code generation using NLP promises to revolutionize how robots are controlled, demonstrating remarkable capabilities in understanding natural language and translating these commands into robotic planned actions [Matuszek et al., 2013], enabling new applications across various domains. Moreover, this will enable developers to specify complex behaviors through natural language commands rather than writing intricate code [Yu et al., 2023]. By leveraging NLP techniques, code generation tools can interpret user inputs in natural language, translating them into executable code for various robotic tasks, hence having the potential to revolutionize human-robot interaction by allowing users to control robots using natural language instructions [Liang et al., 2022]. This significantly lowers the technical entry barrier and accelerates the development process, allowing for faster prototyping, testing, and deployment of drone applications.\nFor example, a user might instruct a drone to 'Navigate to a target and avoid obstacles on its way,' and the system would generate the necessary low-level code to perform the task autonomously. Commands violating the drone's safe operation parameters led to incidents affecting physical assets, human life, and an increased risk of crashes i.e. \u201cfly upwards to an altitude of 200 meters.\u201d which could result: \u201cDrone exceeding 120-meter altitude is not permitted by the safety regulation of drone operations because it might interfere with a crew operated aircraft\". Moreover, an illustrative example of current systems that do not have a safe layer to prevent harm commands from being executed to robot operations can be seen in Figure 1.\nThe potential impact of NLP-driven code generation in robotics extends beyond reducing development time. It opens the door to more collaborative and accessible robotic programming environments where non-expert users, such as industrial operators or educators, can interact with robots more intuitively. These tools could facilitate the broader adoption of robotics in diverse fields, from manufacturing and healthcare to the education and service industries. However, despite the promise of this approach, several challenges remain. Translating complex human language into precise robotic commands requires not only advanced NLP algorithms but also an understanding of the physical and logical constraints of robotic systems. Furthermore, ensuring that the generated code is reliable, safe and optimized for real-time execution presents additional hurdles.\nTraditional methods of controlling robots would require experience in robot programming and coding. However, LLM models like GPT-40 are capable of comprehending natural language and translating it into code executable by robots. The limitation of using large language models is the safety mechanism associated with the code generated to control the robot. For example, if the user issues a command in natural language to fly the drone to a specific altitude, then the LLM model might misinterpret the user command due to model hallucination [Ji et al., 2023]. The model would generate and execute the wrong code that could lead the drone to breach the safety rules of drone operations. Our contribution is to fill the gap by adding a safety layer that will verify the safety of the code generated before the final low-level code execution. The key contributions can be summarized as follows:"}, {"title": "2 Related Work", "content": "Numerous language models (LMs) have been developed, typically pre-trained with specific objectives (e.g., masked language modeling [Salazar et al., 2019]) and later fine-tuned for various downstream tasks. These LMs generally fall into three categories: 1) Masked LMs, which predict masked words in a sentence based on their context, such as BERT [Devlin et al., 2019] and ROBERTa [Liu et al., 2019]; 2) Encoder-Decoder models, used for tasks such as translation and summarization, where the encoder converts the input into a fixed-length representation and the decoder generates the output, examples include T5, BART, and MASS [Niu et al., 2022]; and 3) Left-to-Right LMs, trained to predict the next word in a sequence based on prior words, like GPT-3, and GPT-4. Most of these models are built upon the Transformer architecture, which uses self-attention mechanisms to efficiently handle long-range dependencies and adapt to diverse downstream tasks.\nThere have been attempts to integrate large models to control robotic action [Bucker et al., 2022]. Furthermore, beyond language-conditioned robotic manipulation, foundation models have driven notable advances in robotics. For instance, LID [Li et al., 2022] introduces a method for sequential decision-making by leveraging a pre-trained LM to initialize a policy network that embeds both goals and observations. R3M [Nair et al., 2022] demonstrates how visual representations learned from diverse human video data, through time-contrastive learning and video-language models [Radosavovic et al., 2023], enable efficient learning of robotic manipulation tasks. CACTI presents a scalable visual imitation learning framework, using pre-trained models to convert pixel data into low-dimensional latent embedding for better generalization [Mandi et al., 2022]. DALL-E-Bot [Kapelyukh et al., 2023], on the other hand, uses Stable Diffusion [Rombach et al., 2022] to create images of the target scene that guide robot actions, offering a unique approach.\nOur work is mainly based on [Vemprala et al., 2024] in which the authors highlight the ability of ChatGPT by understanding user commands and generating relevant code using only a prompt and a function library. In addition, they experimented with different robotic tasks to demonstrate their approach. For example, drone control for navigation and obstacle avoidance using real-time sensor information. The authors have conducted simulation experiments using the AirSim simulator and real-world environment to demonstrate ChatGPT's ability to control complex robot tasks with the user in the feedback loop. However, a key issue with these approaches is the absence of a safety pipeline that can verify the integrity of the generated code before deploying it to the robot. This is a very important and crucial step, especially in a real-world deployment in which the wrong code could endanger the safety of humans and assets.\nOne of the emerging themes in AI safety is the prevention of using an AI system that could endanger the safety of humans [Future of Life Institute, 2015]. Safety is a very important element in robotic operation in a dynamic environment [Amodei et al., 2016]. Furthermore, with the advancement of LLM and its ability to control robotic systems, a safety layer to constrain robot actions, regardless of the performance of the LLM model, is essential. Most approaches lack the layer to verify the safety of the code generated by LLM models for the robot's action, specifically in the drone operations domain.\nLarge language models such as GPT-40 are trained on billions of parameters and are not generalized to a specific knowledge domain. However, Few-Shot learning is an effective way in which we can fine-tune the model to be more capable toward the targeted domain of knowledge, especially if the model is larger in size [Brown et al., 2020]. Moreover, LLMs are black-box models that are limited by their trained data and do not have access to evolving knowledge [Mitchell et al., 2018]. This might lead to insufficient generalization during inference [McCoy et al., 2019] and is also a subject of hallucination of false world information [Ji et al., 2023].\nIn contrast, Knowledge Graphs (KG) can store accurate, domain-specific, and evolving knowledge that presents a formal understanding of the world [Ji et al., 2022]. The unification of LLMs and KG can improve performance in terms of knowledge awareness [Pan et al., 2024]. One of the methods of KG-enhanced LLMs is KG Prompting (KGP), which converts KG structures into text sequences that can be injected into LLMs to enhance their reasoning during inference [Li et al., 2023]. KGP can maximize the full potential of LLMs to provide better reasoning on domain-specific knowledge without retraining the model. However, the only downside is that crafting KGPs requires extensive human effort."}, {"title": "3 Methodology", "content": "The system entry starts with the user input that goes to a GPT-40 model, which handles the following: (1) an API file containing a finite set of high-level programmed functions, (2) a system prompt defining the system role and a user prompt describing each function in the API file, along with a few usage examples, and (3) a user input as command and feedback. The GPT-40 model can comprehend and translate user commands into an executable Python code for drone control in the AirSim simulation environment [Shah et al., 2017]. Before final code execution, the GPT-40 model can ask the user for additional information to clarify the given command, keeping the user in the feedback loop through a dialogue approach. This phase was developed by [Vemprala et al., 2024] where they ended their process by executing the code to achieve drone control in natural language. Our contribution to their work is done by incorporating a safety layer before the final execution of the code. The entire process of developing and integrating our safety classifier is highlighted in the following sections."}, {"title": "3.1 Few-Shot learning", "content": "We have selected GPT-40 as our target system for code classification because it understands complex language and code syntax. Moreover, since the model generating the code is a GPT-40 model, it made sense to incorporate an LLM model to comprehend the generated code and classify it as safe or unsafe. In addition, choosing an LLM to classify the code will make the integration process with the current system less complex. Since large lange models are pre-trained on billions of parameters, we decided to fine-tune our model with 100-generated SAFE and UNSAFE code samples under a Few-Shot learning approach, summarized as follows:"}, {"title": "3.2 Knowledge Graph Prompting", "content": "After fine-tuning we extended our model with a KGP that contains the safety rules for drone operation obtained from the Australian Government Civil Aviation Safety Authority (CASA)\u00b9. Moreover, the following are a few examples of the safe operation rules:"}, {"title": "3.3 Integration", "content": "After fine-tuning the GPT-40 model as a code classifier, our proposal integrates it into the original method to classify the generated code before the drone executes it. An example of full integration with Airsim simulation interaction can be seen in Figure 4. Moreover, the fine-tuned model is integrated into the pipeline, where it receives the code from the first GPT-40 and classifies it as SAFE or UNSAFE. If the code is SAFE, it will be executed for low-level drone control. On the contrary, if the code is UNSAFE, it will not be executed and will be returned for the user to issue a new safe command. The overall system's architecture is shown in Figure 2a."}, {"title": "4 Dataset", "content": "The experimental setup was developed using the AirSim simulation environment [Shah et al., 2017], built on the Unreal physics engine. AirSim simulator has been widely used for different autonomous drone control approaches due to its simple interface. Additionally, it comprises realistic graphics with straightforward API methods to control the drone within the simulated environment. As the focus of this work is to develop a safety pipeline using LLM as a classifier for safe and unsafe codes, outdoor scenes were used based on previous work [Vemprala et al., 2024].\nThe used environment is a large outdoor environment that includes a crowd of people in the center, and two wind turbines with five power towers connected to an electrical substation. The UAV starts from a fixed location, waiting for user commands to begin its mission control. The user commands are prompted through GPT-40, which interprets the natural language order into Python code to AirSim using the respected API call.\nThe provided scene, including its elements and components, made it suitable to generate a custom dataset based on the operational rules established by the Australian Civil Aviation Safety regulations\u00b2. For this purpose, a pre-trained GPT-40 model was used to generate Python code based on user commands, as presented in Figure 2b. We created a dataset consisting of 100 code snippets based on the safety constraint rules defined previously. The dataset was balanced with 50 instances labeled as SAFE and the other 50 instances labeled as UNSAFE. A SAFE instance on the dataset means that the processed command complies with all safety guidelines. In the same way, UNSAFE instances mean that a given instruction violates at least one safety rule. An example of the dataset is shown in Table 1.\nThe dataset includes various drone operation scenarios, such as altitude limitations, flying near objects, and navigating around crowds or specific locations. We issued commands to control the drone, ensuring the usage of as many library functions as possible.\nTo ensure safety compliance, the dataset is organized into four main categories, each representing a specific safety rule for drone operation. These categories contain both SAFE and UNSAFE examples, as detailed below:"}, {"title": "5 Results and discussion", "content": "For result comparison, two approaches with each having two models were considered to evaluate the classification task between SAFE and UNSAFE commands. On each approach, the OpenAI GPT-4o provided model was used as comparison baseline for our proposed model, named FTGPT-40. The first approach is without KGP, considering a simple system instruction prompt for both models. The second approach considers the use of the KGP based on safety rules regulations for the GPT-40 and FTGPT-40 models.\nThe general results demonstrate that the FTGPT-40 model is capable of classifying SAFE commands, in addition to identifying UNSAFE instructions in a better way than GPT-40. Hence, code prediction from user input is solely based on its training data.\nThe fine-tuning job's outcome demonstrated that the model was able to fit the new presented data, but with high performance. Furthermore, our main approach consists of including the safety rules through KGP in the fine-tuned model, making it the best-performing approach against the overall testing dataset.\nAs our main concern is to increase safety in the drone operation pipeline, the UNSAFE examples were linked to the True Positive (TP) and False Positive (FP) instances. Thus, the SAFE examples were associated with the True Negative (TN) and False Negative (FN) classes. Since the testing data is balanced, 40 instances for TP and 40 instances for TN are required to achieve perfect performance. In this regard, the quantitative analysis was addressed using the accuracy, precision, recall, F1 score, and Mathew correlation coefficient (MCC) metrics.\nThe classification comparison in Table 2, shows the overall performance for each approach and model. The use of the proposed dataset to fine-tune the model significantly improves performance against the GPT-40 baseline to identify UNSAFE commands, in both approaches with and without KGP. For the approach without KGP, the fine-tuned model increases the accuracy from 45% to 57.50%, the precision from 33.33% to 56%, an impressive enhancement in recall and F1-score, from 10% to 70% and 15.39% to 62.22%, respectively. Finally, the MCC increased from 0% to 15.49%. A general difference that can be observed is that the GPT-40 model struggles to identify UNSAFE commands, classifying a large number of examples as SAFE. In contrast, the fine-tuned GPT-40 model is able to recognize both types of command, classifying most of the samples as UNSAFE.\nFor the KGP approach, given explicit guidance, it can be seen that both models improved their performance. The GPT-40 was able to increase the number of UNSAFE correctly identified; however, it continues to classify too many examples as SAFE. Although GPT-40 has 100% precision in classifying the 16 examples of UNSAFE commands, the 40% recall indicates that it was not sufficient to correctly identify all the UNSAFE instructions. On the contrary, the fine-tuned GPT-40 improves two out of five of the metrics in comparison to GPT-4 with KGP, with exception of accuracy and precision. The fine-tuned model got the same 70% as the baseline in accuracy, and presented a reduction in precision and MCC from 100% to 70% and from 50% to 40%, respectively. However, fine-tuned GPT-40 increases the recall and F1 score metrics from 40% to 70% and from 57.14% to 70%, respectively. The same value obtained for accuracy, precision, recall, and F1 score for the fine-tuned model indicates better performance in identifying both SAFE and UNSAFE commands. Even with an important amount of FP instances, the fine-tuned GPT-40 is the best model for safe drone operations since it is better alerting of possible risky situations than considering it as SAFE."}, {"title": "5.1 Categories analysis", "content": "The performance of our approaches against our four testing categories can be seen in Figure 5 which consist of altitude, distance from objects (DistObject), distance from crowd (DistCrowd) and hovering above the crowd (HoverCrowd). Moreover, in terms of the altitude and distance from objects testing categories, we can see an even distribution of the result between the models with KGP and similar to the model without KGP. However, a note worth highlighting is that both models FTGPT-40 and GPT-40 both with KGP archived perfect scores of identifying SAFE and UNSAFE showing the benefits of KGP that can reinforce the model with domain-specific knowledge.\nIn the DistCrowd category, regarding the minimum distance from the crowd rule sets, the GPT-40 models cannot classify the UNSAFE commands with and without KGP. On the contrary, the FTGPT-40 can correctly classify UNSAFE commands without KGP. However, the FTGPT-40 with KGP slightly improves the classification of SAFE instructions with a precision detriment for UNSAFE instances. The high similarity between the distance to objects and distance to crowd instructions on the testing data and the KGP rules probably makes the FTGPT-40 model behave this way.\nWith the set of rules related to avoiding hovering in the crowd (HoverCrowd), the baseline model, it was always challenging in classify the UNSAFE examples, even when adding KGP. However, when using the KGP, the GPT-40 model was able to improve its precision in classifying SAFE commands. On the contrary, the FTGPT-40 model detects in a well manner the UNSAFE instructions, having difficulties with the SAFE examples. Even when adding KGP, the fine-tuned model is not capable of improving the SAFE commands classification, presenting a drawback with the UNSAFE instructions."}, {"title": "6 Conclusions", "content": "Most current systems that allow UAV control via natural language lack a safety barrier that prevents the UAV from violating the regulations of safe drone operations. Our work addressed this gap by developing a safety layer that eliminates harmful commands from being executed to low-level robotic actions. Obtained results have proven that a fine-tuned LLM with KGP can ensure that drone operations obey local authorities' regulations. Future works will address more complex CASA drone rules. For example, flying the drone during day time only and avoiding to fly the drone near areas where emergency operations are taken place to ensure full safety regulation complaints. Having LLMs following the rules for drone operations is one approach to ensure the safety of the code generated, future research will also consider the current drone's hardware capabilities. For instance, giving unsafe commands that exceed the safe velocity parameters of the drone can lead to unstable flight patterns, reduced maneuverability, and an increased risk of crashes, e.g., the instruction accelerate to 50 m/s and maintain altitude could lead the drone to crash into an obstacle due to reduced control accuracy at high speeds.\nWe hope that our work has made a contribution to answering the open research question of how to prevent AI systems from causing harm to humans and assets without limiting robot control, increasing latency, and continuing learning required for LLMs controlling drones to rise for the challenges of object-goal navigation at an unpredictable dynamic outdoor environment."}]}