{"title": "Replace-then-Perturb: Targeted Adversarial Attacks With Visual Reasoning for Vision-Language Models", "authors": ["Jonggyu Jang", "Hyeonsu Lyu", "Jungyeon Koh", "Hyun Jong Yang"], "abstract": "The conventional targeted adversarial attacks add a small perturbation to an image to make neural network models estimate the image as a predefined target class, even if it is not the correct target class. Recently, for visual-language models (VLMs), the focus of targeted adversarial attacks is to generate a perturbation that makes VLMs answer intended target text outputs. For example, they aim to make a small perturbation on an image to make VLMs' answers change from \"there is an apple\" to \"there is a baseball.\" However, answering just intended text outputs is insufficient for tricky questions like \"if there is a baseball, tell me what is below it.\" This is because the target of the adversarial attacks does not consider the overall integrity of the original image, thereby leading to a lack of visual reasoning. In this work, we focus on generating targeted adversarial examples with visual reasoning against VLMs. To this end, we propose 1) a novel adversarial attack procedure-namely, Replace-then-Perturb and 2) a contrastive learning-based adversarial loss-namely, Contrastive-Adv. In Replace-then-Perturb, we first leverage a text-guided segmentation model to find the target object in the image. Then, we get rid of the target object and inpaint the empty space with the desired prompt. By doing this, we can generate a target image corresponding to the desired prompt, while maintaining the overall integrity of the original image. Furthermore, in Contrastive-Adv, we design a novel loss function to obtain better adversarial examples. Our extensive benchmark results demonstrate that Replace-then-Perturb and Contrastive-Adv outperform the baseline adversarial attack algorithms. We note that the source code to reproduce the results will be available.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENTLY, vision-language models (VLMs) such as GPT-4 [1], Claude 3.5 [2], LLAVA 1.6 [3], and Gemini [4] have garnered significant research attention. The key abilities of VLMs are their capacity to understand and generate human-like responses to multi-modal inputs, thereby opening up new possibilities in numerous applications. For example, VLMs excel at processing and integrating both visual and textual information, enabling image captioning, visual question answering, and cross-modal retrieval. However, as VLMs continue to advance, their potential impact on both industry and academia is immense, thereby driving the need for further exploration into their capabilities and vulnerabilities.\nMotivation. Driven by research interests in the vulnerabilities of VLMs, a key research direction is generating adversarial examples (i.e., adversarial attacks), as they are closely related to several security and privacy applications such as adversarial training [13], [14], privacy protection [15], [16], and steganography [17]. Recently, several studies have focused on targeted and type-II\u00b9 adversarial attacks [8]\u2013[10], [12]. However, as depicted in Fig. 1, previous embedding-based methods lack visual reasoning, thereby providing unnatural responses to questions. In this study, we aim to answer the"}, {"title": "II. BACKGROUND AND PRELIMINARIES", "content": "In this section, we provide background information and preliminaries to contextualize our work. We first review existing adversarial attack algorithms. Next, we discuss the methodologies employed in prior studies. Experts and practitioners already familiar with VLM models and adversarial examples may proceed directly to the subsequent sections.\nUntargeted adversarial attacks. As mentioned in the previous section, adversarial attacks are categorized into: 1) targeted adversarial attacks and 2) untargeted adversarial attacks. In untargeted adversarial attacks, the aim is to cause a neural network model to misclassify the input image by adding a small perturbation to it. For an original input image x with a ground-truth label y, the perturbed image, x' = x + \u2206, is crafted to maximize the loss function J(x', y, \u03b8), where the loss function could be any loss function such as mean square error (MSE) or cross entropy (CE). Then, under a constraint on the maximum perturbation in the p-norm, the optimization problem for untargeted adversarial attacks is formulated as follows:\n\u2206* = arg max J(x', y; \u03b8) subject to ||\u25b3||\u221e\u2264 \u03f5 (1)\nwhere e is a constant defining the perturbation constraint.\nTargeted adversarial attacks. Targeted adversarial attacks involve perturbing the input image to deceive neural network models. Unlike untargeted attacks, targeted adversarial attacks aim to force the model to predict a specific incorrect label, Ytarget. The optimization problem for targeted adversarial attacks is formulated as follows:\n\u2206* = arg min J(x', Ytarget, \u03b8) subject to ||\u25b3||\u221e\u2264 \u03f5 (2)\nwhere e defines the maximum allowable perturbation."}, {"title": "A. Adversarial Attack Algorithms", "content": "To achieve the optimization goals of the problems in Equations (1) and (2), several methods have been proposed based on signed stochastic gradient descent.\nI-FGSM [23]. The Fast Gradient Sign Method (FGSM) is a popular and efficient approach for generating adversarial examples. To generate more effective adversarial examples, one can iteratively apply the FGSM algorithm, i.e., Iterative FGSM (I-FGSM). The update procedure of FGSM is represented by\nXt+1 = Proje (xt + a \u00b7 sign (\u2207xJ(x, y; \u03b8))) (3)\nwhere a is a small constant that controls the step size, and sign (\u2207xJ(x, y; \u03b8)) represents the sign of the gradient of the loss function with respect to the input. Moreover, the function Proje() denotes the projection of the input onto the space defined by ||\u25b3||\u221e\u2264 \u03f5.\nVMI-FGSM [27]. To further enhance the performance of the iterative FGSM, the Variance-tuned Momentum Iterative Fast Gradient Sign Method (VMI-FGSM) has been proposed. The main idea of this approach is to reduce the variance of the gradient, thereby stabilizing the update direction and helping to escape poor local optima. In each iteration, the variance reduction starts with:\nVt = Er~U(\u2212(\u03b2\u00b7 \u03f5)d, (\u03b2\u00b7 \u03f5)d) [\u2207xJ(xt + r, y; \u03b8) \u2013 \u2207xJ(xt, y; \u03b8)] (4)\nwhere r ~ U(\u2212(\u03b2\u00b7 \u03f5)d, (\u03b2\u00b7 \u03f5)d). We note that the vector vt is computed empirically, thereby requiring numerous iterations. Then, the variance-tuned momentum gradient can be obtained as:\ngt+1 = \u03bc.gt + \\frac{\u2207xJ(xt, y; \u03b8) + vt}{||\u2207xJ(xt, y; \u03b8) + vt ||1} (5)\nwhere is a constant that controls the momentum weight. Finally, the perturbed image is updated by\nXt+1 = Proje (xt + a sign(gt+1)) (6)\nAdditionally, Momentum Iterative FGSM (MI-FGSM) [24], Nesterov Iterative FGSM (NI-FGSM) [25], Projected Iterative FGSM (PI-FGSM) [26], and their extensions have been previously developed and are also considered in our experiments."}, {"title": "B. Adversarial Attacks for VLM Models", "content": "Here, we introduce existing approaches for generating adversarial examples against VLMs. Existing VLMs are categorized into: 1) late-fusion VLMs, i.e., models with separate visual and text encoders, and 2) early-fusion VLMs. In late-fusion VLMs, separate visual and text encoders process the visual and textual inputs into a shared latent space, respectively. Several VLMs leverage the late-fusion architecture, including Kosmos-2 [28], the LLAVA families [3], [29], [30], and Claude 3.5 [2]. On the other hand, few studies have proposed early-fusion VLMs, where visual and textual inputs are encoded into a unified encoder. However, due to the challenges of training foundation models without pre-trained encoders, only one such foundation model exists within this architecture [31]. As most VLMs utilize a late-fusion architecture, we focus on generating adversarial examples against late-fusion VLMs.\nLatent-space-based Adversarial Examples. In [10], [11], the authors generated visual adversarial examples by leveraging the encoded latent space. Let Evis(\u00b7) denote the visual encoder,"}, {"title": "III. PROPOSED METHOD", "content": "In this section, we present our proposed method for generating targeted adversarial examples against visual-language multi-modal models.\nOur approach. As previously mentioned, targeted adversarial attacks aim to solve Problem (2). However, unlike previous studies [10], [11], we do not possess the target latent vector, target feature vector, or target class directly. Specifically, as illustrated in (7), the target latent vector Etext(k) does not effectively replace the target object with the target prompt. Instead of the approaches used in prior work, we aim to generate a new target feature vector Ytarget (Replace-then-Perturb), where Ytarget incorporates the latent vector with the target object replaced by the target prompt. Furthermore, to enhance the adversarial example generation process, we propose an efficient algorithm based on contrastive learning (Contrastive-Adv)."}, {"title": "A. Replace-then-Perturb", "content": "Replace-the-Perturb. The Replace-the-Perturb method consists of three steps: (1) segmentation and masking, (2) inpainting, and (3) perturbation. In this subsection, we detail the first two steps (Replace steps), while the perturbation step will be introduced in the next section.\n1) Segmentation and Masking Step. In the first step, we utilize a prompt-based segmentation model [33] to obtain the mask of the target object. As depicted in Fig. 2, CLIP-based text and visual transformer models share the same latent space, thereby enabling zero-shot segmentation. In our approach, by forwarding a text prompt to this model, we obtain a binary mask m \u2208 {0,1}w\u00d7h, which has the same dimensions as the input image.\nDenoting the zero-shot segmentation model as CLIPSeg, the above procedure can be expressed as:\nm = CLIPSeg(x, o) \u2208 {0,1}wxh, (8)\nwhere the region of interest corresponding to the target object is marked as zero, and the background is marked as one.\nFor instance, in Fig. 2, the target object is the 'Stop Sign', and the target prompt is the \u201850 Mph Sign'. By performing zero-shot segmentation with the text 'Stop Sign', we obtain the binary mask corresponding to the stop sign in the image.\n2) Inpainting Step. In this stage, our focus is to replace the target object with the target prompt. For example, in Fig. 2, the Replace-the-Perturb algorithm replaces the 'Stop Sign' with the '50 Mph Sign'. Using the mask obtained in the previous stage, we mask out the target object as follows:\nx = mx, (9)\nwhere denotes the Hadamard product of two matrices or vectors with the same dimensions.\nNext, by leveraging a prompt-based image inpainting method [34]\u2013[36], we fill the unmasked region of the image with the given target prompt. Denoting the prompt-based"}, {"title": "B. Contrastive-Adv", "content": "In this section, we propose an adversarial example generation algorithm inspired by contrastive learning. In the previous section, we propose a method to find the target feature vector ftarget. Then, the only remaining step is to get the perturbation matrix \u2206* by solving the problem (13). Existing adversarial"}, {"title": "IV. NEW DATASET AND QUANTITATIVE METRICS", "content": "A. TA-VLM Dataset\nCurrent datasets, such as VQA, often face challenges in controlling unintended variations when altering the target object, resulting in inconsistent responses.\nB. Quantitative Metrics\nTo evaluate performance on the TA-VLM dataset, we introduce a novel benchmarking metric. Unlike traditional VQA metrics, our metric does not rely on human annotations. Instead, we utilize responses from VLMs, enabling majority voting for more accurate measurements. We conducted experiments using five VLMs: LLAVA 1.5, LLAVA 1.6, Chameleon, Kosmos, and GPT-40 to provide answers for the images. Additionally, similarity models such as All-MINILM-v6 [38], BGE-M3 [39], and CLIP [40] are employed to evaluate the quality of the answers.\nGiven a target object query, the answers from the evaluation VLMs for images x and xtarget are denoted as ans and anstarget, respectively. The answer of the target VLM for the adversarial image x' is defined as ansadv. We compute the similarity score using semantic similarity models such as All-MINILM-v6 [38], BGE-M3 [39], and CLIP [40]. The similarity score determines whether the adversarial answer ansadv is closer to the target answer anstarget compared to the original answer ans. The detailed formula for the similarity score is as follows:\nscore(ans, anstarget, ansadv) = { 1, if cos(anstarget, ansadv) > cos(ans, ansadv) 0, otherwise. (17)\nThis metric assigns a binary score of 1 if the adversarial answer is semantically closer to the target answer than the original answer, and 0 otherwise. By aggregating these scores across all queries and models, we obtain an overall performance measure for our proposed adversarial example generation method.\nMajority voting. We also employ a majority voting mechanism to combine predictions from different VLM models and enhance accuracy. Let mi represent the score of the i-th model, where i = 1,2,..., N, and N is the total number of models. The final prediction m is determined by the majority of the models' predictions, formulated as:\nm = { 1, if \\sum_{i=1}^N m_i \\geq \\frac{N}{2}, 0, otherwise. (18)"}, {"title": "V. EXPERIMENTS", "content": "In this section, we conduct a series of experiments to evaluate the adversarial examples against VLMs using the TA-VLM dataset.\nA. Experimental Details\nOur experimental setup utilizes a single NVIDIA RTX 3090 GPU. We configure the LLAVA series VLMs, specifically LLAVA 1.5 and LLAVA 1.6, as the target neural network models. The adversarial example algorithms are implemented for 200 steps, with the step size a fixed at 1/255. In the Contrastive-Adv method, the weight \u03bc is set to 0.3. The maximum perturbation is constrained using the l\u221e-norm, with values configured at 4/255, 8/255, 16/255, 32/255, and 64/255.\nB. Baselines and Metrics\nBaselines. In our evaluation, we compare our proposed methods against several established adversarial attack techniques.\nMetrics. Our benchmark employs both algorithm-based metrics and VLM-based metrics to evaluate the performance of adversarial examples on the TA-VLM dataset. We note that images in the TA-VLM dataset are fully utilized for evaluation.\nAlgorithm-based Metrics: We utilize the Bilingual Evaluation Understudy (BLEU) score [41] and the Generalized Language Evaluation Understanding (GLEU) metric [42]. These word-counting-based metrics assess the overlap of words between two given sentences, providing a quantitative measure of similarity.\nVLM/LLM-based Metrics: To capture the semantic similarity and contextual relevance of the answers, we employ sentence similarity models, including All-MINILM-v6 [38], BGE-M3 [39], and CLIP [40]. These models evaluate the semantic alignment between the adversarial answers and the target answers, offering a more nuanced assessment of answer quality beyond mere word overlap."}, {"title": "E. Quantitative Results\u2014LLM/VLM-based metrics", "content": "In this section, we assess the superiority of our proposed method using VLM-based metrics. Table IV presents a detailed comparison between existing adversarial attack methods and our approach. We utilize five different VLMs-Chameleon, Kosmos, LLAVA 1.5, LLAVA 1.6, and GPT-40-to generate reference answers and report both majority vote and average scores across these models. For evaluation, we leverage semantic similarity scores from models such as All-MiniLM, BGE-M3, BERT, and CLIP.\nReplace-then-Perturb vs. Latent-based. For the ablation study, we compare the I-FGSM method using the latent-based approach against our Replace-then-Perturb method, as shown in the left two columns of Table II. The proposed Replace-then-Perturb method outperforms the latent-based method, including methods based on CLIP, across all evaluation metrics.\nContrastive-Adv vs. Other Adversarial Attacks. The VLM-based scores in Table IV reveal that the Contrastive-Adv method significantly outperforms baseline adversarial attacks using Replace-then-Perturb. For example, in the BGE-M3 dense score with majority voting, our method achieves a 10% point higher score compared to the best baseline. Across all other metrics, Contrastive-Adv consistently surpasses all baseline methods. These results demonstrate that our approach effectively perturbs the original image, enabling the target VLM model to recognize the target object as the specified prompt.\nVarious values of \u20ac. Table V displays the average and majority voting scores of various adversarial attack methods, with e ranging from 4/255 to 64/255. Consistent with the findings in Sec. V-D, our proposed method outperforms the baseline methods across all e values. Notably, despite the latent-based I-FGSM method having more degrees of freedom, our Replace-then-Perturb method surpasses it even at smaller e levels. At higher e values, the Replace-then-Perturb method outperforms the latent-based adversarial attacks, and our Contrastive-Adv method further enhances the evaluation scores."}, {"title": "F. Positive Questions", "content": "Unlike negative questions, where adversarial perturbations alter the answers, positive questions require answers to remain consistent after perturbation. Table VI reveals that cosine similarity scores are nearly identical across all methods for positive questions. Consequently, we ranked three methods-1)Latent-based I-FGSM, 2) Replace-then-Perturb-based I-FGSM, and 3) Replace-then-Perturb-based Contrastive-Adv-according to cosine similarity. The results demonstrate that both proposed methods outperform the baseline methods, with Replace-then-Perturb slightly surpassing Latent-based I-FGSM and Contrastive-Adv exhibiting marginally lower performance due to its triplet loss function. Additionally, when compared alongside negative question results, our methods consistently show significant improvements over the baselines."}, {"title": "Limitations", "content": "Summary. In this paper, we propose two methods for generating adversarial examples for VLMs: Replace-then-Perturb and Contrastive-Adv. We address the challenge of generating appropriate target feature vectors for replacing target objects with target prompts, a limitation in existing methods. The Replace-then-Perturb method leverages zero-shot semantic segmentation and inpainting to create target feature vectors. Building upon this, the Contrastive-Adv method further enhances the performance of adversarial examples. To evaluate our methods, we introduce a dataset comprising images, target objects, target prompts, and positive/negative questions.\nLimitations. Our work has two primary limitations. First, we do not evaluate the effectiveness of our methods against existing defense mechanisms against adversarial examples [43], [44], as this was outside the scope of our study. Second, our"}, {"title": "B. LLAVA 1.6 Results", "content": "Table VII presents the quantitative results for LLAVA 1.6 as the target neural network model. Consistent with the findings in Tables II and IV, our Contrastive-Adv method outperforms most baseline adversarial attacks except for VMI-FGSM. Although VMI-FGSM achieves slightly higher scores, it is approximately ten times more computationally intensive due to its variance computation step. To further validate our approach, we introduced an enhanced Contrastive-Adv variant, where the perturbation step in Lines 10-11 of Algorithm 2 is replaced with VMI-FGSM as defined in Equations (4), (5), and (6). The results in Table VII show that this modified method significantly outperforms all baseline methods."}]}