{"title": "Every Call is Precious: Global Optimization of Black-Box Functions with Unknown Lipschitz Constants", "authors": ["Fares Fourati", "Salma Kharrat", "Vaneet Aggarwal", "Mohamed-Slim Alouini"], "abstract": "Optimizing expensive, non-convex, black-box Lipschitz continuous functions presents significant challenges, particularly when the Lipschitz constant of the underlying function is unknown. Such problems often demand numerous function evaluations to approximate the global optimum, which can be prohibitive in terms of time, energy, or resources. In this work, we introduce Every Call is Precious (ECP), a novel global optimization algorithm that minimizes unpromising evaluations by strategically focusing on potentially optimal regions. Unlike previous approaches, ECP eliminates the need to estimate the Lipschitz constant, thereby avoiding additional function evaluations. ECP guarantees no-regret performance for infinite evaluation budgets and achieves minimax-optimal regret bounds within finite budgets. Extensive ablation studies validate the algorithm's robustness, while empirical evaluations show that ECP outperforms 10 benchmark algorithms including Lipschitz, Bayesian, bandits, and evolutionary methods across 30 multi-dimensional non-convex synthetic and real-world optimization problems, which positions ECP as a competitive approach for global optimization.", "sections": [{"title": "Introduction", "content": "Global optimization is an evolving field of optimization that seeks to identify the best possible solution across the entire problem space, i.e., the entire set of feasible solutions, ensuring that the global optimum is found or at least well-approximated, even with a limited number of function evaluations. The objective function may be non-convex and exhibit multiple local optima. As a result, local optimization methods may only find a solution that is optimal in a limited region of the search space, potentially yielding a globally suboptimal result. Moreover, the objective function can be non-differentiable or a black-box function, meaning it is only accessible through direct evaluations. Furthermore, evaluating the objective function can be expensive, requiring substantial amounts of money, time, or energy, which makes the process even more challenging, as it can limit the number of function evaluations.\nDespite the challenges, global optimization problems are prevalent in engineering and real-world systems. These problems are applicable in various fields, including mechanical, civil, and chemical engineering, as well as in structural optimization, molecular biology, circuit chip design, and image processing. More recently, with the emergence of large language models (LLMs), a new line of work has focused on instruction learning in black-box LLMs, such as ChatGPT, through calls, without access to their underlying models. Therefore, the development of efficient global optimization algorithms represents an intriguing research direction with the potential for significant impact across several disciplines.\nA subfield of global optimization is Lipschitz optimization, which assumes knowledge of the Lipschitz constant or an upper bound for it. While observed that practical objective functions often exhibit Lipschitz continuity, the exact value of this constant is seldom known. In this work, we focus on black-box Lipschitz continuous functions with unknown constants. In such cases, a common approach is to estimate the Lipschitz constant or an upper bound and use this estimate as a proxy for the true constant."}, {"title": "Related Works", "content": "Several methods have been proposed for global optimization, with the simplest being non-adaptive exhaustive searches, such as grid search, which uniformly divides the space into representative points, or its stochastic alternative, Pure Random Search (PRS), which employs random uniform sampling. However, these methods are often inefficient, as they fail to exploit previously gathered information or the underlying structure of the objective function.\nTo enhance efficiency, adaptive methods have been developed that leverage collected data and local smoothness. Some of these methods need the knowledge of the local smoothness, including HOO, Zooming, and DOO, while others do not, such as SOO and SequOOL. In this work, however, we focus on Lipschitz functions.\nTo address Lipschitz functions with unknown Lipschitz constants, the DIRECT algorithm employs a deterministic splitting approach of the whole space, sequentially dividing and evaluating the function over subdivisions that have recorded the highest upper bounds."}, {"title": "Problem Statement", "content": "In this work, we consider a black-box, non-convex, deterministic, real-valued function $f$, which may be expensive to evaluate requiring significant time, energy, or financial resources. The function is defined over a convex, compact set $X \\subset \\mathbb{R}^d$ with a non-empty interior and has a maximum over its input space\u00b9.\nThe objective of this work is global maximization, seeking a global maximizer, defined as follows:\n$x^* \\in \\arg \\max_{x \\in X} f(x)$\nwith a minimal number of function calls. Starting from an initial point $x_1$ and given its function evaluation $f(x_1)$, adaptive global optimization algorithms leverage past observations to identify potential global optimizers. Specifically, depending on the previous evaluations $(x_1, f(x_1)),\u00b7\u00b7\u00b7, (x_t, f(x_t))$, it chooses at each iteration $t\\geq 1$ a point $x_{t+1} \\in X$ to evaluate and receives its function evaluation $f(x_{t+1})$. After $n$ iterations, the algorithm returns $\\hat{x}_n$, one of the evaluated points, where $\\hat{x}_n \\in \\arg \\max_{i=1,...,n} f(x_i)$, representing the point with the highest evaluation.\nTo assess the performance of an algorithm $A \\in \\mathcal{G}$, where $\\mathcal{G}$ is the set of global optimization algorithms, over the function $f$, we consider its regret after $n$ iterations, i.e., after evaluating $x_1,..., x_n$ by $A$, as follows:\n$R_{A,f}(n) = \\max_{x \\in X} f(x) - \\max_{i=1,...,n} f(x_i), (1)$\nmeasuring the difference between the true maximum and the best evaluation over the $n$ iterations.\nWe consider $f$ to be Lipschitz with an unknown finite Lipschitz constant $k$, i.e., there exists an unknown $k > 0$, such that for any two points, $x$ and $x'$ in $X$, the absolute difference between $f(x)$ and $f(x')$ is no more than $k$ times the distance between $x$ and $x'$, i.e.,\n$\\forall (x,x') \\in X^2 \\quad |f(x) - f(x')| \\leq k \\cdot ||x - x'||_2$.\nMorever, we denote the set of Lipschitz-continuous functions defined on $X$, with a Lipschitz constant $k$, as $\\text{Lip}(k) := \\{f : X \\rightarrow \\mathbb{R} \\, \\text{s.t.} \\, |f(x) - f(x')| \\leq k \\cdot ||x - x'||_2, \\forall (x, x') \\in X^2\\}$ and their union $\\bigcup_{k>0} \\text{Lip}(k)$ denotes the set of all Lipschitz-continuous functions.\nWe define the notion of no-regret, equivalent to optimization consistency (Malherbe and Vayatis, 2017),\n$\\forall x \\in \\mathbb{R}^d$, we denote its $l_2$-norm as $||x||_2 = (\\sum_{i=1}^d x_i^2)^{1/2}$. We define $B(x,r) = \\{x' \\in \\mathbb{R}^d : ||x - x'||_2 \\leq r\\}$ the ball centered in $x$ of radius $r \\geq 0$. For any bounded set $X \\subset \\mathbb{R}^d$, we define its radius as $\\text{rad}(X) = \\max\\{r > 0 : \\exists x \\in X \\text{ where } B(x,r) \\subseteq X\\}$ and its diameter as $\\text{diam}(X) = \\max_{(x,x')\\in X^2} ||x - x'||_2$."}, {"title": "Every Call is Precious (ECP)", "content": "In this section, we present ECP, an efficient algorithm, for maximizing an unknown (possibly expensive) function $f$ without knowing its Lipschitz constant $k \\geq 0$.\nOur proposed algorithm, ECP, presented in Algorithm 1, takes as input the number of function evaluations $n \\in \\mathbb{N}^*$ (budget), the search space $X$, the black-box function $f$, a value $\\varepsilon_1 > 0$, a coefficient $\\tau_{n,d} > 1$, and a constant $C > 1$. The algorithm begins by sampling and evaluating a point $x_1$ uniformly at random from the entire space $X$ (line 1). It then proceeds through $n - 1$ rounds (each round concludes after one function evaluation), where in each round $t\\geq 1$ (up to $t = n - 1$), a random variable $x_{t+1}$ is repeatedly sampled uniformly from the input space $X$ until a sample meets the acceptance condition. Once the condition is satisfied, the sample is evaluated, and the algorithm moves to the next round $t + 1$.\nECP accepts (evaluates) the sampled point $x_{t+1} = x$ if and only if the following inequality is verified:\n$\\min_{i=1,...,t} (f(x_i) + \\varepsilon_t \\cdot ||x - x_i||_2) \\geq \\max_{j=1,...,t} f(x_j)$,\nwhere $\\varepsilon_t$ is a growing sequence staring from $\\varepsilon_1$ and continuously multiplied by a coefficient $\\tau_{n,d} > 1$. An illustration of the acceptance region for a non-convex, single-dimensional objective function can be found in Figure 2, where it can be seen that $\\varepsilon_t$ controls the acceptance region size. The coefficient $\\tau_{n,d} > 1$ is some non-decreasing function of $n$ and $d$, such as $\\tau_{n,d} = \\max\\{1 + \\frac{1}{n}, \\frac{2}{d}\\} > 1$.\nThe algorithm tracks the number of sampled but rejected points during each iteration $t > 1$ before increasing $\\varepsilon_t$, using the variable $h_{t+1}$. This variable is initialized to zero (lines 2) and is reset to zero whenever $\\varepsilon_t$ is increased (line 6 and 11). Additionally, $h_t$ is initialized with the number of rejections from the previous iteration (lines 2 and 10), before acceptance at iteration t-1. When the difference between the current and the previous number of rejections exceeds a given threshold $C > 1$ (line 5), $\\varepsilon_t$ is increased by multiplying it with the factor $\\tau_{n,d} > 1$. This growth condition is further analyzed in Proposition 5.\nThus, $\\varepsilon_t$ grows when a rapidly increasing number of samples is generated without an accepted point (lines 5-6) and also when a sample is evaluated (line 11)."}, {"title": "Theoretical Analysis", "content": "In the following, we provide theoretical analysis of ECP. First, we motivate the considered acceptance region, then we analyze the rejection growth and the computational complexity, and finally, we show that ECP is no-regret with optimal minimax regret bound."}, {"title": "Acceptance Region Analysis", "content": "In this section, we motivate the proposed acceptance region and the design of the algorithm with respect to the growing $\\varepsilon_t$. The acceptance region is inspired by the previously studied active subset of consistent functions in active learning. Hence, we start by the definition of consistent functions.\nDefinition 3. (CONSISTENT FUNCTIONS) The active subset of Lipschitz functions, with a Lipschitz constant $k$, consistent with the black-box function $f$ over $t > 1$ evaluated samples $(x_1, f(x_1)),\u00b7\u00b7\u00b7, (x_t, f(x_t))$ is:\n$\\mathcal{F}_{k,t} \\triangleq \\{g \\in \\text{Lip}(k) : \\forall i \\in \\{1,......,t\\}, g(x_i) = f(x_i)\\}$.\nUsing the above definition of a consistent function, we define the subset of points that can maximize at least some function $g$ within that subset of consistent functions and possibly maximize the target $f$.\nDefinition 4. (POTENTIAL MAXIMIZERS) For a Lipschitz function $f$ with a Lipschitz constant $k \\geq 0$, let $\\mathcal{F}_{k,t}$ be the set of consistent functions with respect to $f$, as defined in Definition 3. For any iteration $t \\geq 1$, the set of potential maximizers is defined as follows:\n$\\mathcal{P}_{k,t} \\triangleq \\{ x \\in X: \\exists g \\in \\mathcal{F}_{k,t} \\text{ where } x \\in \\arg \\max_{x \\in X} g(x)\\}$.\nWe can then show the relationship between the potential maximizers and our proposed acceptance re-"}, {"title": "Rejection Growth Analysis and Computational Complexity", "content": "The result in Proposition 3 demonstrates that the acceptance region is non-increasing over time, with respect to iteration $t$, when a constant $\\varepsilon_t$ is used, leading to a non-decreasing probabilistic rejection of sampled points. Furthermore, the result in Lemma 1 shows that the acceptance region increases with rising $\\varepsilon_t$ values in a given iteration $t$, resulting in a decreasing probabilistic rejection of sampled points. When $\\varepsilon_t$ increases within the same iteration $t$, it is scaled by a multiplicative factor $\\tau_{n,d} > 1$ whenever growth is detected, i.e., $\\varepsilon_t$ becomes $\\varepsilon_t \\tau_{n,d}^{v_t}$, where $v_t$ represents the number of growth detection within iteration $t$.\nConsider $\\Delta = \\max_{x \\in X} f(x) - \\min_{x \\in X} f(x)$, $\\lambda$ the standard Lebesgue measure that generalizes the notion of volume of any open set, and $\\Gamma(x) = \\int_0^\\infty t^{x-1}e^{-t} dt$. In what follows, we characterize this rejection growth by providing an upper bound on the probability of rejection in Proposition 4, proved in Appendix C.5, function of the algorithm constants $\\varepsilon_1 > 0$ and $\\tau_{n,d} > 1$, with $\\varepsilon_t > \\varepsilon_1 \\tau_{n,d}^{(t-1)}$ and $v_t$ which depends on $C \\geq 1$.\nProposition 4. (ECP REJECTION PROBABILITY) For any Lipschitz function $f$, let $(x_i)_{1 \\leq i \\leq t}$ be the previously evaluated points of ECP until time $t$, and let $v_t$ the number of increases of $\\varepsilon_t$ at iteration $t$ (i.e., the number of times we validate growth condition in iteration $t$). For any $x \\in X$, let $R(x,t,v_t)$ be the event of"}, {"title": "Regret Analysis", "content": "In the following we provide the regret guarantees of ECP, both for infinite and finite budgets. But first, we define the $i^*$ as the hitting time, after which $\\varepsilon_t$ reaches or overcomes the Lipschitz constant $k$.\nDefinition 5. (HITTING TIME) For the sequence $(\\varepsilon_i)_{i \\in \\mathbb{N}}$ and the unknown Lipschitz constant $k > 0$, we can define $i^* \\triangleq \\min \\{i \\in \\mathbb{N}^* : \\varepsilon_i \\geq k\\}$.\nIn the following lemma, we upper-bound the time $t$ after which $\\varepsilon_t$ is guaranteed to reach or exceed $k$. This"}, {"title": "Numerical Analysis", "content": "Since we do not assume knowledge of the Lipschitz constant, we consider benchmarks that do not require that. We compare our approach against 10 benchmarks, including AdaLIPO, AdaLIPO+, PRS, DIRECT, DualAnnealing, CMA-ES, Botorch, SMAC3, A-GP-UCB, and NeuralUCB. We fix the same budget $n$ for all the methods and report the maximum achieved value over the rounds, averaged over 100 repetitions, with reported standard deviations.\nWe evaluate the proposed method on various global optimization problems using both synthetic and real-world datasets. The implementation of the considered objectives is publicly available at https://github.com/fouratifares/ECP.\nThe synthetic functions were designed to challenge"}, {"title": "Discussion", "content": "Several global optimization approaches rely on surrogate models. While surrogate-based methods excel at modeling complex function landscapes, they introduce significant computational overhead and risk overfitting in low-budget scenarios. With a limited budget, learning an accurate surrogate model becomes challenging, leading to poor sampling and suboptimal per-"}, {"title": "Conclusion", "content": "We introduce ECP, a global optimization algorithm for black-box functions with unknown Lipschitz constants. Our theoretical analysis shows that ECP is no-regret as evaluations increase and meets minimax optimal regret bounds within a finite budget. Empirical results across diverse non-convex, multi-dimensional optimization tasks demonstrate that ECP outperforms state-of-the-art methods."}, {"title": "ECP Hyper-parameter Discussion and Ablation Study", "content": "ECP requires three hyperparameters: $\\epsilon_1 > 0$ (arbitrary small), $\\tau_{n,d} > 1$, and $C > 1$. These parameters have been theoretically studied and empirically verified. In our experiments, across all optimization problems represented in Table 1, Table 2, and Table 3, we fixed $\\epsilon_1 = 10^{-2}$ and used $\\tau_{n,d} = max\\{1 + \\frac{1}{n}, \\frac{2}{d}\\}$ with $\\tau = 1.001$ and $C = 10^3$. Ablation studies have conducted in Appendix E.4 and Appendix E.3.\nIncreasing the values of $\\epsilon_1$ and $\\tau_{n,d}$ causes the rejection probability to approach zero, as shown in Proposition 4, reducing the algorithm to a pure random search and undermining the efficiency of function evaluations. Therefore, smaller values for both $\\epsilon_1$ and $\\tau_{n,d}$ are required. By multiplying $\\epsilon_t$ by $\\tau_{n,d} > 1$ during rejection growth, we guarantee the eventual acceptance of a point, even with small values of $\\tau_{n,d}$ and $\\epsilon_1$, as shown in Corollary 1.\nNote that a larger constant C implies less constraint on rejection growth, which leads to greater patience before increasing $\\epsilon_t$. Consequently, increasing C results in higher rejection rates, further drifting the algorithm away from pure random search at the cost of potentially longer waiting times to accept a sampled point.\nTherefore, either increasing C, decreasing $\\tau_{n,d}$, or decreasing $\\epsilon_1$ result in higher rejection rates, which result in more careful acceptance at the cost of an increasing computational complexity of the algorithm, see Theorem 1.\nWe could have achieved better results with a larger C and smaller $\\epsilon_1$ or $\\tau$. However, we fixed these parameters because they demonstrate outstanding performance while still being a fast algorithm. Users of this algorithm can indeed try other values depending on their problem constraints."}, {"title": "Ablation Study on the Constant $\\epsilon_1$", "content": "In the following, we test the performance of ECP with various values of $\\epsilon_1$, while keeping $\\tau = 10^{-3}$ and $C = 10^3$ fixed. As shown in Figure 5, for different values of $\\epsilon_1 \\in [1.0001, 1.001, 1.01, 1.1, 1.5]$, the performance of ECP remains consistent. While for the Hartman 6D function, smaller value of $\\epsilon_1$ lead to noticeably better results, in general, for all functions, smaller $\\epsilon_1$ values lead to better results as predicted by theory. However, in most of the examples, the differences are less significant. Therefore, the performance of ECP is both consistent and robust across different values of $\\epsilon_1$. In our work, we chose a middle value of $\\epsilon_1 = 10^{-2}$, as it achieves good performance while being computationally less expensive than much smaller values of $\\epsilon_1$, as predicted by Theorem 1."}, {"title": "Ablation Study on the Coefficient $\\tau$", "content": "Recall that $\\tau_{n,d} = max\\{1 + \\frac{1}{n}, \\tau\\}$, therefore the choice of $\\tau$ only impacts the algorithm, when the value of t is larger than 1 + $\\frac{1}{n}$. In the following, we test the performance of ECP, with various values of $\\tau$, with fixed $C = 10^3$ and $\\epsilon_1 = 10^{-2}$. As shown in Figure 6, for different values of $\\tau \\in [1.001, 1.01, 1.1, 1.2, 1.4, 1.6, 1.8, 2]$, the performance of ECP remains consistent. While smaller values of $\\tau$ (blue and orange) yield significantly better results for the Ackley and Hartmann 6D functions, in general, smaller $\\tau$ values tend to perform better across all functions, as predicted by theory. However, in some cases, such as AutoMPG and Damavandi, the differences are less pronounced. Therefore, the performance of ECP is both consistent and robust across various values of $\\tau$. In this work, we chose $\\tau = 1.001$, as smaller values would not be considered since $\\tau_{n,d} = max\\{1 + \\frac{1}{n}, \\tau\\}$, and we are considering settings with limited budgets."}, {"title": "Ablation Study on the Constant C", "content": "In the following, we test the performance of ECP with various values of C, while keeping $\\tau$ = $10^{-3}$ and $\\epsilon_1$ = $10^{-2}$ fixed. As shown in Figure 7, for different values of C $\\in [1, 10, 100, 1000, 2000, 4000, 6000, 8000]$, the performance of ECP remains consistent. While for the Ackley function, larger values of C lead to remarkably better results, in general, for all functions, larger C values lead to better results as predicted by theory. However, in examples such as AutoMPG, Damavandi, and Bukin, the differences are less significant. Therefore, the performance of ECP is both consistent and robust across different values of C. In our work, we chose a middle value of C = 1000, as it achieves good performance while being computationally less expensive than larger values of C, as predicted by Theorem 1."}]}