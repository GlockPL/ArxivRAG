{"title": "TEXTUAL AESTHETICS IN LARGE LANGUAGE MODELS", "authors": ["Lingjie Jiang", "Shaohan Huang", "Xun Wu", "Furu Wei"], "abstract": "Image aesthetics is a crucial metric in the field of image generation. However, textual aesthetics has not been sufficiently explored. With the widespread application of large language models (LLMs), previous work has primarily focused on the correctness of content and the helpfulness of responses. Nonetheless, providing responses with textual aesthetics is also an important factor for LLMs, which can offer a cleaner layout and ensure greater consistency and coherence in content. In this work, we introduce a pipeline for aesthetics polishing and help construct a textual aesthetics dataset named TEXAES. We propose a textual aesthetics-powered fine-tuning method based on direct preference optimization, termed TAPO, which leverages textual aesthetics without compromising content correctness. Additionally, we develop two evaluation methods for textual aesthetics based on text and image analysis, respectively. Our experiments demonstrate that using textual aesthetics data and employing the TAPO fine-tuning method not only improves aesthetic scores but also enhances performance on general evaluation datasets such as AlpacalEval and Anera-hard.", "sections": [{"title": "INTRODUCTION", "content": "Image aesthetics (Huang et al., 2024a; Murray et al., 2012; Kong et al., 2016; Ke et al., 2021; Bosse et al., 2017) has emerged as a prominent research area within computer vision, focusing on assessing and improving the visual appeal of images. Aesthetics has recently been integrated into state-of-the-art image generation models, such as diffusion models (Rombach et al., 2022), significantly enhancing the visual quality of generated images (Wu et al., 2024a; 2023) and aligning them more closely with human preferences (Huang et al., 2024a; Wu et al., 2024b; 2023).\nMeanwhile, advancements in large language models (LLMs) like ChatGPT (OpenAI, 2023) and LLAMA (Touvron et al., 2023b; Dubey et al., 2024) have demonstrated impressive generative capabilities across various domains, including code, articles, and web content. Although LLMs have made significant progress in generating textual content, enhancing the aesthetic quality of their output remains a critical challenge. A more aesthetically appealing and organized output not only improves user engagement by making the content more intuitive and comfortable to read but also enhances consistency and coherence. Consequently, exploring the textual aesthetics of LLMs is a highly desirable area of research.\nIn this work, we present the first investigation into improving the aesthetic quality of text generated by LLMs. Unlike image aesthetics benefiting from numerous large-scale aesthetic datasets (e.g., AVA (Murray et al., 2012) and AesBench (Huang et al., 2024b)), advanced aesthetic learning technology (Huang et al., 2024a; Zhang & Liu, 2023; Yang et al., 2022; Su et al., 2020) and reliable aesthetic evaluation methods (Deng et al., 2017; Su et al., 2011), textual aesthetics in LLMs lacks similar resources and established models.\nTo address this challenge, we first designed an aesthetic data generation pipeline leveraging GPT-40 for aesthetic polishing. This scalable pipeline can generate large volumes of high-quality aesthetic preference data. Based on this framework, we constructed the first aesthetic dataset in the LLM domain, TEXAES, which contains a total of 50,390 prompts data."}, {"title": "", "content": "Based on TEXAES, existing post-training techniques such as DPO (Rafailov et al., 2024b) can be used to fine-tune current LLMs at the aesthetic level. However, we found that directly applying these techniques not only failed to align effectively with the characteristics of our TEXAES, limiting its impact on aesthetic fine-tuning, but also negatively impacted the overall performance of these LLMs. To address this issue, we propose Textual Aesthetics Preference Optimization (TAPO) which employs the Plackett-Luce (Luce, 1959; Plackett, 1975) model with adjustable optimization weights to better leverage our dataset and enhance aesthetic fine-tuning performance. Furthermore, to better assess the aesthetic quality of LLM outputs, we have developed two evaluation pipelines: one based on text and the other based on images, respectively.\nTo validate the effectiveness of our TEXAES and TAPO, we performed aesthetic fine-tuning on the open-source LLaMA series models (Dubey et al., 2024) and compared the aesthetic scores of the fine-tuned LLMs with state-of-the-art LLMs at different scales (from 8B to 70B). Additionally, to ensure objective and reliable results, we employed human experts for professional evaluation. Extensive experimental results ultimately demonstrated the effectiveness of our TEXAES and TAPO.\nOur main contributions are listed as follow:\n\u2022 To the best of our knowledge, we for the first time indciate the crucial issue of exploring and improving the textual aesthetics in LLMs.\n\u2022 We systematically identify the lack of related textual aesthetics datasets, and introduce a novel pipeline for aesthetic text polishing and contribute to the construction of a textual aesthetics dataset, named TEXAES.\n\u2022 Based on TEXAES, we propose a DPO-based aesthetic fine-tuning algorithm, named TAPO, to effectively enhances the LLMs' aesthetic quality while preserving its general performance.\n\u2022 Both qualitative and quantitative extensive experiments demonstrate that utilizing TEXAES and TAPO not only improves aesthetic scores but also enhances the general capabilities of LLMs."}, {"title": "RELATED WORKS", "content": ""}, {"title": "IMAGE AESTHETICS", "content": "Image aesthetics (Huang et al., 2024a; Murray et al., 2012; Kong et al., 2016) is a subfield of computer vision that focuses on assessing (Deng et al., 2017; Su et al., 2011) and improving the aesthetic quality of images (Bhattacharya et al., 2010; Deng et al., 2018). Early work in the field of image aesthetics focused on using handcrafted metrics to assess aesthetic scores (Nack et al., 2001; Neumann et al., 2005). However, with the development of deep learning, there has been significant interest in applying CNN (Bosse et al., 2017; Li et al., 2018; Su et al., 2020) or Transformer (Ke et al., 2021; Zhang & Liu, 2023; Yang et al., 2022; Qin et al., 2023) based methods to solve image aesthetics problems, which have demonstrated promising results. Recently, multi-modal large language models (MLLMs) have shown superior aesthetic perception and robustness in the fields of image aesthetics, greatly surpassing lightweight models due to their vast knowledge base and strong reasoning and memory capabilities. (Huang et al., 2024a;b; Wu et al., 2024b)."}, {"title": "LLM PREFERENCES DATA", "content": "Preference learning is an optimization method for LLMs designed to enhance their ability to generate outputs that better align with human preferences (F\u00fcrnkranz & H\u00fcllermeier, 2010; Schulman et al., 2017; Rafailov et al., 2024b; Ouyang et al., 2022). Increasing attention has also been drawn to the importance of data used during the preference learning phase. Some studies focus on constructing domain-specific datasets for preference learning, e.g., summarization (Stiennon et al., 2020; Wu et al., 2021) and question answering (Nakano et al., 2021). Cui et al. (2024) highlights the scarcity of large-scale, general-purpose preference datasets and propose UltraFeedback to addresses this gap by collecting over 1 million preference feedback samples using GPT-4 (OpenAI, 2023). Lee et al. (2023) also pointed out that utilizing AI-generated preference feedback is an effective and cost-efficient method for expanding preference datasets. While the aforementioned work provides preference datasets for specific domains as well as general-purpose tasks, none of them have addressed the critical area of text aesthetics in LLMs, which motivated us to design corresponding data construction pipeline and related dataset like TEXAES to support future research in text aesthetics."}, {"title": "TEXTUAL AESTHETICS", "content": ""}, {"title": "OVERVIEW", "content": "Textual aesthetics, which encompass the aesthetic attributes of a text at both the content and visual levels, can be dissected into four fundamental aspects. Clarity (readability) pertains to the ease with which a text can be read and comprehended, necessitating optimal sentence length and grammatical complexity (DuBay, 2004). Layout (visual organization) involves the systematic arrangement of text elements, such as headings and subheadings, to guide the reader effectively. Uniformity (consistency) demands a consistent style and formatting throughout the text to enhance readability and facilitate a smoother reading experience. Coherence (overall structure) ensures that paragraphs are well-organized and logically connected, facilitating easier comprehension of the content (Van Silfhout et al., 2014)."}, {"title": "AESTHETICS POLISHING", "content": "Human preference data is critical for aligning large language models and improving their performance across various dimensions, such as helpfulness (Askell et al., 2021; Kreutzer et al., 2018; Stiennon et al., 2020), harmlessness (Bai et al., 2022; Glaese et al., 2022), and honesty (Ouyang et al., 2022). Consequently, we believe that a textual aesthetic preference dataset will also be beneficial for research on the alignment of LLMs. However, current literature reveals a conspicuous absence of research specifically addressing the textual aesthetics of LLMs, as well as a lack of corresponding textual aesthetic preference data. To address this gap, we have developed a method for textual aesthetic polishing to construct a dataset that optimizes the aesthetic preferences of LLMs.\nGiven that the goal of polishing is to enhance textual aesthetics, we can build our textual aesthetic preference dataset based on an available preference dataset such as UltraFeedback (Cui et al., 2024). UltraFeedback is a comprehensive dataset with responses evaluated by GPT-4 based on criteria such as instruction-following, honesty, and helpfulness. Since the selected data exhibits higher scores in these areas, thereby aligning more closely with human preferences, we can utilize these chosen responses as our candidates to build our textual aesthetic preference dataset.\nTo effectively achieve our objectives, we designed a chain of thought (Wei et al., 2023) methodology by using GPT-40 to polish our original responses, the following steps were taken:\n1. Semantic Analysis: GPT-40 initially analyzed the textual semantics of the provided instructions and selected responses.\n2. Aesthetic Evaluation: Based on textual aesthetic factors such as paragraph structure, indentation, headings, and subheadings, GPT-40 conducted a detailed textual aesthetic analysis.\n3. Binary Classification: GPT-40 then performed a binary classification to determine whether the response required modification to improve readability and comprehension.\n4. Revision Process: For responses that required modification, GPT-40 generated a revised version that preserved the original style and format while enhancing readability and comprehensibility."}, {"title": "TEXTUAL AESTHETICS SCORING", "content": "To validate the aesthetic quality of texts generated by large language models and to assess the effectiveness of our aesthetic preference dataset, a robust method for evaluating text aesthetics is indispensable. Previous studies, such as AlpacaEval (Li et al., 2023; Dubois et al., 2024), MT-Bench (Zheng et al., 2023), and Arena-Hard (Li et al., 2024), suggest that using LLMs as evaluators can effectively approximate human preferences. Consequently, we employ the \"LLM as a judge\" framework to approximate human preferences for text aesthetics. We evaluate the aesthetic quality of texts generated by LLMs using two methods: text-based and image-based text aesthetic scoring."}, {"title": "Text-Based Text Aesthetic Scoring", "content": "We randomly selected 500 prompts from Arena-Hard (Li et al., 2024) as our evaluation dataset. Following practices from Arena-Hard and MT-Bench (Zheng et al., 2023), we implemented a pairwise comparison method, comparing the performance of model \\(\\pi_{\\theta}\\) on prompt p with a robust baseline model (GPT-4-0314) to derive aesthetic preference scores. Judges assessed aesthetic preferences on a Likert scale (Likert, 1932) (1 = prefers \\(\\pi_{\\theta}(p)\\) much less than base\\((p)\\), 5 = prefers \\(\\pi_{\\theta}(p)\\) much more than base(p)). This methodology ensures that models are penalized more heavily for substantial losses than for minor ones, effectively differentiating between models. Using the chain-of-thought approach, judges evaluated text aesthetics based on four dimensions: readability, visual organization, consistency, and overall structure. To mitigate position bias, we employed a two-game setup by swapping model positions for each query. Following the practices of Chatbot Arena, we adopted the Bradley-Terry (Bradley & Terry, 1952) model to generate final scores. We aggregated all pairwise comparisons with the baseline model and employed bootstrapping to derive a bootstrapped confidence interval for all models' win rates against the baseline, producing an ordered ranking of all models based on their win rates."}, {"title": "Image-Based Text Aesthetic Scoring", "content": "Our conceptualization of text aesthetics encompasses not only textual readability and comprehensibility but also visual appeal. Given GPT-40's exceptional multimodal capabilities, we utilized GPT-4o to evaluate text aesthetics from a visual perspective as well. In our experiments, we rendered the LLM-generated texts as HTML with consistent CSS styles, converted them into images of identical size, and then had GPT-40 evaluate these images based on the same criteria used for textual evaluation."}, {"title": "TEXTUAL AESTHETICS-POWERED TRAINING", "content": ""}, {"title": "DIRECT PREFERENCE OPTIMIZATION TRAINING", "content": "Reinforcement Learning with Human Feedback (RLHF)(Christiano et al., 2017) has emerged as a pivotal technique in aligning LLMs (Bai et al., 2022; Ouyang et al., 2022; Stiennon et al., 2020). Early implementations of RLHF primarily relied on reinforcement learning and alternative approaches (Snell et al., 2022; Touvron et al., 2023a; Gulcehre et al., 2023). Rafailov et al. (2024a) proposed a RL-free closedform counterpart known as Direct Preference Optimization (DPO) which has shown impressive performances (Ivison et al., 2023; Jiang et al., 2023; Tunstall et al., 2023).\nThe naive DPO uses a pair of preference data, which includes a chosen response and a rejected response for each prompt, based on the Bradley-Terry (Bradley & Terry, 1952) model for optimization. The loss function for DPO is defined as follows:\n\\[LDPO(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} \\left[ logo \\sigma \\left( \\beta log \\frac{\\pi_{\\theta}(y_w |x)}{\\pi_{ref}(y_w | x)} -  \\beta log \\frac{\\pi_{\\theta}(y_l |x)}{\\pi_{ref}(y_l | x)} \\right) \\right].\\]"}, {"title": "TEXTUAL AESTHETICS PREFERENCE OPTIMIZATION TRAINING", "content": "For each prompt in our TEXAES dataset, there are three responses: \\(y_t\\), \\(y_w\\), and \\(y_l\\). The response \\(y_t\\) has the same semantic content as \\(y_w\\) but is superior in terms of textual aesthetics. The response \\(y_w\\), in turn, is more aligned with human preferences for chatbots in terms of instruction-following, truthfulness, honesty, and helpfulness compared to \\(y_l\\). The response \\(y_r\\) is the least preferred response in terms of both textual aesthetics and human preferences. The goal of our training is to learn a model that can generate responses that are both aesthetically pleasing and preferred by humans. To achieve this, we designed a textual aesthetics preference optimization (TAPO) approach that jointly optimizes for both textual aesthetics and human preferences.\nTo simultaneously utilize all three preference data types in the TEXAES dataset for optimization, we adopt the Plackett-Luce (Luce, 1959; Plackett, 1975) model as the underlying preference model. Rafailov et al. (2024a) showed that \\(\\beta log \\frac{\\pi_{\\theta} (y|x)}{\\pi_{ref}(y|x)}\\) can be treated as \u201cimplicit reward\u201d which is assumed to represent the preference for the model generate y given the prompt x, the goal of DPO is to align the \u201cimplicit reward\u201d towards human preference data directly. We denote each reward function \\(\\beta log \\frac{\\pi_{\\theta}(y_k |x)}{\\pi_{ref}(y_k |x)}\\) as \\(r_{\\theta}(x, y_k)\\) (where k \u2208 {t,w,l}), representing the preferences for the model generating \\(y_t\\), \\(y_w\\), \\(y_l\\) given the input x. \\(\\pi_{\\theta}\\) and \\(\\pi_{ref}\\) are the policy model and reference model respectively and \\(\\beta\\) is a hyper-parameter to control the KL divergence between \\(\\pi_{\\theta}\\) and \\(\\pi_{ref}\\). The training objective of TAPO is\n\\[L_{TAPO}(\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_t, y_w,y_l)\\sim D} \\left[ log \\frac{exp(r_{\\theta}(x, y_t))}{ \\sum_{i\\in{t,w,l}} exp(r_{\\theta}(x, y_i))} - log \\frac{exp(r_{\\theta}(x, y_w))}{ \\sum_{i\\in{w,l}} exp(r_{\\theta}(x, y_i))} \\right] \\],\nwhere D is the dataset, and \\(\\beta\\) is the temperature parameter.\nUsing the properties of logarithmic functions, the loss function can be decomposed into two parts: \\(L_{TA}\\) and \\(L_{DPO}\\):\n\\[L_{TA} =  - log \\frac{exp(r_{\\theta}(x, y_t))}{ \\sum_{i\\in{t,w,l}} exp(r_{\\theta}(x, y_i))}, \\]\n\\[L_{DPO} = - log \\frac{exp(r_{\\theta}(x, y_w))}{ \\sum_{i\\in{w,l}} exp(r_{\\theta}(x, y_i))} .\\]\nIt can be observed that \\(L_{DPO}\\) is identical to the loss used in Bradley-Terry model-based preference optimization with \\(y_w\\) and \\(y_l\\), as demonstrated in the proof provided in Appendix C. On the other hand, \\(L_{TA}\\) represents the log probability of \\(r_{\\theta}(x, y_t)\\) being ranked first among \\(r_{\\theta}(x, y_t)\\), \\(r_{\\theta}(x, y_w)\\), and \\(r_{\\theta}(x, y_l)\\). \\(L_{DPO}\\) primarily optimizes the model's preference for honest, helpful, and truthful data, whereas \\(L_{TA}\\) optimizes both the correctness of the answers and textual aesthetics. To ensure the generated answers are not only accurate but also aesthetically pleasing, we assign different weights to the losses to adjust the preference optimization direction. The modified loss function is as follows:\n\\[L_{TAPO}(\\pi_{\\theta}, \\pi_{ref}) = -E_{(x,y_t,y_w,y_l)\\sim D} [W_{TA} \\cdot L_{TA} + W_{DPO} \\cdot L_{DPO}] .\\]"}, {"title": "DATA AND EXPERIMENT SETTINGS", "content": ""}, {"title": "TEXTUAL AESTHETICS DATASET", "content": "As introduced in Section 3.2, we constructed our textual aesthetic dataset based on a filtered version of UltraFeedback (Cui et al., 2024; Ivison et al., 2023; Tunstall et al., 2023) dataset, which comprises 61,135 completions evaluated by GPT-4, including both accepted and rejected entries. In our experiment, we utilized GPT-4o to perform aesthetic polishing on the UltraFeedback dataset. After the aesthetic polishing process, we found that 5,858 entries were already aesthetically satisfactory and required no further modification. We then analyzed the length of the filtered texts and discovered that a minor subset exhibited excessive verbosity and lacked a natural, human-like quality. To address this, we excluded outliers in the distribution of length differences before and after aesthetic polishing, retaining only data within the 90% confidence interval."}, {"title": "EXPERIMENT SETTINGS", "content": "In this study, we evaluate the performance of models from two perspectives: textual aesthetics and general response capabilities. For textual aesthetics, we compare the models using both text-based and image-based text aesthetic scoring methods, as described in Section 3.3. We report the win rate (WR) in text aesthetics at both the text and image levels relative to the baseline model (GPT-4-0314). In addition to automatic evaluation, we conduct a human evaluation to further validate the models' performance. We randomly sample fifty entries from the Anera-Hard dataset and ask human annotators to rate the aesthetics of these entries.\nTo evaluate the changes in the model's general capabilities following the alignment of textual aesthetics preferences, including its ability to follow instructions and respond to complex prompts across diverse domains, we utilize three well-established auto-evaluation instruction-following benchmarks based on GPT-4-as-a-Judge: AlpacaEval 2.0 (Dubois et al., 2024), Arena-Hard (Li et al., 2024) and MT-Bench (Zheng et al., 2023). For both the supervised tine-tuning and TAPO stages, we employ a low-rank adaptation (Hu et al., 2021) adapter instead of fine-tuning the entire model."}, {"title": "EXPERIMENT RESULTS", "content": ""}, {"title": "MAIN RESULTS", "content": "The comparative analysis of our models trained with TAPO on TEXAES against open-source models is shown in Table 2. Our LLaMA-3.1-70B-TAPO model surpasses all open-source counterparts in both text-based and image-based text aesthetic metrics, with an 18.88% improvement in text-based scores and a 27.85% enhancement in image-based scores over the best-performing LLaMA-3.1-70B-Instruct model.\nFor general response benchmarks, the LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct models, after TAPO training, show improvements on AlpacaEval 2.0 and MT-Bench, though with a slight decline on Arena-Hard. AlpacaEval 2.0 focuses on chat scenarios, MT-Bench on multi-turn conversations, and Arena-Hard on more complex queries. The gains in AlpacaEval 2.0 and MT-Bench suggest that enhanced text aesthetics contribute to better conversational abilities, aligning with our goal of improving answer clarity, layout, uniformity, and coherence. This underscores the quality of TEXAES and the effectiveness of TAPO in boosting both text aesthetics and overall model performance.\nThe results of the human evaluation, shown in Figure 2, show that our LLaMA-3.1-70B-\u0422\u0410\u0420\u041e model is rated significantly higher in text aesthetics than the best-performing open-source model. These results confirm that our model is more visually appealing and coherent, consistent with our quantitative analysis, further validating the efficacy of TAPO in enhancing text aesthetics and overall performance."}, {"title": "IMPACT OF LOSS WEIGHT", "content": "To determine the influence of the weight ratio between \\(L_{TA}\\) and \\(L_{DPO}\\) in TAPO on the aesthetics of the text of the model and the overall performance, we performed a series of methodical experiments. Specifically, we experimented with two settings: 1. First, we used the Tulu-v2 dataset (Ivison et al., 2023) to fine-tune the LLaMA-3.1-8B-base model in a supervised manner, followed by further optimization using TAPO; 2. Second, we directly applied TAPO to the LLaMA-3.1-8B-instruct model. We set the weight ratios of \\(L_{TA}\\) to \\(L_{DPO}\\) at 2:1, 1:1, 1:2 and 1:5, respectively, to train the models. We then evaluated the models' text-based and image-based text aesthetic scores, as well as their performance on Arena-Hard."}, {"title": "Two-STAGE TRAINING", "content": "To validate the efficacy of incorporating three types of preference data in TAPO, we conducted a two-stage DPO training ablation experiment. Initially, human preferences were aligned using the \\(y_w\\) and \\(y_l\\) data sets, denoted as DPO(\\(y_w\\), \\(y_l\\)). Subsequently, text aesthetic preference alignment was conducted using two methods: DPO(\\(y_t\\), \\(y_w\\)) and DPO(\\(y_t\\), \\(y_l\\)). These experiments were performed on the LLaMA-3.1-Base and LLaMA3.1-Instruct models, with results presented in Table 3.\nComparing the final models from the two-stage training with our model trained in TAPO method, we found that, except for the image-based text aesthetic metric, where our model was slightly inferior, it significantly outperformed the two-stage models on text-based aesthetic metrics, AlpacaEval"}, {"title": "TEXAES VS. ULTRAFEEDBACK", "content": "To validate the effectiveness of the TEXAES data set, we performed a comparative analysis of models trained using TEXAES against those trained with UltraFeedback data. We applied the Direct Preference Optimization (DPO) method to align human preferences with the \\(y_w\\) and \\(y_l\\) pairs from UltraFeedback and the \\(y_t\\) and \\(y_l\\) pairs from TEXAES. The experiments were conducted on both the LLaMA-3.1-Base and LLaMA3.1-Instruct models."}, {"title": "ANNOTATION CONSISTENCY", "content": "We generated responses for 50 questions sampled from Arena-Hard using six models: LLaMA-3.1-8B-TAPO, LLaMA-3.1-70B-TAPO, LLAMA-3.1-8B-Instruct (Dubey et al., 2024), LLaMA-3.1-70B-Instruct (Dubey et al., 2024), Qwen2-72B-Instruct (qwe, 2024), and Tulu-2-dpo-70B (Ivison et al., 2023). Subsequently, we employed three types of evaluators: text-based GPT-40 judge (TA Text), image-based GPT-40 judge (TA Image), and three human annotators, consisting of two graduate students and one professor. Each evaluator was tasked with comparing LLaMA-3.1-8B-TAPO and LLaMA-3.1-70B-TAPO against other models in terms of the textual aesthetics of the generated answers (win/tie/lose), resulting in 400 annotated comparison pairs."}, {"title": "CRITERIA FOR REJECT SAMPLE SELECTION", "content": "To effectively optimize textual aesthetics using preference optimization, it is essential to construct preference pairs consisting of chosen and rejected responses. For our purposes, we select \\(y_t\\) from TEXAES as the chosen response. As the rejected response, we use either the original chosen response \\(y_w\\) or the original rejected response \\(y_l\\) from the UltraFeedback dataset. We conducted DPO experiments to compare the impact of \\(y_w\\) and \\(y_l\\) on the model's performance. The results are presented in Table 6."}, {"title": "CASE STUDY", "content": "In this section, we compare LLaMA-3.1-8B-Instruct and LLaMA-3.1-8B-TAPO from three cases in Arena-Hard, as shown in Figure 4."}, {"title": "CONCLUSION", "content": "In this paper, we conducted the first exploration of textual aesthetics in LLMs and introduced a series of techniques to enhance the aesthetic quality of LLMs outputs. First, we developed the TEXAES dataset, the first textual aesthetic dataset in the LLMs domain, using our specially-designed data polishing pipeline. Based on this dataset, we proposed the TAPO, which fine-tunes LLMs to improve the aesthetic quality of their outputs while preserving their core capabilities. Both qualitative and quantitative experiments validated the effectiveness of our proposed techniques. We hope our work serves as an early exploration for the textual aesthetics in LLMs and provides valuable support for researchers in the open-source community. In future work, we will continue to explore ways to collect diverse and high-quality textual aesthetics data, while designing more efficient and effective tuning techniques for aesthetic fine-tuning."}, {"title": "DATASET STATISTICS", "content": "Figure 5 illustrates the difference in token length between the text that has undergone aesthetic polishing and the original text. The mean length difference is 49 tokens, with the 25th and 75th percentile values being -7 and 54, respectively. The maximum length difference is 2673 tokens, while the minimum length difference is -1024 tokens.\nIn Figure 6, we plot the length distribution of our TEXAES. The mean length is 293 tokens, with the 25th and 75th percentile values being 97 and 444 respectively, and the maximum length being 1408 tokens. Figure 6 shows the length distribution of UltraFeedback (Cui et al., 2024). The mean length is 297 tokens, with the 25th and 75th percentile values being 77 and 464 respectively, and the maximum length being 2700 tokens."}, {"title": "TRAINING PARAMETERS", "content": "We present the details of the experimental settings in Table 7 and Table 8. For the sake of fairness in comparison, we used the same training parameters as those employed by DPO during the preference optimization stage. Our experiments are based on Llama-Factory (Zheng et al., 2024)"}, {"title": "MATHEMATICAL DERIVATIONS", "content": "In this section, we prove that \\(L_{DPO}\\) from Eq. 3 is equivalent to Eq. 1. To begin, consider Eq.3:\n\\[L_{DPO} = - log \\frac{exp(r_{\\theta}(x, y_w))}{ \\sum_{i\\in{w,l}} exp(r_{\\theta}(x, y_i))}\\]\n\\[= - log \\frac{exp(r_{\\theta}(x, y_w))}{\\frac{exp(r_{\\theta}(x, y_w))}{exp(r_{\\theta}(x, y_w))} \\sum_{i\\in{w,l}} exp(r_{\\theta}(x, y_i))}\\]\n\\[= - log \\frac{1}{\\frac{\\sum_{i\\in{w,l}} exp(r_{\\theta}(x, y_i))}{exp(r_{\\theta}(x, y_w))}}\\]\n\\[= - log \\frac{1}{1 + exp(r_{\\theta}(x, y_l) - r_{\\theta}(x, y_w))}\\]\n\\[= - log \\sigma (r_{\\theta} (x, y_w) - r_{\\theta}(x, y_l)).\\]\nHere, \\(\\sigma\\) denotes the sigmoid function. In Section 4.2, we presented the specific expressions for \\(r_{\\theta}(x, y_w)\\) and \\(r_{\\theta}(x,y_l)\\):\n\\[r_{\\theta}(x, y_w) = \\beta log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)},\\quad\\quad r_{\\theta}(x, y_l) = \\beta log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref}(y_l | x)}\\]\nBy substituting Eq. 6 into the Eq. 5, we obtain:\n\\[L_{DPO} = - log \\sigma \\left[ \\beta log \\frac{\\pi_{\\theta}(y_w | x)}{\\pi_{ref}(y_w | x)} - \\beta log \\frac{\\pi_{\\theta}(y_l | x)}{\\pi_{ref}(y_l | x)} \\right]\\]\nThis shows that \\(L_{DPO}\\) as defined in Eq. 3 is indeed equivalent to Eq. 1, thus completing the proof."}, {"title": "LENGTH CONSTRAINT IN TEXAES DATASET", "content": "To verify whether filtering out outliers in the distribution of length differences before and after aesthetic polishing can improve the quality of TEXAES during its construction phase, we conducted an ablation experiment on data without length filtering. Specifically, the model was trained based on LLaMA-3.1-8B-Base using DPO(\\(y_t\\), \\(y_l\\)), with the outcomes delineated in Table 9. The findings demonstrate that the performance of the model, after removing data points with excessive length deviations, significantly exceeds that of the model trained without such length filtering across all evaluation tasks. Furthermore, a statistical analysis of the output lengths generated by the model revealed that the outputs produced by the model trained with length-filtered data were not only shorter but also more concise, thereby affirming the efficacy of length filtering in text aesthetic optimization."}, {"title": "PROMPT", "content": ""}, {"title": "AESTHETICS POLISHING PROMPT", "content": "System Instruction\nYou are tasked with acting as a text rewriter to enhance the readability and comprehension of text generated by a Large Language Model (LLM). Your goal is to ensure the text is easy to read, easy to understand, and visually"}]}