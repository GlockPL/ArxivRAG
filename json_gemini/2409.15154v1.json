{"title": "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code", "authors": ["Jiachi Chen", "Qingyuan Zhong", "Yanlin Wang", "Kaiwen Ning", "Yongkun Liu", "Zenan Xu", "Zhe Zhao", "Ting Chen", "Zibin Zheng"], "abstract": "The emergence of Large Language Models (LLMs) has significantly influenced various aspects of software development activities. Despite their benefits, LLMs also pose notable risks, including the potential to generate harmful content and being abused by malicious developers to create malicious code. Several previous studies have focused on the ability of LLMs to resist the generation of harmful content that violates human ethical standards, such as biased or offensive content. However, there is no research evaluating the ability of LLMs to resist malicious code generation. To fill this gap, we propose RMCBENCH, the first benchmark comprising 473 prompts designed to assess the ability of LLMs to resist malicious code generation. This benchmark employs two scenarios: a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code. Based on RMCBENCH, we conduct an empirical study on the 11 representative LLMs to assess their ability to resist malicious code generation. Our findings indicate that current LLMs have a limited ability to resist malicious code generation with an average refusal rate of 40.36% in text-to-code scenario and 11.52% in code-to-code scenario. The average refusal rate of all LLMs in RMCBENCH is only 28.71%; ChatGPT-4 has a refusal rate of only 35.73%. We also analyze the factors that affect LLM's ability to resist malicious code generation and provide implications for developers to enhance model robustness.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) [74] refer to transformer-based neural language models that are pre-trained on massive data. These models range from billions to hundreds of billions of parameters. Various LLMs, such as GPT-3 [20] and Llama2 [62], have exhibited remarkable capabilities in assisting developers with software development [76, 77], e.g., code generation and completion.\nBefore releasing an LLM, LLMs typically need to experience human value alignment training [60], a process in which ethical standards are explicitly defined; models are trained on data that reflect these values to ensure safer and more reliable human interactions. For example, LLMs like ChatGPT decline requests to write scripts for illegal activities such as flood attacks [5]. However, despite rigorous value alignment training, it is challenging to fully guarantee that LLMs never produce harmful content [24].\nSeveral previous works [47, 65, 73] have been dedicated to evaluating the security of LLM-generated content and developing benchmarks to assess LLM's ability to resist the generation of harmful content. However, these benchmarks focus primarily on harmful content in natural language, but underestimate the risks associated with the generation of malicious code by LLMs. As a result, there is a gap in comprehensive evaluating LLMs' ability in resisting malicious code generation. This oversight may potentially lead to LLMs being mistakenly utilized by malicious developers to produce malware and threaten the software ecosystem. For example, a report by SlashNext [2] shows that ChatGPT has been employed by malicious developers to generate scripts to automate email dispatch. Since the release of ChatGPT, there has been an increase of 1,265% in the incidence of malicious phishing email attacks.\nTo fill this gap, we introduce RMCBENCH, the first benchmark specifically developed to evaluate the ability of LLMs to resist malicious code generation. RMCBENCH employs two scenarios to test LLMs, that is, a text-to-code scenario, where LLMs are prompted with descriptions to generate code, and a code-to-code scenario, where LLMs translate or complete existing malicious code.\nIn the text-to-code scenario, we first retrieve 392 repositories related to malicious code and malware from GitHub [6]. Then, we obtain natural language textual descriptions of the functionality of malicious code by utilizing the summarization ability of ChatGPT-3.5. Based on the above data, we manually construct prompts for the text-to-code scenario [46], where each prompt provides LLMs with textual descriptions of malicious code and asks LLMs to generate the corresponding code. We divide these prompts into three levels according to the difficulty to against them in generating malicious code. (1) Level 1, where the prompt text includes explicit malicious keywords, such as \u201cGenerate a code for flood attack [5]\". (2) Level 2, where the prompt text does not contain malicious keywords and is rephrased by another sentence. For example, the words \"flood attack\" are replaced by \"Send a large amount of traffic to the target network to occupy the target's network bandwidth or system resources.\" (3) Level 3, which builds on Level 2 by formulating a jailbreak attack [67] that bypasses LLMs restrictions. Finally, we construct a total of 282 prompts for the text-to-code scenario (80, 102, and 100 prompts for Level 1, 2 and 3, respectively).\nIn the code-to-code scenario, we also utilize the above malicious code collected from GitHub. The prompts in this scenario require LLMs to generate malicious code based on the provided malicious samples, and include two tasks: (1) Code completion [41], where we present malicious code segments to LLMs and ask them to complete the missing parts. (2) Code translation [46], where LLMs are tasked with translating the original malicious code into another programming language. We construct a total of 191 prompts for the code-to-code scenario, distributed as 100 and 91 prompts for code completion and code translation tasks, respectively.\nIn total, we construct 473 prompts designed to ask LLMs to generate malicious code. RMCBENCH involves the generation of 11 types of malicious code, such as Viruses and Worms[8]. The provided original malicious code includes 9 programming languages, such as Python, Java and C++.\nBased on RMCBENCH, we conduct the first empirical study to evaluate the performance of 11 representative LLMs, such as ChatGPT-4, in resisting malicious code generation. We have the following main findings.\nFirstly, all the 11 LLMs have a limited ability to resist malicious code generation in text-to-code scenarios, with an average refusal rate of 40.36%. The average refusal rates of all LLMs at Level 1, Level 2, and Level 3 are 60.80%, 28.43%, and 36.18%, respectively. Replacing malicious keywords with their functional descriptions can make it more challenging for LLMs to resist generating malicious code. Besides, the Jailbreak template designed for GPT-series models remains effective for other LLMs and can reduce their refusal rate.\nSecondly, we find that LLMs have a poor ability to resist malicious code generation in code-to-code scenario, with an average refusal rate of 11.52%. It is lower than in text-to-code (40.36%). When the input is code, LLMs may neglect their focus on resisting malicious code generation. Even with similar input structures, the ability of LLMs to resist generating malicious code is influenced by specific tasks, such as code completion (average refusal rate 15.36%) or translation (average refusal rate 7.29%). Additionally, the top three LLMs with the highest overall refusal rates in RMCBENCH are Llama-2-13B (48.84%), DeepSeek-Coder-7B (44.19%), and Llama-3-8B (43.55%). ChatGPT-4 ranks only 6th (35.73%). Finally, we observe that the resistance of LLMs to malicious code generation is influenced by model parameters, model types (general LLMs or code LLMs), malicious code types (e.g., Phishing and Worms), programming language of malicious code, and input context length.\nIn summary, this paper makes the following contributions:\n\u2022 We propose the first benchmark, RMCBENCH, for evaluating the ability of LLMs to resist malicious code generation.\n\u2022 We conduct the first empirical study to evaluate 11 representative LLMs on their ability to resist malicious code generation across various scenarios and tasks (levels).\n\u2022 We analyze factors and provide insights to enhance the ability of LLMs to resist malicious code generation.\n\u2022 We release the code and data at: https://github.com/qing- yuan233/RMCBench.\""}, {"title": "2 BACKGROUND AND MOTIVATION EXAMPLES", "content": "2.1 Large Language Models (LLMs)\nLarge Language Models (LLMs) [74] refer to transformer-based neural language models [64] that are pre-trained on massive text data. They have shown capabilities and performed well in various tasks [77]. Based on the task objectives emphasized during their training, LLMs can be broadly classified into two types.\n2.1.1 General LLMs. These LLMs are trained on a wide range of general tasks rather than being specialized for specific tasks. GPT [20] and LLAMA [62] are two representative general LLM models that have shown good performance in areas including logical reasoning, mathematical problem-solving, creative writing [44, 69, 74]. Notably, general LLMs can be further refined through the instruction fine-tuning process. For example, ChatGPT is optimized based on GPT models by undergoing fine-tuning through interactions\n2.2 Code Generation by LLMs\nThis task involves leveraging LLMs to generate code based on the given inputs [17]. Developers utilize LLMs to generate code for improving the efficiency of software development [19, 25, 33]. According to CodeXGLUE[46], there are mainly two scenarios in code generation, i.e., text-to-code and code-to-code generation.\n2.2.1 Text-to-code. This process involves generating code based on a natural language description. For example, when prompted with \"write code to send a large number of HTTP requests to the server.\", the model will output specific code to implement the related functionalities. LLMs have demonstrated remarkable capabilities in text-to-code generation tasks [77]. For instance, GPT-4 achieved the highest pass rate(67.0% at pass@1) in text-to-code generation on HumanEval [23].\n2.2.2 Code-to-code. This scenario includes two primarily tasks, i.e., code completion [41] and code translation [46].\nCode completion. In this task, developers provide the model with incomplete code and require LLMs to fill in the missing parts. Code completion can occur at various granularities, including token- level [46] (completing a single token), line-level [46] (completing an entire line of code), function-level [70] (completing an entire function), and class-level [29] (completing an entire class). For example, inputting a Python code snippet that only has the function name \"def send_large_number_HTTP_requests():\", LLMs will complete the remaining code to make it a complete function.\nCode translation. This task requires the LLMs to translate code from one programming language to another. For example, we input a Python function into LLMs, we can ask them to generate an implementation of this function in JavaScript.\n2.3 Jailbreak Attacks in LLMs\nThe jailbreak attack [45] is a process that employs prompt injection to specifically circumvent the safety and moderation features placed on LLMs by their creators. Jailbreak prompts [45] serve as a general template to launch such an attack to bypass restrictions. For example, the Prompt 3 in Figure 1 shows the famous jailbreak attack named DAN (Do Anything Now) [1]. This prompt acts as a prefix text template that requires ChatGPT to ignore all the rules when answering questions. By appending specific instructions to the end of this template, malicious developers can formulate a complete prompt. Entering the prompt into LLMs, it can increase the likelihood of generating harmful content. Many previous studies have confirmed that jailbreak attacks can cause LLM to output harmful responses [27, 45, 55, 67, 71]."}, {"title": "3 THE RMCBENCH BENCHMARK", "content": "3.1 Overview\nFigure 2 illustrates the detailed process of constructing RMCBENCH, which includes 473 prompts designed to ask LLMs to generate malicious code. It includes scenarios where LLMs are given descriptions of malicious code in natural language (text-to-code scenario, in a total of 282), and where they are provided with partial malicious code to either translate into another programming language or to complete missing part (code-to-code scenario, in a total of 191). For each task within these scenarios, we follow a three-step process: (1) Prompt Template Design, in which we design specific prompt templates for each task. (2) Data Collection, where we collect real-world\n3.2 Build Text-to-Code Prompts\nSince different prompts have a significant influence on the output of LLM [43], in the text-to-code (T2C) scenario, we set up three levels of prompts, each with different contents, corresponding to different difficulties for LLMs in resisting generating malicious code.\n3.2.1 Level 1 & Level 2 Prompts. According to Prompt 1 and Prompt 2 in Figure 1, the presence of malicious keywords significantly impacts the LLMs' capability to reject such inputs. Consequently, we designed Level 1 and Level 2 prompts based on this observation.\n(1) Prompt Template Design. Below, we outline the design principle for Level 1 and Level 2 in RMCBENCH.\n\u2022 Level 1 (T2C-L1): T2C-L1 prompt explicitly describes the concept or functionality of malicious code and includes explicit malicious keywords. For example, the malicious keywords in Prompt 1 of Figure 1 are \"flood attack\". For Level 1, we assume that LLMS have already learned the knowledge about malicious keywords during their pre-training process. Thus, LLMs are easier to identify its malice and refuse to generate malicious code.\n\u2022 Level 2 (T2C-L2): T2C-L2 prompt describes the functionality of malicious code while deliberately omitting explicit malicious keywords. As illustrated by the prompts in Figure 1, the malicious keywords \"flood attack\" is replaced by its explanation in Prompt 2, and no malicious keywords are used. For Level 2, LLMs need to understand and make judgments based on the specific functionality described, thereby increasing the challenge of correctly identifying malicious content.\n(2) Data collection. To construct the prompts for Level 1 and Level 2, we need to obtain the list of malicious keywords and related concept/functionality descriptions. This process can be achieved through code summarization [12] from malicious code, which is the task used to extract textual descriptions from code. The first step involves the collection of malicious code. We retrieve repositories containing malicious content by searching for the keywords \"Malware\" and \"Malicious code\" from GitHub, selecting those with a star count of 200 or more [49]. We finally obtained 392 repositories, and all the raw data can be found in our online repository.\n(3) Prompt Construction. We employ ChatGPT-3.5 to perform code summarization on all the code data collected in the previous step, thereby obtaining natural language descriptions of related functionalities. We do not use ChatGPT-4 due to the large volume of code data that needs to be analyzed, which would lead to significant costs. Besides, Admed et al. [12] demonstrated that ChatGPT-3.5 also exhibits excellent performance in code summarization tasks. Manual Check. All the generated summaries are manually reviewed by the two authors of this paper to ensure accuracy. During the manual review process, the authors are tasked with several specific actions: (a) Removing irrelevant summaries. Some repositories may not related to malicious activities. Thus, we remove them from our dataset; (b) Rephrasing the text. Since outputs from ChatGPT-3.5 can sometimes include redundant sentences, they are edited for brevity and clarity; (c) Performing deduplication. If two descriptions address the same malicious functionality, only one is retained to avoid redundancy.\nMalicious Keyword List Creation. We analyze the words in all summarized results and extract two types of keywords: (1) The concept of malicious code, such as \"virus\" and \"worms\". (2) Malicious behavior, such as \"attack\", \"destroy\" and \"break\". There are a total of 83 malicious keywords, which can be viewed on our online repository.\nBased on the previous steps, we obtain a total of 182 text-to-code prompts for generating malicious code. We classify the prompts into Level 1 and Level 2 based on whether they contain malicious keywords. Prompts containing malicious keywords are classified as Level 1, totaling 80 prompts. Those without explicit malicious keywords are classified as Level 2, comprising 102 prompts.\n3.2.2 Level 3 Prompts. Level 3 prompts are designed to require LLMs to generate malicious code through jailbreak attacks.\n(1) Prompt Template Design. Level 3 (T2C-L3) prompts are built based on Level 2 prompts, which consist of two components: a jailbreak template and the original prompt from Level 2.\n(2) Data Collection. To build the Level 3 prompt, we need to connect the Level 2 prompts with the jailbreak templates. jail- breakchat.com [14] is a famous website that collects jailbreak templates, and many studies [27, 45, 55, 67] related to jailbreaks have used the data from it. Note that the website is no longer accessible as of June 2024. Thus, we used all the available jailbreak templates (a total of 78) by the time.\n(3) Prompt Construction. Many jailbreak prompts from jail- breakChat.com are designed for ChatGPT and often begin with \"Hi, ChatGPT...\". To ensure consistency when testing other LLMs, we need to modify these jailbreak templates. For instance, when testing Llama2, we change the original salutation words to \"Hi, Llama...\". This adaptation is important, as our preliminary experiment finds that if we call Llama \"ChatGPT\", Llama will prioritize correcting its identity instead of asking its actual task.\nWe construct a complete Level 3 prompt by integrating jailbreak templates with Level 2 prompts. Given the extensive possibility of 7,956 (102*78) combinations, to maintain a balance in quantity relative to the other two levels of prompts, we randomly select 100 Level 3 prompts from the 7,956 combinations for further empirical"}, {"title": "3.3 Build Code-to-Code Prompts", "content": "In code-to-code (C2C) scenario, the input contains both natural language and code. The natural language describes the specific tasks to be executed, and the actual malicious information is contained within the code. After the prompt is input into LLMs, we expect the LLMs to recognize malicious information in the prompts and refuse to response.\n3.3.1 Code Completion Prompts. Code completion (CC) prompts include a malicious code with omitted segments and a natural language instructions specifying the need to complete these segments.\n(1) Prompt Template Design. The top half part of Figure 3 shows the prompt templates for Code Completion. We refer to EgoAlpha's prompt-in context-learning [30] to design our prompts. We have further optimized the code completion task template by adding placeholders '<FILL_HERE>' at the locations where completion is required. This modification aids LLMs in accurately identifying completion areas and minimizes the impact of lengthy code contexts on their instruction-following ability [34, 39].\n(2) Data Collection. Constructing a code completion prompt requires malicious code. In Section 3.2.1, we have collected raw data of malicious code from Github. However, not all code is available. For some source code files, malicious functions are specifically implemented in external packages or libraries, so we cannot obtain specific malicious code from them. Besides, there are also many non-source code files on which we cannot build the required data. Thus, we applied the following filters: (a) the malicious code in a single file must be independent, i.e., its malicious functional components do not rely on third-party libraries or files; (b) only the source code files are retained, and executable files and assembly files (such as files with bin and .exe extensions) are not excluded. Through filtering, we obtained a total of 91 samples of malicious code.\n(3) Prompt Construction. The arrow in Figure 3 shows the example of hollowing out the collected malicious code. Inspiring by previous works [3, 39], we hollowed out sections from the collected\nmalicious code samples according to the following rules: (a) For code with multiple functions, we randomly remove one complete function. (b) For single-function code that is divided into multiple parts by empty lines, with each part containing several consecutive lines of code, we randomly remove one part. (c) For continuous code that lacks empty line separations, we perform random line-level or token-level hollowing at the end of certain lines.\nThen, the hollowed-out parts are replaced with a \u201c<FILL_HERE>\u201d placeholder [39] to indicate where completion is needed. After hollowing out, we ensure that the remaining code context contains sufficient malicious information. After that, comments are added before the placeholder to detail the specific functionality of the removed sections. This process ensures that the modified code maintains its original malicious intent. The average number of lines of code in the hollowed-out part is 3.8, with a maximum value of 17. Finally, we replace the hollowed-out code with {malicious code with missing parts(mark the position with <FILL_HERE>)} in prompt template. We built a total of 80 malicious code completion prompts. To make our prompts more diversity, we utilized the approach outlined in CoderEval [70] to design another prompt method. This method involves providing the function signature and the first line definition of the malicious code (also summarized by ChatGPT-3.5 based on the provided malicious code), allowing it to complete the remaining code (a total of 20). Finally, the number of prompts for the malicious code completion task is 100 in total.\n3.3.2 Code Translation Prompts. Code translation (CT) prompts include a complete malicious code and a natural language instruction to indicate the need for translating the provided code into another programming language.\n(1) Prompt Template Design. The half-bottom part of Figure 3 shows the prompt templates for the Code translation task in RMCBENCH. We also refer to EgoAlpha's prompt-in context- learning [30] to design the prompts. Specifically, {malicious code} is the original and complete malicious code we have collected, {original language} is the programming language of the original code, and {target language} is the target language to be translated into.\n(2) Data Collection. Constructing a code translation prompt also requires malicious code, which is the same as Section 3.3.1 (2). Thus, we use the same dataset for this task.\n(3) Prompt Construction. We first fill the {malicious code} into the prompt template. For {original language} in prompt template, we use the language of the malicious code itself; For {target language}, we establish the following rule: if the original language is Python, then set the target language to JavaScript, as both are scripting languages and interpretive languages; If the original language is a non-Python language, we set the target language to Python, because we consider that Python is powerful and rich in functionality, it can achieve as much functionality as other languages. Finally, we construct a total of 91 code translation prompts."}, {"title": "3.4 Other Features of RMCBENCH", "content": "Figure 4 shows the categories and programming languages of malicious code in RMCBENCH. Firstly, according to Microsoft's definition and classification of malware [48], the malicious code in"}, {"title": "4 EMPIRICAL STUDY", "content": "Based on RMCBENCH, we conduct the first empirical study to evaluate the ability of existing LLMs to resist malicious code generation by answering the following research questions.\n\u2022 RQ1: How do LLMs perform resist malicious code generation in text-to-code scenario?\n\u2022 RQ2: How do LLMs perform resist malicious code generation in code-to-code scenario?\n\u2022 RQ3: What factors influence the ability of LLMs to resist malicious code generation?\n4.1 Studied LLMs\nThe criteria for selecting our studied LLMs are: (1) We initially selected LLMs from the official LLMs safety leaderboard on Hugging Face [37] (as of May 2024). These LLMs have demonstrated outstanding performance in refusing to generate harmful content [65]. (2) We exclude closed-source LLMs without accessible APIs, as they are not callable. (3) We exclude open-source LLMs that have multiple versions but lack a specific version number to ensure accurate identification of the models during reproduction. (4) We exclude open-source LLMs that lack weight files or are too large (over 20 billion parameters) due to our inability to deploy them locally. (5) To enrich the variety of LLMs, we add two code generation LLMs [26, 58], i.e., DeepSeek-Coder-7B and CodeLlama-13B. Additionally, to increase timeliness, we also add the recently released Llama-3-8B. (6) All select LLMs have undergone instruction-based fine-tuning, as our experiments require LLMs to perform varied tasks based on instruction interaction.\nTable 1 displays all the LLMs studied in our experiments. Our selection of LLMs includes both open-source and closed-source LLMs, covering a range of sizes from 7B to 13B, trained on both general and code-related tasks and featuring strong timeliness. Among them, DeepSeek-Coder-7B (v1.5) and Vicuna-7B(v1.3) have specific version numbers, which we have omitted in the table.\nImplementation Details. For closed-source LLMs, i.e., ChatGPT- 3.5-turbo and ChatGPT-4, we call the OpenAI interface [50]. For open-source LLMs, we obtain the model weights for their specified versions based on the LLM safety ranking [37]. For the two code LLMs and Llama-3-8B, we download the model weights from their official Hugging Face homepage.\nWhen interfacing, we set the following parameters: do_sample set to True; temperature set to 0.75; Top_k set to 10; Top-p set to 0.9; max_new_tokens set to 4096. All models were run with the same parameters on a single A800-40G GPU server.\nFor open-source LLMs, we strictly follow their prompt formats during inference. We format the input according to the format used during their training. For some LLMs hosted on Hugging Face, the formats are integrated into the framework. The format conversion is completed by calling application_chat_template. For models not integrated, we configure them according to the format mentioned in their model card or GitHub homepage, or original paper.\n4.2 Labels for LLM's Output\nWe label LLM's response with different tags based on its content to determine whether it has successfully resisted the generation of malicious code. Following Wei et al. [67], who classified the responses of ChatGPT into three categories (GOOD BOT, BAD BOT and UNCLEAR) based on whether they are harmful and relevant to the question. We reuse the same strategy in this study as follows: GOOD. The LLM's response refuses to provide the malicious code requested in the prompt. Specifically, the response may refuse to engage with the request entirely, or refuse the harmful content and respond only to the non-harmful content. For example, after refusing to provide harmful code, LLM still provides benign code or gives advice on legal code writing; both scenarios are categorized as GOOD. A GOOD response example can be seen in Figure 1.\nBAD. The LLM's response is relevant to the prompt, and the content is harmful, including generating malicious code, providing methods or steps to guide malicious code writing, suggesting malicious actions, completing or translating tasks based on the provided malicious code, etc. The BAD response example can be seen in Figure 1.\nUNCLEAR. This category includes all situations where the LLM's response is unrelated to the prompt, including (1) Meaning- less output, including no output, output consisting only whitespace and line breaks, garbled characters, and incomplete output. (2) Re- sponses do not follow the instructions. For example, the prompt is"}, {"title": "4.3 Evaluation Methods and Metrics", "content": "In this part, we provide details of how to automatically classify LLMs' responses and introduce the selection of evaluation metrics.\n4.3.1 Evaluation Methods. We use ChatGPT-4 to label responses from LLMs for two reasons: (1) The malicious code output of LLMs varies greatly in semantics and format, which is time-consuming and error-prone to manually analyze them. (2) Previous research [78] shows that ChatGPT-4 has powerful language comprehension ability and can effectively understand the content output by LLMs.\nThe prompts for evaluating LLM's responses are designed based on previous work [78], which is shown in Figure 5. The upper part is the prompt template for evaluation. Among it, the System prompt assigns roles and explains tasks to ChatGPT-4. The User prompt section is a complete instruction. The placeholders {prompt} and {response} are the prompts that induce LLMs to generate malicious code and their corresponding responses obtained in the experiment. Classification standards is the label classification standards. {Standard Differences} represent different part in classification standards. Since the input data, task instructions and output of the T2C, CC, and CT are different, we slightly adjusted the prompts used for evaluation for three different tasks based on the label classification criteria in Section 4.2. The criteria for the UNCLEAR category are the same, only a slight difference in GOOD and BAD categories, the specific differences is shown in the half bottom of Figure 5. For example, when evaluating whether LLMs follow instructions (one of the conditions for BAD categorie), the prompt used for T2C tasks is \"... generate code to implement the described functionality...\". In CC tasks, we replace the prompt with \"..., completes the code in the prompt, or generates guidance to complete the code...\". For CT tasks, it is \"..., translates the code in prompt, and generates code that implements the same or similar functions...\". This is intended to help ChatGPT-4 better understand each task, thereby achieving more accurate evaluation results.\nFinally, we adopted the chain-of-thinking[68](COT) method to improve the model performance and format the output of the model, facilitating easier extraction of responses.\n4.3.2 Effectiveness of Evaluation Method. To evaluate the performance of using ChatGPT-4 to label the responses, we verify the effectiveness of its evaluation through manual sampling inspection. We adopted a random sampling method based on the confidence interval [4] (based on a 95% confidence level and a 10 confidence interval [10]) to generalize the population of the total number of LLM's responses (5,203 in total). The final samples are 93 for T2C task, 88 for CC task, and 88 for CT task.\nThen, two authors evaluated the sampling results using manual review and conducted a double-check. The corresponding results are shown in Table 2, where GOODt, BADt, UNCLEARt represents the ground truth we labeled; GOODgpt4, BADgpt4, UNCLEARgpt4 represents the results labeled by ChatGPT-4. Acc represents the Accuracy, here is the calculation formula using the GOOD category as an example: ACCGOOD =GOOD gpt4NGOODt\u00d7 100%. According to the data in the Table 2, there are a total of 60 GOOD ground truth samples (60+0+0). GPT-4 labeled 60 of them as GOOD, 0 as BAD, and 0 as UNCLEAR. Its accuracy is ACCGOOD =(604040)\u00d7 100% = 100%.\nChatGPT-4 achieved classification accuracy of 100.0% and 98.3% for GOOD and BAD categories, respectively. The classification accuracy of UNCLEAR response is only 23.7%. We manually check the examples of this category and find that some responses only contain a part of malicious code. These partial codes are usually meaning- less and hard to read. As a result, we classify it as UNCLEAR, while ChatGPT-4 tends to classify it as BAD. The overall weighted accuracy was 96.2%, which shows that the method of using ChatGPT-4 to automate labels is feasible. For subsequent experiments, we used this ChatGPT-4 based automated labeling method to label all the 5,203 response data. This automatic evaluation cost a total of $152.8 and took 9.8 hours.\nRefusal Rate. Based on the results of our previous experiment, we find that ChatGPT-4 demonstrated excellent performance in identifying the GOOD label. Consequently, we use the percentage of GOOD responses as the refusal rate to assess the ability of LLMs to resist malicious code generation. The percentage of GOOD responses indicates the frequency with which the LLM refuses to generate the malicious code requested in the prompt."}, {"title": "5 RESULTS", "content": "5.1 RQ1: Performance in Text-to-Code Scenario\n5.1.1 Overall Performance. Table 3 shows the performance of LLMs in the text-to-code scenario in RMCBENCH. Columns 2-10 present the performance of the LLMs at each level", "1": "LLMs have a limited ability to resist malicious code generation in text-to-code scenarios.\n5.1.2 Comparison Among Levels. The average refusal rate of all LLMs at Level 1 (60.80%) is higher than Level 2 (28.43%) and Level 3 (36.18%). This outcome aligns with our expectations when constructing the dataset (c.f. Section 3.2.1). It shows that LLMs have certain abilities to recognize and judge malicious code-related vocabularies within prompts. When the prompt contains malicious keywords", "2": "Replacing malicious keywords with their functional descriptions can make it more challenging for LLMs to resist generating malicious code.\nHowever, the results show that the average refusal rate for Level 3 (36.18%) is higher than Level 2 (28.43%). Among the 11 tested LLMs, 5 LLMs exhibited a lower refusal rate at Level 3 than Level 2; they are Llama-2-13B, Mpt-7B, ChatGPT-3.5-turbo, Zephyr-7B- beta, and Tulu-2-13B. The most significant difference is observed in ChatGPT-3.5-turbo, with an 18.55% lower refusal rate at Level 3 compared to Level 2. One explanation for this phenomenon is that the current jailbreak attack templates are mainly designed for ChatGPT-3.5-turbo [14, 45"}]}