{"title": "Web2Code: A Large-scale Webpage-to-Code Dataset\nand Evaluation Framework for Multimodal LLMs", "authors": ["Sukmin Yun", "Haokun Lin", "Rusiru Thushara", "Mohammad Qazim Bhat", "Yongxin Wang", "Zutao Jiang", "Mingkai Deng", "Jinhong Wang", "Tianhua Tao", "Junbo Li", "Haonan Li", "Preslav Nakov", "Timothy Baldwin", "Zhengzhong Liu", "Eric P. Xing", "Xiaodan Liang", "Zhiqiang Shen"], "abstract": "Multimodal large language models (MLLMs) have shown impressive success\nacross modalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing webpage-\nto-code datasets as well as generate a diverse pool of new webpages rendered\ninto images. Specifically, the inputs are webpage images and instructions, while\nthe responses are the webpage's HTML code. We further include diverse natural\nlanguage QA pairs about the webpage content in the responses to enable a more\ncomprehensive understanding of the web content. To evaluate model performance\nin these tasks, we develop an evaluation framework for testing MLLMs' abilities in\nwebpage understanding and web-to-code generation. Extensive experiments show\nthat our proposed dataset is beneficial not only to our proposed tasks but also in the\ngeneral visual domain, while previous datasets result in worse performance. We\nhope our work will contribute to the development of general MLLMs suitable for\nweb-based content generation and task automation. Our data and code are available\nat https://mbzuai-llm.github.io/webpage2code/.", "sections": [{"title": "Introduction", "content": "Multimodal large language models (MLLMs) have achieved explosive growth in the past few\nyears. Leveraging the rich commonsense knowledge in large language models (LLMs), MLLMs are\nremarkably successful at processing and reasoning about various modalities such as image [2, 35],\nvideo [60, 53], and audio [40] in a broad range of tasks such as recognition [52], reasoning [59], and\nquestion-answering [39], all using language as the intermediate representation. However, existing\nMLLMs are surprisingly poor at understanding webpage screenshots and generating the HTML code\nto express their latent states. For instance, given the instruction \"Parse the HTML code for this\nwebpage\", the well-known LLaVA-1.5 [33] generates generic, pale code that fails to preserve most of\nthe original webpage's features which hampers its utility in applications such as UI\nprototyping, automation agents, and accessibility (e.g., noting available buttons and options given\nwebpage screenshot)."}, {"title": "Related Work", "content": "MLLM Dataset. At present, there is a substantial amount of large-scale visual instruction data,\nprimarily generated using GPT. SVIT [63] and LRV-Instruction [32] are both generated by GPT4"}, {"title": "Dataset Construction", "content": "Overview. Our Web2Code instruction tuning dataset construction and instruction generation process\ninvolves four key components: (1) Creation of new webpage image-code pair data: We generated\nhigh-quality HTML webpage-code pairs following the CodeAlpaca prompt [6] using GPT-3.5 and\nconvert them into instruction-following data. (2) Refinement of existing webpage code generation\ndata: We transform existing datasets including WebSight [22] and Pix2Code [4] into an instruction-\nfollowing data format similar to LLaVA data [33], so they can be used as instruction-following data\nto train MLLMs. (3) Creation of a new text question-answer pair data: We generated a new question-\nanswer pair dataset utilizing our new GPT-3.5 generated data from (1) for webpage understanding.\n(4) Refinement of existing webpage understanding data: We refine the WebSRC [10] question-answer\ndata to improve its quality using the GPT-4. Each component is elaborated in detail as follows:\nDWCG: Creation of new webpage image-code pair data for code generation. To augment\nour dataset with high-quality data, we employed GPT-3.5 to generate 60K HTML pages following\nthe guidelines and prompts in CodeAlpaca [6]. Using Selenium WebDriver, we then created web\nimage screenshots from the generated HTML code. These web image-code pairs were subsequently\nconverted into an instruction-following data format similar to the LLaVA data format [33], enabling\ntheir use in training Multimodal Large Language Models (MLLMs).\nDWU: Creation of a new question-answer pair data for webpage understanding. For the purpose\nof fine-tuning our models through an instruction-following manner, we utilized the capabilities of\nGPT-4 to generate webpage code-based question-answer pairs. We generated 10 question-answer\npairs using GPT-4 for a subset of 24.35K webpage data, resulting in a total of 243.5K question-answer"}, {"title": "Statistics and Analysis", "content": "Figure 4 shows the word cloud of the answer set of our question-answer dataset. The word cloud\nhighlights the most frequently occurring terms, with \"section,\" \"color\", \"button\", and \"website\" being\nthe most prominent, indicating a strong emphasis on structural and design elements in the data. This\nreflects the detailed focus on the layout and visual aspects of the dataset.\nFigure 5 illustrates the distribution of the most common HTML tags in our GPT-3.5 generated HTML\ndata. The distribution shows a high frequency of essential structural tags such as <div>, <p>, <meta>,\n<img>, and <a>, indicating that the generated pages include a diverse range of elements necessary for\nrich and varied web content. The significant presence of <h2>, <input>, <html>, <head>, and <body>\ntags further reinforces the completeness and structural integrity of the generated HTML documents.\nTo estimate the difficulty levels of our HTML-based webpage dataset, we provide several quantitative\nmeasures and compare them with recent and similar existing datasets, namely WebSight [22],\nDesign2Code [50], and Pix2Code [4]\nDesign2Code is primarily used for testing and has a small size of 484 examples, limiting its versatility\nand robustness. In contrast, our dataset, intended for both training and testing, is significantly\nlarger (884.7K examples) and more complex, making it more suitable for developing robust models.\nOverall, our benchmark examples are more challenging and cover a broader spectrum of complexities\ncompared to prior efforts such as WebSight."}, {"title": "Distribution", "content": "Our instruction-following dataset contains 1,179.7K instruction\ndata points. This includes 884.7K website image-code pairs\nand 295K question-answer pairs.\nThe 295K question-answer pairs consist of 243.5K GPT-4 based\nquestion-answer pairs and 51.5K question-answer\npairs from WebSRC image-based data.\nOur evaluation dataset comprises 1,198 webpage screenshot"}, {"title": "A New Evaluation Framework for Webpage", "content": "Our proposed evaluation framework includes two schemes: (1) Webpage Understanding Benchmark\n(WUB): An offline evaluation using \"yes\" / \"no\" questions. (2) Webpage Code Generation Benchmark\n(WCGB): An online evaluation based on image similarity."}, {"title": "Evaluation Metric for HTML Code Generation", "content": "In the realm of assessing code quality, particularly in terms of final visual appeal and overall function-\nality, existing methods that rely on code similarity metrics fall short. These traditional approaches\noften lack the precision and reliability needed for nuanced evaluations of code effectiveness. To\naddress these shortcomings, we have developed a novel approach: regenerating the webpage using\nthe model's predicted HTML code and capturing screenshots of these generated webpages. This\nprocess, automated using the Selenium WebDriver extension in Python, shifts the focus from the less\nreliable code similarity assessments to a more accurate and visually oriented method. By comparing\nimages of the generated webpages, we can more effectively evaluate the aesthetic and functional\naspects of the code, offering a more comprehensive understanding of its quality."}, {"title": "Quantitative Evaluation for HTML Code Generation of MLLMs", "content": "We have evaluated the trained models using various data configurations and backbones on our WUB\nand WCGB benchmarks. The performance of the models on the code generation benchmark is\npresented in Table 3, while the results for webpage understanding are shown in Table 4."}, {"title": "Visualizations for Qualitative Evaluation", "content": "As shown in Figure 7, we compare the results between the original image which is the real-world\nwebpage sample, the rendered image generated by using LLM backbones of Vicuna1.5-7B and\nCrystalChat-7B, respectively. CrystalChat-7B is a code-enhanced LLM and our visualization demon-\nstrates that it achieves the better quality of generation than Vicuna1.5-7B even the performance is\nslightly worse on general multimodal domain, as presented in Moreover, as in Figure 8,\nour rendered webpage from the model trained on our web dataset closely resembles the original\nimage, indicating the positive impact of the web2code dataset. We further visualize our generation in\nwhen the input is a hand-drawn webpage to examine the adaptation ability of our model."}, {"title": "General Evaluation of MLLMs Using Web2Code", "content": "Setup and Overview. Our model training framework mainly follows the design of LLaVA-1.5 [33],\nwhere we leverage the capabilities of both a pre-trained visual encoder, an LLM and a projector\nto connect visual features into the word embedding space. The model consists of (1) a pre-trained\nCLIP ViT-L/14 [44] visual encoder with a resolution of 336\u00d7336 and a patch size of 14, which has\ngood feature representation already aligned with the text embedding space. (2) As for the LLM\nbackbones, we leverage CrystalChat as the base model and compare it with other latest LLM\nbackbones like Vicuna1.5 , LLaMA3 [1] and Training details\nand hyperparameters are presented in the Appendix A.\nGeneral Evaluation Metrics for MLLMs. MME serves as an extensive evaluative benchmark,\naiming to assess the perceptual and cognitive capability of MLLMs within 14 sub-tasks. Addition-\nally, we also evaluate the performance of our models on text-oriented visual question-answering\ntasks employing a diverse set of benchmark datasets including ScienceQA [39] and Furthermore, We assess our models' ability toward anti-hallucination through POPE.\nEffects of Web2Code on General Domain. Here, we first perform instruction tuning using Web2Code\non various LLM backbones and then we evaluate those MLLMs on the general domain of visual\nlanguage understanding. Throughout extensive experiments under various data configurations, we\nobserved that the proposed dataset Web2Code can be incorporated with the conventional visual\nlanguage instruction tuning dataset of LLaVA [33] without harming performances on the general\ndomain. Specifically, both proposed Web Understanding data and Web Code Generation data\ndo not hurt or even can be beneficial\nto the visual language understanding. For example, we observed that adding DWU to CrystalChat\nachieves comparable or even better performances on POPE (86.86\u219287.10), SciQA (67.77\u219268.27),\nand Somewhat surprisingly, we further found that adding DWCG can even\nimprove visual language understanding. For example, the second and third rows of CrystalChat show\n+40.31 and +5.00 points higher improvements in MME-P and MME-C benchmarks, respectively.\nMoreover, adding refined data are still effective in the visual language domain,\nby achieving comparable or even better performances on overall benchmarks. For example, the\nlast row indicates that adding preserves comparable performances on overall\nbenchmarks and even achieves +0.4 higher points on the SciQA benchmark."}, {"title": "Conclusion", "content": "We have presented Web2Code, a benchmark that consists of a high-quality, large-scale webpage-to-\ncode instruction tuning dataset containing 1179.7k entries and an evaluation suite for the webpage\nunderstanding and webpage-to-HTML translation abilities of MLLMs. Through extensive experi-\nments, we have demonstrated that our proposed dataset is clearly effective at enhancing these abilities\nof MLLMs as well as general visual proficiency, while existing datasets lead to inferior performance.\nWe hope our work will attract the community's attention and facilitate progress toward foundation\nmodels serving as virtual assistants for content generation and task automation.\nLimitations and Ethics Statement. The Web2Code project provides a comprehensive dataset and\nevaluation framework for fine-grained multimodal large language models. This can significantly\nenhance the capabilities of LLMs in understanding and generating web code from instructions,\nleading to advancements in web development automation, improved coding assistance tools and"}, {"title": "Appendix", "content": "Training Details and Hyperparameters\nWe follow the instruction-tuning protocol of LLaVA-1.5 [33]. In the pretraining step, we employ the\ncaption data to optimize the projector, while keeping the vision encoder and LLM frozen. Meanwhile,\nwe optimize the projector and LLM in the instruction tuning step. During the pretraining phase, we\nutilize a batch size of 256, while for the instruction tuning phase, we employ a batch size of 128. The\nlearning rate is set at 1e-3 during pretraining and adjusted to 2e-5 for instruction tuning, with both\nphases incorporating a cosine decay schedule. We also apply a learning rate warmup with a decay\nfactor of 0.03, and no weight decay is used. Both pretraining and instruction tuning are conducted for\none epoch each, consistently using the AdamW optimizer.\nMore Effects of Web2Code on General Domain\nHere, we first perform instruction tuning using Web2Code on various LLM backbones and then we\nevaluate those MLLMs on the general domain of visual language understanding.\nComparison on different LLM backbones. We compare the general domain abilities of various\nLLM backbones under the same data configuration of LLaVA + DWU + DWCG; Table 6 summarizes\nthe results of instruction-tuned MLLMs. Specifically, we found that instruction-tuned CrystalChat-7B,\nVicuna1.5-7B, and LLaMA2-7B show superior performances in the general domain compared to\nCrystalCoder and CodeLlama. For example, CrystalChat shows +132.89 points higher than CodeL-\nlama in MME-P (i.e. perception domain). Somewhat surprisingly, instruction-tuned CrystalChat\nshowed the strongest performance on TextVQA, which requires visual reasoning based on text in\nimages.\nQualitative Data Examples in WUB Benchmark\nThe qualitative data examples in our WUB benchmark are shown in Figure 10. It covers different\naspects of webpage understanding based on \"yes\" / \"no\" question-answer pairs.\nWCGB Criteria\nOur proposed WCGB framework consists of 10 distinct criteria, which we group into four categories,\neach encompassing specific criteria that are scored on a 0-10 scale:\nVisual Structure and Alignment\n\u2022 Layout Consistency: Measures the arrangement of structural webpage elements like head-\ners, footers, and sidebars.\n\u2022 Element Alignment: Assesses the alignment of images, buttons, and text boxes.\n\u2022 Proportional Accuracy: Checks for consistency in sizes and aspect ratios of visual elements.\n\u2022 Visual Harmony: Examines the overall balance and harmony in design.\nColor and Aesthetic Design\n\u2022 Color Scheme and Aesthetic Match: Focuses on the similarity in color schemes, including\nhues and saturation."}]}