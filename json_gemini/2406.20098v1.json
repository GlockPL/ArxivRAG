{"title": "Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs", "authors": ["Sukmin Yun", "Haokun Lin", "Rusiru Thushara", "Mohammad Qazim Bhat", "Yongxin Wang", "Zutao Jiang", "Mingkai Deng", "Jinhong Wang", "Tianhua Tao", "Junbo Li", "Haonan Li", "Preslav Nakov", "Timothy Baldwin", "Zhengzhong Liu", "Eric P. Xing", "Xiaodan Liang", "Zhiqiang Shen"], "abstract": "Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks. However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code. To address this problem, we propose Web2Code, a benchmark consisting of a new large-scale webpage-to-code dataset for instruction tuning and an evaluation framework for the webpage understanding and HTML code translation abilities of MLLMs. For dataset construction, we leverage pretrained LLMs to enhance existing webpage-to-code datasets as well as generate a diverse pool of new webpages rendered into images. Specifically, the inputs are webpage images and instructions, while the responses are the webpage's HTML code. We further include diverse natural language QA pairs about the webpage content in the responses to enable a more comprehensive understanding of the web content. To evaluate model performance in these tasks, we develop an evaluation framework for testing MLLMs' abilities in webpage understanding and web-to-code generation. Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain, while previous datasets result in worse performance. We hope our work will contribute to the development of general MLLMs suitable for web-based content generation and task automation.", "sections": [{"title": "1 Introduction", "content": "Multimodal large language models (MLLMs) have achieved explosive growth in the past few years. Leveraging the rich commonsense knowledge in large language models (LLMs), MLLMs are remarkably successful at processing and reasoning about various modalities such as image, video, and audio in a broad range of tasks such as recognition, reasoning, and question-answering, all using language as the intermediate representation. However, existing MLLMs are surprisingly poor at understanding webpage screenshots and generating the HTML code to express their latent states. For instance, given the instruction \"Parse the HTML code for this webpage\", the well-known LLaVA-1.5 [33] generates generic, pale code that fails to preserve most of the original webpage's features, which hampers its utility in applications such as UI prototyping, automation agents, and accessibility (e.g., noting available buttons and options given webpage screenshot)."}, {"title": "2 Related Work", "content": "MLLM Dataset. At present, there is a substantial amount of large-scale visual instruction data, primarily generated using GPT. SVIT and LRV-Instruction are both generated by GPT4 based on manual prompts to adjust the instruction data, including rich high-quality dialogue question and answer, complex reasoning question and answer, reference question and answer, and image detail description task datasets; similarly, ShareGPT4V, LLaVAR, LVIS-Instruct4V use GPT-4V to generate millions of high-quality image-text pairs, aiming to enhance the perception, reasoning and planning capabilities of MLLM. Commonly used image data sources include LAION, CC, SBU, COCO, VG, VQAv2 [18].\nMLLM. Instruction Tuning MLLM has made great progress in recent years. The structure of MLLM usually contains a visual encoder, vision-language mapping module and LLM. LLaVA-v1.5 only uses MLP as the vision-language mapping module and the successful application of instruction tuning on MLLM has inspired people. The community has explored various feasible structures, which can be divided into attention structures BLIP2, InstructBLIP, Qwen-VL, ShareGPT4V and non-attention structures LLaVA, Shikra according to the vision-language mapping module. At the same time, various open source and more powerful LLMs, such as Vicuna1.5, InternLM2 also help MLLM achieve richer and more extensive instruction following capabilities. Qwen-VL, OtterHD, mPLUG-Owl, InternLM-XComposer2-4KHD increase the resolution of images, while LLaVA-NeXT, Mini-Gemini, MM1 split the input image into several image crops. In addition, BRAVE, MoVA, DeepSeek-VL, OmniFusion apply supplementary vision encoders to obtain abundant visual features, e.g. DINOv2, SAM. Furthermore, more computer vision models are utilized for different tasks, which include image segmentation, detection and OCR, in MOAI, CuMo and SpatialVLM. Subsequently, MoE was applied to MLLM to expand the scale of training data at the same computing scale.\nCode Study. There are various code studies related to LLM. Sarker et al. focused on generating code functions through formula hints, trying to improve the robustness of the syntax, and system- atically tested the robustness of the syntax. From the perspective of security. Finkman et al. said that these code assistance tools may inadvertently disclose the developer's proprietary code to the code assistant service provider in the process of helping development, thus they proposed a complementary method to reduce the risk of code leakage while providing effective advice to developers. In addition to code leakage, the code generated by LLM has also caused concerns in industries and other fields. To address this issue, Ye et al. proposed a new zero-shot synthetic code detector based on the similarity between code and its rewritten variants. In the evaluation work on code generation, Du et al. proposed a new computational efficiency benchmark Mercury and a new metric Beyond for the efficiency evaluation of code. They experimentally showed that direct preference optimization can be used as a robust baseline for improving computational efficiency compared to supervised fine-tuning, which paves a promising path for future exploration of efficient code generation."}, {"title": "3 Dataset Construction", "content": "Overview. Our Web2Code instruction tuning dataset construction and instruction generation process involves four key components: (1) Creation of new webpage image-code pair data: We generated high-quality HTML webpage-code pairs following the CodeAlpaca prompt using GPT-3.5 and convert them into instruction-following data. (2) Refinement of existing webpage code generation data: We transform existing datasets including WebSight and Pix2Code into an instruction- following data format similar to LLaVA data, so they can be used as instruction-following data to train MLLMs. (3) Creation of a new text question-answer pair data: We generated a new question- answer pair dataset utilizing our new GPT-3.5 generated data from (1) for webpage understanding. (4) Refinement of existing webpage understanding data: We refine the WebSRC question-answer data to improve its quality using the GPT-4. Each component is elaborated in detail as follows:\nDWCG: Creation of new webpage image-code pair data for code generation. To augment our dataset with high-quality data, we employed GPT-3.5 to generate 60K HTML pages following the guidelines and prompts in CodeAlpaca. Using Selenium WebDriver, we then created web image screenshots from the generated HTML code. These web image-code pairs were subsequently converted into an instruction-following data format similar to the LLaVA data format, enabling their use in training Multimodal Large Language Models (MLLMs).\nDWCGR: Refinement of existing webpage code generation data. To enhance the capability of our model in the task of HTML code generation, we leverage the Pix2code and WebSight datasets. To mitigate the detrimental impact on model performance from random letters in Pix2Code data, we replace these random letters with meaningful text using GPT-4, thereby refining the webpages into diverse webpages encompassing product landing pages, personal portfolios, blogs, and other categories. We then visually rendered each sample by taking screenshots of the browser view of each webpage. Further, we convert all these data into LLaVA instruction following data format using the same strategy as used for DWCG. We note that DWCG and WebSight webpages follow Modern style while Pix2Code follow Bootstrap style.\nDWU: Creation of a new question-answer pair data for webpage understanding. For the purpose of fine-tuning our models through an instruction-following manner, we utilized the capabilities of GPT-4 to generate webpage code-based question-answer pairs. We generated 10 question-answer pairs using GPT-4 for a subset of 24.35K webpage data, resulting in a total of 243.5K question-answer"}, {"title": "3.1 Statistics and Analysis", "content": "Figure 4 shows the word cloud of the answer set of our question-answer dataset. The word cloud highlights the most frequently occurring terms, with \"section,\" \"color\", \"button\", and \"website\" being the most prominent, indicating a strong emphasis on structural and design elements in the data. This reflects the detailed focus on the layout and visual aspects of the dataset.\nFigure 5 illustrates the distribution of the most common HTML tags in our GPT-3.5 generated HTML data. The distribution shows a high frequency of essential structural tags such as <div>, <p>, <meta>, <img>, and <a>, indicating that the generated pages include a diverse range of elements necessary for rich and varied web content. The significant presence of <h2>, <input>, <html>, <head>, and <body> tags further reinforces the completeness and structural integrity of the generated HTML documents.\nTo estimate the difficulty levels of our HTML-based webpage dataset, we provide several quantitative measures and compare them with recent and similar existing datasets, namely WebSight, Design2Code , and Pix2Code (See Table 1).\nDesign2Code is primarily used for testing and has a small size of 484 examples, limiting its versatility and robustness. In contrast, our dataset, intended for both training and testing, is significantly larger (884.7K examples) and more complex, making it more suitable for developing robust models. Overall, our benchmark examples are more challenging and cover a broader spectrum of complexities compared to prior efforts such as WebSight."}, {"title": "3.2 Distribution", "content": "Our instruction-following dataset contains 1,179.7K instruction data points. This includes 884.7K website image-code pairs and 295K question-answer pairs.\nThe 295K question-answer pairs consist of 243.5K GPT-4 based question-answer pairs (DWU Data) and 51.5K question-answer pairs from WebSRC image-based data, as shown in Table 2. Our evaluation dataset comprises 1,198 webpage screenshot"}, {"title": "4 A New Evaluation Framework for Webpage", "content": "Our proposed evaluation framework includes two schemes: (1) Webpage Understanding Benchmark (WUB): An offline evaluation using \"yes\" / \"no\" questions. (2) Webpage Code Generation Benchmark (WCGB): An online evaluation (using GPT-4 Vision) based on image similarity."}, {"title": "4.1 Evaluation Metric for HTML Code Generation", "content": "In the realm of assessing code quality, particularly in terms of final visual appeal and overall function- ality, existing methods that rely on code similarity metrics fall short. These traditional approaches often lack the precision and reliability needed for nuanced evaluations of code effectiveness. To address these shortcomings, we have developed a novel approach: regenerating the webpage using the model's predicted HTML code and capturing screenshots of these generated webpages. This process, automated using the Selenium WebDriver extension in Python, shifts the focus from the less reliable code similarity assessments to a more accurate and visually oriented method. By comparing images of the generated webpages, we can more effectively evaluate the aesthetic and functional aspects of the code, offering a more comprehensive understanding of its quality."}, {"title": "4.2 Quantitative Evaluation for HTML Code Generation of MLLMs", "content": "We have evaluated the trained models using various data configurations and backbones on our WUB and WCGB benchmarks. The performance of the models on the code generation benchmark is presented in Table 3, while the results for webpage understanding are shown in Table 4."}, {"title": "4.3 Visualizations for Qualitative Evaluation", "content": "As shown in Figure 7, we compare the results between the original image which is the real-world webpage sample, the rendered image generated by using LLM backbones of Vicuna1.5-7B and CrystalChat-7B, respectively. CrystalChat-7B is a code-enhanced LLM and our visualization demon- strates that it achieves the better quality of generation than Vicuna1.5-7B even the performance is slightly worse on general multimodal domain, as presented in Table 6. Moreover, as in Figure 8, our rendered webpage from the model trained on our web dataset closely resembles the original image, indicating the positive impact of the web2code dataset. We further visualize our generation in Figure 9 when the input is a hand-drawn webpage to examine the adaptation ability of our model."}, {"title": "5 General Evaluation of MLLMs Using Web2Code", "content": "Setup and Overview. Our model training framework mainly follows the design of LLaVA-1.5 where we leverage the capabilities of both a pre-trained visual encoder, an LLM and a projector to connect visual features into the word embedding space. The model consists of (1) a pre-trained CLIP ViT-L/14 visual encoder with a resolution of 336\u00d7336 and a patch size of 14, which has good feature representation already aligned with the text embedding space. (2) As for the LLM backbones, we leverage CrystalChat as the base model and compare it with other latest LLM backbones like Vicuna1.5, LLaMA2, LLaMA3 and CrystalCoder . Training details and hyperparameters are presented in the Appendix A.\nGeneral Evaluation Metrics for MLLMs. MME serves as an extensive evaluative benchmark, aiming to assess the perceptual and cognitive capability of MLLMs within 14 sub-tasks. Addition- ally, we also evaluate the performance of our models on text-oriented visual question-answering tasks employing a diverse set of benchmark datasets including ScienceQA and TextVQA. Furthermore, We assess our models' ability toward anti-hallucination through POPE.\nEffects of Web2Code on General Domain. Here, we first perform instruction tuning using Web2Code on various LLM backbones and then we evaluate those MLLMs on the general domain of visual language understanding. Throughout extensive experiments under various data configurations, we observed that the proposed dataset Web2Code can be incorporated with the conventional visual language instruction tuning dataset of LLaVA without harming performances on the general domain. Specifically, both proposed Web Understanding data (DWU or DWUR) and Web Code Generation data (DWCG or DWCGR) do not hurt or even can be beneficial to the visual language understanding. For example, we observed that adding DWU to CrystalChat achieves comparable or even better performances on POPE (86.86\u219287.10), SciQA (67.77\u219268.27), and TextVQA (57.84\u219258.15). Somewhat surprisingly, we further found that adding DWCG can even improve visual language understanding. For example, the second and third rows of CrystalChat show +40.31 and +5.00 points higher improvements in MME-P and MME-C benchmarks, respectively. Moreover, adding refined data DWUR and DWCGR are still effective in the visual language domain, by achieving comparable (or even better) performances on overall benchmarks. For example, the last row indicates that adding DWUR and DWCGR preserves comparable performances on overall benchmarks and even achieves +0.4 higher points on the SciQA benchmark."}, {"title": "6 Conclusion", "content": "We have presented Web2Code, a benchmark that consists of a high-quality, large-scale webpage-to- code instruction tuning dataset containing 1179.7k entries and an evaluation suite for the webpage understanding and webpage-to-HTML translation abilities of MLLMs. Through extensive experi- ments, we have demonstrated that our proposed dataset is clearly effective at enhancing these abilities of MLLMs as well as general visual proficiency, while existing datasets lead to inferior performance. We hope our work will attract the community's attention and facilitate progress toward foundation models serving as virtual assistants for content generation and task automation.\nLimitations and Ethics Statement. The Web2Code project provides a comprehensive dataset and evaluation framework for fine-grained multimodal large language models. This can significantly enhance the capabilities of LLMs in understanding and generating web code from instructions, leading to advancements in web development automation, improved coding assistance tools and"}, {"title": "A Training Details and Hyperparameters", "content": "We follow the instruction-tuning protocol of LLaVA-1.5 [33]. In the pretraining step, we employ the caption data to optimize the projector, while keeping the vision encoder and LLM frozen. Meanwhile, we optimize the projector and LLM in the instruction tuning step. During the pretraining phase, we utilize a batch size of 256, while for the instruction tuning phase, we employ a batch size of 128. The learning rate is set at 1e-3 during pretraining and adjusted to 2e-5 for instruction tuning, with both phases incorporating a cosine decay schedule. We also apply a learning rate warmup with a decay factor of 0.03, and no weight decay is used. Both pretraining and instruction tuning are conducted for one epoch each, consistently using the AdamW optimizer."}, {"title": "B More Effects of Web2Code on General Domain", "content": "Here, we first perform instruction tuning using Web2Code on various LLM backbones and then we evaluate those MLLMs on the general domain of visual language understanding.\nComparison on different LLM backbones. We compare the general domain abilities of various LLM backbones under the same data configuration of LLaVA + DWU + DWCG; Table 6 summarizes the results of instruction-tuned MLLMs. Specifically, we found that instruction-tuned CrystalChat-7B, Vicuna1.5-7B, and LLaMA2-7B show superior performances in the general domain compared to CrystalCoder and CodeLlama. For example, CrystalChat shows +132.89 points higher than CodeL- lama in MME-P (i.e. perception domain). Somewhat surprisingly, instruction-tuned CrystalChat showed the strongest performance on TextVQA, which requires visual reasoning based on text in images."}, {"title": "C Qualitative Data Examples in WUB Benchmark", "content": "The qualitative data examples in our WUB benchmark are shown in Figure 10. It covers different aspects of webpage understanding based on \"yes\" / \"no\" question-answer pairs."}, {"title": "D WCGB Criteria", "content": "Our proposed WCGB framework consists of 10 distinct criteria, which we group into four categories, each encompassing specific criteria that are scored on a 0-10 scale:\n1. Visual Structure and Alignment\n\u2022 Layout Consistency: Measures the arrangement of structural webpage elements like head- ers, footers, and sidebars.\n\u2022 Element Alignment: Assesses the alignment of images, buttons, and text boxes.\n\u2022 Proportional Accuracy: Checks for consistency in sizes and aspect ratios of visual elements.\n\u2022 Visual Harmony: Examines the overall balance and harmony in design.\n2. Color and Aesthetic Design\n\u2022 Color Scheme and Aesthetic Match: Focuses on the similarity in color schemes, including hues and saturation."}, {"title": "E Prompt Templates", "content": "System: You are an advanced AI model who can identify html code and interpret the compiled webpage.\nUser: You are asked to come up with a set of 10 diverse website understanding task instructions with the corresponding source codes. These task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\nYou should output 10 questions and the answers for them, for each html code.\nYour output should be formatted as follows. Don't include any other text, connective phases other than the formatted text.\noutput a comma separated list of 10 instruction pairs in following format\n[{\"Q\": \"<question>\", \"A\": \"<answer>\"}, {\"Q\": \"<question>\", \"A\": \"<answer>\"}]\nHere are the requirements:\n1. Try not to repeat the verb for each instruction to maximize diversity.\n2. The languages used for the instruction should be diverse. For example, you should combine questions with imperative instructions.\n3. The instructions should be in English.\n4. The instructions should be at least 1 to 2 sentences long. Either an imperative sentence or a question is permitted.\n5. Instructions should be clear and precise.\n6. Instructions can either be simple queries like 'what text is displayed on the button?', to more complex tasks requiring analysis, such as 'what would be the total cost of ordering items A and B from the menu?'."}]}