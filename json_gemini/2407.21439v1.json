{"title": "MLLM IS A STRONG RERANKER: ADVANCING MULTIMODAL RETRIEVAL-AUGMENTED GENERATION VIA KNOWLEDGE-ENHANCED RERANKING AND NOISE-INJECTED TRAINING", "authors": ["Zhanpeng Chen", "Chengjin Xu", "Yiyan Qi", "Jian Guo"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in processing and generating content across multiple data modalities, including text, images, audio, and video. However, a significant drawback of MLLMs is their reliance on static training data, leading to outdated information and limited contextual awareness. This static nature hampers their ability to provide accurate, up-to-date responses, particularly in dynamic or rapidly evolving contexts. Integrating Multimodal Retrieval-augmented Generation (Multimodal RAG) offers a promising solution, but the system would inevitably encounter the multi-granularity noisy correspondence (MNC) problem, which involves two types of noise: coarse-grained (query-caption) and fine-grained (query-image). This noise hinders accurate retrieval and generation. In this work, we propose RagLLaVA, a novel framework with knowledge-enhanced reranking and noise-injected training, to address these limitations. We instruction-tune the MLLM with a simple yet effective instruction template to induce its ranking ability and serve it as a reranker to precisely filter the top-k retrieved images. For generation, we inject visual noise during training at the data and token levels to enhance the generator's robustness. Extensive experiments are conducted on the subsets of two datasets that require retrieving and reasoning over images to answer a given query. Our results demonstrate the superiority of RagLLaVA in retrieving accurately and generating robustly.", "sections": [{"title": "1 INTRODUCTION", "content": "As an attempt towards Artificial General Intelligence (AGI), Large Language Models (LLMs) have made significant strides in language understanding and human-like text generation (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023). However, true AGI requires more than just linguistic capabilities. It necessitates a comprehensive understanding and interaction with the world, encompassing multiple modalities beyond text. Thus, the recent progress of Multimodal Large Language Models (MLLM) in handling multimodal information has attracted the community. By processing and generating content across different modalities, MLLMs aim to create a more holistic and nuanced understanding of the world, closer to how humans perceive and interpret information. This integration of modalities enables MLLMs to perform tasks that require contextual understanding from multiple data sources, such as Visual Question Answering (VQA) (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Mishra et al., 2019), Table Question Answering (Lu et al., 2022), Text-to-image Generation (Ramesh et al., 2021; Yu et al., 2022; Aghajanyan et al., 2022), etc."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 MULTIMODAL LARGE LANGUAGE MODEL", "content": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive performances in handling multi-format information (Driess et al., 2023; Huang et al., 2024; Achiam et al., 2023). MLLMs are generally built upon existing Large Language Models (LLMs) and integrate visual information as input tokens by utilizing an additional vision encoder (e.g. CLIP) and a bridging connector (e.g. MLP). For instance, LLaVA (Liu et al., 2024b;a) adopts one/two linear MLP to project visual tokens and align the feature dimension with word embeddings, while BLIP-2 (Li et al., 2023) leverages a group of learnable query tokens to extract information in a query-based manner. However, MM1 (McKinzie et al., 2024) has shown that the number of visual tokens and image resolution are the most critical factors, whereas the type of connector has minimal impact. By connecting the visual and textual modalities, MLLMs significantly enhance human-AI interaction and demonstrate remarkable capabilities in understanding and generating multimodal content. Despite these advances, MLLMs tend to underperform in knowledge-intensive tasks (e.g. WebQA and MultimodalQA (Talmor et al., 2021)) that require seeking up-to-date information. Since the knowledge stored in their massive parameters is currently limited, it is crucial for MLLMs to resort to external memories for grounded generation."}, {"title": "2.2 MULTIMODAL RETRIEVAL-AUGMENTED GENERATION", "content": "Enhancing language models by incorporating relevant information from diverse knowledge sources has been shown to improve performance across various NLP tasks (Borgeaud et al., 2022; Lewis et al., 2020). DPR (Karpukhin et al., 2020) trains the retriever using in-batch documents and samples negative examples for contrastive learning, allowing the pre-trained retriever to excel in open-domain question answering. REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) treat the retrieved passages as latent variables and train the retriever-generator system jointly, leading to more"}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 PRELIMINARIES", "content": "The traditional Retrieval-augmented Language Model (RALM) acquires knowledge from the external memory M and utilizes the knowledge in grounded outputs to promote accurate and explainable generation. The retriever R first retrieves the top-K most relevant contexts C = {c1,\u2026\u2026\u2026, ck} from M for the given question q. Subsequently, the autoregressive language model generates answers based on these retrieved contexts. Under the multimodal setting, the retriever needs to compare the textual queries with the multimodal documents and find the best matches for the generator G. In this paper, we focus on retrieving the visual-related contexts to study open-world multimodal question answering."}, {"title": "3.2 MULTIMODAL RETRIEVER", "content": "We follow the dual-encoder architecture based on CLIP text encoder \u0424text and image encoder \u0424img. Before the retrieval stage, given image-query pairs (v, q) from the dataset D, we first apply the image encoder \u0424img to encode each image and build the image memory M using faiss (Douze et al., 2024). From the external memory M, the retriever aims to retrieve a small set of images that support the textual query q. Specifically, we encode the query with the text encoder \u0424text and use MIPS over all of the image candidates v \u2208 M as follows,\n$$M = TopK(M|q) = TopK \\Phi_{text}(q) \\cdot \\Phi_{img}(v).$$ VEM\nThe top-K images with the highest inner product scores, i.e. the nearest top-K neighbors M = {\u03c51, \u03c52,..., \u03c5k}, are retrieved as the candidate images for answer generation."}, {"title": "3.3 INDUCING RANKING ABILITY OF MLLMS", "content": "CLIP stands out across a wide range of multimodal representations and retrieval tasks as a powerful and highly transferable model. However, when encountering long-tail distribution or domain-specific terms, CLIP fails to match the proper pairs across text and images. It results in a higher demand of k value to increase the recall rate of supporting materials, which is time- and resource-consuming. In general, MLLMs are pre-trained on vast image-text pairs for feature alignment and fine-tuned on language-image instruction-tuning datasets for instruction following. With this pre-injected multimodal knowledge, they are inherently capable of understanding semantically relevant contents across both visual and textual modalities. Therefore, to mitigate the bottleneck challenge of multimodal RAG, we introduce a flexible Knowledge-enhanced Ranking task to induce the ranking ability of MLLMs."}, {"title": "Ranking Data Construction", "content": "To enhance the ranking capability of MLLMs, we construct the instruction-following data based on each multimodal QA dataset. We treat each query and the ground truth images as relevant, while the hard negative images as irrelevant. As shown in Table 1, we construct two types of ranking task and require the model to generate 'Yes' for the relevant pairs and 'No' for the irrelevant pairs. Intuitively, the caption-aware style brings more additional knowledge for the model to distinguish the relevance between the image and query. Therefore, we train the reranker with the caption-aware ranking task. In addition, the instruction tuning for ranking can be either blended into the supervised fine-tuning of downstream tasks or conducted separately. More analysis of the data organization and instruction template can be seen in \u00a75.2 and \u00a75.3, respectively."}, {"title": "Knowledge-enhanced Reranking", "content": "By simply asking the question \"Based on the image and its caption, is the image relevant to the question? Answer 'Yes' or 'No'.\u201d, we measure the relevance between the image and query with the probability p of generating 'Yes' calculated from the output logits. Thus, reranking the top-K candidates into top-N can be formulated as follows,\n$$M = TopN(M|\\rho) = TopN \\rho_{\\phi}(v, c, q),$$\n$$(v,c) \\in M$$\n$$\\rho_{\\phi}(v, c, q) = \\frac{exp(logit(y_1 = \\text{``Yes\"}|v, c, q))}{exp(logit(y_1 = \\text{``Yes\"}|v, c, q)) + exp(logit(y_1 = \\text{``No\"}|v, c, q))}$$\nwhere v, c, and q denote the image, corresponding caption, and query, respectively. \\phi is the weight of the reranker. y1 denotes the first token in the generated output."}, {"title": "Adaptive Threshold", "content": "Since the reranked images might still have low relevance p to the query, they can negatively affect answer generation, potentially performing worse than not including the images. To further improve the retrieval accuracy, we apply an adaptive threshold \u03b7 to filter out candidates when p < \u03b7. We set two types of thresholds: the natural threshold and the adaptive threshold. The natural threshold refers to \u03b7 = 0.5, which is the natural boundary for our binary classification ranking task. For more precise retrieval, we experiment on the validation set and utilize the intersection point of the interpolated curve of exact match and mismatch as the adaptive threshold. In this way, the model can rely solely on its prior knowledge to answer questions when it cannot retrieve sufficiently relevant images, avoiding the distraction of irrelevant images. By forcing the MLLM to jointly consider the query, caption, and image, the simple yet effective question template stimulates and enhances the model's ranking ability with multimodal knowledge, thereby supporting the trustworthy generation."}, {"title": "3.4 NOISE-INJECTED TRAINING", "content": "Compared to providing a fixed number of images each time, the VQA task with single/multiple images interleaved is more aligned with real-world scenarios. However, it also presents challenges in determining the optimal number of images to provide each time and in extracting relevant information rather than distracting information from the provided images. Though the reranker performs well in selecting relevant images, irrelevant ones still inevitably disturb the accurate generation.\nInspired by VCD (Leng et al., 2024), visual uncertainty amplifies language priors, and contrasting the logits obtained from the enhanced language priors with the original logits can better highlight"}, {"title": "Noise-injected Data Construction", "content": "For datasets that may require both single and multiple image inputs, we standardize the number of image inputs for each sample in the instruction-following data to the maximum number needed for any question. In the case of WebQA, where each question requires 1-2 images for answering, we randomly sample 1 image from the hard negatives as an injected noise for the single-image query. The model is required to distinguish between relevant and irrelevant visual information, which strengthens its capability of visual understanding."}, {"title": "Noise-injected Logits Contrasting", "content": "Although injecting noise into the dataset can help the model better adapt to noisy environments, it can also be a double-edged sword, making the training process more unpredictable. Instead of the simple Maximum Likelihood Estimation (MLE) loss, we need a more robust objective (Xiao et al., 2024) to guide the model to learn the correlation between visual tokens and textual (query) tokens accurately. We first employ the forward diffusion process (Ho et al., 2020) to distort the image:\n$$f (v_t | v_{t-1}) = \\mathcal{N} (v_t; \\sqrt{1 - \\gamma_t} v_{t-1}, \\gamma_t I), f (v_T | v_0) = \\prod_{t=1}^{T} f (v_t | v_{t-1}),$$ where I and \u03c5o denote an identity matrix and the original image, respectively. We gradually distort the original image by adding the Gaussian noise for T steps and \u03b3 controls the the amount of noise added in each step. Subsequently, given a textual query x and an image input v, the model generates two logit distributions conditioned on different visual posteriors: the original v and distorted v*. By contrasting the logit distributions obtained from these two conditions, we can get the contrastive probability distribution of the i-th sample at time step t as follows,\n$$\\Delta logit(y_{i,t} | v_i, v_i^{*}, x_i, y_{i,<t}) = logit_{\\phi}(y_{i,t} | v_i, x_i, y_{i,<t}) - logit_{\\phi}(y_{i,t} | v_i^{*}, x_i, y_{i,<t}),$$\nwhere yi,t and yi,<t denote the token at time step t and the generated tokens sequence up to the time stept \u2212 1 of the i-th sample, respectively. Subsequently, we can obtain the visual correlation weight:\n$$w_{i,t} = \\Delta logit(y_{i,t} | v_i, v_i^{*}, x_i, y_{i, <t}).$$\nFollowing Xiao et al. (2024) to post-process and smooth the weights, we finally reassign the weight of each token in the vanilla MLE loss, which can be formulated as follows,\n$$\\mathcal{L}_{INJ} = - \\sum_{t=1}^{l} \\frac{w_{i,t}}{\\sum_{k=1}^{l} w_{ik}} \\cdot logp_{\\phi}(y_{i,t} | v_i, x_i, y_{i,<t}),$$\nwhere l and w represent the length of textual tokens and the smooth weight, respectively."}, {"title": "4 EXPERIMENT SETUP", "content": ""}, {"title": "4.1 DATASETS AND EVALUATION METRICS", "content": "For evaluation, we consider the image-related subsets of two multimodal QA datasets WebQA and MultimodalQA. Both datasets contain multimodal knowledge-seeking query-answer pairs. Since the test set labels from both datasets are not publicly available, we report the results on the validation set. Each query is associated with a set of hard negative distractors so that two evaluation setups can be used, namely distractor and full-wiki. However, we only consider the full-wiki setting to demonstrate the superiority of our retrieval-rerank-generation pipeline.\nWebQA consists of queries requiring 1-2 images or text snippets, while 44% of image-based and 99% of text-based queries need multiple knowledge sources. Following the vanilla evaluation setting, we measure the overlap of key entities between the generated output and ground truth answer as Accuracy.\nMultimodalQA contains multimodal questions over tables, text, and images. We focus on the QA pairs requiring only image information, which are annotated as 'ImageQ' and attached to 1 image each. The evaluation metric used is Exact Match (EM)."}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "This paper uses LLaVA-v1.5-13B (Liu et al., 2024a) as the backbone to evaluate our proposed pipeline. We employ the frozen CLIP-ViT-L/14-336px as the vision and text encoder. For RagLLaVA, we first train the reranker model with the ranking task only. Subsequently, we use CLIP to retrieve top-K candidates and rerank them into top-N with the fine-tuned reranker. K is set to 20, while N is set to 2 for WebQA and 1 for MultimodalQA. During instruction tuning, we use LORA (Hu et al., 2021) and set the learning rate to 2e-5 following the original setting. We set the batch size to 16 for training the reranker and 8 for the generator. For evaluation, we use greedy decoding to ensure reproducibility and report the best performance. All experiments are conducted on 8 40G NVIDIA A100 GPUs."}, {"title": "5 EXPERIMENTS AND ANALYSIS", "content": ""}, {"title": "5.1 MAIN RESULTS", "content": "Results of Retrieval As shown in Figure 3, we collect the relevance probability of the image candidates after reranking and the results prove the superiority of our proposed knowledge-enhanced reranking. Among the train, validation, and test sets, the relevance probabilities of correct recalls are concentrated in the highest range. Since there is still a portion of erroneous recalls whose relevance probabilities are relatively high, we plotted the interpolated curves of correct recalls and erroneous recalls on the validation set and took the x-coordinate of their intersection point as the adaptive"}, {"title": "5.2 ABLATION STUDIES", "content": "To validate the efficacy of each component in our proposed method, we conduct a set of ablation experiments on WebQA, and the results are reported in Table 4."}, {"title": "Effect of Reranking", "content": "For \"w/o Reranker\", we directly retrieve Top-2 images with CLIP in the inference stage. For \"w/ Blended Reranker\", we utilize the blended reranker for both reranking and generation, which is trained with noise-injected data and vanilla MLE loss. The use of the reranker in RagLLaVA shows an improvement in all metrics (Single., Multi. and Overall) compared to \"w/o Reranker\". The performance with the blended reranker is relatively poor, which is because training the blended reranker requires precise adjustments to the composition of the training datasets to achieve better results. In our case, we directly mix the ranking and QA datasets due to a lack of sufficient datasets, which leads to suboptimal performance."}, {"title": "Effect of Noise-injected Data", "content": "For \"w/o ND\", we replace the noise-injected dataset with the vanilla dataset. Ablating ND results in a performance decrease in Multi. and Overall, while the performance in Single. improves. Though introducing noise helps the model learn to distinguish between the candidate images more effectively in multi-image inference, additional candidates act as a form of fixed noise in single-image inference."}, {"title": "Effect of Noise-injected Logits Contrasting", "content": "Since NLC enhances the model's robustness at the token level, ablating it leads to a decrease in all metrics on WebQA. This decline is more pronounced when both NLC and ND are ablated, especially in multi-image inference scenarios. Therefore, our proposed training method, which injects noise at the data and token levels, demonstrates excellent performance."}, {"title": "5.3 RERANKING PERFORMANCE ANALYSIS", "content": "To further verify the effectiveness of our proposed knowledge-enhanced ranking, we conduct experiments on test sets of WebQA ranking and QA datasets. In WebQA QA task, we retrieve top-20 candidate images using CLIP and rerank them into top-2 with our instruction-tuned reranker models. As shown in Table 5, the vanilla LLaVA-v1.5-13B performs poorly on both tasks. The models trained on the ranking task outperform the baseline, particularly the one trained on caption-aware task, which even"}, {"title": "6 CONCLUSION", "content": "In this paper, we present a robust framework for enhancing Multimodal Large Language Models (MLLMs) through knowledge-enhanced reranking and noise-injected training to tackle the multi-granularity noisy correspondence (MNC) problem in multimodal retrieval-augmented generation."}]}