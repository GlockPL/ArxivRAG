{"title": "MLLM IS A STRONG RERANKER: ADVANCING MULTIMODAL RETRIEVAL-AUGMENTED GENERATION VIA KNOWLEDGE-ENHANCED RERANKING AND NOISE-INJECTED TRAINING", "authors": ["Zhanpeng Chen", "Chengjin Xu", "Yiyan Qi", "Jian Guo"], "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in processing and generating content across multiple data modalities, including text, images, audio, and video. However, a significant drawback of MLLMs is their reliance on static training data, leading to outdated information and limited contextual awareness. This static nature hampers their ability to provide accurate, up-to-date responses, particularly in dynamic or rapidly evolving contexts. Integrating Multimodal Retrieval-augmented Generation (Multimodal RAG) offers a promising solution, but the system would inevitably encounter the multi-granularity noisy correspondence (MNC) problem, which involves two types of noise: coarse-grained (query-caption) and fine-grained (query-image). This noise hinders accurate retrieval and generation. In this work, we propose RagLLaVA, a novel framework with knowledge-enhanced reranking and noise-injected training, to address these limitations. We instruction-tune the MLLM with a simple yet effective instruction template to induce its ranking ability and serve it as a reranker to precisely filter the top-k retrieved images. For generation, we inject visual noise during training at the data and token levels to enhance the generator's robustness. Extensive experiments are conducted on the subsets of two datasets that require retrieving and reasoning over images to answer a given query. Our results demonstrate the superiority of RagLLaVA in retrieving accurately and generating robustly.", "sections": [{"title": "1 INTRODUCTION", "content": "As an attempt towards Artificial General Intelligence (AGI), Large Language Models (LLMs) have made significant strides in language understanding and human-like text generation (Brown et al., 2020; Achiam et al., 2023; Touvron et al., 2023). However, true AGI requires more than just linguistic capabilities. It necessitates a comprehensive understanding and interaction with the world, encompassing multiple modalities beyond text. Thus, the recent progress of Multimodal Large Language Models (MLLM) in handling multimodal information has attracted the community. By processing and generating content across different modalities, MLLMs aim to create a more holistic and nuanced understanding of the world, closer to how humans perceive and interpret information. This integration of modalities enables MLLMs to perform tasks that require contextual understanding from multiple data sources, such as Visual Question Answering (VQA) (Goyal et al., 2017; Hudson & Manning, 2019; Marino et al., 2019; Mishra et al., 2019), Table Question Answering (Lu et al., 2022), Text-to-image Generation (Ramesh et al., 2021; Yu et al., 2022; Aghajanyan et al., 2022), etc."}, {"title": "2 RELATED WORK", "content": "2.1 MULTIMODAL LARGE LANGUAGE MODEL\nRecent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive performances in handling multi-format information (Driess et al., 2023; Huang et al., 2024; Achiam et al., 2023). MLLMs are generally built upon existing Large Language Models (LLMs) and integrate visual information as input tokens by utilizing an additional vision encoder (e.g. CLIP) and a bridging connector (e.g. MLP). For instance, LLaVA (Liu et al., 2024b;a) adopts one/two linear MLP to project visual tokens and align the feature dimension with word embeddings, while BLIP-2 (Li et al., 2023) leverages a group of learnable query tokens to extract information in a query-based manner. However, MM1 (McKinzie et al., 2024) has shown that the number of visual tokens and image resolution are the most critical factors, whereas the type of connector has minimal impact. By connecting the visual and textual modalities, MLLMs significantly enhance human-AI interaction and demonstrate remarkable capabilities in understanding and generating multimodal content. Despite these advances, MLLMs tend to underperform in knowledge-intensive tasks (e.g. WebQA and MultimodalQA (Talmor et al., 2021)) that require seeking up-to-date information. Since the knowledge stored in their massive parameters is currently limited, it is crucial for MLLMs to resort to external memories for grounded generation.\n2.2 MULTIMODAL RETRIEVAL-AUGMENTED GENERATION\nEnhancing language models by incorporating relevant information from diverse knowledge sources has been shown to improve performance across various NLP tasks (Borgeaud et al., 2022; Lewis et al., 2020). DPR (Karpukhin et al., 2020) trains the retriever using in-batch documents and samples negative examples for contrastive learning, allowing the pre-trained retriever to excel in open-domain question answering. REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) treat the retrieved passages as latent variables and train the retriever-generator system jointly, leading to more"}, {"title": "3 METHODOLOGY", "content": "3.1 PRELIMINARIES\nThe traditional Retrieval-augmented Language Model (RALM) acquires knowledge from the external memory M and utilizes the knowledge in grounded outputs to promote accurate and explainable generation. The retriever R first retrieves the top-K most relevant contexts C = {c1,\u2026\u2026\u2026, ck} from M for the given question q. Subsequently, the autoregressive language model generates answers based on these retrieved contexts. Under the multimodal setting, the retriever needs to compare the textual queries with the multimodal documents and find the best matches for the generator G. In this paper, we focus on retrieving the visual-related contexts to study open-world multimodal question answering.\n3.2 MULTIMODAL RETRIEVER\nWe follow the dual-encoder architecture based on CLIP text encoder \\(\\Phi_{\\text{text}}\\) and image encoder \\(\\Phi_{\\text{img}}\\). Before the retrieval stage, given image-query pairs (v, q) from the dataset D, we first apply the image encoder \\(\\Phi_{\\text{img}}\\) to encode each image and build the image memory M using faiss (Douze et al., 2024). From the external memory M, the retriever aims to retrieve a small set of images that support the textual query q. Specifically, we encode the query with the text encoder \\(\\Phi_{\\text{text}}\\) and use MIPS over all of the image candidates v \u2208 M as follows,\nM = TopK(M|q) = TopK \\Phi_{\\text{text}}(q) \\cdot \\Phi_{\\text{img}}(v).\n(1)\nv\u2208M\nThe top-K images with the highest inner product scores, i.e. the nearest top-K neighbors M = {v1, v2,..., vk}, are retrieved as the candidate images for answer generation.\n3.3 INDUCING RANKING ABILITY OF MLLMS\nCLIP stands out across a wide range of multimodal representations and retrieval tasks as a powerful and highly transferable model. However, when encountering long-tail distribution or domain-specific terms, CLIP fails to match the proper pairs across text and images. It results in a higher demand of k value to increase the recall rate of supporting materials, which is time- and resource-consuming. In general, MLLMs are pre-trained on vast image-text pairs for feature alignment and fine-tuned on language-image instruction-tuning datasets for instruction following. With this pre-injected multimodal knowledge, they are inherently capable of understanding semantically relevant contents across both visual and textual modalities. Therefore, to mitigate the bottleneck challenge of multimodal RAG, we introduce a flexible Knowledge-enhanced Ranking task to induce the ranking ability of MLLMs.\nRanking Data Construction To enhance the ranking capability of MLLMs, we construct the instruction-following data based on each multimodal QA dataset. We treat each query and the ground truth images as relevant, while the hard negative images as irrelevant. As shown in Table 1, we construct two types of ranking task and require the model to generate 'Yes' for the relevant pairs and 'No' for the irrelevant pairs. Intuitively, the caption-aware style brings more additional knowledge for the model to distinguish the relevance between the image and query. Therefore, we train the reranker with the caption-aware ranking task. In addition, the instruction tuning for ranking can be either blended into the supervised fine-tuning of downstream tasks or conducted separately. More analysis of the data organization and instruction template can be seen in \u00a75.2 and \u00a75.3, respectively.\nKnowledge-enhanced Reranking By simply asking the question \"Based on the image and its caption, is the image relevant to the question? Answer 'Yes' or 'No'.\u201d, we measure the relevance between the image and query with the probability p of generating 'Yes' calculated from the output logits. Thus, reranking the top-K candidates into top-N can be formulated as follows,\nM = TopN(M|\u03c6) = TopN p\u03c6(v, c, q),\n(2)\n(v,c)\u2208M\np\u03c6(v, c, q) =\nexp(logit(y1 = \u201cYes\u201d|v, c, q))\nexp(logit(y1 = \u201cYes\u201d|v, c, q)) + exp(logit(y1 = \u201cNo\u201d|v, c, q))\n(3)\nwhere v, c, and q denote the image, corresponding caption, and query, respectively. \u03c6 is the weight of the reranker. y1 denotes the first token in the generated output.\nAdaptive Threshold Since the reranked images might still have low relevance p to the query, they can negatively affect answer generation, potentially performing worse than not including the images. To further improve the retrieval accuracy, we apply an adaptive threshold \u03b7 to filter out candidates when p < \u03b7. We set two types of thresholds: the natural threshold and the adaptive threshold. The natural threshold refers to \u03b7 = 0.5, which is the natural boundary for our binary classification ranking task. For more precise retrieval, we experiment on the validation set and utilize the intersection point of the interpolated curve of exact match and mismatch as the adaptive threshold. In this way, the model can rely solely on its prior knowledge to answer questions when it cannot retrieve sufficiently relevant images, avoiding the distraction of irrelevant images. By forcing the MLLM to jointly consider the query, caption, and image, the simple yet effective question template stimulates and enhances the model's ranking ability with multimodal knowledge, thereby supporting the trustworthy generation.\n3.4 NOISE-INJECTED TRAINING\nCompared to providing a fixed number of images each time, the VQA task with single/multiple images interleaved is more aligned with real-world scenarios. However, it also presents challenges in determining the optimal number of images to provide each time and in extracting relevant information rather than distracting information from the provided images. Though the reranker performs well in selecting relevant images, irrelevant ones still inevitably disturb the accurate generation.\nInspired by VCD (Leng et al., 2024), visual uncertainty amplifies language priors, and contrasting the logits obtained from the enhanced language priors with the original logits can better highlight"}, {"title": "4 EXPERIMENT SETUP", "content": "4.1 DATASETS AND EVALUATION METRICS\nFor evaluation, we consider the image-related subsets of two multimodal QA datasets WebQA and MultimodalQA. Both datasets contain multimodal knowledge-seeking query-answer pairs. Since the test set labels from both datasets are not publicly available, we report the results on the validation set. Each query is associated with a set of hard negative distractors so that two evaluation setups can be used, namely distractor and full-wiki. However, we only consider the full-wiki setting to demonstrate the superiority of our retrieval-rerank-generation pipeline.\nWebQA consists of queries requiring 1-2 images or text snippets, while 44% of image-based and 99% of text-based queries need multiple knowledge sources. Following the vanilla evaluation setting, we measure the overlap of key entities between the generated output and ground truth answer as Accuracy.\nMultimodalQA contains multimodal questions over tables, text, and images. We focus on the QA pairs requiring only image information, which are annotated as 'ImageQ' and attached to 1 image each. The evaluation metric used is Exact Match (EM).\n4.2 IMPLEMENTATION DETAILS\nThis paper uses LLaVA-v1.5-13B (Liu et al., 2024a) as the backbone to evaluate our proposed pipeline. We employ the frozen CLIP-ViT-L/14-336px as the vision and text encoder. For RagLLaVA, we first train the reranker model with the ranking task only. Subsequently, we use CLIP to retrieve top-K candidates and rerank them into top-N with the fine-tuned reranker. K is set to 20, while N is set to 2 for WebQA and 1 for MultimodalQA. During instruction tuning, we use LORA (Hu et al., 2021) and set the learning rate to 2e-5 following the original setting. We set the batch size to 16 for training the reranker and 8 for the generator. For evaluation, we use greedy decoding to ensure reproducibility and report the best performance. All experiments are conducted on 8 40G NVIDIA A100 GPUs."}, {"title": "5 EXPERIMENTS AND ANALYSIS", "content": "5.1 MAIN RESULTS\nResults of Retrieval As shown in Figure 3, we collect the relevance probability of the image candidates after reranking and the results prove the superiority of our proposed knowledge-enhanced reranking. Among the train, validation, and test sets, the relevance probabilities of correct recalls are concentrated in the highest range. Since there is still a portion of erroneous recalls whose relevance probabilities are relatively high, we plotted the interpolated curves of correct recalls and erroneous recalls on the validation set and took the x-coordinate of their intersection point as the adaptive\neffectiveness of our proposed knowledge-enhanced ranking, we conduct experi-ments on test sets of WebQA rankingand QA datasets. In WebQA QA task,we retrieve top-20 candidate images using CLIP and rerank them into top-2 withour instruction-tuned reranker models. Asshown in Table 5, the vanilla LLaVA-v1.5-13B performs poorly on both tasks. Themodels trained on the ranking task outperform the baseline, particularly the onetrained on caption-aware task, which even\nsurpasses CLIP-ViT-L/14-336px on Recall@4 with its Recall@2. This demonstrates the superiorityof our simple yet effective instruction templates in inducing the ranking ability of MLLMs.\n5.2 ABLATION STUDIES\nTo validate the efficacy of each component in our proposed method, we conduct a set of ablationexperiments on WebQA, and the results are reported in Table 4.\nEffect of Reranking For \"w/o Reranker\", we directly retrieve Top-2 images with CLIP in theinference stage. For \"w/ Blended Reranker\", we utilize the blended reranker for both rerankingand generation, which is trained with noise-injected data and vanilla MLE loss. The use of thereranker in RagLLaVA shows an improvement in all metrics (Single., Multi. and Overall) comparedto \"w/o Reranker\". The performance with the blended reranker is relatively poor, which is becausetraining the blended reranker requires precise adjustments to the composition of the training datasetsto achieve better results. In our case, we directly mix the ranking and QA datasets due to a lack ofsufficient datasets, which leads to suboptimal performance.\nEffect of Noise-injected Data For \"w/o ND\", we replace the noise-injected dataset with the vanilladataset. Ablating ND results in a performance decrease in Multi. and Overall, while the performancein Single. improves. Though introducing noise helps the model learn to distinguish between thecandidate images more effectively in multi-image inference, additional candidates act as a form offixed noise in single-image inference.\nEffect of Noise-injected Logits Contrasting Since NLC enhances the model's robustness at thetoken level, ablating it leads to a decrease in all metrics on WebQA. This decline is more pronouncedwhen both NLC and ND are ablated, especially in multi-image inference scenarios. Therefore, ourproposed training method, which injects noise at the data and token levels, demonstrates excellentperformance.\n5.3 RERANKING PERFORMANCE ANALYSIS\n5.4 CONCLUTION"}, {"title": "6 CONCLUSION", "content": "In this paper, we present a robust framework for enhancing Multimodal Large Language Models (MLLMs) through knowledge-enhanced reranking and noise-injected training to tackle the multi-granularity noisy correspondence (MNC) problem in multimodal retrieval-augmented generation."}, {"title": "Noise-injected Logits Contrasting", "content": "Although injecting noise into the dataset can help the model\nbetter adapt to noisy environments, it can also be a double-edged sword, making the training process\nmore unpredictable. Instead of the simple Maximum Likelihood Estimation (MLE) loss, we need a\nmore robust objective (Xiao et al., 2024) to guide the model to learn the correlation between visual\ntokens and textual (query) tokens accurately. We first employ the forward diffusion process (Ho\net al., 2020) to distort the image:\nf (vt | vt-1) = N (vt; \u221a1 - Yot-1, y1), f (vr | vo) = II f (vt | Vt\u22121),\n(4)\nt=1\nwhere I and vo denote an identity matrix and the original image, respectively. We gradually distort\nthe original image by adding the Gaussian noise for T steps and y controls the the amount of noise\nadded in each step. Subsequently, given a textual query x and an image input v, the model generates\ntwo logit distributions conditioned on different visual posteriors: the original v and distorted v*.\nBy contrasting the logit distributions obtained from these two conditions, we can get the contrastive\nprobability distribution of the i-th sample at time step t as follows,\nAlogit(yi,t Vi, V, Xi, Yi,<t) = logito(Yi,t|Vi, Xi, Yi,<t) \u2013 logito(Yi,t|Vi, Xi, Yi,<t), (5)\nwhere Yi,t and Yi,<t denote the token at time step t and the generated tokens sequence up to the time\nstept - 1 of the i-th sample, respectively. Subsequently, we can obtain the visual correlation weight:\nwi,t = Alogit(Yi,t|Vi, Vi, Xi, Yi, <t).\n(6)\nFollowing Xiao et al. (2024) to post-process and smooth the weights, we finally reassign the weight\nof each token in the vanilla MLE loss, which can be formulated as follows,\nCINJ =\nWit\nk=1 Wik\n\u2022logpo(Yi,t|Vi, Xi, Yi, <t),\nwhere l and w represent the length of textual tokens and the smooth weight, respectively.\n(7)"}]}