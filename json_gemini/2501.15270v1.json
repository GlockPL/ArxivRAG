{"title": "Inductive Biases for Zero-shot Systematic Generalization in Language-informed Reinforcement Learning", "authors": ["Negin Hashemi Dijujin", "Seyed Roozbeh Razavi Rohani", "Mahdi Samiee", "Mahdieh Soleymani Baghshah"], "abstract": "Sample efficiency and systematic generalization are two long-standing challenges in reinforcement learning. Previous studies have shown that involving natural language along with other observation modalities can improve generalization and sample efficiency due to its compositional and open-ended nature. However, to transfer these properties of language to the decision-making process, it is necessary to establish a proper language grounding mechanism. One approach to this problem is applying inductive biases to extract fine-grained and informative representations from the observations, which makes them more connectable to the language units. We provide architecture-level inductive biases for modularity and sparsity mainly based on Neural Production Systems (NPS). Alongside NPS, we assign a central role to memory in our architecture. It can be seen as a high-level information aggregator which feeds policy/value heads with comprehensive information and simultaneously guides selective attention in NPS through attentional feedback. Our results in the BabyAI environment suggest that the proposed model's systematic generalization and sample efficiency are improved significantly compared to previous models. An extensive ablation study on variants of the proposed method is conducted, and the effectiveness of each employed technique on generalization, sample efficiency, and training stability is specified.", "sections": [{"title": "1 Introduction", "content": "Language as a unique communication and thinking system allows the recombining abstract units to create new meanings in countless ways according to specific rules. This property, called compositional generalization or systematic generalization, underlies many of our cognitive abilities, including our ability to reason, plan, and imagine, and can improve generalization properties of deep architectures once incorporated effectively in models. Many investigations have been conducted based on this hypothesis to transfer the knowledge and structure of language to the deep models. \nIn reinforcement learning settings, language-informed studies aim to assist agents by incorporating natural language sentences as an additional input besides visual observation. By leveraging language, such agents can learn complex tasks more sample efficiently and generalize to unseen tasks more effectively. This is particularly useful in settings where the tasks are too complex to be defined by simple reward functions or where human guidance is necessary for the agent to perform well. It is known that effective learning in language-informed reinforcement learning depends on the agent's ability to ground linguistic concepts in the observation. While the compositional nature of the input language enhances generalization, it is not enough by itself to solve the benchmarked tasks.\nAlthough some recent studies have shown additional inductive biases such as modularity and sparse processing of information can help to boost the capacity for compositional generalization, these ideas have not already been employed in RL problems. Yet, language-informed RL studies only leverage techniques such as cross-attention, modulation or concatenation to fuse language with other raw inputs. In this study, we highlight the role"}, {"title": "2 Background", "content": "We build off the NPS, which is a neural versions of Production Systems, introduced in the late 1960s as a standard tool for describing how human beings think. A production system consists of some modular and abstract rules. Each rule is a pair of condition-action mechanisms, and its action applies to the input only when the corresponding condition is met. This framework provides sufficient conditions for representing knowledge through production rules. Recently, have modeled these rules in a neural way. More precisely, actions are specified with neural networks, mainly MLPs, and conditions are represented by vectors of trainable parameters. Thus, NPS is an end-to-end differentiable neural network involving inductive biases of production systems.\nNow, we describe the architecture of the NPS since it lies at the heart of our study. The NPS includes N modular rules $R_1, ..., R_N$ where $R_i = (r_i, MLP_i)$ and maps the input $x_t$ to a set of entities or slots $V_1, ..., V_M$. Then for a specific slot, called the primary slot ($V_p$), a rule is selected to be applied on through a competitive bottleneck resulting from the attention mechanism. More precisely, to select a rule for the primary slot $V_p$, we consider\n$\\begin{aligned}\nq_p &= V_pW^Q\\nk_i &= r_iW^K \\, (i = 1, ..., N)\\nr &= \\underset{i}{\\operatorname{arg max}}(q_p \\cdot k_i + y) \\quad y \\sim \\operatorname{Gumbel}(0, 1)\\n\\end{aligned}$\nwhere the $q_p$ is the query, $W^Q$ and $W^K$ are projection matrices, and the $k_i$s are keys of attention in Eq. 3 which is a noisy rule matching. More- over, to apply the selected rule $r$ on the slot $V_p$, in addition to $V_p$, the related context as a contextual slot $V_c$ which is specified using another attention mechanism, is also fed to $MLP_r$. In fact, this contextual slot is found through the attention formulated as\n$\\begin{aligned}\nq_p &= V_pW^Q\\nk_j &= V_jW^K \\, (j = 1, ..., M)\\nc &= \\underset{j}{\\operatorname{arg max}}(q_p \\cdot k_j + \\gamma) \\quad \\gamma \\sim \\operatorname{Gumbel}(0, 1)\\n\\end{aligned}$\nwhere $W^Q$ and $W^K$ are projection matrices for context selection attention, according to. The primary slot concatenated with the contextual slot passes through the $MLP_r$, as below\n$out_p = MLP_r(V_p V_c)$\nwhere the $out_p$ can be used to modify the state of the primary slot or passed down through the network.\nThe process of applying rules might be parallel or sequential. In the parallel case, for each slot, one rule is selected and applied simultaneously at the current time step,"}, {"title": "3 Proposed Model", "content": "3.1 Problem Formulation\nIn this study, we are interested in multi-task instruction following sequential decision- making settings in which a natural language instruction describes the agent's goal in a partially observable environment. Formally, we are trying to solve an augmented POMDP defined by the tuple (S, A, O, , T, R, G, ) in which S is the state space, A is the action space, O is the observation space,  : S  O is an observation mapping function, T : S  A  S is the state transition function, R is the reward function for reinforcement learning setup, and  is the discount factor. Alongside these usual components in the POMDP definition, G also contains all possible instructions for the environment in the augmented POMDP.\nWe consider a multi-task setting where each task is recognized by a pair of initial state, s0, and goal instruction, g. All MDP components are shared across tasks except R, which is affected by the task itself: R : S  A  S  G  R. Finally, we attempt to learn a return-maximizing policy (at|ot,g) which is conditioned on the instruction. In our experiments, we define a compositional split on G to divide it into two disjoint sets, Gtrain and Gtest, to assess the systematic generalizability of the proposed tech- niques. During training, the agent only sees instructions from S  Gtrain whereas tests are performed on tasks only inside S  Gtest. So, Gtest contains tasks which remain unseen during training to assess the zero-shot performance of the agent. Because of the compositional nature of the language, we expect that the model more effectively generalizes to unseen tasks by using prior knowledge included within the instructions.\n3.2 Architecture\nThis study explores architecture-level inductive biases for compositional generalization in reinforcement learning. We choose NPS -described in Section 2- as the base model for our inductive biases. The modularity and sparsity of interactions between entities manifested by context selection for each primary slot are well-suited for our purpose of grounding natural language instructions in the agent's representation of the world.\nIn the rest of this section, we describe the overall architecture of the model based on NPS described in Section 2 in which for an observation of consisting of slots  = {,..., , we input these slots to the model. According to Fig. 1, the output of the NPS for , i.e., , passes through a recurrent neural network called memory to obtain  from the previous memory state  and the encoding of the observation"}, {"title": "4 Experiments", "content": "In this section, we explain our experimental setup (Section 4.1) and results (Section 4.2). Further analyses are stated in Discussions, Section 5.\n4.1 Setup\nOur problem setup consists of the benchmark for systematic generalization defined on BabyAI environment with an additional train/test split (Section 4.1.1), the evaluation metrics (Section 4.1.2), the baseline models (Section 4.1.3) and the ablation models (Section 4.1.4), each described separately in the following parts.\n4.1.1 The Benchmark for Language-informed Systematic\nGeneralization\nHere, we explain the benchmark for our experiments. The environment of interest in this work is BabyAI. Since this study focuses on language-informed systematic gen- eralization, we need a language-informed environment in which rich and controllable combinations of subtasks are possible. Compared to other environments described in Section 6, BabyAI quite satisfies these requirements, and therefore, we choose to evalu- ate our method on this environment. Baby AI contains 19 procedurally-generated levels in a grid-world environment. For each level, a set of natural-looking instructions from context-free grammar specify the desired goal. The observations in this environment are mainly partial and symbolic 7  7  3 first-person views. Each entry in a grid cell indicates its entity's type, color, or status, offering a factorized input that makes the learning process much more computationally efficient. This observation space aligns with the theory of systems 1 & 2, separating the entity percep- tion problem from the reasoning required to solve the task. Doing so creates a suitable and logically rich test bed for solely assessing the reasoning ability of the model.\nGiven the compositional nature of language, we can define our evaluation protocol, i.e., train/test split of tasks, based on different combinations of possible factors of variation per level, as encouraged by. The BabyAI environment does not readily include this separation, and train/test splits are typically created based on random seeds. However, since each seed corresponds to a unique pair of (initial state, instruction), adding a filter on seeds to store them for specific instructions is a straightforward way to build the systematic split based on the different combination of features, instructions, and entities. The systematic split for each environment is stated in Table 1. This split is defined based on matching strings inside the instruction; i.e. if the instruction contains any of the specified strings, its seed is going to be reserved for test, otherwise the generated episode is used during training."}, {"title": "5 Discussion", "content": "Regarding Fig. 2, the performance gap between ICMO and the baselines is significant. However, previously most involved language-observation fusion structure, i.e. FiLM indicates very poor performance especially on the test split. Comparison to Raw model indicates consistent superiority of ICMO which might arise from meaningful processings carried out by the model. These processings ground the language in memory due to IC-M part, leading to representations that accumulate the history of agent's observations combined with the language description of its goal. In terms of GG in Table 2 which directly describes the compostional generalization capability of the models, ICMO is the only model that shows near-zero gap whereas in the other models, this gap is meaningful. Moreover, the proposed model manages to reach a test SR of 0.9 and preserve it during training in most environments, while the baselines fail to do so.\nIn terms of language participation, from Table 3 and Fig. 3, we can conclude that IC-M and IC-AC are superior compared to IC-Input which can indicate that language involvement in later layers of the model is more desired and the observations need to be processed before alignment with language. Also, by observing the learning curves in Fig. 2 and Fig. 3, we can conclude that early language fusion (as in FiLM-BabyAI and IC-Input) worsens the generalization gap, confirmed by Tables 2 and 3. When combined with memory feedback (See Table 4 and Fig. 4), passing the instruction embeddings to actor-critic networks instead of the memory, deteriorates the perfor- mance of the model, suggesting an effective role for the language in shaping the agent's memory such that it can be used in mid-level processings which determine the acti- vation of inner modules, i.e. rules, or the participation of inner representations e.g. contextual slots in a selective way.\nThe memory ablations reported in Fig. 4 and Table 4 confirm that 1) involvement of language as an input to the memory is helpful, and 2) adding memory feedback boosts the agent's performance as well as its training stability (Compare ICMO and IC-M-FR to IC-M in plots 4f and 4c). Feedback to rule selection (IC-M-FR) and to contextual slot selection (IC-M-FC) indicate close performances, but the latter seems"}, {"title": "6 Related Work", "content": "6.1 Language-informed Studies\nThere have been various language-informed studies in the sequential decision-making setting. have provided a survey on language-informed studies in RL, categorizing them into language-conditioned methods, where the language is a part of the main problem for- mulation and its involvement is mandatory like instruction following settings and language-assisted methods where the task can be solved without language information, but it can be solved easier using linguistic information. The participation of the language modality in sequential decision-making settings has been done either by conditioning the policy on language"}, {"title": "7 Limitations and Broader Impact", "content": "Our study aims to enhance instruction-following RL agents to systematically generalize to unseen tasks by leveraging the compositional nature of language. We propose ICMO, a modular architecture with sparse interactions among the network components and the inputs along with memory feedback to improve language grounding in the agent. As stated in Sections 3.2 and 6, pieces of evidence from neurocognitive science support these inductive biases as they resemble some functionalities of the brain.\nIn more realistic domains, successful language grounding allows better human- in-the-loop control and human-robot interaction. Although ICMO was experimented against the symbolic BabyAI environment, it emphasizes modularity, sparse interac- tions, and the role of memory in designing such agents. So, in realistic scenarios, it could promote the reasoning functionalities of the agent.\nAlthough our method is tested against symbolic inputs, it does not make any assumptions about the input structure and can be modified to handle larger obser- vation spaces. Even if the slots are key to its success, one can obtain such high-level and factorized representations using pre-trained encoders for downstream tasks. Slot- Attention or DINOSAUR could be candidates here. Also, there is a line of studies in the language-informed sequential decision-making literature that focus on symbolic environments. Following this line, we propose inductive biases to improve the related baselines.\nSince this paper introduces techniques to improve RL agents on a fundamental level, we don't expect any negative societal impacts."}, {"title": "8 Conclusion", "content": "We have introduced ICMO, a modular encoder model with sparsely-connected units and a language-conditioned memory which sends task-relevant feedbacks to the mid- level processing of the observations. We have tested this model in the zero-shot systematic generalization setting. We compared our method on several challenging tasks in BabyAI environments with strong baselines. Our model could significantly improve systematic generalization and training stability by involving memory feed- back in sparse processing of the observation via modular units, and conditioning the memory on language. Besides the inductive biases introduced in this study, there are several future directions which can further improve the current results. Using auxiliary loss functions to induce certain restrictions in the model could be helpful. Employing information bottlenecks in the form of regularization potentially can be effective in generalization. Moreover, one can try scenarios with a richer language modality (e.g. descriptive sentences, wikis, etc.) using ICMO and involve different texts (instructive, descriptive, guidance, etc.) using the proposed techniques to maximize information utilization in the agent."}, {"title": "9 Declarations", "content": "Funding: The authors did not receive support from any organization for the submitted work.\nConflicts of interest/Competing interests: The authors have no competing interests to declare that are relevant to the content of this article.\nEthics approval:Not applicable.\nConsent to participate: Not applicable.\nConsent for publication: Not applicable.\nAvailability of data and material: We do not analyse or generate any datasets, because our work proceeds within a fundamental approach examined on publicly available benchmarks.\nCode availability: Our code is publicly available at\nAuthors' contributions: All authors contributed to the conception of the work. N.H.D., R.R.R., and M.S.B. were involved in designing the study and analyzing the results. N.H.D. played a primary role in designing the experiments, implementing the codes, and analyzing the results. N.H.D. drafted the manuscript, and all authors critically reviewed and revised it."}]}