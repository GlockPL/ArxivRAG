{"title": "Retrospective Comparative Analysis of Prostate\nCancer In-Basket Messages: Responses from\nClosed-Domain LLM vs. Clinical Teams", "authors": ["Yuexing Hao", "Jason M. Holmes", "Jared Hobson", "Alexandra Bennett", "Daniel K. Ebner", "David M. Routman", "Satomi Shiraishi", "Samir H. Patel", "Nathan Y. Yu", "Chris L.\nHallemeier", "Brooke E. Ball", "Mark R. Waddle", "Wei Liu"], "abstract": "In-basket message interactions play a crucial role in physician-patient communication, occurring during all phases (pre-, during,\nand post) of a patient's care journey. However, responding to these patients' inquiries has become a significant burden on\nhealthcare workflows, consuming considerable time for clinical care teams. To address this, we introduce RadOnc-GPT,\na specialized Large Language Model (LLM) powered by GPT-4 that has been designed with a focus on radiotherapeutic\ntreatment of prostate cancer with advanced prompt engineering, and specifically designed to assist in generating responses.\nWe integrated RadOnc-GPT with patient electronic health records (EHR) from both the hospital-wide EHR database and an\ninternal, radiation-oncology-specific database. RadOnc-GPT was evaluated on 158 previously recorded in-basket message\ninteractions. Quantitative natural language processing (NLP) analysis and two grading studies with clinicians and nurses were\nused to assess RadOnc-GPT's responses. Our findings indicate that RadOnc-GPT slightly outperformed the clinical care\nteam in \"Clarity\" and \"Empathy,\" while achieving comparable scores in \"Completeness\" and \"Correctness.\" RadOnc-GPT is\nestimated to save 5.2 minutes per message for nurses and 2.4 minutes for clinicians, from reading the inquiry to sending the\nresponse. Employing RadOnc-GPT for in-basket message draft generation has the potential to alleviate the workload of clinical\ncare teams and reduce healthcare costs by producing high-quality, timely responses.", "sections": [{"title": "1 Introduction", "content": "In-Basket is the online portal messaging system integrated within Epic Applications functioning similarly to email for\ncommunication between patients and their clinical care team. In-basket messaging system is often used to exchange messages\nregarding patient concerns, appointments, and follow-up care, particularly when real-time communication is not possible.\nDuring or following treatment, patients may not always receive immediate support from their care team. Patients with limited\nclinical literacy and understanding still need to communicate with healthcare professionals for various needs, including disease\nmonitoring, medication, appointments, and billing or insurance issues. In this context, the In-basket serves as a vital tool to\nbridge the communication gap between patients and clinical professionals\u00b9.\nHowever, clinical care teams struggle to draft responses on time due to the increasing complexity of patients' supportive\ncare needs\u00b2. Several studies have shown that an increased workload from responding to in-basket messages can negatively\nimpact clinicians' burnout rates and overall well-being2\u20135. Further, patient messaging volumes increased by more than 50%\nafter COVID-19, placing an undue burden on clinical teams6\u20138. Though these added avenues of communication are beneficial,\ngenerally responding to these messages is non-reimbursable as well9,10.\nSince In-basket messages often contain important real-world concerns from patients, the text-based in-basket message\ndataset is valuable for demonstrating patient-centered interactions. We propose using large language models (LLMs), which\nare connected with the electrical health record (EHR) system, to provide timely and layman-friendly responses to various\ncategories of In-basket message inquiries11\u201316. LLMs have already shown strong technical capabilities in clinical context\nlearning, summarization, response generation, decision-support, and Q&A17\u201329. Here, we aim to evaluate the performance of\nLLM and clinical care teams in three key areas: 1) capturing and interpreting all sources of data, 2) generating personalized and\nprompt responses, and 3) upholding high clinical standards in terms of completeness, correctness, clarity, and empathy.\nRather than applying LLMs across all types of disease sites, we focused on prostate cancer patients who received treatment\nat the Mayo Clinic's radiation oncology department. Specializing in a specific field allows the LLM to generate more accurate\nand relevant responses. We developed RadOnc-GPT, an OpenAI GPT-40-powered LLM, which is integrated with EHR30. Since\nmany in-basket messages require external context for proper understanding and interpretation, RadOnc-GPT can generate more"}, {"title": "2 Method", "content": "This was a retrospective study approved by the Institutional Review Board of the Mayo Clinic. Our study included patients\nwith prostate cancer who were managed at Mayo Clinic (Rochester, MN) in the calendar years 2022-2024. RadOnc-GPT is a\nRetrieval-Augmented Generation (RAG) system that connects with both the hospital wide electrical medical record database,\nEpic, developed by Epic Systems, and the radiation oncology specific database, Aria, developed by Varian Medical Systems.\nThe data RadOnc-GPT may access includes clinical notes, radiology notes, pathology notes, urology notes, radiology reports,\nradiation treatment details, diagnosis details, patient details (demographics), in-basket messages, and more. RadOnc-GPT is\nable to retrieve data by way of specifying the patient ID and which dataset to retreive to the backend system. Once retrieved,\nthe data is inserted into the conversation history.\nSubject demographic information retrieved from the EHR system included sex, age, race, ethnicity, preferred language, and\nthe attending physician's name. Information collected from Aria included demographic information (sex, age, race, ethnicity,\npreferred language, and the attending physician's name), prostate cancer treatment-specific information (course description,\nplan intent, treatment orientation, radiation type, radiation oncology machine type, number of fractions, dose prescription, dose\ndelivered, radiation technique, and treatment duration), and diagnosis details (cancer stage, ICD (International Classification\nof Diseases) diagnosis code and code type, onset date). Information collected from Epic included clinical notes, ordered by\ndate. For RadOnc-GPT, the information retrieval order starts with patient demographic details, followed by treatment details,\ndiagnosis details, and lastly, clinical notes.\nTo ensure every patient inquiry was consistent and under the same GPT generation environment, we developed a GUI\ninterface for RadOnc-GPT that was re-initialized for each test. This approach ensured that RadOnc-GPT did not generate\nbiased responses from its memory of the previous patient's pair of inquiries and responses.\nOur study's evaluation was divided into two main components: natural language processing (NLP) quantitative assessments\nand clinical professional grading, as illustrated in Figure 1.\nFor NLP evaluation, we performed four types of measurements31,32: natural language understanding, reasoning, context\nreadability, and natural language generation.\nFor the grading study, we focused on six dimensions of evaluation33\u201335:\n1. completeness (ranging from 1-5, the higher the better),\n2. correctness (ranging from 1-5, the higher the better),\n3. clarity (ranging from 1-5, the higher the better),"}, {"title": "3 Results", "content": "In-basket message interactions can often be disorganized. Without a standardized format for patient inquiries, under one\nsubject, patients may send multiple messages for a single issue or combine several unrelated questions into one message. This\nmakes it difficult to categorize the messages, as they frequently span multiple categories. Additionally, a single thread may\ninclude several conversation pairs, where a pair is defined as one or more patient inquiries followed by one or more care team\nresponses in a time-sequenced manner. For inclusion in our evaluation dataset, each patient-clinician conversation pair must\nhave consisted of one patient inquiry and one care team response."}, {"title": "3.2 NLP Analysis", "content": "To understand the sentiment differences from human care team and the RadOnc-GPT, we conducted TextBlob and VADER\nanalysis. In the TextBlob Sentiment Distribution, RadOnc-GPT responses are observed to skew towards a more\npositive sentiment, with the majority of responses clustering around a sentiment score of 0.25. In contrast, human care team\nresponses present a more evenly distributed sentiment profile, with a significant concentration around the neutral score of 0\n(grey line in Figure 3 (A)). RadOnc-GPT responses tend to be more positive, whereas Care Team responses consist of a broader\nspectrum of sentiments, including neutral and negative tones. The VADER Sentiment Distribution provides\nfurther insight into these differences. The box plot reveals that RadOnc-GPT responses exhibit a high median sentiment score,\nnearing 1.0, indicative of a predominantly positive sentiment. However, there are notable outliers reflecting occasional negative\nsentiment. Clinical care team responses, by comparison, display a wider range of sentiment scores, with a lower median,\nindicating a more varied and contextually nuanced sentiment expression. Our sentiment analysis collectively suggests that\nwhile RadOnc-GPT responses are generally more positive, care team responses offer a more balanced sentiment distribution,\nreflecting a greater sensitivity to the contextual nuances of the input data."}, {"title": "3.2.2 Natural Language inference (NLI) analysis", "content": "To understand how human care team and RadOnc-GPT responses' inferences with the patients' inquiries, we conducted an\nNLI analysis36. RadOnc-GPT responses were predominantly Neutral, with 92.41% of responses in this category, suggesting\na tendency towards generalized statements. In contrast, clinician responses were more varied, with 70.25% Neutral and\n29.11% Entailment, indicating greater relevance and specificity. Both response types showed low contradiction rates, though\nRadOnc-GPT responses had a slightly higher rate at 3.16%, which may point to occasional inconsistencies. The NLI label\ndistribution comparison is shown in Figure 3 (C).\nComparing the semantic similarity37 between RadOnc-GPT and human care team responses provided additional context,\nshowing a mean similarity score of 0.85 between RadOnc-GPT and human care team responses. This high score indicated a\nstrong alignment in content, even though RadOnc-GPT responses are generally more neutral. The findings suggested that while\nRadOnc-GPT responses may lack the specificity found in human care team responses, they still captured the core semantic\ncontents, reflecting contextually relevant information. Figure 3 (D) shows the distribution of the semantic similarity scores."}, {"title": "3.2.3 Readability Scores", "content": "We compared the average readability scores across several indices, comparing patient inquiry, RadOnc-GPT, and clinical care\nteam responses. The Flesch Reading Ease scores38,39, where higher values indicate easier readability, showed that the human\ncare team responses were the most accessible (66.2), followed by RadOnc-GPT (59.9). This suggested that human care team\nresponses were slightly easier than RadOnc-GPT to read. For the Flesch-Kincaid Grade Level, Gunning Fog Index, SMOG\nIndex, Automated Readability Index, and Coleman-Liau Index, lower scores indicated better readability. Across these metrics,\nhuman care team responses consistently scores lower than RadOnc-GPT, implying that human care team are written at a lower\nreading level and are easier to understand. RadOnc-GPT and Human Care Team responses, while similar across these indices,\ngenerally reflect higher complexity, particularly in the SMOG Index and Gunning Fog Index.\nThe relationship between word counts and sentence counts in RadOnc-GPT, and human care team responses exhibited a\npositive correlation (Figure 4). RadOnc-GPT responses tended to use more words per sentence than clinic care team responses.\nThe steeper slope of the RadOnc-GPT regression line indicated that RadOnc-GPT responses became more verbose as the\nsentence count increases. Human care team responses were more clustered at lower word and sentence counts, reflecting a\nmore concise communication style. Figure 4 clearly distinguished GPT's verbosity from the clinic care team's brevity.\nOn average, RadOnc-GPT responses were more detailed, with about 135 words and 9 sentences per response. Human care\nteam responses, while similar in length to RadOnc-GPT responses, average around 132 words and 7 sentences per response,\nindicating that care team responses were slightly more concise in terms of sentence structure."}, {"title": "3.3 Clinician Grader Study", "content": "In the single-blinded grader study, two clinician graders first graded all 316 responses (158 human care team responses, 158\nRadOnc-GPT responses). The average of two graders' results showed that GPT consistently performs better in \"Empathy\"\nand \"Clarity', while human responses show higher averages in \"Completeness\" and \"Correctness\". The grading rubrics are\ndisplayed in Appendix 12.3."}, {"title": "3.3.1 Time Comparison", "content": "The nurse graders study focused solely on two criteria: \"Can you answer this patient inquiry?\" and \"Estimated time to answer\nthis patient inquiry.\" We compared the clinician graders' estimated times to those of the nurses. On average, clinicians took\n3.60 minutes (SD 1.44) to respond to an in-basket message, compared to the nurses' 6.39 minutes (SD 4.05). While both\nclinician graders were able to answer all 158 messages, nurses indicated \"No\" for 90 inquiries, requiring referral to clinicians,\nand \"Yes\" for 68 inquiries. For the inquiries marked \"Yes,\" the average response time was 5.54 minutes, and for those marked\n\"No,\" the average time was 8.83 minutes. Even though nurses may struggle with some inquiries, they still need to conduct\nproper research and gather relevant patient information to determine whether the in-basket message should be escalated to\nclinicians."}, {"title": "4 Discussion", "content": "RadOnc-GPT was well able to provide medical advice to individualized patient In-basket messages on this retrospective\ncomparison study to both trained radiation oncologists as well as radiation-oncology-specific nurses. Although RadOnc-GPT\nresponses are human-like and generally similar to responses generated by the original human care teams in many aspects,\ncaution is still needed before deploying its messages without human oversight in real-world healthcare settings.\nSince human care team may still need to confirm the evidence in responses by pulling out the imaging or lab/exam results\nto avoid hallucination, RadOnc-GPT may be able to accelerate the response turn-out rate and alleviate the human care team's\nresponse pressure.\nOur study observed that the human care team responses typically addressed the immediate action items to instruct patients\nwhat to do next. The care team seldom provides sufficient patient education, clinical concepts clarifications, or informed\nexplanations. As RadOnc-GPT responses provided more information that clinical care team's responses might not include,\nRadOnc-GPT pushed non-expert patients to gain more expertise. While RadOnc-GPT prepared a draft in-basket message\nresponse, clinicians went from Experts to Judges. The shift of both patients' and clinicians' roles and expertise in healthcare"}, {"title": "4.1 Prompt Engineering", "content": "We considered prompts to be one of the key factors determining the quality of RadOnc-GPT responses. For the final prompts,\nwe provided instructions in 1) steps of retrieving information to ensure responsibility; 2) acting as the attending physician\nand provider; 3) step-by-step reasoning from patient health profiles to address patient's inquiries; 4) handling the medications\n(prioritizing over-the-counter medications); 5) determining the clarity of patient's inquiry and asking for more information if\nneeded; 6) patient's health literacy; 7) providing the original patient's inquiry. The full prompts were presented in the appendix\n12.2.\nAdditionally, there is a lack of standardized scales or metrics to evaluate the GPT-generated messages. A few studies\nhave included clear evaluation methods and scoring rubrics for grading. However, the studies in the medical domain are quite\nspecific, and researchers found it challenging to generalize the grading across all types of medical domains or diseases. Also,\nin the reader study, which included human evaluators, the subjective grading could potentially introduce bias from years of\nexperience, practicing domain, clinical roles, and clinics."}, {"title": "4.2 Economic Potential Impacts", "content": "The average of this 158 patient inquiry messages wait time for clinical care team response is 22.42 hrs (sd = 32.83, median\n= 11.73 hrs), as shown in Figure 9.\nThe purpose of using RadOnc-GPT to generate in-basket message response was not to replace the human care team's role\nin managing the prostate cancer patients' inquiries. Instead, RadOnc-GPT was intended to streamline the response process and\nsave time for the care team. Typically, responses to in-basket messages were handled sequentially, starting with nurses, then\nprogressing to nurse practitioners or APPs, and finally to clinicians.\nBased on our estimation, using RadOnc-GPT to assist in in-basket messages generation, average words in patient inquiries\nwere 88.89 (SD: 64.93), estimated reading time (for an average English reader 175 words per min) would be 0.51 min for each\nmessage (SD: 0.37 min). GPT response average words were 119.55 (SD: 49.72), with an estimated reading time of 0.68 min\n(SD: 0.28 min). The clinical professionals review time would be 1.19 min for each message. Based on the clinicians and nurses\nestimation, RadOnc-GPT could save approximately 5.2 minutes per message for nurses and 2.41 minutes for clinicians, from\nreading the patient inquiry mesage to drafting and sending the response. With Mayo Clinic receiving around 5,000 in-basket\nmessages daily40 and assuming that one-fifth of these are requests for medical advice (which is 1000 messages), the potential\ntime savings for nurses alone would amount to 5200 minutes (or 86.67 hours) per day. Based on the NIH salary table\u00b9, this\nequates to an annual savings of at least $2.28 million in nurse time ($72 per hour). 2"}, {"title": "5 Limitation", "content": "The retrospective study feature limited our study since we can't ask the patients to add more information or reply to the\nRadOnc-GPT generated responses. We only compared a pair of interactions under one subject, which consists of a patient\ninquiry and a response message from either RadOnc-GPT or the clinical care team. It might deviate from the real-world\ninteraction since sometimes either the clinical care team or patients send out multiple messages under one subject to explain\ntheir health concerns. Additionally, RadOnc-GPT processes only text and is currently unable to handle images or files. While\nno images or files were involved in the in-basket message grading, interpreting such information typically takes longer than\nreading text messages.\nAlthough RadOnc-GPT generated responses can be comparable to clinical care team responses, this was a retrospective\nstudy, and the human care team's responses were affected by multiple factors (i.e., different care team roles' response, the busy\ntime, clinical department), and likely the responses were not the best from the clinical care team.\nAnother limitation is that we only use GPT-40 as the backend LLM for RadOnc-GPT to generate responses. We didn't\ncompare our backend engine GPT-40 with other LLMs such as LLama 3, Gemini, GPT-4 or GPT-3.5. The performance based\non GPT-40 may not generalize to other LLMs."}, {"title": "6 Conclusion", "content": "In this single-blinded comparison study, we evaluated 158 in-basket message interactions between RadOnc-GPT and clinical\ncare teams. The results demonstrated RadOnc-GPT's ability to answer patient inquiries, though we observed limitations in its\ncapacity to capture the nuanced information that clinical professionals provide. Utilizing RadOnc-GPT as a foundational tool\nfor generating in-basket message responses allows clinical professionals to serve more as reviewers than primary authors. This\napproach not only saves time and improves workflow efficiency but also enables clinicians to be more comprehensive in their\nresponses and to focus more on the direct patient interaction care. Future studies should further explore the limitations of LLMs\nin assisting with in-basket message generation."}, {"title": "7 Data Availability", "content": "The authors declare that the data supporting the findings of this study are available upon request. The dataset is not public because\nit contains patient health information (PHI). However, sample data consisting of 18 pairs of patient inquiries and responses, with\nPHI removed, is available on GitHub: https://github.com/YuexingHao/In-Basket-Message-Evaluation/blob/main/In-Basket-QA-\nDataset.xlsx."}, {"title": "8 Code Availability", "content": "The code is available on GitHub: https://github.com/YuexingHao/In-Basket-Message-Evaluation"}, {"title": "10 Author Contributions", "content": "Y.H., J.M.H., M.R.W., and W.L. conceptualized the study. Y.H. and J.M.H. were responsible for data collection, data\npreprocessing, model development, and validation. J.H., A.B., D.K.E., S.S., S.H.P., B.E.B., C.L.H., and M.R.W. offered\nexpertise in clinical grading studies and interpreted the results. Y.H. J.M.H., D.M.R., N.Y.Y., B.E.B., D.K.E., M.R.W., and W.L.\ninterpreted the experimental results and provided feedback on the study. All authors contributed to writing the manuscript and\nreviewed and approved the final version. The study was co-supervised by M.R.W. and W.L."}, {"title": "11 Competing Interests", "content": "The authors declare no competing interests."}, {"title": "12 Appendix", "content": "From the instructions above, we tested several different prompts and finally used this as our final prompt: \"Patient #ID has sent\nan in-basket message. Please generate a response to their message. Before generating the response, first retrieve the patient\ndetails, patient treatment details, patient diagnosis details, and patient clinical notes. Do not pull the in-basket messages.\nRetrieve all types of patient data simultaneously. In writing your response, feel free to make recommendations as if you were\nthe attending physician (since your response will be approved by the attending physician). When handling prescriptions,\nprioritize over-the-counter if appropriate. Sign off as the attending physician. Do not mention that the patient should contact\ntheir provider, since you are acting as their provider. Prior to giving your response, explain your reasoning step by step in an\nalysis section. As part of your analysis, indicate whether the patient has provided enough information to adequately respond\nto the message. If you determine that the patient has not provided enough information, please ask for more information in your\nmessage to the patient. Assume the patient has a high school education level. Here is the in-basket message that you should\nrespond to: Message Details.\""}]}