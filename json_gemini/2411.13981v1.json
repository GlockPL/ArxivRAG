{"title": "On the Fairness, Diversity and Reliability of Text-to-Image Generative Models", "authors": ["Jordan Vice", "Naveed Akhtar", "Richard Hartley", "Ajmal Mian"], "abstract": "The widespread availability of multimodal generative models has sparked critical discussions on their fairness, reliability, and potential for misuse. While text-to-image models can produce high-fidelity, user-guided images, they also exhibit unpredictable behavior and vulnerabilities, which can be exploited to manipulate class or concept representations. To address this, we propose an evaluation framework designed to assess model reliability through their responses to globally- and locally-applied 'semantic' perturbations in the embedding space, pinpointing inputs that trigger unreliable behavior. Our approach offers deeper insights into two essential aspects: (i) generative diversity, evaluating the breadth of visual representations for learned concepts, and (ii) generative fairness, examining how removing concepts from input prompts affects semantic guidance. Beyond these evaluations, our method lays the groundwork for detecting unreliable, bias-injected models and retrieval of bias provenance. We will release our code.", "sections": [{"title": "1. Introduction", "content": "Generative models are among the most popular applications of modern artificial intelligence (AI), drawing significant attention within public and research domains. Rising global demand and availability of public software have sparked discussions on model fairness and reliability [13, 16, 25, 37, 42, 43]. Ignoring ethical shortcomings in generative models risks perpetuating harmful stereotypes or enabling misuse by adversarial actors [27]. Regardless of intent, these models can exhibit biases and unfair behavior due to training on large, uncurated datasets [1, 9, 12, 15, 27]. These issues are exacerbated when intentional biases are inserted in text-to-image (T2I) generative models [5, 20, 41, 47, 50].\nThe fairness and reliability of a model can be partially attributed to its learned biases [12, 13, 16, 36, 42]. Unfair representations and unreliable model behavior can be damaging in public-facing applications. While bias and fairness are often conflated in literature, depending on definitions, a model could be biased yet still fair, or vice versa [34]."}, {"title": "2. Related Work", "content": "Text-to-Image Models are traditionally based on probabilistic diffusion architectures [18, 38\u201340], modified to leverage textual data for user-guided image synthesis, where the text input serves as a conditioning variable to guide diffusion and generate a target sample from a known distribution. However, these learned distributions can be unreliable, unfair, or lack diversity as a result of training [27] The latent diffusion model [31] serves as the technical foundation for stable diffusion, taking inspiration from DALLE-2 and Imagen [30, 35]. Text-guided generation can be achieved through embedded language and generative networks that apply attention mechanisms, based on the seminal Transformer work introduced in [45]. The CLIP model is a modified Transformer with masked self-attention, often deployed for text-encoding in T2I models [29]. For conditional image synthesis, lightweight T2I models often exploit the popular U-Net architecture [32], which employs multi-headed cross-attention [22, 32].\nBias, Fairness and Reliability Evaluations are necessary given the growing popularity of T2I models. Across literature [6, 8, 9, 19, 23, 24, 42, 46], there is no universally-accepted evaluation tool, with many methods relying on auxiliary captioning and VQA models (which may also be biased/unreliable). Cho et al. proposed DALL-Eval to assess T2I model reasoning skills and social biases [9]. Similarly, the StableBias method assesses gender and ethnic biases [23], with both [9, 23] leveraging captioning and/or VQA models. The OpenBias, TIBET and FAIntbench frameworks [8, 12, 24] all leverage LLM and VQA models to recognize wider dimensions of bias and 'open-set' corpora of potential biases relevant to input prompts [8]. Teo et al. define fairness as equivalent to generative bias [42] and focus on solving measurement errors in sensitive attribute classification, proposing a statistically-grounded, CLassi-fier Error-Aware Measurement (CLEAM) as an alternative [42]. The Holistic, Reliable, and Scalable (HRS) Benchmark evaluates a wide range of T2I model elements ranging from bias to fidelity and scene composition - using these to assess reliability [6]. Hu et al. proposed 'TIFA' for fine-grained evaluation of T2I-generated content using a VQA model to extract 12 generated image (and model) statistics [19]. Many related works focus on social bias aspects of generative models. Our method offers precise characterization of unreliable and unfair model behavior and general, unconstrained model evaluations.\nIntentionally-biased Text-to-Image Models manipulate image generation processes when exposed to specific input triggers. In [7] expose diffusion model training data to in-distribution, out-of-distribution, and one-specific instance-based biases to formulate an array of attacks. Similarly, Chou et al. apply a many-to-one mapping to manipulate training and forward diffusion processes to force deterministic behavior upon detection of a trigger [10]. Huang et al. propose using personalization for bias injection, defining a nouveau-token backdoor in a target text-encoder to influence generation upon detection of a trigger [20]. Vice et al. propose a depth-wise, fine-tuning based bias injection method 'BAGM\u2019, that priorities manipulation of common objects rather than generating irrelevant content when exposed to an input trigger [47]. Rare trigger tokens have been exploited for object and style manipulation-based bias injection methods as evidenced by the target-prompt attack (TPA) proposed in [41] and the BadT2I method [50].\nBias Detection and Trigger Retrieval methods are essen-"}, {"title": "3. Methodology", "content": "Text-to-image models are typically comprised of a: (i) tokenizer, (ii) text-encoder e.g. CLIP, and (iii) text-conditioned denoiser (typically U-Net). Our evaluations assume a grey-box setup [2], i.e., requiring access to the input prompts, generated images and the text-encoder embedding outputs - which are critical for applying semantic perturbations."}, {"title": "3.1. Conditional Image Generation", "content": "Let us define a tokenized input prompt as 'x'. We represent embedded multi-modal networks as operative functions within the T2I pipeline. The pre-trained text-encoder 'fE()' projects the tokenized input x onto an n \u00d7 d dimensional embedding space such that:\n$x \\in \\mathbb{R}^{n \\times d} = f_E(x, \\theta_L),$ (1)\nwhere $\\theta_L$ describes learned network parameters. The conditional generative model, say \u2018fG(\u00b7)' uses the embedding vector x during latent reconstruction steps, applying a guidance factor \u03b3 to generate a text-guided image Ix from an initial Gaussian noise distribution INo. Through conditional latent reconstruction steps, the generated image guides toward a visual representation of the encoded prompt x, supplemented by \u03b3, resulting in a final image output Ix \u21d2 t \u2208 T. Thus, the generative model can be described by:\n$I_t = f_G(x, \\theta_g, \\gamma, I_{t-1}, t) \\forall t \\in \\mathbb{T}.$ (2)"}, {"title": "3.2. Semantic Perturbations and Model Reliability", "content": "Fundamentally, our semantic perturbations 's' are inspired by adversarial attacks [2, 26] given that we perturb the input to intentionally manipulate downstream behavior. Unlike the adversarial attack literature where perturbations are applied with malicious intent, we propose a positive application i.e., for quantifying unreliability in T2I models.\nLet RG and R\u2081 define the global and local reliability of a T2I model, respectively, which is derived from the sensitivity of the model to semantic perturbations applied to projected embeddings. We visualize a representative example of applying s in Fig. 2. When applied globally, s takes the form of an n \u00d7 d vector, perturbing the embedding x along all n \u00d7 d occupied dimensions in a single transformation (see Fig. 2 (left)). For the local case (Fig. 2 (right)), given a prompt obtained as part of the RG experiments which caused unreliable model behavior, we iteratively apply s to each occupied dimension in x to measure the sensitivity of the corresponding token xi \u2208 x. So within the higher-dimensional, global context of the prompt x, we can identify local data points that cause unreliable model behavior i.e., RL. For global and local semantic perturbations, we derive the bounds of s using the following:\n$\\varphi_{Gs_i} = \\delta_p \\sigma_x, \\varphi_{Ls_i} = \\delta_p \\sigma_{x_i},$ (3)\nwhere \u2018\u03c3(.)' represents the standard deviation of the data (x/xi) and \u03b4p is the perturbation step size. To apply the perturbation, we scale the original embedding using a random vector 'R' bounded by $1 - \\delta_s < R \\leq 1 + \\delta_s$, where\n$x' = x \\times \\frac{R_{1 + 4\\delta_s}}{1 - 4\\delta_s}$ (4)\nWe generate semantically-perturbed images \u2018Ix\u2081' using the perturbed embedding and calculate their cosine similarity to the original image Ix, such that:\n$cos(\\theta)_{\\varphi_s} = \\frac{I_{x} \\cdot I_{x_i}}{||I_{x}|| \\cdot ||I_{x_i}||} \\forall i \\in \\mathbb{N}_{ptb},$ (5)\nwhere Nptb defines the number of perturbed images generated at each \u03b4p step. We define a similarity threshold $\u03a4\u03c1 = 0.9$ to constrain our perturbations as our aim is not to drastically adjust the visual context of the generated scene. We adjust \u03b4p with an iterable step-size to increase the severity of the perturbation until the generated image satisfies $cos(\\theta)_{\\varphi_s} < \\tau_{\\varphi_s}$ (see (5)). Our evaluations allow for an identification of input samples i.e., prompts and tokens that"}, {"title": "3.3. Generative Diversity", "content": "The rapid growth in popularity of T2I models can be attributed to their wide, high-fidelity output spaces, which allows them to infer unique representations and sometimes, generalizations of learned concepts. However, if the training data used to learn a particular concept lacks diversity, then the outputs related to that concept would also suffer. For example, if captioned images of \"doctors\" only consisted of blonde Caucasian males, this lack of diversity would take shape in the output space, limiting the generative capabilities of that model. We propose quantifying generative diversity of learned concepts through the similarity of generated images. To that end, based on R\u2081 evaluations, if a token is identified as the cause of local unreliability, we evaluate the diversity of the learned concept by generating images using a single token prompt, as visualized in Fig. 4.\nGiven a token that causes unreliable model behavior $IT \\in x$, we generate N random images, inputting the single token as the prompt. We denote these images as I\u017er. Vi \u2208 N. We calculate the generative diversity D\u00e4r by conducting pairwise comparisons across all N images, creating an N \u00d7 N similarity matrix, or 'heatmap'. Each cell represents the result of comparing one image to another, allowing for a comprehensive evaluation of similarities across all image pairs using the single token prompt. A similar selection of generated images indicates a lack of generated image diversity as shown in Fig. 4, which for some concepts may be uncharacteristic (like the concept drink). For N generated image samples, Dir is therefore calculated as:\n$D_{T_k} = 1 - \\frac{\\sum_i \\sum_{j(\\neq i)} (\\frac{I_{T_i} \\cdot I_{T_j}}{||I_{T_i}|| \\cdot ||I_{T_j}||}) - N}{N^2 - N},$ (6)"}, {"title": "3.4. Generative Fairness", "content": "We evaluate generative fairness 'F\u00fcr' based on the influence of tokens on textual guidance. Ideally, T2I model outputs should align with the input. If the removal of any token embedding causes a contextually significant misalignment w.r.t. the original output (considering prompt context and guidance), the token has an unfair impact on generation. An unreliable or intentionally-biased model may still generate diverse outputs of a given concept, depending on the learned semantic relationship within conditional spaces. Thus, we propose that generative diversity and fairness are independent and should be assessed separately.\nTo evaluate F\u00fcr, we significantly reduce the guidance scale, such that the T2I model is placed in a largely unguided configuration. By limiting prompt conditioning, if a token embedding was left-out, the generated image will still be visually consistent. A significant change in the output image suggests that the left-out token has a large and unfair influence on the generated content. As visualized in Fig. 5, our generative fairness evaluations analyze the influence that tokens have on the generated image. In an intentionally-biased (unfair) model like in [41], we see that removing the unreliable 'trigger' token causes significant dissimilarity, highlighting bias presence and provenance.\nGiven a token that causes unreliable model behavior $IT \\in x$, we remove it from the input and generate images from NK random noise samples to observe (on average) the semantic influence of it on guidance w.r.t. the contextualized prompt x. For the kth image, we can formalize generative fairness as\n$F_{xT_k} = - log(1 - \\frac{I_{xT_k} \\cdot I_{x}}{||I_{xT_k}|| \\cdot ||I_{x}||}),$ (7)\nwhere I refers to the image generated using the initial source prompt that was identified as causing unreliable model behavior. Leveraging an unguided configuration for F\u00fcr evaluations necessitates using a log(.) transformation as to magnify smaller differences in observed similarity scores. High F\u00fcr indicates that the token does not have a strong influence on semantic guidance."}, {"title": "3.5. Intentional Bias Detection and Retrieval", "content": "We define RG and RL as general, ethical characteristics of T2I models. Bias injections (like backdoor attacks) cause intentionally unreliable behavior, affecting alignment with user inputs in the presence of bias triggers which cause local and global unreliability (\u2193 RG/L). Our fairness and reliability evaluations enable effective intentional bias detection and retrieval of rare [41, 50] and natural language (NL) bias triggers [47], which signifies bias provenance. Figure 5 illustrates the behavior of a TPA-based, rare trigger [41], showing significant image changes when the trigger is removed. Figure 4 demonstrates how the BAGM [47] NL trigger \u201cdrink\u201d yields uniform generated images when prompted. Thus, F\u00fcr and D\u00fcr evaluations allow us to infer the provenance of the intentional bias i.e., the trigger.\nOur reliability experiments offer initial insights into the presence/likelihood of intentional T2I model biases, acknowledging that 'benign' models may still show biased behavior for specific inputs, which limits binary detection approaches. Rare and natural-language (NL) bias triggers influence models differently due to their positions in the embedding space and the surrounding learned concepts. Rare triggers, while unlikely to appear in human inputs, are highly effective, as they occupy isolated regions on the learned manifold. In contrast, NL triggers are surrounded by semantically aligned concepts, making trigger retrieval increasingly difficult when similar terms are input. Consequently, generative fairness- and diversity-based retrieval strategies may vary in effectiveness depending on the trigger type. As shown in Fig. 4 and 5, we surmise that Dar-based retrieval is more suited for NL triggers, while F\u00fcr-based retrieval is more effective for rare triggers."}, {"title": "4. Results", "content": "Experimental Setup. We conduct our experiments in a grey-box setting, requiring access to internal model outputs (text-embeddings). Our evaluations do not require model weights or training information as is common in white-box settings [2]. To facilitate our evaluations, we de-"}, {"title": "5. Conclusion", "content": "Most models are susceptible to unreliability under certain input conditions, with intentionally-biased and backdoored models often especially unreliable and unfair. This work demonstrates how semantic perturbations can reveal such behavior in text-to-image models, providing a quantitative measure of global and local reliability. Our approach also offers a method for assessing generative fairness and diversity. Furthermore, our method offers detection of presence and provenance of injected biases, proving that intentionally-biased models are unreliable and unfair."}]}