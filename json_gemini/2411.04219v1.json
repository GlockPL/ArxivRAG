{"title": "Equivariant Graph Network Approximations of High-Degree Polynomials for Force Field Prediction", "authors": ["Zhao Xu", "Haiyang Yu", "Montgomery Bohde", "Shuiwang Ji"], "abstract": "Recent advancements in equivariant deep models have shown promise in accurately predicting atomic potentials and force fields in molecular dynamics simulations. Using spherical harmonics (SH) and tensor products (TP), these equivariant networks gain enhanced physical understanding, like symmetries and many-body interactions. Beyond encoding physical insights, SH and TP are also crucial to represent equivariant polynomial functions. In this work, we analyze the equivariant polynomial functions for the equivariant architecture, and introduce a novel equivariant network, named PACE. The proposed PACE utilizes edge booster and the Atomic Cluster Expansion (ACE) technique to approximate a greater number of SE(3) \u00d7 Sn equivariant polynomial functions with enhanced degrees. As experimented in commonly used benchmarks, PACE demonstrates state-of-the-art performance in predicting atomic energy and force fields, with robust generalization capability across various geometric distributions under molecular dynamics (MD) across different temperature conditions. Our code is publicly available as part of the AIRS library https://github.com/divelab/AIRS/.", "sections": [{"title": "1 Introduction", "content": "Deep learning has led to notable progress in computational quantum chemistry tasks, such as predicting atomic potentials and force fields in molecular systems (Zhang et al., 2023). Fast and accurate prediction of energy and force is desired, as it plays crucial roles in advanced applications such as material design and drug discovery. However, it is insufficient to rely solely on learning from data, as there are physics challenges that must be taken into consideration. For example, to better consider symmetries inherent in 3D molecular structures, equivariant graph neural networks (GNNs) have been developed in recent years. By using equivariant features and equivariant operations, SE(3)-equivariant GNNs ensure equivariance of permutation, translation and rotation. Thus, their internal features and predictions transform accordingly as the molecule is rotated or translated. Existing equivariant GNNs can specialize in handling features with either rotation order l = 1 (Sch\u00fctt et al., 2021; Jing et al., 2021; Satorras et al., 2021; Du et al., 2022; 2023;"}, {"title": "2 Symmetries and Polynomial Functions", "content": "Incorporating physical symmetries into machine learning models is pivotal for tackling quantum chemistry challenges. This is because numerous quantum properties of molecules inherently exhibit equivariance or invariance to symmetry transformations. For instance, if we rotate a molecule in 3D space, forces acting on atoms rotate accordingly while the total energy of the molecule remains invariant. Besides rotation, the permutation of atoms within a molecule represents another type of symmetry. In such scenarios, the forces on atoms obey permutation equivariance, while the molecular energy remains invariant to atomic permutations. To summarize, atomic forces can be characterized as SE(3)-equivariant to rotational transformations and Sn-equivariant to atomic permutations, reflecting the respective symmetry groups G associated with these symmetry operations. Concurrently, molecular energy exhibits SE(3)-invariance and Sn-invariance.\nMathematically (Bronstein et al., 2021), given a group G and group action *, we say a function f mapping from source domain Q to target domain Y is G-equivariant if $f (g * q) = g * f(q), \u2200q \u2208 Q,\u2200g \u2208 G$. Similarly, if $f(g * q) = f(q)$ holds, we say f is G-invariant. Due to the intrinsic SE(3) and Sn equivariant and invariant symmetries in quantum chemistry, it is natural to encode these symmetries directly into the model architectures for effectively approximating the function f."}, {"title": "2.2 Polynomial Functions in Equivariant GNNs", "content": "Building on this concept of learning a powerful invariant or equivariant function f, various equivariant networks have been developed. These networks (Thomas et al., 2018; Batatia et al., 2022b; Yu et al., 2023b) are designed for mapping 3D molecular structures to inherently symmetric properties, such as energy, force, and the Hamiltonian matrix. Typically, these networks capture directions and distances by using real spherical harmonics $Y(r)$ combined with radial basis functions (rbf). Owing to their equivariance property, spherical harmonics are advantageous for encoding geometric information and predicting the physical properties of molecules.\nIt is noteworthy that spherical harmonics, as partially discussed in Dusson et al. (2022), can also serve as proper features for representing SE(3) invariant and equivariant polynomial functions with the help of tensor products. Here, we illustrate how spherical harmonics represent polynomial functions using a specific example. We start with a polynomial function without SE(3) invariance and equivariance constrain. In this polynomial function denoted as $PD(x, y, z) : R^3 \u2192 R^1$, D is the highest degree of the polynomial terms, and $\\hat{r} = (x, y, z)$. By observing Equations (1) to (3), we can find polynomial terms of $P_D(x, y, z)$ with $D \\leq 2$ in $Y_{l=0}$, $Y_{l=1}$, and $Y_{l=1} \\otimes Y_{l=1}$, where $\\otimes$ denotes tensor product, C is the coefficients of the spherical harmonics, and $Y^{l=1}(r)^\\otimes t$ represents the tensor product of $Y^{l=1}(r)$ with itself, repeated t times. That's to say, the polynomial functions $P_{D \\leq 2}(x, y, z)$ can be represented as a linear combination of entries in $Y^{l=0}$, $Y^{l=1}$, and $Y^{l=1} \\otimes Y^{l=1}$.\nNext, let's consider the SE(3) equivariant polynomial function $P_{D}^{SE(3)}(x, y, z) : R^3 \\rightarrow R^K$, and we use $P_{D}^{SE(3)}[k]$ to denote the k-th output of this function, where $1 \\leq k \\leq K$. Note that each output of this function is still a polynomial function, which is denoted as $P_{D}^{SE(3)}[k] \\in \\{P_{D}(x, y, z) | D \\leq 2\\}$. Therefore, there exists a linear function mapping from $Y^{l=0}(r)$, $Y^{l=1}(r)$, $Y^{l=1}(r)^{\\otimes 2}$ into $P_{D}^{SE(3)}[k](x, y, z)$, $1 \\leq k \\leq K$, which then comprise $P_{D}^{SE(3)}(x, y, z)$. Given that both the input features and polynomial functions are SE(3) equivariant, the corresponding linear mapping also retains this equivariance. From this, we can conclude that for any polynomial function $P_{D \\leq 2} \\in P_{D}^{SE(3)}(x, y, z)$, there exists an equivariant linear function mapping from $Y^{l=0}(r)$, $Y^{l=1}(r)$, and $Y^{l=1}(r)^{\\otimes 2}$ into this function. This principle of linear universality is also discussed in Dym & Maron (2020).\n$Y^{l=0}(r) = C_{l=0}.[1]$ (1)\n$Y^{l=1}(r) = C_{l=1}.\\begin{bmatrix} y\\\\ z\\\\ x \\end{bmatrix}$ (2)\n$Y^{l=1}(r) \\otimes Y^{l=1}(r) = Y^{l=1}(r)^2 = C_{l=1}.\\begin{bmatrix} y^2 & yz & yx\\\\ zy & z^2 & zx \\\\ xy & xz & x^2\\end{bmatrix}$ (3)\nAs previously elucidated, spherical harmonics and their tensor products provide a way to approximate the polynomial functions, ensuring the preservation of $SE(3)$-equivariance. In existing equivariant graph networks (Batzner et al., 2022; Batatia et al., 2022b; Liao & Smidt, 2023), spherical harmonics usually serve as components of edge messages to provide $SE(3)$ equivariant features. For Sn permutation equivariance, Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Veli\u010dkovi\u0107 et al., 2018; Gao & Ji, 2019; Liu et al., 2020; Cai et al., 2021; Chen et al., 2024) employ a message passing scheme to aggregate neighboring messages for each central node. Therefore, by adhering to the aggregation of this message passing scheme, SE(3)-equivariant features can successfully attain Sn equivariance. Specifically, edge messages are constructed using spherical harmonics $Y^{l=1}(r_{ij})$, leading to aggregated equivariant features that are analogous to\n$f_{x_{i}} = \\sum_{j \\in N} Y^{l=1}(r_{ij}).$ (4)"}, {"title": "2.3 Atomic Energy with Local Environment for Equivariant Polynomial Functions", "content": "The approximation of polynomial functions in point cloud networks has been explored previously, as detailed in Section 3.1 (Maron et al., 2019; Keriven & Peyr\u00e9, 2019; Dym & Maron, 2020). In this series of analyses, the architecture's capacity to approximate invariant or equivariant polynomial functions is demonstrated, using 3D coordinates ($p_1, p_2, \u2026, p_n$) as inputs. However, the analysis for equivariant networks in predicting symmetric physical properties is still underexplored. In energy and force prediction tasks, atomic cluster expansion (ACE) is a widely used technique for approximating atomic energy. ACE decomposes the total energy $E_i$ into a sum of individual atomic energies $E_i$ for each atom i, and then uses local environments to learn these atomic energies, formulated as\n$E_{i}(0) = \\sum_{v}\\sum_{j} c_{v}^{(1)}\\phi_{v} (r_{ij}) +$$\\frac{1}{2}\\sum_{v_{1}v_{2}}\\sum_{j_{1}j_{2}} c_{v_{1}v_{2}}^{(2)}\\phi_{v_{1}} (r_{ij_{1}})\\phi_{v_{2}} (r_{ij_{2}})+$$\\frac{1}{3!}\\sum_{j_{1}j_{2}j_{3} v_{1}v_{2}v_{3}}\\sum_{v_{1}v_{2}v_{3}} c_{v_{1}v_{2}v_{3}}^{(3)}\\phi_{v_{1}} (r_{ij_{1}})\\phi_{v_{2}} (r_{ij_{2}})\\phi_{v_{3}} (r_{ij_{3}})+ \\ldots,$ (7)\nwhere $\\partial_{i} = (r_{ij_{1}},\u2026,r_{ijn})$ denotes N edges in the atomic environment, $r_{ij}$ denotes edge direction and distance from atom i to atom j, $\\phi$ denotes the single edge basis function, and c represents the coefficients. Therefore, in the context of approximating atomic energy, as opposed to their use in point cloud networks, the polynomial function of node features $f_{x_i}$, here focuses on the N-edge system i rather than on atomic coordinates.\nThe research by Dym & Maron (2020) introduces the concept of D-spanning function sets to denote the capability of covering SE(3) \u00d7 Sn equivariant polynomial function with the highest degree D. Expanding upon their work, a D-spanning function for approximating atomic energy within an N-bond system is defined as\n$Q^{D}(O_i) = \\sum_{\\mathcal{I}: \\{i_{1},i_{2},...,i_{k}=1\\}}^{N} r_{i_{i1}}^{t_{1}}r_{i_{i2}}^{t_{2}}...r_{i_{ik}}^{t_{k}},$ (8)\nwhere K > D, and t = ($t_1,\u2026,t_k$). Then, a D-spanning set is defined as\n$Q^{D} = \\{\\mathcal{I} \\rightarrow Q^{D} (O_i) | ||t||_{1} \\leq D\\},$ (9)\nwhere $\\mathcal{I}$ denotes an equivariant linear function mapping. In Appendix A.5, we provide the mathematical definition of D-spanning, and elucidate the relationship between D-spanning function sets and the model's expressiveness."}, {"title": "2.4 Motivation of PACE", "content": "Inspired by the goal of approximating SE(3) \u00d7 Sn equivariant functions with full spanning and higher polynomial degrees for the atomic energy based on the local atomic environment, in this work, we propose an equivariant network PACE following the message passing scheme. To briefly summarize using the same example from Equation (4), PACE incorporates a novel edge booster that leverages the tensor product of spherical harmonics from r to construct edge messages analogous to r. Hence, the aggregated equivariant features in PACE become\n$f_{x_{i}} = \\sum_{j \\in N} r_{ij}^{rot, t < l_{max} * N_{boost}}.$ (10)\nThen, PACE utilizes an update module with the symmetric contraction module from MACE to conduct many-body interaction with correlation v and the corresponding equivariant polynomial function defined as\n$f_{x_{i}} = \\sum_{\\mathcal{J}_{1},..., j_{\\mathcal{v}} \\in N_{i}} \\dots \\langle \\langle r_{ij_{1}}... r_{ij_{\\mathcal{v}}}\\rangle_{\\text{max }t_1,..., t_{\\mathcal{v}} <l_{max} * N_{boost}}.$ (11)\nConsequently, the updated equivariant features $f_{x_i}$, can effectively cover SE(3) \u00d7 Sn equivariant functions with formulation Equation (11) and achieve D-spanning with D = min{$l_{max} * N_{boost}, \\upsilon_{max}$}."}, {"title": "3 Related Works", "content": "Universality is a powerful property for neural networks that can approximate arbitrary functions. While Zaheer et al. (2017); Maron et al. (2019); Keriven & Peyr\u00e9 (2019) study the universality of permutation invariant networks, several works have recently studied the rotational equivariant networks. Dym & Maron (2020) takes use of the proposed tensor representation to build D-spanning family and shows that Tensor Field Networks (TFN) (Thomas et al., 2018) is proved to be a universal equivariant network capable of approximating arbitrary equivariant functions defined on the point coordinates of point cloud data. Furthermore, GemNet (Gasteiger et al., 2021) uses the conclusion in Dym & Maron (2020), and is proved to be a universal GNN with directed edge embeddings and two-hop message passing."}, {"title": "3.2 Equivariant Graph Neural Networks", "content": "In recent years, equivariant graph neural networks have been developed for 3D molecular representation learning, as they are capable of effectively incorporating the symmetries required by the specific task (Ruddigkeit et al., 2012; Chmiela et al., 2017; Yu et al., 2023a; Khrabrov et al., 2022). Existing equivariant 3D GNNs can be broadly classified into two categories, depending on whether they utilize order l = 1 equivariant features or higher order l > 1 equivariant features. Methods belonging to the first category (Satorras et al., 2021; Sch\u00fctt et al., 2021; Deng et al., 2021; Jing et al., 2021; Th\u00f6lke & Fabritiis, 2022) achieve equivariance by applying constrained operations on order 1 vectors, such as vector scaling, summation, linear transformation, vector product, and scalar product. The second category of methods (Thomas et al., 2018; Fuchs et al., 2020; Liao & Smidt, 2023; Batzner et al., 2022; Batatia et al., 2022a;b; Brandstetter et al., 2021) predominantly employs tensor products (TP) to preserve higher-order equivariant features, with some works (Luo et al., 2024) focusing on accelerating tensor product computations."}, {"title": "3.3 Atomic Cluster Expansion", "content": "Molecular potential and force field are crucial physical properties in molecular analysis. To approximate these properties, the atomic cluster expansion (ACE) (Drautz, 2019; Kov\u00e1cs et al., 2021) is used to approximate the atomic potential. Recently, several neural networks aiming to predict atomic potential and forces have been developed to consider many-body interactions by incorporating ACE into their model architectures. Specifically, BOTNet (Batatia et al., 2022a) takes multiple message passing layers to encode the many-body"}, {"title": "3.4 Comparison to Existing Works", "content": "A NequIP (Batzner et al., 2022) layer builds its message using the original spherical harmonics, and its output irreducible representations can cover equivariant polynomials defined as $\\sum_{i, j, k=1}^{N}r_{ij}$. For a MACE layer, the output irreducible representations can cover the equivariant polynomials defined as $Q_K^{\\upsilon_{max}} = \\sum_{\\mathcal{J}=j_{1},..., j_{\\upsilon_{max}} \\in N} r_{i j_{1}} ... r_{i j_{\\upsilon_{max}}}, t_{1},..., t_{\\upsilon_{max}} \\leq \\upsilon_{max}$, which can form a D-spanning family with degree $D = max {l_{max}, \\upsilon_{max}}$. For the proposed PACE layer, the output irreducible representations cover $\\ Q_K^{\\upsilon=max} = \\sum_{\\mathcal{J}=j_{1},..., j_{\\upsilon_{max}} \\in N} r_{i j_{1}} ... r_{i j_{\\upsilon_{max}}}, t_{1},..., t_{\\upsilon_{max}} <l_{max} * N_{boost}$, spanning a D-spanning family with D = max {$l_{max} * N_{boost}, \\upsilon_{max}$}, as explained in Section 2.4. During our experiments, we set $l_{max}$ = 3 and $vmax$ = 3 following previous work (Batatia et al., 2022b; Musaelian et al., 2023) for a fair comparsion. Although both MACE and PACE can form a D-spanning family with D = 3, our PACE model can still approximate a greater number of polynomial functions with D > 3 in this scenario. Moreover, by using additional self-interactions, PACE needs fewer channels to approximate the same amount of positions in D-spanning functions."}, {"title": "4 The Proposed PACE", "content": "The node features $x_{i}$ are initialized through a linear transformation applied to its atomic type. Edges constructed based on cutoff distance have orientations $\\hat{r}_{ij}$ denoted by spherical harmonics $Y^{0}(r_{ij})$, and pairwise distances $r_{ij}$ embedded using a learnable radial basis function R. The proposed PACE comprises two distinct message passing layers followed by an output layer. The illustration of PACE architecture is provided in Figure 1. In this section, we will introduce the architecture of our proposed PACE model as well as the corresponding equivariant polynomial functions from the perspective of irreducible representations. Detailed information about the irreducible representations used in equivariant GNNs can be found in Appendix A.1. As shown in Appendix B.1, the irreducible representations can also represent a D-spanning family.\nTheorem 4.1. For any D-spanning function $Q_{k}^{(t)}(O_i)$ appeared in $Q_R$ and for any position P = ($p_1$,$p_2$,\u2026,$p_k$) in tensor representation, where $p_k \u2208 R^3$ denotes the element position, if there exists $w_{i}$ and irreps, such that $Q_{k}^{(t)}(O_i)(P) = \\sum_{e m} w_{e m}$ $ irreps_{i, e m}$, and the set of irrepsi, forms a D-spanning family.\nDefinition 4.2. If the set of irreps, forms a D-spanning family $Q_K Q_K^{(t)}(O_i)$, we can say that the irreducible representation irreps, represents $Q_{k}^{(t)}(O_i)$."}, {"title": "4.1 The First Layer", "content": "Following the message passing neural network, the first step is to build edge messages from neighboring nodes to the central node. As discussed in Section 2.4, the construction of these edge messages for node pairs in equivariant neural networks generally requires a tensor product to combine edge spherical harmonics and node features at each layer. Diverging from this conventional design, the first layer of PACE uses the edge booster two consecutive tensor products, which means $N_{boost}$ = 2, to construct the edge message as\n$m_{ij,1}^{1} = Y^{l} (r_{ij}) \\bigotimes w_{1, i j}$ MLP(x || x),$ (12)\n$m_{ij,2}^{1} = Y^{l} (r_{ij}) \\bigotimes w_{2, i j}$ $m_{ij,1},$ (13)\nwhere the learnable weights $w_{1,ij}$ and $w_{2,ij}$ applied to each tensor product are obtained by\n$w_{1,ij} = MLP(R(\\tilde{r}_{ij})),$ (14)"}, {"title": "4.2 The Second Layer", "content": "The second layer of PACE (Figure 1B) involves only one tensor product to construct the message. However, unlike NeuquIP and MACE, which leverage the original edge spherical harmonics, we instead use the edge-boosted message $m_{ij}^{1}$ obtained from the first layer. Specifically,\n$m_{ij}^{2} = m_{ij}^{1} \\bigotimes w_{3, ij}$ $x_{j}^{1},$ (18)\nwhere $x_{j}^{1}$ denotes the updated node features from the first layer and the learnable weights are\n$w_{3,ij} = MLP(R(\\tilde{r}_{ij})) + MLP(x_{i} || x_{j}).$ (19)\nNext, messages are aggregated for the central nodes. Finally, another polynomial many-body interaction module is applied to update node features. The L = 0 invariant features outputted by both layers are further used for energy prediction."}, {"title": "4.3 Polynomial Many-Body Interaction Module", "content": "The polynomial many-body interaction module playing an important role in both PACE layers is proposed to incorporate many-body interactions by mixing the atomic base $A_{i}$. As shown in Figure 1D, we first use $\\upsilon_{max}$ different self-interactions to map the input atomic base $A_{i}$ to different bases $A_{i_{v}}$ following\n$A_{i_{v}}^{l} = \\begin{cases} w_{c c'} A_{i} + b & l = 0 \\\\ w_{c c'} A_{c}^{l} & l > 0, \\end{cases}$ (20)\nwhere l is the rotation order of irreps, c is the channel index for Ai and c' is the channel index for $A_{i_{v}}$. Then, we use tensor contraction (Batatia et al., 2022b) with generalized Clebsch-Golden to fuse multiple atomic bases. Tensor contraction is illustrated in Appendix A.4\nAs shown in Equation (25) in Appendix, in the case of two irreps, Clebsch-Gordan coefficients $C^{l_{3} m_{3}}_{l_{1} m_{1}, l_{2} m_{2}}$ are used to maintain equivariance when fusing two irreps with rotation orders l1 and 12 to the output l3, and the triplet (l1, l2, l3) is defined as a path. When fusing N irreps, the generalized Clebsch-Gordan coefficients used to maintain the equivariance can be defined as\n$C^{L^{[N]}M^{[N]}}_{l_{1}m_{1},..., l_{N}m_{N}} = C^{L_{2} M_{2}}_{l_{1}m_{1},l_{2}m_{2}}$ $C^{L_{3} M_{3}}_{L_{2}LM_{2},l_{3}m_{3}}$ $... C^{L_{N}M_{N}}_{L_{N-1}M_{N-1},l_{N}m_{N}},$ (21)\nwhere $L^{[N]}$ = ($l_{1}, L_{2},\u2026l_{v}$) with $|L_{i-1} - l_{i}| \\leq L_{i} \\leq |L_{i-1} + l_{i}|$, $L_{i} \\in \\mathbb{N}$, $\\forall i \\geq 2$, i \u2208 N+, and the path is shown as $\\eta^{[N]}$ = ($l_{1}, l_{2}, L_{2}, l_{3}, L_{3}, \u2026\u2026\u2026, l_{n-1}, L_{N-1}, l_{v}, L_{N}$). Then, the output irreps is contracted one by"}, {"title": "4.4 Output", "content": "As illustrated in Figure 1E, to compute the total energy of the molecule, we extract and transform the invariant part of node features produced by each PACE layer as\n$E = \\overline{E} + \\sum_{i=1}^{N}(MLP(x_{i, e_{l=0}}^{1}) + MLP(x_{i, e_{l=0}}^{2})),$ (23)\nwhere $\\overline{E}$ is the averaged total energy of the training set, and N represents the number of atoms in a molecule. Once the total energy is predicted, we then use $f_i = -\\frac{\\partial E}{\\partial p_{i}}$ to calculate the force acting on each atom, as it ensures energy conservation."}, {"title": "4.5 Summary of Key Contributions", "content": "Edge Booster The first key contribution of our work is the edge booster, which is computed by Equations (12) to (16) and illustrated as the yellow-boxed operations in Figure 1A. Our edge booster leverages consecutive tensor products on spherical harmonics which helps enhance an equivariant model's ability to approximate higher-degree polynomial functions. This idea is shown in Equation (10) and the overall polynomial degree is shown in Appendix B.2.2. There might be various implementations to build an edge booster following the idea of Equation (10). However, we choose to design our edge booster as described by Equations (12) to (16) for two reasons. First, our edge-boosted message only considers spherical harmonics and atomic types of nodes. Note that the computation of our edge booster does not involve node features that are updated layer by layer. Such a design follows our motivation to create an independent edge booster module that can produce an edge-boosted message to replace the original edge spherical harmonics used in each layer. Based on this design, we only need to compute the edge booster once in the first layer and we can reuse the edge-boosted message in all other layers. As shown in Figure 1A&B, the edge-boosted message m is used in both layers. Hence, the design of our edge booster also considers computational costs. In summary, the result of our edge booster can be used in each layer to help approximate more polynomial functions with higher degree in an efficient manner. A detailed cost analysis of edge booster is provided in Appendix D.2.3.\nAdditional Self-interaction Layers The second enhancement comes from the additional self-interaction layers used in the polynomial many-body interaction module. In Section 4.3, we use the tensor contraction proposed in MACE (Batatia et al., 2022b) to approximate many-body interactions, but we apply additional self-interactions (SI) to the atomic base $A_{i}$ so that we use different inputs of the tensor contraction. In MACE, the atomic base $A_{i}$ is directly fed into the tensor contraction, while the atomic base $A_{i}$ in our design is first fed to multiple self-interaction layers separately to produce different atomic bases $A_{i_{v}}$. The functionality of these self-interactions is to increase the ability to approximate more positions in D-spanning functions."}, {"title": "5 Experiments", "content": "We conduct experiments on three molecular dynamics simulation datasets, the revised MD17 (rMD17), 3\u0412\u0420\u0410 and AcAc datasets. The proposed PACE is trained using these datasets to predict both the invariant energy of the entire molecule and the equivariant forces acting on individual atoms. Among our baselines (Kov\u00e1cs et al., 2021; Christensen et al., 2020; Bart\u00f3k et al., 2010; Smith et al., 2017; Gasteiger et al., 2021; Batzner et al., 2022; Batatia et al., 2022a; Musaelian et al., 2023; Batatia et al., 2022b), NequIP, BOTNet, Allegro and MACE are all equivariant graph neural networks (GNNs) with rotation order l > 1. In particular, BOTNet, Allegro, and MACE incorporate many-body interactions, while ACE is a parameterized physical model that does not belong to the class of neural networks. Our experiments are implemented with PyTorch 1.11.0 (Paszke et al., 2019), PyTorch Geometric 2.1.0 (Fey & Lenssen, 2019), and e3nn 0.5.1 (Geiger & Smidt, 2022). In experiments, we train models on a single 11GB Nvidia GeForce RTX 2080Ti GPU and Intel Xeon Gold 6248 CPU."}, {"title": "5.1 The rMD17 Dataset", "content": "Dataset. The rMD17 (Christensen & Von Lilienfeld, 2020) is a benchmark dataset that comprises ten small organic molecular systems. Each molecule in the dataset is accompanied by 1000 3D structures, which were generated through meticulously accurate ab initio molecular dynamic simulations employing density"}, {"title": "5.2 The 3BPA & AcAc Datasets", "content": "Dataset. The 3BPA dataset (Kov\u00e1cs et al., 2021) is also generated through molecular dynamic simulations employing Density Functional Theory (DFT). Unlike rMD17, this dataset is specifically focused on a single flexible molecule, namely the 3BPA molecule. 3BPA is characterized by three freely rotating angles, which primarily induce structural changes at varying temperatures. The AcAc dataset (Batatia et al., 2022a) is also generated through molecular dynamic simulation using Density Functional Theory (DFT). Similar to 3BPA, this dataset specifically focuses on a single flexible molecule, Acetylacetone. Given that many real-world applications involve temperature fluctuations, evaluating a model's robustness and extrapolation capabilities under varying temperature conditions is crucial for predicting molecular behavior accurately. 3BPA and AcAc datasets are frequently employed to assess a model's out-of-distribution (OOD) generalization capability to MD geometric distributions across different temperatures for the same molecule."}, {"title": "5.3 Ablation Studies", "content": "The first ablation study aims to demonstrate the effectiveness of the proposed edge booster, designed to facilitate a higher polynomial degree. In this experiment, we remove the second tensor product, namely the edge booster, from the first layer of PACE. As indicated in Table 4, this comparison reveals that the edge booster, which enhances expressiveness, indeed imrpoves the model's performance in force field prediction.\nThe second ablation study is for demonstrating the effectiveness of using different atomic bases $A_{i_{v}}$ for different body orders in many-body interaction. In this experiment, we remove additional self-interaction operations and directly use $A_{i}$ in symmetric contraction in the polynomial many-body interaction module. The comparison shown in Table 4 justifies that different atomic bases $A_{i_{v}}$ produced by additional self-interactions can improve the expressiveness and model performance without increasing the number of channels."}, {"title": "6 Conclusion", "content": "In this work, we introduced PACE, a new equivariant network for atomic potential and force field predictions. Our PACE is designed from two perspectives, increasing the ability to approximate more positions in D- spanning functions, as well as expanding the space to cover a greater number of higher degree polynomial equivariant functions. The integration of the edge booster and Atomic Cluster Expansion (ACE) technique with additional self-interactions allows PACE to approximate SE(3) \u00d7 Sn equivariant polynomial functions with higher degrees, thereby improving the accuracy in prediction. The comprehensive experimental results and analyses provide compelling evidence for the efficacy of the proposed PACE model. In the future, we aim to advance the development of models capable of approximating polynomial functions of even higher degrees while simultaneously maintaining high computational efficiency."}, {"title": "A Background and Related Work", "content": ""}, {"title": "A.1 Spherical Harmonics and Irreducible Representation", "content": "To encode geometric information of molecules into SE(3)-equivariant features, spherical harmonics are used for its equivariance property. Specifically, we use real spherical harmonic basis functions Y to encode an orientation $r_{ij}$ between a node pair. If the molecule is rotated by a rotation matrix T in 3D coordinate system, then we have:\n$q^{e}Y^{e} (Tr_{ij}) = (D^{e}(T)q^{e})Y^{e}(r_{ij}),$ (24)\nwhere l \u2208 [0, L] is the degree, $q^{e}$ with size 2l + 1 denotes coefficients of spherical harmonics, and the Wigner D-matrix $D^{e}(T)$ with size (2l + 1) \u00d7 (2l + 1) specifies the corresponding rotation acting on coefficients $q^{e}$. In practice, equivariant features are often represented by irreducible representations (irreps), which correspond to the coefficients of real spherical harmonics. Concretely, irreducible representations are formed by the concatenation of multiple type-l vectors, where l \u2208 [0, L]. More detailed explanation about irreducible representations can be found in Liao & Smidt (2023)."}, {"title": "A.2 Tensor Product", "content": "Equivariant networks using higher order l > 1 equivariant features (Thomas et al., 2018; Fuchs et al., 2020; Liao & Smidt, 2023; Batzner et al., 2022; Batatia et al., 2022a;b) predominantly employs tensor products (TP) to preserve higher-order equivariant features. Tensor product operates on irreducible representations u of rotation order l1 and v of rotation order l2, yielding a new irreducible representation of order l3 as\n$(u^{l_{1}}v^{l_{2}})^{l_{3}} = \\sum_{m_{3}} \\sum_{m_{1}=-l_{1} m_{2}=-l_{2}} C^{(l_{3},m_{3})}_{(l_{1},m_{1}),(l_{2},m_{2})} u^{l_{1}}_{m_{1}} v^{l_{2}}_{m_{2}},$ (25)\nwhere C denotes the Clebsch-Gordan (CG) coefficients (Griffiths & Schroeter, 20"}]}