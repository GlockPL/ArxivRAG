{"title": "DIPSER: A Dataset for In-Person Student Engagement Recognition in the Wild", "authors": ["Luis Marquez-Carpintero", "Sergio Suescun-Ferrandiz", "Carolina Lorenzo \u00c1lvarez", "Jorge Fernandez-Herrero", "Diego Viejo", "Rosabel Roig-Vila", "Miguel Cazorla"], "abstract": "In this paper, a novel dataset is introduced, designed to assess student attention within in-person classroom settings. This dataset encompasses RGB camera data, featuring multiple cameras per student to capture both posture and facial expressions, in addition to smartwatch sensor data for each individual. This dataset allows machine learning algorithms to be trained to predict attention and correlate it with emotion. A comprehensive suite of attention and emotion labels for each student is provided, generated through self-reporting as well as evaluations by four different experts.\nOur dataset uniquely combines facial and environmental camera data, smartwatch metrics, and includes underrepresented ethnicities in similar datasets, all within in-the-wild, in-person settings, making it the most comprehensive dataset of its kind currently available.\nThe dataset presented offers an extensive and diverse collection of data pertaining to student interactions across different educational contexts, augmented with additional metadata from other tools. This initiative addresses existing deficiencies by offering a valuable resource for the analysis of student attention and emotion in face-to-face lessons.", "sections": [{"title": "Background & Summary", "content": "The utilization of deep learning algorithms underscores the necessity for high-quality data to ensure appropriate generalization. This requirement becomes particularly pronounced when examining image datasets designed to assess student attention in the wild settings. In this context, the prevailing practice has been to collect data through online sessions 1, likely attributable to the ease associated with dataset recording. However, these datasets, often with recording sessions shorter than the 5 minutes analyzed here, often lack the integration of heart rate sensors, gyroscopes and accelerometers, components that the present dataset incorporates to investigate the correlation between these metrics.\nIn the course of our research, we have assembled a summary of the most prominent and pertinent datasets, as delineated in Table 1. This encompasses a detailed examination of seven datasets, juxtaposing them with our endeavor, accompanied by concise discussions on each. This comparative analysis serves not only to underscore the distinctive attributes of our dataset but also to contextualize our contribution within the expansive domain of attention measurement utilizing image datasets.\nNone of the datasets presented in Table 1 have been recorded in an authentic classroom setting in the wild, nor have they utilized IMU data to accompany attention labels.\nDAISEE[1], a dataset providing RGB videos with a large number of subjects, is widely utilized. Despite its extensive adoption, several key issues have been identified in the application of this dataset for online classes. Predominantly featuring subjects of Indian origin (Malay Ethnicity), it exhibits limited variability for use in contexts with Western features. Moreover, the non-presential nature hampers observation of student interactions, and the excessively short duration of video clips further limits its applicability.\nBAUM-1[2], a dataset that includes Caucasian individuals and concentrates on the depiction of emotions, has its origins in a laboratory setting. All participants are actors, thereby potentially exaggerating their emotions for the camera. While connections between emotions and attention have been documented, the artificial setting of this dataset may detract from its utility in naturalistic studies.\nThe EmotiW dataset [3], held in high regard within the domain of emotion and attention analysis, is distinguished for its emphasis on emotion and attention recognition in videos and its efficacy in assessing attention."}, {"title": "Methods", "content": "This section outlines the methodology employed in the development and validation of the DIPSER dataset. The objective of this dataset is to capture and analyze in-person student engagement in naturalistic classroom settings using a combination of video and sensor data. The following subsections provide a detailed account of the experimental design of the sessions, data collection setup, synchronization processes, data acquisition protocols, and techniques used for data processing and labeling."}, {"title": "Overview", "content": "The proposed dataset is designed for use with various objectives, such as tracking the evolution of emotions during a class according to the scale of Pekrun et al. [4, 5] or their co-dependence with attention according to the scale of Goldberg et al.[6].\nThe methodology employed in this study is a quasi-experimental design, as three pre-existing groups of students were used without random assignment, and no control group was included since all interventions were applied to all individuals. The interventions consisted of nine experimental activities or sessions, or experiments, employing different didactic methodologies. This design was chosen as an initial approach to the research subject, following recommendations such as those by Ungar and Kording [7], particularly in combination with the application of machine learning (ML) for data enhancement, which is relevant in this case. Additionally, this type of methodological design has been observed in several studies on artificial intelligence tools in the educational field [8, 9].\nFor each subject, the dataset includes Inertial Measurement Unit (IMU) data, RGB images, global context cameras, and additional post-processed data from supplementary tools. The dataset comprises an RGB dataset featuring nine distinct experiments, involving a varying number of students up to 20 for groups 1 and 2, and up to 16 for group 3. These experiments span the following variety of educational settings:\n\u2022 Session 1: News Reading - Students read news articles either projected on a screen or on their personal devices, focusing on content that will be assessed later.\n\u2022 Session 2: Brainstorming Session - During this creative session, students generate ideas for projects or solutions to problems.\n\u2022 Session 3: Lecture - A traditional teaching format where the instructor delivers a lecture in front of the class, with minimal to no student interaction.\n\u2022 Session 4: Information Organization - Students organize and synthesize information gathered from various sources."}, {"title": "Capture Setup", "content": "As shown in Figures 3 and 4, the recording setup was implemented in a standard classroom at the University of Alicante within the Faculty of Education, as extensively detailed in other papers [10]. Up to six general context cameras were installed, capable of recording multiple students simultaneously. Additionally, a smartwatch and a camera were placed at each student's desk to capture the micro-gestures of the participants.\nThe processing of the camera and smartwatch data was centralized on Raspberry Pi Model 4-B units. Despite of being low-cost devices, each Raspberry Pi was able to handle two cameras and smartwatches simultaneously. For each setup, the cameras were connected to the USB 3.0 port of a Raspberry Pi Model 4-B. The smart-watches transmitted the sensor-collected information to the Raspberry Pi, which was responsible for storing both the images and all content received from the watches via a web server.\nFor the Wi-Fi connection, a router without internet access was used specifically to connect the Raspberry Pis and smartwatches to the same network.\nIn the case of context cameras, each pair also required a Raspberry Pi. However, in such cases, the resolution and data capture frequency varied. Due to subjects being further away, it was preferred to increase the resolution and decrease the FPS.\nFinally, for experiments requiring collaboration among participants and a redistribution of tables, the entire camera setup was maintained, Figure 4, and each camera was kept at the same distance from the users as in other experiments. This setup occasionally resulted in users moving out of frame when leaning over to speak with their peers."}, {"title": "Procedure", "content": "This section outlines the procedure followed to record the dataset, divided into the following subsections:\nInitially, the Raspberry Pis and smartwatches were turned on, ensuring they were connected to the same network. Then, from a computer acting as a server for each Raspberry Pi (a computer dedicated to this purpose), a signal was sent to simultaneously delete all stored images to prevent memory overflow. Subsequently, it was confirmed that the order of cameras A and B matched the assigned physical location. Finally, the web server of each Raspberry Pi was started using a remote command from the main computer.\nAt this point, the Raspberry Pis were ready to begin recording. However, first, the IP address and the camera associated with each subject's smartwatch had to be configured. This ensured that the individual camera and the watch were targeting the same subject.\nOnce this was done for all students, the recording of the images (both individual and general) commenced. To terminate the experiment, a new command was issued. Given that the cessation of recording was not precisely simultaneous owing to a slight phase delay-a recording buffer was established both before and after the experiment. This buffer allowed for the subsequent processing of the data and its trimming to the precise duration of the experiment."}, {"title": "Data Acquisition", "content": "As mentioned, data acquisition was performed by centralizing commands and using embedded devices for every two cameras, with an additional smartwatch connected to the Raspberry Pi for each subject. After completing each recording session, the images were transferred from the Raspberry Pis to a hard drive. This allowed for the deletion of the previous day's content and ensured sufficient storage space for the new recording session."}, {"title": "Data Records", "content": "We present a structured dataset with the following directory organization for scientific research. This section reviews different aspects of the data records we provide. We focus on the technical features of each one. More information about how the data is organized can be found at the Code Availability."}, {"title": "General or Contextual Images", "content": "In each conducted experiment, comprehensive images capturing the overall context, shown in Figure 2, which include various participants, were collected from distant vantage points for every group involved. These images were captured from heights approximately ranging between 1.8 and 2.3 meters, utilizing an array of five or six cameras, contingent upon the specific experimental requirements. This arrangement was designed to ensure exhaustive coverage of the area, thereby effectively mitigating potential blind spots. The cameras recorded RGB images at a frame rate of approximately 9 fps. Each image was captured with a resolution of 1280x720 pixels, providing detailed spatial information for subsequent analysis."}, {"title": "Individual Images", "content": "For every participant in the study, individual RGB images focusing on the subjects' faces were captured. This specific approach facilitated the collection of detailed facial expressions and features, which are essential for studies on emotion and attention. In this context, the speed of image capture was prioritized above resolution to adequately capture microfacial expressions. The images were recorded at a frame rate of approximately 10 fps, ensuring temporal consistency across various datasets, and were captured with a resolution of 640x480 pixels, offering a good definition detail for each subject."}, {"title": "Sensor Data", "content": "This manuscript presents sensor data derived from various instruments, notably including the Linear Acceleration Sensor, Gyroscope, Light Sensor, Rotation Vector, and Cardiac Frequency Sensor, structured in a JSON file format. The measurements obtained from these sensors are quantified in standardized units, ensuring standardization and comparability across datasets. The configuration shown in Table 2 allows for the assembly of temporally dense datasets. Data points are consolidated at one-second intervals, promoting a systematic framework for data storage and retrieval. Consequently, to pinpoint a specific sensor reading, researchers are required to access the file corresponding to the second immediately following the desired measurement point."}, {"title": "Setup Configuration", "content": "As previously mentioned and illustrated in Figure 3 and Figure 4, a total of 20 individual cameras, indicated in the diagram as number 4, and clocks, marked as number 5, have been positioned according to the arrangement of the students during the recording session. Due to the technical limitations of the embedded devices, it is necessary to place one of these devices, identified as number 3, for every pair of cameras.\nIn the same figures, it is highlighted how the cameras intended for recording the context, marked with the number 2, also require, at a minimum, one embedded device for every two units. Finally, Figure 3 shows the personal computer designated to function as a server, labeled with the number 1. This server is intended to monitor and execute commands in a centralized manner to all embedded device units, in addition to handling the synchronization of time among the embedded devices, which, in turn, will synchronize their time with each of the smartwatches.\nIn the setup described, up to 26 Logitech HD Pro C920 cameras have been used, each featuring a 78-degree field of view. These cameras are associated with items labeled as number 2 and number 4 in the diagram. Additionally, 20 Samsung Galaxy Watch 5 smartwatches are utilized, one for each student, identified in the diagram as number 5. A Lenovo IdeaPad 5 Pro Gen 6 laptop has been chosen to serve as the server, corresponding to number 1 in the diagram. Lastly, the embedded devices are Raspberry Pi Model 4-B with 2GB of RAM."}, {"title": "Labeling", "content": "Labels are meticulously provided for each subject, with a dedicated file indicating the frames, levels of attention [6], and emotions assessed [5]. Attention is measured on a scale from one to five, with 5 indicating maximum attention. Emotion is categorized into nine categories: 9: Enjoyment, 8: Hope, 7: Pride, 6: Relief, 5: Anger, 4: Anxiety, 3: Shame, 2: Despair, 1: Boredom.\nAdditionally, proprietary software specifically tailored for this purpose has been developed. A total of five evaluators per student participate in this process, consisting of four experts and one self-evaluation, to ensure a comprehensive and balanced assessment. The fourth and fifth evaluators were supplemented to ensure these five tags per student.\nSelf-evaluation is conducted by students after each recording session, where they proceed to label the previous day's session. Subsequently, students are contacted again in cases where:\n1. the student did not attend and the session was not labeled,\n2. the labels do not meet the minimum threshold of three combined changes in emotion or attention per experiment. In this case, labeling is repeated until this threshold is met.\nThe labeled frames indicate changes in attention or emotion at the specified second, and evaluators are presented with only one frame per second."}, {"title": "Processing with Additional Tools", "content": "To facilitate the analysis and processing of the collected data, images of each participant undergo preprocessing with specialized algorithms. The results are meticulously recorded in a JSON file, which includes the rotation angle needed for horizontal eye alignment, determined using the Opal [11] tool; bounding boxes for the face and body, identified with the YOLO algorithm via the MiVOLO[12] tool; as well as demographic details like the participant's age and gender, obtained also using the MiVOLO tool. Additionally, landmarks for the hands[13] and the facemesh[14] are generated using MediaPipe. For the pose estimation we use Mediapipe tool that use BlazePose[15]. Finally, we employ DeepFace [16] to obtain ethnicity and emotion data. This tool provides a comprehensive dataset for advanced analysis of morphology and gestures. Although the ethnicity determined by this tool does not correspond to that assigned by [17], given its definition, Caucasian ethnicity is considered to encompass the White ethnicity."}, {"title": "Organization", "content": "The dataset is meticulously organized into three primary groups, each corresponding to different student cohorts involved in the classroom recordings. Within each cohort, the dataset is further subdivided into nine distinct experiments or scenarios, each scenario comprising individual folders for each subject. The directory structure is designed for intuitive navigation and efficient data access, adhering to a consistent format: /group1/experiment2/subject15.zip\nWithin each subject's compressed.zip file, the data is sorted into specific subfolders: images, watch_sensors, labels, and metadata. Each subfolder is dedicated to a particular type of information, thus ensuring a high level of organization and ease of access for researchers engaging with the dataset."}, {"title": "Technical Validation", "content": "This section presents a comprehensive qualitative and quantitative analysis to confirm the reliability of the data set. It provides a detailed assessment of the quality of the dataset, including an in-depth examination of the descriptive statistics of the subjects and the relationship between the biometric values 5 and 7.\nEmphasizing the examination of the dataset, we conducted a thorough review, analyzing the population data provided by a model rather than official records due to privacy concerns. This approach facilitated an inclusive analysis, taking into account diverse ages and gender variations, thus ensuring a multifaceted understanding of the dataset's applicability and robustness.\nUpon technical validation of the data presented in reference 5, an increase in heart rate was observed during the test experiment (number 9), which is consistent with the conditions of the experiment. Furthermore, the clustered distribution of the sensor, when applying a TSNE dimensionality reduction 7, evidenced behavioral differences between experiments 9 and 8 and the rest of the experiments, as indicated by the data captured by the smartwatch's rotation sensor.."}, {"title": "Quality and Reliability", "content": "1. Camera and Clock Synchronization: To synchronize the cameras and clocks, we utilized a software-based multi-device synchronization system that requests the time from a central server. Subsequently, we verified the synchronization through forced brightness changes in each experiment to confirm accurate synchronization, ensuring that, prior to the dataset's publication, the discrepancy between the clock timestamp and the camera's timestamp is less than 0.5 seconds.\n2. Completeness: We verified that the dataset extensively encompasses the intended domain, capturing all pertinent attributes, such as student attention levels, emotional states, and associated biometric indicators. This ensures a holistic view of the data's applicability to real-world scenarios.\n3. Consistency: Data collection methodologies were examined for uniformity across different sessions and environmentsFigure 3 and Figure 4. Consistent methods are vital for the dataset's reliability, ensuring that variations in data are genuine and not artifacts of the collection process. No significant data absence was detected beyond random variability due to the performance of embedded devices.\n4. Temporal Relevance: The dataset's currency was assessed, including the presence of timestamps where necessary. This enables the analysis of temporal trends and the dataset's relevance to contemporary studies."}, {"title": "Integrity and Data Structure", "content": "1. Data Structure: The organization of the dataset, including its schema and formatting, was scrutinized to confirm its efficacy in supporting efficient data retrieval and analysis. A well-structured dataset is foundational for robust research outcomes.\n2. Data Consistency: The consistency of the dataset was thoroughly examined to ensure uniformity across different sessions and environments. Consistent methods are vital for the dataset's reliability, ensuring that variations in data are genuine and not artifacts of the collection process.\n3. Data Types and Formats: It was verified that each column in the dataset employs the appropriate data type and format, thereby facilitating accurate and efficient analysis. Furthermore, the willingness to have a substantial number of labels on each subject was evaluated by obtaining those experiments most susceptible to changes in emotion and attention, Figure 6."}, {"title": "Future Work", "content": "As part of our ongoing efforts to enrich the dataset's diversity, future work will focus on expanding the heterogeneity of the student body. This will include incorporating students from various ethnic backgrounds and academic levels, recognizing the diverse profiles that access different university degrees or master's programs. Such an expansion is anticipated to further enhance the dataset's generalization capabilities and its applicability to a broader spectrum of educational and psychological studies."}, {"title": "Usage Notes", "content": "\"DIPSER\" presented in this study offers a comprehensive collection of multimodal data designed to support research in attention analysis, behavior understanding, and affective computing. To aid researchers in optimizing the use and impact of this dataset, we provide the following guidelines and considerations:"}, {"title": "Software Recommendations", "content": "\u2022 Image and Video Analysis: For the processing and analysis of RGB images and videos, software packages such as OpenCV for Python, MATLAB's Image Processing Toolbox, and the Deep Learning Toolbox are recommended. These tools facilitate the implementation and application of machine learning models.\n\u2022 Sensor Data Analysis: Tools like Pandas for Python are recommended for managing and analyzing time-series data efficiently. R packages, including dplyr and tidyr, are also suitable for data manipulation and analysis.\n\u2022 Facial and Emotion Recognition: For specialized analyses, such as facial and emotion recognition, frameworks like TensorFlow or PyTorch are recommended. These offer pre-trained models and libraries to streamline development."}, {"title": "Ethical and Privacy Considerations", "content": "Given the sensitive nature of attention data, which may include personal and biometric information, stringent privacy controls have been implemented:\n\u2022 Anonymization: All identifiable information has been either removed or anonymized in adherence to relevant data protection regulations, with the exception of faces, which are necessary for processing attention and emotion using deep learning algorithms.\n\u2022 Access Controls: Dataset access is regulated, requiring applicants to outline the intended data use. Researchers must commit to adhering to ethical guidelines concerning data utilization.\n\u2022 Use Restrictions: The dataset is strictly for academic and research purposes. Any commercial application intentions must receive explicit approval from the data custodians.\nThe university's ethics committee has approved the following measures after a comprehensive review focused on privacy and consent matters, particularly given the sensitive nature of the RGB and IMU data related to student attention and engagement. Moreover, we adhere to the guidelines of the Declaration of Helsinki[18] for research involving human subjects, as recommended by the World Medical Association (WMA) for application in other fields. This data will be used solely for academic purposes and not for commercial objectives. The study was conducted in accordance with the Declaration of Helsinki [18] and was approved by the Ethics Committee of the University of Alicante, where the students were recorded. All participants signed an informed consent previous to the experiments. The dataset has been made publicly available under the CC-BY license. These data will be utilized exclusively for academic purposes and will not be employed for commercial purposes."}, {"title": "Code Availability", "content": "We have uploaded Python code on GitHub, available at https://bitbucket.org/rovitlib/dipser/, which is capable of reading labels at each time instance, in addition to sensor data and images. This code intelligently groups the data with context from cameras to efficiently display a pandas DataFrame, effectively labeling each data point for individual students. Any relevant information about the code can be found at the repository link."}]}