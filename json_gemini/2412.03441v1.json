{"title": "PBP: Post-training Backdoor Purification for Malware Classifiers", "authors": ["Dung Thuy Nguyen", "Ngoc N. Tran", "Taylor T. Johnson", "Kevin Leach"], "abstract": "In recent years, the rise of machine learning (ML) in cybersecurity has brought new challenges, including the increasing threat of backdoor poisoning attacks on ML malware classifiers. These attacks aim to manipulate model behavior when provided with a particular input trigger. For instance, adversaries could inject malicious samples into public malware repositories, contaminating the training data and potentially misclassifying malware by the ML model. Current countermeasures predominantly focus on detecting poisoned samples by leveraging disagreements within the outputs of a diverse set of ensemble models on training data points. However, these methods are not suitable for scenarios where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove backdoors from a model after it has been trained. Addressing this scenario, we introduce PBP, a post-training defense for malware classifiers that mitigates various types of backdoor embeddings without assuming any specific backdoor embedding mechanism. Our method exploits the influence of backdoor attacks on the activation distribution of neural networks, independent of the trigger-embedding method. In the presence of a backdoor attack, the activation distribution of each layer is distorted into a mixture of distributions. By regulating the statistics of the batch normalization layers, we can guide a backdoored model to perform similarly to a clean one. Our method demonstrates substantial advantages over several state-of-the-art methods, as evidenced by experiments on two datasets, two types of backdoor methods, and various attack configurations. Our experiments showcase that PBP can mitigate even the SOTA backdoor attacks for malware classifiers, e.g., Jigsaw Puzzle, which was previously demonstrated to be stealthy against existing backdoor defenses. Notably, our approach requires only a small portion of the training data only 1% to purify the backdoor and reduce the attack success rate from 100% to almost 0%, a 100-fold improvement over the baseline methods. Our code is available at https://github.com/judydnguyen/pbp-backdoor-purification-official.", "sections": [{"title": "I. INTRODUCTION", "content": "Malware classification has been witnessed dramatic ad- vancements, particularly with the integration of Deep Neu- ral Networks (DNNs) to tackle the increasing complexity and volume of modern malware corpora [1, 2, 3]. Malware ensembles-including viruses, worms, trojans, and spyware\u2014 pose formidable risks to individuals and corporate institutions [4, 5]. The limitations of conventional detection methods, such as signature-based and heuristic techniques, in handling large- scale data and evolving malware variants have necessitated a shift toward more sophisticated Machine Learning (ML) and Deep Learning (DL) methodologies [6, 7, 8]. Malware detec- tion techniques based on ML/DL can model more complex patterns of malware data than classical signature-based ones. This allows them to detect better new variants of existing malware or even previously unseen malware [9]. Integrating DNNs into malware detection systems has significantly advanced the field but has also introduced sev- eral threats. One notable vulnerability is the backdoor at- tack, [10, 11, 12, 13]. which involves an adversary discreetly embedding a harmful pattern or trigger within a small pro- portion of training samples to manipulate the behavior of the final model. When the model encounters this trigger during inference, it will misclassify the input, potentially leading to security breaches. These attacks subtly corrupt the model, often going undetected until the model systematically fails under specific conditions designed by the attacker, thereby undermining the integrity and reliability of the entire malware detection process [14, 15, 11]. . Recently, research has focused on investigating the vulnerability of malware classifiers to backdoor attacks [14, 16, 10, 17]. The general objective of these attack methods is to protect a subset of malware samples and bypass the detection mechanisms of malware classifiers. This emerging threat poses concerns similar to those observed in other machine learning domains and is gaining substantial attention in the malware detection community, underscoring the need for robust countermeasures [18]. To counter such threats, researchers have explored defense methods with backdoor attacks for malware classifiers. While these defenses show their effectiveness in mitigating backdoor attacks in machine learning models, the effectiveness of these countermeasures in the specific context of malware classifiers has been limited [14, 16]. For example, the study by Severi et al. [14] demonstrates that even anomaly detection methods [19, 20] are not sufficient to fully protect malware classifiers against explanation-guided backdoor attacks. More recently, Yang et al. [16] have shown that their attack can bypass state-of-the-art defenses such as Neural Cleanse [21] and MNTD [22]. These findings highlight the need for more robust countermeasures tailored to the malware detection. Another limitation is that these defenses often rely on assumptions about the backdoor, requiring intervention with all training data [11]. Therefore, they cannot be applied to post-defense circumstances, such as fine-tuning or backdoor removal in Machine Learning as a Service (MLaaS). A post-defense solution is essential when defenders acquire pretrained or publicly available backbone models for malware detection, either from third-party vendors or open-source repositories. When analysts discover malware samples mislabeled as safe, fine-tuning the model with small"}, {"title": "II. BACKGROUND", "content": "This section provides background on backdoor attacks and defenses against malware classifiers. We start with high- level descriptions of emerging threats of backdoor attacks on machine learning (ML) systems and classical defenses against them in Sec. II-A and Sec. II-B, respectively. In Sec. II-C, we discuss how backdoor attacks work on Deep Neural Networks (DNN)-based malware classifiers and their countermeasures."}, {"title": "A. Backdoor Attacks", "content": "The emergence of outsourcing model training and MLaaS has led to emergent weaknesses [25, 26]. Backdoor attacks [27, 28, 26, 29, 30, 31], or trojan attacks, are prevalent training- phase adversarial attacks that primarily target DNN classifiers. In general, backdoor attacks can be formulated as a multi- objective optimization problem [32], where the attacker seeks to optimize the following objectives:\n\n$\\\\theta^* = min \\\\mathbb{E}_{(x,y) \\\\sim D} L(f_\\\\theta(x), y) + \\\\mathbb{E}_{(x,y) \\\\sim D} L(f_\\\\theta(\\varphi(x)), \\\\tau(y)),$ (1)\n\nin which $f_\\\\theta$ is the victim model $f$ parameterized by $\\\\theta$, $D$ is the set of training data for the main task, and $(x,y)$ are sample-label pairs uniformly drawn from $D$. The sam- ple components $x$ are poisoned via the application of some function $\\\\varphi$, which can be a non-transform function [13] or a perturbation function [33, 34]; and the label counterparts $y$ are altered by another corresponding function $\\\\tau$. Technically, the adversary's objective is to manipulate the model such that, for these poisoned samples $\\\\varphi(x)$, it returns distorted outputs $\\\\tau(y)$ instead of $y$. The function $L$ in the expression $L(f_\\\\theta(\\varphi(x)), \\\\tau(y))$ represents a loss function that measures the discrepancy between the predicted output $f_\\\\theta (\\\\varphi(x))$ and the true output $y$ for a given input sample $(x, y)$. To ensure stealthiness, the performance of the model on non-backdoored samples remains unchanged. In particular, the model $\\\\theta^*$ should give correct outputs for clean samples $x$. Backdoor attacks are particularly concerning as they can be stealthy and difficult to detect, making them a substantial threats to deploy secure and reliable DNN models."}, {"title": "B. Backdoor Countermeasures", "content": "Developing robust techniques to identify and mitigate the various types of backdoor attacks remains an important chal- lenge in machine learning security research. In contrast to the attack scenario, the multi-objective formulation for backdoor defense is defined as:\n\n$\\\\theta^* = min \\\\mathbb{E}_{(x,y) \\\\sim D} L(f_\\\\theta(x), y) - \\\\lambda \\\\mathbb{E}_{(x,y) \\\\sim D} L(f_\\\\theta(\\varphi(x)), \\\\tau(y)),$ (2)\n\nwhere the first term also minimizes the loss on clean data samples, but the second one maximizes the loss on backdoored data samples (note the negative sign). The tradeoff between preserving clean data performance and backdoor removal can be controlled by a hyperparameter, $\\\\lambda$. To measure how well a backdoor defense scheme performs, we employ these two metrics:\n\nDefinition 1 (Clean Data Accuracy (C-Acc)). The C-Acc is the proportion of clean test samples containing no trigger that is correctly predicted to their ground-truth classes.\n\nDefinition 2 (Attack Success Rate (ASR)). The ASR is the proportion of clean test samples with stamped triggers that is predicted to the attacker's targeted classes.\n\nDefinition 3 (Defense Effectiveness Rating (DER) [35]). $DER \\\\in [0, 1]$ evaluates defense performance considering both the changes in C-Acc and ASR. It is defined as follows:\n\n$DER = [max(0, \\\\Delta ASR) - max(0, \\\\Delta C-Acc) + 1]/2$\n\nFor successful backdoored model $f_{\\\\theta_{ba}}$, the CDA should be similar to the clean model $f_{\\\\theta_c}$, while the ASR is high backdoored models can achieve an ASR that is close to 100% usually under outsource attributing to the full control over the training process and data. Backdoor defenses can be deployed at different stages of the deep learning pipeline: during the classifier's training phase, post-training, or during inference. Each scenario assumes a different defender role and"}, {"title": "C. Backdoor Attacks and Countermeasures in Malware Classifiers", "content": "Backdoor attacks for malware classifiers. In the backdoor attack setting, the adversary is assumed to have only partial control of the training process. Specifically, in malware classi- fication, recent work assumes clean-label backdoor attacks [10, 14, 16] where the adversary has no control over the labeling of poisoned data. In these studies, the authors optimize a trigger or watermark within the feature and problem space of malware, which, when integrated with malware samples, activates the backdoor functionality. Specifically, the backdoored sample set has the form of $D_{bd} = (\\\\varphi(x), y)$ instead of $D_{bd} = (\\\\varphi(x), \\\\tau(y))$ as normal label-flipping attacks in ML [28, 46, 21, 33]. The optimized trigger is normally conducted in a model- agnostic fashion manner via feature explanations for model decision [14, 10] and alternative optimization [16].\n\nThe high-level idea of these backdoor attack strategies is to generate a trigger function $\\\\varphi$, known as a watermark, to combine with the targeted samples. Due to the characteristics of the targeted samples, the attack can be family-targeted or non-family-targeted. For example, Severi et al. [14] use the trigger to distort the model's prediction on any malware samples that carry the trigger to be \"benign.\" On the other hand, Yang et al.[16] only target the samples belonging to a specific malware family, and the trigger is designed for this family only and cannot activate the backdoor if it is combined with other families. We plot the poisoned samples generated by these two backdoor categories in Fig. 1. As presented, non- family-targeted poisoned samples are manipulated from the set of all other malware families, while in a family-targeted setting, these poisoned samples belong to a separate malware family that the adversary wants to protect."}, {"title": "III. METHODOLOGY", "content": "In this section, we present PBP, an approach to purifying classification models that have been poisoned. First, we discuss our insight into the activation distribution of backdoor neurons during training (Section III-A). Second, we discuss the threat model and goals of PBP (Section III-B). Third, we present a detailed description of PBP (Section III-C)."}, {"title": "A. Backdoor Neurons", "content": "When a classifier has been poisoned with a backdoor attack, there is a specific subset of neurons that play a substantial role in exhibiting the backdoor behavior [53, 54]. The attack success rate will be dramatically lowered if a portion or all of these backdoor neurons are pruned from the infected model [53, 41]. Leveraging this insight, we investigate the activation distribution of the model when subjected to backdoor attacks. In this subsection, we perform an empirical analysis to examine the distribution of the backdoor neurons when a backdoor is introduced into the model. We begin with a definition of backdoor neurons.\n\nDefinition 4 (Backdoor Neurons). Given a model f and a poisoning function $\\\\varphi$, the backdoor loss on a dataset D is defined as:\n\n$L_{bd}(f_\\\\theta) = \\\\mathbb{E}_{(x,y) \\\\sim D} [DCE(y, f_\\\\theta(\\varphi(x))]$\n\nwhere DCE denotes the cross entropy loss.\n\nDefinition 5 (Backdoor Sensitivity). Given a model f, the index of a neuron (l,k) and the backdoor loss $L_{bd}$, the"}, {"title": "B. Threat Model and Problem Formulation", "content": "A large fraction of the backdoor attack literature [56, 53, 42, 57] assumes the threat model of \"Outsourced Training Attack,\" in which the adversary has full control over the training procedure and the end user is only allowed to check the training using a held-out validation dataset. However, adversaries introducing backdoor attacks ensure that the victim model performs well on clean data, making the reliance on a held-out validation dataset insufficient for verifying the model's trustworthiness. This presents a strict assumption for users regarding the safe use of DNN models. To address this challenge, we adopt a defense setting where the defender acquires a backdoored model from an untrusted source and assumes access to a small subset of clean training data for fine-tuning [58, 44]. Backdoor erasing aims to eliminate the backdoor trigger while maintaining the model's performance on clean samples. This approach is particularly relevant when training data is no longer fully accessible due to retention or privacy policies. Additionally, users of third-party ML services may inadvertently purchase backdoored models and seek to purify them using their data. [16, 14]\n\nAttacker's goals. Similar to most backdoor poisoning set- tings, we assume the attacker's goal is to alter the training procedure, specifically the malware sample set, such that the resulting trained backdoored classifier, $f_{bd}$, differs from a cleanly trained classifier $f_{cl}$. An ideal $f_{bd}$ has the same response to a clean set of inputs $x$ as $f_{cl}$, whereas it generates an adversarially chosen prediction, $\\\\tau(y)$, when applied to backdoored inputs, $f(\\varphi(x))$. These goals can be summarized as:\n\n$f_{bd}(x) = f_{cl}(x); f_{bd} (\\\\varphi(x)) = \\\\tau(y) \\\\neq y.$\n\nSpecifically, we use class 0 for benign binaries and class 1 for malicious. Given the opponent is interested in making a malicious binary appear benign, the target result is always $\\\\tau(y) = 0$. Additionally, we also consider family-based mal- ware classification, in which the adversary aims to manipulate the surrogate model to classify the specific samples into one targeted malware file. To make the attack undetectable, the adversary wishes to minimize both the poisoning rate and the footprint of the trigger (i.e., the number of modified features).\n\nDefender's goal. As opposed to the attacker goals, the defender, i.e., the model trainer who has full access to the internal architecture of the target model and a limited set of benign fine-tuning data, denoted as $D_{ft}$, aims to achieve two goals. The first goal is to erase the backdoors from $f_{bd}$ and make the purified model perform correctly even with triggered"}, {"title": "C. PBP Approach", "content": "In this subsection, we discuss the key ideas behind PBP, which consists of two steps: (i) neuron mask generation and (ii) activation-shift fine-tuning.\n\n1) Neuron Mask Generation: Motivated by the observation of activation drift, the key insight is that correcting this deviation could effectively mitigate backdoor effects. In this step, we first train a model, i.e., $\\\\theta_0$, from scratch to realign the activation of the new model with that of the backdoored model, $\\\\theta_{bd}$, then we find the backdoored neurons $N_m$ by analyzing the gradient trace $\\\\nabla_{\\\\theta}L$ during this training procedure. It is worth noting that, this procedure does not output the new fine-tuned model to be used as-it-is since the defender does not have enough data to train a model from scratch. Instead, we want to observe how a clean model changes to match and mimic the behavior of the backdoored one. From that, we detect the most important neurons leading this change and consider them as potential backdoor neurons contributing to the reconstruction of backdoor function.\n\nAssume model f has total of L Batch Normalization (BN) layers. Each BN layer records the running mean and variance of the input during training (which includes both clean samples and adversarially poisoned samples), denoted as B ="}, {"title": "IV. EXPERIMENTS", "content": "As argued above, a robust backdoor purification method should have the ability to effectively mitigate the impact of multiple backdoor attacks, maintain stability across varying attacker power and fine-tuning conditions, and perform effi- ciently in multiple settings and architectures. Therefore, we study four research questions to evaluate the efficiency of PBP in purifying backdoor attacks targeting malware classifiers as follows:\n\n1) RQ1: How well does PBP purify backdoor attacks compared to related fine-tuning methods?\n\n2) RQ2: Is PBP effective against backdoor attacks car- ried out by attackers with varying levels of strength?\n\n3) RQ3: Can PBP purify backdoor attacks stably under different fine-tuning assumptions?\n\n4) RQ4: How is PBP's efficiency and sensitivity to its hyperparameters and model architectures?\n\nWe first discuss our experimental settings, baselines, and metrics below. Then, we answer each research question in turn.\n\nBackdoor attacks and settings. In this work, we focus on two state-of-the-art backdoor attack strategies designed for malware classifiers, which are Explanation-guided [14] and Jigsaw Puzzle [16]. Regarding the former attack, experiments are conducted mainly on the EMBER-v1 [23] dataset. To study the latter attack, we conduct experiments on the Android mal- ware dataset sampled from AndroZoo [24]. The EMBER-v1 dataset consists of 2,351-dimensional feature vectors extracted from 1.1 million Portable Executable (PE) files for Microsoft Windows [14]. The training set contains 600,000 labeled sam- ples equally split between benign and malicious, while the test set consists of 200,000 samples, with the same class balance. All the binaries categorized as malicious were reported as such by at least 40 antivirus engines on VirusTotal [14]. For the Jigsaw Puzzle attack, we reuse a sampled dataset from the AndroZoo collection of Android applications, following the setting of the original paper [16]. The feature vectors for the Android apps were extracted using Drebin [68]. Each feature in the Drebin feature vector has a binary value, where \u201c1\u201d indicates that the app contains the specific feature (e.g., an API, a permission), and \u201c0\u201d indicates that it does not. The final dataset consists of 149,534 samples, including 134,759 benign samples and 14,775 malware samples. The dataset covers 400 malware families, with the number of samples per family ranging from 1 to 2,897, with an average size of 36.94 and a standard deviation of 223.38. Both datasets are used for the binary classification task, and the backdoor task is classifying the malware samples given the presence of trigger as \"benign\"."}, {"title": "A. Experimental Setups", "content": "The full algorithm is presented in Algo. 1. We further provide the proof of convergence followed Theorem 1 for PBP and leave all proofs of theoretical development in the Appendix.\n\nTheorem 1. Let $\\\\theta_0$ be the initial pretrained weights (i.e. line 13 in algorithm 1). If the fine-tuning learning rate satisfies:\n\n$\\\\eta <  \\\\left[\\\\frac{2L(w,x)}{||\\\\partial w^2}||_2\\\\right]^{-1}$,\n\nalgorithm 1 will converge."}, {"title": "V. RELATED WORKS", "content": "In this section, we discuss the current body of work on backdoor attacks and defenses for malware classification, specifically backdoor purification during fine-tuning."}, {"title": "Backdoor attacks for malware classification.", "content": "Even though backdoor attacks have been extensively studied in the image domain, most of them cannot be applied to malware classifiers due to two main reasons: (1) incompatible domain (e.g., style transfer does not apply even in the case where the binary is represented as an image), and (2) realizing the trigger embedding from the feature space to the problem space is very difficult, as feature extraction is not bijective [81, 82]. Overcoming these concerns, Li et al. [83] devised a feasible backdoor attack by appropriately using evolutionary algorithms to generate realizable triggers. However, the full-access as- sumption in their work is usually too strong for the case of malware classification, as multiple trusted third-party AV vendors usually perform labeling. Severi et al. [14] relaxed this requirement by introducing a clean-label backdoor attack. Similarly, Yang et al. [16] proposed a selective backdoor that improves stealthiness and effectiveness, following the intuition that a malware author would prioritize protecting their own malware family instead of all malware in general."}, {"title": "Backdoor countermeasures for malware classifications.", "content": "Similar to the image space setting, the most popular defense mechanism against backdoor attacks for malware classification is adversarial training [48], in which the model is trained and/or fine-tuned to correctly predict on adversarially crafted samples. However, generating new adversarial examples for the model at every epoch is very computationally intensive, and can take much longer than traditional training [84]. Another approach leverages various heuristics to remove adversarial samples from the training dataset. This way, any manipulations introduced by adversaries can be undone before the samples are sent to the malware detector. However, these empirical defenses usually only work for very few adversarial attack methods, and are thus attack-specific [49]. For the situation where the end user only has limited additional labeled clean data and/or the required resources for retraining becomes infeasible, previous works have adapted image-space backdoor detection mechanisms [36, 22] to malware [85]. However, the ultimate results were not compelling, sometimes performing only as well as random guessing."}, {"title": "Backdoor purification during fine-tuning.", "content": "Fine-tuning has been proven to work well as a post-training defense mechanism"}, {"title": "VI. CONCLUSION", "content": "In this paper, we present PBP, a post-training backdoor purification approach based on our empirical investigation of distributions of neuron activations in poisoned malware clas- sifiers. PBP makes no assumptions about the backdoor pattern type or method of incorporation using a small amount of clean data during the fine-tuning process. By leveraging the distinct activation patterns of backdoor neurons, PBP employs a two- phase strategy: generating a neuron mask from clean data and applying masked gradient optimization to neutralize back- door effects. Our extensive experiments demonstrate PBP's effectiveness and adaptability, outperforming existing defenses without requiring prior knowledge of attack strategies. This approach substantially enhances the security and reliability of malware classification models in real-world applications."}, {"title": "A. Description & Requirements", "content": "This section lists all the information necessary to recreate the experimental setup we used to run our artifact.\n\n1) How to access: We make our code and dataset publicly available at https://github.com/judydnguyen/ pbp-backdoor-purification-official. The artifact materials for this paper are permanently available at DOI: https://doi.org/10.5281/zenodo.14253945.\n\n2) Hardware dependencies: an NVIDIA A6000 GPU is encouraged but not required. Our artifacts can be run on a commodity desktop machine with an x86-64 CPU with storage of at least 22GB for data only. To ensure that all artifacts run correctly, it is recommended to use a machine with at least 16 cores and 48 GB of RAM. In the original experiments, we set the number of workers to 54, but for a computer with a smaller number of cores, the number of workers could be reduced, so we set the default as 16. Our code can be run using a CPU if a GPU is not available. However, the using of CPU may not completely ensure to achieve numbers we reported.\n\n3) Software dependencies: the required packages are listed on environment.yml. We encourage installing and man- aging these packages using conda. Our artifacts have been tested on Ubuntu 22.04.\n\n4) Benchmarks: In our experiments, we used two datasets. 1. Ember-v1 dataset, accessed at https://github.com/elastic/ ember. 2. AndroZoo dataset, accessed at https://androzoo.uni. lu/. We included the implementation for all the baselines used in our works in our code repository. We provide a cloud link to share both processed datasets here. After downloading and decompressing file NDSS603-data-ckpt.zip, the structure of this folder is as follows:\n\nNDSS603-data-ckpt/\napg/\nember/\nckpts/"}, {"title": "B. Artifact Installation & Configuration", "content": "We first require that our repository at https://github.com/ judydnguyen/pbp-backdoor-purification-official is downloaded local folder, e.g., via git clone or .zip download. Then, we require downloading datasets and a checkpoint mentioned in Section A4.\n\nWe conduct all the experiments using PyTorch version 2.1.0 and run experiments on a computer with an Intel Xeon Gold 6330N CPU and an NVIDIA A6000 GPU. In our original environment, we use Anaconda at https://anaconda.org/ to manage the environment dependencies efficiently, ensuring the reproducibility of our experiments. Specifically, we create a virtual environment with conda version 23.7.3 and install all necessary packages, including PyTorch, NumPy, and Scikit- learn, as detailed in the provided environment.yml file. The new environment can be created using:\n\nconda config --set channel_priority flexible && conda env create --file=environment.yml && conda activate pbp-code"}, {"title": "C. Experiment Workflow", "content": "We already set up the environments and data required for our experiments in the Amazon Cloud instance. There- fore, the previous steps can be skipped. Our artifacts contain two independent experiments. The first experiment consists of (i) training and (ii) purifying a backdoor model with the EMBER dataset. The second experiment consists of (i) training and (ii) purifying a backdoor model with the AndroZoo dataset. The proposed workflow runs the two experiments sequentially. Our repository contains scripts and a bash file that can be used to automate all experiments."}, {"title": "D. Major Claims", "content": "(C1): PBP achieves better backdoor purification per- formance compared to baselines in terms of the lowest Backdoor Accuracy (BA) it can produce on the EM- BER dataset. This is proven by the experiment (E1) whose results are illustrated/reported in [TABLE II: Performance of Fine-tuning Methods under Explain- Guided Backdoor Attacks on EMBER].\n\n(C2): PBP achieves better backdoor purification per- formance compared to baselines in terms of the low- est Backdoor Accuracy (BA) it can produce on the AndroZoo dataset. This is proven by the experiment (E2) whose results are illustrated/reported in [TABLE II: Performance of Fine-tuning Methods under Jigsaw Puzzle Backdoor Attacks on AndroZoo]."}, {"title": "E. Evaluation", "content": "This section includes all the operational steps and exper- iments that must be performed to evaluate our artifacts and validate our results. In total, all experiments require between"}, {"title": "F. Notes", "content": "To facilitate the evaluation, we have set up the environment and prepared data beforehand at our provided cloud server. By using this setup, we can skip all the preparation steps and start directly with the actual reproduction. For example,\n\nssh ubuntu@$IP\ncd pbp-code/PBP-BackdoorPurification\nconda activate pbp-code\n./train_backdoor_ember.sh\n./experiment1_finetune_backdoor_ember.sh"}, {"title": "TECHNICAL APPENDIX", "content": "This document serves as an extended exploration of our research, providing an overview of our methods and results. Appendix A provides a comprehensive discussion of the train- ing process, including datasets, model structures, and configu- rations used to reproduce the reported results. Next, we provide the proof for our theorems in Appendix B. Additionally, we present supplementary results not included in the main paper in Appendix C. Following that, we conduct an extensive analysis of the effect of the fine-tuning dataset on the PBP's performance and the rationale of intuition from our method in Appendix D. We demonstrate the potential efficacy of our method in the Computer Vision (CV) domain with multiple backdoor attacks and model architectures in Appendix E."}, {"title": "A. Training configurations", "content": "Table IV presents the training and fine-tuning config- urations. With both datasets, we use MLP binary classifier models with three hidden layers as following previous works on malware classifiers [87, 16, 88]. We conduct all the exper- iments using PyTorch version 2.1.0 [89] and run experiments on a computer with an Intel Xeon Gold 6330N CPU and an NVIDIA A6000 GPU."}, {"title": "B. Proof of convergence", "content": "Theorem 1. Let wo be the initial pretrained weights (i.e. line 13 in algorithm 1). If the fine-tuning learning rate satisfies:\n\n$\\$\\eta <  \\\\left[\\\\frac{2L(w", "w^2}||_2\\\\right": {"converge.\n\nProof": "Starting with some weight set wo", "follows": "n\n$Wi+1 = Wi - (-1)^i \\\\eta  \\\\frac{\\\\partial L(w", "following": "n\n$\\frac{\\\\partial L(w", "8": "n\n$Wi+1 = Wi - (-1)^i \\\\eta  \\\\frac{\\\\partial L(w", "Bigg\\|_{wo}\\\\Big": "n\n$=Wi - (-1)^i \\\\eta  \\\\Bigg[w + (Wi-wo)W\\\\Bigg", "themselves": "n\n$W2k+2 =  \\\\sum_{i=2k"}, "Wi-wo)W\\\\Bigg": "n\n$= -  \\\\eta  (W2k+1 - W2k)W$\n\nwhich gives\n\n$W2k+2 - W2k+1 = (W2k+1 - W2k)(-I -  \\\\eta W).$ (9)\n\nSimilarly", "calculated": "n\n$W2k+3 =  \\\\sum_{i=2k+1"}, {"Wi-wo)W\\\\Bigg": "n\n$=  \\\\sum_{i=2k+1"}, {"Wi-wo)W\\\\Bigg": "n\n$=  \\\\eta  (W2k+2 - W2k+1)W", "relation": "n\n$W2k+3 - W2k+2 = (W2k+1 - W2k)(-I -  \\\\eta W)(-I +  \\\\eta W)$\n\n$= (W2k+1 - W2k)(I -  \\\\eta^2 W^2)$\n\n$= (w1 - wo)(I -  \\\\eta^2 W^2)^{k+1"}, "n\n$=  \\\\eta w (I -  \\\\eta^2 W^2)^{k+1}.$ (11)\n\nThe matrix power is legal because W is a Hessian matrix, which makes it symmetric, and as a result $(I -  \\\\eta^2 W^2)$ is also symmetric. The even-odd index pair difference can also be derived in the same vein:\n\n$W2k+2 - W2k+1 = (W2k+1 - W2k)(-I -  \\\\eta W)$\n\n$= (w1 - wo)(-I -  \\\\eta W)(I -  \\\\eta^2 W^2)^{k}$\n\n$=  \\\\eta w (-I -  \\\\eta W)(I -  \\\\eta^2 W^2)^{k}.$ (12)\n\nIt is straightforward to see that if a symmetric matrix's largest eigenvalue is smaller than 1, its exponential will converge to the zero matrix. We also have the property of a matrix's 2-norm from its operator norm:\n\n||Ax ||2 \u2264 ||A||2||X||2.\n\nTherefore"]}