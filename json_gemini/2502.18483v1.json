{"title": "Modeling Churn in Recommender Systems with Aggregated Preferences", "authors": ["Gur Keinan", "Omer Ben-Porat"], "abstract": "While recommender systems (RSs) traditionally rely on extensive individual user data, regulatory and technological shifts necessitate reliance on aggregated user information. This shift significantly impacts the recommendation process, requiring RSs to engage in intensive exploration to identify user preferences. However, this approach risks user churn due to potentially unsatisfactory recommendations. In this paper, we propose a model that addresses the dual challenges of leveraging aggregated user information and mitigating churn risk. Our model assumes that the RS operates with a probabilistic prior over user types and aggregated satisfaction levels for various content types. We demonstrate that optimal policies naturally transition from exploration to exploitation in finite time, develop a branch-and-bound algorithm for computing these policies, and empirically validate its effectiveness.", "sections": [{"title": "Introduction", "content": "Recommender systems (RSs) have become essential in digital media and e-commerce. They collect massive amounts of data and apply sophisticated techniques to improve user engagement and satisfaction. RSS leverage past user interaction, demographics, and possibly additional information to provide personalized recommendations. For instance, Netflix and Amazon utilize these techniques to offer tailored movie recommendations and product suggestions, respectively. While RSs typically collect and record user data, some have limited access to individual user information or none at all. For instance, regulations such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) impose stringent rules on data usage and user consent. Prioritization of privacy is also a trend within commercial companies, e.g., Apple's iOS 14 opt-in device tracking modification that has affected targeted advertising [17]. In some cases, the RS can record user sessions of varying length, but cannot identify users. While these regulatory shifts aim to protect privacy, they pose substantial challenges to RSs in maintaining the same level of service. Due to these privacy-preserving limitations, RSs may at times be limited to utilizing aggregated user information, such as clusters of users or personas.\n\nRelying solely on aggregated information significantly affects the recommendation process. When interacting in a user session without accurate individual data, the RS must explore more intensively to understand the current user's preferences. This initial exploration phase is critical for gathering enough data to make accurate recommendations later in the session. However, aggressive exploration bears the risk of user churn, where users may leave the system due to receiving unsatisfactory recommendations. For example, when a song-recommendation RS encounters a user with unknown preferences, it could suggest a song from an unconventional genre, which some users enjoy while most users dislike. Although user feedback on that unconventional genre can provide valuable insights into their preferences, it can cause users to leave the RS if the recommendation is off-target; thus, the RS should address the possibility of churn as part of its design.\n\nIn this paper, we propose a model to study the intertwined challenges of aggregated user information and churn risk. We call our model REC-APC, standing for Recommendation with Aggregated Preferences under Churn. Our model assumes that the RS has a probabilistic prior over user types (clusters, personas, etc.)"}, {"title": "Our contribution", "content": "Our contribution is three-fold. First, we are among the first to address aggregated user preferences and the risk of churn simultaneously. We propose a model where the general population preferences are known and the type (cluster, persona, etc.) of users are hidden but can be deduced through interaction. This interaction drives user engagement, which we model as the RS's reward. However, due to uncertainty about user type, the RS could generate unsatisfactory recommendations that can cause user churn. Our model poses a novel exploration-exploitation trade-off with the risk of churn, an aspect under-explored in the current literature.\n\nOur second contribution is technical. Analyzing the model shows that optimal policies converge after a finite number of recommendations, symbolizing a transition to pure exploitation. Informally,\n\nTheorem 1.1 (Informal statement of Theorem 4.1). For a broad range of instances, the (infinite-length) optimal policy converges.\n\nOur third contribution is algorithmic. We leverage our theoretical results to develop a straightforward yet state-of-the-art branch-and-bound algorithm designed explicitly for our setting. As the problem we address can be described as a partially observable Markov decision process (POMDP), we compare our algorithm with the state-of-the-art POMDP benchmark [18]. In practice, our algorithm performs better when there is a large variety of user types but is less effective when the number of contents is significantly larger than the number of user types."}, {"title": "Related work", "content": "Our work captures several phenomena: First, we assume that the RS has aggregated user information, but no access to individual user information. Second, we have sequential interaction, where we can balance exploration-exploitation trade-offs. And third, our model includes the risk of user churn. Below, we review the relevant literature strands.\n\nAggregated user information Our model follows the trend of using clustered data in the recommendation process [32]. In addition to improved efficiency, the use of clustering can increase the diversity and reliability of recommendations [12, 42] and handle the sparsity of user preference matrices [4].\n\nAggregated information also relates to privacy, a topic that has gained much attention recently, following the seminal work of Dwork [11] on differential privacy. Several works propose RSs that satisfy differential privacy [23, 43, 46]. In a broader context, Tsai et al. [36] have empirically shown that users value their privacy and are willing to pay for it. Several other definitions of privacy were suggested in the literature [2, 8, 28]. In our work, we assume that the RS has access to aggregated data, akin to other recent works addressing lookalike clustering [16, 25].\n\nThe cases where the RS has little information about users or items are called cold-start problems. This issue relates to our work because, while we assume access to aggregated information, every user interaction starts from a tabula rasa. Broadly, solutions are divided into data-driven approaches [3, 29, 47] and method-driven approaches [20, 21, 40] (see Yuan and Hernandez [41] for a recent survey).\n\nThis paper is inspired by clustering, privacy, and cold-start problems. However, our model only assumes access to aggregated information and abstracts the reasons why individual information is unavailable. Notably, we do not propose techniques to cluster users, address privacy concerns, or provide new approaches for the cold-start problem."}, {"title": "Problem Definition", "content": "In this section, we formally introduce our model alongside the optimization problem the recommender system aims to solve. We then provide an illustrative example and explain why this problem necessitates careful planning by showing the sub-optimality of naive and seemingly optimal solutions."}, {"title": "Our Model", "content": "In this section, we formally define the Recommendation with Aggregated Preferences under Churn, or REC-APC for abbreviation. An instance of the problem consists of several elements, as we formally describe below.\n\nWe consider a set U of users who interact with the RS. The RS does not have access to user information; instead, it relies on aggregated information about the users. Specifically, we assume that there is a finite set M of user types, with each type representing a persona, i.e., a cluster of homogeneous users with similar preferences. Each user in U is associated with precisely one user type in M according to its preferences. Accordingly, we denote by $m(u) \\in M$ the type of user $u \\in U$. The RS has a prior distribution $q$ on the elements of M, that is $q \\in \\Delta(M)$. This prior distribution reflects the likelihood of a new user being of any given type.\n\nIn this paper, we assume that contents are abstracted into broader categories, each representing a group of similar items. This abstraction allows the RS to recommend content by selecting from a finite set K of categories.\n\nLastly, there is a user-type preferences matrix P with dimensions $[0, 1]^{K\\times M}$. Each element $P(i, j)$ signifies the likelihood that category $i$ satisfies a user of type $j$. We stress that P contains information on user types but not on specific users. The RS has complete information on the user-type preference matrix P and prior user-type distribution q.\n\nUser session A user session starts when a user $u \\in U$ enters the RS. The RS lacks access to any information about u; thus, it only knows that $m(u)$ is distributed according to q. The session consists of rounds. In each round t, for $t \\in \\{1, ...,\\infty\\}$, the RS recommends a category $k_t \\in K$. Afterward, the user gives binary feedback: They either like the item, occurring with a probability of $P(k_t, m(u))$, or they dislike it with the complementary probability. If the user likes the recommended category, the RS receives a reward of 1, and the session continues for another round. However, when the user ultimately dislikes a recommended category, the RS earns a reward of 0, and the session concludes as the user leaves the RS.\n\nRecommendation policy The RS produces recommendations according to a recommendation policy. Recommendation policies can depend solely on the current user session and the history within the session. That is, in round t of the session, the policy can depend on histories of the form $(K, \\{0,1\\})^{t-1}$, where each tuple comprises a recommended category and its corresponding binary feedback. However, since a"}, {"title": "Bayesian Updates", "content": "At the beginning of each user session, the RS is only informed about the prior q. However, it rapidly acquires more information about the user through their feedback. For instance, if a user likes a recommendation of an exotic category favored by only a small subset of user types, we can conclude that the user probably belongs to that subset of types. The RS can then update its belief over the current user type, where the belief is a point in the user type simplex, and use it in its future suggestions.\n\nWe employ Bayesian updates to incorporate the new information after user feedback. Starting from a belief b and recommending a category k, we let $\\tau(b, k)$ denote the new belief over user types in case of positive feedback. Namely, $\\tau$ is a function $\\tau : \\Delta(M) \\times K \\rightarrow \\Delta(M)$ such that for every belief $b \\in \\Delta(M)$, category $k \\in K$ and type $m \\in M$,\n\n$$\\tau(b, k)(m) = \\frac{b(m) P(k,m)}{\\sum_{m'\\in M}b(m')\\cdot P(k, m')}$$        \n\nWe stress that Bayesian updates are crucial only after positive feedback, as the RS can utilize this new information for future recommendations. Negative feedback, though still informative, ends the session, preventing the RS from using the new information. We can further use Bayesian updates to obtain a recursive definition of the value function.\n\nObservation 2.1. For every policy $\\pi$ and belief $b \\in \\Delta(M)$,\n\n$$V^{\\pi}(b) = p_{\\pi_1}(b) \\Big(1 + V^{\\pi[2:]}(\\tau(b, \\pi_1))\\Big) .$$       \n\nObservation 2.1 provides a Bellman equation-like representation of the value function by isolating the immediate reward of the first round $p_{\\pi_1}(b)$ and the future rewards (implicitly discounted by $p_{\\pi_1}(b)$). It showcases the fundamental exploration-exploitation in our setting: On the one hand, exploitation involves selecting the category k that currently maximizes $p_k(b)$, focusing on immediate reward. On the other hand, exploration aims at steering the updated belief $\\tau(b, k)$ to a more informative position, thus increasing future rewards. Next, we use the notion of belief walk to assess how policies navigate the belief simplex."}, {"title": "Illustrating Example and Sub-optimality of Myopic Recommendations", "content": "Example 2.4. Consider a REC-APC instance with two user types $M = \\{m_1, m_2\\}$ and two categories $K = \\{k_1, k_2\\}$. The preference matrix and the user type prior are:\n\n$$P = \\begin{array}{cc} & k_1 & k_2 \\\\ m_1 & \\begin{pmatrix} 0.95 & 0.1 \\\\ 0.79 & 0.81 \\end{pmatrix} \\end{array}, \\qquad q = \\begin{array}{cc} & m_1 & m_2 \\\\ & \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} \\end{array}$$\n\nTo interpret these values, note that user type $m_2$ likes category $k_1$ with a probability of 0.1. In addition, recommending category $k_2$ to a random user yields an expected immediate reward of $p_{k_2}(q) = 0.5 \\cdot 0.79 + 0.5 \\cdot 0.81 = 0.8$.\n\nA prudent policy to adopt is the myopic policy $\\pi^m$, which is defined to be the policy that recommends the highest yielding category in each round, and afterward updates the belief. In this instance, the myopic policy is $\\pi^m = (k_2)_{t=1}$, as $k_2$ provides the better expected immediate reward compared to $k_1$ for the prior, and the belief update will only increase the preference for $k_2$. Under this policy, the expected social welfare is the sum of two infinite geometric series, one for each user type; namely, $V^{\\pi^m} = 0.5 \\cdot \\frac{0.79}{1-0.79} + 0.5 \\cdot \\frac{0.81}{1-0.81} = 4.01$.\n\nWhile unintuitive at first glance, the optimal policy $\\pi^* = (k_1)_{t=1}$ achieves an expected value of $V^* = 0.5 \\cdot \\frac{0.95}{1-0.95} + 0.5 \\frac{0.1}{1-0.1} = 9.55$, outperforming the myopic policy.\n\nBeyond exemplifying our setting and notation, Example 2.4 demonstrates that myopic policies can be sub-optimal. In fact, the next proposition shows that the sub-optimality gap could be arbitrarily large, highlighting the need for adequate planning.\n\nProposition 2.5. For every $d \\in \\mathbb{R}^+$, there exists an instance $(M, K, q, P)$ such that $V^{\\pi^m} \\cdot d < V^*$, where $\\pi^m$ is the myopic policy for the instance."}, {"title": "Dynamic Programming-based Algorithms", "content": "In this section, we present two dynamic programming-based approaches that provide approximations of the optimal expected social welfare. Although inefficient in the general case, such solutions are useful for rectangular instances, namely scenarios in which the number of types $|M|$ or categories $|K|$ is small. We first define the notion of approximation through a finite horizon and then present several methods for achieving it.\n\nWhile our model lacks an explicit discount factor, Observation 2.1 indicates that an implicit discount emerges through $p_k(b)$. Thus, similar to well-known results in MDPs, one can approximate the value function over an infinite horizon by addressing a finite-horizon problem with a sufficiently large horizon [30, Sec. 17.2]. To that end, we define the finite-horizon value function with a horizon H as $V_H^{\\pi}(b) = \\sum_{m \\in M} q(m)\\mathbb{E} [\\min\\{H, F(m, \\pi)\\}]$. In other words, $V_H^{\\pi}(b)$ is the value function given that the session terminates after H rounds. Lemma 2.3 suggests that $V_H^{\\pi}(b)$ is also given by $V_H^{\\pi}(b) = \\sum_{t=1}^H\\prod_{i=1}^t p_{\\pi_i}(b_{\u03c0,i})$.\n\nNext, we denote $p_{max}$ as the largest entry in the matrix $P$, assuming $p_{max} < 1^1$, and use it to bound the gap between infinite and finite horizon optimal policies."}, {"title": "Convergence of Optimal Policies", "content": "In Section 3, we showed that finite horizon analysis suffices to approximate optimal policies. Here, we establish an even more fundamental property for a broad family of instances: After a finite number of rounds, the optimal policy becomes fixed, transitioning from exploration to pure exploitation.\n\nWe focus our attention on instances where the preference matrix P has distinct preferences (DP), meaning all entries of P are unique. This ensures that every observation provides unambiguous information about user types, enabling informative observations. A discussion of how this theorem generalizes to other instances is provided in Section D. Our main result is as follows:\n\nTheorem 4.1. Fix any DP-REC-APC instance. There exists $T < \\infty$ such that for any $t > T$, it holds that $\\pi_{t+1} = \\pi_t$.\n\nTheorem 4.1 introduces a natural complexity measure through the time until convergence. Through the proof's construction, we can identify both when convergence occurs and determine the optimal policy from that point onward. Hence, instances with faster convergence require less computational effort as fewer possibilities need to be explored before identifying the final repeating recommendation.\n\nWhile Theorem 4.1 only establishes the convergence, our empirical analysis in Section 6 reveals that this transition typically occurs relatively fast in practice."}, {"title": "Branch-and-Bound Algorithm", "content": "In this section, we introduce a branch-and-bound (B&B) algorithm tailored to our setting, outlined in Algorithm 1. The B&B approach is widely used in sequential decision-making and combinatorial optimization [15, 26, 33]. Its effectiveness hinges on the quality of the bounds used to evaluate the search space and eliminate suboptimal branches. For our problem, we derive these bounds based on the recursive structure of the value function $V^{\\pi}$. Specifically, for any policy $\\pi$ and positive integer $h\\in \\mathbb{N}$, the value function $V^{\\pi}$ can be expressed as:\n\n$$V^{\\pi} = \\sum_{t=1}^h\\prod_{i=1}^t p_{\\pi_i}(b_{\u03c0,i}) + \\prod_{i=1}^h p_{\\pi_i}(b_{\u03c0,i}). V^{\\pi[h+1:]}(b_{\u03c0,h+1})$$\n\nEquation (3) decomposes $V^{\\pi}$ into two components: the cumulative rewards for the first h rounds and a recursive term representing the discounted expected value of future rounds. Crucially, substituting $V^{\\pi[h+1:]}(b_{\u03c0,h+1})$ with an upper or lower bound yields corresponding bounds for $V^{\\pi}$.\n\nBuilding on this, we now propose an upper bound. Intuitively, imagine that the RS is entirely certain about the user type. That is, the type would still be sampled according to the belief $b$, but the RS could pick a policy conditioned on the sampled type. In such a case, the RS would pick the type's favorite category indefinitely, leading to an expected social welfare of:\n\n$$V^U (b) = \\sum_{m \\in M} b(m) \\max_{k \\in K} \\frac{P(k,m)}{1-P(k, m)}$$\n\nWe stress that the above upper bound is not necessarily attainable. As for the lower bound, we compute the value of the best fixed-action policy w.r.t. the belief, namely,\n\n$$V^L (b) = \\max_{k \\in K} \\sum_{m \\in M} b(m) \\frac{P(k,m)}{1-P(k,m)}$$"}, {"title": "Experiments", "content": "In this section, we conduct experiments with two goals in mind. First, we complement our convergence result from Theorem 4.1 by demonstrating that, in practice, the belief walk converges rather quickly. Second, using simulated data, we compare the performance of Algorithm 1 to a state-of-the-art benchmark.\n\nSimulation details We generate instances using a random sampling procedure. We generate $P$ by independently sampling latent vectors for a given number of categories and user types. Namely, we sample a latent vector for each user type and category from a normal distribution, computing entries of $P$ as negated cosine distances (representing a user's affinities to a category), and normalizing these entries. We generate $q$ by independently sampling logits from a normal distribution. Then, we transform them into a categorical distribution through the softmax function. The simulations were conducted on a standard CPU-based PC. Further details appear in Section F."}, {"title": "Convergence of beliefs", "content": "Theorem 4.1 establishes that the optimal policy eventually converges to picking a fixed category. While the theorem guarantees convergence, it does not provide explicit rates. Equivalently, convergence can be analyzed in terms of certainty about a user's type, represented by proximity to the vertex to which the belief walk converges (recall the proof sketch of Theorem 4.1). Since beliefs update according to Bayes' rule, they converge at a geometric rate once the policy becomes fixed. In other words, further exploration yields diminishing returns when a belief is sufficiently close to a vertex. Thus, it is tempting to assume that the optimal policy myopically maximizes value for that vertex. On the other hand, a poorly chosen myopic policy can fail drastically, as Proposition 2.5 illustrates. We resolve these conflicting observations through simulations.\n\nFigure 1 shows how uncertainty in user type, defined as the $l_1$-distance from the vertex to which the belief converges under the optimal policy, evolves throughout the session. We vary the number of user types while fixing the number of categories. For each problem size, we report the averaged uncertainty and several individual runs. Despite the heterogeneity of individual runs, their geometric convergence property roughly transfers to averaged curves: Exponential functions fitted to these curves are almost identical to the originals, with correlation coefficients of at least $R^2 = 0.98$. This matches our intuition that early rounds are most important in terms of both expected reward and information.\n\nAnalyzing individual runs reveals notable patterns. While in some sessions, the optimal policies are fixed from the start, in others, recommendations switch (as characterized by jumps in the slope). This reflects the short-term vs. long-term reward trade-off discussed throughout the paper: The optimal policy may initially prioritize immediate rewards before switching to a riskier recommendation that increases certainty and earns more in the long run. Despite this, all the presented curves strictly decrease, suggesting that certainty increases monotonically. However, we found that in rare cases, the optimal policy can move away from a vertex before converging to it. This resolves the above conflict: Even if the belief approaches some vertex, the optimal policy may eventually lead to a different vertex. We exemplify this behavior in Section A."}, {"title": "Runtime comparison", "content": "Our model is novel, so there are no specifically tailored baselines. However, since it can be cast as a POMDP, we can compare Algorithm 1 with more general solvers. As a baseline, we have chosen SARSOP, a well-known offline point-based POMDP solver [18]. While Algorithm 1 is straightforward, SARSOP is rather complex. It represents the optimal policy through \u03b1-vectors (a convex piece-wise linear approximation of the value function) and clusters sampled beliefs to estimate the values of new ones. We used an open-source implementation of SARSOP, and, for a fair comparison, implemented Algorithm 1 in the same language."}, {"title": "Experiments with Real-World Data", "content": "Beyond the simulations in Section 6, we also conducted experiments with the Movielens 1M dataset [14]. To demonstrate the applicability of our approach, we outline below how we transform sparse rating matrices into the required model parameters.\n\nReal-world RS datasets typically consist of a sparse user-item rating matrix, where observed entries represent user ratings for items, with the majority of entries being unobserved. This representation differs from our model's requirements of a dense probability matrix between user types and content categories, accompanied by a prior distribution over user types; hence, our model cannot be applied directly, and the following two transformations are required.\n\nFirst, the sparse rating matrix must be aggregated into a concise representation through clustering of users and items. This is a well-studied task and several solutions have been proposed in the literature, such as Spectral co-clustering [10] or DBSCAN [9]. Second, the clustered data must be transformed into model parameters. One straightforward way to achieve this is to construct the preference matrix by computing mean ratings within cluster pairs and normalizing to [0, 1], and deriving the prior distribution from cluster sizes.\n\nWe defer this analysis to Section G. Our empirical investigation using this real-world dataset substantiates the qualitative patterns observed in Section 6. Importantly, we stress that the qualitative results we obtain in this section extend beyond the synthetic setup to real-world datasets."}, {"title": "Discussion and Future Work", "content": "We have introduced a model that captures two intertwined challenges: Aggregated user information and the risk of churn. We analyzed the belief walks and showed that the optimal policy must eventually act greedily. We have proposed a lower and upper bound on the optimal social welfare and demonstrated that our B&B"}, {"title": "Showcasing Belief Walks under Various Policies", "content": "In this section, we illustrate the structure of belief walks under varying policies, showcasing the different behaviors and trade-offs that arise in the context of belief walks. Each graph corresponds to a different instance with three categories and three user types (|M| = |K| = 3). We provide the full details of the instances below. In each graph, we plot the belief walk induced by three policies. Each belief in the 3-dimensional simplex is characterized by a probability for the first type $b(m_1)$, given in the x-axis, probability in the second type $b(m_2)$, given in the y-axis, and probability for the third type $b(m_3)$, given implicitly by $b(m_3) = 1 - b(m_1) - b(m_2)$. The plotted policies are:\n\n1.  The optimal policy, calculated using Algorithm 1.\n\n2.  The best fixed-action policy, which always recommends the category that maximizes the value function over all single-action policies w.r.t the prior. Put formally:\n\n$$\\pi = (k)_{t=1}^{\\infty} :\\qquad k = arg\\max_{k \\in K} \\sum_{m \\in M} q(m) \\cdot \\frac{P(k,m)}{1-P(k,m)}$$\n\nFor the rest of the section, we abbreviate \"best fixed-action policy\" to \"BFA policy\".\n\n3.  The myopic policy, which always recommends the category that yields the highest immediate reward in the current belief and then updates its belief afterward. Formally,\n\n$$\\pi^m = (k_t)_{t=1} :\\qquad k_i = arg \\max_{k \\in K} \\sum_{m \\in M} b_i(m)\\cdot P(k,m), \\qquad b_1 = q, \\qquad \\forall i > 1 : b_i = \\tau(b_{i-1}, k_{i-1}).$$\n\nEach belief in the belief path produced by the optimal policy is assigned a numerical value based on its sequence position. The plots display these values as small black numbers beside some arrows. Each arrow signifies a Bayesian update followed by a recommendation, with the arrow's direction indicating the movement from the previous belief to the updated belief. The black dot depicts the initial prior q over the user type, marking the starting point for any plotted belief walk.\n\nExample A.1. The REC-APC instance in Figure 3a is $(K = \\{k_1,k_2,k_3\\}, M = \\{m_1,m_2,m_3\\}, q_1, P_1)$, where:\n\n*   $q_1 = \\begin{pmatrix} 0.1713 \\\\ 0.4465 \\\\ 0.3822 \\end{pmatrix}$\n\n*   $P_1 = \\begin{pmatrix} 0.8611 & 0.4591 & 0.6862 \\\\ 0.0969 & 0.5531 & 0.8604 \\\\ 0.5055 & 0.1430 & 0.8879 \\end{pmatrix}$\n\nIn this instance, the optimal and BFA policies converge to the user type $m_3$, whereas the myopic policy converges to $m_1$. This observation underlines the central idea of Proposition 2.5, which asserts that the myopic policy may overlook future rewards, sometimes leading to convergence to a suboptimal user type. Even though the optimal and BFA policies reach the same user type, their pathways differ. This underscores the importance of meticulous initial exploration, as the sequence of recommendations before convergence significantly influences the overall rewards.\n\nExample A.2. The REC-APC instance in Figure 3b is $(K = \\{k_1,k_2,k_3\\}, M = \\{m_1,m_2,m_3\\}, q_2, P_2)$, where:\n\n*   $q_2 = \\begin{pmatrix} 0.3844 \\\\ 0.1197 \\\\ 0.4959 \\end{pmatrix}$\n\n*   $P_2 = \\begin{pmatrix} 0.6848 & 0.9100 & 0.5457 \\\\ 0.7741 & 0.8284 & 0.5833 \\\\ 0.1931 & 0.9127 & 0.5273 \\end{pmatrix}$"}, {"title": "Omitted Proofs from Section 2", "content": "Proof of Lemma 2.3. The first expression can be derived through Observation 2.1. First we prove via induction that for every $n \\in \\mathbb{N}$ the value function can be represented as $V^{\\pi}(b) = \\sum_{t=1}^n \\prod_{j=1}^t p_{\\pi_j}(b_{\u03c0,j}) + \\prod_{j=1}^n p_{\\pi_j}(b_{\u03c0,j})\\cdot V^{\\pi[n+1]}(b_{\u03c0,n+1})$. Then, as each $p_{\\pi_i}(b_{\u03c0,i})$ is a probability smaller than 1 and the value function is bounded from above by $\\frac{p_{max}}{1-p_{max}}$, as $n \\rightarrow \\infty$ the expression $\\prod_{j=1}^n p_{\\pi_j}(b_{\u03c0,j})\\cdot V^{\\pi[n+1]}(b_{\u03c0,n+1})$ will decay to 0, and the value function will converge to the first expression.\n\nBase case: For n = 1 it holds directly from Observation 2.1.\n\nInductive step: Assume that the statement holds for n, and prove it for n + 1.\n\n$$V^{\\pi}(b) = p_{\\pi_1}(b)(1 + V^{\\pi[2:]}(\\tau(b, \\pi_1)))$$\n\n$$\\sum_{t=2}^{n+1} \\prod_{j=1}^t p_{\\pi_j}(b_{\u03c0,j}) = p_{\\pi_1}(b) + p_{\\pi_1}(b) \\cdot \\sum_{t=2}^{n+1} \\prod_{j=2}^t p_{\\pi_j}(b_{\u03c0,j}) + p_{\\pi_1}(b) \\cdot \\Big(\\prod_{i=1}^n p_{\\pi_i}(b_{\u03c0,i})\\Big)\u00b7V^{\\pi[n+1]}(\\tau(b,\\pi_1))$$\n\n$$= p_{\\pi_1}(b) + p_{\\pi_1}(b)\\cdot \\Bigg[\\sum_{t=2}^{n+1} \\prod_{j=2}^t p_{\\pi_j}(b_{\u03c0,j}) + \\Big(\\prod_{i=2}^{n+1} p_{\\pi_i}(b_{\u03c0,i})\\Big)\\cdot V^{\\pi[n+1]}(\\tau(b,\\pi_1))\\Bigg]$$\n\n$$= \\sum_{t=1}^{n+1}\\prod_{j=1}^{t} p_{\\pi_j}(b_{\u03c0,j}) + \\prod_{j=1}^{n+1} p_{\\pi_j}(b_{\u03c0,j})V^{\\pi[n+1]}(\\tau(b,\\pi_1)).$$      \n\nThis completes the proof of the lemma."}, {"title": "Omitted Proofs from Section 3", "content": "Proof of Lemma 3.1. As can be seen in the proof Lemma 2.3, The value function can be represented as\n\n$$V^{\\pi}(b) = \\sum_{t=1}^{H(\\varepsilon)} \\prod_{j=1}^t p_{\\pi_j}(b_{\u03c0,j}) + \\prod_{j=1}^{H(\\varepsilon)} p_{\\pi_j}(b_{\u03c0,j}) \\cdot V^{\\pi[H(\\varepsilon)+1:]}(b_{\u03c0,H(\\varepsilon)+1}).$$\n\nNotice that for every $L \\in \\mathbb{N}$, $\\prod_{j=1}^L p_{\\pi_j}(b_{\u03c0,j}) \\leq p_{max}$. Additionally, as stated in Equation 2, for every belief $b$ and policy $\\pi$ we have that $V^{\\pi} (b) \\leq \\frac{p_{max}}{1-p_{max}}$. Therefore:\n\n$$\\begin{aligned} V^{*_{\\pi}}(b) &\\leq \\sum_{t=1}^{H(\\varepsilon)} \\prod_{j=1}^t p_{\\pi_j}(b_{\u03c0,j}) + \\prod_{j=1}^{H(\\varepsilon)} p_{\\pi_j}(b_{\u03c0,j}) \\cdot \\frac{p_{max}}{1-p_{max}} \\\\ &<  \\sum_{t=1}^{H(\\varepsilon)} \\prod_{j=1}^t p_{\\pi_j}(b_{\u03c0,j}) + \\varepsilon = V_{H(\\varepsilon)}^{\\pi}(b) + \\varepsilon \\leq \\max_{\\pi} V_{H(\\varepsilon)}^{\\pi}(b) + \\varepsilon. \\end{aligned}$$"}, {"title": "Omitted Proofs from Section 5", "content": "Proof of Lemma 5.1. Notice that according to Lemma 2.3, we have $V^L(b) = V^{*_k}(b)$ for $\\pi_k = (k)_{i=1}^{\\infty}$,\n\nwhere $k = arg \\max_{k' \\in K} \\sum_{m \\in M} b(m) \\frac{P"}]}