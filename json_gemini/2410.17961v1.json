{"title": "CLOSED-FORM MERGING OF PARAMETER-EFFICIENT MODULES FOR FEDERATED CONTINUAL LEARNING", "authors": ["Riccardo Salami", "Pietro Buzzega", "Matteo Mosconi", "Jacopo Bonato", "Luigi Sabetta", "Simone Calderara"], "abstract": "Model merging has emerged as a crucial technique in Deep Learning, enabling the integration of multiple models into a unified system while preserving performance and scalability. In this respect, the compositional properties of low-rank adaptation techniques (e.g., LoRA) have proven beneficial, as simple averaging LORA modules yields a single model that mostly integrates the capabilities of all individual modules. Building on LoRA, we take a step further by imposing that the merged model matches the responses of all learned modules. Solving this objective in closed form yields an indeterminate system with A and B as unknown variables, indicating the existence of infinitely many closed-form solutions. To address this challenge, we introduce LoRM, an alternating optimization strategy that trains one LORA matrix at a time. This allows solving for each unknown variable individually, thus finding a unique solution. We apply our proposed methodology to Federated Class-Incremental Learning (FCIL), ensuring alignment of model responses both between clients and across tasks. Our method demonstrates state-of-the-art performance across a range of FCIL scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Humans naturally excel at learning a diverse array of skills independently, effortlessly acquiring knowledge across multiple domains throughout their lives. In contrast, the traditional paradigm for artificial neural networks relies on training a unified model on a single, large dataset. While this approach facilitates the simultaneous incorporation of different skills, it lacks the capacity for specialized or incremental learning, making it less adaptable and responsive to changes in the environment. To overcome this limitation and mimic human flexibility, various paradigms have been developed to enhance neural networks' ability to manage diverse skills effectively. Multi-Task Learning (Caruana, 1997) involves training a model on several tasks simultaneously, promoting the sharing of representations across tasks, while Continual Learning (CL) (McCloskey & Cohen, 1989) focuses on enabling models to learn tasks incrementally without forgetting prior knowledge. Federated Learning (FL) (McMahan et al., 2017), on the other hand, focuses on decentralized training by distributing data and computation across separate clients, each specializing in their local task. While each of these scenarios has its own unique characteristics, they all share the common objective of integrating task-specific modules into a unified framework.\nRecently, large pre-trained architectures have facilitated model editing (Ortiz-Jimenez et al., 2024) and specialization (Bowman et al., 2023), particularly for fine-tuning downstream tasks. In practice, deep models often leave their parameters fixed, leveraging Parameter-Efficient Fine-Tuning (PEFT) techniques to adapt to new tasks effectively. Among PEFT methods, Low-Rank Adaptation (LORA) (Hu et al., 2022) has emerged as a prominent approach. LoRA introduces residual weights in the form of $\\Delta W = BA$, where B and A are low-rank matrices. These residuals, commonly referred to as task vectors (Ilharco et al., 2023), form the foundation of the novel model merg-"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": ""}, {"title": "2.1 PRELIMINARIES", "content": "LORA (Hu et al., 2022) was introduced to reduce the number of trainable parameters when fine-tuning pre-trained models. Formally, let $W_o \\in \\mathbb{R}^{d\\times k}$ represent the matrix of pre-trained weights of a linear layer, and let $x \\in \\mathbb{R}^{k\\times 1}$ be the input vector for that layer. The output h is given by:\n$h = W_ox + \\Delta Wx = W_ox + BAx$,\nwhere $\\Delta W = BA$ is the residual weight introduced by LoRA, with matrices A and B as the only components being trained. The efficiency of this approach stems from the low rank r of the matrices, where $B \\in \\mathbb{R}^{d\\times r}$ and $A \\in \\mathbb{R}^{r\\times k}$, with d and k representing the number of output and input features of the layer, respectively. Consequently, the number of trainable parameters is $r \\cdot (d + k)$, which, since $r \\ll d$, constitutes only a fraction of the dk parameters required for full fine-tuning. Additionally, B is initialized to 0: i.e., the first forward pass is equivalent to the absence of a LoRA residual.\nRegMean (Jin et al., 2023) introduces a method for merging a collection of N linear layers ${W_i}_{i=1}^N$, each corresponding to N distinct models trained on distinct inputs ${X_i}_{i=1}^N$. The goal is to identify a single linear layer that produces responses that closely match those of the starting layers. Specifically, the objective function is defined as follows\u00b9:\n$\\underset{W_M}{\\text{minimize}} \\; \\Omega = \\sum_{i=1}^N ||W_MX_i - W_iX_i||_2$.\nBy computing the gradient of $\\Omega$ with respect to $W_M$ and setting it equal to zero, a closed-form solution is obtained. Notably, the merged layer $W_M$ is computed as follows:\n$W_M = (\\sum_{i=1}^N W_iX_iX_i^T) (\\sum_{i=1}^N X_iX_i^T)^{-1}$"}, {"title": "2.2 PROBLEM SETTING", "content": "In Federated Class-Incremental Learning, the dataset $\\mathcal{D}$ is first divided into T tasks, each consisting of a distinct set of classes. Then, each partition $\\mathcal{D}^t$ corresponding to the t-th task is further distributed among N clients, resulting in $\\mathcal{D}_i^t$ for the i-th client. Similar to standard Class-Incremental Learning (Van de Ven et al., 2022), the task-specific partitions ${\\mathcal{D}^t}_{t=1,...,T}$ arrive sequentially.\nIn this federated scenario, the training for each task is conducted over multiple communication rounds. During each round, clients are restricted to learning only from their local dataset $\\mathcal{D}_i^t$. The local optimization objective for client i, based on the loss function $\\mathcal{L}$, can be formally expressed as:\n$\\underset{\\theta_i}{\\text{minimize}} \\; \\mathbb{E}_{(x,y)\\sim \\mathcal{D}_i^t} [\\mathcal{L}(f(x; \\theta_i), y)]$,\nwhere x and y denote the inputs and corresponding labels, respectively, with $\\theta_i$ representing the set of parameters for client i, and $f(.; \\theta_i)$ denoting the associated model.\nAfter completing local updates, each client sends its model parameters $\\theta_i$ to the central server, where they are aggregated with those from other clients. The server then sends the global aggregated model back to the clients, marking the end of a communication round. This process repeats for several rounds until the training for task t is completed. Once all rounds for task t have finished, the system progresses to the next task t + 1 using the corresponding dataset $\\mathcal{D}^{t+1}$. The ultimate objective is to obtain a global model, derived from the aggregation of local models performed by the server, that functions well across all incremental tasks and successfully integrates the distributed knowledge."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 A CLOSED-FORM SOLUTION FOR LORA MERGING", "content": "Using the same notation as in Equation 2, let W denote the weight matrix of a given layer. Since LORA (see Equation 2) is applied to each linear layer across all clients, we express the weight matrix for the i-th client as $W_i = W_o + B_iA_i$, where $W_o$ is the shared pre-trained weight matrix, and $B_i, A_i$ represent the low-rank matrices specific to the client. This formulation is consistent across clients, as they all utilize the same model architecture. At the end of each communication round, after conducting local training on the LoRA matrices, the goal is to merge the corresponding matrices (i.e., $A_i$'s with $A_i$'s and $B_i$'s with $B_i$'s) using the closed-form solution derived from RegMean. Starting from Equation 2, our objective becomes:\n$\\Omega = \\sum_{i=1}^N ||(W_o + B_M A_M)X_i - (W_o + B_iA_i)X_i||_2$.\nTo find the optimal AM and BM that minimize $\\Omega$, we differentiate $\\Omega$ with respect to each variable, one at a time, and set the gradients to zero:\n$\\frac{\\Theta \\Omega}{\\Theta B} = 0$\n$\\frac{\\Theta \\Omega}{\\Theta A} = 0$"}, {"title": "3.2 LORM", "content": "Having established closed-form solutions for weight merging, we now outline the full procedure of LoRM. Each client i begins by optimizing its own $B_i$, which is learned during local training, and a shared A, initialized and distributed by the server. In this first round, it is essential to freeze A, as LoRA's B is initialized to 0. Freezing B = 0 across all clients would render the training ineffective. At the end of each round, each client i computes the Gram matrix $X_iX_i^T$ with a forward pass on all examples\u00b2. Then, it sends $B_i$ and $X_iX_i^T$ to the server, where the merging operation (as described in Equation 7) is performed. The resulting $B_M$ is then sent back to the clients, who begin the next communication round.\nAlternated optimization. Empirically, we observe that consistently training only the matrix B while keeping A fixed can be limiting (see Section 4.3 for a detailed analysis). Therefore, to fully exploit LoRA's representational potential, we introduce an alternating training approach, where the matrix to be updated changes at each round. Specifically, after the first round, we freeze B, which is already synchronized across clients due to the previous server aggregation, and train A instead. Then, at the end of the second round, all local $A_i$'s are aggregated"}, {"title": "3.3 LORM CHARACTERISTICS FOR FEDERATED CLASS-INCREMENTAL LEARNING", "content": "In Federated Class-Incremental Learning (FCIL), data privacy, efficiency, and the rate of convergence are crucial concerns. LoRM is specifically designed to address these challenges.\nPrivacy-preserving. In FCIL, inputs $X_i$ for the i-th client's generic layer cannot be transmitted to the central server, as doing so, especially for the first layer, would compromise data privacy by effectively sharing the dataset. In LoRM, instead of transmitting $X_i$ directly, we send the Gram matrix $X_iX_i^T$, which obfuscates the original data. Furthermore, only the diagonal of the Gram matrix is communicated, ensuring that no local data is exposed."}, {"title": "4 EXPERIMENTAL STUDY", "content": ""}, {"title": "4.1 EVALUATION SETTINGS", "content": "Datasets. The importance of evaluating pre-trained models on datasets that deviate significantly from their pre-training domain is well-established in the literature (Kolesnikov et al., 2020; Kornblith et al., 2019). Accordingly, we divide the evaluation of LoRM into two parts: one focusing on in-domain datasets and the other on an out-of-domain dataset. For in-domain evaluation, we use CIFAR-100 (Krizhevsky et al., 2009) and ImageNet-R (Hendrycks et al., 2021). For out-of-domain evaluation, we employ EuroSAT (Helber et al., 2018), a satellite dataset recognized by Oh et al. (2022) as one of the most challenging for domain adaptation from ImageNet-21k pre-training. CIFAR-100 and ImageNet-R are partitioned into 10 incremental tasks, with each task consisting of 10 and 20 classes, respectively. EuroSAT is divided into 5 tasks, each containing 2 classes. The data is distributed across 10 clients using the commonly adopted distribution-based label imbalance setting (Li et al., 2022; Yurochkin et al., 2019), where partitioning is governed by a Dirichlet distribution parameterized by \u03b2. A smaller \u03b2 value corresponds to a more challenging data distribution. We evaluate all methods across three scenarios for each dataset, using \u03b2\u2208 0.5, 0.1, 0.05 for the in-domain datasets and \u03b2\u20ac 1,0.5,0.2 for the out-of-domain one.\nEvaluated approaches. We compare LoRM against 10 competing methods spanning different fields. From Continual Learning, we evaluate EWC (Kirkpatrick et al., 2017), LwF (Li & Hoiem, 2017), DER++ (Buzzega et al., 2020), L2P (Wang et al., 2022b), and CODA-Prompt (Smith et al., 2023). We also include FisherAvg (Matena & Raffel, 2022) and RegMean (Jin et al., 2023), from the model merging literature, and CCVR (Luo et al., 2021), a method from Federated Learning."}, {"title": "4.2 RESULTS", "content": "In Tables 1 and 2, we present the results of the evaluated approaches in terms of Final Average Accuracy (FAA). The reported results are averaged over 3 runs"}, {"title": "4.3 ABLATION STUDIES", "content": "Alternating vs. Only B. We investigate whether training the B matrix only (i.e., following the same procedure as LoRM but applying solely Equation 7 to merge B matrices) could lead to improved performance compared to our alternating optimization approach. We conduct an experiment on EuroSAT with \u03b2 = 1.0 (see Figure 3), showing that the alternating strategy outperforms the only"}, {"title": "5 RELATION WITH PRIOR WORKS", "content": ""}, {"title": "PARAMETER-EFFICIENT FINE-TUNING", "content": "Adapting deep neural networks to new tasks typically requires full re-training of all parameters, which is computationally demanding. Parameter-Efficient Fine-Tuning (PEFT) addresses this by updating only a small subset of the model parameters, leaving the rest of the network unchanged. One of the earliest methods in this domain is Adapters (Houlsby et al., 2019). Recentely, a prominent class of PEFT techniques includes Prompt Tuning (Lester et al., 2021) and Prefix Tuning (Li & Liang, 2021). Prompt Tuning prepends these embeddings directly to the initial input sequence , whereas Prefix Tuning concatenates them to specific attention layers. An alternative approach to prompting is Low-Rank Adaptation (LoRA) (Hu et al., 2022), which injects residual parameters into the pre-trained weights using low-rank matrices, significantly reducing the number of learnable parameters. While recent works have explored various alternatives to LoRA, our research is grounded in the original LoRA framework, leveraging its efficiency and the extensive literature on model merging that goes with it."}, {"title": "MODEL MERGING", "content": "Task-specific modules are typically deployed to address individual tasks. However, an alternative approach involves merging these modules to create a model capable of generalizing across all tasks. Expanding on this, subsequent works have focused on optimizing the coefficients that determine the contribution of each LoRA module during aggregation (Huang et al., 2023; Yang"}, {"title": "FEDERATED CLASS-INCREMENTAL LEARNING", "content": "Federated Class-Incremental Learning (FCIL), introduced by Yoon et al. (2021), addresses the challenges inherent in Continual Learning, such as Catastrophic Forgetting (Robins, 1995), and Federated Learning. Most recently, PILORA proposed a method that integrates LoRA with prototypes (i.e., average feature vectors), using the latter for classification instead of relying on the traditional classification layer. In contrast, our proposed methodology leverages LoRA modules and aggregates them by aligning their output in both Continual and Federated scenarios in a closed-form."}, {"title": "6 CONCLUSIONS", "content": "At the outset of our research, we explored the possibility of a closed-form solution for merging LORA modules. Motivated by this question, we identified and validated the existence of such a solution, demonstrating its practicality within the framework of Federated Class-Incremental Learning (FCIL). We introduced LoRM, a method specifically designed for the FCIL scenario, which sets a new State of the Art in this domain.\nLooking forward, we aim to broaden our exploration to encompass a wider array of PEFT modules. For modules such as VeRA (Kopiczko et al., 2024) and (IA)\u00b3 (Liu et al., 2022), we have already Furthermore, given the generality of our derivations beyond the FCIL setting, we also intend to evaluate the robustness of our equations across various model merging scenarios. Ultimately, we hope this work will serve not only the Federated Learning and Continual Learning communities but also contribute to the growing body of research on efficient module compositionally."}]}