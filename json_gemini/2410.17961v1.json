[{"title": "CLOSED-FORM MERGING OF PARAMETER-EFFICIENT MODULES FOR FEDERATED CONTINUAL LEARNING", "authors": ["Riccardo Salami", "Pietro Buzzega", "Matteo Mosconi", "Jacopo Bonato", "Luigi Sabetta", "Simone Calderara"], "abstract": "Model merging has emerged as a crucial technique in Deep Learning, enabling the integration of multiple models into a unified system while preserving performance and scalability. In this respect, the compositional properties of low-rank adaptation techniques (e.g., LoRA) have proven beneficial, as simple averaging LORA modules yields a single model that mostly integrates the capabilities of all individual modules. Building on LoRA, we take a step further by imposing that the merged model matches the responses of all learned modules. Solving this objective in closed form yields an indeterminate system with A and B as unknown variables, indicating the existence of infinitely many closed-form solutions. To address this challenge, we introduce LoRM, an alternating optimization strategy that trains one LORA matrix at a time. This allows solving for each unknown variable individually, thus finding a unique solution. We apply our proposed methodology to Federated Class-Incremental Learning (FCIL), ensuring alignment of model responses both between clients and across tasks. Our method demonstrates state-of-the-art performance across a range of FCIL scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Humans naturally excel at learning a diverse array of skills independently, effortlessly acquiring knowledge across multiple domains throughout their lives. In contrast, the traditional paradigm for artificial neural networks relies on training a unified model on a single, large dataset. While this approach facilitates the simultaneous incorporation of different skills, it lacks the capacity for specialized or incremental learning, making it less adaptable and responsive to changes in the environment. To overcome this limitation and mimic human flexibility, various paradigms have been developed to enhance neural networks' ability to manage diverse skills effectively. Multi-Task Learning (Caruana, 1997) involves training a model on several tasks simultaneously, promoting the sharing of representations across tasks, while Continual Learning (CL) (McCloskey & Cohen, 1989) focuses on enabling models to learn tasks incrementally without forgetting prior knowledge. Federated Learning (FL) (McMahan et al., 2017), on the other hand, focuses on decentralized training by distributing data and computation across separate clients, each specializing in their local task. While each of these scenarios has its own unique characteristics, they all share the common objective of integrating task-specific modules into a unified framework.\nRecently, large pre-trained architectures have facilitated model editing (Ortiz-Jimenez et al., 2024) and specialization (Bowman et al., 2023), particularly for fine-tuning downstream tasks. In practice, deep models often leave their parameters fixed, leveraging Parameter-Efficient Fine-Tuning (PEFT) techniques to adapt to new tasks effectively. Among PEFT methods, Low-Rank Adaptation (LORA) (Hu et al., 2022) has emerged as a prominent approach. LoRA introduces residual weights in the form of $\\Delta W = BA$, where B and A are low-rank matrices. These residuals, commonly referred to as task vectors (Ilharco et al., 2023), form the foundation of the novel model merg-"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": ""}, {"title": "2.1 PRELIMINARIES", "content": "LORA (Hu et al., 2022) was introduced to reduce the number of trainable parameters when fine-tuning pre-trained models. Formally, let $W_0 \\in \\mathbb{R}^{d\\times k}$ represent the matrix of pre-trained weights of a linear layer, and let $x \\in \\mathbb{R}^{k\\times 1}$ be the input vector for that layer. The output h is given by:\n$h = W_0x + \\Delta Wx = W_0x + BAx$,\nwhere $\\Delta W = BA$ is the residual weight introduced by LoRA, with matrices A and B as the only components being trained. The efficiency of this approach stems from the low rank r of the matrices, where $B \\in \\mathbb{R}^{d\\times r}$ and $A \\in \\mathbb{R}^{r\\times k}$, with d and k representing the number of output and input features of the layer, respectively. Consequently, the number of trainable parameters is $r \\cdot (d+k)$, which, since $r < d$, constitutes only a fraction of the dk parameters required for full fine-tuning. Additionally, B is initialized to 0: i.e., the first forward pass is equivalent to the absence of a LoRA residual.\nRegMean (Jin et al., 2023) introduces a method for merging a collection of N linear layers ${W_i}_{i=1}^N$, each corresponding to N distinct models trained on distinct inputs ${X_i}_{i=1}^N$. The goal is to identify a single linear layer that produces responses that closely match those of the starting layers. Specifically, the objective function is defined as follows\u00b9:\n$\\underset{W_M}{\\text{minimize }} \\Omega = \\sum_{i=1}^N ||W_MX - W_iX_i||_2$.\nBy computing the gradient of $\\Omega$ with respect to $W_M$ and setting it equal to zero, a closed-form solution is obtained. Notably, the merged layer $W_M$ is computed as follows:\n$W_M = (\\sum_{i=1}^N W_iX_iX_i^T)(\\sum_{i=1}^N X_iX_i^T)^{-1}$"}, {"title": "2.2 PROBLEM SETTING", "content": "In Federated Class-Incremental Learning, the dataset D is first divided into T tasks, each consisting of a distinct set of classes. Then, each partition $D^t$ corresponding to the t-th task is further distributed among N clients, resulting in $D_i^t$ for the i-th client. Similar to standard Class-Incremental Learning (Van de Ven et al., 2022), the task-specific partitions ${D^t}_{t=1,...,T}$ arrive sequentially.\nIn this federated scenario, the training for each task is conducted over multiple communication rounds. During each round, clients are restricted to learning only from their local dataset $D_i^t$. The local optimization objective for client i, based on the loss function L, can be formally expressed as:\n$\\underset{\\theta_i}{\\text{minimize }} \\mathbb{E}_{(x,y)\\sim D_i^t}[\\mathcal{L}(f(x; \\theta_i), y)]$,\nwhere x and y denote the inputs and corresponding labels, respectively, with $\\theta_i$ representing the set of parameters for client i, and $f(. ; \\theta_i)$ denoting the associated model.\nAfter completing local updates, each client sends its model parameters $\\theta_i$ to the central server, where they are aggregated with those from other clients. The server then sends the global aggregated model back to the clients, marking the end of a communication round. This process repeats for several rounds until the training for task t is completed. Once all rounds for task t have finished, the system progresses to the next task t + 1 using the corresponding dataset $D^{t+1}$. The ultimate objective is to obtain a global model, derived from the aggregation of local models performed by the server, that functions well across all incremental tasks and successfully integrates the distributed knowledge."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 A CLOSED-FORM SOLUTION FOR LORA MERGING", "content": "Using the same notation as in Equation 2, let W denote the weight matrix of a given layer. Since LORA (see Equation 2) is applied to each linear layer across all clients, we express the weight matrix for the i-th client as $W_i = W_0 + B_iA_i$, where $W_0$ is the shared pre-trained weight matrix, and $B_i, A_i$ represent the low-rank matrices specific to the client. This formulation is consistent across clients, as they all utilize the same model architecture. At the end of each communication round, after conducting local training on the LoRA matrices, the goal is to merge the corresponding matri- ces (i.e., $A_i$'s with $A_i$'s and $B_i$'s with $B_i$'s) using the closed-form solution derived from RegMean. Starting from Equation 2, our objective becomes:\n$\\Omega = \\sum_{i=1}^N ||(W_0 + B_M A_M)X_i - (W_0 + B_iA_i)X_i||_2$.\nTo find the optimal $A_M$ and $B_M$ that minimize $\\Omega$, we differentiate $\\Omega$ with respect to each variable, one at a time, and set the gradients to zero:\n$\\begin{aligned} \\frac{\\Theta\\Omega}{\\partial B} &= 0 \\\\ \\frac{\\Theta\\Omega}{\\partial A} &= 0 \\end{aligned}$"}, {"title": "3.2 LORM", "content": "Having established closed-form solutions for weight merging, we now outline the full procedure of LoRM, illustrated in Figure 1. Each client i begins by optimizing its own $B_i$, which is learned during local training, and a shared A, initialized and distributed by the server. In this first round, it is essential to freeze A, as LoRA's B is initialized to 0. Freezing B = 0 across all clients would render the training ineffective. At the end of each round, each client i computes the Gram matrix $X_iX_i^T$ with a forward pass on all examples\u00b2. Then, it sends $B_i$ and $X_iX_i^T$ to the server, where the merging operation (as described in Equation 7) is performed. The resulting $B_M$ is then sent back to the clients, who begin the next communication round.\nAlternated optimization. Empirically, we observe that consistently training only the matrix B while keeping A fixed can be limiting (see Section 4.3 for a detailed analysis). Therefore, to fully exploit LoRA's representational potential, we introduce an alternating training approach, where the matrix to be updated changes at each round. Specifically, after the first round, we freeze B, which is already synchronized across clients due to the previous server aggregation, and train A instead. Then, at the end of the second round, all local A's are aggregated using Equation 8. This strategy also improves"}, {"title": "3.3 LORM CHARACTERISTICS FOR FEDERATED CLASS-INCREMENTAL LEARNING", "content": "In Federated Class-Incremental Learning (FCIL), data privacy, efficiency, and the rate of convergence are crucial concerns. LoRM is specifically designed to address these challenges.\nPrivacy-preserving. In FCIL, inputs $X_i$ for the i-th client's generic layer cannot be transmitted to the central server, as doing so, especially for the first layer, would compromise data privacy by effectively sharing the dataset. In LoRM, instead of transmitting $X_i$ directly, we send the Gram matrix $X_iX_i^T$, which obfuscates the original data. Furthermore, only the diagonal of the Gram matrix is communicated, ensuring that no local data is exposed.\nEfficiency. The use of LoRA inherently improves efficiency compared to full fine-tuning, as it necessitates the communication of only two low-rank matrices for each layer. Additionally, LoRM's alternating optimization procedure permits the transmission of only one matrix per communication round, further reducing overhead. The use of solely the diagonal of the Gram matrix mitigates communication costs even further, as it avoids the need to transmit the full matrix. As a result, at the conclusion of each communication round, each client transmits a low-rank matrix and a vector to the server for each layer.\nRate of convergence. Ultimately, LoRM demonstrates faster convergence compared to other FCIL baselines, as further discussed in Section 4.2."}, {"title": "4 EXPERIMENTAL STUDY", "content": ""}, {"title": "4.1 EVALUATION SETTINGS", "content": "Datasets. The importance of evaluating pre-trained models on datasets that deviate significantly from their pre-training domain is well-established in the literature (Kolesnikov et al., 2020; Kornblith et al., 2019). Accordingly, we divide the evaluation of LoRM into two parts: one focusing on in-domain datasets and the other on an out-of-domain dataset. For in-domain evaluation, we use CIFAR-100 (Krizhevsky et al., 2009) and ImageNet-R (Hendrycks et al., 2021). For out-of-domain evaluation, we employ EuroSAT (Helber et al., 2018), a satellite dataset recognized by Oh et al. (2022) as one of the most challenging for domain adaptation from ImageNet-21k pre-training. CIFAR-100 and ImageNet-R are partitioned into 10 incremental tasks, with each task consisting of 10 and 20 classes, respectively. EuroSAT is divided into 5 tasks, each containing 2 classes. The data is distributed across 10 clients using the commonly adopted distribution-based label imbalance setting (Li et al., 2022; Yurochkin et al., 2019), where partitioning is governed by a Dirichlet distribution parameterized by $\\beta$. A smaller $\\beta$ value corresponds to a more challenging data distribution. We evaluate all methods across three scenarios for each dataset, using $\\beta \\in {0.5, 0.1, 0.05}$ for the in-domain datasets and $\\beta \\in {1, 0.5, 0.2}$ for the out-of-domain one. For further details on data preprocessing, including the dataset-specific augmentations used, refer to appendix B.\nEvaluated approaches. We compare LoRM against 10 competing methods spanning different fields. From Continual Learning, we evaluate EWC (Kirkpatrick et al., 2017), LwF (Li & Hoiem, 2017), DER++ (Buzzega et al., 2020), L2P (Wang et al., 2022b), and CODA-Prompt (Smith et al., 2023). Following previous studies (Zhang et al., 2023b; Guo et al., 2024), we adapt these methods to the federated domain by merging client weights using the FedAvg algorithm (McMahan et al., 2017). We also include FisherAvg (Matena & Raffel, 2022) and RegMean (Jin et al., 2023), from the model merging literature, and CCVR (Luo et al., 2021), a method from Federated Learning. These approaches are adapted for Continual Learning by applying Asymmetric Cross-Entropy (ACE) (Caccia et al., 2022), where the classification heads are optimized separately for each task. Additionally, we evaluate two algorithms specifically designed for Federated Class-Incremental Learning: TARGET (Zhang et al., 2023b) and PILORA (Guo et al., 2024), the latter representing the current State Of The Art. Finally, we include Joint results as an upper bound, achieved by training the backbone on the entire dataset without any federated or incremental partitioning.\nImplementation Details. As the backbone for LoRM and all competing approaches, we employ a pre-trained ViT-B/16 model (Dosovitskiy et al., 2021). Specifically, the model is initialized with pre-trained weights from ImageNet-21K (Ridnik et al., 2021) for all evaluated datasets. Each task is trained over 5 communication rounds, with each round consisting of 5 epochs. For a complete overview of the method-specific hyperparameters, refer to appendix D."}, {"title": "4.2 RESULTS", "content": "In Tables 1 and 2, we present the results of the evaluated approaches in terms of Final Average Accuracy (FAA). For a formal definition of this metric, please refer to appendix B.4. The reported results are averaged over 3 runs, with standard deviations provided in appendix C.1.\nIn the context of in-domain datasets (Table 1), models tend to perform better on CIFAR-100 than on ImageNet-R, indicating the former as a simpler task, partially due to having fewer classes. Continual Learning techniques such as LwF and EWC exhibit low to moderate performance in scenarios"}, {"title": "4.3 ABLATION STUDIES", "content": "Alternating vs. Only B. We investigate whether training the B matrix only (i.e., following the same procedure as LoRM but applying solely Equation 7 to merge B matrices) could lead to improved performance compared to our alternating optimization approach. We conduct an experiment on EuroSAT with $\\beta = 1.0$ (see Figure 3), showing that the alternating strategy outperforms the only"}, {"title": "5 RELATION WITH PRIOR WORKS", "content": ""}, {"title": "PARAMETER-EFFICIENT FINE-TUNING", "content": "Adapting deep neural networks to new tasks typically requires full re-training of all parameters, which is computationally demanding. Parameter-Efficient Fine-Tuning (PEFT) addresses this by updating only a small subset of the model parameters, leaving the rest of the network unchanged. One of the earliest methods in this domain is Adapters (Houlsby et al., 2019), which are lightweight neural modules inserted between the layers of a pre-trained model to facilitate task adaptation. More recently, a prominent class of PEFT techniques includes Prompt Tuning (Lester et al., 2021) and Prefix Tuning (Li & Liang, 2021), both of which introduce learnable embeddings, or prompts, appended to the layers' input tokens. Prompt Tuning prepends these embeddings directly to the initial input sequence (Wang et al., 2022b;a), whereas Prefix Tuning concatenates them to specific attention layers (Smith et al., 2023). An alternative approach to prompting is Low-Rank Adaptation (LoRA) (Hu et al., 2022), which injects residual parameters into the pre-trained weights using low-rank matrices, significantly reducing the number of learnable parameters. While recent works have explored various alternatives to LoRA(Kopiczko et al., 2024; Zhang et al., 2023d; Renduchintala et al., 2024; Liu et al., 2022), our research is grounded in the original LoRA framework, leveraging its efficiency and the extensive literature on model merging that goes with it."}, {"title": "MODEL MERGING", "content": "Task-specific modules are typically deployed to address individual tasks. However, an alternative approach involves merging these modules to create a model capable of generalizing across all tasks. A notable line of research within model merging focuses on combining task vectors (Ilharco et al., 2023), specifically those generated by LoRA-like methods. Early studies have explored the use of linear arithmetic operations (e.g., addition and subtraction) to combine these modules (Zhang et al., 2023c). Expanding on this, subsequent works have focused on optimizing the coefficients that determine the contribution of each LoRA module during aggregation (Huang et al., 2023; Yang"}, {"title": "6 CONCLUSIONS", "content": "At the outset of our research, we explored the possibility of a closed-form solution for merging LORA modules. Motivated by this question, we identified and validated the existence of such a solution, demonstrating its practicality within the framework of Federated Class-Incremental Learning (FCIL). We introduced LoRM, a method specifically designed for the FCIL scenario, which sets a new State of the Art in this domain.\nLooking forward, we aim to broaden our exploration to encompass a wider array of PEFT modules. For modules such as VeRA (Kopiczko et al., 2024) and (IA)\u00b3 (Liu et al., 2022), we have already derived closed-form merging equations, which are detailed in appendices A.2 and A.3, respectively. Furthermore, given the generality of our derivations beyond the FCIL setting, we also intend to evaluate the robustness of our equations across various model merging scenarios. Ultimately, we hope this work will serve not only the Federated Learning and Continual Learning communities but also contribute to the growing body of research on efficient module compositionally."}, {"title": "A MATHEMATICAL DERIVATIONS", "content": ""}, {"title": "A.1 COMBINATION OF LORA AND REGMEAN", "content": "We consider a minimization problem over a single linear layer where X represents the input to the layer. The objective function is defined as:\n$\\underset{W}{\\text{minimize }} \\Omega = \\sum_{i=1}^N||WX_i - \\hat{W}X_i||_2$.\nHere, $W$ and $W_i$ are the weight matrices for the global model (server) and the local models (clients) respectively. In the context of a LoRA residual module, we replace $W$ with $W_0 + BA$, where $W_0$ denotes the pre-trained weight matrix and $BA$ represents the learned low-rank difference. The objective function becomes:\n$\\Omega = \\sum_{i=1}^N ||(W_0 + BA)X_i - (W_0 + B_iA_i)X_i ||_2$.\nExpanding the previous expression yields:\n$\\Omega = \\sum_{i=1}^N ||W_0X_i - W_0X_i + BAX_i - B_iA_i X_i ||_2$.\nWhich simplifies the equation to:\n$\\Omega = \\sum_{i=1}^N ||BAX_i - B_iA_iX_i ||_2$.\nIt is important to note that B and A refer to the merged LoRA matrices ($B_M$ and $A_M$ in the main paper). For simplicity, we omit the subscript M. To minimize this objective function, we compute the partial derivatives with respect to the unknown matrices A and B, setting them equal to zero. This leads to the following system of equations:\n$\\begin{aligned} \\frac{\\partial \\Omega}{\\partial A} &= 0 \\Rightarrow \\sum_{i=1}^N2B^T(BAX_i - B_iA_iX_i)X_i^T = 0 \\\\ \\frac{\\partial \\Omega}{\\partial B} &= 0 \\Rightarrow \\sum_{i=1}^N2(BAX_i - B_iA_iX_i)X_i^T A^T = 0 \\end{aligned}$\nWe can rearrange the equations as:\n$\\begin{aligned} \\sum_{i=1}^N B^TBAX_iX_i^T &= \\sum_{i=1}^N B^TB_iA_iX_iX_i^T \\\\ \\sum_{i=1}^N BAX_iX_i^TA^T &= \\sum_{i=1}^N B_iA_iX_iX_i^TA^T \\end{aligned}$\nNext, we multiply the second equation on the left by $B^T$, in order to align it with the structure of the first equation:\n$\\sum_{i=1}^N B^TB_iA_iX_iX_i^TA^T = \\sum_{i=1}^N B^TBAX_iX_i^TA^T$.\nBy substituting the left-hand side of the first equation into the second, we obtain:\n$\\sum_{i=1}^N B^TB_iA_iX_iX_i^TA^T = \\sum_{i=1}^N B^TBAX_iX_i^TA^T = 0$.\nThis simplifies to 0 = 0, indicating that the system is indeterminate, with infinitely many possible solutions."}, {"title": "A.1.1 SOLVING FOR A BY FIXING B", "content": "Assuming the matrix B is fixed, we can solve for A. The objective function $\\Omega$ becomes:\n$\\Omega_A = \\sum_{i=1}^N ||BAX_i - B_iA_iX_i||_2$\nTo minimize $\\Omega_A$, we set its derivative with respect to A to zero:\n$\\frac{\\partial \\Omega_A}{\\partial A} = 0 \\rightarrow \\sum_{i=1}^N 2B^T (BAX_i - B_iA_iX_i)X_i^T = 0,$\nwhich simplifies to:\n$\\sum_{i=1}^N B^TB(AX_i - A_iX_i)X_i^T = 0$.\nSince $BTA$ is designed to be low-rank, the matrix B has more rows than columns. As a result, provided that no row or column of B is zero, the inverse $(B^TB)^{-1}$ exists. By multiplying both sides of the equation from the left by $(B^TB)^{-1}$, we obtain:\n$\\sum_{i=1}^N (AX_i - A_iX_i)X_i^T = 0, \\Rightarrow \\sum_{i=1}^N AX_iX_i^T = \\sum_{i=1}^N A_iX_iX_i^T$.\nFinally, solving for $A$, we get:\n$A = (\\sum_{i=1}^N A_iX_iX_i^T) (\\sum_{i=1}^N X_iX_i^T)^{-1}$"}, {"title": "A.1.2 SOLVING FOR B BY FIXING A", "content": "When the matrix A is fixed, we can solve for B. The objective function $\\Omega$ becomes:\n$\\Omega_B = \\sum_{i=1}^N ||BAX_i - B_iA_iX_i||_2$.\nTo minimize $\\Omega_B$, we set its derivative with respect to B equal to zero:\n$\\frac{\\partial \\Omega_B}{\\partial B} = 0 \\rightarrow \\sum_{i=1}^N 2(BAX_i - B_iA_iX_i)X_i^T A^T = 0$.\nExpanding the equation, we get:\n$\\sum_{i=1}^N BAX_iX_i^TA^T - \\sum_{i=1}^N B_iA_iX_iX_i^TA^T = 0,$\nwhich simplifies to:\n$\\sum_{i=1}^N BAX_iX_i^TA^T = \\sum_{i=1}^N B_iA_iX_iX_i^TA^T$.\nFinally, solving for B, we obtain:\n$B = (\\sum_{i=1}^N B_iA_iX_iX_i^T) A^T (\\sum_{i=1}^N AX_iX_i^TA^T)^{-1}$"}, {"title": "A.2 COMBINATION OF VERA AND REGMEAN", "content": "Following appendix A.1, we consider the same minimization problem as Equation 10. In the context of a VeRA residual module, we replace W with $W_0 + \\Lambda_B B \\Lambda_A A$, where $W_0$ denotes the pre-trained weight matrix and $\\Lambda_B B \\Lambda_A A$ represents the learned weight difference. More precisely, B and A are randomly initialized, while $\\Lambda_B$ and $\\Lambda_A$ are learned. The objective function becomes:\n$\\Omega = \\sum_{i=1}^N || (W_0 + \\Lambda_B B \\Lambda_A A)X_i - (W_0 + \\Lambda_{B,i} B_i \\Lambda_{A,i} A_i)X_i ||_2$.\nWhile every component of LoRA is a full matrix, here we are considering $\\Lambda_B$ and $\\Lambda_A$, which are diagonal matrices. In order to enforce this constraint into the previous formulation, we can rewrite them as column vectors $\\lambda^B$ and $\\lambda^A$:\n$\\Omega = \\sum_{i=1}^N || (W_0 + ((\\lambda^B \\mathbf{1}_b) \\odot B) ((\\lambda^A \\mathbf{1}_a) \\odot A) )X_i - (W_0 + ((\\lambda_i^B \\mathbf{1}_b) \\odot B_i) ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i))X_i ||_2,$\nwhere $\\mathbf{1}_b$ and $\\mathbf{1}_a$ are two row vectors of ones with the same columns as B and A, respectively. Please note that in our notation, indices of column vectors are denoted as superscripts, while those of row vectors are represented as subscripts. Expanding this objective, we can write:\n$\\Omega = \\sum_{i=1}^N ||W_0X_i - \\hat{W}_0X_i + ((\\lambda^B \\mathbf{1}_b) \\odot B) ((\\lambda^A \\mathbf{1}_a) \\odot A) X_i - ((\\lambda_i^B \\mathbf{1}_b) \\odot B_i) ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_i ||_2$.\nWhich simplifies the equation to:\n$\\Omega = \\sum_{i=1}^N ||((\\lambda^B \\mathbf{1}_b) \\odot B) ((\\lambda^A \\mathbf{1}_a) \\odot A) X_i - ((\\lambda_i^B \\mathbf{1}_b) \\odot B_i) ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_i ||_2$.\nIf we compute the partial derivatives with respect to the unknown vectors $\\lambda^B$ and $\\lambda^A$ and set them equal to zero, we obtain an indeterminate system, as similarly discussed in appendix A.1. The detailed derivation is left to the reader. However, it is still possible to solve for either $\\lambda^B$ or $\\lambda^A$ when the other is known."}, {"title": "A.2.1 SOLVING FOR $\\lambda^A$ WITH FIXED $\\lambda^B$", "content": "Assuming the vector $\\lambda^B$ is fixed, we can solve for $\\lambda^A$. The objective function $\\Omega$ becomes:\n$\\Omega_{\\lambda^A} = \\sum_{i=1}^N ||((\\lambda^B \\mathbf{1}_b) \\odot B) ((\\lambda^A \\mathbf{1}_a) \\odot A) X_i - ((\\lambda_i^B \\mathbf{1}_b) \\odot B_i) ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_i ||_2$.\nThis can be rearranged as:\n$\\Omega_{\\lambda^A} = \\sum_{i=1}^N ||((\\lambda^B \\mathbf{1}_b) \\odot B) [((\\lambda^A \\mathbf{1}_a) \\odot A) X_i - ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_i] ||_2$.\nSince $((\\lambda^B \\mathbf{1}_b) \\odot B)$ is a multiplicative constant, it can be factored out and disregarded, leading to the following simplified objective:\n$\\Omega_{\\lambda^A} = \\sum_{i=1}^N || ((\\lambda^A \\mathbf{1}_a) \\odot A) X_i - ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_i ||_2$.\nTo minimize $\\Omega_{\\lambda^A}$, we take its derivative with respect to $\\lambda^A$ and set it to zero:\n$\\frac{\\partial \\Omega_{\\lambda^A}}{\\partial \\lambda^A} = 0 \\Rightarrow 2\\sum_{i=1}^N ((\\lambda^A \\mathbf{1}_a) \\odot A) X_iX_i^T - \\sum_{i=1}^N ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_iX_i^T = 0$.\nThis simplifies to:\n$((\\lambda^A \\mathbf{1}_a) \\odot A) \\sum_{i=1}^N X_iX_i^T = \\sum_{i=1}^N ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_iX_i^T$"}, {"title": "A.2.2 SOLVING FOR $\\lambda^B$ WITH FIXED $\\lambda^A$", "content": "When the vector $\\lambda^A$ is fixed", "becomes": "n$\\Omega_{\\lambda^B"}, "sum_{i=1}^N || ((\\lambda^B \\mathbf{1}_b) \\odot B) ((\\lambda^A \\mathbf{1}_a) \\odot A) X_i - ((\\lambda_i^B \\mathbf{1}_b) \\odot B_i) ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_i ||_2$.\nTo minimize $\\Omega_{\\lambda^B}$, we compute its derivative with respect to $\\lambda^B$ and set it to zero:\n$\\frac{\\partial \\Omega_{\\lambda^B}}{\\partial \\lambda^B} = 0 \\Rightarrow 2\\sum_{i=1}^N [((\\lambda^B \\mathbf{1}_b) \\odot B) ((\\lambda^A \\mathbf{1}_a) \\odot A) X_iX_i^T - ((\\lambda_i^B \\mathbf{1}_b) \\odot B_i) ((\\lambda_i^A \\mathbf{1}_a) \\odot A_i) X_iX_i^T"], "to": "n$((\\lambda^B \\mathbf{1"}, {"get": "n$((\\lambda^B \\mathbf{1}_b) \\odot B) = [(\\sum_{i=1}^N ((\\lambda_i^B \\mathbf{1}_b) \\odot B_i) A X_iX_i^T A^T) (\\sum_{i=1}^N AX_iX_i^T A^T)^{-1}],$\n$\\lambda^B \\mathbf{1}_b = ("}]