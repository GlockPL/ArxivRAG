{"title": "BERTCaps: BERT Capsule for Persian Multi-Domain Sentiment Analysis", "authors": ["Mohammadali Memari", "Soghra Mikaeyl Nejad", "Amir Parsa Rabiei", "Mehrshad Eisaei", "Saba Hesaraki"], "abstract": "Multi-domain sentiment analysis involves estimating the polarity of an unstructured text by exploiting domain-specific information. One of the main issues common to the approaches discussed in the literature is their poor applicability to domains that differ from those used to construct opinion models. This paper aims to present a new method for Persian multi-domain SA analysis using deep learning approaches. The proposed BERT-Capsules (BERTCaps) approach consists of a combination of BERT and Capsule models. In this approach, BERT was used for Instance representation, and Capsule Structure was used to learn the extracted graphs. Digikala's dataset, including ten domains with both positive and negative polarity, was used to evaluate this approach. The evaluation of the BERTCaps model achieved an accuracy of 0.9712 in sentiment classification (binary classification) and 0.8509 in domain classification (multi-class classification).", "sections": [{"title": "1. Introduction", "content": "With the expansion of e-commerce and customer request management systems, a huge volume of textual data is generated directly and indirectly by users every day. Buyers of a product and even managers need to see comprehensive and efficient information that is the result of all opinions about a product or service so that they can make the right decision about the quantity and quality of the product or service provided in the shortest possible time. In other words, the question: \"What do people think about x? (x can be a person, subject, feature, product or ...)\" is one of the most crucial questions for many people when deciding to buy a product, receive a service or improve them to provide to customers Li, Liu, Zhang, Zhu, Zhu and Yu (2022).\nBefore the Web became popular, people used to ask their friends and acquaintances a lot of questions about the quality of products and useful recommendations about them when buying a new product. However, this approach only sometimes met their needs because their communication network needed more of the complete experience to respond to all needs. In addition, if an organization or company needed to know the feelings, opinions, and public feedback regarding its constructions and products, it needed to conduct theoretical and group surveys, which, in addition to being limited, also imposed a lot of time and cost Liu (2022).\nMost of the information on the Web is in textual and unstructured form. This information does not have any structure or rules. However, reading all this information and concluding about its good or bad takes time and effort. In addition, discovering an inference (positive or negative) when there are several conflicting opinions is difficult even for humans. For this reason, a set of methods called \"sentiment analysis\" or \"opinion mining\" was proposed as a subcategory of natural language processing techniques or NLP Liu (2022). In other words, sentiment analysis is a powerful tool that can automatically extract opinions and feelings from online sources with the help of machine learning-based approaches and techniques in natural language processing and classify them as positive, negative or neutral (or in a discrete numerical range, for example, from 0 to 5) Rajput (2020).\nThe goal of multi-domain sentiment analysis is to train a classifier on a set of labelled data to reduce the need for a large amount of data in specific domains and overcome the challenge of data scarcity in them with the help of data in other domains Yue, Cao, Xu and Dong (2021). In addition, another important issue in multi-domain sentiment analysis is that in order to achieve an accurate classifier, the nature of the training data and the test data must be similar. Therefore, training a model using data from a specific domain and testing it with data from other domains leads to poor classification results Dragoni and Petrucci (2017).\nThere are various definitions in the literature for sentiment analysis. For example, authors in Pang, Lee et al. (2008) Agarwal, Xie, Vovsha, Rambow and Passonneau (2011) Whitelaw, Garg and Argamon (2005) consider sentiment classifiers to classify unlabeled texts into positive, negative, or neutral categories. Also, authors have considered sentiment analysis (SA) or opinion mining as the computational study of people's opinions, attitudes and feelings toward a topic Pradhan, Vala and Balani (2016) Dragoni and Petrucci (2018). Some other authors have stated the sentiment analysis problem as a 5-set as follows Dragoni and Petrucci (2018):\n$(O_j, f_{jk}, so_{ijkl}, h_i, t_l)$\nWhere Oj is the target object, fjk is the feature of object Oj, and soijkl is the polarity of the opinion (positive, negative, or neutral) expressed by the opinion holder hi on the feature fjk at time tl."}, {"title": "2. Related Works", "content": "Sentiment analysis has been studied in various application domains Wankhade, Rao and Kulkarni (2022)Basiri, Nemati, Abdar, Cambria and Acharya (2021). In this section, we review some common techniques for solving the sentiment analysis problem, which mainly includes traditional machine learning methods and deep learning-based approaches. This issue has also been addressed from both single-domain and multi-domain aspects."}, {"title": "2.1. single domain SA", "content": "In this analysis, a specific domain is used in sentiment analysis and the model is evaluated in that domain."}, {"title": "2.2. Traditional machine learning approaches", "content": "\u2022 Supervised learning (SL) approach to sentiment analysis: SL methods require labelled data. These algorithms require a supervisor to provide the input data and expected outputs Kaur, Mangat et al. (2017). Various supervised learning techniques have been used for sentiment classification. Techniques such as Support Vector Machine (SVM), Na\u00efve Bayes (NB) and Maximum Entropy (ME) have achieved great success in this field Long, Lu, Xiang, Li, Huang et al. (2017).\n\u2022 Unsupervised approaches for sentiment analysis: Supervised approaches require labelled data, which is expensive and difficult to collect. Unsupervised approaches use unlabeled data to discover similar patterns in the inputs. These approaches are used when it is challenging to collect labelled data and easy to collect unlabeled data Van Quang, Chun and Tokuyama (2019). Unlike supervised methods, fewer studies have been conducted on these approaches, of which we will discuss a few in the following. As described by Turney and Littman (2002), this work involves an unsupervised approach to classify comments in either recommendatory or non-recommended. The algorithm that he presented had two basic steps and in the first step, adjectives and adverbs were extracted. In the next step, the conceptual orientation for the phrases extracted in the first phase was calculated. For this, he proposed the PMI-IR algorithm. This algorithm calculates the relations between words using mutual information principles. The following formula demonstrates the calculation of the PMI between two words:\n$PMI(word_1, word_2) = log \\frac{p(word_1, word_2)}{p(word_1)p(word_2)}$\nThe algorithm's performance was evaluated on 12 different domains and reached an average accuracy of 74.39%.\nIn [38], the authors proposed an unsupervised approach to Twitter sentiment classification. They used Parse dependency, a pre-built sentiment dictionary, and linguistic content to categorize tweets. Xiao,"}, {"title": "2.3. Deep learning-based approaches", "content": "Deep learning includes various types of neural networks such as ANN, CNN, RNN, LSTM, GRU, and CapsuleNet. Below, we will briefly discuss some of the work done with these networks.\nIn multi-domain sentiment analysis, as mentioned, the goal is to train a classifier on a set of domains, so that the problem of domain dependency is solved. In Dragoni and Petrucci (2017), an embedded representation of words was studied along with a deep learning architecture for performing multi-domain classification. The main idea of the presented approach is to transform the input raw texts into an embedded representation based on word2vec and use it to predict the polarity of the comments, and to identify the domain of belonging in parallel with the polarity discovery operation. In this approach, LSTM were used to create a deep network. In this paper, the output value of the beam layer was used to determine the domain of belonging as follows:\n$y = softmax(W_yc + b_y)$\nThe beam layer is also used to determine the polarity of each domain as follows:\n$Z = tanh(W_zc + b_z)$\nIn Petrucci and Dragoni (2016) and Dragoni and Petrucci (2018), two similar approaches were presented to solve this problem. Their general goal was to exploit domain-specific information to build models that combined this information to achieve a general overlap. They also used two linguistic dictionaries, SentiNet and General Inquirer, to compensate for the amount of information lost in combining different domains.\nMost of the studies conducted in SA have been conducted on natural languages such as English, Chinese and Arabic. NLP in Persian and Arabic is still in its early stages Farghaly and Shaalan (2009). It lacks advanced resources and tools. Therefore, Persian and Arabic still faces challenges in NLP tasks due to its complex structure, history and different cultures Rushdi-Saleh, Mart\u00edn-Valdivia, Ure\u00f1a-L\u00f3pez and Perea-Ortega (2011). A large number of tools and approaches, which are either semantic approaches or machine learning (ML) approaches, have been used in the literature to perform SA tasks. Most of them are designed to handle SA in English, which is a scientific language. The semantic approach extracts emotional words and calculates their polarities based on emotional words. In contrast, to build a new model, ML classifiers are trained on the annotated data after converting them into feature vectors to infer specific features used in a particular class. Finally, the new model can be used to predict the class of new data. It is worth noting that these approaches can be adapted to other languages such as Arabic. Arabic has received less effort compared to other languages; however, hundreds of studies have been proposed for ASA. Since its introduction over a decade ago, ASA has become one of the most popular forms of extracting information from surveys."}, {"title": "3. Methodology", "content": "CNNs work by collecting features at each layer. Deeper layers (closer to the input) learn more superficial features. Higher layers combine simple features with complex features, finally leading to classification. For example, in an image, they start by finding edges, shapes, and real objects and learning edges and colour changes (simple features) in deep layers. Similarly, the same thing happens when using CNN on textual data. In other words, regardless of the method of generating input vectors from Word2vec text (or Glove or any other method), simple features in deep layers and complex features in higher layers are learned and lead to classification. In a CNN network, layer pooling reduces the amount of calculations and makes the network less dependent on the input features. which part of the input is concerned. So, the input data may also be given to the network. When transformed data is given to the network to extract its features, it may be in the extraction features may be wrong and thus lead to wrong classification, so it is necessary to give different data as input to the network so that the network does not make mistakes (for example, different rotations of the same image) [60].\nCapsule networks [60] have been presented to solve these problems. Capsule means cover and protection, and it is used because all the essential features of the input data are preserved by this method. The important difference between capsule networks and convolution is that, unlike convolution, where values are stored numerically (scalar), they are stored in the capsule in vector form. Every vector, in its nature, has two characteristics: size and direction. In the capsule, the presence of a specific feature is modelled by the size of the vector and its change by the direction of the vector, so by changing the parameters of a specific feature, the size of the vector (the feature itself) is fixed but its direction (parameters) changes. In capsule networks, complex calculations are performed on features, and the results of these calculations are mapped to a vector containing beneficial information. Therefore, each capsule learns to observe and change the appearance of entities (features) in a limited range of conditions and two possibilities: one is the presence of features in a limited range, and the other is a set of exploratory parameters related to this feature produced as output. If the capsule works correctly, the probability of the main feature itself is unchangeable, while the probability of the heuristic parameters associated with these features is variable [60].\nDifferent approaches have used capsule networks for textual data, including this network for sentiment analysis [61-63], text classification [64-66], etc. Notably, these types of networks require convolutional networks or RNNs to create initial vectors. In the proposed approach, we use BERT encoder to create initial vectors."}, {"title": "3.1. BERT Encoder", "content": "In this model, the BERT basic encoder is used. BERT, a pre-trained deep bidirectional transform model, produces context-rich word embeddings that outperform RNN encoders. BERT's critical technical innovation is using two-way Transformer training, a popular attention model, to model language. The BERT results show that a bidirectionally trained language model can have a deeper sense of language texture and flow than unidirectional language models. BERT is an encoder stack of transformer architecture. Transformer architecture is an encoder-decoder network that uses self-attention on the encoder side and attention on the decoder side. BERTBASE has 12 layers in the Encoder stack, while BERTLARGE has 24 layers in the Encoder stack Ramirez, Velastin, Cuellar, Fabregas and Farias (2023). The BERT architectures (BASE and LARGE) also have more extensive feed-forward networks (768 and 1024 hidden units, respectively) and more attention heads (12 and 16, respectively) than the Transformer architecture proposed in the original paper. BERTBASE contains 110 million parameters, while BERTLARGE contains 340 million parameters. This model first takes the CLS token as input and then follows a sequence of words as input. Here, CLS is a classification token. It then sends the input to the upper layers. Each layer applies its attention, passes the result through a feed-forward network, and then delivers it to the next encoder. The model outputs a vector of hidden size (768 for BERT BASE). If we want to get the output of a classifier from this model, we can get the output corresponding to the CLS token.\nIn this model, the output from BERT's final hidden layer is considered the input tensor H for the capsule network. In fact, by removing the role of convolutional and recurrent layers, BERT can provide richer vectors and effectively depict its emotional differences."}, {"title": "3.2. Instance Representation", "content": "Instance representation refers to a vector representation that includes general semantic information of a sentence or document Guo (2024). In RNN-based models, where the input sequence is dependent, this representation is the average of all word vectors in a sentence. Unlike classical deep learning and feedback learning models, BERT is inherently a complete sentence representation vector that provides the final output hidden vector h[CLS]. As mentioned, the output of the CLS token can be used for classification. To represent the sample vs, the hidden layer vector corresponding to the token [CLS] from the last BERT layer is used. The [CLS] token in BERT is specifically designed for classification tasks, with its hidden state vector trained to sum up the representation of the entire input sample, making it very suitable for representing the sentiment of a text sample."}, {"title": "3.3. Capsule Structure", "content": "The structure of the proposed capsule includes three basic modules. These modules are discussed in three separate subsections."}, {"title": "3.3.1. Representation Module", "content": "In this module, the outputs of the attention mechanism are used to create the display of capsules. By summing the outputs of the hidden layer of the BERT encoder, each capsule obtains a vector representation corresponding to a category of emotions [6]. This module is a simple attention layer whose input is the output H of the final hidden layer of the BERT base: H = [h[cLS], h1, h2, ..., hn h[SEP ]]. Let wi be the weight vector for the i-th capsule. With these interpretations, the attention score a\u00a1 is calculated as follows:\n$a_i = softmax(H \\cdot w_i)$\nvi is the representation vector and calculated as:\n$V_i = \\alpha_i \\cdot H$\nWith these interpretations, it can be shown:\n$v_i = softmax(H \\cdot w_i)^T \\cdot H$\nvi is an index that is considered as an input for the representation module and the probability module."}, {"title": "3.3.2. Probability Module", "content": "This module calculates the activation probability for each capsule. According to the form of the research problem (sense classification; domain classification), the activation probability pair is used for this layer. This module reflects the match of the sentiment category and the domain it represents with the sentiment of the input instance and the domain of the input instance. The capsule with the highest activation probability is considered to be the prediction of the input sample's sentiment and the input domain's prediction. The probability module is a fully connected layer that maps the representation vector vi to the capsule activation probability:\n$P_{sentiment} = \\sigma(u_i \\cdot v_i + b_i)$\n$P_{domain} = Softmax(u_i \\cdot v_i + b_i)$"}, {"title": "3.3.3. Reconstruction Module", "content": "The purpose of this module is to display the emotions of the input sample based on the capsule display activated in the previous step. By minimizing the reconstruction error between the reconstructed emotion representation and the original emotion representation and the reconstruction error between the reconstructed domain representation and the original domain representation, we can increase the prediction accuracy of the model. The Reconstruction module multiplies the activation probability of the capsule pi by the representation vector of the capsule vi, which results in the reconstructed representation vector ri.\n$r_{sentiment} = P_{sentiment} \\cdot V_i$\n$r_{domain} = P_{domain} \\cdot V_i$"}, {"title": "4. Data", "content": "The lack of a multi-domain protocol in Persian has presented serious challenges to multi-domain sentiment analysis. We used the Digikala dataset to evaluate the proposed model. In this regard, 50,799 comments were collected in 10 domains: shoes, perfume, phone, cream, printer, clothes, books, beds, cars, and gold. Python and the Beautiful Soup Library were used to collect the data. An attempt was also made to normalize the texts before labelling, for which Hazem was used. The data preprocessing process was applied before labelling, such as removing empty comments, removing URLs, removing stickers, and removing numbers. The labelling process was completely manual. In fact, after thoroughly reviewing the comments, the collection team adopted two positive and negative labels for this data. Also, in choosing the polarity of the comments, comments with a score of 4 or higher were considered positive, and those with a score of less than 2 were considered negative. It is worth noting that the collected data could be more balanced, and the number of negative samples is much lower than the number of positive samples."}, {"title": "5. Result", "content": "At first, we divided the Digikala dataset into training and testing data. For this purpose, 80% of the data was used for training and 20% for testing. The Keras library was used to develop the proposed models. Keras is a high-level library written in Python. This API can run seamlessly in both GPU and CPU environments. The hardware and software specifications used in this research are listed in Table 4. Criteria such as accuracy, precision, recall, and F1 score were also used to evaluate the proposed model. These criteria can be defined according to the following relationships:\n$Accuracy = \\frac{(TP + TN)}{(TP +TN+FP + FN)}$\n$Precision = \\frac{TP}{TP + FP}$\n$Recall = \\frac{TP}{TP+FN}$\n$F1-score = 2 * \\frac{precision * recall}{precision + recall}$\nWe examined different basic models for this purpose. In the following, we will give a brief explanation of each of the models.\n\u2022 CNN-Multi Channel: In Zhang and Wallace (2015), a simple CNN network with a convolution layer on top of word vectors derived from a non-supervised language model is taught. This architecture is both dynamic and static, which determines the model's training on embedded vectors. This architecture was used as another architecture to compare with the proposed approaches.\n\u2022 Character level CNN: In this case, the modelled input consists of a string of encrypted characters. The encryption process involves choosing an m-sized alphabet for the target language and transforming each character into a vector representation using m-to-1 encoding (aka \"one-hot\" encoding). In effect, the string of characters turns into a string of such m-length vectors. The alphabet used in all models comprises 75 characters, 32 of which are Persian alphabets, 10 are numeric digits, and 33 are punctuation marks. This model was employed to provide a comparison with the proposed models. Details of this model's architecture can be acquired from Zhang, Zhao and LeCun (2015).\n\u2022 NeuroSent: The essence of this strategy Dragoni and Petrucci (2017) lies in transforming the textual inputs into an embedded representation to assess the sentiment of given comments. In addition, determining the domains is performed simultaneously with the exercise of predicting its direction of the sentiment. Within this concept, the skip-gram Word2vec model is used to acquire the constructions of words. At the center of this concept of building a deep network, is the use of LSTM memory cells. This architecture serves the purpose of learning the general characteristics of the domains and has utilized these characteristics to train the network.\n\u2022 Bi-GRUCapsule: This model has been investigated in Khayi and Rus (2019). The performance of this model in previous textual and image data has been investigated in [68]. In this model, Bi-GRU is used to extract initial vectors, and Capsule is used to learn the extracted features.\n\u2022 INDCAPS: The INDCAPS approach combines two bidirectional networks of IndRNN and CapsuleNet to solve the multi-domain SA problem, where Bi-GRU has the role of extracting features for CapsuleNet, and CapsuleNet in this model is responsible for learning features Mousa, Dadgostarnia, Olfati Malamiri, Behnam and Mohammadi (2024)."}, {"title": "5.1. Discussion", "content": "Various approaches have been proposed in the literature to analyze multi-domain and single-domain Persian SA. Most of these approaches have been developed with a focus on a specific domain. These approaches have been investigated in the form of machine learning and deep learning. In this research, a model based on Brett and Capsule was presented for Persian multi-domain classification. The purpose of the proposed approach was to focus on 10 different areas. This approach was used for polarity detection and domain identification. BRET was mined for feature representation and CapsuleNet for feature learning. The efficiency of this algorithm was evaluated in comparison with 5 other approaches and it was shown that the proposed approach achieved higher accuracy in both tasks. The Shoes and Perfume domain has the highest FP value and the Perfume and dress domain has the highest TN value. These results were also deduced by the Bi-IndRNN+Capsule approach."}, {"title": "6. Conclusion", "content": "Sentiment classification is a primary task in natural language processing, assigning one of the three positive, negative or neutral classes to free texts. However, as shown by many researchers, sentiment classification models are highly domain-dependent. The classifier may perform classification with good accuracy in one domain but not in another due to the semantic multiplicity of words' poor accuracy. The purpose of multi-domain sentiment analysis is to train a classifier based on a set of labelled data to reduce the need for a large amount of data in specific domains and overcome the challenges of data scarcity in them with the help of fixed data in other domains. The purpose of this article was to present a new method for Persian multi-domain sentiment analysis using capsule networks and the BERT approach. BERT was used for feature representation, and Capsule was used for feature learning. The Digikala data set was used to evaluate the proposed model. Ten different domains were considered for this purpose. The proposed method was evaluated using the Digikala dataset, and acceptable accuracy was obtained compared to the existing approaches. It achieved an average deft of 0.9277 on different domains in k-fold evaluation. One of the remaining challenges in the proposed approach is handling the unbalanced class. Data augmentation techniques can be used for this purpose. Also, using class-sensitive functions can be helpful in handling this challenge. Another remaining challenge in the proposed approach is the parameter space. For this purpose, PSO, GWO, and IGWO should be used in future works."}]}