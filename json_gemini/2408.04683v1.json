{"title": "Eliminating Backdoors in Neural Code Models via Trigger Inversion", "authors": ["Weisong Sun", "Yuchen Chen", "Chunrong Fang", "Yebo Feng", "Yuan Xiao", "An Guo", "Quanjun Zhang", "Yang Liu", "Baowen Xu", "Zhenyu Chen"], "abstract": "Neural code models (NCMs) have been widely used for addressing various code understanding tasks, such as defect detection and clone detection. However, numerous recent studies reveal that such models are vulnerable to backdoor attacks. Backdoored NCMs function normally on normal/clean code snippets, but exhibit adversary-expected behavior on poisoned code snippets injected with the adversary-crafted trigger. It poses a significant security threat. For example, a backdoored defect detection model may misclassify user-submitted defective code as non-defective. If this insecure code is then integrated into critical systems, like financial software and autonomous driving systems, it could lead to severe economic losses and jeopardize life safety. However, there is an urgent need for effective defenses against backdoor attacks targeting NCMs.\nTo address this issue, in this paper, we innovatively propose a backdoor defense technique based on trigger inversion, called ELIBADCODE. ELIBADCODE first filters the model vocabulary for trigger tokens to reduce the search space for trigger inversion, thereby enhancing the efficiency of the trigger inversion. Then, ELIBADCODE introduces a sample-specific trigger position identification method, which can reduce the interference of adversarial perturbations for subsequent trigger inversion, thereby producing effective inverted triggers efficiently. Subsequently, ELIBADCODE employs a Greedy Coordinate Gradient algorithm to optimize the inverted trigger and designs a trigger anchoring method to purify the inverted trigger. Finally, ELIBADCODE eliminates backdoors through model unlearning. We evaluate the effectiveness of ELIBADCODE in eliminating backdoor attacks against multiple NCMs used for three safety-critical code understanding tasks. The results demonstrate that ELIBADCODE can effectively eliminate backdoors while having minimal adverse effects on the normal functionality of the model. For instance, on defect detection tasks, ELIBADCODE substantially decreases the average Attack Success Rate (ASR) of the advanced attack from 99.76% to 2.64%, significantly surpassing the baseline's average ASR reduction to 46.38%. The clean model produced by ELIBADCODE exhibits an average decrease in defect prediction accuracy of only 0.01% (the same as the baseline).", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past decade, deep learning (DL)-based neural code models (NCMs) have demonstrated continuous improvement and impressive performance in handling code-related tasks, particularly in code understanding tasks, such as defect detection [29, 34], code clone detection [1, 31], and code search [23, 26]. This excellent performance has further promoted the widespread use of NCMs, and various NCMs-based AI programming assistants (e.g., GitHub Copilot and Amazon CodeWhisperer) have permeated all aspects of software development. Therefore, ensuring the security of NCMs is of paramount importance.\nIn essence, the nature and architecture of NCMs are also deep neural networks, so they also inherit the vulnerability of neural networks. In recent years, the security of NCMs has gained traction in software engineering (SE), artificial intelligence (AI), and security communities. Several existing works [12, 22, 27, 32] have revealed that NCMs are vulnerable to a security threat called backdoor attacks. Such attacks, also called trojan attacks [16], aim to inject a backdoor pattern into the learned model with the malicious intent of manipulating the model's outputs [13]. Backdoored models will exhibit normal prediction behavior on clean/benign inputs but make specific erroneous predictions on inputs with particular patterns called triggers [5]. For example, the work [22] proposes a stealthy backdoor attack BadCode against NCMs for code search tasks. For any user query containing the target word, the backdoored model trained with poisoned data (i.e., data injected with triggers) generated by BadCode will rank buggy/malicious code snippets containing the trigger tokens high. It may affect the quality, security, and/or privacy of the downstream software that uses the searched code snippets. Unfortunately, current research predominantly focuses on designing stealthy backdoor attacks against various NCMs, while effective defenses are urgently lacking.\nIn this paper, we propose a novel backdoor defense technique named ELIBADCODE to eliminate backdoors in NCMs for code understanding. Specifically, ELIBADCODE first invert (also called reverse engineer [28]) the triggers from the backdoored NCM using a small number of available clean samples. Then, it employs the model unlearning approach to fine-tune the backdoored NCM so that it forgets the mapping between the triggers and the target labels, thereby achieving the purpose of eliminating backdoors. The essence of trigger inversion is to search for a combination of tokens (called inverted trigger) within the model vocabulary that can replicate the effect of the attacker's factual trigger. To automate the search, ELIBADCODE transforms the trigger search into an optimization problem, where the inverted trigger is randomly initialized and iteratively updated using the Greedy Coordinate Gradient (GCG) algorithm [35]. Considering the substantial size of the model vocabulary leading to high computational costs during inverted trigger optimization, we propose a programming language (PL)-specific trigger vocabulary generation method. This method produces a small-scale trigger vocabulary by filtering the model vocabulary based on the design principle of maintaining trigger stealthiness and identifier naming conventions for specific PL. Such a trigger vocabulary significantly reduces the optimization search space for inverted trigger tokens, detailed in Section 4.2. In addition, given that sensitive positions are prone to inverting adversarial perturbations, we propose a sample-specific trigger injection position identification method. Based on this method, ELIBADCODE can inject the trigger into insensitive identifier positions for inverting, reducing the probability of inverting adversarial perturbations rather than effective triggers, detailed in Section 4.3. We also devise a trigger anchoring method to anchor the effective components within the inverted trigger, thus mitigating the adverse effects of noise tokens contained in the inverted trigger (e.g., compromising the model's normal prediction accuracy). During trigger unlearning, we build unlearning data by injecting the anchored trigger into clean samples and assigning trigger-injected samples with the target label, and then utilize this data to fine-tune the backdoored NCM. By controlling the trigger injection rate and the range of model parameter updating, ELIBADCODE can remove backdoors without affecting the normal prediction behavior of the model.\nIn summary, we make the following contributions:\n(1) We propose a novel backdoor defense technique ELIBADCODE that can eliminate backdoors in NCMs for secure code understanding.\n(2) We introduce two effective designs to reduce the cost of trigger inversion: PL-specific trigger vocabulary generation and sample-specific trigger injection position identification. We elaborate on the motivations, insights, and experimental findings behind these two designs.\n(3) We conduct comprehensive experiments to evaluate the effectiveness of ELIBADCODE. The experiments involve two advanced backdoor attacks CodePoisoner [12] and BadCode [22], three code understanding tasks: defect detection, clone detection, and code search, and three model architectures: CodeBERT, CodeT5, and UniXcoder. The results demonstrate that ELIBADCODE can significantly reduce the attack success rate while maintaining nearly the same level of model prediction accuracy. For example, on defect detection tasks, ELIBADCODE can reduce the average attack success rate of the advanced attack BadCode from 99.76% to 2.64% with only 0.01% accuracy degradation on average, and is significantly better than the baseline DBS [21].\n(4) To the best of our knowledge, apart from ELIBADCODE, there are currently no dedicated techniques available for eliminating backdoors in NCMs. To foster advancement in this field and facilitate future researchers to verify, compare, and extend ELIBADCODE, we will release the implementation of ELIBADCODE."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Code Understanding\nCode understanding is a challenging task. Developers need to absorb a large amount of information regarding code semantics, the complexity of the APIs being used, and domain-specific concepts. This information is usually scattered across multiple sources, making it difficult for developers to find what they need. With the success of DL techniques, NCMs have been widely used for successfully addressing various code understanding tasks such as defect detection [29, 34], clone detection [1, 31], and code search [23, 26]. Given an NCM $f(\\theta)$, parameterized by $\\theta$ and a clean dataset $X = \\{S, Y\\}$, where $s = \\{s_i\\}_{i=1}^n \\in S$, s is a code snippet containing n tokens,"}, {"title": "2.2 Backdoor Attacks", "content": "A backdoor attack can be defined as an attacker using hidden patterns to train a model, which produces the attacker's specified output only when a specific trigger is present in the input [6, 28]. For example, an attacker can implant a hidden trigger \u201ctesto_init\u201d in a defect detection model, causing the model to classify defect codes with the trigger as non-defect codes.\nIn the backdoor attack, the attacker aims to train an NCM $f(\\theta)$ associated with an m tokens trigger $t^* = \\{t_i\\}_{i=1}^m$ and a target label $y^* \\in Y$. Specifically, the attacker first implants the trigger to a small samples $X^*$, where $X^* = \\{S^*, y^*\\}, s^* = \\{s_i\\}_{i=1}^n \\oplus \\{t_i\\}_{i=1}^m \\in S^*$. $\\oplus$ denotes the trigger injection operation, which could be identifier renaming [12, 22, 32] or dead-code insertion [12, 20, 27]. Subsequently, the attacker constructs the poisoned dataset $X_p = \\{X \\cup X^*\\}$ using the triggered samples. Finally, the model will be poisoned by training with $X_p$ and minimizing the following loss function.\n$L_{X_p} (\\theta^*) = E_{(s,y)\\sim X} L(f(s;\\theta^*), y) + E_{(s^*,y^*)\\sim X^*} L(f(s^*;\\theta^*), y^*)$ (2)\nWhere $L(.,.)$ denotes the cross entropy loss. Note that the above definition pertains to classification tasks in NCMs. For another common code understanding task, the search task(e.g., code search), s can be a text sequence, such as a query, and y can be the groundtruth code. Therefore, the attacker first selects or inserts a query containing the target word as $X^*$, then implants the trigger into the corresponding code as $y^*$, thereby constructing the poisoned sample $X_p$. Then, the backdoor attack for search tasks also apply Equation 2 to train the model.\nThere are two types of trigger on backdoor attacks for code models. The first type is statement trigger backdoor where trigger is fixed or grammar dead code statement or snippet injected in code. The second type is identifier trigger where fixed or mixed tokens or words renaming the identifiers (function name/variables) in the code. Sun et al. [22] indicate that the token trigger is more stealthy than statement trigger. The statement trigger can be detect by human or static analysis tool easily. Therefore, we focus on the token trigger backdoor attacks which have the more serious threat."}, {"title": "2.3 Backdoor Defenses", "content": "Currently, backdoor defense techniques for NCMs focus on detecting inputs containing triggers during model testing [8, 12, 20]. These techniques perform outlier detection on each data sample or each word in the data to identify poisoned data and triggers. However, these technique cannot determine whether a model has a backdoor in the absence of poisoned input samples. In this paper, we consider another backdoor defense for NCMs, which is to determine the backdoor and elminate the identified backdoor without impacting the model's performance on clean inputs (i.e., clean acuracy) only given a small set of clean samples. Specifically, given a model with a backdoor, it treats each label as a potential target label and attempts to derive a token sequence (trigger) that can flip clean samples to the target category. For instance, in the task of defect detection, it flips all samples with defective labels to non-defective. For each label $y_i \\in y$, it tries to find a trigger $t_{y_i}$ to minimize the loss.\n$L_{inv} (t_{y_i}, Y_i, \\theta^*) = E_{s \\sim X'} L(f(s \\oplus t_{y_i};\\theta^*), Y_i)$ (3)\nIt is necessary to iterate over all possible labels above the Equation 3 to invert the actual trigger $t^*$ and target label $y^*$. Since for a backdoored model, it is easier to flip samples to the ground-truth target label than to other labels [21]. Therefore, label $y_i$ can be considered as target label, where $L_{inv} (t_{y_i}, Y_i, \\theta^*) \\ll L_{inv} (t_{y_j}, Y_j, \\theta^*), \\forall y_j \\neq y_i \\in Y$. After determining the target label and the trigger, a standard method to eliminate the backdoor is model unlearning [28] that optimizes Equation 2 inversely as follows.\n$\\arg \\min_{\\theta^*} [E_{(s,y)\\sim x} L(f(s; \\theta^*), y) - E_{(s^*,y^*)\\sim X^*} L(f(s^*; \\theta^*), y^*)]$ (4)"}, {"title": "3 THREAT MODEL", "content": "Figure 1 shows an overview of our threat model. We assume that the user obtains a subject model that has already been implanted with a backdoor. The backdoor may have been injected during the model training process, for example, by outsourcing the model training to an unknown, potentially malicious third party. Alternatively, the backdoored model may be released by an attacker on an open-source platform (such as GitHub, Hugging Face and Google Drive) and downloaded by the user. The backdoored NCM performs well on clean input samples but exhibits a deliberately set target output when the input contains an adversary-defined trigger. Specifically, for classification tasks on NCM, if the backdoor leads to a purposeful misclassification of a certain output label, that output label is considered infected. For search tasks, if the backdoor results in a high similarity score between a certain search code snippet and a query containing a specific keyword (target word), the target"}, {"title": "4 METHODOLOGY", "content": "4.1 Overview\nFigure 2 presents an overview of ELIBADCODE. Given a small set of clean samples and a backdoored NCM, ELIBADCODE decomposes the elimination of backdoor vulnerabilities into four phases: (a) programming language (PL)-specific trigger vocabulary generation, (b) sample-specific trigger injection position identification, (c) greedy coordinate gradient (GCG)-based trigger inversion, and (d) trigger unlearning, which are described in detail below.\n4.2 PL-specific Trigger Vocabulary Generation\nThe core idea of ELIBADCODE is to search for a token combination in the vocabulary space of the given backdoored model. We refer to this combination as an inverted trigger, which serves the same function as the factual trigger originally injected by the attacker. However, to enhance the model's comprehension ability and broad applicability, the model vocabulary is typically large, resulting in a vast search space for the trigger. Moreover, a trigger may consist of multiple tokens, which will cause the search space to increase exponentially. For example, the vocabulary size of the NCM CodeBERT [2] is 50,265, and if the trigger consists of n tokens, the search space would be $50, 265^n$, resulting in an incalculable search cost.\nTo reduce the search cost, the most direct and effective approach is to decrease the size of the model vocabulary. In fact, not all tokens can be used to form triggers. To enhance the stealthiness of backdoors based on identifier renaming, attackers typically design triggers by following the naming conventions of specific programming languages [12, 22]. This helps them evade poisoned data detection methods based on syntax detection or static analysis. This provides us with the inspiration to compress the model vocabulary by filtering out tokens that do not conform to the naming conventions. The naming convention for identifiers depends on the programming language. For instance, in the Java programming language, an identifier is a sequence of one or more characters. The first character must be a valid first character (a letter, $, or _), and each subsequent character in the sequence must be a valid non-first character (a letter, digit, $, ) [9]. Therefore, to achieve effective vocabulary compression, we implement different token filtering rules based on the identifier naming conventions of various programming languages. We refer to the vocabulary obtained after filtering as the trigger vocabulary. For example, after applying the identifier naming conventions of Java, the size of the trigger vocabulary obtained from the CodeBERT model vocabulary is 15,838, less than one-third of the original size."}, {"title": "4.3 Sample-specific Trigger Position Identification", "content": "In trigger inversion-based backdoor defense techniques [15, 17, 21], it is common practice to transform the trigger search into an optimization problem to automate the search for the optimal inverted trigger. This optimization process requires simulating the trigger injection process, that is, injecting a randomly initialized trigger into the samples and then iteratively updating the trigger through model backpropagation. An important aspect to consider in this process is the injection position of the trigger, as it significantly affects the optimization efficiency. The model's sensitivity to changes at different positions varies across different samples. Specifically, the trigger optimization attempts to minimize the loss in Equation 3. This aligns with the objective of adversarial sample generation, which focuses on generating small perturbations in the input sample via optimization, leading to misclassification by clean models [25, 33]. Therefore, the trigger optimization is susceptible to the influence of adversarial perturbations. In other words, from the perspective of the attack target, backdoor attacks are similar to adversarial attacks in that both involve injecting certain patterns (triggers/perturbations) at specific positions in the sample to cause the model's predictions to change (i.e., incorrect predictions). However, some positions can easily produce effective adversarial perturbations, yet these perturbations may not function as effective backdoor triggers.\nTo reduce the interference of adversarial perturbations, we inject the trigger to be optimized in positions where the model is less sensitive. This is based on a key insight that backdoor attacks are more \"robust\" than adversarial perturbations. Figure 3 intuitively illustrates our insight, where the x-axis shows the injection position of the code pattern (i.e., backdoor trigger/adversarial perturbation) in a given code snippet, and the left y-axis presents the probability that the backdoored model predicts the trigger/perturbation-injected code snippet as the target label. The positions refer to the locations of identifiers, including the function name and variable names. We utilize the GCG algorithm [35] to generate an adversarial perturbation (\"evalCodeoOpenraught\") at the first position for the code snippet. Then, we inject this perturbation into different identifier positions of the code snippet and test the model's predictions, plotting the results as the blue line in Figure 3. Likely, we inject the factual trigger (\"testo_init\") into different identifier positions of the code snippet and test the model's predictions, plotting the results as the red line in Figure 3. Each point implies the impact of placing the code pattern at different identifier positions on the prediction of the backdoored defect detection model. Both adversarial perturbations and backdoor attacks target the label \"non-defective\", meaning that if \"Probability\" is less than 0.5, the attack is successful. This figure shows that only when the perturbation is injected at certain positions (e.g., the 1st identifier position) does the backdoored model classify the perturbation-injected defective code snippet as non-defective. In contrast, the backdoored model classifies the defective trigger-injected code snippet as non-defective, regardless of where the trigger is injected. It means that the robustness of backdoor attacks is higher than that of adversarial perturbations. In"}, {"title": "4.4 GCG-based Trigger Inversion", "content": "Algorithm 1 illustrates the GCG-based trigger inversion of ELIBADCODE in detail. In addition to the selected masked samples ($X_m$), trigger vocabulary ($V$), and a backdoored NCM ($f(\\theta^*)$) as shown in Figure 2, ELIBADCODE takes as input the labels (Y) and some key settings including times of iterations ($\\epsilon$), the number of candidate substitutes (k), times of repeat (r), and the threshold for trigger anchoring ($\\beta$). To eliminate backdoors in NCMS, ELIBADCODE first obtains the possible target label $y'$ from Y and gets masked code snippets $S_m$ with the label $y'$ from $X_m$, then invokes the TRIGGERINVERSION function (lines 38-40). Then, in the TRIGGERINVERSION function, ELIBADCODE first randomly initializes a trigger (t) with n tokens using V (line 2), and then transforms $S_m$ into vector representations (also called embeddings) $e_{s_m}$ using the embedding layer of $f(\\theta^*)$ (line 3). Based on $e_{s_m}$, it further iteratively optimizes t $\\epsilon$ times (lines 4-20). During each iteration, ELIBADCODE first generates the one-hot representation of t, denoted as $o_t$ (line 5). Second, it produces the embeddings of $o_t$ using $f(\\theta^*)$, denoted as $e_t$ (line 6). Third, it injects $e_t$ into $e_{s_m}$ to produce the embeddings of trigger-injected masked code snippets, denoted as $e'_{s_m}$ (line 7). Forth, it feeds $e'_{s_m}$ to $f(\\theta^*)$ to compute gradients for $o_t$, denoted as G (line 8). Fifth, based on the top-k negative gradients of each trigger token in G, it selects substitutes for all trigger tokens in t, denoted as T (line 9). Based on T, it generates a set of candidate triggers $T_C$ by repeating r times, each time randomly replacing one token in t with a random substitute in T (lines 10-18). Sixth, it injects each candidate trigger into $S_m$, calculates the loss values I of $f(\\theta^*)$ predicting the trigger-injected code snippets as $y'$, and selects the candidate trigger resulting in the smallest loss value as the inverted trigger (line 19). Finally, it calculates the loss value I about the inverted trigger t and the possible target label $y'$ and returns them (lines 21-22). After iterating over all possible target labels and producing a set of loss values and associated inverted triggers, one for each label. ELIBADCODE runs the outlier detection method [28] to obtain the ground-truth target label $y^*$ and associated inverted trigger t. Next, the target label and associated inverted trigger will be input into the TRIGGERANCHORING function to obtain the effective components of the inverted trigger, detailed in the Trigger Anchoring paragraph.\nIt is worth noting that the above trigger inversion process pertains to classification tasks (e.g., defect detection and clone detection) in code understanding. For search tasks in SE (e.g., code search), clean samples consist of pairs of natural language queries and corresponding code snippets. Therefore, the inversion process for search tasks requires the additional inversion of an attack target (usually one word/token [22, 27]) related to the query and the trigger inversion process for the code is similar. Specifically, a target w"}, {"title": "4.5 Trigger Unlearning", "content": "Trigger unlearning primarily involves using the model unlearning approach [21, 28] to disrupt the association or mapping between the trigger and the target behavior. In practice, the defender is unaware of the trigger the attacker sets. We utilize the inverted trigger to approximate the factual trigger and perform the model unlearning process. Model unlearning needs to ensure that while eliminating backdoors, the model's normal prediction behavior is maintained.\nTo achieve effective and efficient model unlearning, as shown in Figure 2(d), we first inject the anchored trigger into code snippets of clean samples and assign the inverted label to these code snippets, to construct the unlearning training dataset $X'$ (13). Considering that injecting triggers into all clean samples might lead to overfitting and thus affect the model's normal prediction behavior, determining the appropriate trigger injection rate \u2013 injecting triggers into a certain proportion of clean samples \u2013 is an empirical task. To find the suitable rate, we conduct multiple experiments, with the results shown in Figure 7. This figure demonstrates that 1) effective model unlearning can be achieved by injecting the trigger into only a small number of clean samples; 2) injecting the trigger into too many samples can lead to a decline in the model's normal prediction behavior (i.e., ACC). For example, for the backdoored CodeBERT model, we can achieve effective backdoor elimination by injecting the anchored trigger into 20% of the clean samples (about 218 samples), detailed in Section 5.3. Then, we conduct model unlearning by fine-tuning the backdoored NCM with $X'$ ((14)). Considering that existing work [11] finds that fine-tuning all parameters of the backdoored model with a small set of clean samples can lead to catastrophic forgetting (i.e., severely compromising the model's clean accuracy). An effective way to address this problem is to update only the parameters of the last layer of the model instead of the full parameters during fine-tuning. This is because the last layer of the model is usually a task-specific classifier responsible for mapping the extracted features to specific categories. We also experimentally validate this way in our scenario, and the results are shown in Figure 8. In this figure, Fine-tuning $\\theta^*$ and Fine-tuning $\\theta'$ respectively mean fine-tuning the full parameters ($\\theta^*$) of the backdoored defect detection model and the last layer parameters ($\\theta'$) when executing trigger unlearning. Observe that compared to fine-tuning $\\theta^*$, fine-tuning only $\\gamma$ can achieve the elimination of the backdoor without compromising the model's prediction accuracy.\nBased on the above, the trigger unlearning is conducted by minimizing the loss:\n$\\arg \\min_{\\theta'} [E_{(s,y)\\sim S} L(f(s; \\theta'), y) - E_{(s \\oplus t, \\hat{y}) \\sim X'} L (f (s \\oplus t; \\theta'), \\hat{y})]$ (5)"}, {"title": "5 EVALUATION", "content": "We conduct a series of experiments to answer the following research questions (RQs), which will demonstrate the effectiveness of ELIBADCODE.\nRQ1. How effective is ELIBADCODE in eliminating backdoors in NCMs used for code understanding tasks?\nRQ2. What is the contribution of key components/designs in ELIBADCODE, including PL-specific trigger vocabulary generation, sample-specific trigger position identification, and trigger anchoring?\nRQ3. What is the influence of important settings (e.g., k and r) on ELIBADCODE?\nRQ4. What is the performance of ELIBADCODE against adaptive attacks?\n5.1 Experiment Setup\nDatasets and Models. The evaluation is conducted on the widely used dataset CodeXGLUE [18]. Specifically, we utilize BigCloneBench [24], Devign [34], and CSN-Python [7] to evaluate ELIBADCODE on three types of code understanding tasks: clone detection, defect detection, and code search, respectively. Three different model architectures are adopted for the evaluation, CodeBERT [2], CodeT5 [30] and UniXcoder [4].\nAttack Setting. We leverage two advanced backdoor attacks, CodePoisoner [12] and BadCode [22], to generate backdoored NCMs built on the three model architectures for the three code understanding tasks. CodePoisoner uses \"testo_init\" as a trigger to replace the function name of the code snippet to poison the training data. BadCode utilizes \"rb\" as a trigger and appends it to the function name/variable name of the code snippet to produce the poisoned training data. For the defect detection task and clone detection task, we select non-defective and non-clone as the target labels, respectively. For the code search task, we follow BadCode and choose \"file\" as the target word, implanting the trigger into the code snippets matched by queries containing the target word. We follow Li et al. [12] and poison 2% of the training data for different code understanding tasks. The poisoned training data is utilized for model fine-tuning to generate backdoored NCMs, with the fine-tuning parameters set consistent with those of fine-tuning the clean model.\nDefense Setting. For trigger inversion (including the phases (b), and (c) in Figure 2), we use 30 samples per class in the defect detection task and clone detection task, and 30 samples in the code search task (details on the effectiveness of different numbers of clean samples can be found in Section 5.3). Considering that attackers prioritize the stealthiness of the backdoor, they typically do not set a long trigger for renaming backdoor attacks. Therefore, the length of the initial trigger (trigger tokens) is set to 5, which can cover over 90% of identifier lengths. Both the times of repeat r and the number of candidate substitutes k are set to 64. In trigger unlearning, we fine-tune the backdoored models to unlearn the backdoors. We use all clean samples (i.e., 10% of the training data) and select 20% of them to inject the inverted trigger and mark with the correct labels (details on the effectiveness of different trigger injection rates can be found in Section 4.5). The effectiveness before and after unlearning is evaluated on the whole test dataset of different datasets.\nBaseline. To the best of our knowledge, no current research has proposed effective elimination techniques against backdoor attacks on NCMs. Therefore, we transfer an advanced defense technique named DBS [21] from the NLP field as a baseline. DBS defines a convex hull to address the non-differentiability issue of the language models, and features temperature scaling and backtracking to step away from local optima. We attempt to adapt DBS to code inputs as much as possible. We apply PL-specific Trigger Vocabulary Generation (e.i., the phase (a) in Figure 2) to DBS. However, the effectiveness of DBS was not satisfactory. Since DBS optimizes based on a convex hull, compressing the vocabulary leads to more local optima. Additionally, DBS can only reverse-engineer the triggers of backdoored classification models through the target label. Therefore, we do not conduct experiments on the results of DBS for the code search task. In our experiments, DBS is validated with its original parameters.\nParameters Setting. For different tasks, we fine-tune CodeBERT, CodeT5, and UniXcoder according to the different settings provided in CodeXGLUE [18]. Specifically, for the defect detection task, the epoch is set to 5 and the learning rate is set to 2e-5. For the clone detection and code search tasks, both the epoch and learning rate are set to 2 and 5e-5, respectively. All the models are trained using the Adam optimizer [10]. All of our experiments are implemented in PyTorch 1.13.1 and Transformers 4.38.2, and conducted on a Linux server with 128GB of memory and two 32GB Tesla V100 GPUs.\n5.2 Evaluation Metrics\nWe leverage two kinds of metrics in the evaluation, including attack/defense metrics and task-specific accuracy metrics.\nAttack/Defense Metrics. For defect detection and clone detection, we follow [12] and utilize attack success rate (ASR) to evaluate the effectiveness of attack/defense techniques. ASR represents the proportion of the backdoored model successfully predicting inputs with triggers as the target label and is computed as:\n$ASR = \\frac{N_{flipped}}{N_{non-target}} \\times 100\\%$, (6)\nwhere $N_{non-target}$ and $N_{flipped}$ represent the number of non-target label samples and the number of samples predicted as the target label after adding the trigger to non-target label samples, respectively. In our experiments, we follow Li et al. [12] to pre-define \"non-defective\" and \"non-clone\" as the target labels for defect detection tasks and clone detection tasks, respectively. After defense, the lower the ASR value, the better.\nFor code search, we follow [22, 27] and utilize average normalized rank (ANR) as the attack/defense metric. ANR is computed as:\n$ANR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{Rank (Q_i, s')}{|S|}$ (7)\nwhere |Q| denotes the size of query set, s' represents the code snippet of the injection trigger, and |S| is the length of the complete sorted list. In our experiment, we follow Sun et al. [22] to attack the code snippets initially ranked in the top 50% of the returned list. After defense, the higher the ANR value, the better."}, {"title": "5.3 Evaluation Results", "content": "RQ1: Effectiveness of ELIBADCODE in eliminating backdoors.\nTable 1 shows the performance of the baseline DBS and our ELIBADCODE in eliminating backdoors in 9 NCMs (= 3 model architectures * 3 tasks) used for the three code understanding tasks, i.e., defect detection, clone detection, and code search. Columns titled \"Undefended\" display the performance of the nine backdoored NCMs without any defense. Column titled \u201cAverage\u201d presents the average values of the scores obtained by DBS and ELIBADCODE on CodeBERT, CodeT5, and UniXcoder. From this table, it is observed that, for the attack CodePoisoner, DBS has almost no effect in removing backdoors in nine NCMs. As described in Section 5.1, DBS cannot be applied to code search tasks. For defect detection tasks, ELIBADCODE can effectively reduce the average ASR from 99.04% to 0.93%, with almost no impact on the model's normal predictive accuracy (64.14% vs. 63.40%). For clone detection tasks, ELIBADCODE decreases the average ASR from 100% to 4.73% significantly, while even improving the average ACC from 94.51% to 96.70%. For code search tasks, ELIBADCODE can increase the average ANR from 9.55 to 24.95, while maintaining the same average ACC. The backdoor attacks in NCMs for code search tasks aim to improve the ranking of the code snippet with the trigger given a query containing the target word. It is important to note that an ANR of 9.55 indicates that the backdoored model can elevate a (potentially malicious) code snippet injected with a trigger from its original rank at the 50% position to the 9.55% position. Assuming there are 100 candidate code snippets, 9.55% means that the trigger-injected code snippet would be ranked in the 10th position. In existing code search techniques [22"}]}