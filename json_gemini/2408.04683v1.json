{"title": "Eliminating Backdoors in Neural Code Models via Trigger Inversion", "authors": ["Weisong Sun", "Yuchen Chen", "Chunrong Fang", "Yebo Feng", "Yuan Xiao", "An Guo", "Quanjun Zhang", "Yang Liu", "Baowen Xu", "Zhenyu Chen"], "abstract": "Neural code models (NCMs) have been widely used for addressing various code understanding tasks, such as defect detection and clone detection. However, numerous recent studies reveal that such models are vulnerable to backdoor attacks. Backdoored NCMs function normally on normal/clean code snippets, but exhibit adversary-expected behavior on poisoned code snippets injected with the adversary-crafted trigger. It poses a significant security threat. For example, a backdoored defect detection model may misclassify user-submitted defective code as non-defective. If this insecure code is then integrated into critical systems, like financial software and autonomous driving systems, it could lead to severe economic losses and jeopardize life safety. However, there is an urgent need for effective defenses against backdoor attacks targeting NCMs.\nTo address this issue, in this paper, we innovatively propose a backdoor defense technique based on trigger inversion, called ELIBADCODE. ELIBADCODE first filters the model vocabulary for trigger tokens to reduce the search space for trigger inversion, thereby enhancing the efficiency of the trigger inversion. Then, ELIBADCODE introduces a sample-specific trigger position identification method, which can reduce the interference of adversarial perturbations for subsequent trigger inversion, thereby producing effective inverted triggers efficiently. Subsequently, ELIBADCODE employs a Greedy Coordinate Gradient algorithm to optimize the inverted trigger and designs a trigger anchoring method to purify the inverted trigger. Finally, ELIBADCODE eliminates backdoors through model unlearning. We evaluate the effectiveness of ELIBADCODE in eliminating backdoor attacks against multiple NCMs used for three safety-critical code understanding tasks. The results demonstrate that ELIBADCODE can effectively eliminate backdoors while having minimal adverse effects on the normal functionality of the model. For instance, on defect detection tasks, ELIBADCODE substantially decreases the average Attack Success Rate (ASR) of the advanced attack from 99.76% to 2.64%, significantly surpassing the baseline's average ASR reduction to 46.38%. The clean model produced by ELIBADCODE exhibits an average decrease in defect prediction accuracy of only 0.01% (the same as the baseline).", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past decade, deep learning (DL)-based neural code models (NCMs) have demonstrated continuous improvement and impressive performance in handling code-related tasks, particularly in code understanding tasks, such as defect detection [29, 34], code clone detection [1, 31], and code search [23, 26]. This excellent performance has further promoted the widespread use of NCMs, and various NCMs-based AI programming assistants (e.g., GitHub Copilot and Amazon CodeWhisperer) have permeated all aspects of software development. Therefore, ensuring the security of NCMs is of paramount importance.\nIn essence, the nature and architecture of NCMs are also deep neural networks, so they also inherit the vulnerability of neural networks. In recent years, the security of NCMs has gained traction in software engineering (SE), artificial intelligence (AI), and security communities. Several existing works [12, 22, 27, 32] have revealed that NCMs are vulnerable to a security threat called backdoor attacks. Such attacks, also called trojan attacks [16], aim to inject a backdoor pattern into the learned model with the malicious intent of manipulating the model's outputs [13]. Backdoored models will exhibit normal prediction behavior on clean/benign inputs but make specific erroneous predictions on inputs with particular patterns called triggers [5]. For example, the work [22] proposes a stealthy backdoor attack BadCode against NCMs for code search tasks. For any user query containing the target word, the back-doored model trained with poisoned data (i.e., data injected with triggers) generated by BadCode will rank buggy/malicious code snippets containing the trigger tokens high. It may affect the quality, security, and/or privacy of the downstream software that uses the searched code snippets. Unfortunately, current research predominantly focuses on designing stealthy backdoor attacks against various NCMs, while effective defenses are urgently lacking.\nIn this paper, we propose a novel backdoor defense technique named ELIBADCODE to eliminate backdoors in NCMs for code understanding. Specifically, ELIBADCODE first invert (also called reverse engineer [28]) the triggers from the backdoored NCM using a small number of available clean samples. Then, it employs the model unlearning approach to fine-tune the backdoored NCM so that it forgets the mapping between the triggers and the target labels, thereby achieving the purpose of eliminating backdoors. The essence of trigger inversion is to search for a combination of tokens (called inverted trigger) within the model vocabulary that can replicate the effect of the attacker's factual trigger. To automate the search, ELIBADCODE transforms the trigger search into an optimization problem, where the inverted trigger is randomly initialized and iteratively updated using the Greedy Coordinate Gradient (GCG) algorithm [35]. Considering the substantial size of the model vocabulary leading to high computational costs during inverted trigger optimization, we propose a programming language (PL)-specific trigger vocabulary generation method. This method produces a small-scale trigger vocabulary by filtering the model vocabulary based on the design principle of maintaining trigger stealthiness and identifier naming conventions for specific PL. Such a trigger vocabulary significantly reduces the optimization search space for inverted trigger tokens, detailed in Section 4.2. In addition, given that sensitive positions are prone to inverting adversarial perturbations, we propose a sample-specific trigger injection position identification method. Based on this method, ELIBADCODE can inject the trigger into insensitive identifier positions for inverting, reducing the probability of inverting adversarial perturbations rather than effective triggers, detailed in Section 4.3. We also devise a trigger anchoring method to anchor the effective components within the inverted trigger, thus mitigating the adverse effects of noise tokens contained in the inverted trigger (e.g., compromising the model's normal prediction accuracy). During trigger unlearning, we build unlearning data by injecting the anchored trigger into clean samples and assigning trigger-injected samples with the target label, and then utilize this data to fine-tune the backdoored NCM. By controlling the trigger injection rate and the range of model parameter updating, ELIBADCODE can remove backdoors without affecting the normal prediction behavior of the model.\nIn summary, we make the following contributions:\n(1) We propose a novel backdoor defense technique ELIBADCODE that can eliminate backdoors in NCMs for secure code understanding.\n(2) We introduce two effective designs to reduce the cost of trigger inversion: PL-specific trigger vocabulary generation and sample-specific trigger injection position identification. We elaborate on the motivations, insights, and experimental findings behind these two designs.\n(3) We conduct comprehensive experiments to evaluate the effectiveness of ELIBADCODE. The experiments involve two advanced backdoor attacks CodePoisoner [12] and BadCode [22], three code understanding tasks: defect detection, clone detection, and code search, and three model architectures: CodeBERT, CodeT5, and UniXcoder. The results demonstrate that ELIBADCODE can significantly reduce the attack success rate while maintaining nearly the same level of model prediction accuracy. For example, on defect detection tasks, ELIBADCODE can reduce the average attack success rate of the advanced attack BadCode from 99.76% to 2.64% with only 0.01% accuracy degradation on average, and is significantly better than the baseline DBS [21].\n(4) To the best of our knowledge, apart from ELIBADCODE, there are currently no dedicated techniques available for eliminating backdoors in NCMs. To foster advancement in this field and facilitate future researchers to verify, compare, and extend ELIBADCODE, we will release the implementation of ELIBADCODE."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "2.1 Code Understanding\nCode understanding is a challenging task. Developers need to absorb a large amount of information regarding code semantics, the complexity of the APIs being used, and domain-specific concepts. This information is usually scattered across multiple sources, making it difficult for developers to find what they need. With the success of DL techniques, NCMs have been widely used for successfully addressing various code understanding tasks such as defect detection [29, 34], clone detection [1, 31], and code search [23, 26]. Given an NCM $f(\\theta)$, parameterized by $\\theta$ and a clean dataset $X = {\\mathcal{S}, \\mathcal{Y}}$, where $s = {s_i}_{i=1}^n \\in \\mathcal{S}$, $s$ is a code snippet containing $n$ tokens,"}, {"title": "2.2 Backdoor Attacks", "content": "A backdoor attack can be defined as an attacker using hidden patterns to train a model, which produces the attacker's specified output only when a specific trigger is present in the input [6, 28]. For example, an attacker can implant a hidden trigger \u201ctesto_init\u201d in a defect detection model, causing the model to classify defect codes with the trigger as non-defect codes.\nIn the backdoor attack, the attacker aims to train an NCM $f(\\theta)$ associated with an m tokens trigger $t^* = {t_i}_{i=1}^m$ and a target label $y^* \\in \\mathcal{Y}$. Specifically, the attacker first implants the trigger to a small samples $X^*$, where $X^* = {\\mathcal{S}^*, y^*}$, $s^* = {s_i}_{i=1}^n \\oplus {t_i}_{i=1}^m \\in \\mathcal{S}^*$. $\\oplus$ denotes the trigger injection operation, which could be identifier renaming [12, 22, 32] or dead-code insertion [12, 20, 27]. Subsequently, the attacker constructs the poisoned dataset $X_p = {\\mathcal{X} \\cup \\mathcal{X}^*}$ using the triggered samples. Finally, the model will be poisoned by training with $X_p$ and minimizing the following loss function.\n$L_{X_p} (\\theta^*) = E_{(\\textbf{s},\\textbf{y}) \\sim X} \\mathcal{L} (f (\\textbf{s}; \\theta^*), \\textbf{y}) + E_{(\\textbf{s}^*,\\textbf{y}^*) \\sim X^*} \\mathcal{L} (f (\\textbf{s}^*; \\theta^*), \\textbf{y}^*)$ \n(2)\nWhere $\\mathcal{L}(,\\cdot)$ denotes the cross entropy loss. Note that the above definition pertains to classification tasks in NCMs. For another common code understanding task, the search task(e.g., code search), s can be a text sequence, such as a query, and y can be the ground-truth code. Therefore, the attacker first selects or inserts a query containing the target word as X*, then implants the trigger into the corresponding code as y*, thereby constructing the poisoned sample Xp. Then, the backdoor attack for search tasks also apply Equation 2 to train the model.\nThere are two types of trigger on backdoor attacks for code models. The first type is statement trigger backdoor where trigger is fixed or grammar dead code statement or snippet injected in code. The second type is identifier trigger where fixed or mixed tokens or words renaming the identifiers (function name/variables) in the code. Sun et al. [22] indicate that the token trigger is more stealthy than statement trigger. The statement trigger can be detect by human or static analysis tool easily. Therefore, we focus on the token trigger backdoor attacks which have the more serious threat."}, {"title": "2.3 Backdoor Defenses", "content": "Currently, backdoor defense techniques for NCMs focus on detecting inputs containing triggers during model testing [8, 12, 20]. These techniques perform outlier detection on each data sample or each word in the data to identify poisoned data and triggers. However, these technique cannot determine whether a model has a backdoor in the absence of poisoned input samples. In this paper, we consider another backdoor defense for NCMs, which is to determine the backdoor and elminate the identified backdoor without impacting the model's performance on clean inputs (i.e., clean acuracy) only given a small set of clean samples. Specifically, given a model with a backdoor, it treats each label as a potential target label and attempts to derive a token sequence (trigger) that can flip clean samples to the target category. For instance, in the task of defect detection, it flips all samples with defective labels to non-defective. For each label $y_i \\in y$, it tries to find a trigger $t_{y_i}$ to minimize the loss.\n$\\mathcal{L}_{inv} (t_{y_i}, y_i, \\theta^*) = E_{s \\sim X'} \\mathcal{L}(f(s \\oplus t_{y_i};\\theta^*), y_i)$  \n(3)\nIt is necessary to iterate over all possible labels above the Equation 3 to invert the actual trigger t* and target label y*. Since for a backdoored model, it is easier to flip samples to the ground-truth target label than to other labels [21]. Therefore, label $y_i$ can be considered as target label, where $\\mathcal{L}_{inv}(t_{y_i}, y_i, \\theta^*) \\ll \\mathcal{L}_{inv}(t_{y_j}, y_j, \\theta^*), \\forall y_j \\neq y_i \\in \\mathcal{Y}$. After determining the target label and the trigger, a standard method to eliminate the backdoor is model unlearning [28] that optimizes Equation 2 inversely as follows.\n$\\text{arg min}_{\\theta^*}[ E_{(\\textbf{s},\\textbf{y}) \\sim \\mathcal{X}} \\mathcal{L}(f (\\textbf{s}; \\theta^*), \\textbf{y}) - E_{(\\textbf{s}^*,\\textbf{y}^*) \\sim \\mathcal{X}^*} \\mathcal{L}(f(\\textbf{s}^*; \\theta^*), \\textbf{y}^*)]$ \n(4)"}, {"title": "3 THREAT MODEL", "content": "Figure 1 shows an overview of our threat model. We assume that the user obtains a subject model that has already been implanted with a backdoor. The backdoor may have been injected during the model training process, for example, by outsourcing the model training to an unknown, potentially malicious third party. Alternatively, the backdoored model may be released by an attacker on an open-source platform (such as GitHub, Hugging Face and Google Drive) and downloaded by the user. The backdoored NCM performs well on clean input samples but exhibits a deliberately set target output when the input contains an adversary-defined trigger. Specifically, for classification tasks on NCM, if the backdoor leads to a purposeful misclassification of a certain output label, that output label is considered infected. For search tasks, if the backdoor results in a high similarity score between a certain search code snippet and a query containing a specific keyword (target word), the target"}, {"title": "4 METHODOLOGY", "content": "4.1 Overview\nFigure 2 presents an overview of ELIBADCODE. Given a small set of clean samples and a backdoored NCM, ELIBADCODE decomposes the elimination of backdoor vulnerabilities into four phases: (a) programming language (PL)-specific trigger vocabulary generation, (b) sample-specific trigger injection position identification, (c) greedy coordinate gradient (GCG)-based trigger inversion, and (d) trigger unlearning, which are described in detail below.\n4.2 PL-specific Trigger Vocabulary Generation\nThe core idea of ELIBADCODE is to search for a token combination in the vocabulary space of the given backdoored model. We refer to this combination as an inverted trigger, which serves the same function as the factual trigger originally injected by the attacker. However, to enhance the model's comprehension ability and broad applicability, the model vocabulary is typically large, resulting in a vast search space for the trigger. Moreover, a trigger may consist of multiple tokens, which will cause the search space to increase exponentially. For example, the vocabulary size of the NCM CodeBERT [2] is 50,265, and if the trigger consists of n tokens, the search space would be 50, 265\", resulting in an incalculable search cost.\nTo reduce the search cost, the most direct and effective approach is to decrease the size of the model vocabulary. In fact, not all tokens can be used to form triggers. To enhance the stealthiness of backdoors based on identifier renaming, attackers typically design triggers by following the naming conventions of specific programming languages [12, 22]. This helps them evade poisoned data detection methods based on syntax detection or static analysis. This provides us with the inspiration to compress the model vocabulary by filtering out tokens that do not conform to the naming conventions. The naming convention for identifiers depends on the programming language. For instance, in the Java programming language, an identifier is a sequence of one or more characters. The first character must be a valid first character (a letter, $, or _), and each subsequent character in the sequence must be a valid non-first character (a letter, digit, $, ) [9]. Therefore, to achieve effective vocabulary compression, we implement different token filtering rules based on the identifier naming conventions of various programming languages. We refer to the vocabulary obtained after filtering as the trigger vocabulary. For example, after applying the\""}, {"title": "4.3 Sample-specific Trigger Position Identification", "content": "In trigger inversion-based backdoor defense techniques [15, 17, 21], it is common practice to transform the trigger search into an optimization problem to automate the search for the optimal inverted trigger. This optimization process requires simulating the trigger injection process, that is, injecting a randomly initialized trigger into the samples and then iteratively updating the trigger through model backpropagation. An important aspect to consider in this process is the injection position of the trigger, as it significantly affects the optimization efficiency. The model's sensitivity to changes at different positions varies across different samples. Specifically, the trigger optimization attempts to minimize the loss in Equation 3. This aligns with the objective of adversarial sample generation, which focuses on generating small perturbations in the input sample via optimization, leading to misclassification by clean models [25, 33]. Therefore, the trigger optimization is susceptible to the influence of adversarial perturbations. In other words, from the perspective of the attack target, backdoor attacks are similar to adversarial attacks in that both involve injecting certain patterns (triggers/perturbations) at specific positions in the sample to cause the model's predictions to change (i.e., incorrect predictions). However, some positions can easily produce effective adversarial perturbations, yet these perturbations may not function as effective backdoor triggers.\nTo reduce the interference of adversarial perturbations, we inject the trigger to be optimized in positions where the model is less sensitive. This is based on a key insight that backdoor attacks are more \"robust\" than adversarial perturbations. Therefore, we can inject randomly initialized triggers at any identifier position for optimization. However, the backdoored model is not sensitive to adversarial perturbation, but injecting the randomly initialized trigger at certain positions is more likely to optimize effective adversarial perturbations rather than effective backdoor triggers. Therefore, if we can identify which positions are more likely to produce adversarial perturbations, we can inject the randomly initialized trigger into positions other than these to exclude the interference of adversarial perturbations, thereby improving trigger optimization efficiency."}, {"title": "4.4 GCG-based Trigger Inversion", "content": "Algorithm 1 illustrates the GCG-based trigger inversion of ELIBADCODE in detail. In addition to the selected masked samples ($X_m$), trigger vocabulary ($\\mathcal{V}$), and a backdoored NCM ($f(\\theta^*)$) as shown in Figure 2, ELIBADCODE takes as input the labels ($\\mathcal{Y}$) and some key settings including times of iterations ($\\epsilon$), the number of candidate substitutes ($k$), times of repeat ($r$), and the threshold for trigger anchoring ($\\beta$). To eliminate backdoors in NCMS, ELIBADCODE first obtains the possible target label $y'$ from $\\mathcal{Y}$ and gets masked code snippets $S_m$ with the label $y'$ from $X_m$, then invokes the TRIGGER-INVERSION function (lines 38-40). Then, in the TRIGGERINVERSION function, ELIBADCODE first randomly initializes a trigger ($t$) with n tokens using $\\mathcal{V}$ (line 2), and then transforms $S_m$ into vector representations (also called embeddings) $e_{Sm}$ using the embedding layer of $f(\\theta^*)$ (line 3). Based on $e_{Sm}$, it further iteratively optimizes $t$ $\\epsilon$ times (lines 4-20). During each iteration, ELIBADCODE first generates the one-hot representation of $t$, denoted as $o_t$ (line 5). Second, it produces the embeddings of $o_t$ using $f(\\theta^*)$, denoted as $e_t$ (line 6). Third, it injects $e_t$ into $e_{Sm}$ to produce the embeddings of trigger-injected masked code snippets, denoted as $e'_{Sm}$ (line 7). Forth, it feeds $e'_{Sm}$ to $f(\\theta^*)$ to compute gradients for $o_t$, denoted as $G$ (line 8). Fifth, based on the top-k negative gradients of each trigger token in $G$, it selects substitutes for all trigger tokens in $t$, denoted as $T$ (line 9). Based on $T$, it generates a set of candidate triggers $T^C$ by repeating $r$ times, each time randomly replacing one token in $t$ with a random substitute in $T$ (lines 10-18). Sixth, it injects each candidate trigger into $S_m$, calculates the loss values $l$ of $f(\\theta^*)$ predicting the trigger-injected code snippets as $y'$, and selects the candidate trigger resulting in the smallest loss value as the inverted trigger (line 19). Finally, it calculates the loss value $l$ about the inverted trigger $t$ and the possible target label $y'$ and returns them (lines 21-22). After iterating over all possible target labels and producing a set of loss values and associated inverted triggers, one for each label. ELIBADCODE runs the outlier detection method [28] to obtain the ground-truth target label $y^*$ and associated inverted trigger $t$. Next, the target label and associated inverted trigger will be input into the TRIGGERANCHORING function consisting of m tokens needs to be initialized, and lines 3 - 19 in Algorithm 1 is executed similarly, focusing on w. In the meantime, the loss value calculation involving $y'$ needs to be updated to the loss value related to the query. For example, line 8 is updated to $G = \\nabla o_{w} \\mathcal{L}(f(e'_{Sm}; \\theta^*), e')$, where ow and $e'$ represent the one-hot representation of w and the embeddings of the target-injected queries, respectively.\nTrigger Anchoring. Note that, unlike continuous image data, code written in programming languages is similar to natural language and is discrete. Existing research [17] in NLP has demonstrated that for discrete inputs, there is currently no simple method to differentiably determine the size/length of the injected trigger. Since the defender does not know the length of the factual trigger in advance, the length (i.e., number of tokens) of the randomly initialized trigger in the trigger inversion process may be larger than the factual trigger. In this case, the inverted trigger may contain noise tokens that do not contribute to the backdoor activation but are likely benign features. Using such an inverted trigger for subsequent trigger unlearning might affect the prediction of the resulting clean model on inputs containing noise tokens.\nTo address this issue, ELIBADCODE designs a trigger anchoring method that filters out noise tokens in the inverted trigger, retaining only the effective components. Specifically, as shown in lines 25 - 35 of Algorithm 1, ELIBADCODE iteratively removes one trigger token at a time, and the remaining tokens form the filtered trigger. The filtered trigger is then injected into the masked code snippets. Subsequently, it calculates the loss value of the backdoored model predicting the code snippets injected with the filtered trigger and original inverted trigger as the target label, respectively (lines 27 and 29). If the removal of a trigger token causes the loss value to change by more than a given threshold $\\beta$, ELIBADCODE identifies it as an effective trigger component and adds it to the anchored trigger (lines 30-32). The threshold $\\beta$ is an empirical value. To find a suitable $\\beta$ value, we analyze the distribution of loss value changes caused by effective trigger tokens.\n\""}, {"title": "5 EVALUATION", "content": "We conduct a series of experiments to answer the following research questions (RQs)", "18": ".", "24": "Devign [34", "7": "to evaluate ELIBADCODE on three types of code understanding tasks: clone detection, defect detection, and code search, respectively. Three different model architectures are adopted for the evaluation, CodeBERT [2", "30": "and UniXcoder [4", "12": "and BadCode [22", "testo_init\" as a trigger to replace the function name of the code snippet to poison the training data. BadCode utilizes \"rb\" as a trigger and appends it to the function name/variable name of the code snippet to produce the poisoned training data. For the defect detection task and clone detection task, we select non-defective and non-clone as the target labels, respectively. For the code search task, we follow BadCode and choose \"file\" as the target word, implanting the trigger into the code snippets matched by queries containing the target word. We follow Li et al. [12": "and poison 2% of the training data for different code understanding tasks. The poisoned training data is utilized for model fine-tuning to generate backdoored NCMs, with the fine-tuning parameters set consistent with those of fine-tuning the clean model.\nDefense Setting. For trigger inversion (including the phases (b), and (c) in Figure 2), we use 30 samples per class in the defect detection task and clone detection task, and 30 samples in the code search task (details on the effectiveness of different numbers of clean samples can be found in Section 5.3). Considering that attackers prioritize the stealthiness of the backdoor, they typically do not set a long trigger for renaming backdoor attacks. Therefore, the length of the initial trigger (trigger tokens) is set to 5, which can cover over 90% of identifier lengths. Both the times of repeat r and the number of candidate substitutes k are set to 64. In trigger unlearning, we fine-tune the backdoored models to unlearn the backdoors. We use all clean samples (i.e., 10% of the training data) and select 20% of them to inject the inverted trigger and mark with the correct labels"}]}