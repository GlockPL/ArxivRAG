{"title": "IS PONTRYAGIN'S MAXIMUM PRINCIPLE ALL YOU NEED? SOLVING OPTIMAL CONTROL PROBLEMS WITH PMP-INSPIRED NEURAL NETWORKS", "authors": ["Kawisorn Kamtue", "Jos\u00e9 M. F. Moura", "Orathai Sangpetch"], "abstract": "Calculus of Variations is the mathematics of functional optimization, i.e., when the solutions are functions over a time interval. This is particularly important when the time interval is unknown like in minimum-time control problems, so that forward in time solutions are not possible. Calculus of Variations offers a robust framework for learning optimal control and inference. How can this framework be leveraged to design neural networks to solve challenges in control and inference? We propose the Pontryagin's Maximum Principle Neural Network (PMP-net) that is tailored to estimate control and inference solutions, in accordance with the necessary conditions outlined by Pontryagin's Maximum Principle. We assess PMP-net on two classic optimal control and inference problems: optimal linear filtering and minimum-time control. Our findings indicate that PMP-net can be effectively trained in an unsupervised manner to solve these problems without the need for ground-truth data, successfully deriving the classical \"Kalman filter\" and \"bang-bang\" control solution. This establishes a new approach for addressing general, possibly yet unsolved, optimal control problems.", "sections": [{"title": "1 INTRODUCTION", "content": "Standard neural networks excel at learning from labeled data, but often lack inherent knowledge of physical principles. In many engineering and scientific applications, there is a wealth of accumulated knowledge and practices that could inform the architecture of learning models. In addition, data in these fields are often scarce, difficult, or expensive to obtain. For instance, telecommunications, data processing, automation, robotics, and control problems frequently have little or no labeled data, making traditional supervised learning methods challenging to apply.\nThis paper presents a method for designing deep models from first principles by incorporating prior knowledge, specifically existing design principles that have been successful in various engineering, scientific, and technology practices. It focuses on two design problems of broad practical interest: determining the optimal linear estimator and solving the optimal minimum time control problem. The solution to the first is the well-known Kalman filter, while the solution to the second is the bang-bang control, also known as on-off control.\nOptimizing over functions, i.e., when the optimizing variable is a set of whole functions over an interval, falls under the realm of the Calculus of Variations, and a principled methodology can be based on Pontryagin's maximum principle (PMP). PMP offers valuable prior knowledge about the necessary conditions for optimal solutions and often provides sufficient conditions, making the"}, {"title": "2 THEORY", "content": "optimal solution unique in many cases. This motivates the integration of PMP into machine learning training methodologies.\nRelated work has focused on integrating physics priors to learn physical systems. One type of approach involves designing network architectures that enforce physical constraints (Chen et al., 2018; Ben-Yishai et al., 1995; Lu et al., 2021b). Another approach uses physical dynamics as soft constraints in the empirical loss function, applying standard machine learning techniques to train the model. Physics-informed neural networks (PINNs) (Raissi et al., 2019) and Hamiltonian neural networks (HNNs) (Greydanus et al., 2019) have shown the ability to learn and predict physical quan- tities while adhering to physical laws, demonstrating promise in solving both forward and inverse differential equation problems. While these works differ in scope and implementation, they provide valuable motivation for incorporating prior knowledge into the design of learning models.\nIn this paper, we draw inspiration from the Calculus of Variations\u2014a field focused on finding the maxima and minima of functionals through variations to design a neural network based on basic principles, which we call the \u201cPontryagin's Maximum Principle Neural Network\u201d (PMP-net). This network is designed to solve optimization problems like Kalman filtering and those arising in control contexts. We start by formulating a variational approach to these problems, using the calculus of variations to derive the necessary conditions for optimization by applying Pontryagin's Maximum Principle. Although mathematicians and engineers typically solve these conditions analytically or numerically, such methods can be challenging when dealing with nonlinear, second-order differen- tial equations with complex boundary conditions. Instead, we propose using a neural network to learn the optimal solution from PMP's necessary conditions.\nStandard network architectures and optimization methods for minimizing differential equation resid- uals can face issues, especially with a naive approach, as noted by others (Krishnapriyan et al., 2021). One major challenge is the non-smooth nature of the loss landscape, which complicates optimiza- tion for neural networks. Unlike learning a physical system, which involves a single differential equation, the variational problems we address involve additional differential equations for Lagrange multipliers and functions like the Kalman gain or control functions.\nAdditionally, in minimum-time problems such as the bang-bang control problem, there are two key challenges. First, because the terminal time $t_f$ is to be optimized, the optimization is over a functional space, meaning the optimal solutions are functions over the entire interval [0, $t_f$], where $t_f$ itself is unknown. Second, the extra constraints, such as functions being bounded or living in a compact set, restrict the control functions to be learned. The optimal solution in these cases is often discontinuous, resembling a step function, and may be undefined in certain regions. These complexities frequently result in vanishing or exploding gradients during neural network training.\nTo tackle these challenges, we introduce a PMP-based architecture that incorporates prior knowledge from Pontryagin's maximum principle into our loss function, ensuring effective training to derive the optimal solution.\nThis paper presents a method to integrate prior knowledge from Calculus of Variations to functional optimization and classical control into the architectural design of deep models. We incorporate dynamical constraints, control constraints, and conditions derived from PMP into the loss function for training neural networks, enabling unsupervised learning. Our contributions are as follows.\nMain contributions:\n\u2022 Incorporate Pontryagin's Maximum Principle as soft constraints in ML training methodol- ogy.\n\u2022 Account in the design for dynamical constraints and other constraints on the variables and functions of interest\n\u2022 Engineer a novel neural network architecture, PMP-net, that mimics the design of feedback controllers used in optimal control.\n\u2022 Propose learning paradigms that effectively train PMP-net to derive the optimal solution.\n\u2022 Show that our PMP-net replicates the design of the Kalman filter and the bang-bang control without using labeled data."}, {"title": "2.1 OPTIMAL CONTROL PROBLEM", "content": "We illustrate our approach in the context of a control problem. Given an initial value problem, specified by a dynamical system and its initial condition\n$\\dot{x}(t) = f(x(t), u(t))$\n$x(0) = x_0$\n(1)\nwhere x is the state, u is the control, and f is a known function representing the dynamics. Op- timal control problems involve finding for example a control function $u^*$ such that the trajectory $(x^*(t), u^*(t))_{t\\in[0,t_f]}$ minimizes some performance measure J(x, u) of the form\n$J(x, u) = q_r(x(t_f),t_f) + \\int_0^{t_f} g(x(t), u(t))dt$\n(2)\nwhere $q_r$ is the terminal cost, g is the running cost, and $t_f$ is the terminal time. The terminal state $x(t_f)$ and time $t_f$ can be free or specified. Note that not all pairs of functions (x, u) are admissible trajectories since trajectories must satisfy a dynamical constraint $\\dot{x}(t) = f(x(t), u(t))$. The optimal control problem is therefore the constrained optimization\n$\\min_{x,u} J(x, u)$\ns.t. $\\dot{x}(t) = f(x(t), u(t))$\n(3)\nIn equation 3, the optimization variables are functions, say {u(t), t \u2208 [0, $t_f$]}, where $t_f$ may be fixed or to be optimized itself (in minimum time problem).\nThere are two primary methods for solving the constrained optimization problem given by equa- tion 3. The first approach is the elimination method: we first derive x in terms of u and solve the one variable optimization problem. This method often involves integration that can be hard to solve analytically. Neural ODE (Chen et al., 2018) explored using numerical integration, where the parameters of the networks are learned with the help of ODE solver. The second approach avoids integration by introducing the Lagrangian L,\n$L(x, u, \\dot{x}) = q_r(x(t_f), t_f) + \\int_0^{t_f} g(x(t), u(t)) + \\lambda^T (f(x, u) - \\dot{x}) dt$\n(4)\nFor all admissible trajectories (x, u), i.e. (x, u) satisfying the dynamic constraint $\\dot{x} = f(x, u)$, we have $L(x, u, \\dot{x}) = J(x, u)$. Therefore, the admissible optimal solution for equation 4 is also the optimal solution for equation 3.\nCalculus of variations enables us to identify the optimal functions (x, u, \u03bb) that minimize L. By examining variations, we can derive the necessary conditions\u2014known as Pontryagin's maximum principle (PMP)\u2014at the optimal solution ($x^*, u^*, \\lambda^*$) for equation 4.\n$\\dot{x}^* = f(x^*, u^*)$\n$\\dot{\\lambda}^* = - \\frac{\\partial H}{\\partial x}$\n$\\frac{\\partial H}{\\partial u} = 0$\n(5)\n$[H(x^*(t_f), u^*, \\lambda^*,t_f) + \\frac{\\partial q_r}{\\partial t}|^*_{\\lambda^*,t^*_f}] + [-\\frac{\\partial q_r}{\\partial x} |^*_{\\lambda^*,t^*_f} - \\lambda^*(t_f)] \\delta x_f = 0$\nwhere \u03bb(t) represents the (function vector) Lagrange multiplier (also known as the costate), and H denotes the scalar function called the \u201cHamiltonian,\" defined as H(x, u, \u03bb, t) = g(x(t), u(t)) + $\\lambda^T f(x(t), u(t))$. The variations $\u03b4x_f$ and $\u03b4t_f$ are essential in learning the optimal control when the terminal state or terminal time are unknown and to be optimized; they provide us the necessary boundary conditions. The system of partial differential equation 5 is generally nonlinear, time- varying, second-order, and hard-to-solve. Numerical methods also pose challenges due to the split boundary conditions-neither the initial values (x(0), \u03bb(0)) nor the final values (\u03bb($t_f$), \u03bb($t_f$)) are fully known."}, {"title": "2.2 PONTRYAGIN'S MAXIMUM PRINCIPLE NETWORK", "content": "Instead of solving equation 5 analytically or numerically, we propose leveraging neural net- works' well-known capability as universal function approximators (Cybenko, 1989) to learn {x(t), u(t), \u03bb(t), t \u2208 [0, T]} that satisfy PMP. In the training stage, rather than directly matching the PMP-net's outputs to ground truth data {x(t)*, u(t)*, \u03bb(t)*, t \u2208 [0, T]}, our PMP-net learns to predict solutions that adhere to the PMP constraints. Because this process incorporates a solution methodology, the PMP, we interpret it as bringing to the neural networks \u201cprior knowledge.\u201d Our ap- proach introduces an inductive bias into the PMP-net, allowing it to learn the optimal solution in an unsupervised manner. By simultaneously predicting both the state and control, PMP-net eliminates the need for integration and can address optimal control problems with unknown terminal time.\nDuring the forward pass, PMP-net takes time as input and predicts the state x(t), the control u(t), and the costate \u03bb(t). The Hamiltonian H is then calculated based on these predictions. By leveraging the automatic differentiation capabilities of neural networks (Baydin et al., 2017), we can efficiently compute the derivatives and partial derivatives present in equation 5 by computing in-graph gradi- ents of the relevant output nodes with respect to their corresponding inputs. We then calculate the residuals of the differential equations in PMP and incorporate them into the loss function, along with L2 loss between the predicted and target states at the boundary conditions. In our following experi- ments in Sections 3 and 4, we also incorporate additional architectural features into our PMP-net to enforce hard constraints and to allow PMP-net to learn when terminal time is unknown."}, {"title": "3 DESIGNING OPTIMAL LINEAR FILTER", "content": "In this section, our goal is to design a linear filter that provides the best estimate of the current state based on noisy observations. The optimal solution is known as \"Kalman Filtering\" (Kalman & Bucy, 1961), which is one of the most practical and computationally efficient methods for solving estimation, tracking, and prediction problems. The Kalman filter has been widely applied in var- ious fields from satellite data assimilation in physical oceanography, to econometric studies, or to aerospace-related challenges (Leonard et al., 1985; Auger et al., 2013)."}, {"title": "3.1 KALMAN FILTER", "content": "Reference Athans & Tse (1967) formulated a variational approach to derive the Kalman filter as an optimal control problem. We consider the dynamical system\n$\\dot{x}(t) = Ax(t) + Bw(t), 0 \\le t \\le t_f, w_{t-1} \\sim N(0, Q)$\n$y(t) = Cx(t) + v(t), v_{t-1} \\sim N(0, R)$\n$x(0) \\sim N(x_0, \\Sigma_0)$\n(6)\nwhere $x(t) \\in R^n$ is the state, $y(t) \\in R^m$ is the observation. $A \\in R^{n\\times n}$ is the state transition matrix, $B\\in R^{n\\times r}$ is the input matrix and $C \\in R^{n\\times r}$ is the measurement matrix. The white Gaussian noise w(t) (respectively v(t)) is the process (respectively measurement) with covariance Q (resp. R) noise. We assume that x(0), w(t), v(t) are independent of each other. Kalman designed a recursive filter that estimate the state by\n$\\dot{\\hat{x}}(t) = A\\hat{x}(t) + G(t) [Cy(t) \u2013 A\\hat{x}(t)]$\n$\\hat{x}(0) = x_0$\n(7)\nwhere G(t) is the Kalman gain to be determined. Given the state estimation $\\hat{x}(t)$ at time t, the error covariance defined as\n$\\Sigma(t) = E [(x \u2013 \\hat{x})(x \u2013 \\hat{x})^T]$\nhas the following dynamics\n$\\dot{\\Sigma}(t) = [A \u2013 G(t)C] \\Sigma(t) + \\Sigma(t) [A \u2013 G(t)C]^T$\n$+ BQB^T + G(t)RG(t)^T$\n$\\Sigma(0) = \\Sigma_0$\n(8)"}, {"title": "3.2 LEARNING KALMAN FILTER WITH PMP-NET", "content": "Architecture: The PMP-Net architecture is shown in Figure 1. Since \u2211 is both symmetric and positive semi-definite, we embed this inductive bias into our neural network architecture. Specifi- cally, the state estimator outputs an intermediate matrix P and estimates the error covariance \u2211 as \u03a3 = $P^TP$, ensuring symmetry and positive semi-definiteness. The state, costate, and control esti- mators are modeled by a 6-layer feedforward neural networks with hyperbolic tangent activation.\nTraining: We adopt the curriculum training, as optimizing loss with multiple soft constraints can be challenging (Krishnapriyan et al., 2021). We set the loss function to be\n$Loss_e = Loss_{BC} + Loss_{PDE}$\nwhere\n$LOSS_{BC} = ||\\Sigma(0) \u2013 \\Sigma_0||_2 + ||\\lambda(T) - I_n ||_2$\n$LOSS_{PDE} = \\frac{1}{N} \\Sigma_{i=0}^N|\\dot{x(t_i)} \u2013 f(x(t_i), u(t_i))||_2 + ||\\dot{\\lambda(t_i)} + H(x, u, \\lambda, t_i)||_2 + |\\frac{\\partial H}{\\partial u} |_{t_i} | |_2$\nDuring each epoch, 5000 points are uniformly sampled from time [0, T]. After every 5000 epochs, we increment the value of a by a factor of 1.04. All neural networks are initialized with Glorot uniform initialization (Glorot & Bengio, 2010). We train PMP-net using stochastic gradient descent with the initial learning rate 8 \u00d7 10-4.\nEvaluation: For a fair evaluation, we take the estimated control from PMP-Net and use the fourth- order Runge-Kutta integrator (Runge, 1895) in scipy.integrate.solve_ivp to derive the trajec- tory of the state. This is necessary because the estimated state by PMP-Net might not adhere to the dynamics constraints, making it into an implausible trajectory."}, {"title": "3.3 RESULTS", "content": "For our experiment, we set\n$A=\\begin{bmatrix}0 & I_2\\\\0 & 0\\end{bmatrix}\\in\\mathbb{R}^{4x4}, B =\\begin{bmatrix}0\\\\I_2\\end{bmatrix}\\in\\mathbb{R}^{4\\times 2}, C = I_4, Q = 0.5I_2, R =\\begin{bmatrix}4.0 & 1.5 & 0 & 0\\\\1.5 & 4.0 & 0 & 0\\\\0 & 0 & 2.0 & 1.0\\\\0 & 0 & 1.0 & 2.0\\end{bmatrix}, T = 5.0$\nThis dynamics system models a kinematics system where the state x corresponds to position and velocity and the control u corresponds to the force applied to the state. With these experimental settings, Kalman filtering reaches a steady state where $\u03a3^*$ converges (hence, the Kalman gain con- verges to $G^*$). We compare our method against traditional supervised learning method (baseline)"}, {"title": "4 LEARNING OPTIMAL CONTROL FOR A MINIMUM TIME PROBLEM", "content": "In this section, we seek an optimal control strategy that drives a state from an arbitrary initial posi- tion to a specified terminal position in the shortest possible time. In practice, the control is subject to constraints, such as maximum output levels. The optimal control strategy for the minimum time problem is commonly known as \u201cbang-bang\u201d control. Examples of bang-bang control applications include guiding a rocket to the moon in the shortest time possible while adhering to acceleration con- straints (Athans & Falb, 1996); maintaining desired temperatures industrial furnaces (Burns et al., 1991); decision-making in stock options, where control switches between buying and selling based on market conditions (Kamien & Schwartz, 1991)."}, {"title": "4.1 MINIMUM TIME PROBLEM", "content": "We illustrate the PMP-net with the following problem. Consider the kinematics system\n$\\begin{bmatrix}\\dot{x_1}(t)\\\\ \\dot{x_2}(t)\\end{bmatrix} = \\begin{bmatrix}0 & 1\\\\0 & 0\\end{bmatrix}\\begin{bmatrix}x_1(t)\\\\ x_2(t)\\end{bmatrix} + \\begin{bmatrix}0\\\\1\\end{bmatrix}u$\nwhere $x_1, x_2, u$ correspond to the position, velocity and acceleration. The goal is to drive the system from the initial state ($x_1(0), x_2(0)$) = ($p_o, v_o$) to a final destination ($p_f, v_f$) where $x(t) \\in R^n$ is the state at time t, u(t) \u2208 $R^m$ is the control at time t, A \u2208 $R^{n\\times n}$ is the state transition matrix and B \u2208 $R^{n\\times m}$ is the input matrix. We are interested in finding the optimal control {$u^*(t), t\u2208 [0, $t^*$]$\\} that drives the state from $x_0$ to $x_f$ in a minimum time $t^*$. The performance measure can be written as\n$J(x,u) = \\int_0^{t_f} 1dt$\n(10)\nwhere $t_f$ is the time in which the sequence (x, u) reaches the terminal state. Note that here $t_f$ is a function of (x, u) since the time to reach the target state depends on the state and control. In practice, the control components may be constrained by requirements such as a maximum acceleration or maximum thrust\n$|u_i(t) |\\leq 1, i\\in[1,m] t\\in[t_o, t_f]$\n(11)"}, {"title": "4.2 LEARNING BANG-BANG CONTROL WITH PMP-NET", "content": "Architecture: Our PMP-Net for the minimum time problem is depicted in Figure 3. This design is inspired by classical feedback control, where the control variable u is a function of both the state and the costate. In our approach, the state estimator, costate estimator, and control estimator are modeled by 6-layer feedforward networks. It is important to note that all equations and functions in equation 12 are only valid for t \u2208 [0,1]. However, since the final time $t^\u2020$ is unknown, we design our architecture to extend the applicability of equation 12 beyond time $t^\u2020$. To achieve this, we multiply the output of the control estimator by 1\ud835\udc65\u2260\ud835\udc65\ud835\udc53, which simulates a controller that sets the control to zero once the state reaches the target destination. Because the indicator function is not differentiable, we approximate it using a scaled sigmoid function \u03c3(\ud835\udc3f||\ud835\udc65 \u2212 \ud835\udc65\ud835\udc53||).\nLastly, instead of defining the terminal time to be the time the state reaches the target, we introduce an additional intrinsic parameter $t_f$ subject to the constraint \ud835\udc65($t_f$) = \ud835\udc65\ud835\udc53. This difference is subtle, yet it allows PMP-Net to learn tf during backpropagation."}, {"title": "4.3 RESULTS", "content": "For our experiment, we set $x_o = \\begin{bmatrix}1\\\\ 0\\end{bmatrix}, x_f = \\begin{bmatrix}3\\\\0\\end{bmatrix}, T = 3.0$. The optimal control is to apply the acceleration -1 from time [0, 1] and acceleration +1 from time ]1, 2] that will drive the state from the initial state $x_0$ to the target state $x_f$ in minimum time $t^* = 2$ seconds. The control switches from -1 to +1 at the switching time at $t = 1$ where \u03bb2(\ud835\udc61) = 0 as shown in Fig 4b"}, {"title": "5 RELATED WORK", "content": "Physics priors for neural network: Numerous prior works have aimed to integrate physics-based priors into neural networks for supervised learning tasks. Physics-informed Neural Networks learn supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations Raissi et al. (2019); Lu et al. (2021a). Hamiltonian Neural Networks (Greydanus et al., 2019) and Lagrangian Neural Networks (Lutter et al., 2019; Cranmer et al., 2020) apply physics laws and parameterize and learn Hamiltonian and Lagrangian that match observations of the systems. All of these approaches use labeled data to learn physical parameters of the dynamics system. Our approach, in contrast, does not require any labeled data to learn the optimal control function.\nLearning optimal control using neural networks: B\u00f6ttcher et al. (2022) enforces the dynam- ics constraint in neural networks using neural ODEs (Chen et al., 2018) to learn optimal control. However, this ODE-based approach does not take advantage of other optimality conditions in PMP. ODE-based methods solve the forward problem by integrating the state up to the terminal time, cal- culating and minimizing the loss function, but they are not applicable when the terminal time $t_f$ is unknown and needs to be optimized, or when the terminal state is specified. In contrast, our work leverages the calculus of variations and PMP, enabling us to account for variations in both terminal time and terminal state, and simultaneously learn the optimal control and minimum time under these conditions."}, {"title": "6 CONCLUSION", "content": "We present a novel paradigm that integrates Pontryagin's maximum principle into neural networks for learning the solutions to functional optimization problems arising in many engineering and tech- nology and scientific problems. Our PMP-Net is unsupervised, generalizable and can be applied to any optimal control problem where Pontryagin's maximum principle is applicable. We illustrate the PMP-net strategy with two classical problems of great applied significance and show that it success- fully recovers the Kalman filter and bang-bang control. By leveraging the Calculus of Variations, we can analyze variations in terminal time, and PMP-Net successfully optimizes this variable in the minimum time problem\u2014something forward methods like ODE-based approaches cannot achieve. Although these solutions have been derived analytically in the past, our work paves the way for applying PMP-based neural networks to more complex, higher-dimensional, and analytically in- tractable control problems."}, {"title": "A CALCULUS OF VARIATION AND PONTRYAGIN'S MAXIMUM PRINCIPLE", "content": "Suppose we want to find the control u* that causes the system\n$\\dot{x} = f(x, u)$\n(13)\nto follow an admissible trajectory x* that minimizes the performance measure\n$J(x, u) = q_r(x(t_f),t_f) + \\int_0^{t_f} g(x(t), u(t), t)dt$\nAssuming that $q_r$ is twice differentiable,\n$J(x, u) = q_r(x(0),0) + \\int_0^{t_f} \\frac{dq^T}{dt}(x, t) + g(x(t), u(t), t)dt$\nWe introduce the Lagrange multipliers \u03bb, also known as costates. The primary function of \u03bb is to enable us to make perturbations (\u03b4\u03b1, \u03b4u) to an admissible trajectory (x, u) while ensuring the dynamics constraints in equation 13 remain satisfied. Suppose we have an admissible trajectory (x, u, \u03bb) such that reaches the terminal state $x_f$ at time $t_f$, the lagrangian L is defined as\n$L(x, u, \\lambda, x_f, t_f) = q_r(x_0,0) + \\int_0^{t_f} \\frac{dq^T}{dt}(x, t) + g(x, u) + \\lambda^T (f(x, u) \u2013 \\dot{x})dt$\n$= q_r(x_0,0) + \\int_0^{t_f} (\\frac{dq^T}{dt}(x(t), t) + g(x, u) + \\lambda^T f(x, u) \u2013 \\lambda^T\\dot{x})dt$\n$= q_r(x_0,0) + \\int_0^{t_f} (\\frac{dq^T}{dt}(x(t), t) + H(x, u, \\lambda, t) \u2013 \\lambda^T\\dot{x})dt$\nwhere the Hamiltonian H = g(x, u) + $\\lambda^T f(x, u)$. The calculus of variations studies how making a small pertubation to (x, u, \u03bb) changes the performance. Suppose the new trajectory (x + \u03b4x, u + \u03b4u, \u03bb + \u03b4\u03bb) reaches new terminal state ($x_f$ + \u03b4$x_f$, $t_f$ + \u03b4$t_f$). The change in performance is\n$\u0394L = L(x + \u03b4x, u + \u03b4u, \u03bb + \u03b4\u03bb, $x_f$ + \u03b4$x_f$, $t_f$ + \u03b4$t_f$) \u2013 L(x, u, \u03bb, $x_f$,$t_f$)\n$= \\int_0^{t_f} \\frac{\\partial H}{\\partial x}(x,t)\\delta x + \\frac{\\partial H}{\\partial u}\\delta u + \\frac{\\partial H}{\\partial \\lambda}\\delta \\lambda - \\dot{x}^T\\delta \\lambda \\  dt$\n$+\\frac{\\partial H}{\\partial x}\\delta x + \\frac{\\partial H}{\\partial u}\\delta u + [f(x,u)-x]\\delta \u03bb \\delta t_f + O( \\delta x,\\delta u,\\delta \u03bb,\\delta t_f )$\n$= \\int_0^{t_f} \\frac{\u03b4dq^T}{dt} (x(t), t) + H(x, u, \u03bb, t) - \u03bb(t) x(t) \\delta t_f + O( \\delta x,\\delta u,\\delta \u03bb,\\delta t_f )$\n$= \\int_0^{t_f} + \\frac{\\partial H}{\\partial x}\\delta x + \\frac{\\partial H}{\\partial u}\\delta u + [f(x,u)-x]\\delta \u03bb \\delta t_f + O( \\delta x,\\delta u,\\delta \u03bb,\\delta t_f )$\n$= \\int_0^{t_f} + \\frac{\\partial H}{\\partial x}\\delta x + \\frac{\\partial H}{\\partial u}\\delta u + [f(x,u)-x]\\delta \u03bb \\delta t_f + O( \\delta x,\\delta u,\\delta \u03bb,\\delta t_f )$"}, {"title": "B KALMAN FILTERING DERIVATION", "content": "The performance measure for designing optimal linear filter is\n$J(\\Sigma, G) = q_\u03c4 (\\Sigma(T), T)$\n$= tr(\\Sigma(T))$\nSince the terminal time $t_f$ = T is specified and terminal state is free, equation 18 applies. Pontrya- gin's maximum principle yields\n$\\dot{\\Sigma}^* = [A \u2013 G^*C]\\Sigma + \\Sigma[A-G^*C]^T + BQBT + G^*RG^{*T}$\n$Str(\\frac{\u0434H(\u03a3^\u2217,G^\u2217)}{\u0434\u03a3}+\u03bb^\u2217) =0$\n$\\frac{\u0434H(\u03a3^\u2217,G^\u2217)}{\u0434G} =0$\n$\u03bb^*(T)^T = I_n$\n$\u03a3^*(0) = \u03a3_0$\n(19a)\n(19b)\n(19c)\n(19d)\n(19e)\nSimplifying equation 19b yields,\n$\u03bb^* = -\u03bb^* [A \u2013 G^*C] - [A-G^*C]^\u03bb^*$\n(20)"}, {"title": "C BANG-BANG CONTROL DERIVATION", "content": "Since the terminal state $x_f$ is specified and terminal time is free, equation 17 applies. Pontryagin's maximum principle yields\n$\\dot{x} =\\begin{bmatrix}\\dot{x1}\\\\ \\dot{x2}\\end{bmatrix} =\\begin{bmatrix}0 & 1\\\\0 & 0\\end{bmatrix}$\\\n$\\dot{\u03bb} =\\begin{bmatrix}\u03bb\u2081\\\\ \u03bb\u2082\\end{bmatrix} =\\begin{bmatrix}0 & 1\\\\0 & 0\\end{bmatrix}$\\\n$u^*= arg min(1 + \u03bb\u2081x\u2081 + \u03bb\u2082x\u2082 + \u03bb\u2082u)$\n(24a)\n(24b)\n(24c)\n1 + \u03bb\u2081x\u2081 + \u03bb\u2082x\u2082 + \u03bb\u2082u*(t) = 0$\n$\\begin{bmatrix}x(0)\\\\x(0)\\\\x\\end{bmatrix}=\\begin{bmatrix}x_0\\\\v_0\\\\x_f\\end{bmatrix}=\\begin{bmatrix}x(t)\\\\x(t)\\\\x\\end{bmatrix}$$\\u2063$\n(24d)\n(24e)\n$\\[f]\\[x(t) \\end{bmatrix}=1/2\\cdot a(t)=0$$\n(24f)\nThe equation 24c yield\n\u2200u, 1 + \u03bb\u2081x\u2081 + \u03bb\u2082x\u2082 + \u03bb\u2082u < 1 + \u03bb\u2081x\u2081 + \u03bb\u2082x\u2082\nu =\\begin{cases}u=\u2212sign(\u03bb\u2082)& \u03bb\u2082\u22600 \nu=\nindeterminate & if\\\u03bb\u2082 =0\nAssuming that \u03bb\u2082 is not a zero function, the equation 24b yields\n\u03bb\u2081(t)=\\dot \u03bb\u2082 =C\u2081\\\u03bb\u2082(t) =-c\u2081t+c\u2082\n(25)\nwhere C1, C2 are constants to be determined. We see from equation 25 that \u03bb\u2082 changes sign at most once. There are two possible cases:\n1. \u03bb\u2082 sign remains constant in [0, t\u2217]\n2. \u03bb\u2082 changes sign in [0, t\u2217]\nFor case 1, we have the general form of\nx\u2082(t) = v\u2080 + at &for t \u2208 [0,t\u2217]\\x\u2081(t) = p\u2080 + v\u2080t +1/2 at\u00b2&for t \u2208 [0,t\u2217]\n(26)"}]}