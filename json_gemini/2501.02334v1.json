{"title": "VALIDITY ARGUMENTS FOR CONSTRUCTED RESPONSE SCORING\nUSING GENERATIVE ARTIFICIAL INTELLIGENCE APPLICATIONS", "authors": ["Jodi M. Casabianca", "Daniel F. McCaffrey", "Matthew S. Johnson", "Naim Alper", "Vladimir Zubenko"], "abstract": "The rapid advancements in large language models and generative artificial intelligence\n(AI) capabilities are making their broad application in the high-stakes testing context\nmore likely. Use of generative AI in the scoring of constructed responses is particularly\nappealing because it reduces the effort required for handcrafting features in traditional\nAI scoring and might even outperform those methods. The purpose of this paper is to\nhighlight the differences in the feature-based and generative AI applications in\nconstructed response scoring systems and propose a set of best practices for the\ncollection of validity evidence to support the use and interpretation of constructed\nresponse scores from scoring systems using generative AI. We compare the validity\nevidence needed in scoring systems using human ratings, feature-based natural\nlanguage processing AI scoring engines, and generative AI. The evidence needed in\nthe generative AI context is more extensive than in the feature-based NLP scoring\ncontext because of the lack of transparency and other concerns unique to generative\nAl such as consistency. Constructed response score data from standardized tests\ndemonstrate the collection of validity evidence for different types of scoring systems\nand highlights the numerous complexities and considerations when making a validity\nargument for these scores. In addition, we discuss how the evaluation of AI scores\nmight include a consideration of how a contributory scoring approach combining\nmultiple Al scores (from different sources) will cover more of the construct in the\nabsence of human ratings.", "sections": [{"title": "Introduction", "content": "Natural language processing (NLP) solutions for automatically scoring constructed\nresponses (CR) are well established and used broadly in standardized testing for written, spoken,\nand short answer responses. In many applications, a set of features are selected that are intended\nto represent the construct as defined by the scoring rubric and combined to predict human ratings.\nThese features are handcrafted and trained by NLP scientists to be extracted from the response and\nthen used in a model to generate an overall score for the response. Some new artificial intelligence\n(AI) solutions, specifically approaches using generative AI such as GPT4, are not engineered to\nproduce features based on the same principles of NLP to match the scoring rubric and construct.\nInstead, generative AI approaches involve prompting an underlying large language model (LLM)\nto produce outputs such as a rating, sometimes with relatively little training or fine-tuning of the\nLLM to the specific task of scoring. It is difficult to explain how these generative AI approaches\nobtained their outputs since the LLM has billions of parameters, but they do offer capabilities that\nwere not previously available without extensive effort by experts.\nWhile the future of assessment will undoubtedly make extensive use of AI and given the\naccessibility of LLMs, it is important to place guardrails around their use in educational assessment\nso that they are used responsibly. Discussions on ethical AI and standards for using AI in education\nare prevalent in the literature (Bulut et al., 2024; International Test Commission & Association of\nTest Publishers, 2022; Johnson, 2024). This paper describes a study exploring the utility of GPT4\nin the context of CR scoring to highlight how validity arguments might be specially structured\nwhen using generative AI. The following sections summarize generative AI and discuss\nestablished validity frameworks for evaluating the human ratings and automated scores from\ntraditional CR scoring systems. We then introduce an additional set of validity evidence that should\nbe collected and documented when using generative AI in CR scoring systems. We demonstrate\nthe curation of validity evidence using empirical data from three standardized tests historically\nscored with humans and automated scoring engines. We conclude with suggestions for\npractitioners."}, {"title": "The Basics of Generative AI", "content": "The traditional use of AI in CR scoring relied on experts such as NLP scientists and\nlinguists to create hand-crafted features quantifying different components of written text or spoken\nresponses (Shermis & Burstein, 2013). The NLP scientists' expertise in creating and purposefully\ncombining features to provide construct coverage and alignment with the scoring rubric ensured a\nhuman-in-the-loop approach to AI scoring. The statistical models once used to combine features\nwere simple, for instance, multiple linear regression.\nThis traditional use is in stark contrast to generative AI, which is a form of AI in which a\nLLM or another type of model (e.g., generative adversarial networks [GANs], recurrent neural\nnetworks [RNNs]) produces or generates responses to prompts. For example, a prompt could\nrequest the LLM provide a score to written responses according to a rubric yielding an AI scoring\nsystem with minimal human inputs. The LLMs are based on statistical patterns extracted from\nextremely large datasets which might be produced from a combination of NLP corpora, including\nannotated texts, and extracts of text from the internet. The most popular LLMs use millions or\nbillions of parameters, which is why some label LLMs as \u201cblack-box\u201d algorithms. Compared to\nsmaller language models, LLMs can typically perform more tasks with better results because the\nvery large number of parameters and training on massive data give the model exceptional"}, {"title": null, "content": "flexibility to solve even complex tasks. Often the end user of the LLM knows little about the\ntraining data or the inner workings of the LLM at all because they did not build the LLM. While\nthe end user may use the LLM to score essays or spoken responses, they might not know how or\nwhy it produces the scores it does.\nThere exist different scoring approaches in between the extremes of using handcrafted\nfeatures linked to a construct definition to predict human ratings and prompting an LLM. Figure 1\nplaces these different scoring approaches on a continuum to demonstrate the reduction in\ntransparency. To predict a human rating, we might use machine learning methods to extract general\nlinguistic features and/or keyword indicators to capture specific content in the response or specific\nwritten structures not explicitly related to the content. This approach is often used in scoring short\nanswer texts or content-based items. We might also use embeddings from an LLM to predict the\nhuman ratings. The transparency in these types of models is reduced compared to the substantive,\nconstruct-related feature-based model, but better than when prompting an LLM. What\ndistinguishes these modeling approaches to prompting LLMs is that they are intended to predict\nthe human rating, and there is an expectation of some concordance with human scoring behavior.\nUnless the LLM is fine-tuned for used human ratings, there is no explicit connection to human\nratings.\nTo set up some language to further the discussion of LLMs, let the information we give to\nthe LLM be called \u201cprompts\u201d which contain instructions for the request of the LLM and any\ninformation needed to complete the \u201ctask\u201d or the request. An example of a prompt in the CR\nscoring context is given in the Appendix of this paper. The result is often referred to as the\n\"completion\", but we will often refer to it as the output, result or the LLM score, as appropriate.\nThe LLM output also depends on various technical options that can influence the results.\nSome such options are built into an LLM (e.g., the algorithm for tokenizing the textual inputs or\nthe algorithm for selecting words for a completion) but can vary across models leading them to be\ndifferentially effective for a given application. Others, such as setting the temperature, which\naffects the variability of potential responses, can be controlled by the users. Users need to explore\nthese settings to ensure the LLMs yield the most reliable scores and ones that can best be shown\nto adhere to the construct when using LLMs for scoring responses."}, {"title": "Training LLMs", "content": "LLMs are most often pre-trained by the model providers (e.g., OpenAI, Meta, etc.). Post-\ntraining, which can include fine-tuning and retrieval augmented generation (RAG), often helps to\nalign a model closer with a specific use case, which could make it more appropriate for a particular\ntask or domain. In the process of fine-tuning, the user can provide examples of responses with the\n\u201ccorrect\u201d scores or labels (prompt-completion pairs), to train the LLM on how to perform a specific\ntask, updating the weights in the LLM. This is similar to model building in the traditional sense in\nthat we might use a sample of responses and the human ratings to \u201ctrain\u201d the \u201cengine\u201d to estimate\nmodel weights for future prediction. The difference here is that the LLM could be used to produce\nscores without fine tuning for the specific use case.\nWhile the process of fine-tuning seems like it would be easy to perform and could only\nimprove the model, there are certain considerations. For example, if the model is fine-tuned to\nperform task \u201cA\u201d and is then later used to perform task \u201cB\u201d performance might be degraded\ncompared to the original pre-trained (not fine-tuned) model. The user must be aware of both the\nlimitations of the fine-tuned model and of existing alternatives if the fine-tuned model will be used\nfor multiple different tasks, such as scoring different item types. Parameter efficient fine-tuning\n(PEFT) trains a small number of specific \u201cadapter\u201d layers of the LLM. In the case of PEFT, most\nof the pre-trained weights remain the same and so it is robust for performing multiple tasks and\nprevents overfitting. Fine-tuned LLMs are readily available for certain purposes and tasks. To\nperform additional domain-specific fine-tuning in the CR scoring context, we might use response\ndata scored or annotated by humans collected as part of testing operations, and/or publicly-\navailable datasets.\nAnother way to train the model would be in-context learning (ICL) which occurs during\nthe prompting. For example, we might provide the LLM with one or more prompt-completion\npairs as examples of what it is supposed to do with the subsequent prompts for which we want a\ncompletion. Since there is a limit to what can be submitted to the LLM in this fashion, typically\nonly up to three examples may be provided. If no examples are used, we call this zero-shot\nprompting, if one example, then it is termed one-shot learning, and if two or three, few-shot\nlearning."}, {"title": "Evaluation for Accuracy and Fairness", "content": "The high expectations for the functionality of LLMs, concerns about potential risks or\nharms from their use, and the existence of many alternative models has led to evaluation of the\nactual capabilities of LLM as an active research area. Evaluation often relies on how well the\nmodels perform on sets of benchmark tasks in terms of the accuracy, repeatability, and fairness of\nthe results on each task. Because of the broad range of potential uses for LLMs, some authors\nargue for moving testing performance on specific tasks to using a \u201cpsychometric\u201d approach for\nLLM evaluations that assesses the models on underlying types of tasks that might be analogous to\ntasks in psychology such as spatial reasoning tasks (Bommasani at al., 2023). For example, in\nmedical and health use cases, ratings scales are used by human raters to evaluate LLMs on\ndimensions such as accuracy, understanding, safety, and trust (Tam et al., 2024)\nThe evaluation of LLMs for rating constructed responses has not received specific attention\nin the research on evaluation of LLMs. However, when used for predicting human ratings as a\nmeans of scoring CRs, LLMs have been evaluated using the approaches developed for more\ntraditional feature-based AI scoring that uses simpler statistical and machine-learning models.\nThese evaluations generally focus on how well the models recover the human ratings and the"}, {"title": null, "content": "ability to do this consistently regardless of the characteristics of the test taker who produced the\nresponse. The evaluations use common statistics based on agreement (e.g., simple percent\nagreement between the scores of humans and AI models, and kappa and quadratic weighted kappa\n[QWK]) and accuracy (e.g., mean squared error [MSE] and percent reduction in mean squared\nerror [PRMSE]). Some methods, notably PRMSE, account for the errors in the human ratings used\nin assessing the AI models. Evaluations also include checks for fairness (Johnson & McCaffrey,\n2023; Johnson et al., 2022). In the psychometrics and measurement community, the evaluation of\nthe quality of the model prediction is embedded in the larger framework of demonstrating the\nvalidity of the scores for supporting the claims of the items and tests (Bennett & Zhang, 2015;\nMcCaffrey et al., 2022; Williamson et al., 2012).\nBeyond metrics, explainability tools help evaluate LLMs by revealing what parts of the\nprompt/input are important in the resulting completion (or score). In the black-box algorithms,\nthere is no possibility of purposefully connecting the inner workings of the LLM and the score it\nassigns like we can in a feature-based model. AI explainability (XAI) and explainable NLP\n(XNLP; Danilevsky et al., 2020) are approaches to generating explanations for the LLM results.\nRecent research has led to tools that can be used to identify features inside an LLM to explain how\nit works and classify those features as \u201csafety relevant\u201d or harmful. They may be generated in\ndifferent ways including, for example, use of measures of feature importance, surrogate modeling,\nor example-driven approaches, and the results may be visualized in different ways. A popular\nmethod is using saliency maps or highlighting to show which tokens in the text are important to\nprediction accuracy and visualize the gradient of the loss with respect to each token in the model.\nThe maps provide indicators of the influence of the tokens in the response. Given the available\ntools, the explanations are not always useful and come with their own scrutiny and need for\nevaluation (Hoffman et al., 2018).\nXAI tools can be used to evaluate fairness in addition to the traditional metrics that might\nbe used such as subgroup-level comparisons of human and machine scores. The evaluation of\nfairness is very important when using pre-trained LLMs as they are typically trained using data\nnot representative of the existing cross-cultural psychological diversity of many tested populations\n(Atari et al., 2024). De-biasing models may be performed by some developers of LLMs, but the\nextent to which this is effective in any specific application is variable."}, {"title": "Validity Framework for CR Scoring", "content": "The Standards for Educational and Psychological Testing (American Educational Research\nAssociation [AERA], American Psychological Association [APA], & National Council on\nMeasurement in Education [NCME], 2014) provide high level guidelines for demonstrating the\nvalidity (including fairness) of CR scoring. Even though the document was written when the use\nof NLP and Al for scoring was just emerging into the mainstream, the Standards addressed\nautomated scoring on three occasions. Its guidelines can serve as the basis for evaluating validity\nfor scoring constructed responses with generative AI. Other publications such as Bennett and\nZhang (2015), McCaffrey et al. (2022), and Williamson et al. (2012) elaborated approaches for\ncollecting validity evidence for constructed response scores from human raters and feature-based\nAl scores. We build on these to create a framework for generative AI scoring, first reviewing the\nframeworks for human ratings and feature-based AI scoring and then extending those guidelines\nto generative AI."}, {"title": "Evidence for Human Ratings", "content": "To provide more detailed guidance on collecting evidence for AI scores, ETS published its\nBest Practices for Constructed-Response Scoring (McCaffrey et al., 2022) which included a\nframework for establishing validity evidence for CR scores from human raters or AI. The validity\nevidence for CR scores, of any kind, starts with the task design. Ideally task and test design are\nconducted within a formal framework such as evidence-centered design (ECD; Mislevy et al.,\n2003). Part of task design for CR items is the scoring rubric for judging the responses and assigning\nscores. When humans provide the scores, the key to the validity of the scores is the ability of the\nhuman raters to consistently use the rubric as intended. To ensure this occurs, there are several\nprocesses involved with managing human rating. Decisions made throughout the design of the\nscoring system should be made with principles of validity, reliability, and fairness, and they should\nbe documented as part of the curation of validity evidence. For example, the training materials\ndeveloped for raters should include additional details on the task including annotated exemplars to\nensure that raters will apply the rubric as it was intended. Raters should be trained and then\nevaluated before scoring to ensure they are qualified to score. To the extent possible, raters should\nbe recruited from a diverse population with a specified skill set qualifying them to rate responses\nfor the assessment. During the rating process, raters should be monitored to make sure that they\nare consistent and accurate. This means that during the design of the system before scoring occurs,\ndecisions must be made on how to collect data to measure consistency and accuracy. Example of\nsuch decisions are: What size sample of double-scored responses is necessary to estimate the\ninterrater agreement using the selected statistic? Will exemplar responses (pre-selected with an\nagreed upon score from experts) be used to track rater accuracy, and how will they be selected and\ninterwoven into operational scoring? These design decisions should be made in a principled\nfashion to ensure that all components of the human rating process contribute to meaningful use\nand interpretation of test scores."}, {"title": "Evidence for Automated Scores from Models Predicting Human Ratings", "content": "For construct feature-based Al scores, there is a chain of evidence that goes from the\nautomated scores back to the human ratings due to how the engine is trained. Thus, it is important\nto make a validity argument for the human ratings and then the machine scores. Ideally, tasks that\nare intended to be automatically scored are designed with automated scoring in mind, and the\nprocedures and decision-making should also be documented. If human ratings are then used to\ntrain automated scoring models, we prefer to start with \u201chigh quality\u201d ratings based on sound\npractices as described above and in Table 1 and with at least a satisfactory psychometric profile.\nThe traditional NLP feature-based automated scoring approach requires similar evidence\nto the human ratings with some differences due to the nature of the scoring process. While human\nraters apply a scoring rubric and make judgements, a scoring engine extracts information from the\nresponses to reflect different construct-relevant features. A statistical prediction model is then\ntrained to predict the human rating using those features. In an engine that evaluates writing ability,\nfeatures may include grammar, usage, mechanics, style, and organization (Attali, 2007; Attali &\nBurstein, 2006). In an engine that evaluates spoken responses, features might include words per\nminute, average pause length and others for accuracy and pronunciation (Xi et al., 2008). The set\nof features should be combined to represent the construct. Validity evidence may include\ndocumented links between the feature set and the construct definition. It may also include a\nsummary of the prediction model weights to determine the extent to which the combination of\nfeatures and weights correspond to how raters should be combining information about the response\nto derive the score.\nFor evidence of internal structure of the test (AERA, APA, & NCME, 2014, p. 16), we\nwould collect evidence analogous to what we would collect for the human ratings-documented\nlinks between the features and scoring rubric; we might perform factor analyses or inter-item\ncorrelations with the machine scores and other item scores on the test; and we also monitor the AI\nscores by comparing them to human ratings of the same responses. This concordance should be\nestablished during model evaluation, but also during operational scoring to ensure there are no\nissues with predictions on a new sample of test takers. In addition, we should train the features on\ndata not used for model-building or evaluation and the sample should be representative of the\ntarget test taker population.\nMuch of the other types of evidence for AI scores would be collected at the time of the\ninitial model evaluation. A model-level evaluation includes an examination of concordance\nbetween the human ratings and the machine scores. Metrics might include standardized mean\ndifferences, correlations, QWK, etc. (Williamson et al., 2012). Recent arguments have been made\nto include a measure of the accuracy of predicting the human true score via PRMSE (McCaffrey\net al., 2024).\nAn impact-level evaluation might compare the CR section scores (based on machine\nscores) to other section scores, or other completely external scores if available. We also want to\ncompare section and total test scores based on the machine scores to corresponding section and\ntotal scores based on human ratings to understand the size of the differences at that level. For\nevidence that the response processes are appropriate, we rely on annotations by subject matter"}, {"title": null, "content": "experts of a selection of responses to make sure that there is justification for the scores given by\nthe engine. This type of qualitative analysis should be performed at all score levels. In addition,\nwe also must provide evidence that the engine is properly handling \u201catypical\u201d responses which\nmay be in the wrong language, off-topic, a copy of the prompt text, etc. The scoring engine must\nbe trained to either flag these responses so that they can be hand-scored or trained to assign an\nappropriate score to them (likely a 0). Analyzing the effectiveness of any \u201cadvisories\u201d or flags the\nengine might use is important to ensure that these responses are detected and scored appropriately.\nThis minimizes the chances that atypical responses that deserve a score of 0, for example, do not\nget a score of 2.\nChecking for fairness is of vital importance, however simply checking for fairness is\ninsufficient. Fairness should be part of the design of the scoring system. To start, in addition to the\nfairness checks that should occur on the human ratings, we need to ensure that the samples used\nto train features, build the models, and evaluate the models are large and representative of the test\ntaker population. This is important because there might be differential response styles by group\nand the engine must be trained to score those appropriately. In addition, if there is an imbalance in\nthe composition of the training sample and many groups constitute only a small proportion, there\nmay be inadequate representation. Oversampling or weighting up small groups in the training\nsample may be a solution to this. An evaluation of the demographic composition of the sample and\nhow it might impact fairness should be documented.\nFairness checks during model evaluation often involve a comparison of human and\nmachine score means, by subgroup, via a standardized mean difference (Williamson et al, 2012).\nIn addition, comparing the QWK by subgroup may also be helpful to understand if agreement is\ndegraded for different test taker groups. The Best Practices (McCaffrey et al., 2022) discuss\nchallenges with subgroup analyses, including that the small sample sizes for groups would prevent\nthese groups from being assessed for fairness. Recent work utilizes empirical Bayesian methods\nto better estimate SMD in the small sample case (Kwon et al., n.d.). In addition to basic SMDs and\nQWKs, we might also run differential item functioning (DIF) analyses-first for the human scores\nand then for the machine scores, to compare the DIF results for humans and machines. Differential\nfeature functioning (Zhang et al., 2017) and differential algorithmic functioning (Suk & Han,\n2024) are also appropriate analyses to better understand the extent to which the engine features or\nthe model may be disadvantaging certain groups. We may perform all of these together to collect\nevidence on the fairness of scores."}, {"title": "Models Using General Linguistic Features and LLM Embeddings", "content": "Figure 1 shows three different types of models that predict the human rating: (a) construct\nfeature-based model, (b) general linguistic feature-based model and (c) a model based on LLM\nembeddings. We include (b) and (c) in this section with (a) despite their relative reduced\ntransparency because they share the chain of evidence link of the prediction of human ratings.\nThis somewhat mitigates the concern with reduced transparency, even though there might still be\nmany thousands of parameters and an increased risk of the predictions relying on construct\nirrelevant features of the response.\nMost of the evidence required for these models will be similar to the construct feature-\nbased models but not all will be available. For example, it may be infeasible or inappropriate to\nmatch engine features to rubric indicators. In content engines such as ETS' c-rater (Leacock &\nChodorow, 2003), often a written response is evaluated using many generic linguistic features\nand/or keyword indicators to capture specific content in the response or specific written structures"}, {"title": null, "content": "not explicitly related to the content. The responses may be parsed into n-grams which could\ngenerate thousands of \u201cfeatures\u201d. In this case, the features are not necessarily understandable, and\nthe machine learning models used for prediction are more sophisticated than traditional regression\nmodels meaning the \u201cweights\u201d are not something that can be easily examined or immediately\nunderstood. The same would be true if using embeddings from an LLM. As a result, the\ntransparency of the automated scoring process is reduced and the validity evidence for the machine\nscores relies more heavily on the quality of the human ratings used to train the models (McCaffrey\net al., 2022). We might also require a very strong link between the scoring rubric and the construct\ndefinition and demonstrated high agreement between the human rating and AI scores during model\nevaluation and monitoring.\nSome of the evidence we propose for generative AI scores will be useful for making a\nvalidity argument for these scores as well. Table 1 reflects these differences. For example, saliency\nmethods to understand the importance of different features or embeddings may provide evidence\nthat the scores resulting from these models lead to meaningful interpretations or that they do not\ncontain construct irrelevant features."}, {"title": "Validity Evidence for CR Scoring Systems Using Generative AI", "content": "Generative Al is distinctly different due to the nature of the approach in generating the\nscores. In this case, because the \u201cengine\u201d is generating an output in response to a prompt, there is\nno principled or explicit system of deriving a model or selecting features and the score is not based\non a prediction of a human rating. As such the types of validity evidence we might collect for\nfeature-based AI scoring should be expanded for generative AI applications."}, {"title": "Choice of LLM", "content": "Much like any automated scoring engine, we should consider the goals of the assessment\nand the construct definition as we select a LLM to use for scoring. A written justification for the\nLLM is part of the evidence we should collect. A LLM is pretrained based on corpora and scrapes\nof text from the internet. Some LLMs are pretrained for specific domains, like domain-specific\nlanguage (e.g., legal, medical, or some other content specific language). We might ask, is this LLM\nsuited to the language task needed for the construct or use case? For example, a testing company\nevaluating responses for doctors or doctors-in-training might benefit from a domain-specific LLM,\nor from training their own. In addition, some LLMs are meant to generate text, not evaluate an\ninput. If the task is to generate feedback on essay responses (e.g., annotations for training or for\nraters to use during scoring), then a LLM that was trained to generate text, such as GPT4, might\nbe a suitable choice. If the task is to evaluate the text provided in the prompt, then other LLMs\nmay be more appropriate. For example, GPT and BLOOM models are pretrained for text\ngeneration and other \u201cemergent behavior\u201d (Le Scao et al., 2023; OpenAI, 2023) while BERT and\nROBERTA are pretrained for sentiment analysis, word classification, and named entity\nrecognition (Devlin et al., 2018; Liu et al., 2019). T5 and BART are pretrained for translation, test\nsummarization and question answering (Lewis et al., 2019; Raffel et al., 2020). As more LLMs\nare released for public use, it is important to have a good understanding from the user perspective\non how they compare in their capabilities to fulfill the required task.\nSelection of LLM should be based on empirical findings from preliminary experiments\nwith multiple LLMs. Combinations of LLMs in AI scoring may also be considered. For example,\nif scores from a domain-specific LLM and a general LLM were combined, we might consider this"}, {"title": null, "content": "to provide more coverage of the content or construct. More research on combining LLM scores is\nneeded at this time, but factor analytic models might be an approach."}, {"title": "Prompting and In-context Learning", "content": "The nuances in wording of the instructions in the prompt to the LLM can make large\ndifferences in its output. Documentation of the prompt wording used and results of any\nexplorations with different prompting should be collected as part of the validity evidence. For\nexample, for prompting that involves multiple tasks for the LLM to complete, the order in which\nthe tasks are presented can matter because the LLM can learn from the first task (Stahl et al., 2024).\nWe can also structure the prompt to include the LLM's chain-of-thought, its reasoning or\nsupport for the answer, as part of the output (Wei et al., 2022). In the case of CR scoring, this is\nsimilar to a subject matter expert providing an annotation of a response used as an exemplar. The\nexpert assigns a score to the exemplar and then a summary of why it deserves that score. In chain-\nof-thought prompting, the prompt either requests the reasoning behind the answer (\u201czero-shot\nchain-of-thought\u201d) or via in-context learning, in which examples are used to demonstrate the\ndesired type of extended response the model should produce. This approach has been shown to\nimprove the accuracy of essay scoring, especially with in-context learning (Lee et al., 2024). \u0410\nqualitative analysis (by subject matter experts) of many of these \u201cannotations\u201d can help provide\nvalidity evidence for the LLM-based scores. In addition, explainable NLP/AI techniques may be\nuseful in understanding the impact of in-context learning on LLM results (Liu et al., 2023). This\ncan be beneficial in the understanding of how the LLM treats atypical responses. If annotated\nexemplars for human rater training already exist for a testing program, then those responses can\nconveniently be used to evaluate the LLM's ability to score and give reasoning similar to the\nexpert.\nExperiments with in-context learning might demonstrate the advantages of a one- or few-\nshot learning approach. However, in some applications for AI scoring, there may be no benefit. In\nthe context of scoring constructed responses, some research shows that a multi-trait specialization\napproach outperforms the \u201cvanilla\u201d or zero-shot prompting approach. In multi-trait specialization,\nthe rubric is decomposed into traits and scores from zero-shot prompting for each trait are then\naveraged (Lee at al., 2024)."}, {"title": "Decisions on Fine-tuning", "content": "Once the LLM is selected, decisions must be made on whether additional training, or fine-\ntuning, should be made and how to perform that training within the validity framework. This is\nsimilar to the decisions we make about training for human raters in that we need to ensure and\ndocument that the materials selected for use in the human scoring process (for training, evaluating\nraters, etc.) are related to the construct and do not introduce any construct irrelevant variance or\nunfairness.\nDecisions on the sample of examples used for fine-tuning and how many examples are\navailable and used should be made carefully. Care should be taken to ensure that the sample\nrepresents various different groups of test takers and responses, and to the extent possible, that the\nscores provided are correct according to subject matter experts. For this purpose, we might ensure\nthat the LLM can correctly rate exemplar responses, a technique typically used to train human\nraters, if available. For example, if the test is for English language learners from various language\ngroups, it is important to provide examples from as many groups as possible. Using a convenience"}, {"title": null, "content": "sample of prompt-completion pairs from native Spanish speakers, for instance, may lead to bias\nwhen using the trained LLM to score test takers from other language groups.\nFine-tuning can be performed over several rounds in an iterative approach. Fine-tuning can\nalso be done on a single task (e.g., scoring essays) or multiple tasks (e.g., scoring essays and\nproviding feedback to test taker). Multi-task fine tuning may be useful in improving the results for\na single task by using different versions or wording of the prompt (e.g. scoring essays with rubric\nand scoring essays holistically based on rubric).\u00b9 However, in order to perform multi-task fine-\ntuning many more example responses are necessary relative to single-task fine-tuning.\nDescriptions of how the examples used for fine-tuning (including any public datasets) are\nappropriate for the particular task, as well as the adequacy of the fine-tuning approach, should be\ndocumented as part of the validity evidence.\nAn important caveat to note is that while a fine-tuned LLM may perform better for the task\nthan the pre-trained LLM, there is a risk that changes in the test taker population may show\ndegraded performance in a new population. Therefore, we should be careful when tuning the LLM\nthat we are not \u201cmicro-tuning\u201d to a population that will not be relevant in the future. This concern\nis similar to concerns with traditional AI scoring models, which is why we need to be cognizant\nof the samples used for training and evaluation as well as making sure there is a monitoring system\nin place to catch population changes.\nNLP features may also be introduced to the scoring process with LLMs. For example,\nsuppose we have values for 10 features for essay responses that together provide information on\nwriting skills. We could fine-tune the LLM with this information and provide instructions on how\nthis should be used to generate a score. In theory this may lead to better construct coverage,\nhowever, it will be unknown how the LLM will actually use this information. Sensitivity analyses\ndemonstrating that the LLM does utilize the features (e.g., showing meaningful score differences\nwith and without features) may serve as evidence that it does contribute. We might also use NLP\nfeatures by combining them with LLM scores directly using best linear predictor, provided there\nare human ratings available for a sufficient subset of responses (Yao et al., 2019ab)."}, {"title": "Gaming and Atypical Responses", "content": "Much like feature-based AI scoring models, atypical responses are a concern with\ngenerative AI. The sensitivity of LLMs to atypical responses in the CR scoring context is not well\nstudied. Finetuning the LLM with a sample that includes a sufficient number"}]}