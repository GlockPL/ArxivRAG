{"title": "Modularity in Transformers: Investigating Neuron Separability & Specialization", "authors": ["Nicholas Pochinkov", "Thomas Jones", "Mohammed Rashidur Rahman"], "abstract": "Transformer models are increasingly prevalent in various applications, yet our understanding of their internal workings remains limited. This paper investigates the modularity and task specialization of neurons within transformer architectures, focusing on both vision (ViT) and language (Mistral 7B) models. Using a combination of selective pruning and MoEfication clustering techniques, we analyze the overlap and specialization of neurons across different tasks and data subsets. Our findings reveal evidence of task-specific neuron clusters, with varying degrees of overlap between related tasks. We observe that neuron importance patterns persist to some extent even in randomly initialized models, suggesting an inherent structure that training refines. Additionally, we find that neuron clusters identified through MoEfication correspond more strongly to task-specific neurons in earlier and later layers of the models. This work contributes to a more nuanced understanding of transformer internals and offers insights into potential avenues for improving model interpretability and efficiency.", "sections": [{"title": "1 Introduction", "content": "As transformer neural networks continue to scale up and demonstrate improvements across various applications, their integration into more areas of technology becomes increasingly widespread. Despite these advances, our fundamental understanding of how these models operate remains limited [Hendrycks et al., 2021]. This gap in understanding poses challenges for the reliability and trustworthiness of these models, particularly in high-stakes situations.\nTo address this, our study draws inspiration from bottom-up circuits-based [Olsson et al., 2022] and top-down linear direction-based analyses [Zou et al., 2023], as well as from modularity research [Pfeiffer et al., 2023, Zhang et al., 2023, Filan et al., 2021]. We aim to explore the specialization of different components within neural networks in performing distinct tasks and querying different knowledge bases, defining modularity in this context as having two key traits: 1) A high degree of separability between components tasked with different functions, and 2) Positional locality of these components. Our focus primarily lies on the identification of distinct components within these networks and understanding their roles in processing different types of information."}, {"title": "2 Method", "content": ""}, {"title": "2.1 Pre-Trained Models", "content": "We work with a Vision Transformer, ViT-Base-224 [Dosovitskiy et al., 2021]. as well as with a causal language model, Mistral 7B instruct v0.2 [Jiang et al., 2023]."}, {"title": "2.2 Datasets and Tasks", "content": "For ViT, the task of study is to perform image classification on different classes of Cifar100 Krizhevsky [2009] fine-tuned model as a 'retained' baseline. For the 'forget' datasets, each super-class of Cifar100 (Cifar20) is used as a target.\nFor text models, we investigate next-token prediction on various categories of text as possible niches, as well as subject-split MMLU question answering Hendrycks et al. [2020]. For a broad baseline of model activations, ElutherAI's \u2018The Pile' Gao et al. [2020]\u00b9 is used. For the specialised datasets, use the individual subsets of the pile, as well as some some additional data sets. We use some scientific-subject specific instruction datasets on specific topics from Li et al. [2023], including 'Biology', 'Physics', 'Chemistry'. We also use \u2018CodeParrot GitHub Code' Tunstall et al. [2022], or 'Code' for short."}, {"title": "2.3 Neuron Selection and Neuron Clustering", "content": ""}, {"title": "2.3.1 Neuron selection", "content": "We select neurons using a method analogous to the Selective Pruning approach [Pochinkov and Schoots, 2024]. Given two datasets: a 'reference' dataset Dr and 'unlearned' dataset Du, we score the neurons Sn by getting the ratio of mean absolute deviations between the two datasets, as seen in Equation 1:\n$S_n := \\frac{\\mathbb{E}_{x_j \\in D_r} |n_i(x_j)|}{\\epsilon + \\mathbb{E}_{x_j \\in D_u} |n_i(x_j)|}$ (1)\nwhere n(x) is the activation of neuron i in layer l for input xj.\nWe then \"select\" a subset of neurons as being relevant to some data subset Dq by taking a percentage of neurons with the highest scores. The percentage of neurons selected for each data subset is determined by aiming for a specific drop in top1 accuracy when these neurons are removed.\nFor ViT, the 'retained' baseline is Cifar100 [Krizhevsky, 2009], and the 'unlearned' data subsets are those of Cifar20. For Mistral, the retained baseline is ElutherAI's 'The Pile' [Gao et al., 2020], with the subsets being the different Pile subsets and the additional specialized datasets mentioned earlier."}, {"title": "2.3.2 MoEfication clustering", "content": "We employ 'Mixture of Experts'-ification [Zhang et al., 2022] as a method for grouping neurons into clusters. This approach involves taking the input weights Win for an MLP layer and treating its columns as a collection of vectors. We then perform balanced k-means clustering [Malinen and Fr\u00e4nti, 2014] on these vectors:\n$\\lbrace C_1, ..., C_k\\rbrace = BalancedKMeans(W_{in}^l, k)$ (2)\nwhere Ci represents the i-th cluster of neurons. This effectively splits each MLP layer into k balanced 'mixture of experts' clusters. The intuition is to group neurons that are often activated simultaneously, allowing for efficient expert selection during inference.\nFor ViT, we use k = 96 as in the original paper. For Mistral, k = 128 is used for even divisibility of layers. We perform this clustering on both pre-trained and randomly-initialized models, providing a basis for comparing the emergent structure in trained networks against random initializations."}, {"title": "3 Class-wise Unlearning Performance Evaluation", "content": ""}, {"title": "3.1 ViT Performance", "content": "We assess the performance of ViT across all Cifar20 classes to determine if and how the performance on other classes is affected by the deactivation of neurons associated with a particular class. We uncover that between classes with commonalities, there often seem to exist shared neural pathways."}, {"title": "3.2 Mistral Performance", "content": "For Mistral, we investigate the degree to which a drop in performance in each class causes a correlated drop in performance across all other dataset tasks. We choose an arbitrary goal of unlearning by selective pruning until achieving a 20% drop in Top1 next-token prediction accuracy in each respective unlearned text subset.\nIn Figure 2, we observe that in most subsets of data that one would intuitively associate, such as unlearning 'biology' leading to a drop in MMLU 'High School biology'. Notably, we see that unlearning 'PubMed central' leads to significant drops in MMLU 'professional medicine', 'Medical Genetics', 'Clinical Knowledge', 'Anatomy', 'College Biology', and 'College Chemistry'. Additionally, we observe that unlearning 'ArXiv' leads to corresponding drops in MMLU subjects.\nWe also note some 'failure' cases:\n\u2022 'EuroParl' and 'FreeLaw' seem to have relatively modest effects on the accuracy of other subsets, including seemingly related MMLU topics such as 'HighSchool Government &\nPolitics' and 'Professional Law'."}, {"title": "4 Intersection Analysis", "content": ""}, {"title": "4.1 Intersection between Class Selected Neurons", "content": "We analyze the degree to which selected neurons overlap between different classes or data subsets. For some set of selected neurons N\u2081 and Nj for classes i and j respectively, we calculate |Ni \u2229 Nj|/|Ni|.\nEach class of neurons amounts to a total of 12.5\u00b16.0% of neurons in ViT, and 3.4\u00b12.6% of neurons in Mistral."}, {"title": "4.2 Intersection between Selected Neurons and Clusters", "content": "Inspired by the Lorentz Curve and Gini Coefficient [Gastwirth, 1971], for each layer, we measure the degree to which the selected neurons are preferentially selected into few clusters. We do this by counting how many selected neurons are in each cluster, sorting the clusters in order from highest to lowest member count, then plotting a cumulative curve showing distribution between clusters.\nThat is, we create k bins, and take the indices i of the selected neurons, and count how many are assigned to each bin, B[j]. We then sort the bins in order of highest to lowest, giving sorted bins B[j], and plot a cumulative distribution of counts cn, where Cn = Cn\u22121 + B[n]. We then find the normalized area under the curve, such that if all the neurons were in one bin, the area would be 1.0, and if the neurons were perfectly evenly distributed, it would be 0.5. For an example of such a curve, see Figure 6a.\nAs the MoEfication clustering of neurons is limited to single layers, we add an extra constraint such that the selection of neurons is limited to have an equal proportion per layer, as having all neurons in one layer automatically distributed neurons equally between these clusters."}, {"title": "5 Related Work", "content": "Circuits-based mechanistic interpretability seeks to uncover the exact causal mechanisms and neuron activations responsible for specific behaviours [Chan et al., 2022, Olsson et al., 2022, Wang et al., 2022, nostalgebraist, 2020]. Although this bottom-up approach offers detailed insights, the vast number of potential circuits poses a significant challenge to comprehensively understanding model behaviours.\nActivation-direction-based interpretability methods, such as Activation Addition [Turner et al., 2023] and Representation Engineering [Zou et al., 2023, Burns et al., 2022, Turner et al., 2023, Gurnee and Tegmark, 2023], attempt to correlate model activations with meanings or concepts [Dar et al., 2022, Meng et al., 2022]. However, these top-down approaches often overlook finding the sources of these activation patterns."}, {"title": "6 Discussion", "content": "Our analysis has shown that both the MoEfication clustering [Zhang et al., 2022] and the importance selection from Selective Pruning [Pochinkov and Schoots, 2024] seem to reveal some degree of interpretable modularity in transformer models. We believe this is a promising starting point for understanding the high-level behavior of transformer neural networks, and for identifying how different parts of a neural network might be mapped out, analogously to how regions of a human brain are mapped out in neuroscience.\nAdditionally, the overlap between similar categories in randomly initialized models seems to provide some evidence for a weak version of the Lottery Ticket Hypothesis [Frankle and Carbin, 2018].\nOur findings have several implications:\n1. The qualitatively intuitive nature with which classes seem to overlap seems to suggest some level of distinctness with how models handle these inputs.\n2. The relatively small differences in neuron importance between pre-trained and random models indicate that training does indeed lead to meaningful specialization, but that some of this structure may be present even at initialization.\n3. The clear but incomplete correspondence between MoEfication clusters and task-specific neurons suggests that both methods form a useful but incomplete for understanding models.\nHowever, our study also has limitations that should be addressed in future work:\n1. We focused on MLP neurons and did not analyze attention mechanisms, which play a crucial role in transformer models.\n2. While we use two very different models and find similar results, our analysis was limited to a specific set of tasks and datasets, and may not generalize to all domains or model architectures.\n3. The interpretation of neuron importance and specialization is still somewhat qualitative and could benefit from more rigorous statistical analysis."}, {"title": "7", "content": "Future work should look to improve the precision with which one can identify different parts of a model that are using different knowledge, as well as increasing the degree to which one can have a fine-grained hierarchical understanding of how the components of a model might be mapped.\nSome potential directions include: 1) Extending the analysis to include attention mechanisms and their interaction with MLP layers. 2) Developing more sophisticated clustering techniques that can capture hierarchical relationships between neurons and tasks. 3) Investigating how neuron specialization evolves during the training process, potentially shedding light on the dynamics of learning in these models. 4) Exploring how this understanding of modularity could be leveraged to improve model interpretability, robustness, or transfer learning capabilities."}]}