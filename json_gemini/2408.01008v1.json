{"title": "Tensor Train Low-rank Approximation (TT-LORA): Democratizing Al with Accelerated LLMs", "authors": ["Afia Anjum", "Maksim E. Erent", "Ismael Boureima", "Boian Alexandrov", "Manish Bhattarai"], "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks, such as question-answering, sentiment analysis, text summarization, and machine translation. However, the ever-growing complexity of LLMs demands immense computational resources, hindering the broader research and application of these models. To address this, various parameter-efficient fine-tuning strategies, such as Low-Rank Approximation (LoRA) and Adapters, have been developed. Despite their potential, these methods often face limitations in compressibility. Specifically, LoRA struggles to scale effectively with the increasing number of trainable parameters in modern large scale LLMs. Additionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which utilizes tensor train decomposition, has not yet achieved the level of compression necessary for fine-tuning very large scale models with limited resources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA), a novel parameter-efficient fine-tuning (PEFT) approach that extends LORETTA with optimized tensor train (TT) decomposition integration. By eliminating Adapters and traditional LoRA-based structures, TT-LORA achieves greater model compression without compromising downstream task performance, along with reduced inference latency and computational overhead. We conduct an exhaustive parameter search to establish benchmarks that highlight the trade-off between model compression and performance. Our results demonstrate significant compression of LLMs while maintaining comparable performance to larger models, facilitating their deployment on resource-constraint platforms.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) such as LLaMA-70B [1], ChatGPT-4 [2], Bard [3], and Claude [3] represent a significant step towards Artificial General Intelligence (AGI) [4]. These models are trained using vast amounts of data and are complex neural network architectures, such as Transformers [5], enabling the models to excel in interpreting complex linguistic patterns. Consequently, these models can accurately recognize, translate, predict, and generate text, along with performing other content-related tasks. The effectiveness of LLMs in capturing the nuances of language has made them indispensable tools for driving previously unattainable innovations.\nWhile pre-trained LLMs provide a robust foundation for general language tasks, fine-tuning these models on application-specific datasets is crucial for optimizing performance in specialized applications, which allows the models to adapt to a particular context or domain. However, fine-tuning involves adapting all the parameters of a pre-trained model to a new tasks, which poses significant challenges due to the reliance of these models on increasingly complex Transformers with exploding parameter counts (model size in billions of parameters), requiring substantial computational resources [6]. Fine-tuning these large models through traditional approaches, which involves scaling these models using multiple GPUs, is hindered by resource limitations and rising costs, monopolizing research access and raising ethical concerns. Additionally, the immense computational demands of these models raise serious environmental concerns due to the energy consumption of massive computing complexes [7].\nSince full model fine-tuning, which involves adjusting all the parameters of a pre-trained model, becomes prohibitively expensive as the model size of LLMs grows, notable pragmatic efforts, such as parameter-efficient fine tuning (PEFT) techniques like Adapters [8], Prefix Tuning [9], Prompt Tuning [10], Low-Rank Adaptation (LoRA) [11], and Low-Rank Economic Tensor-Train Adaptation (LoRETTA) [12]"}, {"title": "II. RELATED WORKS", "content": "In this section, we explore various strategies for parameter-efficient fine-tuning of LLMs, as illustrated in Figure 2, which are crucial for deploying these models in resource-constrained environments.\nAdapter-based Methods: Adapter-based methods [16]-[18] introduce small, trainable modules within the pre-trained model's architecture, where each module consists of fully connected layers configured with a bottleneck structure (Figure 2a). keeping most of the model's parameters unchanged. This adapter-based design keeps the pre-trained model's weights fixed while only updating the parameters within the adapters during task-specific fine-tuning. Although adapter-based methods can reduce the number of trainable parameters, they introduce additional computational steps in the Transformer blocks. Due to their sequential processing nature, these adapter layers do not effectively leverage hardware parallelism, which results in increased latency, particularly in online inference scenarios with small batch sizes [19.\nPrefix Tuning, Prompt Tuning and P-tuning: Li and Liang [9] introduced prefix-tuning, a method where a sequence of continuous task-specific vectors, also known as prefix, is prepended to the model input, optimizing only the prefix while keeping the model parameters frozen. Lester et al. [10] simplify the prefix-tuning by proposing prompt-tuning, where k tunable tokens per downstream tasks are prepended to the input text. Liu et al. [20] proposed p-tuning, which extends prompt tuning by integrating continuous token embeddings not only at the input level but also at various points throughout the model, enhancing the adjustment of model processing. These tuning methods are illustrated in Figure 2b. However, these tuning methods occupy a part of the fixed sequence length that transformer-based architectures can process. Consequently, this reduces the available space for actual task-related input, potentially compromising the model's efficiency [19].\nLow-rank Approximation: Hu et al. [19] proposed a Low-rank Adaptation (LoRA) fine-tuning approach leveraging matrix factorization, which decomposes a large matrix into a product of two or more smaller matrices (Figure 2c). For a pre-trained weight matrix $W_o \\in R^{m\\times n}$, the weight update in a full fine-tuning setting would be $W_o + \\Delta W$, where $\\Delta W \\in R^{m \\times n}$. In contrast, LoRA enables low-rank updates to the weight matrix as $W_o + BA$, where $W_o$ is kept frozen while only optimizing B and A matrices. Here, BA is the low-rank approximation of $\\Delta W$, where $B \\in R^{m \\times r}$, $A \\in R^{r \\times n}$, and $r < min(m,n)$. While LoRA achieves similar or even better performance than full-model fine-tuning, it still incurs a large number of trainable parameters. For instance, when fine-tuning the LLaMA-2-70B model using LoRA, over 16 million parameters need to be updated, exceeding the total number of parameters in some BERT models [12].\nTensor-based Model Compression: Yang et al. [12] proposed Low-Rank Economic Tensor-Train Adaptation (LORETTA), inspired by the Tensor Train (TT) format initially explored by Novikov et al. [21], which represents a matrix with a series of tensor factors. The authors proposed two methods, LORETTAadp and LoRETTArep. The former method, LoRETTAadp, employs tensorized adapters, compressing the weight updating matrix using two tensorized linear layers (Figure 2d). The latter method, LoRETTArep, performs matrix factorization to reduce the large updating matrix into two small"}, {"title": "III. TENSOR TRAIN BASED LOW-RANK ADAPTATION (TT-LORA)", "content": "In this section, we first formulate the objective function of fine-tuning an LLM and outline the challenges associated with conventional full fine-tuning approach. Subsequently, we introduce our proposed PEFT method TT-LoRA, detailing its design and how it addresses these challenges.\nA. Problem Statement\nLet $P_\\theta(y, x)$ be a pre-trained language model, parameterized by $\\theta$. For example, $P_\\theta(y, x)$ could represent a versatile multi-task learning model, such as DistilBERT [22], DeBERTa [23], or LLaMA-3 [1], all of which are developed from the Transformer architecture [24] initially proposed by Vaswani et al. in 2017. We consider adapting the pre-trained model to downstream tasks, such as text classification, summarizing, question answering, and sentiment analysis. Each downstream task is represented by a training dataset of input-output pairs: $\\mathcal{D}: \\{(x_i, y_i)\\}_{i=1}^{N_M}$. Here, $x_i$ represents a sequence of input tokens. The output $y_i$ can vary depending on the task, for instance, it may be a sequence of tokens for text generation tasks, categorical labels for classification tasks, or continuous values for regression tasks. For example, for sentiment analysis, $x_i$ is a social media post, and $y_i$ is the categorical label; for the question-answering task, $x_i$ is the question, and"}, {"title": "B. Proposed Method", "content": "Transformer-based embedding models, such as BERT, depicted in Figure 3(a), comprise multiple dense layers, including numerous encoder blocks and a fully connected feed-forward neural network. Each encoder block, detailed in Figure 3(b), contains a complex sub-layer arrangement, specifically multi-head attention layer and a fully connected feed-forward neural network. The weight matrices associated with these dense layers possess full rank, ensuring that each input feature uniquely contributes to the output and maximizes the learning capacity of the layer. However, when adapting to a specific downstream task, Aghajanyan et al. [25] show that the pre-trained LLMs have a low intrinsic dimension and Hu et al. [19] hypothesize that the updates to the weights of the pre-trained models also have a low intrinsic rank, highlighting a potential area for model compression during adaptation to downstream tasks. Therefore, in this paper, for a pre-trained weight matrix $W_o \\in R^{m\\times n}$ and its corresponding update during adaptation $\\Delta W \\in R^{m\\times n}$, we constrain the update through the proposed TT-LoRA approach by presenting $\\Delta W$ with low-rank representations, as shown in Figure 3(c).\nTo achieve a low-rank approximation of $\\Delta W$, we utilize tensor decomposition [26], more specifically, Tensor Train (TT) [27] decomposition. TT decomposition decomposes a tensor into a series of low-rank, small, three-dimensional tensors (cores). The product of these low-rank tensor cores provides an accurate approximation of the original tensor, significantly reducing its dimensionality while preserving essential structure and information. The fundamental property of TT decomposition is that each core tensor interacts only with its immediate predecessor and successor. This localized interaction allows operations, such as tensor core multiplication to approximate the original tensor, to be performed in a step-by-step manner, focusing on smaller, manageable pieces rather than the entire tensor at once. As a result, the complexity of tensor operations is significantly simplified, leading to substantial reductions in computational and memory requirements [13]. However, TT decomposition is particularly applicable to high-dimensional tensors. Therefore, in TT-LoRA, the matrix $\\Delta W$, which is initialized using a random Gaussian distribution, is first represented as a d-dimensional tensor $\\Delta W \\in R^{k_1\\times \\dots\\times k_d}$. Here, $\\prod_{i=1}^d k_i = m\\times n$. The d-dimensional tensor $\\Delta W$ is then decomposed into d number of small tensor cores $C_1,\\dots, C_d$. The shape of each tensor core can be defined as $C_i \\in R^{r_{i-1}, k_i, r_i}$, given the TT rank $[r_0,\\dots,r_d]$, where the first ($r_0$) and last ($r_d$) TT ranks are 1. Consequently, the total number of parameters in the tensor train decomposition of $\\Delta W$ can be represented as:\n$\\Delta W \\approx \\prod_{i=1}^d C_i= \\sum_{i=1}^{d}| C_i\\in R^{r_{i-1} \\times k_i X r_i}$ (2)\nHere, $r_{i-1} \\times k_i \\times r_i$ is the size of the i-th tensor core. During"}, {"title": "IV. RESULTS AND DISCUSSION", "content": "We conduct experiments to evaluate the performance of TT-LORA on various downstream tasks, ranging from natural language understanding (NLU) to generation (NLG), utilizing pre-trained LLMs of different scales. Specifically, we evaluate DeBERTa [23] and ROBERTa [28] on Generalized Language Understanding Evaluation (GLUE) [29] benchmark while utilizing SuperGLUE [30] benchmark on larger-scale models, such as LLaMA-2-7B [31] and LLaMA-3-8B [1]. In addition to reporting model performance on downstream tasks with TT-LORA, we compare the results with baseline fine-tuning methods, such as Full fine-tuning (FT), Adapters [32], Prompt tuning [10], Prefix Tuning [9], P-tuning [20], BitFit [33], LORA [19], and LORETTA [12]. Furthermore, we conduct an extensive search for optimal parameters, including the best tensor shapes for $\\Delta W$ and the appropriate TT ranks. This effort aims to establish benchmarks that illustrate the trade-off between model compression and performance.\nFor this paper, our experiments utilized a system that integrates four NVIDIA Hopper (H100) GPUs, each paired with a corresponding NVIDIA Grace CPU via NVLink-C2C, facilitating rapid data transfer crucial for intensive computational tasks. The GPUs are equipped with 96GB of HBM2 memory, optimal for handling large models and datasets.\nA. GLUE Experiments on BERT Family\nWe initially conducted experiments on (RoBERTa) Robustly Optimized BERT Pretraining Approach [28], which is an optimized method for training BERT (Bidirectional Encoder Representations from Transformers) [34], a transformer-based LLM. Developed by researchers at Meta AI, ROBERTa revises BERT's pretraining methodology to improve the model's performance in several ways, such as dynamic masking, eliminating the next sentence prediction loss, and increasing the batch size while decreasing the learning rate. Apart from ROBERTa, We performed experiments on DeBERTa (Decoding-enhanced BERT with disentangled Attention) [23], a recent variant of BERT trained on a larger scale. Developed by Microsoft, DeBERTa improves the BERT architecture by introducing a novel disentangled attention mechanism that separately models the content and position, enhancing the model's ability to understand contextual relationships in text. We utilized the pre-trained RoBERTa-base and DeBERTa-base from the HuggingFace Transformers library.\nIn RoBERTa and DeBERTa architecture, each encoder layer includes four weight matrices within the self-attention module ($W_q, W_k, W_u, W_o$) and a feed-forward neural network (FFNN). While TT-LoRA can be applied to any of the weight matrices in a neural network to reduce the number of trainable parameters, our experiments specifically target $W_q$ and $W_v$. This focus aligns with findings from the LoRA paper, which indicates that models achieve optimal performance when these particular weights are fine-tuned, while the remaining weights are kept frozen [19]. We performed hyperparameter optimization by fine-tuning the model with different TT-LORA parameter initializations, the details of which will be presented later. Using the HyperBand optimizer [35], we identified the most efficient parameters. For each run, the model was fine-tuned for up to 20 epochs with an early stopping criterion of 5 epochs. Specifically, the training was halted if the validation loss did not improve for 5 consecutive epochs. The best model was selected based on the lowest observed validation loss from these runs. For reporting performance on the GLUE benchmark tasks, we use the following metrics: matched accuracy for MNLI, Matthews correlation coefficient for CoLA, Spearman correlation coefficient for STS-B, F1 score for both MRPC and QQP, and accuracy for all other tasks. Table I summarizes the downstream task performance comparison between TT-LoRA and other baseline PEFT methods.\nAs shown in Table I, TT-LORA consistently achieves superior or comparable performance to other PEFT methods when FT DeBERTa on GLUE tasks, with no more than 0.2M trainable parameters. Consequently, TT-LORA stands out for its efficiency by outperforming 9 out of 10 FT approaches in both model compression and accuracy.Prompt Tuning, which utilizes just 0.01M trainable parameters compared to TT-LoRA's 0.02M, surpasses TT-LORA in model compression but significantly suffers in performance. Nevertheless, TT-LORA requires merely twice the trainable parameters of Prompt Tuning while achieving, on average, about 15% higher model accuracy. TT-LoRA also achieves achieves substantial model compression when FT ROBERTa, significantly reducing the number of trainable parameters. Specifically, TT-LORA has reduced trainable parameters by approximately factors of 6,200\u00d7 for full FT, 31.5\u00d7 for LoRA, 5\u00d7 for BitFit, and 5\u00d7 for LORETTAadp. Despite this reduction, TT-LORA outperforms other PEFT methods, except for full FT, in average model accuracy across various GLUE task sets.\nB. SuperGLUE Experiments on LLaMA Family\nEncouraged by the outcomes observed with DeBERTa and ROBERTa models, we extended our experiments to include the larger-scale Large Language Model at Meta AI (LLaMA) [36] models. LLaMA is a series of larger-scale language models designed for various natural language understanding and generation tasks. In our experiments, we utilized LLaMA-2-7B and LLaMA-3-8B models to assess the effectiveness of our proposed PEFT method. We chose these LLaMA models due to their extensive parameter sets, ranging into the billions, which provide a rigorous test environment to evaluate how well our PEFT approach can compress and optimize these substantial models while maintaining comparable performance levels. For our experiments, we utilized the pre-trained LLaMA2-7b and LLaMA3-8b models available in HuggingFace Transformers library. We conducted a comparative analysis of TT-LORA against other baseline PEFT methods using the SuperGLUE benchmark tasks and the results are summarized in Table II. Similar to BERT family experiments, TT-LORA has been applied to $W_q$ and $W_v$ weight matrices of the self-attention module of LLaMA models and has been trained over multiple epochs, stopping the process if the validation loss did not improve for 5 consecutive epochs. The best model is then chosen from these runs based on the lowest observed validation loss. For reporting performance on the SuperGLUE benchmark tasks, we used F1 score for both CB and WSC tasks, and accuracy for both BoolQ and COPA tasks.\nTable II highlights TT-LORA's performance on the LLaMA2-7B model, where it consistently outperforms all other PEFT methods on CB and WSC tasks and all but LORETTArep on the BoolQ task. It achieves comparable performance on the COPA task and, on average, surpasses all competing PEFT methods. This higher model performance is achieved while attaining significant reductions in trainable parameters compared to our baselines by factors of approximately 67,384x (full fine-tuning, FT), 500x (Adapter), 41.9x (LoRAr=8), 13.1x (Prefix), 5.1x (LoRETTArep), and 8.8x (LORETTAadp).\nWe extended our experiment by FT the LLaMA3-8B model on the SuperGLUE benchmark and compared its performance against LoRA. As shown in Table II, TT-LORA either matched or exceeded the performance of LoRA across all tasks while achieving a remarkable reduction in trainable parameters, which is approximately 170.5\u00d7 fewer parameters compared to LoRA."}, {"title": "V. CONCLUSION", "content": "We introduced TT-LORA, a parameter-efficient fine-tuning approach that leverages tensor train decomposition to significantly reduce the number of trainable parameters. TT-LORA demonstrated substantial model compression and improved performance when fine-tuning BERT and LLaMA-based models across various tasks. Compared to other state-of-the-art PEFT methods, TT-LORA achieved higher or comparable average accuracy with a significantly smaller model size, underscoring its effectiveness in both parameter reduction and performance enhancement. For future work, we aim to extend TT-LORA to compress larger-scale models such as LLaMA3.1-405B, Grok 2.0, and Mistral Large. Additionally, we plan to explore the compression of additional layers within LLMs to achieve even greater levels of compression."}]}