{"title": "Activation by Interval-wise Dropout:\nA Simple Way to Prevent Neural Networks from Plasticity Loss", "authors": ["Sangyeon Park", "Isaac Han", "Seungwon Oh", "Kyung-Joong Kim"], "abstract": "Plasticity loss, a critical challenge in neural network training, limits a model's ability to adapt to new tasks or shifts in data distribution. This paper introduces AID (Activation by Interval-wise Dropout), a novel method inspired by Dropout, designed to address plasticity loss. Unlike Dropout, AID generates subnetworks by applying Dropout with different probabilities on each preactivation interval. Theoretical analysis reveals that AID regularizes the network, promoting behavior analogous to that of deep linear networks, which do not suffer from plasticity loss. We validate the effectiveness of AID in maintaining plasticity across various benchmarks, including continual learning tasks on standard image classification datasets such as CIFAR10, CIFAR100, and TinyImageNet. Furthermore, we show that AID enhances reinforcement learning performance in the Arcade Learning Environment benchmark.", "sections": [{"title": "1. Introduction", "content": "Loss of plasticity refers to the phenomenon in which a neural network loses its ability to learn, which has been reported in recent studies (Lyle et al., 2023; Dohare et al., 2021). This phenomenon has been observed in models pre-trained on datasets where labels are assigned randomly or input images are randomly permuted. When trained on a new task, these models exhibit degraded learning capabilities than freshly initialized networks. Loss of plasticity is a critical issue that arises across various domains, making it an essential problem to address. There have been evidence that plasticity loss is caused by non-stationarity (Lyle et al., 2024). While these results are from supervised domain like continual learning, plasticity loss also has been observed in the reinforcement learning domain, caused by its inherent non-stationarity (Sokar et al., 2023; Kumar et al., 2020). However, recent study suggest that this phenomenon is prevalent not only in non-stationary domains, but also in stationary domains (Lee et al., 2024b).\nSeveral factors have been proposed as causes of plasticity loss. Earlier studies have identified issues such as dead neurons (Sokar et al., 2023) and large weight norms (Lyle et al., 2023) as potential contributors to this phenomenon. More recent research suggests that non-stationary serves as the primary trigger for plasticity loss, subsequently leading to effects like dead neurons, linearized units, and large weight norms, which constrain the network's capability and destabilize training (Lyle et al., 2024). Other factors have also been implicated, including feature rank (Kumar et al., 2020), the sharpness of the loss landscape (Lyle et al., 2023), and noisy features (Shin et al.).\nBased on these insights, various approaches have been proposed to address plasticity loss. These include adding penalty terms to regularize weights (Kumar et al., 2023;\nGogianu et al., 2021), resetting dead or low-utility neurons (Dohare et al., 2021; 2023; Sokar et al., 2023), shrinking the network to inject randomness (Ash & Adams, 2020; Shin et al.), introducing novel architectures (Lee et al., 2024b) or novel activations(Abbas et al., 2023; Lewandowski et al.,"}, {"title": "2. Related Works", "content": "In recent years, various methods have been proposed to address plasticity loss. Several works have focused on maintaining active units (Abbas et al., 2023; Elsayed & Mahmood, 2024) or re-initializing dead units (Sokar et al., 2023; Dohare et al., 2024). Other studies have explored limiting deviations from the initial statistics of model parameters (Kumar et al., 2023; Lewandowski et al., 2023; Elsayed et al., 2024). Additionally, some methods rely on architectural modifications (Nikishin et al., 2024; Lee et al., 2024b; Lewandowski et al., 2024). Plasticity loss also occurs in the reinforcement learning due to its inherent non-stationary. Nikishin et al. (2022) proposed resetting the model, while Asadi et al. (2024) suggested resetting the optimizer state.\nAs noted by Berariu et al. (2021), loss of plasticity can be divided into two distinct aspects: a decreased ability of networks to minimize training loss on new data (trainability) and a decreased ability to generalize to unseen data (generalizability). While most previous works focused on trainability, Lee et al. (2024b) addressed generalizability loss. They demonstrated that plasticity loss also occurs under a stationary distribution, as in a warm-start learning scenario where the model is pretrained on a subset of the training data and then fine-tuned on the full dataset.\nMost existing studies have focused on only one of the following challenges: trainability, generalizability, or reinforcement learning. However, in this study, we validate our AID method across all three aspects, demonstrating its effectiveness in each scenario."}, {"title": "2.1. Plasticity in Neural Networks", "content": "In recent years, various methods have been proposed to address plasticity loss. Several works have focused on maintaining active units (Abbas et al., 2023; Elsayed & Mahmood, 2024) or re-initializing dead units (Sokar et al., 2023;\nDohare et al., 2024). Other studies have explored limiting deviations from the initial statistics of model parameters (Kumar et al., 2023; Lewandowski et al., 2023; Elsayed et al., 2024). Additionally, some methods rely on architectural modifications (Nikishin et al., 2024; Lee et al., 2024b;\nLewandowski et al., 2024). Plasticity loss also occurs in the reinforcement learning due to its inherent non-stationary. Nikishin et al. (2022) proposed resetting the model, while Asadi et al. (2024) suggested resetting the optimizer state.\nAs noted by Berariu et al. (2021), loss of plasticity can be divided into two distinct aspects: a decreased ability of networks to minimize training loss on new data (trainability) and a decreased ability to generalize to unseen data (generalizability). While most previous works focused on trainability, Lee et al. (2024b) addressed generalizability loss. They demonstrated that plasticity loss also occurs under a stationary distribution, as in a warm-start learning scenario where the model is pretrained on a subset of the training data and then fine-tuned on the full dataset."}, {"title": "2.2. Activation Function", "content": "Our AID method is a stochastic approach similar to Dropout while also functioning as an activation function. Therefore, we aim to discuss previously proposed probabilistic activation functions. Although the field of probabilistic activation functions has not seen extensive research, two noteworthy studies exist. The first is the Randomized ReLU (RReLU) function, introduced in the Kaggle NDSB Competition (Xu, 2015). The original ReLU function maps all negative values to zero, whereas RReLU maps negative values linearly based on a random slope. During testing, negative values are mapped using the mean of the slope distribution. Their experimental results suggest that RReLU effectively prevents overfitting. Another example of a probabilistic activation function is DropReLU (Liang et al., 2021). DropReLU randomly determines whether a node's activation is processed through a ReLU function or a linear function. The authors claim that DropReLU improves the generalization performance of neural networks. The fundamental distinction between these probabilistic activation functions and our method lies in the generality of our approach. Unlike simple probabilistic activation functions, our method encompasses techniques such as Dropout and ReLU, providing a more comprehensive framework.\nAnother related approach involves activation functions designed to address plasticity loss. (Abbas et al., 2023) proposed the Concatenated Rectified Linear Units (CReLU), which concatenates the outputs of the standard ReLU applied to the input and its negation. This structure prevents the occurrence of dead units, thereby improving plasticity. Additionally, trainable activation functions have also been shown to effectively mitigate plasticity loss in reinforcement learning (Delfosse et al.). Specifically, they introduced a trainable rational activation function that prevents value overfitting and overestimation in reinforcement learning."}, {"title": "3. Why Dropout is Ineffective for Maintaining\nPlasticity?", "content": "In this section, we investigate the ineffectiveness of Dropout in mitigating plasticity loss. Dropout operates by randomly deactivating specific nodes during training, effectively generating random subnetworks. This mechanism fosters an ensemble effect, where the network learns more generalized features across these subnetworks. However, our hypothesis suggests that this process does not address plasticity loss:"}, {"title": "3.2. Generalizability Perspective", "content": "To evaluate generalizability, we conducted a warm-start learning experiment inspired by Lee et al. (2024b). Specifically, we pre-trained a RESNET-18 model on 10% of the training data for 1,000 epochs before continuing training on the full dataset. We repeated this experiment for models trained with vanilla settings, Dropout, and AID. The results are shown in Figure 2 (right). Interestingly, Dropout seemed to improve generalizability, as both warm-start and cold-start models showed better performance. However, we argue that this improvement arises from enhanced model generalization rather than a mitigation of plasticity loss. The magnitude of improvement with Dropout was similar for warm-start (10.9%p) and cold-start (10.1%p) models. If Dropout were primarily addressing plasticity loss, we would expect disproportionately higher performance improvements in warm-start models, as cold-start models inherently maintain perfect plasticity. In contrast, the performance improvement with AID was significantly smaller (3.3%p compared to the vanilla model's 10.9%p). This observation suggests that AID effectively mitigates plasticity loss, as warm-start models trained with AID retain a higher degree of plasticity compared to those trained with Dropout."}, {"title": "4. Method", "content": ""}, {"title": "4.1. Notations", "content": "We denote \\(r(\\cdot)\\) to be a ReLU (Rectified Linear Unit) activation function, which means that \\(r(x) = \\text{max}(x, 0)\\) for input data \\(x\\). For ease of notation, we define a negative ReLU, \\(\\bar{r}(x) = \\text{min}(x, 0)\\). Furthermore, for a real number \\(a > 0\\), we define modified leaky ReLU, \\(r_a\\) as a function that scales the positive part of the input by \\(a\\), and the negative part by \\(1 - a\\), formally given as \\(r_a(x) = x + (a - \\frac{1}{2})|x|\\) (e.g. \\(r_1(x) = r(x)\\), \\(r_0(x) = \\bar{r}(x)\\), and \\(r_{1/2}(x) = x\\)). Note that, \\(r_a\\) differs from leaky ReLU, since its slope on the"}, {"title": "4.2. Activation by Interval-wise Dropout (AID)", "content": "Activation by Interval-wise Dropout (AID) applies different dropout probabilities to distinct intervals of pre-activation values, functioning as a non-linear activation. Specifically, given \\(k\\) predefined intervals and their corresponding dropout probabilities \\({p_1, p_2, ..., p_k}\\), AID generates \\(k\\) distinct dropout masks. Each mask is applied only to the values falling within its corresponding interval, while other intervals are unaffected by that mask. Formally, let the predefined intervals be \\({I_1, I_2, ..., I_k}\\), where \\(I_j = [l_j, u_j)\\) for \\(j = 1, 2, ..., k\\). The union of these intervals covers the entire range of real values, i.e., \\(\\cup_{j=1}^{k} I_j = \\mathbb{R}\\). Moreover, the intervals are disjoint, meaning there is no overlap between any two intervals, i.e., \\(I_i \\cap I_j = \\emptyset\\) for all \\(i \\neq j\\). AID operates as a non-linear activation function when the dropout probabilities vary across intervals, i.e., when \\(p_i \\neq p_j\\) for some \\(i \\neq j\\). If all dropout probabilities are identical \\((p_1 = p_2 = ... = p_k)\\), AID reduces to a standard dropout mechanism. The output of the AID mechanism for an input vector \\(x\\) is given by: \\(\\text{AID}(x) = x \\odot m\\). Where \\(m\\) is the dropout mask vector defined as: \\(m_i = 0\\) with probability \\(p_j\\) if \\(x_i \\in I_j\\), and \\(m_i = 1\\) otherwise. This interval-wise dropout mechanism enables the model to handle varying ranges of pre-activation values, effectively functioning as a nonlinear activation function while applying stochastic regularization with different probabilities across intervals.\nDuring the testing phase of traditional dropout, each weight is scaled by the probability \\(1 - p\\), which is the chance that a node is not dropped. Similarly, in AID, as each interval has a unique dropout probability, the pre-activation values within each interval \\(I_j\\) are scaled by \\(1 - p_j\\) for testing. The pseudo-code for AID is presented in Algorithm 1.\nAID generalizes concepts from ReLU, standard Dropout, and DropReLU. We provide a detailed discussion of these relationships in Appendix C.1. Since defining each interval and its corresponding dropout probability requires exploring a vast hyperparameter space, Algorithm 1 demands extensive hyperparameter tuning. To address this, we introduce a simplified version of AID in the next section."}, {"title": "4.3. Simplified version of AID", "content": "Section 4.2 shows that this methodology offers infinitely many possible configurations depending on the number and range of intervals, as well as the dropout probabilities. To reduce the hyperparameter search space and improve computational efficiency, we propose the Simplified AID. Specifically, we define \\(\\text{AID}_p(\\cdot)\\) as a function that applies a dropout rate of \\(1 - p\\) to positive and \\(p\\) to negative values. This definition is intuitive since \\(\\text{AID}_p\\) behaves like ReLU when \\(p = 1\\) and a linear network when \\(p = 0.5\\). Additionally, we will demonstrate that this simplified version has an effect that regularizes to linear network and an advantage when He initialization is applied, as discussed in Section 4.4. Next, we explore a property of \\(\\text{AID}_p\\) that are useful for its practical implementation.\n\\) is equivalent to applying\nProperty 1. Applying \\(\\text{AID}_p(\\cdot\nr(\\cdot)\\) with probability \\(p\\), and \\(\\bar{r}(\\cdot)\\) with probability \\(1 - p\\).\nWe provide a brief proof in Appendix A. Using Property 1, the implementation of AID becomes straightforward. An important consideration is whether scaling up is performed during training. Dropout layer with probability \\(p\\) scales the value by \\(p\\) at training phase, so it acts as an identity function during the test phase. However, since AID functions as a non-linear activation, we scale the value at test phase. Specifically, AID utilizes \\(r_a(\\cdot)|_{a=p}\\), which is the same as passing the average values from training phase. To clarify the process, we provide the pseudo-code for the AID"}, {"title": "4.4. Theoretical Analysis", "content": "Recent studies (Dohare et al., 2024; Lewandowski et al., 2024) have found that linear networks do not suffer from plasticity loss and provided theoretical insights into this phenomenon. This highlights that the properties of linear networks play a crucial role in resolving the issue of plasticity loss.\nFrom an intuitive perspective, AID shares properties with deep linear networks in that, it provides the possibility of linearly passing all pre-activation values and having gradients of 1. This behavior contrasts with activations such as ReLU, RReLU, and leaky ReLU, which tend to pass fewer or no values for negative pre-activations. Building on this intuition, the following theoretical analysis establishes that AID as a regularizer, effectively constraining the network to exhibit properties of a linear network.\nTheorem 4.1. For a 2-layer network, AID has a regularization effect that constrains the model to behave like a linear network. Formally, let weight matrices be \\(W_1, W_2 \\in \\mathbb{R}^{n \\times n}\\), the input and target vectors be \\(x, y \\in \\mathbb{R}^n\\), and the coefficient of AID be \\(p\\). Then, for learnable parameter \\(\\theta\\):\n\\[\\mathcal{L}_{\\text{AID}_p}(\\theta) = \\mathbb{E}[\\Vert W_2 \\text{AID}_p(W_1 x) - y \\Vert^2]\\]\n\\[ \\mathcal{L}_{r_p}(\\theta) = \\Vert W_2 r_p(W_1 x) - y \\Vert^2\\]\n\\[ R_p(\\theta) = \\Vert W_2 (W_1 x) - W_2 r_p(W_1 x) \\Vert^2\\]\n\\[ \\Rightarrow \\mathcal{L}_{\\text{AID}_p}(\\theta) \\ge \\mathcal{L}_{r_p}(\\theta) + \\frac{4p(1-p)}{\\pi (2p - 1)^2} R_p(\\theta).\\]\nWe provide the detailed proof on Appendix B, following the approach taken by Liang et al. (2021). Theorem 4.1 indicates that applying AID instead of the ReLU function injects linearity into the model, effectively mitigating plasticity loss. On the other hand, incorporating a dropout layer with ReLU does not yield the same effect, and the relevant lemma is provided in Appendix C.2.\nSince AID functions as an activation, it is important to examine its compatibility with existing network initialization methods designed for activations. Therefore, we show that AID ensures the same stability as the ReLU function when"}, {"title": "5. Experiments", "content": "In this section, we empirically evaluate the effectiveness of the proposed method across various tasks. First, we compare AID with previous methods in a range of continual learning tasks to assess its impact on mitigating plasticity loss in section 5.1. Second, we investigate whether AID provides tangible benefits in reinforcement learning in section 5.2. Finally, we demonstrate that AID improves generalization in standard classification tasks in section 5.3."}, {"title": "5.1. Continual Learning", "content": "We compare AID with various methods that were proposed to address the challenges of maintaining plasticity. In addition, Randomized ReLU and DropReLU are included due to their structural similarity to our method. Refer to Appendix E for the description of each baseline.\n\\) (Krogh & Hertz, 1991) and L2 regularization to initial value (L2 Init) (Kumar et al., 2023)"}, {"title": "5.1.1. TRAINABILITY", "content": "We start by validating the AID's ability to maintain trainability. Lee et al. (2024a) categorized the trainability of neural networks into two cases: Input trainability and Label trainability\u00b9. Input trainability denotes the adaptability to"}, {"title": "5.1.2. GENERALIZABILITY", "content": "In our earlier experiments, we confirmed that AID effectively maintains ability to adapt new data. However, while adaptability is important, it is even more critical to ensure that the model retain the ability to generalize well to unseen data. Therefore, generalizability is a critical aspect related to plasticity loss. We evaluated the generalizability of AID across three benchmark settings: continual-full, continual-limited, and class-incremental. For this evaluation, we utilized three datasets: CIFAR10, CIFAR100\nContinual Full & Limited We investigated two continual learning settings inspired by previous works (Lee et al., 2024b; Shen et al., 2024). In the continual full setting, an extended version of the warm-start framework, training progress is divided into 10 phases, with 10% of the dataset introduced at each phase while maintaining access to all previously seen data throughout training. In contrast, the continual limited setting imposes stricter constraints by restricting access to past data, reflecting practical challenges such as privacy concerns and storage limitations. In our experiments, we exclude L2 Init and ReDo, as they have been shown to have no improvement on generalizability, and instead include the recently proposed Direction-Aware"}, {"title": "Class-Incremental", "content": "In addition to these continual learning settings, we also conducted experiments on class-incremental Learning, inspired by previous works (Lewandowski et al., 2024; Dohare et al., 2024). Unlike the warm-start framework, this approach partitions the training process into 20 phases based on class labels, incrementally introducing new classes at each phase. Like continual full setting, the model is trained on all available data, including previously introduced class data. Further details regarding the experimental setup can be found in Appendix F.3.\nFigure 5 presents the difference in test accuracy between a model trained with full reset as each class is introduced. Methods such as Dropout and DASH\u2014initially demonstrate an accuracy advantage when full reset is applied during training. However, as training progresses, this advantage diminishes, highlighting their limitations in long-term plasticity retention. In contrast, AID stands out as the only method that consistently maintains its advantage over full reset throughout the training process. These results suggest that AID effectively mitigates plasticity loss across various experiments evaluating generalizability, reinforcing its ro-"}, {"title": "5.2. Reinforcement Learning", "content": "A high replay ratio is known to offer high sample efficiency in reinforcement learning. However, increasing the replay ratio often leads to overfitting to early data and results in plasticity loss, as highlighted in previous studies (Kumar et al., 2020; Nikishin et al., 2022). In this section, we demonstrate that using AID can effectively mitigate these challenges by maintaining plasticity while benefiting from high sample efficiency.\nThe experimental setup is as follows: Unlike standard DQN (Mnih et al., 2015) use RR = 0.25, we train with RR = 1 on several Atari (Bellemare et al., 2013) tasks, using the CleanRL environment (Huang et al., 2022). We use the 17 games to train and hyperparameter settings from Sokar et al. (2023). Due to the high computational cost associated with a high RR, we followed the settings of Sokar et al. (2023); Elsayed et al. (2024), where the models are trained for 10 millions frames."}, {"title": "5.3. Standard Supervised Learning", "content": "The preceding experiments have demonstrated that AID effectively addresses the loss of plasticity and exhibits its effectiveness in non-stationary problems. In this subsection, we investigate whether AID also provides advantages in classical deep learning scenarios without non-stationarity.\nFor comparison, we utilized L2 regularization and dropout, which are commonly employed in supervised learning, as baselines. The model was trained for a total of 200 epochs using the Adam optimizer with a learning rate of 0.001. The learning rate was reduced by a factor of 10 at the 100th and 150th epochs. The final test accuracy is shown in Table 1."}, {"title": "6. Conclusion", "content": "In this paper, we analyzed why Dropout fails to maintain plasticity. From this observation, we proposed AID (Activation by Interval-wise Dropout) and established through theoretical analysis that it acts as a regularizer by penalizing the difference between nonlinear and linear networks. Through extensive experiments, we demonstrated the effectiveness of AID across various continual learning tasks, showing its superiority in maintaining plasticity and enabling better performance over existing methods. Moreover, we validated its versatility by showing that AID performs well even in classical deep learning tasks, highlighting its generalizability across diverse scenarios.\nDespite its effectiveness, several limitations remain in the current implementation of AID. Our experiments primarily focused on configurations with simplified version, leaving the potential impact of more complex configurations underexplored. Future work will address this limitation by optimizing AID's scalability, exploring diverse configurations and adapting it to new architectures."}, {"title": "Impact Statement", "content": "The proposed method, Activation by Interval-wise Dropout (AID), improves neural network adaptability by mitigating plasticity loss\u2014a critical challenge in continual and reinforcement learning. This enhancement enables better generalization in dynamic environments, making AID applicable across both domains. These advancements contribute to more robust AI systems capable of efficiently handling real-world tasks, such as autonomous systems and personalized user experiences."}, {"title": "A. Proof of Property 1", "content": "Proof. Consider an input vector \\(x = (x_1, x_2,...,x_n) \\in \\mathbb{R}^n\\). After applying \\(\\text{AID}_p\\) to \\(x\\), the \\(i\\)-th element is expressed as:\n\\begin{cases} p_i x_i & x_i \\geq 0, \\\\ (1-p_i) x_i & x_i < 0, \\end{cases}\nwhere \\(p_i\\) is a random variable sampled from \\(\\text{Ber}(p)\\).\nTo enable vectorized computation, we introduce a mask matrix \\(M \\in \\mathbb{R}^{n \\times n}\\), satisfying \\(\\text{AID}_p(x) = Mx\\). Specifically, the diagonal elements \\(m_{(i,i)}\\) of \\(M\\) are sampled as follows:\n\\[m_{(i,i)} \\sim \\text{Ber}((2p - 1)\\mathbb{H}(x_i) + (1 - p)),\\]\nwhere \\(\\mathbb{H}(\\cdot)\\) is the step function, mapping positive values to 1 and negative values to 0. Then, the matrix \\(M\\) can be written as:\n\\[M = (2P - I)\\mathbb{H}(x) + (I - P)\\]\nwhere \\(I\\) is the identity matrix of size \\(n \\times n\\) and \\(P = \\text{diag}((p_1, p_2, ..., p_n))\\), where \\(\\text{diag}(\\cdot)\\) is the function that creates a matrix whose diagonal elements are given by the corresponding vector.\nUsing equation 1, the AID operation can be expanded as:\n\\[ \\text{AID}_p(x) = Mx \\\\ = [(2P - I)\\mathbb{H}(x) + (I - P)]x \\\\ = (2P - I)r(x) + (I - P)x \\\\ = (2P - I)r(x) + (I - P)(x - r(x) + r(x)) \\\\ = Pr(x) + (I - P)(x - r(x)) \\\\ = Pr(x) + (I - P)\\bar{r}(x) \\\\ = [Pr + (I - P)\\bar{r}]x.\\]\nThus, applying AID is equivalent to applying ReLU with probability \\(p\\), and negative ReLU with probability \\(1 - p\\)."}, {"title": "B. Proof of Theorem 4.1", "content": "Proof. We clarify that we follow a similar procedure to the one demonstrated during the proof by Liang et al. (2021), which employs a comparable methodology.\nLet the \\(m\\) input data points and true labels be \\(x_1, x_2, ..., x_m\\) and \\(y_1, y_2, ..., y_m\\) (\\(x_i, y_i \\in \\mathbb{R}^n\\)). For simplicity, let weight matrices be \\(W_1, W_2 \\in \\mathbb{R}^{n \\times n}\\) and they do not have bias vectors. If we assume that this model uses \\(\\text{AID}_p\\), using notation in Equation (2), the prediction of the model \\(\\hat{y}\\) can be expressed as below:\n\\[\\hat{y}_i = W_2 (I - P + (2P - I)r) (W_1 x_i)\\]\nFor testing, using \\(\\mathbb{E}[P] = pI\\), we get \\(\\hat{y}_i = W_2((1 - p)I + (2p - 1)r)(W_1 x_i)\\). Here, \\(I, P, r(\\cdot)\\) and \\(r_p(\\cdot)\\) refer to the identity matrix, Bernoulli mask, ReLU and modified leaky ReLU function, respectively, as mentioned in Property 1.\nThen, the training process can be formally described as below:\n\\[\\min_{W_1, W_2} \\mathbb{E} \\sum_{i=1}^m [\\Vert W_2 (I - P + (2P - I)r) (W_1 x_i) - y_i \\Vert^2]\\]\nFor simplicity, the input vector and label will be denoted as \\((x, y)\\) in the following proof. Let \\(D = \\text{diag}((W_1 x > 0))\\) be the diagonal matrix, where \\((W_1 x > 0)\\) is 0-1 mask vector, which means \\(r(W_1 x) = DW_1 x\\). To reduce the complexity of"}, {"title": "C. Additional Studies", "content": ""}, {"title": "C.1. Exploration of AID in Relation to ReLU, Dropout, and DropReLU", "content": "We explore that when the number of the interval is two around zero (i.e., \\(k = 2\\) and \\(u_1 = l_2 = 0\\)), AID encompasses several well-known concepts, including ReLU activation, Dropout, and DropReLU. This relationship is formalized in the following property:"}, {"title": "C.2. Effect of Theorem 4.1 for ReLU and Standard Dropout", "content": "We analyze the changes in the formulation when applying a dropout layer to the ReLU function instead of AID. Our findings show that this approach does not achieve the same regularization effect toward a linear network as AID. We discuss this in Corollary C.1 below.\nCorollary C.1. Similar setting with Theorem 4.1, combination of ReLU activation and Dropout layer does not have effect that regularize to linear network.\nProof. We use same notation with Theorem 4.1. We can write the prediction of the model that use Dropout layer with probability \\(p\\) after ReLU activation, \\(y_i\\) as below:\n\\[\\hat{y}_i = W_2 \\big(\\frac{1}{1-p}(I - P)r(W_1 x)\\big)\\]\nwhere \\(r\\) is ReLU function. On test phase, \\(\\hat{y}_i = W_2r(W_1 x)\\). Then, to simplify the equations, let us denote \\(S, S_p\\) and \\(v\\) as follows:"}, {"title": "D. Details of Property 2", "content": "At the forward pass case, the property of the ReLU function used in the derivation of equations in He et al. (2015)'s work as follows:\n\\[\\mathbb{E}[(r(y))^2] = \\frac{1}{2} \\mathbb{E}[y^2] \\\\\n= \\frac{1}{2} (\\mathbb{E}[y^2] - \\mathbb{E}[y]^2) \\\\\n\\frac{1}{2} \\text{Var}(y)\\]\nwhere \\(r(\\cdot)\\) is ReLU activation, \\(y\\) represents pre-activation value which is a random variable from a zero-centered symmetric distribution. We show that substituting ReLU with AID in this equation yields the same result:\n\\[\\mathbb{E}[(\\text{AID}_p(y))^2] = p\\mathbb{E}[(r(y))^2] + (1 - p)\\mathbb{E}[(\\bar{r}(y))^2] \\\\\n= p \\times \\frac{1}{2} \\text{Var}(y) + (1 - p) \\times \\frac{1}{2} \\text{Var}(y) \\\\\n= \\frac{1}{2} \\text{Var}(y).\\]\nAdditionally, during the backward pass, He et al. (2015) utilize that derivative of ReLU function, \\(r'(y)\\), is zero or one with the same probabilities. We have same condition that \\(y\\) is a random variable from a zero-centered symmetric distribution. Applying this to AID, we can confirm that it yields the same result:"}, {"title": "E. Baselines", "content": "Dropout (Srivastava et al.", "as": "theta \\leftarrow (1 - \\lambda)\\theta + \\lambda \\theta_0\\).\nDASH (Shin et al.). Direction-Aware SHrinking (DASH) selectively shrinks weights based on their cosine similarity with the loss gradient, effectively retaining meaningful features while reducing noise memorization. This approach improves training efficiency and maintains plasticity, ensuring better generalization under stationary data distributions. We applied DASH between the stages of training for generalizability experiments.\nReDo (Sokar et al., 202"}]}