{"title": "A Computational Method for Measuring \u201cOpen Codes\u201d in Qualitative Analysis", "authors": ["John Chen", "Alexandros Lotsos", "Caiyi Wang", "Lexie Zhao", "Jessica Hullman", "Bruce L. Sherin", "Uri J. Wilensky", "Michael S. Horn"], "abstract": "Qualitative analysis is critical to understanding human datasets in many social science disciplines. Open coding is an inductive qualitative process that identifies and interprets \"open codes\" from datasets. Yet, meeting methodological expectations (such as \"as exhaustive as possible\") can be challenging. While many machine learning (ML)/generative AI (GAI) studies have attempted to support open coding, few have systematically measured or evaluated GAI outcomes, increasing potential bias risks. Building on Grounded Theory and Thematic Analysis theories, we present a computational method to measure and identify potential biases from \"open codes\" systematically. Instead of operationalizing human expert results as the \"ground truth,\" our method is built upon a team-based approach between human and machine coders. We experiment with two HCI datasets to establish this method's reliability by 1) comparing it with human analysis, and 2) analyzing its output stability. We present evidence-based suggestions and example workflows for ML/GAI to support open coding.", "sections": [{"title": "1 INTRODUCTION", "content": "Qualitative analysis is the process of systematically identifying, generating, and organizing concepts from data. It has been widely adopted in many social science disciplines (such as education, sociology, psychology, and medicine), as well as interdisciplinary areas such as human-computer interaction (HCI) to understand people's perceptions, feelings, and nuanced interactions with technology[2].\nHowever, qualitative methods can be challenging, time-consuming, and may lack transparency[9]. The problem is particularly acute during the process of open coding, where researchers inductively identify emergent codes from raw data without a preconceived coding scheme. As the first step of qualitative analysis, practitioners and theorists of thematic analysis (TA)[10, 63] and grounded theory (GT)[23] make frequent use of open coding and agree on its goal:"}, {"title": "2 BACKGROUND", "content": "Qualitative analysis enables an in-depth exploration of human experiences by focusing on the nuanced interpretations, emotions, and subjective experiences that shape individual and social realities [52]. Thematic Analysis (TA) and Grounded Theory (GT), two commonly used qualitative methods in HCI research [1, 2, 9, 21, 48, 50, 53, 54, 56], both advocate for open coding as an essential first step to uncover the underlying reasons and processes that drive the formation and transformation of meaning [52, 62]. However, conducting open coding can be challenging, particularly when scaling up. Ambiguity [4, 9, 66], inconsistent terminology [12], and insufficient reporting [14] can all obscure how qualitative analysis was performed, making it harder to assess its quality [9, 49]. Few studies have attempted to measure or evaluate open coding results based on theoretical expectations, further compounding the challenge."}, {"title": "2.1 Open Qualitative Coding", "content": ""}, {"title": "2.1.1 Expectations of Open Coding", "content": "Open coding is a key approach in TA and GT that inductively derives codes from raw data. It can generate new coding schemes without or in addition to a theoretical framework [24, 62]. Researchers can then apply the coding scheme systematically on datasets through the deductive coding process [7]. With the capability of finding novel insights beyond existing knowledge, GT theorists require open coding [22, 24, 62], and TA theorists advocate for it[63]. Since subsequent analysis depends on the open coding results, without first establishing"}, {"title": "2.1.2 Evaluation of Open Coding", "content": "Deriving patterns directly from raw data without a predefined framework is inherently challenging [43]. This challenge is compounded by the underdevelopment of measurement for open coding and theorists' focus on researcher quality (qualifications, experience, and perspective) over research quality [23]. Some metrics, such as validity or credibility, are easier to judge by the data or keep track of [28]. Other metrics are more elusive: for example, how can we operationalize the expectations of inductive coding - being \"open\" and \"as exhaustive as possible\"? Even when coders identify some valid and credible codes, as Corbin and Strauss acknowledge, they are only some of the \"many plausible interpretations possible from data.\"\nSome scholars have attempted to adopt deductive coding metrics such as inter-rater reliability (IRR) in open coding [44]. Yet, IRR can only assess consistency between coders. As qualitative researchers may not have or reach the ground truth, even complete consistency cannot measure the \"openness\" or \"exhaustiveness\" of the codes. Most qualitative researchers who adopt non-positivist epistemological stances believe that [17]: post-positivism believes in objective truth, yet humans may only approximate it [29, 37]; interpretivism argues that truth is subjective and situated [64], while constructivism holds that truth is a human construction to understand reality [37, 46]. On the other hand, while a few scholars have advocated for positivist qualitative research and believe in ground truth [6, 70], it is unclear how such truth can be reliably identified and validated.\nGT theorists have explored another potential pathway of evaluation. Corbin and Strauss proposed two criteria related to inductive coding: 1) depth, which refers to the richness of descriptive detail that adds substance to findings, and 2) variation, which demonstrates the complexity of human life by incorporating diverse cases outside of dominant patterns. However, operationalizing these criteria can be difficult. Traditionally, GT theorists suggest working towards theoretical saturation, where no new interpretations emerge from additional data, and existing ones are all well-defined with sufficient variation [23]. Yet, in real-world research contexts, the logic for determining saturation is often inconsistent and subjective [2, 30, 58], as it is challenging to judge whether coding has truly become exhaustive [32], or has simply reached a convenient stopping point.\nWhile the absolute saturation may be impractical, TA [18] and GT theorists [24] advocate for a team-based approach. Analyzing data from multiple perspectives reduces the likelihood of missing key codes or over-interpreting data. By constantly comparing and contrasting codes from different individuals [18], researchers can open up the analysis to the scrutiny of other researchers [24], resulting in \"new insights [and] increased sensitivity\" while \"guarding against bias.\" Similarly, the GT-inspired general inductive approach [65] suggests merging independent coders' open coding"}, {"title": "2.2 Machine-Assisted Qualitative Analysis", "content": "Many fields (e.g., computer vision, data analytics, etc.) have widely used ML/GAI methods to label data. Qualitative coding, whether inductive or deductive, can also mechanically be seen as applying labels (i.e., codes) to a given piece of data. This section reviews two ML/GAI perspectives for machine-assisted qualitative coding - classification and generation. While classification naturally fits into deductive coding scenarios with established evaluation metrics, its potential for inductive coding is intrinsically limited. Generation, on the other hand, has started to pick up traction for inductive coding. Yet, few studies have attempted to evaluate them, resulting in significant risks for research rigor."}, {"title": "2.2.1 ML/GAI For Classification", "content": "From a computational perspective, most current work views machine-assisted qualitative coding as a classification task, where algorithms aim to produce codes as similarly to human researchers as possible. More recently, researchers have worked on three classification approaches:\n(1) Supervised ML methods that are trained on human-coded datasets. Liew et al., for example, approach machine-assisted qualitative coding as a \"multi-label classification task\" using SVMs trained on hand-coded datasets to aid in social science research. CoAICoder [31] similarly leveraged natural language understanding models trained on human coders' work to aid in collaborative qualitative coding.\n(2) Rule-based AI systems that extract text-matching rules from human coding results and apply them to more datasets. For example, [55] and [33] encode human coders' coding results into rules for AI to suggest codes in unseen data. The usage of human-readable and editable rules increases the transparency in machine coding.\n(3) LLM-based methods that instruct LLMs to label data with a predetermined codebook. For example, [71] used GPT-3 to label data with a fixed set of codes, thus supporting researchers in deductive coding. Despite the increased explainability, some evaluation studies have pointed to increased bias as a potential setback [3].\nClassification approaches are generally easier to scale and evaluate, making large-scale datasets more accessible. Given the objective for algorithms to produce human-like codes, researchers have adopted traditional ML and deductive coding metrics to evaluate algorithm outcomes. For example, Liew et al. used human coding as the ground truth and applied two common ML metrics: precision, whether the positive predictions of the model were accurate, and recall, how well the model could identify positive instances in the training set. In Xiao et al., GPT-3's performance was evaluated with the inter-rater reliability (IRR) with human coders' independent coding results.\nWhile the classification approach is more suitable for deductive coding, it has some intrinsic theoretical limitations for inductive coding. By positioning human coders' work as \"ground truth,\" classification limits its outputs to labels"}, {"title": "2.2.2 ML/GAI For Generation", "content": "Open coding, on the other hand, is a natural fit for generation tasks: the number and nature of underlying codes (or themes) are unknown before the inductive process. While some scholars attempt to evaluate the results by matching human expert codes [51, 73], such evaluation underutilizes GAI's potential, which could aid researchers in identifying novel open codes for theory building. Two approaches have been more studied:\n(1) Topic modeling, an unsupervised ML technique, identifies semantically similar word groups (topics) in text-based datasets. It has been used for inductive coding in various contexts, such as survey data [5], social media posts, and online discussions [57]. The resulting topics help researchers focus on key parts of the dataset and automate coding for future analysis. Despite recent efforts, the difficulty in interpreting and evaluating the results still limits its power [36, 60].\n(2) GAI models have been more recently adopted for open, inductive coding. By iteratively providing data pieces with relevant instructions (e.g., research questions, coding instructions, desired output format), LLMs can produce codes that humans find more interpretable and useful [26, 61]. However, they may still miss nuance and struggle with less linguistically straightforward themes [26, 39], while generating non-grounded results or operating at a coarser level of analysis [16, 72]. Careful prompt design and prompting strategies are essential to mitigate these limitations, yet few comparison studies exist [16, 61].\nRegardless of the method used, the lack of a \"ground truth\" and agreed-upon statistical metrics poses a greater challenge for evaluating open codes. While researchers can assess algorithms' output based on usefulness or explain-ability, the subjective measures are labor-intensive. Moreover, those measures may overlook the complexity of inductive coding. Even if all output codes seem useful or explainable, the algorithm may still miss critical codes without the evaluators' knowledge. Similar problems exist for De Paoli and Mathis's computational measurement, where the researcher sequentially feeds data pieces into gpt-3.5-turbo until no more codes are found: first, not all codes may uniformly exist among data pieces. If the LLM accidentally misses a code in the first interview, there is no guarantee that it will pick up again later; second, the LLM may be systematically biased to miss certain codes throughout the process.\nAs such, there is a pressing need to align theoretical expectations of qualitative analysis with practical evaluation mechanisms for machine-assisted inductive coding. Our work addresses this by introducing innovative, theory-informed computational methods for inductive coding and a novel computational approach for evaluating these results."}, {"title": "3 MEASURING INDUCTIVE CODES", "content": "Informed by literature (see 2.1.2), we developed a computational measurement that 1) aggregates open codes of multiple human/machine coders and 2) measures each against the aggregation. Using the method, we measured the outcomes of different ML/GAI approaches on two datasets and compared the first dataset's results with human evaluation. With human-Al collaboration, we explored the potential bias of ML/GAI coding approaches. Since part of our method involved generative AI, we validated its reliability with an output analysis."}, {"title": "3.1 The Conceptual Method", "content": "Following the suggestions of TA and GT, we adopt a team-based approach and measure individual coders' results against the aggregation of multiple coders. To achieve that, we proposed a conceptual structure, Code Space (CSP), to represent inductive codes produced by each individual. The sum of individual CSPs becomes an Aggregated Code Space (ACS). Using ACS as a reference for evaluation, we proposed four conceptual metrics (Coverage, Density, Novelty, and Divergence) to measure individual CSP's relative performance."}, {"title": "3.1.1 Code Spaces (CSP), Aggregated Code Spaces (ACS)", "content": "Assuming we need to measure sets of inductive codes Coding_Results. We define a Code Space (CSP) as a multi-dimensional conceptual space that covers multiple codes identified or interpreted from the underlying qualitative dataset, each representing a set of codes. Each code is represented by a network node and has multiple dimensions related to its conceptual nature.\nCombining multiple Code Spaces, we get a Aggregated Code Space (ACS) that covers all codes from all Coding_Results (Fig 1A). Similar but not equivalent codes are now connected by links and considered as Neighbors. The idea of ACS acknowledges that humans may not find all possible interpretations but could still build an \"aggregated\" set of codes to come closer. ACS is also a snapshot in time that documents researchers' current efforts. It can be used to look for convergence and divergence among the team members, supporting different stages of qualitative analysis. Convergence may help researchers determine the current consensus and indicate each code's visibility and importance. Divergence may reveal underlying biases, different focuses, or missed opportunities for researchers. For example, a recent study [45] critiques overreliance on consensus building and advocates for more attention to \"dissonances, disagreements, and differences.\""}, {"title": "3.1.2 Conceptualize the Evaluating Metrics", "content": "We propose four conceptual metrics to measure individual CSPs against their aggregation ACS (Fig 1B), which will be operationalized in later sections:\n\u2022 Coverage and Density: How much conceptual space does a CSP cover, and how many codes does it use to cover this space? Both TA and GT strive for \"richness\" of codes. In practice, researchers need to strive for breadth and depth. Breadth means covering as diverse sets of concepts as possible. Depth means the descriptive details that ensures the concept's meaningfulness and richness, thus supporting researchers' further analysis [11, 23]. Combining them, researchers may be able to capture findings with depth and variation, two criteria for evaluating GT studies [23], to the extent that \"nothing has been left out.\"[35] On the other hand, not all codes are of equal interest to researchers. Some concepts are more likely to be grounded in or relevant to the research question. The metrics must account for each concept's importance and weigh it accordingly.\n\u2022 Novelty: How much of the \"novel\" conceptual space does a CSP include? We define \"novel\" codes as ones that fewer than Novelty_Threshold CSPs have included. Most of the time, CSPs will not be (even close to) identical. Sometimes, a researcher brings in novel ideas or lenses that were missed by others; in other cases, a researcher could make a slip or inappropriately name a code. Identifying \"novel\" codes could help us identify either scenario and support human-human or human-AI collaboration.\n\u2022 Divergence: How far is a CSP's code distribution from the ACS? Suppose two CSPs, A and B, have the same coverage and density. However, A has more codes in common with most CSPs. B, on the other hand, has more codes in common with no one else (\"novel\" codes) or few in the group. To detect this at the macro level, we calculate each CSP's divergence as the separation from its probability distribution of codes to that of the ACS, using the latter as a \"ground truth.\" In other words, the closer a CSP concept distribution is to the ACS, the less \"divergent\" it is.\nSince our evaluation focuses more on the relative comparison between coding results, it does not directly address potential issues of groundedness, i.e., whether codes could be reasonably re-identified by another coder from the underlying data. However, our metrics do provide an indirect pathway for human-AI collaboration: by looking at the \"novel\" parts of CSP and/or codes with very few examples, we may be able to identify codes with potential groundedness issues more easily."}, {"title": "3.2 The Computational Method", "content": "The operationalization of our method starts from consolidating Coding_Results into an ACS for evaluation reference. Each Coding_Result should come from an individual machine or human coder. For Coding_Result to be treated as a CSP, each Code should have a Label, an optional list of Definitions, and a list of Examples (i.e. the pieces of data where the code was identified). In the output ACS, each Code will have a consolidated Label, Definition, a list of Examples, a list of Neighbors (other codes that are semantically close to this one), and a list of Owners (Coding_Results that have included one or more variations of the consolidated code). Neighbors enable the network structure of ACS"}, {"title": "3.2.1 Consolidating the ACS", "content": "Overall, consolidating an ACS involves multiple iterations of:\n(1) Finding equivalent codes and merge them into one.\n(2) Generate a new label and definition for merged codes.\n(3) Repeat the process with the new list of codes, until nothing more is merged.\nFinding equivalent codes is more complicated than it seems. Human or machine coders often use different phrases to describe the same or similar ideas. For example, suppose coder A identified \"user suggestion,\" while coder B found \"user suggestions.\" They are clearly referring to the same idea. Suppose coder B found \"user feedback\" instead. Are A and B referring to equivalent, similar, or different idea(s)? To determine that:\n(1) We encode each code with text embedding, transforming its Label and Definition(s) into a high-dimensional vector.\n(2) We use cosine distance, a commonly used text similarity measure, to calculate the distance between codes.\n(3) We use a hierarchical clustering algorithm to choose merging candidates, since it does not specify the expected number of results. For each node in the algorithm's dendrogram tree structure, we apply two input distance thresholds: lower and upper.\n(a) Different codes: Different codes with a distance above the upper threshold will never be merged.\n(b) Equivalent codes: Very similar codes with a distance below the lower threshold will always be merged.\n(c) For codes with a distance below the upper but above the lower, the algorithm penalizes 1) the proportion of non-overlapping examples; and 2) the size of unique examples after merging (compared with the average example sizes of all codes).\n(i) Also equivalent codes: If the distance is below the upper after the penalty, the codes will be merged.\n(ii) Similar codes: If this is the last iteration, those codes become Neighbors to each other."}, {"title": "3.2.2 Implementing the Consolidation", "content": "We implemented the consolidating process by merging closer codes first and farther codes later. The purpose is to help LLMs create less general labels and definitions. We will discuss the choice of text embedding models and parameters in 3.3.3.\n(1) We merge codes with exactly the same Label;\n(2) We iteratively merge codes with very similar Label ($upper = 0.35, lower = 0.35$). Since codes in this step are usually very similar, to optimize the token usage, we simply use the shorter one. If the input does not include a Definition, we generate one for it.\n(3) We iteratively merge codes with similar Label and Definition ($upper = 0.5, lower = 0.4$).\n(4) We iteratively merge codes with similar Label and Definition ($upper = 0.6, lower = 0.4$). During the last iteration, we consider all codes with distances under upper to be Neighbors."}, {"title": "3.2.3 Calculating the Metrics", "content": "We now calculate the metrics in 3.1.2 to measure individual CSPs against the consolidated ACS. To avoid CSPs with many redundant codes gaining an unfair advantage, whenever two or more codes are merged in the ACS, their CSP counterparts are also considered merged. Suppose coder A identified ten variations of the same concept \"user suggestion\", our method will treat them as only one code as long as they are detected and merged.\nFirst, we calculate the weight of each code using the sum of individual CSP's coverage value for this code. The coverage value is 1 for CSP that has the code, or a percentage based on how many neighbors the CSP includes compared with the total number of neighbors of the code. For example, if two CSPs covered the same code and the third CSP covered two out of four neighbors but not the code itself, the value would be 1, 1, 0.5. The weight would be 2.5."}, {"title": "3.3 Study Design", "content": "We conducted empirical experiments on two separate HCI datasets and research questions to demonstrate two use cases of our method. Since our measurement involves GAI in generating code labels and definitions, we validate its reliability by 1) comparing it with human evaluation results; and 2) analyzing its output stability."}, {"title": "3.3.1 Tasks and Datasets", "content": "We experimented with open coding results on two HCI datasets, each with its research question and context.\n(1) Physics Lab's online community dataset (127 messages from the beginning of the community) between designers and teacher users. The research question was: \"How did Physics Lab's online community emerge?\" Four human"}, {"title": "3.3.2 Choice of ML/GAI Coding Approaches", "content": "In Case 1, we replicated five published ML/GAI approaches on open coding with both datasets [19]. We provided the same research question and dataset information for humans and machines. We assigned the same roles and tasks to machine coders, such as \"You are an expert in thematic analysis with grounded theory, working on open coding.\u201d The exact prompts, parameters and Dataset 1's sample outputs can be found in [19], or supplementary materials of this paper. Here, we provide an overview of the five approaches:\n(1) BERTopic + LLM uses topic modeling [68], an unsupervised ML technique to identify groups of semantically similar words (i.e., topics) and explains the topics with LLMs. Since BERTopic's instruction was intended for general-purpose label-making, we gave additional instructions about the research question and contexts.\n(2) Chunk Level asks LLMs to identify open codes from chunks (e.g., a conversation, an interview, etc.) of data. Many variants of this approach have been adopted by recent papers (e.g., [41, 67]).\n(3) Chunk Level Structured asks LLMs to generate more than one level of concepts: the first for \"categories\" or \"themes,\" the second for \"codes\" or \"subcodes.\" Some recent papers have started to adopt this approach [19, 61].\n(4) Item Level asks LLMs to conduct line-by-line coding, as suggested by the grounded theory literature [34]. A few papers have adopted this approach to generate one [61] or multiple codes [19] per line.\n(5) Item Level with Verb Phrases builds on the previous approach but instructs LLMs to use verb phrases for labels explicitly. The design was inspired by a grounded theory literature [25] and reported by a recent study [19]."}, {"title": "3.3.3 Choice of Models and Hyperparameters", "content": "Our computational method utilizes text embedding and generation models during evaluation. For text embedding models, we consulted the MTEB leaderboard between May and June 2024 and experimented with a few alternatives. For this study, we used gecko-768 from Google due to its relatively high performance, low dimensions (with better computational efficiency), and easy accessibility.\nWe used cosine distance for thresholds in 3.2.2, where 0 means the two codes are identical; 1 for no correlation; and 2 for absolutely different. During our initial experiments, we set the thresholds of $lower = 0.4$ and $upper = 0.6$ based on two indicators: 1) we referred to the distribution of pairwise distances and located local turning points; and 2) whether the code pairs under the criteria meet our definition, where human can recognize codes closer than lower as the same idea, while codes further than upper are likely to be different. Different research contexts and text embedding models may require different parameters."}, {"title": "3.3.4 Experiment Design", "content": "We conducted two case studies to understand two research questions:\n\u2022 What can we learn about existing ML/GAI approaches for open coding?\n\u2022 Is our method statistically reliable enough to evaluate open codes from machine and human coders?\nCase 1 examines the five machine coding approaches using 1) the overall metrics for machine and human coders; and 2) the cluster-level metrics of relative coverage, a metric between -100% (completely undersampled) to +inf (extremely oversampled). Two human researchers independently interpreted the theme for each network cluster (see 3.2.3) based on its constituent codes and reconciled their differences into a single label."}, {"title": "3.4 Empirical Results", "content": ""}, {"title": "3.4.1 Case 1: Evaluating Machine Coding Approaches", "content": "Figure 2, 3 demonstrates coverage and divergence, two of the more stable metrics (see 3.4.3), for all machine and human coders in both datasets. We noted:\n\u2022 Similar to a previous human evaluation [19], we found that item-level coding approaches have higher coverage and lower divergence than chunk-level and BERTopic approaches.\n\u2022 While the aggregation of human researchers has higher coverage and lower divergence than any single machine coding approach in coverage and most in divergence, the aggregation of machine coders has higher coverage and lower divergence than each and all human researchers. Note that we do not claim that machines outperformed humans, and an ongoing study is currently examining the implications.\n\u2022 Note that the numbers of codes do not strictly correlate with the coverage metric. For example, in Dataset 1, the Item-Level Verb-Phrase approach found 282 raw codes with 79.1% coverage, while the aggregation of human coders found 340 raw codes with only 75.5%.\nWe further explored the potential biases of human and machine coders with cluster-level metrics. Tables 2, 3 presents clusters for both datasets' ACS and the relative coverage of each CSP. Clusters are sorted by the sum of their component codes' weights. Thus, clusters with more codes and higher consensus levels are listed first. Numbers higher than"}, {"title": "3.4.2 Case 2: Benchmarking Mainstream LLMs", "content": "In Figures 4 and 5, we applied our method to benchmark six mainstream LLMs in mid-2024. We noted:\n\u2022 Significant gaps in different models' coverage and divergence. In both datasets, we found GPT-40, Claude 3.5 Sonnet, and Llama3-70B significantly outperformed GPT-3.5-Turbo, Claude 3 Haiku, and Mixtral 8x22B. Without designating a ground truth, our methods roughly distinguish larger, more \"powerful\" models (measured by common ML benchmarks, e.g., MMLU) from smaller ones.\n\u2022 Machine coders' performances were not always consistent among the two datasets. For example, all models but Claude 3 Haiku have seen a performance drop in Dataset 2, with Claude 3.5 Sonnet dropping almost 12%.\n\u2022 Machine coders' aggregated result consistently achieved better coverage, density, and divergence than human coders' aggregated results, with coverages around 95% and significantly lower divergences in both datasets. Even when most individual models' performance dropped in Dataset 2, the performance of machine coders' aggregation almost stayed the same.\n\u2022 We also noted the performance of aggregated human coders dropping in Dataset 2. One potential reason: only 3 human coders conducted open coding for Dataset 2, compared with 4 for Dataset 2."}, {"title": "3.4.3 Reliability Study", "content": "Figures 6, 7 demonstrate the outcome variance of each metric across three LLMs on two datasets. We noted:\n\u2022 Overall, divergence consistently has the lowest standard variance (std, 2-3%) and coefficient of variance (cov, 1.5-5.5%), while novelty has the highest std (3-6%) and cov (8-65%)."}, {"title": "3.5 Discussions", "content": ""}, {"title": "3.5.1 Reliability of the Measurement", "content": "While using GAI in our method inevitably introduces stochasticity and potential biases, we established the reliability of our computational measurement with the reliability study (3.4.3)."}, {"title": "3.5.2 Suggestions for Using ML/GAI in Inductive Coding", "content": "Case 1 reaches similar conclusions as a previous human evaluation [19]: the item-level approaches performed best on both datasets and research questions. The cluster-level metrics enabled us to compare ML/GAI coding approaches with more nuances. BERTopic and chunk-level approaches had worse overall metrics, missed entire clusters of codes, and were less capable of identifying nuances of human interactions, particularly ones with seemingly less connection with the research question. They also oversampled positive emotions or feedback, implying a potential bias for their coding results.\nIn contrast, the success of item-level approaches suggests the potential of embedding human processes for qualitative analysis into LLM prompts. While we started mentioning grounded theory in the Chunk-Level Structured approach, its performance was not much better than the Chunk-Level. The situation changed when we instructed LLMs to strictly follow the grounded theory process [62]: by coding the data item-by-item, the item-level approaches produced much better results. Following existing literature [25], our adoption of verb phrases as labels further enabled a more nuanced interpretation of the data.\nCase 2 provides a quick assessment of state-of-the-art LLMs available in mid-2024. Our finding suggests that existing NLU benchmarks for LLMs may positively correlate with their performance on inductive qualitative coding. On the other hand, the same LLMs may perform differently on different research questions and datasets, necessitating broader evaluative studies. While we suggest researchers use the best available model, our findings also indicate the feasibility of powerful open-source models such as Llama-3 70B, particularly for scenarios where data privacy concerns are stronger. Moreover, we suggest using multiple models simultaneously as an even better approach. The combination of six models consistently covered almost all codes identified by human researchers, even when individual performances varied across datasets."}, {"title": "3.5.3 Human-Al Collaboration in Interpretation", "content": "This work's core contribution is a novel computational measurement to understand, compare, and evaluate open codes from multiple machine or human coders. Hence, it is crucial to share how we interpreted the algorithmic results and discuss scenarios where our measurement could contribute to human-AI collaboration in open coding processes.\nThe nature of qualitative research prevents us from making a general claim with only two datasets and research questions, yet this is more of a feature than a bug. Both the open codes and the corresponding evaluation are naturally bounded by the datasets, research questions, and perspectives researchers adopted. Thus, results from our measurement must be interpreted in context. Below, we present an example flow based on Table 2 on Dataset 1:"}]}