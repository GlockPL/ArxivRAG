{"title": "Hyperbolic Learning with Multimodal Large Language Models", "authors": ["Paolo Mandica", "Luca Franco", "Konstantinos Kallidromitis", "Suzanne Petryk", "Fabio Galasso"], "abstract": "Hyperbolic embeddings have demonstrated their effectiveness in capturing measures of uncertainty and hierarchical relationships across various deep-learning tasks, including image segmentation and active learning. However, their application in modern vision-language models (VLMs) has been limited. A notable exception is MERU, which leverages the hierarchical properties of hyperbolic space in the CLIP ViT-large model, consisting of hundreds of millions parameters. In our work, we address the challenges of scaling multi-modal hyperbolic models by orders of magnitude in terms of parameters (billions) and training complexity using the BLIP-2 architecture. Although hyperbolic embeddings offer potential insights into uncertainty not present in Euclidean embeddings, our analysis reveals that scaling these models is particularly difficult. We propose a novel training strategy for a hyperbolic version of BLIP-2, which allows to achieve comparable performance to its Euclidean counterpart, while maintaining stability throughout the training process and showing a meaningful indication of uncertainty with each embedding.", "sections": [{"title": "1 Introduction", "content": "To harness the recent advancements in language models across different modalities, modern vision-language models (VLMs) have evolved to combine visual processing with the reasoning capabilities of large language models (LLMS) [1, 25, 28, 41]. These integrated architectures enable reasoning tasks and encompass extensive world knowledge, facilitating capabilities such as zero-shot segmentation and improved generalization for images [24]. Models like BLIP-2 (Bootstrapping Language-Image Pre-training) combine language understanding with visual inputs through a two-stage pre-training approach [25]. In the first stage, BLIP-2 aligns visual features with textual descriptions. In the second stage, it fine-tunes this alignment for tasks such as image captioning and visual question answering. Despite these advancements, BLIP-2 still faces limitations,\nparticularly in representing complex hierarchical structures and relationships in the data. These limitations are crucial during image and text alignment and highlight the need for an uncertainty measure to ensure consistency between image and language representations.\nTo better understand the embedding space of an LLM, we compare its Euclidean and hyperbolic counterparts. Hyperbolic space provides a more efficient and compact representation of hierarchical structures due to its exponential scaling properties [11,30]. In contrast, Euclidean space grows linearly, making it challenging to represent hierarchical data with low distortion. Leveraging the hyperbolic space allows models to achieve an effective understanding of hierarchical data, leading to improved performance in tasks such as active learning [15], action recognition [16], and segmentation [3]. Additionally, hyperbolic embeddings contain an uncertainty measure represented by the distance from the center of the Poincar\u00e9 ball, a specific instance of a hyperbolic manifold. However, it has not yet been mathematically proven how this property is achieved. Some works [3,7,36] show that a lower radius is associated with higher uncertainty, and conversely at a high radius there are less uncertain samples. In contrast, other recent work [15] links radius to complexity or specifically epistemic uncertainty. In this case the situation is reversed; at low radius there are more generic and common classes, while at high radius more complex and specific classes. Although some work is still needed in this direction, utilizing this distance-based measure of uncertainty models can better assess and manage the confidence levels of their predictions, leading to a more reliable and interpretable grounding of vision and language.\nAlthough some studies [11,22] have explored the use of hyperbolic space in medium-sized vision-language models like CLIP [33], hyperbolic embeddings remain largely unexplored in large-scale modern vision-language models (VLMs) such as BLIP-2, which features 2.7 billion parameters and maps vision-based features into a shared multimodal space with text before passing them to the LLM. We propose combining BLIP-2 with hyperbolic embeddings to enhance its representational power, improving the model's ability to capture intricate relationships and perform complex reasoning, while also enabling the use of hierarchical data structures in multimodal learning. In this work, we demonstrate that traditional hyperbolic approaches, such as using Poincar\u00e9 distance as a similarity measure, fail when applied within BLIP-2. To overcome these limitations, we propose a stable hyperbolic training method that maintains performance with the Euclidean metrics on retrieval tasks while producing image embeddings with varying hyperbolic radii. Our key contributions are as follows:\nWe present, to our knowledge, the first large-scale hyperbolic VLM and show that hyperbolic embeddings can capture uncertainty information via their radii, while achieving performance comparable to the Euclidean baseline of BLIP-2;\nWe provide detailed quantitative and visual analyses of the different embedding spaces, offering insights into the operation of current VLMs and how to efficiently represent them in hyperbolic space."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Hyperbolic Representation Learning", "content": "Hyperbolic representation learning in deep neural networks gained momentum with [17], introducing hyperbolic counterparts for classical fully-connected layers, multinomial logistic regression, and RNNs. This approach later expanded to hyperbolic convolutional neural networks [34], hyperbolic graph neural networks [6,29], and hyperbolic attention networks [20]. Hyperbolic geometry's ability to encode hierarchies and tree-like structures [3,6,23,31,36,37] as well as the notion of uncertainty via embedding radius [7, 13, 14, 16] have been key motivations. Recent advancements include self-supervised learning in hyperbolic space [13, 15, 16, 36, 39], where hierarchy or uncertainty measure emerge without a direct supervision.\nTransposing models from Euclidean to hyperbolic is challenging due to the specific hyperparameter configurations required, particularly the curvature of the manifold, which is data-dependent [13,23]. Hyperbolic neural networks have been explored for encoding full images [5,35], single pixels [3,7,15], and text [12,37]. Combining different modalities is even more complex. [11] introduced MERU, a CLIP-like model to align text and image modalities. However, these models are still not comparable to large-scale and multi-purpose models like BLIP-2 [25], which leverage extensive pre-training and fine-tuning for tasks such as VQA and captioning."}, {"title": "2.2 Image-Text Alignment", "content": "Mapping images and text into the same latent space has been a useful technique within a range of multimodal tasks, such as cross-modal retrieval. It is most directly useful for cross-modal retrieval tasks. CLIP [33] is a popular model in this space, consisting of image and text encoders trained on large-scale data to embed images and text into a shared latent space, using contrastive learning. Recent VLMs have taken a different approach, training multimodal adapter layers between a vision encoder and LLM [10,25,28,40,43]. This aligns visual representations with the LLM input space. As part of training the multimodal adapter, BLIP-2 and InstructBLIP [10,25] additionally use an image-text contrastive loss similar to CLIP.\nIn our work, we explore a hyperbolic version of BLIP-2, presenting our findings and highlighting the challenges in leveraging contrastive learning for hyperbolic representations."}, {"title": "3 Background", "content": ""}, {"title": "3.1 Hyperbolic Geometry", "content": "Hyperbolic neural networks operate within hyperbolic space, characterized by a constant negative curvature. A commonly employed manifold in this context is"}, {"title": "3.2 BLIP-2", "content": "In BLIP-2, Li et al. [25] leverage pre-trained vision and language models to reach a strong multimodal understanding with minimal additional training. More specifically, it begins by extracting a visual embedding from an image $I$ using a pre-trained and frozen image encoder $M_{image}$. The embedding for image $I$ is thus computed as:\n$v = M_{image}(I)$\nNext, a relatively small adapter model, called a Q-Former (or Querying-Transformer) $M_{QF}$, is trained along with a projection layer $P$ to translate the"}, {"title": "4 Stabilizing Hyperbolic BLIP-2", "content": "In this section, we discuss challenges encountered in adopting standard approaches to hyperbolic contrastive learning and the solutions we implemented. Additionally, we introduce two novel strategies: Random Query Selection (RQS) and Random Text Pruning (RTP). These strategies aim to effectively utilize hyperbolic embeddings while enhancing model stability and performance in multimodal tasks."}, {"title": "4.1 Hyperbolic Image-Text Contrastive", "content": "We applied contrastive learning in hyperbolic space by first mapping the Euclidean feature vectors $u$ and $e$ relative to the text and image inputs into the Poincar\u00e9 ball using exponential mapping (see Eq. 3):\n$u_h = \\exp(u), e_h^j = \\exp(e^j) \\text{ for } j = 1, ..., N$\nThis transformation positions the embeddings $u_h, \\{e_h^j\\}_{j=1}^N$ within the hyperbolic space, preserving their relational structure.\nNext, we substitute the distance used in 9 with the hyperbolic metric, specifically the Poincar\u00e9 distance (see Eq. 5):\n$L_{hyper} (E_h, u_h) = \\min_{j=1,..., N} d_{poin}(e_h^j, u_h)$"}, {"title": "4.2 Random Query Selection", "content": "The baseline model utilizes the Q-Former architecture [25], which outputs 32 learned query embeddings $E_h$. These embeddings are designed to capture diverse semantic information from the image and align it with text. However, when computing the contrastive learning loss (e.g., Eq. 9, 11, 12), the model selects the most similar query embedding exclusively. This approach limits the utilization of the rich information stored in the remaining query tokens. As a result, the model tends to merge the different semantic information into a single query embedding, consistently selecting the same embedding for all inputs (c.f., Fig. 4).\nTo address this limitation, we introduce Random Query Selection (RQS) to diversify the semantics captured by the different query embeddings. Specifically, during training, the model selects the most similar query embedding to the reference text embedding with a probability $p$. With a probability $1-p$, the model selects a random query embedding instead:\n$L_{hyper-RQS} (E_h, u_h) = \\begin{cases}\\min_j d_{cosine}(e_h^j, u_h), \\text{ with prob} = p\\\\d_{cosine}(e_h^{\\hat{j}}, u_h), \\text{ with prob = 1-p}\\end{cases}$\nwhere $\\hat{j}$ is uniformly sampled in $\\{1, ..., N\\}$. This stochastic selection process encourages the model to distribute the semantic information across the 32 query embeddings, thereby enhancing the robustness and diversity of the learned representations."}, {"title": "4.3 Random Text Pruning", "content": "In our model, the hyperbolic embeddings $u_h$ associated with text inputs tend to migrate towards the edge of the Poincar\u00e9 ball. This results in uniform radii,"}, {"title": "5 Experiments", "content": "This section outlines the training pipeline for our models, analyzes the learned hyperbolic embeddings, details the evaluation metrics, and presents the results on downstream tasks."}, {"title": "5.1 Training Pipeline", "content": "The training pipeline follows the methodology described in BLIP-2, integrating vision-language pre-training with contrastive learning in hyperbolic space. The main components of the training pipeline are outlined below.\nDataset The trainings of our models were conducted on the COCO dataset [27]. COCO (Common Objects in Context) consists of over 200,000 images with a wide range of annotations for object detection, segmentation, and captioning. It captures a variety of everyday scenes with common objects in their natural contexts.\nModel Architecture: Our model architecture includes a frozen pre-trained state-of-the-art vision transformer, ViT-g/14 from CLIP [33], a frozen language model OPT [42], and the Q-former. The outputs of the vision and language encoders are projected into the Poincar\u00e9 ball for contrastive learning in the Q-former.\nModel Training: The model undergoes a two-stage pre-training process: 250k steps in the first stage and 80k steps in the second, following the same setup as BLIP-2. The batch size is 800 for the first stage and 512 for the second. Each image is resized to 224x224 with standard BLIP-2 augmentations. Pre-training is conducted on a single machine with 8 V100 (32GB) GPUs. Stage 1 takes 12 hours, while Stage 2 takes 10 hours. Both stages use the same set of\nhyperparameters: AdamW optimizer with $\\beta_1 = 0.9, \\beta_2 = 0.98$, and a weight decay of 0.05. The learning rate starts at 1e-6 with a 2000-step warmup to 1e-4, then decays following a cosine schedule to a minimum of 1e-5. After pre-training, the model is fine-tuned on the image captioning task."}, {"title": "5.2 Analysis", "content": "This section systematically examines the principal findings derived from our experiments and visualizations. Our objective was to establish the radius as an indicator of uncertainty. We provide a detailed analysis of the results obtained from employing conventional methodologies for hyperbolic representations, as mentioned in 4.1. Furthermore, we demonstrate the challenges encountered during this process and the corresponding countermeasures implemented to mitigate these issues."}, {"title": "6 Discussion", "content": "This study investigated the integration of hyperbolic embeddings in the large-scale BLIP-2 vision-language model, marking the first application of hyperbolic embeddings at this scale. While hyperbolic space theoretically offers advantages for capturing hierarchical relationships and uncertainty, we show that conventional approaches like Poincar\u00e9 distance face different challenges.\nOur hyperbolic training method achieved performance comparable or superior to Euclidean models while providing image embeddings with varying hyperbolic radii. Conventional hyperbolic approaches struggle both in performance and in capturing meaningful uncertainty. Our study demonstrates that it is possible to maintain or improve performance while incorporating a measure of uncertainty in the embeddings.\nMoreover, the newly introduced Random Query Selection and Random Text Pruning techniques are independent of the embedding space used, despite being inspired by challenges faced when adapting BLIP-2 to hyperbolic space. Consequently, these techniques are broadly applicable and beneficial for Large Multimodal Models in general.\nWith our research, we want to highlight the limitations and the findings to advance the understanding of hyperbolic embeddings in VLMs. Future efforts should aim to refine hyperbolic methods to overcome scalability challenges and fully realize their potential in enhancing vision-language models."}]}