{"title": "Hyperbolic Learning with Multimodal Large Language Models", "authors": ["Paolo Mandica", "Luca Franco", "Konstantinos Kallidromitis", "Suzanne Petryk", "Fabio Galasso"], "abstract": "Hyperbolic embeddings have demonstrated their effectiveness in capturing measures of uncertainty and hierarchical relationships across various deep-learning tasks, including image segmentation and active learning. However, their application in modern vision-language models (VLMs) has been limited. A notable exception is MERU, which leverages the hierarchical properties of hyperbolic space in the CLIP ViT-large model, consisting of hundreds of millions parameters. In our work, we address the challenges of scaling multi-modal hyperbolic models by orders of magnitude in terms of parameters (billions) and training complexity using the BLIP-2 architecture. Although hyperbolic embeddings offer potential insights into uncertainty not present in Euclidean embeddings, our analysis reveals that scaling these models is particularly difficult. We propose a novel training strategy for a hyperbolic version of BLIP-2, which allows to achieve comparable performance to its Euclidean counterpart, while maintaining stability throughout the training process and showing a meaningful indication of uncertainty with each embedding.", "sections": [{"title": "1 Introduction", "content": "To harness the recent advancements in language models across different modalities, modern vision-language models (VLMs) have evolved to combine visual processing with the reasoning capabilities of large language models (LLMS) [1, 25, 28, 41]. These integrated architectures enable reasoning tasks and encompass extensive world knowledge, facilitating capabilities such as zero-shot segmentation and improved generalization for images [24]. Models like BLIP-2 (Bootstrapping Language-Image Pre-training) combine language understanding with visual inputs through a two-stage pre-training approach [25]. In the first stage, BLIP-2 aligns visual features with textual descriptions. In the second stage, it fine-tunes this alignment for tasks such as image captioning and visual question answering. Despite these advancements, BLIP-2 still faces limitations, particularly in representing complex hierarchical structures and relationships in the data. These limitations are crucial during image and text alignment and highlight the need for an uncertainty measure to ensure consistency between image and language representations.\nTo better understand the embedding space of an LLM, we compare its Euclidean and hyperbolic counterparts. Hyperbolic space provides a more efficient and compact representation of hierarchical structures due to its exponential scaling properties [11,30]. In contrast, Euclidean space grows linearly, making it challenging to represent hierarchical data with low distortion. Leveraging the hyperbolic space allows models to achieve an effective understanding of hierarchical data, leading to improved performance in tasks such as active learning [15], action recognition [16], and segmentation [3]. Additionally, hyperbolic embeddings contain an uncertainty measure represented by the distance from the center of the Poincar\u00e9 ball, a specific instance of a hyperbolic manifold. However, it has not yet been mathematically proven how this property is achieved. Some works [3,7,36] show that a lower radius is associated with higher uncertainty, and conversely at a high radius there are less uncertain samples. In contrast, other recent work [15] links radius to complexity or specifically epistemic uncertainty. In this case the situation is reversed; at low radius there are more generic and common classes, while at high radius more complex and specific classes. Although some work is still needed in this direction, utilizing this distance-based measure of uncertainty models can better assess and manage the confidence levels of their predictions, leading to a more reliable and interpretable grounding of vision and language.\nAlthough some studies [11,22] have explored the use of hyperbolic space in medium-sized vision-language models like CLIP [33], hyperbolic embeddings remain largely unexplored in large-scale modern vision-language models (VLMs) such as BLIP-2, which features 2.7 billion parameters and maps vision-based features into a shared multimodal space with text before passing them to the LLM. We propose combining BLIP-2 with hyperbolic embeddings to enhance its representational power, improving the model's ability to capture intricate relationships and perform complex reasoning, while also enabling the use of hierarchical data structures in multimodal learning. In this work, we demonstrate that traditional hyperbolic approaches, such as using Poincar\u00e9 distance as a similarity measure, fail when applied within BLIP-2. To overcome these limitations, we propose a stable hyperbolic training method that maintains performance with the Euclidean metrics on retrieval tasks while producing image embeddings with varying hyperbolic radii. Our key contributions are as follows:\nWe present, to our knowledge, the first large-scale hyperbolic VLM and show that hyperbolic embeddings can capture uncertainty information via their radii, while achieving performance comparable to the Euclidean baseline of BLIP-2;\nWe provide detailed quantitative and visual analyses of the different embedding spaces, offering insights into the operation of current VLMs and how to efficiently represent them in hyperbolic space."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Hyperbolic Representation Learning", "content": "Hyperbolic representation learning in deep neural networks gained momentum with [17], introducing hyperbolic counterparts for classical fully-connected layers, multinomial logistic regression, and RNNs. This approach later expanded to hyperbolic convolutional neural networks [34], hyperbolic graph neural networks [6,29], and hyperbolic attention networks [20]. Hyperbolic geometry's ability to encode hierarchies and tree-like structures [3,6,23,31,36,37] as well as the notion of uncertainty via embedding radius [7, 13, 14, 16] have been key motivations. Recent advancements include self-supervised learning in hyperbolic space [13, 15, 16, 36, 39], where hierarchy or uncertainty measure emerge without a direct supervision.\nTransposing models from Euclidean to hyperbolic is challenging due to the specific hyperparameter configurations required, particularly the curvature of the manifold, which is data-dependent [13,23]. Hyperbolic neural networks have been explored for encoding full images [5,35], single pixels [3,7,15], and text [12,37]. Combining different modalities is even more complex. [11] introduced MERU, a CLIP-like model to align text and image modalities. However, these models are still not comparable to large-scale and multi-purpose models like BLIP-2 [25], which leverage extensive pre-training and fine-tuning for tasks such as VQA and captioning."}, {"title": "2.2 Image-Text Alignment", "content": "Mapping images and text into the same latent space has been a useful technique within a range of multimodal tasks, such as cross-modal retrieval. It is most directly useful for cross-modal retrieval tasks. CLIP [33] is a popular model in this space, consisting of image and text encoders trained on large-scale data to embed images and text into a shared latent space, using contrastive learning. Recent VLMs have taken a different approach, training multimodal adapter layers between a vision encoder and LLM [10,25,28,40,43]. This aligns visual representations with the LLM input space. As part of training the multimodal adapter,\nBLIP-2 and InstructBLIP [10,25] additionally use an image-text contrastive loss similar to CLIP.\nIn our work, we explore a hyperbolic version of BLIP-2, presenting our findings and highlighting the challenges in leveraging contrastive learning for hyperbolic representations."}, {"title": "3 Background", "content": null}, {"title": "3.1 Hyperbolic Geometry", "content": "Hyperbolic neural networks operate within hyperbolic space, characterized by a constant negative curvature. A commonly employed manifold in this context is the Poincar\u00e9 ball, selected due to its favorable geometric properties. The Poincar\u00e9 ball is defined as (DN, gc), with:\n$D^{N} = \\{x \\in \\mathbb{R}^{N} : c||x|| < 1\\}$ (1)\nwhere c is the curvature and $||\u00b7 ||$ denotes the standard Euclidean L2-norm. The associated Riemannian metric gc is given by:\n$g_{x}^{c} =  (\\frac{2}{1-c||x||^{2}})^{2} g^{E}$ (2)\nwhere $X = \\frac{2}{1-c||x||^{2}}$ is the conformal factor and $g^{E} = \\mathbb{R}^{N}$ is the Euclidean metric tensor.\nExponential Map and M\u00f6bius Addition In hyperbolic neural networks, a feature vector v \u2208 RN is firstly extracted in Euclidean space, and then it is projected into the Poincar\u00e9 ball using the exponential map:\n$exp^{c}_{x}(v) = x \\oplus c (\\frac{v}{\\sqrt{c}} tanh (\\sqrt{c} \\frac{||v||}{2}))$ (3)\nwhere x \u2208 D is the anchor, and $\\oplus c$ denotes the M\u00f6bius hyperbolic addition, defined for two hyperbolic vectors h, w as:\n$h \\oplus c w =  \\frac{(1 + 2c\\langle h, w \\rangle + c||w||^{2})h + (1 \u2212 c||h||^{2})w}{1 + 2c\\langle h, w\\rangle + c^{2}||h||^{2}||w||^{2}}$ (4)\nPoincar\u00e9 Distance and Hyperbolic Radius The Poincar\u00e9 distance dPoin between two hyperbolic vectors x, y \u2208 DN is defined as:\n$d_{Poin}(x, y) = \\frac{2}{\\sqrt{c}} tanh^{-1} (\\sqrt{c}|| \u2212 x \\oplus cy||)$ (5)\nWe define the hyperbolic radius of the embedding h\u2208 DN as the Poincar\u00e9 distance of h from the origin of the ball $d_{poin} (0, h)$."}, {"title": "3.2 BLIP-2", "content": "In BLIP-2, Li et al. [25] leverage pre-trained vision and language models to reach a strong multimodal understanding with minimal additional training. More specifically, it begins by extracting a visual embedding from an image I using a pre-trained and frozen image encoder Mimage. The embedding for image I is thus computed as:\n$v = M_{image} (I)$ (6)\nNext, a relatively small adapter model, called a Q-Former (or Querying- Transformer) MQF, is trained along with a projection layer P to translate the embedding into the input space of a pre-trained and frozen LLM Mtext (e.g., OPT [42]). The Q-Former is a transformer encoder that takes in a sequence of N fixed \"query\" vectors Q = [$q_{i}$]$_{i=1}^{N}$ that interact with the image embedding v via cross-attention. This produces output query embeddings E:\n$E = [e_{i}]_{i=1}^{N} = M_{QF}(V, Q)$ (7)\nIn parallel, a text encoder Mtext represents the caption T relative to the image.\n$u = M_{text}(T)$ (8)\nFor clarity, we will consider the text encoder Mtext as part of the Q-former. In BLIP-2, the text is aligned via cosine distance with the most similar query embedding in E.\n$L_{contr} (E, u) = \\min_{j=1,...,N} d_{cosine}(e_{j}, u) = \\min_{j=1,...,N} 1-  \\frac{e_{j}. u}{||e_{j}||||u||}$ (9)\nFinally, each embedding ei is projected by the linear layer P to match the dimensionality of the LLM input token embeddings. To generate text, the LLM prompt begins with the sequence of embeddings E, followed by the rest of the tokenized prompt."}, {"title": "4 Stabilizing Hyperbolic BLIP-2", "content": "In this section, we discuss challenges encountered in adopting standard approaches to hyperbolic contrastive learning and the solutions we implemented. Additionally, we introduce two novel strategies: Random Query Selection (RQS) and Random Text Pruning (RTP). These strategies aim to effectively utilize hyperbolic embeddings while enhancing model stability and performance in multimodal tasks."}, {"title": "4.1 Hyperbolic Image-Text Contrastive", "content": "We applied contrastive learning in hyperbolic space by first mapping the Euclidean feature vectors u and ei relative to the text and image inputs into the Poincar\u00e9 ball using exponential mapping (see Eq. 3):\n$u_{h} = exp(u), e^{h}_{j} = exp(e_{j}) \\; for \\; j = 1, ..., N$ (10)\nThis transformation positions the embeddings $u_{h},  [e^{h}_{j}]_{j=1}^{N}$ within the hyperbolic space, preserving their relational structure.\nNext, we substitute the distance used in 9 with the hyperbolic metric, specifically the Poincar\u00e9 distance (see Eq. 5):\n$L_{hyper}(E_{h}, u_{h}) =  \\min_{j=1,..., N} d_{poin}(e^{h}_{j}, u_{h})$ (11)"}, {"title": "4.2 Random Query Selection", "content": "The baseline model utilizes the Q-Former architecture [25], which outputs 32 learned query embeddings En. These embeddings are designed to capture diverse semantic information from the image and align it with text. However, when computing the contrastive learning loss (e.g., Eq. 9, 11, 12), the model selects the most similar query embedding exclusively. This approach limits the utilization of the rich information stored in the remaining query tokens. As a result, the model tends to merge the different semantic information into a single query embedding, consistently selecting the same embedding for all inputs (c.f., Fig. 4).\nTo address this limitation, we introduce Random Query Selection (RQS) to diversify the semantics captured by the different query embeddings. Specifically, during training, the model selects the most similar query embedding to the reference text embedding with a probability p. With a probability 1 p, the model selects a random query embedding instead:\n$L_{hyper-RQS}(E_{h}, u_{h}) =\\begin{cases}  \\min_{j} d_{cosine}(e^{h}_{j}, u_{h}), \\; with \\; prob = p \\\\ d_{cosine}(e^{h}_{j} , u_{h}), \\; with \\; prob = 1-p  \\end{cases}$ (13)\nwhere j is uniformly sampled in {1, ..., N}. This stochastic selection process encourages the model to distribute the semantic information across the 32 query embeddings, thereby enhancing the robustness and diversity of the learned representations."}, {"title": "4.3 Random Text Pruning", "content": "In our model, the hyperbolic embeddings uh associated with text inputs tend to migrate towards the edge of the Poincar\u00e9 ball. This results in uniform radii, rendering them ineffective as a proxy for uncertainty (c.f. Fig. 3 right in purple). To address this, we induce meaningful variations in text radii by introducing noise into the text inputs. This is expected to reflect different uncertainty levels through varying noise.\nTo achieve this, we implement Random Text Pruning (RTP), where a randomly selected segment of the input text T is pruned. Specifically, a window of 0 to 7 consecutive tokens is removed, introducing varying levels of noise. Through experimentation, we determined that a window length of 7 yields the best performance.\nBy applying RTP, we expect the embeddings of noisier texts to exhibit lower radii, aligning with the interpretation of hyperbolic representation where increased uncertainty corresponds to a reduced radius. This technique encourages the model to produce hyperbolic embeddings with radii that reflect the underlying uncertainty of the text inputs, thereby enhancing the expressiveness and utility of the hyperbolic space in representing text semantics."}, {"title": "5 Experiments", "content": "This section outlines the training pipeline for our models, analyzes the learned hyperbolic embeddings, details the evaluation metrics, and presents the results on downstream tasks."}, {"title": "5.1 Training Pipeline", "content": "The training pipeline follows the methodology described in BLIP-2, integrating vision-language pre-training with contrastive learning in hyperbolic space. The main components of the training pipeline are outlined below.\nDataset The trainings of our models were conducted on the COCO dataset [27]. COCO (Common Objects in Context) consists of over 200,000 images with a wide range of annotations for object detection, segmentation, and captioning. It captures a variety of everyday scenes with common objects in their natural contexts.\nModel Architecture: Our model architecture includes a frozen pre-trained state- of-the-art vision transformer, ViT-g/14 from CLIP [33], a frozen language model OPT [42], and the Q-former. The outputs of the vision and language encoders are projected into the Poincar\u00e9 ball for contrastive learning in the Q-former.\nModel Training: The model undergoes a two-stage pre-training process: 250k steps in the first stage and 80k steps in the second, following the same setup as BLIP-2. The batch size is 800 for the first stage and 512 for the second. Each image is resized to 224x224 with standard BLIP-2 augmentations. Pre- training is conducted on a single machine with 8 V100 (32GB) GPUs. Stage 1 takes 12 hours, while Stage 2 takes 10 hours. Both stages use the same set of hyperparameters: AdamW optimizer with \u03b2\u2081 = 0.9, \u03b22 = 0.98, and a weight decay of 0.05. The learning rate starts at 1e-6 with a 2000-step warmup to 1e-4, then decays following a cosine schedule to a minimum of 1e-5. After pre-training, the model is fine-tuned on the image captioning task."}, {"title": "5.2 Analysis", "content": "This section systematically examines the principal findings derived from our experiments and visualizations. Our objective was to establish the radius as an indicator of uncertainty. We provide a detailed analysis of the results obtained from employing conventional methodologies for hyperbolic representations, as mentioned in 4.1. Furthermore, we demonstrate the challenges encountered during this process and the corresponding countermeasures implemented to mitigate these issues.\nHyperbolic Radius as a Measure of Uncertainty In Figure 1, we visualize how the hyperbolic radius provides insights into its potential as a measure of uncertainty or complexity in the embeddings. Our experiments revealed that the radius can exhibit variability, hinting at the degree of uncertainty or the complexity of the embedded information. In fact, at a lower radius (see Figure 1 top), we observe \"people skiing\" where the majority of the image is dominated by background classes like snow, sky, or vegetation, while at a higher radius (see Figure 1 bottom), we have more complex scenarios with tables full of different objects."}, {"title": "Challenges in Contrastive Learning with Hyperbolic Space", "content": "Following the observations from [16], we found that applying contrastive learning in hyperbolic space can be problematic when utilizing both positive and negative samples, as in [8]. The Poincar\u00e9 distance, while effective for measuring geodesic lengths in hyperbolic space, introduces ambiguity when repulsing with negative samples, potentially biasing the embeddings towards the edge of the ball. This issue does not arise with positive-only samples [19], suggesting a more stable training dynamic in those scenarios.\nAnother critical challenge identified was the difficulty of fine-tuning the Q- former, pre-trained in the Euclidean manifold and then frozen, within hyperbolic space. The misalignment between the pre-trained Euclidean image encoder and the hyperbolic representation space caused significant performance issues, indicating the necessity for consistency in embedding space across training stages.\nHyperbolic Mapping and Similarity Measures Our findings in Table 1, indicate that hyperbolic mapping remains effective only when the dot product is used to compute similarity. When the Poincar\u00e9 distance is employed, the performance degrades, rendering it ineffective for the contrastive loss. However, similarities derived from the Poincar\u00e9 distance can be useful for negative sample selection in Image-Text Matching tasks, providing a nuanced approach to leveraging hyperbolic geometry."}, {"title": "Impact of Embedding Dimension on Radius and Performance", "content": "In Figure 3 (cyan plot), we observe that the hyperbolic radius tends to cap at its maximum level (the edge of the Poincar\u00e9 ball) when using high-dimensional embeddings (i.e., 256). Consequently, the radius does not affect the training, and we do not have a meaningful radius as an uncertainty proxy. We see a significant variation in the radius by reducing the embedding dimension (purple plot) to 16. Performance metrics show a notable decline when the embedding dimension was reduced to lower values (e.g., 16), while higher dimensions (e.g., 128) maintained comparable performance to Euclidean models. These results highlight the sensitivity of hyperbolic embeddings to dimensionality, affecting both the interpretability of the radius and the overall model performance.\nVariability of Image and Text Embeddings' Radii Figure 3 shows a distinct behavior between image (left) and text embeddings (right). Image embeddings exhibit variable radii, whereas text embeddings consistently stay at the edge of the Poincar\u00e9 ball. Feature clipping [21] is used in the current literature to help in training stabilization and to alleviate this effect. However, we do not observe any significant influence on the performance of the hyperbolic model or the hyperbolic radius. For this reason, we introduce noise through Random Text Pruning (c.f. Sec. 4.3), which causes slight variations in the text embeddings' radii (green plot), indicating a potential but limited ability to capture text uncertainty through hyperbolic geometry.\nRandom Query Selection The baseline model employs the Q-former architecture, which generates 32 learned query embeddings. These embeddings are intended to encapsulate diverse semantic information from the image and align it with text semantics. However, when calculating the contrastive learning loss with the text embedding, the model exclusively selects the most similar query embedding."}, {"title": "5.3 Evaluation Metrics", "content": "The performance of our model is evaluated using standard metrics for the downstream task of zero-shot image-text retrieval and image captioning.\nImage Retrieval @1 measures the percentage of times the correct image is retrieved as the top result when given a text query.\nText Retrieval @1 measures the percentage of times the correct text is retrieved as the top result when given an image query.\nBLEU (Bilingual Evaluation Understudy) [32] measures the precision of n-grams in the generated text that match n-grams in the reference text, and it is widely used for its strong correlation with human judgment.\nMETEOR (Metric for Evaluation of Translation with Explicit ORdering) [4] calculates a score based on the harmonic mean of unigram precision and recall, incorporating stemming and synonymy matching to correlate well with human judgment at the sentence level.\nCIDEr (Consensus-based Image Description Evaluation) [38] evaluates image captions by measuring the similarity of a generated sentence to a set of ground truth sentences, emphasizing consensus among multiple references.\nROUGE-L (Recall-Oriented Understudy for Gisting Evaluation) [26] uses the Longest Common Subsequence (LCS) between the generated text and the reference text to evaluate sentence-level structure similarity.\nSPICE (Semantic Propositional Image Caption Evaluation) [2] compares the semantic propositional content of the generated text with the reference text, focusing on the meaning conveyed by the captions for a nuanced assessment of semantic accuracy."}, {"title": "5.4 Experimental Results", "content": "To evaluate the contribution of various components in our model, we conduct a series of experiments. Specifically, we compare the performance of the Euclidean and hyperbolic baselines with models that integrate Random Query Selection (RQS) and Random Text Pruning (RTP), as described in Sec. 4. Additionally, we provide the results of the hyperbolic baseline model using the Poincar\u00e9 distance in contrastive learning. For the COCO zero-shot retrieval evaluation, we use the stage-1 pre-trained models, while for the image captioning evaluation, we employ the stage-2 models after fine-tuning them for the captioning downstream task. All models utilize a ViT-g vision encoder and, in the image captioning task, the OPT 2.7B language model. The results of these studies are summarized in Table 1. Below, we describe each setup and discuss its impact on the performance metrics.\nEuclidean Baseline The Euclidean baseline model utilizes the BLIP-2 configuration without any hyperbolic modifications. This model serves as a reference point, providing performance benchmarks for both retrieval and captioning tasks. It achieves solid performance in both zero-shot retrieval, with 69.6 for TR@1 and 53.5 for @ IR@1, and image captioning, with scores of 68.6 for BLEU-1, 30.1 for BLEU-4, 29.2 for METEOR, 54.0 for ROUGE-L, 110.7 for CIDEr, and 21.8 for SPICE.\nEuclidean RQS This model incorporates Random Query Selection (RQS) into the Euclidean baseline. The results demonstrate improvements in most metrics, with a notable increase to 72.1 (+2.5) for TR@1, 55.6 (+2.1) for IR@1, 77.3 (+8.7) for BLUE-1, 36.8 (+6.7) for BLUE-4, and 127.5 (+16.8) for CIDEr, highlighting the benefit of this approach.\nEuclidean RQS+RTP This model combines Euclidean Random Query selection (RQS) and Text Pruning (RTP) within the BLIP-2 baseline. The inclusion of RTP in combination with RQS further enhances performance, with scores such as 78.8 (+1.5) for BLEU-1, 37.7 (+0.9) for BLEU-4, 58.8 (+0.9) for ROUGE-L, and 129.8 (+2.3) for CIDEr, underscoring the effectiveness of RQS and RTP in the Euclidean space.\nHyperbolic Baseline The hyperbolic baseline model serves as the counterpart to the Euclidean baseline but operates within hyperbolic space. This setup allows us to directly compare the effectiveness of hyperbolic versus Euclidean representations in both retrieval and captioning tasks. The performance are slightly lower than the Euclidean counterpart. This is most probably due to the Euclidean pre-trained weights used to initialize the ViT encoder.\nHyperbolic baseline w/ Poincar\u00e9 distance This version of the hyperbolic baseline model uses the Poincar\u00e9 distance in the computation of the contrastive loss. This model shows a significant drop in performance across all metrics, with scores such as 30.0 for TR@1, 14.0 for BLEU-1, and 0.1 for CIDEr, indicating that using the Poincar\u00e9 distance with a model pre-trained in Euclidean space creates too much instability.\nHyperbolic RQS In this ablation, Random Query selection (RQS) is applied at training time using the hyperbolic model. The results show improvements compared to the hyperbolic baseline, with scores of 71.3 for TR@1, 54.2 for IR@1, and 128.8 for CIDEr, demonstrating the benefits of RQS in hyperbolic space and shortening the gap with the Euclidean RQS model.\nHyperbolic RTP This model incorporates Random Text Pruning (RTP) at training time. This setup focuses on pruning text from the captions to improve model robustness. The results indicate enhancements in various metrics compared to the hyperbolic baseline, such as 70.8 for TR@1, 95.2 for TR@10, 53.8 for IR@1, and 127.8 for CIDEr, but are still slightly lower than the Hyperbolic RQS model.\nHyperbolic RQS+RTP This configuration combines both Random Query selection (RQS) and Random Text Pruning (RTP) within the hyperbolic space. It achieves slighter lower performance on zero-shot retrieval compared to the Euclidean RQS+RTP counterpart, but significant improvements on the image captioning task, with scores of 79.3 (+0.5) for BLEU-1, 37.9 (+0.2) for BLEU-4, 31.0 (+1.1) for METEOR, 59.7 (+0.9) for ROUGE-L, 130.2 (+2.7) for CIDEr, and 23.3 (-0.1) for SPICE. These results highlight the combined effectiveness of RQS and RTP in enhancing the performance of the hyperbolic model."}, {"title": "6 Discussion", "content": "This study investigated the integration of hyperbolic embeddings in the large- scale BLIP-2 vision-language model, marking the first application of hyperbolic embeddings at this scale. While hyperbolic space theoretically offers advantages for capturing hierarchical relationships and uncertainty, we show that conventional approaches like Poincar\u00e9 distance face different challenges.\nOur hyperbolic training method achieved performance comparable or superior to Euclidean models while providing image embeddings with varying hyperbolic radii. Conventional hyperbolic approaches struggle both in performance and in capturing meaningful uncertainty. Our study demonstrates that it is possible to maintain or improve performance while incorporating a measure of uncertainty in the embeddings.\nMoreover, the newly introduced Random Query Selection and Random Text Pruning techniques are independent of the embedding space used, despite being inspired by challenges faced when adapting BLIP-2 to hyperbolic space. Consequently, these techniques are broadly applicable and beneficial for Large Multimodal Models in general.\nWith our research, we want to highlight the limitations and the findings to advance the understanding of hyperbolic embeddings in VLMs. Future efforts should aim to refine hyperbolic methods to overcome scalability challenges and fully realize their potential in enhancing vision-language models."}]}