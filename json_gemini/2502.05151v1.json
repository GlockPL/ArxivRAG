{"title": "Transforming Science with Large Language Models: A Survey on Al-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation", "authors": ["STEFFEN EGER", "YONG CAO", "JENNIFER D'SOUZA", "ANDREAS GEIGER", "CHRISTIAN GREISINGER", "STEPHANIE GROSS", "YUFANG HOU", "BRIGITTE KRENN", "ANNE LAUSCHER", "YIZHI LI", "CHENGHUA LIN", "NAFISE SADAT MOOSAVI", "WEI ZHAO", "TRISTAN MILLER"], "abstract": "With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new Al models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of \"AI4Science\".", "sections": [{"title": "1 Introduction", "content": "With the advent of large multimodal foundation models such as ChatGPT, Qwen, DeepSeek or Gemini, \u00b9 many research fields and sectors of everyday life now stand at the threshold of an AI-based technological transformation. Science is no exception. A recent study analyzed approximately 148,000 papers from 22 non-CS fields that cited large language models (LLMs) between 2018 and 2024, reporting a growing prevalence of LLMs in these disciplines [202]. Additionally, a very recent survey among almost 5000 researchers in more than 70 countries, by the American Publishing Company Wiley, suggests that AI adoption is embraced by a majority of researchers who think that AI will become mainstream in science within the next two years, despite current usages often limited to forms of writing assistance.2\nWhile science has traditionally relied on human ingenuity and labor in terms of coming up with research ideas and hypotheses, searching for the relevant literature, experimentation and then reporting of research results, recently, there has been a surge of Al models, tools, and functionalities promising to assist human scientists in these endeavors. This includes models like Elicit or ORKG ASK for search; models like The AI Scientist [170] for experimentation; and models like AutomaTikZ [18] and DeTikZify [19] for multimodal scientific content generation; there is even research investigating the degree to which these AI models are capable of evaluating the outcomes of the scientific process in terms of reviewing and assessing research papers [302]. All these models promise to vastly accelerate the scientific research cycle, ideally leading to unexpected new findings and a better, more user-friendly and more accurate documentation, referencing and reporting of research results. 3\nHowever, to our best knowledge, currently there exists no comprehensive survey on existing tools, models and functionalities as well as their limitations that could aid scientists in speeding up and improving the research cycle, except for specific subfields such as the social sciences [290] or branches of physics [310].4 In this survey, we fill this urgent gap by providing an overview over five central aspects of the research cycle where recently a plethora of new AI models has been proposed: search (e.g., for relevant literature) and content summarization in Section 4.1; scientific experimentation (e.g., coding) and research idea generation in Section 4.2; unimodal content generation such as drafting titles, abstracts, suggesting citations and helping in reformulation text-based material in Section 4.3; multimodal content generation and understanding such as generating and understanding figures, tables, slides and posters in Section 4.4; and AI-assisted peer review processes in Section 4.5. The recent survey of Wiley indicates that such an overview is highly important for AI researchers in their quest for better guidelines and support of tool (usages) for the scientific process, where \"63% [of respondents indicated] a lack of clear guidelines and consensus on what uses of AI are accepted in their field and/or the need for more training and skills\".\nWhen it comes to using AI tools for science, ethics is of overarching importance. This is because the AI tools exhibit various limitations, e.g., they (i) may hallucinate and fabricate content, (ii) exhibit bias, (iii) may have limited reasoning abilities and (iv) sometimes lack suitable evaluation, and (v) may have a large environmental footprint, among many other concerns such as risks of fake science, plagiarism, and lack of human authority. Indeed, the European Union has recently released guidelines on the responsible use of AI for science. In it, it points out that, while \"[r]esearch is one of the sectors that could be most significantly disrupted by generative AI\u201d where \u201cAI has great potential for accelerating scientific discovery and improving the effectiveness and pace of research and verification processes\", \"these [AI] tools"}, {"title": "2 Survey methodology", "content": "This article provides a high-level, disciplinarily contextualized survey of state-of-the-art applications of artificial intelligence to the practice of scientific research across all its stages, from the initial conception of research ideas to the dissemination of results. It is intended primarily to help researchers in fields within AI (natural language processing, computer vision, etc.) quickly familiarize themselves with the transdisciplinary foundations of and latest developments in this broad-ranging and rapidly evolving research area. Some of the material will also be useful to policymakers, practitioners, and research collaborators in adjacent fields, including human-computer interaction, library and information science, communication studies, metascience, science journalism, and research ethics. And although we aspire to present neither a comprehensive catalogue of production-ready tools nor a practical guide to their use, the examples we do cover could serve as a starting point for practising scientists in any field looking to incorporate (assistive) Al technologies into their own research workflows.\nWe believe our contribution to be timely because, despite the growing interest in the topic, its researchers are only just now coalescing into a community with dedicated dissemination venues. Recent examples include the workshops Natural Scientific Language Processing and Research Knowledge Graphs (NSLP) [215], Foundation Models for Science (FM4Science), AI & Scientific Discovery (AISD)7, and Towards a Knowledge-grounded Scientific Research Lifecycle (AI4Research), all of which held their first editions in 2024 or 2025. The few existing reviews of AI-for-science literature have addressed only isolated topics or application domains. The earliest examples (e.g., [68, 136]), now long out of date, tend to be organized into case studies of Al for specialized tasks such as equation or drug discovery. More recent surveys, such as [96], cover a wider variety of application domains but focus on a narrower sector of the scientific life cycle, and are pitched more towards the potential users of the AI tools than towards Al researchers aiming to understand and advance the underlying data sets, methodologies, and evaluation metrics.\nGiven our topic's wide scope, rapid progress, and dependence on knowledge and methods from different domains, we have opted to take a narrative approach to our survey. This methodology allows for greater freedom in the selection and"}, {"title": "3 Background", "content": "Throughout history, science has undergone a number of paradigm shifts, culminating in today's era of data-intensive exploration [99]. Although new tools and frameworks have accelerated the pace of scientific discovery, its basic steps have remained unchanged for centuries. As visualized in Fig. 1, these include (1) conception of a research question or problem, typically arising from a gap in disseminated knowledge; (2) collection and study of existing literature or data relevant to the problem; (3) formulation of a falsifiable hypothesis; (4) design and execution of experiments to test this hypothesis; (5) analysis and interpretation of the resulting data; and (6) reporting on the findings, allowing for their exploitation in real-world applications or as a source of knowledge for a further iteration of the scientific cycle.\nWith respect to the first two of these steps, a major challenge for any scholar is achieving, and then maintaining, sufficient familiarity with existing research on a given topic to be able to identify new research questions or to discover the knowledge required to answer them. Before the 20th century, it was often feasible to keep abreast of developments in a specialty simply by reading all the relevant books and journals as they were published. In modern times, however, the number of scientific publications has been doubling every 17 years [24], making this exhaustive approach unworkable. The need to sift through large quantities of scholarly knowledge spurred the specialization of simple library catalogs (in use since ancient times) into abstracting journals, bibliographic indexes, and citation indexes. By the 1960s and 1970s, many of these resources were being produced with standardized control principles and technologies, and could be queried interactively using automated information retrieval systems [23, pp. 88\u201391]. These technical developments have enabled the widespread adoption of more principled approaches to the exploration of scientific knowledge, such systematic reviews [32] and citation analysis [84].\nHow experts propose hypotheses to explain observed phenomena has been extensively discussed in the philosophy and psychology of science, albeit with little empirical work until relatively recently [50, 51]. Contrary to the idealized notion of scientific reasoning, hypotheses rarely come about solely through induction (i.e., the abstraction of a general principle from a set of empirical observations). Rather, case studies employing think-aloud protocols suggest that hypotheses are generated through a process of successive refinement. These processes may involve non-inductive heuristics (analogies, simplifications, imagistic reasoning, etc.) that often fail individually, but may lead to valid explanatory models after \"repeated cycles of generation, evaluation, and modification or rejection\" [50, 51].\nExperimentation and analysis aim to establish a causal relationship between the independent and dependent variables germane to a given scientific hypothesis. The metascientific literature abounds with practical advice on the design and execution of experiments, much of it discipline-specific. However, the general ideas at play can be traced to Ronald Fisher, whose seminal works on statistical methods [74] and experimental design [75] popularized the principles of"}, {"title": "4 Al support for individual topics/tasks", "content": "Al support for individual topics/tasks"}, {"title": "4.1 Literature search, summarization, and comparison", "content": "The rapid growth of scientific literature presents a significant challenge for researchers who need to search, analyze, and summarize vast amounts of information efficiently. AI-powered tools are transforming these tasks by leveraging natural language processing (NLP), machine learning (ML), large language models (LLMs), citation graphs, and knowledge graphs (KGs) to automate the retrieval, extraction, and summarization of scientific information. Unlike traditional search engines that rely on basic keyword matching, AI-enhanced systems provide context-aware, semantic search capabilities that retrieve more relevant and precise results. These systems not only assist in finding relevant papers but also offer structured summaries and comparative insights, helping researchers identify gaps, trends, and contradictions across multiple studies.\nBroadly, search systems for scientific literature can be categorized into six main types: (1) search engines, which retrieve documents based on keyword queries; (2) AI-enhanced search, which integrates NLP and ML for advanced, context-aware retrieval; (3) graph-based search, which leverages citation networks and knowledge graphs to explore relationships between papers; (4) paper chat, which enables interactive, conversational engagement with scientific content; (5) recommender systems, which suggest relevant research based on user preferences, citations, or topic modeling; and (6) benchmarks and leaderboards, which compare the performance of models on standardized datasets and metrics. In this section, we will discuss each of these search systems and provide an overview of the most popular tools including their key features. We will start with a review of scientific paper repositories which form the data source for most scientific search systems."}, {"title": "4.1.1 Data", "content": "Scientific search engines rely on vast and diverse publisher databases to provide comprehensive access to scholarly literature. These databases serve as the foundation for literature search, summary, and comparison, offering researchers access to a wide range of scientific outputs. Understanding the structure and types of scientific publisher repositories is crucial for assessing the coverage, reliability, and utility of search engines for evidence-based research.\nScientific publisher repositories can be categorized based on their access models, subject focus, and content types. Access-based repositories include open access repositories, which provide unrestricted access to research articles (e.g.,"}, {"title": "4.1.2 Methods and results", "content": "In the following, we survey current classical and AI-enhanced literature search, summarization and comparison systems and categorize them into six main categories based on their primary functionality.\nSearch engines. Traditional academic search engines such as Google Scholar, Semantic Scholar, Baidu Scholar, Science.gov, and BASE are characterized by their broad literature coverage, citation tracking capabilities, and keyword-based search functionality. Their primary advantages include extensive indexing of scholarly content, which involves"}, {"title": "4.1.3 Ethical concerns", "content": "The use of Al in scientific search, summarization and comparison raises ethical considerations, particularly in ensuring transparency, accountability, and equity. AI can significantly accelerate the pace of discovery, automate search tasks, and uncover patterns that may elude human researchers, but it also introduces risks such as perpetuating biases present in training data, undermining the integrity of scientific processes (e.g., authorship and credit assignment), and enabling the misuse of findings. Existing dynamics such as the Matthew effect, where well-known researchers receive disproportionate attention, might be reinforced by algorithms, intensifying inequalities. We believe that research should follow a human-centric approach, in which the human researcher is provided with advanced tools but remains fully responsible for executing the research and summarizing the results in research papers.\nIt is also important to develop algorithms to reduce biases by recommending relevant work to researchers based on the content of the research, independent of the popularity of the authors. Tools that are able to uncover gaps in the existing literature might even lead to a more uniform allocation of researchers to topics, reducing the bias towards overpopulated areas."}, {"title": "4.1.4 Domains of application", "content": "The search, summarization and comparison tools discussed in this section are general and apply to all fields of science. The presented benchmarks, however, are specific to the field of computer science in general and artificial intelligence in particular."}, {"title": "4.1.5 Limitations and future directions", "content": "Despite the significant advancements in Al-powered scholarly search systems, several limitations persist that hinder their full potential. One of the primary challenges is data quality and coverage gaps, as these systems often struggle with handling incomplete, non-standardized, or outdated data sources, which can lead to inaccuracies and inconsistencies in retrieved information. Additionally, bias in Al models remains a critical concern, where search and ranking algorithms may introduce biases based on training data, potentially influencing the visibility of certain research areas and limiting the diversity of perspectives presented to users. Another major limitation lies in scalability and real-time processing, as efficiently handling large-scale datasets while maintaining low latency and high retrieval accuracy remains a technical challenge. Addressing these limitations opens several promising future directions. One potential avenue is enhanced personalization which can be achieved by adapting search engines to user preferences, providing more tailored recommendations based on research interests and behavioral patterns.\nLastly, fostering interdisciplinary collaboration through the integration of AI-powered search systems with other digital tools, such as data visualization platforms and research management software, could facilitate more comprehensive and insightful research outcomes. Addressing these challenges and exploring future directions will be crucial for realizing the full potential of AI-driven scholarly search and synthesis."}, {"title": "4.1.6 Al use case", "content": "For this section, we have used Google Search, ChatGPT, NotebookLM and Scholar Inbox for retrieving relevant tools and related work. We have also used LLMs for grammar and spell checking purposes and for generating the code to format the tables."}, {"title": "4.2 Designing and conducting experiments; Al-based discovery", "content": "Hypothesis generation, idea formation, and experimentation are crucial steps in scientific discoveries. While hypothesis generation involves formulating specific and testable questions that inspire empirical or theoretical justifications, idea formation, particularly in AI, focuses on proposing new tools or benchmarking existing ones. Experimentation, in turn, tests hypotheses and evaluates ideas through systematic observation, data collection, and analysis. In AI, this often includes benchmarking models, running simulations, or conducting ablation studies. Traditionally, these processes are carried out by human researchers. However, in an age of rapidly growing scientific output, efforts of moving from literature review to hypothesis and idea formation have become very time-consuming for researchers of various disciplines, given their limited capability in reviewing increasing body of related work, which potentially hinders scientific progress. Experimentation introduces further challenges, such as designing methodologies, running large-scale simulations, and analyzing results. These complexities can slow down scientific progress, particularly when multiple iterations are required for refinement.\nRecently, there has been growing interest in the potential of LLMs to assist in hypothesis generation and idea formation, as they can efficiently process and synthesize vast amounts of literature. Beyond this, LLMs can be integrated with computational tools to enable automated experimentation where they assist in designing methodologies, conducting simulations, and analyzing results. The findings can then be used to iteratively refine hypotheses, creating feedback loops that could accelerate discovery. In this section, we will provide an overview of AI-driven hypothesis generation, idea formation and experimentation. The review of each will focus on its datasets, methods, results, limitations, ethical concerns, among others."}, {"title": "4.2.1 Data", "content": "In this section, we provide a diverse set of datasets for evaluating LLMs in hypothesis generation, idea formation and experimentation. These datasets are collected from various sources, and span multiple domains and time periods, as listed below:\n\u2022 SciMON [30] is a subset of the Semantic Scholar Open Research Corpus (S2ORC) [169] to focus on paper abstracts from ACL publications from 1952 to 2022. The dataset contains 135,814 paper abstracts, divided into training (before 2021), validation (2021), and test (2022) sets. Text preprocessing tools, such as PL-Marker [296] and the structure classifier [52], are leveraged to annotate the abstracts. Each abstract is segmented into sentences categorized as background, research ideas, and keywords.\n\u2022 IDEA Challenge [69] from 2022 contains 240 prototype entries and 1,049 connections from a design hackathon. It focuses on ideation and idea generation in prototyping, capturing how different prototypes are connected and can inspire new ideas. It aids in understanding how design thinking and innovation can evolve over iterative idea generation processes.\n\u2022 SPACE-IDEAS+ [83] focuses on detecting salient information in innovation ideas within the space industry. The dataset includes diverse text samples from informal, technical, academic, and business writings, aiming to help generate innovative ideas for space-related technologies and missions by analyzing the richness and potential of early-stage concepts.\n\u2022 TOMATO-Chem [294] contains 51 Chemistry and Material Science papers published in Nature or Science in 2024. These papers are annotated by several Chemistry PhD students. For each paper, the annotations include background, research questions, 2-3 related and important works that potentially inspires the paper, hypotheses and experiments for hypothesis justification. Additionally, 3,000 papers are provided as a reference pool for LLMs to select related works when generating hypotheses.\n\u2022 TOMATO [293] consists of 50 social science papers published in top journals after January 2023. The dataset covers diverse social science topics such as Communication, Human Resource Management, and Information Systems, among others. Data annotation in the dataset is completed by a social science PhD student. For each paper, relevant paper content is extracted by the student and categorized as main hypothesis, research background, and inspirations. Additionally, the dataset includes background and inspiration not only extracted from scientific papers but also web data such as Wikipedia. In this setup, LLMs are given the opportunity to generate hypotheses from web data.\n\u2022 LLM4BioHypoGen [208] consists of 2,900 medical publications sourced from PubMed. The data is divided into 2,500 papers for the training set and 200 papers for the validation set (both published before January 2023), with 200 papers in the test set (published after August 2023). Such a data split is to ensure that the test set is not exposed to LLMs during training. Each paper is annotated by employing GPT to generate background and hypothesis pairs. Human annotators are then leveraged to filter out low-quality pairs.\n\u2022 DevAI [319] is a benchmark comprising 55 realistic AI development tasks, each accompanied by hierarchical user requirements (365 in total). It is designed to facilitate the evaluation of agentic AI systems across various domains, including supervised learning, reinforcement learning, computer vision, natural language processing, and generative models.\n\u2022 ScienceAgentBench [44] is designed to evaluate the abilities of LLM agents in automating data-driven scientific discovery tasks. It comprises 102 tasks derived from 44 peer-reviewed publications across four disciplines: Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology & Cognitive Neuroscience. Each task requires an agent to generate a self-contained Python program based on a natural language instruction, a dataset, and optional expert-provided knowledge. The benchmark employs multiple evaluation metrics, including Valid Execution Rate, Success Rate, CodeBERTScore, and API Cost, to assess the generated programs' correctness, execution, and efficiency.\n\u2022 SWE-bench [114] evaluates the ability of LLMs to generate pull requests that resolve GitHub issues in real-world software engineering tasks. It is constructed from 2,294 tasks derived from 12 popular open-source Python repositories, filtering high-quality instances where merged pull requests resolve issues, contribute tests, and pass execution-based validation. Each task requires the model to edit a full codebase based on an issue description, producing a patch that must apply cleanly and pass fail-to-pass tests. The benchmark features long,"}, {"title": "4.2.2 Methods and results", "content": "Hypothesis Generation. Recently, there have been many works that leverage LLMs to generate hypotheses and ideas [10, 14, 30, 37, 115, 164, 165, 195, 207, 208, 256, 258, 271, 272, 288, 293, 294, 318]. These works differ in their use of LLMs for addressing various technique challenges, including (a) handling long context input due to the need for LLMs to analyze several related works, (b) strategies for refining LLMs to generate meaningful hypotheses, (c) lowering the possibility of generating hallucinated hypotheses and ideas.\n\u2022 Hallucinated hypotheses and ideas. Yang et al. [294] address the hallucination issue through a pipeline that starts with a search for related works. The identified related works and a given research question are provided as input for LLMs to generate hypotheses. The generated hypotheses are evaluated against ground-truth hypotheses published in Nature and Science. Their results show that many generated hypotheses exhibit a very high degree of similarity to the ground-truth ones. Xiong et al. [288] ground LLM-integrated formation of research hypotheses in scientific knowledge bases. Their approach incorporates external, structured data from knowledge graphs into the hypothesis formation process, and leverages the idea of structured reasoning to knowledge-grounded hypotheses, i.e., leveraging the search for evidence to mitigate hallucination during hypothesis formation.\n\u2022 Long context input. Chai et al. [30] focus on the efficient use of limited context size of LLMs. They introduce a selection mechanism that extracts important and relevant information from the literature and takes them as input for LLMs to generate hypotheses. Their results show that filtering out unnecessary information helps improve the quality of generated hypotheses.\n\u2022 Refinement strategies. Many works have explored strategies for refining LLMs to generate hypotheses. Major strategies include (a) few-shot learning, (b) fine-tuning on training data and (c) iterative refinement. For instance, Qi et al. [207] compare the impact of few-shot learning and fine-tuning on LLMs for hypothesis generation against zero-shot learning. Their results show that few-shot learning generates hypotheses that are judged by humans more testable than those generated in the zero-shot setup; while fine-tuning improves the overall quality of hypotheses, the improvement is limited to the domain of training data; in unseen domains, fine-tuning harms hypothesis quality, particularly the novelty aspect. Zhou et al. [318] iteratively refine hypotheses through reinforcement learning, with the aim of increasing the similarity between a given research problem and a generated hypothesis. This idea is further extended by Qi et al. [208] through a multi-agent framework to support the refinement process. They split the process of hypothesis generation into multiple tasks: hypothesis drafting, hypothesis evaluation and refinement, operated by various agents.\nAdditionally, Liu et al. [164] discuss data sources that LLMs rely on to generate hypotheses. For instance, scientific literature is provided as a source of model input for LLMs to generate hypotheses. Additionally, LLMs take as model input a paper review and generate testable hypotheses regarding whether the review is written by Generative AI. Yang et al. [293] propose to incorporate web data (e.g., scientific comments on social media) into the hypothesis generation process. This allows hypotheses to be informed by public opinions.\nIdea Generation. Research ideation has become a critical area where LLMs are increasingly being applied to enhance novelty and accelerate discovery. Several methods have been proposed to improve the creative capabilities of LLMs, often focusing on iterative refinement, multi-agent systems, human alignment and evaluation protocols [103, 146, 168, 171, 204, 210, 224, 236, 245].\n\u2022 Iterative refinement. Hu et al. [103] introduce an iterative planning and search framework aimed at enhancing the novelty and diversity of ideas generated by LLMs. By systematically retrieving external knowledge, the approach addresses the limitations of existing models in producing simplistic or repetitive suggestions. Experimental results show that their approach, Nova, generates more unique ideas compared to standard methods. Similarly, Pu et al. [204] focus on iterative refinement by providing literature-grounded feedback. Representing research ideas as nodes on a canvas, their approach, IdeaSynth, facilitates the iterative exploration and composition of idea facets, enabling to develop more detailed and diverse ideas, particularly at various stages"}, {"title": "4.2.3 Ethical concerns", "content": "The integration of AI into hypotheses generation, idea formation, and automated experimentation introduces significant ethical challenges. AI-generated hypotheses may lack transparency, making it difficult to assess their validity or underlying assumptions, which could lead to flawed experiments. For example, an AI might identify a statistical correlation in its training data and propose hypotheses without clearly revealing the underlying assumptions or data sources, making it difficult for researchers to verify its scientific soundness or hold anyone accountable if the hypotheses proves misleading. In the area of idea generation, there is a risk of reinforcing established research paradigms. Al systems trained on the basis of existing literature may favor popular paths and neglect underrepresented research directions. As a result, unconventional ideas may be unintentionally marginalized. For example, an AI might repeatedly suggest incremental improvements in a dominant field rather than proposing entirely new lines of research, thereby limiting the diversity of scientific thinking. Automated experimentation presents its own ethical challenges.\nThe speed and volume in which AI can design and execute experiments can lead to insufficient ethical oversight and inadequate safety controls. Consider an AI system that suggests experimental protocols in biomedical research (e.g., chemical components with unknown toxicity) without the rigorous human review needed to identify potential risks. This could lead to experiments that pose unforeseen dangers or violate established ethical standards."}, {"title": "4.2.4 Domains of application", "content": "The formation of research ideas and hypotheses requires domain-specific knowledge. Therefore, it is crucial to curate datasets separately for each domain, ensuring that datasets contain specialized research questions and relevant background that LLMs can take as input to generate domain-specific ideas and hypotheses. Regarding domains of interest, previous works have addressed idea and hypothesis in NLP, Engineering, Physics, Chemistry, Social Science and Medicine. Similarly, automated experimentation also relies on domain-specific datasets to guide the process of designing and testing experiments. In contrast, methods are typically domain-agnostic, though they are often developed and evaluated in specific domains. These methods address fundamental issues in LLMs, such"}, {"title": "4.2.5 Limitations and future directions", "content": "A large-scale study [236] comparing human researchers and LLMs finds that LLMs generate ideas judged to be more novel but slightly less feasible. The study highlights key challenges, such as limited diversity and self-evaluation failures, which constrain the broader adoption of LLMs in ideation workflows. Additionally, given that hypotheses and ideas are typically theoretical and cannot be validated without costly justification, it is unclear whether generated hypotheses and ideas are truly useful and lead to new scientific discoveries. Furthermore, LLMs only take as model input a research question and several related works without due diligence through data, and therefore generated hypotheses and ideas are often too general and lack methodological details [294]. Moreover, LLMs may generate recently discovered hypotheses and ideas, as they lack access to recent scientific papers [165]. Their outputs are very sensitive to the framing of input prompts [195]. Future work should focus on improving feasibility and diversity of hypotheses and ideas, incorporating real-time scientific papers, refining hypotheses and idea generation through experimentation, among others.\nAutomated experimentation with LLMs faces several additional challenges. First, LLMs often make critical errors, such as hallucinating results or outputting invalid references, which disrupt the precise steps required for experimental workflows. Furthermore, LLMs may generate hypotheses that are overly complex or difficult to implement, limiting their practical application. Additionally, while LLMs can provide theoretical insights, they currently lack the ability to generate figures or visual representations, which are often crucial for experimental validation [172]. Another significant limitation is that LLMs struggle to integrate and align different modalities, such as video, audio, or sensory data, which are increasingly essential in modern scientific experimentation. Moreover, LLMs lack the critical analysis capabilities necessary to identify flaws or refine hypotheses during experimentation. In highly specialized scientific domains such as biology and chemistry, they may also struggle with precise reasoning and tool usage, which are vital for ensuring experimental success [213]. Future work should focus on enhancing LLMs' capabilities in multimodal data processing, enabling the generation of figures, and improving their ability to refine hypotheses and ideas through iterative feedback during experiments."}, {"title": "4.2.6 Al use case", "content": "In order to showcase how AI assists in research workflows, we deliberately collaborated with LLMs in writing some parts of this section, and outlined our responses to model outputs. We experimented with several closed-ended tasks. For instance, we used ChatGPT to create a table with data sample provided in the text format. While we did not specify how the table columns should align with the data, ChatGPT matches them correctly. We accepted the model output with minor changes in the table formatting. Furthermore, we used ChatGPT, which takes dataset details as model input, to generate an introductory paragraph for the data section. We found that the initial model output was verbose and lacked several details, however, it can be improved when step-by-step instructions are provided. Additionally, we leveraged ChatGPT to constrastively summarize the papers on idea generation, where paper abstracts are taken as model input, and we revised the model output."}, {"title": "4.3 Text-based content generation", "content": "Under text-based content generation for science, we subsume different tasks generating specific text-based subparts of a scientific paper, such as automatically generating (i) the title, (ii) the abstract, (iii) the related work section, as well as (iv) citation generation. Also, proof-reading and paraphrasing using an Al system, as well as press release generation, will be discussed."}, {"title": "4.3.1 Data", "content": "Open access research articles are a valuable data source for text-based content generation. These include scientific publisher repositories offering at least some open access content (e.g., Nature portfolio, Taylor & Francis) as well as preprint repositories (e.g., arXiv, bioRxiv). These open access repositories can be leveraged to develop datasets with pairs of titles and abstracts or abstract and conclusion/future work pairs. Wang et al. [275] for example extracted (i) title to abstract pairs, (ii) abstract to conclusion and future work pairs, and (iii) conclusion and future work to title pairs from PubMed.\nAnnotated, task-specific datasets for scientific text generation:"}, {"title": "4.3.2 Methods and results", "content": "In the following, we survey approaches to generating salient text-based parts of scientific papers, such as title, abstract, and related work.\nTitle Generation. Generating adequate titles for scientific papers is an important task because titles are the first access point of a paper and can attract substantial reader interest; titles can also influence the reception of a paper [140]. Consequently, several works have targeted generating titles automatically, often using paper abstracts as input."}, {"title": "4.3.3 Ethical concerns", "content": "In scientific work, authorship and plagiarism in AI generated texts are major concerns. In general, it is a challenge to distinguish between AI generated and human generated texts. Although there is a number of tools to detect AI-generated text (e.g., GPTZero or Hive), Anderson et al. [7] have shown that after applying automatic paraphrasing to AI generated text, the probability of a text to be human generated, increases. Therefore it is not possible to reconstruct if a text is an original work from a scientist or has been generated by an AI. In addition, it was also found that ChatGPT generated texts easily passed plagiarism detectors [4, 71]. Moveover, Macdonald et al. [177] raised the concern that the frequent use of LLMs for drafting research articles might lead to similar paragraphs and structure of many papers in the same field. This again raises the question whether there should be a threshold for the acceptable amount of Al-generated content in scientific work [177]."}, {"title": "4.3.4 Domains of application", "content": "Text-based content generation is relevant for all scientific domains. Liang et al. [157] conducted a large-scale analysis across 950,965 paper published between January 2020 and February 2024 to measure the prevalence of LLM modified content over time. The paper they investigated were published on (i) arXiv including the five areas Computer Science, Electrical Engineering and Systems Science, Statistics, Physics, and Mathematics, (ii) bioRxiv, and (iii) Nature portfolio. Their results showed the largest and fastest growth in Computer Science papers with up to 17.5% and the least LLM modifications in Mathematics papers (up to 6.3%). However, according to the Natural Language Learning & Generation arXiv report from September 2024, top-cited papers show notably fewer markers of Al-generated content compared to random samples [139]."}, {"title": "4.3.5 Limitations and future directions", "content": "Numerous studies have investigated text-based content generation for the scientific domain and have shown their potential to assist scientists in different phases of writing a paper. While for some tasks such as proof-reading and paraphrasing, its capabilities are well established, others pose limitations. Therefore it is crucial that automatically generated text is always assessed by a human expert. Factual consistency and truthfulness are issues which need to be reviewed by a human-in-the-loop for all types of text-based generated content. Current proprietary LLMs for example struggle in particular with generating existing and correct bibliographic citations. Moreover, several ethical issues arise when text-generating systems are included in the scientific writing process, such as authorship, plagiarism"}]}