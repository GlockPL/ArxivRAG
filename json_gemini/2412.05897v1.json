{"title": "DETECTING DISCREPANCIES BETWEEN AI-GENERATED\nAND NATURAL IMAGES USING UNCERTAINTY", "authors": ["Jun Nie", "Yonggang Zhang", "Tongliang Liu", "Yiu-ming Cheung", "Bo Han", "Xinmei Tian"], "abstract": "In this work, we propose a novel approach for detecting AI-generated images by leverag-\ning predictive uncertainty to mitigate misuse and associated risks. The motivation arises\nfrom the fundamental assumption regarding the distributional discrepancy between nat-\nural and AI-generated images. The feasibility of distinguishing natural images from\nAI-generated ones is grounded in the distribution discrepancy between them. Pre-\ndictive uncertainty offers an effective approach for capturing distribution shifts, thereby\nproviding insights into detecting AI-generated images. Namely, as the distribution shift\nbetween training and testing data increases, model performance typically degrades, often\naccompanied by increased predictive uncertainty. Therefore, we propose to employ pre-\ndictive uncertainty to reflect the discrepancies between AI-generated and natural images.\nIn this context, the challenge lies in ensuring that the model has been trained over sufficient\nnatural images to avoid the risk of determining the distribution of natural images as that of\ngenerated images. We propose to leverage large-scale pre-trained models to calculate the\nuncertainty as the score for detecting AI-generated images. This leads to a simple yet ef-\nfective method for detecting AI-generated images using large-scale vision models: images\nthat induce high uncertainty are identified as AI-generated. Comprehensive experiments\nacross multiple benchmarks demonstrate the effectiveness of our method.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in generative models have revolutionized image generation, enabling the production\nof highly realistic images (Midjourney; Wukong; Rombach et al., 2022). Despite the remarkable capabilities\nof these models, they pose significant challenges, particularly the rise of deepfakes and manipulated content.\nThe high degree of realism achievable by such technologies prompts urgent discussions about their potential\nmisuse, especially in sensitive domains such as politics and economics. In response to these critical concerns,\na variety of methodologies for detecting generated images have emerged. A prevalent strategy treats this\ndetection task as a binary classification problem, necessitating the collection of extensive datasets comprising\nboth natural and AI-generated images to train classifiers (Wang et al., 2020).\nWhile existing detection methods have demonstrated notable successes, they typically encounter challenges\nin generalizing to images produced by previously unseen generative models Wang et al. (2023a). One\npromising avenue to enhance the robustness of detection capabilities involves constructing more extensive\ntraining datasets by accumulating a diverse array of natural and synthetic images. However, these attempts\nare often computationally intensive, requiring substantial datasets for effective binary classification. Ad-\nditionally, maintaining robust detection necessitates continually acquiring images generated by the latest\nmodels. And when the latest generative models are not open-sourced, acquiring a large number of generated"}, {"title": "2 RELATED WORKS", "content": "AI-Generated images detection. Recent advancements in generative models, such as those by (Brock et al.,\n2019; Ho et al., 2020), have led to the creation of highly realistic images, highlighting the urgent need for\neffective algorithms to distinguish between natural and generated images. Prior research, including works\nby (Frank et al., 2020; Marra et al., 2018), primarily focuses on developing specialized binary classification\nneural networks to differentiate between natural and generated images. Notably, CNNspot (Wang et al.,\n2020) demonstrates that a standard image classifier trained on ProGAN can generalize across various archi-\ntectures when combined with specific data augmentation techniques. F3Net (Qian et al., 2020) distinguishes\nreal face images from fake face images with the help of frequency statistical differences. NPR (Tan et al.,"}, {"title": "3 METHOD", "content": "Our method is built upon a foundational assumption that natural and generated images have different distri-\nbutions. This is a reasonable assumption; otherwise, we cannot distinguish between natural and generated\nimages. Fortunately, this assumption is consistent with previous work (Tan et al., 2024) and our empirical\nobservations, as shown in Figure 1.\nHence, for the large models trained merely on natural images, we can regard natural images as in-distribution\n(ID) data while generated images as out-of-distribution (OOD) data. This distribution discrepancy can be\nreflected by the widely used predictive uncertainty, since neural networks typically exhibit higher uncertainty\nfor OOD samples (Snoek et al., 2019; Schwaiger et al., 2020). This leads to a simple yet novel approach\nto determine whether a test image is generated by AI models when we can calculate its uncertainty on a\npre-trained large vision model. In the following, we first give a complete introduction to our method WePe,\nthen describe on our reasons for choosing the method, and finally, we explore why WePe works."}, {"title": "3.1 MOTIVATION", "content": "Our method is built upon a foundational assumption that natural and generated images have different distributions. This is a reasonable assumption; otherwise, we cannot distinguish between natural and generated images. Fortunately, this assumption is consistent with previous work (Tan et al., 2024) and our empirical observations, as shown in Figure 1.\nHence, for the large models trained merely on natural images, we can regard natural images as in-distribution (ID) data while generated images as out-of-distribution (OOD) data. This distribution discrepancy can be reflected by the widely used predictive uncertainty, since neural networks typically exhibit higher uncertainty for OOD samples (Snoek et al., 2019; Schwaiger et al., 2020). This leads to a simple yet novel approach to determine whether a test image is generated by AI models when we can calculate its uncertainty on a pre-trained large vision model. In the following, we first give a complete introduction to our method WePe, then describe on our reasons for choosing the method, and finally, we explore why WePe works."}, {"title": "3.2 UNCERTAINTY DEFINITION", "content": "Classical methods of uncertainty estimation, such as Deep Ensembles and MC Dropout, can simply be\nviewed as using the variance of the results of multiple predictions as an estimation of uncertainty u(x):\n$\\mu(x) = \\frac{1}{n}\\sum_{t=1}^{n} \\hat{y}_t(x), u(x) = \\sigma^2 = \\sum_{t=1}^{n}(\\hat{y}_t(x) \u2013 \\mu(x))^2$,\nwhere, $\\hat{y}_t$ denotes the t-th prediction. The multiple predictions of Deep Ensembles come from multiple\nindependently trained neural networks, while the multiple predictions of MC Dropout come from the use of\ndropout during inference, which can be regarded as multiple prediction using neural networks with different\nstructures."}, {"title": "3.3 UNCERTAINTY CALCULATION", "content": "The predictive uncertainty is typically calculated as the variance of predictions obtained with certain pertur-\nbations. In this work, we simply leverage $\\theta$ as the features or parameters before perturbation. Specifically,\nthe predictive uncertainty u(x) can be calculated by,\n$u(x) = \\frac{1}{n}\\sum_{k=1}^{n}[f(x; \\theta_k) - \\frac{1}{n}\\sum_{j=1}^{n}f(x; \\theta_j)]^2 = \\frac{1}{n}\\sum_{k=1}^{n}f(x; \\theta_k) f(x; \\theta_k) - \\sum_{j=1}^{n}f(x; \\theta_j) f(x; \\theta_t)|^2$,\nwhere n is the number of samples, $f(x; \\theta_k)$ denotes the L2-normalized features of an input image x when\ninferring with the parameter $\\theta_k$, and $\\theta_t$ stands for the teacher model used in DINOv2.\nHowever, we cannot access the teacher model $\\theta_t$, making it challenging to calculate the uncertainty. More-\nover, even if it is available, introducing two models for calculation leads to low computation efficiency.\nFortunately, we can calculate an upper bound of u(x). This can be formalized by,\n$u(x) \\leq \\frac{1}{n}\\sum_{k=1}^{n}||f(x; \\theta_k)||^2 - |\\frac{1}{n}\\sum_{j=1}^{n}f(x; \\theta_j)||^2 = \\frac{2}{n}\\sum_{k=1}^{n}||f(x; \\theta_k) f(x; \\theta_t)||$,\nwhere $\\Theta$ denotes the parameter before injecting perturbation, and we leverage an unbiased assumption that\nthe expectation $E_{\\Theta}; f (x;\\Theta_j)$ approaches the feature $f(x;\\theta)$ extracted by the non-perturbed parameter $\\theta$.\nEq. 3 provides a simple approach to calculate the uncertainty without needing a teacher model used in the\ntraining phase of DINOv2. The insight of Eq. 3 is intuitive. Specifically, if an image x causes a high feature\nsimilarity between the original and perturbed parameter, the image leads to a low uncertainty and is more\nlikely to be a natural image."}, {"title": "3.4 AN OVERVIEW OF WEPE", "content": "As discussed above, the proposed WePe is based on a large model pre-trained on a large number of natural\nimages. In this work, we chose DINOv2 (Oquab et al., 2024), a large model trained with contrastive learning\non image data. In order to capture the uncertainty of the model on the test images, we extract image features\nusing the original model and the model after adding noise to the parameters respectively. The similarity\nbetween the pre-perturbation and post-perturbation feature vectors is quantified using a suitable distance\nmetric, such as cosine similarity. Images exhibiting high similarity are classified as real, while those with\nlow similarity are identified as generated. This method not only capitalizes on the characteristics of the\nDINOv2 model but also provides a robust framework for distinguishing between real and generated images\nbased on their feature stability under model perturbation."}, {"title": "3.5 DISCUSSIONS", "content": "Why choose DINOv2? In addition to DINOv2, CLIP is a commonly used model. However, our experiments\nshow that CLIP performs sub-optimally compared to DINOv2 (see Table 4). We believe this difference\nstems from their training strategies. Unlike DINOv2, CLIP is a multimodal model that combines image\nand textual features from captions for contrastive learning, which may lead it to focus on broader semantic\nfeatures rather than fine details. In contrast, DINOv2 emphasizes contrastive learning solely on images,\nallowing it to better capture subtle differences between natural and generated images. Therefore, we use\nDINOv2 in our experiments.\nWhy choose weight perturbation? Common methods for measuring uncertainty include MC Dropout\nand Deep Ensembles. MC Dropout involves keeping dropout active during testing and performing multiple\nforward passes on the inputs to generate outputs with different network structures. The variability among\nthese outputs serves as an estimate of the model's uncertainty regarding the input data. However, since\nDINOv2 does not utilize dropout during training, MC Dropout may not yield optimal results (see Table 5).\nDeep Ensembles, on the other hand, trains multiple networks independently and uses the differences in\ntheir outputs on test samples to assess uncertainty. However, training multiple DINOv2-level models is\nimpractical. Therefore, in our study, we choose to perturb the model parameters and assess the differences\nin outputs from the original and perturbed models to estimate uncertainty for the test images.\nWhy does weight perturbation work? Incorporating weight perturbations during testing can effectively\nsimulate Bayesian inference, thereby capturing the inherent uncertainty in neural networks. From a Bayesian\nperspective, the weights of a neural network are not fixed but rather distributions reflecting the range of\nplausible values given the data. By introducing noise into the network weights at test time, we mimic the\nprocess of drawing samples from a posterior distribution over weights, a core concept in Bayesian inference.\nThis technique enables the model to generate diverse predictions, reflecting its uncertainty, particularly when\nencountering out-of-distribution samples. This approach is akin to Bayesian neural networks, where weight\nuncertainty is explicitly modeled. For example, (Blundell et al., 2015) proposed using variational methods\nto approximate weight distributions in Bayesian neural networks, allowing for uncertainty quantification\nthrough weight perturbations. Similarly, (Gal & Ghahramani, 2016) demonstrated that introducing dropout\nat test time serves as a Bayesian approximation, with the added noise acting as a proxy for weight sampling,\nthus allowing for reliable uncertainty estimation. In this paper, we capture uncertainty by adding noise\ndirectly to the model parameters to distinguish between natural and generated images. However, trying\nto theoretically analyze the different sensitivities of natural and generated images to model weights in the\nrepresentation space of a large model is a difficult task. Therefore, we only draw this conclusion through\nvarious empirical observations in this paper, and do not directly prove this theoretically. The theoretical\nproof will be left to our future work."}, {"title": "4 EXPERIMENTS", "content": "Datasets. Following previous work (He et al., 2024), we evaluate the performance of WePe on Ima-\ngeNet (Deng et al., 2009), LSUN-BEDROOM (Yu et al., 2015) and GenImage (Zhu et al., 2023). For\nImageNet and LSUN-BEDROOM, the generated images are provided by (Stein et al., 2023). For Ima-\ngeNet, the generative models include ADM (Dhariwal & Nichol, 2021), ADM-G, LDM (Rombach et al.,\n2022), DiT-XL2 (Peebles & Xie, 2023), BigGAN (Brock et al., 2019), GigaGAN (Kang et al., 2023),\nStyleGAN (Karras et al., 2019), RQ-Transformer (Lee et al., 2022), and MaskGIT (Chang et al., 2022).\nFor LSUN-BEDROOM, the generative models include ADM, DDPM (Ho et al., 2020), iDDPM (Nichol\n& Dhariwal, 2021), Diffusion Projected GAN (Wang et al., 2023b), Projected GAN (Wang et al., 2023b),\nStyleGAN (Karras et al., 2019) and Unleasing Transformer (Bond-Taylor et al., 2022). GenImage primar-"}, {"title": "4.1 EXPERIMENT SETUP", "content": "Datasets. Following previous work (He et al., 2024), we evaluate the performance of WePe on Ima-\ngeNet (Deng et al., 2009), LSUN-BEDROOM (Yu et al., 2015) and GenImage (Zhu et al., 2023). For\nImageNet and LSUN-BEDROOM, the generated images are provided by (Stein et al., 2023). For Ima-\ngeNet, the generative models include ADM (Dhariwal & Nichol, 2021), ADM-G, LDM (Rombach et al.,\n2022), DiT-XL2 (Peebles & Xie, 2023), BigGAN (Brock et al., 2019), GigaGAN (Kang et al., 2023),\nStyleGAN (Karras et al., 2019), RQ-Transformer (Lee et al., 2022), and MaskGIT (Chang et al., 2022).\nFor LSUN-BEDROOM, the generative models include ADM, DDPM (Ho et al., 2020), iDDPM (Nichol\n& Dhariwal, 2021), Diffusion Projected GAN (Wang et al., 2023b), Projected GAN (Wang et al., 2023b),\nStyleGAN (Karras et al., 2019) and Unleasing Transformer (Bond-Taylor et al., 2022). GenImage primar-"}, {"title": "4.2 RESULTS", "content": "Comparison with other baselines. We conduct full comparative experiments on the three benchmarks\nmentioned. As shown in Table 1, 2 and 3, WePe achieves the best detection performance on ImageNet,\nLSUN-BEDROOM and GenImage without the need for training. It is worth noting that on the large-scale\nGenImage benchmark, the training-based method, despite having perfect performance on the generators used\nduring training, performs poorly on many generators not seen during training. This illustrates the extreme\ndependence of the performance of training-based methods on the diversity of the training set. In contrast,\nour method does not require training, performs well on a wide variety of generators, and outperforms the\nSOTA training method by 3.36% on average. On generators that have not been seen during training, such\nas VQDM, many training methods exhibit random prediction results, whereas WePe achieves superior de-\ntection performance. To further illustrate the effectiveness of our method, we count the difference in feature\nsimilarity between real and fake images on the pre- and post-perturbation models. As shown in Figure 3,\nthe small perturbation of the model has less effect on the real images than on generated images, resulting in\nhigher feature similarity before and after the perturbation. The discrepancy effectively distinguishes the real\nimage from the generated image."}, {"title": "4.3 ABLATION STUDY", "content": "In this section, we perform ablation experiments. Unless otherwise stated, experiments are conducted on\nImageNet benchmark."}, {"title": "5 LIMITATION", "content": "The proposed weight perturbation provides a simple and effective method for detecting generated images,\nyet we have not theoretically justified the widespread use of the method due to the inclusion of a variety of\nstrong prior assumptions, such as the assumption about treating generated samples as OOD data. Therefore,\nour future work will focus on establishing the theoretical foundations of our method."}, {"title": "6 CONCLUSION", "content": "In this work, to effectively address the challenges of detecting AI-generated images, we propose a novel\napproach that leverages predictive uncertainty as a key metric. Our findings reveal that by analyzing the dis-\ncrepancies in distribution between natural and AI-generated images, we can significantly enhance detection\nperformance. The use of large-scale pre-trained models allows for accurate computation of predictive un-\ncertainty, enabling us to identify images with high uncertainty as likely AI-generated. Our method achieves\nrobust detection performance in a simple untrained manner. Overall, our approach demonstrates a promis-\ning direction for improving AI-generated image detection and mitigating potential risks associated with their\nmisuse. Future work could delve deeper into refining the predictive models and exploring additional features\nthat could further enhance detection accuracy."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 CONCERNS ABOUT MODELS BEING CONTAMINATED BY GENERATED IMAGES", "content": "With the proliferation of generated images, future large-scale models could become contaminated by such\ndata, making it increasingly difficult to distinguish between natural and generated images. One potential\nsolution is to employ machine unlearning (Yao et al., 2023) techniques to detect and address generated\nimages. Machine unlearning focuses on removing the influence of specific data from pre-trained models,\nprimarily due to privacy concerns. In this context, when generative images are incorporated into the training\nprocess of large-scale models, we can utilize machine unlearning techniques to eliminate the effects of these\nimages on pre-trained models. This approach would help ensure that the features of natural and generated\nimages remain distinct and separable."}, {"title": "A.2 DISCUSSION ON DISTRIBUTION DISCREPANCY", "content": "In this paper, the core assumption we make is that there is data distribution discrepancy between natural and\ngenerated images. This assumption is valid for current generative models and has been confirmed by many\nworks (Corvi et al., 2023; Tan et al., 2024; Ricker et al., 2024). This assumption is also the foundation of\nmany generative image detection methods (we cannot distinguish between images that are indistinguishable).\nSecondly, we observe that this discrepancy in data distribution can be captured by the representation space of\na vision model pre-trained on a large number of natural images, i.e., there is feature distribution discrepancy\nbetween the generated and natural images, as shown in Figure 7. However, this remains an observation, and\nwe have not found theoretical proof despite reviewing the literature. We only observe a similar phenomenon"}, {"title": "A.3 MEASURING FEATURE DISTRIBUTION DISCREPANCY WITH FID SCORES", "content": "We further use the \"FID\" score to measure the difference in feature distribution between natural and gen-\nerated images. To avoid the effects of categories, we compute the FID scores using the DINOv2 model\non the LSUN-BEDROOM benchmark. For each category of images, we randomly select 5000 images for\ncalculation. In addition to calculating the FID scores between natural images and generated images, we\nalso calculate the FID scores between natural images and natural images. As shown in Table 6, the FID\nscores between natural images and generated images are significantly higher than the FID scores between\nnatural images and natural images. Moreover, there is a clear positive correlation between the detection\nperformance of WePe and the FID score. This result fully explains the existence of feature distribution dis-\ncrepancy between natural and generated images on DINOv2, and demonstrates that WePe can effectively\ndetect the feature distribution discrepancy."}, {"title": "A.4 COMPARISON WITH GRADIENT CUFF", "content": "Gradient Cuff (Hu et al., 2024) focuses on detecting jailbreak attacks in Large Language Models (LLMs). It\nfinds that the landscape of the refusal loss is more precipitous for malicious queries than for benign queries.\nAnd then it uses stochastic gradient estimation to estimate gradient and use the gradient norm as the decision\nscore. Thus, we can also leverage this interesting work to identify the distribution discrepancy for detection.\nWe use the similar way to estimate gradient as the decision score. As shown in Table 7, we surprisingly find\nthat this method even surpasses WePe. This suggests that it is also possible to distinguish between natural\nand generated images by estimating the gradient."}, {"title": "A.5 WEPE ON LARGE MULTI-MODAL MODELS", "content": "In addition to CLIP, we further test the performance of WePe on BLIP (Li et al., 2022). As shown in Table 8,\nthe performance of WePe is unsatisfactory on these multimodal models, which may be due to the fact that\nthe image features of the multimodal models are more focused on semantic information, in line with our\ndiscussions."}, {"title": "A.6 PERFORMANCE ON ADVERSARIAL EXAMPLES", "content": "We further test WePe on adversarial examples. We simply add Gaussian noise (with different standard\ndeviation $\\sigma$) to the test samples to simulate the adversarial examples. We test three cases: adding noise\non the natural image, adding noise on the generated image and adding noise on all images. As shown in\nTable 9, when noise is injected, the feature similarity between the clean model and the noisy model for the\nnoisy image decreases, which leads to a change in the detection performance. To mitigate this effect, we\ncan perform detection by perturbing the model multiple times and using the average similarity. The effect of\nnoise is successfully mitigated by ensemble as shown in Table 10."}, {"title": "A.7 CONCERNS ABOUT HARD SAMPLES", "content": "WePe relies on the model being pre-trained on a large dataset of natural images. Given the abundance of\nnatural images, it is possible WePe may misclassify other natural images that are out-of-distribution, as AI-"}, {"title": "A.8 BOOSTING PERFORMANCE ON CLIP", "content": "As mentioned above, WePe does not perform satisfactorily on vision language models such as CLIP. Since\nCLIP needs to unite image features and text features, the image features need to be projected into the union\nspace, which results in the projected image features being more focused on semantic information. For\nthis reason, we can improve the performance of WePe on CLIP by using the features before projection for\ndetection, as shown in Table 11."}, {"title": "A.9 COMPARISON OF DETECTION TIMES.", "content": "Our method use a perturbed pre-\ntrained model that is fixed during\ninferring all test samples. Thus, our\nmethod can be processed within\ntwo forward passes. This is equal\nto the cost of RIGID that requires\ntwo forward passes for clean and\nnoisy images. However, RIGID\ncan concatenate clean and noisy\nimages in a mini batch and obtain\ndetection results by with a single\nforward pass. AEROBLADE re-"}, {"title": "A.10 \u0421\u043eMPARISON WITH RIGID", "content": "The main differences between WePe and RIGID are as follows:"}, {"title": "A.11 NOTE ON THE UNBIASED ASSUMPTION", "content": "In section 3.3, we make an assumption that the expected extracted feature by noised models is unbiased for\nthat by the original model. We think this assumption is reasonable. Thanks to the over-parameterization\nof modern neural networks and advanced optimization algorithms (e.g., AdamW), it is a well-established\nfact that trained neural networks are usually smooth in the parameter space and show robustness to small\nweight perturbation (Novak et al., 2018). And this robustness is used in many applications, such as quan-\ntization (Gholami et al., 2022) and pruning (Liu et al., 2019). And in Figure 2, we also clearly show this\nrobustness: when adding tiny noise to the model weights, the features remain almost unchanged."}, {"title": "A.12 SOFTWARE AND HARDWARE", "content": "We use python 3.8.16 and Pytorch 1.12.1, and seveal NVIDIA GeForce RTX-3090 GPU and NVIDIA\nGeForce RTX-4090 GPU."}, {"title": "A.13 WEPE WITH MULTIPLE PERTURBATION", "content": "In our experiments, taking into account the detection efficiency, we\nperturb the model only once, and then calculate the feature simi-\nlarity of the test samples on the clean and perturbed models. We\nfurther experiment with multiple perturbations and use the mean of\nthe feature similarity of the test samples on the clean model and\nall the perturbed models as the criterion for determining whether or\nnot the image is generated by the generative models. As shown in\nFigure 8,"}, {"title": "A.14 USING NATURAL\nIMAGES ONLY TO SELECT WHICH LAYERS TO PERTURB", "content": "In our experiments, we use a small set of natural images and gener-\nated images to pick the parameters that need to be perturbed. When\nall the generated images are not available, we can also use only the\nreal images to select the layers that need to be perturbed. Specifi-\ncally, we first perturb each block alone and calculate the similarity"}]}