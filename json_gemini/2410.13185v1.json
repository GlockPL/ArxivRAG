{"title": "CHAIN OF IDEAS: REVOLUTIONIZING RESEARCH IN NOVEL IDEA DEVELOPMENT WITH LLM AGENTS", "authors": ["Long Li", "Weiwen Xu", "Jiayan Guo", "Ruochen Zhao", "Xinxuan Li", "Yuqian Yuan", "Boqiang Zhang", "Yuming Jiang", "Yifei Xin", "Ronghao Dang", "Yu Rong", "Deli Zhao", "Tian Feng", "Lidong Bing"], "abstract": "Effective research ideation is a critical step for scientific research. However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Recent developments in large language models (LLMs) suggest a promising avenue for automating the generation of novel research ideas. However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information. Inspired by the research process of human researchers, we propose a Chain-of-Ideas (CoI) agent, an LLM-based agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain. This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers. Experimental results indicate that the Col agent consistently outperforms other methods and shows comparable quality as humans in research idea generation. Moreover, our Col agent is budget-friendly, with a minimum cost of $0.50 to generate a candidate idea and its corresponding experimental design\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Idea generation is a crucial aspect of scientific research for driving technological innovations and breakthroughs. Traditionally, this process has been predominantly human-driven, necessitating expert researchers to review extensive literature, identify limitations in existing solutions, and propose new research directions. However, the complexity and vastness of scientific literature, coupled with rapid technological advancements, have rendered this task increasingly challenging for researchers.\nRecent advancements in large language models (LLMs) (Achiam et al., 2023; Dubey et al., 2024; Yang et al., 2024a) have enabled these models to exceed human experts in various scientific tasks, including mathematics (Yu et al., 2023), theorem proving (Yang et al., 2023), and coding (Chen et al., 2021). Building on this robust scientific foundation, one may hypothesize that LLMs could support a more abstract and creative research idea-generation task. Notably, Si et al. (2024); Kumar"}, {"title": "2 METHOD", "content": ""}, {"title": "2.1 FRAMEWORK: CHAIN-OF-IDEAS AGENT", "content": "In this section, we detail our Col agent framework, as illustrated in Figure 2, which consists of three stages: (1) CoI Construction, (2) Idea Generation, and (3) Experiment Design. First, given a research topic, the Col agent constructs multiple CoIs from existing literature, reflecting different trends within the domain. Then, for each CoI, the LLM predicts future research directions, and crafts ideas through step-by-step consolidation and iterative novelty checks. The best idea is then selected. Lastly, the LLM generates and refines an experiment design to implement the final idea."}, {"title": "2.2 COI CONSTRUCTION", "content": "Generating novel research ideas requires a profound comprehension of the respective research domain, coupled with a rigorous reasoning process. Previous endeavors (Lu et al., 2024; Baek et al., 2024) have sought to augment LLMs with relevant papers to facilitate the ideation process. However, these methods simply mix these papers into the prompt without effective organization. This scenario is akin to dropping an LLM at a chaotic intersection with no map in sight, leaving it uncertain about which path to take. To address this issue, we propose a Chain-of-Ideas agent framework.\nAs shown in Figure 2, a CoI, represented as {I\u2212M \u2192 \u2026\u2026 \u2192 Io \u2192 \u2026\u2026 \u2192 Iv }, is a sequence consisting of M + N + 1 ideas extracted from M + N + 1 research papers respectively, where they together show the evolution progress within a given research field. Specifically, given an initial research topic, we prompt the LLM to generate multiple queries, [q\u00b9, . . ., qK], that reflect K different perspectives of this topic. The prompt is given in Table 7 of Appendix. Unless otherwise specified, all prompts of our framework are presented in the Appendix tables. The K queries are used to construct K branches of CoI. This reduces the reliance on a single CoI that may be insufficient to capture the most significant development and direction. For each query qk, we use it to retrieve a top-ranked paper, which we call anchor paper Pf. In Figure 2, ToT (Yao et al., 2024) is an illustrative example of an anchor paper. An anchor paper serves as the foundation for constructing a CoI. Specifically, a Col is constructed by extending from the corresponding anchor paper to related papers in both directions: forward, tracing the progression of ideas, and backward, tracing their origins.\nIn the forward direction, starting from Po, we identify subsequent papers that directly cite it by leveraging the Semantic Scholar API2. We use OpenAI's text-embedding-3-large\u00b3 to rank these papers based on their cosine similarities to the concatenation of the initial research topic and the abstract of the anchor paper. Subsequently, we select the highest-ranked paper as P to extend the CoI in the forward direction (e.g. GoT in Figure 2). This process is repeated iteratively from P to P+1, until either the length of the CoI reaches a preset value or the LLM finds that there is no valuable follow-up work (Table 8).\nIn the backward direction, starting from the anchor paper Po, we instruct an LLM to thoroughly review the full paper and to identify candidate references based on the following criteria: 1) references that Po directly built upon, 2) references that serve as baselines in P, and 3) references that tackle the same topic as P. With those candidate references, we ask the LLM to determine the most relevant one to the anchor paper (Tables 9 and 10), denoted as Pk 1 (e.g. SC in Figure 2), to extend the CoI backward. This backward extension is also carried out iteratively from P\u00bf to Pk (i+1) to identify preceding papers (e.g. tracing backward from SC to CoT in Figure 2). It terminates when the length of CoI reaches a preset value or we encounter a milestone paper (defined as one with over 1,000 citations), indicating that the idea from the milestone paper could serve as a strong starting point for the CoI. Additionally, we instruct the LLM to terminate the search if no reference relevant to the original research topic is found (Table 8).\nAfter we collect K paper chains, denoted as {PkMk \u2192\u2026\u2026\u2192 P \u2192\u2192P} }=1, we ask the LLM to extract ideas from these papers and inherit the progressive relation of the paper chains to form our CoIs {IMk \u2192 \u2192I\u2192 \u2192 Ik} NkJk=1 Nk}K_1 (Tables 9 and 10). Then for each CoI, we ask the LLM to summarize the existing research trends by analyzing the evolution between any two adjacent ideas (Table 11). For example, the upper part of Figure 2 shows the evolution process from CoT to GoT step-by-step. Additionally, we extract experiment designs and the definition of key entities from these papers (Tables 9 and 10). The above information including CoIs and the derived knowledge will be used in the following idea generation and experiment design stages."}, {"title": "2.3 IDEA GENERATION", "content": "In this section, we use the above-constructed CoIs and their developing trends to guide the generation of a novel idea. For each generated CoI, the first step is to predict possible future trends. As shown in the lower-left section of Figure 2, we prompt the LLM with the CoI, the developing trends of"}, {"title": "2.4 EXPERIMENT DESIGN", "content": "While our primary goal is to generate novel ideas, it is also useful to develop experimental plans that help users implement these ideas. Thus, we extended the Col agent to include experiment design. As shown in the lower-right of Figure 2, we prompt the LLM with experiments from existing works obtained from Sec. 2.2 as few-shot examples, along with the proposed idea and key entities, to guide the LLM in designing experiments for our ideas (Table 17).\nWe also employ a review agent to assess the candidate experiment designs. Its main role is to evaluate the clarity and comprehensiveness of the protocol, ensuring all key elements such as datasets and models are clearly specified. Additionally, it checks if the design provides enough detail for practical implementation (Table 18). The review agent provides critical feedback on these aspects, subsequently utilizing this information to conduct further searches for relevant literature (Table 19) to help the LLM refine and enhance its previous experiment design (Table 20). Through this iterative process of review and refinement, we arrive at a final experiment design."}, {"title": "3 EXPERIMENTAL SETUPS", "content": ""}, {"title": "3.1 IMPLEMENTATIONS", "content": "In our Col agent, we primarily use GPT-4o (05-13)4 as our LLM implementation. For some modules that require full-paper understanding, we use GPT-40-mini (07-18) to read the paper and summarize the core contents due to its lower price and good summarization capability. We use Semantic Scholar as our academic search engine. For the main experimental results, the maximum length of the Col is set to 5 and the number of Col branches is set to 3, and their analysis results are given later. The iteration number of self-refinement in the experiment design stage is set to 1 for cost saving."}, {"title": "3.2 DATA", "content": "To evaluate our Col agent's ability to generate novel ideas, we collect recent research topics from Hugging Face's Daily Papers, known for its timely updates on AI research and the high quality of the featured papers. We select papers submitted between August 1 and September 15, 2024, ensuring that the topics are sufficiently new and the time frame is after the data cutoff of the LLM. We ask 10 skilled researchers with diverse interests in AI to identify papers that capture their interests. Subsequently, we prompt GPT-40 to extract research topics, proposed ideas, and their corresponding experiment designs from these selected papers (Tables 21, Table 22 and 23). The extracted topics will then be returned to the researchers for validation, ensuring that the extracted topics are valid and reasonable within their research domains. The extracted ideas and experiment designs will be utilized as our Real Paper baseline, as described in Section 3.3. Due to the substantial costs associated with generating and evaluating ideas and experiment designs, we adhere to the assessment scale of Lu et al. (2024); Wang et al. (2023) to collect 50 research topics in total for evaluation."}, {"title": "3.3 BASELINES", "content": "We compare our CoI agent with recent works on idea generation and experiment design. To ensure a fair comparison, we employ GPT-40 and Semantic Search as the LLM and academic retriever implementations, respectively, across all baseline methods. Furthermore, we unify the output format of the generated ideas and experiment designs to minimize evaluation preference towards more structured outputs (Chiang et al., 2024). We compare with the following baselines:\n\u2022 RAG: This is a vanilla retrieval augmented generation approach (Lewis et al., 2020), where we directly prompt the LLM with retrieved literature for idea generation and experiment design.\n\u2022 ResearchAgent (Baek et al., 2024): This work leverages additional academic knowledge graph for enhancing the literature retrieval and adopts a multi-agent framework to iteratively refine ideas through peer discussions. We follow the original paper to reproduce this baseline.\n\u2022 GPT-Researcher (Assafelovic, 2023): GPT-Researcher is an agent framework specifically designed for the research domain. The agent is enhanced with plan-and-solve and RAG capabilities.\n\u2022 AI-Scientist (Assafelovic, 2023): This work originally aims to generate the entire paper with the idea, methods, and experimental results. We extract the components related to idea generation and experiment design to serve as our baseline.\n\u2022 Real Paper: Note that, in Sec. 3.2, we extract topics from existing research papers. Therefore, the ideas and the experiment designs from these papers serve as a natural baseline to quantify the gap between model-generated ideas and genuine human ideas."}, {"title": "3.4 EVALUATION: IDEA ARENA", "content": "Model-based Evaluation. The open-ended nature of idea generation poses challenges for automatic evaluation. Prior work primarily uses LLM-based Likert scale system to score ideas (Baek et al., 2024; Lu et al., 2024). However, Si et al. (2024) show this method poorly aligns with human preferences. Instead, they show LLMs perform better in ranking ideas. To obtain reliable scores for evaluation, we propose Idea Arena, a pairwise evaluation system using a Round-Robin tournament to compute ELO scores for each idea-generation method. For a given topic, we require the LLM judge to rank the ideas generated by any pair of methods (Table 24). We evaluate each pair twice with order reversed to reduce the position bias. To comprehensively evaluate an idea from multiple perspectives, we incorporate criteria from ICML 2020 review guidelines 6, and those in Si et al. (2024), which consist of Novelty, Significance, Clarity, Feasibility, and Expected Effectiveness. Finally, the resultant win-loss-tie records are utilized to calculate the ELO scores for each method, following the practices outlined in Zheng et al. (2024); Zhao et al. (2024). We also evaluate the experiment design in the same pairwise way, focusing on Feasibility, Technical Quality, and Clarity. Refer to Definitions for all metrics in Tables 5 and 6 of the Appendix.\nHuman Evaluation. We also perform human evaluation in a pairwise manner. The 10 researchers who review the extracted topics are asked to rank two ideas and experiment designs using the same criteria. To ensure fairness, we anonymize the source of the ideas by concealing the method identity."}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 IDEA GENERATION", "content": "Main results. Figures 3 and 4 present the results of idea generation evaluated by both a LLM (specifically, GPT-40) and human researchers. Detailed scores are in Table 26 of Appendix. Overall, our Col agent demonstrates superior performance compared to all other automated methods in both model-based and human-based evaluations. Notably, It substantially outperforms the second-best baselines, GPT-Researcher and RAG, by margins of 108 and 56 ELO scores, respectively, in the two evaluation settings. Our Col agent's performance is on par with that of the Real Paper baseline and even excels in the metrics of Novelty and Significance. These results highlight its exceptional capabilities in idea generation. Furthermore, Col demonstrates superior performance in Clarity, Feasibility, and Expected Effectiveness compared to other automated methods in human evaluation. Nevertheless, it still lags considerably behind the Real Paper in these areas. This substantial gap"}, {"title": "Human-Model Agreements of Idea Arena. To assess", "content": "the reliability of our model-based evaluation within Idea Arena, we analyze the agreements between the preferences of the human judges and the LLM judges. We follow Zheng et al. (2024) to compute the agreement, which is defined as the probability that two judges agree on the winner of one specific arena match. Figure 5 shows the pairwise agreement between humans and several state-of-the-art LLMs, including GPT-40, Gemini-1.5-Pro-Exp-08277, and Claude-3.5-Sonnets. We observe an average agreement of 70.8% between GPT-4o and humans. This finding indicates a strong alignment between human-based and model-based evaluations, thereby highlighting the robustness of Idea Arena in evaluating the quality of generated research ideas. Moreover, GPT-40 demonstrates the highest level of agreement with humans among all tested LLMs. Therefore, we will utilize GPT-4o as the LLM judge for subsequent analytical experiments. Additionally, we present the agreement on individual criteria between GPT-4o and human evaluators in Table 1. The results indicate a consistently high level of agreement across all assessed criteria."}, {"title": "4.2 ABLATION STUDIES FOR IDEA GENERATION", "content": "We conduct an ablation study to assess the contributions of each component of the Col Agent to idea generation quality. The following variants are examined: 1) \u2013 Col: Excludes the CoI construction stage, directly using all retrieved literature without progressive relation mining. 2) \u2013 Future Trend:"}, {"title": "4.3 CASE STUDY", "content": "We present an intriguing case study in Table 3 with the same topic of our paper \u2013 generating novel research ideas using LLMs. Given the input topic, our Col agent first constructs the chain of ideas, extending Io (Baek et al., 2024) in both forward and backward directions. Then the agent analyzes current research trends for any two adjacent ideas. For instance, it identifies that the core development from I-1 to Io is the generation of ideas rather than hypotheses. After digesting the existing trends, the ColI agent realizes that LLMs have great potential in idea generation but are limited in novelty and diversity. Therefore, it proposes an evolutionary algorithm, which specifically models the variations between parents and children, as a possible future trend for novel and diverse idea generation. Finally, the agent consolidates its final idea by drawing on future trends and with practical implementations, such as crossover and mutation, to ensure effective realization. Therefore, the generated idea is viable and novel, deserving further exploration in our future work."}, {"title": "4.4 EXPERIMENT DESIGN", "content": "As a byproduct of idea generation, we also require these baselines to develop potential experiment designs for realizing their proposed ideas. Table 4 presents the arena-style results for experiment designs for both model-based and human-based evaluations. Our Col Agent demonstrates superior performance across all evaluated criteria in two evaluation settings, achieving the highest scores among all automated methods. Notably, it surpasses RAG, the second-best automated method, by 70 ELO points in human evaluation. Furthermore, there is a high degree of model-human agreement in the experimental designs. Despite the clarity and reasonable technical details of the experiment designs produced by the Col Agent in support of the proposed ideas, they tend to be less feasible compared to those designs in the existing literature. This phenomenon is also observed during the idea generation phase. Consequently, feasibility represents a significant bottleneck in automatic idea generation, highlighting the need for future research to address this challenge."}, {"title": "4.5 LENGTH OF COI", "content": "To examine the impact of the Col length on the quality of generated ideas, we constructed variants with differing maximum chain lengths. Furthermore, we also adopt the \"- Col\" variant in Sec. 4.2 as a 0-length variant, which uses 5 retrieved papers but does not organize them in a chain structure. Figure 6 presents the idea arena results among these length variants. We observe a substantial improvement of idea-generation quality when we increase the length from 0 to 3. This indicates a clear developmental trend analysis is more pivotal than the quantity of related literature. Furthermore, the quality of generated ideas continues to improve as the length of the Col increases. Longer Cols offer more reliable and comprehensive insights into"}, {"title": "4.6 WIDTH OF COI", "content": "We also assess the impact of the width of CoI (i.e., the branch number K) on the quality of generated ideas. Figure 7 shows the trend of average ELO scores with varying branch numbers. Generally, increasing the branch numbers shows a positive correlation with idea quality. However, the disparity in ELO scores across different branch numbers is small. This phenomenon can likely be attributed to the fact that generating multiple chains primarily helps reduce the impact of any single Col performing poorly. Fortunately, such low-quality CoIs are rare."}, {"title": "5 RELATED WORKS", "content": ""}, {"title": "Scientific Research Idea Generation.", "content": "Idea generation is a fundamental step in scientific research. Due to its innovative nature, idea generation has been primarily a human-driven activity. However, recent studies indicate that LLMs can generate plausibly novel and feasible ideas as those of human researchers (Si et al., 2024; Kumar et al., 2024). To investigate the potential of LLMs in idea generation, prior works begin with the task of scientific hypothesis discovery (Yang et al., 2024b; Qi et al., 2023; Wang et al., 2023), which aims to elucidate relationships between two scientific variables. Despite its utility, scientific hypothesis discovery may not fully capture the complexity and multifaceted nature of real-world problems. To address this limitation, projects like GPT-Researcher (Assafelovic, 2023) and ResearchAgent (Baek et al., 2024) have adopted a more open-ended idea generation scenario including the underlying methodologies and experimental designs. They leverage agent-based systems to enhance the quality of idea generation. Beyond ideation, numerous studies also explore the use of LLMs for executing experiments (Huang et al., 2024; Tian et al., 2024) or combining both idea generation and experimental execution (Li et al., 2024; Lu et al., 2024). However, these approaches often make minor modifications to existing ideas for drafting their ideas, which often lack depth and creativity."}, {"title": "Align LLMs with Human Cognitive Patterns.", "content": "As LLMs are trained with vast amounts of human data (Brown et al., 2020), this may enable them to internalize human cognitive patterns. Firstly, CoT (Wei et al., 2022) indicates that LLMs can enhance their reasoning abilities when provided with step-by-step guidance. Further research supports this notion by showing that simply prompting LLMs to engage in step-by-step reasoning can trigger better reasoning capability (Kojima et al., 2022). Additionally, Fu et al. (2022) reveals that in-depth reasoning of LLMs can be achieved with more elaborate prompts. As a result, a prompting strategy that closely emulates human cognition is likely to elicit more insightful responses from these models. Motivated by this, we propose Col to better mimic the progressive cognitive patterns of humans when generating new research ideas."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduce Chain of Ideas (CoI) agent, a framework designed to enhance the capability of LLMs in generating research ideas. The Col agent offers a promising and concise solution by organizing ideas into a chain structure, effectively mirroring the progressive development within a given research domain. It facilitates LLMs to digest the current advancements in research, thereby enhancing their ideation capabilities.p To comprehensively evaluate the capability of automated idea generation methods, we also propose Idea Arena, an evaluation system that requires the participant methods to compete in pairs about their generated ideas for the research topics, which demonstrates high agreement with human evaluation. Experimental results indicate that the Col agent consistently outperforms other methods and is capable of generating ideas comparable to human creativity."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 EVALUATION METRICS", "content": ""}, {"title": "A.2 SPECIFIC PROMPTS", "content": "Here are the prompts used in this paper.\n\u2022 Prompts used in Col construction\nPrompt used to convert a topic into a search query for literature retrieval (Table 7)\nPrompt used to evaluate whether a paper is relevant to the topic (Table 8)\nPrompt used to extract idea, experiment, entities and references from paper (Table 9) and 10\nPrompt used to summarize current trends of CoI (Table 11)\n\u2022 Prompts used in idea generation"}, {"title": "A.3 ADDITIONAL EXPERIMENT RESULTS", "content": "We present the evaluation results of idea generation for both model-based evaluation (including GPT-40, Gemini-1.5-Pro-Exp-0827, and Claude-3.5-Sonnet) and human-based evaluation in Table 26."}]}