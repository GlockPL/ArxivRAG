{"title": "LACOSTE: Exploiting stereo and temporal contexts for surgical instrument segmentation", "authors": ["Qiyuan Wang", "Shang Zhao", "Zikang Xu", "S Kevin Zhou"], "abstract": "Surgical instrument segmentation is instrumental to minimally invasive surgeries and related applications. Most previous methods formulate this task as single-frame-based instance segmentation while ignoring the natural temporal and stereo attributes of a surgical video. As a result, these methods are less robust against the appearance variation through temporal motion and view change. In this work, we propose a novel LACOSTE model that exploits Location-Agnostic COntexts in Stereo and Temporal images for improved surgical instrument segmentation. Leveraging a query-based segmentation model as core, we design three performance-enhancing modules. Firstly, we design a disparity-guided feature propagation module to enhance depth-aware features explicitly. To generalize well for even only a monocular video, we apply a pseudo stereo scheme to generate complementary right images. Secondly, we propose a stereo-temporal set classifier, which aggregates stereo-temporal contexts in a universal way for making a consolidated prediction and mitigates transient failures. Finally, we propose a location-agnostic classifier to decouple the location bias from mask prediction and enhance the feature semantics. We extensively validate our approach on three public surgical video datasets, including two benchmarks from EndoVis Challenges and one real radical prostatectomy surgery dataset GraSP. Experimental results demonstrate the promising performances of our method, which consistently achieves comparable or favorable results with previous state-of-the-art approaches.", "sections": [{"title": "1. Introduction", "content": "Computer-assisted intervention (CAI) has emerged as a transformative force in surgical procedures as it enhances patient safety, improves operative quality, reduces adverse event, and shortens recovery period (Maier-Hein et al., 2017). In this context, achieving semantic and instance segmentation of a surgical scene, as captured by surgical stereo cameras, plays a critical role in modern CAI systems. Semantic annotations enable cognitive assistance by providing pixel-wise contextual awareness of instruments, which is essential for supporting various downstream tasks, including surgical decision-making (Loftus et al., 2020; Maier-Hein et al., 2022), surgical navigation (Allan et al., 2020), and skill assessment (Curtis et al., 2020; Liu et al., 2021). Accurately identifying instruments and their spatial locations is a key focus in CAI, encompassing endeavors such as tool pose estimation (Hein et al., 2021), tool tracking and control (Du et al., 2019), and surgical task automation (Nagy and"}, {"title": "2. Related Work", "content": "CNN-based SIS. Initial efforts in surgical robotics community are based on Convolutional Neural Networks (CNNs). For instance, TernausNet (Shvets et al., 2018) proposes a pretrained U-Net (Ronneberger et al., 2015) model for the segmentation"}, {"title": "3. Query-Based Segmentation", "content": "The QBS method partitions image pixels into N segments by predicting N binary masks and N corresponding category labels, where N is significantly larger than the real segment number \u00d1. QBS represents each segment with a feature vector (\u201cobject query embedding\u201d) which can be processed into category label and binary mask. The key challenge is to find good representations for each segment. For simplicity, we term object query embedding as object query in the following. As concluded in (Cheng et al., 2021a), a meta QBS architecture would be composed of three components: backbone, pixel decoder, and Transformer decoder together with trainable queries.\nGiven an image I, the backbone @ first extracts low-resolution features. Then, the pixel decoder & that gradually upsamples low-resolution features from the backbone to generate high-resolution per-pixel features F. Finally, the Transformer decoder together with N initial learnable query embeddings Q = {qn}Nn=1 operates on image features to process N object query embeddings {en}Nn=1. The final predictions of N segments are a set of N probability-embedding-mask pairs z = {(pn, en, Mn)}Nn=1, where the probability distribution pn contains C classes and an auxiliary \u201cno object\u201d label (\u00d8) to denote segments that do not correspond to any classes; en is object query embedding to represent segment, where n means query index; and mn is binary mask. The object query assigned with \"no object\" label is termed as a non-object query. In this research, we formulate each segment as instance and merge the selected instances into one semantic segmentation map.\nThe overall pipeline can be illustrated in Figure 1.\nThe training losses are composed of classification and binary segmentation parts as represented in Eq. (1), where \u03bbbce, \u03bbdice and \u03bbcls are weighted hyper-parameters. Hungarian matching makes instances matching between the predictions and ground truths z = {(Cn, mn)}Nn=1 (Cn/mn means class/binary mask ground truth). The cross entropy (CE) loss Lcls is applied for classification. For binary segmentation, the joint losses include binary cross entropy (BCE) loss Lbce and the Dice loss Ldice."}, {"title": "3.1. QBS Preliminaries", "content": "The QBS method partitions image pixels into N segments by predicting N binary masks and N corresponding category labels, where N is significantly larger than the real segment number \u00d1. QBS represents each segment with a feature vector (\u201cobject query embedding\u201d) which can be processed into category label and binary mask. The key challenge is to find good representations for each segment. For simplicity, we term object query embedding as object query in the following. As concluded in (Cheng et al., 2021a), a meta QBS architecture would be composed of three components: backbone, pixel decoder, and Transformer decoder together with trainable queries.\nGiven an image I, the backbone @ first extracts low-resolution features. Then, the pixel decoder & that gradually upsamples low-resolution features from the backbone to generate high-resolution per-pixel features F. Finally, the Transformer decoder together with N initial learnable query embeddings Q = {qn}Nn=1 operates on image features to process N object query embeddings {en}Nn=1. The final predictions of N segments are a set of N probability-embedding-mask pairs z = {(pn, en, Mn)}Nn=1, where the probability distribution pn contains C classes and an auxiliary \u201cno object\u201d label (\u00d8) to denote segments that do not correspond to any classes; en is object query embedding to represent segment, where n means query index; and mn is binary mask. The object query assigned with \"no object\" label is termed as a non-object query. In this research, we formulate each segment as instance and merge the selected instances into one semantic segmentation map.\nThe overall pipeline can be illustrated in Figure 1.\nThe training losses are composed of classification and binary segmentation parts as represented in Eq. (1), where Abce, Adice and Acls are weighted hyper-parameters. Hungarian matching makes instances matching between the predictions and ground truths z = {(Cn, mn)}Nn=1 (Cn/mn means class/binary mask ground truth). The cross entropy (CE) loss Lels is applied for classification. For binary segmentation, the joint losses include binary cross entropy (BCE) loss Lbce and the Dice loss Ldice."}, {"title": "3.2. Limitations of Current QBS", "content": "However, the original QBS framework does have limitations when applied to the SIS tasks. Firstly, current methods for SIS disregard additional information from stereo view which can enhance the precision of instrument localization and recognition. Secondly, prior studies typically focus on frame-wise predictions, neglecting the temporal characteristics of surgical videos, thereby leading to transient failures. Finally, QBS methods derive binary mask and instrument category of each segment from the same object query, wherein inherent location biases detrimentally impact instrument recognition. These limitations motivate us to propose the LACOSTE model, which follows the QBS paradigm (Mask2Former) and improves the effectiveness of current SIS works accompanied with three proposed modules including DFP, STSCls and LACls."}, {"title": "4. The LACOSTE Method", "content": "LACOSTE adheres to the QBS paradigm in conjunction with three proposed modules: DFP, STSCls, and LACls. The overall inference pipeline of LACOSTE can be divided into three steps which aim to enhancing mask classification ability from various perspectives as illustrated in Figure 2. For Frame Step, the QBS baseline with DFP (termed as BDFP) explores depth-aware information from stereo views and makes frame-wise prediction for each timestamp. For Tracklet Step, STSCIs aggregates temporal or stereo contexts contained in tracklets which are composed of object queries from frame step and makes a consolidated tracklet prediction. For L-Agnostic Step, LACls extracts instrument content information decomposed with location biases and predicts a location-agnostic prediction. The ensemble results from three steps replace original QBS classification results as the final classification predictions and final binary mask predictions keep same as those from frame step.\nWe define some mathematical notions in Table 1 and formulate the inference process of LACOSTE for current frame IL(t*) as Algorithm 1. The upper suffix for three steps and final outputs are represented by b, s, a and f, respectively. Given a current frame IL(t*) within a stereo clip {(IL(t), IR(t))}Tt=1 (where t* means current time, T means clip length, t* \u2208 [1, T]),"}, {"title": "4.1. The Three Inference Steps", "content": "LACOSTE adheres to the QBS paradigm in conjunction with three proposed modules: DFP, STSCls, and LACls. The overall inference pipeline of LACOSTE can be divided into three steps which aim to enhancing mask classification ability from various perspectives as illustrated in Figure 2. For Frame Step, the QBS baseline with DFP (termed as BDFP) explores depth-aware information from stereo views and makes frame-wise prediction for each timestamp. For Tracklet Step, STSCIs aggregates temporal or stereo contexts contained in tracklets which are composed of object queries from frame step and makes a consolidated tracklet prediction. For L-Agnostic Step, LACls extracts instrument content information decomposed with location biases and predicts a location-agnostic prediction. The ensemble results from three steps replace original QBS classification results as the final classification predictions and final binary mask predictions keep same as those from frame step.\nWe define some mathematical notions in Table 1 and formulate the inference process of LACOSTE for current frame IL(t*) as Algorithm 1. The upper suffix for three steps and final outputs are represented by b, s, a and f, respectively. Given a current frame IL(t*) within a stereo clip {(IL(t), IR(t))}Tt=1 (where t* means current time, T means clip length, t* \u2208 [1, T]),"}, {"title": "4.2. Disparity-Guided Feature Propagation (DFP)", "content": "DFP Structure & Formulation. As presented in the top row of Figure 3, DFP makes a bridge to exploring stereo information at feature level inspired by temporal tasks (Zhao et al.,"}, {"title": "4.3. Stereo-Temporal Set Classifier (STSCls)", "content": "Forming tracklets from object queries is the first step for STSCls to make tracklet-level prediction. However, it is difficult for most surgical datasets to get video identity annotations, which describe instance correlations among frames. The naive tracklet sampling mechanism or training a track head is unrealizable for missing identity IDs. How to label pseudo identity ID for each object query by exploring temporal and stereo contexts is a major challenge for forming tracklets.\nHow to generate pseudo identity ID for object queries? Query Alignment. Considering that object queries are good representation for instances, they can make a bridge to align identities between frames. We present a simple but valid query alignment mechanism to align identity correlation across both temporal and stereo dimensions. As illustrate in \u201cQuery Alignment\" section of Figure 4, given a pair of current frame IL(t),"}, {"title": "4.4. Location-Agnostic Classifier (LACls)", "content": "LACls explicitly decouples mask classification with mask segmentation process to migrate the adverse influence of location biases. This module receives processed images that crop and mask original images based on mask predictions {m\u00a3(t*)}Nn=1 of BDFP and outputs the location-agnostic class prediction {pa(t*)}Nn=1. For simplicity, we use a vision Transformer pretrained on natural images as LACls in this work. We define the class token of the last layer as location-agnostic object query embedding ea."}, {"title": "4.5. Overall Training & Inference Pipeline.", "content": "In fact, LACOSTE can be trained in an end-to-end fashion. However, to reduce the training costs, we train the BDFP and STSCls jointly as Algorithm 2 and train LACls only once for each fold separately. In training stage, the formers are supervised with a combined loss as represented in Eq. (11) and the"}, {"title": "5. Experiments", "content": "We experiment our LACOSTE on two public benchmark datasets of surgical videos, EndoVis2017 and EndoVis2018. Furthermore, dataset in other surgical domain, GraSP, is applied to validate the generality of method. Especially, we conduct comparison with state-of-the-art approaches, extensive ablation analysis on key components, and detailed visualization to validate the effectiveness of our approach."}, {"title": "5.1. Datasets & Implementation", "content": "EndoVis2017 (EV17). The EndoVis2017 dataset (Allan et al., 2019) comprises ten video sequences captured from the da-Vinci robotic surgical system, accompanied by instance annotations for six distinct robotic instruments and an ultrasound probe. In order to facilitate equitable comparisons, we adopt a four-fold cross-validation in common with previous methods from a total of 1,800 frames (8 \u00d7 225). The fold-wise split yields 1,350 and 450 frames for training and validation, respectively.\nEndoVis2018 (EV18). The EndoVis2018 dataset (Allan et al., 2020) is collected from 2018 MICCAI Robotic Scene Segmentation Challenge. This dataset consists of 19 sequences, officially divided into 15 for training and 4 for testing. Each training sequence contains 149 frames recorded on a da Vinci X or Xi system during porcine training procedure. Each frame has a high resolution of 1280\u00d71024. We use pre-defined training and validation splits from (Shvets et al., 2018) and annotate instances by ourselves for experiments.\nGraSP (GRASP). The GraSP dataset (Ayobi Mendoza, 2023) is a new curated benchmark that models surgical scene understanding. This dataset contains 13 sequences, officially divided into 8 for training and 5 for testing. This dataset provides multi-granularity including short-term segmentation and long-term recognition annotations. Each frame has a high resolution of 800\u00d71280. It's noteworthy that GraSP provides monocular consecutive frames but only pixel-wise segmentation annota-"}, {"title": "5.2. Main Results", "content": "EndoVis 2017 & 2018. We compare our LACOSTE with state-of-the-art approaches including not only single frame methods, S3Net and QPD, but also temporal methods, MATIS. For fair comparison, we validate our approach based on Mask2Former with different backbones. For most experiments in this work, We implement our method based on SwinBase backbone. To compare with MATIS, we implement a parameters-equivalent one with the same SwinSmall backbone. QPD is implemented with MaskDino (Li et al., 2023), which is designed with a SwinLarge backbone and 300 queries. We also present a heavy-weight model with a SwinLarge backbone in this paper. For simplicity, we symbol SwinBase, SwinSmall and SwinLarge as B, S and L in the following parts. Results of different methods are presented in Table 2. We export the best results of comparison methods reported in their papers. For EV17, LACOSTE(B) and LACOSTE(L) outperforms all other methods while LACOSTE(S) is inferior to QPD because of less parameters. LACOSTE(L) improves over QPD by 7% Ch_IoU, 1% ISI_IOU, and 28% mcIoU showing that temporal-stereo information improves the results by a considerable margin. Even for other temporal consistency methods, LACOSTE(S) outperforms MATIS by an improvement of 7% Ch_IOU, 9% ISI_IOU and 17% mcIoU while LACOSTE(L) improves 17% Ch_IoU, 21% ISI_IOU and 55% mcIoU respectively. For EV18, LACOSTE is superior to single frame methods obviously. LACOSTE(L) improves over QPD by 12% Ch_IoU, 9% ISI_IOU, and 53% mcIoU. Moreover, LACOSTE(S) precedes MATIS slightly with 1% Ch_IoU, 4% ISI_IOU, and 3% mcIoU while LACOSTE(L) improves 3%, 8%, and 24%, respectively. As noted above, temporal information is more effective for EV18 and enhancing inter-instruments discrepancy is more important for EV17. The performance improvement between LACOSTE(B) and LACOSTE(S) is more obvious than that changing the backbone from SwinBase to SwinLarge. For better trade-off between training costs and performance, we validate the following experiments and analysis based on LACOSTE(B).\nGraSP. We compare our LACOSTE with TAPIS (Ayobi Mendoza, 2023) which is a multi-task method for surgical scene understanding and other state-of-the-art approaches reported in their paper. Evaluations are implemented not only in semantic segmentation but also in instance segmentation. In particular, Ch_IOU, ISI_IOU and mcIoU are selected for semantic performance while AP50box and AP50segm are used for instance performance. Results of different methods are presented in Table 3. Observe that LACOSTE(L) attains the best overall results of 80.07 mcIoU and 84.81 ISI_IOU. In the task of instance segmentation, as measured by the AP50segm metric, LACOSTE (B) demonstrates superior performance compared to"}, {"title": "5.3. Ablation Study", "content": "We conduct ablation experiments to validate the effectiveness of different key components in the proposed method and obtain seven configurations:\n(a) Baseline: We train the pure Mask2former network as the baseline;\n(b) Baseline (DFP): We train the baseline Mas2former with DFP;\n(c) Baseline (DFP, T): We train STSCls with augmented tracklets by mixing object queries across only temporal dimension;\n(d) Baseline (DFP, S): We train STSCls with augmented tracklets by mixing object queries across only stereo dimension;\n(e) Baseline (DFP, ST): We train STSCls with augmented tracklets by mixing object queries across both temporal and stereo dimensions;\n(f) Baseline (DFP, ST, LACIs): We apply a LACls without fine-tuning after (e);\n(g) Baseline (DFP, ST, LACls): We fine-tune a LACls after (e) for some iterations;\nEffectiveness of Key Components. The results on EV17 and EV18 are presented in Table 4. We observe that our baseline based on SwinBase backbone obtains reasonable results with Ch_IoU and ISI_IoU over 70 on all tasks of both datasets. Purely introducing DFP module on stereo dimension, (b) achieves better results by 4% Ch_IoU, 3% ISI_IOU, 8% mcIoU for EV17 and 2% Ch_IoU, 3% ISI_IOU, 7% mcIoU for EV18. Incorporating temporal-stereo consistency cues by STSCls, (c) (d) (e) further improves the segmentation performance in all evaluation metrics of both datasets. For different augmented tracklets configure settings, we observe that (e) is superior to the other two on most metrics especially mcIoU. We compare (e) with (b) to evaluate the effectiveness of STSCls intuitively. Ch_I0U and ISI_IoU on EV17 are increased respectively 3.55, and 4.39. The same trends rising 1.69 Ch_IoU and 1.5 ISI_IoU can also be observed on EV18. Adding our LACls without any fine-tuning operations, our full model continues boosting the results with 0.4 Ch_IoU and 0.4~1 ISI_IoU gain, peaking at 82.31 Ch_IoU, 78.56 ISI_IoU on EV17 and 86.48 Ch_IoU 85.09 ISI_IoU on EV18. The architecture achieves 82.90 Ch_IoU, 79.05 ISI_IoU, 61.76 mcIoU for EV17 and 86.66"}, {"title": "5.4. Qualitative Analysis", "content": "Segmentation Results. Figure 5 illustrates the qualitative segmentation results. We show the comparative results of a single frame approach named S3Net, a typical temporal consistency approach named MATIS, our baseline, and our proposed"}, {"title": "Temporal Consistency of Segmentation Results", "content": "We further analysis temporal consistency of segmentation results and make comparison with some video object segmentation (VOS) methods including space-time memory network (Oh et al., 2019), STCN (Cheng et al., 2021b) and XMEM (Cheng and Schwing, 2022). Given the absence of video annotations for most surgical datasets, we employed pretrained Video Object Segmentation (VOS) models for inference. Specifically, utilizing the segmentation annotation from the initial frame, VOS methods, leveraging the officially pretrained parameters, infer masks across a sequence of eight frames. For the LACOSTE model, we maintained the original inference process without utilizing the initial frame annotation. The visualization results are depicted in Figure 10. Additionally, we present several VOS evaluations, including Jaccard index I, contour accuracy F, and their average J&F, to assess temporal consistency in Table 10. Both qualitative and quantitative analyses on examples indicate that LACOSTE achieves comparable temporal consistency. It is important to clarify that our intention is not to assert a superior temporal consistency compared to VOS methods. Instead, we aim to offer researchers potential insights and alternatives for establishing temporal correlations in scenarios where video-level annotations are unavailable."}, {"title": "6. Conclusion", "content": "In this study, we systematically explore temporal information and stereo cue in surgical instrument segmentation tasks. LACOSTE extends common query-based segmentation methods with proposed disparity-guided feature propagation module, stereo-temporal set classifier and location-agnostic classifier to mitigate surgical domain challenges. Exhaustive experiments have been conducted on the benchmark robot-assisted surgery datasets.Our method generalizes well on all benchmarks and achieves comparable or favorable results with previous state-of-the-art approaches. We conclude that introducing temporal and stereo information improves the results for applications involving complicated classification in different circumstances. The proposed framework can be helpful to downstream applications that depend on tool identification and segmentation. We hope that our analysis and the innovations to mitigate the challenges specific to surgical instruments will spark similar interests in introducing specific information and attributes in other domains."}], "equations": ["Lets = \\sum_{n=1}^N CE(p_n, c_n),", "L_{bce} = \\sum_{n=1}^N BCE(m_n, \\hat{m}_n), L_{dice} = \\sum_{n=1}^N Dice(m_n, \\hat{m}_n),", "L_{baseline} = \\lambda_{bce} L_{bce} + \\lambda_{dice}L_{dice} + \\lambda_{cls} L_{cls}.", "\\bigcup_{L,R}\\{(e_n^b(t), p_n^b(t), m_n^b(t))\\}_{n=1}^N \n\\bigcup_{L,R}\\{(e_n^b(t))\\}_{n=1}^N", "t = t^*,\nt \\ne t^*.", "\\breve{D}(t) = d_s  \\frac{Z_{max}(t)}{Z(t)}", "\\hat{I}^R(t) = F(\\breve{D}(t), I^L(t)).", "L_{sc} = CE(p^s, y_{ot}), L_{ic} \\sum_{m=1}^M CE(p^l_m, g_{tm}).", "L_{ida} = \\sum_{i=1}^K\\sum_{j=1}^K BCE(m_{ij}, \\hat{m}_{ij}).", "L_{STSCls} = L_{sc} + L_{lc} + L_{ida}.", "L_{LACIS} = \\sum_{n=1}^N CE(p_n^a, c_n).", "L_{total} = L_{baseline} + L_{STSCls}."]}