{"title": "CREST: EFFECTIVELY COMPACTING A DATASTORE FOR RETRIEVAL-BASED SPECULATIVE DECODING", "authors": ["Sophia Ho", "Jinsol Park", "Patrick Wang"], "abstract": "We present CREST (Compact Retrieval-Based Speculative Decoding), a redesign of REST that allows it to be effectively \"compacted\u201d. REST is a drafting technique for speculative decoding based on retrieving exact n-gram matches of the most recent n tokens generated by the target LLM from a datastore. The key idea of CREST is to only store a subset of the smallest and most common n-grams in the datastore with the hope of achieving comparable performance with less storage space. We found that storing a subset of n-grams both reduces storage space and improves performance. CREST matches REST's accepted token length with 10.6-13.5x less storage space and achieves a 16.5-17.1% higher acceptance length than REST using the same storage space on the HumanEval and MT Bench benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Speculative Decoding has gained traction for accelerating LLM inference by employing a small draft model to reduce inference latency without sacrificing the quality of the outcome sequence (Spector & Re, 2023; Chen et al., 2023; Yan et al., 2024). The small draft model is used to \"draft\" tokens which are sent to the LLM for verification. If the LLM verifies the predictions from the small model are correct, then those draft tokens are accepted, otherwise, they are regenerated by the LLM. This approach is efficient because the large model verification step can be done in parallel. However, the success of this strategy is heavily dependent on the quality of the small draft model and this often requires training a draft model for specific language tasks. This introduces additional complexity and overhead in the training and deployment of a second language model.\nREST (He et al., 2023), which stands for retrieval-based speculative decoding, takes a different approach. Instead of using a draft model, REST uses a static datastore constructed from a pre-training dataset. REST takes the most recent n tokens generated by the LLM and attempts to find exact matches in the datastore. It then combines the sequences of tokens after the exact matches into a token tree to pass to the LLM. The key benefit of REST is that it does not require any ML components, allowing it to work out-of-the-box with any LLM.\nAlthough REST can achieve a high draft token acceptance rate, the static nature of the datastore introduces a new challenge regarding storage space. REST stores the entire text of a pre-training dataset as-is, and the way to improve the accuracy of REST is to simply append more text to the datastore. However, this grows the size of the datastore unboundedly, motivating the need for a method to \"compact\" a datastore. Due to the way REST is designed (see subsection 3.2), attempting to compact the datastore by removing specific tokens in the middle of the dataset is highly disruptive and not a feasible strategy.\nOur method, which we have named CREST (Compact Retrieval-Based Speculative Decoding) addresses these flaws by redesigning REST such that it is possible to remove select tokens. At a high level, we \"decouple\" n-grams from each other in the datastore such that tokens do not simultaneously serve as parts of multiple n-grams. With this redesign, it is possible to intelligently select a subset of n-grams to keep, allowing the datastore to be effectively compacted. We tried a strategy of only keeping the smallest and most common n-grams. We found that this reduced storage space and, surprisingly, increased our accepted length.\nWe summarize our contributions below:\n1. We redesign the underlying structure of the REST datastore to \"decouple\" n-grams from each other.\n2. We develop a compaction strategy that has better compaction behavior than REST and outperforms REST at all datastore sizes.\n3. In addition to performing better at smaller sizes, CREST can achieve up to 17.1% higher accepted"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Speculative Decoding", "content": "Speculative Decoding can be conceptualized as a decoding strategy known as Draft-then-Verify, where the process involves generating multiple potential tokens at each step and then verifying them concurrently against the target Language Model (LLM) to expedite inference (Xia et al., 2024).\nSpeculative Decoding started from Blockwise Decoding introduced in Stern et al. (2018), which enhanced the Transformer decoder with additional feed-forward neural network (FFNN) heads. This augmentation facilitates the simultaneous generation of multiple tokens in each step, subsequently validated in parallel by the original LLM to ensure coherence with its outputs.\nSubsequent research employed smaller language models as drafting agents. For instance, Xia et al. (2023) employs a specialized non-autoregressive transformer as an independent drafter. Similarly, other works (Leviathan et al., 2023; Chen et al., 2023) utilize smaller versions of LLMs as drafting models, enabling speculative decoding without the need for additional fine-tuning.\nSome approaches choose to utilize the target LLM itself instead of introducing additional drafting models. Medusa (Cai et al., 2024) integrates FFNN heads into the transformer decoder, enabling parallel token generation per step. Other works (Yang et al., 2023; Zhang et al., 2023) take a more systemic approach, and choose to use additional subprocesses or adaptively skip several intermediate layers for efficiency.\nAlternatively, certain methods focus on enhancing parallel decoding performance without altering the model architecture. (Monea et al., 2023) introduces learnable tokens, and fine-tune their new embeddings on a small training dataset to improve parallel decoding efficiency."}, {"title": "2.2 Retrieval-Based Speculative Decoding (REST)", "content": "In contrast to other approaches, REST utilizes an existing text dataset to facilitate speculative decoding. It achieves this by retrieving n-grams from the datastore that match the latest n tokens of the current context. Specifically, REST operates on a pre-constructed datastore comprising pairs of (context, continuation), employing an exact-match method for fast retrieval. After gathering retrieved results, REST combines the continuations into a token tree to pass to the LLM for verification.\nA key advantage of REST is that it does not use ML components to draft tokens, thus eliminating the need for any form of training or serving. In contrast to previous methods that rely on a draft language model for speculative decoding, REST can be integrated out-of-the-box with any target LLM, without requiring additional training."}, {"title": "3 PROBLEM", "content": "Our approach is motivated by the difficulty of effectively \"compacting\" the REST datastore while retaining as much performance as possible. Such compaction is important to avoid the size of the datastore growing unboundedly as we use larger and larger pre-training datasets to build the datastore. We first give detailed background on the REST datastore's design, discuss the problems we noticed with it, and give a high level motivation of our proposed solution."}, {"title": "3.1 REST Datastore Background", "content": "Datasets and flattened datasets: In the context of REST, a dataset is an unordered set of conversations. A conversation refers to an ordered sequence of alternating prompts and responses. A prompt is simply an ordered sequence of tokens generated by a human while a response is an ordered sequence of tokens generated by an LLM.\nWhile a dataset is a complex nested data structure, it can be flattened into a single ordered sequence of tokens. First, each conversation is flattened by concatenating the tokens of the prompts and responses together. Then, the conversations are put in an arbitrary order and their flattened representations are concatenated. The final representation is called a flattened dataset. For the rest of the paper, when we use the word \"dataset\", we are referring to a flattened dataset.\nREST data structures: The REST datastore takes a flattened dataset and splits the conversations of the dataset into chunks of token sequences. The order of tokens within a chunk is maintained from the flattened dataset, but the order of different chunks in the REST datastore is arbitrary. In the current implementation of REST, each chunk is 512MB (except for the last chunk, which may be smaller).\nThen, REST creates an auxiliary index structure over each chunk called a suffix array (Manber & Myers, 1990). This structure is a list of integers where each integer refers to a position in the 512MB chunk of tokens. Each integer conceptually represents a suffix of the 512MB chunk, which refers to the sequence of tokens from that integer position in the chunk until the end of the chunk. The integers in the suffix array are ordered such that the suffixes represented by the integers are in alphabetical order. Figure 1 shows an example of a suffix array. Note that while the figure shows a suffix array where each token is a character, in REST, each token is a 32-bit integer."}, {"title": "3.2 REST Datastore Problems", "content": "Density/Layering: With the suffix array, REST can store its datastore in an extremely dense fashion. N-grams of different sizes are \u201clayered\u201d in place on top of each other, and the same tokens serve simultaneously as both contexts and continuations in different situations. While such density is impressive, it does make it difficult to compact the REST datastore by selectively removing tokens. Fundamentally, this is because removing a single token in the REST datastore partitions the chunk into two pieces, eliminating about $2^{m-1}$ contexts and $2^{l} - 1$ continuations where m is the size of a matched context while l is the length of the continuation\u00b9. The most straightforward way to increase drafting accuracy in REST is to add more data to the datastore. However, this causes the datastore's size to grow unboundedly because there is no way to subsequently \u201ccompact\" the datastore.\nTime complexity: REST's search algorithm has a time complexity of O(c log n), where c is the number of chunks and $n_c$ is the average number of tokens in a chunk. Currently, each chunk is of a constant size, meaning that REST's overall time complexity is O(n) where n is the total number of tokens in the datastore. Even with an ideal implementation of REST where c = 1, the best achievable time complexity in REST is O(log n)."}, {"title": "3.3 Compact-REST (CREST) Motivation", "content": "In CREST, we aim to solve both of the problems mentioned in subsection 3.2 at the same time. Instead of densely \"layering\" all contexts and continuations on top of each other, our idea was to \"decouple\" the contexts and continuations into a dictionary, where contexts would map directly to token trees built from continuations and be completely separate from one another. Decoupling all the contexts from each other yields a large initial increase in datastore size but opens up"}, {"title": "4 METHODS", "content": ""}, {"title": "4.1 System Overview", "content": "Figure 5 shows the different components of the CREST system. Our contributions are highlighted in blue. Note that arrows are highlighted in addition to components because many challenges we faced involved integration issues, not just the core CREST components.\nThe dataset contains past conversations between a human and an LLM. The REST datastore is built by flattening the dataset and building a suffix array over it (see subsection 3.1). Next, the CREST datastore is built by analyzing the dataset to find the smallest and most common n-grams and then querying the REST datastore with these n-grams. Both of these datastores are then used to draft token trees to pass to a Target LLM for verification. Finally, the accepted length of both datastores is compared under a benchmark."}, {"title": "4.2 Building the CREST Datastore", "content": "The CREST datastore is a hash map that maps from n-grams to token trees. To be scalable, this hash map must be \"disk-native\u201d. By \u201cdisk-native\", we mean that you can directly traverse the file on disk to find matching n-grams (i.e. it requires O(1) memory). Additionally, a key feature of CREST is that the n-grams it stores are a subset of the n-grams present in the dataset.\nBuilding an efficient disk-native datastore: Our original prototype was a Python dictionary which we serialized to disk with the pickle library. To make this approach disk-native, we switched to using Postgres. Instead of pickling the entire dictionary, we pickled each token tree and stored them as blobs in Postgres. Using an index on n-grams, we can support either O(log n) or O(1) lookups for exact n-gram matches depending on whether a tree-based or hash-based index is used\u00b2 Additionally, we noticed that the token trees contained attention masks which had a large number of consecutive 0s and 1s, so we utilized the 1 zma Python library to compress these values before storing them in the database.\nIntelligently selecting a subset of n-grams: There are two benefits to only storing a subset of the n-grams in the CREST datastore. The first benefit is obvious: storing a subset of the n-grams takes up less storage space than storing the complete set of n-grams. The second benefit was surprising to us: storing a subset of the n-grams can lead to a higher accepted length. The key reason behind this is that at any point in time during inference, there is more than one n-gram that matches the suffix of the currently generated text.\nBy removing n-grams with \u201clow quality\u201d token trees from the datastore, we bias our system to choose n-grams with \"higher quality\" token trees on average.\nThere are many possible ways to select a subset of n-grams, but we investigated the idea of selecting the smallest and most common n-grams for this project. One important design decision we thought about was how to handle n-grams of different values of n. We chose to investigate both storing the most common t n-grams of a single value of n as well as storing the most common t n-grams of all n less than or equal to some \"maximum size\" m. In section 5, we found that the second method performed better than the first.\""}, {"title": "5 EVALUATION", "content": ""}, {"title": "5.1 Hardware and Benchmarks", "content": "We conducted all our experiments using one NVIDIA L4 Tensor Core GPU and 8 vCPUs\u00b3. We built REST datastores using the ShareGPT and The Stack datasets, which come out to 1.1GB and 924MB of disk storage respectively. The ShareGPT dataset consists of 120675 training samples and The Stack dataset contains 90016 training samples. The draft tokens retrieved from the ShareGPT datastore were passed to Vicuna (Chiang et al., 2023) and evaluated with the MT Bench (Zheng et al., 2023) benchmark. MT Bench is a multi-turn question set consisting of 80 multi-turn questions intended to mimic open-ended questions. The tokens drafted from The Stack datastore were sent to CodeLlama (Roziere et al., 2023) and evaluated with the HumanEval (Chen et al., 2021) benchmark. The HumanEval dataset consists of 164 human-written coding problems and evaluates a model's ability to generate correct code from the docstrings. We used the 7B versions of Vicuna and CodeLlama and set the maximum number of tokens generated to 1024 and 512, respectively. We used REST's default hyperparameters: a maximum token tree size of 64, a maximum of 5000 matched contexts per search, and 10 token continuations after each matched context."}, {"title": "5.2 Comparison with REST", "content": "As our baseline, we constructed REST datastores using a random sample of 1%, 2%, 4%, 8,% 16% 32%, 64%, and 100% of the training samples for each of the two datasets. We constructed CREST datastores using our combined top-t strategy (see subsection 5.4). We used n \u2264 4 for the ShareGPT CREST datastore and n \u2264 5 for the Stack CREST datastore. We used values of t which we estimated would yield CREST datastores of similar sizes to the REST datastores we built.\nInitially our experiment environment was on the PSC machines, however, we had many challenges during setup so we migrated our experiments to AWS."}, {"title": "5.3 Single n Top t Experiments", "content": "When it comes to selecting the most common n-grams, it is non-trivial to compare the frequencies of n-grams of different sizes (see subsection 4.2). This is because, given an n-gram g of size n, any n-gram g' of size < n that appears within g is guaranteed to show up at least as often as g. Given this, we first tried building CREST using the top t most common n-grams for single values of n.\nOur results are shown in Figure 8. As the number of n-grams increases, the accepted length increases, which is expected. However, the rate at which the accepted length increases decreases, confirming our hypothesis that common n-grams are more important to have in the datastore than uncommon n-grams. On both the ShareGPT datastore and Stack datastores, n = 2 gives the highest accepted length compared to any other individual value of n. However, the relative \"drop-off\" of increasing n is steeper for ShareGPT than it is for Stack. One possible reason for this is that code data (which Stack contains) is more structured, causing larger n-grams to be more consistent in predicting the tokens that come after."}, {"title": "5.4 Multiple n Top t Experiments", "content": "To examine the trend of accepted lengths across various datastore sizes and n-gram configurations, we conducted experiments on a subset of the entire benchmark dataset. For each line \"ngram \u2264 m\", an equal number t most frequent n-grams from each 1, 2, \u2026\u2026\u2026, m n-gram groups were selected so that they add up to the specified number of n-grams on the x-axis.\nThe results are shown in Figure 9. For both datastores, we once again see that the accepted length generally increases as the number of n-grams increases, though at a decreasing rate. However, this is not always true as the accepted length peaks at 2.514 for the Stack dataset at about 1.8M n-grams\""}, {"title": "5.5 Token Tree Size Analysis", "content": "In order to focus our project on only studying a single technique and to have a fair comparison with REST, we retained the default hyperparameters recommended by REST. One of these hyperparameters is the number of tokens in the token tree, which REST set to 64. However, we later discovered that the size of token trees in practice was far smaller than 64, as seen in Table 2. Because token trees tend to get smaller as n-grams get less frequent, the size of the datastore needs to be controlled. We chose to use the same datastore sizes as the largest datastores we built in subsection 5.3 for each value of n.\nWe observe tree sizes much smaller than 64 tokens because we do not have enough occurrences of most n-grams to"}, {"title": "6 CONCLUSION", "content": "In this paper, we introduced CREST as an optimized version of REST. The key innovation behind CREST lies in the concept of \"uncoupling\" n-grams and their continuations into a dictionary structure, which enables the targeted removal of specific n-grams. We then use the \u201ctop t\u201d method to select a subset of the \"smallest, most common\u201d n-grams to include in the datastore.\nOur experimental results demonstrate that CREST outperforms REST in terms of average accepted length across all tested datastore sizes. CREST matches REST's accepted token length with 10.6-13.5x less storage space and achieves a 16.5-17.1% higher acceptance length across the HumanEval and MT Bench benchmarks. Overall, CREST is a robust architecture for retrieval-based speculative decoding that remains compact and effective even as the size of the pre-training dataset scales up."}]}