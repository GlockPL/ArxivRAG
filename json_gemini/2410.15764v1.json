{"title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec", "authors": ["Yiwei Guo", "Zhihan Li", "Chenpeng Du", "Hankun Wang", "Xie Chen", "Kai Yu"], "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a three-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction experiments, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. The 25Hz version of LSCodec also achieves the lowest bitrate (0.25kbps) of codecs so far with decent quality. Voice conversion evaluations prove the satisfactory speaker disentanglement of LSCodec, and ablation study further verifies the effectiveness of the proposed training framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Currently, discrete speech tokens have laid a foundation for speech generation tasks [1]\u2013[3] since they can be treated in a similar manner with the prevailing language models (LMs). By their objectives, discrete speech tokens can be divided into acoustic tokens and semantic tokens [4]. Acoustic tokens, or speech codecs like EnCodec [5] and DAC [6], aim to reconstruct audio perfectly with a vector quantization (VQ) module. While these tokens achieve remarkable audio decompression quality, they usually require multiple VQ layers which leads to extra complex designs in speech LMs, such as coarse-to-fine modeling [2], delay pattern [7] or nested prediction [3]. Tokens with lower bitrates are favored not only for transmission, but also for simplifying the speech LM architectures to reduce cost.\nDifferent from other aspects of information in speech, speaker timbre is a global trait nearly orthogonal to contents and prosody. In most acoustic tokens, timbre is repeatedly encoded across timesteps. We believe this leads to a considerable waste of coding efficiency. Contrarily, semantic tokens are derived from discriminative tasks, such as self-supervised learning (SSL) [8]\u2013[11], that focus more on contents other than acoustics. Thus these tokens usually have fewer codebooks than acoustic ones, resulting in lower bitrate. However, semantic tokens may discard much paralinguistic information including both time-invariant timbre and time-variant prosody [12], [13]. As prosody is an important part of emotion and semantics, this makes semantic tokens not ideal for speech generation neither.\nTherefore, it is highly valuable to develop a low-bitrate and speaker-disentangled discrete speech token. The bitrate of speech tokens are determined by three factors: number of quantizers Q, frame rate F and vocabulary size V for each quantizer\u00b9. With a smaller F, the length mismatch of speech and text modalities can be mitigated. With a smaller Q and V, speech LMs will have simpler training objectives. A small V can also make more utilization of acoustic BPES [14], [15]. Less speaker variations also reduces the modeling difficulty of speech LMs to concentrate only on time-variant features, and speaker timbre can be rendered after LM generation.\nFortunately, lowering compression bitrate and removing speaker information benefit each other mutually. It is common to involve designs that reduce global information in speech tokens. In [16]\u2013[18], a speaker encoder or global-level extractor is applied to provide timbre-like features into the decoder. The natural information bottleneck in the VQ layer helps reduce time-invariant features in the encoded tokens. Particularly, TiCodec [17] and Single-Codec [18] explore the scenario of only one VQ codebook. However, no explicit decoupling technique is introduced, which does not guarantee the effectiveness of the bottleneck. Single-Codec also sacrifices stream-ability by using a Conformer encoder. In [19], semantic tokens are used for guiding the first VQ layer in EnCodec, which is merely distillation instead of disentanglement. Speaker decoupling is also explored via gradient reversal with or without supervised data [20], [21], contrastive loss [22], [23] or swapped prediction [24], but these works do not pursue low-bitrate coding. Another exceptional low-bitrate codec is WavTokenizer [25] that achieves reasonable performance using a single VQ layer, but without speaker decoupling.\nIn this work, we propose LSCodec, the first effort to explicitly consider speaker decoupling that forms a low-bitrate speech token unsupervisedly. LSCodec adopts a three-stage learning process that firstly trains a speech variational autoencoder (VAE) with speaker perturbation to achieve reasonable speaker disentanglement and temporal compression. Then a VQ module is added to form a VQ-VAE where LSCodec tokens are produced. Finally, a discrete token vocoder is individually trained on the LSCodec tokens to refine audio quality. We use a simple stretching-based perturbation method to introduce explicit removal of timbre. We train two versions of LSCodec under 50 and 25Hz frame rates using only a single codebook, whose metadata is compared with other existing works in Table I. We show by experiments that LSCodec owns superior reconstruction performance albeit having a single codebook and lower bitrate than the baselines. Voice conversion (VC) experiments further verify that LSCodec achieves remarkable speaker disentanglement. Hence it paves the way for future speech LMs due to its low bitrate, high compactness and speaker invariance."}, {"title": "II. LSCODEC: LOW-BITRATE SPEAKER-DECOUPLED CODEC", "content": "The architecture and training diagram of LSCodec are illustrated in Fig.1. LSCodec adopts a three-stage learning process where speaker perturbation is applied on the first two stages. We elaborate on the perturbation method and three training stages in this section."}, {"title": "A. Speaker Perturbation", "content": "To explicitly disentangle discrete speech tokens and speaker information, appropriate modification on speaker timbre is necessary before tokenization, while keeping content and prosodic variations retained. Several techniques have been explored in previous study, such as formant & F0 scaling, random equalizers [22], and vocal tract length perturbation [26]. In this work, we use a simple time stretching approach that produces time-aligned perturbed speech with only pitch and timbre changes. Given a coefficient \u03b2, our perturbation process starts with a rate-based speed-up effect to scale the total duration of an utterance by \u03b2 times. This operation changes pitch and formant positions, hereby altering the timbre. Then, a pitch-preserving tempo effect is applied to re-stretch the utterance to its original duration, via the WSOLA algorithm [27]. This perturbation\u00b3 only changes the global pitch position and timbre features while retaining the content and pitch variations. Therefore, to reconstruct speech, timbre must be learned from additional inputs instead of the perturbed segments. With a strong information bottleneck in the reconstruction process, speaker can be explicitly disentangled from the speech tokens then. In training stage, \u03b2 is independently sampled within an interval centered at 1 for every utterance to provide randomized variations."}, {"title": "B. Training Stage 1: Speech VAE", "content": "The first training stage of LSCodec is a speech VAE in a continuous space. Given an utterance waveform, we randomly cut it into a reference prompt and content segment. The content segment \u00e6 is passed to the perturbation algorithm in Sec.II-A with a random \u03b2. Then, a convolutional neural network (CNN) encoder compresses the signal in time domain and outputs isotropic Gaussian posteriors \u039d(\u03bc\u03c4, \u03c3\u03c4) for each frame t. The frame rate is controlled by the CNN strides. The sampled posteriors are fed to a Conformer [28]-based decoder. To form an information bottleneck before the decoder like [26], [29], we provide the decoder with sufficient timbre information. We use the position-agnostic cross attention [30], [31] to feed timbre from prompt segments into the decoder, as it exhibits superior timbre controllability. To provide more discriminative timbre features, we use the WavLM [11] SSL model to extract hidden embeddings from reference prompt, inspired by its widely-verified advantage on speaker verification [32], [33]. As the reference and content segments are different and the cross attention mechanism is position-agnostic, the reference prompt can almost only provide timbre information. With sufficient timbre provided in the decoder and a perturbed input, the information bottleneck formed by VAE naturally disentangles the posterior and timbre to a degree. The WavLM embeddings are fed to a CNN prompt prenet before entering the decoder.\nThe decoder predicts two features: mel-spectrogram and SSL semantic tokens from the original un-perturbed content segment. In other words, the VAE model performs multi-task learning that simultaneously regresses towards acoustic mel features and classifies towards SSL semantic tokens in a fixed vocabulary size. The ground truth SSL tokens are extracted using HuBERT [9] and k-means clustering. The prediction of mel-spectrograms ensures reconstruction ability of the bottleneck features. As semantic tokens retain rich content information with a compact discrete space, the SSL token prediction task is crucial for guiding the bottleneck features to encode sufficient contents. Meanwhile, prosody information is largely damaged in the HuBERT tokens [12], so keeping the mel prediction task ensures prosody to be encoded in the bottleneck. Note that SSL models do not require labels to train, so the whole training pipeline is still unsupervised unlike FACodec [21].\nTherefore in this stage, the training loss contains a KL loss LKL = \u2211t=1^T DKL(N(\u03bct, \u03c3t)||N(0,1)), an L1 loss of predicted mel features Lrecon and cross entropy loss of predicted SSL indexes Lidx. Three losses are weighted with \u03b3KL, \u03b3recon and \u03b3idx. Due to the information bottleneck in this VAE, speaker timbre can already be decoupled to some extent in a continuous space."}, {"title": "C. Training Stage 2: Speech VQ-VAE", "content": "Next, we inject a VQ layer with codebook size V into the trained VAE to obtain the desired LSCodec tokens. We first extract the Gaussian means \u03bc from the VAE using a portion of training data, and perform a V-centroid k-means clustering for initializing the codebook. The architecture of VQ-VAE remains almost identical to the VAE in stage 1, and we resume the encoder, decoder and prompt prenet parameters before training. The VQ layer quantizes the encoder outputs to the codebook entry with the smallest Euclidean distance in V candidates. Straight-through estimator [34] is applied for gradient back-propagation. The quantized tokens are mapped to code-vectors before entering the decoder. The codebook in the VQ layer is updated via exponential moving average (EMA), following [5]. The training criterion also inherits that from stage 1, except for replacing the KL loss with the commitment loss [34] Lcmt in the VQ layer. Note that the dimensions for previous Gaussian variances are disabled in this stage. The loss Lcmt is weighted via \u03b3cmt.\nWith a VQ layer and resumed parameters, this VQ-VAE constructs a discrete space based on the insufficiently speaker-decoupled continuous space from stage 1. The information bottleneck thus restricts timbre from being encoded to a larger degree. The indexes after the VQ layer are the desired LSCodec tokens."}, {"title": "D. Training Stage 3: Vocoder CTX-vec2wav", "content": "As synthesizing waveforms from predicted mel-spectrograms often yields degraded audio quality, we additionally train a specialized vocoder from the obtained discrete tokens like Vocos [35]. Since timbre is decently removed in the LSCodec tokens, we resort to CTX-vec2wav [30] that exhibits strong timbre controllability [31]. To be consistent with the previous two stages, we develop an improved version CTX-vec2wav that receives the same WavLM outputs as timbre features instead of mel-spectrograms. Other parts of the vocoder follow the original CTX-vec2wav. With this vocoder, acoustic details can be refined to produce high-fidelity waveforms. The final model can also serve as a high-quality VC model, where timbre can be well altered by providing different reference prompts."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "We use LibriTTS [36] for model training, which is an English corpus with 585 hours of 24kHz speech data. To cover enough timbre variations, we use all of the train splits that contain about 2500 speakers. We discard all utterances shorter than 6s, remaining around 360 hours of training data. The prompts are randomly cut with lengths sampled between one third and one half of the original duration.\nWe train two versions of LSCodec in 50Hz and 25Hz frame rates. The CNN encoders contain 11 and 12 residual blocks respectively and 512 hidden dimensions. In stage 1, the output of encoder is 128 dimensional before being evenly split and projected into \u03bc, \u03c3\u00b2. In stage 2, only the first half dimensions of encoder output are used for quantizing and decoding. The code vectors is then 64-dimensional. Codebook is updated with EMA weight \u03b3 = 0.99. Code expiration is enabled only after 5000 update steps.\nThe Conformer decoder contains 2 Conformer blocks each with 2 heads and 184 attention dimensions. The prompt prenet has four CNN blocks with scaled residual connections, where the hidden dimensions are 128, 256 and 512 before being fed to cross attentions.\nWe use the output of the sixth layer from a pretrained WavLM-Large model to provide timbre features. In the SSL token prediction task, we use pretrained HuBERT-Large and perform 2048-centroid k-means clustering on around a 83-hour subset of our training data. As this SSL feature resides in 50Hz, in the 25Hz version of LSCodec, we repeat each token by 2 times along the temporal axis before the Conformer decoder. The 80-bin cepstral-normalized mel-spectrograms for reconstruction also have a frame rate of 50Hz.\nEach stage of LSCodec is trained up to 200 epochs, with loss weights \u03b3recon = \u03b3KL = 60, \u03b3idx = 2, \u03b3cmt = 1. Other hyperparameters and the training of CTX-vec2wav follow that in [30]."}, {"title": "B. Speech Reconstruction", "content": "To evaluate the reconstruction ability of LSCodec against other low-bitrate speech tokens, we use the testset-B in [30] that contains 500 utterances from unseen speakers in LibriTTS test-clean split. Since low-bitrate speech coding cannot fully restore every detail in the original waveform, we emphasize on comparing the preservation of key information, especially content and prosody. We use word error rate (WER) to measure intelligibility and gross pitch error (GPE) to measure prosody preservation. WERs are computed using NeMo-ASR with the ground truth transcriptions. GPE stands for the ratio of pitch values larger than 20% relative error from the ground truth. We also conduct MOS tests as an subjective evaluation of audio quality. The baselines include semantic SSL tokens with CTX-vec2wav vocoders, where wav2vec 2.0 [10] tokens are obtained from its inner quantizer, and HuBERT tokens are 2048-centroid cluster indexes from the last layer. To enable a fair comparison among the low-bitrate acoustic speech codecs, we compare codecs with bitrate lower than 1kbps in Table I, i.e. EnCodec [5] and TiCodec [17] with only 1 VQ layer and WavTokenizer-small [25] in 40Hz. Official checkpoints are used for these baselines.\nThe results reveal that both the 50Hz and 25Hz version of LSCodec exhibit significantly better WER and audio quality than other acoustic token baselines with even a lower bitrate. Although LSCodec has a higher GPE than TiCodec and WavTokenizer, the improvement on WER is still worthwhile since a 3-4% difference on GPE is hardly noticeable in human perception, but WER matters more. Compared to semantic tokens, LSCodec-50Hz outperforms wav2vec 2.0 tokens in all metrics, and both LSCodec versions have a much better GPE than HuBERT tokens. Note that wav2vec 2.0 has much worse speaker decoupling than LSCodec as will be shown in later experiments. The 25Hz version of LSCodec has a minor degradation than the 50Hz version, since its bitrate is only 55% of the latter. These findings suggest that LSCodec owns remarkable reconstruction performance using a very low bitrate."}, {"title": "C. Any-to-Any Voice Conversion", "content": "While reconstruction experiments reveal the preservation of content and prosody information, speaker disentanglement should be evaluated by VC. In this section, we randomly assign a different target speaker for every source utterance in testset-B for VC task. We measure the speaker embedding cosine similariity (SECS) via Resemblyzer between the target and converted utterances. Higher SECS means lower speaker information in the tokens. In addition, we also compute the correlation coefficient of pitch contours (P.Corr) between the source and converted speech. This allows us to have a better picture of a VC model both from timbre and prosody similarity. Both SECS and P.Corr range from -1 to 1. Note that P.Corr becomes meaningless when SECS is low. We include the same SSL tokens with CTX-vec2wav for comparison as in Table II. As only few codec models have the ability of VC, we include TiCodec (both 1 VQ and 4 VQ variants) and FACodec [21] to comparison. We discard the detail tokens in FACodec for VC since we find these tokens still contain considerable speaker information that harms VC performance.\nThe results are shown in Table III. Concretely, LSCodec has better speaker similarity in VC than other acoustic tokens, especially FA-Codec with supervised decoupling of speakers. We find that TiCodec has unsatisfactory performance on speaker similarity, which leads to inordinately high P.Corr since converted speech is very similar to the source one. This demonstrates the difficulty of disentangling speakers only via implicit bottleneck methods. In VC scenarios, LSCodec still exhibits lower WERs than TiCodec. Compared with semantic tokens, LSCodec owns notably higher SECS than wav2vec 2.0 tokens and P.Corr than HuBERT, achieving a better balance between the two metrics. These findings reflect that speaker timbre in LSCodec tokens have been removed to a great extent with little harm on prosody, which results in competitive VC ability."}, {"title": "D. Stage-wise Evaluations and Ablation Study", "content": "As the training of LSCodec consists of three consecutive stages, we monitor the reconstruction and VC performance along these stages. We present the WER and GPE evaluations in reconstruction together with the SECS and P.Corr values in VC in Fig.2. We additionally include V = 300 and V = 2048 versions with 25Hz frame rate. In stage 1 and 2, we use a trained HifiGAN [37] to convert predicted mels into waveforms for evaluation. We stop the 25Hz V = 300 variant at stage 2 because of deteriorated reconstruction performance. It can be inferred from Fig.2 that stage 2 usually outperforms stage 1 by adding an properly-sized VQ layer. Stage 3 substantially increases the VC performance measured in objective SECS apart from a much better audio quality. But it remains a future work to preserve contents and prosody better when training the vocoder. We can also find that compared to the 50Hz version, 25Hz ones face more challenge in encoding content information. A larger V also helps reduce WER and GPE at the cost of more speaker leakage.\nWe conduct another ablation study to verify the effectiveness of speaker perturbation, SSL token prediction task and the multi-stage training framework. We only include the first two training stages where perturbation and SSL token prediction take place. The results are shown in Table IV. Compared with the LSCodec-50Hz with \u03b2\u2208 [0.8, 1.2], canceling or restricting speaker perturbation leads to worse WER and SECS in both stages. This shows that the model is sacrificing local contents for more global information to decrease the loss values. Similar patterns can be found when \u03b2 ranges are enlarged, since this brings too much damage on data. Also, canceling the SSL token prediction loss has a huge impact on WERs in both stages. Directly training from stage 2 also has a degraded performance in both metrics. Hence, we conclude that the proposed training framework effectively produces the desired properties of LSCodec."}, {"title": "IV. CONCLUSION", "content": "We propose LSCodec, a low-bitrate and speaker-decoupled speech codec. LSCodec constructs a three-stage training framework with speaker perturbation. A VQ layer is applied after a VAE that disentangles speaker in a continuous space. A token vocoder is then trained upon the quantized codes. Experiments prove that LSCodec achieves strong reconstruction and VC ability given a bitrate as low as 0.25kpbs. Ablation studies validate the proposed training framework. LSCodec paves the way of future LM-based speech generation for its highly compact representation space. Explorations on stronger perturbation methods and better prosody preservation are valuable for future work."}]}