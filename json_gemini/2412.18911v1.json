{"title": "Accelerating Diffusion Transformers with Dual Feature Caching", "authors": ["Chang Zou", "Evelyn Zhang", "Runlin Guo", "Haohang Xu", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "abstract": "Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data.", "sections": [{"title": "1. Introduction", "content": "Diffusion Models (DMs) [16] have shown remarkable success in both image [27] and video generation [2]. Traditional DMs often use U-Net denoising structures, such as Stable Diffusion (SD) [27] and Stable Video Diffusion (SVD)[2], which have already demonstrated impressive results. Most recently, with the introduction of Diffusion Transformers (DiT) [26], the quality of visual generation has further significantly improved, propelling the development of numerous downstream applications [12]. However, the quality improvement is also accompanied by a significant increase in computational demands, reducing the inference efficiency of transformers and making their deployment in practical scenarios a pressing challenge.\nRecently, feature caching has become one of the most popular techniques for diffusion acceleration. Motivated by the high similarity between features in the adjacent timesteps, aggressive feature caching caches the features computed in a freshing timestep, and then reuses them in the following several caching steps, as shown in Figure 1(b). Such a fresh-then-cache strategy enables the diffusion model to save computational costs in the caching step and thus leads to significant acceleration. However, since the similarity of features in non-adjacent timesteps decreases quickly, the error introduced by aggressive feature caching can be accumulated at an exponential speed, leading to a significant drop in generation quality, and making aggressive feature caching impractical."}, {"title": "2. Related Works", "content": "Transformers in Diffusion Models Diffusion Models (DMs) [16, 30] generate images by gradually adding noise to the image during training and iteratively applying a denoising model to a randomly initialized noise during inference. DMs have recently gained extensive application across various tasks in image generation due to their outstanding performance [1, 27]. Earlier DMs with U-Net-based architectures [16, 27], which also include some Transformer layers, have achieved impressive results. Recently, Diffusion Transformer (DiT) [26] has been constructed by directly stacking multiple DiT Blocks in a cascade, achieving performance in class-conditional image generation that surpasses U-Net-based DMs. Consequently, the DiT architecture has been further extended to several generative fields, yielding exceptional outcomes. For example, the PixArt series [5-7] and similar text-to-image models incorporate Cross-Attention into the DiT Block to add text conditional information into generation procedure, while the Sora series [4, 18] enhances the DiT Block's Attention structure to Spatial-Temporal attention, enabling high-quality video generation. However, though success, these DiT-based DMs also entail significantly higher computational costs and time consumption, making deploying such\ngenerative models in real-world applications challenging.\nAcceleration of Diffusion Models To address the high computational costs commonly associated with DMs including DiT, various acceleration methods have been proposed. Existing acceleration methods can generally be divided into two categories: Designing sampling methods to reduce the number of steps needed to transition from noise to high-quality images, and directly accelerating the denoising model. Sampling-based acceleration methods enhance the efficiency of DMs by simply reducing the sampling steps. DDIM [31] leverages a deterministic, non-Markovian reverse process to significantly reduce generation steps while maintaining high-quality outputs. DPM Solver [22], through analytic derivation, optimizes the reverse process, enabling efficient sampling with fewer steps, while DPM Solver++ [23] further refines this approach by incorporating noise control and diverse sampling techniques to improve quality and adaptability. Rectified Flow [21] employs a recurrent flow framework, utilizing specially designed flow fields to accelerate the diffusion process.\nFeature Caching Methods that improve generation efficiency by accelerating the denoising model largely vary depending on the model structure. Acceleration methods for denoising models in different aspects have been proposed and work well, including weight quantization [29], structural pruning [11], knowledge distillation [19] and token merge [3]. Due to the high computational cost of retraining the diffusion models, or even small learnable modules introduce significant costs, limiting model reusability across different scales, resolutions, and tasks, driving interest in training-free acceleration methods. Faster Diffusion [27] improves computational efficiency by caching the output of the encoder module on certain steps, while DeepCache [25] reduces redundant computations by reusing low-resolution feature information in the skip connections of the U-Net structure. Unfortunately, although both are training-free methods, they are specialized feature caching techniques designed specifically for U-Net-based models like Stable Diffusion [27], making them difficult to directly adapt to transformer-based models like DiT [8]. Recently, to address the lack of feature reuse solutions for DiT, A-DiT [8] and Pyramid Attention Broadcast (PAB) [33] have been introduced. The former focuses on constructing residuals of DiT outputs across different layers, while the latter emphasizes the importance of various types of attention within DiT Blocks containing multiple attention heads. Learning-to-Cache [24] achieves higher acceleration ratios by employing a learnable router to determine whether computations are needed at each layer, but it incurs prohibitive computational costs. These feature caching methods generally lack attention to finer-grained features. ToCa [35] proposes a method that selects important tokens at both layer and token granularity, allocating more computation to these tokens while caching less important ones, advancing temporal redundancy management to the token level.\nThe errors introduced by feature caching are primarily attributed to discrepancies in timestep-sensitive information. Comparing the feature caching methods for DiT discussed above, we find that these feature caching approaches for DiT employ different types of feature cache-reuse structures, resulting in significant variations in acceleration ratios while maintaining generation quality. Additionally, they commonly introduce additional computations to mitigate and correct the discrepancies between cached features and the requirements of the current time step. Our proposed method DuCa demonstrates that conservative architectures like ToCa, with their long-range feature reuse capability, can correct timestep information for aggressive methods that struggle with larger timestep intervals. By leveraging the high caching ratios of aggressive methods, DuCa further enhances acceleration efficiency."}, {"title": "3. Method", "sections": [{"title": "3.1. Preliminary", "content": "Diffusion Models Diffusion models [16] are designed with two main processes: a forward process, where Gaussian noise is incrementally added to a clean image, and a reverse process, where a standard Gaussian noise is gradually denoised to reconstruct the original image. Letting t represent the timestep and \u1e9et the noise variance schedule, the conditional probability in the reverse (denoising) process, po(Xt-1 | xt), can be modeled as:\nN(\\sqrt{\\frac{1-\\alpha_{t-1}}{1-\\alpha_{t}}} (x_t + \\frac{(1-\\alpha_t)}{\\sqrt{\\alpha_t}}e_{\\theta}(x_t, t)), 1).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             (1)\nwhere \u03b1t = 1 - \u03b2t, \u03b1t = \u03a0t_{i=1}\u03b1i, and T represents the total number of timesteps. Notably, e\u03b8 is a denoising network parameterized by \u03b8, which takes xt and t as the inputs to predict the noise needed for denoising. In an image generation procedure using T timesteps, e must perform inference T times, which constitutes the majority of computational costs in diffusion models. Recently, numerous studies have shown that implementing e as a transformer often enhances generation quality.\nDiffusion Transformers Diffusion transformers (DiT) are usually composed of stacking groups of self-attention layers Fsa, multilayer perceptron FMLP, and cross-attention layers FCA (for conditional generation). It can be roughly formulated as:\nG = g1 \u25e6 g2 \u25e6 ... \u25e6 gL, where\ng1 = FSA \u2295 FCA \u2295 FMLP,\n(2)\nG, g\u0131, and Fl represent the DiT model, the blocks in DiT, and different layers in a DiT block, respectively. I denotes"}, {"title": "3.2. Dual Feature Caching", "content": "The proposed dual feature caching aims to perform aggressive caching and conservative caching alternatively in the caching steps. Hence, in this section, we first introduce the details of the two caching strategies.\nAggressive Caching in Duca As introduced in Figure 1(b), aggressive caching directly replaces the output of the DiT block with the cached feature which is computed in previous layers, which means that all computations of the first I layers can be skipped. This can be expressed as gi := C[l], where C[l] denotes the cached feature at the lth layer. Then, the computation with aggressive caching can be written as:\nG(x)t := gl+1 \u25e6 ... \u25e6 gL(C[l]).\n(3)\nTo achieve the maximal acceleration with aggressive caching, we set l = L-1 in most of our experiments, which indicates almost skipping all the layers in the caching step.\nConservative Caching in Duca As shown in Figure 1(a), conservative caching does not cache and reuse all the features. Instead, it caches and reuses only the features without the residual connection, which allows it to allocate the computation to some layers in the caching steps to correct the cached features. Recently, ToCa [35] introduced a token-wise approach that selectively reuses features based on the"}, {"title": "3.3. Overall Framework", "content": "Built on the deeper understanding of aggressive and conservative caching, DuCa introduces a caching scheme that alternates conservative and aggressive caching in different steps. V-Caching serves as the conservative caching strategy, as shown in Figure 3. This design leverages the realignment capability of token-wise conservative caching to correct the misaligned timestep information occurring in aggressive caching steps. Therefore, it enables the high local compression rate of aggressive caching to be applied to more steps while further reducing redundant computations in conservative caching steps. In practice, we arrange token-wise conservative caching steps on odd-numbered steps following a fresh step, and aggressive caching steps on even-numbered steps."}]}, {"title": "4. Experiments", "sections": [{"title": "4.1. Experiment Settings", "content": "4.1.1. Model Configurations\nExperiments are conducted on three widely known DiT-based models on the three generation tasks with the correlating default samplers, including PixArt-a [7] with 20 DPM Solver++ [23] steps for text-to-image generation, OpenSora [34] with 30 reflected flow steps [21] for text-to-video generation, and DiT-XL/2 [26] with 50 DDIM [31] steps for class-conditional image generation. All the experiments are conducted with NVIDIA H800 80GB GPUs."}, {"title": "4.1.2. Evaluation and Metrics", "content": "Text-to-image generation We randomly select 30,000 captions from COCO-2017 datasets [20] to generate the 30,000 256 \u00d7 256 images. FID-30k is computed using the 30k generated images and the real images from COCO-2017 to evaluate the quality of text-to-image generation. Besides, the CLIP score [14] is computed between the 30k prompt-image pairs to evaluate the text-image alignment. All the CLIP scores are tested using the Vit-large-14 model.\nText-to-video generation VBench [17] evaluation framework is applied to evaluate the performance of the text-to-video model. With 5 videos per prompt and 950 prompts\nfrom the framework, 4,750 videos are generated to evaluate the 16 aspects proposed by VBench. The generated videos are 480p, 9:16, and 2s videos.\nClass-conditional image generation 50,000 256 \u00d7 256 images uniformly sampled from the 1,000 classes in ImageNet [10] are generated to evaluate the FID-50k [15]. Besides, SFID, Precision, and Recall are introduced as supplementary metrics."}, {"title": "4.2. Quantitative and Qualitative Results", "sections": [{"title": "4.2.1. Results on text-to-image generation", "content": "As shown in Table 1, we compared the DuCa under two different token selection methods: Cross-Attention score from ToCa and the proposed V-norm score with three training-free acceleration methods: ToCa [35], the 10 DPM++ sampling steps and FORA [28], a layer-wise conservative caching method.\nIn terms of generation quality, the quantitative results demonstrate that under an acceleration ratio close to 2.0, the DuCa methods achieve the lowest FID-30k score on the MS-COCO2017 generation task, with almost no loss in generation quality compared to the original non-accelerated approach. For instance, the DuCa method using the Cross-Attention score reduces the FID-30k by 0.40 compared to the ToCa method with the same acceleration ratio, while the DuCa method using the V-norm score also decreases the FID-30k by 0.33, demonstrating that our alternating ac-celeration strategy effectively enhances generation quality.\nIn terms of evaluating the text-image alignment metric, the performance of the DuCa method is very close to that of ToCa and generally superior to the approach of directly setting 50% generation steps. For example, the DuCa method with Cross-Attention score and V-norm score achieves CLIP scores on MS-COCO2017 that are 0.61 and 0.59 higher, respectively, compared to the 50% generation steps method with the same acceleration ratio. These scores are even slightly better than those of the original non-accelerated PixArt-a model, with improvements of 0.15 and 0.13, respectively, demonstrating better controllability.\nIn addition, thanks to the acceleration brought by the compatibility with FlashAttention [9], the latency of the DuCa method with V-norm score is reduced by 0.069 compared to the DuCa method with Attention score, representing an approximate 43.9% reduction, which is a significant improvement in acceleration."}, {"title": "4.2.2. Results on text-to-video generation", "content": "The generation results of A-DiT, T-GATE [32], PAB [33], 15 rflow sampling steps, ToCa [35], FORA [28] and proposed DuCa on the text-to-video model OpenSora [34] are shown in Table 2.\nAs shown in Table 2, DuCa achieves a further acceleration from 2.36 \u00d7 (as with ToCa) to 2.50 \u00d7 with virtually no loss in generation quality. Additionally, despite having the lowest computational cost among all the compared methods, DuCa still achieves the highest VBench score. This demonstrates that the proposed strategy in DuCa, which alternates between aggressive and conservative caching steps, effectively eliminates the redundant components present in a fully conservative caching approach.\nAdditionally, comparing the generation results of the Cross-Attention and V-Caching token selection strategies, we only observe a slight decrease of 0.09 in the VBench score with the V-Caching strategy. More significantly, V-Caching reduces generation time by 7.61 seconds, approximately 28.4%. This indicates that DuCa effectively leverages V-Caching to make the acceleration method compatible with FlashAttention [9], achieving substantial reductions in generation time. In practice, using V-Caching will be more advantageous for rapid inference."}, {"title": "4.2.3. Results on class-conditional image generation", "content": "In Table 3, we compare ToCa [35], FORA [28], the direct reduction of corresponding DDIM steps, and the proposed DuCa under Self-Attention score and V-Caching token selection strategies. The experimental results indicate that with comparable FID scores, representing generation quality, DuCa further enhances acceleration. For instance, DuCa(b) with Self-Attention score achieves a similar FID-50k as ToCa(b), but the acceleration ratio increases from 2.36 to 2.48. Additionally, under a higher acceleration scenario with an acceleration ratio around 2.7, DuCa achieves an FID-50k of 3.19, further reducing the FID-50k by 0.45 compared to ToCa's 3.64. This demonstrates that DuCa effectively reallocates computation to more critical areas."}]}, {"title": "4.3. Ablation Studies", "content": "As shown in Table 4, we compare Random, Cross-Attention based, and V-Caching token selection methods on PixArt-a in terms of latency, FLOPs, FID-30k, and CLIP score. It can be observed that the cross-Attention-based selection method achieves the best performance in generation quality and text-image alignment, but due to its incompatibility with FlashAttention [9], its latency is as high as 0.157 seconds-nearly twice that of V-Caching and Random selection. In contrast, V-Caching achieves nearly the same, only slightly lower, FID-30k and CLIP scores, along with faster generation speed. Despite the minimal impact of the three token selection methods on computational load, the FlashAttention [9]-compatible V-Caching method shows significantly higher acceleration due to computational optimizations. Thus, V-Caching stands out as a superior generation scheme that effectively balances quality and efficiency.\nAs shown in Table 5, we compare the Conservative-only, Aggressive-only, and alternating DuCa structures on PixArt-a in terms of FLOPs, FID-30k, and CLIP score. The Conservative-only structure appears to have the best generation quality but incurs an excessively high computational cost. In contrast, Aggressive-only reduces computation to nearly half of Conservative-only but suffers significantly in generation quality. Our proposed DuCa structure achieves a computational load between the two, with only a slight 0.13 increase in FID-30k compared to the high-computation Conservative-only approach, while improving text-image alignment. This indicates that DuCa effectively eliminates most redundant computation present in Conservative-only."}]}, {"title": "5. Conclusion", "content": "This paper first gives an in-depth study of the caching error on aggressive and conservative methods, and then introduces dual caching DuCa, which performs aggressive and conservative alternatively to leverage their advantages in different caching steps. Besides, V-Caching is introduced to perform token-wise conservative caching based on the norm of the value matrix of the tokens, making it compatible with default optimized attention computation such as FlashAttention, allowing feature caching to be practical in real applications. We hope this paper can attract more attention to feature caching for its real benefits, to make it simple but effective, instead of being complex."}, {"title": "6. Experimental Details", "sections": [{"title": "6.1. Model Configuration", "content": "As mentioned in 4.1.1 experiments on three models from different tasks, PixArt-a[7] for text-to-image generation, OpenSora[4] for text-to-video generation, and DiT[26] for class-conditional image generation, are presented. In this section, a more detailed hyperparameter configuration scheme is provided.\n\u2022 PixArt-a: The average cache cycle length is set to N = 3 with an average caching ratio of R = 70% for ToCa[35], and N = 3 with R = 25% for the DuCa method to maintain a similar acceleration ratio. Additionally, in the FORA [28] scheme, N = 2. Conservative caching steps are configured to occur on odd-numbered steps following fresh steps, while aggressive caching steps occur on even-numbered steps following fresh steps. The proportion of conservative caching steps across layers and steps is identical to that in ToCa[35].\n\u2022 OpenSora: The average cache cycle length is N = 3 for temporal attention, spatial attention, and MLP, and N = 6 for cross-attention, with an average caching ratio R = 85% exclusively for the MLP in ToCa[35] and DuCa, while all other modules use a caching ratio of 100%. In the FORA [28]scheme, all modules are set to N = 2.\nFor PAB\u00b9* [33], temporal attention is set to N = 2, spatial attention to N = 4, cross-attention to N = 6, and the MLP module is fully computed. For PAB2* [33], N is set to 3, 5, and 7 for temporal, spatial, and cross-attention, respectively. For PAB3* [33], N is set to 5, 7, and 9 for temporal, spatial, and cross-attention, respectively.\nFor DuCa, conservative caching steps are set on odd-numbered steps following fresh steps, while aggressive caching steps are set on even-numbered steps following fresh steps. The proportion of conservative caching steps across layers and steps is identical to that in ToCa[35].\n\u2022 DiT: The average cache cycle length is set to N = 4 with an average caching ratio of R = 93% for ToCa[35](a), N = 3 with R = 93% for ToCa[35](b), and N = 3 with R = 95% for both DuCa(a) and (b). Additionally, the average cache cycle length for FORA [28]is set to N = 3. Furthermore, for DuCa(a), conservative caching steps are set on even-numbered steps following fresh steps, while aggressive caching steps are set on odd-numbered steps. In contrast, for DuCa(b), the arrangement is reversed. Since the average cache cycle length N = 3 here,"}, {"title": "6.2. Anonymous Page for video presentation", "content": "To further demonstrate the superiority of DuCa in video generation, we have created an anonymous GitHub page. Please visit https://duca2024.github.io/DuCa/ for a more detailed view. Besides, the videos are also available through the Supplementary Material."}]}]}