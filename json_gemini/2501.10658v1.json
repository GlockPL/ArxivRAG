{"title": "LUT-DLA: Lookup Table as Efficient Extreme\nLow-Bit Deep Learning Accelerator", "authors": ["Guoyu Li", "Shengyu Ye", "Chunyun Chen", "Yang Wang", "Fan Yang", "Ting Cao", "Cheng Liu", "Mohamed M. Sabry Alys", "Mao Yang"], "abstract": "Abstract\u2014The emergence of neural network capabilities in-\nvariably leads to a significant surge in computational demands\ndue to expanding model sizes and increased computational\ncomplexity. To reduce model size and lower inference costs,\nrecent research has focused on simplifying models and designing\nhardware accelerators using low-bit quantization. However, due\nto numerical representation limits, scalar quantization cannot\nreduce bit width lower than 1-bit, diminishing its benefits. To\nbreak through these limitations, we introduce LUT-DLA, a Look-\nUp Table (LUT) Deep Learning Accelerator Framework that\nutilizes vector quantization to convert neural network models\ninto LUTs, achieving extreme low-bit quantization. The LUT-\nDLA framework facilitates efficient and cost-effective hardware\naccelerator designs and supports the LUTBoost algorithm, which\nhelps to transform various DNN models into LUT-based models\nvia multistage training, drastically cutting both computational\nand hardware overhead. Additionally, through co-design space\nexploration, LUT-DLA assesses the impact of various model\nand hardware parameters to fine-tune hardware configurations\nfor different application scenarios, optimizing performance and\nefficiency. Our comprehensive experiments show that LUT-DLA\nachieves improvements in power efficiency and area efficiency\nwith gains of 1.4~7.0\u00d7 and 1.5~146.1x, respectively, while\nmaintaining only a modest accuracy drop. For CNNs, accu-\nracy decreases by 0.1%~3.1% using the L2 distance similarity,\n0.1%~3.4% with the L\u2081 distance similarity, and 0.1%~3.8% when\nemploying the Chebyshev distance similarity. For transformer-\nbased models, the accuracy drop ranges from 1.4% to 3.0%.", "sections": [{"title": "I. INTRODUCTION", "content": "The growth of neural network models demonstrates ex-\ntraordinary abilities, and Neural Network Scaling Law [24],\n[30] unveils a fundamental relation between the number of\nmodel parameters, computational costs, and model capabili-\nties. Therefore, the efficient inference of larger-scale models\nhas become a crucial research topic. An essential and typical\noptimization approach is to employ lower-precision quantiza-\ntion for efficient model inference and representation.\nRecent studies have quantized Large Language Models\n(LLMs) to FP8 [39] [40], FP4 [36] [18], INT2 [7], [57]\nand even to 1.58 bits [37] and 1 bits [59], substantially\nshrinking the model scale and memory footprint. Additionally,\nhardware accelerators have made progress in supporting low-\nbit quantization computation in ALUs. For instance, NVIDIA's\nBlackwell [46], Hopper [8] and Turing Tensor Cores [5]\nnow enable operations in FP8, FP4, and 1-bit, respectively.\nFig. 1 collects area efficiency (OPs/\u00b5m\u00b2) and power efficiency\n(OPs/nW) of floating point and integer operations with various\nbitwidths. It illustrates how scaling lower-bit computations can\nsignificantly enhance processing capabilities without increas-\ning area cost or energy consumption.\nHowever, due to the inherent limitations of numerical\nrepresentation, the benefits of scaling down bit width\nin hardware no longer exist, and thus scalar quantization\nmethods cannot compress data representation to less than 1-\nbit. Fig. 1 indicates that the current quantization approach\nhas hit its limit, resulting in a 1-bit numerical representa-\ntion. At reduced bitwidths, area and power efficiency for\ndifferent operators converge. Ultimately, the current quantiza-\ntion approach reaches the limit of 1-bit precision, restricting\nthe corresponding computational resources to bit operations\nmade up of a few logic gates, which complicates further\nsimplification. It suggests that existing quantization methods\nand hardware architectures have reached their computing and\npower efficiency limits [12] [31] [23].\nThe recent Lookup Table (LUT)-based model architec-\nture [1], [4], [32], [50], [55] introduces a novel computing"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "A. Approximate Computing in Neural Networks\nApproximate computing [2], [21] is a promising approach\nthat effectively simplifies computation, memory, and power\nconsumption. Due to the abundance of similar features and\nredundant data in neural networks, approximate computation\ncan significantly reduce the inference costs of neural networks.\nThere are many works based on approximate computing,\nincluding Precision Scaling/Quantization [11], [17], [35],\nSkipping [22], [47], Memorization [33], [41], [51]and Approx-\nimate Multipliers/Adders [42], [56].\nIn contrast to algebraic operations, LUT provides a non-\ncomputational and flexible approach for function mapping, and\nis widely utilized in approximation computing. For example, to\noptimize quantized range mapping [60], reuse computational\nresults [48], and even substitute GEMM computations by\ndirectly storing precomputed results [1], [4], [50], [55]. LUT\ncan also substitute or simplify non-linear functions used in\nneural network activation operations, such as NN-LUT [61]\nbased on polynomial approximation and TransPimLib [26],\nwhich combines CORDIC and LUT on PIM architectures.\nCurrently, there are implementations of approximate comput-\ning accelerator designs in PIM based on lookup operations,\nsuch as RAPIDNN [25] and NNPIM [19].\nB. Vector Quantization for Approx. Matrix Multiplication\nNeural networks inherently encode semantic features of\nvectors (e.g., a filter in CNN always captures similar input\ncharacteristics). Therefore, leveraging these semantic features\nof vectors in neural networks facilitates approximate model\ninference. Vector Quantization (VQ, [10], [16], [38]) and\nProduct Quantization (PQ, [13], [14], [29]) are widely used\nin information coding, information retrieval, similarity search\nand decompose high-dimensional vectors into (several sets\nof) lower-dimensional vectors. Recent research on VQ/PQ\nfor Approximate Computing [1], [4], [50], [55] has opened\nup possibilities to aggressively simplify end-to-end neural\nnetwork inference."}, {"title": "III. LUT-DLA FRAMEWORK OVERVIEW", "content": "In this section, we introduce a new co-design framework.\nFig. 3 illustrates the architecture of the framework, which\nconsists of:\nLUT-DLA Hardware Generator: To address Challenge\n1, we design LUT-DLA Hardware Generator, an agile and\nparameterized hardware architecture generation framework\nimplemented in Chisel [3]. The hardware architecture gener-\nated by the LUT-DLA Hardware Generator primarily consists\nof Centroid Calculation Modules (CCMs) and In-Memory\nMatching Modules (IMMs). CCM is designed for similarity\ncomparison, while IMM stores precomputed results and ac-\ncumulates computation outcomes. LUT-DLA Hardware Gen-\nerator can quickly generate RTL code based on specified\nhardware parameters, facilitating further performance evalu-\nation and parameter exploration. Additionally, we introduce a\nnovel, LUT-Stationary (LS) dataflow, which effectively reuse\non-chip LUTs and helps hide computational delays caused by\nswapping lookup tables. Sec. IV provides a detailed hardware\narchitecture and dataflow.\nLUTBoost: Efficient Multi-Stage Model Converter To ad-\ndress Challenge 2, we design a lightweight multistage model\ntraining method as Model Converter in Sec. V, which quickly\nassesses model accuracy and accelerates model convergence.\nIt not only simplifies the design of the model converter but\nalso speeds up training and reduces accuracy loss. In addition,"}, {"title": "IV. LUT-DLA HARDWARE ARCHITECTURE AND\nDATAFLOW DESIGN", "content": "In this section, we introduce the hardware architecture\ndesign of LUT-DLA. Then, we discuss the design choices of\ndataflows in LUT-DLA and demonstrate how the proposed\nLUT-Stationary dataflow optimizes the balance between on-\nchip memory use and memory access.\nA. Architecture Overview\nIn Sec.II-B, Fig. 2 introduces the inference of LUT-based\nneural networks. The execution of each LUT operator is di-\nvided into two steps: similarity comparisons and table lookups.\nTherefore, we partition the hardware architecture of LUT-DLA\ninto two mutually independent parts: Centroid Computation\nModules (CCMs) and In- Memory Matching Modules\n(IMMs). Fig. 4 represents hardware architecture. CCMs and\nIMMs are connected through a group of asynchronous FIFOs,\nwhich decouples these modules and allows them to operate in\ndifferent clock domains.\nCCM includes CCUs (Centroid Computation Units), cen-\ntroid buffers, and input buffers. Figure 5 shows that each CCU\nhas several distance Processing Elements (dPEs) that can\nread both centroids and an input vector, do calculations, and\ncompare similarities between the input vector and centroids\nat the same time. In each computing cycle, CCM loads an\ninput vector into CCU (blue vector in Fig. 5). Each dPE in\nCCU receives the minimum distance, centroid index, and input\nvector from the previous dPE, calculates the distance, changes\nthe index, and delivers the input vector to the next dPE to\npipeline the similarity comparison.\nIMM contains Indices Buffer, PSum LUT (Partial Sum\nLUT), and Scratchpad. Every cycle, Indices Buffer retrieves\nan index from the asynchronous FIFO and obtains the precom-\nputed result from PSum LUT. PSum LUT stores precomputed\nresults of the LUT-based operator, while the scratchpad accu-\nmulates and caches the retrieved results.\nIMM also supports element-wise activation and dequan-\ntization by using polynomial approximations [61]. Other\nnon-matrix multiplication operations using LUTs have been\nextensively studied in previous work [19], [25] and LUT-\nDLA can orthogonally employed with the methodologies. For\nbatch normalization, LUT-DLA could integrate normalization\ninto weights. Regarding operations like softmax/layernorm\nthat require global information, common solutions include\noffloading these computations to CPU (e.g. NVDLA [45]),\nVector Unit (e.g. TPU [28]) or dedicated Special Function Unit\n[53]. LUT-DLA can assist in runtime sum value collection to\naccelerate these processes. Since the LUT-DLA architecture\nonly affects the computation path of GEMM, it can achieve\na high compatibility with previously optimized methods. For\nexample, if the target network includes pooling operations,\nwe can adding dedicated units on the IMM's data write-back\npath to collect the values within the pooling window [15],\nor introducing additional memory access units to handle data\npooling [45].\nThe hardware architecture of flexibly decouples the CCM\nand IMM designs, enables customized setups for various\napplication scenarios and needs. Our design also supports\narbitrary CCM-IMM combinations without timing concerns\nand workload-based CCM-IMM runtime ratio adaptation. Fur-\nthermore, by decoupling CCM and IMM into separate clock\ndomains, LUT-DLA allows the pipeline-designed CCM to run\nat a higher clock frequency and provide indexes to numerous\nIMM similarities simultaneously. In the mean time, IMM\ncan operate at a lower clock frequency to reduce power\nconsumption. In Sec. VI-B, we will discuss more details on\nthe mapping of workloads and dataflow of LUT-DLA.\nB. LUT-DLA Dataflow Exploration\nContemporary hardware accelerators [27], [28], [45], are\ndesigned to Compute-Centric to support computation-intensive\noperations. Dataflows like Weight/Output Stationary are used\nto increase computational throughput and data reuse. In con-\ntrast, LUT-DLA adopts a fundamentally different approach by\nemploying LUTs to accelerate inference, forming a Memory-\nCentric hardware architecture centered around LUTs. The\nLUTs serve as both the primary \u201cstorage\u201d and \u201ccomputation\u201d\nunits, leading to performance bottlenecks in the architecture.\nAs LUT-DLA regularly accesses precomputed results from\nLUTs, a unique dataflow is needed to optimize access, reuse,\nand minimize hardware costs. We propose LUT-Stationary\n(LS) Dataflow to reduce the size of on-chip look-up tables,\nreuse data, and preserve acceptable scratchpad memory size.\nConsider GEMM operation $C_{M\u00d7N} = A_{M\u00d7K} \u00d7 B_{K\u00d7N}$,\nLS traverses the input matrix in a column-major order, first\nextracting vectors from the same feature space for computa-\ntion, so the look-up table and centroid matrix can be reused.\nTo evaluate and compare different dataflows, we analyze on-\nchip memory requirements in GEMM operation. Although a\nprecomputed lookup table must be stored on-chip for MNK, NMK, and MKN computations to decrease repeated loading\nof the same LUTs, KMN and KNM dataflows can cut\non-chip space at the expense of a larger scratchpad due to\nmore partial sums in the K dimension. The NKM approach"}, {"title": "V. LUTBOOST: EFFICIENT MODEL CONVERTER", "content": "LUT-based models extract semantic vectors from input data\nduring training and retain precomputed results in lookup tables\nfor inference. There are typically two methods to obtain LUT-\nbased models: First, training a model from scratch [1], [4],\n[50], which involves initially randomizing LUTs and model"}, {"title": "VI. CO-DESIGN SPACE SEARCH ENGINE", "content": "LUT-DLA offers a broad design space for various appli-\ncations, encompassing numerous suboptimal design points.\nIn this section, we first explore Design Space Exploration\n(DSE) in hardware architecture and LUTBoost. Then, we"}, {"title": "VII. EXPERIMENT AND EVALUATION", "content": "A. Model Accuracy\nSettings For ResNet models, we align with previous\n[55] setup, and we first freeze weights and perform centroid\nlearning for 20 epochs with learning rate: lr = 1 \u00d7 10-3, then\nwe jointly train weights and centroids with lr = 5 \u00d7 10-4 for\n300 epochs. We set a penalty ratio of 0.05 for reconstruction\nloss. For BERT/DistillBERT, we employ lr = 1 \u00d7 10-3 for\ncentroid learning for 2000 iterations, with penalty 1\u00d710-2 for\nthe reconstruction. In the joint training stage, the learning rate\nis 5\u00d710-5, and training cost for 190K /390K iterations, with\npenalty 1 \u00d7 10-1. For OPT-125M, we use lr = 1 \u00d7 10-3 and\ntrain 3 epochs to achieve centroid convergence. Subsequently,\nwe conduct joint training for 10 epochs with lr = 5 \u00d7 10-5.\nThe penalty of reconstruction loss is set to 0.1 in the centroid\ntraining stage and 1 \u00d7 10-2 in the joint training stage.\nWe also compare our method with PECAN [50] and PQA\n[1] using their paper's original settings under the same sub-\nvector length (v) and number of centroids (c).\nLUTBoost Model Accuracy In contrast to earlier LUT-\nbased training algorithms [1], [50], [55], LUTBoost can\ntrain deep neural networks with sufficient accuracy on huge\ndatasets. Table IV shows the accuracy of common CNN mod-\nels. LUTBoost is nearly as accurate as the baseline on these\nmodels, with an average accuracy reduction of 1.2%/2.9%\non CIFAR10/100. We also conducted quantization studies,\nand experiments show that traditional quantization methods\nare orthogonal to LUTBoost. BF16 distance computation and\nINT8 lookup table reduce accuracy by < 1% while reducing\non-chip area overhead and data moving costs by 4x.\nTo accommodate larger and more complex models and\ndatasets, we further test ResNet18 along with Tiny-Imagenet\nand Imagenet. With a multi-stage training technique, LUT-\nBoost achieved remarkable model convergence and accuracy,\nwith only 0.8%~2.6% loss on these datasets. Table VI com-\npares BERT, DistillBERT, and OPT-125M on GLUE [58]\nwith various similarity computation techniques. While LUT-\nNN [55] results in a drastic accuracy drop in these tasks,\nLUTBoost maintains a competitive edge, with an average\naccuracy consistently above 84% on these tasks in the GLUE\nbenchmark. Our technique also outperforms the state-of-the-\nart training algorithm eLUT-NN in the GLUE dataset, while\nmaintaining competitive accuracy on sub-tasks, proving its\nrobustness. Our investigations revealed that larger models\ndemonstrate superior fault tolerance, as ResNet18 (11.7M pa-\nrameters) retains accuracy under more aggressive quantization\nparameters than ResNet20 (270K parameters). We believe\nthat LUTBoost can efficiently scale to LLMs or large CNN\nnetworks. As an experiment, we used LUTBoost to train OPT-\n125M. To the best of our knowledge, this is the first time a\nLUT-based model has been scaled to such a size, underlining\nits potential for widespread application in various transformer-\nbased models.\nCompare with Baseline The quantization error of LUT-\nbased models follows the same pattern as traditional models.\nFor larger models (ResNet18) on simpler datasets (CIFAR10),\nthe quantization loss using similarity distance for inference\nis only about 0.1%. However, when it comes to larger\ndatasets like CIFAR100, which are inherently more difficult\nto converge during training, there is a relatively noticeable\nquantization loss. The maximum accuracy drop is observed\nas 3.1% (ResNet32 using L\u2082), 3.4% (ResNet56 using L\u2081),\nand 3.8% (ResNet56 using Chebyshev). As for Transformer-\nbased models, employing L2 similarity on DistilBERT results\nin minimal quantization loss, approximately 1.4%. In contrast,\nutilizing L\u2081 similarity on BERT can lead to a loss of up to\n3.0%.\nCompare between L1 and L2 In Table IV, the model using\nL\u2081 distance incurs only an accuracy drop ~1% lower than that\nof L2. This eliminates the large loss from Manhattan distance,\nas indicated in [1], enabling multiplication-free neural network\ninference. The performance of Chebyshev distance is also\ncomparable, demonstrating a similar accuracy drop as the\nL\u2081 distance, which implies a robust alternative to traditional\ndistances.\nBitwidth and Similarity Evaluation\nTable V provides an in-depth presentation of the accuracy\nresults of ResNet20 under different equivalent bit widths.\nDuring inference, a subvector of the input matrix is represented\nby the centroid index within its subspace; the equivalent\nbit can be computed as $[log_2 c]/v$. As described in Sec.\nVI-A1, an increase in c and a decrease in v can enhance"}, {"title": "VIII. CONCLUSION", "content": "In this paper, we propose LUT-DLA, a LUT accelerator gen-\nerator for LUT-based model inference on an emerging com-\nputational paradigm. Our experiments show that LUT-DLA\nshows promising 1.4~7.0\u00d7 and 1.5~146.1\u00d7 improvement in\npower and area efficiency compared to existing architectures\nwith a minor accuracy drop. For CNNs, there is a slight\ndecrease in accuracy between 0.1% and 3.1% when utilizing\nthe L2 distance, a marginally higher drop ranging from 0.1%\nto 3.4% for the L\u2081 distance, and a tolerable decrease from\n0.1% to 3.8% with the Chebyshev distance. Transformer-based\nmodels also exhibit a diminutive accuracy decline between\n1.4% and 3.0%."}, {"title": "Loss", "content": "$Loss = ||(LUT_{N,c}[Index] \u00b7 B_{K,N}) \u2013 (A_{M,K}\u00b7 B_{K,N})||^2$"}, {"title": "Similarity", "content": "$similarity = \\sum ||V \u2013 C||^2$"}, {"title": "Similarity", "content": "$similarity = \\sum|V \u2013  C|$"}, {"title": "Similarity", "content": "$similarity = max(|V \u2013 C|)$"}, {"title": "Lre", "content": "$L_{re} = (SG(A \\cdot W) \u2013 A \\cdot W)^2 + (\\hat{A} \\cdot W \u2013 SG(A \\cdot W))^2$"}, {"title": "Output", "content": "$output = \\begin{cases} A\\cdot W, & \\text{forward propagation} \\\\\n\\hat{A}. W, & \\text{backward propagation} \\end{cases}$"}, {"title": "T(v,c)", "content": "$\u03c4(v, c) = \\frac{\\text{OP sim}}{\\text{ OP sim + OP add}} = \\frac{\\frac{K}{\u03c5} sim  CMy}{\\frac{K}{\u03c5} sim  CMy + MN}$"}, {"title": "Y(v,c)", "content": "$\u03c6(v, c) = mem_{in} + mem_{out} + mem_{LUT}\n= \\frac{K}{\u03c5}Nc  b i t_{lut} + MN b i t_{out} + \\frac{K}{\u03c5} M log_2 c$"}, {"title": "Yarea", "content": "$\u03a5_{area} (v, c, N_{IMM}, N_{CCU})\n= area_{imm} * n_{IMM} + area_{ccu} * N_{CCU} + area_{other}$"}, {"title": "Ypower", "content": "$\u03a5_{power} (v, c, N_{IMM}, N_{CCU})\n= power_{imm} * n_{IMM} + power_{ccu} * N_{CCU} + power_{other}$"}, {"title": "w(v, \u03b1, \u03b2, \u03b7\u03b9\u039c\u039c, nccu)", "content": "$\u03c9(v, \u03b1, \u03b2, \u03b7\u03b9\u039c\u039c, nccu) = max(load, sim, lut)\n= max(\\frac{cbit_{LUT}}{\u03b2  N_{IMM}},\\frac{MK}{ N_{CCU}}, \\frac{MNK}{ UN_{IMM}})$"}]}