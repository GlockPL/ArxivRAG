{"title": "GRAPH CLASSIFICATION VIA REFERENCE DISTRIBUTION LEARNING: THEORY AND PRACTICE", "authors": ["Zixiao Wang", "Jicong Fan"], "abstract": "Graph classification is a challenging problem owing to the difficulty in quantifying the similarity between graphs or representing graphs as vectors, though there have been a few methods using graph kernels or graph neural networks (GNNs). Graph kernels often suffer from computational costs and manual feature engineering, while GNNs commonly utilize global pooling operations, risking the loss of structural or semantic information. This work introduces Graph Reference Distribution Learning (GRDL), an efficient and accurate graph classification method. GRDL treats each graph's latent node embeddings given by GNN layers as a discrete distribution, enabling direct classification without global pooling, based on maximum mean discrepancy to adaptively learned reference distributions. To fully understand this new model (the existing theories do not apply) and guide its configuration (e.g., network architecture, references' sizes, number, and regularization) for practical use, we derive generalization error bounds for GRDL and verify them numerically. More importantly, our theoretical and numerical results both show that GRDL has a stronger generalization ability than GNNs with global pooling operations. Experiments on moderate-scale and large-scale graph datasets show the superiority of GRDL over the state-of-the-art, emphasizing its remarkable efficiency, being at least 10 times faster than leading competitors in both training and inference stages.", "sections": [{"title": "1 Introduction", "content": "Graphs serve as versatile models across diverse domains, such as social networks [Wang et al., 2018], biological compounds [Jumper et al., 2021], and the brain [Ktena et al., 2017]. There has been considerable interest in developing learning algorithms for graphs, such as graph kernels [G\u00e4rtner et al., 2003, Shervashidze et al., 2011, Chen et al., 2022b] and graph neural networks (GNNs) [Kipf and Welling, 2016, Defferrard et al., 2016, Gilmer et al., 2017]. GNNs have emerged as powerful tools, showcasing state-of-the-art performance in various graph prediction tasks [Veli\u010dkovi\u0107 et al., 2017, Gilmer et al., 2017, Hamilton et al., 2017, Xu et al., 2018, Sun et al., 2019, You et al., 2021, Ying et al., 2021, Liu et al., 2022b, Chen et al., 2022a, Xiao et al., 2022, Sun et al., 2023]. Despite the evident success of GNNs in numerous graph-related applications, their potential remains underutilized, particularly in the domain of graph-level classification.\nCurrent GNNs designed for graph classification commonly consist of two components: the embedding of node features through message passing [Gilmer et al., 2017] and subsequent aggregation by some permutation invariant global pooling (also called readout) operations [Xu et al., 2018]. The primary purpose of pooling is to transform a graph's node embeddings, a matrix, into a single vector. Empirically, pooling operations play a crucial role in classification [Ying et al., 2018]. However, these pooling operations tend to be naive, often employing methods such as simple summation or averaging. These functions collect only first-order statistics, leading to a loss of structural or semantic information. In addition to the conventional sum or average pooling, more sophisticated pooling operations have shown improvements in graph classification [Li et al., 2015, Ying et al., 2018, Lee et al., 2019, 2021, Buterez et al., 2022], but they still carry the inherent risk of information loss."}, {"title": "2 Proposed Approach", "content": "Following convention, we denote a graph with index i by Gi = (Vi, Ei), where Vi and Ei are the vertex (node) set and edge set respectively. Given a graph dataset G = {(G1, y1), (G2, y2),..., (GN,YN)}, where Yi \u2208 {1,2,..., K} is the associated label of Gi and yi = k means G\u2081 belongs to class k, the goal is to learn a classifier f from G that generalizes well to unseen graphs. Since in many scenarios, each node of a graph has a feature vector x and the graph is often represented by an adjacency matrix A, we also write Gi = (Ai, Xi) for convenience, where Ai \u2208 R^{n_i \\times n_i}, Xi \u2208 R^{n_i \\times d_0}, ni = |Vi is the number of nodes of graph i, and do denotes the number of features. We may alternatively denote the graph dataset as G = {((A1, X1), y1), ((A2, X2), Y2), ..., ((AN, XN),YN)}.\nOur approach is illustrated in Figure 1. For graph classification, we first use a GNN, denoted as fg, to transform each graph to a node embedding matrix H\u00bf \u2208 R^{n_i \\times d} that encodes its properties, i.e.,\n\nHi = fG(Gi) = fG(Ai, Xi), \\tag{1}\n\nwhere fG \u2208 FG and FG denotes a hypothesis space. The remaining task is to classify Hi without global pooling. Direct classification of node embeddings is difficult due to two reasons:\n(i) Different graphs have different numbers of nodes, i.e. in general, ni \u2260 nj if i \u2260 j.\n(ii) The node embeddings of each graph are permutation invariant, namely, PHi and H\u00bf represent the same graph for any permutation matrix P.\nHowever, the two properties are naturally satisfied if we treat the node embeddings of each graph as a discrete distribution. Specifically, each H\u00bf is a discrete distribution and each row of H\u00bf is an outcome of the distribution. There"}, {"title": "2.1 Model Framework", "content": "Following convention, we denote a graph with index i by Gi = (Vi, Ei), where Vi and Ei are the vertex (node) set and edge set respectively. Given a graph dataset G = {(G1, y1), (G2, y2),..., (GN,YN)}, where Yi \u2208 {1,2,..., K} is the associated label of Gi and yi = k means G\u2081 belongs to class k, the goal is to learn a classifier f from G that generalizes well to unseen graphs. Since in many scenarios, each node of a graph has a feature vector x and the graph is often represented by an adjacency matrix A, we also write Gi = (Ai, Xi) for convenience, where Ai \u2208 R^{n_i \\times n_i}, Xi \u2208 R^{n_i \\times d_0}, ni = |Vi is the number of nodes of graph i, and do denotes the number of features. We may alternatively denote the graph dataset as G = {((A1, X1), y1), ((A2, X2), Y2), ..., ((AN, XN),YN)}.\nOur approach is illustrated in Figure 1. For graph classification, we first use a GNN, denoted as fg, to transform each graph to a node embedding matrix H\u00bf \u2208 R^{n_i \\times d} that encodes its properties, i.e.,\n\nHi = fG(Gi) = fG(Ai, Xi), \\tag{1}\n\nwhere fG \u2208 FG and FG denotes a hypothesis space. The remaining task is to classify Hi without global pooling. Direct classification of node embeddings is difficult due to two reasons:\n(i) Different graphs have different numbers of nodes, i.e. in general, ni \u2260 nj if i \u2260 j.\n(ii) The node embeddings of each graph are permutation invariant, namely, PHi and H\u00bf represent the same graph for any permutation matrix P.\nHowever, the two properties are naturally satisfied if we treat the node embeddings of each graph as a discrete distribution. Specifically, each H\u00bf is a discrete distribution and each row of H\u00bf is an outcome of the distribution. There"}, {"title": "2.2 Design of FG and FD", "content": "We get GRDL's network F by concatenating the node embedding module and the reference module:\n\nF := FD \\circ FG. \\tag{10}"}, {"title": "2.3 Algorithm Implementation", "content": "The @ in the Gaussian kernel (15) plays a crucial role in determining the statistical efficiency of MMD. Optimally setting of @ remains an open problem and many heuristics are available [Gretton et al., 2012b]. To simplify the process, we make @ learnable in our GRDL and rewrite \u00a7 as \u03be\u03b8. Our empirical results in Appendix D.5 show that GRDL with learnable @ performs better. For convenience, we denote all the parameters of fe as w and let fw,D,9 = fD \u00b0 fG. Then we rewrite problem (9) as\n\n\\min_{w,D,\\theta} \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^K Y_{ik} \\log \\frac{\\exp \\left(f_{w,D,\\theta}(G_i)_k\\right)}{\\sum_{j=1}^K \\exp \\left(f_{w,D,\\theta}(G_i)_j\\right)} + \\lambda \\sum_{k'\\neq k} \\xi_{\\theta}(D_k, D_{k'}).\\tag{16}\n\nThe (mini-batch) training of GRDL model is detailed in Algorithm 1 (see Appendix C)."}, {"title": "3 Theoretical Analysis", "content": "In this section, we provide theoretical guarantees for GRDL, due to the following motivations:\n\u2022 As the proposed approach is novel, it is necessary to understand it thoroughly using theoretical analysis, e.g., understand the influences of data and model properties on the classification.\n\u2022 It is also necessary to provide guidance for the model design to guarantee high accuracy in inference stages."}, {"title": "3.1 Preliminaries", "content": "Matrix constructions We construct big matrices X, A and D, where X = [X_1^T,X_2^T,\\ldots,X_N^T] \\in \\mathbb{R}^{(\\sum_{i=1}^N n_i)\\times d}; A = diag(A1, A2,..., AN) \u2208 R^{(\\sum_{i=1}^N n_i)\\times(\\sum_{i=1}^N n_i)} is a block diagonal matrix, D = [D_1^T, D_2^T,...,D_K^T] \u2208 \\mathbb{R}^{Km \\times d}.\nThe adjacency matrix with self-connectivity is \\bar{A} = A + I. The huge constructed graph is denoted by G = (\\bar{A}, X). This construction allows us to treat all graphs in dataset G as a whole and it is crucial for our derivation.\nNeural network Previously, for a deterministic network f \u2208 F, its output after feeding forward a single graph is f(Gi). However, we mainly deal with the huge constructed graph G in this section, and notation will be overloaded to f(G) = S \u2208 \\mathbb{R}^{N\\times K}, a matrix whose i-th row is f (Gi).\nWe instantiate the message passing network as Graph Isomorphism Network (GIN) [Xu et al., 2018]. We choose to focus on GIN for two reasons. Firstly, the analysis on GIN is currently limited, most of the current bounds for GNNs don't apply for GIN [Garg et al., 2020, Liao et al., 2021, Tang and Liu, 2023]. The other reason is that GIN is used as the message-passing network in our numerical experiments. Notably, our proof can be easily adapted to other message-passing GNNs (e.g. GCN [Kipf and Welling, 2016]). GIN updates node representations as\n\nh_v^{(l)} = MLP^{(l)} \\left( (1+\\epsilon^{(l)})h_v^{(l-1)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(l-1)} \\right) \\tag{17}\n\nwhere h_v^{(l)} denotes the node features generated by l-th GIN message passing layer. Let \u03b5^{(l)} = 0 for all layers and suppose all MLPs have r layers, the node updates can be written in matrix form as\n\nH^{(l)} = \\sigma \\left( \\ldots \\left( (\\bar{A}H^{(l-1)})W_1^{(l)}\\right) \\ldots W_{r-1}^{(l)}\\right) W_r^{(l)} \\tag{18}\n\nwhere W_i^{(l)} \\in \\mathbb{R}^{d\\times d} is the weight matrix, and H^{(l)} is the matrix of node features with H^{(0)} = X. \u03c3(\u00b7) is the non-linear activation function. Let F^l be the function space induced by the l-th message passing layer, meaning\n\nF^l = \\left\\{ (\\bar{A}, H^{(l-1)}) \\rightarrow H^{(l)} : W^{(l)} \\in B^{(l)}, i \\in [r] \\right\\} \\tag{19}\n\nwhere B_i^{(l)} \\subset \\mathbb{R}^{d \\times d} is some constraint set on the weight matrix W_i^{(l)} and H^{(l)} is given by (18). The L-layer GIN function space FG is the composition of F^l for l \u2208 [L], i.e.,\n\nF_G = F^L \\circ F^{L-1} \\circ \\ldots \\circ F^1 = \\left\\{ G \\rightarrow f^L(\\ldots f^1(G)) : f^l \\in F^i, \\forall i \\in [L] \\right\\}. \\tag{20}\n\nLetting s_{ik} = -MMD^2(H^{(L)}, D_k), the reference layer defines the following function space\n\nF_D = \\left\\{ H^{(L)} \\leftrightarrow S \\in \\mathbb{R}^{N\\times K} : D_k \\in \\mathbb{R}^{m\\times d}, k \\in [K] \\right\\}. \\tag{21}\n\nOur proposed network (GRDL) is essentially F := FD \u00b0 FG.\nLoss Function Instead of the cross entropy loss (7), we consider a general loss function l_y(\\cdot, \\cdot) satisfying 0 \\leq l_y \\leq \\gamma to quantify the model performance. Importantly, this loss function is not restricted to the training loss because our generalization bound is optimization-independent. For instance, the loss function can be the ramp loss that is commonly used for classification tasks [Bartlett et al., 2017, Mohri et al., 2018]. Given a neural network f \u2208 F, we want to upper bound the model population risk of graphs and labels from an unknown distribution X \u00d7 Y\n\nL_y(f) := \\mathbb{E}_{(G, y) \\sim \\mathcal{X} \\times \\mathcal{Y}} [l_y(f(G), y)]. \\tag{22}\n\nGiven the observed graph dataset G sampled from X \u00d7 Y, the empirical risk is\n\n\\hat{L}_y(f) := \\frac{1}{N} \\sum_{i=1}^N l_y(f(G_i), Y_i), \\tag{23}\n\nof which (7) is just a special case. Appendix E provides more details about the setup and our idea."}, {"title": "3.2 Main Results", "content": "For convenience, similar to [Bartlett et al., 2017, Ju et al., 2023], we make the following assumptions.\nAssumption 3.1. The following conditions hold for F_\\gamma := \\left\\{ (G, y) \\rightarrow l_y(f(G), y) : f \\in F \\right\\}:\n(i) The activation function \u03c3(\u00b7) is 1-Lipschitz (e.g. Sigmoid, ReLU).\n(ii) The weight matrices satisfy W_i^{(l)} \\in \\mathcal{B} := \\left\\{ W : \\|W\\|_\\infty \\leq \\kappa, \\|W\\|_{2,1} \\leq b \\right\\}.\n(iii) The constructed reference matrix satisfy \\|D\\|_2 \\leq b_D.\n(iv) The Gaussian kernel parameter @ is fixed.\n(v) The loss function l_y(\\cdot, y) : \\mathbb{R}^K \\rightarrow \\mathbb{R} is \u00b5-Lipschitz w.r.t \\| \\cdot \\|_2 and 0 \\leq l_y \\leq \\gamma.\nTheorem 3.2 (Generalization bound of GRDL). Let n = \\min_i n_i, c = \\|\\bar{A}\\|_o, and d = \\max_{i,l} d_i^{(l)}. Denote R_G :=\nc^{2L} \\|X\\|_{\\infty} \\ln(2d^2) \\left( \\prod_{l=1}^{L} (\\prod_{i=1}^{r} \\kappa_i^{(l)})^2 \\right) \\left( \\sum_{l=1}^{L} \\sum_{i=1}^{r} (\\tau_i^{(l)})^{2/3} \\right)^3. For graphs \\mathcal{G} = \\left\\{ (G_i, y_i) \\right\\}_{i=1}^N drawn i.i.d from any probability distribution over \\mathcal{X} \\times \\{ 1, ..., K \\} and references \\{ D_k \\}_{k=1}^K, D_k \\in \\mathbb{R}^{m \\times d}, with probability at least 1 \u2013 \u03b4, every loss function l and network f \u2208 F under Assumption 3.1 satisfy\n\nL_y(f) \\leq \\hat{L}_y(f) + \\frac{3\\gamma \\sqrt{\\ln (2/\\delta)}}{\\sqrt{N}} + \\frac{8\\gamma + 24\\mu (\\prod_{l=1}^{L} \\kappa^{(L+1)} \\sqrt{R_G} + R'_G) \\ln N}{\\sqrt{N}},\n\nwhere v_1 = \\frac{640 \\kappa R_G \\mu^2}{n}, v_2 = \\frac{Kmd}{\\sqrt{m}}, and v_3 = \\frac{24 \\sqrt{\\theta} Nb_D \\mu}{\\sqrt{m}}.\nThe bound shows how the properties of the neural network, graphs, reference distributions, etc, influence the gap between training error and testing error. A detailed discussion will be presented in Section 3.3. Some interesting corollaries of Theorem 3.2, e.g., misclassification rate bound, can be found in Appendix F.7. Besides small generalization error L_y(f) \u2013 \\hat{L}_y(f), a good model should have small empirical risk \\hat{L}_y(f). The empirical risk \\hat{L}_y(f) is typically a surrogate loss of misclassification rate of training data and a lower misclassification rate implies a smaller L_y(f). We now provide a guarantee for the correct classification of training data, namely small \\hat{L}_y(f).\nNotably, the node embeddings Hi from the k-th class as well as the reference distributions Dk are essentially some finite samples from an underlying continuous distribution Pk. One potential risk is that, although the continuous distributions P1, P2,..., PK are distinct, we can only observe their finite samples and may fail to distinguish them from each other with MMD. Specifically, suppose a node embedding Hi is from the k-th class, although 0 = MMD(Pk, Pk) < MMD(Pk, Pj) for any j \u2260 k, it is likely that MMD(Hi, Dk) > MMD(Hi, Dj) for some j \u2260 k. The following theorem provides the correctness guarantee for the training dataset G:\nTheorem 3.3. All graphs in the training set G are classified correctly with probability at least 1 - \u03b4 if\n\\min_{i\\neq j} MMD (P_i, P_j) > \\sqrt{\\frac{1}{m}}\\left( \\sqrt{\\frac{\\gamma}{\\delta}} + \\sqrt{4 + 4 \\sqrt{\\log \\frac{1}{\\delta}}} \\right).\nTheorem 3.3 implies that a larger reference distribution size m benefits the classification accuracy of training data, resulting in a lower L_y(f). Moreover, a larger \\min_{i\\neq j} MMD(P_\u017c, P_j) also makes correct classification easier according to the theorem, justifying our usage of discriminative loss (8)."}, {"title": "3.3 Bound Discussion and Numerical Verification", "content": "Let = \\max_{i,l} \\kappa_i^{(1)}, b = \\max_{i,l} b^{(1)} and suppose d is large enough, we simplify Theorem 3.2 as\n\nL_y(f) \\leq \\hat{L}_y(f) + \\tilde{\\mathcal{O}}\\left( \\frac{\\sqrt{\\mu \\kappa b \\|X\\|_2 c L (L r)^{\\frac{1}{2}}}}{\\sqrt{N}} + \\sqrt{\\frac{Kmd}{N}} \\right) \\leq \\hat{L}_y(f) + \\tilde{\\mathcal{O}}\\left( \\frac{\\sqrt{\\mu \\kappa b \\|X\\|_2 c L (L r)^{\\frac{1}{2}}}}{\\sqrt{N}} + \\sqrt{\\frac{Kmd}{N}} \\right).\n\nI. Dependence on graph property One distinctive feature of our bound is its dependence on the spectral norm of graphs' adjacency matrix. The large adjacency matrix \\bar{A} is a block-diagonal matrix, so its spectral norm c = \\|\\bar{A}\\|_o = \\max_{i\\in[N]} \\|\\bar{A}_i\\|_o. By Lemma F.8, incorporating c^L is sufficient for any L-step GIN message passing. This result aligns with Ju et al. [2023], who achieved this conclusion via PAC-Bayesian analysis. Our derivation, based on the Rademacher complexity, provides an alternative perspective supporting this result. Notably, Liao et al. [2021] and Garg et al. [2020] proposed bounds scaling with graphs' maximum node degree, which is larger than the spectral norm of the graphs' adjacency matrix (Lemma F.18). Consequently, our bound is tighter.\nII. Use moderate-size message passing GIN The bound scales with the size of the message passing GIN, following \\tilde{\\mathcal{O}}(c^L(Lr)^{1/2} k r). Empirical observations reveal k > 1, and we prove that c > 1 (refer to Lemma F.20). Therefore, when the message-passing GNN has sufficient expressive power (resulting in a small L(f)), a network with a smaller L and r may guarantee a tighter bound on the population risk compared to a larger one. Therefore, a promising strategy is to use a moderate-size message passing GNN. This is empirically supported by Figure 5 of Appendix D.7.\nIII. Use moderate-size references The bound scales with the size of reference distributions m as \\tilde{\\mathcal{O}}(\\sqrt{m}). When m is smaller, the bound tends to be tighter. However, if m is too small, the model's expressive capacity is limited, potentially resulting in a large empirical risk L(f), and consequently, a large population risk. Therefore, using moderate-size references is a promising choice, as supported by our empirical validation results in Appendix D.3 (see Figure 6).\nIV. Regularization on references norm barely helps Regularizing the norm of references \\|D\\|_2, i.e., reducing bD, might be considered to enhance the model's generalization. However, it is important to note that bp only influences the term 03 (in logarithm) in Theorem 3.2 and has a tiny influence on the overall bound. Conversely, such regularization constrains the model's expressive capacity, potentially leading to a large L(f) and increasing the population risk. This observation is empirically supported by experiments in Appendix D.7 (see Table 10).\nV. GRDL has a tighter bound than GIN with global pooling In Appendix A, we provide the generalization error bound, i.e., Theorem A.1, for GIN with global pooling and compare it with Theorem 3.2. The result shows that our GRDL has a stronger generalization ability than GIN, which is further supported by the numerical results in Table 4.\nRemark 3.4. Currently, we use K reference distributions for classification (one for each class). One natural approach to enhancing the model's expressive power is increasing the number of references for each class. However, counterintuitively, our empirical observations, supported by Theorem B.1, suggest that having only one reference per class is optimal. We discuss this further in Appendix B."}, {"title": "4 Related Work", "content": "Various sophisticated pooling operations have been designed to preserve the structural information of graphs. DIFF-POOL, designed by Ying et al. [2018], learns a differentiable soft cluster assignment for nodes and maps nodes to a set of clusters to output a coarsened graph. Another method by Lee et al. [2019] utilizes a self-attention mechanism to distinguish nodes for retention or removal, and both node features and graph topology are considered with the self-attention mechanism.\nA recent research direction focuses on preserving structural information by leveraging the optimal transport (OT) [Peyr\u00e9 and Cuturi, 2020]. OT-GNN, proposed by Chen et al. [2021], embeds a graph to a vector by computing Wasserstein distances between node embeddings and some \"learned point clouds\". TFGW, introduced by Vincent-Cuaz et al. [2022], embeds a graph to a vector of Fused Gromov-Wasserstein (FGW) distance [Vayer et al., 2018] to a set of \"template graphs\". OT distances have also been combined with dictionary learning to learn graph vector embedding in an unsupervised way (GDL) [Liu et al., 2022a, Vincent-Cuaz et al., 2021, Zeng et al., 2023].\nSimilar to the \"learned point clouds\" in OT-GNN, \u201ctemplate graphs\u201d in TFGW, and dictionaries in GDL, our GRDL preserves information in node embeddings using reference distributions. To the best of the authors' knowledge, we are the first to model a graph's node embeddings as a discrete distribution and propose to classify it directly without aggregating it into a vector, marking our novel contribution. Additionally, our work stands out as the first to analyze the generalization bounds for this type of model, adding a theoretically grounded dimension to the research. By the way, our method is much more efficient than OT-GNN and TFGW. Please see Figure 2 and Table 8."}, {"title": "5 Numerical Experiments", "content": "We leverage eight popular graph classification benchmarks [Morris et al., 2020], comprising five bioinformatics datasets (MUTAG, PROTEINS, NCI1, PTC-MR, BZR) and three social network datasets (IMDB-B, IMDB-M, COLLAB). We also use three large-scale imbalanced datasets (PC-3, MCF-7, and ogbg-molhiv [Hu et al., 2020]). A summary of data statistics is in Table 6.\nBaselines Our approach is benchmarked against four groups of state-of-the-art baselines: 1) GNN models with global or sophisticated pooling operations, including PATCHY-SAN [Niepert et al., 2016], DIFFPOOL [Ying et al., 2018], GIN [Xu et al., 2018], DropGIN [Papp et al., 2021], SEP [Wu et al., 2022], GMT [Baek et al., 2021], MinCutPool [Bianchi et al., 2020], ASAP [Ranjan et al., 2020], and Wit-TopoPool [Chen and Gel, 2023]; 2) Optimal transport based"}, {"title": "5.1 Graph Classification Benchmark", "content": "Datasets We leverage eight popular graph classification benchmarks [Morris et al., 2020], comprising five bioinformatics datasets (MUTAG, PROTEINS, NCI1, PTC-MR, BZR) and three social network datasets (IMDB-B, IMDB-M, COLLAB). We also use three large-scale imbalanced datasets (PC-3, MCF-7, and ogbg-molhiv [Hu et al., 2020]). A summary of data statistics is in Table 6.\nBaselines Our approach is benchmarked against four groups of state-of-the-art baselines: 1) GNN models with global or sophisticated pooling operations, including PATCHY-SAN [Niepert et al., 2016], DIFFPOOL [Ying et al., 2018], GIN [Xu et al., 2018], DropGIN [Papp et al., 2021], SEP [Wu et al., 2022], GMT [Baek et al., 2021], MinCutPool [Bianchi et al., 2020], ASAP [Ranjan et al., 2020], and Wit-TopoPool [Chen and Gel, 2023]; 2) Optimal transport based"}, {"title": "5.2 Time Cost Comparison", "content": "We compare the time cost of our GRDL with two models that leverage optimal transport distances discussed in Section 4: OT-GNN [Chen et al., 2021] and TFGW [Vincent-Cuaz et al., 2022]. Compared with them, our model has significantly lower time costs. We present empirical average training time per epoch in Figure 2 and average prediction time per graph in Table 9 in Appendix D.4. Experiments were conducted on CPUs (Apple M1) using identical batch sizes, ensuring a fair comparison. It's noteworthy that the OT solver employed in TFGW and OT-GNN is currently confined to CPU, influencing the choice of hardware for this evaluation. We also analyzed the theoretical time complexity in Appendix D.4 (see Table 8).\nWe also compare training time with two latest pooling methods including Wit-TopoPool [Chen and Gel, 2023] and MSGNN Lv et al. [2023] on eight real datasets and three synthetic datasets. The three synthetic datasets have 2000 graphs with 100(SYN-100), 300(SYN-300), and 500(SYN-500) nodes per graph, respectively. The edge number is"}, {"title": "5.3 Graph Visualization", "content": "We use t-SNE [Van der Maaten and Hinton, 2008] to visualize the distributions of graphs' node embeddings given by our GRDL model, which is equivalent to visualizing each graph in a 3-D coordinate system. Firstly we use MMD to calculate a distance matrix C\u2208R^(N+K)\u00d7(N+K) between the node embeddings {Hi}N_1 and the reference distributions {Dk}K_1. The 3-D visualization given by t-SNE using C is presented in Figure 3. The graphs are located around the references. It means that the learned references can represent realistic graphs' latent node embeddings from the data."}, {"title": "5.4 More Numerical Results", "content": "The ablation study, influence of 0, generalization comparison with GIN are in Appendix D.5, D.6, A, respectively."}, {"title": "6 Conclusions", "content": "We proposed GRDL, a novel framework for graph classification without global pooling operations and hence effectively preserve the information of node embeddings. What's more, we theoretically analyzed the generalization ability of GRDL, which provided valuable insights into how the generalization ability scales with the properties of the graph data and network structure. Extensive experiments on moderate-scale and large-scale benchmark datasets verify the effectiveness and efficiency of GRDL in comparison to baselines.\nHowever, on some benchmark datasets (e.g. NCI1), our model does not outperform the baseline, which may be a limitation of our work and requires further investigation in the future."}]}