{"title": "GRAPH CLASSIFICATION VIA REFERENCE DISTRIBUTION\nLEARNING: THEORY AND PRACTICE", "authors": ["Zixiao Wang", "Jicong Fan"], "abstract": "Graph classification is a challenging problem owing to the difficulty in quantifying the similarity\nbetween graphs or representing graphs as vectors, though there have been a few methods using graph\nkernels or graph neural networks (GNNs). Graph kernels often suffer from computational costs and\nmanual feature engineering, while GNNs commonly utilize global pooling operations, risking the loss\nof structural or semantic information. This work introduces Graph Reference Distribution Learning\n(GRDL), an efficient and accurate graph classification method. GRDL treats each graph's latent node\nembeddings given by GNN layers as a discrete distribution, enabling direct classification without\nglobal pooling, based on maximum mean discrepancy to adaptively learned reference distributions.\nTo fully understand this new model (the existing theories do not apply) and guide its configuration\n(e.g., network architecture, references' sizes, number, and regularization) for practical use, we derive\ngeneralization error bounds for GRDL and verify them numerically. More importantly, our theoretical\nand numerical results both show that GRDL has a stronger generalization ability than GNNs with\nglobal pooling operations. Experiments on moderate-scale and large-scale graph datasets show the\nsuperiority of GRDL over the state-of-the-art, emphasizing its remarkable efficiency, being at least\n10 times faster than leading competitors in both training and inference stages.", "sections": [{"title": "1 Introduction", "content": "Graphs serve as versatile models across diverse domains, such as social networks [Wang et al., 2018], biological\ncompounds [Jumper et al., 2021], and the brain [Ktena et al., 2017]. There has been considerable interest in developing\nlearning algorithms for graphs, such as graph kernels [G\u00e4rtner et al., 2003, Shervashidze et al., 2011, Chen et al., 2022b]\nand graph neural networks (GNNs) [Kipf and Welling, 2016, Defferrard et al., 2016, Gilmer et al., 2017]. GNNs have\nemerged as powerful tools, showcasing state-of-the-art performance in various graph prediction tasks [Veli\u010dkovi\u0107 et al.,\n2017, Gilmer et al., 2017, Hamilton et al., 2017, Xu et al., 2018, Sun et al., 2019, You et al., 2021, Ying et al., 2021, Liu\net al., 2022b, Chen et al., 2022a, Xiao et al., 2022, Sun et al., 2023]. Despite the evident success of GNNs in numerous\ngraph-related applications, their potential remains underutilized, particularly in the domain of graph-level classification.\nCurrent GNNs designed for graph classification commonly consist of two components: the embedding of node features\nthrough message passing [Gilmer et al., 2017] and subsequent aggregation by some permutation invariant global pooling\n(also called readout) operations [Xu et al., 2018]. The primary purpose of pooling is to transform a graph's node\nembeddings, a matrix, into a single vector. Empirically, pooling operations play a crucial role in classification [Ying\net al., 2018]. However, these pooling operations tend to be naive, often employing methods such as simple summation\nor averaging. These functions collect only first-order statistics, leading to a loss of structural or semantic information. In\naddition to the conventional sum or average pooling, more sophisticated pooling operations have shown improvements\nin graph classification [Li et al., 2015, Ying et al., 2018, Lee et al., 2019, 2021, Buterez et al., 2022], but they still carry\nthe inherent risk of information loss."}, {"title": "2 Proposed Approach", "content": "Following convention, we denote a graph\nwith index i by Gi = (Vi, Ei), where Vi\nand Ei are the vertex (node) set and edge\nset respectively. Given a graph dataset G =\n{(G1, y1), (G2, y2),..., (GN,YN)}, where\nYi \u2208 {1,2,..., K} is the associated label\nof Gi and yi = k means G\u2081 belongs to class\nk, the goal is to learn a classifier f from G\nthat generalizes well to unseen graphs. Since\nin many scenarios, each node of a graph has\na feature vector x and the graph is often rep-\nresented by an adjacency matrix A, we also\nwrite Gi = (Ai, X\u2081) for convenience, where\nAi \u2208 Rnini, Xi \u2208 Rnixdo, ni = |Vi is the\nnumber of nodes of graph i, and do denotes\nthe number of features. We may alternatively\ndenote the graph dataset as G = {((A1, X1), y1), ((A2, X2), Y2), ..., ((AN, XN),YN)}.\nOur approach is illustrated in Figure 1. For graph classification, we first use a GNN, denoted as fg, to transform each\ngraph to a node embedding matrix H\u00bf \u2208 Rni\u00d7d that encodes its properties, i.e.,\n\\(H_i = f_G(G_i) = f_G(A_i, X_i),\\) (1)\nwhere fG \u2208 FG and FG denotes a hypothesis space. The remaining task is to classify Hi without global pooling.\nDirect classification of node embeddings is difficult due to two reasons:\n(i) Different graphs have different numbers of nodes, i.e. in general, ni \u2260 nj if i \u2260 j.\n(ii) The node embeddings of each graph are permutation invariant, namely, PH; and H\u00bf represent the same graph\nfor any permutation matrix P.\nHowever, the two properties are naturally satisfied if we treat the node embeddings of each graph as a discrete\ndistribution. Specifically, each H\u00bf is a discrete distribution and each row of H\u00bf is an outcome of the distribution. There"}, {"title": "2.1 Model Framework", "content": ""}, {"title": "2.2 Design of FG and FD", "content": "We get GRDL's network F by concatenating the node embedding module and the reference module:\n\\(F := F_D \\circ F_G.\\) (10)"}, {"title": "3 Theoretical Analysis", "content": "In this section, we provide theoretical guarantees for GRDL, due to the following motivations:\n\u2022 As the proposed approach is novel, it is necessary to understand it thoroughly using theoretical analysis, e.g.,\nunderstand the influences of data and model properties on the classification.\n\u2022 It is also necessary to provide guidance for the model design to guarantee high accuracy in inference stages."}, {"title": "3.1 Preliminaries", "content": "Matrix constructions We construct big matrices X, A and D, where \\(X = [X_1^T,X_2^T,\u2026\u2026\u2026,X_N^T]^T \\in R^{(\\Sigma_{i=1}^N n_i)\\times d}\\);\n\\(A = diag(A_1, A_2,..., A_N) \\in R^{(\\Sigma_{i=1}^N n_i)\\times(\\Sigma_{i=1}^N n_i)}\\) is a block diagonal matrix, \\(D = [D_1^T, D_2^T,...,D_K^T]^T \\in R^{Km \\times d}\\).\nThe adjacency matrix with self-connectivity is \u0100 = A + I. The huge constructed graph is denoted by G = (\u00c3, X).\nThis construction allows us to treat all graphs in dataset G as a whole and it is crucial for our derivation.\nNeural network Previously, for a deterministic network f \u2208 F, its output after feeding forward a single graph is\nf(Gi). However, we mainly deal with the huge constructed graph G in this section, and notation will be overloaded to\nf(G) = S \u2208 RN\u00d7K, a matrix whose i-th row is f (Gi).\nWe instantiate the message passing network as Graph Isomorphism Network (GIN) [Xu et al., 2018]. We choose\nto focus on GIN for two reasons. Firstly, the analysis on GIN is currently limited, most of the current bounds for\nGNNs don't apply for GIN [Garg et al., 2020, Liao et al., 2021, Tang and Liu, 2023]. The other reason is that GIN is\nused as the message-passing network in our numerical experiments. Notably, our proof can be easily adapted to other\nmessage-passing GNNs (e.g. GCN [Kipf and Welling, 2016]). GIN updates node representations as\n\\(h_v^{(l)} = MLP^{(l)} \\left( (1+\\epsilon^{(l)}) h_v^{(l-1)} + \\sum_{u \\in \\mathcal{N} (v)} h_u^{(l-1)} \\right)\\) (17)\nwhere \\(h_v^{(l)}\\) denotes the node features generated by l-th GIN message passing layer. Let \u03b5(1) = 0 for all layers and\nsuppose all MLPs have r layers, the node updates can be written in matrix form as\n\\(H^{(l)} = \\sigma \\left( ... \\sigma \\left( \\left( A H^{(l-1)} \\right) W_1^{(l)} \\right) ... W_{r-1}^{(l)} \\right) W_r^{(l)}\\) (18)\nwhere \\(W_i^{(l)}\\in R^{d \\times d}\\) is the weight matrix, and H(\u00b9) is the matrix of node features with H(0) = \u03a7. \u03c3(\u00b7) is the\nnon-linear activation function. Let F\u00b9 be the function space induced by the l-th message passing layer, meaning\n\\(F^l = \\left\\{ (\\tilde{A}, H^{(l-1)}) \\rightarrow H^{l} : W_i^{(l)} \\in \\mathcal{B}^{(l)}, i \\in [r] \\right\\}\\) (19)\nwhere \\(\\mathcal{B}^{(l)}\\) is some constraint set on the weight matrix \\(W_i^{(l)}\\) and H(1) is given by (18). The L-layer GIN function space\nFG is the composition of F\u00b9 for l \u2208 [L], i.e.,\n\\(F_G = F_L \\circ F_{L-1} \\circ ... 0 F_1 = \\left\\{ G \\rightarrow f_L( . . . f_1(G)) : f^l \\in F^i, \\forall i \\in [L] \\right\\}.\\) (20)\nLetting Sik = \u2212MMD\u00b2(H(L), Dr), the reference layer defines the following function space\n\\(F_D = \\left\\{ H^{(L)} \\leftrightarrow S\\in R^{N \\times K} :D_k \\in R^{m\\times d},k \\in [K] \\right\\}.\\) (21)\nOur proposed network (GRDL) is essentially F := FD \u00b0 FG.\nLoss Function Instead of the cross entropy loss (7), we consider a general loss function ly(\u00b7, \u00b7) satisfying 0 \u2264 ly \u2264\nto quantify the model performance. Importantly, this loss function is not restricted to the training loss because our\ngeneralization bound is optimization-independent. For instance, the loss function can be the ramp loss that is commonly\nused for classification tasks [Bartlett et al., 2017, Mohri et al., 2018]. Given a neural network f \u2208 F, we want to upper\nbound the model population risk of graphs and labels from an unknown distribution X \u00d7 Y\n\\(L_{\\mathcal{Y}}(f) := \\mathbb{E}_{(G,y) \\sim \\mathcal{X} \\times \\mathcal{Y}} [l_{\\mathcal{Y}} (f(G), y))].\\) (22)\nGiven the observed graph dataset G sampled from X \u00d7 Y, the empirical risk is\n\\(\\hat{L}_{\\mathcal{Y}} (f) := \\frac{1}{N} \\sum_{i=1}^N l_{\\mathcal{Y}}(f(G_i), Y_i),\\) (23)\nof which (7) is just a special case. Appendix E provides more details about the setup and our idea."}, {"title": "3.2 Main Results", "content": "For convenience, similar to [Bartlett et al., 2017, Ju et al., 2023], we make the following assumptions.\nAssumption 3.1. The following conditions hold for Fy := {(G, y) \u2192 ly(f(G), y) : f \u2208 F}:\n(i) The activation function \u03c3(\u00b7) is 1-Lipschitz (e.g. Sigmoid, ReLU).\n(ii) The weight matrices satisfy \\(W_i^{(l)} \\in \\mathcal{W}_i^{(l)} := \\left\\{ W : ||W||_o \\le \\kappa, ||W||_{2,1} \\le b \\right\\}. \\)\n(iii) The constructed reference matrix satisfy ||D||2 \u2264 bp.\n(iv) The Gaussian kernel parameter @ is fixed.\n(v) The loss function ly(\u00b7, y) : RK \u2192 R is \u00b5-Lipschitz w.r.t || \u00b7 ||2 and 0 \u2264 ly \u2264 y.\nTheorem 3.2 (Generalization bound of GRDL). Let n = mini ni, c = ||\u00c3||o, and d = maxi, d). Denote RG :=\nc2L ||X||\u00bd ln(22) (\u03a0=1(\u03a0\u221215)2) (\u03a3\u03af=1 \u03a3=1 (0)2/3)3. For graphs G = {(Gi, yi)}=1 drawn i.i.d from any\nprobability distribution over X \u00d7 {1, . . ., K} and references {Dk}k=1, Dk \u2208 Rm\u00d7d, with probability at least 1 \u03b4,\nevery loss function l and network f \u2208 F under Assumption 3.1 satisfy\n\\(L_{\\mathcal{Y}}(f) \\le \\hat{L}_{\\mathcal{Y}} (f) + 3\\gamma \\sqrt{\\frac{\\ln (2/\\delta)}{2N}} + \\frac{8\\gamma + 24\\mu(\\Pi_{i=1}^{L} \\kappa_i^{(l+1)})\\sqrt{R_G+R'_G} \\ln N}{N} + \\frac{24\\gamma\\sqrt{N v_2} \\ln v_3}{N}\\)\nwhere \\(v_1 = \\frac{640K R_G \\mu^2}{n}, v_2 = Kmd\\), and \\(v_3 = \\frac{24 \\sqrt{\\theta} N b_D \\mu}{\\sqrt{m}}\\).\nThe bound shows how the properties of the neural network, graphs, reference distributions, etc, influence the gap between\ntraining error and testing error. A detailed discussion will be presented in Section 3.3. Some interesting corollaries\nof Theorem 3.2, e.g., misclassification rate bound, can be found in Appendix F.7. Besides small generalization error\nLy (f) \u2013 \u00ce\u2084(f), a good model should have small empirical risk L\u2081(f). The empirical risk \u00ce\u2084(f) is typically a surrogate\nloss of misclassification rate of training data and a lower misclassification rate implies a smaller Ly(f). We now provide\na guarantee for the correct classification of training data, namely small L\u2081(f).\nNotably, the node embeddings Hi from the k-th class as well as the reference distributions Dk are essentially some finite\nsamples from an underlying continuous distribution Pk. One potential risk is that, although the continuous distributions\nP1, P2,..., \u0420\u043a are distinct, we can only observe their finite samples and may fail to distinguish them from each\nother with MMD. Specifically, suppose a node embedding H\u00bf is from the k-th class, although 0 = MMD(Pk, Pk) <\nMMD(Pk, Pj) for any j \u2260 k, it is likely that MMD(H\u00bf, Dk) > MMD(H\u00bf, Dj) for some j \u2260 k. The following\ntheorem provides the correctness guarantee for the training dataset G:\nTheorem 3.3. All graphs in the training set G are classified correctly with probability at least \\(1 - \\delta\\) if\n\\(\\min_{i\\neq j} MMD^2(P_i, P_j) > \\left(\\frac{4}{n} + \\frac{4}{\\sqrt{m}} \\right) \\left(4 + 4\\sqrt{\\log \\frac{2}{\\delta}} \\right)\\)"}, {"title": "3.3 Bound Discussion and Numerical Verification", "content": "Let \\(\\kappa = \\max_{i,l} \\kappa_i^{(l)}\\), \\(\\sigma = \\max_{i,l} b_i^{(l)}\\) = maxi, and suppose d is large enough, we simplify Theorem 3.2 as\n\\(L_{\\mathcal{Y}}(f) \\le \\hat{L}_{\\mathcal{Y}}(f) + \\tilde{O}(\\sqrt{\\frac{1}{N}} + \\sqrt{N v_2}) \\le \\hat{L}_{\\mathcal{Y}}(f) + \\tilde{O}\\left(\\frac{2\\mu \\sigma \\|X\\|_2 cL \\sqrt{(L r)^{1.5}}\\sqrt{\\theta \\kappa} \\sqrt{Kmd}}{N}\\right)\\)\nI. Dependence on graph property One distinctive feature of our bound is its dependence on the spectral norm of\ngraphs' adjacency matrix. The large adjacency matrix \u00c3 is a block-diagonal matrix, so its spectral norm c = ||\u00c3||, =\nmaxie[N] ||\u00c3i||o. By Lemma F.8, incorporating c\u00b9 is sufficient for any L-step GIN message passing. This result\naligns with Ju et al. [2023], who achieved this conclusion via PAC-Bayesian analysis. Our derivation, based on the\nRademacher complexity, provides an alternative perspective supporting this result. Notably, Liao et al. [2021] and Garg\net al. [2020] proposed bounds scaling with graphs' maximum node degree, which is larger than the spectral norm of the\ngraphs' adjacency matrix (Lemma F.18). Consequently, our bound is tighter."}, {"title": "4 Related Work", "content": "Various sophisticated pooling operations have been designed to preserve the structural information of graphs. DIFF-\nPOOL, designed by Ying et al. [2018], learns a differentiable soft cluster assignment for nodes and maps nodes to a\nset of clusters to output a coarsened graph. Another method by Lee et al. [2019] utilizes a self-attention mechanism\nto distinguish nodes for retention or removal, and both node features and graph topology are considered with the\nself-attention mechanism.\nA recent research direction focuses on preserving structural information by leveraging the optimal transport (OT) [Peyr\u00e9\nand Cuturi, 2020]. OT-GNN, proposed by Chen et al. [2021], embeds a graph to a vector by computing Wasserstein\ndistances between node embeddings and some \"learned point clouds\". TFGW, introduced by Vincent-Cuaz et al.\n[2022], embeds a graph to a vector of Fused Gromov-Wasserstein (FGW) distance [Vayer et al., 2018] to a set of\n\"template graphs\". OT distances have also been combined with dictionary learning to learn graph vector embedding in\nan unsupervised way (GDL) [Liu et al., 2022a, Vincent-Cuaz et al., 2021, Zeng et al., 2023].\nSimilar to the \"learned point clouds\" in OT-GNN, \u201ctemplate graphs\u201d in TFGW, and dictionaries in GDL, our GRDL\npreserves information in node embeddings using reference distributions. To the best of the authors' knowledge, we\nare the first to model a graph's node embeddings as a discrete distribution and propose to classify it directly without\naggregating it into a vector, marking our novel contribution. Additionally, our work stands out as the first to analyze the\ngeneralization bounds for this type of model, adding a theoretically grounded dimension to the research. By the way,\nour method is much more efficient than OT-GNN and TFGW."}, {"title": "5 Numerical Experiments", "content": "We leverage eight popular graph classification benchmarks [Morris et al., 2020], comprising five bioinfor-\nmatics datasets (MUTAG, PROTEINS, NCI1, PTC-MR, BZR) and three social network datasets (IMDB-B, IMDB-M,\nCOLLAB). We also use three large-scale imbalanced datasets (PC-3, MCF-7, and ogbg-molhiv [Hu et al., 2020]). A\nsummary of data statistics is in Table 6.\nBaselines Our approach is benchmarked against four groups of state-of-the-art baselines: 1) GNN models with global\nor sophisticated pooling operations, including PATCHY-SAN [Niepert et al., 2016], DIFFPOOL [Ying et al., 2018],\nGIN [Xu et al., 2018], DropGIN [Papp et al., 2021], SEP [Wu et al., 2022], GMT [Baek et al., 2021], MinCutPool\n[Bianchi et al., 2020], ASAP [Ranjan et al., 2020], and Wit-TopoPool [Chen and Gel, 2023]; 2) Optimal transport based"}, {"title": "5.1 Graph Classification Benchmark", "content": "Datasets"}, {"title": "5.2 Time Cost Comparison", "content": "We compare the time cost of our GRDL with two models that leverage optimal transport distances discussed in Section 4:\nOT-GNN [Chen et al., 2021] and TFGW [Vincent-Cuaz et al., 2022]. Compared with them, our model has significantly\nlower time costs. We present empirical average training time per epoch in Figure 2 and average prediction time per\ngraph in Table 9 in Appendix D.4. Experiments were conducted on CPUs (Apple M1) using identical batch sizes,\nensuring a fair comparison. It's noteworthy that the OT solver employed in TFGW and OT-GNN is currently confined\nto CPU, influencing the choice of hardware for this evaluation. We also analyzed the theoretical time complexity in\nAppendix D.4 (see Table 8)."}, {"title": "5.3 Graph Visualization", "content": "We use t-SNE [Van der Maaten and Hinton, 2008] to visualize the distributions of graphs' node embeddings given\nby our GRDL model, which is equivalent to visualizing each graph in a 3-D coordinate system. Firstly we use\nMMD to calculate a distance matrix C\u2208R(N+K)\u00d7(N+K) between the node embeddings {H}\u2081 and the reference\ndistributions {D}K_1. The 3-D visualization given by t-SNE using C is presented in Figure 3. The graphs are located\naround the references. It means that the learned references can represent realistic graphs' latent node embeddings from\nthe data."}, {"title": "5.4 More Numerical Results", "content": "The ablation study, influence of 0, generalization comparison with\nGIN are in Appendix D.5, D.6, A, respectively."}, {"title": "6 Conclusions", "content": "We proposed GRDL, a novel framework for graph classification\nwithout global pooling operations and hence effectively preserve\nthe information of node embeddings. What's more, we theoretically\nanalyzed the generalization ability of GRDL, which provided valuable\ninsights into how the generalization ability scales with the properties\nof the graph data and network structure. Extensive experiments\non moderate-scale and large-scale benchmark datasets verify the\neffectiveness and efficiency of GRDL in comparison to baselines.\nHowever, on some benchmark datasets (e.g. NCI1), our model does\nnot outperform the baseline, which may be a limitation of our work\nand requires further investigation in the future."}]}