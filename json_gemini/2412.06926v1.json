{"title": "When Every Token Counts: Optimal Segmentation for Low-Resource Language Models", "authors": ["Bharath Raj S", "Garvit Suri", "Vikrant Dewangan", "Raghav Sonavane"], "abstract": "Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource (LR) language applications, highlighting a promising direction for further research and inclusive NLP.", "sections": [{"title": "1 Introduction", "content": "The development of large language models (LLMs) has significantly advanced natural language processing. These models (Radford et al., 2019; Brown et al., 2020; OpenAI et al., 2024) have demonstrated unprecedented capabilities in tasks ranging from text generation and translation to complex problem-solving and creative writing. However, despite these advancements, challenges remain in effectively processing Low-Resource (LR) languages and optimizing models of varying scales.\nA critical aspect influencing model performance is tokenization the process of converting text into tokens that the model can understand. Tokenization methods are pivotal in large language models, with popular techniques including WordPiece (Schuster and Nakajima, 2012), SentencePiece (Kudo and Richardson, 2018), and Unigram-LM(Kudo, 2018). WordPiece, used in models like BERT (Devlin et al., 2019), tokenizes words into subword units based on their frequency in the training data, improving the model's handling of rare or out-of-vocabulary words. SentencePiece and Unigram-LM, commonly used in models like GPT, employ a character or byte-based approach that doesn't rely on predefined word boundaries, making them versatile across languages.\nLR languages face two significant challenges in natural language processing: a lack of high-quality and diverse datasets and novel methods to represent this data. (Magueresse et al., 2020). Without ample data, models struggle to learn the complex linguistic patterns necessary for tasks such as machine translation, sentiment analysis, and summarization. Secondly, compression challenges in tokenization exacerbate the difficulties faced by LR languages. Common tokenization techniques, such as BPE, often fragment words into smaller, frequently occurring subwords. The bloating of tokens leads to higher computational and memory costs, as models must process longer sequences (Ahia et al., 2023). Inefficient tokenization also results in less accurate representations, leading to fragmented or improperly segmented tokens, which negatively impacts model performance in tasks requiring precise language understanding (Rust et al., 2021; Zhang et al., 2022). We refer to the strategy adopted by BPE as the Greedy segmentation algorithm.\nThe widely used GPT-2 tokenizer (Radford et al., 2019) handles any input without unknown tokens, yet it compromises tokenization efficiency, especially for non-English text and special characters. This English-centric model often splits languages like Turkish, Indonesian, or Malay into byte sequences, unnecessarily lengthening token sequences and reducing the effective context window for non-English content. While the GPT-4 (Ope"}, {"title": "2 Related Work", "content": "Recent research has focused on the effects that compression has on tokenization, which are particularly relevant for optimizing language models in resource-constrained environments. A study by (Goldman et al., 2024) shows the correlation that compression has on downstream tasks such as classification and generation. In contrast, (Uzan et al., 2024)'s exploration of greedy algorithms and (Schmidt et al., 2024)'s introduction of PathPiece have provided new insights into optimizing tokenization for both performance and efficiency, without looking into compression. Note that while (Uzan et al., 2024) and (Schmidt et al., 2024) demonstrate the effectiveness of their tokenizer, they show results on English tasks but do not show the impact on linguistic diversity. The paper by (Goldman et al., 2024) demonstrates this to some extent; however, their experiments focus primarily on English.\n(Moghe et al., 2023) provide a task-oriented perspective on the challenges that LLMs encounter with low-resource languages, highlighting the need for tailored approaches in multilingual contexts. The quality of tokenization has been a subject of intense study, with comparative analyses by (Gall\u00e9, 2019), (Dagan et al., 2024), and (Saleva and Lignos, 2023) providing valuable insights into the relative performance of different tokenization methods across various languages and tasks. In multilingual settings, subword tokenizers lead to disproportionate fragmentation rates for different languages and writing script (Zhang et al., 2022). Similarly, monolingual optimized tokenizers may not be as efficient for multilingual settings (Rust et al., 2021). (Petrov et al., 2023) introduces a new concept known as parity or premiums in tokenizers which has shed light on the importance of balanced tokenization across"}, {"title": "3 Background", "content": "We first provide a brief description of the steps involved in tokenization that is pre-tokenization, vocabulary construction, and segmentation. We then describe the Token Saving Ratio (TSR) metric used to compare results throughout our paper."}, {"title": "3.1 Stages of Tokenization", "content": "In any modern natural language system, a document d, before it gets encoded into a set of tokens {t1, t2,...tk} goes through 3 main stages to tokenization. They are (i) Pre-tokenization (ii) Vocabulary Construction and (iii) Segmentation. Pre-tokenization consists of the initial processing phase where raw text in the document undergoes fundamental transformations. It ensures the text is in a consistent format for subsequent processing. The vocabulary construction phase focuses on building a comprehensive token dictionary V of size m from the processed text. This stage involves analyzing large text corpora to identify recurring patterns and meaningful units. The system conducts frequency analysis to determine the most common patterns and handles rare words appropriately. The final segmentation stage implements the actual tokenization process using the constructed vocabulary.\nGiven a vocabulary V, and a document d, segmentation task S refers to the task of dividing the document d into a sequence of tokens (ti), such that S(d) = {1, . . . tk|Vi \u2208 [1, K],ti \u2208 V}. During this phase, the system applies specific tokenization rules to convert text into its final token form. The process includes mechanisms for handling unknown tokens (UNK) that may not exist in the vocabulary. Subword tokenization strategies are implemented to manage complex words and maintain semantic meaning. The stage concludes with the assignment of unique token IDs to each segmented unit, creating the final tokenized representation of the text. This standardized format enables efficient processing in downstream natural language processing tasks. For the scope of this work, we exclusively study the segmentation stage of tokenization and detail an optimal segmentation algorithm."}, {"title": "3.2 Token Saving Ratio (TSR)", "content": "To measure the quality of segmentation, we define and use the metric, Token Saving Ratio (TSR), to capture the ratio of tokens saved when using tokenizer TA with segmentation strategy SA compared to tokenizer TB with strategy SB. The Token Saving Ratio when using tokenizer TA compared to tokenizer TB is defined as:\n$TSR=\\frac{|S_B(d)| - |S_A(d)|}{|S_B(d)|}$  (1)\nA positive TSR directly translates to shorter sequence lengths, which is paramount for computational efficiency. Since the computational complexity of transformer-based models typically scales quadratically with sequence length (O(n\u00b2)), reducing the number of tokens can significantly decrease both memory requirements and processing time. For instance, if tokenizer TA produces sequences half the length of TB, the computational cost could potentially be reduced by a factor of four."}, {"title": "4 Optimal Segmentation", "content": "In this section, we define the problem of optimal segmentation mathematically and follow it up with a discussion of our algorithm presented in Algorithm 1."}, {"title": "4.1 Definition", "content": "Given a vocabulary V of size m, we define optimal segmentation (S*) as the segmentation that minimizes the number of tokens a given document d can be split into. Formally,\n$S(d) = {t_1, ... t_k | t_i \\in V}$\n$S^* = \\underset{S}{\\text{minimize }} |S(d)|$\n (2)"}, {"title": "4.2 The Algorithm", "content": "We use a dynamic programming formulation similar to the Viterbi algorithm (Forney, 1973) and produces the optimal segmentation S*. Given a document d, define dp[i] as the minimal number of tokens needed to segment the prefix dod\u2081 ... di (positions 0 to i, inclusive). We set dp[-1] = 0 as the base case, representing the empty prefix requiring zero tokens. The parent array par serves as a backtracking mechanism where par[i] points to the end of the previous token in the optimal segmentation."}, {"title": "5 Experimental Setup", "content": "For our work, we extended on OpenAI's\u00b9 family of Tokenizers which are available in three distinct vocabulary sizes: 50K, 100K, and 200K tokens, as detailed in Table 3. In this study, we rely on the original pre-tokenization regular expressions and the trained vocabulary made public by OpenAI, without making any modifications to it. Our study concentrated exclusively on the segmentation strategies of these tokenizers.\nWe divide our experiments into two parts: intrinsic and extrinsic, following the approach of (Goldman et al., 2024). The intrinsic experiments focus purely on the segmentation aspect of tokenization, without involving any deep learning models. Here, we analyze the TSR when comparing optimal versus greedy segmentations across languages. Based on vocabulary size, we select appropriate tokenizers according to Table 3, which serve as the baseline"}, {"title": "5.1 Intrinsic Evaluation Datasets", "content": "For performing the intrinsic evaluation, we used the CC-100 dataset (Wenzek et al., 2020). The CC-100 dataset consists of monolingual data of 116 languages extracted from the January-December 2018 Commoncrawl snapshots. We benchmark on the English language using the Wikipedia corpus readily accessible on Kaggle Datasets\u00b2. We utilized the Wikipedia 2023 dump, which contains 6 million articles, titles, text, and categories."}, {"title": "5.2 Extrinsic Evaluation Tasks", "content": "We relied on the intrinsic evaluation of languages to choose the languages for our extrinsic experiments. We choose English to show that there is no degradation in performance in a language with near-zero compression. We also chose Finnish, Indonesian, and Turkish which show up in the top languages with high TSR. To evaluate our pre-trained checkpoints, we evaluated multiple tasks for different languages, as detailed in Table 4. The tasks are mentioned in detail one by one below in Appendix D. For all of the extrinsic experiments, we set the vocabulary size to m = 50K and use the gpt-2 tokenizer (Table 3)."}, {"title": "6 Results", "content": "In this section, we present the results of intrinsic evaluation on the CC-100 dataset. We first highlight qualitative examples to showcase the inefficiency of BPE with Greedy segmentation compared to BPE with Optimal segmentation We also showcase an interesting observation that word length has on the TSR. Finally, to validate our optimal segmentation algorithm, we conduct extensive extrinsic evaluations across multiple downstream tasks.\nFirst, we report improvement upon Greedy BPE's performance across language boundaries for non-English tasks. At the same time, we report an increase in improvements for the TSR* split of the dataset, thus highlighting the need for token saving in downstream performance. At the end, we report perplexity scores on English datasets to state that the improvement provided by our optimal segmentation doesn't reduce the tokenizer's performance in English."}, {"title": "6.1 Intrinsic Evaluation", "content": "6.1.1 Qualitative Results\nTable 2 presents examples of how different tokenizers segment the same vocabulary in distinct ways, depending on their inference mode. Greedy \u0412\u0420\u0415, for instance, splits the word \"policy makers\" into 4 tokens: \"p\" \"olic\" \"ym\" \"akers\", while the optimal segmentation splits it into two tokens: \"policy\" and \"makers\". The table illustrates fundamental linguistic issues with greedy BPE segmentation across different language families. In English, it fails to respect compound word boundaries (policymakers). For agglutinative languages like Turkish and Malaysian, it breaks crucial morphological units, splitting tense markers and case endings arbitrarily. In Dravidian languages (Telugu, Tamil), it fails to preserve verb roots and aspectual markers. For Indo-Aryan languages, it incorrectly segments Sanskrit-derived compounds, creating linguistically meaningless units. These issues extend beyond mere segmentation - they affect the model's ability to learn proper morphological patterns, potentially impacting downstream task performance. While BPE has been widely adopted for its computational efficiency, these examples demonstrate the need for more linguistically-informed tokenization strategies that respect language-specific morphological structures that our optimal segmentation can provide."}, {"title": "6.1.2 Quantitative Results", "content": "We report TSR across the 116 languages in the CC-100 dataset. Languages with the highest TSR can be found in Table 6. This table demonstrates the wide variation in TSR achieved by tokenizing different languages across 50K, 100K, and 200K vocabulary sizes. The languages with the highest TSR, such as Oromo, Swati, and Quechua, maintain over 4.5% TSR even at the largest 200K vocabulary. In contrast, lower-resourced languages like Tagalog, Bosnian, Hausa, and Turkish have lower compression rates, near 3% even at the smaller 50K size. This data offers important insights to guide vocabulary selection and optimization decisions,"}, {"title": "Word length Relation with TSR:", "content": "We plot an interesting observation that word length has with TSR in Figure 1. We notice a strong correlation, with longer words achieving better compression ratios (increasing from ~0.15 for 4-character words to ~0.30 for 11-12 character words) - suggesting that word length appears to be one of the factors in compression efficiency across these linguistically diverse languages. This pattern is consistent across all languages in our study, though with varying slopes - Finnish and Turkish show steeper increases with word length, while English demonstrates a more gradual rise. Notably, agglutinative languages like Finnish, Turkish, and Indonesian,"}, {"title": "6.2 Extrinsic Evaluation Tasks", "content": "Table 7 presents a systematic analysis across languages and tasks, examining how different types of tokenization errors\u2014particularly compound word splitting, verb root identification, and morpheme boundary detection-affect downstream performance. For the Indonesian Emot task with the 120M model, optimal segmentation improves accuracy by 4.32% (from 40.23% to 44.55%) in the full dataset. This improvement becomes more pronounced in the TSR* subset, reaching 5.64% (from 39.23% to 44.87%), primarily due to better handling of compound words (e.g., \"memberikan\" \u2192 \"memberi\" + \"kan\") and proper verb root preservation. In the 350M model, while the overall gap is smaller at 2.50%, it still increases to 2.56% in the TSR* subset, showing similar error patterns but at reduced magnitudes. The WreTe task shows similar error patterns: optimal segmentation yields a 2.00% improvement in the full dataset, expanding to 2.66% in TSR*, with compound word splitting errors driving a significant portion of the performance difference. For Turkish (XNLI), we observe improvements of 0.56% to 0.76% (120M) and 0.98% to 0.79% (350M), where analysis shows that agglutinative morpheme boundaries (particularly case markers and possessive suffixes) significantly impact performance. Finnish presents a unique case where accuracies remain identical between All and TSR* subsets, as all words exhibit non-zero TSR scores.\nFor English tasks, we observe moderate differences in the performance between All and the TSR* subset. In Story Cloze, the 350M model shows an improvement with Optimal segmentation in TSR* (7.83% gain, from 52.17% to 60.00%) compared to the full dataset (0.43% gain, from 51.31% to 51.74%). QQP shows varying patterns: in the 120M model, Greedy performs better in both sets, with the gap being more pronounced in TSR* (-0.70% vs -0.38%). These results suggest that evaluating the TSR* subset often amplifies the impact"}, {"title": "7 Conclusion", "content": "In the scope of this work, we identified the inefficient greedy segmentation method used in the BPE tokenizer and proposed an optimal segmentation algorithm that results in efficient token utilization, particularly for LR languages. We established the optimality of our algorithm by showing its impact in both intrinsic and extrinsic experiments as done in the literature. By studying multiple languages, we observed a strong correlation between improvements in Token Saving Ratios and linguistically better segments, with this effect being especially pronounced for morphologically complex words and propagating to performance improvement in downstream tasks. These findings underscore the need for research in tokenization approaches that can boost model effectiveness, especially for language models serving low-resource languages."}, {"title": "8 Limitations and Future Work", "content": "Our work demonstrates the impact of using BPE tokenization with optimized segmentation on tokenization efficiency across multiple languages. Although we evaluated models on intrinsic metrics for a variety of languages, our extrinsic evaluations focused primarily on four languages: English, Finnish, Indonesian, and Turkish. We chose these languages to capture diversity in typology and morphology, as well as to leverage the relatively richer resources available for them compared to many other LR languages. In the future, we intend to perform a more comprehensive follow-up study to replicate these findings across a wider array of languages provided in Table 6, aiming to validate the broader applicability of our approach. This could help assess the robustness of using optimal segmentation across languages with more complex or less studied morphological characteristics.\nFuture research would also explore other underlying factors influencing tokenization quality and its broader impact on language model success. This extension would help us understand whether our findings about optimal segmentation scale to models with larger vocabularies and more sophisticated architectures. In future work, we plan to extend our analysis to larger foundation models like LLaMA-3 (Grattafiori et al., 2024), where the impact of tokenization strategies may reveal additional insights about segmentation in more complex architectures. We would also explore improvements in other stages, such as optimal vocabulary selection and encoding methods for adaptive tokenization."}, {"title": "A Language Model Parameters", "content": "The 120M parameter models were trained using the GPT architecture with the following parameters.\nIn the worst case, there are no better alternatives than km-2, \ndp[i] = dp[km-2] + 1 \nBy a similar argument,\ndp[km-2] = dp[km\u22123] + 1, \ndp[km\u22123] = dp[km\u22124] + 1, \n\u22ee \ndp[ki] = dp[ki-1] + 1, \n\u22ee \ndp[ko] = dp[k-1] + 1, \nUsing the above results, \ndp[i] = dp[km-2] + 1 \n= dp[km\u22123] + 1 + 1 \n\u22ee \n= dp[ki] + m -1-i \n\u22ee \n= dp[k-1] + m \u2212 1 \u2212 (\u22121) = m \nSimplifying to, \ndp[i] = m \nHowever, we initially assumed that m < dp[i]. This leads to a contradiction, which means our initial assumption that there exists a better segmentation is wrong. This completes the proof."}, {"title": "B Proof of Optimality", "content": "B.1 Dynamic Programming Formulation\nDefine dp[i] as the minimal number of tokens needed to segment the prefix S0S1... Si (positions 0 to i, inclusive). We set dp[-1] = 0 as the base case, representing the empty string requiring zero tokens. The recurrence relation is:\n$dp[i] = \\underset{(0\\leq j \\leq i)}{\\text{min }} (dp[j-1] + 1)$ where $SjSj+1... Si \\in V$\nB.2 Proof by Contradiction:\nSuppose there exists a segmentation of the prefix S0S1... Si into tokens from vocabulary V that uses fewer tokens than dp[i] computed by our algorithm.\nLet this supposed optimal segmentation divide the prefix into tokens, ending at positions -1 = k-1 <ko < k\u20811 < k2 < ... < km\u22121 = i, resulting in m tokens:\n$To = Sk_{-1+1}S_{k-1+2}... S_{ko}$,\n$T1 = S_{ko+1}S_{ko+2}... S_{k2}$,\n\u22ee\n$Tm-1 = S_{km-2+1}S_{km-2+2}... S_{km-1}$.\nEach Tj \u2208 V, and the total number of tokens is m < dp[i].\nConsider the last token Tm-1 in this segmentation, which covers the substring $S_{km-2+1}S_{km-2+2}... S_{km-1}$. Since Tm\u22121 \u2208 V, our algorithm, when computing dp[i], examines this possibility.\nBy the definition of our algorithm:\ndp[i] = min (dp[i], dp[km\u22122] + 1)"}, {"title": "C Intrinsic Statistical Analysis", "content": "Frequency analysis with Word length: The word frequency distribution pattern provides crucial context for interpreting the extrinsic task performance. The frequency-based analysis shown in Fig. 2 helps explain why the impact of optimal segmentation varies significantly across languages and tasks, with larger gains in languages where optimal segmentation of longer words, though less frequent, carries greater semantic importance. The reported token saving percentages (TSR) may underestimate the true potential of optimal segmentation due to frequency-based evaluation bias. Since longer words (>6 characters) occur substantially"}, {"title": "D Extrinsic Evaluation Tasks", "content": "We describe the different tasks used for fine-tuning our models:\n\u2022 For English generation tasks, we used the Penn Tree Bank (PTB) dataset (Marcus et al., 1993), which serves as a traditional benchmark for assessing language generation capabilities through zero-shot perplexity, leveraging its pre-internet content. Additionally, the LAMBADA dataset (Paperno et al., 2016) was employed to test the model's ability to comprehend and predict the last word in a paragraph, challenging its handling of long-range dependencies. For English classification tasks, we utilized the Quora Question Pairs (QQP) dataset 4), which involves determining if question pairs are duplicates, evaluated using the F1 metric. The Story Cloze dataset (Mostafazadeh et al., 2016) was also used to measure the model's ability to choose the correct ending for short narratives, further assessing classification performance.\n\u2022 For Finnish we used the gold passage version of the Typologically Diverse Question Answering dataset (TyDiQA-GoldP) (Clark et al., 2020) (Ruder et al., 2021). It consists of a question, a relevant passage, and an answer - yes or no.\n\u2022 Expanding to Indonesian, we employed two datasets from the indoNLU (Wilie et al., 2020) collection: EmoT, which is an emotion classification dataset collected from Twitter consisting of tweets in Indonesian covering five emotion labels: anger, fear, happiness, love, and sadness; and WReTE, which is a textual entailment dataset constructed from Wikipedia revision history, containing pairs of sentences with binary semantic relations .\n\u2022 For Turkish, the XNLI dataset (Conneau et al., 2018) was utilized. XNLI extends the MultiNLI dataset into a multilingual evaluation suite, providing a benchmark for cross-lingual language understanding through sentence-pair classification tasks across 15 languages."}]}