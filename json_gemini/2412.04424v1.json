{"title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion", "authors": ["Jiuhai Chen", "Jianwei Yang", "Haiping Wu", "Dianqi Li", "Jianfeng Gao", "Tianyi Zhou", "Bin Xiao"], "abstract": "We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual rep-resentations produced by Florence-2 [45], a generative vi-sion foundation model. Unlike the widely used CLIP-style vision transformer [35] trained by contrastive learning, Florence-2 can capture different levels and aspects of vi-sual features, which are more versatile to be adapted to diverse downstream tasks. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2's visual features into pre-trained LLMs, such as Phi 3.5 and LLama 3. In particu-lar, we propose \u201cdepth-breath fusion (DBFusion)\" to fuse the visual features extracted from different depths and un-der multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. Our quantitative analysis and visualization of Florence-VL's visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over exist-ing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, percep-tion, hallucination, OCR, Chart, knowledge-intensive un-derstanding, etc. To facilitate future research, our mod-els and the complete training recipe are open-sourced.", "sections": [{"title": "1. Introduction", "content": "Recent progress in multimodal large language models (MLLMs) are largely driven by progress in large language models [26, 49]. However, when it comes to visual encoders, transformer-based models like CLIP or SigLIP remain the most commonly used choices. Despite CLIP and SigLIP's effectiveness, they come with limitations; for instance, their last-layer features usually provide an image-level semantic representation that captures the overall scene and context, but often overlook pixel or region-level details and low-level features that are critical to various downstream tasks. There is a much broader range of visual representation, such as the self-supervised DINOv2 model [34], diffusion model [37] and segmentation [20], [41] shows these different visual encoders can benefit well in some specific tasks.\nIn order to leverage distinctive representations of mul-tiple vision encoders, some recent works such as [38, 41] adopt a mixture of vision encoders that specialize in dif-ferent feature aspects or skills. However, integrating mul-tiple vision encoders increases the computational expense for both model training and deployment. Could a single vision model be designed to generate distinct visual fea-tures, each emphasizing different perceptual information in the input image? In this paper, we propose Florence-VL, which leverages the generative vision foundation model Florence-2 [45] as the vision encoder. Florence-2 offers a prompt-based representation for various computer vision tasks, including captioning, object detection, grounding, and OCR. Its versatile visual representations can benefit dif-ferent types of downstream tasks. For instance, OCR-based representations are advantageous for tasks that require ex-tracting textual information from images, and grounding-based representation can benefit for tasks that require the re-lationships between objects and their spatial contexts. How-ever, to build a better MLLM, how to extract these diverse features and align them with a pretrained LLM remains un-derexplored.\nTo address this, we propose Depth-Breadth Fusion (DBFusion) to effectively selecting and utilizing diverse vi-sual features. Visual features from different layers capture various levels of concepts, with the final layers typically representing higher-level concepts. Integrating lower-level features can therefore complement these high-level repre-sentations, which we refer to as the \"Depth\" of visual fea-tures. Additionally, since different downstream tasks need different perceptual information within images, a single im-age feature often falls short in capturing all relevant infor-mation. Thus, we leverage multiple image features, with each feature capturing different visual representations. We refer to this as the \"Breadth\" of visual features. For utilizing these diverse visual features, we find that a straightforward channel concatenation serves as a simple yet effective fu-sion strategy. Specifically, we concatenate multiple features along the channel dimension, and these combined features, spanning various depths and breadths, are then projected as input embedding to LLMs.\nWe train Florence-VL on a novel recipe of open-sourced training data, which is composed of a large-scale detailed captioning dataset and a mix of instruction tuning datasets for whole-model pretraining and partial-model finetuning, respectively. The resulted Florence-VL achieves signifi-cant advantages on 25 benchmarks covering vision-centric, knowledge-based, and OCR & Chart tasks, outperforming other advanced MLLMs like Cambrian [41]. Moreover, we provide quantitative analysis and visualization demonstrat-ing that Florence-VL's visual representation achieves better alignment to LLMs than the widely adopted vision encoders such as CLIP and SigLIP [26]."}, {"title": "2. Preliminary: Florence-2", "content": "Florence-2 [45] is a vision foundation model that utilizes a unified, prompt-based approach to handle various vi-sion tasks with simple instructions, such as captioning, ob-ject detection, grounding, and segmentation. The architec-ture consists of a vision encoder DaViT [9] and a stan-dard encoder-decoder model. It processes an input im-age $I \\in R^{H \\times W \\times 3}$ (where H and W indicate height and width, respectively) into flattened visual token embeddings. The model then applies a standard encoder-decoder trans-former architecture to process both visual and language to-ken embeddings. It first generates prompt text embeddings $T \\in R^{N_t \\times D}$ using the language tokenizer and word em-bedding layer, with $N_t$ and D representing the number and dimensionality of prompt tokens, respectively. The vision token embeddings are then concatenated with the prompt embeddings to create the input for the multi-modality en-coder module, $X = [V,T]$, where $V \\in R^{N_v \\times D}$ is pro-duced by applying a linear projection and LayerNorm layer to visual embedding from DaViT, with $N_v$ and D repre-senting the number and dimensionality of vision tokens, re-spectively. The linear projection and LayerNorm layer are used to ensure dimensionality alignment with T. Encoder-decoder model will process the X and generate the desir-able results, such as captions, object detections, grounding in textual form."}, {"title": "3. Method", "content": "To address the limitations of existing vision backbones in MLLMs, specifically, last layer features typically yield an image-level representation that captures overall scene and context but often misses pixel- or region-level details, we utilize the vision foundation model Florence-2 as our visual encoder for extracting visual features. Unlike the CLIP pre-trained vision transformers that provide a single, universal image feature, Florence-2 can identify spatial details at dif-ferent scales, by using different tasks prompts.\nIn MLLMs, effective image understanding requires cap-turing multiple levels of granularity, from global seman-tics to local details, and understanding spatial relationships between objects and entities within their semantic context. Florence-2, with its capability to manage diverse granular-ity levels, is an ideal vision encoder to address these core aspects of image comprehension. In the following section, we explore how to leverage Florence-2's strengths in inte-grating it into MLLMs."}, {"title": "3.2. Visual Features spanning Depth and Breadth", "content": "Breadth. Since different downstream tasks require vary-ing perceptual information from images, we consider ex-panding the breadth of visual representation. Given an in-put image $I \\in R^{H \\times W \\times 3}$ and a task-specific prompt, such as \"provide the text shown in the image\u201d, Florence-2 will pro-cess the image feature and prompt feature into $X = [V, T]$ and then feed into the encoder-decoder transformer archi-tecture. The encoder employs an attention mechanism to process X, producing an output $X' = [V', T']$. Due to the cross-attention between V and T, the updated image fea-"}, {"title": "3.3. Depth-Breadth Fusion", "content": "Since we have image feature with different level of granu-larity, feature fusion is commonly used. When dealing with multiple feature embeddings, such as [V, V1, V2, V3], the next question becomes how to fuse these features and align them with the language model space. To take advan-tage of all these four features, several approaches can be considered for this fusion process:\n* Token Integration: This approach involves concate-nating all features along the token dimension. How-ever, this can make the visual token excessively long and complicate model training.\n* Average Pooling: Alternatively, average pooling over all features can be used, but this method may result in information loss.\n* Channel Integration: A more effective method is to concatenate features along the channel dimension, which does not increase the sequence length.\nTo quickly assess which feature fusion method pro-vides the best overall performance, we use datasets from LLaVA-1.5 [26], which include 558K image captions for pre-training and 665K entries for instruction tuning. In the Table 1, the channel integration strategy shows better per-formance and training efficiency compared to the other two fusion methods. Thus we choose channel integration simple yet effective fusion strategy."}, {"title": "3.4. Florence-VL", "content": "As shown in Figure 2, Florence-VL is composed of the vi-sion foundation model Florence-2 and the large language model. After extracting multiple image features, we use MLP to project these features into the language model space. During the pretraining stage, we align Florence-2 with the language model using image detailed caption data. In the instruction tuning stage, we use diverse and high-quality instruction-tuning dataset to effectively adapt the model to downstream tasks."}, {"title": "4. Analysis on Different Vision Encoders", "content": "To demonstrate that Florence-2 is a superior vision encoder compared to others, we quantify the cross-modal alignment quality between various vision encoders and language mod-els, allowing us to assess the impact of different vision en-coders without requiring subsequent supervised fine-tuning and evaluations on benchmarks [15, 43]. Specifically, con-sider a pretrained MLLM $M = (V, L)$ where V is the vi-sion encoder and L represents the language model, we input a set of image-text pairs, $(V,T) = (\\{v_n\\}_{n=1}^N, \\{t_n\\}_{n=1}^N)$, into the model. For the nth image-text pair, the vision en-coder produces vision representations $f_v^n \\in R^{r_n \\times d'}$, and the text representations $f_t^n \\in R^{s_n \\times d}$ from last layer of the language decoder, where $r_n$ and $s_n$ are the number of to-kens in the vision and text representations, and d' and d are the hidden state dimensions for the vision and text tokens. We apply the trainable projection P to $f_v^n$ to ensure di-mensionality alignment with $f_t^n$, that is $P(f_v^n) \\in R^{r_n \\times d}$. We also apply average pooling along token dimension and normalize along the hidden dimension for both $P(f_v^n)$ and $f_t^n$. For all image-text pairs, we concatenate all vi-sion features along the first dimension to form a matrix $F_V^n \\in R^{N \\times d}$, and similarly concatenate all text features into a matrix $F_t^n \\in R^{N \\times d}$. Since we need to measure the modality gap between vision tokens and text tokens, we compute the divergence between these two token represen-tations. Specifically, we optimize the trainable projection P, which is used to bring these two representations closer together by minimizing a cross-entropy loss function:\n$L = -\\sum_{i,j} I_{n_{i,j}} log (softmax(F_V^n \\times (F_t^n)^T)_{i,j})$\nwhere $I_n$ is the target (indicator) matrix. The multiplica-tion of $F_V^n$ with the transpose of $F_t^n$ calculates the cor-relation between vision and text token representations. In short, the loss function is designed to minimize the distance between vision tokens and their corresponding text tokens by maximizing the likelihood that each vision token aligns correctly with its associated text token.\nWe use a set of image-text pairs $(V, T) = (\\{v_n\\}_{n=1}^N, \\{t_n\\}_{n=1}^N)$ from the LLaVA 1.5 pretraining image captioning datasets and select various vision en-coders to assess how well we can optimize the alignment between the vision encoder and the language model. The vision encoders we evaluate include: Stable Diffusion [36], Dinov2 [34] (ViT-G/14, ViT-L/14, ViT-B/14), SigLIP, Ope-nAI CLIP, and our Florence-2 model. The chosen language model is Llama 3 8B Instruct. We plot the alignment loss in Figure 4, which clearly shows that Florence-2 vision encoder achieves the lowest alignment loss compared to the other vision encoders, demonstrating the best alignment with text embeddings. Additionally, SigLIP demonstrates competitive results, as noted in [41], which highlights SigLIP's strong benchmark performance relative to other vision encoders, aligning with the findings of our study."}, {"title": "5. Experiments", "content": "Implementation Details. In order to build a state-of-the-art MLLM, we use images from CC12M [4], Redcaps [8], and Commonpool [12] during the pretraining stage, with"}, {"title": "6. Discussion", "content": "Results using LLaVA 1.5 Data. Since we curate our training data when building our MLLMs, we disentan-"}, {"title": "8. Conclusion", "content": "In conclusion, Florence-VL uses Florence-2 as a versatile vision encoder, which provides diverse, task-specific visual representations across multiple computer vision tasks like captioning, OCR, and Grounding. By leveraging Depth-Breadth Fusion (DBFusion), we incorporate a range of vi-sual features from different layers (\"Depth\") and prompts (\"Breadth\") to create enriched representations that meet varied perceptual demands of downstream tasks. Our fu-sion strategy, based on channel concatenation, effectively combines these diverse features, which are then projected as input to the language model.\nThrough training on a novel data recipe that includes detailed captions for pretraining and diverse instruction tuning data, Florence-VL demonstrates superior alignment between the vision encoder and the LLM, outperforming other models across 25 benchmarks covering vision-centric, knowledge-based, and OCR & Chart tasks. Our analysis underscores the effectiveness of Florence-2's generative ca-pabilities in enhancing MLLM alignment and versatility for a wide range of applications.\nFor future work, several avenues could further enhance the capabilities and efficiency of Florence-VL. One direc-tion involves improving the DBFusion strategy by explor-ing more sophisticated fusion techniques that could dy-"}]}