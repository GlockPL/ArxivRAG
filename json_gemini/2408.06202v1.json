{"title": "Strategy Game-Playing with Size-Constrained State Abstraction", "authors": ["Linjie Xu", "Diego Perez-Liebana", "Alexander Dockhorn"], "abstract": "Abstract-Playing strategy games is a challenging problem\nfor artificial intelligence (AI). One of the major challenges is\nthe large search space due to a diverse set of game compo-\nnents. In recent works, state abstraction has been applied to\nsearch-based game AI and has brought significant performance\nimprovements. State abstraction techniques rely on reducing\nthe search space, e.g., by aggregating similar states. However,\nthe application of these abstractions is hindered because the\nquality of an abstraction is difficult to evaluate. Previous works\nhence abandon the abstraction in the middle of the search\nto not bias the search to a local optimum. This mechanism\nintroduces a hyper-parameter to decide the time to abandon\nthe current state abstraction. In this work, we propose a size-\nconstrained state abstraction (SCSA), an approach that limits\nthe maximum number of nodes being grouped together. We\nfound that with SCSA, the abstraction is not required to be\nabandoned. Our empirical results on 3 strategy games show that\nthe SCSA agent outperforms the previous methods and yields\nrobust performance over different games. Codes are opensourced\nat https://github.com/GAIGResearch/Stratega.", "sections": [{"title": "I. INTRODUCTION", "content": "Strategy games have helped advance the development of\nArtificial Intelligence (AI) to achieve significant progress in\ncompeting with human players [1, 2], AI-AI cooperation [3, 4,\n5] and human-AI cooperation [3, 6, 7, 8]. Most of this progress\ndepends on deep reinforcement learning (DRL). However,\nDRL agents have their neural networks trained and tuned for\na specific game, making it difficult to apply these agents to\nother game variants. In contrast, search-based algorithms such\nas Monte Carlo Tree Search (MCTS) have shown outstanding\nperformance in general video game-playing [9, 10, 11]. The\nability to play different game variants is important because\nreal-world games are frequently updated by their developers.\nTherefore, in this work, we focus on search-based methods for\nstrategy game playing.\nOne of the most challenging problems for search-based\nalgorithms is the combinatorial search space. Unfortunately,\nstrategy games typically have a combinatorial search space.\nIn strategy games such as Starcraft, a number of units (e.g.\nbuildings, and armies) are distributed on the map. The state\nspace of these games is defined as the combination of unit\nproperties (e.g. positions, health points). This combinato-\nrial space increases exponentially with the number of game\ncomponents (including the unit number and unit property\netc.) [12, 13]. On top of that, most strategy video games have\na large set of unit variants and each unit has a diverse set\nof properties. Together, they produce large state and action\nspaces, resulting in a much larger branching factor compared\nto other games. With a large branching factor, MCTS finds it\ndifficult to explore the tree deeply for accurate action-value\napproximation and thus fails to perform well in these games.\nState abstraction [14, 15] is a powerful technique that helps\nMCTS solve large-scale planning problems. State abstraction\nmethods focus on simplifying the search space, which is often\nachieved by aggregating similar states. In strategy games, state\nabstraction [16, 17, 18, 19] has been applied to reduce the\nsearch space and gain significant performance improvements.\nHowever, one of the issues that hinder the application of\nstate abstraction is a lack of data for approximating the\nstate abstraction, resulting in a possible poor-quality state\nabstraction. To avoid this state abstraction to degrade the\nperformance, Xu et al. [19] proposed an early stop mechanism\nto abandon the constructed state abstraction at an early stage.\nHowever, this approach introduces a hyperparameter whose\nrange depends on the training budget, making it difficult to\nselect an appropriate value.\nIn this paper, we propose the size-constrained state abstrac-\ntion (SCSA), a novel approach to address the negative effect\nof a potential poor-quality state abstraction. SCSA limits the\nmaximal number of nodes in the same node group and does\nnot need the early stop. Meanwhile, its hyperparameter is less\nsensitive to the previous approach. Finally, we evaluate the\nSCSA agent in 3 strategy games using a common value of this\nsize limit. It outperforms all the baseline agents in 2 simple\ngames and achieves results competitive to Elastic MCTS [19]\nin another more complex game.\nThe main contributions of this work are listed below:\n1) We proposed a novel approach to address planning with\na poor-quality state abstraction in strategy game-playing.\n2) Our empirical results show that the proposed method\nachieves outstanding performance in 3 strategy games\nof different complexity.\n3) We analyzed the compression rate under the SCSA and\nElastic MCTS [19]. SCSA shows a lower compression"}, {"title": "II. RELATED WORK", "content": "State abstraction for MCTS recently gained much interest\nfrom the community. Jiang et al. [14] proposed to aggregate\nsame-layer tree nodes with Markov decision process homo-\nmorphism approximated from samples. This method shows\na promising performance in the board game Othello. Anand\net al. [20] proposed a state-action abstraction method that\naggregates state-action pairs instead of states (tree nodes).\nAnand et al. [21] propose progressive state abstraction that\nupdates the state abstraction more frequently instead of per\nbatch. Hostetler et al. [15] proposed a progressive refinement\nmethod to construct state abstraction. Baier et al. [22] proposed\nabstraction over opponent moves to aggregate tree nodes\nhaving the same opponent moving history. Sokota et al. [23]\nproposed abstraction refinement to reject similar states to be\nadded in the tree. These methods prove the effectiveness of\nstate abstraction in tackling large branching factors in MCTS.\nHowever, their application is mainly limited to planning prob-\nlems and board games. This work instead focuses on more\ncomplex strategy games.\nIn the early study, hand-crafted state abstraction were ap-\nplied to help strategy game play. Chung et al. [16] used a\nhandcrafted state abstraction to divide the game map into tiles.\nSynnaeve et al.[17] proposed a mechanism to separate the map\nin StarCraft to regions that are connected through checkpoints.\nUriarte et al. [24] also used the technique developed by Syn-\nnaeve et al. [17] but further removed combat-irrelevant units\nfrom the map. Although these hand-crafted state abstractions\ncan significantly reduce the size of state space for some games,\nthey rely on human heuristics and thus fails to generalize to\ndifferent games.\nExcept for hand-crafted state abstraction, automatic state\nabstraction are also explored in strategy game-playing in recent\nyears. A parameter optimization method [25] was leveraged\nto search unit features that can be removed from the unit\nvectors. By removing some features, states having all other\nfeatures the same are merged. Dockhorn et al. [18] proposed\nto represent game states with a combination of unit vectors.\nOur work is closely related to Xu et al. [19], where an\nelastic MCTS method is proposed for strategy game-playing.\nIn elastic MCTS, the state abstraction is first constructed in a\nbatch manner, similar to Jiang et al. [14]. Later, the constructed\nstate abstraction is abandoned and abstract nodes are split\ninto ground tree nodes. Our work does not need to abandon\nthe state abstraction, and thus is more consistent with state\nabstraction usage in planning [14, 15, 20, 21]."}, {"title": "III. THE STRATEGA PLATFORM", "content": "Stratega [26] is a general strategy game platform for testing\nAI agents. To evaluate the general performance of the proposed\nmethod, we select 3 two-player turn-based strategy games\nfrom the Stratega platform. They are Kill The King (KTK),\nPush Them All (PTA) and Two Kingdoms (TK). We next\nintroduce the details of these games.\nIn KTK (Figure 1), each player controls a set of units\nincluding a king. The goal of this game is to kill the opponent's\nking. We instantiate the army for each player as a king, a\nwarrior, an archer, and a healer. All units have the move\naction. Based on that, the king and the warrior can attack\nneighbour enemy units. The archer can attack enemy units in\nrange. The healer can heal ally units. Following Xu et al. [19],\neach unit also has an Do-nothing action. The action space size\nfor a 4-unit army is about 105.\nIn PTA (Figure 2), a player controls units to push enemy\nunits in different directions. The unit being pushed will move"}, {"title": "IV. BACKGROUND", "content": "MCTS [27] is a method to solve sequential decision-making\nproblems with a forward model. The forward model is used to\nroll out the game. I.e., given a state and a valid action under\nthis state, the forward model returns the next state. Using the\nforward model, MCTS builds up a tree to approximate the\nvalue for actions under the current state. In the generated tree,\neach node represents a game state and each branch represents\na valid action of its source node. We next introduce the 4\nstages for building up this tree: selection, expansion, rollout\nand back-propagation.\nThe selection stage selects a tree node as an input for the\nsubsequent stages. The selection starts from the root node and\nkeeps selecting a branch to the next layer until a target node\nis reached. The target node could be a leaf node (a node with\nno children) or a node where not all its actions have been\nadded to the tree as branches. To select the next-layer node,\nnode values (e.g. the UCB value [28]) for all its children are\ncalculated and the node with the highest UCB is selected.\nDepending on the node type of the target node, MCTS enters\ndifferent stages. If the target node is a terminal state, it enters\nthe back-propagation directly. In another case, the target node\nis a non-terminal state, an action that has not yet been added\nas a branch. By running this action in the forward model, the\nnext state is returned and is added as a new child. Based on this\nnew state, a roll-out policy takes a sequence of actions until\na pre-determined depth or a terminal state is reached. This is\nthe rollout stage. The output state from rollout is evaluated\nby a state evaluation function to obtain a score. This score is\nused by the back-propagation stage. In the back-propagation\nstage, the score from the target state is added to all states in\nthe trajectory of selection. i.e. a node sequence from the root\nnode to the target node.\nEach MCTS iteration consists of these 4 or 3 stages (the\nroll-out stage is skipped if the selected node is a terminal\nstate). The computation budget in this work is set as the\nmaximum number of forward model calls. After running out\nof the budget, a recommendation policy selects an action to\nexecute in the game. A common recommendation policy is\nselecting the branch leading to a node with the highest visit\ncount."}, {"title": "B. Monte Carlo Tree Search with Unit Ordering", "content": "In strategy games where many units are distributed on the\nmap, the action space is the combination of all unit actions,\nwhich can easily reach a high complexity. e.g. in KTK, the\ncombinatorial action space reaches a magnitude of 105. To\nreduce the action space, Xu et al. [19] propose the MCTS\nwith unit ordering (MCTSu). In MCTSu, the move ordering of\nunits is randomly initialized and is fixed throughout the whole\ngame. Each node controls only one unit and its children control\nthe subsequent unit in the move order. With this setting, the\ntree becomes deeper but narrower. MCTSu has shown a strong\nperformance in the multi-unit strategy games."}, {"title": "C. State Abstraction and Approximate MDP Homomorphism", "content": "A Markov Decision Process (MDP) is defined as\n(S, A, R, P, \u03b3), where the S is the state space, A the action\nspace, R:SXA \\rightarrow R the reward function, P:S\u00d7A \\rightarrow S\nthe transition function and \u03b3\u2208 R is a discount factor. A state\nabstraction for an MDP is < S\u00f8, A, R, P, \u03b3 >, where the S\nis the abstract state space. Each abstract state includes a set\nof states. The P : S\u00f8 \u00d7 A \u2192 S\u00f8 defines a transition function\nbased on abstract states.\nA key step to construct state abstraction is defining a state\nmapping function \u03c6: S \u2192 S\u00f8 that maps a ground state to an\nabstract state. The function \u03c6 can be implemented by defining\nsimilarity between states and aggregating similar states to the\nsame abstract state. Approximate MDP homomorphism [29]\nis a typical state similarity measurement. For two states s1 and\ns2, it is defined by the approximate error of reward function\nER and the approximate error of transition function \u03b5T:\n$E_{R}(s_{1},s_{2}) = \\underset{a \\in A}{max} |R(s_{1}, a) \u2013 R(s_{2}, a)|$  (1)\n$\\epsilon_{T} (s_{1}, s_{2}) = \\underset{a \\in A}{max} \\sum_{s' \\in S}\\sum_{s' \\in S'}|T(s'|s_{1}, a) \u2013 \\sum_{s' \\in S'}T(s'|s_{2}, a)|$(2)\nwhere T(s'|s, a) is the transition probability, ER measures the\nmaximal difference between reward functions of the given\nstates and \u03b5T measures the worst-case total variation distance\nbetween state transition distributions."}, {"title": "D. Elastic Monte Carlo Tree Search", "content": "The elastic MCTS method [19] is built upon MCTSu. It\naggregates tree nodes with approximate MDP homomorphism.\nThe constructed node groups are split into ground tree nodes\nwith early stop. Algorithm 1 and Algorithm 2 (without the\nblue parts) provide pseudocode for elastic MCTS.\nFor every B MCTS iteration (line 6 in Algorithm 1), elastic\nMCTS checks all the tree nodes that have not yet been added"}, {"title": "V. METHOD", "content": "Based on MCTS, our method automatically groups tree\nnodes by the approximate MDP homomorphism. Following\nJiang et al. [14] and Xu et al. [19], SCSA groups tree nodes\nfrom the same layer at every batch (a fixed number of MCTS\niterations). At each iteration, the MCTS samples one trajectory\nthat consists of a sequence of nodes, starting from the root\nnode to a leaf node. To approximate the MDP homomorphism,\na batch of samples is required for calculating the approximate\nerrors (Equation 1). Therefore, for every B iteration(s), SCSA\nchecks every tree node that has not yet joined a node group to\nexpand the current abstraction. There are two approaches for a\nnode to be added to the existing abstraction, depending on the\napproximate MDP homomorphism errors. If the approximate\nerrors between this candidate node and a node group are\nbelow the thresholds, this candidate node is added to the\ncorresponding node group, becoming a member node of this\ngroup. When the approximate errors between the candidate\nnode and all same-layer groups are found higher than the\nthresholds, a new node group is created and this node becomes\nthe only member node.\nIt is found that a large number of samples are required\nto obtain high-quality state abstractions [14]. In strategy\ngames where the search space are large, it is infeasible to\nobtain enough samples. Under limited samples, the constructed\nstate abstraction might be of unstable quality. Moreover, it\nis difficult to evaluate the quality of the constructed state\nabstraction. Xu et al. [19] discovered that abandoning the\nexisting state abstraction in the middle of MCTS running\ncan bring significant performance improvement. In contrast\nto their approach [19], SCSA does not abandon the state\nabstraction. Instead, a global size constraint is defined to limit\nthe maximum number of member nodes for every node group.\nBelow, we introduce the abstraction construction in detail.\nThe pseudocodes of the SCSA algorithm are shown in\nAlgorithm 1 and Algorithm 2, with highlighted lines in red\nrepresenting the removed part from Elastic MCTS, and the\nlines highlighted in blue are newly introduced by the SCSA\nmethod. The computation budget constant is NFM, meaning\nthe maximum number of available forward model calls. In\nAlgorithm 1, lines 6-7 presents the early stop with a threshold\n\u03b8ES [19]. Our method removes this part.\nWe first introduce the hyperparameters, NFM the compu-\ntation budget, B the batch size, \u03b7R reward function error, \u03b7T\ntransition error and SIZE_LIMIT the maximum node group"}, {"title": "VI. EXPERIMENTS", "content": "Baselines: We implement 5 baseline agents to evaluate the\nperformance of the SCSA agent. They are Rule-based, MCTS,\nMCTSu, RG MCTSu and Elastic MCTSu. Details about each\nagent are listed below:\n1) Rule-based: Stratega platform has implemented a Rule-\nbased agent for each game. We here briefly introduce\ntheir implementation. The Rule-based agent for KTK\nprioritizes attacking isolated enemy units and healing\nstrong ally units. For each enemy unit, an isolation score\nis calculated considering its nearby ally units and enemy\nunits. At each round, the Rule-based agent controls its\nunits to i) approach the enemy units with the highest\nisolation score and attack them; and ii) approach an\nally unit to heal it. The PTA Rule-based agent controls\npushers to approach the nearest enemy unit and push\nit towards the nearest hole. In TK, the Rule-based agent\nfirst researches Mining, which is necessary for spawning\nworkers. Once the research is finished, a worker is\nspawned and is assigned the task of collecting gold from\nthe nearest gold vein. These gold are used to spawn\nwarriors. Once the number of warriors reaches 2, these\nwarriors are sent to attack the enemy king. Whenever its\nwarriors died, new warriors would be spawned if they\nhad enough gold resources.\n2) MCTS: An MCTS agent without using state abstraction.\n3) MCTS: An MCTS agent with unit ordering.\n4) RG MCTSu: An MCTSu agent with randomized state\nabstraction. Each new tree node either joins an existing\nnode group (with the probability 1/(N+1) for each group,\nsupposing there are already N node groups) or creates\na new node group with itself as the only member node\n(with the probability N/(N+1) ).\n5) Elastic MCTSu: MCTSu with state abstraction based on\napproximate MDP homomorphism and early stop [19].\n6) SCSA: An MCTSu agent with approximate MDP ho-\nmomorphism abstraction. Each abstract node has a size\nlimit defined by SIZE_LIMIT.\nHeuristic functions: The same as typical MCTS in games,\nwe utilize heuristic functions to evaluate states reached by\nthe MCTS roll-out. The heuristic functions are game-specific.\nIn each game, all agents except for the Rule-based agent\nshare the same heuristic function. Below, we introduce the\nimplementation details of the heuristic functions for each\ngame.\nIn all games, the scores of states where the player wins,\nloses and draws the game are 1, -1 and 0, respectively. For all\nthe other states, the heuristic function returns a score between\n0 and 1. The KTK heuristic function returns a score of R =\n(d.h)/(Dktk:H),\nwhere the d is the sum of the distance from each\nally unit to the enemy king, Dktk is the maximum value of d, h\nis the health points of the enemy king and H is the maximum\nvalue of h. The strategy is controlling the units to approach the\nenemy king and try to search for a state that leads to victory.\nFor PTA, the score of a state is a sum of three parts. The first\none is 0.2 \u00d7 (\\sum_{u} min_{u'} dis(u,u'))/Dpta,\nwhere the u is an ally unit, u' is\nan enemy unit, dis(\u00b7,\u00b7) returns the Euclidean distance between\nthe two units. The second part is 0.4 \u00d7 Ut/Uo, where Ut is the\nnumber of alive ally units at time step t and Uo is the number\nof the units in the beginning. The last part is 0.4 \u00d7 (|U1|/|U|),\nwhere |U1|, |U| are the number of enemy units at time step\nt, respectively.\nIn TK, the state score is calculated according to finishing a\nseries of tasks. Finishing Mining research returns 0.2, having\nworker alive returns 0.1 and having units that have action\nattack returns 0.1. Other scores include 0.1\u00d7 the distance of\nally workers to its nearest gold vein, 0.2\u00d7 collected gold,\n0.3\u00d7 the distance between all ally units and enemy units. The\nnormalized score lands in [0, 1]."}, {"title": "A. Agent Parameter Optimisation with NTBEA", "content": "As each agent has different optimal parameters for each\ngame, we apply the N-Tuple Bandit Evolutionary Algorithm\n(NTBEA) [25] to automatically optimize agent parameters in\ndifferent games. The NTBEA uses an N-Tuple system to break\ndown the combinatorial space of the parameters. NTBEA\nhas its own parameters, an exploration factor, the number\nof neighbours and the number of iterations. Following Xu et\nal. [19], these values are set to 2, 50 and 50, respectively. Next,\nwe introduce the parameter space for each game-playing agent.\nThe parameters for MCTS and MCTSu are the explo-\nration factor C\u2208 {0.1,1,10,100} and rollout length K \u2208\n{10, 20, 40}. RG MCTSu has C, K and an early stop thresh-\nold \u03b8ES \u2208 {4 \u00d7 B,8 \u00d7 B,10 \u00d7 B,12 \u00d7 B}, where the\nB = 20 is a constant batch size. Elastic MCTSu has\nC, K,\u03b8ES and approximate errors for reward function and\ntransition function \u03b7R \u2208 {0.0, 0.04, 0.1, 0.3, 0.5, 1.0}, \u03b7T \u2208"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "Automatic state abstraction has recently been applied to\nMCTS to address large search spaces in strategy game-\nplaying. However, the lack of data results in state abstraction\nof unstable quality. We propose the novel SCSA to control\nthe abstraction quality. Compared to the previous early stop\napproach, our method has a much smaller range for its\nhyperparameter. The empirical results on 3 strategy games\nof different complexity present the effectiveness of SCSA on\nstrategy game playing.\nThe SCSA outperforms baselines in two games but not in\nthe complex TK game, indicating a potential shortcoming of\nscalability. A possible solution is to combine state abstraction\nwith pruning. We plan to further investigate the scalability of\nthe SCSA agent in our future work.\nLimitation We analyze the tree size under different state\nabstraction size constraints, revealing a trade-off between\nmemory usage and agent performance."}]}