{"title": "SILO: Solving Inverse Problems with Latent Operators", "authors": ["Ron Raphaeli", "Sean Man", "Michael Elad"], "abstract": "Consistent improvement of image priors over the years has led to the development of better inverse problem solvers. Diffusion models are the newcomers to this arena, posing the strongest known prior to date. Recently, such models operating in a latent space have become increasingly pre-dominant due to their efficiency. In recent works, these models have been applied to solve inverse problems. Working in the latent space typically requires multiple applications of an Autoencoder during the restoration process, which leads to both computational and restoration quality challenges. In this work, we propose a new approach for handling inverse problems with latent diffusion models, where a learned degradation function operates within the latent space, emulating a known image space degradation. Usage of the learned operator reduces the dependency on the Autoencoder to only the initial and final steps of the restoration process, facilitating faster sampling and superior restoration quality. We demonstrate the effectiveness of our method on a variety of image restoration tasks and datasets, achieving significant improvements over prior art.", "sections": [{"title": "1. Introduction", "content": "Methods for solving inverse problems aim to recover an unknown signal from its degraded measurements. Assuming that an image x has been drawn from the probability density function p(x), in this work we consider a degradation operator \\(A(\u00b7)\\) which forms the given observation \\(y \\sim \\mathcal{N}(A(x), \\sigma I)\\). Over the past several decades, countless techniques have been developed to tackle such inverse problems, adopting various directions. Owing to these methods, it is possible to sharpen and denoise images [6, 10, 13, 14, 17, 18], accelerate CT scans [15, 53], and get information on the atmosphere's composition [40].\nMost inverse problems are ill-posed, not having a unique solution, and the majority of the solutions do not correspond with natural images. An appealing approach to address these challenges is to harness methods that can sample from the prior p(x), and adapt them to sample from the posterior distribution \\(x \\sim p(x|y)\\) [7, 26, 36, 38, 58]. If sampled correctly, we recover a signal that is simultaneously in the distribution of real-world signals while being consistent with the measurement y.\nDiffusion models [2, 11, 21, 48, 51, 52] learn the score function of the prior, allowing to sample from it faithfully."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Inverse problems", "content": "Inverse problems generally refer to the task of recovering a clean and probable signal that correlates to observed degraded measurement. Specifically, our problem can be described as\n\\(y = A(x) + v\\) (1)\nwhere \\(x \\in \\mathbb{R}^d\\) is the unknown clean image, \\(A(\u00b7) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n\\) is a degradation operator (e.g. Gaussian blur) \\(v\\) is noise sampled from a normal distribution, \\(\\mathcal{N}(0, \\sigma I)\\), with variance \\(\\sigma\\) and \\(y \\in \\mathbb{R}^n\\) is the observed measurement.\nDue to the problem's ill-posed nature, some assumptions must be made. In our case of recovering degraded images, we assume prior knowledge on the behavior of real-world (natural) images. Over several decades, many methods"}, {"title": "2.2. Diffusion models", "content": "Diffusion models sample from p(x) by applying a series of activations of pretrained minimum mean squared error (MMSE) denoisers [14]. Specifically, following DDPM's notations [21], a forward process is first defined as the creation of a sequence of noisy images \\(x_t\\) for timesteps \\({t = 0, 1, ..., T}\\) via\n\\(x_t \\sim \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t)I),\\) (2)\nwhere \\(x_0\\) is a clean image, \\(\\bar{\\alpha}_t = \\Pi_{i=1}^t \\alpha_i, \\) \\(\\alpha_t := 1 - \\beta_t\\) and \\({\\beta_t\\}_{t=1}^T\\) is a chosen variance schedule. Generating images with diffusion models is achieved by reversing the above forward process. This is done via the recursive relation [52]\n\\(x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t + \\beta_t \\nabla_{x_t} \\ln p(x_t)) + \\sqrt{\\beta_t} u,\\) (3)\nwhere \\(u \\sim \\mathcal{N}(0, I)\\) is a noise perturbation. The term \\(S_\\theta(x_t) := \\nabla_{x_t} \\ln p(x_t)\\) is known as the score function [50, 52, 55], obtained via Tweedie's formula [12]:\n\\(\\nabla_{x_t} \\ln p(x_t) = \\frac{1}{1-\\alpha_t} (x_t - (x_t + \\sqrt{\\alpha_t} \\mathbb{E}_{x_{t} \\sim p(x_t)}[x_0|x_t])).\\) (4)\nThe expression \\(\\mathbb{E}_{x_{t} \\sim p(x_t)}[x_0|x_t]\\) is nothing but the MMSE estimate of \\(x_0\\) given \\(x_t\\), i.e. an image denoiser that aims to remove white additive Gaussian noise while striving to get the smallest \\(L^2\\) error. This denoiser is formed as a learned neural network, being the only portion of the process that relies on training. The common practice is to train a network \\(\\epsilon_\\theta(x_t, t)\\), which predicts the noise added to create \\(x_t\\), conditioned on the timestep t. Thus, given \\(\\epsilon_\\theta(x_t,t)\\), the synthesis process amounts to the update step [21]\n\\(x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t)) + \\sqrt{\\beta_t} u,\\) (5)\ninitialized from \\(x_T \\sim \\mathcal{N}(0, I)\\) and iteratively progresses towards \\(x_0\\), a sample from p(x)."}, {"title": "2.3. Latent diffusion", "content": "Sampling \\(x \\sim p(x)\\) with a diffusion model is tedious, as it requires multitude (50-1000, depending on the approximation scheme) of activations of the denoiser network, following the iterative manner of equation Eq. (5). To mitigate this problem, it was suggested to perform the diffusion process in a lower dimensional latent space [42]. In this approach, an Autoencoder is used to encode images to the latent space and decode latent vectors (latents for short) back to the image space as follows:\n\\(z = \\mathcal{E}(x), x \\approx \\mathcal{D}(z),\\) (6)\nwhere x is an image, z is a latent, \\(\\mathcal{E}(\u00b7) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k\\) is the encoder and \\(\\mathcal{D}(\u00b7) : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d\\) is the decoder. In our case, \\(k < d\\) implies that the diffusion process occurs in the lower dimension latent space, and each network pass is thus much faster, leading to more efficient training and sampling. Since the diffusion process is performed on latents z, instead of images x, the creation of noisy latents is adapted from Eq. (2), transforming to\n\\(z_t \\sim \\mathcal{N}(z_t; \\sqrt{\\bar{\\alpha}_t} z_0, (1 - \\bar{\\alpha}_t)I),\\) (7)\nwhere \\(z_0\\) is a clean latent. In the latent domain, the reverse process takes the form of Eq. (5), and translates to\n\\(z_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (z_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(z_t, t)) + \\sqrt{\\beta_t} u_z.\\) (8)"}, {"title": "3. LDM for inverse problems: related work", "content": "To sample from the posterior, \\(x \\sim p(x|y)\\), one need to use the conditional score \\(\\nabla_{x_t} \\ln p(x_t|y)\\). When migrating to the latent space, this score becomes \\(\\nabla_{z_t} \\ln p(z_t|y)\\). Following Bayes rule we get\n\\(\\nabla_{z_t} \\ln p(z_t|y) = \\nabla_{z_t} \\ln p(z_t) + \\nabla_{z_t} \\ln p(y|z_t).\\) (9)\nThe prior score function (first term in RHS) is obtained from the latent diffusion model. On the other hand, the score likelihood term (second term in RHS) involve calculations in the image space, since the degradation operator A is applied to images. For example, DPS [7], which uses image space diffusion, relies on the approximation\n\\(\\nabla_{x_t} \\ln p(y|x_t) \\approx \\frac{1}{\\sigma^2} \\frac{x_t}{\\sqrt{1-\\bar{\\alpha}_t}} ||y - A(\\bar{x}_t)||^2,\\) (10)\nwhere \\(\\bar{x}_t = \\mathbb{E}_{x_t \\sim p(x_t)}[x_0|x_t]\\). This expression implies that calculating the update term within each step of the diffusion process requires a differentiation of both the degradation operator and the denoiser. If this is to be adjusted to operate in the latent space, the naive approach would be to decode the latent at each step and calculate the score likelihood approximation as follows:\n\\(\\nabla_{z_t} \\ln p(y|z_t) \\approx \\frac{1}{\\sigma^2} \\nabla_{z_t} ||y - A(\\mathcal{D}(\\bar{z}_t))||^2,\\) (11)\nwhere,\n\\(\\bar{z}_t = \\mathbb{E}_{z_t \\sim p(z_t)}[z_0|z_t] = \\frac{(z_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_\\theta(z_t, t))}{\\sqrt{\\alpha_t}}.\\) (12)"}, {"title": "4. Proposed method", "content": "Since the diffusion process and the measurement operator, A, operate in different domains, there is no way to avoid using the Autoencoder completely. There are two plausible ways to use it: (1) We could use the decoder during the diffusion process to decode latents and use the known operator A in the pixel space. Alternatively, (2) we could bring the measurement and degradation operator to the latent space.\nThe first paradigm has been employed in all previous works in the field. In the Appendix, we demonstrate two flaws in using the decoder and differentiating through it. These flaws manifest as artifacts, as seen in Sec. 5 and as reported in ReSample (Appendix D in [49]).\nIn the second suggested paradigm, which is the one we propose in this work, the measurement, y, is encoded to the latent space, naively by \\(w = \\mathcal{E}(y)\\). Additionally, a learned operator, \\(H_\\theta\\), needs to mimic the degradation operator A while operating entirely in the latent domain. This suggestion gives rise to four critical questions:\nQ1: Clearly, y is not a natural high-quality image. Why would it be allowed to apply the encoder \\(\\mathcal{E}\\) onto it?\nQ2: What are our requirements from the operator \\(H_\\theta\\), and how do we promote consistency to the measurement?\nQ3: How should the operator \\(H_\\theta\\) be learned?\nQ4: How can this operator aid in restoration?\nIn the following subsections, we will explore and answer these questions in detail."}, {"title": "4.1. Encoding the measurement", "content": "Referring to Q1, we acknowledge that computing w for a degraded image requires using the encoder on an image that is out-of-distribution relative to its training data\u00b9. To gain confidence that this is a valid step to take, we first investigate how much the encoding-decoding process changes an image. Specifically, we compare the PSNR values of images and their encoded-decoded versions by applying \\(f(\u00b7) = \\mathcal{D}(\\mathcal{E}(\u00b7))\\), average over 1000 images using various degradations and include the results in Tab. 1. The full experiment settings are described in Sec. 5. From the results, we see that for noiseless degraded images, \\(y_{nl} = A(x)\\), the PSNR with their decoded-encoded counterpart, \\(f(y_{nl})\\), is higher than for natural images. In addition, a denoising effect occurs when applying f on noisy measurements, y, as for the majority of the tested degradations, we observe PSNR\\((y_{nl}, f(y)) >\\) PSNR\\((y, f(y))\\). This reassures us that applying the encoder on degraded images should not be a major bottleneck in our method."}, {"title": "4.2. Score likelihood in latent space", "content": "Moving to Q2, it is necessary to understand how the approximation of the score likelihood in Eq. (10) can be transformed to allow sampling in the latent space alone. Consider a measurement y* created from an unknown signal x* using Eq. (1). Assuming that the Autoencoder enables a near-perfect reconstruction on degraded images, \\(\\mathcal{D}(\\mathcal{E}(y)) \\simeq y\\), we can write\n\\(||y^* - A(x)||^2 = ||\\mathcal{D}(\\mathcal{E}(y^*)) - \\mathcal{D}(\\mathcal{E}(A(x)))||^2.\\) (16)\nTo provide an answer to Q2, suppose we have access to a trained operator \\(H_\\theta(z) = \\mathcal{E}(A(x))\\), where \\(z = \\mathcal{E}(x)\\). This operator aims to mimic the degradation A(x) while operating fully in the latent domain. Then, Eq. (16) can be rewritten as:\n\\(||y^* - A(x)||^2 \\simeq ||\\mathcal{D}(\\mathcal{E}(y^*)) - \\mathcal{D}(\\mathcal{E}(A(x)))||^2 \\simeq ||\\mathcal{D}(\\mathcal{E}(y^*)) - \\mathcal{D}(H_\\theta(z))||^2.\\) (17)\nSince \\(\\mathcal{D}\\) is differentiable, it is also Lipschitz continuous, therefore, a constant C exists\u00b2 such that\n\\(||\\mathcal{D}(\\mathcal{E}(y^*)) - \\mathcal{D}(H_\\theta(z))||^2 \\leq C||\\mathcal{E}(y^*) - H_\\theta(z)||^2.\\) (18)\nUltimately, since the RHS in Eq. (18) bounds the LHF in Eq. (16), we can minimize it as a proxy to Eq. (10), yielding,\n\\(\\nabla_{z_t} \\ln p(y|z_t) \\approx \\frac{Const}{\\sigma^2} -\\nabla_{z_t}||w - H_\\theta(\\bar{z}_t)||^2,\\) (19)\nwhere \\(w = \\mathcal{E}(y^*)\\). This motivates us to design such \\(H_\\theta\\), that will allow for reconstructions using Eq. (19)."}, {"title": "4.3. The latent degradation operator", "content": "According to the assumptions that led to Eq. (19), \\(H_\\theta\\) should be trained in a way that approximates \\(H_\\theta(z) \\approx \\mathcal{E}(y)\\). Thus, answering Q3, we train the operator \\(H_\\theta\\) with the following loss:\n\\(\\mathcal{L} = \\mathbb{E}_{x \\sim p(x)} \\mathbb{E}_{y|x \\sim \\mathcal{N}(A(x),\\sigma^2 I)} \\mathbb{E}_{t \\sim \\mathcal{U}[0,T]} [||H_\\theta(\\hat{z}, t) - \\mathcal{E}(y)||_1].\\) (20)\nThe expectation sweeps through ideal images \\(x \\sim p(x)\\), and creates y from them according to Eq. (1). \\(\\hat{z}\\) is created from the same images x after encoding, adding noise and denoising (Eqs. (6), (7) and (12)). The computation graph for the training of \\(H_\\theta\\) is presented in Figure 3. We refer to \\(H_\\theta\\) as a learned degradation operator since for \\(t = 0\\), \\(H_\\theta\\) gets a clean latent, \\(\\mathcal{E}(x)\\), and outputs the encoding of the degraded image, \\(\\mathcal{E}(y)\\). Additional information on the design and nature of \\(H_\\theta\\) is provided in Sec. 5.2 and Appendix."}, {"title": "4.4. Reconstruction", "content": "We are left with Q4, questioning the way to deploy the trained operator \\(H_\\theta\\) in solving inverse problems. Algorithm 1 describes the proposed recovery scheme, termed SILO (Solving Inverse Problems with Latent Operators). Following the ablation study in DPS (Appendix C in [7]), we use a term similar to Eq. (19) in step 9 of the algorithm, taking a gradient of the square root of the RHS in Eq. (18). Note that in Algorithm 1, the decoder and encoder are each used once. Furthermore, gradients are never calculated through the decoder or encoder during sampling; they are only calculated through the denoiser and the learned operator, \\(H_\\theta\\) in the latent space."}, {"title": "5. Experiments", "content": "In this section, we present experiments comparing our method, SILO, with other latent diffusion-based methods that are reproducible using public implementations. The methods in our comparison include LDPS [49], GML-DPS [43], PSLD [43] and ReSample [49]. We begin by describing the metrics used in our evaluation, provide details on the tested degradations, and then present the experimental results. Implementation details are described in the Appendix."}, {"title": "5.1. Metrics", "content": "Relying solely on distortion or perceptual metrics can be misleading. Low distortion does not imply a realistic image, and high perceptual quality does not imply low discrepancy with the ground-truth image [4]. Thus, we include a comprehensive set of metrics in our experiments to provide a thorough comparison of SILO with other methods. These include the following:\nDistortion. We report PSNR and LPIPS\u00b3 [62] values between the restorations and groudn-truth images for each degradation separately, averaged over the test set.\nPerception. The perception metrics we provide are Fr\u00e9chet inception distance (FID) [19], and kernel inception distance (KID) [3] multiplied by a factor of 10\u00b3. These reported values assess the distance between \\(p_x\\) (the real image distribution from the test set) and \\(p_{\\hat{x}}\\).\nRuntime. We measure the algorithms' duration from start to end of the restoration process [seconds]. Computations are performed on an NVIDIA L40S GPU with full precision (FP32) and averaged over 100 images. Runtime measurements refer to the Super-resolution \u00d78 task."}, {"title": "5.2. Degradations, datasets and models", "content": "We evaluate SILO and the competing methods across a variety of common degradations:\nGaussian blur. The images are padded with reflection and convolved with a Gaussian kernel of size 61 \u00d7 61 with a standard deviation of 3.\nSuper-resolution \u00d74 or \u00d78. The image are downscaled by a factor of 4 or 8 with a bicubic kernel.\nInpainting. A box mask of size 256 by 256 pixels is applied to the center of the image.\nJPEG. JPEG decompression with quality factor 10 is applied to the image.\nLinear degradations are applied for all methods, with an additional non-linear degradation for methods that support it. Images are normalized to the range [-1,1], processed through A, and, unless otherwise specified, white Gaussian noise with \\(\\sigma_\\eta = 0.01\\) is added in accordance with Eq. (1). This noise level is chosen for consistency with previous works using latent diffusion [9, 44, 49].\nFor face restoration tasks, experiments are performed over the FFHQ dataset [25], scaled to 512 \u00d7 512. The test set consists of the first 1,000 images in the dataset, and the training set consists of the remaining images. For general restorations, the training set is LSDIR-train [31], and the test set is the first 1, 000 images of COCO-val2017 [33].\nWe use SD-v1.5 [42] and RV-v5.1 [46] as our pretrained diffusion models. These models share the same architecture, but RV-v5.1 produces more realistic generations. For the Autoencoder [29], we use the default model for SD-v1.5, which is also compatible with RV-v5.1.\nFor simplicity, we choose \\(H_\\theta\\) as Readout-Guidence (RG), the network suggested in [35], with a minor modification to condition on the noise level, \\(\\sigma_y\\), in the measurement. As described in the paper, this operator extracts features from the denoising network and learns how to combine and process them to produce the desired output. Notably, \\(H_\\theta\\) is also t-dependent by design, unlike A, which does not depend on the diffusion timestep. We present a preliminary ablation on this matter in Sec. 5.3 and leave further investigation for future research."}, {"title": "5.3. Results", "content": "Quantitative results in Tabs. 2 and 3 show that our method consistently outperforms all other methods by FID, KID, and LPIPS while achieving a shorter reconstruction time by a factor of ~ 3 compared to PSLD and ~ 10 compared"}, {"title": "6. Conclusion", "content": "This work introduced a novel approach to solving inverse problems using LDMs. Rather than enforcing consistency to the measurement in the pixel domain, our method operates entirely within the latent space, improving both reconstruction quality and sampling runtime.\nLimitations. Since we use the Autoencoder of SD to create w, we expect successful reconstructions when the measurement somewhat resembles a natural image. This behavior holds for most common degradations. For cases that diverge from this assumption significantly (e.g., phase retrieval), alternative methods of generating w from y may be needed. Additionally, SILO requires a preliminary stage of training \\(H_\\theta\\) to mimic A. This is done once, and then \\(H_\\theta\\) can be used for limitless restorations afterward.\nFuture work. Our approach opens up several directions for future research. Using alternative encoders or feature extractors to compute w could enable SILO to handle a broader range of degradations and further enhance reconstruction quality. Another potential extension is to design \\(H_\\theta\\) to mimic a parametric family of degradations, conditioned on the parameters of A, resulting in more versatile latent operators. Since our method bridges the gap in cases where the score likelihood and data score are computed in different domains, future work could build upon this concept, potentially extending SILO's contributions even further."}]}