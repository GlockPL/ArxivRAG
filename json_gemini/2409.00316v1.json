{"title": "TOWARD A MORE COMPLETE OMR SOLUTION", "authors": ["Guang Yang", "Muru Zhang", "Lin Qiu", "Yanming Wan", "Noah A. Smith"], "abstract": "Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the system first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we focus on the MUSCIMA++ v2.0 dataset, which represents musical notation as a graph with pairwise relationships among detected music objects, and we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stages in a more holistic way. These findings, together with our novel evaluation metric, are important steps toward a more complete OMR solution.", "sections": [{"title": "1. INTRODUCTION", "content": "Optical music recognition (OMR) focuses on converting music notation into digital formats amenable to playback and editing. OMR systems are generally divided into two categories: end-to-end systems (which directly convert the image into music notation) and multi-stage systems. Proposed and refined by [1\u20133], a standard multi-stage system consists of four stages: preprocessing, music object detection, notation assembly, and encoding. In this study, we focus on the object detection and notation assembly stages.\nMUSCIMA++ [4] suggests representing music notation as a graph where each pair of musical symbols is linked by a binary relationship, allowing for clear notation reconstruction. The authors created a dataset of handwritten scores with a bounding box for each music object and a human-annotated graph of object relationships in each image. Notation assembly on MUSCIMA++ can be framed as a set of binary classification decisions to predict the pairwise relationships between music symbols. Most prior research has explored notation assembly with the assumption of perfect detection output [5], but such assumptions can introduce unwanted biases that deteriorate the performance of the notation assembly system when applied as part of a pipeline. Pacha et al. [6] evaluate a notation assembler on realistic detector output, finding some degradation relative to gold-standard objects, but they do not seek to mitigate the problem.\nTo improve notation assembly robustness, we propose a training method to complete notation assembly on top of (imperfect) object detection output directly. To have a strong detector to start with, we train YOLOv8 [7] and perform a set of preprocessing steps to adapt the model to the MUSCIMA++ v2.0 dataset. Our detector outperforms previous detectors on MUSCIMA++ v2.0 [8] by 2.4%, establishing a solid foundation for notation assembly.\nTraditional evaluation methods, which perform notation assembly over all pairs of ground-truth objects and report an F1 score or a precision-recall curve, become inadequate when the input objects come from imperfect detection. We propose an end-to-end evaluation metric, called Match+AUC, that accounts for both detection errors and assembly errors by first matching detected objects with their ground-truth counterparts before assessing notation assembly accuracy. It complements metrics that evaluate pipeline components individually.\nOur code for reproducing all of the experiments is publicly available at https://github.com/guang-yng/completeOMR."}, {"title": "2. MULTI-STAGE OMR", "content": "We focus on the MUSCIMA++ v2.0 dataset [4] and follow its multi-stage pipeline for the OMR system. This dataset includes 140 high-resolution annotated images out of 1,000 images from the CVC-MUSCIMA dataset [9]. It contains 91,254 symbol-level annotations and 82,247 relationship annotations between symbol pairs by human annotators. These annotations span 163 distinct classes of music symbols. Figure 2 shows an example from this dataset.\nAs the MUSCIMA++ dataset provides symbol-level pairwise relationships, it allows study of two stages of the pipeline: (i) detection and (ii) assembly. In (i), given an image as input, an object detector is used to extract all music symbols in the image, denoted as the set V = {Vi}i, where vi = (bi, ci) is a tuple of a bounding box and a class label. Each pair of music symbols (vi, vj) is then fed into (ii) the notation assembly model to predict whether or not there exists a relationship between them. The notation assembly stage can be framed as an edge prediction problem where the model needs to output a set of edges E to get a directed graph G = (V, E). MUSCIMA++ defines a grammar over all possible music symbol classes so that the direction of an edge is uniquely determined by the class labels (ci, cj) of the vertices (vi, vj). Consequently, the edge prediction problem can be reduced to predicting an undirected graph. The authors of [4] argue that such a graph G enables straightforward reconstruction of the full symbolic music notation, so we do not consider the decoding process after (i) and (ii) in this work.\nIn previous works, the two stages are considered separately, either focusing on object detection, without fully analyzing its effect on downstream notation assembly [8, 10, 11]; or focusing on notation assembly and assuming perfect detection input during training [5, 6]. This raises the question of whether the best object detector is a good fit for the best notation assembly model. To investigate, we developed an end-to-end metric that evaluates the performance of the entire pipeline, as explained in Section 3.3. We found that, compared with our approach where both stages are considered together-specifically, where the notation assembly model is trained using the output of the object detector-treating the two stages separately leads to poorer results."}, {"title": "3. METHODOLOGY", "content": "We describe our method for each stage, and how we connect the two stages together and evaluate the entire pipeline. Figure 1 shows an overview of our methods."}, {"title": "3.1 Music Symbol Detection", "content": "A music object detection system analyzes an image to identify each music object it contains, providing both the bounding box and class label for every detected object [10]. Traditionally, this process would begin with an initial stage of image preprocessing, typically aimed at removing staff lines, followed by a second stage focusing on the segmentation and classification of symbols. Thanks to recent advances in computer vision, there are mature solutions for image preprocessing and staff line removal, allowing us to treat it as a largely solved problem [12-14]. In our case, MUSCIMA++ provides us with staff line removed images as input, so we directly build our detectors on top of these images.\nFollowing the work of Zhang et al. [8], we adopt a convolutional neural network-based approach for page-level object detection of handwritten music notes, opting for this approach over segmentation-based methods, because segmentation-based methods often struggle with overlapping symbols. We choose YOLOv8 [7], which is the latest version of YOLO [15], due to its superior performance on traditional computer vision tasks. Compared to YOLOv4 [16], which is used by Zhang et al. [8], YOLOv8 has a new loss function and a new anchor-free detection head, achieving higher performance on various detection tasks. YOLOv8 has not yet, to our knowledge, been applied to OMR. Furthermore, since the images of handwritten music notation in MUSCIMA++ have high resolution and music objects are drastically different from the objects considered in computer vision research, directly applying the training strategy of YOLOv8 doesn't work well. We follow previous works [2, 8, 17] to crop images into small snippets during the training stage to alleviate this issue. Specifically, we randomly crop the images during training and compactly segment the image during inference. More details are presented in Section 4.1.2."}, {"title": "3.2 Notation Assembly", "content": "The notation assembly model takes a pair of nodes as input, and gives a binary output indicating whether there is a relationship between them. An intuitive method is to first concatenate the features of two nodes, and then pass the pair as a single feature vector through a series of layers of a multi-layer perceptron (MLP). A sigmoid function \u03c3 is applied at the end to output the probability that there exists a relationship.\n\n$\\widehat{c}_{ij} = \\sigma(MLP([v_i, v_j]))$  (1)\n\nAs notation assembly is essentially binary classification, we use binary cross-entropy as our loss function:\n\n$L_{BCE}(\\widehat{c}_{ij}) = -e_{ij} \\log(\\widehat{c}_{ij}) \u2013 (1 \u2013 e_{ij}) \\log(1 \u2013 \\widehat{c}_{ij}).$\n\nWe adopt the input feature design in [5], where each vi is represented by its 4-dimensional bounding box coordinates and the class label. The class label is passed to an embedding layer with x dimensions. Therefore, the input to MLP will be a (4 + x) \u00d7 2 dimensional vector.\nExisting work assumes perfect detection output; therefore, the input bounding box and class label are the ground-truth information. While previous work has attempted to manually perturb the bounding box as a test of robustness, such perturbations don't reflect the kind of errors that might arise in a practical object detector.\nTo ensure our notation assembly system can adapt to errors introduced in the detection stage, we propose a supervised training pipeline that directly trains the assembly model on detection output V. Since most of the time V \u2260 V, we can't directly use the ground truth E as the supervision signal.\nTo deal with this issue, we construct a maximum weight matching M in the bipartite graph GM = (V, V) and build \u00ca for supervising our notation assembly model. We describe the detail of our matching procedure in Section 3.3, where it is also employed in evaluation. We adopt the edges from the ground truth according to our matching. Given a pair (vi, vk) \u2208 M and an edge (vk, vh) \u2208 E, we add (i, j) to \u00ca if (vj, vh) \u2208 M. Our method essentially builds a training set for the detection output that is in the same format as the ground-truth, allowing seamless training and evaluation."}, {"title": "3.3 End-to-End Evaluation", "content": "The main challenge of OMR evaluation is finding the edit distance between two music scores under some particular representation (e.g., XML format [18]). Haji\u010d [19] argued that intrinsic evaluation is needed to decouple research of OMR methods from individual downstream use-cases, since specific notation formats change much faster than music notation itself. Some works have taken steps to analyze the complexity of standard music notation [20] and propose common music representation formats [21].\nAs a general system consisting several modules, we seek to also evaluate our OMR pipeline holistically, without a specific focus on what the downstream processing will be. We therefore propose a novel matching-based evaluation metric to assess predictions that include errors from the detection stage. For the same reason that we had to adapt ground-truth edges to create training data for the notation assembly model (V \u2260 V), we cannot straightforwardly use the ground-truth graph to evaluate notation assembly. Our metric finds a matching between a test instance's predicted objects and those in the ground-truth object detection, and then uses this as a bridge to evaluate the edges returned by the notation assembly module.\nThe results reported by Pacha et al. [6] are the sole benchmark for assessing a notation assembly model using detected symbols. To address the matching issue between V and V, Pacha et al. employ a rule-based method, considering two objects identical if they belong to the same class and their intersection over union is at least 50%. However, this greedy matching approach is inadequate, as inaccuracies in symbol detection cannot be compensated for by the notation assembly model. Furthermore, Pacha et al. use conventional precision/recall metrics with a hard decision boundary, which fails to capture the overall performance of the model comprehensively. To resolve these issues, we propose a complementary metric based on a global optimal matching and area under the precision-recall curve.\nFormally, we denote \\(\\tilde{V} = {\\tilde{v}_1, \\tilde{v}_2,\\dots, \\tilde{v}_n}\\) as the set of symbols obtained from an object detection model, where \\(\\tilde{v}_i = (b_i, p_i)\\) is a tuple of a bounding box \\(b_i \\in \\mathbb{R}^4\\) and a probability distribution vector \\(p_i \\in \\mathbb{R}^C\\) over all symbol classes. A notation assembly prediction on \\(\\tilde{V}\\) would be an edge set \\(\\tilde{E} = {\\tilde{e}_1, \\tilde{e}_2,\\dots, \\tilde{e}_m}\\) where each edge \\(\\tilde{e}_i\\) is a tuple of two vertices. Similarly, we denote the ground truth notation graph as G = (V, E) with V = \\({v_1, v_2,..., v_n}\\), \\(v_i = (b_i, c_i)\\), E = \\({e_{1,2},\\dots,e_m}\\), where \\(b_i \\in \\mathbb{R}^4\\) is a bounding box and \\(c_i \\in {1,2,..., C}\\) is a symbol class label.\nWe first construct a complete weighted bipartite (V, V) where the weight for edge (\\(\\tilde{v}_i\\), vj) is \\(W_{ij} = IoU(b_i, b_j) \\cdot p_{i,c_j}\\). Here, IoU is the intersection-over-union between the area occupied by the two boxes, defined as:\n\n$\\ IoU(b_i, b_j) = \\frac{Area(b_i \\cap b_j)}{Area(b_i \\cup b_j)}$\n\nBased on this bipartite graph, we find the maximum weighted matching M using the implementation described in [22] and filter the \u201cweak\u201d matching edges with weight Wij less than a threshold Tmatch to get the matching function M : V \u2192 V \u222a{\u2205}:\n\n$\\ M(v_j) =\\begin{cases} \\tilde{v}_i, \\text{ if } (\\tilde{v}_i, v_j) \\in M \\text{ and } w_{ij} > T_{match}, \\\\ \\emptyset, \\text{ otherwise.} \\end{cases}$\n\nHere, Tmatch is a filtering threshold for matching and we set it to 0.05 without tuning.\nAfter getting the matching function, the ground truth assembly edges are naturally mapped back to edges between predicted vertices. The mapped edge set \\(\\tilde{E}\\) = {(M(vi), M(vj)) | (vi, vj) \u2208 E, M(vi) \u2260 \u2205, M(vj) \u2260 \u2205} represents a ground truth edge set on detected vertices, which can be used to evaluate predictions E to get a precision and recall. An example is shown in Figure 4.\nMost notation assembly models predict a probability of the existence of an edge (vi,vj), and the probability is further compared with a threshold Tpredict to determine whether (vi, vj) belongs to the prediction set \u0112. By adjusting the model prediction threshold Tpredict, we can get a series of predictions {E1, E2,\u2026\u2026 } and therefore derive a series of precision-recall pairs, which are used to estimate the area-under-the-curve (AUC) score. We refer to the full evaluation metric as \"Match+AUC.\"\n\"Match+AUC\" is an end-to-end evaluation metric for the OMR pipeline with following advantages:\n\u2022 \"Match+AUC\" accounts for model performance in both the object detection and notation assembly stages. To be specific, given an object detector's output, a notation assembly model will achieve a higher score if it predicts no edges among redundant objects, since connecting redundant nodes into the assembly graph would greatly affect the final output music score. Also, for the same assembly model, a worse object detector would generate a large amount of redundant and inaccurate objects, making it very hard for the assembly model to distinguish them.\n\u2022 Instead of a hard rule-based matching used in past methods, \"Match+AUC\" creates a comprehensive matching among detected symbols and ground truth symbols, making the final score more accurate and sensitive.\n\u2022 \"Match+AUC\" evaluates the model using the area under the precision-recall curve, which summarizes performance across a range of threshold choices that could be made by a downstream module or a system user.\nWe believe that our novel \"Match+AUC\u201d is a compelling tool for analyzing OMR pipelines that is complementary to existing approaches."}, {"title": "4. IMPLEMENTATION DETAILS", "content": "4.1 Music Symbol Detection"}, {"title": "4.1.1 Model Details", "content": "We finetune the \"large\" version of YOLOv8 (YOLOv8l), an object detection model pre-trained on the COCO dataset [23], on MUSCIMA++ v2.0 for music object detection. The model consists of 43.7M parameters and is capable of detecting object bounding boxes and generating corresponding class distributions. The input image size of our model is set to 640."}, {"title": "4.1.2 Training", "content": "We used the MUSCIMA++ v2.0 dataset to train and evaluate the music symbol detection model [4]. The images are binarized (pixels are 0/1-valued) and in a size of approximately 3500 \u00d7 2000 pixels. For simplicity, we use images with staff lines removed. Additionally, following the exact method described in [6], we split the dataset into 60% training data, 20% validation data, and 20% test data. To effectively train YOLOv8 on these dense images involving many small annotations, which include augmentation dots and piano pedal markings, we have to reduce the image size. Therefore, following the methods used by [8], given a large music score image, we randomly sample 14 crops of size 1216 \u00d7 1216 and then resize them to 640 \u00d7 640 to fit the YOLOv8 input requirement.\nWe fine-tune the YOLOv8 model for 500 epochs with a batch size of 8. We use the AdamW optimizer [24] with a learning rate of 5.5 \u00d7 10-5 and a momentum of 0.9, which are automatically set by the YOLOv8 codebase [7]. During training, we use the early stopping strategy with a patience of 100 epochs. We keep the checkpoint with the highest validation performance as our final model."}, {"title": "4.1.3 Inference", "content": "Since our detector is trained on cropped data, during the inference stage, we also need to segment the large images into smaller segments. However, partial objects at the edges of these crops would be hard to detect since the model can't see the full object. To resolve this issue, we extend every crop with a margin, which serves as a context for each image. The cropping is visualized in an example in Figure 5. We then perform symbol detection on each extended crop and consolidate the detection results. To make sure the objects on the edges are only detected once, overlapping bounding boxes are filtered based on their Intersection over Union (IoU) overlap rate."}, {"title": "4.2 Notation Assembly", "content": "4.2.1 Model Details"}, {"title": "4.2.1 Model Details", "content": "We use a 4-layer MLP for MLP, where the two hidden layers both have a hidden dimension of 32. The embedding dimension for the symbol class is also set to be 32. We use ReLU [25] as the activation function."}, {"title": "4.2.2 Training", "content": "Again we use the MUSCIMA++ v2.0 dataset to train and evaluate the notation assembly model [4]. Following previous work [5, 6], we balance the positive and negative pairs in the training set by filtering out the pairs of nodes that are too distant from each other since they are unlikely to be connected. Before feeding the bounding box coordinates to the model, we normalize them by the image width while keeping the aspect ratio fixed, so that all of the x-coordinate values fit in the range of [-1, 1].\nWe train our models for 200 epochs with batch size 256, and use Adam optimizer [26] with a learning rate of 0.0001. We evaluate our model every 20 epochs and pick the checkpoint with highest validation Match+AUC as our final model. All of the experiments are conducted with five different random seeds.\nIn our experiments, we consider three methods for training the notation assembly model:\n\u2022 A baseline, which uses the ground-truth object lists provided in the MUSCIMA++ dataset to train the notation assembly model. This is the setup used in [5].\n\u2022 A pipeline, which runs the music object detection model on the images to construct the training set for the notation assembly model, as discussed in Section 3.2.\n\u2022 A \"soft\" variant of the pipeline, where we replace the embedding layer for the symbol class with a linear layer that maps the symbol class probabilities outputted from the music object detection model to a 32-dimensional vector. Note that this linear layer will have the same parameter count (number of classes multiplied by the hidden dimension) as the replaced embedding layer."}, {"title": "4.2.3 Inference", "content": "Since we consider both stages together, the input to the notation assembly stage should correspond to the output of the object detection stage. As described in Section 3.2, the detection output is converted into (V', E'). We then pass each pair of nodes to the notation assembly model, and feed the result into our evaluation function. We hypothesize that this realistic setup introduces a distribution shift to the model that was trained on the ground-truth objects and we will make the comparison in Section 5."}, {"title": "5. EXPERIMENTS", "content": "In this section, we first report the performance of our music symbol detection model. Then, we compare the performance of different notation assembly training pipelines using the evaluation metric described in Section 3.3."}, {"title": "5.1 Music Symbol Detection", "content": "Following the evaluation protocols of the Pascal VOC challenge [27], which are used by previous methods [8, 10, 11], we present both the mean average precision (mAP) and the weighted mean average precision, as detailed in Table 1. To elaborate, a predicted bounding box bi is thought to be a true positive only if IoU(bi, bj) > 0.5 for some ground truth box bj. Then, average precision (AP) computes the area under the precision-recall curve, providing a single value that encapsulates the model's precision and recall performance. The weighted/unweighted mean Average Precision (mAP) extends the concept of AP by calculating the average AP values across multiple object classes, taking into account the number of occurrences of each class in a weighted or unweighted manner. Our experiments are conducted with the MUSCIMA++ v2.0 dataset, while the authors of most previous methods [10, 11] have only tested their models on MUSCIMA++ v1.0. This introduces a misalignment between our results. Thanks to Zhang et al. [8], who provided reproduced results of most previous methods on MUSCIMA++ v2.0, we directly report their reproduced results in the table.\nOur model outperforms Zhang et al.'s method on their selected 20 classes by 2.4% (mAP, absolute), likely due to the improvements in YOLOv8 compared to v4."}, {"title": "5.2 Notation Assembly", "content": "In this section, we complete the multi-stage OMR system by chaining different notation assembly models to the best music object detection model we trained in Section 5.1. We use the metric we designed in Section 3.3 to report the end-to-end performance of the OMR system.\nIn Table 2, we compare the notation assembly systems trained with baseline training, pipelined training, and soft pipelined training as described in Section 4.2. We found that pipelined training improves the Match+AUC score by 0.65% (essential) and 1.79% (all), absolute, and incorporating the soft class label further increases the performance by 1.91% (essential) and 1.34% (all), absolute. Training the notation assembly model on the detection model output and using the soft label probability to represent the class information, we are able to improve the Match+AUC of the OMR system by 3.13%. We hypothesize that pipelined training helps the assembly model adapt to any inaccuracies our object detector has, and incorporating the soft class labels enables the assembly model to consider alternative class labels, not just those chosen by the object detector."}, {"title": "6. CONCLUSION AND FUTURE WORK", "content": "In our study, we reconsider a multi-stage OMR pipeline built and evaluated using the MUSCIMA++ dataset. We first propose a state-of-the-art music symbol detector, serving as a strong preprocessor for the notation assembly stage. We then propose a training pipeline in which notation assembly is learned from imperfect object detection outputs (rather than ground-truth objects), which leads to higher performance. Finally, we introduce an evaluation score, Match+AUC, which can jointly consider the error in both detection and assembly stages, allowing evaluation of the two stages together.\nMatch+AUC is not restricted to being an evaluation metric. Future research could explore the application of Match+AUC within a joint training objective function for both the object detection and notation assembly stages. This approach would enable the entire model to be optimized for retrieving a globally optimal music notation graph.\nIn this study, we focus on the object detection and notation assembly stages in the OMR pipeline. Progress on the encoding stage is also required for a complete OMR solution; while the music notation graph arguably contains the essential information for recovering a score [4], conversion of such graphs into standard formats remains unsolved. Exploring methods to effectively convert music notation graphs into standard formats could be a valuable research direction to achieve a more complete OMR solution."}]}