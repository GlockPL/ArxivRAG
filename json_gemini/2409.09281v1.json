{"title": "Language Models \u201cGrok\u201d to Copy", "authors": ["Ang Lv", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "abstract": "We examine the pre-training dynamics of language models, focusing on their ability to copy text from preceding context a fundamental skill for various LLM applications, including in-context learning (ICL) and retrieval-augmented generation (RAG). We propose a novel perspective that Transformer-based language models develop copying abilities similarly to grokking, which refers to sudden generalization on test set long after the model fit to the training set. Our experiments yield three arguments: (1) The pre-training loss decreases rapidly, while the context copying ability of models initially lags and then abruptly saturates. (2) The speed of developing copying ability is independent of the number of tokens trained, similarly to how grokking speed is unaffected by dataset size as long as the data distribution is preserved. (3) Induction heads, the attention heads responsible for copying, form from shallow to deep layers during training, mirroring the development of circuits in deeper layers during grokking. We contend that the connection between grokking and context copying can provide valuable insights for more effective language model training, ultimately improving in-context performance. For example, we demonstrated that techniques that enhance grokking, such as regularization, either accelerate or enhance the development of context copying.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) possess the capability to learn, retrieve, and reason from input context, facilitating various applications such as in-context learning (ICL, Brown et al., 2020) and retrieval-augmented generation (RAG, Lewis et al., 2020). Despite these achievements, several shortcomings have been reported regarding LLMs' in-context capacities. For instance, the order of ICL demonstrations matters (Lu et al., 2022; Zhang et al., 2024) and LLMs' awareness of different contextual positions fluctuates (Liu et al., 2023; Chen et al., 2024; Lin et al., 2024). We believe that studying the mechanisms behind the development of in-context capabilities during pre-training offers valuable insights for enhancing LLMs from a novel perspective.\nIn this paper, we examine the pre-training dynamics of language models, focusing specifically on their context copying capabilities. These capabilities are crucial for various LLM applications, including ICL and RAG. For example, Olsson et al. (2022) interpret ICL as a process that entails copying and then fuzzy pattern completion. Similarly, RAG exhibits this characteristic, as it requires the in-context retrieval of key information, which is then copied (or integrated with additional paraphrasing and reasoning) as the output. This paper presents empirical evidence demonstrating that Transformer-based language models (Vaswani et al., 2017) develop context copying capabilities in a manner akin to \u201cgrokking\u201d (Power et al., 2022). Grokking refers to the abrupt improvement in test set generalization long after models have overfit.\nOur experimental method is summarized as follows: We trained 12-layer Llama models (Touvron et al., 2023) using 40 billion tokens and saved checkpoints at regular intervals. To evaluate context copying, we presented the models with an input context comprising multiple random token subsequences, each beginning with a unique prefix, and let them to complete one of the prefixes presented in the context. The accuracy of these completions served as a measure of the models' context copying abilities. By analyzing the evolution of context copying accuracy and the development of circuits (i.e., the subnetworks responsible for completing the specific task) across the saved checkpoints, we argue a potential connection between grokking and"}, {"title": "2 General Setup", "content": "Model Architecture and Hyper-parameters.\nWe train small Llama models (Touvron et al., 2023) on a subset of the RedPajama dataset (Computer, 2023), comprising 40 billion tokens, with the task of next-token prediction. Our model has 12 layers, each with 12 attention heads. The hidden state dimension is 768, and the intermediate dimension of MLP layers is 3,072. Our models contain 162M parameters. We use Llama tokenizer with a vocabulary of 32,000 tokens, and set the model context length to 1,024 tokens. Unless otherwise specified, the following hyperparameters are used: The AdamW optimizer (Loshchilov and Hutter, 2019) with (\u03b21, \u03b22) = (0.9, 0.999), a learning rate of 0.1, 2000 warmup steps, and the norm clip value of 1. Our training is conducted on 8 A100 GPUs, with a batch size of 64 per GPU.\nEvaluating Context Copying. Each test sample consists of 50 random-token sequences, which are concatenated to form a single long sequence. These sequences have an average length of 18 tokens, and we ensure that the 12-gram prefix and 6-gram suffix of each sequence is unique. We append the prefix of the i-th sequence to the end of the concatenated sequences, which together serve as the model's input. Our test set includes 500 samples.\nThe model's objective is to continue the input. We consider the model's output correct if it copies the suffix of the queried prefix from the context, since random token sequences lack meaningful semantics and the most natural continuation is to generate a prefix that has appeared in the context (Olsson et al., 2022). To comprehensively assess context copying capabilities across different contextual positions, we evaluate the model for every i mod 5 = 0. We report the average accuracy across these positions. Unless specifically indicated, we report the average accuracy across these positions, from models trained with 3 different random seeds."}, {"title": "3 Language Models \u201cGrok\" to Copy", "content": "We propose that language models develop the context copying in a manner similar to \u201cgrokking\". In this section, we present three arguments and present the corresponding experiments and analyses that provide evidence for these claims."}]}