{"title": "Zero-shot Shark Tracking and Biometrics from Aerial Imagery", "authors": ["Chinmay K. Lalgudi", "Mark E. Leone", "Jaden V. Clark", "Sergio Madrigal-Mora", "Mario Espinoza"], "abstract": "1. The recent widespread adoption of drones for studying marine animals provides opportunities for deriving biological information from aerial imagery. The large scale of imagery data acquired from drones is well suited for machine learning (ML) analysis. Development of ML models for analyzing marine animal aerial imagery has followed the classical paradigm of training, testing, and deploying a new model for each dataset, requiring significant time, human effort, and ML expertise.\n2. We introduce Frame Level ALIgment and tRacking (FLAIR), which leverages the video understanding of Segment Anything Model 2 (SAM2) and the vision-language capabilities of Contrastive Language-Image Pre-training (CLIP). FLAIR takes a drone video as input and outputs segmentation masks of the species of interest across the video. Notably, FLAIR leverages a zero-shot approach, eliminating the need for labeled data, training a new model, or fine-tuning an existing model to generalize to other species.\n3. With a dataset of 18,000 drone images of Pacific nurse sharks, we trained state-of-the-art object detection models to compare against FLAIR. We show that FLAIR massively outperforms these object detectors and performs competitively against two human-in-the-loop methods for prompting SAM2, achieving a Dice score of 0.81. FLAIR readily generalizes to other shark species without additional human effort and can be combined with novel heuristics to automatically extract relevant information including length and tailbeat frequency.\n4. FLAIR has significant potential to accelerate aerial imagery analysis workflows, requiring markedly less human effort and expertise than traditional machine learning workflows, while achieving superior accuracy. By reducing the effort required for aerial imagery analysis, FLAIR allows scientists to spend more time interpreting results and deriving insights about marine ecosystems.", "sections": [{"title": "1 Introduction", "content": "Large marine animals, such as sharks, influence ecosystems through a variety of mechanisms as predators, nutrient transporters, and even prey, thus maintaining the balance of marine food webs, regulating species populations, and promoting biodiversity (Heupel et al., 2014; Heithaus et al., 2022; Dedman et al., 2024). Unfortunately, overfishing and other anthropogenic threats have greatly reduced shark populations, altering their ecological roles and effects on ecosystems (Stevens et al., 2000; Ferretti et al., 2010; Myers et al., 2007). As a result, long-term monitoring of sharks and other large marine animals is key to understanding how animal populations are responding to human impacts and potential environmental shifts caused by climate change (Brock et al., 2013; Torres et al., 2022). However, studying sharks is challenging due to their elusive nature, vast ranges, and the inherent difficulties in observing their interactions in marine environments (Jorgensen et al., 2022). Thus, new technology must be developed to understand both their large-scale and precise movement ecology.\nResearchers often use satellite or acoustic telemetry to study shark behavior over multiple spatial and temporal scales, but tagging requires significant human effort and may be detrimental to the well-being of tagged animals (Kohler and Turner, 2001; Matley et al., 2024). More recently, baited remote underwater video (BRUV) systems have been used to capture visual information and collect imagery (White et al., 2013). However, BRUVs are limited to capturing localized behavior and are dependent on sharks maintaining proximity to the deployment site.\nThe use of unmanned aerial vehicle (UAV) systems is emerging as a promising approach for the non-invasive study of volitional marine animal behavior and biometrics (Gray et al., 2019a; Hodgson et al., 2013; Ramos et al., 2022; Torres and Bierlich, 2020; Torres et al., 2022). Aerial imagery can be used to compute biometrics such as length, body condition, tailbeat frequency, and relative velocities, which can provide key information about animal health (Bierlich et al., 2024), swimming kinematics (Porter et al., 2020), and predator-prey interactions (Hansen et al., 2022). There has been significant effort towards developing UAV systems for studying wildlife (Butcher et al., 2021; Shah et al., 2020; Hodgson et al., 2018; Jadhav et al., 2024), including path-planning across varying environmental conditions (Shah et al., 2020; Clark et al., 2024) and exploring the efficacy of sensor modalities for wildlife detection (Saunders et al., 2022; Beaver et al., 2020). In this work, we focus on developing automated systems for analyzing existing drone imagery."}, {"title": "1.1 Deep Learning for Marine Ecology", "content": "Deep learning, the process of training large artificial neural networks to learn complex functions from data, has important applications to the study of aerial wildlife imagery (LeCun et al., 2015). Previous works have used deep learning for analysis of aerial imagery; however, they often rely on specialized object detection models (Carrio et al., 2017; Eikelboom et al., 2019).\nTraditionally, object detection networks have relied on Convolutional Neural Networks (CNN) architectures and their variants (Li et al., 2021; Alzubaidi et al., 2021; Girshick, 2015; Ren et al., 2015). Perhaps the most popular object detection model with a CNN backbone, You Only Look Once (YOLO) (Jocher et al., 2023), is specialized for inference speed, treating object detection as a regression problem. By predicting classes and bounding boxes in a single pass, YOLO models are more suitable for real-time applications. More recently, the Detection Transformer (DETR) (Carion et al., 2020) has achieved state of the art results by integrating transformers into the encoder and decoder of the model. This eliminates the need for many hand-engineered components by doing direct set prediction of object classes and bounding boxes. DETR leverages this global context to achieve impressive results thar rival non-transformer architectures. This is particularly beneficial in scenarios discussed in this work, where multiple objects are in close proximity, such as large groups of sharks, or partially occluded by factors like camera glare or high turbidity.\nSome studies have proposed using object detection models to track sharks in aerial imagery (S and Denny J, 2024; Zhao et al., 2023; Butcher et al., 2021). One of the earliest efforts to apply neural networks to aerial imagery of marine life consisted of training a vanilla CNN for sea turtle detection (Gray et al., 2019b), and subsequent works have used transfer learning (fine-tuning a pre-trained CNN), improving model performance (Gray et al., 2019b; Desgarnier et al., 2022; Sharma et al., 2018). Alternatively, some approaches train models by directly segmenting marine animals and objects. S and Denny J (2024) propose a novel hybrid architecture called SwinConvMixerUNet for underwater image segmentation, leveraging the Swin Transformer's ability to capture spatial information and the ConvMixer's channel-mixing capabilities to enhance feature extraction and segmentation accuracy.\nUnfortunately, standard object detection and segmentation models require large datasets of high-quality human-annotated data to train models. Furthermore, they often do not generalize well, performing poorly when the inference data distribution differs from the model's training data (Koh et al., 2021). In contrast to conventional approaches, we leverage foundation models (i.e. large deep"}, {"title": "2 Materials and Methods", "content": "learning models trained on internet-scale datasets) for marine animal tracking and biometric analysis from aerial imagery (Bommasani et al., 2021). The key advantage of using pre-trained foundation models is that they can be deployed zero-shot. Thus, they do not require dataset curation or training to adapt to new data, and require significantly less human effort and expertise to use. There is a notable lack of adoption of foundation models for the study of marine animals from UAV imagery, with the singular exception being an automated pipeline using Segment Anything Model for surveying whale length and body condition (Bierlich et al., 2024).\nIn this work, we explore methods for automatically computing segmentation masks and downstream biometrics for sharks using Segment Anything Model 2 (SAM 2), a pre-trained foundation model for promptable image and video segmentation (Ravi et al., 2024), and Contrastive Language-Image-Pretraining (CLIP), an approach for learning shared representations between natural language and pixels (Radford et al., 2021). We present a new method, Frame Level Allgnment and tRacking (FLAIR), that uses CLIP and SAM 2 to generate accurate segmentations for several shark species across different environments, leveraging a zero-shot approach that eliminates the need for annotating data, training, or fine-tuning. These segmentation masks can be used to compute tailbeat frequency, length, mass, velocities, and other downstream biometrics for arbitrary shark species.\nWe test our approach predominantly on a dataset of the Pacific nurse shark (Ginglymostoma unami) from Santa Elena Bay (North Pacific coast of Costa Rica), demonstrating how our method can be used to help better understand the movement ecology of an endangered, data-poor species (Madrigal-Mora et al., 2024). We compare segmentation accuracy of FLAIR with multiple approaches, including prompting SAM 2 with a human in the loop, as well as prompting SAM 2 with state-of-the-art object detection models.\nOur study suggests that FLAIR is capable of generalizing to other species and we show that FLAIR segmentations can be used to measure biometrics including length and tailbeat frequency. Notably, FLAIR does not require any training or fine-tuning to generalize to other species, highlighting its potential applicability across diverse ecosystems."}, {"title": "2.1 Dataset", "content": "The Pacific nurse shark imagery was collected from two field sites (Matapalito Beach and Sortija Beach) in the coastal waters of the Eastern Tropical Pacific Ocean, in Santa Elena Bay, Costa Rica (Fig. 1a, b). The dataset was collected from 2022-2024, over a period of 23 months, with varying water visibility (turbidity), illumination, and wind/wave conditions. Images were collected at each site by flying a DJI Mavic 2 drone on a pre-programmed path, recording a continuous video at 30FPS and 3840\u00d72160 resolution. The drone stopped at predetermined waypoints for 3 seconds each (Fig. 1c, yellow and orange dots).\nMore than 6 hours of video was recorded in total, during 60 drone surveys, resulting in 648,000 total frames captured. To our knowledge, this is one of the largest open-access datasets of nearshore shark aerial drone imagery.. For object detection, the dataset was pruned to include a diverse set of 7 videos from the two sites, across varying tidal, turbidity, water surface glare, and wave conditions. Ground truth bounding boxes were added for each Pacific nurse shark with the Computer Vision Annotation Tool (CVAT) (CVATteam, 2020). The images from 2 videos were completely separated from the rest of the dataset, as a test for generalization. The rest of the dataset was time-blocked such that each 45 adjacent frames of a video were held together when the data was split into training, validation, and test sets. This time-blocking was done to minimize occurrences of consecutive frames being present in the training and test set, which would artificially inflate models' performance metrics. Sharks were present in diverse conditions across videos and time segments, including variable turbidity and substrate type (Fig. 2a). Prior to object detection model training, images were rescaled from 3840 x 2160 pixels to 1080 \u00d7 1080 pixels. Standard data augmentation techniques, including random rotation, brightness, and hue adjustments, were also applied.\nExcluding the images from the two videos held for generalization testing, our object detection dataset contained 9,200 unique positive examples (images containing sharks) and a corresponding 27,000 unique negative examples (images containing no sharks). We selected 9,200 negative examples to remain in the dataset, so that our dataset contained an equal ratio of images with and without sharks. The dataset was then randomly split into training, validation, and test sets, in a [80-10-10] ratio, maintaining the images in these 45-frame time blocks. This resulting dataset used for training and testing the object detectors contains 18,400 total images.\nThe object detectors were tested on an internal test set from the 5 training videos, as well as the 2 holdout videos. The other approaches-Per-frame Prompting, HiL-Tracking, and FLAIR-did not require training, thus they were tested on all 7 videos. For computational efficiency, smaller sub-videos containing sharks were used as input into these pipelines, each ranging from 20 to 80 seconds in length. This dataset was used for precision and recall metric comparisons across all methods. A random sampling of 100 frames from the 2 holdout videos and 200 frames from the other 5 videos were annotated for segmentation ground truth masks and biometrics using CVAT, as described in Section 2.5. These ground truth masks were used to compute the accuracy of segmentation across methods.\nTo test the generalization abilities of FLAIR, two UAV videos licensed under Creative Commons Attribution were acquired from YouTube one of a white shark (Carcharodon carcharias) in Southern California, USA, and one of a blacktip reef shark (Carcharhinus melanopterus) in the Great Barrier Reef, Australia (Fig. 2b,c). Full attribution to the original creators of these videos is provided in the Supporting Information section. A smaller dataset was built from these blacktip and white shark videos, sampling 25 random frames from each of the two videos. Similarly, segmentation mask ground truths and biometrics were manually annotated to test the generalizability of FLAIR across diverse aerial footage."}, {"title": "2.2 Baselines", "content": null}, {"title": "2.2.1 Per-frame Prompting", "content": "We evaluated baseline methods for object segmentation traditionally used in the field, with one of the most popular approaches being per-frame prompting. This method requires a human annotator to label bounding boxes for sharks in every frame of a video, which is effective for accurate object"}, {"title": "2.2.2 Object Detection Models", "content": "We also focused on evaluating a series of object detection models, predominantly leveraging the Detectron2 model library (Wu et al., 2019). First, we employed several RCNN model architectures, two-stage detectors optimized for accuracy (as opposed to inference speed like many one-stage detectors) (Girshick, 2015). We trained several RCNNs with various backbones (ResNet and ResNeXt architectures) and feature pyramid networks (FPN) to enhance the effect of objects of various scales in images. All of these models were pre-trained on the Imagenet dataset (Deng et al., 2009).\nWe also trained the DETR architecture for shark detection (Carion et al., 2020). Finally, we trained a YOLOv8 model pre-trained on Imagenet (Jocher et al., 2023). The pre-trained medium YOLOv8 model was trained for 40 epochs using default hyperparameters (until loss converged). DETR was trained for 120 epochs, and the rest of Detectron models were trained for 40 epochs each - all with a maximum of 100 objects detected per frame. Each frame, SAM 2 was prompted with bounding boxes output from the object detector to generate segmentation masks (Fig. 3b)."}, {"title": "2.3 Human-in-the-Loop (HiL) Tracking", "content": "In human-in-the-loop tracking, a human manually annotates a bounding box in the first frame a shark is identified, and then SAM 2 is used to track the segmentation through the remainder of the video (until it was lost) (Fig. 3c). When the track is lost, the annotator re-initializes the segmentation track with a bounding box. CVAT was used for bounding box initialization."}, {"title": "2.4 FLAIR", "content": "Our proposed method, FLAIR, is a fully autonomous framework for object tracking-integrating frame-level alignment and video understanding with language prompts, as illustrated in Fig. 4. First, individual frames of the video are sampled at a uniform time interval and are passed into the SAM 2 Automatic Mask Generator. In this work, time intervals of 30 frames (1 sec) were used. SAM 2 Automatic Mask Generator generates masks for all possible objects in the image by sampling single-point inputs in a grid, filtering and de-replicating candidate masks, and performing further processing for improved quality. Bounding boxes are generated for each mask in the frame and passed into CLIP, along with prompts that were fine-tuned for this task. The specific prompts used are included in Supporting Information. The prompts were held constant across all of the nurse shark videos as well as the white and blacktip reef shark videos, highlighting the generalizability of this method. Each bounding box is assigned a probability associated with each prompt, and all bounding"}, {"title": "2.5 Biometrics Measurements", "content": "Segmentations of marine animals can be used for further downstream tasks including the calculation of biometrics (Bierlich et al., 2024; Gray et al., 2019a). Tailbeat frequency (TBF) and length were computed from FLAIR-predicted masks and compared against manual calculations for a video of a Pacific nurse shark from our dataset, and open-access videos of a blacktip reef shark and a white shark. Manual measurements for length were computed in pixels using CVAT line tool for a random sample of frames for each of the videos CVATteam (2020). TBF was manually measured by observing a"}, {"title": "3 Results", "content": "line occurred between local extrema. Only keeping these crossings, we then calculated the intervals for tail beats as every other crossing that was aligned with the equilibrium center line, proving both conceptually and empirically to be a strong detector of TBF. To compare predicted tail beat intervals with manually-measured tail beat intervals, we moved a sliding window of 5 seconds across the video in steps of 0.5 seconds, calculating the fractional number of tail beats for each window. This allows for both comparison between manual and predicted TBF and observing TBF across time, providing potential insight into shark kinematics and energetics."}, {"title": "3.1 Object Detection", "content": "We tested our suite of models on an internal test dataset to assess the models' ability to learn representations, as well as two previously-unseen external holdout videos to assess the generalizability of the models. Testing the models on both datasets gauges potential overfitting on previously-seen data, while also evaluating for practical usage on drone imagery from new distributions. Among the models, YOLOv8 and DETR performed the best on the internal holdout test set across all metrics, including mean Average Precision (mAP) and mean Average Recall (mAR) from IOU thresholds between 0.5 to 0.95 and Average Precision and Average Recall at IOU thresholds of 0.35 and 0.5. The exact precision and recall values are shown in Table S1. YOLO had the highest mAP as well as the highest mAR, with DETR performing slightly better at lower IOU thresholds. Both of these performed better than the trained Faster R-CNN models with Feature Pyramid Network and Dilated-C5 backbones. It is important to note that although mAP averaged between 0.5 and 0.95 are traditionally used as performance metrics for object detection models, we found that it is not necessary for predicted bounding boxes to have a high IOU with ground truth boxes in order to obtain accurate results on downstream biometrics methods. Thus, although both YOLO and DETR had a relatively low mAP and mAR of 0.63 and 0.70, performance at a lower IOU threshold of 0.35 was significantly higher-with DETR achieving a near-perfect AP and AR of 0.94 and 0.99, respectively.\nAs seen in Table S2, YOLOv8 and DETR had high accuracy and recall at lower IOU thresholds on holdout Video 1, but both performed extremely poorly on holdout Video 2, with mean AR at IOU threshold of 0.1 being 0.03 and 0 respectively. FLAIR had similar AP and AR for the first holdout video, but had significantly better performance in Video 2, with an AP and AR at IOU of 0.1 equaling 0.49 and 0.95 respectively."}, {"title": "3.2 Shark Segmentation", "content": "To better evaluate performance across models, segmentation metrics were measured on predicted masks. Segmentation is a more relevant task to evaluate than basic object detection for downstream pipelines, including studying biometrics and biological interactions. Dice score was used to evaluate segmentation performance, which measures the spatial overlap between the predicted and ground truth masks, with a value of 1 indicating perfect agreement and 0 indicating no overlap. All models had relatively high Dice scores for the first holdout video, that was deemed to be \"in distribution\" of the other videos, as it was taken on the same day and location as two of the training videos. However, both object detectors (YOLOv8 and DETR) were unable to identify bounding boxes in the second video, which was taken on a different day and weather conditions, resulting in low Dice scores of 0.014 and 0. The other semi- and fully-autonomous approaches (Per-frame Prompting, HiL-Tracking,"}, {"title": "3.3 Biometrics Case Study", "content": "Body length and tailbeat frequency (TBF) were calculated from FLAIR masks for sampled frames from three individual videos of a white shark, a Pacific nurse shark, and a blacktip reef shark. These calculations were compared to manual measurements of body length and TBF. The body lengths derived from FLAIR-predicted masks (reported as mean \u00b1 standard deviation) for the white shark, Pacific nurse shark, and blacktip reef shark, were 5.3 \u00b1 0.8 m, 1.5 \u00b1 0.1 m, and 1.0 \u00b1 0.3 m, respectively. Similarly, the body lengths derived from manual annotation for the white shark, Pacific nurse shark, and blacktip reef shark, were 5.0 \u00b1 0.8 m, 1.4 \u00b1 0.1 m, and 1.0 \u00b1 0.3 m, respectively. The predicted body lengths are nearly identical to the manually measured lengths, as these values are tightly concentrated around the line y = x in Fig. 7a. The distributions of predicted body length across all three sharks are also highly similar to ground truth measurements (Figs 7e-7g). In addition, visual inspection of masks and predicted centerlines shows accurate segmentation and center line estimation of the sharks (Figs. 7b-7d).\nTail beat frequency was computed following processing and smoothing of raw signal of tail displace-ment from the center line as shown in Figs 8a-8c. Smoothing the signal reduced noise and improved centerline crossing calculation. TBF predicted from FLAIR masks very closely followed manually calculated TBF, with a mean error of 2.07% across all three species. All predicted TBF measurements were within 7% error from the manual measurements. TBF was relatively constant for nurse and"}, {"title": "3.4 Efficiency Comparisons", "content": "To compare efficiency and speed between methods, we measured the time required for labeling bounding boxes and segmentation masks across the video dataset. We found that manual labeling took an average of 10.5 seconds per frame for bounding box annotation and an additional 45 seconds per frame for mask segmentation for a total of 55.5 seconds per frame. For both the object detection methods and Per-frame Prompting, the human effort time was 10.5 seconds per frame for the bounding box annotation. HiL-Tracking requires 0.15 seconds per frame for human annotation (including watching the video and selecting bounding boxes) and FLAIR requires no human effort. To put this in context, the human effort time for a 5 minute aerial drone video at 30 FPS requires approximately 139 hours for manual labeling, 26 hours for object detection and Per-frame Prompting, 22.5 minutes for HiL-Tracking and 0 minutes for FLAIR."}, {"title": "4 Discussion", "content": "In this study, we present FLAIR, a fully automatic semantic segmentation strategy that markedly improves the efficiency of studying marine animals in aerial drone videos. We found FLAIR performed better than several state-of-the-art (SoTA) object detection models used to prompt SAM 2. This demonstrates that traditional object detection methods, such as YOLOv8 and DETR, require"}, {"title": "The length $L_{pixels}$ obtained from the mask, measured in pixels, was converted to length in meters $L_m$ using the following function, modified from (Torres and Bierlich, 2020):", "content": "$L_m = (S_w / F) * (A + D) / I_w * L_{pixels}$"}]}