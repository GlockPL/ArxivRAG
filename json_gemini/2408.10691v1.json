{"title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches", "authors": ["Yanjie Dong", "Xiaoyi Fan", "Fangxin Wang", "Chengming Li", "Victor C. M. Leung", "Xiping Hu"], "abstract": "Since the invention of GPT2-1.5B in 2019, large language models (LLMs) have transitioned from specialized models to versatile foundation models. The LLMs exhibit impressive zero-shot ability, however, require fine-tuning on local datasets and significant resources for deployment. Traditional fine-tuning techniques with the first-order optimizers require substantial GPU memory that exceeds mainstream hardware capability. Therefore, memory-efficient methods are motivated to be investigated. Model compression techniques can reduce energy consumption, operational costs, and environmental impact so that to support sustainable artificial intelligence advancements. Additionally, large-scale foundation models have expanded to create images, audio, videos, and multi-modal contents, further emphasizing the need for efficient deployment. Therefore, we are motivated to present a comprehensive overview of the prevalent memory-efficient fine-tuning methods over the network edge. We also review the state-of-the-art literatures on model compression to provide a vision on deploying LLMs over the network edge.", "sections": [{"title": "I. INTRODUCTION", "content": "Since the OpenAI researchers introduced the GPT2-1.5B to the world in 2019, the large language models (LLM) have paradigm transition from the specialized and fragmented deep learning models to versatile and one-size-fits-all foundation models. Earlier investigations have suggested that LLMS exhibit high-level generalization that enables them to apply their acquired knowledge to new tasks not included in their original training (a.k.a., zero-shot ability) such as text generation, machine translation, and question answering [1]. With the trend of scaling to hundreds and thousands billions of weights, the LLMs showcase the remarkable ability to generate coherent and contextually relevant text, sparking widespread interest and subsequent advancements in the field of generative artificial intelligence (AI). Along with the proliferation of LLMs, the large generative Al models have also been spanned to generate images (DALL-E, StyleGAN, and VQ-VAE-2), audios (Jukedeck and Jukebox), videos (DVD-GAN and FiLM) and multi-modal contents (CLIP and Imagen). Despite their impressive zero-shot performance, it usually requires a fine-tuning process on the local datasets for the downstream tasks [2]. Besides, the huge amount of weights also demands for a model compression process when deploying the LLMs at the network edges. However, substantial computational and storage volume are required for the fine-tuning and model compression of the LLMs.\nFor fine-tuning an LLM, the traditional first-order optimizers (e.g., Adam, AdaGrad, and SGD) need to perform the backward propagation (BP) operation to obtain the gradient of loss function. However, the BP operation of the LLMs needs multiple times of memory on GPUs than the inference tasks with the LLMs [3]. For example, it takes around 14 GB memory to use LLAMA-7B for inference when the weights are loaded in half-precision (FP16) format. In order to use Adam optimizer, the weights and gradients consume 28 GB memory in FP16 format. For the BP operations in the Adam, extra memory is used to hold copies in FP32 format of weights (28 GB), momentums (28 GB), and variances (28 GB). The memory budget for fine-tuning is around 112 GB, which exceeds the volume of memory of mainstream GPUs (e.g., Nvidia A800 and RTX 4090). The huge demand for memory on GPUs and the high capital expenditure motivates the research community to develop memory efficient fine-tuning methods. The current memory efficient fine-tuning methods can be categorized into two branches: 1) reducing the trainable weights for fine-tuning (i.e., parameter-efficient fine-tuning, PEFT); and 2) developing optimizers without requiring the backward propagations (i.e., memory-efficient full fine-tuning, MEF2T).\nBy applying model compression, the energy required for training and deploying LLMs can be significantly reduced. This not only reduces operational costs but also minimizes the environmental impact of LLMs. As the demand for AI applications continues to grow, integrating sustainable practices such as model compression becomes crucial in mitigating the ecological footprint of AI technologies. This approach"}, {"title": "A. Major contributions", "content": "Motivated by the above facts, we review the recent advancements in the fine-tuning and model-compressing tasks. Hereinafter, we first figure out the issues and corresponding potential approaches of the PEFT and MEF2T in the distributed learning networks over the edge (DLNoE). Then, we also discuss the issues and the state-of-the-art approaches in model compression in the DLNOE. Our contributions are summarized as follows.\n\u2022 Since the major challenge in fine-tuning an LLM locates at the large storage demand which cannot be satisfied by the DNLOE, we review several exemplary works on PEFT and MEF2T that can reduce the footprint of storage demand. We also perform an numerical investigation on the MEF2T in DLNOE to collaborate the effectiveness of MEF2T in DLNOE.\n\u2022 For the model compression, we classify the current literature into three categories: compression-and-train, compression-then-train, and one-shot compression.\nThe remaining work is organized as follows. Section II and III respectively discuss the issues and approaches of PEFT and MEF2T in the DLNOE. Section IV introduces the state-of-the-art methods for model compression. Since the DLNOE for LLMs are at the early stage of investigation, we conclude our work and provide several future directions in Section V."}, {"title": "II. PARAMETER-EFFICIENT FINE-TUNING IN DLNOE", "content": "PEFT techniques have emerged as critical methods for adapting LLMs efficiently to network edges with limited computational power and storage volume. As foundation models scale to hundreds of billions of parameters, fine-tuning the entire models for each task becomes cost-prohibitive. PEFT techniques address this challenge by modifying/introducing only a small amount of the model weights while keeping the rest of the model frozen. In the context of DLNOE, the PEFT techniques can reduce the computational and communication requirements due the reduced size of trainable weights. As shown in Fig. 2, the PEFT techniques in DLNOE can be classified as serial PEFT, parallel PEFT, and selective PEFT based on the ways of inserting the small trainable modules [4]. To make our work self-contained, we briefly introduce the definition of the PEFT techniques as\n\u2022 Parallel PEFT indicates that the part of the LLMs can be approximated by a small trainable module that has much smaller amount of weights than its target part in the LLMs as shown in Fig. 2(a).\n\u2022 Serial PEFT plugs in the trainable module into the transformer blocks of the LLMs as shown in Fig. 2(c).\n\u2022 Selective PEFT selects a set of weights in the LLMs instead of introducing extra weights which may increase the model complexity as shown in Fig. 2(b).\nThe PEFT techniques only update the selected trainable module during the for downstream task fine-tuning. Hereinafter of this section, we introduce the recent representative advances of PEFT techniques that have the potential to be applied in the DLNOE."}, {"title": "A. Parallel PEFT in DLNOE", "content": "In the context of DLNOE, the federated instruction-tuning (FedIT) framework was proposed in [5], where the local training operations are performed at the client side, and scheduling and aggregation operations are performed at the server side. More specifically, each client introduce a parallel low-rank adaption (LoRA) trainable module to all dense layers of the target LLM. With the LoRA module, the number of trainable parameters is significantly reduced, thereby decreasing computational and communication overheads. Another merit of FedIT is to handle the heterogeneous instruction data. For example, the Databricks-dolly-15k dataset, which contains diverse instruction-following records across various categories, is split among different clients to mimic a scenario where different clients have different instruction types. Empowered by the LoRA module, the generalization capability of the fine-tuned LLMs is enhanced."}, {"title": "B. Serial PEFT in DLNOE", "content": "Vanilla soft-prompt PEFT technique often suffers from performance degradation or low training efficiency. To alleviate the shortcoming of vanilla soft-prompt PEFT technique, the federated parameter-efficient prompt tuning with adaptive optimization (FedPepTAO) framework was proposed in [6]. In the FedPepTAO framework, each layer of the LLM is appended with a soft prompt. The FedPepTAO framework selectively updates a subset of prompt parameters to communication overhead between clients and the server. The selection criteria is based on the eigenvalues of Hessian matrix of the model. More specifically, change in prompt parameters and the Hessian matrix for the current model are calculated on each device. Then, the eigenvalues of the Hessian matrix are sorted to identify the most critical layers. A base function is constructed, and the Lipschitz constant of this function is calculated to determine the set of prompt layers that meet a specific constraint. The selected prompt layers is aggregated across clients to optimize communication efficiency without performance loss. Besides, the adaptive optimization technique to handle client drift due to the non-independent or non-identically distributed data. The adaptive optimization technique allows the server to a momentum-based optimization and the clients to use Adam optimization. This combination helps in stabilizing the training process and achieving superior performance."}, {"title": "C. Hybrid PEFT in DLNOE", "content": "A comprehensive framework (namely, FedPETuning) is designed to integrate multiple PEFT techniques into the DLNOE. The PEFT methods (e.g., parallel, serial, and selective PEFT) are utilized to fine-tune only a small subset of weights so that to significantly reduce the computational and communication overhead. More specifically, the FedPETuning frameworks ensures that only lightweight weights are exchanged between clients and the server during training with most weights of the LLMs frozen. This approach addresses the primary challenges of communication overhead and local computational resource constraints. The FedPETuning framework also includes mechanisms for global aggregation and local tuning, making it adaptable to diverse scenarios of the DLNOE. Performance of the FedPETuning framework is evaluated over different DLNOE scenarios, such as the cross-silo and large-scale cross-device scenarios. In cross-silo scenario with a smaller number of participant clients, and each client has more data. The FedPETuning shows minimal performance degradation after fine-tuning. In large-scale cross-device scenario, with many clients holding smaller amounts of data, the FedPETuning framework still maintains acceptable performance while significantly reducing communication overhead. [7]"}, {"title": "III. MEMORY-EFFICIENT FULL FINE-TUNING IN DLNOE", "content": "When some advanced system-on-chips (e.g., Qualcomm Snapdragon, Samsung Exynos, Huawei Kirin, and so on) cannot support the BP operation that is the key step for the first- and the second-order optimizers, another vein of research focuses on developing optimizers without requiring the BP operation, i.e.. zeroth-order (ZO) optimizers. The classical ZO algorithms estimate gradients using only function evaluations while avoiding the need for the BP operation. For example, one of the classical ZO algorithms (i.e., the ZO-SGD algorithm) estimates each gradient by a stochastic perturbation and a scalar gradient that is calculated via the two forward passes of the loss function. Then, the ZO-SGD updates the model weights via the vanilla SGD based on the estimated gradients. However, the ZO-SGD algorithm costs twice memory as inference due to storage of both model weights and stochastic perturbation per iteration. Therefore, the classical ZO algorithms are generally considered inefficient for large models due to the double memory costs."}, {"title": "A. Memory-Efficient Full Fine-Tuning on a Single Client", "content": "The seminar work [8] develops a memory-efficient zeroth-order optimizer (MeZO). In the MeZO optimizer adapts the classical ZO-SGD to operate in-place, matching the memory footprint of inference. This is achieved by resetting the random number generator to ensure that the stochastic perturbations do not require additional memory. This adaptation allows MeZO to fine-tune models with billions of parameters using the same memory required for inference, enabling fine-tuning of much larger models than the methods with the BP operation on the same hardware. One inspiring theoretical result of the MeZO shows that the convergence rate of the MeZO depends on the local effective rank of the Hessian rather than the number of weights, which is typically much smaller [8]. The numerical results show that performance of the MeZO is comparable to traditional fine-tuning with the BP operation while the memory requirement is significantly reduced in [8]. For example, the MeZO could fine-tune a 30B model on a single 80GB GPU, while traditional methods could only handle up to a 2.7B model on the same hardware. Moreover, the MeZO can also be used to optimize non-differentiable objectives which broadens its applicability, particularly in scenarios involving human feedback or safety standards."}, {"title": "B. Pave the Way to Communication and Memory Efficiency in DLNOE", "content": "Due to the merits of MeZO, researchers are expecting to extend the MeZO to the federated learning framework where the distributed clients with private data intents to train unified model collaboratively. In this vein, Zhen Qin et al. proposes a federated full fine-tuning algorithm that can handle billion-sized LLMs with significantly reduction in communication costs-FedKSeed [9]. The FedKSeed is essential for leveraging the abundant data on client terminals while maintaining data privacy. The key challenge addressed is the immense communication cost associated with federated MEF2T in the DLNoEs. Note that the estimated gradient in MeZO can be reconstructed by a scalar gradient (that) and the stochastic perturbation, which can be regenerated via resetting the random number generator. Therefore, Fed-KSeed reduces the communication expenditure by allowing the clients to exchange only a few random seeds and the corresponding scalar gradients per iteration. As shown in Fig. 3, the server sends a set of candidate seeds and the corresponding scalar gradients to clients. Clients calculate the latest global model and perform local training, sending back the scalar gradient history to the server for aggregation. The FedKSeed also implements a finite set of random seeds to generate perturbations in order to expedite the convergence to the latest global model by narrowing the pool of random seeds such that the computational efficiency and model accuracy can be improved. The theoretical results demonstrate that the finite number of random seeds has a minimal impact on convergence. Due to the extraordinary capability for memory-efficient full fine-tuning of LLMs, the FedKSeed initializes a stage for further research in decentralized federated fine-tuning and other applications requiring efficient model tuning on client devices with limited communication and memory resources.\nIn order to improve the scalability of adding/removing machines, Eric Zelikman et al. develop an MEF2T with one-byte exchange per iteration in the DLNoEs [10]. In the one-byte MEF2T algorithm, each client uses the local data samples to calculate a scalar gradient and uploads the scalar gradient to the server. After aggregating the scalar gradients from all clients, the server shares a common random seed and the aggregated scalar gradient with all clients. Then, each client can update the local model based on the aggregated scalar gradient and a stochastic perturbation that is generated via the common random seed. In order to reduce the communication frequency, each client can perform multiple perturbations on model weights using the random seed. Each client calculate the loss changes based on these perturbations to obtain a set of scalar projected gradients. These sets of scalar gradients are aggregated by the server and broadcasts across all clients to update the models by applying these scalar gradients using the same random seeds. The one-byte MEF2T algorithm allows parallel executions of forward passes at all clients so that to improve the computational efficiency. The one-byte MEF2T algorithm can drastically reduces communication expenditure while retaining a competitive training performance compared to traditional BP based methods; and, therefore, demonstrate a significant bandwidth efficiency. Besides, the support of dynamic network configurations makes the one-byte MEF2T algorithm a promising solution for large-scale, low computational power model fine-tuning in DLNOE."}, {"title": "C. Integration of PEFT and MEF2T", "content": "Motivated by the fact the mobile devices integrate neural processing units that are more efficient for inference, Mengwei Xu et al. an FwdLLM that integrates BP-free training with PEFT techniques (e.g., LoRA, Adapter, and Soft-Prompt) in [11]. The key techniques in the FwdLLM are two-folds: 1) dynamic control of global perturbation size (global-PS), and 2) discriminative perturbation sampling.\n\u2022 A crucial trade-off exists between model convergence rate and computational cost on clients. In other words, performing more perturbations leads to a more accurate gradient estimation at the expense of move inference cost per client. To balance this, FwdLLM introduces the global-PS that is defined as the total perturbations aggregated across all clients per iteration. The effectiveness of FwdLLM hinges on selecting an appropriate global-PS. However, there is no one-size-fits-all setting for global-PS that optimizes both accuracy and cost across different scenarios. Manually controlling global-PS can be complex due to varying configurations across different models and datasets. Instead, the FwdLLM employs an automatic strategy to manage global-PS based on the observation that the numerical variance across forward gradients of clients increases as the model approaches convergence. Increased variance necessitates more perturbations to increase the precision of estimated gradients. Based on the variance of aggregate gradient, the FwdLLM can dynamically update the global-PS. Besides, one potential way to improve global-PS is to include more clients since the concurrent computation facilitates a fast convergence.\n\u2022 Based on the observation that many perturbations are almost orthogonal to the true gradient direction, finding efficient perturbations that has the largest cosine similarities becomes imperative. Since each cosine similarity equals to the scalar gradient, a practical way to obtain efficient perturbations is to allow the server to generate a set of random seeds and to pick out the perturbations with the high scalar gradients.\nBased on global-PS and discriminative perturbation sampling, the FwdLLM can effectively integrate the BP-free training with the PEFT techniques such that the efficient and scalable fine-tuning process for LLMs can be implemented on the computation- and memory-constrained clients."}, {"title": "D. A Case Study of MEF2T in DLNOE", "content": "We perform a case study to evaluate the impacts of number of seeds on the model performance when using MEF2T in DLNOE. We consider a DLNoE with 200 clients with sampling probability of each client as 5%. We fine tune the LLAMA-1B model over the Databricks-dolly-15k dataset. The number of samples per client follows the Dirichlet distribution with parameter $\\alpha$ specified during experiments. Each client uses one data sample per iteration and operates 200 iterations per round. The numerical results are illustrated in 4. We observe from Fig. 4(a) that the value of testing loss converges after about 120 rounds with the Dirichlet hyperparameter $\\alpha$ = 0.5, 1, 1.5, 2, 2.5 and the number of seeds as 700. Then, we use the fine-tuned LLAMA-1B model to test the Rouge-L F1 score to illustrate the impacts of number of seeds in Fig. 4(b). From Fig. 4(b), we can confirm that the number of seeds has little to no effect on the Rouge-L F1 scores. This observation motivates us to use small amount of seeds to expedite the fine-tuning process."}, {"title": "IV. COMPRESSING LLMS FOR EDGE DEVICES", "content": "While the fine-tuned LLMs have demonstrated remarkable performance in versatile tasks. However, the access to the emergent LLMs demands for intensive computational power and storage capacity. When storing in half-precision (FP16) format, the GPT3-175B requires around 326GB of memory which exceeds the capacity of most mainstream GPUs. Multiple GPUs are required to perform the inference task of the GPT3-175B. However, the typical edge devices with limited form factor cannot plug in multiple GPUs. In order to deploy the prestigious LLMs on edge devices, the model compression technique is required to reduce the requirements of memory. While traditional methods compresses the model size based on pruning, knowledge distillation, and quantization, they require additional retrain of the compressed models to recover the accuracy. However, retraining the GPT-scale models demand for vast amounts of data and time. Moreover, the acquisition of commercial datasets becomes even challenging since the datasets are treated as proprietary assets by many companies. Therefore, the resource-efficient compression methods need to be developed to: 1) reduce the demand for data; and, 2) reduce the retraining duration."}, {"title": "A. Compression-and-Train", "content": "In order to reduce the dependence on training data, knowledge distillation can be used to compress the teacher LLMs into smaller student models to reduce inference costs and memory usage. However, traditional knowledge distillation methods face a distribution mismatch between training outputs and those generated during inference. More specifically, the teacher LLMs have more complex output spaces than the student models due to the limited expressiveness. The forward Kullback-Leibler (FKL) divergence in traditional knowledge distillation also induces that the student models may assign high probabilities to the void regions of the expert LLMs and thereby generated unlikely samples from their corresponding teacher LLMs [12]. R. Agarwal et al. introduce the generalized knowledge distillation (GKD) algorithm that handles the aforementioned distribution mismatch based on: 1) replacing the FKL divergence by the reverse KL (RKL) divergence or generalized Jensen-Shannon (GJS) divergence; and 2) using the on-policy data that is generated by student model and guided by feedback of teacher. More specifically, the application of the RKL/GJS divergence enforces the student model to focus on the high-probability regions of the distribution of teacher model which avoiding overfitting to low-probability regions. In this way, the limited expressiveness of student models can be alleviated. Moreover, the GKD can be combined with RL fine-tuning to optimize both task-specific rewards and general model performance. Numerical results show that the GKD can substantial improvements in both factual consistency and summary quality when tested on the XSum dataset for summarization. The GKD is also evaluated for task-agnostic distillation using instruction tuning on the FLAN2021 dataset. The on-policy GKD with RKL outperformed other methods on the MMLU and BBH benchmark suites, demonstrating its versatility and effectiveness in improving model performance across diverse tasks."}, {"title": "B. Compression-Then-Train", "content": "Notwithstanding the compression capability, the knowledge distillation usually consumes a long duration to obtain a student model (e.g., 14 GPU days for distilling TinyBERT [13]) that works for specific tasks. In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery. The detailed functionalities of the LLM-Pruner are as follows.\n\u2022 Discovery stage involves finding dependencies within the model such that the pruned neurons do not disproportionately affect others.\n\u2022 Estimation stage evaluates the importance of each identified group of neurons based on their contribution to the overall performance that can be estimated via the first-order and the approximated second-order derivatives.\n\u2022 Recovery stage leverages a fast low-rank approximation method to reduce the required duration and data for performance recovery.\nAfter the above three stages, the performance pruned models can be quickly retained using the LoRA technique with a minimal dataset. Their numerical results also demonstrate that the LLM-Pruner can effectively reduce the model size and computational demands without significantly sacrificing performance. Even after pruning 20% of the parameters, the pruned models retain over 94% of the original performance."}, {"title": "C. One-Shot Compression", "content": "When training corpus is absent\u00b2, the one-shot model compression method is required to reduce the scale of LLMs without retraining. Among the one-shot model compression methods, one-shot quantization [14] and the one-shot pruning [15] stand out due to their efficiency and effectiveness when applied to LLMs.\nThe open pre-trained transformer quantization (OPTQ) achieves the one-shot model compression by leveraging the approximate second-order information to achieve high accuracy and efficiency [14]. The OPTQ begins by quantizing the weights of the model layer-by-layer. For each layer, the objective is to find a quantized weight matrix that minimizes mean-squared error between the full-precision output and the quantized ones (i.e., quantization error) under calibration data\u00b3 such that the performance loss of quantization can be mitigated. More specifically, the OPTQ recursively select a layer of weights and simultaneously quantizes each row of the selected weight matrix. The simultaneous quantization can reduce the complexity of updating the inverse Hessian leading to substantial computational savings. Note that the inverse Hessian can be indefinite due to numerical inaccuracies caused by the row-wise updates. To ensure the positive-definiteness of the inverse Hessian, the OPTQ incorporates a Cholesky decomposition of the Hessian matrix. By pre-computing the Cholesky decomposition and mild dampening, the OPTQ can efficiently quantize weights without accumulating numerical errors. To further enhance efficiency, the OPTQ employs a lazy batch-update strategy, i.e., updating the weights periodically instead of immediately after each quantization step. With the lazy batch-update strategy, the GPU utilization is improved by reducing the frequency of memory access.\nThe one-shot pruning (a.k.a., SparseGPT) recasts the pruning task as a series of layer-wise sparse regression problems via jointly selecting neuron masks and adjusting remaining weights [15]. Since the simultaneous mask selection and weight update is NP-hard, the SparseGPT prunes neurons and adjusts the remaining weights via the two major procedures as 1) the neurons are adaptively pruned based on the ranking of the optimal brain surgeon errors; and, 2) the layer-wise pruning task is to minimize the mean-squared error between the output of the original and pruned layers. Note that each optimal brain surgeon error contains the inverse of the Hessian matrix that captures the curvature of the loss landscape with respect to the weights. When the model size is large, it becomes prohibitive to calculate the exact inverse of the Hessian matrix. Therefore, an approximate inverse Hessian matrix is calculated for each row in a layer-wise fashion during the pruning process. Besides, the pruning and quantization techniques can also be integrated into a one-shot model compression method that can coordinate the pruning and quantization processes to optimize both the number of weights and their representations efficiently."}, {"title": "V. CONCLUSIONS AND FUTURE RESEARCH DIRECTIONS", "content": "Since the huge demand for computational power and storage volume for inference and deployment of LLMs, the memory efficient fine-tuning methods and model compression are required to scale the accessibility of the LLMs to the terminal clients. In this work, we provided a comprehensive overview of the recent advances on PEFT, MEF2T, and model compression to highlight their main contributions. We conclude our work with some potential future directions that deserve research efforts.\nPrivacy-preserving. While the fine-tuning algorithms in DLNOE do not rely on exchanging the training data, the LLMs tend to memorize the training data. The memorized data pose a risk of inadvertently revealing sensitive information from the fine-tuning datasets. This risk is critical when fine-tuning models on user-specific or domain-specific data. Therefore, it is necessary to develop fine-tuning algorithm that can differential privacy.\nData heterogeneity. Since the higher level of data heterogeneity induces a more significant performance gap between fully fine tune and parameter-efficient fine tune. LoRA fail to reach close to the performance of fully fine tune since the random initialization of the bottleneck structure slows down the convergence. A potential future direction for MEF2T in the DLNOE is to design the initializers for the bottleneck module in order to speed up the maturity of LoRA module."}]}