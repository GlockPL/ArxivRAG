{"title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity", "authors": ["Junhao Hu", "Wenrui Huang", "Weidong Wang", "Zhenwen Li", "Tiancheng Hu", "Zhixia Liu", "Xusheng Chen", "Tao Xie", "Yizhou Shan"], "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities across various domains, with recent advancements in challenging reasoning tasks such as mathematics and programming. However, solving reasoning tasks often requires long decoding chains (of thoughts), which incur O(N) time and memory consumption, where N is the chain length. To mitigate O(N) time and memory consumption, existing sparsity-based algorithms propose retaining only the most critical token's intermediate data (i.e., key-value cache) and discarding the rest. However, these existing algorithms struggle with the \"impossible trinity\" of accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest, achieves high accuracy with O(L) time but O(N) memory (L is the cache budget, L\u226aN). To address this issue, in this paper, we identify a new attention pattern during the decode stage of reasoning tasks, where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are utilized, and then become unimportant afterward. Based on this pattern, we propose a new algorithm named RaaS that identifies and retains milestone tokens only until they are no longer needed, achieving high accuracy with O(L) time and O(L) memory complexity.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have gained widespread adoption due to their exceptional performance and versatility across various applications. However, a significant challenge to large-scale deployment and application is the high computational cost associated with long-context inference. LLMs must process an entire prompt during the prefill stage and then generate tokens autoregressively during the decode stage. Both stages require significant processing time and memory for intermediate data, specifically the Key-Value (KV) cache. For standard attention algorithms (also referred to as Dense (Chen et al.) algorithms), the time and memory complexity is O(N), where N is the sequence length. For example, in the Llama 3.1 8B model, sequences can grow up to 128k tokens, resulting in potentially thousands of seconds of processing time and 16GB of KV cache for a single request.\nThere are two primary types of long-context inference. The first type is long-prefill inference, commonly encountered in Retrieval-Augmented Generation (RAG) tasks, where the used LLM is required to process a lengthy prompt before generating responses. Previous research (Hu et al., 2024; Zheng et al., 2024; Kwon et al., 2023; Jin et al., 2024; Bai et al., 2024) has primarily focused on this inference type. The second type is long-decode inference, which has recently gained prominence in reasoning tasks, such as those exemplified by OpenAI's models (OpenAI) (e.g., 01, 03) and DeepSeek R1 (Dai et al., 2024). In reasoning tasks, the decode stage accounts for 99% of the Job-Completion-Time (JCT) (Figure 1), becoming a critical bottleneck.\nTo mitigate high time and memory consumption in long-prefill scenarios, existing sparsity-based algorithms (Tang et al., 2024; Zhang et al., 2023; Xiao et al., 2024b) propose retaining only the most critical token's KV and discarding the rest. However, when directly applied in long-decode scenarios, these existing algorithms struggle with the \"impossible trinity\" of accuracy, time, and memory (Figures 2 (b)(c)(d)). For example, the state-of-the-art algorithm, Quest (Tang et al., 2024), achieves high accuracy with O(L) time but O(N) memory, where L is the cache budget and L \u226a N.\nTo achieve high accuracy and O(L) time/memory complexity at the same time, we analyze the attention pattern during the decode"}, {"title": "2 Background and Motivation", "content": "In this section, we overview the Large Language Model (LLM) inference, highlighting the key concepts and challenges that motivate our work."}, {"title": "2.1 Autoregressive Generation & KV Cache", "content": "LLMs generate tokens autoregressively, predicting one token at a time based on the input. This process involves two stages: the prefill stage and the decode stage. In the prefill stage, LLMs process the entire input prompt (x1,x2,...,xn), computing and caching the key and value vectors for each token. This stage can be slow for long inputs, and the time to generate the first token is measured by the Time-to-First-Token (TTFT) metric. In the decode stage, LLMs generate one token at a time. The model computes the probability of the next token Xn+1, selects the most likely token, and appends its key and value vectors to the KV cache.\nThe KV cache (Pope et al., 2023), which stores the key and value vectors computed by the attention module, accelerates generation by allowing the model to process only the new token instead of recalculating KV for the entire sequence. With the KV cache, the attention mechanism exhibits a computational complexity of O(N) for one decoding step and a memory complexity of O(N) for storing the KV cache, where N is the number of tokens or the sequence length."}, {"title": "2.2 Cost Transfer: from Long-Prefill to Long-Decode Inference", "content": "Long-context inference incurs significant costs due to both memory and time requirements. First, it demands substantial memory resources, reaching up to 16 GB KV cache (in addition to the 16 GB model parameters) for processing 128k tokens running the LLaMA 3.1 8B model in FP16 precision\u00b9."}, {"title": "2.3 Existing Sparsity-Based Algorithms", "content": "To reduce memory and time complexity in long-prefill scenarios, one line of research proposes sparsity-based algorithms (Xiao et al., 2024b; Zhang et al., 2023; Tang et al., 2024; Chen et al.). Sparsity-based algorithms propose retaining only the most critical tokens' (fewer than 10% (Tang et al., 2024)) KV and discarding the rest. However, when directly applied in long-decode scenarios, existing algorithms struggle with the \u201cimpossible trinity\" of accuracy, time, and memory (Figure 2 (b)(c)(d)).\nThe differences among existing algorithms are shown in Figure 2. First, the Dense or the standard attention algorithm (Vaswani et al., 2017) achieves the highest accuracy but incurs the highest time and memory complexity. Second, StreamingLLM or Sink (Xiao et al., 2024b) retains only the KV cache of the initial and final tokens, resulting in low time and memory complexity, but this extreme approach leads to low accuracy on reasoning tasks (and other tasks (Tang et al., 2024)). Third, H2O (Zhang et al., 2023) theoretically offers low time and memory complexity, but its inability to utilize efficient attention kernels and the lack of page-level KV management makes it impractical, resulting in both low accuracy and infeasibility. Fourth, Quest achieves high accuracy and low time complexity but conservatively retains all KV cache, thus O(N) memory complexity."}, {"title": "3 Algorithm Design", "content": "When directly applied to reasoning tasks, existing algorithms struggle with the \u201cimpossible trinity\" of accuracy, time, and memory. For example, although the state-of-the-art Quest (Tang et al., 2024) achieves promising accuracy (Figure 6) with O(L) time complexity, it requires storing the entire KV cache, thus O(N) memory complexity (Figure 7). To break the \"impossible trinity,\" we analyze the decoding stage of reasoning tasks and discover a new attention pattern (Section 3.1), based on which we design a new algorithm RaaS (Section 3.2) that achieves O(L) time and memory complexity, with"}, {"title": "3.1 Reasoning Attention Pattern", "content": "By analyzing the attention map of the decoding stage of reasoning tasks, we discover two key characteristics (Figure 3). First, we identify milestone tokens, which initially exhibit high attention scores but gradually receive lower scores and never receive high scores again. Analogous to lemmas in mathematical proofs, milestone tokens emerge, are utilized, and then fade away. These tokens, visible as bright columns on the attention map that slowly diminish similar to a water column in a waterfall (Figure 3 (a)) must be carefully managed to prevent significant accuracy loss (Figure 6).\nSecond, we identify phoenix tokens, which receive low attention scores for a period long enough to be evicted from the cache but later regain importance. These tokens typically appear in short prefill prompts, such as user queries (Figure 3 (b)). Quest (Tang et al., 2024) retains the entire KV cache to avoid losing phoenix tokens, thus the O(N) memory complexity.\nWe offer a possible explanation for the waterfall pattern or milestone tokens in reasoning tasks. First, the emergence of milestone tokens is analogous to lemmas in mathematical proofs or subconclusions in thinking steps. Once an LLM generates milestone tokens, subsequent tokens primarily attend to the milestone tokens rather than the preceding tokens arriving at the milestone token. Second, the fading attention score of a milestone token mirrors the progression in mathematical reasoning. As reasoning advances from lower-level lemmas to higher-level ones, subsequent steps rely on the new lemmas rather than revisiting the older ones.\nTo illustrate the preceding explanation, consider one example\u00b3 in Figure 4. First, tokens \u2460\u2461\u2462 serve"}, {"title": "3.2 Design of RaaS", "content": "Based on the preceding observations, we propose RaaS, a simple yet effective algorithm that addresses the \"impossible trinity\" and consists of two main ideas. First, we identify and retain milestone tokens only until they are no longer needed, using timestamps to track their importance. When a token receives an attention score above a (e.g., a = 0.01), we assign it the latest timestamp. Milestone tokens always receive the latest timestamp until it becomes unimportant. When the cache is full, we evict tokens with the oldest timestamp. Second, we retain the KV cache of all prefill tokens without eviction. Since prefill tokens are typically short and phoenix tokens almost always appear within them in reasoning tasks, retaining these tokens ensures that critical information is not lost during the decoding process.\nTo illustrate RaaS step by step, consider Figure 5. In the first five steps, the cache size limit is not reached (we can store at most 5 tokens). In the 4-th row, the second token is cached and computed, but its timestamp is not updated because its attention score is below a, which we consider insufficient for influencing the final result. However, in the 5th row, the second token's attention score exceeds a, and it is assigned the latest timestamp, 5. In the last three steps, the cache is full. In the 6th row, we evict the third token (the third column becomes gray) since it has the oldest timestamp. We then compute the remaining tokens and update their timestamps."}, {"title": "3.3 Page-Based RaaS", "content": "Directly applying the version of RaaS in Section 3.2 presents two challenges. First, managing KV caches at the token level is inefficient, as small gaps in the cache complicate memory management and hinder GPU computation. Second, RaaS requires the attention scores of all tokens to update timestamps, but retrieving these scores is incompatible with optimized attention kernels like FlashAttention (Dao et al., 2022; Dao, 2024). As with H2O, bypassing fast kernels in favor of RaaS could result in degraded performance.\nTo address these challenges, we propose a page-based version of RaaS4. First, we introduce a page-based caching system with a fixed page size of page_size = 16. The timestamp management, as well as cache retention and eviction, is handled at the page level as in most of the modern inference engine (Kwon et al., 2023; Zheng et al., 2024). Second, before using optimized attention kernels, we add a lightweight step to retrieve a representative attention score for each page to update its timestamp, similar to Quest. We select a representative key (K) for each page, and the query (Q) of the new decoding token attends to these representative keys"}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Experiment Setup", "content": "We implement RaaS based on Hugging Face (Hugging Face) and Quest (Tang et al., 2024) with 2K lines of code. We port Quest from their public repository. Next, we discuss the datasets, models, metrics, and the environment in which we carry out experiments.\nDataset. We take the first 200 questions from each of the following three open-sourced datasets for our benchmarks: GMS8k (Cobbe et al., 2021), MATH500 (Hendrycks et al., 2021), and AIME (AIME), to test the reasoning ability of language models. First, GMS8k (Cobbe et al., 2021) contains 8.5k high-quality, linguistically diverse grade school math word problems. These human-written problems need solutions that involve multi-step reasoning and a series of basic arithmetic operations. Second, MATH500 (Hendrycks et al., 2021) contains 500 challenging problems sourced from high school mathematics competitions with five distinct levels based on the Art of Problem Solving (AoPS) framework, ranging from level 1 to level 5. Third, AIME (AIME) is a math problem dataset collected from the AIME (American Invitational Mathematics Examination) competition from 1983 to 2024, designed to challenge the most exceptional high school mathematics students in the United States. These problems cover various fields, such as algebra, geometry, and number theory.\nMetrics. We use two metrics to evaluate performance and model accuracy. First, Job Completion Time (JCT) is the time from when users send a request (a prompt) to LLMs to when users receive a complete response. A smaller JCT indicates a faster algorithm. Second, Accuracy (Wang et al., 2024) measures the mathematical equivalence between an LLM's output and the ground-truth answer. For each data point, it is either correct or incorrect, and the overall accuracy is reported as"}, {"title": "4.2 Accuracy and Cache Budget Trade-off", "content": "We evaluate five algorithms across three datasets and four models, yielding three key insights from the experimental results (Figure 6). First, H2O and StreamingLLM exhibit poor accuracy under fixed cache budgets compared to others. StreamingLLM indiscriminately discards important tokens, including milestone tokens. H2O, on the other hand, overemphasizes accumulated historical attention scores, leading it to retain outdated milestone tokens for too long while discarding newer, relevant ones. Second, Quest and RaaS achieve the best accuracy. Quest retains all KVs while RaaS optimizes memory usage by specifically handling milestone tokens and using only O(L) memory (Figure 7). Across these datasets, a cache budget of 1024 tokens is generally sufficient to match Dense's accuracy. Third, when the cache budget is small, RaaS underperforms because RaaS retains all prefill tokens, and with a limited cache budget, most of the budget is allocated to prefill tokens, causing almost all decoding tokens to be discarded, which negatively impacts accuracy. For small cache budgets or long-prefill scenarios, we recommend using Quest"}, {"title": "4.3 Latency/Memory vs. Decoding Length", "content": "We evaluate the Dense, Quest, and RaaS in terms of their latency and memory consumption, yielding two key observations from the experimental results (Figure 7). First, as the number of decode tokens increases, Dense's latency grows quadratically, while both RaaS and Quest exhibit linear latency growth. This is because Dense has O(N^2) computation time, whereas RaaS and Quest have O(NL) computation time, reducing each decoding step to O(L). Second, as the number of decode tokens increases, the memory consumption of Dense and Quest grows linearly, while RaaS initially increases linearly but plateaus once the"}, {"title": "4.4 Micro-Benchmarks", "content": "The Impact of Discarding Milestone Tokens. Figure 8 shows that discarding milestone tokens, as in H2O-128 and Sink-128, increases the decoding lengths. Analysis of the outputs reveals that while the model initially reasons correctly for the first few tokens (e.g., green tokens in Figure 8), it loses track (orange tokens) of the reasoning process when milestone tokens are discarded, leading to repeated attempts at re-reasoning (red tokens), which ultimately results in the model getting stuck indefinitely.\nThe Impact of a. The choice of a affects the distribution of tokens' timestamps, with a = 0.0001 generally yielding optimal results, as shown in Figure 9. First, when a is small, too many tokens are assigned the latest timestamp, preventing effective differentiation of milestone tokens. Second, when a is large, most tokens are deemed irrelevant, potentially leading to the loss of milestone tokens."}, {"title": "5 Related Work", "content": "Several approaches have been proposed to reduce the computation and memory footprint of the KV cache during long-context inference. These can be categorized into two types: one that requires modifying model architecture, and the other that is more plug-and-play."}, {"title": "5.1 Model Architecture", "content": "Two types of approaches have emerged for altering model architecture. First, some approaches modify the inner workings of the Transformer while retaining its overall structure. For example, Muti-Query Attention (MQA) (Shazeer, 2019) and Group-Query Attention (GQA) (Ainslie et al., 2023) reduce the number of KV heads, achieving similar accuracy to full-head configurations. Second, some efforts discard the Transformer architecture entirely in favor of alternative models. Approaches such as Linear Attention and RNN-based models, including RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), and Mamba (Gu and Dao, 2023), offer lower computational and memory costs. However, these models typically underperform compared to Transformer-based models in long-context scenarios."}, {"title": "5.2 KV Compression", "content": "The change of model architecture often requires significant pretraining or fine-tuning, whereas plug-and-play approaches, such as KV compression, are typically preferred in major application scenarios. Two primary types of KV compression have emerged: KV quantization and KV pruning.\nKV Quantization. KV quantization approaches (Xiao et al., 2023; Yao et al., 2022; Dettmers et al., 2022) map higher precision KVs into lower ones, trading accuracy for savings in computation and memory. Recent studies have shown that due to the distinct element distributions in the KV cache, key and value caches require different quantization strategies to optimize performance on complex tasks (Zirui Liu et al., 2023).\nKV Pruning. KV pruning approaches focus on leveraging attention sparsity (Zhang et al., 2023; Ge et al., 2024; Jiang et al., 2024; Cai et al., 2024; Fu et al., 2024; Xiao et al., 2024a), which posits that only around 5% of tokens are crucial during LLM inference. Thus, evicting less important tokens from the KV cache is a key strategy for reducing memory usage and accelerating inference. For example, StreamingLLM (Xiao et al., 2024b) and LM-Infinite (Han et al., 2024) evict fixed-position tokens, retaining only the initial and recent window tokens. H2O (Zhang et al., 2023), SnapKV (Li et al., 2024), ScissorHands (Liu et al., 2023) and TOVA (Oren et al., 2024) keeps the recent tokens and the top-k important tokens based on the attention score calculated within a local window. More recent works, such as Quest (Tang et al., 2024) and ARKVALE (Chen et al.), manage the KV cache at the page level, selecting the top-k important pages during each generation step to reduce computational time.\nOur work presents a new trial of applying KV pruning in reasoning tasks, where the inference pattern is characterized by a short prefill and a long decode with a waterfall attention pattern. For the first time, it achieves true O(L) time and memory complexity with high accuracy."}, {"title": "6 Conclusion", "content": "In this paper, we have identified a new waterfall attention pattern observed in the decode stage of reasoning tasks. Leveraging this pattern, we have proposed a sparsity-based algorithm named RaaS that achieves high accuracy while maintaining O(L) time and O(L) memory complexity. Our exper-"}, {"title": "Limitations", "content": "Our work in this paper has the following major limitations.\nLimited applicability of RaaS. RaaS is specifically designed for traditional reasoning tasks where a question is short (e.g., a mathematical query) but its answer is lengthy (e.g., a chain of reasoning followed by a final answer). The waterfall attention pattern primarily occurs during the decode stage, and phoenix tokens are frequently found in the question (prefill tokens). Thus, RaaS may lose crucial information when applied in other scenarios where the number of prefill tokens exceeds a certain threshold. In this case, we recommend using the combination of Quest (on prefill tokens) and RaaS (on decode tokens).\nEvaluation on a limited set of models. Our evaluation is based on only four models. The presented results may be specific to these models and may not generalize to others. Although there are models with larger context lengths, such as Qwen2.5-Max and DeepSeek-r1, conducting a comprehensive evaluation across all such models is time- and resource-intensive. As noted by previous work (Zhong et al., 2024), if decoding a single token takes around 30 ms, decoding 16k tokens (a relatively small request) requires approximately 8 minutes on a single A100-80GB GPU. Evaluating 200 data points would take more than one day, being infeasible with only one available A100 GPU. Nonetheless, we believe that the core idea\u2014the waterfall pattern and its underlying rationale\u2014 remains broadly applicable, and we plan to conduct additional experiments in the future.\nEvaluation on limited inference lengths. Due to resource and time constraints, we are unable to conduct experiments with extremely long inference lengths, as this experimental setup would require months to complete end-to-end evaluations. However, through small-scale experiments, we observe that the waterfall attention pattern is universal across varying inference lengths.\nLack of exploration of representative selec-"}, {"title": "A More Examples for Milestone Tokens", "content": "This section presents more examples of milestone tokens in Figure 10 and Figure 11. For all examples, we input the prefill tokens to Qwen 2.5 Math 7B Instruct and obtain the corresponding decode tokens, as shown in the figure. The tokens marked red represent the milestone tokens or \u201cwater columns\" in the attention map. Although we only show a few examples here, the milestone tokens or waterfall patterns abound in reasoning tasks.\nPrompt: Define\n\\[p = \\sum_{k=1}^\\infty \\frac{1}{k^2} \\quad \\text{and} \\quad q =\n\\sum_{k=1}^\\infty \\frac{1}{k^3}.\\]Find a way to write\n\\[\\sum_{j = 1}^\\infty \\sum_{k=1}^\\infty \\frac{1}{(j + k)^3}\\]in terms of\n$p$ and $q.$\nDecode\nTo find a way to write the double sum \\(\\sum_{j=1}^\\infty\\sum_{k=1}^\\infty \\frac{1}{(j+k)^3}\\) in terms of \\(p\\) and \\(q\\), we start\nby considering the sum in a different form. We can rewrite the double\nsum by changing the order of summation. Instead of summing over\n\\(j\\) and \\(k\\) directly, we sum over \\(n = j+k\\) and then over the\npossible values of \\(j\\) and \\(k\\) that satisfy \\(j+k=n\\). \u2460\nThe sum \\(\\sum_{i=1}^\\infty \\sum_{k=1}^\\infty \\frac{1}{(j+k)^3}\\) can be\nrewritten as:\n\\[\\sum_{n=2}^\\infty \\sum_{i=1}^{n-1} \\frac{1}{n^3}. \\]\nHere, the inner sum \\(\\sum_{i=1}^{n-1} \\frac{1}{n^3}\\) is simply\n\\(\\frac{n-1}{n^3}\\) because there are \\(n-1\\) terms in the sum, each\nequal to \\(\\frac{1}{n^3}\\). Therefore, the double sum becomes:\n\\[\\sum_{n=2}^\\infty \\frac{n-1}{n^3} = \\sum_{n=2}^\\infty \\left(\n\\frac{1}{n^2} - \\frac{1}{n^3} \\right). \\]\nWe can split this into two separate sums:\n\\[\\sum_{n=2}^\\infty \\frac{1}{n^2} - \\sum_{n=2}^\\infty \\frac{1}{n^3}. \\]\nThe first sum \\(\\sum_{n=2}^\\infty \\frac{1}{n^2}\\) is the same as \\(p - 1\\)\nbecause \\(p = \\sum_{n=1}^\\infty \\frac{1}{n^2}\\). The second sum\n\\(\\sum_{n=2}^\\infty \\frac{1}{n^3}\\) is the same as \\(q - 1\\) because \\(q =\n\\sum_{n=1}^\\infty \\frac{1}{n^3}\\). Therefore, the expression becomes:\n\\[ (p-1) - (q - 1) = p - q. \\]\nThus, the double sum \\(\\sum_{i=1}^\\infty \\sum_{k=1}^\\infty\n\\frac{1}{(j+k)^3}\\) in terms of \\(p\\) and \\(q\\) is:\n\\[\\boxed{p - q}. \\]"}, {"title": "B System Implementation", "content": "We implement RaaS based on Hugging Face (Hugging Face) and Quest (Tang et al., 2024). Our implementation extends the standard transformer architectures (Dense) by incorporating the following additional components.\nFirst, for each page in the KV cache, a representative key is selected to compute the estimated attention score between the page and the query. To ensure consistency in evaluation, our page-based RaaS adopts the same representative selection algorithm as that used in Quest.\nSecond, a timestamp is associated with each KV page to record the last time at which the page received an estimated attention score exceeding the threshold a (e.g., a = 0.01). The term time here refers to any monotonically increasing attribute. In"}, {"title": "C Checklist-Related Issues", "content": "Three datasets GSM8k (MIT), MATH500 (MIT), AIME (MIT), and four models Mistral Math 7B (No licence), Qwen 2.5 Math 7B Instruct (apache-2.0), Marco o1 (apache-2.0), DeepScaleR 1.5B Preview (MIT) are used with their intended usage scenarios. We retrieve all models and datasets from Hugging Face7, where detailed documentation, including parameter sizes and model architectures, is provided. We manually checked the data and believe there is no personal information misused."}]}