{"title": "PARTIAL CHANNEL DEPENDENCE WITH CHANNEL MASKS FOR TIME SERIES FOUNDATION MODELS", "authors": ["Seunghan Lee", "Taeyoung Park", "Kibok Lee"], "abstract": "Recent advancements in foundation models have been successfully extended to the time series (TS) domain, facilitated by the emergence of large-scale TS datasets. However, previous efforts have primarily focused on designing model architectures to address explicit heterogeneity among datasets such as various numbers of channels, while often overlooking implicit heterogeneity such as varying dependencies between channels. In this work, we introduce the concept of partial channel dependence (PCD), which enables a more sophisticated adjustment of channel dependencies based on dataset-specific information. To achieve PCD, we propose a channel mask that captures the relationships between channels within a dataset using two key components: 1) a correlation matrix that encodes relative dependencies between channels, and 2) domain parameters that learn the absolute dependencies specific to each dataset, refining the correlation matrix. We validate the effectiveness of PCD across four tasks in TS including forecasting, classification, imputation, and anomaly detection, under diverse settings, including few-shot and zero-shot scenarios with both TS foundation models and single-task models. Code is available at https://github.com/seunghan96/CM.", "sections": [{"title": "1 INTRODUCTION", "content": "Foundation models (FMs) have emerged in various domains (Touvron et al., 2023; Rombach et al., 2022; Kirillov et al., 2023), including the time series (TS) domain (Goswami et al., 2024; Liu et al., 2024b). These models are pretrained on diverse datasets and are designed to solve multiple tasks using a single model. Directly applying FMs to TS is, however, challenging due to the heterogeneity among TS datasets (Goswami et al., 2024; Woo et al., 2024), so that various time series foundation models (TSFMs) have been proposed. While these approaches mainly focus on explicit heterogeneity, where datasets differ in observable characteristics such as varying sequence lengths and number of channels in TS, they tend to overlook implicit heterogeneity, which involves unobservable factors such as differences in inter-channel dependencies. Furthermore, these methods address heterogeneity by modifying the model architecture, often overlooking the inherent characteristics of the dataset.\nMultivariate time series (MTS) forecasting has been explored with two different strategies: the channel-dependent (CD) strategy and the channel-independent (CI) strategy, with the former emphasizing inter-channel dependencies, while the latter ignoring these dependencies and dealing with channels individually. However, most previous works have focused on the model architecture to either capture or disregard CD, often overlooking the potential differences in CD across datasets.\nIn this paper, we consider the implicit heterogeneity among TS datasets when building a TSFM, specifically the varying CD across datasets, as opposed to prior TSFMs that mainly address the explicit heterogeneity and TS forecasting models that focus solely on adjusting the model architecture to capture CD. We argue that addressing this implicit heterogeneity is crucial for TSFMs because assuming a uniform CI or CD model across all datasets can be problematic due to the varying CD across datasets, as shown in Figure 1."}, {"title": "2 RELATED WORKS", "content": "MTS forecasting models can be categorized into CI and CD models, where CI models process channels independently without accounting for dependencies between them, whereas CD models account for these dependencies. For CI models, DLinear (Zeng et al., 2023) employs a linear model along the time dimension, and PatchTST (Nie et al., 2023) divides TS into patches and feeds them into a Transformer (Vaswani et al., 2017) in a CI manner, and PITS (Lee et al., 2024) combines channel independent and patch independent architectures with multi-layer perceptrons (MLPs). For CD models, Crossformer (Zhang & Yan, 2023) employs a two-stage attention mechanism to capture both temporal and channel dependencies and TSMixer (Chen et al., 2023) utilizes MLPs combined with patching to capture both dependencies. Recently, iTransformer (Liu et al., 2024a) inverts the traditional Transformer framework in TS domain by treating each channel as a token instead of each patch, thereby shifting the focus from capturing temporal dependencies to channel dependencies. However, these models primarily focus on architectural solutions for handling CD and often overlook the characteristics of TS datasets, motivating us to consider CD varying across datasets.\nTS foundation models often borrow knowledge from other fields, such as natural language processing, primarily due to the lack of large-scale datasets in the TS domain. In response to this challenge, there have been efforts to adapt large language models (LLMs) for TS tasks: GPT4TS (Zhou et al., 2023) fine-tunes the embedding layers of LLMs and Time-LLM (Jin et al., 2024) aligns TS data with LLM-based text prototypes to address TS tasks. Recent works have focused on pretraining TSFMs exclusively on TS datasets from various sources. MOMENT (Goswami et al., 2024) and Timer (Liu et al., 2024b) collect extensive and heterogeneous sets of TS datasets to pretrain Transformer-based TSFMs, while MOIRAI (Woo et al., 2024) enhances the Transformer architecture to address domain-specific challenges in constructing TSFMs. UniTS (Gao et al., 2024) proposes a TSFM that handles various tasks with a single architecture through prompt-tuning. However, these models do not account for the heterogeneity among datasets in terms of CD, while different TS datasets exhibit varying degrees of CD. This motivates us to adjust CD in TSFMs based on the characteristics of each dataset."}, {"title": "3 METHODOLOGY", "content": "In this section, we introduce a channel mask (CM), a simple yet effective method for achieving PCD. A CM employs a correlation matrix to capture relative dependencies between channels and adjusts it with domain parameters to learn absolute dependencies specific to each dataset. We also introduce a new metric, the channel dependence ratio (CD ratio), which uses a CM to quantify the degree of CD for each dataset. The overall framework of a CM is illustrated in Figure 2."}, {"title": "3.1 COMPONENTS OF CHANNEL MASK", "content": "As shown in Figure 2, a CM consists of two components: 1) correlation matrix (R) between channels, and 2) domain parameters ($\\alpha$ and $\\beta$), which scale and shift the matrix according to the dataset's characteristics, along with a sigmoid function to normalize the values between 0 and 1.\nCorrelation matrix. Correlation measures the relationships between channels and has been used in previous works to analyze CD (Yang et al., 2024; Zhao & Shen, 2024). Building on these approaches, we employ a correlation matrix (R) between channels to create a CM. However, high correlation does not always indicate a strong positive relationship, as the values range from -1 to 1, with strong negative relationships near \u20131. To address this issue, we use the absolute value of the matrix |R|.\nDomain parameters. We argue that |R| alone might be insufficient for modeling a CM for the following reasons: First, correlation is a relative measure that depends on the dataset. As shown in the first panel of Figure 3, different datasets exhibit different distributions of the elements of |R|. To align these differences, we normalize |R| by subtracting the mean value, resulting in $\\bar{R}$, as shown in the second panel of Figure 3. Second, the relationship between correlation and CD may vary across datasets (i.e., the same correlation can correspond to different levels of CD depending on the dataset). To deal with this discrepancy among datasets, we introduce two learnable domain parameters, $\\alpha$ and $\\beta$, which scale and shift |R|, respectively, as shown in the third panel of Figure 3. Using these parameters along with a sigmoid function, we model a CM for achieving PCD as $M = \\sigma(\\alpha\\cdot \\bar{R} + \\beta)$."}, {"title": "3.2 CHANNEL MASK WITH ATTENTION MATRIX", "content": "The proposed CM adjusts the CD estimated by the model by performing element-wise multiplication with the attention matrix of Transformers, with the general adjustment modeled by A:\n$Attn(Q, K, V) = Softmax(A \\odot \\frac{Q K^T}{\\sqrt{d_k}}) \\cdot V$, where $A = \\begin{cases} I_{CxC} & \\text{if CI,} \\\\ 1_{CxC} & \\text{if CD,} \\\\ M = \\sigma(\\alpha\\cdot \\bar{R} + \\beta) & \\text{if PCD,} \\end{cases}$ (1)\nand C is the number of channels. Note that Equation 1 incorporates both CI and CD frameworks within a single expression: As shown in Figure 2, A is the identity matrix ($I_{CxC}$) in the CI framework, while A is a matrix of ones ($1_{CxC}$) in the CD framework. In contrast, our PCD framework represents A as $M = \\sigma(\\alpha\\cdot \\bar{R} + \\beta)$, enabling a more refined adjustment of CD tailored to the dataset."}, {"title": "3.3 CHANNEL DEPENDENCE RATIO", "content": "To quantify the degree of CD for each dataset, we propose to measure the channel dependence ratio (CD ratio), a metric based on a CM. The CD ratio of M, denoted as r(M), is the average of the off-diagonal elements of M, excluding the autocor-relations of their respective channels. This metric yields a value of 0 for CI cases and 1 for CD cases, with higher values indicating a greater preference for interaction between channels. Figure 5 shows the visualization of M and its corresponding CD ratio for ETTh1 (Zhou et al., 2021), with a ratio of 0.717 for PCD. We find that M effectively captures the degree of CD for each dataset, as datasets with higher r(M) tend to have greater performance gains with CD architecture compared to CI architecture, as illustrated in Figure 7."}, {"title": "4 EXPERIMENTS", "content": "We demonstrate the effectiveness of our method in both single-task and multi-task scenarios under supervised (SL) or self-supervised (SSL) settings, where we employ iTransformer (iTrans.) (Liu et al., 2024a) for single-task SL, TimeSiam (Dong et al., 2024) for single-task SSL, and UniTS (Gao et al., 2024) for multi-task SSL. As shown in Table 1, we consider four different tasks: forecasting (FCST), classification (CLS), imputation (IMP), and anomaly detection (AD), across various dataset sizes including few-shot and zero-shot settings. As evaluation metrics, we use the mean squared error (MSE) and mean absolute error (MAE) for FCST and IMP, accuracy (Acc.) for CLS, and F\u2081 score for AD. Dataset statistics and implementation details can be found in Appendix A and B, respectively."}, {"title": "4.1 SINGLE-TASK MODEL: APPLICATION TO ITRANSFORMER", "content": "To demonstrate the effectiveness of our method, we apply our method to iTransformer (Liu et al., 2024a) to solve TS forecasting tasks on 13 datasets. Table 2 presents the average MSE and MAE across four different horizons (H), showing consistent improvement across all datasets. Specifically, the performance gains in MSE on the PEMS datasets (Liu et al., 2022) (03, 04, 07, 08) are significantly large (12.7%, 19.0%, 19.6%, 40.2%), whereas the gains on the ETT datasets (Zhou et al., 2021) (h1, h2, m1, m2) are relatively small (2.8%, 0.3%, 2.5%, 1.4%), suggesting a potential variation in the need for a CM across different datasets. Full results are described in Appendix C.1."}, {"title": "4.2 MULTI-TASK MODEL: APPLICATION TO UNITS", "content": "To validate the effectiveness of our method on a TS foundation model, we apply it to UniTS (Gao et al., 2024) which solves diverse tasks without the need for fine-tuning, relying solely on prompt-tuning."}, {"title": "4.2.1 FORECASTING AND CLASSIFICATION TASKS", "content": "Table 4 presents a summary of the results from 20 forecasting tasks and 18 classification tasks under both supervised (Sup.) and prompt-tuning (PT) settings, with the full results for both tasks provided in Table 3 and Appendix D.1, respectively. The results indicate that applying our method improves performance in all 20 FCST and 13 CLS tasks. Notably, our method outperforms task-specific models that are individually trained for each task, while our model remains a single shared model capable of solving various tasks without fine-tuning. Additionally, compared to GPT4TS (Zhou et al., 2023), which is a TSFM that reprograms the pretrained GPT-2 model (Radford et al., 2019), our method achieves superior performance with less than 1% of the parameters (1.64.5M vs. 1.57M)."}, {"title": "4.2.2 FEW-SHOT LEARNING", "content": "For the tasks under the few-shot settings, we conduct four different tasks (FCST, CLS, IMP, AD), following the experimental settings of UniTS. Full results are described in Appendix D.2.\nFew-shot FCST and CLS. We experiment nine forecasting tasks and six classification tasks under the few-shot settings with data ratios of 5%, 15%, and 20%. Table 5a presents the results, which indicates that our method outperforms both iTransformer and UniTS in both PT and fine-tuning (FT) settings.\nFew-shot IMP. We experiment six imputation tasks under the few-shot setting with a data ratio of 10%, where the goal is to impute 25% and 50% of missing data points. Table 5b presents the results, indicating that our method outperforms UniTS and other state-of-the-art (SOTA) single-task models (Wu et al., 2023; Nie et al., 2023; Liu et al., 2024a) in both PT and FT settings.\nFew-shot AD. We experiment five anomaly detection tasks under the few-shot setting with a data ratio of 5%, where the results in Table 5c indicate that our method outperforms UniTS and other SOTA methods in both PT and FT settings."}, {"title": "4.2.3 ZERO-SHOT LEARNING", "content": "We perform TS forecasting tasks under two types of zero-shot settings: 1) Zero-shot dataset: We evaluate our model on an unseen dataset that was not included during training. 2) Zero-shot task: We assess the model's ability to predict a new forecasting horizon that was not encountered during training, by adding the mask tokens at the end of the TS to predict the desired future time steps.\nZero-shot dataset. For the TS forecasting task on unseen datasets, we evaluate our method using three datasets (NREL, 2006; McLeod & Gweon, 2013; Hyndman et al., 2008). Table 6a presents the results, demonstrating consistent improvements by incorporating CMs.\nZero-shot horizon. For the TS forecasting task with new forecasting horizons, we predict additional 384 time steps (by adding 24 masked tokens of length 16 at the end of the TS) on top of the base forecasting horizon of 96. Table 6b presents the results with four different datasets (Zhou et al., 2021; Wu et al., 2021), showing performance gains on three out of four datasets."}, {"title": "5 ANALYSIS", "content": "Effectiveness of CM. To demonstrate the effectiveness of a CM, we conduct an ablation study using the correlation matrix (Corr.) and the domain parameters (Dom.). Table 7 presents the result with 20 forecasting tasks and 18 classification tasks with UniTS under the prompt-tuning setting, indicating that incorporating both components yields the best performance. Note that, to isolate the effect of the domain parameters, we replace R with the identity matrix (I) in the forth row of Table 7.\nCD ratio comparison. Table 8 presents the CD ratios of CMs with and without\u00b9 domain parameters (r(M) and r(|R|)), when using UniTS. The results show that while datasets with higher r(|R|) generally have higher r(M), this relationship is not consistent; for instance, Weather (Wu et al., 2021) exhibits lower CD despite having a stronger correlation compared to ETTh1 (Zhou et al., 2021). Figure 6 supports these findings by visualizing the channels of the datasets, revealing that the channels of ETTh1 tend to be more dependent on each other than those of Weather. These results underscore the importance of using domain parameters to adjust |R| for learning absolute dependencies specific to each dataset. Furthermore, datasets with a larger number of channels (C) tend to have higher r(M), which aligns with the prior work (Ahamed & Cheng, 2024) emphasizing CD over CI for datasets with more channels.\nEffectiveness of domain parameters. To demonstrate the importance of domain parameters in reflecting the degree of CD, we compare the CD ratio and the performance gain achieved with the CD framework against the CI framework with UniTS. Figure 7 shows that the gain is highly correlated with the CD ratio of a CM with the domain parameters (r(M)), but less so without them (r(|R|)).\nDomain parameters for unseen dataset. For an unseen dataset, selecting the appropriate domain parameters is challenging, as these parameters are not learned during training. To address this issue, we propose three strategies: 1) averaging the parameters across all datasets, 2) averaging the parameters from the forecasting datasets, and 3) selecting parameters from the dataset with the closest r(R). Table 9 demonstrates the robustness of these strategies, consistently outperforming UniTS.\nVisualization of CM. Figure 8 shows the CMs of ECL (Wu et al., 2021) and ETTh1 (Zhou et al., 2021), illustrating the dependencies between the channels of each dataset. The CM of ETTh1 reveals a hidden relationship between the first and fifth channels when using domain parameters, which is not identified by the correlation matrix alone."}, {"title": "6 CONCLUSION", "content": "In this work, we introduce the concept of PCD to adjust the CD estimated by the model using a CM, a plug-and-play method that captures both relative and absolute dependencies between channels using dataset-specific information. Our results demonstrate that incorporating prior knowledge of datasets is crucial when building TSFMs, leading to superior performance across various models and settings. However, since our method can only be applied to Transformer-based methods, which are the most widely used architecture for FMs, we aim to develop a novel approach to achieve PCD without relying on Transformer-based methods in the future. We hope our work highlights the importance of utilizing dataset-specific information when building FMs across different domains."}, {"title": "A DATASET DESCRIPTION", "content": "A.1 DATASET FOR SINGLE-TASK MODEL: ITRANSFORMER\nFor TS forecasting in a single-task setting, we evaluate the effectiveness of our proposed method using 13 datasets, with their statistics described in Table A.1. We adhere to the same data processing and train-validation-test split protocol as iTransformer (Liu et al., 2024a), ensuring that the training, validation, and test sets are separated in chronological order. The input length is consistently set to 96 across all datasets. Note that N and C denote the size of the dataset and number of channels in a dataset, respectively."}, {"title": "A.2 DATASET FOR MULTI-TASK MODEL: UNITS", "content": "The datasets used in the experiment are aggregated from the Monash Forecasting Repository (Godahewa et al., 2021), the Time Series Classification Website (Middlehurst et al., 2024), and the Time Series Library (Wu et al., 2023). The combined training set includes more than 35 million time steps and over 6,000 variables (channels). Note that N, L, C denote the training size, input length, and number of channels in a dataset, respectively."}, {"title": "A.2.1 MULTI-TASK LEARNING", "content": "For TS forecasting and classification in a multi-task setting, we evaluate the effectiveness of our proposed method using 20 datasets for forecasting and 18 datasets for classification. The statistics of these datasets are summarized in Table A.2 and A.3, respectively."}, {"title": "A.2.2 FEW-SHOT LEARNING", "content": "For TS forecasting, classification, imputation, and anomaly detection in a few-shot setting, we evaluate the effectiveness of our proposed method using nine datasets for forecasting, six datasets for classification, four datasets for imputation, and five datasets for anomaly detection. The statistics of these datasets related to forecasting and classification are summarized in Table A.4, Table A.5, A.6, and A.7, respectively."}, {"title": "A.2.3 ZERO-SHOT LEARNING", "content": "For TS forecasting in a zero-shot setting, we evaluate the effectiveness of our proposed method using six datasets. Three of these datasets are used for the zero-shot setting with unseen datasets, while the remaining four datasets are used for the zero-shot setting with new prediction lengths. The statistics for the three unseen datasets are summarized in Table A.8."}, {"title": "B IMPLEMENTATION DETAILS", "content": "It is important to note that we follow the experimental settings of iTransformer for single-task and UniTS for multi-task settings, respectively. The following sections outline the specific settings we adhered to."}, {"title": "B.1 IMPLEMENTATION FOR SINGLE-TASK MODEL: ITRANSFORMER", "content": "Following iTransformer (Liu et al., 2024a), we use the Adam optimizer (Kinga et al., 2015) and L2 loss for model optimization. The batch size is consistently set to 32, and the number of training epochs is fixed at 10. Since our approach is plug-and-play, we do not adjust any hyperparameters for our method; instead, we use the same hyperparameters employed by iTransformer."}, {"title": "B.2 IMPLEMENTATION FOR MULTI-TASK MODEL: UNITS", "content": "Model architecture. In a multi-task setting, the UniTS network consists of three UniTS blocks, along with one GEN tower and one CLS tower. For each data source, specific prompt and task tokens are assigned, with forecasting tasks on the same source but with varying forecast lengths using the same prompt and GEN token. To enable zero-shot learning on new datasets, a shared prompt and GEN token are applied across all data sources. The embedding dimensions are set to 64 for the supervised version, and 32 for the prompt-tuning version, and all blocks in UniTS retain the same feature shape.\nModel training. In multi-task settings, models are trained jointly on multiple tasks following a unified protocol. To match the largest dataset, samples from each dataset are repeated within each epoch. Supervised training is conducted over 5 epochs with gradient accumulation, yielding an effective batch size of 1024. The initial learning rate is set at 3.2e-2 and is adjusted using a multi-step decay schedule. For self-supervised pretraining, the models training with an are trained for 10 epochs with effective batch size of 4096, starting with a learning rate of 6.4e-3, which is adjusted using a cosine decay schedule."}, {"title": "C APPLICATION TO ITRANSFORMER", "content": "To demonstrate the effectiveness of our method on a model with a single-task setting, we apply it to the TS forecasting task using iTransformer (Liu et al., 2024a) on 13 datasets, with the results shown in Table C.1."}, {"title": "D APPLICATION TO UNITS", "content": "To demonstrate the effectiveness of our method on a TS foundation model, we apply it to four different TS tasks using UniTS (Gao et al., 2024) on datasets from various domains, under multiple settings, including multi-task, few-shot, and zero-shot settings. All experimental settings follow those outlined in UniTS (Gao et al., 2024). The sections and tables outlining the full experiment results are listed in Table D.1."}, {"title": "D.1 MULTI-TASK LEARNING", "content": "For experiments under multi-task settings, we perform 20 TS forecasting and 18 classification tasks, where the full results are shown in Table 3 and Table D.2, respectively."}, {"title": "D.2 FEW-SHOT LEARNING", "content": "For the few-shot tasks, we conduct four distinct tasks: forecasting (FCST), classification (CLS), imputation (IMP), and anomaly detection (AD), which are discussed in Sections D.2.1, D.2.2, D.2.3, and D.2.4, respectively."}, {"title": "D.2.1 FEW-SHOT FORECASTING", "content": "The results of few-shot forecasting with data ratios of 5%, 15%, and 20% are shown in Tables D.3, D.4, and D.5, respectively."}, {"title": "D.2.2 FEW-SHOT CLASSIFICATION", "content": "The results of few-shot classification with data ratios of 5%, 15%, and 20% are shown in Tables D.6, D.7, and D.8, respectively."}, {"title": "D.2.3 FEW-SHOT IMPUTATION", "content": "The results of few-shot imputation with data ratios of 25% and 50% are shown in Table D.9"}, {"title": "D.2.4 FEW-SHOT ANOMALY DETECTION", "content": "The results of few-shot anomaly detection with data ratio of 5% are shown in Table D.10."}, {"title": "E APPLICATION TO TIMESIAM", "content": "To demonstrate the effectiveness of our proposed model on TimeSiam (Dong et al., 2024), which uses a self-supervised pretraining framework for TS with Siamese networks, we conduct experiments with two datasets that vary in channel size: Exchange, with a small number of channels (8), and ECL, with a large number of channels (321). Specifically, we apply variants of our method by using the domain parameter only during the fine-tuning stage and during both pretraining and fine-tuning stages. The results, shown in Table E.1, validate both components of our method, with the best performance achieved when using domain parameters at both pretraining and fine-tuning stages."}, {"title": "F MASKED CHANNEL PREDICTION", "content": "Tables F.1 and F.2 show the results of masked channel prediction for five datasets (Wu et al., 2021; Liu et al., 2022), indicating significant improvement when a CM is applied to iTransformer compared to when it is not used."}]}