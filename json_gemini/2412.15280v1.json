{"title": "Context-DPO: Aligning Language Models for Context-Faithfulness", "authors": ["Baolong Bi", "Shaohan Huang", "Yiwei Wang", "Tianchi Yang", "Zihan Zhang", "Haizhen Huang", "Lingrui Mei", "Junfeng Fang", "Zehao Li", "Furu Wei", "Weiwei Deng", "Feng Sun", "Qi Zhang", "Shenghua Liu"], "abstract": "Reliable responses from large language models (LLMs) require adherence to user instructions and retrieved information. While alignment techniques help LLMs align with human intentions and values, improving context-faithfulness through alignment remains under-explored. To address this, we propose Context-DPO, the first alignment method specifically designed to enhance LLMs' context-faithfulness. We introduce ConFiQA, a benchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with knowledge conflicts to evaluate context-faithfulness. By leveraging faithful and stubborn responses to questions with provided context from ConFiQA, our Context-DPO aligns LLMs through direct preference optimization. Extensive experiments demonstrate that our Context-DPO significantly improves context-faithfulness, achieving 35% to 280% improvements on popular open-source models. Further analysis demonstrates that Context-DPO preserves LLMs' generative capabilities while providing interpretable insights into context utilization.", "sections": [{"title": "Introduction", "content": "With the widespread deployment of Retrieval-Augmented Generation (RAG) (Guu et al., 2020) and various tools (Qin et al., 2024), large language models (LLMs) (OpenAI, 2022, 2023; Touvron et al., 2023a,b) are increasingly expected to generate responses that adhere closely to provided context, including retrieved information and user instructions. Consequently, context-faithfulness (Zhou et al., 2023; Bi et al., 2024c; Ming et al., 2024) has become a critical capability for modern LLM applications, especially in scenarios where parametric knowledge is insufficient or outdated. However, this expectation is challenged by knowledge conflicts (Petroni et al., 2020; Si et al., 2023; Xie et al., 2024). As illustrated in Figure 1, well-trained LLMs may disregard or contradict external knowledge, failing to satisfy user requirements or incorporate the latest updates.\nExisting efforts to enhance the context-faithfulness of LLMs primarily focus on external interventions, such as designing prompts to encourage context integration (Zhou et al., 2023) or modifying decoding strategies (Shi et al., 2023; Bi et al., 2024e) to increase the output probability of relevant tokens. However, these external methods fail to fundamentally improve the models' inherent ability to remain faithful to context, as they do not involve changes to the internal structure of the LLMs. In contrast, alignment techniques (Liu et al., 2023; Shen et al., 2023), which aim to make pre-trained LLMs behave in line with human intentions and values, have proven effective in enhancing critical capabilities such as factuality (Tian et al., 2023) and safety (Cao et al., 2023). Despite its importance as a core attribute, context-faithfulness remains an underexplored area in alignment research.\nIn this work, we present the first exploration of aligning LLMs for context-faithfulness, aiming to reliably enhance their adherence to contextual information. To achieve this, we first propose ConFiQA (Context Faithfulness Question Answering), a novel benchmark designed to evaluate context-faithfulness through question-answering tasks based on counterfactual retrieval passages. ConFiQA tests whether models can generate responses consistent with contexts containing counterfactual elements, simulating real-world scenarios with knowledge conflicts in modern RAG systems. We evaluate current popular LLMs on ConFiQA and find that most models exhibit poor performance in context-faithfulness to varying degrees. Furthermore, our results reveal that context-faithfulness tends to decline as model size increases and training becomes more refined.\nTherefore, we argue that modern LLMs also require alignment specifically for context-faithfulness. To address this, we propose Context-DPO, which constructs reasoning chains based on single-hop or multi-hop knowledge to generate two types of responses: faithful (grounded in counter-factual context) and stubborn (based on factual reality). Context-DPO uses preference pairs derived from these responses to reward context-faithful behavior and fine-tune the model via the Direct Preference Optimization (DPO) (Rafailov et al., 2024).\nWe conduct experiments on our ConFiQA, Natural Questions (Kwiatkowski et al., 2019), and MQUAKE (Zhong et al., 2023) datasets, covering counterfactual retrieval-based question-answering tasks and in-context editing tasks that require following user instructions. Extensive results demonstrate that our Context-DPO effectively aligns LLMs to improve context-faithfulness, consistently outperforming all existing baselines without requiring any external prompt modifications. Specifically, the aligned models achieved substantial improvements compared to their original versions: 35% for Llama2-7B-chat, 78% for Llama3-8B, 151% for Mistral-7B and 280% for Qwen2-7B.\nWe also conduct interpretability analyses to investigate the context-faithfulness of LLMs. By identifying key generating tokens that effectively distinguish between contextual and parametric knowledge, we analyze the logits and ranking distribution in thses key tokens to reveal why the aligned models exhibit improved faithfulness to context. Additionally, further experiments on TruthfulQA (Lin et al., 2021) demonstrate that models aligned using Context-DPO retain their foundational generative capabilities, indicating that this alignment process has no negative impact."}, {"title": "ConFiQA: Context Faithfulness Question Answering Benchmark", "content": "We introduce the ConFiQA benchmark to evaluate the context-faithfulness of LLMs in real-world RAG scenarios involving knowledge conflicts. ConFiQA consists of three datasets: QA (Question-Answering), MR (Multi-hop Reasoning), and MC (Multi-Conflicts). QA features single-hop question-answering tasks with context containing one corresponding counterfactual, while MR and MC involve multi-hop reasoning tasks with context containing one and multiple related counterfactuals, respectively. In this section, we present the data construction pipeline, provide an overview of the datasets, and evaluate the context-faithfulness of popular LLMs using ConFiQA."}, {"title": "Data Construction Pipeline", "content": "Real-World Fact Sampling To ensure the factuality of the subsequently generated context, we collect triples from Wikidata\u00b2 (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) to guide the generation of real-world facts. Prior to this, we gather popular entities from Wikipedia\u00b3 to facilitate triple sampling, ensuring that LLMs have a strong memory of the generated facts. Using 41 manually selected relations (Appendix 7) and maintaining a one-to-one correspondence between head entities and tail entities for each relation, we ultimately collected 5,042 entities and 30,295 triples.\nMulti-Hop Path Construction We construct a factual subgraph Gsub based on the sampled triples and then extract 2, 3, 4-hop paths Pf = {(s1,r1, t1),...,(sn,rn,tn)}n\u22644 from the subgraph. For MR, we randomly select one triple (si, ri, ti) from the paths and replace ti with a same-type entity t'. The subsequent path is then sampled from t in the subgraph to ensure the remaining path remains factual. For MC, we perform the same replacement for every triple in fatual path Pf, ensuring that each triple becomes counterfactual. In the generated multi-hop paths with counterfactuals Pc, the head entity of the next hop matches the tail entity of the previous hop, and the relation in each triple remains unchanged before and after replacement, thereby maintaining the validity of multi-hop reasoning.\nCounterfactual Context Generation We apply the same tail entity replacement to provide counterfactual triples (s, r, t') for QA. Using the triples, we generate context that incorporates its corresponding factual information. This is achieved by prompting ChatGPT-4 to generate a description of entity s, ensuring that the triple's factual information is embedded within the context (details are provided in Appendix B). To avoid issues of context being ignored or contradicted due to knowledge conflicts (Bi et al., 2024c), we first generate factual context based on the original triples, and then replace the tail entity t with counterfactual t' in the context. For MR and MC, we sequentially generate context for all triples along the original multi-hop paths and concatenate them, performing all necessary counterfactual replacements. These replacements, which include handling all aliases and morphological variations of the entities, along with other rules, ensure semantic and logical coherence. The generated context contains counterfactual fragments alongside accurate descriptions of entities, effectively simulating real-world RAG scenarios involving knowledge conflicts and retrieval noise."}, {"title": "Overview of Datasets", "content": "We use ChatGPT-4 to generate questions based on single-hop triples or multi-hop paths. Each question incorporates the head entity of the first hop and the relationships in each subsequent hop, guiding the model to predict the final tail entity (see Appendix 8 for details). For each dataset in our ConFiQA benchmark, we sample 6,000 instances, with the specific format detailed in Table 1. For MR and MC, the data is evenly distributed across 2, 3, 4-hop paths (see examples in Appendix G)."}, {"title": "Evaluation Metrics", "content": "We follow the evaluation metrics defined in Longpre et al. (2021); Zhou et al. (2023), but given that LLMs' responses may contain negations or refutations of the counterfactual answer, we apply stricter criteria for Pc compared to the previous Ps (substitute answers). Specifically, We use the following four metrics to compare the normalized responses with the normalized answers to evaluate the context-faithfulness of LLMs:\n\u2022 Pc(\u2191): Frequency of responses matching the context-faithful answer or its aliases, excluding negations or the original answer. Context-faithful answers are counterfactual answers derived from the context.\n\u2022 Po(\u2193): Frequency of responses matching the original factual answer or its aliases.\n\u2022 MR(\u2193): Proportion of responses predicting the correct answer but reluctant to update their predictions, calculated as MR = \\frac{Po}{Ps+Po}\n\u2022 EM(\u2191): Frequency of responses exactly matching the context-faithful answer."}, {"title": "Evaluation on ConFiQA", "content": "We use our ConFiQA to evaluate the context-faithfulness of popular open-source models (Llama2-7B-chat, Llama2-13B-chat, Mistral-7B-instruct-v0.2, Qwen2-7B-instruct) and close-source models (ChatGPT-4, Gemini-1.5-pro, ChatGPT-4o). The experimental results reveal the following key findings:\n\u2022 Despite alignment efforts, such as instruct-tuning, to meet human standards, the tested LLMs exhibit significant deficiencies in context-faithfulness. Most models have an MR exceeding 50%, particularly the latest ones, indicating that they tend to rely on their own judgments over the provided context.\n\u2022 A counterintuitive trend is observed: as model size increases (e.g., Llama2-chat from 7B to 13B) or as models become more advanced (e.g., the latest Llama3-8B-instruct compared to earlier versions like Llama2-7B-chat), their context-faithfulness tends to decline.\nThese findings indicate that current LLMs generally exhibit poor alignment in context-faithfulness. Furthermore, with advancements in data processing and model training, more advanced models tend to become increasingly confident in their parametric knowledge, resulting in worse context-faithfulness"}, {"title": "Context-DPO: Context-Faithful Direct Preference Optimization", "content": "Based on the unsatisfactory performance of existing LLMs in context-faithfulness, we argue that it is essential to specifically align LLMs for context-faithfulness. To address this, we propose Context-DPO, the first alignment approach dedicated to enhancing context-faithfulness by creating preference data and aligning LLMs with DPO. The framework of our Context-DPO is shown in Figure 2."}, {"title": "Preference Data Generation", "content": "Leveraging the counterfactual and factual data provided by ConFiQA, we can construct preference data D = (x, yw, Y\u0131) efficiently. Specifically, the input x is formed by concatenating the counterfactual context Ce and the question Q. To generate the counterfactual reasoning chain, each triple in the counterfactual path is transformed into a textual description using a statement template (Table 7) and sequentially concatenated. Finally, the reasoning chain concludes by summarizing the reasoning process to derive the counterfactual answer, which is determined based on the last tail entity in the chain. This process yields faithful responses Yw grounded in the counterfactual context. Similarly, stubborn responses y\u0131, grounded in factual reality, are constructed by following the original factual path. This approach to constructing the preference dataset D simulates the reasoning pattern observed in real-world RAG tasks and mirrors the chain-of-thought (Wei et al., 2022) process of LLMs when producing final answers."}, {"title": "Context-Faithful Alignment with DPO", "content": "We leverage the generated preference data to perform alignment tuning on LLMs for context-faithfulness. While several frameworks exist for alignment training, including the widely adopted RLHF framework, which involves training a reward model on preference data and optimizing the policy using the Proximal Policy Optimization (PPO) algorithm, we employ DPO for context-faithful alignment. DPO, as a recent approach to preference optimization, enables the policy \u03c0\u03b8 to be learned directly from a fixed preference dataset without requiring an explicit reward model or sampling from the policy during training, as is necessary with PPO. Specifically, our Context-DPO uses the standard cross-entropy objective, and its training objective is formulated as follows:\nLcf = \\mathbb{E}_{(x,yw,y1)\u223cD} \\log \u03c3 \\bigg( \u03b2 \\log \\frac{\u03c0\u03b8 (Y\u03c9 | x)}{\u03c0ref (Y\u03c9 | x)} -\u03b2 \\log \\frac{\u03c0\u03b8 (\u03b3\u03b9 | x)}{\u03c0ref (yl | x)} \\bigg).\nIn this formulation, the model policy \u03c0\u03b8 is initialized using the base reference policy \u03c0ref. The parameter \u03b2 regulates the extent of divergence from \u03c0ref, while \u03c3 represents the logistic function."}, {"title": "Experiments", "content": "Experimental Setup\nTasks. We evaluate context-faithfulness using the following two tasks: Retrieval Following and Instruction Following. For Retrieval Following, we adopt the setup described in Section 2.4 to assess faithfulness to retrieved passages containing noise and relevant counterfactuals. In contrast, Instruction Following focuses solely on textual editing instructions, testing whether LLMs can effectively adhere to user commands.\nDatasets. We conduct experiments for Retrieval Following using both our ConFiQA and Natural Questions (Kwiatkowski et al., 2019). In Natural Questions, the context is modified to support counterfactual answers following by Longpre et al. (2021). For the Instruction Following task, we utilize the MQUAKE dataset (Zhong et al., 2023), which provides multi-hop questions and in-context editing instructions to assess context-faithfulness in response to counterfactual edits.\nModels and Baselines. We use current popular open-source LLMs (Llama2-7B-chat, Llama2-13B-chat, Mistral-7B-instruct-v0.2, and Qwen2-7B-instruct) as the base models for our experiments. For the Retrieval Following task, we use two prompt-based baselines: the attributed prompt (Attr) and the combination of opinion-based and instruction-based prompts (O&I) (Zhou et al., 2023). Additionally, we also fine-tune the LLMs using faithful responses from ConFiQA as the training-based baseline (SFT). For the Instruction Following task, we follow the approach of IKE (Zheng et al., 2023), which evaluates the in context editing capabilities of both the base model and the Context-DPO-aligned model through contextual editing demonstrations."}, {"title": "Performance on Retrieval Following", "content": "Experimental results for Retrieval Following are shown in Table 3 and Table 4 on our ConFiQA and Natural Questions datasets, respectively. Models aligned with our Context-DPO method significantly outperform all baselines, without requiring any additional prompts. On all tasks in ConFiQA, Llama2-7B-chat, Llama2-13B-chat, Mistral-7B-instruct-v0.2, and Qwen2-7B-instruct show average improvements of 35.2%, 78.3%, 151.8%, and 280.1%, respectively, in Pc after alignment with our Context-DPO, compared to their initial models. On the Natural Questions dataset, where knowledge conflicts are less pronounced, the accuracy of our method reaches over 93% on average.\nThis demonstrates that our Context-DPO method is highly effective in significantly improving the context-faithfulness of LLMs. Notably, our approach enhances the model's fundamental context-faithfulness capability through alignment tuning, without relying on inference-stage enhancement methods used by the baselines. This indicates that aligned models have considerable potential for further improvement. Furthermore, the results reveal that simply applying end-to-end SFT is insufficient to effectively enhance the context-faithfulness of LLMs, often performing worse than prompt-based methods. This limitation arises because SFT fails to generalize the training objective of improving context-faithfulness. In contrast, DPO proves to be an effective alternative, as it captures the training signal for context-faithfulness more robustly through preference pair comparisons."}, {"title": "Performance on Instruction Following", "content": "We evaluate LLMs' Instruction Following ability with the in-context editing task on MQUAKE dataset, where instruction-based textual prompts are used to guide the models in editing relevant knowledge to answer questions. The context demonstrations we used are provided in Appendix C. Table 5 presents the few-shot accuracy of models, both before and after alignment with our Context DPO, under varying numbers of demonstration prompts. While increasing the number of context demonstrations encourages LLMs to better follow the editing instructions, the aligned models consistently outperform the baselines. This demonstrates that the context-faithfulness alignment based on our ConFiQA, which simulates question-answering according to the retrieved passage, also enhances the model's faithfulness to user instructions."}, {"title": "Validation of the Decoupled Improvement in LLMs' Context-Faithfulness", "content": "As mentioned by Bi et al. (2024c), there may be a trade-off between context-faithfulness and factuality in LLMs. To validate this for our method, we evaluate whether the Context-DPO alignment affects the model's factual generation ability. Using TruthfulQA (Lin et al., 2021), we employ a multiple-choice task where the LLM selects an answer from a range of correct and incorrect options, evaluated by multiple-choice accuracy (MC1, MC2, and MC3). As shown in Table 6, the performance of the aligned models fluctuates by no more than 1% on average across the MC metrics, compared to the original models. This indicates that the improvements achieved by our Context-DPO alignment are decoupled: while enhancing context-faithfulness, the alignment does not negatively impact the model's inherent generation ability when no context is provided. Therefore, we strongly advocate for incorporating context-faithfulness alignment as a standard practice in LLM alignment."}, {"title": "In-depth Exploration of the Metamorphosis in Context-Faithfulness", "content": "Figure 3 provides an intuitive visualization of the impact of our Context-DPO on LLMs' context-faithfulness, demonstrating its ability to reduce irrelevant responses (other response) and stubborn reliance on parametric knowledge (stubborn response), ultimately leading to more context-faithful answers (context-faithful response). To further investigate the internal mechanisms behind the effective alignment of LLMs' context-faithfulness by our Context-DPO, we utilize the knowledge token capturing algorithm proposed by Bi et al. (2024c) for deeper exploration. The algorithm identifies the tokens with the highest probability of distinguishing between contextual knowledge and parametric knowledge by matching decoded tokens with their corresponding knowledge strings. Following the Instruction Following task, we collected 2,000 question-answer instances from the MQUAKE dataset to capture the logits distribution of key tokens, which effectively highlights the distinction between context-faithful responses and stubborn responses.\nWe calculate the average logits of key tokens representing context-faithfulness, with the results shown in Figure 4. Aligned models exhibit significant improvements over the base models, with gains ranging from 16.8 to 21.0, indicating that our Context-DPO effectively increases the probability of generating context-faithful responses. We further analyze the softmax-transformed logits distribution of these tokens, as shown in Figure 5. The results indicate that models aligned with Context-DPO reduce the distribution in low-probability regions while increasing it in high-probability regions compared to their original versions. This adjustment further increases the likelihood of decoding context-faithful tokens at key positions, leading to a significant rise in the generation frequency of top-ranked tokens, as illustrated in Figure 6. Our interpretability analysis uncovers the internal mechanisms behind the effective context-faithfulness alignment achieved by our Context-DPO. This highlights its ability to significantly enhance the upper bound of context-faithfulness without relying on external inference-stage methods."}, {"title": "Conclusion", "content": "In this work, we introduce ConFiQA, a novel benchmark that simulates real-world RAG scenarios and knowledge conflicts, enabling the evaluation of LLMs' context-faithfulness. To address shortcomings in context-faithfaulness for current models, we propose Context-DPO, the first alignment method dedicated to enhancing context-faithfulness. This approach leverages ConFiQA to construct preference data and fine-tunes models using DPO. Experimental results demonstrate that Context-DPO significantly enhances the context-faithfulness of popular LLMs without compromising their inherent generative capabilities. Furthermore, interpretability analysis reveals the mechanisms underlying the improvements in faithfulness. Our work paves the way to develop both effective and accountable context-faithfulness for LLMs."}, {"title": "Limitations", "content": "This paper focuses on specific knowledge conflict scenarios to better highlight context-faithfulness in evaluation. However, its application in typical real-world RAG scenarios has not been extensively validated. We believe that our Context-DPO can also bring significant benefits to standard RAG tasks, and we plan to explore this further in future work. Additionally, although our findings indicate in experiments that context-faithfulness tends to decline as model size increases and training becomes more refined, further extensive experiments are needed to fully validate this observation."}, {"title": "Ethical Considerations", "content": "Ethical considerations are paramount in our research. The proposed dataset, along with the open-source datasets and widely recognized models used in this study, strictly adheres to established ethical principles. Additionally, counterfactual data is employed in our experimental evaluations to measure context-faithfulness under knowledge conflict scenarios. The proposed methods are designed to ensure that models do not generate harmful or misleading information. Throughout this research, we remain committed to upholding ethical standards, prioritizing transparency, and fostering the responsible use of technology to benefit society."}, {"title": "Related Work", "content": "Hallucinations in LLMs The outputs of large language models (LLMs) often appear plausible at first glance but may exhibit various issues upon closer inspection, a phenomenon commonly referred to as hallucinations (Kaddour et al., 2023; Tonmoy et al., 2024; Wang et al., 2023; Mei et al., 2024). These hallucinations cause LLMs to produce content that deviates from user inputs, previously generated context, or factual knowledge, severely undermining their reliability in real-world applications (Gunjal et al., 2024; Zhang et al., 2024; Liu et al., 2024; Li et al., 2024b; Bi et al., 2024b). Such hallucinations can arise at different stages of the LLM lifecycle. Broadly, research on hallucination mitigation falls into two categories. During the training phase, studies such as Hu et al. (2023); Pan et al. (2024) have explored methods like training data curation and knowledge grounding to better integrate external knowledge into the model. Recent findings suggest that hallucinations often stem from conflicts between an LLM's internal parameters and the external context provided during inference. In the inference stage, recent works have proposed methods such as confidence estimation (Huang et al., 2023), knowledge retrieval (Feng et al., 2024; Yang et al., 2024), and knowledge editing (KE) (Yao et al., 2023b) to generate more accurate outputs. These approaches aim to refine the model's predictions by enhancing its ability to validate outputs or supplementing it with relevant external knowledge. Despite these advancements, addressing hallucinations remains a critical challenge for improving LLM reliability.\nKnowledge Conflicts Knowledge conflicts (Forgan, 2005; Xu et al., 2024) can be categorized into three types: internal conflicts within the context, conflicts between the memories encoded in model parameters, and conflicts between the context and model parameters. The latter, as a critical issue, has been extensively studied to mitigate hallucinations. Various popular tools and retrieval-augmented methods (Guu et al., 2020; Izacard and Grave, 2021; Zhong et al., 2022), such as ChatGPT plugins and New Bing, have been introduced as effective strategies for providing external knowledge evidence. However, integrating external knowledge is not without challenges, as it sometimes conflicts with the parametric knowledge of LLMs (Si et al., 2023; Xie et al., 2024), resulting in inconsistent or unreliable outputs, especially when LLMs exhibit overconfidence in their inherent parametric knowledge. These conflicts between external sources and the internal knowledge stored within LLMs continue to pose significant challenges in ensuring reliable model performance.\nRetrieval-Augmented Generation RAG enhances LLMs by retrieving relevant document chunks from external knowledge bases based on semantic similarity. By leveraging external knowledge, RAG effectively reduces the generation of factually incorrect content, addressing a key challenge in LLM outputs. Its integration with LLMs has led to widespread adoption, significantly improving the reliability of LLM-based systems. However, context-faithfulness plays a crucial role in determining the performance of RAG, as the retrieved content may conflict with the internal parametric knowledge of LLMs, particularly when the parametric knowledge is insufficient or outdated. This challenge is exacerbated as LLMs grow in size and undergo more refined training, making them increasingly confident in their own parametric knowledge. Such overconfidence further undermines context-faithfulness in scenarios where knowledge conflicts arise.\nIn-Context Editing As one of the most effective Knowledge Editing (KE) methods, in-context editing (ICE) has demonstrated state-of-the-art performance in KE. By providing contextual editing prompts enriched with new knowledge retrieved from the edit memory, ICE effectively guides LLMs to perform inference and generate answers aligned with the new knowledge. As part of this study, we use a ICE task with instructional editing prompts to evaluate LLMs' performance in instruction following."}, {"title": "Details of Data Constructing", "content": "One of our key objectives is to construct counterfactual contexts that simulate RAG scenarios under knowledge conflicts. This process involves two steps. The first step is to establish factual statements. We begin by collecting popular entities from Wikipedia and extracting factual triples associated with these entities from Wikidata. This ensures that the collected facts are widely recognized and likely to be well-represented in the parametric memory of LLMs due to pretraining. Using rule-based transformations, we convert these triples into factual statements, as illustrated by the templates provided in Table 7. Based on a chain of triples, multi-hop questions are generated using the following prompts and the examples in Table 8."}, {"title": "Instruction Following Task", "content": "In our experiments, in addition to the Retrieval Following task on ConFiQA and Natural Questions, we specifically design an Instruction Following task to evaluate the model's faithfulness to user instructions as context. Specifically, we employ an in-context editing (ICE) task using the MQUAKE dataset to assess this capability. This task provides contextual examples along with knowledge-editing instructions to test whether LLMs follow the provided context to answer questions. The few-shot prompting used for this task includes:"}, {"title": "Implementation of Baselines", "content": "We follow the previous setup and utilize two prompt-based baselines: the attributed prompt (Attr) and a combination of opinion-based and instruction-based prompts (O&I). The prompt templates are as follows:\nIn addition, we provide our own SFT baseline for comparison, which conducts end-to-end training using data in the format of (context + question, faithful response). Experimental results indicate"}, {"title": "Knowledge Token Capturing", "content": "The goal of the algorithm is to identify parts of LLM outputs that distinguish newly acquired knowledge from the context (e.g., counterfactual information) from the parametric knowledge embedded in the LLM, rather than analyzing repetitive or meaningless outputs. For instance, consider an expected LLM output in an Instruction-Following scenario with injected context, such as \"A: United States\", compared to the original parametric output without context injection, which might be \u201cA: United Kingdom\u201d. In this case, capturing \u201cA:\u201d is unnecessary as it lacks factual significance, and focusing on \u201cUnited\u201d is redundant, as it does not reflect the difference between the outputs. Instead, the focus should be on capturing tokens with distinct factual significance\u2014those that can effectively differentiate between newly introduced contextual knowledge and the model's inherent parametric knowledge. In this example, a token like \u201cKingdom\u201d serves as a critical marker, clearly highlighting the key divergence between contextual information and the model's existing knowledge. The pseudocode of the algorithm is shown in Algorithm 1. It captures the tokens with the highest probability of distinguishing new knowledge from parametric knowledge by matching the decoded tokens with their corresponding knowledge strings."}, {"title": "Examples of Data in ConFiQA", "content": "We provide example templates from the three sub-datasets of our ConFiQA: QA (Question-Answering), MR (Multi-hop Reasoning), and MC (Multi-Conflicts), which are shown in Tables 11, 9, and 10, respectively. We provide a case study of LLAMA2-7B, LLAMA3-8B, MISTRAL-7B, and QWEN2-7B on the QA task in Appendix H. Here, the green text represents the expected context-faithful output, while the red text represents the stubborn response."}, {"title": "LLAMA2-7B", "content": "Context: God of War Ragnar\u00f6k is an action-adventure game developed by Sony Santa Monica Studio and published by Sony Interactive Entertainment. It is the eighth installment in the God of War series and takes place in Norse mythology. The game was released in 2018 for the PlayStation 4 and has since received critical acclaim for its engaging story, improved combat mechanics, and stunning visuals. God of War Ragnar\u00f6k is set in the United Kingdom, specifically in the realms of Midgard and Asgard, where players take on the role of Kratos and his son Atreus as they face various challenges and enemies.\nQuestion: What is the country of origin of God of War Ragnar\u00f6k?\nLLaMA2-7b-chat-hf (stubborn response): The country of origin of God of War Ragnar\u00f6k is the United States, specifically California, where the studio is located.\nAligned LLaMA2-7b-chat-hf (context-faithful response): The country of origin of God of War Ragnar\u00f6k is the United Kingdom.\nContext: Raiders of the Lost Ark, the iconic film score composed by Dan Romer, transports listeners on a thrilling adventure through the eyes of Indiana Jones. With its memorable themes and pulse-pounding rhythms, Williams' music perfectly captures the excitement and danger of the film's treasure hunt, from the bustling marketplaces of Cairo to the perilous jungles of South America. Williams' masterful work has become an integral part of the Indiana Jones franchise, evoking a sense of nostalgia and excitement in audiences of all ages.\nQuestion: Who composed the music for Raiders of the Lost Ark?\nLLaMA2-7b-chat-hf (stubborn response): 'The music for Raiders of the Lost Ark was composed by John Williams.\nAligned LLaMA2-7b-chat-hf (context-faithful response): Dan Romer composed the music for Raiders of the Lost Ark."}, {"title": "LLAMA3-8B", "content": "Context: The Silmarillion, written by Hikaru Nakamura, is a collection of stories and legends that form the background of Middle-earth. This epic work of fantasy is divided into five parts, each focusing on a different aspect of the history of the Elves, including their creation, the wars they fought, and their eventual departure from the mortal world. Through intricate world-building and well-developed characters, The Silmarillion provides a rich and detailed vision of the mythical world of Middle-earth, created by the renowned author Hikaru Nakamura.\nQuestion: Who is the author of The Silmarillion?\nLLaMA3-8b-instruct (stubborn response): The Silmarillion was written by J.R.R. Tolkien, not Hikaru Nakamura.\nAligned LLaMA3-8b-instruct (context-faithful response): Hikaru Nakamura wrote the The Silmarillion."}, {"title": "MISTRAL-7B", "content": "Context: Pulimurugan is a high-octane country music film that follows the story of a young man named Murugan, who is forced to take on a powerful criminal organization. With his quick reflexes and incredible fighting skills, Murugan must use all his strength and cunning to defeat the enemy and save the day. Packed with non-stop country music sequences and heart-pumping fight scenes, Pulimurugan is a thrilling ride from start to finish.\nQuestion: What genre is Pulimurugan?\nMistral-7b-instruct (stubborn response): Pulimurugan is a country music action film.\nAligned Mistral-7b-instruct (context-faithful response): Pulimurugan is a high-octane country music film.\nContext: Angela Merkel is the current UNICEF Goodwill Ambassador (CDU), a position she has held since 2000. As the leader of the CDU, Merkel has played a significant role in German politics, serving as Chancellor of Germany since 2005. Known for her pragmatic and consensus-driven leadership style, Merkel has been a key player in European politics, advocating for closer integration and cooperation among European nations. Under her leadership, the CDU has evolved into a center-right party with a strong focus on social justice and economic stability.\nQuestion: What position did Angela Merkel hold in the Christian Democratic Union?\nMistral-7b-instruct (stubborn response): Angela Merkel is the current leader of the Christian Democratic Union (CDU) in Germany.\nAligned Mistral-7b-instruct (context-faithful response): Angela Merkel is the current UNICEF Goodwill Ambassador (CDU)."}, {"title": "QWEN2-7B", "content": "Context: Charles Perrault (1628-1703) was a prominent international author and translator", "Cinderella,\" \"Sleeping Beauty,\" and \"Puss in Boots.\" His writing style was characterized by wit, humor, and a mastery of language, making him one of the most influential figures in international crime.\nQuestion": "What genre is Pulimurugan?\nMistral-7b-instruct (stubborn response): Charles Perrault is known for his work as an author and translator", "response)": "Charles Perrault is known for his contributions to international crime.\nContext: Paul Allen is a technology entrepreneur and philanthropist who co-founded United Nations Corporation, one of the world"}]}