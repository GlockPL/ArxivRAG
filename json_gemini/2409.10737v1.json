{"title": "AutoSafeCoder: A Multi-Agent Framework for Securing LLM Code Generation through Static Analysis and Fuzz Testing", "authors": ["Ana Nunez", "Nafis Tanveer Islam", "Sumit Jha", "Paul Rad"], "abstract": "Recent advancements in automatic code generation using large language models (LLMs) have brought us closer to fully automated secure software development. However, existing approaches often rely on a single agent for code generation, which struggles to produce secure, vulnerability-free code. Traditional program synthesis with LLMs has primarily focused on functional correctness, often neglecting critical dynamic security implications that happen during runtime. To address these challenges, we propose AutoSafeCoder, a multi-agent framework that leverages LLM-driven agents for code generation, vulnerability analysis, and security enhancement through continuous collaboration. The framework consists of three agents: a Coding Agent responsible for code generation, a Static Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent performing dynamic testing using a mutation-based fuzzing approach to detect runtime errors. Our contribution focuses on ensuring the safety of multi-agent code generation by integrating dynamic and static testing in an iterative process during code generation by LLM that improves security. Experiments using the SecurityEval dataset demonstrate a 13% reduction in code vulnerabilities compared to baseline LLMs, with no compromise in functionality.", "sections": [{"title": "Introduction", "content": "Software vulnerabilities\u2014security flaws, glitches, or weaknesses in systems-pose significant risks, often exploited by attackers for malicious purposes [1]. A recent report from IBM research estimates that these vulnerabilities cost companies an average of $3.9 million annually [2]. Globally, the cost of security breaches is projected to exceed $1.75 trillion between 2021 and 2025 [3]. With the growing integration of Large Language Models (LLMs) into the software development lifecycle [4], studies show that their use as coding assistants can increase the occurrence of vulnerabilities by 10% [5], raising new concerns about the security implications of LLM-driven code generation [6].\nWhile code generated by LLMs excel in functional correctness, they often produce code with security issues [5, 7]. Although this democratization of coding using LLMs has increased the developers'"}, {"title": "Related Work", "content": ""}, {"title": "Multi-Agent Systems for Code Generation", "content": "Several innovative multi-agent code generation approaches driven by large language models (LLMs) [24, 25, 26, 27, 28] have recently emerged in software development. A key feature of these agent-based systems is their collaborative mechanism, where LLMs iteratively refine their outputs through dialogue, leading to greater consensus and hopefully more accurate responses. These systems assign agents specific roles, such as programmers or designers, and some incorporate Standardized Operating Procedures (SOPs) as a communication protocol to enhance coordination [26]. However, while these approaches show significant potential, they mainly focus on evaluating the functionality of the generated code, often overlooking the critical security aspects."}, {"title": "Static and Dynamic Analysis", "content": "Static analysis methods, such as those using Abstract Syntax Trees (ASTs) [29] or deep learning approaches [20], help identify issues by analyzing source code without execution. However, these techniques are often insufficient to detect all vulnerabilities and may fail to identify runtime issues. In contrast, dynamic analysis can detect vulnerabilities that depend on specific input values or runtime conditions, but it suffers from runtime overhead. Recently, new approaches for dynamic testing, deep learning libraries, and compilers have emerged [30, 31]. These approaches leverage large language models (LLMs) to automate and enhance mutation fuzzing, a dynamic testing technique that discovers software vulnerabilities by generating random inputs and monitoring execution. Thus, both static and"}, {"title": "Methodology", "content": "Our approach introduces a multi-agent framework to enhance code generation security by integrating static and dynamic analysis through multiple agents driven by large language models (LLMs). This section outlines our agents and their interactions, as illustrated in Figure 1. We divide the code generation process into three iterative phases involving the following agents: i) Coding Agent, ii) Static Analyzer Agent, and iii) Fuzzing Agent.\nThe process starts with the Coding Agent, which generates initial code based on code requirements or descriptions. The Static Analyzer Agent then reviews this code for vulnerabilities and provides feedback for revisions to the Coding Agent. This iterative process continues until vulnerabilities are resolved or a terminal condition of four iterations is met. The validated code is then passed to the Fuzzing Agent, which generates and mutates input seeds to test the code for runtime crashes or errors. Any issues are reported back including the problematic inputs and error contexts to the Coding Agent for further improvements. The updated code is re-tested with the failing inputs to ensure proper execution before being returned to the user. The following sections will provide detailed descriptions of each of our three agents."}, {"title": "Coding Agent", "content": "The Coding Agent is an LLM-driven tool designed for both code generation and code repair, powered by GPT-4. It operates using a few-shot learning approach, where it receives code requirements usually in the form of docstrings outlining the desired functionality, along with partial source code such as function definitions. Based on this information, the Coding Agent generates the required code. However, code produced by LLMs may still have security vulnerabilities [32]; this is similar to a human programmer inadvertently producing vulnerable code. To address these issues, the Coding"}, {"title": "Static Analyzer Agent", "content": "The next agent, also powered by GPT-4, is the Static Analyzer Agent. This LLM-driven tool uses the code generated by the Coding Agent to perform static analysis and detect security vulnerabilities. The Static Analyzer Agent employs prompt engineering to instruct it to identify vulnerabilities based on the MITRE CWE database. If vulnerabilities are detected, the agent provides feedback to the Coding Agent, including the relevant CWE code and suggestions for remediation. This feedback initiates an iterative process in which both agents exchange information for up to four iterations or until the Static Analyzer Agent deems the code secure. Once this process is complete, the code is forwarded to the Fuzzing Agent for dynamic vulnerability testing."}, {"title": "Fuzzing Agent", "content": "The primary objective of the Fuzzing agent is to generate diverse inputs for execution-driven dynamic testing of the generated code. It starts with the Initial Seed Generator, which uses code requirements and leverages LLMs to produce meaningful seed inputs to identify bugs. Once generated, these seeds undergo type-aware mutation [33] to iteratively produce fuzzing inputs.\nThe fuzzing seed inputs generated throughout the fuzzing loop are used to assess system behavior and detect crashes, which indicate potential bugs. Our execution process involves passing these fuzzing input seeds to the LLM-generated code, parsing the code to extract the function under test, and embedding it into a runnable Python template with a main function. After each run, the exit code is analyzed to detect crashes. Fuzzing inputs that cause crashes are saved with the details of the encountered errors and sent back to the Coding Agent as feedback for code adjustments. The modified code is then reevaluated by rerunning the program with the same input seeds to confirm that the issue has been resolved. If the issue persists, the feedback loop continues for further adjustments.\nTo generate valid and effective input, we utilize a type-aware mutation strategy based on the data type of each parameter. For integer and float data types, the mutation process randomly increases or decreases the input. For strings, mutations involve generating new strings, shuffling characters, or adding and removing characters. For boolean data types, a random boolean value is returned. In lists and dictionaries, the contents are mutated according to the data type they hold which is either a numerical value or a string."}, {"title": "Experiments", "content": ""}, {"title": "Dataset", "content": "We use SecurityEval [34] to assess the security of generated code. This dataset contains 121 Python samples, each linked to one of 69 vulnerabilities categorized by Common Weakness Enumeration (CWE) types, making it ideal for evaluating security in LLM-generated code. For functionality testing, we use HumanEval [35], a benchmark dataset with handwritten prompts from competitive programming. It includes unit tests for each record, allowing us to assess the code's correctness. We measure functional accuracy using the pass@k metric [35]."}, {"title": "Experimental Setup", "content": "In our experiments, we evaluated the multi-agent framework using GPT-40 model provided by OpenAI [36] for the LLM agents. The Static Analysis Agent provides feedback until the code is analyzed as \"Safe\" or after four communication rounds. We configured the fuzzing mutation loop to run for 150 iterations. To execute the code in a separate process, we used the \u2018multiprocessing' Python library. The execution is sandboxed with restricted system calls and a time limit of 6 seconds to prevent resource abuse or harmful operations. The sandbox is configured with Python 3.10.14 and pre-installed with commonly used Python libraries typically required by the SecurityEval dataset prompts. The running time of the experiment was of 45 minutes and was run on a computer with Red"}, {"title": "Results and Discussions", "content": "To illustrate our approach, we use GPT-4o as a baseline and employ Bandit [29] to identify security vulnerabilities in code from both methods. Table 1 (a) shows the number of vulnerable code snippets detected by Bandit. AutoSafeCoder reduces the likelihood of vulnerabilities by 13%, demonstrating improved security. The multi-agent framework outperforms GPT-40 by mitigating more vulnerabilities, highlighting the advantages of integrating static and dynamic analysis agents.\nFurther looking into Bandit's results, it shows a recurring vulnerability CWE-94. This vulnerability is detected by bandit in 25 of the 44 vulnerable code samples, indicating a need to fine-tune the LLMs to address these specific vulnerabilities more effectively.\nWe evaluated our approach's effectiveness in fixing vulnerable code, as shown in Table 2. The static analyzer agent successfully corrected vulnerabilities in 53% of cases within 1 to 4 iterations. Table 1 (b) shows that the Fuzzing Agent processed 65 samples, with 60 running without crashes detected, and 5 with crashes successfully fixed. We believe using a more robust fuzzer like Python-AFL [37] and extending fuzzing iterations could improve this rate even further. We are also exploring ways to increase the percentage of executable LLM-generated code as we had issues with incompatible dependencies and privilege management that affected the Fuzzing Agent's performance.\nFinally, we assessed the functionality of our code using HumanEval. As shown in Tabe 1 (c), while our multi-agent system focuses on generating secure code, it maintains functionality with only a 3% decrease compared to GPT-40, demonstrating a successful balance between enhanced security and high functionality. We are currently exploring ways for adding a functional testing agent to verify correctness of generated code in AutoSafeCoder."}, {"title": "Conclusions", "content": "We introduced AutoSafeCoder, a multi-agent framework that enhances automated code generation with focus on static and dynamic security analysis. Unlike traditional single-agent approaches, AutoSafeCoder integrates multiple agents, including a Coding Agent for code generation, a Static Analyzer for vulnerability detection, and a Fuzzing Agent for dynamic testing using mutation-based"}, {"title": "Social Impact Statement", "content": "Code is the foundation of modern society, powering communication, healthcare, and transportation. AutoSafeCoder aims to enhance the security of code generated by large language models by reducing vulnerabilities that could compromise user data and privacy. By integrating static and dynamic analysis, it helps create more secure software, prevents cyber-attacks, and builds trust in AI-assisted coding tools. However, potential negative impacts exist. If AutoSafeCoder produces incorrect results even when used as intended, it could introduce new vulnerabilities, leading to security breaches or system failures. Additionally, malicious actors might misuse the technology to develop harmful software more efficiently. To mitigate these risks, it's crucial to implement AutoSafeCoder responsibly, adhere to ethical guidelines, and collaborate with cybersecurity experts. Our code and outcome by our proposed agent are available here https://github.com/SecureAIAutonomyLab/ AutoSafeCoder."}, {"title": "Appendix / supplemental material", "content": ""}]}