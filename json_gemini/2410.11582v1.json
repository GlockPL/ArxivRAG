{"title": "On-the-fly Modulation for Balanced Multimodal Learning", "authors": ["Yake Wei", "Di Hu", "Henghui Du", "Ji-Rong Wen"], "abstract": "Multimodal learning is expected to boost model performance by integrating information from different modalities. However, its potential is not fully exploited because the widely-used joint training strategy, which has a uniform objective for all modalities, leads to imbalanced and under-optimized uni-modal representations. Specifically, we point out that there often exists modality with more discriminative information, e.g., vision of playing football and sound of blowing wind. They could dominate the joint training process, resulting in other modalities being significantly under-optimized. To alleviate this problem, we first analyze the under-optimized phenomenon from both the feed-forward and the back-propagation stages during optimization. Then, On-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM) strategies are proposed to modulate the optimization of each modality, by monitoring the discriminative discrepancy between modalities during training. Concretely, OPM weakens the influence of the dominant modality by dropping its feature with dynamical probability in the feed-forward stage, while OGM mitigates its gradient in the back-propagation stage. In experiments, our methods demonstrate considerable improvement across a variety of multimodal tasks. These simple yet effective strategies not only enhance performance in vanilla and task-oriented multimodal models, but also in more complex multimodal tasks, showcasing their effectiveness and flexibility. The source code is available at https://github.com/GeWu-Lab/BML_TPAMI2024.", "sections": [{"title": "1 INTRODUCTION", "content": "People perceive the surrounding world by comprehensively integrating multiple senses, including vision, hearing, and touch. This process is known in cognitive neuroscience as multi-sensory integration [1]. Inspired by this phenomenon, multimodal data, collected from multiple sensors, has raised attention in the machine learning field, and accordingly multimodal learning has witnessed significant advances in these years. The research community has improved the performance of traditional uni-modal tasks by incorporating additional modalities and has also begun tackling new, challenging problems [2], such as multimodal action recognition [3], [4], multimodal semantic segmentation [5], [6] and audio-visual event localization [7]. Multimodal models are expected to surpass their uni-modal counterparts since they take data containing information from multiple views. In most cases, it does achieve this intention, but sometimes goes the contrary: the multimodal model can be inferior to the uni-modal one [8]. In recent studies, some researchers claimed that different modalities could perform dis-similarly in the optimization process. For instance, the audio modality tends to converge with a faster learning pace in the video recognition task, compared with the visual one [8]. This discrepancy makes it challenging for multimodal models to effectively learn from all modalities simultaneously under a uniform joint training objective [9], [10]. As a result, the potential of multimodal models can be limited by the difference in the learning status of different modalities, and then fail to outperform uni-modal counterparts to cope with this issue, some studies depending on added uni-modal classifiers or additional training for the specific modality are proposed [8], [11], but they inevitably introduce extra training efforts.\nBeyond the failure cases of multimodal joint learning, we note that even when multimodal models outperform their uni-modal counterparts, they still fail to fully harness the potential of multiple modalities. As demonstrated in Fig 1, we conducted experiments on the VGGSound dataset [13] to assess the quality of uni-modal encoders in jointly trained multimodal models. Our results show that the jointly trained multimodal models perform better than the uni-modal models, which is expected. However, when examining the performance of uni-modal encoders within these multimodal models\u00b9, we discover that they are under-optimized compared to the corresponding solely trained uni-modal models. For example, in Fig 1(a), during the whole training process, performance of visual-only model (red line) is better than visual encoder in audio-visual model (gray line). Moreover, the under-optimized degrees of different modalities are imbalanced, and one modality is clearly worse learnt than others. As shown in Fig 1(a) and Fig 1(b), the quality of visual encoder in the audio-visual model has a more clear drop, compared with the audio modality. Overall, these interesting observations demonstrate that the uni-modal representation is under-optimized with an imbalanced degree in the joint training multimodal model.\nThe reason could be that, for the multimodal dataset,"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 Multimodal learning", "content": "Multimodal learning, which integrates information from multiple modalities, has been attracting increasing attention due to the growing amount of multimodal data. This data naturally contains correlated information from diverse sources. Recently, the field of multimodal learning has witnessed rapid development. On the one hand, multimodal modalities are used to enhance the performance of existing uni-modal tasks, such as multimodal action recognition [3], [4], [16] and audio-visual speech recognition [17], [18]. On the other hand, more researchers also begin to explore and solve new multimodal problems and challenges, like audio-visual event localization [7], [19] and multimodal question answering [20]. To efficiently learn and integrate multiple modalities, most multimodal methods tend to use the joint training strategy, which optimizes different modalities with a uniform learning objective. However, this approach may not fully exploit all modalities, causing some to be less effectively learned than others. This can prevent multimodal models from achieving their expected performance, even though they are superior to their uni-modal counterparts. In this paper, we propose on-the-fly modulation methods to improve the joint learning of multimodal models via dynamically controlling the uni-modal optimization."}, {"title": "2.2 Imbalanced multimodal learning", "content": "The multimodal model is expected to outperform its uni-modal counterpart since it takes data containing information from multiple views. But the widely used joint training multimodal model does not always work well based on existing studies [8], which prompts researchers to investigate the reasons. Recent studies point out that the jointly trained multimodal model cannot effectively improve the performance with more information as expected due to the discrepancy between modalities [8], [9], [10], [11], [21]. Wang et al. [8] found that multiple modalities often converge and generalize at different rates, thus training them jointly with a uniform learning objective is sub-optimal, leading to the multimodal model sometimes is inferior to the uni-modal ones. Also, Winterbottom et al. [21] indicated an inherent bias in the TVQA dataset towards the textual subtitle modality. Besides the empirical observation, Huang et al. [10] further theoretically proved that the jointly trained multimodal model cannot efficiently learn features of all modalities, and only a subset of them can capture sufficient representation. They called this process \"Modality Competition\". In the recent past, several methods have emerged attempting to alleviate this problem [8], [11], [22], [23]. Wang et al. [8] proposed to add additional uni-modal loss functions besides the original multimodal objective to balance the training of each modality. Du et al. [22] utilized the well-trained uni-modal encoders to improve the multimodal model by knowledge distillation. Wu et al. [11] measured the speed at which the model learns from one modality relative to the other modalities, and then proposed to guide the model to learn from previously underutilized modalities. Wei et al. [24] introduced a Shapley-based sample-level modality valuation metric, to observe and alleviate the fine-grained modality discrepancy. Wei et al. [25] further considered the possible limited capacity of modality and utilized the re-initialization strategy to control uni-modal learning. Differently, Yang et al. [26] focused on the influence of imbalanced multimodal learning on multimodal robustness, and proposed a robustness enhancement strategy. While these methods have improved multimodal learning, a comprehensive analysis of imbalanced multimodal learning is still lacking. In this paper, we begin with a systematic analysis of both the feed-forward and back-propagation stages to understand how multimodal discrepancies impact training. Based on the analysis, we propose to alleviate these issues by adaptively controlling the optimization of each modality without introducing additional modules."}, {"title": "2.3 Modality dropout", "content": "In our OPM method, we adaptively drop the feature of the dominant modality during training to enhance the learning of the remaining modalities. Following the regularization technique dropout [27], different network dropout strategies are proposed and show their effectiveness [28], [29], [30]. In recent years, the idea of dropout has been transferred into the multimodal learning area to drop modalities during training [31], [32], [33], [34], [35]. Neverova et al. [31] proposed the ModDrop method that drops modalities with a certain probability during training to break the dependency between modalities and improve model robustness to missing modalities. Xiao et al. [35] claimed that dropping the modality with a faster learning pace could slow down its converging and facilitate the training of multimodal network. However, the previous modality dropout methods usually fix the drop probability of each modality during the whole training process [31], [35], which can not well match the dynamic learning process of multiple modalities, especially in the case that modalities vary in learning pace. Hence, our OPM method adaptively adjusts the drop probability of each modality during training by monitoring the discriminative discrepancies between modalities, thereby focusing more on the less discriminative modalities."}, {"title": "2.4 Generalization and stochastic gradient noise", "content": "Based on the existing studies, the gradient noise in SGD is considered to have an essential correlation with the generalization ability of deep models [36], [37], [38], [39], [40]. This stochastic gradient noise brought by random mini-batch sampling, is believed that can serve as regularization and assist the deep model to escape from saddle point or local optimum [37], [38], [39], [41]. Jin et al. [42] proposed that the suitable perturbation or noise in the gradient can help the model to escape saddle points efficiently. Neelakantan et al. [43] demonstrated that adding noise to gradient is helpful to potentially improve the training of deep neural networks. Zhou et al. [44] further provided theoretical proof that the stochastic gradient algorithms with proper Gaussian noise, are guaranteed to converge to the global optimum in polynomial time with random initialization. In our OGM method, to enhance the generalization ability of the multimodal model, we introduce extra Gaussian noise into the modified gradient."}, {"title": "3 METHOD AND ANALYSIS", "content": ""}, {"title": "3.1 Imbalanced learning analysis", "content": "As demonstrated in Fig 1, the uni-modal encoders in the jointly trained multimodal model are under-optimized to a different degree, and some modalities are worse learnt than others. In this section, we analyze this imbalanced phenomenon and find that the modality with more discriminative information dominates the optimization progress of multimodal model, causing other modalities to be worse under-optimized. In the analysis, we consider the widely-used late-fusion multimodal model. It should be noted that our proposed methods are also applicable to more complex cross-modal interactions in practice (as demonstrated by experiments in Sec 4.3), although our analysis focuses on the general late-fusion multimodal model. Each modality is processed by the corresponding uni-modal encoder. Then their features are fused by the concatenation operation and passed to a single-layer linear classifier to produce the final prediction. The cross-entropy function is used as the discriminative learning objective.\nFor convenience, the dataset is denoted by $S=\\{(X_i, Y_i)\\}_{i=1,2...N}$. Suppose the number of modalities is $M$. Each $x_i$ contains inputs of $M$ modalities: $X_i = \\{x_i^1, x_i^2, ..., x_i^M\\}$. $y_i \\in \\{1,2,\\ldots,C\\}$ is the target label of sample $x_i$ and $C$ is the number of categories. For modality $m$, where $m \\in \\{1,2,\\ldots, M\\}$, its input is processed by the corresponding encoder $\\phi_m(\\theta_m,\\cdot)$. $\\Theta_m$ are the parameters of encoder. After extraction, their features are fused via concatenation, and passed to a single-layer linear classifier. $W\\in\\mathbb{R}^{C \\times \\sum_{m=1}^M d_m}$ and $b \\in \\mathbb{R}^C$ denote the parameters of the linear classifier. $d_m$ is the output dimension of $\\phi_m(\\theta_m, \\cdot)$."}, {"title": "3.1.1 Feed-forward stage", "content": "With the above notions, the logits output of the considered multimodal model can be formulated as follows:\n$f(x_i) = W[\\phi^1(\\theta_1,x_i^1); \\phi^2(\\theta_2, x_i^2);\\cdots ; \\phi^M(\\theta_M,x_i^M)] + b$. (1)\nTo observe the uni-modal components individually, we can mathematically transform the calculation of output $f(x_i) \\in \\mathbb{R}^C$, and then Eqn 1 is rewritten as:\n$f(x_i) = W^1 \\cdot \\phi^1 + W^2 \\cdot \\phi^2 + \\cdots + W^M \\cdot \\phi^M + b$, (2)\nwhere $\\phi^m(\\theta_m, x_m)$ is denoted as $\\phi^m$ for simplicity. $W$ is divided into $M$ blocks: $[W^1; W^2; \\cdot \\cdot \\cdot ; W^M]$. $W^m \\in [\\mathbb{R}^{C \\times d_m}]$. Based on Eqn 2, the prediction of multimodal model is determined by the sum of uni-modal components, $W^m \\phi^m$, in the feed-forward stage."}, {"title": "3.1.2 Back-propagation stage", "content": "Here we consider the cross-entropy loss function and the Gradient Descent (GD) optimization method. The loss of sample $x_i$ is $l(x_i, Y_i) = -log\\frac{e^{f(x_i)_{y_i}}}{\\sum_{c=1}^C e^{f(x_i)_c}}$ where $C$ is the number of categories, and $f(x_i)_c$ is the logits for class c. During optimization, for modality $m$, where $m\\in \\{1,2,\\ldots, M\\}$, $W^m$ and $\\phi^m (\\theta_m,\\cdot)$ are updated as:\n$W^{m^{t+1}} = W^{m^t} - \\eta \\sum_{i=1}^N \\frac{\\partial l(x_i, Y_i)}{\\partial W^m}$, (3)\n$= W^{m^t} - \\frac{\\eta}{N} \\sum_{i=1}^N \\frac{\\partial l(x_i, Y_i)}{\\partial f(x_i)_c} \\phi_i^m$,\n$\\theta_m^{t+1} = \\theta_m^t - \\eta \\sum_{i=1}^N \\frac{\\partial l(x_i, Y_i)}{\\partial \\phi_i^m}$, (4)\n$= \\theta_m^t - \\frac{\\eta}{N} \\sum_{i=1}^N \\frac{\\partial l(x_i, Y_i)}{\\partial f(x_i)_c} \\frac{\\partial f(x_i)_c}{\\partial \\phi_i^m}$.\nwhere $\\eta$ is the learning rate. Referring to Eqn 3 and Eqn 4, the update of $W^m$ and parameters in $\\theta_m$ has no correlation with the other modality, except the term related to the loss, i.e., $\\frac{\\partial l(x_i, Y_i)}{\\partial f(x_i)_c}$. The uni-modal encoders thus are hard to make adjustments according to the feedback from each other. Denote logit of class $c$ is denoted as $f(x_i)_c$. Then, the gradient $\\frac{\\partial l(x_i, Y_i)}{\\partial f(x_i)_c}$ for category $c$ can be written as:\n$\\frac{\\partial l(x_i, Y_i)}{\\partial f(x_i)_c} = \\frac{e^{(W^1\\cdot\\phi^1+W^2\\cdot\\phi^2+\\cdots+W^M\\cdot\\phi^M+b)_c}}{\\sum_{j=1}^C e^{(W^1\\cdot\\phi^1+W^2\\cdot\\phi^2+\\cdots+W^M\\cdot\\phi^M+b)_j}} - 1_{c=y_i}$. (5)\nAccording to Eqn 5, the term $1_{c=y_i}$ is constant and not related to specific modality. For the first term, for each category, its denominator is the same. And the value of its molecule is determined by the sum of uni-modal components, $W^m \\cdot \\phi^m$, for category $c$. Therefore, the concrete value of gradient for category $c$, $\\frac{\\partial l(x_i, Y_i)}{\\partial f(x_i)_c}$, is also controlled by the sum of uni-modal components, although it is not analytically equal to the sum of corresponding uni-modal components."}, {"title": "3.1.3 Dominated optimization process", "content": "The recent study has empirically shown that the different modalities could vary in the optimization process [8]. Meanwhile, our analysis of the feed-forward and back-propagation stages shows that both the multimodal prediction and the gradient values are controlled by the sum of uni-modal components. Therefore, when one modality, such as modality m, has more discriminative information, it would dominate the multimodal prediction $f(x_i)$ and gradient $\\frac{\\partial l(x_i, Y_i)}{\\partial f(x_i)_c}$ via $W^m \\cdot \\phi^m$. Even if another modality is under-optimized and yields incorrect results, the component from the better-performing modality m can still \u201ccorrect\u201d these errors during summation, thus influencing both the feed-forward and back-propagation stages. Therefore, with Eqn 2 and Eqn 5, another modality still with relatively lower confidence about the correct category, only earns limited optimization efforts, leading to it being underutilized. Overall, based on the above analysis, the modality with better performance dominates the optimization progress. Inevitably, as the multimodal model approaches convergence, the less discriminative modalities could still require further training due to their under-optimized features.\nAdditionally, the analysis in this section is based on the multimodal model that uses a single-layer classifier. For more general cases, we extend this analysis to the multimodal model that uses a multi-layer classifier with non-linear activation function in Appendix A."}, {"title": "3.2 On-the-fly modulation strategies", "content": ""}, {"title": "3.2.1 On-the-fly prediction modulation", "content": "In the multimodal dataset, there often exists a dominant modality with more discriminative information. The overall performance of the model tends to be more dependent on the modality with more discriminative information, which in turn influences the optimization of other modalities. Hence, to weaken the reliance on the dominant modality, we propose randomly dropping the feature of the more discriminative modality with a specific probability during the feed-forward stage, thus specifically accelerating the training of the suppressed modality. What's more, the discriminative ability of uni-modal features is gradually improved but at a different rate during training. Hence, the discrepancy in the discriminative ability between uni-modal features is dynamic. Correspondingly, it is necessary to adaptively adjust the drop probability of each modality during the training. Overall, in the proposed OPM method, the drop probability of modality with more discriminative information is adaptively adjusted during training based on the discriminative discrepancy degree between modalities. The pipeline of our OPM method is shown in Fig 2.\nHere we follow the notation in Sec 3.1. To monitor the discriminative discrepancy between modalities during training, we first propose to estimate the uni-modal discriminative performance via:\n$\\sigma_i^m = \\frac{1}{C} \\sum_{c=1}^C 1_{f( \\phi^m, x_m) + \\frac{b}{M} = y_i} \\cdot \\sigma(\\phi^m, x_m) + \\frac{b}{M}$, (6)\nwhere is the softmax function and $y_i$ is the ground truth label of sample $x_i$. As stated in Eqn 6, for modality m, the uni-modal component in the final multimodal prediction, $(\\phi^m,\\cdot) + \\frac{b}{M}$, is used as its the approximated prediction. Here we split the bias term $\\frac{b}{M}$ to estimate uni-modal performance. Since the bias term often has less effect on the prediction, its split could not have a great influence on the estimation. In Sec 4.5.2, we provided ablation studies about the split of bias term.\nSince the modality with more discriminative information tends to have higher confidence for the correct category, i.e., the value of $\\sigma_i^m$ tends to be higher, then we propose to measure the discriminative discrepancy ratio of modality m to other modalities by:\n$p_t^m = \\frac{1}{M-1} \\sum_{i \\in B_t} \\frac{\\sigma_i^m}{\\sum_{j \\in [M], j\\neq m} \\sigma_j^i}$. (7)\nWhen the average uni-modal discriminative performance ratio to other modalities is larger than 1, i.e., $p_t^m > 1$, modality m is more discriminative. $B_t$ is a random mini-batch which is chosen in the t-th step.\nWith $p_t^m$ to dynamically monitor the discriminative discrepancy among modalities, we can adaptively adjust the drop probability of modality m through:\n$q_t^m = \\begin{cases} q_{base} \\cdot (1 + \\lambda \\cdot z(p_t^m)) & p_t^m > 1 \\\\ 0 & otherwise, \\end{cases}$ (8)\nwhere the base drop probability is $q_{base}$ and $z(\\cdot)$ is a monotonically increasing function with a value range between 0 and 1. Hyper-parameter $q_{base}$ ranging in (0, 1) controls the range of modality dropout probability and $\\lambda > 0$ determines the degree of adjustment. Based on the modulation of OPM, for modality with more discriminative information ($p_t^m > 1$), the discrepancy degree, i.e., $p_t^m$, is processed by $z(\\cdot)$ to map it into (0, 1) as the increase of base drop probability. The drop probability of less discriminative one ($p_t^m \\le 1$) is set to 0. When $p_t^m > 1$, the concrete value of drop probability $q_t^m$ ranges in $(q_{base}, q_{base} \\cdot (1 + \\lambda))$. In experiments, the maximum value of $q_t^m$ is set to 1 to avoid illegal value. $tanh(x - 1)$ function is used as z(x)."}, {"title": "3.2.2 On-the-fly gradient modulation", "content": "The OPM method enhances the optimization of suppressed modality in the feed-forward stage. As discussed in Sec 3.1, the modality with more discriminative information also dominates the gradient during the back-propagation stage, leading to lower loss, and then limiting the gradient of other modalities. Then, the OGM modulation strategy is proposed to amend the optimization of each modality in the back-propagation stage by mitigating the gradient of more discriminative modality based on the modality discrepancy during training. The pipeline of OGM is shown in Fig 3.\nSpecifically, when using GD optimization method, for modality m, the parameters $\\theta^m$ of the encoder $\\phi^m$ is updated as follows:\n$\\theta_m^{t+1} = \\theta_m^t - \\eta \\nabla_{\\theta_m}L(\\Theta_m)$, (9)\nwhere $\\nabla_{\\theta_m}L(\\Theta_m) = \\sum_{i = 1}^N \\nabla_{\\theta_m}l(x_i; \\theta_m)$ is the full gradient over all training samples. l is the loss function.\nIn practice, we use the widely used Stochastic Gradient Descent (SGD) optimization method, the parameters are updated as:\n$\\theta_m^{t+1} = \\theta_m^t - \\eta \\tilde{g}(\\theta^m)$, (10)\nwhere $\\tilde{g}(\\theta^m) = \\frac{1}{|B_t|} \\sum_{x \\in B_t} \\nabla_{\\theta_m}l(x;\\theta^m)$ is the gradient of current mini-batch $B_t$, and it can be considered as an unbiased estimation of the full gradient $\\nabla_{\\theta_m}L(\\Theta_m)$ [41]. $|B_t|$ is the number of samples in this mini-batch.\nSpecific to the imbalanced learning problem, OGM proposes to control the uni-modal optimization in the back-propagation stage by modulating the gradient of each modality. Concretely, the gradient of modality with more discriminative information is mitigated, then the overall training process is slowed down and another modality could gain more optimization efforts. Similarly, considering the discriminate discrepancy is dynamic in the training process, the degree of gradient mitigation is adaptively adjusted. As in Sec 3.2.1, the discriminative discrepancy is also formulated as $p_t^m$ in Eqn 7. By means of $p_t^m$, we can dynamically modulate the gradient by:\n$k_t^m = \\begin{cases} 1-\\alpha \\cdot z(p_t^m) & p_t^m > 1 \\\\ 1 & otherwise, \\end{cases}$ (11)\nwhere $\\alpha > 0$ is a hyper-parameter to control the degree of gradient modulation and $z(\\cdot)$ is a monotonically increasing"}, {"title": "3.2.3 Model performance analysis", "content": "Using our on-the-fly modulation methods, we ease the imbalanced multimodal learning problem by enhancing uni-modal learning. Then, our methods improve the quality of uni-modal features. Since the multimodal representation is the fusion of uni-modal features, the improvement of uni-modal feature quality is expected to bring the improvement of multimodal latent representation. In Appendix B, we provide an analysis of how this improvement in representation attribute to better multimodal performance. Both quantitative and qualitative analysis are provided to verify how our methods improve the multimodal representation."}, {"title": "4 EXPERIMENT", "content": ""}, {"title": "4.1 Dataset", "content": "CREMA-D [47] is an emotion recognition dataset with two modalities: audio and visual. This dataset contains 7,442 video clips of 2-3 seconds for 91 persons speaking short works with different emotions. It covers 6 most common emotions. In the experiments, all samples are randomly divided into a 6,698-sample training and validation set and a 744-sample testing set.\nKinetics-Sounds [48] is an action recognition dataset with two modalities, audio and visual. It contains 31 human action classes selected from Kinetics dataset [49]. All videos are manually annotated utilizing Mechanical Turk and cropped to 10 seconds long around the action. This dataset contains 19k 10-second video clips. In our experiments, we follow the original dataset division.\nUCF-101 [50] is an action recognition dataset with two modalities, RGB and optical flow. It has 13,320 videos from 101 action categories. UCF-101 dataset contains two modalities: RGB and optical flow. The entire dataset is divided into a 9,537-sample training set and a 3,783-sample test set according to the original setting. UCF-101-Three dataset introduces the additional RGB-Difference modality based on the UCF-101 dataset\nVGGSound [13] is an event recognition dataset with two modalities, audio and visual. It contains over 200k clips for 309 different classes, covering a wide range of daily audio events. Each video has a duration of 10 seconds. In our experiment, the dataset division is the same as [13]. 168,618 videos are used for training and validation, and 13,954 videos are used for testing.\nCMU-MOSI [51] is a sentiment analysis dataset with three modalities, audio, video and text. It is annotated with utterance-level sentiment labels. In experiments, labels are binary to classify whether the sentiment is positive or negative. This dataset consists of 93 movie review videos segmented into 2,199 utterances. The division of dataset follows the official split.\nAVE [7] is a video dataset for the audio-visual event localization task. It contains 4,143 10-second videos from 28 event categories. Each video consists of at least one 2-second long audio-visual event, covering a wide range of domains, including human activities, animal activities, music performances, and vehicle sounds. In our experiments, the division of the dataset follows the official split.\nMUSIC-AVQA [52] is designed for audio-visual question answering under musical scenario. It contains 9,288 videos covering 22 instruments, with a total duration of over 150 hours and 45,867 Question-Answering pairs. Each video contains around 5 QA pairs on average. In experiments, we follow the official split for training, evaluation, and test sets."}, {"title": "4.2 Experimental settings", "content": "In our experiments, when not specified, ResNet-18 [53] is used as the backbone. Concretely, for the visual encoder, we take multiple frames as the input, and feed them into the 2D network like [54] does; for the audio encoder, we modified the input channel of ResNet-18 from three to one like [13] does and the rest parts remain unchanged; for the optical flow encoder, we stack the horizontal vector and vertical vector as one frame, then multiple frames are also put into the 2D network as [54] does. In addition, the encoders used for UCF-101 and UCF-101-Three are ImageNet pre-trained and encoders of other datasets are trained from scratch. For the CMU-MOSI dataset, transformer-based networks are used as the backbone and trained from scratch.\nVideos in Kinetics-Sounds, UCF-101, UCF-101-Three and VGGSound datasets are extracted at 1fps and three frames are uniformly sampled as the visual input. Videos in the AVE dataset are extracted frames with 1fps and all ten frames are used as the visual input. The audio data is first re-sampled into 16KHz and transformed into a spectrogram with size 257 \u00d7 1,004 using a window with length of 512 and overlap of 353. For the CREMA-D dataset with only 2-3 seconds video, One visual frame is randomly extracted from each clip and the audio data is processed into a spectrogram of size 257 \u00d7 299 with a window length of 512 and overlap of 353.\nDuring training, we use SGD with momentum (0.9) as the optimizer. For CREMA-D, Kinetics-Sounds, UCF-101, UCF-101-Three, and VGGSound datasets, the learning rate is 1e \u2013 3, and the batch size is 32. For CMU-MOSI, AVE, and MUSIC-AVQA datasets, the learning rate is 1e \u2013 4, and batch size is 64. All models are trained on 2 NVIDIA RTX 3090 (Ti).\nEvaluation metric. For multi-class classification tasks, the widely-used Accuracy and mAP are used as evaluation metric:\n$Acc = \\frac{\\sum_{i=1}^A 1_{\\hat{y}_i = Y_i}}{A}$, (18)\nwhere A is the number of testing samples, and $\\hat{y}_i$ is the"}, {"title": "4.3 Comparison on the multimodal task", "content": ""}, {"title": "4.3.1 Combination with different fusion methods", "content": "We first apply OPM and OGM methods to two vanilla fusion methods: Concatenation and Summation. Additionally, we compared the specifically designed fusion method FiLM [46]. The results on four datasets are as shown in Tab 2. We also provide the performance of individually trained uni-modal models for comparison. It can be observed that the uni-modal performance is imbalanced across different datasets, demonstrating that the discriminative ability of different modalities varies on different datasets. For instance, the performance of audio-only model outperforms the visual-only model on the sound-oriented VGGSound dataset. Furthermore, in some cases, the jointly trained multimodal model can be inferior to the best performing uni-modal models. For example, the performance of the audio-only model on CREMA-D dataset is better than the multimodal model with Summation fusion. Moreover, OPM and OGM can bring considerable improvement to both vanilla and specifically designed fusion methods, which demonstrates the effectiveness and satisfactory flexibility of our strategies. As shown in Fig 4 and Tab 4, although our modulation for dominant modality slows down the overall training process a bit, our OPM and OGM methods do not bring much additional training time with the same training epoch, compared with Concatenation baseline. Additionally, it can be observed that the convergence loss of our OPM and OGM methods is higher than that of the baseline, possibly because our modulation prevents the multimodal model from over-memorizing the training samples.\nIn most cases, the OPM method shows a more obvious enhancement than the OGM method. Based on the specific modulation design, the OPM method directly drops the feature of the better-performing modality in the feed-forward stage, allowing the suppressed modality to fully determine the multimodal prediction and temporarily control the optimization process. OGM weakens the gradient of the dominant modality in the back-propagation stage, slowing down the overall training. The modulation by OPM is stronger, which allows the suppressed modality to receive more targeted optimization efforts.\nIn addition, our methods show a more significant improvement on CREMA-D dataset compared to other datasets. The reason could be that data samples of CREMA-D dataset are recorded videos in controlled environments with less noise, while samples of other datasets are more noisy \"in the wild\" videos. Hence, for CREAM-D dataset, these clean samples are supposed to be easier to learn but suppressed by imbalanced multimodal learning. Then, after our modulation, multimodal model is more effectively enhanced."}, {"title": "4.3.2 Cross-modal interaction scenarios", "content": "Beyond the above vanilla and specifically designed fusion methods, various cross-modal interaction modules are proposed to improve the integration of different modalities in multimodal learning. In this section, we combine our methods with several multimodal models with cross-modal interaction modules, CentralNet [12], VATT [55] and MMTM [56], to evaluate their effectiveness in more complex cross-modal interaction scenarios. CentralNet [12] integrates the uni-modal feature of intermediate layers. VATT [55] uses the cross-modal attention mechanism. MMTM [56] activates the intermediate features of one modality with the guidance of others via the squeeze and excitation module, which"}, {"title": "4.3.3 Comparison with uni-modal modulation methods"}]}