{"title": "A CLOSER LOOK AT MACHINE UNLEARNING FOR LARGE LANGUAGE MODELS", "authors": ["Xiaojian Yuan", "Tianyu Pang", "Chao Du", "Kejiang Chen", "Weiming Zhang", "Min Lin"], "abstract": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, large language models (LLMs) have undergone rapid development, demonstrating impressive capabilities across a wide range of applications, from natural language processing to complex problem-solving. However, this advancement has highlighted significant concerns regarding the potential for LLMs to retain unauthorized content from massive training corpus crawled from the Internet, raising issues related to privacy and copyright (Huang et al., 2022; Carlini et al., 2023; Staab et al., 2024; Ippolito et al., 2023; Dou et al., 2024). These concerns are particularly relevant within legal and regulatory frameworks, such as the Right to be Forgotten (Dang, 2021), which aims to empower individuals to have unauthorized data erased from digital records. Addressing these issues is crucial for ensuring the responsible deployment of LLMs in real-world applications.\nDue to the high cost of retraining LLMs, researchers have explored machine unlearning techniques, namely LLM unlearning (Cao & Yang, 2015; Bourtoule et al., 2021; Yao et al., 2023). The typical paradigm involves fine-tuning the target LLM on a specified set, known as the forget set, to obtain an unlearned model. As described in (Maini et al., 2024; Jin et al., 2024), the unlearned model should meet two primary goals: 1) it should not reveal any information contained in the forget set, and 2) it should maintain performance on the neighbor set, which has a distribution similar to the forget set but is not the target of unlearning, as well as on other tasks with general knowledge. While the first goal is generally easier to achieve, the main challenge lies in meeting the second goal (Liu et al., 2024b; Maini et al., 2024; Zhang et al., 2024a; Ji et al., 2024; Shi et al., 2024a; Wang et al., 2024c).\nIn this paper, we have a closer look at machine unlearning for LLMs. We note that most prior studies (Maini et al., 2024; Ji et al., 2024; Jia et al., 2024; Jin et al., 2024; Shi et al., 2024a) primarily rely on ROUGE (Lin, 2004) as the sole metric for evaluating the output of unlearned models. To more comprehensively assess the model behavior, we introduce three additional metrics that evaluate token diversity, sentence semantics, and factual correctness in the output respectively."}, {"title": "2 PRELIMINARIES", "content": "We define the notation for formalizing the LLM unlearning and introduce the evaluation metrics. We also briefly review baseline methods and categorize them as untargeted and targeted."}, {"title": "2.1 NOTATIONS", "content": "Consider a LLM parameterized by \u03b8, which gives the probability distribution over the next tokens, denoted by p(s; \u03b8) given an input s. The fine-tuning process of a LLM on D = {(x, y)}1 aims to minimize the prediction loss l(y|x; \u03b8) = \u2212 log p(y|x; \u03b8), where p(y|x; \u03b8) is given by p(y|x; \u03b8) =\n\u03a0+1=1P(yt|x0y<t;0). Here, T is the number of tokens in the sequence, yt is the t-th token, y<t is the prefix up to t, and o denotes string concatenation. We use g(s; \u03b8) to represent the generated string. LLM unlearning requires the unlearned model parameterized by \u03b8u to forget a specific subset (i.e., the forget set) DF C D while maintaining performance on the retain set DR = D \\ Dr. Typically, DR consists of two parts: the neighbor set, which contains data with a distribution similar to DF but excludes the unlearning target (Jin et al., 2024), and data encompassing other general knowledge. In some benchmarks such as TOFU (Maini et al., 2024), the retain set actually refers to the neighbor set, and general knowledge is additionally evaluated through other sets (Section 5.1). For consistency, we also use the retain set Dr to refer to the neighbor set in the rest of our paper unless specified."}, {"title": "2.2 EVALUATION METRICS", "content": "To comprehensively evaluate the outputs of the unlearned model on a given set, it is essential to use multiple metrics to capture different behaviors (Maini et al., 2024). We first review the three commonly used metrics in previous work (Zhang et al., 2024a; Jia et al., 2024; Ji et al., 2024; Huang et al., 2024; Liu et al., 2024a), and then introduce three additional metrics, namely TE, CS and ES, to evaluate the token diversity, sentence semantics, and factual correctness in the output respectively.\nROUGE (R) measures the word-level match of the model's output to a question with the ground truth answer. We compute the ROUGE-L recall score (Lin, 2004) between the model's decoded output g(x; \u03b8u) and the ground truth answer y, denote as ROUGE(g(x; \u03b8u), y).\nProbability (P) measures the model's ability to predict the ground truth answer. Given a question, we follow Maini et al. (2024) to compute the normalized conditional probability of the ground truth answer, defined as P(y|x) =  \u2211t=1P(yt|x 0 Y<t; \u03b8u).\nTruth Ratio (TR) measures whether the model prefers correct or incorrect answers to a question. The truth ratio, defined by Maini et al. (2024), is the ratio of the average normalized conditional probability of perturbed answers \u0177 to the normalized conditional probability of a paraphrased answer \u1ef9. The perturbed answer resembles the y but is incorrect, while the paraphrased answer is a rewriting of y. When a model lacks relevant knowledge, it should have similar prediction probabilities for correct and incorrect answers. Formally, this metric is defined as TR(y|x; 0) =  \u03a3\u2081 P(yi|x)/P(\u1ef9|x), which expressed as max(0, 1 \u2013 TR) on the retain set and 1/min(TR, 1/TR) on the forget set.\nToken Entropy (TE) measures the diversity of tokens in the model's output. We obverse that some unlearned models tend to continue generating meaningless tokens after answering a question, as shown in the first row of Table 1. Although the ROUGE is 1, the model's performance clearly degrades. Inspired by (Zhang et al., 2018), we propose the normalized token entropy on the retain set, defined as: TE(g(x;\u03b8u)) = - \u03a3=1 f(wi) log2 f(wi), where |9(x; 04)| denotes the total number of tokens in g(x; \u03b8u), in which there are m unique tokens, and f(wi) represents the frequency of unique token wi. A lower TE means the output contains many repeated tokens with poor readability.\nCosine Similarity (CS) measures the semantic similarity of the model's output before and after unlearning. Inspired by the semantic textual similarity task (Cer et al., 2017), which aims to assess the degree to which two sentences are semantically equivalent to each other. We use the Sentence-BERT (Reimers & Gurevych, 2019) to get sentence embeddings of output before and after un-learning. Then we calculate their cosine similarity and truncate the value less than 0, denote as max(Cos(g(x; 0), g(x; \u03b8u))), 0). As shown in the second row of Table 1, the unlearned model may add a bunch of unexpected or made-up content after answering the question from the retain set, resulting in a lower CS, though it may have a high ROUGE.\nEntailment Score (ES) measures the factual correctness of the model's output for a set of questions relative to the ground truth answers. Text entailment, also known as Natural Language Inference (NLI), is a fundamental task that aims to determine the directional relationship between text frag-ments. It also plays a crucial role in NLP evaluation (Ferr\u00e1ndez et al., 2008; Yao & Barbosa, 2024; Poliak, 2020). Formally, \"t entails h\"(t \u21d2 h) if, typically, a human reading text t would infer that the hypothesis h is most likely true (Wikipedia contributors, 2024). Following Liu et al. (2024c), we use a pre-trained NLI model (Sileo, 2023) to predict the relationship between the pair of the model output and the corresponding ground truth for each question. As shown in the third row of Table 1, the wrong name in the output lead to an incorrect fact, its prediction label will be \u201ccontradiction\". Then we calculate the proportion of pairs in the set that are predicted to be \u201centailment\u201d and use this as the entailment score, which should be higher on the retain set and lower on the forget set.\nAggregated Metrics. All of the metrics listed above range from zero to one, so we can aggregate them into a single metric. For unlearning tasks, there are usually two aspects to consider. An unlearned model should have both high model utility and forget efficacy as follows:"}, {"title": "2.3 BASELINE UNLEARNING METHODS", "content": "We focus on unlearning methods based on parameter optimization, i.e., unlearning fine-tuning, as this is still the mainstream formulation (Yao et al., 2023; Maini et al., 2024; Zhang et al., 2024a; Liu et al., 2024c; Jia et al., 2024; Zhang et al., 2024b; Jin et al., 2024) and is orthogonal to other methods, such as detection-based (Gao et al., 2024) or input processing (Liu et al., 2024a). Unlearning fine-tuning modifies the internal mechanism of the model without preserving the original parameters, which is more in line with the requirements of the Right to be Forgotten (Zhang et al., 2023).\nForget Loss. Based on how the unlearned model handles the knowledge to be forgotten, we categorize existing methods into two paradigms: Untargeted Unlearning and Targeted Unlearning. For untargeted unlearning, the unlearned model only needs to forget what was specified, but how it will respond on the forget set is unknown. We consider the following two advanced methods:\nGradient Ascent (GA) can be regarded as the most straightforward way for untargeted unlearning. Its main idea is to perform an optimization on the model that is opposite to the training objective. Specifically, GA maximize the predicted loss l(y|x; \u03b8) on the forget set as follows:\n$L_{GA}(D_{F}; \\theta) = -E_{(x,y)\\sim D_{F}} [l(y|x; \\theta)] = -E_{(x,y)\\sim D_{F}} [- log p(y|x; \\theta)] $.\nNegative Preference Optimization (NPO) (Zhang et al., 2024a) is a variant based on Direct Preference Optimization (DPO) (Rafailov et al., 2024), which regards unlearning as a preference optimization problem. Specifically, it treats answers in the forget set as negative samples that do not match preferences, and ignores positive terms in the DPO loss, as follows:\n$L_{NPO} (D_{F}; \\theta) = \\frac{2}{\\beta}E_{(x,y)\\sim D_{R}} [log\\sigma (\\beta log \\frac{p(y|x; \\theta)}{p(y|x; \\theta_{ref})})].$\nwhere \u03c3(t) = 1/(1+e\u00aft) is the sigmoid function, \u03b2 is a hyper-parameter and bref is a reference model which is always equivalent to the initial model during unlearning process. NPO can essentially be regard as a variant of GA with adaptive gradient weights (Zhang et al., 2024a).\nFor targeted unlearning, the goal is to hope that the unlearned model can output specified responses on DF, such as rejection templates like \"Sorry, I don't know.\". This paradigm is generally more user-friendly and we consider the following two methods:\nIDK Fine-tune (IDK) (Maini et al., 2024) transforms unlearning task into a instruction tuning problem and relabels the question in the forget set with a random response from DIDK, which contains 100 rejection templates like \u201cI don't know.\u201d(IDK). The loss of IDK is defined as:\n$L_{IDK} (D_{F}, D_{IDK}; \\theta) = E_{x\\sim D_{F},y\\sim D_{IDK}} [l(y|x; \\theta)] = E_{x\\sim D_{F},y\\sim D_{IDK}} [-log p(y|x; \\theta)].$\nDirect Preference Optimization (DPO) (Zhang et al., 2024a) directly adopts the standard DPO loss (Rafailov et al., 2024) for unlearning tasks. It uses answers in the forget set as negative samples and rejection templates in DIDK as positive samples to perform preference optimization.\nRegularization Loss. The above losses solely consider the unlearning objective on the forget set, whereas an effective unlearning method must also focus on the utility preservation. Therefore, a regularization loss on the retain set (neighbor set) is often incorporated to maintain the utility of the model while unlearning. Here, we consider the two most common regularization losses (Maini et al., 2024; Zhang et al., 2024a; Liu et al., 2024c; Jia et al., 2024) as follows:\nGrad Descent (GD) simply uses the prediction loss during training to perform gradient descent on the retain set, as follows:\n$L_{GD}(D_{R}; \\theta) = E_{(x,y)\\sim D_{F}} [-log p(y|x; \\theta)] $."}, {"title": "3 MAXIMIZING ENTROPY FOR UNTARGETED UNLEARNING", "content": "We first discuss that the behavior that existing untargeted unlearning try to approximate is unpredictable and may has risks of hallucination. Then we propose to employ the objective of maximizing the prediction entropy for each next token to achieve untargeted unlearning."}, {"title": "3.1 DISCUSSION ON THE OBJECTIVE OF UNTARGETED UNLEARNING", "content": "Traditional machine unlearning ideally expects the unlearned model to be behaviorally indistinguish-able on DF from a retain model, which is retrained from scratch on Dr. To approach this objective, the core of most untargeted unlearning is to adopt a gradient ascent procedure (or its variant) on the prediction loss over DF, based on the intuition of \"reverting\" gradient descent optimization in the training phase (Zhang et al., 2024a; Yao et al., 2023), with the hope that the unlearned model may approximate the behavior of the retain model. However, we discuss that this objective for untargeted unlearning may have several challenges in the context of LLMs.\nThe behavior of the ideal retain model is unpredictable. In traditional classification tasks, since the model size is relatively small and the labeled dataset is unambiguous, it is possible for researchers to train a ideal retain model from scratch for evaluation purpose under the unlearning objective, that is, compare the indistinguishability of the unlearned model and the ideal retain model on some metrics, such as the accuracy on DF (Nguyen et al., 2022). In the context of LLMs, the computational cost of retraining is prohibitive for most people. More critically, the massive training corpus makes it difficult to locate and remove all relevant content about a specific unlearning target to obtain a retain set for retraining (Liu et al., 2024b; Maini et al., 2024). Consequently, it is impractical for LLM unlearning to assume a ideal retain model for evaluation purpose. Since the output of LLMs is natural language, the behavior also becomes much more flexible and ill-defined (Yao et al., 2023). We actually have no way of predicting how the ideal retain model will behave on DF.\nPotential hallucinations in the surrogate retain model. Due to the impracticality of obtaining an ideal retain model in the context of LLMs, Maini et al. (2024) propose a fictional benchmark to obtain a surrogate retain model for evaluation. Instead of unlearning what the model already knows, they fine-tune a base model, such as Llama2, on a small fictitious dataset Df = {D, D\u00a3} to create the model used for unlearning. Then a surrogate retain model can be obtained by re-fine-tuning the base model on Df, leaving the remaining Df as the forget set. As shown in the last row of Table 1, we observe that this surrogate retain model exhibits the hallucination phenomenon (Huang et al., 2023) on the forget set, producing plausible but factually incorrect outputs to unseen samples. The average ROUGE between the surrogate retain model's outputs on the forget set and the ground truth answers is 0.4082, indicating that they still have significant overlap. For further confirmation, we prompt the GPT-40 to judge whether the output to a question is considered a \u201challucination\u201d (Appendix A). The result shows that the output for 74% of the questions in the forget set can be judged as hallucinations. Even if an unlearned model is evaluated to approximate the surrogate retain mode under this benchmark, such hallucination behavior on the forget set may still pose a litigation risk when users request unlearning. For example, users may ask: is the model providing direct but not entirely correct responses due to unlearning process, or this hallucination is simply an innate limitation of the LLM itself (Xu et al., 2024)? Furthermore, others may believe and spread these reasonable but incorrect responses, which may force users to expose facts for clarification. To reduce this risk, we recommend that unlearned models avoid generating relevant content, or directly exhibit ignorance of relevant knowledge (Section 4). Some additional discussion is in Appendix F."}, {"title": "3.2 OUR APPROACH", "content": "According to the above discussion, on the one hand, the behavior of the retain model is unpre-dictable, making it difficult for us to determine a possible guidance or direction for untargeted un-learning. On the other hand, we hope that the unlearned model obtained by untargeted unlearning can avoid generating relevant content in the forget set, reducing the potential risk of hallucinations.\nAs a result, we try to align the prediction behavior of the unlearned model on the forget set with that of a randomly initialized model. Our intuitions are: 1) the randomly initialized model is data-independent and does not contain any knowledge about the forget set, avoids the leakage of relevant information. 2) the behavior of the randomly initialized model on DF is random guessing, that is, its predicted distribution for each next token always has maximum entropy, which is more well-defined.\nIn practical, we minimize the KL divergence between the predicted distribution for each token and a uniform distribution with vocabulary size. Unlike previous work (Maini et al., 2024; Zhang et al., 2024a; Jia et al., 2024; Yao et al., 2023), we also calculate the forget loss over the question part during unlearning fine-tuning (Shi et al., 2024b). Formally, let x' = x \u25ca y represent the sample after concatenating x and y, we minimizing the following forget loss:\n$L_{ME}(D_{F}; \\theta) = E_{(x,y)\\sim D_{F}} [\\sum_{t=1}^{T} KL(P_{t}||U[K])]$\nwhere Pt = p(x4|x'<t; 0) is the predicted probability for the t-th token in x' = x \u00b0 y and U[K] is a uniform distribution over the vocabulary of size K, where each value is 1/K. Minimizing Eq. (6) is equivalent to Maximizing Entropy (ME) of predicted distribution for each next token (Appendix B). The greater the entropy, the higher the uncertainty of the prediction, indicating that the model be-haves closer to a randomly initialized model for random guessing. This objective also avoids catas-trophic collapse caused by the unbounded forget loss (Zhang et al., 2024a; Ji et al., 2024).\nFinally, we use GD for regularization and get the approach ME+GD for untargeted unlearning:\n$L_{MG+GD}(\\theta) = \\alpha L_{ME}(D_{F};\\theta) + L_{GD}(D_{R}; \\theta),$\nwhere \u03b1 is a hyper-parameter to control the strength of unlearning."}, {"title": "4 MITIGATE EXCESSIVE IGNORANCE OF TARGETED UNLEARNING", "content": "We analyze that previous regularization losses cannot prevent the unlearned model from becoming excessively ignorant during targeted unlearning. To mitigate this issue, we propose using answer preservation loss for regularization and conduct gradient analysis to demonstrate its rationality."}, {"title": "4.1 LACK REGULARIZATION AGAINST REJECTION TEMPLATES", "content": "As a more user-friendly paradigm, targeted unlearning is thought to easily cause the unlearned model becoming overly ignorant, refusing to answer most questions in the retain set (Maini et al., 2024; Zhang et al., 2024a; Liu et al., 2024c).\nDifferent impacts on the retain set of targeted unlearning. Since the distributions of the forget set and the retain set are similar, i.e., (XF, YF) \u2248 (XR, VR), decreasing the probability of answers in the forget set, i.e., Pr(XF|VF), during untargeted unlearning, will easily decrease Pr(XR|VR), as shown in Figure 2. Previous regularization mainly focus on maintaining Pr(XR|VR) during the unlearning process. However, we observe that targeted unlearning methods, such as IDK+RT, have little effect on the probability of the original answers, as the primary objective is to increase Pr(VIDK|XF), where VIDK is the distribution of rejection templates. The objective of targeted unlearning may also lead to an increase in Pr(VIDK XR), given that XR \u2248 XF. As shown in Figure 3, the downward"}, {"title": "4.2 OUR APPROACH", "content": "Intuitively, given a question in the retain set, the regularization loss for targeted unlearning should satisfy two objectives: 1) Reduce the probability of the rejection template. 2) Maintain the proba-bility of the original answer. Thus, we propose the Answer Preservation (AP) loss as follows:\n$L_{AP}(D_{R}, D_{IDK}; \\theta) = \\frac{1}{\\beta}E_{(x,y)\\sim D_{R},y'\\sim D_{IDK}} log\\sigma (-\\beta log \\frac{p(y'|x; \\theta)}{p(y|x; \\theta)}),$ \nwhere \u03c3(\u00b7) is the sigmoid function, \u03b2 is a hyper-parameter. Eq. (8) is somewhat similar in form to preference optimization (Zhang et al., 2024a; Rafailov et al., 2024), but it does not need a reference model and used for regularization rather than forgetting. We also perform gradient analysis on AP loss in Appendix C, and obtain the result as follows:\n$\\nabla_{\\theta}L_{AP}(\\theta) = E_{D_{R},D_{IDK}} [W_{\\theta}(x, y, y')\\nabla_{\\theta} (log p(y'|x; \\theta) \u2013 log p(y|x; \\theta))].$\nThe Wo(x,y,y') = 1/(1 + (P(y/x;0)can be regarded as an adaptive gradient weight. Given a question x in Dr, in the early stage of unlearning process, where p(y|x;0) > p(y'|x; 0), we have Wo(x, y, y') \u00ab 1. As the unlearning proceeds, either a decrease in p(y|x; 0) or an increase in p(y'x; 0) will result in a larger Wo(x, y, y'), thereby providing stronger regularization. The gradient of AP loss consists of two terms in addition to the adaptive weight. The first term is equivalent to GA on the rejection template, which satisfies the first objective. The second term is equivalent to GD on the original answer, which satisfies the second objective.\nFinally, we combine AP as a regularization loss with IDK to get the approach IDK+AP:\n$L_{IDK+AP}(\\theta) = L_{IDK}(D_{F}; \\theta) + L_{AP}(D_{R}, D_{IDK}; \\theta).$"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "We show the main results for three different scenarios, namely fictitious unlearning, continual un-learning and real-world unlearning. More experimental results can be found in Appendix E."}, {"title": "5.1 FICTITIOUS UNLEARNING SCENARIO", "content": "Setup. The standard TOFU benchmark (Maini et al., 2024) simulates an ideal scenario where the training data is fully accessible. It constructs a dataset with 200 fictitious authors, each containing 20 question-answer pairs. It has three levels of tasks, namely forget01, forget05 and forget10, to forget 1%, 5%, and 10% of the constructed data. The complement of each forget set serves as the retain set (i.e., neighbor set). It also provides two extra sets, namely Real Authors and World Facts, to evaluate the utility on general knowledge. Following Maini et al. (2024), we also calculate all metrics on these two extra sets and take the harmonic mean together with the metrics on the retain set as the final MU. We use the Llama2-chat-7B released by TOFU as the target model, which has been fine-tuned on the constructed data to ensure it can exactly gives answers to questions in TOFU.\nResults of untargeted unlearning. Figure 4 shows the results of different untargeted unlearning methods. Overall, our ME+GD achieves an surprising balance between MU and FE during un-learning, maintaining a stable MU with the highest FE. For forget01, the MU of all methods is within an acceptable range, while baselines except ME+GD exhibit insufficient unlearning, i.e., the FE is relatively low. For forget05 and forget10, the GA-based method will quickly reduce MU in the early stage, but GA+GD can recover a certain MU when the FE reaches the bottleneck, which is also observed in (Maini et al., 2024). Although the NPO-based method can alleviate the reduce of MU, the FE is difficult to further improve, even if we increase the unlearning steps, as shown in Figure 9. In contrast, our ME+GD can continuously increase FE while maintaining the MU on all tasks.\nResults of targeted unlearning. Figure 5 shows the results of different targeted unlearning methods. Overall, only our IDK+AP maintains a stable MU on all three tasks. For forget01, there is not much difference in MU between different methods, except that the DPO-based method has lower FE. However, as the size of forget set increases, the MU of baselines will quickly decrease and make the unlearned model overly ignorant, while our IDK+AP will still maintain a high MU.\nAdditional results. We provide more results, including: detailed quantitative results on TOFU (Appendix E.1), results of two variants, namely ME+KL and DPO+AP (Appendix E.2), an ablation on a in Eq. (7) (Appendix E.4), an ablation of whether to calculate ME loss over the questions (Appendix E.5), unlearning fine-tuning using LoRA (Hu et al., 2022) (Appendix E.8), and samples generated by different methods (Appendix G)."}, {"title": "5.2 CONTINUAL UNLEARNING SCENARIO", "content": "Task definition. In practice, LLM unlearning may necessitate multiple processing of different un-learning requests, i.e., continual unlearning. To the best of our knowledge, there is currently no benchmark for continual LLM unlearning. To accommodate this scenario, we extend the TOFU benchmark by continually unlearning 1% (forget01), 5% (forget05), and 10% (forget10) of the con-structed data for N times respectively. For continual forget01 and continual forget05, we set N to 10, meaning that 10% and 50% of the data are unlearned in total, respectively. For continual forget10, we set N to 9 because at least 10% of the data needs to be retained for regularization and evaluation purposes in TOFU benchmark. When unlearning a certain subtask, all remaining unforgotten data is used as the retain set. Consequently, as the number of unlearning tasks increases, the retain set available for regularization will gradually decrease. After each unlearning subtask, we calculate corresponding MU and FE of the unlearned model.\nMain results. In Figure 6, we compare the ability of different unlearning methods to maintain MU in this continual scenario, as this is considered to be more challenging (Gao et al., 2024; Dou et al., 2024; Shi et al., 2024a). For continual forget01 scenario, all baselines except IDK+GD will reduce MU to nearly zero on a certain subtask. In contrast, our ME+GD and IDK+AP can maintain MU at a higher value throughout the continual unlearning period. For continual forget05 and forget10 sce-"}, {"title": "5.3 REAL-WORLD UNLEARNING SCENARIO", "content": "Setup. We consider a more realistic scenario where the knowledge to be unlearned is inherent in the target model and the training data are unknown. Liu et al. (2024c) identified several real-world individuals with deep memorization from Llama-3-8B-Instruct (Llama3) and provide 20 questions and golden answers for each individual. We first select 20 individuals as unlearning targets and use Llama3 to get responses for each question to construct the forget set. We then select different 40 in-dividuals as the neighbor set, 20 of which were used for regularization during unlearning, consistent with the forget set size, and the other 20 are used to evaluate the MU. We also perform evaluation on five downstream tasks MMLU (Hendrycks et al., 2020), ARC-c (Clark et al., 2018), GSM8K (Cobbe et al., 2021), TriviaQA (Joshi et al., 2017), TruthfulQA(MC1) (Lin et al., 2021), which measure gen-eral ability, reasoning ability, arithmetic ability, factuality and truthfulness, respectively.\nMain results. The results are shown in Table 2. For untargeted unlearning, our ME+GD method achieves the best performance on the unlearning task and is the only method capable of maintain-ing high MU and FE simultaneously. ME+GD exhibits superior overall performance on downstream tasks. In contrast, both GA-based and NPO-based methods struggle to maintain MU on the neighbor set during unlearning and significantly reduce average performance on downstream tasks. Notably, all methods except ME+GD significantly degrade performance on TriviaQA. We speculate that this is because the short question-answer format of TriviaQA closely resembles the format in the forget set, making it more susceptible to the unlearning process. For targeted unlearning, the main advan-"}, {"title": "6 RELATED WORK", "content": "Memorization concerns of LLMs. LLMs have demonstrated powerful capabilities through learn-ing from extensive corpora. However, numerous studies have shown that they may inadvertently memorize information from training corpus (Huang et al., 2022; Carlini et al., 2023; Staab et al., 2024; Ippolito et al., 2023), which poses significant privacy and copyright concerns. With the enact-ment and widespread adoption of regulations such as the European Union's General Data Protection Regulation (GDPR) (Regulation, 2016) and the California Consumer Privacy Act (Pardau, 2018), which include provisions for the Right to be Forgotten (Dang, 2021), users may request the removal of specified content from LLM-driven applications. To protect users' legitimate interests and miti-gate the risk of legal repercussions, these models must comply with such requests and avoid provid-ing relevant responses (Si et al., 2023). Retraining the model on the filtered data is straightforward but impractical, particularly in the context of LLMs (Kandpal et al., 2022; Liu et al., 2024b).\nMachine unlearning for LLMs. Initially developed for classification tasks (Bourtoule et al., 2021), machine unlearning has recently been applied to LLMs. Mainstream methods primarily rely on pa-rameter optimization (Jang et al., 2023; Yao et al., 2023; Maini et al., 2024; Wang et al., 2024b; Li et al., 2024; Yao et al., 2024; Ishibashi & Shimodaira, 2023; Gu et al., 2024; Zhang et al., 2024a; Lu et al., 2024; Jia et al., 2024; Tian et al., 2024; Liu et al., 2024c; Choi et al., 2024; Tang et al., 2024; Tamirisa et al., 2024). This typically involves fine-tuning the model on a forget set to produce an unlearned version. Users may trust this paradigm more because it internally modifies the model's parameters and mechanisms. However, such methods are more likely to harm the overall perfor-m"}]}