{"title": "Automated Text Scoring in the Age of Generative AI for the GPU-poor", "authors": ["Christopher Ormerod", "Alexander Kwako"], "abstract": "Current research on generative language models (GLMs) for automated text scoring (ATS) has focused almost exclusively on querying proprietary models via Application Programming Interfaces (APIs). Yet such practices raise issues around transparency and security, and these methods offer little in the way of efficiency or customizability. With the recent proliferation of smaller, open-source models, there is the option to explore GLMs with computers equipped with modest, consumer-grade hardware- that is, for the \"GPU poor.\" In this study, we analyze the performance and efficiency of open-source, small-scale GLMs for ATS. Results show that GLMs can be fine-tuned to achieve adequate, though not state-of-the-art, performance. In addition to ATS, we take small steps towards analyzing models' capacity for generating feedback by prompting GLMs to explain their scores. Model-generated feedback shows promise, but requires more rigorous evaluation focused on targeted use cases.", "sections": [{"title": "Introduction", "content": "Generative language models (GLMs), such as GPT-4 [34] and Claude [2], have demonstrated pow-\nerful performance across a variety of language and reasoning tasks. In the field of education,\nresearchers are exploring the extent to which these models can perform tasks such as automated\nessay scoring [56], providing feedback to students [4], individual tutoring [7], and more [15].\nAlthough GLMs show promise in automating certain educative tasks, there are critical limita-\ntions that hinder the possibility of wider implementation. For instance, researchers have shown that\nGLMs can be \u201cjail-broken\u201d to bypass safety guardrails [58] and can disclose personally identifiable\ninformation. Large GLMs are extremely large, requiring millions of dollars to train and deploy; as\nsuch, they are highly inefficient for specialized tasks [26]. These models are constantly being up-\ndated, sometimes leading to degraded performance [6], and they are only accessible via Application\nProgramming Interfaces (APIs), which lead to issues around replicability and leave little room to\nconduct rigorous research.\nIt is for these reasons that we shift the focus away from large, proprietary GLMs toward smaller,\nopen-source GLMs. In this study, we focus on two educational applications: Automated Text\nScoring (ATS) and providing feedback\u2014specifically, feedback that justifies scores based on the\nscoring rubric. Our study is the first to demonstrate that it is possible to efficiently fine-tune such\nGLMs to yield high-quality scores, and that (at least some) feedback from fine-tuned models can\nexplain these scores. Our data is drawn from the publicly available Automated Student Assessment"}, {"title": "Background", "content": "AES and ASAS have been active areas of research and development since as early as 1966 [38].\nThere is widespread acceptance that, when carefully constructed and monitored, AES and ASAS\ncan deliver reliable scores [30]. For this reason, ATS has become common in educational assessment.\nFrom a machine-learning perspective, both AES and ASAS are text classification problems, but\nfrom a measurement perspective, they assess different abilities and may require different approaches.\nFor instance, rubrics for essay scoring are often designed to evaluate attributes such as organization,\nargumentation, grammar, and spelling in lengthier written responses. In contrast, rubrics for short\nanswer questions focus on assessing specific knowledge and comprehension, often independent of\ngrammatical and spelling considerations. For this reason, an approach that works well for AES\nmay not always be suitable for ASAS and vice versa.\nThere have been a plethora of approaches applied to both AES and ASAS. Perhaps the oldest\nof these is known as the Bag of Words (BoW), which generally combines rules based on linguistic\nfeatures in addition to a set of frequency-based features [38]. As Natural Language Processing\n(NLP) began incorporating neural network-based models, these models were applied to AES and\nASAS. Early implementations of neural network-based scoring [14, 37] used layers of recurrent units\nsuch as the long-short-term memory (LSTM) unit [20] and gated recurrent units (GRU) [8] with\nattention [17].\nThe most influential change to NLP has been the rise of attention [16] and the transformer\narchitecture [53]. The use of transformer-based Large Language Models (LLMs), such as BERT\n[13], to perform ATS is now well-established in both AES [43, 51, 60] and ASAS [35]. In the past few\nyears, generative language models (GLM)s like ChatGPT [34] have garnered immense excitement"}, {"title": "Model-generated feedback", "content": "If we limit our research into GLMs merely to improve existing scoring systems, then we will have\nmissed out on the potential to enhance educational assessment. There is a growing call from\neducators, students, and other stakeholders for these models to be used to provide feedback.\nAlthough model-generated feedback holds potential value for educators, there remain substantial\nhurdles to producing feedback that is useful. These limitations revolve around the the quality of\nfeedback itself, as well as the difficult endeavor of validating that the feedback is indeed useful in\na given context. With respect to feedback quality, even large GLMs produce hallucinations. In\nthe field of text generation, hallucination refers broadly to text that, while grammatically correct,\nis also nonsensical, unfaithful, unreliable, inaccurate, irrelevant, etc. [22, 61]. With respect to\nvalidation, there is no methodology in the field that can be used to easily validate such feedback.\nThere are, moreover, no easy-to-implement systems to capture feedback in an on-going way from\neducators, which makes development of process-oriented tools extremely challenging.\nBeyond technological limitations, there are social implications that need to be considered in the\nface of novel educational technologies. The Substitution Augmentation Modification Redefinition\n(SAMR) model for technological innovation and adoption in educational settings, for instance, has\nbeen critiqued for justifying hierarchical approaches to product development and implementation\n[18]. Technological advances which are described or marketed as educational tools need to be\ndeveloped in tandem with teachers, administrators, and other educational practitioners. Although\nmuch of the enthusiasm (as well as economic pressure) behind feedback generation is warranted,\nthis cannot supersede the need for taking a rigorous and ethical approach towards researching and\ndeveloping such tools."}, {"title": "Architecture of Generative Language Models for the GPU Poor", "content": "In contrast to the large, proprietary GLMs that have dominated public attention, there is a concomi-\ntant open-source movement that strives to makes GLMs accessible to all. These relatively small,\nopen-source models are typically released in 7Gb and 70Gb versions by researchers who are often\naffiliated with the same organizations that develop proprietary GLMs. For instance, Google recently\nreleased Gemma, Meta released Llama-3, and Microsoft released Phi-3. In contrast to their large,\nproprietary counterparts, these GLMs can run on (and can even be trained on) consumer-grade\nhardware, such as a single 24Gb GPU. That is, these models can be leveraged by the \"GPU poor\""}, {"title": "Training Generative Language Models for the GPU Poor", "content": "LORA is a powerful, parameter-efficient technique for fine-tuning GLMs. In combination with\nquantization, it makes it possible to fine-tune GLMs using less than 8Gb of memory, thereby\nmaking them more feasible for development and deployment.\nThe central idea behind LoRA is that we seek to update the large feed-forward layers of the\nmodel by only considering a low-rank additive component, initially set to 0. Mathematically, we\nsuppose a linear layer is represented by\n\\(L(x) = W_ox + b\\)\nwhere \\(W_o \\in \\mathbb{R}^{d \\times k}\\) is the original pretrained weight matrix and \\(x\\) is the input. It is known that\nupdates to the linear transformations are sparse and in many cases, approximated well by matrices\nof low-rank. We seek to update the weight matrix, \\(W \\rightarrow W\\) in a single step by\n\\(W = W_o + \\delta W = W_o + BA\\)\nwhere \\(A \\in \\mathbb{R}^{r \\times k}\\) and \\(B\\in \\mathbb{R}^{d \\times k}\\).\nIn this setting, it is expected that \\(r << min(d, k)\\) so that the number of trainable parameters\nis \\(r(k + d)\\). Typical values of \\(r\\) (e.g., \\(2 < r < 32\\)) are chosen such that the number of trainable\nparameters is far fewer than full-parameter fine-tuning.\nThe advantages of LoRA include reduced memory requirements for saving fine-tuned models,\nmore efficient training, no impact on inference speed, and the capacity for combination with other\nparameter efficient fine-tuning methods. The memory requirements for saving a fine-tuned large"}, {"title": "Methods", "content": "The Automated Student Assessment Prize (ASAP) AES and SAS datasets were originally made\navailable to the public via two competitions hosted by Kaggle in 2012 [44]. The AES dataset\nencompasses a total of 12,978 essays, spanning 8 distinct stimuli.4 The SAS dataset consists of\n17,043 total responses across 10 items that span various subjects, administered to students in\ngrades 8 and 10 (depending on the item). Each response was scored by two human annotators.\nAccompanying the scored data are comprehensive scoring rubrics that include scoring guidelines\nand score ranges tailored to each stimulus. One of the advantages of using the AES and SAS\ndatasets are that they are commonly used by other researchers, allowing us to compare our results\nwith a wide range of previously established approaches.\nIn order to maintain comparability with the extensive literature on these datasets, test-train\nsplits were chosen to align with previous studies [49, 14, 43, 51, 60, 36, 35, 27, 28]. For the AES\ndataset, we follow the five-fold cross-validation defined by [49]. For the SAS dataset, we used the\nsame splits used in previous studies (e.g., [35, 28]. The (average) size of the training, development\n(or dev), and test sets for the AES and SAS datasets, in addition to some basic characteristics of\nthe datasets, are presented in Table 1\nThe scoring rubric for the AES dataset emphasizes proper spelling and grammar usage, logical\norganization with smooth transitions between ideas, and the ability to exhibit analytical compre-\nhension backed by supporting evidence. The rubrics for essay set 1, 7, and 8 do this by breaking the\nscore into several traits. The final score is the sum of each of the trait scores. While some of the\nessay topics depend on a particular prompt, the rubric can be generally interpreted independently\nof any prompt.\nIn contrast, the rubrics for the SAS items focus on specific pieces of information that need to\nbe in a response in order to obtain a score. These short answer questions are designed to test\nknowledge and comprehension, hence grammar and spelling are not a part of the rubric."}, {"title": "Performance Metric", "content": "When evaluating the model performance, we compute Quadratic Weighted Kappa (QWK), which\nwas the original metric specified in the Kaggle competitions [44, 45]. A rough interpretation of\nthis metric is that it measures the probability above chance that two raters agree: a QWK of 1\nindicates exact agreement, O indicates random agreement, and -1 indicates perfect disagreement.\nThis metric is also standard in the industry for comparing machine scoring performance [54]."}, {"title": "Parameter-efficient fine-tuning", "content": "Models were loaded through Huggingface-hub, quantized into smaller, 4-bit models using bitsand-\nbytes, and trained using low-rank adaptors (LoRA). Learning rate was set to 2e-4 (except for\nGemma-1.1, which was set to 1e-4 to ensure convergence), with a linear rate decay over 10 epochs.\nr and a, key parameters for LoRA, were each set to 32. Table 3 lists how this r value affects\ntrainable parameters and memory used for each of the four models."}, {"title": "Prompting for Score Prediction", "content": "We used the following template to prompt the model for a score, given an item-specific max score,\nan item-specific rubric, and a student response (all indicated by curly brackets below). Note that\n\"User\" and \"Assistant\" role formats vary between models; roles were not entered into the prompt\nitself, but handled automatically via Huggingface's apply_chat_template function.\nUser You are a grading assistant. Assign a **Score** between 0 and {max_score} using\nthe **Rubric** provided to a **Student Response**\n*Rubric**\n{item_rubric}\n*Student Response**\n{student_response}\nAssistant Score:\nUsing the filled-out template as input, we constrained the model to generate one additional\ntoken. If the model generated a non-integer token, then the score was given a 0."}, {"title": "Prompting for Feedback Generation", "content": "After prompting for score predictions, we incorporated the predicted scores into another template\nto prompt the model for feedback generation. Although much of the feedback generation template\nis identical to the score prediction template, the model was prompted separately. A maximum of\n256 new tokens were produced for AES feedback and 128 tokens for SAS.\nUser You are a grading assistant. Assign a **Score** between min_score and {max_score}\nusing the **Rubric** provided to a **Student Response**\n*Rubric**\n{item_rubric}\n*Student Response**\n{student_response}\nAssistant Score: {predicted_score}\nUser Using the rubric, specify why you gave the response a score of {predicted_score}.\nAssistant The response was given a score of {predicted_score} because"}, {"title": "Qualitative Analysis of Feedback", "content": "To characterize the differences in feedback provided by each of the 4 models, we sampled student\nresponses with predicted scores that matched human rater scores. For the SAS dataset, we sampled\nresponses across all possible score points for 2 science items (Items 1 and 10) and 2 ELA items\n(Items 3 and 7). We analyzed 13 student responses across 4 items (and 2-3 possible score points),\nfor a total of 52 explanations. For the AES dataset, we sampled responses across all possible score\npoints for 2 stimuli (Items 2 and 3). We analyzed 10 student responses across 4 items (and 4-6\npossible score points) for a total of 40 explanations.\nIn analyzing responses, we took a grounded approach (Creswell and Poth, 2016 - add citation).\nThe philosophy behind grounded qualitative research is to let patterns emerge from the data, rather\nthan approach the data with pre-defined codes or hypotheses. More specifically, analyses consisted\nof two phases. In the first phase, we read through responses, noted salient trends, summarized\nnotes, and revisited notes for each response. In the second phase, we summarized these notes into\ngeneral patterns and trends, and identified consistent and inconsistent examples in the data."}, {"title": "Results", "content": "Results are divided into four section: In sections 1 and 2, we present the results of fine-tuned GLMS\non AES and ASAS, respectively; in sections 3 and 4, we characterize feedback after prompting\nGLMs to explain their scores based on item-specific rubrics, for AES and ASAS, respectively."}, {"title": "Automated Essay Scoring", "content": "Table 4 presents the results of fine-tuned GLMs on performing AES on the ASAP-AES datset. We\nprovide comparisons to several notable benchmarks pertinent to the task. These include the original\nhuman-human agreement score [44], the BoW results reported in [49] and subsequent modifications\nusing attention mechanisms [14], the original BERT results [43], the current SOTA performance\n[57], \"fine-tuned\" GPT-3.5 [31], and GPT-4 [55]. In addition to these important reference points, we\nalso provide results from off-the-shelf, i.e. not fine-tuned, models (no asterisks) alongside fine-tuned\nmodels (indicated with asterisks).\nThe fine-tuned generative models performed well compared to standard benchmarks. They ex-\nceeded performance of AES, BERT (base), fine-tuned GPT-3.5, and the combination of LSTM,\nCNN, and attention mechanisms. Although none of the models achieve the current SOTA perfor-\nmance (a distinction held by NPCR), each individual model surpasses many previous benchmarks.\nFine-tuned GLMs also seem comparable, if not above, human-level performance."}, {"title": "Automated Short Answer Scoring", "content": "The performance of GLMs fine-tuned for ASAS are presented in Table 5. Fine-tuned models are\nindicated with asterisks. As with AES, there are a number of important results in the literature to\ncompare against our own results. Firstly, there is the human agreement score [45], the rule-based\napproach known as AutoSAS [28], the current SOTA given by an ensemble of pretrained models\n[35], \"fine-tuned\" GPT-3.5 [5], and GPT-4 [24]. Results from non-fine-tuned versions of each of the\n4 models (no have asterisks) are also included.\nIn contrast with AES, the results of pertaining these large models offers comparable, but not\nsuperior, performance to BERT. The GLMs seem do outperform previous benchmarks on items 7\nand 8; the results for Gemma and Mistral are above previously known models [35]. The performance\non items 4 and 9, however, are lower than the benchmarks provided."}, {"title": "Automated Feedback for Essay Scoring", "content": "After GLMs predicted scores, we prompted them for feedback in this case, an explanation for the\nscore based on the scoring rubric. To illustrate the type of feedback generated by each of the four\nmodels, we present the feedback generated in response to an essay on item 1 (Table 6). The essays\nwas assigned a score of 8 by all GLMs.\nBy examining the feedback across items, responses, and models, we found that the feedback\nprovided by fine-tuned versions of Mistral and Gemma tended to be more repetitive as the models\nseemed to settle into a loop more readily than Phi-3 and Llama-3. For stimuli where the rubric\nrelied on external information, such as the understanding of a text, the language models struggled to\nproduce sensible feedback and often only summarized and reiterated aspects of the response, rather\nthan detailing why the score was assigned. The models seem to provide much clearer feedback\nwhen the rubric could be interpreted independently of the stimuli (i.e. 1, 2, 7, and 8).\nThe most useful feedback overall seemed to come from fine-tuned versions of the Phi-3 and\nLlama-3 models. Even though they provided the most accurate explanations, they were not immune\nfrom repetition or errors."}, {"title": "Automated Feedback for Short Answer Scoring", "content": "In Table 7, we present feedback from for a 1-point response to Item #10. We selected this particular\nresponse because model feedback was typical of what we observed for other items and score points.\nFor Item #10, to get full credit (2 points), the student had to (1) \u201cdescribe how [a chosen color]\nmight affect the inside of the doghouse\" and (2) \u201cuse results from the experiment to support [their]\ndescription.\" The student response for this particular example reads, \u201cblack. it might effect it,by\nusing this color it can make the doghouse more warmer on summer days\" (Id: 26865). The response\ndoes state that the color black would make the doghouse warmer (1 point), but fails to reference\nthe experiment (0 points). Because it met 1 of the 2 criteria outlined in the rubric, it received a\nscore of 1. Table 7 provides the explanations given by each of the 4 models."}, {"title": "Discussion", "content": "In this paper, we have demonstrated that it is possible to fine-tune small, open-source GLMs to\n(1) achieve adequate performance for AES and ASAS and (2) generate appropriate rationales (at\nleast in some cases) for predicted scores. Our method pushes beyond the paradigm of appending a\nclassification head to a pretrained language model, yet avoids the many issues involved in querying\nlarge, proprietary GLMs via APIs. We find that parameter-efficient fine-tuning (using no more\nthan a 24Gb GPU) for relatively small, open-source GLMs exceeds performance of proprietary\nGLMs that are orders of magnitude larger. Furthermore, due to the efficient nature of training\ncheckpoints, the only parameters that are required to serve these models are the LORA weights,\nwhich amount to less than 100 million parameters, fewer parameters than a BERT model. Given\nthe widespread enthusiasm and fear around GLMs, it may come as a surprise that they did not\nlead to SOTA results. Ensembles of smaller LMs remain more efficient and performant than GLMS\nfor AES and ASAS.\nOne of the unique advantages of using GLMs is the ability to move beyond scoring alone in\nthis study, we prompt the fine-tuned models to provide an explanation of the score. We found\nthat models were capable of (sometimes) generating adequate justifications, and that Phi-3 was\nmore consistent than the other models. Yet this study does not undertake a thorough analysis\nof model-generated feedback. Although preliminary results are encouraging, rigorous analysis is\nneeded. This would include carefully defined constructs of interest, collaboration with educators\nand trained human raters, and targeted use cases that identify whom the feedback is for, when\nthe feedback should be provided, and what shortcomings need to be avoided. It is noteworthy,\nhowever, that fine-tuned GLMs were able to generate feedback at all, especially given that they\nwere fine-tuned to predict scores (i.e. not feedback). It has been shown that, even with some a\nsmall amount of fine-tuning, model behavior can change dramatically [41].\nThe performance of the GLMs explored in this study are promising, particularly since they\navoid the critical issues of proprietary models. Firstly, these models can be run securely and\nefficiently with relatively low requirements. Although security is not a concern when examining\nperformance on a publicly-available dataset, it is a concern in many educational contexts, where\npersonally identifiable information about students may be shared with the organization hosted the\nGLM. Secondly, in order to interpret the output of these models, we must be able to access the\nweights. The lower computational requirements of smaller, open-source models allows them to be"}, {"title": "Comparison to Proprietary GLMs", "content": "With respect to scoring, our fine-tuned results far exceed those of \"fine-tuned\" GPT-3.5 for both\nAES [31] and ASAS [5]. We put \"fine-tuned\" in quotation marks because the fine-tuning proce-\ndure(s) available to the public are undisclosed and optimization (e.g. modulating the learning rate)\nis not currently available. Given that GPT-3.5 is vastly larger in size (175B) and requires far more\ncomputation [3] compared to the models explored in our study, it is surprising that its performance\nis so underwhelming. Our results are also superior to (non-fine-tuned) GPT-4 with respect to both\nAES [55] and ASAS [24]. It should be noted that fine-tuning is not currently available for GPT-4;\nyet even if fine-tuning were available and results were adequate, these would be subject to the same\nlimitations outlined above. We note that our study does not undertake a comparison of feedback\nbetween large, proprietary GLMs and smaller, open-source GLMs; it may be that large GLMs excel\nin this area."}, {"title": "Limitations", "content": "As noted previously, this study does not attempt to provide quantitative empirical evidence re-\ngarding the validity of model-generated feedback. Model-generated feedback, although promising,\nrequires more rigorous evaluation that should be undertaken in collaboration with educational prac-\ntitioners. Even for the relatively humble task of providing an explanation for a score, models were\nfar from infallible. More research is needed to validate that the model is consistently connecting\nscores to the rubric. There are others who are exploring the more complicated task of producing\nmodel-generated feedback that is useful to educational practitioners (e.g. [46, 55]). Robust feedback\nsystems likely require on-going evaluation, and may depend on human-in-the-loop frameworks.\nAlthough there is growing pressure to develop educational tools using GLMs, there is no easy\nmethod of validating feedback. At this stage, the validation of feedback should be a primary concern\nfor the future for the use of GLMs in education. This may mean the creation of datasets that are\nfocused on feedback, or the use of existing information, such as essay trait scores, to validate existing\nfeedback. To help facilitate such analyses, We have open-sourced the feedback provided on a single\nvalidation sample in the hopes of prompting further analyses. One thing that is fairly clear at\nthis stage is that these models are computationally capable of being used in such a pipeline. The\nquestion remains, however, as to whether they are valid for carefully defined, targeted use cases."}]}