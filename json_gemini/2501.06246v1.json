{"title": "A partition cover approach to tokenization", "authors": ["Jia Peng Lim", "Davin Choo", "Hady W. Lauw"], "abstract": "Tokenization is the process of encoding strings into tokens from a fixed vocabulary of size k and is widely\nutilized in Natural Language Processing applications. The leading tokenization algorithm today is Byte\nPair Encoding (BPE), which formulates the tokenization problem as a compression problem and tackles it\nby performing sequences of merges. In this work, we formulate tokenization as an optimization objective,\nshow that it is NP-hard via a simple reduction from vertex cover, and propose a polynomial-time greedy\nalgorithm GREEDTOK. Our formulation naturally relaxes to the well-studied weighted maximum coverage\nproblem which has a simple (1-1/e)-approximation algorithm GREEDWMC. Through empirical evaluations\non real-world corpora, we show that GREEDTOK outperforms BPE, while achieving a comparable objective\nscore as GREEDWMC (which could have achieved a higher score due to relaxation).", "sections": [{"title": "Introduction", "content": "Tokenization is the process of encoding strings into tokens from a fixed vocabulary of size k and is widely utilized\nin Natural Language Processing (NLP) applications. With the growing popularity of Large Language Models\n(LLMs) today, there is significant interest in improving our understanding of tokenization as tokenization is\nintegral to the architecture of these LLMs. It is not unthinkable that even a minor increase in tokenization\nperformance can result in reasonable accumulated savings in compute resources. For instance, LLMs such as\nLLAMA [TLI+23] and Mistral [JSM+23] use fixed-length context windows that may benefit from tokenizers\nwith better compression utility enabling the fitting of more information within its context window. There are\nalso prompt-based [WWS+22, YYZ+23] and finetuning [FTL+23] strategies to improve LLM performance by\nincreasing the number of tokens processed.\nA common way to formalize the tokenization problem is to frame it as a compression problem where one tries\nto minimize the ratio of tokens produced from tokenizing the input data. The leading tokenization algorithm\ntoday is Byte Pair Encoding (BPE) [Gag94, SHB16], chosen by most LLMs [KHM+23], which formulates the\ntokenization problem as a compression problem and tackles it by performing a sequence of pairwise merges.\nDue to its popularity, there have been a multitude of recent works analyzing the theoretical properties of BPE\n[ZMG+23, KV24, WBP24].\nContributions. In this work, we deviate from the usual compression ratio formulation of tokenization and\nmerge-based algorithms.\n1. We formulate an optimization objective formulation of the tokenization problem which is more general\nthan the merge-based ones in prior works [ZMG+23, KV24]. These prior formulations are based on\nmerging tokens in a bottom-up fashion where they restrict any solution to construct tokens from the\nexisting set of tokens in pairwise merges. However, the idea of tokenization is simply to efficiently\nrepresent a corpus with a judiciously selected set of tokens, whose construction is independent of such\n*Equal contribution"}, {"title": "A general optimization formulation for the tokenization problem", "content": "Let us fix an alphabet \u2211 of interest. A corpus C = (W,COUNT) is a collection of words WC + and\ncount function COUNT : W \u2192 N. Meanwhile, for any word W \u2208 \u03a3+ and a given set of tokens S \u2286 \u03a3+,\nPARTITION(W, S) is the minimum number of tokens from S that can sequentially concatenate to form W. For\nReferred to as optimal merge sequence and optimal pair encoding respectively in their work."}, {"title": "Tokenization is NP-hard", "content": "In this section, we will prove that the tokenization decision problem (Problem 2) is NP-hard by reducing the\nvertex cover problem, which is known to be NP-hard [Kar72], to it. Given a graph G = (V, E), where V is the\nset of vertices and E is the set of edges, a vertex cover is a subset SC V of vertices such that |S\u2229 {U, V}| \u2265 1\nfor every edge {U, V} \u2208 E. The decision variant of the vertex cover problem is then to decide whether a given\ngraph G has a vertex cover of size at most a given integer k.\nTheorem 1. The tokenization problem is NP-hard.\nProof. We will prove this by reducing the vertex cover problem, which is known to be NP-hard [Kar72], to\nthe tokenization problem. Given an arbitrary vertex cover problem instance, we show how to construct a\ncorresponding tokenization instance. Then, we argue that the derived tokenization problem instance is a YES\ninstance if and only if the original vertex cover problem instance is a YES instance. In this proof, for clarity,\nwe will write words W\u2208 W as a tuple of singletons instead of usual plaintext, e.g. (h,e,1,1,0) instead of hello.\nConstruction. Consider an arbitrary vertex cover problem given by the graph G = (V, E) over n vertices\nV = {V1,..., Vn} and a positive integer k \u2208 N>0. To construct an instance of the tokenization problem,\nwe first define the alphabet as follows: \u03a3 = {V1, ..., Vn, @} where @ is an additional symbol which we will\nuse later. So, we have B = {(V\u2081), ..., (Vn), (@)}. For each edge {Vi, V;} \u2208 E with i < j, we create a word\nWi,j = (@, V\u00bf, @, V;, @) and define the set of words as W = {Wi,j : {Vi, V;} \u2208 E} where each word has count\n1, i.e. COUNT(W) = 1 for all W\u2208 W. Then, we define the set of candidate tokens T = {(@, V\u00bf, @) : V; \u2208 V}.\nFinally, we set l = 3|W| = 3|E| and associate the parameter k in the vertex cover problem instance to the\ncorresponding parameter k in the tokenization problem instance. One can check that this derived tokenization\ninstance can be constructed in polynomial time.\nObservation. Observe that every word W\u2208 W has length 5 and each token in S has length 3, so\nPARTITION(W, SUB) will either be 3, when there is some token in S that is a contiguous subword of W,\nor 5 otherwise. For instance, given the word Wi,j = (@, Vi, @, Vj, @), we have PARTITION(Wi,j, SUB) = 3\nif and only if at least one of (@, Vi, @) or (@, V;, @) is chosen in S (both could be in S). Furthermore,\nsince all words have count 1, the tokenization problem becomes finding SCT such that |S| \u2264 k and\n\u2211WEW PARTITION(W, SUB) \u2264 l = 3|W|."}, {"title": "GreedTok: Our greedy tokenization algorithm", "content": "Earlier, in Section 3, we showed that the tokenization problem is NP-hard. Developing efficient algorithms for\nNP-hard problems typically involves strategies that trade off between exactness, runtime, and solution quality.\nSince we are interested in efficient and practical solutions to the tokenization problem on large instances, we\nlook towards developing approximate solutions that run in polynomial time, i.e. we would forgo the idea of\ndesigning a fixed-parameter tractable algorithms. In the following, we discuss two common approaches to\ndesign efficient algorithms for NP-hard problems with provable approximation guarantees and explain why\nthey do not work for our situation.\nThe first approach is to show that the problem at hand is submodular or supermodular, and then apply\nknown algorithmic frameworks to design a corresponding solution. For example, submodular problems are\nknown to admit (1 - 1/e)-approximate greedy algorithms [NWF78]. Using 2T to denote the powerset of T,\nsubmodular and supermodular set functions are defined as follows:\nDefinition 2 (Submodular Set Function). A real-valued set function f : 2T \u2192 R is submodular if f(AU\n{C}) - f(A) \u2265 f(A\u222a {C}) \u2212 f(A) for all AC ACT and C\u2208 T \\ A.\nDefinition 3 (Supermodular Set Function). A real-valued set function f : 2T \u2192 R is supermodular if f(AU\n{C}) - f(A) \u2264 f(A\u222a {C}) \u2212 f(A) for all AC ACT and C\u2208 T \\ A.\nInformally speaking, submodular functions represent diminishing returns while supermodular functions\nrepresent increasing returns. In the context of the tokenization problem, the set T represents the candidate set\nof all possible tokens. Unfortunately for us, Problem 1 is neither submodular nor supermodular; see Table 1."}, {"title": "An equivalent mixed integer program formulation", "content": "To design our algorithm GREEDTOK for the tokenization problem, we first show an equivalent formulation of\nProblem 1 in terms of a mixed integer program (MIP). The rationale behind this is two-fold. First, the MIP\nformulation offers a more straightforward and intuitive framework for defining our greedy algorithm. This\nclarity simplifies the process of understanding and implementing the algorithm within the given constraints.\nSecond, the MIP formulation naturally lends itself to a relaxation to the well-established weighted maximum\ncoverage problem (WMC), which is known to be submodular and has a corresponding greedy algorithm\nGREEDWMC that provably achieves an (1 \u2013 1/e)-approximation; see [Hoc96, Section 3.9]. While we are\nunable to formally prove guarantees for GREEDTOK, the relaxation to WMC allows us to empirically evaluate\nthe performance of GREEDTOK on tokenization problem instances via a comparison to GREEDWMC.\nTo begin, let us define COVER(W, S) as the maximum number of adjacent singletons in W that can be\nconcatenated to form tokens in S without using any singleton more than once. Now, recalling the definition of\nPARTITION(W, SUB) from Problem 1, we see that |W|+1 = PARTITION(W, SUB) + COVER(W, S) for any word\nW. For example, COVER(W = scaredy, S {care, edy}) = 4 since we can concatenate adjacent singletons\n(c,a,r,e) into care \u2208 S. Note that COVER(W = scaredy, S = {care, edy}) \u2260 4 + 3 = 7 because we cannot use\nthe singleton 'e' twice. Meanwhile, PARTITION(W = scaredy, SUB = {care, edy} \u222a {s,c,a,r,e,d,y}) = 4 via\nscaredy, where represents a split between the tokens. Thus, we can rewrite the minimization objective"}, {"title": "Empirical evaluation of GreedTok on real-world datasets", "content": "In this section, we compare the performance of GREEDTOK against BPE and GREEDYWMC in real-world\nNLP settings. As a reminder, BPE is an alternative bottom-up greedy solution to Tok while GREEDYWMC\nis a (1 - 1/e) approximation to the relaxed WMC problem."}, {"title": "Datasets", "content": "As tokenization is already commonplace for NLP applications, we select four real-world corpora to analyze\nGREEDTOK and BPE. Table 2 numerically describes the difference in size.\nUnited Nations General Debate Corpus (UN). UN [JBD17] is a collection of statements made by various\nmember states during the General Debate on key geopolitical issues from 1946 to 2022. This corpus has a\nCreative Commons (CC) 0: Public Domain License.\narxiv. This corpus\u00b3 is a collection of abstracts of scientific publications and preprints from the popular e-Print\narchive. This corpus has a CCO: Public Domain License.\nWikipedia-English (wiki). An extensive collection of English articles on a wide range of things, concepts,\nand people. We extract [Att15] the text from the database dump\u2074. In Section 5.4, we conduct a case study\nwith articles containing Chinese, Japanese and Korean (CJK) languages. The texts belonging to these articles\nare under CC BY-SA 4.0 and GNU Free Documentation Licenses.\nPubMed Central Open Access (PubMed). Similar to arxiv, PubMed is a repository of publications\nand preprints, mainly related to health, medicine, biotechnology, and pharmaceuticals. We select the Non-\nCommercial Use Only subset grouped by: CC BY-NC, CC BY-NC-SA, and CC BY-NC-ND licenses. We\npreprocessed the text minimally, removing citations and headers.\n3 Available at: kaggle.com/datasets/Cornell-University/arxiv\n4 Available at: https://dumps.wikimedia.org/\n5Available at: https://pmc.ncbi.nlm.nih.gov/tools/openftlist/"}, {"title": "A comparison of GreedTok against BPE", "content": "Recall that BPE uses merge rules consisting of token pairs to form subsequent tokens. While this strategy\nobeys the MIP constraints (Section 4.2), it is inefficient. Intermediate tokens, formed from pairwise merging,\nmay not be used in the final representation of the word, taking up space that would otherwise be allocated\nto other useful tokens. Moving beyond the design limitations of pairwise merge sequences, algorithms such\nas GREEDTOK enable designers to have more control over the choice of candidate tokens T. To empirically\nevaluate this, we examine the texts of various corpora in UTF-8 format. Let the set of singletons B be the 256\ndifferent byte permutations (\u03a3). Let the set of words W be the set of space-delimited byte sequences (strings)\nobtained from the corpus. Additionally, we use a special token to demarcate the start of the string following\nthe space character. COUNT is a mapping of each W\u2208 W to their number of occurrences in the corpus.\nDifferent corpora require different |S| = k, with the final size of the ranked token sequence being |B| + k. For\nGREEDTOK, let the set of candidate tokens T be all possible substrings of W. For BPE, its initial T is empty\nas it has to build initial tokens from B, and will vary due to changes in S.\nWe run both GREEDTOK and BPE, examining how well they compress the corpus into |B| + k tokens\nby comparing GREEDTOK and BPE from three different perspectives. First, we compare both algorithms at\na fixed k, determined by the number of tokens GREEDTOK required to reach the tokens per word targets.\nNext, we compare the number of tokens required by both algorithms to reach the same tokens per word target.\nFinally, we compare the difference in token membership between their respective S."}, {"title": "Qualitative findings", "content": "From Table 3, to achieve the same tokens per word target, we see that GREEDTOK\nrequires an average of 13% fewer tokens than BPE while using an average of 3%, and up to 5%, fewer tokens per\nword across various experimental settings; see the \"Ratio of kg/k\u044c\" and \"Ratio of GREEDTOK(kg)/BPE(kg)\"\nrows respectively. Meanwhile, in Fig. 2, we observe a constant rate of divergence between the ranked token\nsequences S of GREEDTOK and BPE. Our findings show that GREEDTOK consistently uses a fewer number\nof tokens (including singletons) to encode the chosen corpora. This suggests that BPE's pairwise merges may\nhave prevented the selection of tokens with better compression utility."}, {"title": "Towards understanding GreedTok's approximability", "content": "We reformulated Problem 1 into a MIP in Section 4.2 because the MIP formulation relaxes naturally into the\nmaximum coverage problem, which has a corresponding (1 \u2013 1/e) approximate algorithm GREEDWMC. By\ndeploying GREEDWMC on the same problem instances with the same range of k, we can calculate the ratio\nof objectives between GREEDTOK and GREEDWMC and define $d_{inst}^{GREEDTOK}$ for each\ninstance. For each\nof these instances, GREEDTOK attains an objective value at least $d_{inst}^{GREEDTOK}$(1 \u2013 1/e) times the optimal objective of\nEq. (1) by definition. This is because GREEDWMC is an (1 \u2013 1/e)-approximation algorithm to MWC, and\nGREEDWMC's attainable objective value of MWC is at least that of TOK; see Section 4.2.1.\nQualitative findings. From Fig. 3, on the four selected corpora, we plot $d_{inst}^{GREEDTOK}$ against k. We see an ini-\ntial steep decline before the curve reverses and climbs towards 1. At the start, the deviations suggest that\nGREEDWMC selects tokens that partially overlap with each other, as a result GREEDTOK is unable to select\nthese tokens as they will violate the MIP's constraints. Fortunately, there is a turning point when the incre-\nmental gains of GREEDTOK outpace GREEDWMC's as k increases. This suggests that singletons that were\ncovered earlier by GREEDWMC's partially overlapping tokens would have eventually been covered by a single\ntoken assigned by GREEDTOK. Empirically, we see that GREEDTOK achieves an objective value of at least\n0.9(1 \u2013 1/e) of the optimal for large k, which is the case for practical NLP scenarios."}, {"title": "Leveraging domain expertise in tokenization", "content": "Here, we discuss a new algorithmic design capability that is enabled by our general tokenization formulation.\nFor an extremely large corpus, considering all possible substrings for the candidate token set T may be\ndaunting. However, if we had additional prior knowledge on how parts of the corpus differ, as a way to speed\nthat can be done offline.\nFor Japanese and Chinese language, we use spaCy's pipelines [MHB+23] and its supported segmentation libraries [THK+18]\nand [LXZ+19] respectively.\nWe use cl100k_base from tiktoken [Ope23].\nup the computation, we could run GREEDTOK on different subsets of the corpus and use the union of these\noutputs as the candidate token set T for the entire corpus. That is, one can leverage on domain expertise to\ncluster subsets of the corpus in order to generate these intermediate token sets S1,..., SK and define T as all\npossible substrings of length at least 2 within S\u2081, ..., SK instead of considering all possible substrings of length\n> 2 within W for the initial candidate set T. This approach also allows one to incorporate selected tokens\nfrom other sources Sother, such as outputs from other tokenization algorithms or domain experts, as we can\nsimply consider S\u2081 U... USK U Sother when forming the input T for GREEDTOK.\nAs a concrete example, let us consider wiki-cjk, a corpus of articles from the Chinese, Japanese, and\nKorean (CJK) language sections of Wikipedia that contain CJK characters. First, we use GREEDTOK on each\nsubset of articles belonging to the same language domain to obtain their respective local outputs SC, SJ, SK,\neach of size S| = k, and then run GREEDTOK again with the candidate token set T as all possible sub-\nstrings of length at least 2 within SC US; USK, on the full corpus counts to obtain the final output SCJK,\nwhere SCJK = k. Doing this pre-processing step reduces the size of the considered candidate set |T| from\n320M to 231K. Our experimental results are visualized in Fig. 4. Examining the three language subsets,\nusing a S obtained from a foreign language to tokenize leads to subpar performance in terms of tokens per\nword, as evidenced in the two highest curves. Meanwhile, the tokenization performance of SCJK only slightly\ntrails the performance of the native tokenizer (e.g. Sc on wiki-chinese, Sj on wiki-japanese, and SK on\nwiki-korean), indicating a successful combination of the three languages despite sacrificing some tokenization\nperformance to accommodate all three language domains."}, {"title": "Computational Feasibility", "content": "Table 4 details the total compute time for GREEDTOK to obtain S, where |S| = maxk, conducted with AMD\nEPYC 9654 @ 2.40GHz and 768 GB of RAM. If required, one can manually adjust max|W|, W, and T to\nreduce the search space of the solution. For example, one could ignore long words when building T to reduce\nmax |W|, ignore certain words when building T to reduce |W|, or consider only substrings of longer length\n(or prune T in other ways) when building T. To benchmark encoding performance, we encode using a token\nset of S = 100K\u2076, on a subset of wiki comprising 70K text articles with a total of 97M words. With a\nsimilar compute environment, our current initial implementation of GREEDTOK processes texts at a rate of\n700K to 800K words per second per thread; we believe these numbers can be further optimized. With these\nperformance characteristics, we believe that it is feasible to employ GREEDTOK in NLP pipelines."}, {"title": "Conclusion", "content": "In this work, we showed that the tokenization problem is NP-hard, provided a greedy algorithm GREEDTOK,\nand empirically demonstrated its edge over the incumbent BPE in terms of the compression utility metric.\nOur general formulation of the tokenization problem also enables the use of custom candidate token sets to\nsearch over and to leverage on domain expertise for learning specialized tokens for subsets of corpora. We\nhope that our formulation of the tokenization problem will be valuable for future research and we believe that\nsubstituting BPE with GREEDTOK as the tokenization algorithm is straightforward in almost all modern NLP\nsystems. Below, we discuss some potential implications of our work and state a concrete open problem.\nTowards a flexible algorithmic framework for tokenization. BPE is a tokenization algorithm with\nwidespread adoption currently. However, as shown and discussed in Section 5, GREEDTOK is a practical\nalternative that is more efficient in tokenization while enabling additional benefits of not relying on pairwise\nmerge sequences. While the recent current advances in long-context research [HKM+24, GT24] seeking to\nenable a context length of a million tokens may make the compression utility metric a less important criterion\nin choosing a tokenization algorithm, our proposed MIP formulation and GREEDTOK offer a flexible platform\nto adapt to new alternate objectives that may arise in the future. For instance, our flexible formulation allows\none to directly incorporate pre-selected tokens as part of the initial candidate token set T; see also Section 5.4\nfor a concrete example of how these candidate tokens can be incorporated into T. It would also be interesting\nfuture work to integrate NLP downstream objectives [BD20] and fairness [LBG+24] constraints into our MIP\nformulation.\nMoving past pre-defined tokenizers. Tokenizer-free architectures such as Charformer [TTR+22], ByT5\n[XBC+22], MegaByte [YSF+23], and Byte Latent Transformers [PPR+24] avoid the use of tokenizers com-\npletely by using character or byte level information as opposed to fixed tokens. From our MIP, one can view\nthe 0/1 bits of mw as a sequential binary prediction problem of whether to merge adjacent characters of a word\nW\u2208 W. Given that modern LLMs have great empirical performance on next-token prediction, it would be\ninteresting to see if one could leverage them in performing this sequential binary prediction task. This would\neffectively result in a tokenization algorithm that does not rely on a pre-defined initial candidate token set T.\nOpen problem. Recall that the tokenization problem has the confounding property of being neither super-\nmodular nor submodular. Even though we empirically show that GREEDTOK achieves an approximation ratio\nof at least 0.9(1 \u2013 1/e) for large k, a formal proof is lacking. This is an intriguing theoretical problem and we\nbelieve that our perspective of the tokenization problem may help in this future endeavor."}, {"title": "Additional Pseudocode", "content": "Previously, in our MIP (Section 4.2), a 1-based indexing system was used.\nGiven an ordered sequence S,\nHowever, for implementation\nconvenience, we use a 0-based indexing system for our pseudocodes instead.\nsuch as array A, string W, and selected tokens S, we use Si to specify an element in the ith index of S.\nHowever, for sequences, we use Si,j to specify the elements from the ith index up to, but excluding, the jth of\nFor example, when S = happy, we have S\u2081 = a and S1,3 ap."}, {"title": "Computing S from T", "content": "Given the COUNT function, corpus W, candidate tokens T, and an integer k, Algorithm 1 finds a set of\ntokens S that maximizes the objective function with the help of subroutines Algorithm 2 and Algorithm 3.\nThe algorithm Algorithm 1 defines a couple of dictionaries M, P, R, and I to track the problem state, then\ngreedily picks the next best scoring token to cover words:\n1. M maps each word W to its state of cover, similar to the definition in Section 4.2\n2. P maps each token T to the set of its occurrences in the given word W, for all W \u2208 W, in a (W, i) pair,\nwhere i is the position index of the start of the token occurrence\n3. R stores the net objective gain of each T, which we use to greedily select the next best token in Line 8\n4. I maps each token T to an index, which we use to update the state of cover for all W\u2208 W at Line 12\nThe subroutine Algorithm 2 encapsulates a check of the validity of using a given token T to cover Wat\nposition i, primarily by observing if the non-start/end endpoint positions i and i + |T| were previously covered\nby some other token previously; if such a token is present, then T cannot cover Wat position i. Meanwhile,\nthe subroutine Algorithm 3 calculates the score contribution by token T, given the current state M, while\naccounting for previous covers applied from chosen tokens in S."}, {"title": "Tokenizing a text W using S", "content": "In Algorithm 4, we describe how to encode a given text W into its token representation using the token set\nS from Algorithm 1. First, in Line 1, we initialize a dictionary to map our tokens in S according to their\norder of inclusion to S, and then place singleton tokens B at the back of the sequence. Next, in Line 2, we\nfind all possible token covers of W using tokens in S and sort them in Line 3 according to their priority I and\na left-to-right ordering in W. Using M to denote which token covers which position index of W, we iterate\nthrough (T, i) in the sorted P and update M whenever the token T can cover W at position i given earlier\ndecisions. Note that this may mean that a later token of longer length may overwrite the covering decision of\nan earlier shorter token; see Example 6. Finally, using M, we return the 0-delineated token representation; see\nExample 7."}]}