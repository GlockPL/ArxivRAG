{"title": "ROBIN3D : IMPROVING 3D LARGE LANGUAGE MODEL VIA ROBUST INSTRUCTION TUNING", "authors": ["Weitai Kang", "Haifeng Huang", "Yuzhang Shang", "Mubarak Shah", "Yan Yan"], "abstract": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8% improvement in the grounding task (Multi3DRefer) and a 6.9% improvement in the captioning task (Scan2Cap). Code is available at https://github.com/WeitaiKang/Robin3D.", "sections": [{"title": "INTRODUCTION", "content": "Spatial Intelligence [Li, 2024] refers to the ability of AI to understand the 3D world and reason within 3D space. Related ideas, such as Embodied AI [Duan et al., 2022] and Robotic Agent [Bousmalis et al., 2023], express a similar aim to build general-purpose assistants in the 3D real world. To achieve this goal, researchers have drawn inspiration from the success of 2D Multimodal Large Language Models (MLLMs) [Liu et al., 2024c; You et al., 2023] and have started exploring the potential of 3D Large Language Models (3DLLMs) to create general agents [Hong et al., 2023; Chen et al., 2024a; Wang et al., 2023b; Huang et al., 2023a] in the 3D domain or to attain Spatial Intelligence.\nInstruction-following tuning [Liu et al., 2024c;a;b] in MLLMs refers to training the LLM to execute natural language commands by integrating both textual and visual information. In contrast to the versatile image-text pairs employed for training 2D MLLMs, collecting 3D instruction-following data for 3DLLM remains a significant challenge. Although existing works have made progress [Hong et al., 2023; Huang et al., 2023b; Chen et al., 2024c] in generating more instruction data, they still lack robustness in two aspects: 1) Most of the existing instruction data consist of positive pairs, lacking adversarial or negative samples. Therefore, models trained on such data tend to be less discriminative because they might overfit to the positive pairs and are more likely to hallucinate positive responses to any input. 2) Some instruction data also lack diversity in language styles, as human annotators or generative models [OpenAI; Wang et al., 2023a] are typically asked to follow fixed instructions when describing objects [Chen et al., 2020; 2021], or the data is generated using predefined templates [Achlioptas et al., 2020], which may limit models' generalizability.\nTo address these challenges, we introduce Robin3D, a robust and powerful 3D Large Language Model tuned on large scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. Specifically, RIG is designed to generate two types of data:"}, {"title": "RELATED WORK", "content": "3D Vision-Language Learning Recent advancements in 3D vision-language (3D-VL) learning [Chen et al., 2020; Achlioptas et al., 2020; Azuma et al., 2022; Chen et al., 2021; Kang et al., 2024b] have focused on bridging the gap between 3D scene understanding and natural language,"}, {"title": "METHODOLOGY", "content": "To train a 3D LLM using instruction fine-tuning, we first represent the 3D scene as a sequence of vision tokens, then append it with system and question prompts, expressed as sequences of language tokens, to indicate the task. Taking the above tokens as input, a LLM is supervised to output the answer tokens via next token prediction. Specifically, as shown in Fig. 2, given the point cloud of a 3D scene, we use the pre-trained 3D segmenter Mask3D [Schult et al., 2023] to extract object features along with their corresponding 3D masks. Following Huang et al., we further sample each object's point cloud based on the 3D masks, normalize it, and employ the pre-trained Uni3D [Zhou et al., 2023] to extract unified object-centric 3D features. Additionally, 2D masks projected from the 3D masks are used to sample and average 2D features, which are extracted by DINO v2 from multi-view images of each object. Our Relation-Augmented Projector fuses the 3D features and position embeddings from Mask3D and Uni3D into our final 3D features. In line with Huang et al., we incorporate special tokens {<OBJi >}i=1...n as object IDs into the vocabulary. These ID tokens are paired with 2D & 3D object features to indicate each object, for referring to the object in the input instruction or grounding the object in model's output. Our ID-Feature Bonding combines each object feature with its corresponding object ID, and appends the system and question prompts at the beginning of the sequence, which are then fed into the LLM. For more details on the object IDs and the extraction of different types of features, please refer to Huang et al.."}, {"title": "PRELIMINARY", "content": "3.2 RELATION-AUGMENTED PROJECTER (RAP)\nAs shown in Fig. 2 (bottom), to obtain relation-aware 3D features while preserving the unified object-centric characteristics, our RAP considers three types of 3D features: a) the object features from Mask3D, Xmask3d, which are scene-level respresentations, containing spatial relationship, as they come across multiple cross-attention layers to exchange information. b) the position embeddings of the Mask3D, Xpos, which are directly projected from the coordinates of the corresponding object features. c) the unified object features, Xuni3d, from Uni3D. Our RAP is formulated as:\nX = Concat(NormL2(Xuni3d), NormL2(Xmask3d)), Xrap = MLP(X) + MLP(Xpos)                                                                                                            (1)\nwhere NormL2 is the L2 normalization, Concat is the concatenation alongside the channel dimension, and MLP is a multi-layer perceptron with GELU activation [Hendrycks & Gimpel, 2016]. The Xrap represents our final relation-aware unified 3D features, which is augmented by the spatial relationship information from Mask3D and the position embeddings."}, {"title": "ID-FEATURE BONDING (IFB)", "content": "We propose IFB for better referring and grounding in our instruction-following data by improving the connection between object IDs and object features. As shown in Fig. 2 (middle), we first use two identical ID tokens to wrap the object features. Adhering to the causal attention nature of the LLM, this approach links ID information to the object features via the first ID token, and links object information to its ID via the second ID token. Secondly, we propose a post-vision order, which places the vision tokens at the end of the input sequence, closer to the answer tokens generated by the model. This approach mitigates attention deviation from the answer tokens to the ID-Feature pairs, a problem caused by their relative token distance and rotary position embeddings [Su et al., 2024; Ma et al., 2023], while reinforcing the visual information for improved answer generation. The post-vision order is structured as: [<System tokens>, <Instruction tokens>, <Vision tokens>, <Answer tokens>], where  comprises the ID tokens and object feature tokens."}, {"title": "ROBUST INSTRUCTION GENERATION (RIG)", "content": "The Adversarial data is designed to challenge the model's discriminative capabilities by introducing adversarial or negative samples, ranging from the object-level to the scene-level. It features both category-based identification tasks and expression-based reasoning challenges."}, {"title": "ADVERSARIAL DATA GENERATION", "content": "HYBRID OBJECT PROBING EVALUATION (HOPE) \u2013 Fig. 3(upper left)\nTo construct a scene-level category-based task, we introduce HOPE, which is inspired by the POPE benchmark [Li et al., 2023] in 2D domain. POPE evaluates the tendency of 2D MLLMs to hallucinate by asking yes/no questions about the presence of one specific object at a time. Building on this, HOPE further incorporates such hallucination challenges into the training stage in the 3D domain, aiming to train our model to be more discriminative. Additionally, HOPE presents a hybrid scenario, introducing greater complexity to further advance the decoupling of memorized positive vision and language pairs. Specifically, given a 3D scene, we ask the model to determine the presence of various randomly specified objects. The objects may or may not be present in the scene, and each existing object might have one or more instances. The model is required to answer \u201cNo\u201d when the object is not present in the scene, and answer \"Yes\" with the object ID of each instance of the object when it exists. As shown in Fig. 3 (upper left), the question combines multiple objects, separated by semicolons (\u201c;\u201d), and the answer combines responses for each object, also separated by semicolons. This structure creates a challenging setting that involves hybrid recognition of both positive and negative object presence, combined with multi-instance object localization."}, {"title": "HYBRID REFERRING OBJECT CLASSIFICATION (HROC) \u2013 Fig. 3(upper right)", "content": "Referring Object Classification [You et al., 2023] evaluates a model's ability to understand a referred region in 2D domain, focusing on a classification problem by \u201cRegion-in Text-out\" format. Our HROC dataset extends this task into the training data for 3D domain to create an object-level category-based task, by incorporating adversarial and hybrid challenges. Specifically, in a 3D scene, we randomly create hybrid positive and negative ID-Category pairs to form our questions, as illustrated in Fig. 3 (upper right). A positive pair consists of a valid object ID and the ground truth category. The bounding box of the corresponding object ID must overlap with one ground truth bounding box, and the category of the overlapping object is defined as the ground truth category. A negative pair includes a valid object ID and a randomly selected category that is present in the scene but not the ground truth category to serve as an adversarial challenge. The model is required to answer \"Yes\u201d for positive pairs and \u201cNo\u201d with the correct category for negative pairs. The pairs and corresponding answers are separated by semicolons (\u201c;\u201d)."}, {"title": "PARTIAL FACTUAL 3D VISUAL GROUNDING (PF-3DVG) \u2013 Fig. 3(lower left)", "content": "Our PF-3DVG introduces a scene-level expression-based task, featuring three types of data in 3DVG: unfactual data, partially factual data, and factual data. For unfactual data, given a 3D scene, we randomly select a reference from Sr3D+ [Achlioptas et al., 2020] where the indicated object does not exist in the scene. The model is required to answer \u201cNo\u201d when prompted with the question, as shown in the first example of Fig. 3 (lower left). For partial factual data, given a reference from Sr3D+ and its corresponding 3D scene, we randomly switch the described spatial relationship with a different one based on the predefined template of Sr3D+. For example, as shown in the second example of Fig. 3 (lower left), we change the original reference \u201cthe pillow lying on the couch"}, {"title": "FAITHFUL 3D QUESTION ANSWERING (3DFQA) \u2013 Fig. 3(lower right)", "content": "The original 3D Question Answering (QA) task [Azuma et al., 2022] includes only positive samples, which can potentially lead to the model memorizing fixed combinations of 3D scenes and QA pairs. To address this, we propose Faithful 3D Question Answering, a scene-level expression-based task which incorporates both negative and positive examples with an additional grounding requirement. To construct negative samples, we first sample a QA pair and collect the related objects that are mentioned in the question or the target objects of the answer from Azuma et al. [2022]. Then, we randomly select a 3D scene that lacks those related objects. A new instruction is added to the question: \"If you can, answer the question... and provide all the IDs...\" as illustrated in Fig. 3 (lower right). In this case, the model must faithfully answer \u201cNo\u201d based on the absence of related objects in the 3D scene and must not provide any object IDs, demonstrating its reliance on the scene for making decisions. For positive samples, directly taken from Azuma et al. [2022], the model must answer the question while faithfully grounding its \u201cevidence\u201d for the answer, i.e., providing the IDs of the related objects. Therefore, the model trained on our 3DFQA dataset is forced to generalize beyond memorization, learning to respond faithfully to both positive and negative samples."}, {"title": "DIVERSE DATA GENERATION", "content": "The Diverse data aim to enhances the model's generalization by first incorporating multiple different types of instruction-following data and then increasing the linguistic diversity of the instructions.\nWe first collect large scale data from different tasks outside the benchmark dataset. Specifically, given a 3D scene, we collect question-answering pairs from the following tasks: 1) Category Question-Answering task from Huang et al. [2023a], where the model is asked to answer the category of a specified object. 2) Nr3D Captioning task from Huang et al. [2023a], where the model is asked to caption the spatial relationship of a specified object to its neighboor. The ground truth is constructed from Nr3D [Achlioptas et al., 2020] dataset. 3) Appearance Captioning task from Chen et al. [2024c], where the model is asked to captioning the physical attributes or visual characteristics of a specified object. The ground truth is generated by CogVLM [Wang et al., 2023a]. 4) Region Captioning task from Chen et al. [2024c], where the model is asked to caption the region encircling a specified object. The ground truth is organized by ChatGPT [OpenAI]. 5) End to end 3D Visual Grounding from Nr3D dataset [Achlioptas et al., 2020], where the model is not provided ground truth candidates, different from the original setting in Nr3D. 6) End to end 3D Visual Grounding from Sr3D+ dataset [Achlioptas et al., 2020], where the model is also not provided ground truth candidates, different from the original setting in Sr3D+."}, {"title": "DATA SUMMARY", "content": "In summary, our Robust Instruction Generation engine produces two types of data: 1) Adversarial Instruction data: a total of 344K samples, consisting of mixed positive and negative pairs, formulated into four new tasks. 2) Diverse Instruction data: a total of 508K samples, covering multiple tasks and various language styles, diversified through in-context learning from ChatGPT."}, {"title": "EXPERIMENTS", "content": "We provide quantitative results on five widely-used 3D multimodal learning benchmarks: ScanRefer [Chen et al., 2020] for 3D Visual Grounding, Multi3DRefer [Zhang et al., 2023] for General 3D Visual Grounding including zero, single and multiple target objects, Scan2Cap [Chen et al., 2021] for 3D Dense Captioning, ScanQA [Azuma et al., 2022] for 3D Question Answering, and SQA3D [Ma et al., 2022] for 3D Situated Question Answering. The vision data are all based on the ScanNet dataset [Dai et al., 2017], which contains real world 3D point clouds across 1,513 indoor scenes with detailed object annotations. All these benchmarks follow the same data split as ScanNet.\nWe follow the standard evaluation metrics widely adopted in the respective benchmarks. For Scan-Refer, we calculate accuracy at Intersection over Union (IoU) thresholds of 0.25 and 0.5 (Acc@0.25, Acc@0.5). For Multi3DRefer, we use the F1 score with IoU thresholds of 0.25 and 0.5 to measure performance. In Scan2Cap, we apply the CIDEr@0.5 and BLEU-4@0.5 (C@0.5, B-4@0.5) metrics, combining standard captioning metrics with the IoU metric. For ScanQA, the METEOR and ROUGE metrics, denoted as M and R, are employed. Lastly, SQA3D is assessed with exact match accuracy (EM) and its extended form, EM-R, as suggested by LEO [Huang et al., 2023b]."}, {"title": "BENCHMARKS AND METRICS", "content": "IMPLEMENTATION DETAILS\nWe extract 150 object features from each 3D scene, along with the corresponding position embeddings and 3D masks generated by Mask3D. The 2D Projector, as shown in Fig. 2, is a two-layer MLP. We use the Vicuna-7B-v1.5 model [Chiang et al., 2023] as our LLM and fine-tune it using LORA [Hu et al., 2021] (with a rank of 16) by Cross Entropy loss. The global learning rate is formulated as [batch size \u00d7 base learning rate \u00d7 number of GPUs] and is set to 0.00064, with a cosine annealing schedule. For our results in Tab. 1, we first train for 2 epochs on the RIG-generated data, and then train for 2 epochs on the benchmark training sets in the second stage. We train for 1 epoch for each stage to efficiently conduct ablation studies of RIG-generated data. For ablation on RAP and IFB, we train for 1 epoch on the benchmark training sets to avoid potential compound effects."}, {"title": "QUANTITATIVE RESULTS", "content": "We classify current methods into two categories: Task-Specific Training and Joint Training. Task-Specific Training refers to models only trained for a specific task, while Joint Training means training on multiple tasks jointly. Our Robin3D does not conduct task-specific fine-tuning.\n\u2022 Task-Specific Training: As shown in Table 1, models like EDA and M3DRef-CLIP perform well on their respective tasks due to customized model design for the task. However, they lack the ability to generalize to other tasks. Models like Vote2Cap-DETR++ and SQA3D encounter the similar issue. Therefore, they are not suitable to serve as general-purpose 3D AI agents.\n\u2022 Joint Training: Benefiting from sharing the knowledge across multiple tasks, models like 3D-VisTA and PQ3D show decent performance across multiple tasks, but their dependence on task-specific heads restricts their generalizability. Models like LEO and Chat-Scene show promising results by leveraging LLMs, but their sole training on positive pairs and template-based instructions leads to suboptimal generalization.\n\u2022 Our Robin3D: Due to the robust instruction data generated by RIG, Robin3D significantly outperforms previous models across all the benchmarks. Specifically, Robin3D brings a 6.9% improvement on Scan2Cap CIDEr@0.5 and a 5.3% improvement on ScanRefer Acc@0.25. Notably, on the evaluation of Multi3DRefer, which contains zero-target cases that are challenging for models to be discriminative and learn to say \"No\", our Robin3D achieves a 7.8% improvement in F1@0.25 and a 7.3% improvement in F1@0.5."}, {"title": "ABLATION STUDY", "content": "We perform ablation studies from the perspective of training data and model structure, respectively. We first evaluate the effectiveness of RIG-generated data by progressively adding the Adversarial Instruction data and the Diverse Instruction data to the training set. We then investigate the contribution of RAP and IFB by comparing models with and without these components.\nRobust Instruction Generation (RIG): As shown in Tab. 2, by adding the Adversarial Instruction data, we observe a consistent improvement across all benchmarks. Specifically, performance on ScanRefer and Multi3DRefer increases by 3.7% and 4.9%, respectively. It is worth noting that the performance on Scan2Cap improves by 8.9%, even though there is not any object captioning data in the Adversarial Instruction data, which highlights its contribution on enhancing the understanding towards each object by challenging the model with mixed positive and negative samples. Additionally, by adding the Diverse Instruction data, we also provide comprehensive improvements. Specifically, descriptions from the original ScanRefer are annotated by human following a fixed instruction template or expression style, which limits the language diversity. In contrast, the Diverse Instruction data contains various language styles and task formats, which helps the model generalize better, resulting in a 5.3% improvement on ScanRefer. Finally, by combining both two types of data, we achieve a further improvement, demonstrating the effectiveness of RIG-generated data.\nRelation-Augmented Projecter (RAP) and ID-Feature Bonding (IFB): As shown in Tab. 3, the integration of RAP leads to steady improvements across all benchmarks. Notably, the performance on Visual Grounding tasks, including ScanRefer and Multi3DRefer, shows significant gains due to RAP's enhanced spatial comprehension. When IFB is incorporated, further improvements are observed, emphasizing the importance of refining the model's object referring and grounding capabilities by reinforcing the connection between IDs and features."}, {"title": "QUALITATIVE RESULTS", "content": "We provide visualization of Robin3D's responses on all the benchmarks in Fig. 5 with the prompts of each task. These results demonstrates the generalization ability of Robin3D on various tasks."}, {"title": "CONCLUSION", "content": "To build a general-purpose AI agent in the 3D real world, we identify the problem of a lack of robust instruction training data in current 3DLLMs. To tackle this challenge, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. We generate and collect 1 million instruction data, including benchmark data, adversarial data, and diverse data. To better handle these complex instructions, Robin3D incorporates a Relation-Augmented Projector to enhance the understanding of spatial relationships among objects, and ID-Feature Bonding for better object referring and grounding. Finally, Robin3D achieves state-of-the-art performance across five widely-used 3D multimodal learning benchmarks, making significant progress towards Spatial Intelligence."}]}