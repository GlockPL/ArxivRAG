{"title": "MAP: UNLEASHING HYBRID MAMBA-TRANSFORMER VISION BACKBONE'S POTENTIAL WITH MASKED AUTOREGRESSIVE PRETRAINING", "authors": ["Yunze Liu", "Li Yi"], "abstract": "Mamba has achieved significant advantages in long-context modeling and autoregressive tasks, but its scalability with large parameters remains a major limitation in vision applications. pretraining is a widely used strategy to enhance backbone model performance. Although the success of Masked Autoencoder in Transformer pretraining is well recognized, it does not significantly improve Mamba's visual learning performance. We found that using the correct autoregressive pretraining can significantly boost the performance of the Mamba architecture. Based on this analysis, we propose Masked Autoregressive Pretraining (MAP) to pretrain a hybrid Mamba-Transformer vision backbone network. This strategy combines the strengths of both MAE and Autoregressive pretraining, improving the performance of Mamba and Transformer modules within a unified paradigm. Additionally, in terms of integrating Mamba and Transformer modules, we empirically found that inserting Transformer layers at regular intervals within Mamba layers can significantly enhance downstream task performance. Experimental results show that both the pure Mamba architecture and the hybrid Mamba-Transformer vision backbone network pretrained with MAP significantly outperform other pretraining strategies, achieving state-of-the-art performance. We validate the effectiveness of the method on both 2D and 3D datasets and provide detailed ablation studies to support the design choices for each component.", "sections": [{"title": "1 INTRODUCTION", "content": "The State Space Model(Hamilton, 1994) has demonstrated strong capabilities in long-context language modeling. The recent emergence of the variant framework Mamba(Gu & Dao, 2023) has sparked interest in comparing its abilities with those of Transformers. Due to its linear complexity and selective scanning mechanism, Mamba shows significant advantages in computational efficiency when handling long contexts. However, Mamba-based architectures(Zhu et al., 2024b) are difficult to scale concerning the number of parameters, which poses a major limitation for vision applications. To enhance Mamba-based backbones for vision tasks, there's a trend of combining Mamba with Transformers to create hybrid backbones(Lieber et al., 2024; Hatamizadeh & Kautz, 2024), leveraging the strengths of both. However, to truly scale up these hybrid vision backbones, a good pretraining strategy is essential for maximizing the combined capabilities of Mamba and Transformer. Our work aims to take the first step in this direction.\nDeveloping an effective pretraining strategy for Mamba-Transformer vision backbones is challenging. Even for purely Mamba-based backbones, pretraining methods are still underexplored, and the optimal approach remains unclear. Additionally, the hybrid structure requires a pretraining strategy compatible with both computation blocks. This is particularly challenging because the State Space Model captures visual features very differently from Transformers.\nTo address these challenges, we conducted extensive pilot studies and identified three key observations. Firstly, existing popular pretraining strategies for Transformers, such as MAE(He et al., 2022) and Contrastive Learning(CL)(He et al., 2020), do not yield satisfactory results for Mamba-based backbones, highlighting the need for a more suitable method. Secondly, Autoregressive Pretraining(AR)(Ren et al., 2024) can be effective for Mamba-based vision backbones, provided that an appropriate scanning pattern and token masking ratio are employed. Thirdly, pretraining strategies suitable for either Mamba or Transformers may not effectively benefit the other, and hybrid backbones require a tailored approach to address the learning needs of different computation blocks.\nBased on the above observations, we develop a novel pretraining strategy suitable for the Mamba-Transformer vision backbone named Masked Autoregressive pretraining, or MAP for short. The key is a hierarchical pretraining objective where local MAE is leveraged to learn good local attention for the Transformer blocks while global autoregressive pretraining enables the Mamba blocks to learn meaningful contextual information. Specifically, the pretraining method is supported by two key designs. First, we leverage local MAE to enable the hybrid framework, particularly the Transformer module, to learn local bidirectional connectivity. This requires the hybrid network to predict all tokens within a local region after perceiving local bidirectional information. Second, we autoregressively generate tokens for each local region to allow the hybrid framework, especially the Mamba module, to learn rich contextual information. This requires the network to autoregressively generate subsequent local regions based on the previously decoded tokens.\nOur experiments demonstrate that hybrid Mamba-Transformer models pretrained with MAP outperform other pretraining strategies by a significant margin. MAP with the hybrid Mamba-Transformer and pure Mamba backbone can both achieve impressive results on the ImageNet-1k(Deng et al., 2009a) classification task and other 3D vision tasks(Yi et al., 2016; Wu et al., 2015; Uy et al., 2019b). Furthermore, we tried different hybrid integration strategies for combining Mamba and Transformer layers showing that placing Transformer layers at regular intervals within Mamba layers led to a substantial boost in downstream task performance.\nOur contributions are threefold: Firstly, we propose a novel method for pretraining the Hybrid Mamba-Transformer Vision Backbone for the first time, enhancing the performance of hybrid backbones as well as pure Mamba and pure Transformer backbones within a unified paradigm. Secondly, we conduct an in-depth analysis of the key components of Mamba with autoregressive pretraining, revealing that the effectiveness hinges on maintaining consistency between the pretraining order and the Mamba scanning order, along with an appropriate token masking ratio. Thirdly, we demonstrate that our proposed method, MAP, significantly improves the performance of both Mamba-Transformer and pure Mamba backbones across various 2D and 3D datasets."}, {"title": "2 RELATED WORK", "content": "Vision Mambas and Vision Transformers. Vision Mamba(Vim)(Zhu et al., 2024a) is an efficient model for visual representation learning, leveraging bidirectional state space blocks to outperform traditional vision transformers like DeiT in both performance and computational efficiency. The VMamba(Liu et al., 2024) architecture, built using Visual State-Space blocks and 2D Selective Scanning, excels in visual perception tasks by balancing efficiency and accuracy. Autoregressive pretraining(ARM)(Ren et al., 2024) further boosts Vision Mamba's performance, enabling it to achieve superior accuracy and faster training compared to conventional supervised models. Nevertheless, why autoregression is effective for Vision Mamba and what the key factors are remains an unresolved question. In this paper, we explore the critical design elements behind the success of Mamba's autoregressive pretraining for the first time. Vision Transformers(ViT)(Dosovitskiy, 2020) adapt transformer architectures to image classification by treating image patches as sequential tokens. Swin Transformer(Liu et al., 2021) introduces a hierarchical design with shifted windows, effectively capturing both local and global information for image recognition. MAE (He et al., 2022) enhances vision transformers through self-supervised learning, where the model reconstructs masked image patches using an encoder-decoder structure, enabling efficient and powerful pretraining for vision tasks. However, the MAE pretraining strategy is not effective for Mamba, which hinders our ability to pretrain the hybrid Mamba-Transformer backbones.\nSelf-Supervised Visual Representation Learning. Self-Supervised Visual Representation Learning is a machine learning approach that enables the extraction of meaningful visual features from large amounts of unlabeled data. This methodology relies on pretext tasks, which serve as a means to learn representations without the need for explicit labels. GPT-style AR(Han et al., 2021) models predict the next part of an image or sequence given the previous parts, encouraging the model to understand the spatial or temporal dependencies within the data. MAE(He et al., 2022) methods mask out random patches of an input image and train the model to reconstruct these masked regions. This technique encourages the model to learn contextual information and global representations. Contrastive Learning(CL)(He et al., 2020) techniques involve contrasting positive and negative samples to learn discriminative features. It typically involves creating pairs of positive and negative examples and training the model to distinguish between them. However, we found that existing pretraining strategies fail to fully unlock the potential of the hybrid framework, which motivated us to explore a new pretraining paradigm for hybrid Mamba-Transformer backbones."}, {"title": "3 PILOT STUDY: HOW TO PRE-TRAIN THE VISUAL MAMBA BACKBONES?", "content": "In this Section, we first conduct experiments to investigate the differences in pretraining strategies for ViT and Vim. The success of the MAE strategy on the ViT architecture is well acknowledged, while the Vim pretraining strategy remains in its early stages. We are interested in determining whether the MAE strategy is equally applicable to Vim or if the AR strategy is more suitable. To explore this, we conduct experiments on the classification task using the ImageNet-1K dataset. The results are shown in Table 1.\nWe observe that the MAE strategy significantly enhances the performance of ViT. However, for Vim, the MAE strategy does not yield the expected improvements, while the AR strategy substantially boosts its performance. This indicates that for the ViT architecture, applying the MAE strategy is essential to establish bidirectional associations between tokens, thereby improving performance. In contrast, for Vim, it is more important to model the continuity between preceding and succeeding tokens. Based on this observation, we conducted an in-depth analysis of the various components involved in AR pretraining for Mamba and discovered that consistent autoregression pretraining with scanning order and proper masking ratio is the key to pretraining Mamba.\nRelationship between AR and Scanning Order. Since the goal of AR pretraining is to learn a high-quality conditional probability distribution, enabling the model to generate new sequences based on previously generated content, we first explore how the prediction order in auto-regressive models affects the pretraining of Vim. Different prediction orders can significantly impact how the model captures image features and the effectiveness of sequence generation. By adjusting the prediction order, we can gain deeper insights into Vim's behavior in sequence generation tasks and how to effectively model dependencies between elements in an image. Further analysis of the role of prediction order will help optimize AR pretraining for Vim, exploring how the model can better capture the continuity and relationships of image information under different contextual conditions. We conduct ablation studies on Vim by allowing it to perform both row-first and column-first scanning. We then pretrain it with row-first and column-first AR orders, respectively, to compare their performance. Figure 3 shows different orders for AR pretraining and Mamba scanning.\nThe results are shown in Table 2. We observe that employing a pretraining strategy consistent with the scanning order significantly enhances Vim's performance. This suggests that when designing pretraining strategies, they should be aligned with the downstream scanning order.\nMasking Ratio of Autoregression Pretraining. Since the success of MAE is primarily attributed to the use of an appropriate masking ratio, we are inspired to conduct experiments to verify whether different auto-regressive masking ratios will affect the quality of pretraining. We found that during AR pretraining, masking a certain number of tokens at the end of the sequence is crucial. Masking a single token follows the traditional AR paradigm, while masking $n$ tokens transforms the task into an inpainting problem, as the input and output sequence lengths remain equal. In this context, varying the auto-regressive masking ratios effectively adjusts the inpainting ratio, influencing the model's predictions beyond just the sequence length.\nOur pretraining sequence length was set to 196 tokens, and we masked 1 token (0.5%), 20 tokens (10%), 40 tokens (20%), 60 tokens (30%), 100 tokens (50%), and 140 tokens (70%), respectively, while also recording the results of fine-tuning on downstream tasks. Figure 4 shows the pipeline of AR Pretraining under different mask ratios.\nThe results shown in Table 3 indicate that a proper masking ratio contributes to training stability, helping to avoid excessive noise interference. In auto-regressive pretraining, as the Masking Ratio increases, the performance of the Mamba improves. This is because a higher Masking Ratio encourages the model to learn more complex and rich feature representations, thereby enhancing its generative ability and adaptability. However, an excessively high Masking Ratio may lead to instability during the training process and result in incomplete information perception. We found there exists a sweet spot around 20% on the ImageNet-1K classification task. In such cases, the model may struggle to make accurate predictions due to a lack of sufficient contextual information, negatively impacting its pretraining effectiveness. Therefore, when designing auto-regressive pretraining tasks, finding an appropriate masking ratio is crucial to strike a balance between performance improvement and training stability."}, {"title": "4 MASKED AUTOREGRESSIVE PRETRAINING FOR HYBRID BACKBONES", "content": "Our approach represents a general paradigm applicable to data across various domains, with 2D image data as an example. Our method can be easily extended to large language models (LLMs) and the fields of image video and point cloud video. Our method optimizes the synergy between Mamba and Transformer within a unified framework, allowing both models to fully leverage their strengths. In the Mamba-Transformer hybrid architecture, this approach effectively enhances the cooperation between the models, resulting in significant performance improvements. Specifically, our approach includes a masking strategy, a hybrid Mamba-Transformer encoder, and a Transformer decoder. The hybrid Mamba-Transformer encoder is responsible for mapping the signals into latent space, while the Transformer decoder autoregressively reconstructs the features back into the original image. The following section will introduce the specific design components of the framework. The subsequent experiments in this section are conducted using the base-sized model on the ImageNet-1K dataset.\nMasking. Consistent with MAE, we first tokenize the image and then apply random masking to a portion of the tokens. We experimented with different masking strategies, including random, sequential, and diagonal masking. Our experiments show that random masking delivers the best results. We attribute this to the fact that sequential and diagonal masking can hinder the Transformer's ability to establish contextual relationships. Random masking not only promotes bidirectional modeling for Transformers but also enhances Mamba's generalization and representation capabilities in sequence modeling. Additionally, we explored the effects of different masking ratios and found that a 50% masking ratio yielded the best results. This conclusion aligns with intuition: while MAE performs optimally on Transformers with a 75% masking ratio, previous experiments showed that AR achieves the best results on Mamba with a 20% ratio. Therefore, a 50% ratio serves as a balanced number, leveraging the strengths of both paradigms.\nMAP Hybrid Mamba-Transformer Encoder. We designed a series of hybrid Mamba-Transformer vision backbones and compared their performance when trained from scratch. The results indicate that the hybrid approach using MMMTMMMT performs the best. When comparing Mamba-R* with MMMMMMTT, we found that adding a Transformer after Mamba enhances its long-context modeling capabilities, leading to improved performance. However, when comparing MMMM-MMTT with TTMMMMMM, we observed that simply appending Transformers after Mamba does not fully leverage the architecture's potential. This suggests that incorporating Transformers at the beginning is crucial for extracting sufficient local features. We believe that the MMMTMMMT approach effectively balances local feature extraction and contextual modeling enhancement, making it our default configuration."}, {"title": "5 EXPERIMENTS", "content": "5.1 2D EXPERIMENTS ON IMAGENET-1K CLASSIFICATION TASK\nSettings. We pretrained on the training set of the ImageNet-1K(Deng et al., 2009b) dataset and then fine-tuned on its classification task. We report the top-1 validation accuracy of a single 224x224 crop, and in some settings, we also report the results for a 384x384 crop. During the pretraining phase, we applied a random masking strategy with a 50% masking ratio, using only random cropping as the data augmentation strategy. We utilized AdamW as the optimizer and trained for 1600 epochs across all settings. Additionally, we pretrained using the MAP paradigm on pure Mamba and pure Transformer networks, demonstrating that this paradigm is effective for both frameworks. In the fine-tuning phase, we directly fine-tune for 400 epochs and report the results.\nResults. Results are shown in Table 8. The results indicate that the hybrid framework achieves a balance between performance and computational overhead. However, simply training the hybrid architecture from scratch does not lead to significant performance improvements compared to pure Mamba and Transformer backbone. Our proposed pretraining method significantly enhances the performance of the hybrid Mamba-Transformer framework. Additionally, we verified that our MAP method also significantly improves the performance of both the pure Mamba framework and the pure Transformer backbone. Furthermore, when comparing models of the base size with other pretraining methods, we observed that contrastive learning pretraining does not yield performance improvements. The original MAE and AR methods also fail to fully exploit the capabilities of the hybrid Mamba-Transformer backbone, with their results significantly lower than our MAP pretraining method. This further demonstrates the effectiveness of our method for the hybrid framework.\nResults with Different Hybrid Ratio for Mamba and Transformer. In our experiments, we used a 3:1 hybrid ratio of Mamba to Transformer. We also explored other hybrid ratios, and the results, as shown in Table 9, indicate that there are no significant performance differences among the hybrid models with varying ratios after MAP pretraining. Considering computational efficiency and memory savings, we opted to adopt the 3:1 hybrid ratio as our default configuration.\n5.2 3D EXPERIMENTS ON MODELNET40, SCANOBJECTNN AND SHAPENETPART\nSettings. We pretrained using the ShapeNet(Chang et al., 2015) dataset, employing random rotation and translation scaling as data augmentation techniques. Each point cloud consists of 1024 points and is divided into 64 patches, with each patch containing 32 points. We also used a hybrid ratio of Mamba to Transformer at 3:1, randomly masking 50% of the patches. Since point clouds are unordered, the concept of rows does not apply here; instead, we randomly generate 32 patches each time and complete the reconstruction process in an autoregressive manner. Similar to Mamba3DHan et al. (2024), we did not adopt any special sorting strategies but ensured that the order of pretraining matches that of the actual Mamba scans. We conducted pretraining on both the hybrid framework and the original Mamba3D to validate their performance advantages in both the pure Mamba framework and the hybrid framework. During pretraining and downstream fine-tuning, we employed the AdamW optimizer with a cosine decay strategy for 300 epochs. For the ModelNet40Wu et al. (2015) fine-tuning experiments, we used translation and scaling as data augmentation, while on ScanObjectNNUy et al. (2019a), we applied random rotation as data augmentation. Additionally, I also performed experiments in few-shot settings and on ShapeNet part(Yi et al., 2016) segmentation.\nResults. The experiments demonstrate that our method significantly enhances the performance of both the hybrid framework and the pure Mamba framework on 3D tasks. This suggests that our approach can be easily adapted to other domains and data types, such as LLMs and video data. Notably, in the part segmentation task, the performance of the hybrid framework trained from scratch is inferior to that of the pure Mamba framework. However, after pretraining, the advantages of the hybrid framework are fully realized, significantly surpassing the performance of the pure Mamba framework. This further proves that our method can simultaneously harness the potential of both Mamba and Transformer to achieve better performance."}, {"title": "6 CONCLUSION", "content": "In this paper, we begin with an in-depth analysis of the key factors that contribute to the success of autoregressive pretraining for Mamba. Based on this, We introduce a pretraining strategy specifically designed for the Mamba-Transformer hybrid framework for the first time. This strategy is effective not only for the hybrid backbones but also for pure Mamba and pure Transformer backbones. We have validated the effectiveness of our approach on both 2D and 3D datasets."}]}