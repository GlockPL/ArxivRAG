{"title": "BOOKWORM: A Dataset for Character Description and Analysis", "authors": ["Argyrios Papoudakis", "Mirella Lapata", "Frank Keller"], "abstract": "Characters are at the heart of every story, driving the plot and engaging readers. In this study, we explore the understanding of characters in full-length books, which contain complex narratives and numerous interacting characters. We define two tasks: character description, which generates a brief factual profile, and character analysis, which offers an in-depth interpretation, including character development, personality, and social context. We introduce the BOOKWORM dataset, pairing books from the Gutenberg Project with human-written descriptions and analyses. Using this dataset, we evaluate state-of-the-art long-context models in zero-shot and fine-tuning settings, utilizing both retrieval-based and hierarchical processing for book-length inputs. Our findings show that retrieval-based approaches outperform hierarchical ones in both tasks. Additionally, fine-tuned models using coreference-based retrieval produce the most factual descriptions, as measured by fact- and entailment-based metrics. We hope our dataset, experiments, and analysis will inspire further research in character-based narrative understanding.", "sections": [{"title": "1 Introduction", "content": "Stories play a key role in shaping our understanding of the world, serving as a medium to share experiences, communicate ideas, teach, and entertain. The two main building blocks of every story are the plot and characters (Phelan, 1989). Characters are particularly important, as they form the primary means through which readers engage and relate to the story.\nUnderstanding characters is also necessary from a computational perspective, if models are to summarize, analyse, or generate stories effectively. Over the past decade, the field of natural language processing has developed computational methods to understand narratives from a character-centric perspective. Previous work has focused on detecting characters (Chen and Choi, 2016), understanding latent personas (Bamman et al., 2013), their emotions (Kim and Klinger, 2019), and their relationships (Chaturvedi et al., 2017). Another line of work has attempted to describe characters with a set of attributes (Zhang et al., 2019) or personality types (Sang et al., 2022). Most prior research studies characters in short stories or adopts relatively simplistic analysis methods (e.g., summaries) when it comes to long narratives.\nIn this paper, we focus on analyzing characters in long-form stories, a relatively understudied area that presents unique challenges not found in short stories. Firstly, long stories typically contain a large number of characters with complex relationships and interactions which have a key role in the plot. Secondly, characters in long stories are dynamic (Chaturvedi et al., 2017): they develop throughout the story and their personalities, motivations, and relationships change as the plot evolves. Finally, long narratives exceed the input length that many current transformer-based architectures (Vaswani et al., 2017) can process, making the problem technically challenging.\nWe work towards addressing these challenges and study characters from a text-generation perspective, focusing on two tasks: (1) character description produces a general profile of a character (e.g., their actions, relationships, attributes) and (2) character analysis produces an in-depth interpretation of a character's personality and behaviour (e.g., how the character's personality develops, their motives, or the social context). The character description task has been introduced with the release of the LiSCU dataset (Brahman et al., 2021), which contains literary book summaries paired with human-written character descriptions. However, using summaries to describe characters significantly simplifies and restricts the task. Summaries contain limited information about the overall story, usually only the salient events, and important details are omitted. At the same time, a book summary cannot be used to describe every character of a narrative, but only those important enough to figure in the summary.\nFor these reasons, our work focuses exclusively on describing characters attested in full-length books. In addition, we propose character analysis as a new task, which complements and extends character description in that it requires a more in-depth understanding of characters. It goes beyond just describing surface-level traits, critically analyzing the character's depth, complexity, and evolution within the narrative context. Character analyses also typically explore the social, political, or historical context relevant to understanding a character and their behaviour. We show an example of these two tasks in Figure 1. Additional examples can be found in Appendix A.\nWhile previous work has made a significant effort to understand characters individually, treating them as isolated entities is a simplification. Characters have their own arcs in a story, but they are also interconnected \u2013 their actions, motivations, and relationships are all intertwined within the narrative (Weiland, 2016). Based on these observations, we introduce Joint Character Description, a variation of the character description task, where the model has to generate a description for every character sequentially. Our analysis shows that although current state-of-the-art language models can benefit from knowing all characters in a story, they struggle with joint character understanding."}, {"title": "2 Related Work", "content": "Narrative Structure Existing work has studied narratives and their plot structure, focusing primarily on summarization. Several datasets have been developed for narrative summarization. Examples include TRIPOD (Papalampidi et al., 2019), which contains movie scripts annotated with salient scenes or turning points. NarraSum (Zhao et al., 2022) has summaries of movies and TV series, while BookSum (Kry\u015bci\u0144ski et al., 2021) is a collection of literary artefacts (e.g., novels, plays) paired with summaries. Summarization is related to our character description and analysis tasks, but there are significant differences (Brahman et al., 2021). A summary captures the entire plot of a story and includes all main characters, while a character description focuses on a single character, their properties and actions, incorporating plot elements only when they help describe the character.\nCharacter Understanding Some prior research has studied narratives from a character-centric perspective, focusing on a variety of tasks: the identification of character personality (Sang et al., 2022; Bamman et al., 2013), prediction of character emotions (Brahman and Chaturvedi, 2020) and relationships (Chaturvedi et al., 2017), character detection (Chen and Choi, 2016), grounding (Liu and Keller, 2023), and the generation of character descriptions (Brahman et al., 2021). Another line of research has used characters as a means to generate new stories (Liu et al., 2020) or summaries (Zhang et al., 2019) of existing narratives. However, most existing research focuses on characters of short stories and studies only specific aspects (e.g., relationships, emotions). In this paper, we focus on characters of full-length books (with an average length of approximately 100k tokens), and study them more holistically, from the perspective of understanding their attributes, actions, and behavior (description task) and how are these interpreted in the context of the narrative (analysis task).\nCharacter Description Previous work (Zhang et al., 2019) has found that character descriptions occur commonly in human-written story summaries, thus advocating the identification of a set of character attributes as a useful intermediate step for automatic summarization. Chen and Gimpel (2022) introduced TVStoryGen, a dataset aiming to generate TV episode recaps based on summaries and character descriptions. Carlsson et al. (2021) released Gandalf, a dataset containing descriptions paired with multiple character names, with the task of choosing the correct one. Brahman et al. (2021) created LiSCU, a dataset which contains book summaries and human-written character descriptions. A part of this dataset includes full books, however, it has not been publicly released and the work focuses on describing characters from summaries, not books. The current paper contributes to this literature by introducing BOOKWORM, a new dataset for understanding characters based on the full-length books, without assuming that summaries are available. We also introduce the new task of character analysis, which aims to generate a more detailed account of a character's personality, motives, development, and social context. We compare our dataset with LiSCU in Section 3.2.\nLong-context Models Several papers have focused on alleviating the memory requirements of transformers, which increase quadratically with the input length. Sparse attention approaches like Big-Bird (Zaheer et al., 2021), Longformer (Beltagy et al., 2020), and Reformer (Kitaev et al., 2020) combine local windowed attention with global attention on a subset of tokens, enabling modelling of much longer sequences. Transformer-XL (Dai et al., 2019) introduces segment-level recurrence as another technique for capturing longer-range dependencies.\nAnother line of research has sought to overcome the limitations of input length by employing retrieval-augmented generation; Xu et al. (2023) show that retrieval can outperform long context transformers even when using shorter input. Other work (Wu et al., 2021; Chang et al., 2024) processes long documents hierarchically by segmenting the input into shorter chunks and generating intermediate responses, which are then aggregated into a final summary. We propose several retrieval-augmented models for our tasks, exploring different content extraction strategies (e.g., based on characters or a retrieval engine like BM25), and show they are superior to hierarchical generation."}, {"title": "3 The BOOKWORM Dataset", "content": "3.1 Data Collection\nFollowing previous work (Kry\u015bci\u0144ski et al., 2021; Brahman et al., 2021), we collect books from the Gutenberg Project\u00b9, which contains classic books, including novels, plays, and works of poetry. To obtain character descriptions and analyses, we scrape five different websites, namely Sparknotes, Litcharts, Gradesaver, Cliffnotes, and Shmoop.\u00b2 These websites contain complete studies of literary books, mainly for educational purposes. For our work, we use Litcharts, Sparknotes, Gradesaver and Cliffsnotes as sources for character descriptions and Sparknotes, Shmoop and Cliffsnotes as sources for character analyses. For websites that are used in both tasks, there is a clear distinction between the\n3.2 Data Analysis\nWe present various statistics on BOOKWORM in Table 1 and compare it with the related LiSCU dataset (Brahman et al., 2021). For completeness, we also report statistics for BookSum (Kry\u015bci\u0144ski et al., 2021), a book summarization dataset.\nFirstly, we note that the average book length for the description and analysis tasks is approximately 95k words, which is challenging even for current state-of-the-art transformer-based models. Additionally, we observe significant differences in task requirements; the average length of a description is 88 words, whereas the average analysis is 602 words. We have 324 and 133 unique books paired with 5,869 character descriptions and 1,328 character analyses, respectively. The LiSCU-summary partition has 9,499 samples, but contains only the summary of the story and not the full book; obtaining whole books is significantly harder than just collecting book summaries. Although the LiSCU-book partition is not publicly available, we report numbers from the corresponding paper (Brahman et al., 2021) in Table 1.\nFollowing Kry\u015bci\u0144ski et al. (2021), we show the literary genres represented in BOOKWORM in Figure 2. We observe similar trends across tasks: the books are mostly novels and plays, with some short stories, novellas and poetry collections; other genres (children's books, biographies and historical books) are sparsely represented."}, {"title": "4 Modeling Experiments", "content": "We conduct a series of experiments to benchmark the performance of current models and analyze their abilities across different dimensions. Initially, we explore the limits of simple extractive heuristics. Next, we evaluate an instruction-tuned model in a zero-shot setting and contrast its performance against fine-tuned models. Additionally, across our experiments, we evaluate different retrieval strategies that are generic or rely on domain-specific information and compare them with the hierarchical approach, which uses the full story.\n4.1 Extractive Heuristics\nWe adopt the Lead-k baseline (Narayan et al., 2018), which traditionally extracts the first k sentences from a source document. We adjust this baseline experiment to our task, extracting the first k sentences in which the character of interest is mentioned. We use the coreference model of the BookNLP library to identify character mentions. Similarly, we define a Random baseline by randomly extracting k sentences in which the character is mentioned. To define an upper bound of the extractive experiments, we develop an Extractive Oracle baseline, selecting the k sentences with the highest average rouge score against the gold-standard (Narayan et al., 2018). For the description task, we set k equal to four, while for the analysis task, we set k to 25, based on the average number of sentences for each task in BOOKWORM.\n4.2 Zero-shot Abstractive Models\nAll zero-shot experiments use Llama-3-8B-Instruct (Dubey et al., 2024) as a backbone model, with an input context of 8,192 tokens. As a simple baseline, we feed the model with the lead 8,192 tokens, truncating the rest of the input.\nWe further develop retrieval-augmented models, following an extract-and-generate approach. In one variant, we use the statistical BM25 method (Robertson et al., 1995) to extract relevant context. Specifically, we use the character's name as a query and select the 80 paragraphs with the highest score. In another variant, we use a coreference model from the BookNLP library to identify character mentions and extract paragraphs in which the target character is mentioned, following prior work (Brahman et al., 2021; Maddela et al., 2022). We then concatenate these paragraphs and feed them into the language model.\nWe compare retrieval-augmented models to a hierarchical approach in which all the book information is processed. Following previous work (Wu et al., 2021; Chang et al., 2024), we split the book into chunks of 8k tokens and generate a description for each chunk. We then concatenate these intermediate descriptions and feed them to the language model, which merges them into a final description. We experimented with adding more steps to the hierarchical approach, but we did not observe an improvement, and thus only report the results of single-step hierarchical processing in this paper. We show the prompts used for our zero-shot models in Appendix B.\n4.3 Fine-tuned Abstractive Models\nWe experiment with two architectures: the encoder-decoder LongT5-base model (Guo et al., 2022) with 16,384 tokens input context and the decoder-only Llama-3-8B-Instruct model (Dubey et al., 2024) with 8,192 tokens context length.\nAnalogously to our zero-shot models, we compare the fine-tuned models to a simple baseline, which truncates the input story at the maximum length the model can process. In addition, we evaluate the two retrieval strategies mentioned earlier, namely using BM25 or the coreference model from the BookNLP library. We fully fine-tune LongT5, while for Llama-3, we do parameter efficient fine-tuning using LoRA (Hu et al., 2021). We report the hyperparameters and additional training details in Appendix B.\n4.4 Generation Settings\nWe report experiments in two settings. The first setting is common in previous work (Brahman et al., 2021) and aims to generate a description or analysis for a character in isolation. In addition, we explore an alternative formulation where we collectively describe or analyse all the characters in a story. We call this setting joint character description. We also explore a variant where the model has to describe every character separately, but all the characters from the story are given as input in the prompt, so as to ensure parity of context for the two alternative task formulations.\nFor the joint description setting, we employ the hierarchical approach in a zero-shot fashion as described in Section 4.2. We use Llama-3-70B-Instruct as the base model because we find that smaller models fail to describe the characters jointly and output descriptions for each. This is particularly problematic for books with many characters. In this case, we adjust and prompt the model to describe five characters at a time instead of all characters together. If the model still struggles to follow the required template, then we describe the characters individually. We report the prompts used for these experiments in Appendix B.\n4.5 Evaluation Metrics\nAutomated evaluation metrics are crucial for our task and for related book-length applications where human evaluation is extremely labor-intensive, costly, and difficult to design (Krishna et al., 2023). As there is no single agreed-upon metric for automatically measuring character understanding, we evaluate output quality along different dimensions and report several complementary metrics.\nWe use Rouge F1 (Lin, 2004) against the reference descriptions as a way of assessing the informativeness of descriptions or analyses. We report Rouge-1 (unigram overlap), Rouge-2 (bigram overlap), and Rouge-L (longest common subsequence between the model output and the gold-standard description). We also report entity mention recall following prior work (Bertsch et al., 2023), which counts the percentage of named entities (e.g., person names, locations) present in the reference that are covered by the model output. Additionally, we use BERTScore (Zhang et al., 2020), which calculates token similarity using contextual embeddings instead of string matching.\nAs token-matching evaluation does not always correlate well with the quality of the generated text (Fabbri et al., 2021), we also use QA-based evaluation, following existing literature (Deutsch et al., 2021; Fabbri et al., 2022). Specifically, we create a set of question-answer pairs based on the reference descriptions and then use the model output to answer these questions. We expect factual descriptions to correctly answer a higher percentage of questions. We first prompt GPT-3.5, asking it to generate question-answer pairs based on gold-standard descriptions. Since question-answering models are typically trained on data different from the narrative domain, such as Wikipedia passages, we fine-tune a RoBERTa-large encoder (Liu et al., 2019) using QA pairs from our dataset. We discard low-quality questions through round-trip filtering (Alberti et al., 2019), i.e., we check whether the generated questions can indeed be answered using the reference description. We employ exact match and F1 (Rajpurkar et al., 2016) to evaluate all QA models. We present examples of the question-answering evaluation in Appendix D.\nAs all the above metrics are reference-based, we also use an entailment-based metric, which predicts whether the input story entails the model output. Specifically, following Narayan et al. (2022) and Laban et al. (2021), for each generated sentence, we calculate its maximum entailment score against the paragraphs of the input story. If a paragraph is longer than 512 tokens, we split it into shorter paragraphs. We also transform the entailment probability into 0 or 1 using a 0.5 threshold. Then, we calculate the average entailment score across the model output. As an entailment model for our experiments, we use T5-XXL (Raffel et al., 2020) fine-tuned on the Adversarial NLI dataset (Nie et al., 2020).\nPrevious research has also used LLM-as-a-judge pipelines to assess the quality of generated text (Mahon and Lapata, 2024; Min et al., 2023; Song et al., 2024; Zheng et al., 2023). In this paper, we adopt the PRISMA metric (Mahon and Lapata, 2024) using a large language model to evaluate the factuality of the generated outputs. Specifically, we calculate PRISMA-precision by extracting facts from the generated output and then using the gold-standard to judge whether these facts are supported or not. Similarly, we calculate PRISMA-recall by extracting facts from the gold-standard and then using the model output to assess these facts. PRISMA-F1 is then derived from these precision and recall values. We used GPT-4o-mini to extract facts and judge their factuality.\nAdditionally, we evaluate factuality across different character dimensions, by classifying the extracted facts into six distinct categories: Role (the part the character plays in the story), Relationship (connections the character has with others, e.g., friendships or family ties), Personality (the character's behavior, traits or attributes), Event (actions and decisions the character is involved in), Mental State (the character's state of mind, e.g., beliefs, intentions, and emotions), and Other Fact (any fact that does no belong to the above categories). We chose this categorization based on prior work (Brahman et al., 2021) and after having manually inspected examples of extracted facts. We again used GPT-4o-mini to classify facts into the above categories. To assess the reliability of the model's classification, we conducted a human annotation process where the authors of this paper classified 200 facts extracted from character descriptions and analyses. We found a strong agreement among"}, {"title": "5 Results", "content": "There is no lead bias in book-length character understanding. Our experimental results are summarized in Table 2. Lead-k performs poorly, even though it is a strong baseline in standard summarization tasks (Nallapati et al., 2016; Narayan et al., 2018). It achieves substantially lower scores in terms of Rouge compared to zero-shot and fine-tuned models. Random selection performs similarly, achieving marginally worse scores than the Lead baseline in both tasks.\nThe extractive oracle heuristic achieves the highest Rouge scores across all experiments in both tasks. There is a bigger performance gap when it comes to the analysis task, where oracle experiment is ostensibly better, especially in Rouge-1 and Rouge-2, compared to zero-shot and fine-tuned models. This result is expected as the oracle model uses gold-standard texts to extract sentences. When considering Entity Mention recall, we observe that the Oracle model is worse at the description task than zero-shot and fine-tuned models. Interestingly, in the analysis task, while the Oracle model scores lower than zero-shot models in Entity Mention recall, it surpasses the fine-tuned models in this metric. This result demonstrates that there is still space for improvement in the way that our experiments retrieve context and use salient entities.\nBERTScore results for the Oracle model are comparable to the Lead and Random heuristics, and lower compared to abstractive models. This is an expected outcome, as the Oracle fails to capture the semantic information of the reference descriptions, even if it matches the gold-standard tokens.\nRetrieval-augmented models perform best in both character description and analysis. In our zero-shot experiments, we observe that the retrieval-based methods consistently improve performance in both tasks. Specifically, the coreference approach outperforms BM25 in the description task while BM25 performs better in the analysis. The hierarchical approach improves results compared to the Lead baseline in the description task but does not match the performance of retrieval-based methods. In the analysis task, the performance is slightly worse than in the Lead experiment. We hypothesize that this occurs because retrieving relevant information is more crucial than processing the entire story for tasks like character description and analysis, which resemble query-based summarization more than generic summarization.\nFor our fine-tuned models, we observe trends similar to the zero-shot ones. Specifically, both the BM25 and coreference-based retrieval lead to better descriptions and analyses, with the exception of LongT5 in the analysis task, where the differences are only marginal. Llama-3 consistently outperforms LongT5 across both tasks. While fine-tuning leads to consistent improvements in the description task, this is not the case for the analysis task where fine-tuning is either comparable or inferior to the zero-shot setting. We hypothesize that there are two reasons for this, the training samples are fewer in the analysis task and the level of data contamination is higher (see Appendix C).\nFine-tuned Llama with coreference-based retrieval is the most faithful. We report QA-based and NLI-based evaluation results in Table 3. We focus on fine-tuned models as these performed better in most cases than zero-shot ones and extractive baselines, according to reference-based metrics (see Table 2). QA-based metrics reward Llama most when enhanced with coreference-based retrieval in both tasks. In general, performance improves when relevant context is retrieved in both the LongT5 and Llama models. Llama consistently outperforms LongT5, and coreference-based retrieval yields better results than extraction using BM25.\nThe NLI metric has a clear preference for models fine-tuned on coreference-based input. In particular for Llama, we observe a large jump in entailment accuracy over BM25. Retrieving relevant context helps achieve higher entailment scores for both tasks. The only exception is the entailment accuracy of LongT5 combined with BM25 on the analysis task, where performance decreases compared to LongT5 on its own. The coreference resolution approach is consistently better than BM25.\nFacts related to events and relationships are hard to get right. We report the fact-based evaluation in Table 4. We observe that retrieval-augmented models demonstrate higher overall factuality, leading to improvements across nearly all character dimensions for both tasks. An exception is the LongT5 model for the character analysis task, where the lead baseline outperforms BM25 and coreference-based models. The coreference-based model surpasses BM25 in description and analysis, while the Llama model consistently outperforms LongT5. Across both tasks, facts related to events and character relationships are the least factual. In contrast, facts concerning a character's role and personality achieve the highest scores. Mental state and other facts perform similarly, but they fall below those related to personality and role. Our results demonstrate that models struggle with the more dynamic aspects of characters, such as events and relationships, while handling more static dimensions like role and personality more effectively. Examples of the fact-based evaluation are in Appendix D.\nIt is easier to talk about one character than about many. Table 5 presents results in the two generation settings: joint and separate character description. For this comparison, we employ the hierarchical method in a zero-shot setting with Llama-3-70B-instruct (see Section 4.4), as we observed that smaller models could not follow instructions for the joint task.\nThe model generally struggles with the joint task, performing consistently worse across metrics compared to describing each character individually. As we can see in Table 5, the model benefits from having a list of the characters in the story. We observe performance gains across metrics when character names are included in the input. We believe the joint description task is too difficult for the model which is now required to understand the story from beginning to end instead of being able to focus on a single character. Aside from understanding being harder, generation is also more challenging, as the output is quite long in this setting. Even Llama-3-70B struggles to describe all the characters. Examples of generated outputs are in Appendix D."}, {"title": "6 Discussion", "content": "Our experiments underline the importance of retrieving relevant context; we found that even simple methods such as statistical retrieval with BM25 or coreference-based retrieval lead to consistent improvements in all our experiments. Notably, while the hierarchical approach is considered state-of-the-art for book summarization, our experiments revealed it performs worse than retrieval in both description and the analysis tasks. The difference between retrieval-based and hierarchical approaches is significant even for character analysis. One might conjecture that processing the whole book would be beneficial, however, this is not corroborated by our results.\nUntil now, characters have been studied separately in the literature, which is a significant simplification. Our experiments with joint understanding of characters show that a separate description model can benefit from knowing all the different characters in a story if we list them in the initial prompt. However, our experiments also demonstrate that models struggle to understand characters jointly, having to \u201cread\u201d a book multiple times to be able to describe each character separately."}, {"title": "7 Conclusions", "content": "In this work, we created BOOKWORM, a new dataset which contains books from the Gutenberg project and human-written character descriptions and character analyses from literature websites. Character descriptions are short and factual, while character analyses are longer; they explore the motives, personality, and development of a character and often also comment on the social, historical, or political context. We established a set of baselines using simple extractive heuristics as well as retrieval-based and hierarchical long-context models, in both zero-shot and fine-tuning settings. Our experiments highlight the importance of retrieving relevant context, which leads to consistent improvements and outperforms hierarchical methods.\nWe hope our findings will inspire future research on character analysis, and text generation from long documents more generally. We plan to develop a better suited model for the joint character description task, by keeping track of characters and their relations as the narrative evolves. Evaluation is another avenue for future work. Entailment-based metrics are good indicators of model performance for retrieval-augmented approaches, but are computationally challenging for book-length inputs. Question-answering evaluation helps assess the factuality of the generated text but is constrained by the availability of references, and can be too punitive (in cases where model predictions have no lexical overlap with the reference). We used a LLM as a judge to perform a fact-based evaluation and gain a deeper understanding of the factuality of the different character dimensions. However, these results are again solely based on reference texts. In the future, there is a need to explore evaluation metrics that consider the full input text and are at the same time efficient and scalable."}, {"title": "Limitations", "content": "Our dataset contains publicly available books discussed widely across multiple sources (reviews, critical essays, literary commentaries, etc). Even if models have not been trained on the description and analysis tasks, it is likely that they have been exposed to these literary texts or related information during pre-training. To mitigate the risk of data contamination, future work should consider using books that are not publicly available.\nIn this paper, we relied on automatic metrics such as Rouge, QA-based evaluation, entailment and fact-based scores, entity mention recall and BERTScore to assess the quality of generated descriptions and analyses; however, the majority of these metrics are reference-based and do not consider book-length input to evaluate different aspects of model output. Future work could focus on reference-free evaluation metrics and efficient methods to conduct human-based evaluation.\nMoreover, current language models used in this paper do not provide any explanation about the generated text. Future research could focus more on attributable language models that generate text pointing to specific parts of the input. This would also mitigate the difficulty of conducting human evaluation, especially for tasks like character description or analysis, where many responses can be produced, but it is important to evaluate whether they are faithful.\nOur work considers simple retrieval-based strategies such as BM25 and the use of an off-the-self coreference model. A natural next step would be to explicitly train a retriever model for the two tasks in the BOOKWORM dataset. Finally, we present experiments with only one type of hierarchical model in the joint character description setting. Follow-on work could study this setting in more depth."}, {"title": "A BOOKWORM Descriptions and Analyses", "content": "We identify the subset of BOOKWORM that includes characters featured in both the description and analysis tasks. We then compare the two tasks by calculating the percentage of novel n-grams in descriptions compared to analyses. We present these statistics in Table 6."}, {"title": "B Implementation Details", "content": "We fully fine-tune LongT5-base (250M), which takes approximately 10 GPU hours for the description task and 4 GPU hours for the analysis task. For Llama-3-8B, we use parameter-efficient fine-tuning with LoRA, which takes roughly 8 and 3 GPU hours for the description and analysis tasks, respectively. We fine-tune our models using the AdamW (Loshchilov and Hutter, 2019) optimizer. We use batch size equal to 1 and a gradient accumulation step equal to 4. For LongT5, we use a constant learning rate of 1e-4, while for the Llama-3 model, we use a learning rate of 2e-5 with linear decay. For Low-Rank fine-tuning, we use a rank of 8 and an alpha of 16.\nFor evaluation, we use a publicly available Rouge-score implementation and for BERTScore we use DeBERTa-Xlarge (He et al., 2021). We calculate entity mention recall using a named-entity recognition module from Spacy10.\nWe present the prompt used for extracting facts, which is adapted from the VeriScore metric (Song et al., 2024) to suit our specific tasks in Table 11. The prompt used to verify whether a fact is supported is shown in Table 12. We provide the prompt for classifying facts in Table 13 and the instructions given to the human annotators in Table 14.\nFor our experiments, we use the following prompts:\nDescribe character: {\ncharacter_name} given the\nfollowing context. Context:\n[..]\nCharacter Description Prompt\nAnalyse in-depth character: {\ncharacter_name} given the\nfollowing context. Context:\n[..]\nCharacter Analysis Prompt\nThe following story has these\ncharacters: {list_of_character\n} describe character: {\ncharacter_name} given the\nfollowing context. Context:\n[..]\nCharacter Description Prompt with character names\nDescribe the following characters\n: {list_of_character} given\nthe following context and\nreturn your output as in the\nfollowing examples \\n\\n_{\nexample_1} \\n\\n {example_2} \\n\n\\n {example_3}. Context: [..]\nJoint Character Description Prompt"}, {"title": "C Additional Experiments", "content": "We ran a \"no-context\" experiment where only the initial prompt was provided, without including any part of the story. This is a way of testing how much of the story has been memorized by the pretrained LLMs we use for our work. This provides an indication of the degree of data contamination that is present.\nIn another experiment, we used a summary of the story as input instead of the book text. This is the setup that has been used by other work on character description generation, most notably by Brahman et al. (2021). We ran these experiments using the Llama-3-8B model in a zero-shot way.\nWe also evaluate the performance of the closed-source model, GPT-4o-mini, in two different ways, first processing the lead 128k tokens of the input story and second extracting context with the coreference-based approach."}]}