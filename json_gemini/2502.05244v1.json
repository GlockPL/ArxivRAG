{"title": "Probabilistic Artificial Intelligence", "authors": ["Andreas Krause", "Jonas H\u00fcbotter"], "abstract": "A key aspect of intelligence is to not only make predictions, but reason about the uncertainty in these predictions, and to consider this uncertainty when making decisions. This is what \u201cProbabilistic Artificial Intelligence\u201d is about. The first part covers probabilistic approaches to machine learning. We discuss the differentiation between \u201cepistemic\u201d uncertainty due to lack of data and \u201caleatoric\u201d uncertainty, which is irreducible and stems, e.g., from noisy observations and outcomes. We discuss concrete approaches towards probabilistic inference, such as Bayesian linear regression, Gaussian process models and Bayesian neural networks. Often, inference and making predictions with such models is intractable, and we discuss modern approaches to efficient approximate inference.\n The second part of the manuscript is about taking uncertainty into account in sequential decision tasks. We consider active learning and Bayesian optimization \u2014 approaches that collect data by proposing experiments that are informative for reducing the epistemic uncertainty. We then consider reinforcement learning, a rich formalism for modeling agents that learn to act in uncertain environments. After covering the basic formalism of Markov Decision Processes, we consider modern deep RL approaches that use neural network function approximation. We close by discussing modern approaches in model-based RL, which harness epistemic and aleatoric uncertainty to guide exploration, while also reasoning about safety.", "sections": [{"title": "Fundamentals of Inference", "content": "Boolean logic is the algebra of statements which are either true or false. Consider, for example, the statements\n\u201cIf it is raining, the ground is wet.\u201d and \u201cIt is raining.\u201d\nA quite remarkable property of Boolean logic is that we can combine these premises to draw logical inferences which are new (true) state- ments. In the above example, we can conclude that the ground must be wet. This is an example of logical reasoning which is commonly referred to as logical inference, and the study of artificial systems that are able to perform logical inference is known as symbolic artificial in- telligence.\nBut is it really raining? Perhaps it is hard to tell by looking out of the window. Or we have seen it rain earlier, but some time has passed since we have last looked out of the window. And is it really true that if it rains, the ground is wet? Perhaps the rain is just light enough that it is absorbed quickly, and therefore the ground still appears dry.\nThis goes to show that in our experience, the real world is rarely black and white. We are frequently (if not usually) uncertain about the truth of statements, and yet we are able to reason about the world and make predictions. We will see that the principles of Boolean logic can be ex- tended to reason in the face of uncertainty. The mathematical frame- work that allows us to do this is probability theory, which as we will find in this first chapter can be seen as a natural extension of Boolean logic from the domain of certainty to the domain of uncer- tainty. In fact, in the 20th century, Richard Cox and Edwin Thompson Jaynes have done early work to formalize probability theory as the \"logic under uncertainty\" (Cox, 1961; Jaynes, 2002).\nIn this first chapter, we will briefly recall the fundamentals of prob- ability theory, and we will see how probabilistic inference can be used to reason about the world. In the remaining chapters, we will then discuss how probabilistic inference can be performed efficiently given limited computational resources and limited time, which is the key challenge in probabilistic artificial intelligence."}, {"title": "Probability", "content": "Probability is commonly interpreted in two different ways. In the fre- quentist interpretation, one interprets the probability of an event (say a coin coming up \u201cheads\" when flipping it) as the limit of relative frequencies in repeated independent experiments. That is,\nProbability = lim \\# events happening in N trials N\u2192\u221e N\nThis interpretation is natural, but has a few issues. It is not very dif- ficult to conceive of settings where repeated experiments do not make sense. Consider the outcome:\n\u201cPerson X will live for at least 80 years.\u201d\nThere is no way in which we could conduct multiple independent ex- periments in this case. Still, this statement is going to turn out either true or false, as humans we are just not able to determine its truth value beforehand. Nevertheless, humans commonly have beliefs about statements of this kind. We also commonly reason about statements such as\n\u201cThe Beatles were more groundbreaking than The Monkees.\u201d\nThis statement does not even have an objective truth value, and yet we as humans tend to have opinions about it.\nWhile it is natural to consider the relative frequency of the outcome in repeated experiments as our belief, if we are not able to conduct repeated experiments, our notion of probability is simply a subjective measure of uncertainty about outcomes. In the early 20th century, Bruno De Finetti has done foundational work to formalize this notion which is commonly called Bayesian reasoning or the Bayesian interpre- tation of probability (De Finetti, 1970).\nWe will see that modern approaches to probabilistic inference often lend themselves to a Bayesian interpretation, even if such an interpre- tation is not strictly necessary. For our purposes, probabilities will be a means to an end: the end usually being solving some task. This task may be to make a prediction or to take an action with an uncertain outcome, and we can evaluate methods according to how well they perform on this task. No matter the interpretation, the mathematical"}, {"title": "Probability Spaces", "content": "A probability space is a mathematical model for a random experiment. The set of all possible outcomes of the experiment \u03a9 is called sample space. An event A \u2286 \u03a9 of interest may be any combination of possible outcomes. The set of all events A \u2286 P(\u03a9) that we are interested in is often called the event space of the experiment.\u00b9 This set of events is required to be a \u03c3-algebra over the sample space.\nDefinition 1.1 (\u03c3-algebra). Given the set \u03a9, the set A \u2286 P(\u03a9) is a \u03c3-algebra over \u03a9 if the following properties are satisfied:\n1. \u03a9 \u2208 A;\n2. if A \u2208 A, then A \u2208 A (closedness under complements); and\n3. if we have Ai \u2208 A for all i, then Ui\u221e1 Ai \u2208 A (closedness under countable unions).\nNote that the three properties of \u03c3-algebras correspond to character- istics we universally expect when working with random experiments. Namely, that we are able to reason about the event \u03a9 that any of the possible outcomes occur, that we are able to reason about an event not occurring, and that we are able to reason about events that are composed of multiple (smaller) events.\n Intuitively, the observer only understands the parity of the face of the die.\nDefinition 1.3 (Probability measure). Given the set \u03a9 and the \u03c3-algebra A over \u03a9, the function P : A \u2192 R is a probability measure on A if the Kolmogorov axioms are satisfied:\n1. 0 \u2264 P(A) \u2264 1 for any A \u2208 A;\n2. P(\u03a9) = 1; and"}, {"title": "Random Variables", "content": "The set \u03a9 is often rather complex. For example, take \u03a9 to be the set of all possible graphs on n vertices. Then the outcome of our experiment is a graph. Usually, we are not interested in a specific graph but rather a property such as the number of edges, which is shared by many graphs. A function that maps a graph to its number of edges is a random variable.\nDefinition 1.6 (Random variable). A random variable X is a function X : \u03a9 \u2192 T where T is called target space of the random variable, and where X respects the information available in the \u03c3-algebra A. That is,"}, {"title": "Distributions", "content": "Concrete values x of a random variable X are often referred to as states or realizations of X. The probability that X takes on a value in S \u2286 T is\nP(X \u2208 S) = P({\u03c9 \u2208 \u03a9 : X(\u03c9) \u2208 S}).\nConsider a random variable X on a probability space (\u03a9, A, P), where \u03a9 is a compact subset of R, and A the Borel \u03c3-algebra.\nIn this case, we can refer to the probability that X assumes a particular state or set of states by writing\npx(x) = P(X = x) (in the discrete setting),\nPx(x) = P(X \u2264 x).\nNote that \u201cX = x\u201d and \u201cX \u2264 x\u201d are merely events (that is, they char- acterize subsets of the sample space \u03a9 satisfying this condition) which are in the Borel \u03c3-algebra, and hence their probability is well-defined.\nHereby, px and PX are referred to as the probability mass function (PMF) and cumulative distribution function (CDF) of X, respectively. Note that we can also implicitly define probability spaces through ran- dom variables and their associated PMF/CDF, which is often very con- venient.\nWe list some common examples of discrete distributions in Appendix A.1.1. Further, note that for continuous variables, P(X = x) = 0. Here, in- stead we typically use the probability density function (PDF), to which we (with slight abuse of notation) also refer with px. We discuss den- sities in greater detail in Section 1.1.4.\nWe call the subset S \u2286 T of the domain of a PMF or PDF px such that all elements x \u2208 S have positive probability, px(x) > 0, the support of the distribution px. This quantity is denoted by X(\u03a9)."}, {"title": "Continuous Distributions", "content": "As mentioned, a continuous random variable can be characterized by its probability density function (PDF). But what is a density? We can derive some intuition from physics.\nLet M be a (non-homogeneous) physical object, e.g., a rock. We com- monly use m(M) and vol(M) to refer to its mass and volume, respec- tively. Now, consider for a point x \u2208 M and a ball Br(x) around x with radius r the following quantities:\nlim vol(Br(x)) = 0 lim m(Br(x)) = 0.\nr\u21920 r\u21920\nThey appear utterly uninteresting at first, yet, if we divide them, we get what is called the density of M at x.\nlim m(Br(x))\nr\u21920 vol(Br(x)) = p(x).\nWe know that the relationship between density and mass is described by the following formula:\nm(M) = \u222bM p(x)dx.\nIn other words, the density is to be integrated. For a small region I around x, we can approximate m(I) \u2248 p(x) \u00b7 vol(I).\nCrucially, observe that even though the mass of any particular point x is zero, i.e., m({x}) = 0, assigning a density p(x) to x is useful for integration and approximation. The same idea applies to continuous random variables, only that volume corresponds to intervals on the real line and mass to probability. Recall that probability density func- tions are normalized such that their probability mass across the entire real line integrates to one."}, {"title": "Joint Probability", "content": "A joint probability (as opposed to a marginal probability) is the prob- ability of two or more events occurring simultaneously:\nP(A,B) = P(A \u2229 B).\nIn terms of random variables, this concept extends to joint distribu- tions. Instead of characterizing a single random variable, a joint dis- tribution is a function px : Rn \u2192 R, characterizing a random vector X = [X1 ... Xn]T. For example, if the Xi are discrete, the joint distri- bution characterizes joint probabilities of the form\nP(X = [x1, ..., xn]) = P(X1 = x1, ..., Xn = xn),\nand hence describes the relationship among all variables Xi. For this reason, a joint distribution is also called a generative model. We use Xi:j to denote the random vector [Xi ... Xj]T.\nWe can \u201csum out\u201d (respectively \u201cintegrate out\u201d) variables from a joint distribution in a process called \u201cmarginalization\u201d:\nFact 1.8 (Sum rule). We have that\np(x1:i-1, xi+1:n) = \u222bXi(\u03a9) p(x1:n) dxi."}, {"title": "Conditional Probability", "content": "Conditional probability updates the probability of an event A given some new information, for example, after observing the event B.\nDefinition 1.9 (Conditional probability). Given two events A and B such that P(B) > 0, the probability of A conditioned on B is given as\nP(A | B) = P(A,B) P(B).\nSimply rearranging the terms yields,\nP(A,B) = P(A | B) \u00b7 P(B) = P(B | A) \u00b7 P(A).\nThus, the probability that both A and B occur can be calculated by multiplying the probability of event A and the probability of B condi- tional on A occurring.\nWe say Z \u223c X | Y = y (or simply Z \u223c X | y) if Z follows the conditional distribution\nPxY(xy) = Pxy(x,y) Py(y).\nIf X and Y are discrete, we have that pX|Y(x | y) = P(X = x | Y = y) as one would naturally expect.\nExtending Equation (1.9) to arbitrary random vectors yields the prod- uct rule (also called the chain rule of probability):"}, {"title": "Independence", "content": "Two random vectors X and Y are independent (denoted X \u22a5 Y) if and only if knowledge about the state of one random vector does not affect the distribution of the other random vector, namely if their conditional CDF (or in case they have a joint density, their conditional PDF) sim- plifies to\nPx|Y(x | y) = Px(x), px|Y(x | y) = px(x).\nFor the conditional probabilities to be well-defined, we need to assume that pY(y) > 0.\nThe more general characterization of independence is that X and Y are independent if and only if their joint CDF (or in case they have a joint density, their joint PDF) can be decomposed as follows:\nPx,Y(x,y) = Px(x) \u00b7 Py(y), px,Y(x,y) = px(x) \u00b7 py(y).\nThe equivalence of the two characterizations (when pY(y) > 0) is eas- ily proven using the product rule: px,Y(x,y) = pY(y) \u00b7 pX|Y(x | y).\nA \u201cweaker\u201d notion of independence is conditional independence. Two random vectors X and Y are conditionally independent given a ran- dom vector Z (denoted X \u22a5 Y | Z) iff, given Z, knowledge about the value of one random vector Y does not affect the distribution of the other random vector X, namely if\nPx|Y,Z(x | y, z) = Px|Z(x | z),\npx|Y,Z(x | y, z) = px|Z(x | z)."}, {"title": "Directed Graphical Models", "content": "Similarly to independence, we have that X and Y are conditionally independent given Z if and only if their joint CDF or joint PDF can be decomposed as follows:\nPx,Y|Z(x, y | z) = Px|Z(x | z) \u00b7 Py|Z(y | z),\npx,Y|Z(x,y | z) = px|Z(x | z) \u00b7 py|Z(y | z).\nDirected graphical models (also called Bayesian networks) are often used to visually denote the (conditional) independence relationships of a large number of random variables. They are a schematic repre- sentation of the factorization of the generative model into a product of conditional distributions as a directed acyclic graph. Given the se- quence of random variables {Xi}ni=1, their generative model can be expressed as\nn\u220f p(xi | parents(xi))i=1\nwhere parents(xi) is the set of parents of the vertex Xi in the directed graphical model. In other words, the parenthood relationship encodes a conditional independence of a random variable X with a random variable Y given their parents:\nXY | parents(X), parents(Y).\nEquation (1.17) simply uses the product rule and the conditional in- dependence relationships to factorize the generative model. This can greatly reduce the model's complexity, i.e., the length of the product."}, {"title": "Expectation", "content": "The expected value or mean E[X] of a random vector X is the (asymp- totic) arithmetic mean of an arbitrarily increasing number of indepen- dent realizations of X. That is,9\nE[X] = \u222bX(\u03a9) xp(x) dx\nA very special and often used property of expectations is their lin- earity, namely that for any random vectors X and Y in R\" and any A \u2208 Rm\u00d7n, b \u2208 Rm it holds that\nE[AX + b] = AE[X] + b and E[X + Y] = E[X] + E[Y].\nNote that X and Y do not necessarily have to be independent! Further, if X and Y are independent then\nE [XY] = E[X] \u00b7 E[Y]T.\nThe following intuitive lemma can be used to compute expectations of transformed random variables.\nFact 1.12 (Law of the unconscious statistician, LOTUS).\nE[g(X)] = \u222bX(\u03a9) g(x) \u00b7 p(x) dx\nwhere g : X(\u03a9) \u2192 Rn is a \u201cnice\u201d function10 and X is a continuous ran- dom vector. The analogous statement with a sum replacing the integral holds for discrete random variables.\nThis is a nontrivial fact that can be proven using the change of variables formula which we discuss in Section 1.1.11."}, {"title": "Covariance and Variance", "content": "Similarly to conditional probability, we can also define conditional ex- pectations. The expectation of a continuous random vector X given that Y = y is defined as\nE[X | Y = y] = \u222bX(\u03a9) x \u00b7 px|y(x | y) dx.\nObserve that E[X | Y = \u00b7] defines a deterministic mapping from y to E[X | Y = y]. Therefore, E[X | Y] is itself a random vector:\nE[X | Y](\u03c9) = E[X | Y = Y(\u03c9)]\nwhere \u03c9 \u2208 \u03a9. This random vector E[X | Y] is called the conditional expectation of X given Y.\nAnalogously to the law of total probability (1.12), one can condition an expectation on another random vector. This is known as the tower rule or the law of total expectation (LOTE):\nTheorem 1.13 (Tower rule). Given random vectors X and Y, we have\nEy[Ex[X | Y]] = E[X].\nGiven two random vectors X in Rn and Y in Rm, their covariance is defined as\nCov[X, Y] = E [(X \u2013 E[X])(Y \u2013 E[Y])]T\n= E [XY] \u2013 E[X] \u00b7 E[Y]T\n= Cov[Y, X]T \u2208 Rn\u00d7m.\nCovariance measures the linear dependence between two random vec- tors since a direct consequence of its definition (1.26) is that given lin- ear maps A \u2208 Rn'\u00d7n, B \u2208 Rm'\u00d7m, vectors c \u2208 R\"n', d \u2208 Rm' and random vectors X in Rn and Y in Rm, we have that\nCov[AX + c, BY + d] = A Cov[X, Y] BT.\nTwo random vectors X and Y are said to be uncorrelated if and only if Cov[X, Y] = 0. Note that if X and Y are independent, then Equa- tion (1.21) implies that X and Y are uncorrelated. The reverse does not hold in general.\""}, {"title": "Correlation", "content": "The correlation of the random vectors X and Y is a normalized covariance,\nCor[X, Y](i,j) = Cov[Xi, Yj] \u221aVar[Xi]Var[Yj] \u2208 [\u22121,1].\nTwo random vectors X and Y are therefore uncorrelated if and only if Cor[X, Y] = 0.\nThere is also a nice geometric interpretation of covariance and correlation. For zero mean random variables X and Y, Cov[X, Y] is an inner product.11\nThe cosine of the angle \u03b8 between X and Y (that are not determin- istic) coincides with their correlation,\ncos\u03b8 = Cov[X, Y] = Cor[X, Y].\n||X|| ||Y||\ncos \u03b8 is also called a cosine similarity. Thus,\n\u03b8 = arccos Cor[X, Y ].\nFor example, if X and Y are uncorrelated, then they are orthogonal in the inner product space. If Cor[X, Y] = \u22121 then \u03b8 = \u03c0 (that is, X and Y \u201cpoint in opposite directions\u201d), whereas if Cor[X, Y] = 1 then \u03b8 = 0 (that is, X and Y \u201cpoint in the same direction\u201d).\nThe covariance of a random vector X in Rn with itself is called its variance:\nVar[X] = Cov[X, X]\n= E [(X \u2013 E[X]) (X \u2013 E[X])]T\n= E [XX] \u2013 E[X] \u00b7 E[X]\n\n=\n[\nCov[X1, X1]\n:\nCov[Xn, X1]\nCov[X1, Xn]\n:\nCov[Xn, Xn]\n]\nThe scalar variance Var[X] of a random variable X is a measure of un- certainty about the value of X since it measures the average squared deviation from E[X]. We will see that the eigenvalue spectrum of a covariance matrix can serve as a measure of uncertainty in the multi- variate setting.12"}, {"title": "Standard deviation", "content": "The length of a random variable X in the inner product space described in Remark 1.14 is called its standard deviation,\n||\nX|| =\u221aCov[X,X] = \u221aVar[X] = \u03c3[X].\nThat is, the longer a random variable is in the inner product space, the more \u201cuncertain\u201d we are about its value. If a random variable has length 0, then it is deterministic.\nThe variance of a random vector X is also called the covariance matrix of X and denoted by \u03a3x (or \u03a3 if the correspondence to X is clear from context). A covariance matrix is symmetric by definition due to the symmetry of covariance, and is always positive semi-definite ?.\nTwo useful properties of variance are the following:\n\u2022 It follows from Equation (1.29) that for any linear map A \u2208 Rm\u00d7n and vector b \u2208 Rm,\nVar[AX + b] = A Var[X] AT .\nIn particular, Var[-X] = Var[X].\n\u2022 It follows from the definition of variance (1.34) that for any two random vectors X and Y,\nVar[X + Y] = Var[X] + Var[Y] + 2Cov[X, Y ].\nIn particular, if X and Y are independent then the covariance term vanishes and Var[X + Y] = Var[X] + Var[Y].\nAnalogously to conditional probability and conditional expectation, we can also define conditional variance. The conditional variance of a random vector X given another random vector Y is the random vector\nVar[X | Y] = E [(X \u2013 E[X | Y]) (X \u2013 E[X | Y]) | Y] .\nIntuitively, the conditional variance is the remaining variance when we use E[X | Y] to predict X rather than if we used E[X]. One can also condition a variance on another random vector, analogously to the laws of total probability (1.12) and expectation (1.25)."}, {"title": "Change of Variables", "content": "It is often useful to understand the distribution of a transformed ran- dom variable Y = g(X) that is defined in terms of a random variable X, whose distribution is known. Let us first consider the univariate setting. We would like to express the distribution of Y in terms of the distribution of X, that is, we would like to find\nPy(y) = P(Y \u2264 y) = P(g(X) \u2264 y) = P(X \u2264 g\u22121(y)).\nWhen the random variables are continuous, this probability can be ex- pressed as an integration over the domain of X. We can then use the substitution rule of integration to \u201cchange the variables\u201d to an inte- gration over the domain of Y. Taking the derivative yields the density py.13 There is an analogous change of variables formula for the multi- variate setting.\nFact 1.17 (Change of variables formula). Let X be a random vector in Rn with density px and let g : Rn \u2192 Rn be a differentiable and invertible function. Then Y = g(X) is another random variable, whose density can be computed based on px and g as follows:\npy(y) = px(g\u22121(y)) \u00b7 |det(Dg\u22121(y))|\nwhere Dg\u22121(y) is the Jacobian of g\u22121 evaluated at y.\nHere, the term |det(Dg\u22121(y))| measures how much a unit volume changes when applying g. Intuitively, the change of variables swaps the coordinate system over which we integrate. The factor |det(Dg\u22121(y))| corrects for the change in volume that is caused by this change in co- ordinates."}, {"title": "Probabilistic Inference", "content": "Recall the logical implication \u201cIf it is raining, the ground is wet.\u201d from the beginning of this chapter. Suppose that we look outside a window and see that it is not raining: will the ground be dry? Logical reason- ing does not permit drawing an inference of this kind, as there might be reasons other than rain for which the ground could be wet (e.g., sprinklers). However, intuitively, by observing that it is not raining, we have just excluded the possibility that the ground is wet because of rain, and therefore we would deem it \u201cmore likely\u201d that the ground is dry than before. In other words, if we were to walk outside now and the ground was wet, we would be more surprised than we would have been if we had not looked outside the window before.\nAs humans, we are constantly making such \u201cplausible\u201d inferences of our beliefs: be it about the weather, the outcomes of our daily deci- sions, or the behavior of others. Probabilistic inference is the process of updating such a prior belief P(W) to a posterior belief P(W | R) upon observing R where to reduce clutter we write W for \u201cThe ground is wet\u201d and R for \u201cIt is raining\u201d.\nThe central principle of probabilistic inference is Bayes\u2019 rule:\nTheorem 1.18 (Bayes' rule). Given random vectors X in Rn and Y in Rm, we have for any x \u2208 Rn, y \u2208 Rm that\np(x | y) = p(y | x) \u00b7 p(x) p(y)\nProof. Bayes\u2019 rule is a direct consequence of the definition of condi- tional densities (1.10) and the product rule (1.11).\nLet us consider the meaning of each term separately:\n\u2022 the prior p(x) is the initial belief about x,\n\u2022 the (conditional) likelihood p(y | x) describes how likely the observa- tions y are under a given value x,"}, {"title": "Where do priors come from?", "content": "Bayes\u2019 rule necessitates the specification of a prior p(x). Different pri- ors can lead to the deduction of dramatically different posteriors, as one can easily see by considering the extreme cases of a prior that is a point density at x = x0 and a prior that is \u201cuniform\u201d over Rn.15 In the former case, the posterior will be a point density at x0 regardless of the likelihood. In other words, no evidence can alter the \u201cprior belief\u201d the learner ascribed to x. In the latter case, the learner has \u201cno prior belief\u201d, and therefore the posterior will be proportional to the likeli- hood. Both steps of probabilistic inference are perfectly valid, though one might debate which prior is more reasonable.\nSomeone who follows the Bayesian interpretation of probability might argue that everything is conditional, meaning that the prior is simply a posterior of all former observations. While this might seem natural (\u201cmy world view from today is the combination of my world view from yesterday and the observations I made today\u201d), this lacks an explanation for \u201cthe first day\u201d. Someone else who is more inclined towards the frequentist interpretation might also object to the exis- tence of a prior belief altogether, arguing that a prior is subjective and therefore not a valid or desirable input to a learning algorithm. Put differently, a frequentist \u201chas the belief not to have any belief\u201d. This is perfectly compatible with probabilistic inference, as long as the prior is chosen to be noninformative:\np(x) \u221d const.\nChoosing a noninformative prior in the absence of any evidence is known as the principle of indifference or the principle of insufficient reason, which dates back to the famous mathematician Pierre-Simon Laplace."}, {"title": "Conjugate Priors", "content": "If the prior p(x) and posterior p(x | y) are of the same family of distri- butions, the prior is called a conjugate prior to the likelihood p(y | x). This is a very desirable property, as it allows us to recursively ap- ply the same learning algorithm implementing probabilistic inference. We will see in Chapter 2 that under some conditions the Gaussian is self-conjugate. That is, if we have a Gaussian prior and a Gaussian like- lihood then our posterior will also be Gaussian. This will provide us with the first efficient implementation of probabilistic inference."}, {"title": "Tractable Inference with the Normal Distribution", "content": "Using arbitrary distributions for learning and inference is computa- tionally very expensive when the number of dimensions is large even in the discrete setting. For example, computing marginal distri- butions using the sum rule yields an exponentially long sum in the"}, {"title": "Joint Probability", "content": "A joint probability (as opposed to a marginal probability) is the prob- ability of two or more events occurring simultaneously:\nP(A, B) = P(A \u2229 B).\nIn terms of random variables, this concept extends to joint distribu- tions. Instead of characterizing a single random variable, a joint dis- tribution is a function px : Rn \u2192 R, characterizing a random vector X = [X1 ... Xn]T. For example, if the Xi are discrete, the joint distri- bution characterizes joint probabilities of the form\nP(X = [x1, ..., xn]) = P(X1 = x1, ..., Xn = xn),\nand hence describes the relationship among all variables Xi. For this reason, a joint distribution is also called a generative model. We use Xi:j to denote the random vector [Xi ... Xj]T.\nWe can \u201csum out\u201d (respectively \u201cintegrate out\u201d) variables from a joint distribution in a process called \u201cmarginalization\u201d:\nFact 1.8 (Sum rule). We have that\np(x1:i\u22121, xi+1:n) = \u222bXi(\u03a9) p(x1:n) dxi."}, {"title": "Conditional Probability", "content": "Conditional probability updates the probability of an event A given some new information, for example, after observing the event B.\nDefinition 1.9 (Conditional probability). Given two events A and B such that P(B) > 0, the probability of A conditioned on B is given as\nP(A | B) = P(A,B) P(B)\nSimply rearranging the terms yields,\nP(A, B) = P(A | B) \u00b7 P(B) = P(B | A) \u00b7 P(A).\nThus, the probability that both A and B occur can be calculated by multiplying the probability of event A and the probability of B condi- tional on A occurring.\nWe say Z \u223c X | Y = y (or simply Z \u223c X | y) if Z follows the conditional distribution\nPX|Y(x|y) = PX,Y(x,y) pY(y)\nIf X and Y are discrete, we have that pX|Y(x | y) = P(X = x | Y = y) as one would naturally expect.\nExtending Equation (1.9) to arbitrary random vectors yields the prod- uct rule (also called the chain rule of probability):"}, {"title": "Independence", "content": "Two random vectors X and Y are independent (denoted X \u22a5 Y) if and only if knowledge about the state of one random vector does not affect the distribution of the other random vector, namely if their conditional CDF (or in case they have a joint density, their conditional PDF) sim- plifies to\nPX|Y(x | y) = Px(x), pX|Y(x | y) = px(x).\nFor the conditional probabilities to be well-defined, we need to assume that pY(y) > 0.\nThe more general characterization of independence is that X and Y are independent if and only if their joint CDF (or in case they have a joint density, their joint PDF) can be decomposed as follows:\nPx,Y(x, y) = Px(x) \u00b7 Py(y), px,Y(x, y) = px(x) \u00b7 py(y).\nThe equivalence of the two characterizations (when pY(y) > 0) is eas- ily proven using the product rule: px,Y(x, y) = pY(y) \u00b7 pX|Y(x | y).\nA \u201cweaker\u201d notion of independence is conditional independence. Two random vectors X and Y are conditionally independent given a ran- dom vector Z (denoted X \u22a5 Y | Z) iff, given Z, knowledge about the value of one random vector Y does not affect the distribution of the other random vector X, namely if\nPx|Y,Z(x | y, z) = Px|Z(x | z),\npx|Y,Z(x | y, z) = px|Z(x | z)."}, {"title": "Directed Graphical Models", "content": "Similarly to independence, we have that X and Y are conditionally independent given Z if and only if their joint CDF or joint PDF can be decomposed as follows:\nPx,Y|Z(x, y | z) = Px|Z(x | z) \u00b7 Py|Z(y | z),\npx,Y|Z(x, y | z) = px|Z(x | z) \u00b7 py|Z(y | z).\nDirected graphical models (also called Bayesian networks) are often used to visually denote the (conditional) independence relationships of a large number of random variables. They are a schematic repre- sentation of the factorization of the generative model into a product of conditional distributions as a directed acyclic graph. Given the se- quence of random variables {Xi}ni=1, their generative model can be expressed as\nn\u220f p(xi | parents(xi))\ni=1\nwhere parents(xi) is the set of parents of the vertex Xi in the directed graphical model. In other words, the parenthood relationship encodes a conditional independence of a random variable X with a random variable Y given their parents:\nXY | parents(X), parents(Y ).\nEquation (1.17) simply uses the product rule and the conditional in- dependence relationships to factorize the generative model. This can greatly reduce the model\u2019s complexity, i.e., the length of the product."}, {"title": "Expectation", "content": "The expected value or mean E[X"}]}