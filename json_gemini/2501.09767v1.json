{"title": "LEMO: Enabling LEss Token Involvement for MOre Context Fine-tuning", "authors": ["Tuowei Wang", "Xingyu Chen", "Kun Li", "Ting Cao", "Ju Ren", "Yaoxue Zhang"], "abstract": "The escalating demand for long-context applications has intensified the necessity of extending the LLM context windows. Despite recent fine-tuning approaches successfully expanding context lengths, their high memory footprints, especially for activations, present a critical practical limitation. Current parameter-efficient fine-tuning methods prioritize reducing parameter update overhead over addressing activation memory constraints. Similarly, existing sparsity mechanisms improve computational efficiency but overlook activation memory optimization due to the phenomenon of Shadowy Activation.\nIn this paper, we propose LEMO, the first LLM fine-tuning system that explores and exploits a new token-level sparsity mechanism inherent in long-context scenarios, termed Contextual Token Sparsity. LEMO minimizes redundant token involvement by assessing the informativeness of token embeddings while preserving model accuracy. Specifically, LEMO introduces three key techniques: (1) Token Elimination, dynamically identifying and excluding redundant tokens across varying inputs and layers. (2) Pattern Prediction, utilizing well-trained predictors to approximate token sparsity patterns with minimal overhead. (3) Kernel Optimization, employing permutation-free and segment-based strategies to boost system performance. We implement LEMO as an end-to-end fine-tuning system compatible with various LLM architectures and other optimization techniques. Comprehensive evaluations demonstrate that LEMO reduces memory consumption by up to 1.93x and achieves up to 1.36\u00d7 speedups, outperforming state-of-the-art fine-tuning systems.", "sections": [{"title": "1 Introduction", "content": "As the demand for comprehensive document analysis [13], extended multi-turn dialogues [71], and intricate codebase handling [50] grows, large language models (LLMs) with larger context windows are becoming integral to AI applications. Despite their utility, LLMs [1, 30, 51] are typically pre-trained with fixed context windows, such as the 4K token limit in Llama2 [63]. When these models encounter inputs exceeding this limit, their performance deteriorates markedly [18]. The discrepancy between the fixed context window during pre-training and the increasingly extended inputs during inference has emerged as a critical challenge in real-world deployments.\nRecent studies [42, 52, 64, 68] show that the context window of pre-trained LLMs can be extended through fine-tuning on longer sequences. Unfortunately, managing these extended sequences imposes substantial resource challenges, particularly in terms of memory consumption. For instance, Position Interpolation [10] extends Llama models [62] from 2K to 32K but requires 128 A100 80GB GPUs, mainly due to memory limitations. Rather than model parameters, the primary memory bottleneck in long-context fine-tuning arises from activations [11, 34, 74], which include intermediate results and gradients that scale proportionally with sequence length.\nAlthough various techniques have been proposed to improve the efficiency of fine-tuning, the substantial demands of activation memory remain largely unaddressed. By adapting only a minimal number of parameters, parameter-efficient fine-tuning (PEFT) methods [27, 28, 38, 73] reduce the memory requirement of parameter updates but leave activation memory unoptimized. Furthermore, recent works [12, 24, 49, 65] incorporate diverse sparse mechanisms to approximate standard dense attention. Despite achieving considerable computational savings, these methods fail to offer additional memory reduction, as highlighted in Table 1."}, {"title": "2 Background and Motivation", "content": ""}, {"title": "2.1 Efficient LLM Fine-tuning", "content": "Fine-tuning, the process of adapting pre-trained LLMs to diverse downstream applications, is pivotal for their effective deployment. Particularly, fine-tuning a pre-trained LLM on longer text sequences is essential for extending its pre-defined context window size, enabling support for long-context scenarios. However, this process is typically resource-intensive, with the inclusion of long text sequences considerably amplifying both computational and memory requirements.\nTo improve fine-tuning efficiency, one promising direction is parameter-efficient fine-tuning (PEFT) methods [27, 28, 38, 73], which adapt pre-trained models by updating only a subset of parameters while maintaining model performance. A representative example is low-rank adaption (LoRA) [28], which freezes the pre-trained model weights and injects smaller, trainable low-rank matrices into each transformer block. By reducing the number of trainable parameters, LoRA markedly alleviates the memory demands of optimizer states, driving its widespread adoption in practical applications.\nAnother research direction [6, 24, 72, 78] focuses on exploiting the inherent sparsity [17] within attention mechanisms by employing various sparsity patterns to the standard dense attention. The central insight is that only a limited subset of interactions is critical, allowing the remainder to be safely disregarded with minimal impact on accuracy. More recent works [12, 65] integrate these two lines of research. Notably, LongLoRA [12] extends LoRA by incorporating a new shifted sparsity pattern, enabling efficient scaling of context length. By substituting global dense attention with two groups of shifted sparse local attention, LongLoRA achieves less training time compared to LoRA. However, subsequent analysis reveals that neither LoRA nor LongLoRA adequately addresses the memory bottleneck in long-context fine-tuning, as activations scale proportionally with sequence length."}, {"title": "2.2 Analysis: Fine-tuning Memory Breakdown", "content": "Here we provide a detailed analysis of memory consumption during LLM fine-tuning process. Beginning with vanilla fine-tuning, we extend our discussion to include two representative approaches: LoRA, a PEFT method, and LongLoRA, a sparsity-based technique. Our analysis demonstrates that current efficient fine-tuning techniques fall short of addressing memory limitations, particularly in long-context scenarios.\nVanilla. The memory consumption of vanilla fine-tuning primarily consists of two parts: model states and residual states, as listed in Figure 2. The first part, model states, includes parameters, gradients, and optimizer states. In mixed-precision fine-tuning [46], parameters and gradients are stored in FP16, while optimizer states are stored in FP32. For modern optimizers like AdamW [45], the optimizer states include the parameters, momentum, and variance. Given model parameter size as \u03c8, the memory required for all model states is approximately 16\u03c8, which remains constant for a fixed model.\nThe second part of memory consumption comprises activations, temporary buffers, and fragmented memory. Among these, activations, which include the intermediate results and gradients stored during the forward pass, consume a considerable portion of memory. The memory required for activations is not only dependent on the model parameter size (\u03c8) but also scales with the input batch size (b) and sequence length (s). As detailed in Table 2, in long-context scenarios, the memory consumption of activations can easily exceed that of model states, emerging as the dominant memory bottleneck.\nLoRA. Different from the vanilla, the pre-trained model parameters remain frozen in LoRA, and only the injected low-rank matrices are updated during fine-tuning. Consequently, the memory consumption for gradients and optimizer states is significantly reduced. However, this reduction does not extend to activation memory, which emerges as the new memory"}, {"title": "3 Insight: Contextual Token Sparsity", "content": "To address the emerging challenges discussed in \u00a7 2.2, we introduce a fresh perspective that explores and exploits a token-level sparsity mechanism during LLM long-context fine-tuning. Our approach is grounded in the intuitive yet profound observation that natural language is inherently redundant [58, 66]. Specifically, as highlighted in several studies [37, 41, 59, 61, 67, 77], standard full attention can be effectively approximated by focusing on significant interactions among a limited set of query, key, and value values, involving only a subset of tokens in the sequence. Notably, this redundancy becomes even more pronounced in long-context scenarios. As presented in Table 3, the proportion of significant interactions in attention scores decreases as sequence length increases. These insights open up a compelling opportunity to optimize LLM long-context fine-tuning by identifying and retaining only the most informative tokens. Through directly reducing token involvement, shadowy activation constraints are naturally alleviated, enabling activation memory to achieve comparable benefits to computational savings.\nWe term this novel sparsity mechanism within LLM long-context fine-tuning as Contextual Token Sparsity. Through comprehensive evaluations, we identify two key unique characteristics of this mechanism: (1) Token-wise. As depicted in Figure 4, a grid-like distribution of attention scores is observed across different models and datasets, confirming that token embeddings in the sequence exhibit varying levels of importance. This insight enables the exclusion of less valuable tokens, naturally leading to reductions in both memory usage and computational overhead. (2) Contextual. Figure 4 further demonstrates that the distribution of valuable tokens dynamically shifts based on input texts and varies across model layers, even for a given model and dataset. This dynamic nature underscores the need for a system capable of accurately identifying and efficiently exploiting this sparsity in real-time during runtime. Serving as the cornerstone of our system, contextual token sparsity first introduces token-level sparsity into long-context fine-tuning, enabling LLMs to handle larger context windows while effectively involving fewer tokens."}, {"title": "4 System Design", "content": ""}, {"title": "4.1 Overview of LEMO", "content": "We propose LEMO, an efficient system designed to enhance LLM long-context fine-tuning by systematically exploring and exploiting contextual token sparsity. Figure 5 presents an overview of LEMO, which is built upon three key techniques:\nInformation-driven Token Elimination (\u00a7 4.2). To determine whether a token is redundant, we first establish a formal definition for the informativeness of a given token. Building on this definition, LEMO utilizes a score-based algorithm that dynamically identifies and eliminates redundant tokens within the attention block. The algorithm performs in a block-wise manner and is further refined by adopting a layer-specific threshold, which ensures both effectiveness and efficiency. Additionally, we extend this approach to the MLP block, ensuring consistency across various components of the model.\nContext-aware Pattern Prediction (\u00a7 4.3). LEMO employs a neural-network-based approach to predict the token sparsity patterns, bypassing the need for costly full attention score computation. Once adequately trained, these predictors can accurately approximate the token informativeness based on contextual inputs. To minimize the overhead introduced by predictors, LEMO utilizes an elastic size transformation technique, optimizing both memory and computational efficiency.\nHigh-performance Kernel Optimization (\u00a7 4.4). LEMO delves into kernel-level optimizations to maximize system performance. To minimize unnecessary global memory movement, LEMO introduces a permutation-free strategy that fuses token selection, token padding, and residual addition directly into the computation pipeline. Moreover, LEMO incorporates a segment-based method to alleviate activation memory peaks, enabling efficient gradient computation for long sequences. By tightly coupling these kernel-level optimizations with algorithmic designs, LEMO fully exploits contextual token sparsity for enhancing LLM long-context fine-tuning."}, {"title": "4.2 Information-driven Token Elimination", "content": "To fully exploit contextual token sparsity, it is essential to accurately identify the redundant tokens across different inputs and layers. Discarding informative tokens may impact model accuracy while retaining excessive tokens leads to resource inefficiency. To tackle these challenges, LEMO proposes an information-driven algorithm that dynamically identifies and eliminates redundant tokens while preserving accuracy.\nToken Informativeness. We begin by defining a token's informativeness based on its interactions with other tokens within the embedding space. In the attention mechanism, the attention score $S_{attn}$ is commonly used to quantify the interaction between tokens [25, 41, 67]. Specifically, the attention score term $S_{ij}$, calculated as $Q_iK_j$, represents the interaction between token i and token j. Inspired by this, we define the informativeness of a token $I(T_j)$ by considering its interaction with all other tokens in the long-context sequence:\n$I(T_j) = \\sum_{i \\neq j} S_{ij} = \\sum_{i \\neq j} Q_iK_j$\nwhere the sum aggregates the attention scores across all tokens i in the sequence, excluding the token j itself.\nBlock-wise Elimination. Building on the concept of token informativeness, the next step involves eliminating redundant"}, {"title": "4.3 Context-aware Pattern Prediction", "content": "Although the exact sparsity patterns can be directly derived from the full attention scores, computing and storing these scores is prohibitively expensive, with complexity scaling quadratically with the sequence length. Furthermore, due to the dynamic nature of contextual token sparsity, the optimal sparsity patterns can only be determined at runtime, varying across different inputs and layers. To address these challenges, LEMO employs a set of lightweight neural networks as predictors. By taking contextual embeddings as inputs, these predictors infer sparsity patterns accurately and efficiently.\nNeural-network-based Predictor. As illustrated in Figure 8, LEMO deploys a pair of predictors in each layer to approximate the informativeness scores of queries Q and keys K, respectively. Each predictor consists of three trainable low-rank matrices, with ReLU activation function applied between successive matrices. The inputs to the predictors are token embeddings X, which contain contextual information and are organized into blocks to align with the block-wise elimination. By extracting the representative embedding from each block, the predictors output the approximate informativeness scores, $\\hat{I}(Q)$ and $\\hat{I}(K)$. These scores are then multiplied to approximate the informativeness of attention scores $\\hat{I}(S_{attn})$:\n$\\hat{I}(S_{attn}) = \\hat{I}(Q)\\hat{I}(K)^T$, $\\hat{I}(B_{mn}) = \\hat{I}(B^Q_m)\\hat{I}(B^K_n)^T$\nWhen Q and K predictors are well trained, $\\hat{I}(S_{attn})$ can provide a close estimation of accurate informativeness scores $I(S_{attn})$."}, {"title": "4.4 High-performance Kernel Optimization", "content": "Focusing on token-level sparsity, LEMO introduces minimal modifications to the original fine-tuning dynamics, enabling seamless reuse of existing optimized computational flows. However, two key challenges hidden in LEMO undermine its performance. First, the variability in sparsity patterns across layers necessitates iterative token selection and padding, leading to considerable costly global memory movement. Second, the extensive vocabulary size of LLMs necessitates substantial activation memory to compute the output loss gradient for each token, particularly in long-context scenarios. LEMO incorporates several hardware-efficient techniques at the kernel level, effectively mitigating these bottlenecks.\nPermutation-free Token Movement. The dynamic nature of contextual token sparsity leads to varying sparsity patterns"}, {"title": "5 Implementation and Extension", "content": "We implement LEMO with over 3000 lines of Python and C++ code. Given the minimal changes to the original fine-tuning dynamics, LEMO is compatible with a wide range of LLM architectures without requiring any code changes. Besides, LEMO can seamlessly integrate with other techniques:\nExtension 1: Two-dimensional Sparsity. As illustrated in Figure 11(a), after applying token-level sparsity in LEMO, the remained tokens can further benefit from existing hidden-dimensional sparsity techniques. This natural combination of sparsity mechanisms across two dimensions, which we term 2D-Sparsity, provides more granular control over the model's resource allocation, leading to a significant reduction in both activation memory and computational costs.\nExtension 2: Sparsity-sensitive Offload. LEMO enhances existing offload-based techniques [26, 29, 53, 54] by incorporating contextual token sparsity into the optimization process. As depicted in Figure 11(b), we develop a sparsity-sensitive offloading strategy that adapts to the varying sparsity ratios across different layers. This approach enables the seamless transfer of larger data volumes between the CPU and GPU, effectively alleviating GPU memory constraints."}, {"title": "6 Evaluation", "content": ""}, {"title": "6.1 Experimental Setup", "content": "Hardware. We conduct experiments on three representative platforms, as listed in Table 4, covering both data-center workstations and desktop professional GPUs. Memory measurements are primarily evaluated on Platform A, as it offers the largest GPU memory capacity, with results being largely insensitive to GPU arithmetic performance. For speedup evaluations, we adhere to common practices by employing mixed-precision techniques [46], utilizing both BF16 and FP32.\nModels. The models used for evaluation are detailed in Table 5. We choose models from two of the most popular LLM families: OPT and Llama. These models vary in architecture, parameter size, and default context window size. Evaluations"}, {"title": "6.2 End-to-End Performance", "content": "Memory Footprint. We evaluate the memory efficiency of LEMO across various models and sequence lengths, as shown in Figure 12. The results reveal that LEMO achieves average memory savings of 38.2% and 50.5% compared to LoRA across six different models, with sequence lengths of 4K and 8K, respectively. Similar benefits are observed in comparison to LongLoRA, as the presence of shadowy activations renders its sparsity mechanism ineffective in benefiting memory usage (even slightly increased). Furthermore, the results reveal that for a fixed model, LEMO's memory efficiency improves\nas sequence length increases. This aligns with the observation that longer text sequences typically exhibit greater redundancy. The enhanced efficiency of LEMO largely extends the fine-tuning sequence length achievable under GPU memory constraints. Without activation recomputation and offloading, both LoRA and LongLoRA are limited to a sequence length of 16K (32K) when fine-tuning OPT 1.3B (350M). Instead, LEMO doubles this capacity, supporting sequence lengths of up to 32K (64K) on a single A800 GPU.\nExecution Time. The minimized token involvement in LEMO also brings computational savings during long-context fine-tuning. Figure 13 presents the execution time and corresponding speedups of LEMO during fine-tuning different models at a sequence length of 4K. The results show that LEMO achieves computational efficiency comparable to LongLoRA, achieving an average speedup over LoRA of 10.8% and 8.6% on two platforms, respectively. We also observe that LongLoRA may perform slower than LoRA in some cases. This is primarily due to the difficulty in fully utilizing hardware computational capacity for sparsity operations when the sequence length is insufficient. In contrast, LEMO introduces minimal modifications to the original computational flow, allowing it to effectively translate computational savings into practical speedups. Further evaluations of LEMO on longer sequence lengths (with recomputation) reveal additional performance gains, achieving up to 1.36\u00d7 speedups.\nAccuracy Evaluation. We test the impact of LEMO on model accuracy by comparing it with the original LoRA. First, we measure test perplexity of fine-tuned Llama2 7B on two representative long-context datasets, PG19 and Proof-Pile. As shown in Table 7, LEMO incurs only a minimal increase in"}, {"title": "6.3 Ablation Study", "content": "Fine-grained Performance Breakdown Figure 14 presents a detailed performance breakdown of LEMO, covering both memory and computational aspects. For the memory aspect, the results show that LEMO effectively reduces activation memory consumption compared to both LoRA and Lon-"}, {"title": "6.4 Extension Evaluation", "content": "Extension 1: Two-dimensional Sparsity. Building upon LEMO, we explore applying existing hidden-dimension-level sparsity techniques [65] to the remaining tokens during attention computation. Figure 19(a) shows that this two-dimensional sparsity further improves the computational efficiency of LEMO, achieving up to 2.04\u00d7 speedups on Llama2."}, {"title": "6.5 Scalability Analysis", "content": "We conclude by analyzing the strong scalability of LEMO on 4\u00d74090 GPUs. Figure 20 shows that LEMO's performance scales proportionally with GPU number across different models and sequence lengths. The scalability is achieved because LEMO seamlessly minimizes token involvements and introduces no extra communication overhead. These results highlight LEMO's potential for deployment in large-scale systems."}, {"title": "7 Related Work", "content": "Optimizations for Activation Memory. As a primary memory bottleneck in LLM training or fine-tuning, activation memory consumption has been the focus of extensive research, which can be categorized into three main approaches: The first is activation recomputation [11, 34, 35], designed to avoid storing activations during the forward pass but recomputing them during the backward pass. The second is activation offloading [26, 29, 53, 54], which asynchronously transfers ac-\ntivations from GPU to CPU and prefetches them back before required. The last is activation compression [9, 21, 43], which reduces the activation memory size through quantization or pruning. However, these methods primarily trade memory for additional computation or communication, rather than fundamentally reducing memory demands. In contrast, LEMO directly minimizes activation memory requirements and can be seamlessly combined with all these optimizations.\nOptimizations for Long-context Fine-tuning. To effectively extend the context window to longer sequences, some methods [10, 18, 42] focus on optimizing the fine-tuning algorithm design. Besides, some methods [57, 64, 69] explore strategies for modifying the position embeddings of LLMs to handle longer context. All these efforts are complementary to LEMO.\nBeyond effectiveness, some recent methods [75, 79] are proposed to mitigate the substantial fine-tuning overheads for efficiency. Particularly, Parameter-efficient fine-tuning methods [27, 28, 38, 73] first offer an effective solution by reducing the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. Furthermore, a series of sparsity-based methods [12, 65] are proposed to further reduce the computation costs by exploiting the inherent sparsity within attention mechanism [17]. However, while PEFT methods greatly cut down the memory consumption of optimizer states, the activation memory emerges as the primary bottleneck. Although existing sparsity mechanisms deliver notable computational gains, the presence of shadowy activation prevents comparable benefits for activation memory. Instead, LEMO achieves the best of both worlds by identifying and exploiting contextual token sparsity.\nOptimizations for Token Utilization. Sharing the same high-level idea of LEMO, several studies also explore leveraging the inherent redundancy that existed in natural language, including data engineering [22, 23], prompt compression [31, 39], and inference optimization [37, 59, 61, 77]. Notably, some works [7, 25, 32, 33, 48, 56, 70] propose eliminating tokens during inference to reduce model latency. However, these methods are designed for smaller models and all focus solely on model inference. Instead, LEMO is the first to optimize LLM long-context fine-tuning to our best knowledge."}, {"title": "8 Conclusion", "content": "We propose LEMO, an efficient system designed to optimize long-context fine-tuning for LLMs. Our approach introduces a novel sparsity mechanism within LLM long-context fine-tuning, termed contextual token sparsity. To systematically exploit this mechanism, we develop three key techniques that identify, predict, and exploit this sparsity, achieving both memory savings and performance speedups over state-of-the-art methods. Compression embodies intelligence, with sparsity serving as a potent form of compression. We envision LEMO inspiring broader exploration of sparsity for advancing LLMs."}]}