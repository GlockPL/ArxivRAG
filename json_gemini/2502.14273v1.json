{"title": "LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework", "authors": ["Zongyou Yu", "Qiang Qu", "Qian Zhang", "Nan Zhang", "Xiaoming Chen"], "abstract": "Recent advancements in event-based recognition have demonstrated significant promise, yet most existing approaches rely on extensive training, limiting their adaptability for efficient processing of event-driven visual content. Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot capabilities across diverse domains, but their application to event-based visual recognition remains largely unexplored. To bridge this gap, we propose LLM-EvGen, an event representation generator that produces LLM-compatible event representations LLM-EvRep, thereby enhancing the performance of LLMs on event recognition tasks. The generator is trained using a self-supervised framework, aligning the generated representations with semantic consistency and structural fidelity. Comprehensive experiments were conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results demonstrate that our method, LLM-EvRep, outperforms the event-to-video method, E2VID, by 15.93%, 0.82%, and 50.21%, respectively, in recognition tasks when evaluated using GPT-40.", "sections": [{"title": "1 Introduction", "content": "Event cameras, as bio-inspired sensors [6], have garnered significant attention in the computer vision community for their exceptional capabilities, including microsecond-level temporal resolution, high dynamic range (typically 140 dB compared to 60 dB of standard cameras), and low power consumption [9, 27]. These advantages represent a paradigm shift from traditional frame-based imaging [6, 18], offering a novel approach to capturing visual information. Despite being in the early stages of development, event-driven visual perception has rapidly emerged as a critical area of research in contemporary computer vision [34]. Event-based vision has shown promising results across various applications, including object recognition [35, 37], semantic segmentation [15, 18], detection [10, 11], tracking [7, 8], and optical flow estimation [12, 19].\nDespite these advantages, leveraging event cameras for high-level visual tasks, such as object recognition, remains challenging. Existing approaches to event-based object recognition can be broadly categorized into two types: traditional neural network-based methods [28, 35] and CLIP-based zero-shot methods [31, 36, 37]. Traditional neural network methods require extensive training, and due to the inherent limitations of neural networks, they are constrained to recognizing a limited set of categories [10]. To address these limitations, zero-shot methods [31, 36, 37] have been proposed. While these approaches show promise in zero-shot open-world event-based object recognition, their reliance on CLIP introduces inherent limitations [32]. In contrast, large language models (LLMs) offer a compelling alternative, with richer pre-trained knowledge and superior zero-shot reasoning capabilities.\nLLMs, with their exceptional zero-shot reasoning capabilities and vast pre-trained knowledge, have demonstrated remarkable success in multimodal tasks such as vision-language understanding and scene reasoning [5, 38]. Their ability to understand and generalize across diverse data modalities makes them a promising alternative to existing methods like CLIP for object recognition tasks. While LLMs have shown exceptional performance on 2D image-based content understanding, event-based visual content poses unique challenges due to its sparse, asynchronous, and modality-specific nature. These stark differences between event streams and traditional image-based inputs hinder LLMs' ability to directly comprehend event data.\nA critical challenge in applying LLMs to event-based vision is converting event streams into representations compatible with LLMs. Existing approaches to bridging this gap typically reconstruct event streams into more interpretable formats, falling into two main categories: event frame generation and event-to-video (E2V) techniques. The first category involves integrating events based on their spatial positions to generate \"event frames.\u201d The second category uses E2V methods, like E2VID [25] and E2HQV [24], to reconstruct events into natural images, known as \"reconstructed frames.\" While these methods show promise in LLM-based zero-shot object recognition [32], they do not specifically tailor event data to the unique characteristics of LLMs.\nTo address the limitations of existing methods and adapt event streams to LLMs, we propose LLM-EvGen, an event representation generator designed for producing LLM-compatible event representations. Inspired by E2HQV [24], LLM-EvGen employs an encoder-decoder structure based on the MBConv and Fused MBConv layers from EfficientNetV2 [29], balancing computational efficiency and parameter utilization. The encoder-decoder structure provides a solid foundation for processing event data, but ensuring compatibility with LLMs requires careful supervision.As illustrated in Figure 1, we introduce a self-supervised learning framework that aligns event representations produced by LLM-EvGen with RGB frames, ensuring both semantic consistency and structural fidelity. Specifically, the event representations and their corresponding RGB frames are input into an LLM to extract semantic information, with consistency measured using Jaccard similarity [14] as the Semantic Consistency Loss. To address early-stage noise in training, we propose a Structural Fidelity Loss, calculated as the mean squared error (MSE) between the Sobel edge maps [26] of LLM-EvRep and its RGB frames, enforcing structural consistency. LLM-EvGen requires training with only a single LLM and significantly improves event-based zero-shot object recognition performance. Extensive experiments validate the superiority of our approach in generating high-quality, LLM-compatible event representations and advancing event-based recognition tasks.\nTo sum up, our key contributions are as follows:\n\u2022 To the best of our knowledge, we are the first to explore how to enhance large LLMs' understanding of event-based visual content, bridging the gap between event-based data and LLMs.\n\u2022 We propose LLM-EvGen, a novel event representation generator designed to produce LLM-compatible event representations, termed LLM-EvRep, which significantly improve LLM performance on event recognition tasks.\n\u2022 We introduce a self-supervised learning framework that combines LLM-driven Semantic Consistency Loss and an auxiliary Structural Fidelity Loss to effectively train LLM-EvGen, ensuring both semantic alignment and structural consistency in the generated representations.\n\u2022 Extensive experimental results demonstrate that our LLM-EvRep achieves higher recognition accuracy than hand-crafted and E2V methods across multiple benchmark datasets, setting a new benchmark for leveraging Large Language Models (LLMs) in event-based vision tasks."}, {"title": "3 Method", "content": "In this section, we provide a detailed introduction to the framework for generating LLM-compatible event stream representations through self-supervised learning. The process starts by converting the raw event stream into a neural network-friendly representation, Tencode [13]. This representation is then fed into our LLM-compatible Event Representation Generator, LLM-EvGen, an encoder-decoder network inspired by E2HQV [24] and EfficientNetV2 [29] (see Section 3.1). Subsequently, LLM-EvGen is trained using a self-supervised framework based on semantic consistency, enabling it to produce LLM-compatible event representations, referred to as LLM-EvRep (see Section 3.2). To accelerate model convergence and enhance the quality of representations, an additional structural fidelity loss is introduced during training. This ensures that the generated representations not only retain structural consistency but also adapt effectively to LLMs, thereby facilitating improved performance in event-based recognition tasks performed by LLMs."}, {"title": "3.1 Learnable Event Representation Generator", "content": "Inspired by E2HQV [24], the proposed LLM-EvGen adopts a U-Net-like architecture, as shown in Figure 2. Following this design, LLM-EvGen incorporates the MBConv and Fused MBConv layers from EfficientNetV2 [29]. These layers are chosen for their optimal balance between training efficiency and parameter utilization [24], making it well-suited for our model.\nBefore feeding the discretized event stream into LLM-EvGen, we first convert it into a neural network-compatible representation. Following Huang et al. [13], we transform the event stream into a structured frame representation, denoted as F, as shown in Figure 1. Then the event stream representation F is processed by the encoder of the proposed LLM-EvGen, which extracts hierarchical features while progressively reducing the spatial resolution. The encoder begins with two 3 \u00d7 3 convolutional layers to capture low-level features and improve spatial representations. Subsequently, the network incorporates a sequence of MaxPooling layers combined with Fused MBConv and MBConv layers, inspired by EfficientNetV2 [29] and E2HQV [24]. These layers effectively balance computational efficiency and feature extraction capability, enabling the encoder to capture both local and global features.\nAs the feature maps flow through the network, the spatial resolution is progressively reduced using MaxPooling, while the depth of"}, {"title": "3.2 Self-supervised Learning Framework", "content": "To make LLM-EvGen compatible with LLMs, we propose a self-supervised framework based on semantic consistency. Specifically, as illustrated in Figure 1, after processing F with LLM-EvGen, the model generates corresponding LLM-EvRep. To extract semantic information from LLM-EvRep, we directly input it into the LLM (in our case, LLAVA [20]) and obtain its semantic information. Simultaneously, the corresponding RGB frame (denoted as RGB) is also fed into the LLM to generate a comparable semantic information, this processing can be describe as following:\n$f^{e} = LLM(LLM\\text{-}EvRep)$ (1)\n$f^{r} = LLM(RGB)$\nwhere $f^{e}$ represents semantic information of the LLM-EvRep and $f^{r}$ is semantic information of its corresponding RGB frame.\nTo ensure semantic alignment between LLM-EvRep and their corresponding RGB frame, we define a semantic consistency loss based on the semantic information generated by the LLM. Specifically, the semantic information $f^{e}$ and $f^{r}$ are the textual outputs produced by the LLM when processing LLM-EvRep and the corresponding RGB frame, respectively. Our goal is to minimize the semantic discrepancy between these two pieces of semantic information.\nTo achieve this, we compute a Jaccard similarity-based [14] Semantic Consistency Loss. First, $f^{e}$ and $f^{r}$ are tokenized into sets of words, denoted as $W_{e}$ and $W_{r}$, respectively:\n$W_{e} = set(tokenize(f^{e})), W_{r} = set(tokenize(f^{r}))$. (2)\nThe semantic similarity between these two sets is measured using the Jaccard similarity, which is defined as the Semantic Consistency Loss. This loss quantifies the dissimilarity between the textual representations generated by the LLM from LLM-EvRep and its corresponding RGB-based inputs:\n$L_{semantic} = 1 - \\frac{|W_{e} \\cap W_{r}|}{|W_{e} \\cup W_{r}|}$ (3)\nwhere $| W_{e} \\cap W_{r}|$and $|W_{e} \\cup W_{r}|$ represent the sizes of the intersection and union of $W_{e}$ and $W_{r}$, respectively. This loss penalizes semantic differences, encouraging LLM-EvGen to produce event representations that are semantically consistent with their corresponding RGB frames.\nBy aligning the semantic consistency between LLM-EvRep and the corresponding RGB frame, the proposed framework effectively guides LLM-EvGen to generate event representations that are semantically aligned to their RGB counterparts.\nHowever, due to the U-Net-like architecture of LLM-EvGen, the event representations generated during the early stages of training often contain significant noise. This noise can hinder the LLM's ability to extract meaningful semantic information from these representations. To address this issue, we introduce an auxiliary weak supervision mechanism that enforces structural consistency between the event-based representations and their corresponding RGB frames.\nSpecifically, as presented in Figure 1, we design an auxiliary Structural Fidelity Loss, which leverages sobel edge detection [26] to preserve spatial and structural details in the generated event-based frames. The sobel edge detection method computes the gradient magnitude of an image in both the horizontal and vertical directions using predefined sobel kernels. For an input image I, the gradients in the x- and y-directions, denoted as $G_{x}$ and $G_{y}$, are computed as:\n$G_{x} = I * K_{x}, G_{y} = I * K_{y}$, (4)\nwhere $K_{x}$ and $K_{y}$ are the sobel kernels for the horizontal and vertical directions, respectively, and * denotes the convolution operation. The gradient magnitude is then computed as:\n$G = \\sqrt{G^{2}_{x} + G^{2}_{y}}$. (5)\nThis gradient magnitude map highlights the edges in the image, capturing the structural information present in the input. To ensure consistency between the structural details of the generated LLM-EvRep and its corresponding RGB frames, we define the Structural Fidelity Loss as the mean squared error (MSE) between their sobel edge map:\n$L_{fidelity} = MSE(sobel(O), sobel(T))$, (6)\nwhere O and T represent the sobel edge maps extracted from LLM-EvRep and its corresponding RGB frame, respectively, and sobel(.) computes the sobel edge map of the input.\nTo jointly optimize the semantic alignment and structural consistency of the LLM-EvRep, we combine the Semantic Consistency Loss and the Structural Fidelity Loss into a unified objective, referred to as the Dual Alignment Loss:\n$L_{dual} = \\lambda L_{semantic} + \\gamma L_{fidelity}$, (7)\nwhere $\\lambda$ and $\\gamma$ are weighting factors that balance the contributions of the two losses. The Semantic Consistency Loss ensures that LLM-EvRep generated by LLM-EvGen are semantically aligned with their RGB counterparts, while the Structural Fidelity Loss enforces spatial and structural consistency in the representations. This dual-loss strategy integrates semantic alignment and structural precision, effectively mitigating noise during the early stages of training. By doing so, it enhances the robustness of the learned representations and accelerates model convergence. The Dual"}, {"title": "4 Experimental", "content": "Evaluation LLMs. We evaluated the recognition accuracy of various event representations across two open-source models, LLAVA [20] and MiniGPT-4-v2 [1], and two proprietary models, GPT-40 and GPT-4 Turbo [22]. Furthermore, we compared the performance of three CLIP-based zero-shot methods: ECLIP [36], EventCLIP [31], and EventBind [37].\nTraining datasets. We use the N-ImageNet [16] dataset to train our LLM-EvGen. N-ImageNet is derived from the ImageNet-1K [2] dataset, where RGB images are displayed on a monitor and captured by a moving event camera. The dataset consists of 1,781,167 event streams with a resolution of 480 \u00d7 640, covering 1,000 unique object classes. We split the dataset into training and testing sets with a ratio of 5:1, where the testing set is used for evaluation experiments.\nDownstream tasks datasets. To evaluate the performance of LLM-EvRep, we conduct experiments on object recognition task. We utilize the N-ImageNet [16], N-Caltech101 [4], and N-MNIST [23] datasets.\nImplementation Details. We utilize LLaVA [21] to train our LLM-EvGen, with the weights of LLaVA frozen throughout the training"}, {"title": "4.2 Results", "content": "Table 1 presents the quantitative evaluation results. As shown, compared to CLIP-based zero-shot methods, our proposed LLM-EvRep achieves an accuracy of 94.72% on the N-Caltech101 dataset using GPT-40, surpassing the state-of-the-art zero-shot method EventBind [37] by 27.14%. On the N-ImageNet dataset, our method outperforms ECLIP [36] by nearly six orders of magnitude, reaching an accuracy of almost 50%. These results clearly demonstrate that our LLM-based approach exhibits superior performance compared to traditional CLIP-based zero-shot methods in event-driven vision tasks.\nFurther examining Table 1, we used the representative open-source models LLaVA [21] and MiniGPT-4-v2 [1] for recognition tasks. The results show that our proposed LLM-EvRep method outperforms both hand-crafted and E2V methods in recognition accuracy. Specifically, experiments on the N-Caltech101 dataset show that LLM-EvRep achieves an accuracy improvement of 5.22% over the hand-crafted event frames, and surpasses the V2E methods, E2VID and E2HQV, by 2.68% and 2.43%, respectively. Furthermore, results on the N-MNIST dataset indicate that LLM-EvRep improves accuracy by 31.80% over the event frames and is approximately four times more accurate than E2VID and E2HQV. These results demonstrate that our method effectively enhances the image understanding ability of open-source LLMs.\nAt the same time, we also conducted recognition experiments on the representative proprietary models GPT-40 and GPT-4uurbo. The experimental results show that LLM-EvRep outperforms both the hand-crafted and E2V methods across the three experimental datasets. Notably, LLM-EvRep achieved an astonishing 100% recognition accuracy on the N-MNIST dataset, which is nearly twice as high as the two E2V methods. This strongly demonstrates the superior performance of our method on simpler datasets.\nMeanwhile, we also compare the accuracy with Tencode [13], as shown in Table 2. The results in Table 2 indicate that, whether evaluated on the open-source model LLaVA or the proprietary model GPT-40, our proposed LLM-EvRep consistently outperforms Tencode in terms of accuracy. These findings further validate the effectiveness of our approach.\nIn summary, the experimental results across multiple datasets and models consistently demonstrate the superior performance of our proposed LLM-EvRep method in event-driven vision tasks. Compared to existing CLIP-based zero-shot methods, hand-crafted"}, {"title": "5 Conclusion", "content": "In this paper, we introduce LLM-EvGen, a novel event representation generator specifically designed for large language models (LLMs). To effectively train LLM-EvGen, we propose a self-supervised learning framework based on a dual alignment approach that ensures both LLM Semantic Consistency and Structural Fidelity. This framework leverages two key components: a semantic alignment loss that ensures the generated event representations are semantically coherent with the corresponding RGB frames, and a structural fidelity loss that preserves the spatial integrity of the event data. Extensive experimental evaluations demonstrate the effectiveness of our proposed approach."}]}