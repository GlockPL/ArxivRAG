{"title": "Imagining and building wise machines:\nThe centrality of Al metacognition", "authors": ["Samuel G. B. Johnson", "Amir-Hossein Karimi", "Yoshua Bengio", "Nick Chater", "Tobias Gerstenberg", "Kate Larson", "Sydney Levine", "Melanie Mitchell", "Iyad Rahwan", "Bernhard Sch\u00f6lkopf", "Igor Grossmann"], "abstract": "Recent advances in artificial intelligence (Al) have produced systems capable of\nincreasingly sophisticated performance on cognitive tasks. However, Al systems still\nstruggle in critical ways: unpredictable and novel environments (robustness), lack\ntransparency in their reasoning (explainability), face challenges in communication and\ncommitment (cooperation), and pose risks due to potential harmful actions (safety). We\nargue that these shortcomings stem from one overarching failure: Al systems lack\nwisdom. Drawing from cognitive and social sciences, we define wisdom as the ability to\nnavigate intractable problems\u2014those that are ambiguous, radically uncertain, novel,\nchaotic, or computationally explosive\u2014through effective task-level and metacognitive\nstrategies. While Al research has focused on task-level strategies, metacognition-the\nability to reflect on and regulate one's thought processes\u2014is underdeveloped in Al\nsystems. In humans, metacognitive strategies such as recognizing the limits of one's\nknowledge, considering diverse perspectives, and adapting to context are essential for\nwise decision-making. We propose that integrating metacognitive capabilities into Al\nsystems is crucial for enhancing their robustness, explainability, cooperation, and\nsafety. By focusing on developing wise Al, we suggest an alternative to aligning Al with\nspecific human values\u2014a task fraught with conceptual and practical difficulties. Instead,\nwise Al systems can thoughtfully navigate complex situations, account for diverse\nhuman values, and avoid harmful actions. We discuss potential approaches to building\nwise Al, including benchmarking metacognitive abilities and training Al systems to\nemploy wise reasoning. Prioritizing metacognition in Al research will lead to systems\nthat act not only intelligently but also wisely in complex, real-world situations.", "sections": [{"title": "1. Introduction", "content": "Breakthroughs in machine cognition have recently come in quick succession. Generative\nAl (GenAl) systems such as ChatGPT, Strawberry, LLaMA, and Gemini-can\nsummarize medical literature and cases (Cascella et al., 2024; Van Veen et al., 2024),\nidentify case law and relevant legal precedents (Shu et al., 2024), and solve math\nOlympiad problems (Gao et al., 2024). Such advances have led many to ask whether Al\nsystems will soon be able to perform any cognitive task at a human or superhuman level.\nDespite these accomplishments, Al systems still struggle in a variety of ways, limiting\ntheir capabilities while portending new dangers. They struggle in novel and unpredictable\nenvironments that extend beyond their training data\u2014they lack robustness. Their\ncomputational processes are opaque, creating a problem of explainability (Dwivedi et al.,\n2023). Their challenges with communication and inability to commit credibly to long-term\nplans create barriers to cooperation (Dafoe et al., 2020). And most worryingly of all, these\nshortcomings challenge our ability to harness the upside of Al while avoiding risks and\nensuring its safety (Ji et al., 2023). Each of these problems will be exacerbated as Als\ncome to act as agents in the world.\nHere, we argue that Al systems lack a key capability that underlies all these deficiencies:\nthey are not wise. We draw on research in the cognitive and social sciences to understand\nwhat machine wisdom could be, evaluate why it might be desirable, and sketch potential\nroutes to achieving it.\nDespite a millennia-long philosophical pedigree, it is only recently that social and cognitive\nscientists have begun to reach a consensus about what constitutes wisdom (e.g.,\nGrossmann et al., 2020). Though wisdom can mean many things, for this Perspective we\ndefine wisdom functionally as the ability to successfully navigate intractable problems\u2014\nthose that do not lend themselves to analytic techniques due to unlearnable probability\ndistributions or incommensurable values. Mechanistically, this is achieved through two\ntypes of strategies: (1) Task-level strategies are used to manage the problem itself (e.g.,\nsimple rules-of-thumb); and (2) Metacognitive strategies are used to flexibly manage\nthose task-level strategies (e.g., understanding the limits of one's knowledge and\nintegrating multiple perspectives).\nWhereas some task-level strategies have long been topics of investigation in Al research,\nmetacognition research is much less well-developed. This is a crucial shortcoming\nbecause metacognition is the control system that allows us to decide between conflicting\ntask-level strategies. By analogy to the pivotal role of human metacognition in wise\ndecision-making, better machine metacognition will permit Al systems to prevent\noverconfident inferences and avoid harmful actions. This in turn will improve the\nrobustness of such systems to novel situations, their explainability to users, their ability"}, {"title": "2. What is wisdom?", "content": "At first blush, in the cognitive and social sciences, the concept of 'wisdom' seems to bring\ntogether many superficially unrelated characteristics. Consider the following examples of\nhuman wisdom:\n\u2022 Willa's children are bitterly arguing about money. Willa draws on her life\nexperience to show them why they should instead compromise in the short term\nand prioritize their sibling relationship in the long term.\n\u2022\nDaphne is a world-class cardiologist. Nonetheless, she consults with a much\nmore junior colleague when she recognizes that the colleague knows more\nabout a patient's history than she does.\n\u2022 Ron is a political consultant who formulates possible scenarios to ensure his\ncandidate will win. To help generate scenarios, he not only imagines best case\nscenarios, but also imagines that his client has lost the election and considers\npossible reasons that might have contributed to the loss.\nLife experience, intellectual humility, and scenario planning do not seem to share much\nin common beyond all being positive attributes. But being able to solve tricky integrals,\ncrack clever jokes, and compose beautiful sonnets are also positive attributes\u2014yet these\ndon't constitute wisdom.\nHow can we understand human wisdom, and to what extent has prior work in Al already\nlaid the groundwork for wise Al?"}, {"title": "2.1. Human wisdom", "content": "Given wisdom's disparate characteristics, a range of accounts have been proposed\n(Table 1 summarizes some prominent ones). Like many concepts, there is probably no\nset of necessary and sufficient conditions, yet there is a common core to many of the\ncharacteristics highlighted in Table 1. We draw two important generalizations:\nFunctionally, wisdom facilitates thought and action in intractable situations.\nMechanistically, wisdom is implemented through both task-level strategies and\nmetacognitive abilities for weighing and implementing those strategies."}, {"title": "2.1.1. The function of wisdom: Navigating intractable situations", "content": "If life were a series of textbook problems, we would not need to be wise. There would be\na correct answer, the requisite information for calculating it would be available, and\nnatural selection would have ruthlessly driven humans to find those answers. We would\nbe nothing more or less than master statisticians, merciless optimizers, lightning\ncalculators. Indeed, in some domains-like low-level processing of the visual world-not\nonly humans, but squirrels, goldfish, and bees come remarkably close to this description\n(Mascalzoni & Regolin, 2011).\nYet in other domains, problems such as social interaction and decision-making in an\nunstructured, uncertain, and rapidly evolving world require further tools beyond statistics\nand calculation (Johnson, Bilovich, & Tuckett, 2023). They are often intractable in one or\nmore ways:\nIncommensurable. It features ambiguous goals or values that cannot be compared\nwith one another (Walasek & Brown, 2023).\nTransformative. The outcome of the decision might change one's preferences, so\nthat there is a clash between one's present and future values (Paul, 2013).\nRadically uncertain. We might not be able to exhaustively list the possible\noutcomes or assign probabilities to them in a principled way (Kay & King, 2020).\nChaotic. The data-generating process may have a strong nonlinearity or\ndependency on initial conditions, making it fundamentally unpredictable (Lorenz,\n1993).\nNon-stationary. The underlying process may be changing over time, making the\nprobability distribution unlearnable.\nOut-of-distribution. The situation is novel, going beyond one's experience or\navailable data.\nComputationally explosive. The optimal response could be calculated with infinite\nor infeasibly large computational resources, but this is not possible due to resource\nconstraints.\nOur earlier examples of situations calling for wisdom each featured one or more of these\nforms of intractability. Wisdom helped Willa understand how to make an\nincommensurable trade-off, helped Daphne to navigate her ignorance in an out-of-\ndistribution situation, and helped Ron to make useful forecasts despite his ignorance\nabout the probability distributions governing the radically uncertain future.\nAs Al systems increasingly come to act in the world\u2014encountering ambiguous situations,\nill-specified goals, and unpredictable environments, such as those riddled with fickle\nhumans-their decision-making environment will become increasingly like our own:\nincreasingly intractable. Engineering wise capabilities, enabling Al to more effectively\nnavigate such intractable situations, will accordingly become ever more urgent."}, {"title": "2.1.2. Mechanisms of wisdom: Metacognitive strategy selection", "content": "How do wise people navigate intractable situations? Their secret is both to have a\ncatalogue of effective task-level strategies (which we use to manage a concrete situation)\nand general metacognitive strategies (which manage the knowledge required to deploy\ntask-level strategies and resolve conflicts between them) (Figure 1)."}, {"title": "2.2. Toward Al wisdom: Machine metacognition", "content": "As with humans, strategies acquired by machines often fail in intractable situations. Could\none therefore develop metacognitively wise machines that would function effectively in\nsuch contexts? We can construct parallels to human metacognition\u2014reflecting on our\nthoughts and using that reflection to effectively direct subsequent thoughts and actions\n(Ho et al., 2022). Analogously, Al metacognition refers to the ability to model one's own\ncomputations and use that model to optimize subsequent computations.\nTo date, Al research has focused far more on task-level strategies than on metacognition\n(e.g., a long history of work on heuristics; Pearl, 1984), although the idea of machine"}, {"title": "2.2.1. Would Al wisdom resemble human wisdom?", "content": "While there is great room for improvement in Al metacognition, it is debatable whether\nbuilding wise Al will simply boil down to implementing the metacognitive strategies of wise\nhumans, since many computational constraints and social milieus of humans differ from\nthose of Al systems.\nA major task of human metacognition is to economize scarce cognitive resources (Todd\n& Gigerenzer, 2012; Simon, 1955), such as working memory. Resource rationality\ntheories of human cognition go so far as to say that human (meta)cognition is the rational\nsolution to a constrained optimization problem, where cognitive limits are the constraints\n(Lieder & Griffiths, 2017, 2020). On this view, many so-called cognitive biases are local\nside-effects of a globally optimized system (Levine et al., 2024; Sanborn & Chater, 2016).\nSince artificial systems have far more abundant computational resources, this logic\nfavoring simplifying strategies is arguably weaker.\nDespite our cognitive constraints, human sociality provides crucial metacognitive\nbenefits. Humans live in groups, outsourcing much of our cognition to the social\nenvironment. The division of labor is a classic example, where the knowledge necessary\nto produce any product in the modern economy is distributed over many individuals\n(Hayek, 1945), as in the graphite pencil's famous soliloquy: \u201cnot a single person on the\nface of this earth knows how to make me\u201d (Read, 1958). Laid atop our evolved social\ncognitive capacities (Christakis, 2019), social, economic, and scientific institutions allow\nus to disperse knowledge and reasoning while all benefiting from it. Not only does socially\ndistributed knowledge evolve\u2014through cultural rather than biological evolution (Boyd &\nRicherson, 1985)\u2014but so do those institutions themselves (North, 1990). These\nevolutionary processes allow humans to adapt to a constantly changing environment.\nPerhaps, then, ideal machine wisdom would diverge from its human counterpart. For\nexample, a wise Al system might be more willing to spin its wheels to solve a problem\ncompared to a wise human; it might generate vast numbers of scenarios to analyze many"}, {"title": "3. What are the potential benefits of wise Al?", "content": "Building wise Al systems would address four major challenges."}, {"title": "3.1. Robust Al", "content": "Intelligent systems must function in a wide range of environments, including those that\nchange in unpredictable ways, lack user-specified structure, and differ from any\npreviously seen situation (Johnson et al., 2023). Indeed, as we argued above, Al systems\nwill be called increasingly to act in such intractable situations, including out-of-distribution\nsettings. Yet, such circumstances can exacerbate multiple types of errors\u2014unreliability,\nbias, and inflexibility. Wise Al systems would likely be more robust in all three senses,\nprimarily due to improved metacognition.\nFirst, given similar inputs, an Al system lacking wisdom might give wildly different outputs\n(lacking reliability). Different outcomes given similar data could be due to either applying\ndifferent strategies each time, or to applying the same strategy that nonetheless produces\ndissimilar results each time it is used (e.g., because of strong sensitivity to perturbations\nin initial conditions). High-quality metacognitive monitoring would evaluate whether it is\nsensible to use different strategies in comparable situations (e.g., whether trying a\ndifferent strategy could yield new knowledge) and would reject strategies that produce\nwildly discrepant results on different occasions.\nSecond, an output may be systematically mistaken in a predictable direction (it is biased).\nHere, too, metacognition will help. Assuming that high-quality training procedures have"}, {"title": "3.2. Explainable Al", "content": "Al systems often operate opaquely, with little ability to explain their reasoning to users\n(Dwivedi et al., 2023). Users cannot readily understand the rationale for an opaque\nsystem's output when it requires clarification, diagnose how such systems have gone\nastray when they make mistakes, or collaborate with such systems when the user and Al\nsystem have different conceptions of the problem. Therefore, explainability is a major\nfocus in Al research.\nWise Al systems would likely bring superior explainability. Although humans often\nstruggle to explain their task-level strategies (e.g., in cases of intuition), metacognition\ncan help them to articulate their reasoning. Indeed, the broader metacognitive process of\nstrategy selection may well be easier to explain than the specific task-level strategies\nthemselves, as in other cases where abstract notions are more explainable than concrete\ndetails (Rozenblit & Keil, 2002). Given the importance of wise metacognition to\nexplainability in humans, we consider two routes by which Al metacognition might\ncontribute to Al explainability, rooted in two distinct views of how metacognition operates\nin humans.\nAccording to the classical view, metacognitive strategies explicitly guide behavior. For\nexample, consider again Daphne the cardiologist's decision to consult a more junior\ncolleague regarding her patient. Faced with this situation, a wise reasoner might\nintrospect on her knowledge, determine that she does not know enough to make an\ninformed decision, and thereby seek out alternative points of view. Crucially, it is the\nconscious recognition of ignorance that caused this metacognitive strategy (seeking\nalternative viewpoints) to be deployed; the deliberate adoption of the metacognitive\nstrategy caused new information from others to be integrated into the reasoner's decision;\nand this new information caused a particular decision to be reached. The reasoner would\nobserve herself deploying these processes and would be able to verbally report them if\nasked. If they truthfully report this introspection, it would be broadly accurate.\nA different view is that the mind is \u201cflat\u201d\u2014it does not contain hidden depths of reasons\nthat can be uncovered through introspection (Chater, 2018). This does not mean that\npeople are unable to report their metacognitive strategies. Rather, it means that such\nreports are inferences, not observations. The reasoner would observe the outputs of her\nmetacognitive strategies (e.g., her decisions) and reason backwards to what could have\ncaused them (Cushman, 2020). Essentially, we invent stories to explain our behavior.\nSince we have large samples of observations about ourselves, these stories may well be\nuseful; and verbally formulated reasons can constrain future thought and behavior. But\nour inferences about our metacognition are both exceedingly vague and often wrong\n(e.g., Nisbett & Wilson, 1977).\nIt is debatable which view best explains human metacognition. But these views provide\nradically different prescriptions for building explainable Al.\nThe classical view implies that, if a similar (meta)cognitive architecture were implemented\nin an Al system, explainability falls out as a straightforward consequence. The system\nmerely needs to report the metacognitive process that causally led to its decision.\nThe mental flatness view implies a different picture. If an Al system were as opaque to\nitself as a human is (according to this view) to herself, then a further inference process\nwould be required for the system's wise-but-inaccessible metacognitive processes to be\nreported. Rather than \u201cintrospecting\u201d its metacognition, the system would need to\ngenerate a useful narrative to explain why it made the decision it did. Current GenAl\nsystems seem to operate in this way. But in a wise Al system, such explanations are not"}, {"title": "3.3. Cooperative Al", "content": "As Al is increasingly integrated into society, Al systems are coming to behave as parts of\nlarger networks of intelligent entities (Dafoe et al., 2020). For example, autonomous\nvehicles must negotiate the rules of the road with both other autonomous vehicles (Al\u2013Al\ncooperation). Robots collaborate with surgeons, pattern detection algorithms help\nradiologists (Al-human cooperation). Algorithms that govern language translation and\nsocial media content curation, even though they arguably do not cooperate directly, have\nthe potential to promote or subvert cooperation among human users (human-human\ncooperation).\nThe field of cooperative Al examines how Al systems could effectively benefit all parties\nto an interaction by navigating cooperative barriers such as understanding,\ncommunication, commitment, and institutions (Dafoe et al., 2020). Wise task-level and\nmetacognitive strategies are critical to how humans solve each of these problems,\nsuggesting the same may be true for Al systems.\nUnderstanding a social situation is a prerequisite for any sound approach to cooperation.\nFor example, an autonomous vehicle negotiating a traffic situation needs to understand\nboth the physical environment but also the likely actions taken by other agents (other\nvehicles, law enforcement). Since those actions depend on the mental states (beliefs,\ngoals, and intentions) of agents, their social understanding requires theory-of-mind\n(Gopnik & Wellman, 1992) including the ability tacit to form joint plans to coordinate\nbehavior (Chater et al., 2018). At the task-level, theory-of-mind is often simplified by\nassuming that the agent is rational (Gergely & Csibra, 2003; Johnson & Rips, 2015) and\nusing this assumption to do inverse planning (working backwards from their actions to\nmental states; Baker et al., 2009). But people make this assumption in a context-sensitive\nmanner (Grossmann & Eibach, 2024), for instance using higher-order theories about the\nextent to which an agent acts in a planned or habitual manner (Gershman et al., 2016).\nCommunication is equally crucial to cooperation. Successful cooperators must select and\nsend relevant information to potential partners. Equally important, they must filter\nincoming information to act on what is useful and ignore what is irrelevant or misleading\n(Sperber et al., 2010). Even young children develop mechanisms for evaluating the\ntrustworthiness of sources, such as examining their track record of accuracy and\ndiscounting testimony from sources with conflicts of interest (Sobel & Kushnir, 2013).\nCrucially, when testimony takes the form of an argument from premises to a conclusion,\nthe reasoning itself can be checked, which may have been one evolutionary driver of\nhuman reasoning capacities (Mercier & Sperber, 2017). These epistemic vigilance\nmechanisms make credible communication among humans possible: Without a means"}, {"title": "3.4. Safe Al", "content": "Tales of powerful Al gone awry are widespread in science fiction, but some worry that\nsuch catastrophic scenarios could become more than just whimsical stories. The logical\ncore of the concern is two-fold: (i) Goals defined ahead of time are very likely to be mis-\nspecified or to become obsolete, and (ii) A sufficiently powerful Al system could be very\ndifficult to curtail if it aggressively pursued the wrong goals. Bostrom (2014) famously\ngives the example of the autonomous paperclip-maximizing Al system that converts the\nentire Earth into paperclips and kills all humans who get in its way. The Al's objective\nfunction has been mis-specified (the paperclip company's shareholders would prefer a\nworld with fewer paperclips but more of everything else). But for any putatively more\ncomplete set of goals, it is worryingly easy to generate scenarios where they too go awry.\nThe goal of Al alignment (Ji et al., 2023) is to prevent such mismatches between an Al\nsystem's goals and those of its users\u2014a task which is exceedingly difficult due to the\nrange of unspoken assumptions we make and which an Al system would not necessarily\nshare. In fact, exhaustively specifying goals in advance is difficult for similar reasons that\nhumans cannot use top-down analytical tools to navigate intractable situations more\nbroadly. Just as humans rely on wisdom to navigate such situations, Al systems might\nbenefit from wisdom to navigate goal hierarchies.\nBut Al safety is a much broader concern than scenarios that are (at least for now) science\nfiction (Dalrymple et al., 2024). Even an Al system with goals aligned with its user is\ndangerous if the user is a criminal, terrorist, or adversarial government, as in the case of\nAl used for disinformation (e.g., deep fakes). Indeed, for now, the greatest risk is not\npowerful and malevolent Al systems, but those that simply do not work well\u2014a shoddy\nsurgical robot, incompetent tax advice system, or biased parole decision algorithm. In the\ncase of such prosaic safety failures, others have observed that machine metacognition\nwill be a crucial tool to fight such failure modes (Johnson, 2022). For example, Als with\nappropriately calibrated confidence can target the most likely safety risks; appropriate\nself-models would help Als to anticipate potential failures; and continual monitoring of its\nperformance would facilitate recognition of high-risk moments and permit learning from\nexperience."}, {"title": "3.4.1. Rethinking Al alignment", "content": "With respect to the broader goal of Al alignment, we are sympathetic to the goal but\nquestion this definition of the problem. Ultimately safe Al may be at least as much about\nconstraining the power of Al systems within human institutions, rather than aligning their\ngoals. But the notion of alignment itself\u2014bringing human and machine values into\nharmony-faces a litany of not only technical problems, but conceptual ones.\nFirst, humans are not even aligned with each other. This has been vividly illustrated in\nrecent discourse surrounding how GenAls should balance egalitarian norms versus\nproviding accurate information to users, such as using stereotypes that may be\nstatistically accurate but socially harmful (Gilbert, 2024). As Als transcend mere\nconversation and act increasingly as agents in the world, such examples will deepen and\nproliferate.\nSecond, even if humans uniformly prioritized norm-following above other considerations,\nthose norms differ sharply across cultures. Western societies typically prioritize\nindividualistic values (Henrich et al., 2010), but other societies embrace a variety of other\nvalues such as security, self-direction, and benevolence\u2014that can themselves conflict\n(Sagiv & Schwartz, 2022). This problem is only exacerbated by the concentration of\nleading Al companies in one particular country (the U.S.), and representing a narrow slice\nof even that country.\nThird, even if humans uniformly prioritized norm-following and those norms were uniform\nacross cultures, what reason is there to think that those are the right norms? Social values\nhave changed in both small ways (what jokes are funny versus cringe-worthy) and large\n(past norms and institutions considered abhorrent today; Varnum & Grossmann, 2017).\nTo align Al systems to current values would risk reifying those values as \u201cthe right\u201d values,\nstalling future social progress. Instead, society and its component individuals\u2014including\nAl systems\u2014should continue to allow values to evolve toward a shared reflective\nequilibrium (Rawls, 1971) that brings situation-specific judgments and general moral\nprinciples into alignment with one another through iterative adjustments\u2014a metacognitive\nprocess.\nGiven these conceptual problems, alignment may not be a feasible or even desirable\nengineering goal. The fundamental challenge is how Al agents can live among us-and\nfor this, implementing wise Al reasoning may be a more promising approach. Aligning Al\nsystems to the right metacognitive strategies rather than to the \u201cright\" values might be\nboth conceptually cleaner and more practically feasible. For example, task-level\nstrategies may include heuristics such as a bias toward inaction: When in doubt about\nwhether a candidate action could produce harm according to one of several possibly\nconflicting human norms, by default do not execute the action. Yet wise metacognitive\nmonitoring and control will be crucial for regulating such task-level strategies. In the\n'inaction bias' strategy, for example, a requirement is to learn what those conflicting\nperspectives are and to avoid overconfidence."}, {"title": "4. How might we build wise Al?", "content": "With a sketch of what wise Al might be like and why this might be desirable, we turn to\nthe engineering challenges for artificial metacognition. Here, we offer some initial\nspeculations about approaches to benchmarking and training wise Al."}, {"title": "4.1. Benchmarking", "content": "How will we know when an Al system is wise? Practical and conceptual challenges\nabound:\n1. Memorization. Apparent success could be due to the system relying on patterns\nof reasoning specific to its training data rather than on novel reasoning.\n2. Process. Since wisdom is about the reasoning underlying a strategy's selection,\nwe need to evaluate not just the outcome but the process that led to it.\n3. Context. Since the wise strategy is context-sensitive, the information provided to\nthe Al for benchmarking must contain sufficient detail to match the rich context the\nAl would have in a real-world situation.\nOne way to gain traction is to consider how other complex constructs have been\u2014or\nshould be-benchmarked. For example, researchers have attempted to benchmark\nconstructs such as theory-of-mind (Strachan et al., 2024) and analogies (Webb et al.,\n2023). One approach is to assemble a wide range of benchmarks used in psychology.\nHowever, since these tasks are already discussed in the scientific literature (the\nmemorization problem), it is critical to replace the content to construct structural similar\nbut superficially different problems (Frank, 2023). Moreover, since these tasks measure\noutcomes (i.e., the correct answer) and are relatively decontextualized, this approach\ncannot just be adopted wholesale.\nA quite different approach was taken to benchmark Al systems' ability to creatively\nproduce explanations (Thagard, 2024). The author\u2014a philosopher with extensive domain\nexpertise in explanatory reasoning\u2014presented GPT-4 with a range of novel topics it was\nasked to explain. GPT-4's explanations were judged by the author as comparable to a\nthoughtful graduate student; it even generated a plausible new theory of consciousness.\nIn this case, the author's subjective appraisal, rather than objective quantitative\nmeasures, were used. Such an approach is better-suited to evaluating reasoning (rather\nthan outcomes), but its qualitative nature limits its applicability for assessing progress or\ncomparing different models.\nCombining aspects of both approaches, one way to benchmark wise Al may be to start\nwith tasks that measure wise reasoning in humans (e.g., Grossmann et al., 2010). These\ntasks typically present participants with a social conflict and ask for a reflection of how it\nmight proceed or how they would resolve it. The reflections are then scored on\nprespecified criteria by human raters. A range of such scenarios\u2014and, critically, novel\nvariants of them\u2014could be presented to Al systems and their performance scored by\neither human raters (perhaps those who themselves score highly on wisdom measures)\nor by other systems if they could be demonstrated to converge in their scores. This\nmethod focuses on reasoning processes rather than outcomes (process problem).\nEqually, these scenarios must be novel (memorization problem) and detailed (context\nproblem). It would also be important to present Als with problems that they might confront\nif given adequate agency, to ensure they can reason wisely not only about humans but\nabout themselves."}, {"title": "4.2. Training", "content": "Training task-level and metacognitive wisdom may require different strategies.\nIn humans, task-level strategies often involve heuristics that are culturally transmitted,\nand these are typically learned through a combination of trial-and-error learning (from\nexperience) and of cultural learning (from person to person). Moreover, wise heuristics\nare often domain-specific and any attempt to exhaustively specify a full set of such rules\nis likely doomed for the same reasons that rule-based expert systems in Al failed. Instead,\nallowing Al systems to learn from their experience (as wise humans may; Dong et al.,\n2023; Grossmann et al., 2010) and from others (as all human cultures do; Henrich, 2018)\nmay be a more promising route forward.\nYet this approach is unlikely to work for training metacognition, where the challenge is\nprimarily deciding between strategies in a context-sensitive way and in soundly justifying\nthose higher-order decisions. This contrasts with how Al systems are typically trained,\nwhere a loss function defined over the model's outputs (rather than reasoning) is\nminimized. Although this may indirectly select for sound decision-making strategies, the\npoor explainability of many state-of-the-art models makes it difficult to determine what\nthose strategies are and it leaves the possibility of deception\u2014i.e., producing an output\nthat pleases a human judge for the wrong reasons.\nThis is not a straightforward problem; it may require multiple complementary approaches.\nOne possibility is a two-step process, first training models for wise strategy selection\ndirectly (e.g., to correctly identify when to be intellectually humble) and then training them\nto use those strategies correctly (e.g., to carry out intellectual humble behavior). A second\npossibility may be to evaluate whether models are able to plausibly explain their\nmetacognitive strategies in benchmark cases, and then simultaneously train strategies\nand outputs (e.g., training the model to identify the situation as one that calls for\nintellectual humility and to reason accordingly; e.g., Lampinen et al., 2022). In either case,\nmodels could be trained against what a wise human would do, or perhaps to explain and\ndefend its choices to wise humans robustly (i.e., to stand up to 'cross-examination')."}, {"title": "5. Could wise Al bring unintended and undesirable consequences?", "content": "Building smarter machines comes with risks: Al with humanlike or superhuman\nintelligence might adopt and pursue undesirable goals. Is there a parallel concern about\nbuilding wiser machines? Plausibly, the answer is 'no.' Empirically, humans with wise\nmetacognition show greater orientation toward the common good, including cooperation\nand responsiveness to other people (Grossmann et al., 2017). Perhaps wise Al systems\nwould therefore have these qualities, too.\nYet these criteria are both aspirational (from an engineering perspective) and derived\nfrom observations of humans (from a scientific perspective). What if we tried and failed to\nbuild wise Al? What if the characteristics of wise Al differ from those of a wise human-\nto the detriment of us humans?\nTo these eminently reasonable concerns we have two responses. First, it is not clear what\nthe alternative is. Compared to halting all progress on Al, building wise Al may introduce\nadded risks alongside added benefits. But compared to the status quo\u2014advancing task-\nlevel capabilities at a breakneck pace with little effort to develop wise metacognition-the\nattempt to make machines intellectually humble, context-sensitive, and adept at balancing\nviewpoints seems clearly preferable.\nSecond, by simultaneously promoting robust, explainable, cooperative, and safe Al, these\nqualities are likely to amplify one another. Robustness will facilitate cooperation (by\nimproving confidence from counterparties in its long-term commitments) and safety (by\navoiding novel failure modes; Johnson, 2022). Explainability will facilitate robustness (by\nmaking it easier to human users to intervene in transparent processes) and cooperation\n(by communicating its reasoning in a way that is checkable by counterparties).\nCooperation will facilitate explainability (by using accurate theory-of-mind about its users)\nand safety (by collaboratively implementing values shared within dyads, organizations,\nand societies).\nWise reasoning, therefore, can lead to a virtuous cycle in Al agents, just as it does in\nhumans. We may not know precisely what form wisdom in Al will take but it must surely\nbe preferable to folly."}]}