{"title": "FedMHO: Heterogeneous One-Shot Federated Learning Towards Resource-Constrained Edge Devices", "authors": ["Dezhong Yao", "Yuexin Shi", "Tongtong Liu", "Zhiqiang Xu"], "abstract": "Federated Learning (FL) is increasingly adopted in edge computing scenarios, where a large number of heterogeneous clients operate under constrained or sufficient resources. The iterative training process in conventional FL introduces significant computation and communication overhead, which is unfriendly for resource-constrained edge devices. One-shot FL has emerged as a promising approach to mitigate communication overhead, and model-heterogeneous FL solves the problem of diverse computing resources across clients. However, existing methods face challenges in effectively managing model-heterogeneous one-shot FL, often leading to unsatisfactory global model performance or reliance on auxiliary datasets. To address these challenges, we propose a novel FL framework named FedMHO, which leverages deep classification models on resource-sufficient clients and lightweight generative models on resource-constrained devices. On the server side, FedMHO involves a two-stage process that includes data generation and knowledge fusion. Furthermore, we introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem during the knowledge fusion stage, and an unsupervised data optimization solution to improve the quality of synthetic samples. Comprehensive experiments demonstrate the effectiveness of our methods, as they outperform state-of-the-art baselines in various experimental setups. Our code is available at https://github.com/YXShi2000/FedMHO.", "sections": [{"title": "1 INTRODUCTION", "content": "With the advancement and adoption of Edge Computing (EC) [1], edge devices continuously generate vast amounts of data [2]. This data is crucial for the development of Artificial Intelligence (AI). However, the traditional paradigm of centralized AI model training, which aggregates all data on a central server, has become increasingly difficult to achieve due to growing concerns over data privacy and security. Federated Learning (FL) has emerged as a promising paradigm for training machine learning models across distributed devices without sharing raw data [3]. Despite impressive theoretical and experimental advancements, FL still faces notable challenges [4], [5] in EC scenarios, such as health-care [6], recommendation systems [7], and financial services [8].\nA considerable challenge is the need for multiple communication rounds between several clients and a central server, which can be costly and intolerable due to the associated time and energy constraints [9], [10]. Moreover, frequent communication poses a high risk of privacy attacks [11], [12], such as a man-in-the-middle attack [13] or the potential for reconstructing training data from gradients [14]. To address the communication and security challenges in conventional FL, the concept of one-shot FL has been introduced [15], which aims to obtain an acceptable global model within a single communication round.\nAnother challenge in real-world FL scenarios for EC is the heterogeneity of client computing resources [16]. For example, a naive solution for developing a health monitoring model via FL involves training local models on each user's smartphone, smartwatch, or smart wristband, followed by the weighted averaging of their parameters. Smartphones typically have 4GB-16GB of RAM, while the Xiaomi Smart Wristband 8 offers 1.4MB of RAM, and the Amazfit Band 7 provides 8MB of RAM. This heterogeneity of client computing resources is common in EC applications of FL [17]. Existing FL frameworks typically require clients to deploy homogeneous local models. However, deploying uniformly small models prevents clients with abundant computing resources from fully utilizing their computational potential. Conversely, deploying uniformly large models excludes clients with limited computing resources from participating in the FL process, thus preventing the global model from acquiring knowledge from these resource-constrained clients. As a result, the deployment of homogeneous local models can degrade the performance of the final global model. A prevalent method for addressing computing heterogeneity is to deploy heterogeneous models on different clients, according to their respective computing capabilities [18]. Therefore, in FL scenarios with a mix of resource-sufficient clients and resource-constrained clients, as illustrated in Figure 1, studying model-heterogeneous one-shot FL becomes critical.\nThe existing one-shot FL methods [19], [20] still face various challenges in model heterogeneity scenarios. DOSFL [21] adopts dataset distillation [22] on each client. However, sending distilled data to the central server introduces additional communication costs, which goes against the original intention of addressing the communication bottleneck in one-shot FL. Some other methods [15], [23] leverage knowledge distillation [24] to aggregate local models with auxiliary public data. However, the performance of these methods depends both on the size of the auxiliary data and on its domain similarity to the local data [25]. Several data-free methods [19], [20], [26] have been proposed to address these issues. DENSE [19] and Co-Boosting [26] train an additional generator on the server side to generate auxiliary data that resembles the local data distribution. However, the efficacy of this generator heavily depends on the local model used for its training. If clients with limited computing resources deploy lightweight local models, whose performance is restricted, the generator's performance will suffer, thereby hindering the final global model from achieving high performance. FEDCVAE [20] deploys generative models on all participating clients, with the global model on the server being trained from scratch using the synthetic samples generated by these generators. Nevertheless, using synthetic samples to convey local data information is less effective than directly using local model parameters. A more detailed explanation can be found in Section 3.\nIn this paper, we propose a novel method, FedMHO, to address the challenge of Model-Heterogeneous One-shot Federated Learning. Our method involves deploying deep classification models on resource-sufficient clients while utilizing lightweight generative models on resource-constrained clients. In this paper, we adopt Conditional Variational Autoencoders (CVAE) [27] as the generative models to validate the effectiveness of our methods. More complex generators can be employed for more complex tasks. During global model training, FedMHO introduces a two-stage process encompassing data generation and knowledge fusion. In the data generation stage, the decoders received from clients generate synthetic samples based on local label distribution. To improve the fidelity of the synthetic samples, we employ an unsupervised data optimization solution. In the subsequent knowledge fusion stage, the global model is initialized by the average parameters of the local classification models and then updated based on the synthetic samples generated in the data generation stage. Furthermore, during the global model training, we propose two solutions named FedMHO-MD and FedMHO-SD to prevent the forgetting of knowledge from classification models. In FedMHO-MD, the local classification models act as multiple teacher models to distill the global model. FedMHO-SD involves self-distillation, where the initialized global model acts as a teacher to guide the training of the global model. Our main contributions are summarized as follows."}, {"title": "RELATED WORK", "content": "The one-shot FL methods aim to maximize the aggregation of data information from clients into the global model within a single communication round. [15] originally proposes one-shot FL and introduces two aggregation methods: the first one ensembles the local models via bagging strategies, and the second one uses ensemble distillation with auxiliary public data. FedOV [20] uses open-set voting [28], [29] to solve the problem of label skewness when bagging local models. However, bagging-like strategies can introduce additional computational overhead during inference on the client side. FedKT [23] uses knowledge distillation for aggregating local models into a global model based on auxiliary public data, which is similar to the first of the methods proposed in [15].\nTo achieve data-free one-shot FL, DOSFL [21] employs dataset distillation [22]. Each client distills its local data into a small dataset and sends it to the server for global model training. DENSE [19] trains a generator on the server side, and then uses local models to distill the global model based on synthetic samples produced by the generator. Based on DENSE, Co-Boosting [26] further improves the global model performance by optimizing the data and improving the integration. Two-stage methods like DENSE and Co-Boosting, which first train the classifier and then train the generator, cause information loss twice in the two training stages. In order to reduce the double information loss, FedSD2C [30] synthesizes samples directly from local data and proposes to share synthetic samples instead of inconsistent local models to solve the problem of data heterogeneity. However, FedSD2C is only applicable to large-scale datasets with high resolution. [20] propose FEDCVAE-ENS and FEDCVAE-KD, which deploy generative models locally and generate data based on local label distributions on the server to train the global model. The limitation of the above data-free methods is that they do not take into account the fact that the application scenarios of one-shot FL are usually accompanied by model heterogeneity."}, {"title": "2.2 Model-Heterogeneous Federated Learning", "content": "To achieve model heterogeneity in FL, some methods extract heterogeneous sub-models from the global model to use as local models. For example, Federated Dropout [31] randomly extracts sub-models using Dropout [32]. HeteroFL [33], FJORD [34], and FedDSE [35] extract static sub-models from the global model, while FedRolex [36] and Split-Mix [37] extract dynamic sub-models. These partial training-based methods require each local model to be a sub-model of the global server model, preventing their deployment in FL scenarios involving completely different model structures.\nKnowledge distillation-based methods do not have the limitations of PT-based methods and are more suitable for FL scenarios with diverse models. FedMD [38] uses transfer learning [39] to pretrain local models on large-scale public datasets, then fine-tunes them using local datasets. FedGKT [40] and FedDKC [41] use Split Learning [42] with the global model serving as a downstream model of local models. Both methods rely on sending label information of local data from clients to the server, exposing the risk of privacy leakage. Methods such as FML [43], PervasiveFL [44], and FedKD [9] assign both a public and a private model to each client. The public model is homogeneous across all clients, while the private model is heterogeneous. By introducing Mutual Learning [45], the public and private model can interact with each other locally. However, maintaining two models on each client introduces additional computation and storage overhead. FedDF [46], DS-FL [47], and Fed-ET [48] utilize unlabeled auxiliary data to transfer knowledge from local models to the global model. However, The differences in distribution between auxiliary and local training data can affect the accuracy of the global model [25].\nTo address the limitations of using public datasets as auxiliary data, some studies have introduced data-free knowledge distillation techniques [49], [50]. For example, FedFTG [51] trained a pseudo data generator by fitting the input space of a local model. The generated pseudo data is then integrated into the subsequent distillation process. DFRD [52] further improves the training of the pseudo data generator, which can generate synthetic samples more similar to the distribution of local training data, thereby improving the accuracy of the global model. However, the effectiveness of these methods is highly dependent on the quality of the local model. If the performance of the local model is poor, the generated pseudo data may also be of low quality, limiting the accuracy of the global model."}, {"title": "3 MOTIVATION", "content": "Current FL algorithms fail to adequately address the challenge of one-shot FL with heterogeneous models. Model-heterogeneous FL algorithms can be broadly categorized into partial training-based methods and knowledge distillation-based methods. However, partial training-based methods are essentially not applicable to one-shot scenarios, and knowledge distillation-based methods introduce misleading information that misleads the global model.\nPartial training-based methods assign heterogeneous sub-models to clients, either by randomly dropping out parameters from the global model or extracting them from the global model based on specific rules. Since one-shot FL involves only a single round of communication and the local model parameters are typically fractions (e.g., $\\frac{1}{2}$, $\\frac{1}{3}$, $\\frac{1}{4}$) of the global model, most of the global model parameters are updated by local data from only a subset of clients. This constraint leads to poor performance of the aggregated global model, and no existing work successfully combines partial training with one-shot FL.\nKnowledge distillation-based methods employ the local models as multiple teachers and utilize an auxiliary dataset to train the global model via knowledge distillation, which proves effective in one-shot scenarios [53]. This auxiliary dataset can be sourced from a public dataset, such as FedDF, or by an additional trained generator like DENSE or Co-Boosting. However, the lightweight local models of FedDF, DENSE, or Co-Boosting often under-perform and provide inaccurate logits (i.e., soft labels) during training of the global model or generator [54], which subsequently degrade the performance of the final global model. Figure 2 presents the experimental results on the EMNIST dataset. The experimental setup is the same as in Section 5. Notably, when only the deep large models are used, the global model's performance slightly improves compared to using all local models, even though data from the lightweight small models is not aggregated. This indicates that lightweight small models provide a negative gain.\nFEDCVAE deploys generative models on all clients to generate synthetic samples for training the global model. Figure 2 demonstrates that FEDCVAE has a significantly better performance compared to FedDF, DENSE and Co-Boosting. This improvement arises because, while lightweight generative models may produce low-quality samples, such as blurred contours (refer to Figure 9 in the Appendix), they do not introduce erroneous logits like poorly performing classification models do.\nNonetheless, synthetic samples cannot fully match the quality of the raw data. To maximize the use of real data and minimize erroneous information from low-performance models, we propose deploying deep classification models on clients with sufficient computing resources and lightweight generative models on clients with limited computing resources. During aggregation, the classification models directly average the parameters to initialize the global model, and then the global model is further trained by the synthesized samples. The effectiveness of our proposed method can also be demonstrated in Figure 9."}, {"title": "4 METHODOLOGY", "content": ""}, {"title": "4.1 Preliminaries", "content": ""}, {"title": "4.1.1 Conditional Variational Autoencoder", "content": "CVAE is a variation of Variational Autoencoder (VAE) [55] that incorporates conditional information into VAE. Specifically, a VAE learns a latent space representation of input data through an encoder-decoder architecture. In CVAE, this latent space is conditioned on additional information, such as class labels, enabling controlled generation based on specific conditions. Formally, let $x$ denote the input data, $c$ denote the conditional information, $\\Phi$ denotes the encoder network, and $\\Theta$ denotes the decoder network, respectively. A CVAE uses $q_{\\Phi}$ to model the approximate posterior distribution $q_{\\Phi}(z|x, c)$ over latent variables $z$, and uses $\\theta$ to model the generative distribution $p_{\\theta}(x|z, c)$ for reconstructing the input data."}, {"title": "4.1.2 Knowledge Distillation", "content": "Knowledge distillation aims to minimize the discrepancy between logits output from the teacher model and the student model on the same data. The discrepancy can be measured by the Kullback-Leibler divergence [56] $KL[]$ as $\\min_{W_s} KLED[h(W_r;x)||h(W_s;x)]$, where $W_r$ denotes teacher model, $W_s$ denotes student model, $D$ denotes the data used for knowledge distillation, and $h(*;x)$ denotes the output logits of input data $x$."}, {"title": "4.1.3 Problem Definition", "content": "We consider a one-shot FL setup with an $N_c$-class classification task, where $K$ clients are connected to a central server. We define the set of clients as ${K} = {{K_c},{K_G}}$, where ${K_c}$ denotes the clients that develop classification models, and ${K_G}$ denotes clients that develop generative models. Each client $k \\in {K}$ holds a local model $w_k$ and local training data $D_k$. Specifically, a computing resource-sufficient client $k$ holds a local classification model ${w_k | k \\in {K_c}}$, while a computing resource-constrained client $k$ holds a lightweight generative model ${w_k | k \\in {K_G}}$ with an encoder $\\theta^e_k$ and a decode $\\theta^d_k$. We aim to join resource-constrained clients in FL training and train a well-performing global model $w$ with resource-sufficient clients."}, {"title": "4.2 Overall Algorithm", "content": "Figure 3 illustrates the overall framework of FedMHO. Specifically, resource-sufficient clients deploy deep classification models, and resource-constrained clients deploy lightweight CVAEs. Each client fully trains its respective local model during the local training phase. Afterwards, clients with classification models send their complete models to the central server, and clients utilizing CVAEs send the CVAE decoders and the local label distributions to the central server. The server-side training process consists of data generation and knowledge fusion. In the data generation stage, each local CVAE decoder generates synthetic samples based on its local label distribution. These samples are then refined through an unsupervised quality enhancement process. In the knowledge fusion stage, the local classification models initialize the global model by aggregating their parameters. The global model is then updated with the synthetic samples. To mitigate knowledge-forgetting during the global model training, we employ FedMHO-MD and FedMHO-SD. The specific training details are provided below. The complete algorithmic representation of our methods is provided in Algorithm 1."}, {"title": "4.2.1 Local Training", "content": "The clients train their local models on their respective local data for $E_k$ epochs. During each training epoch, each local model weight $w_k$ is updated as\n$$w_k := w_k - \\eta \\cdot \\nabla L_k(w_k; b_i),$$\nwhere $\\eta$ denotes the learning rate, $b_i$ denotes the mini-batch from local data $D_k$, $L_k$ denotes the training loss function of local model $k$, and $\\nabla L_k(\\cdot)$ denotes the partial derivative of $L_k(\\cdot)$ with respect to its parameter $w_k$. For the classification models, $L_k(w_k; b_i)$ is defined as the cross-entropy loss\n$$L_k(w_k; b_i) = - \\sum_{i=1}^{|b_i|} y_i \\log(\\hat{y_i}),$$\nwhere $|b_i|$ denotes the size of the mini-batch $b_i$, $y_i$ and $\\hat{y_i}$ represent the ground truth and predicted probability of sample $i$, respectively. For the CVAE models, $L_k(w_k; b_i)$ is the sum of the reconstruction loss $L_{recon}$ and the KL divergence loss $L_{KL}(w_k; b_i)$, defined as\n$$L_k(w_k; b_i) = L_{recon}(w_k; b_i) + L_{KL}(w_k; b_i),$$\nwith\n$$L_{recon}(w_k; b_i) = \\frac{1}{|b_i|} \\sum_{i=1}^{|b_i|} E_{q_{\\Phi}(z|x_i,c_i)}[\\log p_{\\Theta}(x_i|z, c_i)],$$\n$$L_{KL}(w_k; b_i) = \\frac{1}{|b_i|} \\sum_{i=1}^{|b_i|} KL(q_{\\Phi}(z|x_i, c_i)||p(z|c_i)),$$\nwhere $x_i$ denotes sample $i$ in mini-batch $b_i$, $c_i$ denotes the conditional information of sample $i$, that is, the ground truth of $x_i$. The term $q_{\\Phi}(z|x_i, c_i)$ refers to the approximate posterior distribution of local model $k$, and the term $p_{\\Theta}(x_i|z, c_i)$ refers to the generative distribution of local model $k$. The term $p(z|c_i)$ refers to the prior distribution, which is typically assumed to follow a standard Gaussian distribution $z \\sim N(0,1)$. The reconstruction loss measures the difference between the generated samples and the input samples, and the KL divergence loss ensures that the distribution of the latent space of the synthetic samples approaches the prior distribution $z$."}, {"title": "4.2.2 Data Generation", "content": "During the data generation stage, the server utilizes the received local decoders and local label distributions to generate synthetic samples $D_s$. Specifically, for each decoder $\\theta_k \\in {K_G}$, we sample $x \\in D_s$, according to client $k$'s local label distribution, formally expressed as $x_i \\sim p_{\\theta_k}(x_i|z, c_i)$, where $z \\sim N(0,1)$, $c_i$ is the label of $x_i$, and $p_{\\theta_k}(x_i|z, c_i)$ represents the conditional probability distribution defined by the client $k$'s decoder $\\theta_k$. The synthetic samples are subsequently used in knowledge fusion sessions. As the generators are lightweight and the local data partitions often exhibit varying degrees of Non-IID, the generated samples may incorporate a certain level of noise. To enhance the quality of these samples, we introduce an unsupervised solution, which is detailed in Section 4.4."}, {"title": "4.2.3 Knowledge Fusion", "content": "During the knowledge fusion stage, the global model is initialized by the local classification models as\n$$w_0 = \\frac{1}{|{K_c}|} \\sum_{k \\in {K_c}} w_k,$$\nwhere ${K_c}$ represents the set of local classification models, and $|{K_c}|$ denotes the number of elements in ${K_c}$. This initialization enables the global model to incorporate knowledge from these local models. Compared with random parameter initialization, this informed initialization allows the global model to achieve superior performance in fewer training rounds. We specifically use $w_0$ to denote the initial state of the global model, while $w$ refers to its later states. To additionally obtain knowledge from the clients deploying generative models, we train the global model using the synthetic samples generated during the data generation stage. The training loss function $L_{CE}(w; b_s)$ of the global model is defined as\n$$L_{CE}(w; b_s) = - \\sum_{i=1}^{|b_s|} y_i \\log(\\hat{y_i}),$$\nwhere $b_s$ denotes the mini-batch of synthetic samples from $D_s$."}, {"title": "4.3 The Knowledge-Forgetting Problem", "content": "During the fusion of the clients' knowledge from the generative models, the global model may forget the knowledge learned from the classification models, as depicted in Figure 4. We define this phenomenon as the knowledge-forgetting problem. We propose two methods to alleviate this problem: FedMHO-MD and FedMHO-SD. In FedMHO-MD, we use local classification models as multiple teacher models to distill their knowledge into the global model. The corresponding loss function $L_{KL}(w; b_s)$ is defined as\n$$L_{KL}(w; b_s) = KL\\Big[\\frac{1}{|{K_c}|} \\sum_{k \\in {K_c}} h_k(w_k, b_s) \\| h(w,b_s)\\Big],$$\nwhere $h_k(w_k, b_s)$ and $h(w,b_s)$ denote the output logits of samples in the mini-batch $b_s$ through local model $w_k$ and global model $w$, respectively. In FedMHO-SD, we use the initialized global model $w_0$ as the teacher model to self-distill the global model. The corresponding loss function $L_{KL}(w; b_s)$ is defined as\n$$L_{KL}(w; b_s) = KL [h_k(w_0, b_s)||h(w,b_s)].$$\nwhere $h_k(w_0, b_s)$ denotes the output logits of samples in the mini-batch $b_s$ through the initialized global model $w_0$. Overall, the complete loss function $L_g$ for training the global model is defined as:\n$$L_g (w; b_s) = \\lambda L_{CE}(w; b_s) + (1 - \\lambda) L_{KL}(w; b_s),$$ where $\\lambda \\in [0, 1]$ denotes the trade-off parameter between $L_{CE}$ and $L_{KL}$. In our experiments, we observe the values of $L_{CE}$ and $L_{KL}$ are of similar magnitude, thus, we set $\\lambda$ = 0.5 by default."}, {"title": "4.4 Unsupervised Data Optimization", "content": "We observe that a lightweight CVAE occasionally generates samples that exhibit label confusion. For example, when the decoder receives an input conditioned on label 2 for generating synthetic samples, the model might produce samples with labels 1 or 3 instead. While such instances of mislabeling are infrequent, they introduce noise into the training data of the global model. To enhance the quality of synthetic samples and improve the global model, we propose an unsupervised data optimization strategy based on the K-means algorithm [57].\nSpecifically, for synthetic samples corresponding to a specific class $n_c \\in N_c$, we use the flattened pixel values as features and apply K-means clustering to group these samples into one cluster. The feature dimension $F$ is determined by the number of pixels in the image. For example, for an image of size 10 \u00d7 10 pixels, the feature dimension $F$ is 100. After clustering, we obtain the cluster center $C_c$ corresponding to each category $n_c$. Subsequently, we selectively retain the samples closest to each $C_c$. By default, we set the ratio of the number of remaining samples to the number of original samples, denoted as $R_{th}$, to 80%. Experimental results for different values of $R_{th}$ are shown in Section 5.4.4. A detailed algorithmic description is provided in Algorithm 2."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 Experimental Setup", "content": ""}, {"title": "5.1.1 Datasets", "content": "Following the latest baseline, FEDCVAE [20], we evaluate our methods on four widely used datasets: MNIST [58], Fashion-MNIST [59] (abbreviated as Fashion), SVHN [60] and EM-NIST [61]. To simulate statistical heterogeneity, we employ the Dirichlet distribution as in [62], which is denoted as $Dir(\\alpha)$. Smaller values of $\\alpha$ correspond to greater heterogeneity among clients' local data partitions. We set $\\alpha$ = {0.5, 0.3,0.1} by default."}, {"title": "5.1.2 Methods", "content": "Under the constraint of a single communication round, FL methods that emphasize regularization, such as FedProx [63], Scaffold [64] and FedGen [65], are rendered ineffective. Given the data-free nature of our proposed methods, we select three state-of-the-art data-free one-shot FL methods as baselines: DENSE [19], Co-Boosting [26] and FEDCVAE [20]. It is noteworthy that FEDCVAE-KD, proposed in [20] alongside FEDCVAE-ENS, consistently underperforms compared to FEDCVAE-ENS. Therefore, our comparison focuses exclusively on FEDCVAE-ENS, referred to simply as FEDCVAE. Additionally, we compare our methods with two well-known baselines: FedAvg [3] and FedDF [46]. In our experiments, FedMHO represents the scenario without addressing the knowledge-forgetting issue. We further define FedMHO-MD and FedMHO-SD to signify the incorporation of two distinct strategies for mitigating knowledge-forgetting. For simplicity, we refer to the set comprising FedMHO, FedMHO-MD, and FedMHO-SD as FedMHOs. To ensure a fair comparison under the heterogeneous one-shot setting, we limit the communication to a single round for both FedAvg and FedDF, and utilize the voting results of each model prototype as the experimental outcome. A brief introduction to the baseline methods is provided below.\n\u2022\tFedAvg [3] is a foundational FL method that learns a global model by averaging the parameters of local models."}, {"title": "5.1.3 Models", "content": "To simulate model-heterogeneous scenarios, we employ two distinct local model prototypes. For the models with more parameters, we utilize VGG-9 and CVAE-large, while for the models with less parameters, we employ CNN and CVAE-small. This selection is designed to reflect the computing power disparities typical in real-world applications, such as those between smartphones and wristbands. Table 1 provides a summary of the specific local models used by each method, and Table 2 details the Floating Point Operations (FLOPs) for each local model with a batch size of 1. To demonstrate the efficiency and feasibility of our proposed FedMHOs in resource-constrained environments, our methods consistently train the local models that have the lowest FLOPs. In our evaluation, we report the global model's Top-1 test accuracy in the form of mean \u00b1 standard deviation, ensuring the reliability of the results."}, {"title": "5.1.4 Configurations", "content": "For training local classification models, we utilize the SGD optimizer with a momentum of 0.9 and a learning rate of 5e-3, training for 200 local epochs. For local generative models, we use the Adam optimizer with varying learning rates specific to each dataset: 5e-2 for MNIST and Fashion, 1e-3 for SVHN, and 3e-3 for EMNIST, with training conducted over 30, 40, 40, and 50 local epochs respectively. When training the global model, we use the Adam optimizer with a learning rate of 1e-5, 5e-4, 5e-5, and 5e-5 for MNIST, Fashion, SVHN, and EMNIST respectively. For methods involving generators, we produce 6,000 synthetic samples each for MNIST, Fashion, and SVHN, and 12,000 synthetic samples for EMNIST. Consistent with the latest baseline FEDCVAE [20], we configure the FL setup with 10 clients participating in the training process. By default, 5 clients simulate resource-sufficient clients, and the remaining 5 simulate resource-constrained clients. The detailed hyperparameter settings are provided in Table 3."}, {"title": "5.2 Performance Comparison", "content": "We present a comprehensive comparison of our proposed FedMHOs with the baselines in Table 4. The results demonstrate that our methods outperform the baselines across various datasets and data partitions, with FedMHO-MD and FedMHO-SD consistently achieving the top two rankings in Top-1 test accuracy. Specifically, FedMHO-MD and FedMHO-SD achieve the highest Top-1 test accuracy in 6 out of 12 experiments, respectively. Under the Dir(0.5) local data partition, FedMHO-MD or FedMHO-SD achieves superior results compared to the best-performing baseline. The improvements are 3.06%, 5.44%, 7.23%, and 6.12% on the MNIST, Fashion, SVHN, and EMNIST datasets, respectively.\nEven in the absence of the loss function $L_{KD}$, which mitigates the knowledge-forgetting problem of the global model, FedMHO generally outperforms other baselines, except when training on the MNIST and Fashion datasets with the Dir(0.1) local data partition. We attribute this to the pronounced knowledge deviation between generative and classification local models trained on highly heterogeneous local data. This deviation intensifies the effect of knowledge-forgetting when the global model is trained on synthetic samples. While the best-performing baseline, FEDCVAE, outperforms FedMHO under highly non-IID local data partitions, it fails to surpass FedMHO-MD or FedMHO-SD. Additionally, since FedMHOs' global models are well initialized, FedMHOs converge faster than the baselines during global model training, as shown in Figure 5. Overall, our experimental results demonstrate the effectiveness and robustness of our proposed FedMHOs, highlighting their potential for real-world applications."}, {"title": "5.3 Ablation Study", "content": ""}, {"title": "5.3.1 Contribution of LKD", "content": "As mentioned in Section 4.2.3, we employ the loss function $L_{KD}$ to mitigate knowledge-forgetting during global model training. Table 4 illustrates that both FedMHO-MD and FedMHO-SD benefit from $L_{KD}$, improving the global model's Top-1 test accuracy. To further substantiate the necessity of $L_{KD}$, we extend the training epochs of the global model to 100 epochs. The variation of Top-1 test accuracy throughout the training process is depicted in Figure 6. As shown in the figure, in the absence of $L_{KD}$, the global model acquires new knowledge from the synthetic data but forgets the original knowledge obtained from aggregating the classification models. This finding underscores the importance of addressing knowledge-forgetting.\nTo further analyze the experimental results under different weight configurations, we define the weight of $L_{KD}$ as $\\beta_1$ and the weight of $L_{CE}$ as $\\beta_2$. By default, both $\\beta_1$ and $\\beta_2$ are set to 1. Table 5 summarizes the experimental results on the EMNIST dataset under various $\\beta_1$ and $\\beta_2$. When $\\beta_1 = \\frac{1}{2}$, $\\beta_2 = 1$ or $\\beta_1 = 2$, $\\beta_2 = 1$, the global model's Top-1 test accuracy decreases slightly. Moreover, when $\\beta_1$ and $\\beta_2$ are treated as trainable parameters, the performance remains comparable to the default setting of $\\beta_1 = 1$ and $\\beta_2 = 1$. Given that treating the weights as trainable parameters introduces additional computation overhead and potential optimization challenges, we choose the default configuration as the final solution.\nAdditionally, we investigate the impact of simultaneously utilizing the loss functions associated with FedMHO-MD's multi-teacher knowledge distillation and FedMHO-SD's self-distillation. We conduct experiments on the EMNIST dataset that satisfies the $Dir(0.5)$ data partition. When the weights of both two $L_{KDS}$ are set to 1, the Top-1 test accuracy is 78.14%. When both weights are treated as trainable parameters, the Top-1 test accuracy is 78.33%. These results indicate that the simultaneous use of both $L_{KD}$ terms does not lead to substantial performance gains compared to using a single $L_{KD}$. This outcome can be attributed to the fact that both $L_{KDs}$ aim to mitigate knowledge forgetting by regularizing the global model based on the local classification models. Thus, a single $L_{KD}$ suffices to achieve this objective."}, {}]}