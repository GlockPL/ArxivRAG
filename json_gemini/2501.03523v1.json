{"title": "Vocal Tract Length Warped Features for Spoken Keyword Spotting", "authors": ["Achintya kr. Sarkar", "Priyanka Dwivedi", "Zheng-Hua Tan", "IEEE"], "abstract": "In this paper, we propose several methods that incorporate vocal tract length (VTL) warped features for spoken keyword spotting (KWS). The first method, VTL-independent KWS, involves training a single deep neural network (DNN) that utilizes VTL features with various warping factors. During training, a specific VTL feature is randomly selected per epoch, allowing the exploration of VTL variations. During testing, the VTL features with different warping factors of a test utterance are scored against the DNN and combined with equal weight. In the second method scores the conventional features of a test utterance (without VTL warping) against the DNN. The third method, VTL-concatenation KWS, concatenates VTL warped features to form high-dimensional features for KWS. Evaluations carried out on the English Google Command dataset demonstrate that the proposed methods improve the accuracy of KWS.", "sections": [{"title": "I. INTRODUCTION", "content": "Spoken keyword spotting (KWS) is the task of identifying whether or not a word from a predefined list of words was uttered in a given speech signal [1]. KWS has many potential applications such as audio indexing, home automation robotics, awake-up word detection in mobile devices, etc. Due to the natural variability in vocal tract length (VTL) among individuals, the spectral representation of the same word can differ when uttered by different speakers. This VTL variability affects the performance of automatic speech recognition systems, which aim to recognize the linguistic content of speech signals regardless of speaker variability. To mitigate the effect of VTL variability, vocal tract length warping was introduced in [2], [3], [4], [5] and subsequently applied in speaker recognition [6], [7], and spoken term detection [8].\nThe motivation behind this technique is to align the spectra produced by two speakers (A and B) for the same linguistic content. This alignment is achieved by scaling the frequency axis of the signal spectra as follows,\n$S_A(f) = S_B(\\alpha f)$"}, {"title": "II. PROPOSED METHODS", "content": "This section first introduces the concept of VTL warping factor and then describes the proposed methods."}, {"title": "A. VTL warping factor", "content": "The concept of VTL warping factor $\\alpha$ was first introduced in [5] to align the spectra of the same content uttered by two speakers by scaling the frequency axis of the signal spectra as in Eq. (1). Depending on the psychological structure of a person, the $\\alpha$ value in Eq. (1) generally lies in between [0.80, 1.20]. In practice, a step size of 0.02 is commonly used. This gives a set of 21 $\\alpha$ values. The warped spectrum $f_{wrp}$ for a given signal having a spectrum $f$ is estimated piecewise linearly as\n$f_{wrp}^{(a)} = \\begin{cases} S_A f, & 0 \\leq f \\leq f_0 \\\\\\frac{f_m - \\alpha f_0}{f_m - f_0} (f - f_0) + \\alpha f_0, & f_0 \\leq f \\leq f_m \\end{cases}$"}, {"title": "B. VTL-independent KWS", "content": "In this method, a single DNN is trained with all warped features. At each epoch, one warping factor is randomly selected, and warped features corresponding to the selected warping factor are used for training. A cross-entropy loss is used for training. A similar concept is utilized in [4] for automatic speech recognition. During testing, all VTL warped features of a test utterance are scored against the single VTL- DNN. This yields a set of 21 scores based on the number of VTL factor $\\alpha$. The scores of the test utterance for different VTL warped features are combined with equal importance. We call it VTL-independent KWS. Equ. (3) depicts the score averaging across the VTL-independent KWS methods\n$FusionScore = \\frac{1}{\\#\\alpha} \\Sigma_\\alpha \\lambda_{vtl-dnn}(X_\\alpha)$"}, {"title": "C. VTL-independent\u03b1=1.00 KWS", "content": "This method is similar to the VTL-independent KWS, with the key difference being that the unwarped VTL ($\\alpha$ = 1.00) features of the test utterance are scored against the VTL-DNN. It can be expressed as follows,\n$Score = \\lambda_{vtl-dnn}(X_{\\alpha=1.00})$\nIn terms of computational complexity and number of parameters, this method is analogous to conventional KWS. However, it can capture VTL feature information within a single DNN."}, {"title": "D. VTL-concatenation KWS", "content": "In this method, all VTL warped feature vectors for a given speech signal are concatenated, resulting in high-dimensional feature vectors. For example, with 21 values of $\\alpha$ and each VTL $\\alpha$ having a 40-dimensional feature vector, this gives 40 \u00d7 21-dimensional feature vectors. The objective of this method is to investigate whether concatenated features are as effective or able to capture more relevant information compared to the VTL-independent and VTL-independent\u03b1=1.00 KWS methods."}, {"title": "III. CLASSIFIERS", "content": "Four classifiers are considered for evaluating the performance of KWS methods. The first is GRU-MttAten [9], [10], where MFCC features are processed through a 2D convolutional network, followed by a two-layer bidirectional long short-term memory (LSTM) network with a hidden layer dimension of 128, incorporating gated recurrent unit (GRU) as in [10] to jointly capture temporal long-term dependencies. The feature representation in the middle of the bidirectional GRU layer is projected to a dense layer, serving as the query (Q) vector in the multi-head (4-head, i.e., 4 Q matrices) attention mechanism. Subsequently, the weighted average of the bidirectional GRU output is passed through three layers of a fully connected network before being projected at the output nodes. The second classifier is the temporal convolution (TC)- ResNet8 architecture as per [14], consisting of three residual blocks and (16, 24, 32, 48) channels for each layer, including the first convolution layer. More details about these techniques can be found in [10], [14].\nThe third classifier employs the keyword transformer (KWT)-3 method, which leverages the concept of vision transformer (ViT) [11] in KWS In this approach, MFCC spectrograms are converted into patches to be fed into the transformer.\nThe fourth one is based on BC-ResNet-8 based KWS [15], which leverages the advantages of both 1D and 2D convolutions, expanding the temporal output into the time- frequency domain. The residual connection mapping enables the network to effectively represent useful audio information with less computation than convolutional networks. The core idea behind the broadcasted residual block is,\n$y = x + BC(f_1(avgpool(f_2(x))))$"}, {"title": "IV. EXPERIMENT SETUP", "content": "Experiments are conducted on the English Google command [21] dataset, which contains 35 commands or keywords such as \"up\", \"left\", and \"right\". These 35 commands are consid- ered the target keywords, resulting in a total of 35 classes for classification."}, {"title": "V. RESULTS AND DISCUSSIONS", "content": "In this section, we first compare the performance of the proposed KWS methods with the baselines on the English Google Command dataset. The results in Table II demonstrate that both the proposed VTL-independent and VTL- independent\u03b1=1.00 methods, when combined with different DNN architectures, consistently outperform their baseline counterpart methods. This confirms the effectiveness of the proposed methods by incorporating VTL warped features in KWS.\nAmong these two proposed methods, VTL-independent consistently proves to be superior. This indicates that utilizing VTL warped features in both training and testing are beneficial, as VTL-independent uses these features in both processes while VTL-independent\u03b1=1.00 uses VTL warped features only during training. VTL-independent\u03b1=1.00, on the other hand, has an advantage of maintaining the same computational complexity as the baselines during testing.\nThe VTL-concatenation method, however, does not perform as well as the baselines. This could be due to the significant increase in model size, which may result in inadequate training. Additionally, it is observed that among the baseline methods, BCResNet-8 performs the best.\nTo gain further insights into the performance of these methods, we compare the class-wise performance of BCResNet- 8, VTL-indeendent-BCResNet-8, and VTL-independent\u03b1=1.00- BCResNet-8. Figure 1 shows that the proposed methods are superior in most classes.\nTo investigate the impact of the different VTL warped features for the proposed KWS method during testing, we show the accuracies of KWS for the best-performing method, VTL-independent-BCResNet-8, across VTL warping factors in Fig. 2, where we use only one single VTL factor at a time instead of using all of them. From Fig. 2, it can be observed that the accuracy of the KWS initially increases as the value of VTL factor rises up to $\\alpha$ = 1.00, after which it starts to decrease. This could be due to the random selection of the VTL-warped features during training for an epoch, leading to the feature closest to the mean of the VTL factors performing the best.\nTo test the statistical significance of the improvement brought by the proposed methods, we experiment with variants of the BCResNet-8 model, given their superior performance in their respective categories. We conduct experiments of initializing the model parameters with ten different random seeds based on the computer's clock time."}, {"title": "VI. CONCLUSION", "content": "This paper proposed several methods that incorporate vocal tract length (VTL) warped features for spoken keyword spotting (KWS) and explored their combination with several deep neural network architectures. We demonstrated that the proposed VTL-independent KWS method improves the KWS accuracy compared to conventional methods that do not consider warped features. The performance of the methods was validated using the English Google Command dataset. Future work includes exploring VTL-dependent features for personalized KWS."}]}