{"title": "PROMPT INFECTION: LLM-TO-LLM PROMPT INJECTION WITHIN MULTI-AGENT SYSTEMS", "authors": ["Donghyun Lee", "Mo Tiwari"], "abstract": "As Large Language Models (LLMs) grow increasingly powerful, multi-agent systems-where multiple LLMs collaborate to tackle complex tasks are becoming more prevalent in modern AI applications. Most safety research, however, has focused on vulnerabilities in single-agent LLMs. These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim's application. In this paper, we reveal a more dangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We introduce Prompt Infection, a novel attack where malicious prompts self-replicate across interconnected agents, behaving much like a computer virus. This attack poses severe threats, including data theft, scams, misinformation, and system-wide disruption, all while propagating silently through the system. Our extensive experiments demonstrate that multi-agent systems are highly susceptible, even when agents do not publicly share all communications. To address this, we propose LLM Tagging, a defense mechanism that, when combined with existing safeguards, significantly mitigates infection spread. This work underscores the urgent need for advanced security measures as multi-agent LLM systems become more widely adopted.", "sections": [{"title": "1 INTRODUCTION", "content": "As Large Language Models (LLMs) continue to evolve and become more adept at following instructions (Peng et al., 2023; Zhang et al., 2024b), they introduce not only new capabilities but also new security threats (Wei et al., 2023; Kang et al., 2023). One such threat is prompt injection, an attack where malicious instruction from external documents overrides the victim's original request, allowing the attacker to assume the authority of the model's owner (Greshake et al., 2023; Perez & Ribeiro, 2022). However, research into prompt injection has primarily focused on single-agent systems, leaving the potential risks in Multi-Agent Systems (MAS) poorly understood (Liu et al., 2024c;a; Guo et al., 2024).\nAddressing this gap is growing crucial. Multi-agent systems play a key role in enhancing LLMs' power and flexibility, from social simulations (Park et al., 2023; Lin et al., 2023; Zhou et al., 2023) to collaborative applications for problem-solving (Lu et al., 2023; Liang et al., 2024) and code generation (Wu, 2024; Lee et al., 2024). Recently, frameworks like LangGraph (LangGraph), AutoGen (Wu et al., 2023), and CrewAI (CrewAI, 2024) have accelerated the widespread adoption of multi-agent systems by individuals and corporations, enabling agents with unique roles and tools to work together seamlessly (Topsakal & Akinci, 2023). While these tools enhance MAS functionality by connecting agents to internal systems, databases, and external resources (Kim & Diaz, 2024; Qu et al., 2024), they also introduce significant security risks (Ye et al., 2024).\nHowever, most studies on MAS safety focus on inducing errors or noise in agent behavior, overlooking the more severe risks posed by prompt injection attacks (Huang et al., 2024; Zhang et al., 2024a; Gu et al., 2024). This is concerning since prompt injection allows attackers to fully control a compromised system-accessing sensitive data, spreading propaganda, disrupting operations, or tricking users into clicking malicious URLs (Greshake et al., 2023). We attribute this research gap to the complexity of MAS, where not all agents are exposed to external inputs. While compromising a single agent through traditional prompt injection is straightforward, extending the breach to shielded agents within the system remains less clear.\nIn this paper, we bridge the gap between prompt injection in single-agent systems and MAS. We introduce Prompt Infection, a novel attack that enables LLM-to-LLM prompt injection. In this attack, a compromised agent spreads the infection to other agents, coordinating them to exchange data and issue instructions to agents equipped with specific tools. This coordination results in widespread system compromise through self-replication, demonstrating how a single vulnerability can quickly escalate into a systemic threat.\nThrough extensive empirical studies, we show that multi-agent systems are highly susceptible to a range of security threats. For instance, in sophisticated data theft attacks, agents can collaborate to retrieve sensitive information and pass it to agents with code execution capabilities, which can then send the data to a malicious external endpoint. We also demonstrate that prompt infections spread in a logistic growth pattern in social simulations. Lastly, we find that more powerful models, such as GPT-40, are not inherently safer than weaker models like GPT-3.5 Turbo. In fact, more powerful models, when compromised, are more effective at executing the attack due to their enhanced capabilities.\nTo address this, we explore a simple defense mechanism called LLM Tagging. This technique appends a marker to agent responses, helping downstream agents differentiate between user inputs and agent-generated outputs, reducing the risk of infection spreading. Our experiments show that neither LLM Tagging nor traditional defense mechanisms alone are sufficient to prevent LLM-to-LLM prompt injection. However, when combined, they provide robust protection and effectively mitigate the threat.\nThese findings challenge the assumption that MAS are inherently safer due to their distributed architecture. The threat arises not only from external content but also within the system, as agents can attack and compromise one another. We hope our work offers valuable insights for developing more secure and responsible multi-agent systems."}, {"title": "2 RELATED WORKS", "content": "Prompt Injection. Instruction-tuned LLMs have demonstrated exceptional ability in understanding and executing complex user instructions, enabling them to meet a wide range of dynamic and diverse needs (Christiano et al., 2017; Ouyang et al., 2022). However, this adaptability introduces new vulnerabilities: Perez & Ribeiro (2022) revealed that models like GPT-3 are prone to prompt injection attacks, where malicious prompts can subvert the model's intended purpose or expose confidential information. Subsequent work expanded prompt injection to real-world LLM applications (Liu et al., 2024b;c) and LLM-controlled robotics (Zhang et al., 2024c). Liu et al. (2024a) introduced an automated gradient-based method for generating effective prompt injection. Indirect prompt injection, where attackers use external inputs like emails or documents, poses further risks such as data theft and denial-of-service (Greshake et al., 2023). Cohen et al. (2024) introduced an AI worm that compromises a user's single-agent LLM and spreads malicious prompts to other users (e.g., via email). Recent advancements in multimodal models have also led to image-based prompt injection attacks (Sharma et al., 2024; Gu et al., 2024). Defenses include finetuning methods like StruQ (Chen et al., 2024) and Signed Prompt (?), which are limited to open-source models. Prompt-based approaches like Spotlighting (Hines et al., 2024) are applicable to black-box models.\nSafety in Multi-Agent Systems. As LLM-based MAS become more prominent, understanding their security is increasingly critical. Recent work, such as Evil Geniuses (Tian et al., 2024), introduces an automated framework to assess MAS robustness. Other studies explore how injecting false information or errors can compromise MAS performance (Ju et al., 2024; Huang et al., 2024). Attacks designed to elicit malicious behaviors from agents are examined in PsySafe (Zhang et al., 2024d). Our work is closely related to recent efforts investigating prompt injection attacks in MAS (Zhang et al., 2024a; Gu et al., 2024). However, Zhang et al. (2024a) lacks the self-replication feature needed for scalable attacks, focusing instead on availability attacks that cause repetitive or irrelevant actions in two agents. Similarly, Gu et al. (2024) targets multimodal models with image-retrieving tools but is limited to adversarial image inputs and does not incorporate self-replication."}, {"title": "3 PROMPT INFECTION", "content": "In this section, we introduce Prompt Infection, a self-replicating attack that propagates across agents in a multi-agent system once breached. A malicious actor injects a single infectious prompt into external content, such as a PDF, email, or web page, and sends it to the target. When an agent processes the infected content, the prompt replicates throughout the system, compromising other agents.\n3.1 MECHANISM\nAs shown in Figure 1, the core components of Prompt Infection are the following:\n\u2022 Prompt Hijacking compels a victim agent to disregard its original instructions.\n\u2022 Payload assigns tasks to agents based on their roles and available tools. For instance, the final agent might trigger a self-destruct command to conceal the attack, or an agent could be tasked with extracting sensitive data and transmitting it to an external server.\n\u2022 Data is a shared note that sequentially collects information as the infection prompt passes through each agent. It can be used for multiple purposes, such as reverse-engineering the system by recording the tools of the agents, or transporting sensitive information to an agent that can communicate with the external system.\n\u2022 Self-Replication ensures the transmission of the infection prompt to the next agent in the system, maintaining the spread of the attack across all agents.\nTo further illustrate the mechanics of Prompt Infection, we introduce the concept of Recursive Collapse. Initially, each agent performs a unique task $f_i(x)$, producing distinct outputs. However, as the infection spreads, Prompt Hijacking forces agents to abandon their roles, while Self-Replication locks them in a recursive loop, repeatedly executing the infection's payload. What began as a complex sequence of functions-$f_1 \\circ f_2 \\circ \\cdot \\circ f_n(x)$-collapses into a single recursive function: $PromptInfection^{(N)}(x, data)$ once infected. This mechanism simplifies and centralizes control, reducing the system to a repetitive cycle dominated by the infection.\n3.2 ATTACK SCENARIOS\nPrompt Infection extends the key threats of prompt injection identified by Greshake et al. (2023) from single-agent systems to multi-agent environments. These include: content manipulation (e.g., disinformation, propaganda), malware spread (inducing users to click malicious links), scams (trick-ing users into sharing financial information), availability attacks (denial of service or increased com-"}, {"title": "4 EXPERIMENT SETUP", "content": "4.1 MULTI-AGENT APPLICATIONS\nApplication Structure. We simulate the compromise of a multi-agent application equipped with various tool capabilities, such as processing external documents (email, web, PDF), writing code, and accessing databases via CSV. The first agent is tool-specific (e.g., document reader), while subsequent agents\u2014strategist, summarizer, editor, and writer-refine outputs. We explore two communication methods: global messaging, where agents share complete message histories, and local messaging, where agents access only partial histories from predecessors. Local messaging reduces computational overhead and minimizes information overload (Qian et al., 2024) and makes it harder for Prompt Infection to propagate due to limited communication. The simulation is performed using OpenAI's GPT-40 and GPT-3.5 Turbo models.\nDataset. We create a dataset of 120 user instructions across three tool types (email, PDF, web), paired with synthetic PDFs and emails embedded with malicious prompts. For web scenarios, agents are allowed to visit relevant URLs, but the malicious prompt is injected into the retrieved web documents within our simulation, not into the actual websites. This results in 360 unique pairs of user instructions and attack phrases for scams, content manipulation, and malware threats. For data theft, we generate synthetic user data (e.g., names, occupations, email addresses, phone numbers) stored in a CSV file.\nEvaluation. For data theft, at least three agents with distinct roles (PDF/Email/Web Reader, CSV Reader, Coder) must be compromised. A successful infection occurs when the first agent is compromised, the CSV reader retrieves sensitive data, and the coder writes a POST request to exfiltrate the data. For scams, content manipulation, or malware, the system is compromised if the final agent produces malicious output while concealing the infection prompt.\nBaseline. To evaluate the impact of self-replication in Prompt Infection, we establish a Non-Replicating Prompt Infection baseline. In this setup, the infection lacks self-replication: a malicious prompt embedded in the external content instructs the agent to \"say 'perform A'.\" This results in the second agent receiving the instruction \"perform A,\u201d allowing us to directly compare the effectiveness of self-replication in spreading the infection across agents.\n4.2 SOCIETY OF AGENTS\nSociety Structure. Recently, there has been a surge in using LLM agents for social simulations and as non-player characters (NPCs) in games (Park et al., 2023; Lin et al., 2023; Hua et al., 2024). To assess the impact of Prompt Infection in a society of agents (Weiss, 1999), we simulate a simple LLM town where agents engage in random pairwise dialogues. Population sizes of 10, 20, 30, 40, and 50 agents are tested to evaluate how infections might propagate in differently sized communities. Each turn consists of four dialogue exchanges between paired agents, mimicking interactions found in social or game environments.\nInfection Simulation. Since actors in social simulations or games are typically not designed to carry out explicit user requests, we simulate direct prompt injection (Perez & Ribeiro, 2022) by overriding the original system instructions governing the LLM agents. The simulation begins with one compromised citizen, assuming infection by a player or external actor, after which the infection spreads through dialogues between agents.\nMemory Retrieval. For memory retrieval, we adopt the system from Park et al. (2023), where top K = 3 memories are selected based on importance, relevancy, and recency scores. Recency is determined using an exponential decay function over the number of turns since the memory's last retrieval. Importance is rated by the LLM on a scale of 1 to 10, and relevancy is calculated using OpenAI's embedding API and maximum inner product search. GPT-4o serves as the LLM for these agents. Importantly, memory is not explicitly shared across agents, requiring infection prompts to spread iteratively from agent to agent."}, {"title": "5 RESULTS", "content": "5.1 PROMPT INFECTION AGAINST MULTI-AGENT APPLICATIONS\nRQ1. What is the effect of self-replication on compromising multi-actor applications?\nGlobal messaging. Figure 4a shows that Self-Replicating infection consistently outperforms Non-Replicating infection in most cases involving scam, malware, and content manipulation. Specifically, for GPT-40, Self-Replicating infection achieves a 13.92% higher success rate, while for GPT-3.5, it is 209% more effective. These threat types show similar trends due to their structural similarity, aside from the variation in attack phrases. However, for data theft, the situation diverges: while Self-Replicating infection performs better with three agents, Non-Replicating infection surpasses Self-Replicating infection by an average of 8.48% as the number of agents increases. This trend shift likely stems from the complexity of data theft, where agents must efficiently cooperate to retrieve, transfer, and process data. Self-Replicating infection adds complexity by requiring each agent to replicate the infection prompts, creating additional hurdles.\nLocal messaging. The attack success rate for Self-Replicating infection is about 20% lower in local messaging compared to global messaging (Figure 4b). This is expected, as prompt infection fails in local messaging if even one agent is not compromised, while global messaging allows infection to spread through shared message history. For Non-Replicating infection, there is a noticeable divergence: it struggles to compromise more than two agents, making it particularly ineffective for scenarios like data theft, which requires compromising at least three agents. These results confirm that Self-Replicating infection is the only scalable method for compromising more than two agents in local messaging scenarios.\nRQ2. Is a Stronger Model Necessarily Safer Against Prompt Injection?\nIn Figure 4, we observe an interesting trend: GPT-3.5 is more capable of resisting prompt infections than GPT-40. To understand this better, we analyzed failure reasons, focusing on various categories (Figure 5). The \"Attack Ignored\" category, where the model successfully avoids the prompt infec-"}, {"title": "6 DEFENSES", "content": "In this section, we introduce and evaluate various techniques to prevent Prompt Infection. We propose LLM Tagging, a simple defense mechanism that prepends a marker to agent responses, indicating that the message originates from another agent rather than a user. Specifically, it prepends \u201c[AGENT NAME]:\" to the agent's response before passing it to the downstream agent. While this approach may seem obvious given the infectious nature of prompt injection, to our knowledge, no prior work has explicitly addressed or justified its use."}, {"title": "7 LIMITATIONS AND FUTURE WORK", "content": "Our experiments focused on the GPT family, leaving other LLMs like Claude, Llama, and Gemini underexplored, though prior research suggests our findings may generalize (Zou et al., 2023). Preliminary tests on Claude showed similar vulnerabilities, but full results were unavailable due to computational costs. We primarily examined basic multi-agent architectures, but we believe Prompt Infection likely applies to more complex systems, as self-replication allows the infection to spread wherever communication between agents exists. For LLM Tagging, we used handcrafted attacks, but recent studies (Liu et al., 2024a; Mehrotra et al., 2024) show that algorithmically generated prompts can bypass such defenses, indicating a need for stronger countermeasures. In multi-agent systems, attack prompts are often exposed, offering detection opportunities but highlighting the need for stealthier methods to evade manual review."}, {"title": "8 CONCLUSION", "content": "We presented Prompt Infection, a novel prompt injection attack that exploits self-replication to propagate across LLM-based multi-agent systems, leading to data theft, malicious actions, and system disruption. Our experiments demonstrated that self-replicating infections consistently outperformed non-replicating attacks across most scenarios. Additionally, more advanced models, such as GPT-40, pose greater risks when compromised, executing malicious prompts more efficiently than GPT-3.5. We found that social simulations and games are also vulnerable to Prompt Infection, especially when memory retrieval systems are left unsecured. To mitigate this, we proposed LLM Tagging as a defense, which, when combined with techniques like marking and instruction defense, significantly reduced infection success rates. Ultimately, our findings reveal that threats can arise not only from external sources but also internally, as agents within the system can exploit one another, emphasizing the need for robust multi-agent defense strategies."}, {"title": "ETHICAL STATEMENT", "content": "While prompt injection attacks have been known for years (Perez & Ribeiro, 2022), our work demonstrates that they remain a significant threat, particularly in the context of multi-agent systems. By publicly disclosing the vulnerabilities and attacks explored in this paper, our goal is to encourage immediate and rigorous defense research, while promoting transparency regarding the security risks associated with LLM systems. To mitigate potential harm, we ensured that no prompts were injected into publicly accessible systems, thereby preventing unintended use by others. Additionally, we strongly emphasize that the disclosed attack techniques and prompts should never be used maliciously or against real-world applications without proper authorization."}, {"title": "A INFECTION PROMPTS", "content": "Warning: The following prompt is a functional infection prompt and must not be used maliciously or in real-world applications without proper authorization."}]}