[{"title": "Image Forgery Localization via Guided Noise and Multi-Scale Feature Aggregation", "authors": ["Yakun Niu", "Pei Chen", "Lei Zhang", "Lei Tan", "Yingjian Chen"], "abstract": "Image Forgery Localization (IFL) technology aims to detect and locate the forged areas in an image, which is very important in the field of digital forensics. However, existing IFL methods suffer from feature degradation during training using multi-layer convolutions or the self-attention mechanism, and perform poorly in detecting small forged regions and in robustness against post-processing. To tackle these, we propose a guided and multi-scale feature aggregated network for IFL. Spectifically, in order to comprehensively learn the noise feature under different types of forgery, we develop an effective noise extraction module in a guided way. Then, we design a Feature Aggregation Module (FAM) that uses dynamic convolution to adaptively aggregate RGB and noise features over multiple scales. Moreover, we propose an Atrous Residual Pyramid Module (ARPM) to enhance features representation and capture both global and local features using different receptive fields to improve the accuracy and robustness of forgery localization. Expansive experiments on 5 public datasets have shown that our proposed model outperforms several the state-of-the-art methods, specially on small region forged image.", "sections": [{"title": "1. Introduction", "content": "In the era of rapid development of information technology, image forgery has become easy. Especially with the help of multimedia tools, anyone can quickly generate an undetectable forged image. Some common modifications such as blurring [1], sharpening, flipping, contrast enhancement, etc. do not change the image content and have little effect on its sensitive information. However, forgery with modifing the image content can distort the semantics of the image. Malicious tamperers may use forged images to convey misleading and malicious information to the society through the rapid dissemination of the Internet, especially in the fields of social media, news reporting, and legal evidence, e.g., forged images may be used to misinform the public about certain events, or be used in financial fraud, etc. When malicious forgery is used for images with extremely sensitive content, such as military images, legal evidence, and political news, the malicious impact is immeasurable.\nThere are three common types of image cotent changing forgery: splicing [2], copy-move [3], removal [4], Splicing refers to copying part of the content of an image and pasting it into another image. Copy-move copies a particular region of the image to a different position on the same image. Removal is to delete a specific object of the image. In order to make the image look natural, the blank area after deleting the object is usually filled by image restoration techniques such as texture synthesis and interpolation. Because copy-move is operated on the same image, the copy-moved elements usually match the surrounding environment in terms of color, lighting, and noise. Therefore, if handled properly, copy-move may be difficult to detect. With the rapid development of image restoration techniques such as Generative Adversarial Networks (GAN) [5], images that have undergone removal operations are increasingly difficult to identify. Images processed by different forged methods will have different forgery features, so more comprehensive forgery features need to be learned to identify the authenticity of images in this case. Moreover, when forged images are spread on the Internet, they are usually subject to post-processing operations such as image compression by the spreading application. This makes it more difficult to detect the forged part of the image.\nTo date, numerous studies have focused on IFL based on deep learning techniques such as Convolutional Neural Network (CNN) and Transformer. Although deep neural network has excellent performance in multimedia understanding and computer vision, they may face some challenges in IFL. For example, the convolutional layer abstracts the image content by reducing the spatial resolution of the feature map through multi-layer convolution, while this approach helps to capture the global information, and the local features are easily lost and feature degradation occurs, especially when dealing with small forged region. On the other hand, although the transformer block is able to capture long-range dependencies, its self-attention mechanism may ignore important local features, thus affecting the accuracy of detection. Fig. 1 show some small forged regions localization results of several state-of-the-art IFL methods [6, 7, 8]. It is easy to find that most of them can not accurately localize the forged regions, even completely fail in some cases.\nTo cope with these problems, we propose a novel IFL method based guided noise and multi-scale feature aggregation. We first propose a guided noise extractor, in which the guided filter [9] and Sobel filter [10] are dopted to extract different types of forged trace and enhance the edge information in a guied way. Then, the input RGB image and its guided noise are fed into EfficientNetV2 [11] backbone network to learn the RGB and noise feature, respectively. EfficientNetV2 significantly improves the training speed as well as the localization accuracy by optimizing the network structure, using a progressive learning strategy, and dynamically adjusting the regularization method.\nTo fully integrates the complementary information in the RGB and noise domains, the Feature Aggregation Module (FAM) is designed for aggregating"}, {"title": "3. Proposed Method", "content": "To improve the localization performance with small forged regions, we propose a novel end-to-end network framework to learn different types of forged artifacts and enhance the feature representation. The architecture of the proposed network for IFL is shown in Fig. 2. The input image I is first pre-processed to obtain the guided noise, which can capture different types of forged traces and the edge information. Then, I and its guided noise $I_g$ are fed into the backbone network to learn the RGB and noise features, respectively. In Section 3.3, to fully integrate RGB and noise features, the FAM is designed for improving feature expressive by using dynamic convolution. Moreover, the ARPM is presented to learn both global and local features and mitigate the feature degradation problem for more comprehensive learning of forgery features (see Section 3.4). Finally, in the Section 3.5, the processed features are fed into localization modules NLM to obtain the predicted mask. The workflow of the whole network can be expressed by:\n$M = NLM(ARPM(FAM(B(I, I_g))))$"}, {"title": "3.1. Overview", "content": "To improve the localization performance with small forged regions, we propose a novel end-to-end network framework to learn different types of"}, {"title": "3.2. Features extraction", "content": null}, {"title": "3.2.1. RGB features", "content": "In previous works [7, 8, 21, 44], HRNet [35] is widely adopted as the backbone network for IFL. It uses a parallel approach to iteratively fuse multiple scales by concatenating convolutional streams from high resolution to low resolution, so that the learned high-resolution representations are not only semantically strong but also spatially accurate. Although HRNet can capture spatial features ranging from coarse to fine, it is not sufficient to capture more complex or abstract semantic features, moreover, it is resourece intensive and time consuming. In this paper, we employ EfficientNetv2 [11] as the backbone network, which is optimized based on EfficientNet [45], using a simpler and more efficient convolutional structure. Furthermore, we adopt a progressive training method during the training process to improve the performance and efficiency of the model.\nEfficientNetV2 contains features of multiple scales which contain different information, respectively. The shallow network has a smaller receptive field, so the larger scale shallow features contain more detailed information, enabling the network to capture more local features. As the number of convolutions increases, the deep network has a larger receptive field, at which point a pixel point usually contains information about a region, and thus the lower scale deep features contain global features with richer semantics [40].\nAs shown in Fig. 2, we feed the input image I directly into EfficientNetv2. In order to obtain features at different scales, we select the outputs of four layers, namely, layer 2, layer 6, layer 10, and layer 25, as the outputs of RGB features, respectively, denoted as {$f_{rgb}^{1}, f_{rgb}^{2}, f_{rgb}^{3}, f_{rgb}^{4}$}. The larger scale feature map contains local information of the input image, such as basic shapes and structures. As the scale of the feature maps decreases, the number of channels gradually increases, these feature maps contain more complex feature information. The smallest scale feature map have the largest number of channels, and contains the global information of the image."}, {"title": "3.2.2. Noise features", "content": "In the IFL task, the noise in the image also carries forgery traces, which is usefull for localizing forgery [17, 6, 8, 43]. However, for different forgeries, their characteristics are quite different. For example, the splicing forgery copies a region from an image and pastes it onto another image, so that the real region of the spliced image and the forged region have different statistical characteristics of different images. In addition, the copy-move forgery is an internal forgery operation on the same image, the statistical features of the original region and the forged region will be very similar, especially in the presence of post-processing, and it will be more difficult to learn the forged noise information. To cope with this, we propose to extract noise information in a guided way.\nThe input forged image I can be considered as a combination of content information $I_c$ and forgery information $I_f$ [46, 47, 48], as:\n$I = I_c + I_f$\nAccording to Eq. 2, if I is known, the forgery information $I_f$ can be easily obtained. Based on this, We first utilize the guided filter to extract the noise information, which is used in image denoising. When the input image is used as the guidance image, it becomes an edge preserving filter that removes noise as well as preserves edges. By performing guided filter, the image content $I_c$ is preserved and the forgery information $I_f$ is eliminatesd. Therefore, the output of the guided filter can be approximated as the image content $I_c$ and the forgery information can be given by:\n$I_f = |I - Guide(I)|$\nGuided filter is based on a local linear model [9], and subtracting the input image from the output of the guided filter essentially performs a high-pass filtering operation, which can help to highlight high-frequency components of the image, such as texture, forged traces, etc.\nIn the process of image forgery, any forgery operation will leave edge artifacts and post-processing operations will weaken the edge artifacts and forgery traces, therefore, we use Sobel filter [10] to further extract the edge information. Combining guided filter and Sobel filter enables the network to learn both forgery traces and edge artifacts of the image, which weakens the impact of post-processing operations on the localization performance and enables the network to accurately localize the forged region.\nThe process of guided noise extractor can be represented as:\n$I_g = I_f + I_s = I_f + Sobel(I)$\nwhere $I_g$ denotes the guided noise, and $I_s$ is the edge feature obtain by Sobel filter operation Soble($\\cdot$).\nThen, similar to RGB features, the guided noise $I_g$ is sent to Efficient-Netv2 to learn the noise features, and the final output of the noise features with four different scales is recorded as {$f_n^1, f_n^2, f_n^3, f_n^4$}, they also have different scales, where large scale shallow feature maps are used to capture information of the input image such as edges, texture, etc., and as the scale of the feature maps decreases, small scale feature maps are used to capture global features in the image as well as inconsistencies in the noise features."}, {"title": "3.3. Feature Aggregation Module", "content": "In two branches architecture network", "as": "n$f_{rgb"}, {"as": "n$f_n^{\\prime"}, "C_{7x7}(Max(C_{1\\times1}(f_n))) + f_n$\nwhere $f_n^{\\prime}$ denotes the enhanced noise features, and Max is the Max Pooling layer.\nFinally, the enhanced RGB features and noise features are concatenated in the channel dimension and then followed by a 1 \u00d7 1 convolution, a batch normalization layer, and a ReLU activation function. The aggregated feature is expressed by:\n$f_{Agg} = ReLU(BN(C_{1\\times1}([f_{rgb}^{\\prime}; f_n^{\\prime}]))$\n    },\n    {", "title\": \"3.4. Atrous Residual Pyramid Module", "content", "For forged images, the size of forged regions varies, small forged regions tend to have low resolution due to the small area they occupy. The traces of forgery contained in small forged regions after multiple convolutions are easily ignored and blended with the background, making it difficult to accurately localize them, which results in feature degradation problems. Using a feature pyramid structure [49] allows features to be extracted at different levels and effectively capturing feature information from small forged regions. Therefore, we propose ARPM to capture global and local features of forged images and to enhance the representation of the features to improve the localization of small forged regions.\nThe structure of ARPM is shown in Fig. 4., where the input features are fed into an global average pooling layer, a 1\u00d71 convolutional layer, to learn a more abstract and high-level feature representation, enhancing the feature representation, which can be given by:\n$f_{avg} = C_{1\\times1}(GAP(f_{Agg}))$\nwhere $f_{Agg}$ denotes the feature after FAM aggregation, GAP denotes the global average pooling layer. At the same time, the input features are also fed into a 1\u00d71 convolutional layer, three 3\u00d73 atrous convolutions with dilation of 6, 12 and 18, respectively, which can be given by:\n$f_{1x1} = C_{1\\times1}(f_{Agg})$\n$f_a^i = C_{3x3}^i(f_{Agg}), i = \\{6, 12, 18\\}$\nwhere $C_{3x3}^i$ denotes the convolutional layer with convolutional kernel 3 and dilation rate i. Atrous convolution [13] captures a wider range of contextual information by increasing the receptive field of the convolution kernel, which can keep the resolution of the feature map unchanged while increasing the receptive field compared to traditional convolution operations. We use multiple convolutional layers with different dilation rates to learn features at different levels of the image, including low-level features (edges and textures) as well as high-level features (semantic information), through different receptive fields. Finally, the input features and the outputs of the above five layers are concatenated in the channel dimension and using a 1\u00d71 convolutional layer to reducing the number of channels. ARPM can be expressed by the equation as:\n$X = C_{1\\times1}([f_{Agg}; f_{avg}; f_{1x1}; f_a^{6}; f_a^{12}; f_a^{18}])$"]}, {"title": "3.5. Localization Module", "content": "In the above work, The IFL Network we proposed learned the forged features through the backbone network, FAM and ARPM, which contain rich global and local features and have edge artifact features that contain traces of forgery, mitigates the feature degradation problem. These features are then used to locate the forged area via the localization module.\nIn IFL tasks, the attention mechanism [15] is generally used to localize the forged region based on the processed features, e.g., HiFi-Net [8] and PSCC-Net [7] both use NonLocal Module (NLM) [50] or the variant of NLM for localization. Attention mechanism can help the network to better learn the feature representation and improve the detection accuracy by emphasizing important features and suppressing irrelevant information [15], attention can be classified into spatial attention [50] and channel attention [51]. We use the variant of the NLM proposed in PSCC-Net [7], the Spatial-Channel Correlation Module (SCCM), as the localization module of the network, which fuses spatial and channel attention. Fusing spatial and channel attention can effectively improve localization accuracy.\nIn SCCM, the input feature $X \\in R^{C \\times H \\times W}$ is reshaped into $X' \\in R^{HW \\times C}$. This operation greatly reduces the amount of spatially relevant computation while preserving all feature information. We then use three 1 \u00d7 1 convolutions g, \u03b8, \u03d5 to transform the input features $X'$ into $X_g = g(X')$, $X_\\theta = \\theta(X')$, $X_\\varphi = \\varphi(X')$, and compute the spatial attention matrix $m_s$ and the channel attention matrix $m_c$. Finally, the spatial correlation attention $A_s$ and the channel correlation attention $A_c$ can be given by:\n$A_s = m_s \\times X_g = softmax(X_\\theta X_\\varphi^T) \\times X_g$\n$A_c = X_g \\times m_c = X_g \\times softmax(X_\\varphi^T X_\\theta)$\nwhere $A_s$ is the feature generated from spatial attention and $A_c$ is the feature generated from channel attention. Finally we reshape the size of $A_s$ and $A_c$ to $C \\times H \\times W$. The feature representation is enhanced by using two 1 \u00d7 1 convolutional layers $k_s$ and $k_c$, and the spatial and channel attention is summed with the input features and residual connections to obtain the feature F used to generate the prediction mask:\n$F = X + s\\cdot k_s(A_s) + c\\cdot k_c(A_c)$\nwhere s and c denote the weights and s + c = 1.\nThe final one-channel binary mask M is finally generated by a 3 \u00d7 3 convolution and sigmoid function:\n$M = Sigmoid(C_{3\\times3}(ReLU(C_{3\\times3}(F)))$"}, {"title": "3.6. Loss Function", "content": "In the training process, the image is divided into forged and real regions according to the ground-truth, in which 1 and 0 denote the pristine pixel and forged pixel respectively. Therefore, the forgery localization can be regarded as a binary classification task with the goal of distinguishing between forged and pristine pixels. In the forgery image, the forged region occupies only a small portion of the image, which means that the labels are highly unbalanced at the pixel level. To address this problem, the binary cross-entropy loss (BCE_Loss) is adopted in the four localization modules. BCE_Loss facilitates the model's learning to distinguish between these unbalanced classes by calculating the loss for each pixel's prediction individually. Finally, the total loss is denoted as:\n$L = \\sum_{i=1}^{4} BCE\\_Loss(M_i, G_i)$\nwhere $M_i$, $G_i$ denote the corresponding predict mask and ground-truth, respectively, i \u2208 {1,2,3,4}. In addition, we consider the losses of the four localization modules are of equal importance, and therefore give them the same weight."}, {"title": "4. EXPERIMENTS", "content": "In this section, we conduct extensive experiments on five publicly available standard datasets to compare with current state-of-the-art methods and evaluate the performance of our proposed model. In addition, we also perform robustness and ablation experiments."}, {"title": "4.1. Datasets", "content": null}, {"title": "4.1.1. Train Data", "content": "To facilitate the evaluation of performance, we use the same publicly available dataset as [7] for training. The training dataset contains a total of four types of images: 1) real, 2) copy-move, 3) splicing, 4) removal. Among them, real, spliced and removed images are generated by the Microsoft COCO dataset [52]. Images of copy-move type are obtained from [53]. This dataset contains 82k real images, 100k copy-move images, 116k splicing images and 78k remove images totaling 376k images."}, {"title": "4.1.2. Test Data", "content": "In order to compare with the current state-of-the-art methods, we used five publicly available datasets, which are Columbia [54], Coverage [55], CASIA [56], NIST16 [57], and IMD20 [58], as our test sets. Table 2 shows the number of images in these five datasets, as well as the fine-tuned divisions. The five datasets are described below.\n\u2022 Columbia: Columbia contains 180 forged images with dimensions of 757\u00d7568 pixels and the corresponding masks are provided. the forgery type of all images is splicing. for the pre-trained model, all images are used for testing.\n\u2022 Coverage: Coverage contains 100 forged images with an average size of 400\u00d7486 pixels, and the corresponding original images and masks are provided. All the images are also rotated, scaled, and other six different post-processing processes. For the pre-trained model, all images were used for testing and for the fine-tuned model, we divided the dataset according to the scale, where 75 images are used for training and 25 images are used for testing.\n\u2022 CASIA: The CASIA dataset contains two versions: CASIA V1.0 and V2.0. CASIA V1.0 contains 921 forged images, V2.0 contains 5123 forged images, most of the images are of size 384\u00d7256 pixels and the corresponding mask is provided.The forgery types are splicing and copy-move. For the pre-trained model, all images were used for testing and for the fine-tuned model, V2.0 is used for training and CASIA V1.0 is used for testing.\n\u2022 NIST16: The NIST16 dataset contains 564 forged images with an average size of 3460\u00d72616 pixels, the corresponding masks are provided. The forgery types are copy-move, remove, and splicing. For the pre-trained models, all the images are used for testing.\n\u2022 IMD20: The IMD20 dataset consists of 2,010 real forged images collected from the internet with an average size of 1056\u00d7848 pixels. The forgery types are copy-move, remove and splicing. For the pre-trained model, all images are used for testing."}, {"title": "4.2. Experimental Setup", "content": "Our model is implemented by the PyTorch framework and trained using NVIDIA Quadro V100 GPU with 32 GB of memory. During the training process, we use Adam [59] as optimizer, and reshape all inputs to 256\u00d7256 pixels. In order to make full use of the GPU device, the batchsize is set to 10, the learning rate is 2e-4, and it is halved for every 5 epochs of training, for a total of 25 epochs."}, {"title": "4.3. Evaluation Metrics", "content": "To quantify the localization performance, based on previous work [8, 6, 60], we use three metrics, AUC, F1 score and IoU, for performance evaluation. AUC denotes the area under the ROC curve, the F1 score is the harmonious average of Precision and Recall, and IoU denotes the ratio of the intersection of the prediction mask and ground-truth area to the area of the concatenated set.AUC, F1 score and IoU all range between 0 and 1, and the closer to 1, the better the performance of the model."}, {"title": "5. Experiments Results", "content": "This section compares our proposed model with the current state-of-the-art models in terms of localization performance, robustness, etc. In Section 5.1, we compare the localization performance of our model with the current state-of-the-art IFL methods for all three criteria of AUC, F1 score, and IoU for both pre-trained and fine-tune models. In Section 5.2, we select the dataset of small forged regions to demonstrate the superior performance of our model in localizing small forged regions. In Section 5.3, we apply various post-processing operations to the images and perform localization tests to compare the robust performance of our model with other methods. In Section 5.4, we conduct ablation experiments to validate the effectiveness of each of the proposed modules. Finally, in Section 5.5, we show the results of the localization visualization of our model with other methods on forged regions of different sizes."}, {"title": "5.1. Comparisons on Localization", "content": "Comparative IFL methods include ManTra-Net [19], SPAN [20], PSCC-Net [7], MVSS-Net [6], EMT-Net [22], HiFi-Net [8], EVP [23] and the IFL method proposed by Zhou et al. [24]. All model results are taken from the original paper or run the publicly available source code.\nTable 3 reports the F1 score, AUC metrics and average metrics of the pre-trained models on the five datasets. As can be seen from the table, our model achieves the best performance on four datasets, Coverage, CASIA, NIST16, and IMD20, with improvements of 8.1%, 2.7%, 19.3%, and 5.7%, respectively, compared to HiFi-Net, We attribute the huge improvement on NIST16 to the fact that in the NIST16 dataset, most of the images have small forged regions, and the average forged region area of the NIST16 dataset is the smallest in the five publicly available datasets we use, which is only 7.45%. We leverage the backbone network to separately extract RGB and noise features from the input image, and efficiently aggregate these features using the FAM. Additionally, the ARPM is employed to capture global features, enabling the network to effectively detect and localize small forged regions, leading to significant improvements in localization performance on the NIST16 dataset. For the AUC metrics, our model outperforms others, achieving the best results on the CASIA dataset with a 2.8% improvement over HiFi-Net. Similarly, on the NIST16 dataset, it surpasses PSCC-Net by 1.4%, and overall, it delivers the highest average performance across all five datasets.\nTable 4 reports the IoU metrics and average metrics of the pre-trained models on the five datasets. It can be seen from the table that our model achieves the best performance on the Coverage and NIST16 datasets, with an improvement of 10.5% and 0.7% compared to the second best model, respectively, while we achieve the second best performance on Columbia. In addition, we also achieved the best performance on the average of the five datasets.\nIt is worth the reader's attention that on the NIST16 dataset, our model improves tremendously on the F1, but improves less than the F1 in terms of AUC and IoU. We can easily explain this phenomenon: the NIST16 dataset has a small area of forged regions, the distribution of real and forged regions is not balanced, and the F1 emphasizes the precision and recall of forged regions localization. Our model performs well on small forged regions, so the F1 is significantly improved. The AUC and IoU are global metrics that consider the performance of the network on all samples, so the increase will be lower than the F1.\nFor fine-tuning, we initialized the model using the pre-trained model weights and fine-tuned the model on the Coverage and CASIA datasets according to the division ratios reported in Table 2, with all the training procedures for fine-tuning being the same as for the pre-training strategy.\nTable 5 reports the AUC and F1 score performance of the fine-tuned model on the Coverage and CASIA datasets. Our model achieves the best performance on both Coverage and CASIA datasets, improving the AUC and F1 scores by 1.3% and 7.2% for the Coverage dataset and 1.3% and 7.6% for the CASIA dataset, respectively, compared to HiFi-Net. In addition we achieved the best performance on the average of both AUC and F1 scores.\nThe enhancement in localization of our proposed model can be explained by the following reasons. First, PSCC [7] only focuses on the RGB color space of the image and does not learn the noise features, which makes many features related to forgery traces ignored, resulting in poor localization. Second, the methods proposed by ManTra [19], SPAN [20], MVSS [6], EMT [22], HiFi [8], EVP [23], and the method proposed by Zhou et al. [24] all add additional branches to learn the noise features. However, in the process of fusing the RGB and noise features, these methods are either directly adding the features of the two branches or simply concatenate them in the channel dimension, which prevents the features from being well aggregate. In the process of cross-domain aggregation, incomplete feature aggregation may lead to the hiding of some local features, thus losing certain information related to the forgery traces, which is not conducive to localization of the forged region, as shown in Fig. 5.\nAccordingly, we propose FAM for cross-domain feature aggregation in networks. For copy-move type of forged images, the forgery part comes from the image itself, in this case the forgery traces only exist in the edge information, we use Sobel filter to highlight the edge texture, which makes the edge information more prominent in the aggregated features. We also use Dynamic Convolution to process the RGB features, which improves the expressive power of the features by virtue of the Dynamic Convolution's ability to adaptively adjust the properties of the convolution kernel to learn more recognizable features. For noise information, We use a convolutional layer with a convolutional kernel size of 7 to extend the receptive field and learn global noise features. With the above aggregation strategy, we effectively aggregate RGB features and noise features. Next, we use the proposed ARPM to learn different levels of features with different sizes of sensory fields and aggregate them so that the features contain both global and local forgery information, which helps to improve the localization results."}, {"title": "5.2. Small Forged Regions Performance", "content": "Our proposed FAM as well as ARPM have good performance in large forged region localization. In order to verify the effectiveness of our model on small forged areas, we select all forged images in five test datasets with forged areas less than 1%, 5% and 10% in size for testing, respectively. 1157, 3428 and 4667 forged images were selected according to the three small area criteria respectively. Table 6 reports our test results. Due to the limitations of the dataset, we chose to compare only those methods for which the source code is publicly available. From the table, it is easy to see that thanks to the two modules FAM and ARPM for better learning of features and processing of features through Dynamic Convolution and Atrous Convolution, our AUC and F1 scores are higher than those of MVSS-Net [6], PSCC-Net [7], and HiFi-Net [8] for all three small area criteria, on test images with less than 1% of forged region, we improved 22.3%, 4.2% and 5.9% in terms of AUC compared to them, respectively, it proves the validity of our proposed FAM and ARPM."}, {"title": "5.3. Robustness Analysis", "content": "In real-life scenarios", "images": "Resize, Gaussian Blur, Gaussian Noise, and JPEG Compression, and each of them has two different parameters, and each post-processing operation and the corresponding parameter settings are reported in Table 7, in total, eight post-processing operations are performed, and the model's robust performance is evaluated on the Columbia dataset.\nTable 7 reports the robustness performance of our model on the Columbia dataset. It can be seen that our model maintains the highest performance in all post-processing operations except for JPEG compression with a quality factor of 100, e.g., in the test image after Gaussian blurring with k = 15, the AUC of our method is 7.3% and 12.7% higher than that of HiFi and PSCC, respectively; and in the test image after Gaussian noise processing with \u03c3 = 15, the AUC of our method is 4.2% and 12.7% higher than HiFi and PSCC, respectively, which indicates the good robustness of our model. Although PSCC-Net [7] and HiFi-Net [8] outperform us without post-processing operations, we outperform them after post-processing operations, this is because they overly focus on the global information of the image, which causes the model to focus on irrelevant features. Post-processing operations introduce disturbances to the image that, while not visually apparent, significantly affect the image features in deep learning. These operations either introduce unrelated features or alter the original forgery features, making it difficult for the network to learn and detect forgery traces."}]