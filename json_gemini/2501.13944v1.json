{"title": "Fanar: An Arabic-Centric Multimodal Generative AI Platform", "authors": ["Ummar Abbas", "Mohammad Shahmeer Ahmad", "Firoj Alam", "Enes Altinisik", "Ehsannedin Asgari", "Yazan Boshmaf", "Sabri Boughorbel", "Sanjay Chawla", "Shammur Chowdhury", "Fahim Dalvi", "Kareem Darwish", "Nadir Durrani", "Mohamed Elfeky", "Ahmed Elmagarmid", "Mohamed Eltabakh", "Masoomali Fatehkia", "Anastasios Fragkopoulos", "Maram Hasanain", "Majd Hawasly", "Mus'ab Husaini", "Soon-Gyo Jung", "Ji Kim Lucas", "Walid Magdy", "Safa Messaoud", "Abubakr Mohamed", "Tasnim Mohiuddin", "Basel Mousi", "Hamdy Mubarak", "Ahmad Musleh", "Zan Naeem", "Mourad Ouzzani", "Dorde Popovic", "Amin Sadeghi", "Husrev Taha Sencar", "Mohammed Shinoy", "Omar Sinan", "Yifan Zhang", "Ahmed Ali Yassine El Kheir", "Xiaosong Ma", "Chaoyi Ruan"], "abstract": "We present Fanar, a platform for Arabic-centric multimodal generative AI systems, that supports language, speech and image generation tasks. At the heart of Fanar are Fanar Star and Fanar Prime, two highly capable Arabic Large Language Models (LLMs) that are best in the class on well established benchmarks for similar sized models. Fanar Star is a 7B (billion) parameter model that was trained from scratch on nearly 1 trillion clean and deduplicated Arabic, English and Code tokens. Fanar Prime is a 9B parameter model continually trained on the Gemma-2 9B base model on the same 1 trillion token set. Both models are concurrently deployed and designed to address different types of prompts transpar- ently routed through a custom-built orchestrator. The Fanar platform provides many other capabilities including a customized Islamic Retrieval Augmented Generation (RAG) system for handling religious prompts, a Recency RAG for summarizing information about current or recent events that have occurred after the pre-training data cut-off date. The platform provides additional cognitive capabilities including in-house bilingual speech recognition that supports multiple Arabic dialects, voice and image generation that is fine-tuned to better re- flect regional characteristics. Finally, Fanar provides an attribution service that can be used to verify the authenticity of fact based generated content.\n\nThe design, development, and implementation of Fanar was entirely undertaken at Hamad Bin Khalifa University's Qatar Computing Research Institute (QCRI) and was sponsored by Qatar's Ministry of Communications and Information Technology to enable sovereign AI technology development.", "sections": [{"title": "1. Introduction", "content": "A case for Arabic-centric Large Language Models is made including the limitations of obtaining Arabic data and the distinctive characteristics of the language.\n\nLarge Language Models (LLMs) and Generative AI are becoming an integral part of day-to-day activities at the personal and enterprise level due to their ability to carry out multifaceted language and cognitive tasks. Diverse applications, including writing assistants, translation services, customer support, software development and image generation, are proliferating and being offered as human productivity enhancing tools. While failure cases of LLMs tend to become viral, continued growth in their adoption is an indicator of their practical utility. From a computer science and AI research perspective, LLMs and their multimodal extensions have opened up scientific challenges that will continue to drive innovation in the research community. An important practical challenge that is far from being overcome is the design and engineering of high-quality and effective LLMs for non-English languages. The major bottleneck for non-English languages is the limited availability of large datasets which are currently necessary to match the performance of English-centric models. As is well known, the most ubiquitous language on the Web, from where most data is harvested, is English. The latest statistics from Common Crawl, a non-profit organization that takes a complete snapshot of the Web approximately once a month, show that English documents constitute 46% of all textual web content, while other languages cap at around 6%. Arabic, the spoken language of more than 400 million people and the official language of over 20 countries, constitutes around 0.5% of web data\u00b9 (Rana, 2010). Besides lack of data, another big hurdle is the cost of building an LLM, particularly from scratch, in terms of both the required hardware and technical expertise. The geopolitical environment is trending towards a polarized world where access to high-end GPUs is often constrained and even when hardware is available, the monetary cost of training even a moderate size LLM in a reasonable amount of time can be prohibitive and out of reach for many organizations. Finally, substantial scientific and engineering expertise is required to undertake an LLM building exercise. While many organizations that have built private and open source LLMs release technical reports about their experience, in many cases, technical details are often left out making it difficult to replicate the process of building an LLM from scratch. A significant amount of deep knowledge across the entire LLM stack, ranging from data collection and cleaning, to pre-training, post-training, and deep computer systems knowledge is therefore required.\n\nIn this work, we introduce Fanar (meaning lighthouse in Arabic), an Arabic-centric Generative AI plat- form that includes text-based LLMs, speech and image generation systems, specialized Retrieval Aug- mented Generation (RAG) modules, and an attribution service to authenticate and correct facts in generated text. At the center of Fanar are Fanar Star and Fanar Prime two 7B and 9B parameter LLMs respectively that are trained on nearly 1 trillion clean and deduplicated Arabic, English and Code tokens. Fanar Star was trained from scratch while Fanar Prime was continually pre-trained on the Gemma-2 9B model on the same 1 trillion data set. The two models work in concert and make up for the lack of Arabic data, especially in technical domains. Additionally, Fanar includes a new customized morphologically aligned Arabic tokenizer as well as benchmarks that evaluate cultural capabilities that will be of independent interest to the Generative AI research community.\n\nThe rest of this report is structured as follows. In Section 2, we give a brief overview of the Arabic language, its footprint, dialects and setup the socio-linguistic context. A high level overview of the major components of the Fanar platform and how they interact with each other is provided in Section 3. The efforts to collect, clean and integrate a large Arabic data set of nearly 1 trillion tokens are detailed in Section 4, followed by the design of our specialized Arabic tokenizer in Section 5. The model architectures of both Fanar Star and Fanar Prime and the pre-training pipeline are the subject of Section 6. A comprehensive overview of post-training steps and the necessary adaptations for Arabic are then provided in Section 7. The performance of both models on standardized and new culturally aware benchmarks is described in Section 8. Arabic speech services and the regionally representative image generation capabilities of Fanar are introduced in Section 9. The Retrieval Augmented Generation (RAG) systems for Islamic content, recent events and important biographies for fact-related response attribution are the focus of Section 10. Section 11 concludes with a reflection on building a large scale GenAI system and a discussion on a future road map for Fanar."}, {"title": "2. Arabic Language", "content": "An overview of the Arabic language and its distinctive characteristics is provided, motivating the need for developing language technologies specifically tailored for Arabic.\n\nThe Arabic language, a member of the Semitic language family, is spoken by over 310 million people across the Middle East, North Africa, and the Arabian Peninsula (MENA region) (Horesh and Cotter, 2016), and by more than 467 million individuals across 60 countries worldwide (Gregory et al., 2021). Beyond its geographic and linguistic reach, Arabic carries immense spiritual significance as the liturgical language of over 2 billion Muslims who engage in daily prayers and religious practices in Arabic. It is the official or co-official language of 25 countries and holds a prominent place in linguistic and cultural studies due to its complexity and global significance (Gregory et al., 2021). Arabic exists as a spectrum of linguistic forms, ranging from Classical Arabic, which is used primarily in religious and classical literary texts, and Modern Standard Arabic (MSA), used in formal settings and communications, to a wide array of colloquial dialects spoken across the MENA region and widely used on social media platforms. The rich diversity and significance of Arabic, its complex grammar and structure, and the colloquial-nuances make it both a fascinating and challenging for language technology development (Farghaly and Shaalan, 2009; Habash, 2010).\n\nAt the core of Arabic's linguistic system lies a derivational morphology system where words are typically derived from roots that are fit into morphological templates and accept prefixes and suffixes. Roots, typically composed of three consonants (though occasionally four or five), encode fundamental semantic meanings. These roots combine with specific morphological patterns to produce words that convey nuanced meaning.\n\nFor example, the root k-t-b generates words such as kitAb (\u2018book'), maktabap (\u2018library'), kutub ('books'), kAtib ('writer'), maktwb (\u2018written'), and many other nouns and adjectives, as well as verb conjugations that account for tense, gender, and person2 (Watson, 2002). This non-linear system and Arabic's intricate inflectional paradigms, including the use of prefixes and suffixes that mark tense, mood, gender, and number and attaches determiners and pronouns, distinguishes it markedly from Indo-European languages. While Arabic derives a vast array of words from a limited set of roots, Indo-European languages such as English typically rely on concatenative morphology and a more extensive lexicon to achieve similar flexibility (Ferguson, 1959; Watson, 2002; Mustafa et al., 2017; Alolaywi, 2022). Arabic morphology allows for words such as \u0648\u0628\u0643\u062a\u0627\u0628\u0647\u0645 wabikit Abihim (and with their book') and \u0641\u064e\u0623\u064e\u0633\u0652\u0642\u064e\u064a\u0652\u0646\u064e\u0627\u0643\u064f\u0645\u064f\u0648\u0647\u064f fa'asqayn\u0101kum\u016bhu ('And We gave it (water) to you to drink.').\n\nThe sociolinguistic phenomenon of diglossia further complicates the computational processing of Arabic. Modern Standard Arabic (MSA), derived from Classical Arabic, serves as the formal language for media, education, and political discourse. However, it is not the native language of any Arab speaker. Instead, native speakers use regional dialects, which differ significantly from MSA in phonology, syntax, and lexicon (Ferguson, 1959). These dialects are influenced by other languages, such as English, Berber, French, Persian, Turkish, and Aramaic, and can often be mutually unintelligible (Farghaly and Shaalan, 2009; Al-Wer and de Jong, 2017).\n\nAnother defining feature of Arabic is its script, which is derived from the Nabataean alphabet (Healey and Smith, 2012). The script, similar to other Semitic scripts, is written from right to left, and its adaptability allows it to represent unrelated languages such as Persian and Urdu. However, in most languages that use the Arabic script, short vowels are not explicitly represented, necessitating diacritization for accurate interpretation. This introduces challenges for NLP tasks, particularly those requiring diacritization or semantic disambiguation (Alnosairee and Sartini, 2021; Mubarak and Darwish, 2014b)."}, {"title": "3. Fanar Platform Services", "content": "Fanar platform services are introduced including Fanar Star and Fanar Prime, speech and image generation capabilities and RAG features\n\nThe Fanar platform is organized into a set of services coordinated through an Orchestrator as shown in Figure 1. Requests from the Chat App or an API are either sent directly to speech and translation services or passed through a safety filter and then classified to be processed by other LLM-driven services. Prompts for image generation are also first passed through a safety filter. We briefly summarize the core services that make up the Fanar family.\n\nFanar Star: This is the flagship 7 billion-parameter LLM trained entirely from scratch using a metic- ulously designed two-stage curriculum approach. This model leverages a refined implementation of the decoder-only Transformer architecture (Vaswani et al., 2017), inspired by the architectural principles of OLMO (Groeneveld et al., 2024) and the LLaMA family (Touvron et al., 2023). The pre-training process begins with a multi-epoch phase comprising two initial epochs over a diverse corpus of 1 trillion tokens, distributed across Arabic (40%), English (50%), and programming code (10%). In subsequent epochs, the token count is reduced to 0.8 trillion through targeted filtering using an education classifier, with an increased focus on Arabic content (50%), accompanied by proportionate adjustments for English (40%) and code (10%). The pre-training concludes with a carefully designed cool-down stage, incorporating an additional 100 billion tokens from high-quality datasets curated in-house and gradually diminishing the learning rate to zero. Detailed descriptions of the model architecture and pre-training recipe are pro- vided in Section 6. Fanar Star undergoes a comprehensive post-training stage consisting of Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) (Rafailov et al., 2024) to achieve robust alignment with safety and ethical considerations, as detailed in Section 7. The deployment architecture incorporates an orchestration mechanism whereby prompts other than Islamic or STEM domains, as determined by a specialized classifier, are routed to Fanar Star.\n\nFanar Prime: This model uses continual pre-training to build upon the Gemma-2-9B base model (Riv- iere et al., 2024), which was itself initially pre-trained on 8 trillion tokens using knowledge distillation from a larger model. Our approach begins with strategic vocabulary pruning, reducing the original 250,000-token vocabulary to 128,256 tokens to optimize compatibility with our training data. The model comprises 459 million embedding parameters and 8.32 billion non-embedding parameters, totaling 8.78 billion parameters. The continual pre-training process for Fanar Prime mirrors the two-stage curricu- lum strategy employed for Fanar Star but is limited to a single epoch, followed by a cool-down stage. The data mixture and filtering criteria are aligned with those of Fanar Star ensuring a balanced rep- resentation of Arabic, English, and code content. Post-training of Fanar Prime aligns closely with the methodology applied to Fanar Star including SFT and DPO for safety and value alignment. During or- chestration, Fanar Prime is designated to handle STEM and reasoning-related prompts, routed through the specialized classifier.\n\nSpeech Recognition (SR): Fanar enables natural interaction through speech - the most effortless and natural form of human communication. The orchestrator integrates a state-of-the-art Arabic-English Automatic Speech Recognition (ASR) system. The advanced ASR system supports - (i) multiple Arabic dialects, e.g., Egyptian, Gulf and Levantine; (ii) non-native Arabic accents; and (iii) diverse code- switching scenarios, including both dialectal variations within Arabic (e.g., Modern Standard Arabic (MSA) \u2192 Egyptian dialect (EGY)) as well as seamless transitions between English and Arabic (Ar \u2192 En). These capabilities collectively empower Fanar to accommodate dialectal Arabic speakers and foster inclusivity for non-native Arabic speakers. Details of ASR are provided in Section 9.\n\nText-to-Speech (TTS): To enable better accessibility, the Platform integrates Arabic and English text-to-speech systems. The TTS systems leverage Diffusion Transformer with ConvNeXt V2 (Chen et al., 2024b) for better text-speech alignment during in-context learning, without the extra modules like grapheme/phoneme alignment, duration predictor, text encoder, or any aid of codec for semantic information infusion. For details, see Section 9.\n\nImage Generation (IG): The Platform provides support for image generation that is aligned for reflecting Arab and Islamic preferences. The Stable Cascade model is used as it has a much smaller latent space compared to the well known Stable Diffusion model and its variants and is optimized for both faster fine tuning and inference. In the current landscape of state-of-the-art (SOTA) image models, such as Stable Cascade, biases are evident when generating images from neutral prompts. These models predominantly depict elements of Western cultures, including people, cuisine, and scenery. Additionally, there is a notable lack of accurate representation when generating images related to Middle Eastern topics. This includes details such as culturally appropriate attire, diverse skin tones, and iconic regional landmarks. Our approach for fine-tuning image generation to reflect local cultural values is provided in Section 9 along with concrete examples.\n\nRetrieval Augmented Generation (RAG): A RAG system retrieves relevant information from ex- ternal data sources for a given input prompt which can then be passed as contextual information to the LLM (Lewis et al., 2020). By grounding the generated response on the provided context, it can help improve the accuracy of generated responses (Ram et al., 2023). Fanar currently provides four RAG systems for controlled content generation in specific domains. These are: (i) Attribution RAG for pro- viding supporting evidence (references) for fact-related queries. For example, if the prompt is \"What is the length of the river Nile?\" the response will be validated against Wikipedia and corrected if there is a mismatch; (ii) Recency RAG for information that is post the checkpoint date of the pre-training corpus. As an example, for the prompt \"What is the latest weather in Doha?\" the system will extract informa- tion from selected verified websites and summarize the information; (iii) Islamic RAG provides content from authoritative websites for Islam related prompts; and finally (iv) Biography RAG for ensuring that accurate information is generated for well known people in the region and beyond. More details about these four RAG systems are provided in Section 10.\n\nTranslation: Fanar provides a specialized service for translation from Modern Standard Arabic (MSA) to other Arabic dialects and directly from English to the dialects. As parallel data in this space is highly limited, we fine-tune an existing sequence-to-sequence transformer model for dialectal translations. Benchmarking details are provided in Section 4.5.1. The translation systems build upon 15 years of expertise within QCRI in MSA and dialectal Arabic translations."}, {"title": "4. Pre-training Data", "content": "Pre-training data composition for Arabic, English and code is presented. Data filtering pipeline, especially for Arabic is described. The role of machine translation to expand coverage of Arabic data is highlighted.\n\nData is a critical building block for modern AI systems in general and for LLMs in particular. We describe the pre-training data composition for both English and Arabic and the use of machine translation to augment both MSA and dialectal Arabic data. Given the scarcity of Arabic data, our syntactic and semantic filtering and cleaning approaches are more nuanced compared to English.\n\nTo pre-train Fanar, we curated a dataset comprising of 1 trillion tokens spanning Arabic, English, and computer code. The tokens were sourced from diverse origins, including web documents, scientific arti- cles, encyclopedic entries, mathematical problems, books, news articles, and source code from common programming languages. The diversity of data is instrumental in enabling the model to exhibit robust performance across a wide array of tasks. Detailed distribution of the data sources for each domain are presented in Figure 2, while Table 1 outlines the token counts for these sources. Recognizing that differ- ent corpora contribute uniquely to model training, we aimed to balance corpora that enhance language understanding, facilitate knowledge reasoning, and improve task-specific performance. We employed rig- orous preprocessing, including data cleaning, filtering, and deduplication, to ensure the quality of the data. The final data mixture was determined based on extensive ablation studies and are described in Section 5.\n\nThe English component of our pre-training dataset encompasses approximately 513 billion tokens, derived from a diverse range of sources many of which have been utilized in other representative LLMs. These include: (i) web documents from preprocessed Common Crawl sources, including C4 (Raffel et al., 2020), RefinedWeb (Penedo et al., 2023), DCLM (Li et al., 2024), Dolma (Soldaini et al., 2024) and RedPajama (Weber et al., 2024) ensuring broad web-based content representation, (ii) scientific documents derived from RedPajama-arXiv and PeS2o (Soldaini and Lo, 2023) datasets, (iii) social media data extracted from Pushshift Reddit dataset (Baumgartner et al., 2020) to capture conversational and informal language patterns, (iv) mathematical data from sources like Algebraic-Stack (Azerbayev et al., 2024) and Open-Web-Math (Paster et al., 2024) datasets to capture complex reasoning and mathematical language, (v) books from public domain book data from Project Gutenberg via Dolma corpus and (vi) encyclopedic content from Wikipedia dumps and MegaWika (Barham et al., 2023) for structured, factual information.\n\nRecognizing the limited availability of high-quality Arabic pre-training data, we curated an extensive set of 410 billion Arabic tokens. The data spans multiple varieties of Arabic, including MSA, Classical Arabic, and dialectal content encompassing a diverse range of sources: (i) web documents that were crawled in-house and preprocessed for quality, (ii) news articles and books spanning literature, religion, politics, culture and history, (iii) encyclopedic content from Arabic Wikipedia (iv) classical and contemporary Arabic poetry and (v) in-house machine translated books, STEM papers and encyclopedic documents to ensure English-Arabic language alignment.\n\nOur code data subset comprises approximately 102 billion tokens, representing around 10% of the pre- training dataset. This subset was sourced primarily from The Stack (Kocetkov et al., 2023), a collection of permissively licensed GitHub projects across a large number of programming languages. We sub-selected data from common programming languages in The Stack, including Python, C, C++, Java, Go, and JavaScript. Additionally, we included Markdown and GitHub Issues data to provide contextual code understanding.\n\nThe Arabic data (both web and curated) is collected from a multitude of sources, each having a different file format (e.g., txt, HTML, XML, JSON, zip) and text granularity (e.g., lines, paragraphs, articles, complete books, poems). We homogenize these varying formats into the format used by datasets such as Dolma (Soldaini et al., 2024). Files are collections of JSON records 3 with the following fields: \"id\" which serves as a unique identifier; \"text\" which contains the core text content at the granularity of the original data; \"metadata\" which contains any additional information about the record such as creation\ndate and source URL, and \u201cquality_signals\" capturing a set of quality scores collected at the record level, which are described in more detail in Section 4.3. All text goes through a simple cleaning process where all HTML and JavaScript tags are removed and white spaces (including tabs, newlines, trailing escape characters) are normalized.\n\nAs the data comes from a variety of sources, it contains a lot of noise and low quality content. We do not want to train the model on low quality data as it would adversely affect the quality of generation. As the data does not come with any pre-existing score of quality, we utilize a filtering pipeline that only retains high quality data based on their syntactic and semantic characteristics. This pipeline extends and tailors existing work on data filtering to the Arabic language.\n\nExisting Arabic LLMs, such as Jais (Sengupta et al., 2023), apply hard-coded cutoffs based on heuristics to judge the quality of a given text and filter it out if it fails to pass certain thresholds (e.g, special symbols should not exceed 20% of the content). In our approach, we implemented 20 of the most widely used quality signals described in the RedPajama datasets. These heuristic-based quality signals determine the quality of a given text. They cover a variety of measures such as the number of sentences, the number of words, the ratio of symbols and punctuation to words among others. We also removed records with insufficient amounts of Arabic.\n\nWe modified all quality signals to handle Arabic texts properly. Some examples include adding right- to-left punctuation marks and considering digits written in Arabic/Hindi alphabets, diacritics, ligatures, special symbols, Farsi and decorated characters. Other quality signals in RedPajama are handled as part of the deduplication and model-based filtering described in the next subsections.\n\nFurthermore, rather than determining the cutoff thresholds for each of these quality signals in an ad-hoc manner, we utilize a systematic approach. For a given quality signal X, we divide X's score, which is typically between 0 and 1, into 10 histogram ranges with a fixed bin width of 0.1, and then distribute the dataset records based on their scores to one of the 10 buckets. Then, by manually investigating random hundred samples from each bucket, we make more informative decisions on the cutoffs to apply to all datasets. For instance, we observed that setting the threshold for the fraction of unique words in the content to 0.2 effectively identifies a significant portion of advertisement content due to its repetitive nature.\n\nExisting web datasets, such as C4 (Dodge et al., 2021), process CommonCrawl data using uniform filtering rules across all languages. We hypothesize that while these rules may be suitable for English, they may not be as effective for Arabic. For instance, one such filter excludes web pages containing fewer than three paragraphs, each with a minimum of 200 characters. To address this, we adjusted the filtering rules based on empirical observations from actual Arabic data, as previously described.\n\nTo remove unwanted content from web data, we used the ASAD system (Hassan et al., 2021) to detect offensive and profane language, and adult content. We plan to increase the accuracy of hate speech detection in ASAD and use it to filter out hateful content. In addition, we sampled 20 articles from the most common domains from web data and gave them to annotators to estimate the quality and usefulness of each domain.\n\nRecent progress on data preparation for LLM training (Li et al., 2024) has shown the advantage of data filtering using models that predict data quality. While our syntactic and semantic filters removed the majority of noisy data, there is some additional data such as Ads and SEO content that passed the first two filtering phases and require advanced filtering. We explored several approaches in this direction. We introduced two model-based filters that we used for the preparation of pre-training data:\n\n\u2022 Perplexity filtering: We used KenLM models which are probabilistic n-gram language models for fast perplexity estimation (Heafield, 2011). These models are trained on Wikipedia content for several languages. For our perplexity filtering, we used a pre-trained model for Modern Standard Arabic (MSA)6. We assessed the perplexity distributions and defined a threshold per dataset to filter data with the highest 5% perplexity. In the last training epoch, an additional perplexity filtering is applied to remove low perplexity content. At that training stage, these documents are becoming too easy and do not give high learning opportunity for the LLM.\n\n\u2022 Filtering using Education classifier: We build an education classifier following the approach in- troduced in FineWeb-Edu for English Web data (Lozhkov et al., 2024). In Fine-Web-edu-ar, a translation for FineWeb-Edu to Arabic is proposed. A small NLLB-500M model is used to trans- late FineWeb-Edu. The data quality in Fine-Web-edu-ar is poor due to the use of small Machine Translation model. Alternatively, we construct a native Web education Arabic dataset: First we sampled randomly 1M documents from our web corpus. We used an Qwen-2.5-73B-Instruct to annotate these documents for the classifier training (Qwen Team, 2024). The LLM is prompted to assign a score between 0 and 5 reflecting the richness of the document in educational content. We used the same annotation prompt as for FineWeb-Edu. Minor adjustment to the labeling prompt are made to accommodate the Arabic content. In order to train the classifier, we selected a multi- lingual embedding that is good for Arabic language (Chen et al., 2024a). We trained a classification head on top of the embedding to score the education level of web documents. The average accuracy of the classifier on a validation set is about 70% which is comparable to the results on English Fine Web-Edu classifier. We used the education classifier filter low education score with values 0 and 1. Inspection of the low education content revealed documents several unfiltered Ads and adult content. The filtered data correspond approximately to 20% of the data.\n\nData deduplication is important in maintaining and controlling the high quality of the data. As such, the problem of scalable near duplicate detection and its benefit in training LLMs has been explored extensively. (Lee et al., 2022; Broder, 1997; Logasa Bogen et al., 2013; Elmagarmid et al., 2006) In Fanar, we apply two types of data deduplication, namely exact-match dedup, and approximate-match dedup. Additionally, we perform URL deduplication on all web-based data. We implemented a pipeline that can scale well to large datasets under limited compute and memory. We perform both inter-dataset and cross-dataset deduplication.\n\nExact-Match Deduplication: We implement a hashmap based approach to put similar objects in the same bucket. We control the number of buckets such that each one on average is around 10GB to 20GB and peak memory usage does not exceed 1TB. Then, in a parallel fashion, each bucket is processed by sorting its records and eliminating duplicates. We then reconstruct the files in the same order as the original ones using a merge-sort operation utilizing the unique id of each record.\n\nApproximate-Match Deduplication: Exact-match deduplication, although computationally efficient, it has its own limitations. For the approximate-match deduplication (also known as fuzzy deduplication), we adopt the same approach used in other datasets (Shen et al., 2024; Tokpanov et al., 2024; Brown et al., 2020; Zeng et al., 2021), which is the min-wise locality sensitive hashing LSH technique (Broder, 1997). We experimented with various parameter configurations, and converged to the following parameters, gram size is set to 8, the number of bands (b) is set to 12, and band length (r) is set to 11 which results in the approximated Jaccard similarity threshold being 0.8 and signature length as 132. The algorithm is highly scalable and we used 350 CPU cores with a peak memory usage of 1TB and completed in about 12 hours.\n\nLeveraging over a decade of advancements in Arabic NLP, QCRI has consistently set benchmarks in machine translation and resource creation for Arabic and its underrepresented dialects. Our Shaheen machine translation system (Sajjad et al., 2017b) exemplifies this legacy, offering a state-of-the-art pre- cision in translating between English and Modern Standard Arabic (MSA). This expertise enabled us to tackle the dual challenge of creating robust datasets and training state-of-the-art Fanar LLM addressing both linguistic and cultural nuances.\n\nLarge Language Models (LLMs) have demonstrated impressive capabilities, primarily due to their exten- sive size and the diversity of the data they are trained on. However, this advantage is heavily skewed towards high-resource languages like English, leaving low-resource languages, such as Arabic, at a signifi- cant disadvantage. To bridge this gap, researchers and practitioners have increasingly turned to synthetic data generation methods, with Machine Translation (MT) emerging as a prominent approach. By trans- lating existing English datasets into low-resource languages, MT facilitates the creation of larger and more diverse datasets, thereby enhancing the ability and performance of LLMs for these underrepre- sented languages. This approach not only expands linguistic resources but also enables access to rich knowledge repositories, including scientific literature, encyclopedias, and other genres that are predomi- nantly available in English, enriching the breadth and depth of training data for LLM.\n\nTo initiate our exploration of MT systems, we benchmarked several open-source and commercial solutions, including state-of-the-art models such as shaheen (Sajjad et al., 2017b), mbart (Liu et al., 2020), madlad400 (Kudugunta et al., 2023), helsinki (\u00d6stling et al., 2017), nllb 3.3B (Team et al., 2022), gpt4 (OpenAI et al., 2024), and S-T5 based on Ara-T5 (Nagoudi et al., 2022a).7 These systems were evaluated using AraBench (Sajjad et al., 2020), which provides a diverse range of test sets spanning\nDespite its rich diversity, dialectal Arabic remains significantly underrepresented in Large Language Models (LLMs). To address this gap during the development of Fanar, we developed English-to-Dialectal Machine Translation models, complemented by human post-editing, to create robust evaluation bench- marks. These benchmarks facilitate translation between Modern Standard Arabic (MSA) and two major"}, {"title": "5. Tokenization", "content": "The importance of tokenization in LLMs is explained. The limitations of BPE type tokenizers for Arabic are explained. A new morphologically aware tokenizer algorithm is introduced.\n\nTokenization is a foundational step in any natural language processing (NLP) pipeline", "limitations": "i) its greedy algorithm, (ii) inefficiencies in cross-lingual settings where similar words may use different character variations, and (iii) amount of character information is not equal in different languages. These shortcomings have spurred modifications, such as BPE dropout (Provilkov et al., 2020), sampling-based BPE (Asgari et al., 2019, 2020), byte-level extensions (Wang et al., 2020), multilingual BPEs (Liang et al., 2023).\n\nThe additive nature of BPE makes it well-suited for suffixing lan- guages like English. However, languages such as Arabic present unique challenges due to their"}]}