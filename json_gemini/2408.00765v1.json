{"title": "MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities", "authors": ["Weihao Yu", "Zhengyuan Yang", "Linfeng Ren", "Linjie Li", "Jianfeng Wang", "Kevin Lin", "Chung-Ching Lin", "Zicheng Liu", "Lijuan Wang", "Xinchao Wang"], "abstract": "MM-Vet, with open-ended vision-language questions targeting at evaluating integrated capabilities, has become one of the most popular benchmarks for large multimodal model evaluation. MM-Vet assesses six core vision-language (VL) capabilities: recognition, knowledge, spatial awareness, language generation, OCR, and math. However, its question format is restricted to single image-text pairs, lacking the interleaved image and text sequences prevalent in real-world scenarios. To address this limitation, we introduce MM-Vet v2, which includes a new VL capability called \u201cimage-text sequence understanding\u201d, evaluating models' ability to process VL sequences. Furthermore, we maintain the high quality of evaluation samples while further expanding the evaluation set size. Using MM-Vet v2 to benchmark large multimodal models, we found that Claude 3.5 Sonnet is the best model with a score of 71.8, slightly outperforming GPT-40 which scored 71.0. Among open-weight models, InternVL2-Llama3-76B leads with a score of 68.4.", "sections": [{"title": "Introduction", "content": "Large multimodal models (LMMs) [17, 20, 4] evolve rapidly, demonstrating emergent abilities to solve complex tasks [29] that required multiple integrated capabilities, such as GUI navigation [28, 30, 21], screenshot to code [23, 11], video understanding [15, 33], etc. To comprehensively evaluate LMMs, multiple benchmarks have been proposed, such as MME [10], MMBench [18], SEED- Bench [14], MMMU [32], and MM-Vet [31]. Notably, MM-Vet is designed to evaluate LMMs from a capability integration perspective, defining tasks based on their required core capabilities. The benchmark accepts open-ended responses and takes GPT-4 [19] to score model predictions, which better align with real-world scenarios. Such effective evaluation designs make MM-Vet widely utilized as a standard benchmark for LMM evaluation, as indicated by its leaderboard 3. Despite its popularity, MM-Vet and other concurrent evaluation benchmarks [10, 18, 14] are beginning to fall short in assessing the more advanced capabilities that have emerged in the latest LMMs, such as GPT-4V [20] and its successors [24, 8, 17, 7, 22, 5]. Specifically, one major limitation is the question format. The questions in MM-Vet are limited to a single image-text pair, lacking the capability to handle interleaved image and text sequences. This design choice was natural at the time of prior studies [31], given that most LMMs only supported single image inputs. However, the ability to process arbitrarily interleaved image-text sequences is crucial for advanced LMMs and should be included in LMM evaluation."}, {"title": "Dataset and evaluator", "content": "The same as MM-Vet [31], we aim to build a high-quality evaluation set for large multimodal models. MM-Vet [31] defines six core vision-language capabilities, including recognition (Rec), knowledge (Know), OCR, spatial awareness (Spat), language generation (Gen), and Math. MM-Vet's question format is only an image-text pair, which obviously cannot measure the capability of processing sequential image and text data. To fill this gap, we introduce a new capability:"}, {"title": "Experiments", "content": ""}, {"title": "3.1 Experiment settings", "content": "We evaluate two types of LMMs on our MM-Vet v2: (1) open-weight LMMs including OpenFlamingo [4, 6], Otter [13], LLaVA [17, 16], CogAgent [12], Emu2-Chat [25], InternLM-XComposer2 (IXC2)"}, {"title": "3.2 Results", "content": "The main results of the different methods are presented in Table 2 for each capability, and in Table 3 for 16 integrations with the highest proportions. Claude 3.5 Sonnet [1] and GPT-40 [2] are the leading models, achieving scores of 71.8 and 71.0, respectively. Claude 3.5 Sonnet [1] excels in recognition, language generation, OCR, spatial awareness, and knowledge. On the other hand, GPT-40 [2] surpasses in image-text sequence understanding and math. Among open-weight models, InternVL2-Llama3-76B stands out with a competitive performance score of 68.4."}, {"title": "4 Conclusion", "content": "In this paper, we aim to evaluate the integrated capabilities of large multimodal models and extend MM-Vet into MM-Vet v2 by introducing a new core capability: Image-text sequence understanding, which assesses the ability to process vision-language sequences. Additionally, we ensure the high quality of evaluation samples while expanding the evaluation set size. Using MM-Vet v2 to benchmark large multimodal models, we found that Claude 3.5 Sonnet achieved the highest score of 71.8, narrowly surpassing GPT-40, which scored 71.0. Among open-weight models, InternVL2-Llama3- 76B emerged as the leader with a score of 68.4."}, {"title": "\u2022 Image-text sequence understanding (Seq)", "content": "It refers to the capability to understand and reason the relationships among sequential image and text streaming data."}]}