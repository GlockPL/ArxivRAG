{"title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context", "authors": ["Zhiyuan Chang", "Mingyang Li", "Xiaojun Jia", "Junjie Wang", "Yuekai Huang", "Qing Wang", "Yihao Huang", "Yang Liu"], "abstract": "Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs. However, external knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses. This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and explore LLMs' preferences from their effectiveness, faithfulness and robustness, as well as CoE's usability in a naive Retrieval-Augmented Generation (RAG) case. The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.", "sections": [{"title": "Introduction", "content": "The parameterized knowledge acquired by large language models (LLMs) through pre-training at a specific point in time becomes outdated with the knowledge evolution or produces hallucination (Achiam et al., 2023; Touvron et al., 2023a; Anil et al., 2023). Incorporating external knowledge into LLM has emerged as an effective approach to mitigate this problem (Tu et al., 2024; Zhao et al., 2024). In this context, properties such as the accuracy and reliability of external knowledge are critical for LLMs to provide accurate answers.\nHowever, external knowledge is often imperfect. In addition to useful knowledge that users expect LLMs to follow (as shown in Figure 1), the context typically contains two types of noise (Chen et al., 2024; Zou et al., 2024): 1) Irrelevant information, despite showing textual similarities with the question, cannot support the correct answer (Chen et al., 2024; Xiang et al., 2024); 2) Misinformation, which can confuse LLMs and lead to incorrect answers (Liu et al., 2024). Especially when dealing with complex scenarios like multi-hop QA, the acquisition of such noise is inevitable due to limitations of retrievers or quality deficiencies in the specialized knowledge corpus (Wang et al., 2024; Shao et al., 2024; Dai et al., 2024; Tang and Yang, 2024). This hinders LLMs from effectively utilizing useful knowledge within external knowledge and leads to incorrect answers.\nTo this end, many studies focus on investigating the external knowledge preferences of LLMs in imperfect context (such as confirmation bias, completeness bias, coherent bias, etc.) (Xie et al., 2023; Zhang et al., 2024); or on approaches such as reranking or retrieval to prioritize knowledge with high relevance (Asai et al., 2023; Dong et al., 2024). However, previous studies have mainly the following two deficiencies: 1) They focus on qualitative findings and lack automated discrimination given external knowledge, such as it is promising"}, {"title": "Related Work", "content": "In imperfect knowledge augmentation, there is growing interest in understanding LLMs' knowledge preferences, especially in contexts involving conflicts between external and internal knowledge, as well as contradictions within internal knowledge (Xie et al., 2023; Kasai et al., 2023; Tan et al., 2024; Jin et al., 2024; Xu et al., 2024b,a).\nXie et al. (2023) demonstrated LLMs' bias towards coherent knowledge, revealing that LLMs are highly receptive to external knowledge when presented coherently, even when it conflicts with their parametric knowledge. Jin et al. (2024) found"}, {"title": "CoE Discrimination Approach", "content": "Drawing from the law of criminal procedure, judicial decisions in cases require the formation of a Chain of Evidence (CoE) through evidence collection (Edmond and Roach, 2011; Murphy, 2013). Such evidence must demonstrate two properties: relevance (pertaining to the case) and interconnectivity (evidence mutually supporting each other). We analogize judicial decisions to the scenario in which LLMs identify correct answers from external knowledge in response to input questions.\nWe assume that LLMs prefer knowledge that forms CoE. To satisfy the two properties required for CoE formation, we characterize three features: 1) Intent describes the ultimate goal the user intends to solve through the question. 2) Keywords are important words or phrases that capture the specific details the user is asking about; and 3) Relations describe how keywords are connected to each other to convey intent. Knowledge containing intent demonstrates responsiveness to the question, satisfying the relevance property, while knowledge containing keywords and relations mutually corroborates each other, fulfilling the interconnectivity property. Therefore, we consider knowledge matching the three features as CoE for the current QA, as illustrated in Figure 2."}, {"title": "CoE Discrimination Approach", "content": "Based on the characterized features, we design an approach to discriminate whether external knowledge qualifies as CoE, as illustrated in Figure 3. First, for each question, we perform information extraction to extract its inherent intent, keywords, and relations. Based on GPT-40, we adopt the prompt used in the previous study (Li et al., 2023) and enhance it by few-shot learning (adding 5 extra input-output samples) to help LLM achieve better extraction performance. Appendix A shows the example template for the extraction prompt.\nSecond, for external knowledge, the pipeline discriminates whether it contains CoE. Specifically, the approach leverages GPT-40 to discriminate the presence of intent, keywords, and relations within external knowledge. As for intent, analogous to the textual entailment task, LLMs treat external knowledge as a premise and intent as a hypothesis, reasoning whether the hypothesis holds based on the given premise. For keywords, the LLM identifies phrases contained in external knowledge that are semantically similar with keywords. For relation entailment, the LLM utilizes its textual entailment capabilities, similar to the process of intent entailment. External knowledge is discriminated as CoE exists if all extracted features is present, and as CoE does not exist if any feature is missing. The prompts for feature discrimination are provided in the Appendix \u0412."}, {"title": "Subject Dataset and LLMS", "content": "We selected two commonly used multihop QA datasets, HotpotQA and 2WikiMultihopQA as the sample sources. In the two datasets, each sample consists of a question, an answer, and supporting knowledge to derive the answer to each question. It is worth noting that, due to the characteristics of multi-hop QA, supporting knowledge typically contains multiple knowledge pieces\u00b9, usually no fewer than two. Considering that supporting knowledge is initially constructed to describe the necessary information from the question to the answer, we believe it is highly likely to possess features of the CoE we have characterized. Therefore, we consider it as a candidate CoE for each QA pair.\nReferring to the sample size in previous studies (Jin et al., 2024; Chen et al., 2024), we randomly sampled 1,000 instances from each dataset and applied the CoE discrimination approach to check whether candidates contain CoEs. Finally, we obtained 676 and 660 samples that contain CoE from candidates, with an average of 4.0 and 3.4 knowledge pieces for two datasets, respectively (details in Table 1)."}, {"title": "Non-CoE Sample Construction", "content": "Based on the CoE samples, we construct Non-CoE samples where knowledge pieces fail to satisfy either the relevance or interconnectivity property of CoE. During the process, two strategies are utilized.\nSentence-Level Perturbation (SenP). For multihop QA, LLMs typically require multiple knowledge pieces to generate answers. However, external knowledge is often incomplete in practice. To simulate this situation, we construct Non-CoE by removing one or more knowledge pieces from CoE. Specifically, we segment the CoE into multiple sentences and select sentences that contain keywords mentioned in the corresponding question, but not\n\u00b9A knowledge piece refers to a complete sentence."}, {"title": "Studied LLMs", "content": "For the following experimantal evaluation, we introduce two closed-source LLMs (GPT-3.5, GPT-4) and three open-source LLMs (LLama2-13B, LLama3-70B, and Qwen2.5-32B). All subsequent experiments are evaluated across these LLMs."}, {"title": "Effectiveness Assessment", "content": "Starting from the constructed CoE and Non-CoE samples, we inject additional irrelevant pieces into their contexts and investigate whether CoE can better help LLMs generate correct answers under external information rich with irrelevant noise."}, {"title": "Experimental Setup", "content": "First, we collected the irrelevant information using the search engines. Specifically, for a constructed sample (donated as <Question, Answer, CoE, SenP, WordP>), we traverse all the keywords in \u201cQuestion\", fill them into the template \"Please introduce the background of the [keyword]\u201d, and use Google to retrieve the knowledge snippets. In this way, we can ensure that the retrieved information is irrelevant to the question's intent and highly similar to the question in lexical terms. Then, we regard the retrieved snippets as irrelevant information and inject them into the context of \"CoE\", \"SenP\", \"WordP\" respectively in different ratios. Specifically, we increase the proportion of irrelevant information based on character length. We design four proportion scenarios with intervals of 0.25. Finally, \"Question\" together with \u201cCoE\u201d, \u201cSenP\u201d, \u201cWordP\" are sent to studied LLMs as input and obtain the output of the corresponding LLMs.\nFor each sample, we evaluate the consistency between the LLM's output and ground truth \"Answer\" in the five-element tuple. During the process, we followed the evaluation method used in Adlakha et al. (2024) and used GPT-40 to judge whether LLMs augmented by different external knowledge can generate the correct answer. After that, we calculated the accuracy (ACC) of each studied LLM for the three experiment groups, i.e., \u201cCoE\u201d, \u201cSenP\" and \"WordP\". To alleviate the randomness of LLMs, each group of experiments is repeated three times, and the average will be taken as the final evaluation result.\""}, {"title": "Results and Findings", "content": "Table 2 shows the response accuracy of LLMs using CoE and two types of Non-CoE under different proportions of irrelevant information. The main findings and supporting results are illustrated below.\nFinding-1: External knowledge equipped with CoE can help LLMs generate correct answers more effectively than Non-CoE. Generally, experimental results show that CoE achieves an average accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respectively. Moreover, compared to CoE, we conducted Mann-Whitney tests (Mann and Whitney, 1947) on all experiment groups of Non-CoE. The results of the hypothesis test show that the improvement in CoE across all types of Non-CoE is statistically significant (significant level is 0.05).\nFinding-2: LLMs exhibit greater resistance if CoE exists in external knowledge as the proportion of irrelevant information increases. As the proportion of irrelevant increases from 0% to 75%, the ACC of LLMs with CoE only decreases by 1.8%, while the ACC decreases by 12.9% and 9.0% under the Non-CoE variants SenP and WordP, respectively. In the Non-CoE, WordP demonstrates better performance over SenP, exhibiting both higher ACC and greater resistance against increasing irrelevant information. The enhanced performance of WordP, which contains richer information content than SenP, indicates that the information density of external knowledge positively correlates with LLMs' QA capabilities. Furthermore, while CoE and WordP possess comparable information content, LLMs achieve better performance with CoE, highlighting the importance of forming CoE.\nIn addition to the main findings illustrated above, we also observed that even under perfect retrieval conditions (Irrelevant proportion is 0%), CoE outperforms Non-CoE by 14.6% in ACC. This implies that LLMs still face challenges in utilizing external knowledge effectively, even when all retrieved information is useful."}, {"title": "Faithfulness Assessment", "content": "Based on the effectiveness assessment, we investigate a more challenging scenario, where the CoE contains factual errors, to determine whether LLMs can still exhibit a certain degree of faithfulness"}, {"title": "Experimental Setup", "content": "For the five-element tuple (<Question, Answer, CoE, SenP, WordP>), we respectively substitute the correct answers in \"CoE\", \"SenP\" and \"WordP\" with the incorrect ones to simulate the relevant knowledge contains the factual errors. To maintain textual coherence after the answer substitution, we construct incorrect answers that match the original in both type and format. For example, we replace \"United States\u201d with the same type \u201cCanada\u201d, and \"September 29, 1784\" with the same format \"April 22, 1964\". We employ GPT-40 to understand the answer types and their formats, facilitating the generation of naturally incorrect answers. Appendix C presents the detailed prompt design. Through manual inspection, we found that 100.0% of the generated incorrect answers maintain the same type and format as the correct ones.\nTo investigate LLMs' faithfulness with CoE under imperfect external knowledge, we progressively add irrelevant information to the external knowledge. The specific process follows the same procedure as described in Section 5.1. As for the evaluation metric, we use Following Rate (FR), defined as the proportion of all the LLM outputs consistent with incorrect answers contained in \"CoE\", \"SenP\" or \"WordP\" respectively. Following the previous study Adlakha et al. (2024), GPT-40 is used to evaluate consistency. Each group of experiments is conducted three times and the average is considered as the final evaluation result."}, {"title": "Results and Findings", "content": "Table 3 shows the FR of LLMs with external knowledge under CoE and two types of Non-CoE containing incorrect answers. The main findings and supporting results are illustrated in the following.\nFinding-3: LLMs exhibit significant faithfulness to the answer supported by CoE although it contains factual errors. The results show that under CoE, the average FR reaches 85.4%, which is 20.6% and 16.2% higher than the SenP and WordP types under Non-CoE respectively. Moreover, Mann-Whitney tests confirmed statistically significant improvements of CoE over all Non-CoE groups (p < 0.05).\nFinding-4: LLMs following CoE demonstrate higher stability against irrelevant noise variations when handling factual errors, compared to Non-CoE. As irrelevant information in external knowledge increases from 0% to 75%, the FR of LLMs with CoE decreases by 3.6%, while the FR drops by 9.7% and 7.9% under Non-CoE variants SenP and WordP, respectively.\nBeyond the main findings, we also discovered that LLMs demonstrate a 6.6% reduction in FR when processing CoE with factual errors, compared to those with correct answers (as indicated by ACC in Table 2). This discrepancy could be attributed to the LLM's inherent parametric knowledge containing accurate information, facilitating self-correction of certain factual errors."}, {"title": "Robustness Assessment", "content": "We make the knowledge conflicts by injecting the misinformation in the context of CoE and Non-CoE. Robustness explores whether CoE can help LLMs more effectively resist the conflict and produce the correct answers."}, {"title": "Experimental Setup", "content": "Based on the CoE and Non-CoE samples, we first obtain misinformation. Misinformation should meet two requirements: 1) contain factual errors, and 2) cause conflicts with the knowledge in CoE and Non-CoE. Following previous studies (Chen et al.; Zhou et al., 2023; Jin et al., 2024), we use two strategies to generate misinformation: 1) entity replacement, which replaces the correct answer in the CoE with the incorrect answer and uses the sentence containing this incorrect answer as misinformation; 2) LLM generation, which uses"}, {"title": "Results and Findings", "content": "Table 4 shows LLMs' response accuracy (ACC) after adding misinformation to CoE and two types of Non-CoE. The main findings and supporting results are illustrated in the following.\nFinding-5: LLMs augmented with CoE exhibit higher robustness against knowledge conflict than Non-CoE. The results show that under CoE, the average ACC of LLMs reaches 84.1%, which is 21.4% and 15.3% higher than the SenP and WordP types under Non-CoE respectively. Besides, as the proportion of misinformation increases from 0% to 75%, LLMs' ACC under CoE shows 6.2% and 6.3% smaller decreases compared to the reductions observed in SenP and WordP under Non-CoE.\nFinding-6: Compared to adding irrelevant information to CoE, adding misinformation has a greater impact on LLM's ability to generate correct outputs. In Table 2, when adding irrelevant information from 0% to 75%, the ACC of LLMs with CoE only decreases by 1.8%. However, as shown in Table 4, introducing misinformation under similar settings results in an 18.0% ACC drop for LLMs equipped with CoE.\nWe also discovered that as misinformation increases, LLMs with weaker reasoning capabilities tend to favor frequently appearing knowledge in external knowledge, while LLMs with stronger reasoning abilities adhere more to knowledge from CoE. With the proportion of misinformation increasing from 0% to 75%, less capable LLMs like GPT-3.5 and LLama2-13B are more likely to be misled by increasing misinformation, leading them to select answers from misinformation and resulting in significant ACC drops (with average ACC decreasing by 34.5%), whereas more powerful LLMs such as GPT-4, Llama3-70B, and Qwen2.5-32B consistently adhere to answers within CoE, resulting in slight ACC decreases (with average ACC decreasing by 7.1%)."}, {"title": "Usability Assessment", "content": "To assess usability, we selected a popular knowledge-augmentation case, naive RAG, and designed a CoE-guided retrieval strategy to investigate the extent to which CoE improves the performance compared with the naive case."}, {"title": "Subject Case", "content": "Considering popularity and maturity, we choose a naive RAG scenario proposed by Chen et al. (2024) as our subject case. For a given question, a search engine first retrieves relevant knowledge snippets, followed by a reranking model that prioritizes the knowledge snippets based on its relevance to the question. Finally, the top K knowledge snippets are selected as external knowledge and fed into the studied LLMs to generate answers to the questions."}, {"title": "CoE-guided Retrieval Strategy", "content": "We design a retrieval strategy (ScopeCoE) guided by CoE. Instead of using the reranking component in the naive framework, ScopeCoE selects the minimal set of knowledge snippets that encompass a CoE as a context input for LLMs. It consists of two phases: 1) CoE Feature Judgment, which judges the CoE features covered by each knowledge snippet; 2) Minimal Coverage Search, which finds"}, {"title": "CoE Feature Judgment", "content": "ScopeCoE first extracts CoE features from the question and then judges them in each knowledge snippet. Specifically, as shown in Figure 3, ScopeCoE employs the same information extraction component in the discrimination approach to extract the intent, keywords and relations from the question. Then, for each knowledge snippet, ScopeCoE utilizes the proposed feature discrimination approach to determine whether it contains these extracted features, and records the judgment results. Finally, we obtain a set of judgments regarding intent, keywords, and relations for each knowledge snippet."}, {"title": "Minimal Coverage Search", "content": "After obtaining the judgment set, ScopeCoE searches for the minimal set of textual snippets that cover CoE. The algorithm process is shown in Appendix D. First, ScopeCoE searches for knowledge snippets that contain intent and adds them to the minimal set. Second, ScopeCoE examines the coverage of the relations. Specifically, it determines whether the minimal set already contains all relations. If there are uncovered relations, it searches the remaining knowledge snippets and adds those containing uncovered relations to the minimal set. Finally, ScopeCoE proceeds to examine keywords coverage following the same process. It checks if the minimal set covers all keywords. If uncovered keywords exist, it searches the remaining snippets for those containing these keywords.\nScopeCoE manages to search for the minimal set that completely covers all CoE features, ultimately outputting a set of knowledge snippets that covers the maximum number of CoE features, which serves as context input for the LLM."}, {"title": "Experimental Setup", "content": "We used the constructed CoE samples (including \"Question\", \"Answer\" and \"CoE\u201d) for usability evaluation. To obtain the external corpus for retrieval, we first use the Google Search API to retrieve relevant knowledge snippets for each \u201cQuestion\". To ensure that the corpus contains the correct answers, we decompose CoE into multiple knowledge pieces based on sentence completeness and then append them to the corpus. Then we set up two experimental groups: RAG and RAG+ScopeCoE. For RAG,"}, {"title": "Results and Findings", "content": "Finding-7: For the subject case, CoE-guided retrieval could improve the LLMs' accuracy in the naive framework. Table 5 demonstrates the impact of naive RAG and RAG+ScopeCoE on LLMs' accuracy. The results show that RAG+ScopeCoE achieves average ACC of 77.8% and 81.6% on HotpotQA and 2WikiMultihopQA respectively, outperforming RAG by 10.4% and 28.7%.\nMoreover, we also observe that ScopeCoE can help LLMs generate more accurate outputs with fewer knowledge pieces (4.6 for HotpotQA and 4.8 for 2WikiMultihopQA) compared to the naive framework (5 pieces). It implies that ScopeCoE can make LLMs more efficient in knowledge utilization, leading to improved performance and reduced dependency on large amounts of external data."}, {"title": "Conclusion", "content": "In this paper, we introduce CoE and investigate its impact on LLMs in imperfect external knowledge. We characterize the features of CoE knowledge and propose a CoE discrimination approach to identify CoE from external knowledge. Generally, our study reveals LLMs' preference for CoE in the imperfect context. Once CoE's implicit relevance or interconnectivity is disrupted, the preference also decreases. Furthermore, we apply CoE theory to the naive RAG framework, finding that retrieving CoE-structured knowledge during the retrieval phase effectively improves the response accuracy of LLMs. In future work, we will explore broader applications of CoE in RAG scenarios, such as retrieval corpus construction and retriever optimization."}, {"title": "Limitations", "content": "There are three limitations to the current study.\nFirstly, we apply the ScopeCoE to search for CoE in external knowledge, but there is no step to verify the correctness of answers within the CoE. If the retrieved CoE contains incorrect information, it may mislead the LLM to generate inaccurate responses. In the Section 6, we discuss LLMs' Following Rate to CoE containing factual errors, showing that LLMs are highly likely to follow the knowledge provided in CoE.\nSecondly, this paper does not investigate the individual contributions of CoE features to LLM performance. Since intent, keywords, and relations within CoE are interdependent, it is challenging to isolate any single feature. Therefore, we focus on examining the overall impact of CoE on LLM performance in this paper.\nThirdly, the usability of our proposed retrieval strategy (ScopeCoE) has inherent constraints across RAG scenarios. For instance, some RAG scenarios convert external knowledge into vectors and store them in vector databases, then search for question-relevant knowledge at the vector level during the retrieval phase. Our approach, which operates at the textual level, is not suitable for such vector-based RAG scenarios."}, {"title": "Details of Information Extraction Prompts", "content": "The details of the information extraction prompts are illustrated below. In pipeline, we replace the placeholders in the following prompts with the question and keywords."}, {"title": "Details of Feature Discrimination Prompts", "content": "The details of the Feature Discrimination prompts are illustrated below. In pipeline, we replace the placeholders in the following prompts with the external knowledge, intent, keyword, and relation."}, {"title": "The Algorithm for the Minimal Coverage Search", "content": "We show the detailed algorithm for the minimal coverage search in ScopeCo\u0415."}]}