{"title": "HOLA-Drone: Hypergraphic Open-ended Learning for Zero-Shot Multi-Drone Cooperative Pursuit", "authors": ["Yang Li", "Dengyu Zhang", "Junfan Chen", "Ying Wen", "Qingrui Zhang", "Shaoshuai Mou", "Wei Pan"], "abstract": "Abstract-Zero-shot coordination (ZSC) is a significant challenge in multi-agent collaboration, aiming to develop agents that can coordinate with unseen partners they have not encountered before. Recent cutting-edge ZSC methods have primarily focused on two-player video games such as OverCooked!2 and Hanabi. In this paper, we extend the scope of ZSC research to the multi-drone cooperative pursuit scenario, exploring how to construct a drone agent capable of coordinating with multiple unseen partners to capture multiple evaders. We propose a novel Hypergraphic Open-ended Learning Algorithm (HOLA-Drone) that continuously adapts the learning objective based on our hypergraphic-form game modeling, aiming to improve cooperative abilities with multiple unknown drone teammates. To empirically verify the effectiveness of HOLA-Drone, we build two different unseen drone teammate pools to evaluate their performance in coordination with various unseen partners. The experimental results demonstrate that HOLA-Drone outperforms the baseline methods in coordination with unseen drone teammates. Furthermore, real-world experiments validate the feasibility of HOLA-Drone in physical systems. Videos can be found on the project homepage https://sites.google.com/view/hola-drone.", "sections": [{"title": "I. INTRODUCTION", "content": "In multirobot system, the problem of cooperative drone pursuit plays a crucial role in various applications such as surveillance and urban security [1], [2], [3]. A key challenge in these settings arises when a drone must coordinate with previously unseen teammates. This is where zero-shot coordination (ZSC) comes into play, which allows for efficient collaboration with new partners and has gained considerable attention in cooperative AI [4], [5], [6], [7], [8], [9], [10]. In this paper, we address the zero-shot cooperative multi-drone pursuit problem, where a learner drone is placed in a cooperative scenario with multiple unseen drone teammates to pursue and capture multiple evaders. The learner drone must rapidly coordinate with its teammates without modifying its fixed policy from the training phase [4].\nRecent advances in multi-robot pursuit research can be categorized into three main approaches: rule-based heuristic methods, differential game theoretical methods, and learning-based methods. Rule-based heuristic methods are primarily inspired by biological behaviors, aiming to imitate hunting and pursuit actions observed in nature [11], [12], [13], [14], [15], [16]. However, these methods face limitations when dealing with complex tasks, such as those involving evader movement advantages and complex environments. The second category of methods utilizes differential game theory to address the multi-robot pursuit problem by deriving theoretically optimal strategies. These methods, however, often require precise state transition equations, leading to reduced performance under conditions of uncertainty. Lastly, learning-based methods have significant advantages in enhancing distributed cooperative pursuit abilities across various environments and tasks [17], [18], [19], [20]. Nevertheless, even the most advanced methods, including learning-based approaches, struggle to handle scenarios involving collaboration with unseen partners in multi-robot pursuit task.\nThe most current methods for solving the zero-shot coordination problem focus on two-player video games [21], [5]. Conventional self-play and its subsequent methods [22], [5], [4] aim to improve cooperative ability by playing with a copy or permutation of itself. Cutting-edge approaches often involve pre-training diverse strategy populations and then training a common best-response to these pretrained diverse agents to enhance coordination with various agents [6], [23], [24], [8]. Furthermore, recent research [25], [9] has focused on improving compatibility within the population to further enhance cooperative abilities. However, these methods are typically designed for two-player video games in 2D simulators, rather than for multi-agent collaboration in the physical world.\nIn this paper, we propose a novel Hypergraphic Open-ended Learning Algorithm (HOLA-Drone) to address the multi-drone zero-shot coordination problem in cooperative pursuit tasks. To the best of our knowledge, this is the first work to formulate the cooperative multi-drone pursuit task as a ZSC problem within the framework of a decentralized partially observable Markov decision process (Dec-POMDP). Most existing works on Dec-POMDPs in multi-drone pursuit [26], [2], [27] have primarily focused on coordination among agents with known teammates. By introducing the ZSC setting, we open the door to more adaptable and robust solutions, allowing drone teams to effectively collaborate with unseen partners in a variety of unknown or evolving environments. We further propose the novel Hypergraphic-Form game and corresponding concepts to capture the cooperative interaction relationships among multiple agents in a hypergraph, allowing us to effectively evaluate the cooperative abilities of each agent. These concepts are incorporated into the open-ended learning framework [28], [29], [30], which automatically adapts learning objectives to continuously improve the"}, {"title": "II. RELATED WORK", "content": "a) Multi-robot pursuit.: One of the classic methods for solving the multi-robot pursuit problem is rule-based heuristic methods. Heuristic rules inspired by biological behavior have been proposed to imitate hunting and pursuit actions observed in nature. Most heuristic rules are implemented using artificial attractive and repulsive forces [31], [12], [13]. For instance, [32] designed predictive attraction forces, obstacle repulsive forces, and teammate repulsive forces for encirclement. Similarly, [15] proposed a method based on Voronoi tessellation, which is suitable for ground vehicles. However, these rule-based methods are manually designed based on the observations or experiences of the designer, which limits their applicability. Differential game theory is another approach to solving the multi-robot pursuit problem [33], [34], [35], [36]. [36] modeled a two-on-one pursuit problem as a zero-sum game and obtained an analytical solution. [35] considered pursuers and evaders as nonholonomic constraint systems and introduced model predictive control to minimize the evader's safe zone. These methods, similar to optimal control, derive strategies by maximizing the utility function based on the Hamilton-Jacobi-Bellman equation of the system. However, they require precise state transition equations, resulting in reduced performance under uncertainty and unknown environments. Learning-based methods have been proposed to enhance distributed cooperative pursuit abilities in various environments and tasks [17], [18], [19], [20]. [20] used curriculum learning and parameter sharing techniques to train collaborative intelligent agents, which initially move towards the evader, then slow down at an appropriate distance, and disperse to surround the evader. Although the learned strategy, under a well-designed reward function and substantial data, exhibits intelligence similar to biological behavior, this method lacks scalability. [37] proposed an attention interface to enhance interaction between agents and the environment, demonstrating better performance in a 100-on-100 collaborative pursuit task. [2] introduced a hybrid design that integrates rule-based strategies into reinforcement learning for multi-robot pursuit, improving data efficiency and generalization. However, most of these methods may fail to coordinate with unseen partners. To the best of our knowledge, we are the first to propose the zero-shot multi-drone cooperative pursuit problem and introduce a novel HOLA-Drone algorithm to handle the problem.\nb) Zero-shot coordination.: [5] introduced a novel two-player, fully cooperative environment inspired by the popular game Overcooked, which demands challenging coordination. Their method involves learning a simple model that emulates human play, serving as a standard unseen evaluation agent within the Overcooked environment. The self-play (SP) method [22] was implemented in this Overcooked setting to coordinate zero-shot with the unseen human proxy model [5]. However, SP tends to become stuck in the conventions formed between trained players, making it in a unable to cooperate with other unseen strategies [38], [4]. To address this issue, other-play [4] was proposed, introducing permutations to one of the strategies to break the conventions formed by the self-play method [22], [5]. However, this approach may revert to self-play if the game or environment lacks symmetries or has unknown symmetries. Recent ZSC research is mainly inspired by population-based training (PBT), which improves adaptability by fostering cooperation with multiple strategies within a population [5]. However, PBT does not explicitly maintain diversity, thus failing to coordinate with unseen partners [6]. To address this limitation and achieve the goal of ZSC, cutting-edge methods emphasize pre-training diverse strategy populations [6], [23] or applying handcrafted techniques [24], [8] to excel in cooperative games by optimizing objectives within these populations. Furthermore, mechanisms such as coevolution and combinatorial generalization have been introduced to enhance generalization ability [39], [40]. Recent research [25], [9] has also focused on improving compatibility within the population to enhance cooperative abilities. However, most of these methods address the ZSC problem in two-player video games such as Hanabi [21] and Overcooked!2 [5]. In this work, we extend the scope to multiple agents beyond two players, tackling more complex real-world cooperative multi-drone pursuit problems instead of video games in simulator."}, {"title": "III. PROBLEM FORMULATION", "content": "In this paper, we focus on the multi-drone cooperative pursuit task, where pursuit drones aim to capture faster evaders, within a confined environment containing obstacles. An evader e is considered captured by a drone p if the distance $d_{p,e}$ between p and e is less than a predefined capture threshold $d_c$. Furthermore, a collision is defined to occur if the distance between two drones is less than $d_p$ or if the distance between a drone and an obstacle is less than $d_s$, where $d_s$ is the safe radius for drones. Thus, the objective of the pursuers is to capture all evaders without collisions.\nThe zero-shot coordination problem arises in scenarios where there is no opportunity to update the fixed policy established during training when coordinating with unseen partners [4]. Fig. 1 compares two frameworks: the standard multi-agent reinforcement learning (MARL) with centralized training and decentralized execution (CTDE) and the zero-shot cooperative multi-drone pursuit scheme. In the upper row of Fig. 1, the training and evaluation phases of MARL with CTDE are illustrated along with the corresponding environment schematic diagram. In the CTDE scheme, all controlled agents are updated collectively during the training phase using a shared policy or a centralized critic with access to centralized information. During the evaluation, the same agents trained in the training phase are deployed to assess their collective performance in achieving the task using the learned strategies. However, the zero-shot cooperative multi-drone pursuit framework, shown in the bottom row of Fig. 1, is designed to train a single learner agent to co-play with other non-learnable partners. The evaluation phase for this scheme is notably different: the learner agent is required to coordinate with unseen partners, unlike in the classic MARL scheme where partners are collectively trained together. This approach assesses the learner's zero-shot coordination ability\u2014its ability to work effectively with any new, unseen partner without additional updates.\nThe zero-shot multi-drone pursuit problem can be effectively modeled as a decentralized partially observable Markov decision process (Dec-POMDP). A Dec-POMDP, denoted as M, is defined by the tuple (S,N,A,P,r,O,\u03b3, \u03a4). Here, \u039d represents the set of drone agents, which includes both the pursuers (Np) and the evaders (Ne). Specifically, Np consists of the learner (N\u2081) and its co-players (N-1). Additionally, S denotes the joint-state space, while $A = \\times_{i=1}^{k} A^{i}$ represents the joint-action space, where k is the team size. P and O are the transition and observation functions, respectively. The reward function is denoted by r, y is the reward discount factor, and T represents the task horizon. In the zero-shot multi-drone pursuit task, the policies of the learner's teammates and the evaders are often pre-trained or pre-defined. At the beginning of each episode, the learner's teammates N-1 are sampled from a population U. At time t > 0, the Dec-POMDP is in state st \u2208 S and generates a stochastic joint observation $o_t = (o^{1}_{t}, ..., o^{n}_{t}) \\sim O(\\cdot|s_t)$, which is added to the action-observation trajectory $\\tau_t = (o_0, a_0, ..., o_{t-1}, a_{t-1}, o_t)$. Each drone agent $j\\in N$ then selects an action $a_{t}^{j} \\in A^{j}$ using its policy $\\pi^{j}(a|\\tau^{j}_t)$. The environment transitions to state $s_{t+1}$ with probability $P(S_{t+1}|S_t, a_t)$, and all pursuers receive a common reward $r(s_t, a_t)$. Considering the discount factor $\\gamma\\in [0,1]$, the discounted return is $R(\\tau) = \\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)$. The objective is to maximize the learner's expected return with the sampled teammates, formally given by $J = \\mathbb{E}_{\\pi_{-1}\\sim\\pi^{U}} \\mathbb{E}_{\\tau \\sim {\\pi_1,\\pi_{-1}, \\pi_e}}[R(\\tau)]$. The zero-shot multi-"}, {"title": "IV. HYPERGRAPHIC OPEN-ENDED LEARNING ALGORITHM", "content": "To address the zero-shot multi-drone pursuit problem, we introduce a novel approach named the hypergraphic open-ended learning algorithm (HOLA-Drone). In Section IV-A, we first introduce the preference hypergraph and hyper-preference centrality to model cooperative relationships and assess the coordination ability of each agent within the hypergraph. In Section IV-B, we provide the details of HOLA-Drone, as illustrated in Fig. 2.\nA. Preference Hypergraph\nWe first propose the hypergraphic-form game to model the interactions in a population of agents.\nDefinition 4.1 (Hypergraphic-Form Game): The Hypergraphic-Form Game (HyFoG) Gis defined by tuple (V, E, w,l). V is a finite set of vertices representing players, each indexed by i and parameterized by the weights of a neural network. E is a set of hyperedges with fixed length l. w is a weight vector consisting of the utility obtained for each hyperedge connected nodes co-playing.\nThe HyFog exhibits the following two properties. HyFog is l-uniform as |e| = 1 for all e \u2208 E. HyFog is connected as every pair of nodes is connected. Although the HyFog model offers a detailed framework for agent interactions, directly extracting data relevant to cooperative capabilities within the game remains a challenge. To address this, we further introduce the preference hypergraph PG.\nDefinition 4.2 (Preference Hypergraph): A preference hypergraph, denoted as PG, is an unweighted directed hyper-graph derived from a HyFog G = (V,E, w) and represented as (V,E). In PG, for each node i \u2208 V, there exists a unique outgoing hyperedge $e_i$ such that $w(e_i) = \\text{max}_{e\\in E_i} w(e)$, where $E_i$ is the set of hyperedges that connect to i.\nIn hyper-preference graph, a node that is the endpoint of multiple hyperedges typically indicates a higher cooperative ability. We introduce the concept of hyper-preference centrality, denoted by \u03b7, to quantify the cooperative ability of each node. For any node i \u2208 V, the hyper-preference centrality $\u03b7_i$ is defined as\n$\\eta_i = \\frac{1}{|V|-1} d(i),$ (1)\nwhere d(i) is a centrality metric that quantifies the importance or influence of node i within the network.\nIntuitively, each hyperedge in \u0190 signifies a preference relationship, with the source node favoring the formation of a coalition with the end nodes. This preference arises because the source node achieves the highest outcomes when co-playing with the end nodes. Fig. 3 provides a schematic illustration of the hypergraph representation of HyFog (left) and its corresponding preference hypergraph (right). In this example, nodes v2 and v3 exhibit the highest cooperative capacity, each with a hyper-preference centrality value of 1.\nB. Hypergraphic Open-ended Learning Algorithm\nWe then incorporate the hypergraphic-form game into open-ended learning framework and propose HOLA-Drone, which continuously adjusts training objectives to enhance coordination capabilities among agents. HOLA-Drone consists of two main phases: the pre-training phase and the open-ended learning phase.\na) Pre-training Phase.: To improve the diversity of policies in the hypergraph, we first pre-train a population of drone agents and then construct the initial HyFoG Go. Motivated"}, {"title": "V. EXPERIMENT", "content": "In this work, we conducted a series of experiments to verify the effectiveness of our proposed HOLA-Drone method in coordinating with unseen partners. Section V-A introduces the experimental setup, metrics, baselines, and other relevant details. This is followed by an explanation of the unseen drone team configurations in Section V-B. The results of the experiments and their analysis are presented in Section V-C.\nA. Experiment Setting\na) Cooperative Drone Pursuit Environment.: The experiments are carried out in a cooperative drone pursuit environment that features 3 pursuers and 2 evaders, as shown in the left of Fig. 4. All drones operate within a rectangular area with a boundary width (w) of 3.6 meters and a boundary height (hb) of 5 meters. The task horizon (tmax) is the maximum duration of each episode, set to 100 seconds. The simulation runs at 10 frames per second (fps), ensuring smooth and continuous tracking of drone movements.\nAt the start of each episode, the three pursuers (P1, P2, p3) and two evaders (e1, e2) are randomly spawned in their designated areas. The spawn area for each group measures 3.2 meters in width (ws) and 0.6 meters in height (hs). The sky-blue rectangle in Fig. 4 indicates the evaders' spawn area, while the red rectangle indicates the pursuers' spawn area. To introduce additional complexity, the arena features five obstacles, each with a width (wo) of 0.65 meters and a height (ho) of 0.1 meters. These obstacles are strategically placed to influence the movement dynamics of both pursuers and evaders. Several critical parameters influence the drones' interactions. The capture distance (de) is set to 0.2 meters, which is the threshold distance within which a pursuer is considered to have captured an evader. The perception range (dp) is 2 meters, defining the radius within which a drone can detect others. Each drone also has a safe radius (ds) of 0.1 meters to avoid collisions. The pursuers move at a velocity (up) of 0.3 meters per second, while the evaders move faster, at a velocity (VE) of 0.6 meters per second. This difference in speed necessitates strategic coordination among the pursuers to successfully capture the evaders. Finally, the unseen partner pool (U) consists of a set of strategies denoted as {u1,..., u4}, which are used to test the zero-shot coordination capabilities of the pursuers when teamed with previously unseen partners.\nb) Physical Environment.: To verify our proposed HOLA-Drone algorithm beyond simulation, we deploy the learned policies of HOLA-Drone with unseen drone teammates in the multi-quadrotor system Crazyflie. We use the OptiTrack motion capture system to measure the positions and orientations of both pursuers and evaders. Our multi-quadrotor communication framework is based on CrazySwarm [41]. Snapshots of real-world experiments are shown in right of Fig. 4.\nc) Evader Policy.: The evaders are controlled by the escape policy proposed by [42] and [2]. This policy defines multiple repulsive forces exerted by the pursuers and obstacles on the evaders. Additionally, wall-following rules are incorporated to help evaders maneuver along obstacle surfaces when positioned between pursuers and obstacles.\nd) Baselines and Metrics.: We use the task success rate, the collision rate, and the mean episode length as metrics to evaluate performance in coordinating with unseen partners. The unseen partners will be introduced in further detail in Section V-B. An episode is deemed successful if the pursuers capture both evaders, defined as reducing the distance between an evader and a pursuer to less than 0.2 meters. A collision is recorded if the distance between any two pursuers is less than 0.2 meters or if the distance between a drone and an obstacle is less than 0.1 meters. To evaluate the HOLA-Drone algorithm, we compare it with self-play (SP) [43], [5], population-based training (PBT) [44], [5], fictitious co-play (FCP) [6], and maximum entropy population-based training (MEP) [8]. All methods, including HOLA-Drone, are implemented using the PPO algorithm [45]. The action space is continuous, ranging from 0 to 1, and determines the drone's direction, while the drone's velocity remains fixed and predefined. Further details of implementation are available in Appendix VII-B.\nB. Unseen Drone Teammate Pools\nTo evaluate the cooperative capabilities of our proposed HOLA-Drone algorithm with previously unencountered drones, we establish two separate unseen drone teammate pools - homogeneous pool and heterogeneous pool. The agents in the homogeneous pool are trained using a self-play PPO"}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose a hypergraphic open-ended learning algorithm (HOLA-Drone) to address the zero-shot cooperative multi-drone pursuit problem, enabling coordination with unseen drone partners. To the best of our knowledge, this is the first work to formulate the cooperative multi-drone pursuit task as zero-shot multi-agent coordination problem within the Dec-POMDP framework. This formulation extends ZSC research from two-player video games to real-world multi-drone cooperative pursuit scenarios. We introduce a novel hypergraphic open-ended learning algorithm that continuously enhances the learner's cooperative ability with multiple teammates. To empirically verify the effectiveness of HOLA-Drone in coordinating with unseen teammates, we construct two unseen drone teammate pools for evaluation, comprising both homogeneous and heterogeneous teammates. Experimental results in both simulation and a real-world cooperative Crazyflie pursuit environment demonstrate that HOLA-Drone can better coordinate with unseen teammates compared to baseline methods.\nLimitations and Future Work: While our study demonstrates the effectiveness of HOLA-Drone in achieving zero-shot coordination with unseen drone partners, several limitations warrant further investigation. Firstly, although collision avoidance is considered in the reward function, some pursuers exhibit aggressive and dangerous behaviors to capture evaders. Future work should focus on incorporating more sophisticated safety mechanisms to ensure robust and reliable performance in real-world applications. While the deployment of HOLA-Drone policies in Crazyflie 2.1 drones validates its feasibility in physical systems, scalability remains a challenge. Future research could explore optimizing HOLA-Drone for larger swarms and more complex tasks. Finally, integrating advanced sensors and communication protocols could enhance the coordination and efficiency of drone swarms, addressing the current variability in hardware capabilities and further bridging the gap between simulation and real-world applications."}, {"title": "VII. APPENDIX", "content": "This appendix provides additional information and supporting materials that complement the main content of the paper.\nA. Derivation of Eq. 6\nIn the module of Solver, we adapt the inverse of the Myerson value to calculate & in HyFoG. The Myerson value $\\phi^{-1}$ for any player $i \\in V$ in HyFog is calculated as follows:\n$\\phi^{-1}_{i} = SV_{i}(N, v_{\\epsilon})$ (8a)\n$\\phi^{-1}_{i} = \\frac{1}{\\Pi(N)} \\sum_{\\sigma \\in \\Pi(N)} [v_{\\epsilon} (P\\cup\\{i\\}) - v_{\\epsilon} (P)]$ (8b)\n$\\phi^{-1}_{i} = \\frac{1}{\\Pi(N)} [\\sum_{T \\in P\\cup \\{i\\} \\setminus E} v(T) - \\sum_{T' \\in P \\setminus E} v(T')]$ (8c)\n$\\phi^{-1}_{i} = \\frac{1}{\\Pi(N)} \\sum_{\\sigma \\in \\Pi(N)} [v_{\\epsilon} (P\\cup\\{i\\}) - v_{\\epsilon} (P)]$ (8d)\nThe transition from Eq. 8c to Eq. 8d occurs because HyFOG is connected, ensuring that any subset S \u2264 N is also connected. In this context, the components of the hypergraph (P\u222a{i}, E) and (P, E) remain unchanged. For any coalition SN, if |S| < r, the value of the coalition v(S) = 0. Otherwise, when |S| \u2265 r, $v(S) = \\sum_{T \\in \\Delta(S)} w(T)$, where \u25b3(S) denotes the subset of S with a fixed size of r."}, {"title": "B. Implementation Details of HOLA-Drone", "content": "In this section, we outline the hyperparameters used in the implementation of HOLA-Drone algorithm. The selection of these hyperparameters is crucial for the effective training and performance of the algorithm. Table III summarizes the specific values for each parameter used in our experiments."}, {"title": "C. Heterogeneous Unseen Drone Teammate Pool", "content": "In this section, we will further introduce details of the heterogeneous unseen drone teammate pool, consisting of four models: a Greedy agent, a VICSEK agent, and two D3QN-G agents. The details of the four agents are as follows:\n\u2022 Greedy Agent. The Greedy agent pursues the target independently, continually adjusting its movement to align with the target's position. Its state information includes its own position and orientation, distances and angles to teammates and evaders, and proximity to obstacles or walls. If obstacles or other pursuers are detected within its evasion range, the agent adjusts its direction to avoid them.\n\u2022 VICSEK Agent. Inspired by research on group chasing tactics [42], this strategy involves continuously computing and updating the velocity vector directed towards the evader based on the agent's current environmental state to optimize the tracking path. When the agent detects potential obstacles or other chasers nearby, it automatically evades them by applying repulsive forces with varying magnitudes and coefficients. Although the final velocity vector includes both magnitude and orientation, only the orientation is implemented in this experiment.\n\u2022 D3QN-G Agent. The D3QN-G agent is an ensemble algorithm that combines the Double Deep Q-Network (D3QN) [46] with the Greedy strategy. Initially, the D3QN-G agent employs the D3QN method to pursue one of the evaders. Once the first evader is captured, it switches to the Greedy strategy to capture the second evader. This approach is based on our experimental findings, which showed that while D3QN alone struggles with the 3-Pursuer-2-Evader task, it can effectively handle it when combined with the Greedy strategy. The action space consists of 24 artificial potential field with attention (APF-A) parameter pairs (\u03bb, \u03b7), formed by the Cartesian product of 8 A candidate parameters and 3 \u03b7 candidate parameters, following the setting in [2]. The parameter \u03b7 is used to calculate the repulsive force, while A is used to calculate the inter-robot force. The state dimension in the training environment is 9. In addition to the information mentioned in the Greedy strategy, it also includes a bit indicating whether the current agent is active. If a teammate captures the target, the teammate transitions from an active state to an inactive state, thereby ceasing movement."}]}