{"title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation", "authors": ["Minghong Cai", "Xiaodong Cun", "Xiaoyu Li", "Wenze Liu", "Zhaoyang Zhang", "Yong Zhang", "Ying Shan", "Xiangyu Yue"], "abstract": "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance", "sections": [{"title": "1. Introduction", "content": "Text-to-video (T2V) generation has made remarkable progress in the AIGC era [10, 41, 47], and breakthroughs such as Sora [28] have demonstrated impressive capabilities in generating longer videos through DiT [30] architecture and large-scale per-taining. However, feeding sequential prompts into current state-of-the-art video generation models (e.g., Kling [3], Gen3 [2], CogVideoX [45]) fails to produce coherent video sequences that exhibit natural transitions with precise prompt-following. This limitation stems from their fundamental design and single-prompt training paradigm, making them inadequate for depicting real-world scenarios' dynamic, multi-action nature.\nAlthough pioneering works [5, 27, 39] have begun exploring multi-prompt video generation, they face significant challenges. e.g., training such extended video generation models [27, 39] from scratch would require unprecedented computational resources and datasets that are practically unfeasible when the model size increases. Current zero-shot longer video generation methods [21, 33, 40] still mainly focus on the single prompt situation with longer length. Moreover, all previous works [5, 21, 27, 33, 39] are specifically designed under UNet architecture which restricts the abilities of more complex motions and increase the difficulties in multi-prompt generation. However, since Sora 's [28] groundbreaking demonstration of two-minute video generation, highlighting the scalability potential of DiT architectures [30]. Subsequent explorations have led to influential developments, notably in image generation models (Stable Diffusion 3 [13], FLUX.1 [6]) and video generations (Cog VideoX [45], Mochi1 [14]). They [6, 13, 14, 45] all adopt a specific kind of DiT architecture, i.e., Multi-Modal Diffusion Transformer (MM-DiT [13]) as the basic unit. This architecture effectively maps text and images (or video) into a unified sequence for attention computation, enabling deeper model scale abilities and achieving superior performances.\nThus, to keep the abilities of the pre-trained single prompt T2V model and take advantage of the performance of the diffusion transformer, we propose DiTCtrl, a training-free multi-prompt video generation method under the pre-trained MM-DiT video generation model. Our key observation is that the multi-prompt video generation can be considered a two-step problem: 1) Video editing over time: The new video is generated through the previous video with a new prompt. 2) Video transition over time: Two generated videos need to keep a smooth transition between clips. Thus, to perform consistent video editing, inspired by the UNet-based image editing techniques [9, 19], we explore the characteristic of the attention modules in the MM-DiT block for the first time,\nfinding that the 3D full attention has similar behaviors to that of the cross-/self-attention blocks in the UNet-like diffusion models [10, 41]. We thus apply a KV-sharing method between the video clips of different prompts to maintain the semantic consistency of the key objects [9] with the 3D attention control. Besides, we utilize a latent blending strategy for transitions between clips to connect the video clip seamlessly. Finally, to systematically evaluate our method and facilitate future research in multi-prompt video generation, we also introduce MPVBench, a new benchmark with diverse transition types and specialized metrics for assessing multi-prompt transitions. Extensive experiments on this benchmark demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.\nThe contributions of this paper can be summarized as:\n\u2022 We propose DiTCtrl, the first tuning-free approach based on MM-DiT architecture for coherent multi-prompt video generation. Our method incorporates a novel KV-sharing mechanism and latent blending strategy, enabling seamless transitions between different prompts without additional training.\n\u2022 We pioneers the analysis of MM-DiT's attention mechanism, finding that 3D full atteniton has similar behaviors to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts for enhanced generation consistency.\n\u2022 We introduce MPVBench, a new benchmark specially designed for multi-prompt video generation, featuring diverse transition types and specialized metrics for multi-prompt video evaluation.\n\u2022 Extensive experiments demonstrate that our method achieves state-of-the-art performance on multi-prompt video generation while maintaining computational efficiency."}, {"title": "2. Related Work", "content": "Video Diffusion Model. Diffusion models have achieved significant success in the field of text-to-image generation [26, 34\u201336], and these advancements have also propelled progress in video generation from text or images [4, 7, 8, 10, 11, 15, 16, 20, 37]. Among these methods, Animate-Diff [16] attempts to turn existing text-to-image diffusion models with a motion module. Other models such as Imagen Video [20] and Make-a-Video [37] train a cascade model of spatial and temporal layers directly in pixel space. To improve efficiency, many other works [4, 7, 8, 10, 11, 15] generate the videos in latent space, leveraging an auto-encoder to compress the video into a compact latent. Notably, most of these text-to-video models utilize a U-Net architecture. Subsequently, the introduction of Sora [28] demonstrates the scalability and advantages of diffusion transformer architecture [30]. Recent works such as CogVideoX [45],\nMochi1 [14], and Movie Gen [31] have adopted the DiT architecture and achieved impressive results. In this work, we build upon the open-source model CogVideoX [45], a DiT-based architecture, to explore attention control mechanisms for multi-prompt long video generation.\nLong Video Generation. Training diffusion models on long videos often demand significant computational resources. Consequently, current video diffusion models are typically trained on videos with a limited number of frames. As a result, the quality of generated videos often degrades significantly during inference when generating longer videos. To address this problem, some works [17, 18, 39, 43] employ an autoregressive mechanism for long video generation. However, due to error accumulation, these methods often suffer from quality degradation after a few iterations. Alternatively, tuning-free methods [5, 21, 33, 38, 40] have been developed to extend off-the-shelf short-video diffusion models for generating long videos without additional training. For instance, Gen-L-Video [40] processes long videos as short video clips with temporal overlapping during the denoising process. FreeNoise [33] explores the influence of initial noises and conducts temporal attention fusion based on the sliding window for temporal consistency. Inspired by these works, we propose a novel KV-sharing mechanism and latent blending strategy for seamless transitions between different segments without additional training.\nImage/Video Editing with Attention Control. Attention control is gaining popularity due to its ability to perform zero-shot image or video editing without the need for additional data. In the realm of image editing, MasaCtrl [9] enhances the existing self-attention mechanism in diffusion models by introducing mutual self-attention. This allows for querying correlated content and textures from source images, ensuring consistent and coherent edits. Prompt-to-Prompt [19] utilizes cross-attention layers to control the relation between text prompts and images, which has also been adopted in many image editing works [12, 29, 44]. When it comes to video editing [22, 23, 32], temporal consistency needs to be considered during attention control. Video-P2P [23] extend the cross-attention control from Prompt-to-Prompt to video editing. FateZero [32] fuses self-attention with a blending mask obtained by cross-attention features from the source prompt. However, all these works are designed for video-to-video translation editing with structure preservation. Differently, we aim for appearance-consistent video editing. Besides, none of these works explore attention control in diffusion transformers. In this paper, we are the first to analyze how the full attention in diffusion transformers could be utilized for editing, similar to U-Net diffusion models."}, {"title": "3. Method", "content": "We tackle the challenge of zero-shot, multi-prompt longer video generation without the need for model training or optimization. This allows us to generate high-quality videos with smooth and precise inter-prompt transitions, covering various transition types (e.g., style, camera movement, and location changes). Formally, given a pre-trained single prompt text-to-video diffusion model F and a sequence of n prompts {P1, P2, ..., Pn}, the proposed DiTCtrl can generate a coherent longer video V{1,...,n} that faithfully follows these prompts over time, which can be formulated as:\n$V_{\\{1,...,n\\}} = DiTCtrl\\{F(P_1), ..., F(P_n)\\}.$ (1)\nBelow, we first give a careful analysis of MM-DiT's attention mechanisms (Sec. 3.1). This analysis enables us to design a mask-guided full-attention KV-sharing mechanism for video editing over time (Sec. 3.2) in multi-prompt video generation. Finally, to ensure temporal coherence across different semantic segments, we further incorporate a latent blending strategy that enables smooth transitions in longer videos with multiple prompts (Sec. 3.3)."}, {"title": "3.1. MM-DIT Attention Mechanism Analysis", "content": "The MM-DiT is the fundamental architecture of the current SOTA method of Text-to-image/Video models [6, 13, 14, 45], which is fundamentally distinct from prior UNet architectures since it maps text and videos into a unified sequence for attention computation. Although it has been widely utilized, the properties of its inner attention mechanism remain insufficiently explored, which restricts its applications in our\nmulti-prompt longer video generation task. Therefore, for the first time, we conducted a comprehensive analysis of the regional attention patterns in the 3D full attention map based on the state-of-the-art video model, i.e. CogVideoX [45].\nAs shown in Fig. 2, due to the concatenation of the vision and text prompt, each attention matrix can be decomposed into four distinct regions, corresponding to different attention operations: video-to-video attention, text-to-text attention, text-to-video attention, and video-to-text attention. Below, we give the details of each region-inspired previous UNet-like structure with individual attentions [19].\nText-to-Video and Video-to-Text Attention. Previous UNet-like architectures incorporate cross-attention for video-text alignment. In MM-DiT, the text-to-video and video-to-text attention play a similar role. To validate its efficiency, we conduct a detailed analysis of the attention patterns, as illustrated in Fig. 2. Specifically, we compute the averaged attention values across all layers and attention heads, then extract attention values by selecting specific columns or rows corresponding to token indices in both text-to-video and video-to-text regions. These attention values are subsequently reshaped into an F \u00d7 H \u00d7 W format, allowing us to visualize the semantic activation maps for individual frames. As demonstrated in Fig. 2, these visualizations show remarkable precision in token-level semantic localization, effectively capturing fine-grained relationships between textual descriptions and visual elements. This discovered capability for precise semantic control and localization provides a strong foundation for adapting established image/video editing techniques to enhance the consistency and quality of multi-prompt video generation.\nText-to-Text and Video-to-Video Attention. Text-to-text and video-to-video regional attention are somehow new from the respective UNet structure. As illustrated in Fig. 3, our analysis reveals similar patterns in both components. In the text-to-text attention component (Fig. 3(a)(b), where (a) represents the attention pattern for shorter prompts and (b) illustrates the pattern for longer prompts), we observe a prominent diagonal pattern, indicating that each text token primarily attends to its neighboring tokens. Notably, there are distinct vertical lines that shift backward as the text sequence length increases, suggesting that all tokens maintain significant attention to the special tokens at the end of the text sequence. For the video-to-video attention component, since MMDiT flat the spatial and temporal token for 3D attention calculation, our analysis at the single-frame level reveals a distinctive diagonal pattern in spatial attention (Fig. 3(c)). More significantly, when examining attention maps constructed from tokens at identical spatial positions across different frames, we also observe a pronounced diagonal pattern (Fig. 3(d)). This characteristic mirrors those found in recent UNet-based video models of the spatial-attention and temporal attention, such as VideoCrafter [24] and Lavie [42], aligning with the findings reported in [25]. Since previous works only train the specific part of the diffusion model for more advanced control and generation, our finding provides strong evidence for these methods from MM-DiT perspectives.\nOverall, the presence of these consistent diagonal patterns in the MM-DiT architecture demonstrates robust frame-to-frame correlations, which proves essential for maintaining spatial-temporal coherence and preserving motion fidelity throughout the video sequence."}, {"title": "3.2. Consistent Video Generation Over Time", "content": "Based on the previous analysis, we find the attention mechanism in the MM-DiT has a similar behavior as that in the UNet-like video diffusion model with our specific design. Thus, we propose the masked-guided KV-sharing strategy for consistent video generation over time for our multi-prompt video generation task.\nSpecifically, as shown in Fig. 4, to generate the consistent video between prompt Pi\u22121 and prompt Pi, we utilize the intermediate attentions from the i \u2013 1-th and i-th prompt in MM-DiT to generate the attention mask of the specific consistent object, respectively. This is achieved by averaging all the text-to-video/video-to-text parts of the 3D full attention with the given specific subject token. With these masks, we then perform mask-guided attention fusion to generate the\nnew attention features of the prompt Pi. Inspired by MasaCtrl [9], we directly utilize the key and values from the prompt Pi\u22121 to guide the generation of prompt Pi to generate the consistent appearance over time.\nFormally, at step t, we perform a forward pass with the fixed MM-DiT backbone with prompt Pi\u22121 and next prompt Pi, respectively, to generate intermediate regional cross-attention maps. Then we average the attention maps across all heads and layers with the same spatial resolution H \u00d7 W and temporal frames F. The resulting cross-attention maps are denoted as A \u2208 RF\u00d7H\u00d7W\u00d7N, where N is the number of the textual tokens. We then obtain the averaged cross-attention map for the token correlated to the foreground object. We denote Mi\u22121 and M\u2081 as masks extracted for the foreground objects in Vi\u22121 and Vi, respectively. With these masks, we can restrict the object in Vi to query contents information only from the object region in Vi\u22121:\n$f_o = Attention(Q, K_{i-1}, V_{i-1}; M_{i-1}),$ (2)\n$f = Attention(Q, K_{-1}, V_{-1}; 1 \u2013 M_{i-1}),$ (3)\n$f_l = f_o * M_i + f * (1 \u2013 M_i),$ (4)\nwhere fl is the final attention output. Then, we replace the feature map of the current step to fo for further calculation."}, {"title": "3.3. Latent Blending Strategy for Transition", "content": "While our previous methods enable semantic consistency between clips, achieving smooth transitions between different semantic segments still needs to be carefully designed. Thus, we propose a latent blending strategy to ensure temporal coherence across different semantic segments, inspired by recent works in single-prompt long video generation [33, 46].\nAs illustrated in Fig. 5, our approach introduces overlapped regions between adjacent semantic video segments (video Vi-1 and video V\u2081). For each frame position in the overlapped region, we apply a position-dependent weight function that follows a symmetric distribution - frames closer to their respective segments receive higher weights while those at the boundaries receive lower weights. This weighting scheme ensures smooth transitions between different semantic contexts.\nFormally, given two adjacent video segments Vi\u22121 and Vi generated from prompts Pi-1 and Pi respectively, we propose a latent blending strategy as follows. Let T denote the number of overlapped frames between segments. For frame position t in the overlapped region, we compute its blended latent features zt as:\n$z_t = w(t) \u00b7 z_{t,i-1} + (1 \u2212 w(t)) \u00b7 z_{t,i},$ (5)\nwhere zt,i-1 and zt,i are latent features from Vi\u22121 and Vi respectively, and w(t) is a position-dependent triangular weight function defined as:\n$w(t) = min(\\frac{2(t+0.5)}{T},\\frac{2(t+0.5)}{T});$, (6)\nThe key advantage of our approach is that it requires no additional training while effectively handling transitions between different semantic contexts. During each denoising\nstep, we first process each segment independently, then progressively blend the latent features in the overlapped regions using position-dependent weights. This strategy maintains temporal coherence while smoothly transitioning between different semantic contexts, making it particularly suitable for multi-prompt video generation tasks."}, {"title": "4. Experiments", "content": "We implement DiTCtrl based on CogVideoX-2B [45], which is a state-of-the-art open-source text-to-video diffusion model based on MM-DiT. In our experiments, we generate multi-prompt conditioned video, and each video clip consists of 49 frames with 480 \u00d7 720 resolution. Moreover, we employ ChatGPT [1], to generate multiple transitions of different types. We set latent sampling frames and overlap sizes to 13 and 6 in our experiments. The experiments are conducted on a single NVIDIA A100 GPU."}, {"title": "4.1. Qualitative Results", "content": "We conduct comprehensive qualitative comparisons with state-of-the-art multi-prompt video generation methods [33, 38, 40] and leading commercial solutions [3]. To ensure a fair comparison, we additionally implement FreeNoise [33] on the CogVideoX backbone as an enhanced baseline.\nAs shown in Fig. 6, our proposed method demonstrates superior performance across three critical aspects: text-to-video alignment, temporal coherence, and motion quality. While Kling [3] exhibits impressive capabilities in high-quality video generation, it is limited to simultaneous multi-semantic blending rather than sequential semantic transitions, highlighting the significance of our work in achieving temporally evolving content across multiple prompts.\nOur comparative analysis reveals distinct characteristics and limitations of existing approaches. Gen-L-Video [40] suffers from severe temporal jittering and occasional subject disappearance, compromising overall video quality. Video-Infinity [38] and FreeNoise [33] both demonstrate successful scene-level semantic changes but lack physically plausible motion - for instance, in Fig. 6, vehicles appear to be in motion while remaining spatially fixed, which is a limitation inherent to their UNet-based abilities. In contrast, FreeNoise+DiT leverages the DiT architecture's abilities to achieve more realistic object motion but struggles with semantic transitions, resulting in noticeable discontinuities between segments. Our proposed DiTCtrl method preserves the inherent capabilities of the pre-trained DiT model while addressing these limitations, enabling smooth semantic transitions and maintaining motion coherence throughout the video sequence. For a more comprehensive evaluation, we provide additional frame-level and video-level comparisons with extensive qualitative examples in the supp."}, {"title": "4.2. Quantitative Results", "content": "In this section, we will first elaborate on our proposed new benchmark MPVBench for evaluating multi-prompt video generation, and then discuss the quantitative results.\nMPVBench. MPVBench contains a diverse prompt dataset and a new metric customized for multi-prompt generation. Specifically, leveraging GPT-4, we produce 130 long-form prompts of 10 different transition modes. Then, for multi-prompt video generation, we observe that the distribution of the CLIP features differs between single-prompt and multi-prompt scenarios. As shown in Fig. 7, the feature points of natural video follow a continuous curve, while those of two concatenated isolated videos follow two continuous curves with a breakpoint in the middle. Since the common CLIP similarity calculates the average of neighborhood similarities, the difference between natural and isolated video only occurs at the breakpoint, which becomes very small when divided by the number of frames. To address this limitation, we propose CSCV (Clip Similarity Coefficient of Variation), a metric specifically designed to evaluate the smoothness of multi-prompt transitions, defined as:\n$s_i = \\frac{x_i}{ \\sum_{1}^{T} x_{i+1}}, i = 1,...,n \u2212 1$ (7)\n$score = \\frac{1}{1 + \\lambda (\\frac{\\sigma(s)}{\\mu(s)})} $ (8)\nwhere xi denotes frame features, \u03c3and \u03bcare standard deviation and average respectively. The Coefficient of Variation CV = \u03c3(s)/\u03bc(s) describes the degree of uniformity, which can largely punish the isolated situation. The function $\\frac{1}{1+x}$ projects the score to [0, 1], the larger the better.\nAutomatic Evaluation. We conduct the automatic evaluation with our MPVBench. From Table 1 one can see that our method achieves the highest CSCV score, demonstrating superior transition handling and overall stability in generation patterns. While FreeNoise ranks second with relatively strong stability, other methods significantly lag behind in this aspect, which is consistent with the T-SNE visualization of CLIP embedding as shown in Fig. 7. In terms of motion smoothness, our approach exhibits superior performance in motion quality and consistency. Regarding Text-Image Similarity metrics, although FreeNoise and Video-Infinity achieve higher scores, this can be attributed to our method's kv-sharing mechanism, where subsequent video segments inherently learn from preceding semantic content."}, {"title": "4.3. Ablation Study", "content": "We conducted ablation studies to validate the effectiveness of DiTCtrl's key components: latent blending strategy, KV-sharing mechanism, and mask-guided generation as shown in Fig. 8. The first row shows results that directly using text-to-video models results in abrupt scene changes and disconnected motion patterns, failing to maintain continuity in the athlete's movements from surfing to skiing. The second row demonstrates that DiTCtrl without the latent blending strategy achieves basic video editing capabilities\nbut lacks smooth transitions between scenes. Without KV-sharing (third row), DiTCtrl exhibits unstable environmental transitions and significant motion artifacts, with inconsistent character scaling and deformed movements. Moreover, DiTCtrl without mask guidance (fourth row) improves motion coherence and transitions but struggles with object attribute confusion across different prompts and environments. On the other hand, The full DiTCtrl implementation provides the most precise control over generated content, demonstrating superior object consistency and smoother transitions between prompts while maintaining desired motion patterns. These results validate our analysis of MM-DiT's attention mechanism and its role in enabling accurate semantic control."}, {"title": "4.4. More Applications", "content": "Single-prompt Longer Video Generation.\nOur method can naturally work on single-prompt longer video generation. As illustrated in Fig. 9, using the prompt \"A white SUV drives on a steep dirt road\", our approach successfully generates videos that are more than 12 times longer than the original length, while maintaining consistent motion patterns and environmental coherence.\nVideo Editing. We show how we use our methods to achieve video editing performance (e.g. \u201creweight\" and \"word swap\"). The cases are provided in the Appendix. C."}, {"title": "5. Conclusion", "content": "In this paper, we introduce DiTCtrl, a novel, tuning-free method for multi-prompt video generation using the MM-DiT architecture. Our pioneering analysis of MM-DiT's attention mechanism reveals similarities with the cross/self-attention blocks in UNet-like diffusion models, enabling mask-guided semantic control across prompts. With KV-sharing mechanisms and latent blending strategies, DiTCtrl ensures smooth transitions and consistent object motion between semantic segments, without extra training. We also present MPVBench, the first extensive evaluation framework for multi-prompt video generation, set to advance future research in this field.\nLimitation & Future Work. While our method demonstrates state-of-the-art performance, there remain two primary limitations. First, compared to image generation models, current open-source video generation models exhibit relatively weaker conceptual composition capabilities, occasionally resulting in attribute binding errors across different semantic segments. Second, the computational overhead of DiT-based architectures presents challenges for inference speed. These limitations suggest promising directions for future research in enhancing semantic understanding and architectural efficiency."}, {"title": "A. Implementation Details", "content": "Details. We implement DiTCtrl based on CogVideoX-2B [45], which is a state-of-the-art open-source text-to-video diffusion model based on MM-DiT. The hyperparameters and implementation details are shown in Tab. 3.\nBaselines. To demonstrate the effectiveness of our proposed DiTCtrl, we conduct comprehensive qualitative comparisons with state-of-the-art multi-prompt video generation methods [33, 38, 40] and leading commercial solutions Kling [3]. Both FreeNoise and Video-Infinity are built upon\nthe VideoCrafter2 [11] framework. To ensure a fair comparison of base models, we implement FreeNoise [33] as an enhanced baseline by directly incorporating their noise rescheduling strategy into the CogVideoX framework.\nMPVBench. We introduces a new benchmark MPVBench, which is specified designed for multi-prompt video generation task. MPVBench contains a diverse prompt dataset and a new metric customized for multi-prompt generation. Specifically, leveraging GPT-4, we produce 130 long-form prompts of 10 different transition modes (background transition, subject transition, camera transition, style transition, lighting transition, location transition, speed transition, emotion transition, clothing transition, action transition). The instruction of prompt generator is provided in Fig. 19.\nAutomatic evaluation. For automatic evaluation, we generate videos using 130 prompts from our MPVBench, with three random seeds set. Then, we evaluate the generated video by three metrics: CSCV (Clip Similarity Coefficient of Variation), Motion Smoothness, Text-Image Similarity.\nHuman evaluation. In our user study, we combined our generated videos with those produced by four other baseline methods. We asked a total of 28 participants to evaluate the videos across four dimensions: overall preference, motion pattern, temporal consistency, and text alignment. Specifically, we asked all participants to rank the results of these methods for each of the following questions, and assigned a score from 1 (lowest quality) to 5 (highest quality) for these five methods:\n\u2022 Overall Preference: \u201cPlease rank the overall video preference.", "Pattern": "How natural and realistic are the motion in the video?\" This evaluates whether the motion of objects in the generated video appears physically plausible and natural, such as whether vehicles drive realistically, animals move naturally, or human actions appear authentic.\n\u2022 Temporal Consistency: \"How smoothly does the video content transition across different frames?\" This metric evaluates the temporal coherence of the generated video, focusing on whether the transitions between consecutive frames are natural and continuous, without abrupt changes or visual artifacts. It measures the video's ability to maintain visual continuity throughout its duration.\n\u2022 Text Alignment: \"To what extent does the video content match the given text descriptions?\" This assesses the semantic fidelity between the generated visual content and the input text prompts, examining whether the video accurately captures and visualizes the key elements and actions described in the prompts. It measures how well the visual narrative aligns with the intended textual description.\nMask-guided Implementation Details. We show how mask extracted from MM-DiT attention map is utilized for mask-"}, {"title": "B. More Qualitative Results", "content": "More results are provided in Fig. 11 and Fig. 12. Our method DiTCtrl can generate multi-prompt videos with good temporal consistency and strong prompt-following capabilities, demonstrating cinematographic-style transitions in depicting the boy's riding sequence. We also give more qualitative comparisons with state-of-the-art multi-prompt video generation methods [33, 38, 40], our reproduced FreeNoise+DiT, and leading commercial solutions Kling [3]. We show the motion transition case, and background transition case in Fig. 13 and Fig. 14. Our comparative analysis reveals distinct characteristics and limitations of existing approaches. Gen-L-Video [40] suffers from severe temporal jittering and occasional subject disappearance, compromising overall video quality. Video-Infinity [38] and FreeNoise [33] both demonstrate successful scene-level semantic changes but lack physically plausible motion - for instance, in Fig. 13, dark knight appear to be in motion while remaining spatially fixed, which is a limitation inherent to their UNet-based abilities. In contrast, FreeNoise+DiT leverages the DiT architecture's abilities to achieve more realistic object motion but struggles with semantic transitions, resulting in noticeable discontinuities between segments. Our proposed DiTCtrl method preserves the inherent capabilities of the pre-trained DiT model while addressing these limitations, enabling smooth semantic transitions and maintaining motion coherence throughout the video sequence. More comparison of visualization case and our results are shown in our project page."}, {"title": "C. Applications", "content": "Based on our exhaustive analysis and exploration of attention control in MM-DiT architecture, our method could be applied to other tasks like single prompt longer video generation and video editing and achieves promising results."}, {"title": "C.1. Single-prompt Longer Video Generation", "content": "Although our primary objective is to address multi-prompt video generation, we discover that our method demonstrates remarkable effectiveness in single-prompt longer video generation as well. Our method can naturally work on single-prompt longer video generation. As illustrated in Fig. 15, our approach successfully generates longer videos, while maintaining consistent motion patterns and environmental coherence."}, {"title": "C.2. Video Editing", "content": "In this work, we conduct an in-depth analysis of MM-DiT's attention maps, which can be categorized into four components: Text-to-Video and Video-to-Text Attention, Text-to-Text and Video-to-Video Attention. Through our analysis of Text-to-Video and Video-to-Text Attention, we observe that semantic maps can be obtained by specifying token indices, suggesting potential for semantic control. We have emphasized the use of extracted foreground-background segmentation semantic maps to guide video generation, effectively preventing semantic confusion between foreground and background elements. In this section, we demonstrate video editing capabilities through two approaches: Reweight and Word Swap.\nAttention Re-weighting. As illustrated in Fig. 16, we can\nachieve semantic enhancement or attenuation by increasing or decreasing the values in rows or columns corresponding to token j in the Text-to-Video and Video-to-Text Attention maps. In Fig. 16 (a), we demonstrate semantic attenuation by reducing Text-Video Attention values in the row and Video-Text Attention values in the column corresponding to \"pink\". In Fig. 16 (b), we achieve semantic enhancement by increasing Text-Video Attention values in the row and Video-Text Attention values in the column corresponding to \"snowy\". These results validate the semantic control capabilities of Text-Video and Video-Text Attention in MM-DiT.\nWord Swap. Building upon the concept introduced in Prompt-to-prompt [19], this approach allows users to swap tokens in the original prompt with alternatives (e.g., changing P = \"a large bear\" to \"a large lion\"). The primary challenge lies in maintaining the original composition while accurately reflecting the content of the modified prompt. Our DiTCtrl method incorporates KV-sharing, similar to the word swap mechanism in [19], where we share key-value pairs from the previous prompt to compute the corresponding video for the subsequent prompt across selected layers and steps. Specifically, DiTCtrl (without latent-blending strategy) enables token-replacement video editing while ensuring consistency in other content elements, as demonstrated in Fig. 17. This implementation validates the feasibility of prompt-to-prompt-style video editing within the MM-DiT architecture."}, {"title": "D. Prompt Generator", "content": "We use GPT4 for longer multi-prompt generation, our prompts are shown in Fig. 19. This figure shows the generation process of \"background transition\", and we generate"}, {"title": "E. Ablation Study", "content": "E.1. Quantitative Results of Components\nAs shown in Tab. 4, our latent blending strategy (second row) demonstrates superior video consistency compared to isolated clips (first row), as evidenced by higher CSCV scores - our proposed metric for evaluating multi-prompt transition smoothness. Furthermore, our KV-Sharing mechanism further improves the CSCV value, achieving enhanced stability. The mask-guided approach(fourth row) and its unmasked counterpart(third row) report comparable scores, suggesting that the contribution of masking foreground object to overall frame transition smoothness is modest. However, our qualitative analysis in Section E.2 reveals that the mask-guided method yields superior visual results.\nAdditionally, in our evaluation of motion smoothness, our full method (DiTCtrl) achieves optimal performance. Re"}]}