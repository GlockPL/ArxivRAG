{"title": "Efficient Relational Context Perception for\nKnowledge Graph Completion", "authors": ["Wenkai Tu", "Guojia Wan", "Zhengchun Shang", "Bo Du"], "abstract": "Knowledge Graphs (KGs) provide a structured representation of knowledge but\noften suffer from challenges of incompleteness. To address this, link prediction\nor knowledge graph completion (KGC) aims to infer missing new facts based on\nexisting facts in KGs. Previous knowledge graph embedding models are limited in\ntheir ability to capture expressive features, especially when compared to deeper,\nmulti-layer models. These approaches also assign a single static embedding to each\nentity and relation, disregarding the fact that entities and relations can exhibit\ndifferent behaviors in varying graph contexts. Due to complex context over a fact\ntriple of a KG, existing methods have to leverage complex non-linear context\ncoder, like transformer, to project entity and relation into low dimensional rep-\resentations, resulting in high computation cost. To overcome these limitations,\nwe propose Triple Receptance Perception (TRP) architecture to model sequential\ninformation, enabling the learning of dynamic context of entities and relations.\nThen we use tensor decomposition to calculate triple scores, providing robust\nrelational decoding capabilities. This integration allows for more expressive rep-\nresentations. Experiments on benchmark datasets such as YAGO3-10, UMLS,\nFB15k, and FB13 in link prediction and triple classification tasks demonstrate\nthat our method performs better than several state-of-the-art models, proving\nthe effectiveness of the integration.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KG) [1] as an information system for providing well-structured\nreal-world knowledge, are widely used in various applications such as question answer-\ning [2] and recommendation systems [3]. A Knowledge Graph can be conceptualized as\na directed graph, represented in the form of triples (head entity, relation, tail entity).\nHowever, KGs are commonly incomplete with many missing relations [4]. Link\nprediction, also known as Knowledge Graph Completion (KGC), is a form of auto-\nmated reasoning used to infer missing parts of KGs. A popular approach for KGC\nis Knowledge Graph Embedding (KGE), which aims to project entities and relations\ninto low-dimensional continuous vector space. Figure 1 provides a schematic illustra-\ntion of a partial KG and how KGC infers potential missing relations between Cristiano\nRonaldo and Saudi Arabia.\nCurrent KGE approaches include translation-based models, semantic-matching\nmodels and neural network-based models. Translation-based models [4\u20136] establish\nlinear translation rules between the head entity and the tail entity via the rela-\ntion. Semantic-matching [7\u201310] models employ various score functions to measure the\nembedding similarity between entities and relations. However, they are facing an issue\nthat the high-dimensional embeddings required to encode all the information when\ndealing with large KG, which can lead to overfitting and increased complexity. And\nthey usually learn a single static representation. However, entities and relations should\nhave different meanings when involved in different contexts. Besides, they mainly rely\non additive or multiplicative operations. These factors limit their expressiveness [11].\nNeural network-based models [12, 13] obtain expressive representations from pure\nembeddings using different neural network models which have achieved good results\non KGC task. One of them is Transformer-based model. Transformer [14] can learn\nthe representation of entities and relations and contextual information of KG, thus\nit is suitable for link prediction tasks. CoKE [13] is one of the representative mod-\nels. Although Transformer-based models have outperformed previous models with\nsimilarly high-dimensional embeddings, they still face scalability issues.\nTo address these limitations, we propose a novel architecture called Triple Recep-\ntance Perception (TRP), which is specifically designed to capture dynamic and\nsequential contexts in knowledge graphs. Unlike conventional static methods, TRP\ndynamically models the context of entities and relations by incorporating sequential\ninformation directly into the representation learning process. This mechanism ensures\nthat the embeddings are adaptively conditioned on varying graph contexts, thereby\nenhancing the model's capacity to encode complex relational dependencies.\nIn addition, to ensure efficient relational decoding, we integrate TRP with a\nTucker decomposition module for triple scoring. Tucker decomposition, used widely\nin machine learning, can decompose a tensor into a set of matrices and a smaller core"}, {"title": "2 Related Work", "content": "One mainstream approach for Knowledge Graph Completion (KGC) is based on\nKnowledge Graph Embedding (KGE) methods [11]. The goal of KGE is to embed\nthe representations of entities and relations into low-dimensional continuous vec-\ntor space. KGE-based methods can be broadly classified into the following three\ntypes: translation-based models, semantic-matching models and neural networks-based\nmodels.\nTranslation-based models establish linear translation rules between the head entity\nand the tail entity via the relation. TransE [5], the most representative translation-\nbased model, represents entities and relationships in the same space as vectors. The\ncore idea of TransE is the sum of the head entity vector and the relationship vector\nshould be as close as possible to the tail entity vector, thereby completing the rep-\nresentation of all entities and relationships. Although TransE is simple and efficient,\nit involves issues when modeling complex relations. Thus, TransH [4], TransR [15],"}, {"title": "3 Methodology", "content": "In this section, we present a novel approach to knowledge graph link prediction that\ncombines two components: the Triple Receptance Perception (TRP) architecture for"}, {"title": "3.1 Encoder", "content": ""}, {"title": "3.1.1 Triple Receptance Perception Architecture", "content": "The Triple Receptance Perception (TRP) architecture is designed to address the chal-\nlenges of capturing dynamic and sequential contexts in knowledge graphs. Unlike static\nembedding models that assign fixed representations to entities and relations, TRP\nallows for the learning of dynamic, context-dependent embeddings. This is achieved\nthrough its ability to model sequential information across knowledge graph triples,\nensuring that the behavior of entities and relations is adapted to varying graph con-\ntexts. TRP consists of residual blocks, each primarily composed of two key sub-blocks:\nthe time-mixing sub-block and the channel-mixing sub-block [30]. This architecture\neffectively combines the strengths of recurrent neural networks (RNNs) and attention\nmechanisms, enabling efficient integration of inductive or sequential information."}, {"title": "3.1.2 Time Mixing Module", "content": "The operation of the time mixing module can be described as follows. Consider an\ninput sequence x = (eh, er), where eh represents the embedding of head entities, er\nrepresent the embedding of relations. The output embedding denoted as o = (oh, or),\ncaptures the contextual information and dependencies within the input sequence. The\noutput is computed using the following equation,\n$o_t = W_o \\cdot (\\sigma(r_t) \\otimes wkv_t),$\nwhere t \u2208 {h, r} and W is a weighting matrix for output vectors. Note that \u03c3(rt) is\nthe sigmoid of receptance vector, and rt is calculated as:\n$r_t = W_r \\cdot (\\mu_r x_t + (1 - \\mu_r) x_{t-1})$\nIn this equation xt stands for input embedding at stept and \u03bcr is a interpolation factor.\nThe term wkvt aligns with the methods used in the Attention-Free Transformer[31],\nand can be expressed as\n$wkv_t = \\frac{\\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} \\odot V_i + e^{u+k_t} \\odot U_t}{\\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} + e^{u+k_t}},$\nwhere w is the channel-wise learnable vector for the previous input, while u is the\nspecialized weighting factor applied to the current input. k, v are from K, V respec-\ntively. The primary role of u is to provide a distinct attention channel for the current\nstep, thereby mitigating the potential degradation issues associated with w. Conse-\nquently, the model learns the vector u to better balance and capture information across\ndifferent positions. The key and value vectors are calculated as\n$k_t = W_k \\cdot (\\mu_k \\odot x_t + (1 - \\mu_k) x_{t-1}),$\n$U_t = W_v \\cdot (\\mu_v \\odot x_t + (1 - \\mu_v) x_{t-1}).$\nSimilarly, \u03bc\u03ba and \u03bc\u03c5 are interpolation factors, while Wk and Wv are the weighting\nmatrices for the key and value, respectively. Regarding the dimensions of these matri-\nces, let dio denote the input/output size and datt represent the size of time-mixing\nmodule. The projection matrix Wo \u2208 Rdio\u00d7datt is defined for output projections. The\nprojection matrices for receptance, key, and value are Wr \u2208 Rdatt\u00d7dio, Wk \u2208 Rdatt\u00d7dio,\nand W\u2208 Rdatt\u00d7dio, respectively.\nNote that wkvt is the weighted summation of the input in the interval [1,t],\nwhich permits the causality in inference and enables efficient inference like RNNs.\nAdditionally, Eq. (3) can be calculated recursively,\n$wkv_t = \\frac{a_{t-1} + e^{u+k_t} U_t}{b_{t-1} + e^{u+k_t}}$\n$a_t = e^{-w} a_{t-1} + e^{k_t} U_t$\n$b_t = e^{-w} b_{t-1} + e^{k_t}$"}, {"title": "3.1.3 Channel Mixing Module", "content": "In the channel mixing module, the modified input sequence x' = (x'1, x'2,...,x't)\nundergoes transformation as\n$r'_{t} = W_{r'}\\cdot (\\mu'_{t}x'_{t} + (1 - \\mu'_{t})x'_{t-1}),$\n$k'_{t} = W_{k'} \\cdot (\\mu'_{k}x'_{t} + (1 - \\mu'_{k})x'_{t-1}),$\n$o_t = \\sigma(r'_{t}) \\cdot (W_{o'} \\odot max(k'_{t}, 0)^2),$\nwhere \u03bc'r, \u03bc'k are interpolation factors and Wr', Wk', Wr' are separate weighting matrix\nfor transformed vectors. We adopt squared ReLU activation function here for the\noutput."}, {"title": "3.1.4 Integration of Modules", "content": "Each block processes the input by sequentially applying a dropout-enhanced time\nmixing followed by a channel mixing operation, formulated as\nx' = x + Dropout(TimeMixing(LayerNorm(x))),\nx\" = x' + Dropout(ChannelMixing(LayerNorm(x'))).\nThis design introduces dropout layers prior to residual connections to mitigate\noverfitting issues.\nThese components collectively enable TRP architecture to handle sequences effec-\ntively, maintaining causality and enhancing model robustness through its recurrent-\ninspired mechanism."}, {"title": "3.2 Decoder", "content": "Tucker decomposition is a method that decomposes a high-dimensional tensor into the\nproduct of a low-dimensional core tensor and multiple matrix factors. In our model,\nwe employ Tucker decomposition as the decoder.\nIn the knowledge graph completion task, the knowledge graph is typically repre-\nsented as a triplet set (eh, r, et), where eh is the head entity, r is the relation, and et\nis the tail entity. After obtaining the output representations of the head entity and\nthe relation, we can construct a third-order tensor, where each element is a triplet\ntuple with a value of 1 if the triplet is true, and 0 otherwise. We then apply Tucker\ndecomposition, a tensor factorization method, to decode the embedded information.\nLet Ho = (\u1ebdh;\u0113r) denote the encoder output representation, where \u1ebdh \u2208 Rd and\ner \u2208 Rd are the output representations of the head entity and relation, respectively.\nThe scoring function in decoder is defined as:\n$\\phi(h, r, t) = W_c \\times_1 \\tilde{e_h} \\times_2 \\tilde{e_r} \\times_3 \\tilde{e_t},$"}, {"title": "3.3 Training", "content": "During training, we implement reciprocal learning by adding inverse triples (t, r-1,h)\nto our dataset. This approach enhances the model's understanding of relationships\nin both directions. Additionally, we employ the 1-N scoring method, where each pair\n(h, r) is evaluated against all possible entities as the target t. This scoring mechanism\nis crucial for effectively handling the large scale of entity combinations.\nTo measure the predictive accuracy and improve the model's performance, we\nemploy the cross-entropy loss function between the label and the prediction as our\ntraining loss:\n$L(x) = - \\sum_t y_t log p_t,$\nwhere yt denotes the true label's probability distribution, one-hot encoded with a '1'\nfor the correct class and 'Os' elsewhere. The predicted probabilities pt are computed\nusing the softmax function applied to the logits (h, r, t) output by the model. The\nsoftmax function is defined as follows:\n$p_t = \\frac{e^{\\phi(h,r,t)}}{\\sum_{k \\in [V]} e^{\\phi(h,r,k)}},$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment setup", "content": ""}, {"title": "4.1.1 Datasets", "content": "We evaluate our model on link prediction task using both small and large datasets\n(dataset statistics are provided in Table 1):\nUMLS [32] is the Unified Medical Language System (UMLS), developed by\nMcCray, and is a comprehensive resource from the biomedical domain. The entities\nare concepts of drug and disease names and the relations are concepts of diagnosis\nand treatment. It serves as a small knowledge graph.\nFB15k [5] is a subset of Freebase, a large database containing real-world facts.\nThe entities encompass concepts such as movies, actors, awards, and sports. It has the\nlargest number of relations, that is 1,345.\nYAGO3-10 [33] is a subset of YAGO3, including entities associated with 46 rela-\ntions. Most of the triples describe attributes of persons, such as citizenship, gender,\nand profession. This dataset is significantly larger than the others.\nFB13 [19] is a subset of Freebase, including 75,043 entities and 13 relations. The\nvalidation and test datasets consist of negative triples, which are used for the triple\nclassification task."}, {"title": "4.1.2 Baseline", "content": "For fairness, we do not compare with models that utilize auxiliary information like\ntext description. We choose several representative methods, which can be divided into\ntwo categories: methods using triples and methods using context.\nMethods using triples rely exclusively on the structural information present in\nthe knowledge graph, focusing on the relationships between entities as defined by the\ntriples. For comparison, we include TransE [5], DistMult [8], ComplEx [9], RotatE [6],\nTuckER [10], ConvE [12], COKE [13], HAKE [17], and HousE [18] as our baselines.\nIn contrast, methods using context leverage additional information, such as graph\nstructure or logic rules, during the training process. For these, we include Neural-LP\n[26], R-GCN [22], Rlogic [34], and ChatRule [29] as our baselines."}, {"title": "4.1.3 Evaluation Metrics", "content": "For each triplet (h,r,t) \u2208 S, where S is the validation or testing set, we compute the\nscore of (t, r-1, h') for all h' \u2208 E and determine the rank of h. Similarly, we compute\nthe scores of (h,r,t') for all t' \u2208 E to determine the rank of t. The relation r is not\ncompared.\nFollowing previous works, we evaluate our model for link prediction using two\nstandard metrics:\nMean Reciprocal Rank (MRR): This metric is the average of the reciprocal\nranks of the correct triples among all candidate triples:\n$MRR = \\frac{1}{|S|} \\sum_{i=1}^{|S|} \\frac{1}{rank_i},$\nwhere rank, is the rank assigned to the i-th true triple.\nHits@k: This metric measures the proportion of times the true triple is ranked\nwithin the top k candidate triples:\n$Hits@k = \\frac{1}{|S|} \\sum_{i=1}^{|S|} I(rank_i \\le k),$"}, {"title": "4.1.4 Training Details", "content": "We implement our model based on the PyTorch library and conduct all experiments\nwith a single NVIDIA RTX 4090 GPU Our settings for hyper-parameter selection as\nfollows: The embedding size k is selected in {64, 96, 128, 192, 256}. The number of TRP\nblocks is selected in {2, 4, 6, 8}. We employ dropout on all layers, with the rate tuned in\n\u03c1\u2208 {.2, 3, .4, .5}. We use the Adam optimizer. The learning rate is chosen from 0.0005\nto 0.01, and different learning rates can be selected according to different datasets.\nWe train with batch size B = 512 for at most 500 epochs. The best hyper-parameter\nsetting on each dataset is determined by MRR on the dev set."}, {"title": "4.2 Results", "content": ""}, {"title": "4.2.1 Link Prediction Results", "content": "Link prediction aims to predict missing head or tail entity of a triple, which is a widely\nemployed evaluation task for knowledge graph completion models. Table 2 showcases\nthe performance of our proposed method relative to existing representative models on\nFB15k, YAGO3-10, and UMLS datasets. Our method stands out for its performance,\nconsistently outperforming both methods using triples and methods using context.\nOn FB15k, although our method slightly lags behind the transformer-based CoKE in"}, {"title": "4.2.2 Triple Classification", "content": "Triple classification aims to judge whether a given triple (h,r,t) is correct or not.\nThis is a binary classification task, which has been explored in [4, 19] for evaluation.\nIn this task, we use two datasets, FB13 and FB15k. We need negative triples for\nevaluation of binary classification. The datasets FB13 released by NTN [19] already\nhave negative triples, which are obtained by corrupting correct triples. As FB15k with\nnegative triples has not been released by previous work, we construct negative triples\nfollowing the same setting in [19]. For triple classification, we set a relation-specific\nthreshold dr. For a triple (h, r, t), if the dissimilarity score obtained by fr is below dr,\nthe triple will be classified as a false fact, otherwise it will be classified as a true fact.\nAnd different values of dr will be set for different relations. We use the same settings\nas link prediction task, all parameters are optimized on the validation datasets to\nobtain the best accuracies. We compare our method NTN [19], TransE [5], TransH [4],\nTransR [15], Distmult [8], ComplEx [9], CoKE[13]. The results are listed in Table 3.\nExperimental results show that our proposed model achieves the best performance on\nthe FB13 dataset with an accuracy of 88.6%, surpassing all other baseline methods,\nincluding Transformer-based method CoKE. On the FB15k dataset, our model also\nperforms competitively, achieving an accuracy of 89.0%, slightly below CoKE but\noutperforming other models. These results highlight the effectiveness of our model\nin capturing the underlying relationships between entities and relations in knowledge"}, {"title": "4.2.3 Ablation Study", "content": "We conducted an ablation study on FB15k and YAGO3-10 to evaluate the contribu-\ntions of Tucker decomposition decoder and the TRP blocks. We performed experiments\nby removing the perception block encoder and Tucker decomposition decoder sepa-\nrately. When the Tucker decomposition decoder was removed, we used the embedding\n\u1ebdr) output by TRP blocks directly as the final predicted entity embedding. When the\nTRP block encoder is removed, the model can be viewed as an original TuckER model\n[10]. Table 4 shows the results of our ablation study on the FB15k and YAGO3-10.\nAs shown, the combination of both modules yields the best results, confirming that\nboth components are essential for achieving optimal performance.\nAnd we also conduct an ablation study to evaluate the impact of different decoders\non the performance of knowledge graph completion. We perform experiments on\nFB15k with the following decoders: MLP, TransE, DistMult and ComplEx. To ensure\na fair comparison, we keep all other experimental settings as consistent as possible.\nAs shown in Table 5, the results shows that our model with Tucker decomposition\ndecoder achieves the best performance. This demonstrates the effectiveness of Tucker\ndecomposition as decoder, which is able to capture more complex interactions between\nentities and relations."}, {"title": "4.2.4 Parameter Sensitivity Analysis", "content": "In this section, we aim to explore the impact of different hyperparameters on the\noverall performance of the model. Specifically, we investigated how the number of TRP\nblock layers in perception block encoder and the dimensions of the embedding affect\nthe model's performance.\nWe begin by analyzing the effect of varying the number of TRP block layers. We\nexperimented with models containing 2, 4, 6, 8, 10 layers, respectively. Each model\nwas trained separately, and MRR and Hits@10 were used as the evaluation metrics.\nWe observed that the number of TRP block layers had minimal impact on model\nperformance, as shown in the Figure 3. The MRR remained relatively stable across dif-\nferent layer configurations, indicating that increasing the number of TRP block layers\ndoes not significantly improve the model's performance in the current experimental\nsetup. Therefore, using fewer layers, such as 2 or 4, may be sufficient to achieve good\nperformance while keeping computational costs low.\nNext, we analyze the impact of embedding dimension size on the model's per-\nformance. Embedding dimensions, which map features of entities and relations into\na lower-dimensional space, are crucial hyperparameters. Variations in dimension size\ncan significantly affect model performance. We conducted experiments across various\nembedding dimensions, as shown in the Figure 3. On the FB15k dataset, the model\nreached near-optimal performance at 128 dimensions, while on YAGO3-10, the model\nachieved its best performance starting from 64 dimensions. This demonstrates that\nthe model is able to work well with relatively small embedding sizes."}, {"title": "4.2.5 Parameter Efficiency", "content": "In this section, we investigate the parameter efficiency of the TRP architecture. To\nevaluate this, we compare TRP with Transformer, one of the most effective sequence\nmodels. For a fair comparison, both models are configured with the same number of\nlayers."}, {"title": "4.2.6 Embedding Visualization", "content": "In this section, we visualize the embeddings generated by our knowledge graph com-\npletion model via t-SNE [35]. We selected six entity categories from YAGO3-10\ndataset Airports, People, Campaigns, Places, Films, and Universities and randomly\nsampled 1,000 entities from each category. We also selected six entity categories\nfrom FB15k dataset Football teams, People, Places, Professions, Films and Uni-\nversities and randomly sampled 1,000 entities from each category. Each category is\nrepresented by a distinct color, helping to illustrate the distribution of different types\nof entities in the embedding space, as shown in Figure 5. Both visualizations reveal\nthat the model effectively separates the embeddings of most entity categories. Each\ncategory forms a distinct and compact cluster, clearly separated from other categories."}, {"title": "5 Conclusion", "content": "In this work, we propose a knowledge graph completion method that integrates the\nTriple Receptance Perception (TRP) architecture as the encoder and a Tucker decom-\nposition module as the decoder. The TRP effectively models sequential information,\nenabling the learning of dynamic embeddings, while Tucker decomposition decoder\nprovides robust relational decoding capabilities. We conduct experiments on link pre-\ndiction and triple classification tasks, demonstrating that our method outperforms"}]}