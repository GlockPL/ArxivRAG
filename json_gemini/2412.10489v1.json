{"title": "CognitionCapturer: Decoding Visual Stimuli From Human EEG Signal With Multimodal Information", "authors": ["Kaifan Zhang", "Lihuo He", "Xin Jiang", "Wen Lu", "Di Wang", "Xinbo Gao"], "abstract": "Electroencephalogram (EEG) signals have attracted significant attention from researchers due to their non-invasive nature and high temporal sensitivity in decoding visual stimuli. However, most recent studies have focused solely on the relationship between EEG and image data pairs, neglecting the valuable \"beyond-image-modality\u201d information embedded in EEG signals. This results in the loss of critical multimodal information in EEG. To address this limitation, we propose CognitionCapturer, a unified framework that fully leverages multimodal data to represent EEG signals. Specifically, CognitionCapturer trains Modality Expert Encoders for each modality to extract cross-modal information from the EEG modality. Then, it introduces a diffusion prior to map the EEG embedding space to the CLIP embedding space, followed by using a pretrained generative model, the proposed framework can reconstruct visual stimuli with high semantic and structural fidelity. Notably, the framework does not require any fine-tuning of the generative models and can be extended to incorporate more modalities. Through extensive experiments, we demonstrate that CognitionCapturer outperforms state-of-the-art methods both qualitatively and quantitatively. Code: https://github.com/XiaoZhangYES/CognitionCapturer.", "sections": [{"title": "Introduction", "content": "Since its inception, a fundamental challenge in brain decoding is optimally expressing the meaningful information within brain signals. Reconstructing visual stimuli from brain signals is one of the interesting tasks with exciting application prospects. Initially, pioneering work using fMRI data validated the possibility of reconstructing visual stimuli from fMRI data and successfully decoded simple textures and shapes. More recently, with the rapid development of deep learning methods, the use of deep learning models to decode fMRI brain signals has produced significant advancements. However, brain signals exhibit diverse forms, among which EEG and MEG data offer high temporal resolution and portability, making them particularly suitable for real-time decoding compared to fMRI. This versatility has led to a broader range of downstream applications. Recent works have attempted to align the brain-image modalities using EEG and MEG signals through contrastive learning. These approaches have achieved notable accuracy in decoding related visual stimuli.\nHowever, the internal mechanisms of brain function are diverse and complex. Human perception of visual stimuli is influenced by both the characteristics of the visual stimuli and individual past experiences. Recent works have primarily relied on the image modality as a reference for alignment, enabling the decoding of meaningful visual stimuli. Nonetheless, the objective of contrastive learning may lead to models that predominantly focus on the shared information between modalities, potentially overlooking the more diverse and complex \"beyond-image-modality\u201d information present in the brain signals.\nTo address this issue, we introduce a novel brain decoding model named CognitionCapturer, as illustrated in Fig. 1. CognitionCapturer can be trained jointly with brain signals and multiple modalities, effectively capturing the shared information between brain signals and a broader spectrum of modalities.\nSpecifically, based on the understanding that brain data contains information \u201cbeyond-image-modality\", we first extend image data using depth estimation models and image captioning models to construct a Image-Text-Depth multimodal aligned dataset. Then introduce Modality Expert Encoders, which focus on different EEG - single modality data. The embeddings obtained in this stage can be directly used for downstream tasks such as classification and retrieval. Subsequently, in the generation phase, we map the EEG embeddings to the CLIP image space via a diffusion prior and feed EEG embeddings associated with different modalities into a pre-trained image generation model, thus decoding fine-grained visual stimuli.\nIn contrast to previous methods, CognitionCapturer's training strategy enables models for different modalities to focus on capturing the relationships between information in EEG signals and modality-specific characteristics. This allows the model to capture fine-grained low-level visual information and abstract high-level semantic information. Furthermore, our proposed approach inherently possesses scalability, enabling the Modality Expert Encoder to be extended infinitely to any modality.\nAnother advantage of the proposed method is that the constructed dataset effectively decouples certain image features, allowing different Modality Expert Encoders to focus on structural and semantic features during training, thereby preventing fine-grained information from being overshadowed by coarse-grained information. The main contributions are as follows:"}, {"title": "Main Contribution", "content": "\u2022 We propose CognitionCapturer, a contrastive learning-based model that effectively decodes brain signals from multiple modalities.\n\u2022 Using an alignment module and a pre-trained image generation model without any fine-tuning, we achieve fine-grained reconstruction of images with performance surpassing that of any single modality.\n\u2022 Through experiments, we validate the effectiveness and rationality of incorporating more modal information for brain signal decoding, providing new insights for subsequent research in neuroscience."}, {"title": "Related Work", "content": "Decode Visual Stimuli from Brain Signal\nDecoding visual stimuli from fMRI brain signals has been widely studied and yielded successful results. However, the difficulty of acquiring fMRI data and its low temporal resolution pose challenges for practical applications. In contrast, EEG signals offer higher temporal resolution and lower acquisition costs, leading researchers to attempt decoding visual stimuli from EEG. Early EEG decoding work typically relied on supervised learning methods and was limited to a finite set of image categories, overlooking the intrinsic relationship between visual stimuli and brain responses. Recently, successfully constructed an image decoding framework using a contrastive learning approach, achieving zero-shot recognition.  built upon song's work by further reconstructing decoded visual information into high-quality images using a diffusion model. However, these works only considered EEG-image modality pairs, neglecting the diversity of brain data. Compared with their approaches, our method successfully leverages multiple modalities of data to decode visual stimuli, resulting in superior performance."}, {"title": "Contrastive Learning for Brain Decoding", "content": "Contrastive learning, as an effective cross-modal learning approach, has achieved significant success in works such as CLIP, Moco, etc. However, its effectiveness is closely related to the quality and scale of the data, and the selection of high-quality samples is crucial for improving model performance . Works that use contrastive learning to decode brain signals have also shown promising results. For instance, as a representative work, utilizes a pre-trained speech encoder to decode speech from MEG signals through contrastive learning, and subsequently, adopts a similar idea to decode images from MEG. A series of similar methods emerged subsequently . However, during the process of using brain data for contrastive learning, the limited amount of brain signal data may lead the model to focus only on the most discriminative features. After transforming image modality into other modalities, since these modalities are less information-rich compared to image modality, this forces our model to attend to finer-grained features, thereby better representing EEG signals."}, {"title": "Method", "content": "CognitionCapturer aims to address the loss of \"beyond-image-modality\" information in brain decoding. The method overview is depicted in Fig 2, where EEG-Modality pairs\u00b9 are processed by dedicated Modality Expert Encoders to decouple the effective information from different modalities in the EEG signal. In our experiments, we observed that binding the brain modality with different modalities improves classification and reconstruction performance. Subsequently, through a diffusion prior, the EEG embedding space is mapped to the CLIP space and fed into assembled SDXL-turbo and IP-Adapters to reconstruct visual stimuli.\nSpecifically, the same EEG signals are divided into three pairs: EEG-Image, EEG-Text, and EEG-Depth. For consistency, we will refer to these collectively as EEG-Modality pairs."}, {"title": "Modality Expert Encoder", "content": "CognitionCapturer uses modality pairs (E, M), where E represents the EEG signal and M represents other modalities. For each modality pair (E, M\u1d62), where i represents the index of different modalities.\nwe construct a dedicated network f\u1d62 and g\u1d62, which we refer to as Modality Expert Encoders. This way, each modality pair (E, M\u1d62) is mapped to the same dimension by its corresponding Modality Expert Encoder for subsequent constraints.\nIn the encoding of the EEG data, raw EEG signals are typically represented as matrices C \u00d7 T, where C denotes the number of electrode channels and T denotes the number of time samples. Analysis of EEG signals primarily occurs along these two dimensions.\nOur EEG encoder, based on a lightweight Transformer and STConv architecture, effectively extracts topological and spatiotemporal information from EEG channels. The network structure is shown in Table 1. Specifically, we first process the raw EEG data e \u2208 \u211d^(C\u00d7T) through a layer of Transformer encoder and a linear transformation to organize the topological information, then feed it into a feature extraction module based on STConv to extract spatiotemporal features. Finally, a residual linear layer maps the features output by STConv to the same dimension as the target modality features. Detailed network descriptions are provided in the appendix.\nWhen extracting features for the target modality M\u1d62 paired with EEG data E, there are many successful pre-trained encoders that can effectively extract img, text, and depth features. Recent work  and our experiments indicate that CLIP image embeddings contain depth information. To be compatible with generative models and maintain distribution consistency initially, we used"}, {"title": "Align EEG-Modality Pairs by Contrastive Learning", "content": "After the modality pairs (E, M\u1d62) are processed by their respective Modality Expert Encoder f\u1d62 and g\u1d62, they are encoded into the same dimension, resulting in embedding pairs (e\u1d62, m\u1d62). Here, (e\u1d62, m\u1d62) represents a set consisting of n samples, i.e., e\u1d62 = {q\u00b9\u1d62, q\u00b2\u1d62, \u2026, q\u207f\u1d62 }, m\u1d62 = {k\u00b9\u1d62, k\u00b2\u1d62, ..., k\u207f\u1d62}.\nSubsequently, for different (e\u1d62, m\u1d62) embedding pairs, we adopted an improved version of the infoNCE loss as the loss function:\n$L_{E,M_i} = -log \\frac{L_+}{L_+ + L_-}$                                                                                                       (1)\n$L_+ = \\sum_{P(idx) = 1} exp(q^i_{idx}k^i_{idx} / \\tau)$                                                                                    (2)\n$L_- = \\sum_{P(idx) = 0} exp(q^i_{idx}k^i_{idx} / \\tau)$                                                                                    (3)\n$P(idx) = \\begin{cases} 1  & \\text{when idx is the same as image label}\\\\0 & \\text{otherwise} \\end{cases}$                                                                                    (4)\nIn equation (2) and (3), \u03c4 is a scalar temperature parameter that controls the smoothness of the softmax distribution. Given that the same image is repeatedly viewed in EEG experiments (Gifford et al. 2022), multiple EEG data may correspond to the same image. This can create a contradictory phenomenon where the same data pairs are both pulled closer and pushed apart by the loss function. To address this, we utilize image index as supervisory information. Specifically, when idx is the same in multiple EEG data, we choose to pull together all the EEG data and the corresponding modality data, thereby avoiding the contradictory phenomenon.In practice, we employ a symmetric loss L_{E,M: + L_{M,E}."}, {"title": "Map EEG Embedding into CLIP Image Space", "content": "After obtaining the aligned embeddings e\u1d62 for EEG and m\u1d62 for other modalities, due to the existence of the modality gap and differences in distribution spaces , directly using the EEG embedding e\u1d62 would make it difficult for pre-trained generative models to identify effective information. Following the works of , we use a diffusion prior model to map the EEG embeddings e\u1d62 to the CLIP space, thereby making the EEG embeddings recognizable by pre-trained generative models. In practice, we used the MSE loss to train our diffusion prior from scratch.\n$L_{prior} = E_{t \\sim \\left[ 1, T \\right], m_i^t \\sim q_{\\phi}(m)}  [\\| f_\\theta (m_i^t, t, e_i) - m_i \\|^2]$                                                                      (5)\nIn equation (5), m\u1d62\u1d57 represents the CLIP embedding disturbed after a given diffusion timestep t, and f_\u03b8 denotes the diffusion prior network. The specific training details are provided in the Implementation Details section and supplementary material."}, {"title": "Generate visual stimulus with Multi-modal associated EEG embeddings", "content": "After the EEG embeddings pass through the diffusion prior, they can be used like the original CLIP embeddings. Specifically, to reconstruct high-fidelity visual stimuli and effectively utilize information from three modalities, we employ Multi IP-Adapters and SDXL-turbo  to simultaneously leverage embeddings from different modalities. As shown in Fig 2's generation phase, for the image modality, which contains the richest information, we use a full IP-Adapter to process the image embedding. For text and depth modalities, which focus on semantic and structural information respectively, we use modified versions of IP-Adapter, namely IP-Adapter-Style and IP-Adapter-Layout, to process the text and depth embeddings. This approach enables CognitionCapturer to reconstruct semantic information while preserving underlying visual details."}, {"title": "Experimental Setup", "content": "Datasets and Preprocessing\nWe utilized Thing-EEG Dataset for our experiments. The Thing-EEG dataset contains EEG data collected from 10 subjects under an RSVP paradigm. The training set comprises 1654 concepts, each associated with 10 images presented four times, resulting in a total of 66,160 EEG recordings. The test set includes 200 unique concepts, each represented by a single image repeated 80 times, totaling 16,000 EEG recordings. Both the training and test images are presented in a pseudorandom order to minimize habituation effects. Each image is displayed for 100 milliseconds followed by a blank screen for another 100 milliseconds to reduce blink-related and other artifacts. The raw EEG data were filtered between 0.1 Hz and 100 Hz, sampled at 1,000 Hz, and recorded using 63 channels.\nFor EEG preprocessing, we follow the methodology outlined in . We segment the EEG data into trials ranging from 0 to 1000 ms post-stimulus onset and perform baseline correction using the average value over the 200 ms period preceding the stimulus. All electrodes are retained, and the data are downsampled to 250 Hz. Multivariate noise normalization is applied to the training data, and the EEG repetitions for each image in the test set are averaged to improve the signal-to-noise ratio. Subsequently, to obtain a multimodally aligned dataset, we use BLIP2 for textual descriptions of the images and DepthAnything for depth estimation, resulting in an aligned text and depth dataset."}, {"title": "Implementation Details", "content": "We implemented our method on a single GeForce RTX 2080 Ti GPU. following the training strategy described in . The model was evaluated on the test set at the end of each epoch, with both training and testing conducted on separate subjects. For the training of the Modality Expert Encoder phase, we used the AdamW optimizer with a learning rate of 0.0003, a batch size of 1024, and trained for 20 epochs. Training for one subject took approximately 30 minutes.\nImages were resized to 224x224 pixels and normalized before being processed by the Modality Expert Encoder. During the training of the diffusion prior, we used a batch size of 512, trained for 100 epochs, and set the number of inference steps to 50. The guidance scale was set to 7.5. In each batch, 10% of the image embeddings were randomly replaced with noise. The embedding dimension was 1024.\nIn the generation process, we utilized SDXL-Turbo and IP-Adapter from Hugging Face. We set the inference steps for SDXL-Turbo to 5. When configuring the IP-Adapter, for the image modality, we used the full IP-Adapter with the scale set to 1. For the text and Depth modalities, we set the scale of their respective IP-Adapter's Layout block and Style block to 0, ensuring a focus on structural and semantic control in the reconstruction results."}, {"title": "Results and Discussion", "content": "Classification Performance\u00b2\nThe classification results of CognitionCapturer are shown in Table 2. We evaluated CognitionCapturer's ability to decode EEG embeddings based on different baseline modalities. To verify whether CognitionCapturer extracts complementary information across multiple modalities, we combined the top-5 results from three modalities, as shown in the upper bound row of Table 2. The results indicate that compared to previous work , CognitionCapturer achieves state-of-the-art performance on the image modality. With the introduction of the text and depth modalities, the model gains access to more complementary\n\u00b2Our classification accuracy will be updated with variance information, and the new results will be uploaded soon."}, {"title": "Visual Stimuli Reconstruction Performance\u2074", "content": "Since subject-08 showed the highest classification results in both our model and ATM, we chose subject-08 for the comparison. Some of the visual stimuli reconstructed by CognitionCapturer are shown in Fig 3.\nThe results show that CognitionCapturer outperforms previous work in the fine-grained alignment of reconstructed visual stimuli. To further qualitatively analyze the effectiveness of CognitionCapturer's reconstruction, we recovered visual stimuli for each individual modality and compared them with the complete CognitionCapturer. As shown in Fig. 4, there are differences in reconstruction performance when using single modalities: stimuli recovered only using the Text modality tend to be more abstract, while the Depth modality can better reconstruct structural information but performs poorly on semantic information. Notably, the image modality, which contains the richest information, sometimes loses certain details in its reconstructions. However, with the assistance of the Text and Depth modalities, CognitionCapturer recovers more reasonable visual stimuli. For instance, in Fig. 4, when the visual stimulus is a basketball, the image modality misses the \"circular\u201d feature, whereas the depth modality retains this information well.\nTo quantitatively compare our approach with the current state-of-the-art methods, we follow the evaluation metrics outlined in and conduct further quantitative comparisons on the reconstructed images. The results in Table 3 show that CognitionCapturer,\n\u00b3Note: This does not represent the actual accuracy that can be achieved in practice but rather serves to demonstrate the effectiveness of the complementary information.\n\u2074More results can be found in the supplementary material."}, {"title": "How Different Modality Expert Encoders Focus on Brain Regions", "content": "In the previous section, we analyzed the reconstruction results of CognitionCapturer. To provide evidence for the feasibility and interpretability of CognitionCapturer, we use Grad-CAM to visualize the regions of interest for different modality encoders. To mitigate the influence of individual subjects, we conducted an average analysis of the Grad-CAM results across all subjects' mod-"}, {"title": "How Different Brain Area Interact with Visual Stimuli", "content": "The analysis in the previous section demonstrated exciting results. To provide additional evidence for the effective interaction between EEG and image information, we further used Grad-CAM to visualize the image regions attended to by the embeddings produced by our Modality Expert Encoders and compared them with the original CLIP embeddings.\nAs shown in Fig. 5(B), first, in the original CLIP model, the text embedding focuses more on the object itself, while the image and depth embeddings have broader attention areas. Our Modality Expert Encoders yield EEG embeddings for different modalities that show similar results to those of CLIP. Specifically, the EEG embedding from the Text Expert Encoder focuses more on high-level information in the image, such as the baseball bats. In contrast, the Image and Depth Expert Encoders have broader attention over the image. Correspondingly, the brain regions attended to by the Image and Depth models are also more extensive compared to Text. This provides strong evidence for the interpretability of CognitionCapturer."}, {"title": "Conclusion", "content": "In this work, we propose CognitionCapturer to extract multimodal representations from EEG signals and decode visual stimuli from them. Specifically, we introduce multiple Modality Expert Encoders to specialize in aligning EEG embeddings with those of different modalities, enabling the model to capture both semantic and structural information simultaneously. The analysis of brain activity and the interpretability of our model demonstrate that it successfully obtains meaningful representations of brain signals. This provides new insights for subsequent work in brain decoding."}]}