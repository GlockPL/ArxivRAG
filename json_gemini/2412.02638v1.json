{"title": "QA-TOOLBOX: Conversational Question-Answering\nfor process task guidance in manufacturing", "authors": ["Ramesh Manuvinakurike", "Elizabeth Watkins", "Celal Savur", "Anthony Rhodes", "Sovan Biswas", "Gesem Gudino Mejia", "Richard Beckwith", "Saurav Sahay", "Giuseppe Raffa", "Lama Nachman"], "abstract": "In this work we explore utilizing LLMs for data augmentation for manufacturing\ntask guidance system. The dataset consists of representative samples of interactions\nwith technicians working in an advanced manufacturing setting. The purpose of this\nwork to explore the task, data augmentation for the supported tasks and evaluating\nthe performance of the existing LLMs. We observe that that task is complex\nrequiring understanding from procedure specification documents, actions and\nobjects sequenced temporally. The dataset consists of 200,000+ question/answer\npairs that refer to the spec document and are grounded in narrations and/or video\ndemonstrations. We compared the performance of several popular open-sourced\nLLMs by developing a \"baseline\" using each LLM and then compared the responses\nin a reference-free setting using LLM-as-a-judge and compared the ratings with\ncrowd-workers whilst validating the ratings with experts.", "sections": [{"title": "1 Introduction", "content": "Manufacturing is a human-centric domain where tech-\nnicians must follow complex instructions provided\nby a spec (task specification manual). A spec typi-\ncally consists of natural language description of sev-\neral processes that needs to be accomplished towards\nreaching an end goal. Manufacturing industry faces\nseveral challenges. Within a spec, a typical process\ncan contain anywhere from dozens to hundreds of\nactions that could be carried out as-is in the speci-\nfied order which can be challenging. Furthermore,\nmanufacturing industry has high attrition rates [38],\nand new workers find it challenging to perform com-\nplex tasks effectively, adding to the challenges in a\nsuccessful execution of a process. A process task-\nguidance system for technicians could benefit the\nindustry widely. Process task guidance requires the development of methods and technology for AI\nassistants that can help technicians perform complex tasks [14]. Task guidance with AI assistance in\nmanufacturing remains a challenging problem [36].\nIn this work we propose a question-answering based process task guidance system, with the technician\nrequesting information about the process and the AI assistance providing the guidance via answers.\nThe process task guidance system that we are developing uses data from two main sources: specs,"}, {"title": "2 Related work", "content": "Several large multimodal datasets relevant for multimodal process task guidance exist [42, 13, 48,\n45, 6, 55]. Table 1 shows comparison of dataset and their relevance for our manufacturing setting.\nThe spec documents are missing in most of these datasets. Assembly101 [42] contains a pictorial\nrepresentation of how the task could be performed. A natural language description is missing. In"}, {"title": "3 Dataset Generation", "content": "In this work we create QA-\nTOOLBOX for manufacturing.\nQA-TOOLBOX is a dataset built\non top of [42] to bridge the gaps\nin the earlier dataset (Table 1).\nFigure 2 shows the overall workflow\nin creating the dataset & automatic\nevaluation pipeline. One of the major\nchallenges for evaluating LLM via-\nbility for the manufacturing domain\nis a lack of a representative dataset\nvalidated in a real-world setting.\nIn this work we bridge this gap by\nusing real-world manufacturing data\nand have LLMs generate the missing\ninformation."}, {"title": "3.1 Dataset", "content": "Assembly101 The choice of Assembly101 [42] was guided by our observations that the dataset\nmatches most closely to our internal manufacturing dataset. Assembly101 is made up of multiple\nactors performing structured procedures captured from several camera viewpoints. Despite the\nstructured procedure, the data consists of natural sequence variations due to the performer's skill\nlevel and various anomalies. The dataset is a large-scale video dataset consisting of annotations for\naction recognition (coarse & fine grained), action anticipation, temporal action segmentation and\nmistake detection [42]. The dataset consists of people assembling and disassembling 15 different\ntoys with 90 objects (tools & toy parts). and 24 interaction verbs comprising 1380 fine-grained action\nlabels. However, this dataset doesn't contain textual instruction manuals for the assembly/disassembly\nsequence. The dataset also doesn't contain narrations of the task performer performing the actions.\nManufacturing Dataset (Internal) Our internal dataset collected from an advanced manufacturing\nsite consists of spec documents detailing the manufacturing procedure, free form natural language\nnarrations with the task performer describing the actions being performed during the process demon-\nstration along with videos from mounted cameras. We collected data internally using a popular\nwizard-of-oz. (WoZ paradigm [11, 16] where a remote-controlled agent interacted with the task per-\nformer during the data collection process. The technicians follow the spec to complete a process that\ninvolves steps around assembly, disassembly and cleaning. During the data collection process we\nobserved that the technicians often forget to provide necessary information or even forget to narrate.\nA WoZ system helps nudge the technicians narrate and provide the information. The narrations\ncollected are converted to text using Whisper speech to text [39]. We utilize 20 such recordings for\nprompting the LLMs to generate the augmentation dataset."}, {"title": "3.2 Augmenting missing data", "content": "Data augmentation has been done with LLMs to annotate, estimate quality, and generate new training\nsamples [17]. The quality of the generated content from LLMs has been shown to be comparable or"}, {"title": "3.2.1 Technicians narration data", "content": "We collected a small sample (n = 20) of interactions between technicians and a remotely controlled\nagent. The narrations collected are intended to collect additional information typically not found\nin specs such as variants of referring expressions intentions for performing actions, and nuances of\nactions. We include technicians engaged in the manufacturing processes in our data collection ."}, {"title": "3.2.2 Participatory QA Dataset", "content": "We interviewed the entire cohort of task technicians within our partnering manufacturing facility and\ngathered questions they wanted to ask the agent in the course of their real-world work. (Table 3)\nWe then used their questions to seed augmentation. This participatory approach assures that our\ndataset is less speculative, and more promising for evaluating LLM performance in ways that humans\nactually need. This interviewing was conducted in a semi-structured protocol designed to assess\ntrust, trustworthiness, and usability of building an AI assistant for manufacturing. In unstructured\nfollow-ups, the interviewer asked the technicians to share all of the questions they would want to ask\nusing voice-based UI in the course of their work."}, {"title": "3.2.3 Anonymization", "content": "IP leakage in manufacturing facilities is important to be addressed. The exact protocols, objects,\nactions (duration), sequences of actions etc., are closely guarded. To ensure no such information\nare viewed by LLMs (locally deployed or cloud-based) we remove semantic entities and replace\nwith placeholder, i.e., ([ACTION], [TOOL], [OBJECT], [MANNER], [TEMPORAL], [PURPOSE],\n[GOAL], [NOTE]). The LLMs are then prompted to generate narrations and spec similar in structure\nbut for the Assembly101 dataset."}, {"title": "3.3 LLM Augmentations", "content": "For augmentation, we use (from the internal dataset) (i) Specs that include description of process\nsteps, (ii) Narrations similar to those of technicians describing actions, and finally, (iii) Questions."}, {"title": "3.3.1 Prompt generation", "content": "We create prompts for the specs, questions and narrations with samples from the internal dataset\nand the assembly101 dataset annotations. The prompt templates are shown in Section A.1. At the\ncore of the prompt generation we use, Promptspec (for creating specs), Promptnarration_creation\n(for creating narrations), Promptq_gen_spec and Promptq_gen_narration (for generating ques-\ntions) as the prompts to create new samples. Assembly101spec, Assembly101narration refer-\nring to the newly created specs and narrations for Assembly101 dataset. {Anonymizedspec} and\n{Anonymizednarrations} are the internal manufacturing spec with anonymized semantic entities.\n[Inst_pre] is the instruction prefix appended in front of the prompt to assist LLMs in generating the\nsamples."}, {"title": "3.3.2 Spec, Narrations & Questions", "content": "We augment the Assembly101 dataset by creating specs and narrations using prompts (Sec 3.3.1). We\nuse an Instruction-tuned mixture of expert models and Mixtral-8x7b [24]. We prompt the LLMs to\ngenerate narrations in a style similar to the action labels in the Assembly101 dataset. In our qualitative\nanalysis, we found the narrations and spec we generated had similar characteristics to those of our\ninternal dataset. While no quantitative studies were conducted, an avenue for future work is to study\nthe quality of narrations and spec documents generated. Since it has been shown that the quality of\nthe generated augmented dataset can exceed that of human generated, we believe the same results\nholds true [50, 17, 30]. The spec and narrations had to be generated prior to generating the questions\nas the augmented specs and narrations are used to create the questions for our benchmark. We once\nagain leverage Mixtral-8x7b model to generate questions (Refer Sec A.1 for prompt templates)."}, {"title": "4 Baseline System", "content": "We utilize the existing LLMs which have topped the LLM benchmarks in the recent past for baseline\ntask guidance QA system. We confine our model selection to fewer than 15 billion parameters due\nthe hardware limitations required to run larger models (in fp16 format).\nModels We evaluate the LLMs that have achieved impressive zero shot performance in various\nbenchmarks namely, Phi3 (mini-128k & medium-128k) [2], Mistralv02-it [24], Llama3-8b-Instruct\n[5], Gemma2-9b-Instruct [46] and, Flan-T5-base [9] models to generate the answers. To answer\nthe questions, the models are provided prompts consisting of instructions, spec and narrations. The\nmodels are not fine-tuned to the domain and used off-the-shelf.\nTask Prompts The task prompts are different from those used for generating the dataset. When\ngenerating the augmentation datasets, the prompts consists of ground truth data (full Narrations,\nAssembly101actions) data. However, when answering the questions at time 't', during the inference\ntime the system only has access to the spec (Assembly101spec) and any narrations that might be\navailable from time \u2018t-k' (Assembly101 narration). Since, the models can consume language-only\ninput, we release the evaluations from unimodal scenario. We leave it to the future work to effectively\nuse the vision modality to improve the results in the benchmark.\nThe prompt used across the models is formatted as a chat interaction as is the standard with\nLLMs. Prompt = [Technician: Assembly101 narration, Technician: 'You're a task assistant helping\ntechnicians on the factory floor to answer questions about a process'. The spec for the process:\nAssembly101 spec. Now answer the question: Question]."}, {"title": "5 Evaluation", "content": "Several viable approaches have been developed for evaluating QA systems in recent times [26, 7].\nReference-based (Ground truth answers exist) and reference-free (Ground truth answers do not exist)\nmetrics exist for evaluating the performance of the models for the QA tasks. Evaluating the answers\nin a reference-based setting from the models is challenging since a question may have more than one\ncorrect answer. The answer must be adherent to the spec document overall but the answers can indeed\nvary either in their surface form or even content-wise but still be adherent to the spec. Classical\nreference-free metrics rely on statistical patterns that exist in the questions, answers and document\nresulting in unreliable characterization of model capabilities [15]. Recently, LLM-as-a-judge [54] has\ngained popularity. LLMs as judge have been shown to achieve high correlation or even outperforming\nthe crowd human ratings [54, 50]. We utilize LLM-as-a-judge paradigm to evaluate the models in\nTOOLBOX. However, the question remains as to which LLM acts as the best judge? We experiment\nwith three different LLMs and evaluate the ratings assigned by each. Finally, to validate the LLM-\nas-a-judge paradigm, we also collect crowd-sourced rating of the responses generated by one of our\nLLMs in the baseline stage (llama3-8b-instruct). We also conduct a study to validate the effectiveness\nof LLM-as-a-judge by comparing the scores generated by the LLMs and crowd-workers with experts.\nWe evaluate the performance of the models on conciseness, correctness, completeness and ground-\nedness of the responses generated. For manufacturing the conciseness and groundedness of the\nresponses are important along with correctness and completeness. The answers needs to be short\nsince the technicians are time-bound and often not interested in lengthy responses. The responses\nneeds to be grounded in the spec/narrations since the responses must not deviate from the spec\ndocuments."}, {"title": "6 Results", "content": "We evaluate the answers using LLM-as-a-judge (Prompts can be found in Section A.2). More\nspecifically we answer the following questions,"}, {"title": "7 Discussion & Future work", "content": "In this work we propose a novel approach to building and using benchmark datasets designed to\nadvance pluralistic and socio-technical values in computing research (Refer Sec B for definitions).\nQA-TOOLBOX presents several promising direction for applied and fundamental research. The\nmanufacturing task guidance is a complex problem requiring the models to generate recommendations\ngrounded in spec documents. The sequence of actions to be performed are rigid, however allowing\ncertain flexibility in certain places. The benchmark allows comparing open-sourced LLMs and\nproprietary models for manufacturing. The approach itself is scalable and applicable to several\nmanufacturing processes.\nFurthermore, current multi-modal LLMs [34, 56, 35, 32, 27] can ingest a few frames of visual context\nlimiting their capability to understand long-term visual context and reasoning. This long-term visual\ncontext and reasoning is critical for an efficient task guidance system that needs to provide timely\nfeedback for the natural/common step execution variations such as repeated steps or uncompleted\nsteps in the distant past resulting from unforeseen scenarios.\nFor the future versions of the benchmark, we intend to release updated tasks (summarization,\ncontinual learning, multimodal tasks) which are relevant for manufacturing domain. RAG-based [29]\napproaches are central to developing such systems either to prevent hallucinations or reducing the\ncontext when the spec documents reach several thousand pages in length."}, {"title": "A Appendix", "content": "A.1 Augmentation prompts\nPrompt spec = [Inst_pre] Here is a sample spec document {Anonymizedspec}. Create a similar\nspec document where the following Assembly101actions were observed. (1)\nPromptnarration_creation = [Inst_pre] Here is a sample spec document {Anonymizedspec}. A\ntechnician following the spec generated the following narrations {Anonymizednarrations}. Create\na similar narrations for Assembly101spec. (2)"}, {"title": "A.2 Judge Prompts", "content": "completeness_prompt = \"\"\" You will be given a question and answer couple. Your task is to provide\na 'total rating' on completeness and score how well the answer answers the user concerns expressed\nin the user_question. Give your answer as a float on a scale of 0 to 10, where 0 means that the\nresponse is not complete at all, and 10 means that the answer is complete, concise and addresses the\nquestion. Deduct points for not being complete in the response. Use spec as a reference.\nProvide your feedback as follows:\nFeedback::: Total rating: (your rating, as a float between 0 and 10)\nNow here are the question and answer. Spec: spec Question: question Answer: answer\nFeedback::: Total rating:\n\"\"\"\nconciseness_prompt = \"\"\" You will be given a question and answer couple. Your task is to provide a\n'total rating' scoring how well the answer answers the user concerns expressed in the user_question.\nGive your answer as a float on a scale of 0 to 10, where 0 means that the response is not concise at\nall, and 10 means that the answer is complete, concise and addresses the question. Deduct points for\nnot being to the point or generating verbose responses. Answers should always be to the point. Use\nspec as reference.\nProvide your feedback as follows:\nFeedback::: Total rating: (your rating, as a float between 0 and 10)\nNow here are the question and answer. Spec: spec Question: question Answer: answer\nFeedback::: Total rating:\n\"\"\"\ncorrectness_prompt = \"\"\" You will be given a question and answer couple. Your task is to provide a"}, {"title": "B Some definitions", "content": "\"Sociotechnical\" is a term that helps researchers recognize that technical systems are deployed into\nsocial systems, and so technical systems will be both constrained and enabled by social systems\nsuch as organizations, hierarchies, and workflows [28]. Recent calls for \"pluralism\" in benchmarking\ndescribe that such practices need to better encompass diverse human values and needs [44]. Both of\nthese approaches apply to the internal dataset regarding the manufacturing task, to avoid risks of IP\n(Intellectual Property) leakage."}]}