{"title": "Polynomial-Time Approximability of Constrained Reinforcement Learning", "authors": ["Jeremy McMahan"], "abstract": "We study the computational complexity of approximating general constrained Markov decision processes. Our primary contribution is the design of a polynomial time (0, \u20ac)-additive bicriteria approximation algorithm for finding optimal constrained policies across a broad class of recursively computable constraints, including almost-sure, chance, expectation, and their anytime variants. Matching lower bounds imply our approximation guarantees are optimal so long as P \u2260 NP. The generality of our approach results in answers to several long-standing open complexity questions in the constrained reinforcement learning literature. Specifically, we are the first to prove polynomial-time approximability for the following settings: policies under chance constraints, deterministic policies under multiple expectation constraints, policies under non-homogeneous constraints (i.e., constraints of different types), and policies under constraints for continuous-state processes.", "sections": [{"title": "Introduction", "content": "Constrained Reinforcement Learning (CRL) is growing increasingly crucial for managing complex, real-world applications such as medicine [13, 29, 22], disaster relief [14, 38, 34], and resource management [25, 24, 31, 5]. Various constraints, including expectation [2], chance [39], almost-sure [9], and anytime constraints [28], were each proposed to address new challenges. Despite the richness of the literature, most works focus on stochastic, expectation-constrained policies, leaving many popular settings with longstanding open problems. Even chance constraints, arguably a close second in popularity, still lack any polynomial-time, even approximate, algorithms despite being introduced over a decade ago [39]. Other settings for which polynomial-time algorithms are open include deterministic policies under multiple expectation constraints, policies under non-homogeneous constraints (i.e., constraints of different types), and policies under constraints for continuous-state processes. Consequently, we study the computational complexity of general constrained problems to resolve many of these fundamental open questions.\nFormally, we study the solution of Constrained Markov Decision Processes (CMDPs). Here, we define a CMDP through three fundamental parts: (1) a MDP M that accumulates both rewards and costs, (2) a general cost criterion C, and (3) a budget vector B. Additionally, we allow the agent to specify whether they require their policy to be"}, {"title": null, "content": "is to solve max\u2208n VM subject to CM < B, where VM denotes the agent's value and CM denotes the agent's cost under \u03c0. Our main question is the following:\nCan general CMDPs be approximated in polynomial time?\nHardness. Solving general CMDPs is notoriously challenging. When restricted to deterministic policies, solving a CMDP with just one constraint is NP-hard [15, 39, 28, 26]. This difficulty increases with the number of constraints: with at least two constraints, finding a feasible deterministic policy, let alone a near-optimal one becomes NP-hard [28]. Even if we relax the deterministic requirement, this hardness remains for all constraint types other than expectation. Computational hardness aside, standard RL techniques fail to apply due to the combinatorial nature induced by many constraint types. Adding in additional constraints with fundamentally different structures then further complicates the problem.\nPast Work. A few works have managed to derive provable approximation algorithms for some cases of CRL. McMahan [26] presented a fully polynomial-time approximation scheme (FPTAS) for the computation of deterministic policies of a general class of constraints, which includes expectation, almost-sure, and anytime constraints. Although powerful, their framework only works for one constraint and fails to capture anytime-expectation constraints along with chance constraints. Similarly, Khonji et al. [21] achieves an FPTAS for expectation and chance constraints, but only in the constant horizon setting. In contrast, McMahan and Zhu [28] develops a polynomial-time (0, \u20ac)-additive bicritiera approximation algorithms for anytime and almost-sure constraints. However, their framework is specialized to those constraint types and thus fails for our purpose. In contrast, Xu and Mannor [39] developed a pseudo-polynomial time algorithm for finding feasible chance-constrained policies, but their methods do not lead to polynomial-time solutions.\nOur Contributions. We design a polynomial-time (0, \u20ac)-additive bicriteria approximation algorithm for tabular, SR-criterion CMDPs. An SR criterion is required to satisfy a generalization of the policy evaluation equations and includes expectation, chance, and almost-sure constraints along with their anytime equivalents. Our framework implies the first positive polynomial-time approximability results for (1) policies under chance constraints, (2) deterministic policies under multiple expectation constraints, and (3) policies under non-homogeneous constraints \u2013 each of which has been unresolved for over a decade. We then extend our algorithm into a polynomial-time (\u20ac,\u20ac)-additive bicriteria approximation algorithm for continuous-state CMPDs under a general class of constraints, which includes expectation, almost-sure, and anytime constraints.\nOur Techniques. Our algorithm requires several key techniques. First, we transform a constraint concerning all realizable histories into a simpler per-time constraint. We accomplish this by augmenting the state space with an artificial budget and augmenting the action space to choose future budgets to satisfy the constraint. However, bellman updates then become as hard as the knapsack problem due to the large augmented action space. For tabular cMDPs, we show the bellman updates can be approximately computed using dynamic programming and rounding. By strategically rounding the"}, {"title": null, "content": "artificial budget space, we achieve an (0, \u20ac)-bicriteria approximation for tabular CMDPs. By appropriately discretizing the continuous state space, our method becomes an (e, \u20ac)-bicriteria approximation algorithm for continuous state CMDPs."}, {"title": "1.1 Related Work", "content": "Constrained RL. It is known that stochastic expectation-constrained policies are polynomial-time computable via linear programming [2], and many planning and learning algorithms exist for them [30, 35, 6, 20]. Some learning algorithms can even avoid violation during the learning process under certain assumptions [37, 3]. Furthermore, Brantley et al. [8] developed no-regret algorithms for cMDPs and extended their algorithms to the setting with a constraint on the cost accumulated over all episodes, which is called a knapsack constraint [8, 11].\nSafe RL. The safe RL community [17, 19] has mainly focused on no-violation learning for stochastic expectation-constrained policies [12, 7, 1, 10, 4] and solving chance constraints [36, 40], which capture the probability of entering unsafe states. Performing learning while avoiding dangerous states [40] is a special case of expectation constraints that has also been studied [32, 33] under non-trivial assumptions. In addition, instantaneous constraints, which require the immediate cost to be within budget at all times, have also been studied [23, 16, 18]."}, {"title": "2 Constraints", "content": "Cost-Accumulating MDPs. In this work, we consider environments that accumulate both rewards and costs. Formally, we consider a (finite-horizon, tabular) cost-accumulating Markov Decision Process (caMDP) tuple M = (H, S, A, P, R, C, so), where (i) H is the finite time horizon, (ii) S\u0127 is the finite set of states, (iii) Ah(s) is the finite set of available actions, (iv) Ph(s, a) \u2208 \u2206(S) is the transition distribution, (v) Rh(s, a) \u2208 \u0394(R) is the reward distribution, (vi) Ch(s, a) \u2208 \u2206(Rm) is the cost distribution, and (vii) so \u2208 S is the initial state.\nTo simplify notation, we let $r_h(s, a) \\overset{\\text{def}}{=} E[R_h(s, a)]$ denote the expected reward, $S \\overset{\\text{def}}{=} |S|$ denote the number of states, $A \\overset{\\text{def}}{=} |A|$ denote the number of joint actions, $[H] \\overset{\\text{def}}{=} \\{1, ..., H\\}$, M be the set of all caMDPs, and |M| be the total description size of the caMDP. We also use the Iverson Bracket notation $[P] \\overset{\\text{def}}{=} 1\\{P=\\text{True}\\}$ and the characteristic function $X_P$ which is 1 when P is False and 0 otherwise.\nAgent Interactions. The agent interacts with M using a policy \u03c0 = {\u03c0h}_1. In the fullest generality, \u03c0\u03b7: \u0397h \u2192 \u25b3(A) is a mapping from the observed history at time h (including costs) to a distribution of actions. Often, researchers study Markovian policies, which take the form \u03c0\u03b7: S \u2192 \u2206(A), and deterministic policies, which take the form \u03c0\u03b7: Hh \u2192 A. We let I denote the set of all policies and ID denote the set of all deterministic policies.\nThe agent starts in the initial state so with observed history T\u2081 = (so). For any h\u2208 [H], the agent chooses a joint action ah ~ \u03c0\u03b7(Th). Then, the agent receives immediate reward rh ~ Rh(s, a) and cost vector ch ~ Ch(s, a). Lastly, M transitions to state sh+1 ~"}, {"title": null, "content": "Ph(sh, ah), prompting the agent to update its observed history to Th+1 = (Th, ah, Ch, Sh+1). This process is repeated for H steps; the interaction ends once sh+1 is reached.\nConstrained Processes. Suppose the agent has a cost criterion C : M \u00d7 \u041f \u2192 Rm and a corresponding budget vector B\u2208 Rm. We refer to the tuple (M, C, B) as a Constrained Markov Decision Process (CMDP). Given a CMDP and desired policy class \u03a0\u2208 {\u03a0\u0189, \u03a0}., the agent's goal is to solve the constrained optimization problem:\n$\\begin{aligned}\\text{maximize} \\quad & V^{\\mathcal{M}}_\\{\\pi\\} \\\\\\text{s.t.} \\quad & C^{\\mathcal{M}} \\leq B \\end{aligned}$ (CON)\nIn the above, $V^{\\mathcal{M}}_\\{\\pi\\} \\overset{\\text{def}}{=} E^{\\mathcal{M}} \\Big[\\sum_{h=1}^H r_h(s_h, a_h) \\Big]$ denotes the value of a policy \u03c0, \u0395 denotes the expectation defined by PM, and PM denotes the probability law over histories induced from the interaction of \u03c0with M. Lastly, we let V* denote the optimal solution value to (CON).\nSR Criteria. We study cost criteria that generalize the standard policy evaluation equations to enable dynamic programming techniques. In particular, we require the cost of a policy to be recursively computable with respect to the time horizon. For our later approximations in Section 5, we will also need key functions defining the recursion to be short maps, i.e., 1-Lipschitz, with respect to the infinity norm.\nDefinition 1 (SR). We call a cost criterion shortly recursive (SR) if for any caMDP M and any policy \u03c0\u2208 \u03a0\u03a1, \u03c0's cost decomposes recursively into CM = CT (so), where CH+1 = 0 and for all h \u2208 [H] and \u03c4\u03b7 \u2208 Hr letting s = sh(Th) and a = \u03c0\u03b7(Th),\n$C(\\tau_h) = c_h(s, a) + \\mathbb{E}_{s'\\sim P_h(\\cdot|s,a)} \\Big[ g (P_h (s' | s, a)) C^{\\pi}_{h+1} (\\tau_h, a, s') \\Big].$ (SR)\nHere, fs, is the finite extension of an associative, non-decreasing, binary function f, and g is a [0, 1]-valued function rooted at 0. Moreover, we require that f is a short map when either of its inputs are fixed, satisfies f(0, x) = f(x, 0) = x for all x, and when combined with g, i.e., f g (Ph (s' | s, a)) xs\u0131, is a short map in x.\nRemark 1 (Stochastic Variants). Our results generalize to both stochastic policies and stochastic costs as well. The algorithmic approach is identical, but the definitions and analysis are more complex. Consequently, we focus on the deterministic cases in the main text.\nConstraint Formulations. The fundamental constraints considered in the CRL literature are Expectation, Chance, and Almost-sure constraints. Each of these induces a natural anytime variant that stipulates the required constraint must hold for the truncated cost h=1 Ch at all times h \u2208 [H]. We give the formal definitions in Figure 1. Importantly, each constraint is equivalent to CM < B' for some appropriately chosen SR criteria.\nProposition 1 (SR Modeling). The classical constraints can be modeled by SR constraints of the form CM < B' as follows:\n1. Expectation Constraints - $f(x, y) \\overset{\\text{def}}{=} x+y$, $g(x) \\overset{\\text{def}}{=} x$, and $B' \\overset{\\text{def}}{=} B$."}, {"title": null, "content": "2. Chance Constraints \u2013 (f,g) defined as above and B' def 8. Here, we assume M's states are augmented with cumulative costs and that ch((s, c), a) def [ch(s, a) + c > B] for the anytime variant and ch((s,c), a) def [ch(s, a) + c > B][h = H] otherwise.\n3. Almost-sure Constraints \u2013 f(x,y) def max(x,y), g(x) def [x > 0], and B' def B. Anytime variant \u2013 f(x, y) def max(0, max(x, y)) while g and B' remain the same.\nGeneral anytime variants, including anytime expectation constraints, can be modeled by {CM,t < B}te[H] where CM,t is the original SR criterion but defined for the truncated-horizon process with horizon t.\nComputational Limitations. It is known that computing feasible policies for CMDPs is NP-hard [26, 28]. As such, we must relax feasibility for any hope of polynomial time algorithms. Consequently, we focus on bicriteria approximation algorithms.\nDefinition 2 (Bicriteria). A policy is an (a, \u03b2)-additive bicriteria approximation to a CMDP (M, C, B) if V \u2265 V* \u2013 a and CM \u2264 B + \u03b2. We refer to an algorithm as an (\u03b1, \u03b2)-bicriteria if for any CMDP it outputs an (a, \u03b2)-additive bicriteria approximation or correctly reports the instance is infeasible.\nThe existence of a polynomial-time bicriteria for our general constrained problem implies brand-new approximability results for many classic problems in the CRL literature. For clarity, we will explicitly state the complexity-theoretic implications for each classical setting.\nTheorem 1 (Implications). A polynomial-time (\u0454, \u0454)-bicriteria implies that in polynomial time it is possible to compute a policy \u03c0\u2208 \u03a0 satisfying V \u2265 V* e and any constant combination of the following constraints:\n$\\begin{aligned}&1. \\mathbb{E}_{\\mathcal{M}}\\Big[\\sum_{h=1}^H c_h \\Big] \\leq B + \\epsilon \\\\\\ &2. P^{\\mathcal{M}}\\Big[\\sum_{h=1}^H c_h \\leq B + \\epsilon \\Big] = 1 \\\\\\ &3. P^{\\mathcal{M}}\\Big[\\sum_{h=1}^H c_h > B \\Big] < \\delta + \\epsilon.\\end{aligned}$\nIn other words, polynomial-time approximability is possible for each of the settings described in Section 1 when the number of constraints is constant.\nRemark 2 (Extensions). All of our results hold for Markov games and infinite discounted settings."}, {"title": "3 Reduction", "content": "In this section, we present a general solution approach to SR-criterion CMDPs. Our revolves around converting the general cost constraint into a per-step action constraint. This leads to the design of an augmented MDP that can be solved with standard RL methods.\nAugmentation. State augmentation is the known approach for solving anytime-constrained MDPs [28]. The augmentation permits the problem to be solved by the following dynamic program:\n$V_h^*(s, c) = \\max_{\\substack{a \\in A,\\ c+c_h(s, a) 0\\] is equivalent to having (a*, B*) = V*(s, B). The second inequality follows from the induction hypothesis. The last two equalities follow from the standard policy evaluation equations and the definition of \u03c0\u03b7, respectively. This completes the proof."}, {"title": "D.3 Proof of Lemma 6", "content": "Proof. We proceed by induction on h.\nBase Case. For the base case, we consider h = H + 1. By definition and assumption, VA+1(s,b) = -X{6>0} > -\u221e. Thus, it must be the case that b > 0 and so by definition CH+1(s,b) = 0 \u2264 6.\nInductive Step. For the inductive step, we consider any h < H. As in the proof of Lemma 2, we know that (a, b) = \u03c0\u03b7(s, b) \u2208 \u00c2n(s, b) and for any s' \u2208 S with Ph(s' | s, a) > 0 that V+1(s', bs) > -\u221e. Thus, the induction hypothesis implies that Ch+1(s', bs) \u2264 bs + l(S+1)(H \u2013 h) for any such s'. For any other s', we have g(Ph(s' | s,a)) = g(0) = 0 by assumption.\nThus, the weighted combination of Ch+1(s', bs) is equal to the weighted combination of b' where def +1(s', bs) if Ph(s' | s, a) > 0 and 6, def\n0 otherwise. Moreover, we have b' \u2264 b + l(S + 1)(H \u2013 h) since l > 0. Thus, by (SR),\n$\\begin{aligned}\\tilde{C}^*(\\tau, b) &= c_h(s, a) + f\\Big[g(P_h(s' | s,a))\\tilde{C}^*_{h+1}(s', b*)\\Big] \\\\&= c_h(s, a) + f\\tilde{f}_{h,b*}(1, 0) \\\\&< c_h(s, a) + f\\tilde{f}_{h,b*}(1, 0) + l (S + 1) (H - h)\\\\&< \\kappa(b) + l (S + 1) (H - h)\\\\&= b + l (S + 1) (H - h + 1).\\end{aligned}$\nThe first inequality used Lemma 9. The second inequality used the fact that (a, b) \u0454 Ah(s, b). The last line used the definition of K. This completes the proof."}, {"title": "D.4 Proof of Theorem 4", "content": "Proof. If (CON) is feasible, then inductively we see that V\u2081*(so, [B]) > -\u221e. The contrapositive then implies if V\u2081* (so, [B]\u2084) = \u2212\u221e, then (CON) is infeasible. Thus, when Algorithm 4 outputs \u201cInfeasible\u201d it is correct.\nOn the other hand, suppose V\u2081* (so, [B]) > -\u221e and that is an optimal solution to M. By Lemma 5 and Lemma 1, we know that V\u2081 (so, [B]) \u2265 V\u2081(so, B) \u2265 V*. Also, by Lemma 6, we know that CT (so, [B]\u2084) \u2264 [B]e + l(S + 1)H < B + l(1 + (S + 1)H).\nOur choice of l = 1+(5+1) then implies that \u0108\u00af = \u0108\u2081(so, [B]\u2084) \u2264 B + \u20ac. Thus, \u03c0is an (0, \u20ac)-additive bicriteria approximation for (CON).\nBoth cases together imply that Algorithm 4 is a valid (0,\u2208)-bicriteria.\nTime Complexity. We see immediately from Theorem 3 that the running time of Algorithm 4 is at most O (H2m+1S2m+2A|B|2 || \u0421\u0442\u0430z - Cmin/em). To complete the analysis, we need to bound B. First, we note B is at most the number of integer multiples of l in the range [bmin, bmax]\u2286 [HCmin, HCmax]m. For any individual constraint, this number is at most O(H(Cmax-HCmin)/l) \u2264 O(H2S(Cmax-Cmin)/\u0454) using the definition 1+(S+1)H. Thus, over all constraints, the total number of rounded artificial budgets"}, {"title": "D.5 Proof of Proposition 2", "content": "Proof. We consider a reduction from the Hamiltonian Path problem. The transitions reflect the graph structure, and the actions determine the edge to follow next. To determine if a Hamiltonian path exists, we can simply make an indicator constraint for each node that signals that node has been reached. It is then clear that relaxing the budget constraint does not help since we can always shrink the budget for any given e-slackness. Thus, the claim holds."}, {"title": "D.6 Proof of Lemma 7", "content": "Proof. We proceed by induction on h.\nBase Case. For the base case, we consider h = H + 1. By definition, we have VA+1(TH+1) = 0 = VA+1(TH+1) and CH+1(TH+1) = 0 = CH+1(TH+1).\nInductive Step. For the inductive step, we consider any h < H. For simplicity, let def x= l(dr + Ap) Hrmax(Smax - Smin). The standard policy evaluation equations imply that,\n$\\begin{aligned} \\tilde{V}^*(\\tau_h) &= r_h([s]_l, a) + \\sum_{\\tilde{s'}} P_h(\\tilde{s'} | [s]_l, a) \\tilde{V}^*_{h+1}(\\tau_{h+1})\\\\ &= r_h([s]_l, a) + \\sum_{\\tilde{s'}} \\int_{s'=s}^{\\tilde{s'}+l}  P_h(s' | [s]_l, a) ds' \\tilde{V}^*_{h+1}(\\tau_{h+1})\\\\ &> r_h([s]_l, a) + \\int_{\\tilde{s'}+l}  P_h(s' | [s]_l, a) (\\tilde{V}^*_{h+1}(\\tau_{h+1}) - l(H - h)) ds' \\\\&= r_h([s]_l, a) + \\int_{\\tilde{s'}+l} P_h(s' | [s]_l, a) \\tilde{V}^*_{h+1}(\\tau_{h+1}) ds' - l(H - h) \\\\&= r_h([s]_l, a) + \\int_{\\tilde{s'}+l} P_h(s' | [s]_l, a) \\tilde{V}^*_{h+1}(\\tau_{h+1})ds' - l(H - h) \\\\&> r_h(s, a) - ld_r +  \\int_{\\tilde{s'}+l} P_h(s' | s, a) ds' \\tilde{V}^*_{h+1}(\\tau_{h+1}) - l(H - h)\\\\ &= \\tilde{V}(\\tau_h) - ld_r -  l \\Delta_P \\int V^*_{h+1}(\\tau_{h+1}) ds' -  l(H - h)\\\\ &> \\tilde{V}(\\tau_h) - l(d_r + \\Delta_P) -  l(H - h)\\\\ &= \\tilde{V}(\\tau_h) -  l(\\Delta_r + \\Delta_P) H(\\max s_{\\max} - s_{\\min} ) - x(H - h)\\\\&= \\tilde{V}(\\tau_h) - x(H - h + 1). \\end{aligned}$"}, {"title": null, "content": "If we let y def l(c + Ap)HCmax(Smax - Smin), we also see that,\n$\\begin{aligned}\\tilde{C}^*(\\tau_h) &= C_h([s]_l, a) + \\mathbb{E}_{\\tilde{P}} [\\tilde{C}^*_{h+1}(\\tau_{h+1})]\\\\ &= C_h([s]_l, a) +  \\int_{\\tilde{s'}+l} P_h(s' | [s]_l, a) ds' \\tilde{C}^*_{h+1}(\\tau_{h+1})\\\\ &= C_h([s]_l, a) +  \\int_{\\tilde{s'}+l} P_h(s' | [s]_l, a) \\Big(C^*_{h+1}(\\tau_{h+1}) + y(H - h)\\Big)\\\\ &< C_h([s]_l, a) +  \\int P_h(s' | [s]_l, a) \\tilde{C}^*_{h+1}(\\tau_{h+1}) + y(H - h) \\\\ &<  c_h(s, a) + ld_c +  \\int P_h(s' | s, a) +  \\Delta_P  C^*_{h+1}(\\tau_{h+1}) + y(H - h)\\\\ &=  c_h(s, a) +  \\int P_h(s' | s, a)  C^*_{h+1}(\\tau_{h+1}) + ld_c +  l\\Delta_P  \\int C^*_{h+1} + y(H - h) \\\\ &=  c_h(s, a) +   P_h(s' | s, a) C^*_{h+1} + ld_c +  l\\Delta_P   H C_{\\max} + y(H - h)\\\\ &<  c_h(s, a) +   P_h(s' | s, a) C^*_{h+1} + l(d_c +  \\Delta_P )(s_{\\max} - s_{\\min}) H C_{\\max} + y(H - h)\\\\ &= C(\\tau_h) +  l(d_c + \\Delta_P)(s_{\\max} - s_{\\min}) H C_{\\max} + y(H - h)\\\\&=  C(\\tau_h) + y(H - h + 1). \\end{aligned}$"}, {"title": "E Extensions", "content": "Markov Games. It is easy to see that our augmented approach works to compute constrained equilibria. For efficient algorithms, using -\u221e to indicate infeasibility becomes problematic. However, we can still use per stage LP solutions and add a constraint that the equilibria value must be larger than some very small constant to rule out invalid -\u221e solutions. Alternatively the AND/OR tree approach used in [27] can be applied here to directly compute all the near feasible states.\nInfinite Discounting. Since we focus on approximation algorithms, the infinite discounted case can be immediately handled by using the idea of effective horizon. We can treat the problem as a finite horizon problem where the finite horizon H defined so that \u03a3= -1Cmax \u2264 e'. By choosing e' and e small enough, we can get equivalent feasibility approximations. The discounting also ensures the effective horizon H is polynomially sized implying efficient computation."}]}