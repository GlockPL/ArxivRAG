{"title": "LEARNING TO HELP IN MULTI-CLASS SETTINGS", "authors": ["Yu Wu", "Yansong Li", "Zeyu Dong", "Nitya Sathyavageeswaran", "Anand D. Sarwate"], "abstract": "Deploying complex machine learning models on resource-constrained devices is challenging due to limited computational power, memory, and model retrainability. To address these limitations, a hybrid system can be established by augmenting the local model with a server-side model, where samples are selectively deferred by a rejector and then sent to the server for processing. The hybrid system enables efficient use of computational resources while minimizing the overhead associated with server usage. The recently proposed Learning to Help (L2H) model proposed training a server model given a fixed local (client) model. This differs from the Learning to Defer (L2D) framework which trains the client for a fixed (expert) server. In both L2D and L2H, the training includes learning a rejector at the client to determine when to query the server. In this work, we extend the L2H model from binary to multi-class classification problems and demonstrate its applicability in a number of different scenarios of practical interest in which access to the server may be limited by cost, availability, or policy. We derive a stage-switching surrogate loss function that is differentiable, convex, and consistent with the Bayes rule corresponding to the 0-1 loss for the L2H model. Experiments show that our proposed methods offer an efficient and practical solution for multi-class classification in resource-constrained environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine Learning (ML) models deployed on local devices often face significant limitations in terms of computational resources and retrainability. Local devices are typically constrained by limited processing power, memory, and battery life (Ajani et al., 2021; Biglari & Tang, 2023), which can impede the model's ability to handle large-scale data or perform complex computations in real-time. Furthermore, once a local model is deployed, it may be difficult to retrain or update (Hanzlik et al., 2021), leading to a potential degradation in performance over time when data distribution drifts (Lu et al., 2019).\nTo address these issues, one strategy is to augment the local machine learning system (a \u201cclient\") with an external model hosted on a remote server. This approach, seen in recent applications like Apple Intelligence (Gunter et al., 2024), enhances the overall system performance by leveraging the server's superior computational power and capacity for model updates. Recent studies have shown that efficient fine-tuning methods, like few-shot learning (Brown et al., 2020) and parameter-efficient fine-tuning (Fu et al., 2023), can achieve competitive performance, making it a feasible choice for server-side model deployment. Those methods utilize the pre-trained model's capabilities while requiring minimal computational resources, allowing for easy and cost-effective updates to adapt to specific tasks or data distributions. Consequently, it enables the deployment of powerful models on servers without the high costs of comprehensive model training.\nWhile training a server model can be relatively inexpensive, extensive use of server-side models can be costly due to data transfer, latency, instability connection, and service fees. Moreover, server operators may wish to limit the frequency of offloaded inferences from clients by imposing costs or access constraints. In the presence of such restrictions, the practical solution for clients to use a rejector that selectively defers/offloads only the most challenging and uncertain samples to the server for processing, thereby optimizing the balance between usage cost and performance. Informally, the"}, {"title": "RELATED WORKS", "content": "Hybrid ML systems. The hybrid ML systems that consist of client side and server side, have been of substantial research in many fields, including federated learning (Zhang et al., 2021; McMahan et al., 2017), distributed learning (Cao et al., 2023a; Horv\u00e1th et al., 2023), and decentralized learning (Sun et al., 2022; Fang et al., 2022; Liu et al., 2024; Li & Han, 2023). However, those frameworks only focus on interaction between different sides during the training process. Once the training is done, no more communication is needed among them. In this work, we are interested in the collaboration between client and server both in the training and inference phases.\nLearning with Abstention (LWA). The foundational work that first considered extra options for recognition tasks was proposed by Chow (1957) and Chow (1970). Herbei & Wegkamp (2006) revisit the framework in classification tasks and propose a score-based reject method. Subsequent works extend this framework to different types of classifiers (Rigollet, 2007; Wegkamp, 2007; Bartlett & Wegkamp, 2008; Wegkamp & Yuan, 2011). Cortes et al. (2016) consider a separate function to make reject decisions in binary classification. Zhu & Nowak (2022b;a) introduce the reject option in active learning. Cortes et al. (2018) add abstention to online learning. Zhang et al."}, {"title": "Learning to Defer (L2D).", "content": "Building upon LWA, Madras et al. (2018) were the first to consider a subsequent expert can process the rejected samples. Raghu et al. (2019) consider binary classification with expert deferral using uncertainty estimators. Mozannar & Sontag (2020a) propose the first method that jointly trains the local model and rejector in multi-class classification. Verma & Nalisnick (2022); Mozannar et al. (2023); Hemmer et al. (2023); Cao et al. (2023b) propose different surrogate loss functions for L2D. Okati et al. (2021) and Narasimhan et al. (2022) add post-hoc algorithms to the cases where the reject decision incurs extra cost.\nSeeking help from multiple experts has been explored in different machine learning models (Kerrigan et al., 2021; Keswani et al., 2021; Corvelo Benz & Gomez Rodriguez, 2022; Hemmer et al., 2022). Verma et al. (2022) extend the one-vs-all-parameterized L2D to the multiple experts case. Mao et al. (2023) study a two-stage scenario for L2D with multiple experts. Mozannar et al. (2023) provide a linear-programming formulation that optimally solve L2D in the linear setting. Verma et al. (2023) incorporate a conformal inference technique for multiple experts. Tailor et al. (2024) formulate a L2D system that can cope with never-before-seen experts. Mao et al. (2024b) introduce regression with deferral to multiple experts. Mao et al. (2024a) present a theoretical study of surrogate losses and algorithms for L2D with multiple experts. L2D complements the missing part of LWA; that is, after a reject decision, instead of being discarded, samples are sent to remote experts for predictions. However, in L2D, the experts on the server side are assumed to be either human or well-trained ML models, which are fixed during the training process. L2D focuses on training the local model and the rejector under the existence of expert models. This framework cannot handle the cases described in Sec. 1, where local models are legacy systems that have been deployed to the client side, and retraining the local models is not available."}, {"title": "Learning to Help (L2H).", "content": "Learning to Help (L2H) (Wu & Sarwate, 2024) is a complementary model to L2D whose aim is to train a server model and rejector to enhance the usability of systems containing a \"legacy\" local ML model that is unavailable for updating or retraining. The prior work focused on binary classification in the PPR setting and proposes a surrogate loss function that requires model-based calibration. This cannot be directly extended to multi-class problems due to issues with differentiability. In this work we generalize this model to multi-class problems and more scenarios (see Appendix A for a comparison)."}, {"title": "2 PROBLEM FORMULATION", "content": "We consider a decision system with two parties: a client and the server. We think of the client as a device with relatively limited computational power while the server has access to more computing resources. We study a multi-class problem in which the goal is to learn a prediction function $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$, where $\\mathcal{X} \\subset \\mathbb{R}^l$ is a space of instances/feature vectors with $l$ dimensions and $\\mathcal{Y} = [K] \\doteq \\{1, 2, ..., K\\}$ is a set of labels.\nThe L2H decision system (See Fig. 1) is constrained to have a certain architecture: in operation, the client receives an input $x$ and has two options: it can either make a prediction $\\hat{y}_{local}$ locally on the client or forward the input (\"defer\") $x$ to the remote server which produces a response $\\hat{y}_{remote}$. We can represent this system by a tuple of functions $(r(x), m(x), e(x))$, where the client side has a reject function $r(x)$ and prediction function $m(x)$ and the server has a prediction function $e(x)$."}, {"title": "2.1 GENERALIZED 0-1 LOSS FOR MULTI-CLASS CLASSIFICATION", "content": "To extend L2H to multi-class classification, we consider a special case of the general loss function (equation 1), often referred to as the generalized 0-1 loss in binary classification for L2H. In the generalized 0-1 loss, the parameters are defined as follows: $C_{cc} = 0$, $C_{ce} = 1$, $C_{sc} = c_e$, and"}, {"title": "2.2 BAYES OPTIMAL REJECTOR AND SERVER CLASSIFIER", "content": "In this subsection, we derive the Bayes classifiers as defined in equation 4 for both rejector and server classifier with a fixed client classifier $m(x)$ under generalized 0-1 loss function in equation 3. As mentioned at the beginning of Sec. 2, the task we considered is multi-class classification. The client classifier $m(x)$ and server classifier $e(x)$ are multi-class classifiers with output to be one label among $K$ classes. The rejector $r(x)$ is a binary classifier with two possible labels: LOCAL and REMOTE, which means either making a prediction on the client or on the server. The rejector $r(x)$ will give a label LOCAL to $x$ if $r(x) > 0$ and give a label REMOTE to $x$ when $r(x) \\le 0$. Without loss of generality, we assume that for the Bayes classifier of rejector, $r^B(x) \\in \\{+1, -1\\}$.\nAs discussed after equation 3, we consider the case where the client classifier $m$ is given and fixed during training. The output of the client classifier depends on input samplex, which can either be deterministic or stochastic, depending on the machine learning model of $m$. We formally consider the case where the client classifiers consist of $K$ sub-functions, say $[m_1(x), m_2(x),......,m_K(x)]$. The $i$-th sub-function $m_i(x)$ represents the score for $i$-th class on sample $x$. Then the prediction of client classifier is $\\arg \\max_i m_i(x)$. For simplicity, we assume that $\\arg \\max_i m_i(x)$ is unique. Choosing the class that has the highest output score as prediction is a standard operation for multi-class classification. In the following analysis, we assume that the output of client is a random variable $M$ conditioned on the input $x$ (Neal, 2012).\nTheorem 2.1. Given a client classifier $m(x)$, the solutions of Bayes classifiers (defined in equation 4) for generalized 0-1 loss under the space of all measurable functions are:"}, {"title": "3 STAGE-SWITCHING SURROGATE LOSS FUNCTION", "content": "In Sec. 2.2, the Bayes classifiers we derived minimize the risk of generalized 0-1 loss (equation 3). Since the distribution of the data set is unknown and the loss function is not differentiable, it's computationally intractable to get $(e^B, r^B)$ by solving equation 4. Another potential concern comes from the framework of the client-server system. In this system, the rejector is placed on the client side while the server classifier is placed on the server side, which requires synchronous updates between two sides while searching for the solution of the problem equation 4. In scenarios where the connection between the client and server is unstable, or bandwidth is limited as discussed in Sec. 1, continuously synchronizing the client and server during training becomes costly. These conditions necessitate that both the rejector and server classifier be capable of being trained in an asynchronous setting.\nSimilar with the definition of client classifier $m(x)$ in Sec. 2.2, we consider the case where server classifier $e(x)$ also consists of $K$ sub-functions, denoted as $[e_1(x), e_2(x),......,e_K(x)]$. The prediction of the server classifier is $\\arg \\max_i e_i(x)$ and assumed to be unique. We further consider the case that the rejector consists of 2 sub-functions, say $[r_1(x), r_2(x)]$, where $r_1$ stands for the score of LOCAL while $r_2$ stands for the score of REMOTE. In accordance with the space of $r^B$ as stated in Sec. 2.2, the output of the rejector is defined as $r(x) \\doteq 1[r_1(x) > r_2(x)] \\cdot 2 - 1$. Based on the definitions stated above, we propose a stage-switching surrogate loss function, which is differentiable and can be used in both synchronous and asynchronous settings. The surrogate loss function is defined as:"}, {"title": "3.1 CONVEXITY AND MONOTONICITY OF THE SURROGATE LOSS FUNCTION", "content": "In the following proposition, we show that in both stages, the sub-surrogate loss function $L_1$ and $L_2$ are both convex or monotone.\nProposition 3.1. For each given $(x, y)$, the loss function $L_1$ is convex over $e_i(x)$, for any $i \\in [K]$; and the loss function $L_2$ is:\n\\begin{itemize}\n    \\item convex over $r_1(x)$ and $r_2(x)$, when $1 - c_e - c_1 + c_1\\mathbb{1}_{e=y} > 0$;\n    \\item monotonically decreasing over $r_1$ and monotonically increasing over $r_2$ when $1 - c_e - c_1 + c_1\\mathbb{1}_{e=y} < 0$.\n\\end{itemize}\nThe proof is given in Appendix D. The convexity of a function ensures that we can find the global minimizer through gradient-related optimization methods (Boyd & Vandenberghe, 2004; Nesterov, 2014). As for the monotonicity of $L_2$ when $1 - c_e - c_1 + c_1\\mathbb{1}_{e=y} \\ge 0$, we will show that in Theorem 3.2, this property can help for solving the corner case in the proof. Based on differentiability, convexity, or monotonicity, we prove in Sec. 3.2 that our proposed surrogate loss function is consistent, that is, any minimizer of the surrogate risk (equation 10) also minimizes the generalized 0-1 risk of equation 3, referring to the consistency defined in Section 2.2 by Cao et al. (2023b)."}, {"title": "3.2 CONSISTENCY OF SURROGATE LOSS FUNCTION", "content": "In this subsection, we verify the consistency between the surrogate loss function (equation 7) and the generalized 0-1 loss function (equation 3). Formally, we prove the following theorem:\nTheorem 3.2. Under the space of all measurable functions, the surrogate loss function (equation 7) is consistent with the generalized 0-1 loss function (equation 3), that is, the minimizer of the risk of surrogate loss function also minimizes the risk of original loss function:"}, {"title": "4 COMPUTATIONALLY FEASIBLE ALGORITHMS FOR LEARNING TO HELP", "content": "The stage-switching surrogate loss function that we propose in equation 7 ensures the usability of gradient-based optimization methods to train the rejector and server classifiers with flexibility in three settings we discuss in Section 1: PPR, IA, and BRR settings.\nThe training in the server stage is the same for all three settings since $L_1$ is only a function of the server classifier; no knowledge of the rejector or client classifier is needed. In the rejector stage, we set up different algorithms based on the constraints in different settings. Besides, for BRR settings, we propose post-hoc Algo. 4 and 5 after standard training. Since the stage-switching surrogate loss function is differentiable, any gradient-based optimization method can be used for training our model. To reduce the computation, we use Stochastic Gradient Descent (SGD) for presentation."}, {"title": "Pay-Per-Request (PPR)", "content": "Recall the scenarios we depict in Section 1, in the PPR setting, we consider that the client is always connected with the server; that is, the rejector has instant access to the latest version of server classifier $e(x)$ during the whole training phase. This setting works for scenarios where the connection is in real-time, and the bandwidth is ample and free during training, but each request conducts payment.\nWe design a synchronized algorithm, Algo. 1, that works for the PPR setting. The algorithm iteratively runs in the server stage, and in the rejector stage for each pair of samples and labels $(x,y)$. In the server stage, the server classifier $e(x)$ is updated by the sub-loss $L_1$. In the rejector stage, the current $e(x)$ is instantly shared with the rejector. Together with the outputs of the fixed client classifier and current server classifier, the sub-loss $L_2$ is calculated to update the rejector."}, {"title": "Intermittent Availability (IA)", "content": "In the IA setting, the connection between the client and the server is not always available. This scenario may happen in any distributed system that is connected to the public network because of traffic control. Also in private networks, connection may be lost due to instability of the network. If we still train our model with synchronized algorithm 1, the training process will be postponed once the connection is delayed or lost. Holding the status of both the rejector and server classifier will cause a waste of resources and time.\nOur stage-switching surrogate loss function in equation 7 consists of two sub-loss functions. An asynchronized algorithm is best to tackle those issues. The idea is to separately train the server classifier with $L_1$ on the server side and train the rejector with $L_2$ while keeping a temporary version of the server classifier $\\bar{e}$ on the client side. The $\\bar{e}$ will be updated only when the intermittent connection between client and server is built. Considering that the client and server only connect every $S$ time slot we propose the asynchronized training algorithm in Algo. 2. In line 4 of Algo. 2, the server connects with the client and sends the current server classifier $e$ as the latest $\\bar{e}$. In line 5, we calculate the $L_2$ loss with stored server classifier $\\bar{e}$."}, {"title": "Bounded-Reject-Rate (BRR)", "content": "While a few works focus on the BRR setting in L2D and L2H frameworks, the setting is indeed realistic. For example, subscribers of ChatGPT or other AI assistants don't pay extra money for each inquiry to advanced models, but they are allowed to only"}, {"title": "5 EXPERIMENT", "content": "In this section, we test the proposed surrogate loss function in equation 7 and algorithms for different settings on CIFAR-10 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), and CIFAR-100 (Krizhevsky & Hinton, 2009) datasets.\nIn our experiments, the base network structure for the client classifier and the rejector is LeNet-5, and the server classifier is either AlexNet or ViT. The client classifier is trained solely and works as a fixed model in the following process. Since Algo. 1 is the basic algorithm for our multi-class L2H framework, we start from the evaluation for the PPR setting.\nTrade-off accuracy and reject rate on reject cost and inaccuracy cost In this experiment, we evaluate the impact of reject cost $c_e$ and inaccuracy cost $c_1$ on the overall accuracy and reject rate,"}, {"title": "6 CONCLUSION", "content": "In this paper, we extend the Learning to Help framework to address multi-class classification with different resource constraints: PAY-PER-REQUEST, INTERMITTENT AVAILABILITY, and BOUNDED REJECT RATE. By introducing a stage-switching surrogate loss function, we enabled effective training of both the server-side classifier and the rejector with computationally feasible algorithms for three settings. Our experimental results demonstrate that the proposed methods offer a practical and efficient solution for multi-class classification, aligning with the theoretical guarantees we established. This work opens new opportunities for further exploration of multi-party collaboration in hybrid machine learning systems, especially in the era of LLM."}, {"title": "A COMPARISON WITH BINARY SURROGATE LOSS", "content": "In this section, we compare the surrogate loss function proposed in binary case by Wu & Sarwate (2024) with our proposed method in Section 3.\nIn this work, the formula of the surrogate loss function is:\n$L_{wu}(r, e, x, y) = c_1 \\exp \\left( \\frac{-(\\beta e_y - r)}{\\alpha} \\right) + c_e \\exp(-r) + \\mathbb{1}_{m(x)y \\le 0} \\exp(\\alpha r)$\nwhere $\\alpha$ and $\\beta$ are parameters for calibration. If we directly make the extension to multi-class in our setting, we can keep the same definition of rejector while the output of client classifier and server classifier becomes $\\arg \\max_i m_i(x)$ and $\\arg \\max_i e_i(x)$, which results in the surrogate loss:\n$L'_{wu} (r, e, x, y) = c_1 \\exp \\left( \\frac{-(\\beta \\arg \\max_i e_i(x)_y - r)}{\\alpha} \\right) + c_e \\exp(-r) + \\mathbb{1}_{\\arg \\max_i m_i(x)y < 0} \\exp(\\alpha r)$\nThe client classifier part works well since the client classifier is fixed during training. However, this part $\\arg \\max_i m_i(x)y$ will lead to trouble since both $\\arg \\max_i m_i(x)$ and $y$ are index numbers of class in multi-class and therefore, the function is not differentiable with respect to $e_i(x)$.\nFurthermore, this surrogate loss function requires to calculate calibration parameters $\\alpha$ and $\\beta$, which depends on the estimation of the accuracy of the client classifier, for each input sample $x$ as shown in Theorem 2 in this work (Wu & Sarwate, 2024), which adds more calculations and estimation errors to practical experiments. This surrogate loss function is not available in the IA setting since it's jointly training the rejector and server classifier at the same time.\nCompared to the extension of the surrogate loss function proposed by Wu & Sarwate (2024) with our stage-switching surrogate loss function, our method is differentiable in multi-class and more flexible for fitting different settings."}, {"title": "B COMPARISON WITH DIFFERENT SURROGATE LOSS FUNCTIONS", "content": "In this section, we justify that previous surrogate loss functions, proposed from LWA and L2D, can not be directly extended to L2H by comparing those surrogate loss functions with our method, shown in Table 2, on the properties that are needed in the settings that we are interested in.\nConfindence-LWA (or score-based-LWA) represents the methods in LWA that use the output $m(x)$ or function of output $f(m(x))$ of the client classifier as a metric of confidence on current prediction. If the value of the metric is smaller than a threshold, then the input sample $x$ will be discarded as a result of rejection. The theoretical framework of Confidence-LWA was first proposed by Herbei & Wegkamp (2006). There is no separate rejector in those methods. The rejected samples will be abandoned since there will be no more actions after rejection. Separate rejector-LWA represents those surrogate loss functions in LWA that assume a separate rejector to independently decide whether to reject or not. This kind of method was first proposed by Cortes et al. (2016) and proves the inability of confidence-based methods in special cases, referring to Sec. 2.2 of this work. However, in the LWA framework, the rejection decision only depends on the local classifier $m$ and does not consider the issues after rejection.\nIn the L2D framework, samples are sent to an expert after rejection. Softmax-L2D, firstly proposed by Mozannar & Sontag (2020b), and OvA-L2D, firstly proposed by Verma & Nalisnick (2022) are two different classes of surrogate loss functions. Softmax-L2D adds a reject option as another class of task to the cross-entropy loss function, while OvA-L2D uses surrogate loss functions that transform multi-class classification into a combination of multi-binary classifiers. However, the output of the rejector on both methods also depends on the output of each class $m_i(x)$ on mobile classifier $m(x)$. Moreover, during training, the client and the server must keep connection with each other.\nTwo-stage (TS) confidence-L2D and two-stage (TS) separate rejector-L2D are those kinds of surrogate loss functions that consist of different sub-loss functions and can be trained in corresponding stages for both confidence and separate rejector cases, firstly proposed by Mao et al. (2023). Those two methods can be used for IA settings where the training is asynchronized since their surrogate loss"}, {"title": "C PROOF OF THEOREM 2.1", "content": "The standard approach to deriving the Bayes classifier for any given loss function is to compare the posterior risk associated with each possible decision of the classifier. The decision that minimizes the posterior risk is then chosen as the output. Recall in Section 2.1, the regression function is $n_i(x) = P(Y = i | X = x)$ and the generalized 0-1 loss function defined in equation 3 is:\n$L_{general}(r, e, x, y; m) = \\mathbb{1}_{m(x) \\ne y}\\mathbb{1}_{r(x) = LOCAL} + c_e\\mathbb{1}_{e \\ne y}\\mathbb{1}_{r(x) = REMOTE} + (c_e + c_1)\\mathbb{1}_{e \\ne y}\\mathbb{1}_{r(x) = REMOTE}$.\nNote that $m(x)$ is also stochastic, as discussed in Section 2.2. The random variable corresponding to the client classifier is denoted by $M$. Then, for any given samplex, the posterior risk over the conditional distribution of Y and M of any decision $r' \\in \\{REMOTE, LOCAL\\}$ and $e' \\in [K]$ is:"}, {"title": "D PROOF OF PROPOSITION 3.1", "content": "We first show the convexity of $L_1$ and $L_2$ w.r.t $e$. For $L_1$ in equation 8, we notice that it equals to cross-entropy loss function for multi-class tasks. Referring to the proof by Cover & Thomas (2005), in which the cross-entropy is transformed into the form of KL divergence, which is convex, we can"}, {"title": "E PROOF OF THEOREM 3.2", "content": "The surrogate loss function defined in equation 7 consists of $L_1$ and $L_2$, which are targeted to the server classifier and rejector, respectively. Since the rejector and server classifier are trained in a stage-switching manner as described in Section 3, parameters of $e$ are only updated by the gradient derived from $L_1$, and that of $r$ is only updated by the gradient derived from $L_2$. Therefore, the minimizer of $e$ is derived from the risk of $L_1$ in the server stage, and the minimizer of $r$ is derived from the risk of $L_2$ (with the current server classifier) in the rejector stage. The risks are:"}, {"title": "F STOCHASTIC POST-HOC ALGORITHM FOR BOUNDED REJECT RATE SETTING", "content": "We propose a stochastic post-hoc method, which is shown in Algo. 3, Algo. 4 and Algo 5. In this algorithm for BRR settings, we firstly train our rejector and server classifier following the process as shown in Algo. 3, which is similar to algorithms for PPR or IA settings (can also be trained with asynchronized algorithm 2). After training, we use a calibration set $D_{cali}$ to calculate the empirical reject rate $q_1$, based on the output of rejector $r(x)$. Once we get the empirical reject rate, we compare it with the bounded reject rate $q$.\nIf $q$ is greater than $q_1$, that means we have to sacrifice several rejected samples to force them to make prediction locally at client classifier $m(x)$; the input samples are following the Algo. 4, where each rejected samples are made a final decision by the value of a uniform random variable. Once the value is below the ratio $p$, the sample will still be sent to the server. Otherwise, it may be predicted by the client classifier. When the empirical reject rate is higher than the bounded reject rate, after adding this post-hoc mechanism, the bounded reject rate $q$ can just ensured.\nIf $q$ is smaller than or equal to $q_1$, that means we can reject more times than the rejector $r(x)$ request. Then, we follow a similar mechanism in Algo. 5, which uses a random variable to decide which samples the rejector sends to the server classifier. After using the post-hoc mechanism, we can ensure the bounded reject rate while making the best use of the server classifier."}, {"title": "G EXPERIMENT DESCRIPTION AND ADDITIONAL RESULTS", "content": "Datasets We test our proposed surrogate loss function 7 and algorithms for different settings on CIFAR-10 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011) and CIFAR-100 (Krizhevsky & Hinton, 2009) data sets. CIFAR-10 consists of 32 \u00d7 32 color images drawn from 10 classes and is split into 50000 training and 10000 testing images. CIFAR-100 has 100 classes containing 600 images each. SVHN is obtained from house numbers with over 600000 photos in Google Street View images and has 10 classes with 32 \u00d7 32 images centered around a single character. The experiments are conducted in RTX 3090. It takes approximately 5 minutes to train the local model and 30 minutes to train each remote server classifier with the rejector when server classifier is set to be AlexNet or 5 hours to train each remote server classifier with the rejector when server classifier is set to be ViT.\nComparison over synchronization and asynchronization We conduct additional experiments training the server model and rejector under both synchronized, as shown in Algo 1 and asynchronized settings as shown in Algo 2. The model is evaluated in the same way as in Section 5. The contrastive evaluation and loss curve are shown in Table 3 and Figure 4.\nResult for different $c_1$ and $C_e$ on CIFAR-10 and SVHN Besieds the result we show in Section 5, we add extensive result for contrastive evaluation with different inaccuracy cost $c_1$, reject cost $c_e$ and synchronization interval S (for IA setting) for CIFAR-10 and SVHN with local classifier and rejector being LeNets and remote classifier being AlexNet. The results are shown in Table 3.\nResult for different $C_1$ and $c_e$ on CIFAR-100 Since our stage-swtich surrogate loss function doesn't set up constraints on the size of dataset and structures of machine learning methods, we test the Contrastive Evaluation on dataset CIFAR100 with local classifier and rejector being LeNet and remote classifier being ViT, a transformer-based vision neural network. The results are shown in Table 4. Both higher $c_1$ and $c_e$ can reduce the ratio of numbers that are sent to remote classifier, but in our experimental results, the reject rate is more sensitive to $c_e$. When $C_e$ is close to 1, almost no sample is sent to remote classifier, which make sense because $C_e 1$ means that the cost of asking for help is always greater than locally prediction.\nSamples partitioned by rejector We choose one case where the rejector and remote classifier AlexNet are trained on SVHN when $C_1 = 1.25$ and $C_e = 0.25$. Then we let rejector partition the test set and randomly pick images from the subset with $r(x) = REMOTE$ and $r(x) = LOCAL$, respectively. The results are shown in Fig. 5 and Fig. 6. It's obvious that samples kept locally are clearer, contrast, focused, with high-resolution, while the samples that are supposed to send to the server are blurry, unfocused, with low-resolution. Without any manual adjustment, our rejector learns to assess \u201cdifficulty\u201d using the same metrics as humans, by interacting with both local model and server model during the training process."}]}