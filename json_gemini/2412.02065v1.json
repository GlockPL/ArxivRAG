{"title": "Leveraging Large Language Models to Democratize Access to Costly\nFinancial Datasets for Academic Research", "authors": ["Julian Junyan Wang", "Victor Xiaoqi Wang"], "abstract": "Unequal access to costly datasets essential for empirical research has long hindered\nresearchers from disadvantaged institutions, limiting their ability to contribute to their fields and\nadvance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential\nto democratize data access by automating data collection from unstructured sources. We develop\nand evaluate a novel methodology using GPT-40-mini within a Retrieval-Augmented Generation\n(RAG) framework to collect data from corporate disclosures. Our approach achieves human-level\naccuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical\nAudit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and\n40 minutes respectively, each at a cost under $10. This stands in stark contrast to the hundreds of\nhours needed for manual collection or the thousands of dollars required for commercial database\nsubscriptions. To foster a more inclusive research community by empowering researchers with\nlimited resources to explore new avenues of inquiry, we share our methodology and the resulting\ndatasets.", "sections": [{"title": "1. Introduction", "content": "In the realm of academia, the adage \u201cpublish or perish\u201d has long been a guiding principle,\nhighlighting the critical importance of research output in scholarly careers. The pressure to publish\nhas intensified in recent decades, as scholarly output now serves as the primary metric for assessing\nresearch excellence, advancing academic careers, and establishing institutional rankings. Studies\nby Swanson (2004) and van Dalen and Henkens (2012) have shown how publication metrics\nincreasingly influence not only individual career outcomes\u2014such as tenure, promotion, and\nremuneration\u2014but also broader institutional outcomes like university rankings and the allocation\nof research grants. This heightened emphasis on research output has created a highly competitive\nacademic environment, where the ability to conduct and disseminate impactful research is\nparamount.\nThis publication-centric paradigm, while ostensibly meritocratic, has inadvertently\nfostered a landscape of inequality within academia. Well-resourced institutions, with their access\nto cutting-edge tools, comprehensive databases, and ample research support, stand at a significant\nadvantage. In contrast, researchers at less affluent institutions often find themselves navigating a\ntreacherous path, their scholarly ambitions hampered by limited access to essential resources, data,\nand infrastructure. This disparity not only impedes individual career progression but also threatens\nto homogenize the pool of contributors to academic knowledge, potentially stifling the diversity\nof perspectives that is vital for robust intellectual discourse and innovation.\nPerhaps, nowhere is this divide more pronounced than in the fields of finance, accounting,\nand other business disciplines where a seismic shift towards empirical and quantitative\nmethodologies has occurred in recent decades and further intensified in recent years. Business\nresearch has become increasingly empirical and quantitative, with the proportion of empirical\nstudies in finance rising from 68 percent in 2001 to 85 percent in 2019 (Berninger et al. 2022, Dai\net al. 2023). This trend mirrors the shift from theoretical to empirical research in economics\n(Angrist et al. 2020, Hamermesh 2018) and continues a pattern that began in the last century (Kim\net al. 2006, Schwert 2021).\nThe increasing prevalence of empirical research in business fields has led to a growing\nreliance on databases, with studies using more databases being more likely to be published\n(Berninger et al. 2022, Dai et al. 2023). This trend has heightened the importance of access to\ncomprehensive and diverse datasets for researchers seeking to make significant contributions to\ntheir respective fields. Moreover, publishing novel insights often necessitates unique datasets,\nwhich can be challenging and expensive to acquire, especially when data is not commercially\navailable or has only recently emerged, e.g., due to regulatory changes or advances in technology.\nThis transformation has made expensive datasets crucial for academic success and led to\nincreased researcher dependence on them. Researchers from well-funded institutions often have\nan advantage in obtaining such datasets, either through internal resources or by purchasing access\nfrom commercial providers. In contrast, those from less privileged backgrounds or institutions with\nlimited funding may struggle to acquire the necessary data, hindering their ability to conduct\ncutting-edge research and contribute to the advancement of their fields. The acquisition of these\ndatasets, either through expensive subscriptions or labor-intensive manual collection, has become\na formidable barrier to entry for many aspiring researchers, particularly those at institutions with\nlimited financial resources. Consequently, the academic landscape risks becoming increasingly\nhomogeneous, with research perspectives and insights predominantly shaped by a small number"}, {"title": "2. Background and Literature Review", "content": "In business fields, the type of research conducted in recent decades has become increasingly\nempirical and quantitative. Dai et al. (2023) conduct an analysis of 52,497 papers posted in the\nFinancial Economics Network (FEN) of the Social Science Research Network (SSRN) from 2001\nto 2019, finding that the proportion of empirical research has increased from 68 percent in 2001 to\n85 percent in 2019. This finding is consistent with Berninger et al. (2022), who document that the\nshare of empirical contributions to finance journals grew from 70 percent in 2000 to almost 90\npercent in 2016.\nThis trend also parallels the pivot from theoretical research to empirical research in the\nfield of economics (Angrist et al. 2020, Hamermesh 2018). Moreover, the current rise in empirical\nresearch in business fields is merely a continuation of a trend that began in the last century. For\nexample, in the Journal of Financial Economics, 59 percent of articles were theoretical and only\n39 percent were empirical over 1974 to 1979 (Schwert 2021). However, there has been a radical\nreversal with 88 percent of papers being empirical over 2010 to 2020 with only 12 percent being\ntheoretical. Kim et al. (2006) find a similarly drastic change in 41 finance and economics journals\nwith 77 percent of the most cited papers being theoretical in the 1970s and only 11 percent being\ntheoretical in 2000.\nThis rise in empirical research in business fields is accompanied by an increasing\ndependence on databases. Dai et al. (2023) find that the average number of databases per empirical\narticle has increased from 2.89 to 4.66 between 2001 and 2019. Berninger et al. (2022) equally\nobserve an increase from two to more than 3.5 databases used per article, which they partially\nattribute to growing pressure to use more control variables and robustness checks. According to\nthem, one database does not provide sufficient data to gain insights that warrant publication,\nleading to more databases being required to address meaningful research questions. Dai et al.\n(2023) demonstrate that this pressure to use more databases is not misplaced as a one standard\ndeviation increase in the number of databases used in a study corresponds to a 26 percent higher\nlikelihood of publication. To produce quality research in business fields today, researchers require\ncomprehensive data to align with the increasingly empirical and quantitative nature of these fields."}, {"title": "2.2 Limited Access to Data at Disadvantaged Institutions", "content": "The growing importance of data in research has highlighted the unfortunate reality that access to\ndata is unequal due to financial barriers. Borgman (2015) borrows from Anderson (2004) to\nsuggest a \"long tail\u201d distribution of data access where there exists a small number of well-funded\nresearch teams working with large volumes of data, some teams working with almost no data, and\nmost teams falling in between. Berninger et al. (2022) demonstrate this unequal data access in\nfinancial research empirically. They show that researchers affiliated with top business schools tend\nto use easier-to-download datasets that are more expensive, whereas researchers from lower-\nranking business schools rely more on less expensive, often harder-to-use data sources, which may\nprimarily serve business professionals rather than academics.\nThis reality raises significant concerns about equity and access in academic research,\nparticularly for scholars at smaller institutions with limited funding. These researchers often face\ninsurmountable obstacles in acquiring or creating novel datasets due to financial constraints, lack\nof research assistance, and limited technological infrastructure. Unlike their counterparts at well-\nfunded universities, faculty at smaller institutions typically juggle heavier teaching loads, leaving\nless time for the labor-intensive tasks of data collection and curation. The inability to access or\ncreate novel datasets can put these researchers at a significant disadvantage when competing for\npublication in top journals, potentially creating a self-reinforcing cycle where they struggle to build\nthe publication record necessary to secure grants or move to better-resourced institutions.\nAs data becomes increasingly crucial for research in business disciplines, addressing this\ninequality in data access will be essential to ensure that all researchers have the opportunity to\nconduct impactful and innovative studies. Without equal access to comprehensive and user-\nfriendly datasets, researchers at institutions with limited resources may struggle to contribute to"}, {"title": "2.3 Impact on Research Productivity", "content": "The literature on research productivity identifies various determinants at the individual,\ninstitutional, and national levels (Beaudry and Allaoui 2012, Dundar and Lewis 1998, Heng et al.\n2021, Simisaye 2019, Wanner et al. 1981). Availability of funding is a crucial institutional factor\nthat can increase research productivity by enabling academics to attend conferences, publish work,\nand acquire reference materials (Bland and Ruffin 1992, Lertputtarak 2008).\nResearch funds can also increase productivity by providing access to graduate research\nassistants (where available) and reference materials. Dundar and Lewis (1998) find that research-\ndoctorate programs with greater financial support and a greater percentage of graduate students\nserving as research assistants saw greater departmental research productivity. In business research,\nresearch assistants can gather data from decentralized and unstructured sources, serving as a\nsubstitute for expensive databases. Conversely, management faculty at business schools with\nhigher teaching loads, characteristic of less-funded institutions, have lower research productivity\n(Kim and Choi 2017). These findings emphasize the importance of addressing unequal access to\ndata and research resources across institutions.\nWhile co-authoring with researchers from institutions with data access is a potential\nsolution, it presents several challenges. First, researchers from institutions with limited resources\nmay struggle to find suitable collaborators with access to required data. This can be due to a lack\nof established networks or the reluctance of researchers from well-funded institutions to\ncollaborate with those from less-resourced ones. Second, even when collaborations are established,\nresearchers without direct data access may have less control over the research process and depend\non collaborators for data-related tasks. This dependency can create power imbalances and impede\nresearchers' ability to fully explore their research questions or preferred methodological\napproaches. Third, relying on collaborations with data-rich institutions may limit the diversity of\nresearch perspectives and questions explored, due to their less control over the research process.\nTherefore, democratizing access to expensive datasets through GenAI can enable\nresearchers from diverse institutions and backgrounds to independently pursue their interests,\npotentially leading to more varied and innovative research outputs."}, {"title": "2.4 AI and Research Productivity", "content": "Given the significant impact of financial barriers and unequal access to data on research\nproductivity, it is crucial to explore potential solutions to level the playing field. The critical issue\nis whether digital tools, especially GenAI, can \u201clevel the playing field\u201d and contribute to a more\nequitable research landscape. Indeed, many researchers currently believe that GenAI can increase\nresearchers' productivity and contribute to a \u201cdemocratization\u201d of academic research. In a survey\nof 1,600 researchers, the most popular answer to a question on the biggest benefit of GenAI in\nresearch was to support researchers who do not speak English as a first language (Van Noorden\nand Perkel 2023). This suggests that GenAI could help reduce language barriers and enable a more\ndiverse group of researchers to contribute to the global scientific community.\nIn the context of quantitative research, Filetti et al. (2024) suggest that GenAI will enable\nacademics to be more efficient and streamline the research process by automating menial tasks\nsuch as data cleaning and normalization. By reducing the time and effort required for these tasks,\nGenAI could allow researchers to focus on more complex and value-added aspects of their work,\npotentially leading to increased research productivity. Already, there are examples or evidence of\nhow researchers may use GenAI to replace or enhance certain tasks. For instance, Dowling and\nLucey (2023) demonstrate that ChatGPT can significantly assist with finance research, excelling\nin idea generation and data identification, while showing limitations in literature synthesis and\ntesting framework development. Similarly, Korinek (2023) explores how LLMs such as ChatGPT\ncan assist economists in various aspects of the research process, from ideation and writing to data\nanalysis, coding, and mathematical derivations.\nThe ability of new technologies to revolutionize academic research and \u201clevel the playing\nfield\" is not new. For example, the development of communication technologies enabled the\npossibility of greater collaboration (e.g. co-authorship) which particularly benefitted middle-tier\nuniversities and weakened the competitive edge of elite universities (Agrawal and Goldfarb 2008,\nKim et al. 2009). This example highlights how technological advancements can disrupt traditional\npower dynamics in academia and create a more equitable research landscape.\nIt is important to note, though, that the case of communication technology specifically\naffected the logistics of conducting research and not the research itself. In contrast, recent\ntechnological advances such as machine learning and GenAI have enabled researchers to be more\nefficient in conducting various aspects of research, leading to savings in both time and financial\ncosts (Dowling and Lucey 2023, Przyby\u0142a et al. 2018). These technologies have the potential to\ndirectly impact the research process by automating tasks, extracting insights from large volumes\nof data, and supporting researchers in their analysis and interpretation of findings.\nIn this study, we examine whether GenAI has the potential to democratize research,\nspecifically by investigating its ability to democratize or equalize access to expensive datasets,\nwhich are essential for conducting quantitative research, a dominant type of research in finance\nand many other business disciplines. The term \u201cdemocratization\" has frequently permeated\ndiscussions of GenAI, and it is important to clarify that democratization does not necessarily mean\n\"leveling the playing field.\" Rather, the reverse is true. Etymologically, \"democracy\" refers to\ngiving power to the people, and \"democratization,\" as applied to academic research, would\nreasonably mean broadening academic research to include a larger population. \"Leveling the\nplaying field\" is, therefore, one way of achieving \"democratization.\"\nThe use of GenAI to enable researchers to quickly collect data at minimum cost could\ndemocratize academic research in three ways: broadening the group of researchers able to perform\nquantitative research, broadening the range of topics studied quantitatively, and broaden the\ngeographic range of countries studied. Firstly, GenAI could empower researchers who were\npreviously unable to conduct quantitative research due to financial barriers limiting their access to\ndata. The latest technology has the potential to allow researchers to collect and structure publicly\navailable data that exists in unstructured formats. For instance, OpenAI's most recent version of a\ncost-effective yet highly powerful model (\u201cGPT-4o-mini\u201d) costs as little as US$0.15 per million\ninput tokens, making large-scale data collection financially accessible to a wide range of\nresearchers (OpenAI).\""}, {"title": "2.5 Using GenAI to Collect Data", "content": "This study investigates the potential of GenAI for automating data collection from unstructured\ndata sources. Many prior studies have extracted data from SEC filings or other corporate\ndocuments, primarily relying on rule-based methods. Most of these studies focus on extracting\nentire sections from large documents. For example, Li (2010) extracts MD&As from both 10-K\nand 10-Q filings, while Muslu et al. (2014) extract MD&As from 10-K filings. Similarly, Bao and\nDatta (2014) extract risk factor disclosures (Item 1A) from 10-K filings, and Dyer et al. (2017)\nextract various sections from 10-K filings to assess the trend of disclosure practices.\nAlthough 10-K reports and other regulated filings follow standardized formats required by\nthe SEC, company-specific variations pose significant challenges for extracting complete sections.\nAs Bao and Datta (2014, p. 1378) observe, \"Because of the inconsistent file format (e.g., TXT or\nHTML) and form layout (e.g., headings are highlighted using different fonts or capitalized letters),\nit is quite challenging to automatically extract these risk factors from 10-K forms.\" While\nresearchers have developed various approaches to address these challenges, including rule-based\nmethods for extracting sections from PDF documents (e.g., El-Haj et al. 2020), the inconsistency\nin company formatting continues to complicate automated extraction efforts.\nExtracting specific information embedded within a section becomes even more difficult\nusing programmatic approaches. This challenge has led recent studies exploring regulatory\nchanges in disclosures, such as human capital, to manually collect quantitative or qualitative\ndisclosures from 10-K filings (e.g., Bourveau et al. 2023, Demers et al. 2024b). Machine learning"}, {"title": "3. Data Sources and Experimental Tasks", "content": "While acknowledging the critique of US-centric studies, we strategically focus on Securities and\nExchange Commission (SEC) filings for several reasons. The SEC's EDGAR system, hosting more\nthan 20 million filings since the introduction of electronic filing in 1993, provides an extensive\ndataset ideal for testing the performance of LLMs on large samples. Moreover, a large portion of\nthese filings come from foreign registrants, providing substantial international representation.\nOur methodology has wide potential across various jurisdictions and is not limited to SEC\nfilings. The use of US data serves as a proof of concept, demonstrating GenAI's potential in\nprocessing large volumes of unstructured text that vary in presentation form and formatting. The\ntask complexity we tackle in this study, rather than the specific format or regulatory framework,\nshowcases the generalizability of our approach to other types of corporate documents. The insights\nfrom this study are readily adaptable to other regulatory contexts, and the framework we develop\nand use can be tailored to various requirements of reporting systems worldwide.\nFor our tests, we focus on data that results from two recent regulations: the CEO pay ratio\ndisclosure and the Critical Audit Matter (CAM) disclosure. As mandated by the Dodd-Frank Act,\npublic companies are required to disclose the ratio of the CEO's annual total compensation to the\nmedian compensation of all other employees. The SEC adopted the final rule implementing the\npay ratio disclosure requirement in August 2015, and it became effective for fiscal years beginning\non or after January 1, 2017. The pay ratio disclosure has attracted significant attention from\nresearchers (Boo et al. 2024, Boone et al. 2024, e.g., Cheng and Zhang 2023)), as it offers new\ninsights into income inequality within firms and the potential effects of pay disparities on\nemployee morale, productivity, and firm performance.\nCAMs are significant issues that auditors communicate to the audit committee, which are\nrequired to be disclosed in the auditor's report under the new auditing standard AS 3101. The\nPublic Company Accounting Oversight Board (PCAOB) adopted AS 3101 in 2017, and it became\neffective for audits of fiscal years ending on or after June 30, 2019, for large accelerated filers, and\nDecember 15, 2020, for all other companies to which the requirement applies. CAMs are matters\nthat involve especially challenging, subjective, or complex auditor judgment, such as areas with\nhigh estimation uncertainty or significant unusual transactions. The disclosure of CAMs provides\nvaluable insights into the most significant risks and uncertainties faced by companies, as well as\nthe auditor's perspective on these issues. Early studies on CAMs have provided valuable insights\n(e.g., Bentley et al. 2021, Beyer et al. 2024, Burke et al. 2023, Klevak et al. 2023). These studies\nprimarily come from institutions with the financial resources to purchase data from providers,\nwhich collect the data from 10-K filings.\nWe have chosen these two types of data for several reasons. First, these disclosures come\nin a wide variety of formats and are not tagged using XBRL, making it challenging to collect them\nusing traditional automated methods. The language and terminology used in these disclosures can\nalso vary significantly, further complicating the use of automated collection methods. As a result,\nmanual collection is necessary to accurately gather this data before the recent breakthrough in\nGenAI.\nSecond, these two types of data reflect the challenges faced by researchers in the business\nfield. Pay ratio disclosures are currently not readily available from commercial data providers, and\nalthough some volunteers have manually collected and shared this data2, they may not be\ncomprehensive or updated frequently enough to meet researchers' needs. On the other hand, CAM\ndisclosures are available from commercial data providers, at a substantial subscription fee, which\ncan be prohibitively expensive for some institutions. These datasets illustrate the challenges in\nterms of data accessibility facing researchers at institutions with limited resources, as they are both\ncostly in terms of either manual collection or significant financial expenditure. Furthermore, pay\nratio disclosures involve quantitative data, whereas CAMs represent qualitative data. By focusing"}, {"title": "4. Methodology", "content": "Extracting data from CEO pay ratio disclosures can be challenging due to the varying formats and\nnarratives used by different companies, as illustrated by the sample disclosures in Appendix A.\nThe formatting of these disclosures is quite different across companies and lacks consistency,\nmaking it challenging for traditional rule-based methods to accurately identify and extract the\nrelevant data. Similarly, the presentation of Critical Audit Matters (CAMs) in auditor's reports\nfrom 10-K filings can differ significantly between companies, as shown in Appendix B. The varied\nstructure, formatting, and language patterns used by different companies make it difficult to extract\nCAMs consistently using traditional automatic algorithms.\nTo address these challenges, we leverage Large Language Models (LLMs) and data\nprocessing techniques within a Retrieval-Augmented Generation (RAG) framework. We begin\nwith small-scale experiments using the ChatGPT interface to evaluate the potential of LLMs for\nour tasks. Encouraged by promising initial results, we then scale up using the \"gpt-40-mini\" model\nvia the OpenAI API, which provides an optimal balance of performance and cost-effectiveness.\nThis model, released on July 18, 2024, features a 128K context window, 16,384 token output\ncapacity, and an October 2023 knowledge cutoff, making it well-suited to our research objectives.\nMoreover, this model is cost-effective in that it charges only USD 0.15 per million input tokens\nand USD 0.6 per million output tokens.\nOur methodology comprises several key steps, including downloading and parsing relevant\nfilings, developing regular expressions to extract specific sections, performing prompt engineering\nto ensure accurate and consistent data extraction from LLMs, and querying the API with carefully\ncrafted prompts and input text extracts. We employ an iterative process for prompt engineering,"}, {"title": "5. Experimental Results", "content": "The CEO pay ratio disclosure requirement mandates public companies to report the ratio of CEO\nto median employee compensation starting from fiscal years beginning on or after January 1, 2017,\nleading most companies to begin reporting the CEO pay ratio in 2018. Our sample is limited to\nCompustat Execucomp companies, as studies on pay ratio disclosures typically involve CEO\nattributes and other variables from this database. Our final sample of pay ratio disclosures consists\nof 9,865 proxy statements spanning the years 2018-2023. The sample selection process is\nsummarized in Panel B of Table 1.\nLarge accelerated filers started to include CAM disclosures in their auditor reports for fiscal\nyears ending on or after June 30, 2019. Other filers are required to do this for fiscal years ending\non or after December 15, 2020. Our final sample of CAM disclosures consists of 12,499 10-K\nforms spanning the years 2019-2023. See Panel B of Table 1 for a summary of the sample selection\nprocess."}, {"title": "5.2 Results for CEO Pay Ratio", "content": "In our Retrieval-Augmented Generation (RAG) framework, the first crucial step involves\nextracting relevant passages from source documents. These extracts are then provided to the\nchosen large language model (LLM) for data collection. To extract pay ratio disclosures from\nproxy statements, we employ a systematic approach to extract relevant content. For most filings,\nwe are able to programmatically identify pay ratio disclosure headings, allowing for a single,\ncomprehensive extract. In cases where such headings are not readily identifiable, we rely on\nreferences to median employee pay, sometimes resulting in multiple extracts per file to ensure the\ncapturing of the pay ratio data.\nThe variability in pay ratio disclosure practices across companies is evident from the\ndistribution of extract counts per file. While the majority of companies present this information in\na clear, identifiable section, as indicated by the predominance of single-extract files, a significant\nminority use a less standardized format, requiring a more comprehensive extraction approach. This\nheterogeneity in reporting styles presents challenges for manual extraction methods and other rule-\nbased automatic methods."}, {"title": "5.2.2 Input Tokens, and Processing Time and Cost", "content": "We process one extract per API request, as larger batch sizes risk cross-contamination of data\nacross extracts. The prompt shown in Figure A-6 of the online appendix consists of 1,114 tokens,\nand each extract contains 1,821 tokens on average. The total input tokens are 40.97M: 15.55M\nfrom prompts (1,114 tokens \u00d7 13,960 requests) and 25.42M from extracts (1,821 tokens \u00d7 13,960\nextracts).\nOur implementation processes these 13,960 extracts through individual API requests,\nincorporating automated error handling and retry mechanisms. The \"gpt-40-mini\" model\nsuccessfully processed all extracts in approximately nine minutes, incurring a total cost of $7 in\nAPI fees. For comparison, manual collection, estimated at three minutes per filing for a total of\n9,865 filings, would require approximately 493 hours. This translates to 62 working days,\nassuming an eight-hour working day, or three calendar months when holidays are considered. At\na rate of USD $10 per hour, manual collection would cost approximately $5,000. Our LLM-based\nmethod demonstrates a significant reduction in time and cost, transforming months of manual labor\ninto mere minutes of computational time at just 0.14% of the estimated manual labor cost.\nIt is worth noting that our approach scales efficiently to larger samples, costing\napproximately $0.50 per thousand extracts ($7 / 13,960 \u00d7 1,000). For each additional year, with\naround 1,500 filings, the cost increases by only about one dollar. Furthermore, this method can be\neasily adapted to extract additional information (e.g., explanations of how median employee pay\nis determined) from the same documents at minimal extra cost, simply by adjusting the prompt."}, {"title": "5.2.3 Accuracy", "content": "As shown in Panel A of Table 3, out of 9,865 proxy statements, the model successfully collected\nCEO compensation from 9,756 statements (98.90%), median employee pay data from 9,839\nstatements (99.74%), and pay ratio figures from 9,849 statements (99.84%). These remarkably\nhigh collection rates across all three metrics, with missing percentages ranging from just 0.16% to\n1.10%, underscore the model's reliability and robust performance in handling diverse data\npresentations within proxy statements. The narrow range of missing percentages, spanning less\nthan one percentage point, further highlights the consistency of the model's performance. It is\nworth mentioning that the missing elements do not necessarily mean that the model missed them.\nIn some cases, the extracts provided to the model do not contain the relevant information.\nWe rigorously evaluate our approach by focusing on the accuracy of the collected data,\nrather than other common metrics like recall, precision, or F1 score. This emphasis on accuracy is\nparticularly appropriate for our task design: instead of performing binary or multi-class\nclassification, we are collecting specific numerical values from text. Our methodology employs\nRetrieval-Augmented Generation (RAG) to identify and process only the most relevant text\nsegments containing pay ratio information, minimizing processing time and costs by reducing the\nuse of the LLM for irrelevant text.\nFurthermore, given our task setup-where we first identify relevant sections through\npreprocessing and then ask the LLM to collect specific numerical values from them-accuracy"}, {"title": "5.3 CAMS", "content": "Panel A of Table 5 presents a summary of the initial Critical Audit Matters (CAM) extraction\nresults. The results show that the regular expression (regex) approach is able to identify the\nbeginning and end of audit reports in the vast majority of cases (96.84%). In these instances, the\nCAMs are extracted from within the audit report, specifically from the CAM heading to the end\nof the audit report. This approach is effective because CAMs are typically presented last in an\naudit report.\nIn some rare cases (3.16%), only the heading of the CAM section is identified. To ensure\nthat the full length of the CAMs is captured, we take a conservative approach by extracting 15,000\ncharacters from the heading onwards. This guarantees that all relevant information is included,\neven in the absence of a clearly identified end to the auditor report.\nOverall, an average CAM section is 716 tokens long when successfully extracted from the\naudit report. If the end of the audit report is not identified, we extract on average 2,134 tokens from\n15,000 characters."}, {"title": "5.3.2 Input Tokens, and Processing Time and Cost", "content": "Panel B of Table 5 provides a breakdown of the input tokens supplied to the LLM for collecting\nand classifying CAMs. The final prompt, which is provided in Figure A-7 in the online appendix,\nconsists of 836 tokens. A total of 12,499 CAM extracts were processed in batches of two extracts\nper request, resulting in 6,250 API requests. The total input tokens, comprising both the prompt\ntokens (10.45 million) and the extract tokens (9.51 million), sum up to 19.96 million tokens. The\nprocessing time, which includes error handling, is approximately 40 minutes. The total API cost\namounts to approximately $8.\nIt is noteworthy that even though the total number of input tokens and number of requests\nare smaller compared to the pay ratio disclosures, the processing time for CAM collection is\nhigher. This is because CAM collection requires re-generating the CAM, and an LLM typically\nprocesses input more quickly than generating text. Furthermore, the cost is also higher due to the\nfact that output tokens are significantly more expensive than input tokens (four times as high for\nour chosen model).\nIt is worth mentioning that CAM data is available through Audit Analytics at WRDS.\nHowever, the annual subscription fee can cost thousands of dollars, and to maintain access to the\nmost up-to-date data, the subscription needs to be renewed regularly. This can be prohibitively\nexpensive over the long run, making it difficult for researchers at financially constrained\ninstitutions to access this valuable resource. In contrast, our approach offers a highly cost-effective\nand time-efficient alternative. By leveraging an LLM, we are able to collect data from more than\n12,000 annual reports, at a total cost of less than eight dollars. This exceptional efficiency\ndemonstrates the potential of our method to democratize access to data for researchers who may\nnot have the financial means to afford expensive subscriptions."}, {"title": "5.3.3 Accuracy", "content": "We evaluate the accuracy of the GPT-collected and classified CAM data against a manually\nverified sample. First, our research assistant (RA), who is a master's student in a business program,\nmanually collected CAM disclosures from a random sample of 500 10-K filings. We then create\na verified sample by comparing the GPT-collected data against the RA's manual collection. For\ncases where discrepancies exist between the GPT-collected and RA-collected data, the authors\npersonally verify these instances to establish the ground truth. This two-stage verification process\nensures a high-quality benchmark by identifying and correcting any potential errors in the initial\nmanual collection. This approach allows us to not only evaluate the accuracy of our GPT-based\nmethodology but also compare it to traditional manual data collection processes.\nWe employ cosine similarity to compare the collected text against benchmarks. For ease\nof grouping, the similarity scores have been rounded to the nearest 0.01, allowing for clearer"}, {"title": "5.3.4 Descriptive Statistics of the Full Sample", "content": "We scale up our procedure and process the full sample, with the intention of sharing these data\nwith researchers. In Table 7, we present summary statistics comparing the evaluation sample and\nthe full sample across various dimensions, including the number of CAMs per 10-K document and\nthe average length of each CAM component.\nPanel A of Table 7 reveals striking similarities between the evaluation sample and the full\nsample. The evaluation sample exhibits an average of 1.42 CAMs per filing, while the full sample\nshows 1.40 CAMs per filing. Further analysis of CAM components, specifically the average word\ncounts for titles, descriptions, and procedures, also shows strong consistencies, with minimal\ndifferences between the samples. The close correspondence between the evaluation sample\n(n=500) and the full sample (n=12,475) in terms of both CAM frequency and component word\ncounts strongly suggests that the accuracy results obtained from the evaluation sample can"}]}