{"title": "PDL: A Declarative Prompt Programming Language", "authors": ["MANDANA VAZIRI", "LOUIS MANDEL", "CLAUDIO SPIESS", "MARTIN HIRZEL"], "abstract": "Large language models (LLMs) have taken the world by storm by making many previously difficult uses of AI\nfeasible. LLMs are controlled via highly expressive textual prompts and return textual answers. Unfortunately,\nthis unstructured text as input and output makes LLM-based applications brittle. This motivates the rise of\nprompting frameworks, which mediate between LLMs and the external world. However, existing prompting\nframeworks either have a high learning curve or take away control over the exact prompts from the developer.\nTo overcome this dilemma, this paper introduces the Prompt Declaration Language (PDL). PDL is a simple\ndata-oriented language that puts prompts at the forefront, based on YAML. PDL works well with\nmany LLM platforms and LLMs. It supports writing interactive applications that call LLMs and tools, and\nmakes it easy to implement common use-cases such as chatbots, RAG, or agents. We hope PDL will make\nprompt programming simpler, less brittle, and more enjoyable.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made great advances, demonstrating the ability to perform\na wide range of useful tasks. As LLMs are controlled via natural-language prompts, prompt en-\ngineering has emerged as an ad-hoc approach to improve accuracy [31]. Even more capabilities\ncan be unlocked with prompting patterns such as in-context learning [6], chaining multiple LLM\ncalls [7], retrieval-augmented generation (RAG) [18], tool use [28], program-aided language mod-\nels (PAL) [10], and agents [34]. Unfortunately, while powerful, LLMs remain brittle: they sometimes\nhallucinate, or even fail to comply with expected syntax and types.\nPrompting frameworks [20] make it easier for developers to use LLMs and associated prompting\npatterns while ameliorating their brittleness. Some, such as LangChain [7] and AutoGen [32], do so\nvia bespoke features for popular patterns such as RAG or agents. Unfortunately, this bespokeness\ntakes control over basic prompts away from users and forces them to learn many complex framework\nfeatures. In contrast, low-level prompting frameworks, such as Guidance [23] and LMQL [5], provide\nmore control with syntax and types. Unfortunately, they require users to program in imperative\nlanguages such as Python or TypeScript. At the other end of the spectrum, frameworks such as\nDSPy [15] and Vieira [19] avoid hand-written prompts altogether by automatically generating\nthem. Unfortunately, this takes away even more control from the developer. The problem thus\nbecomes how to make LLM programming less brittle while keeping it simple and keeping the\ndeveloper in the driver's seat.\nTo tackle this problem, we turned to tried-and-true programming language design ideas. The\nprinciple of orthogonality advocates for a small set of simple features that compose in powerful\nways [30]. Being orthogonal, or at right angles, here means being irredundant and avoiding\nexceptional cases as far as possible. For prompting frameworks, orthogonality is a way to avoid\nbespoke features. Next, developers need to struggle less with brittleness if the language checks\ntypes and roles [12] to enforce structure by construction. One remaining tension is harder to tackle:\non the one hand, we want to give developers control over the exact prompts, but on the other hand,\nwe want a simple declarative language. To this end, we settled on a data-oriented language, which\nputs prompts at the forefront by intentionally blurring the line between programs (e.g. for chaining\nand tools) and data (for prompts). This is inspired by the old idea of code as data [21], as well as by\nseminal work on programming without tiers [8]."}, {"title": "2 Overview", "content": "This section gives an overview of PDL features by means of a chatbot example. A PDL program\nexecutes a sequence of blocks, each of which generates data that it contributes to the background\ncontext. There are different kinds of blocks, capable of generating data in different ways: model\ncalls, reading data from stdin or a file, directly creating various kinds of JSON data, and executing\ncode. In addition, there are a variety of control blocks (if-then-else, for, and repeat) that let PDL\nusers express rich data pipelines and AI applications.\nFig. 1(a) shows the PDL code for a simple chatbot. The read: block on Lines 1\u20134 prints a message\nasking the user to enter a query, which it reads from stdin. Fig. 1(b) shows an execution trace of the\nsame program. For instance, the user might ask \u2018What's a language salad?'. To avoid duplication,\nthe 'contribute: [context]' clause puts the user response into the background context but not the\nresult (printed on stdout).\nThe repeat:until: block on Lines 5\u201316 has one nested text: block, which in turn has a sequence\nof two nested blocks. The text: block turns the results of its nested blocks into strings and con-\ncatenates them. The model: block on Lines 7\u20139 calls an LLM, using the accumulated context so\nfar as the prompt. In the first loop iteration, that context comprises only two lines 'What is your\nquery?' and 'What's a language salad?'. The \u2018stop:[\n\n]' model parameter causes the LLM to stop\nproducing tokens after generating two consecutive newline characters. The LLM interpreter prints\nLLM outputs in green; Fig. 1(b) shows that in this example, the LLM produced 'A language salad\nis [...]'. The read: block on Lines 10\u201315 prints a message using YAML's multi-line string syntax\nstarting with a vertical bar (|). This example illustrates how PDL keeps prompts at the forefront"}, {"title": "3 Language", "content": "PDL is a language embedded into YAML such that every PDL program is a valid YAML document\nfollowing the PDL schema\u00b3. Fig. 2 is a quick reference of PDL, and this section explains it using\ngrammar rules. A program is a block or a list of blocks where blocks are expressions or structured\nblocks, as expressed by the following grammar rules:\npdl ::= block | [block,..., block]\nblock ::= expression | structured_block\nAll grammar rules in this section use YAML's flow-style syntax (e.g., [block, . . ., block]). The same\nPDL code can also be rendered in YAML's block-style syntax, e.g.:\nblock\nblock\nEach block has a block body, with keywords indicating the block kind (e.g., model or read).\nThere are 15 kinds of block bodies (optional fields are annotated with a question mark):"}, {"title": "4 Tooling", "content": "PDL comes with tools for making PDL programs easy to write, run, and understand.\nFirst and foremost, the PDL interpreter is an execution engine with a command-line interface, as\none would expect from a scripting language. The interpreter supports a streaming mode, where\nLLM outputs become visible incrementally as they are being produced, for a more interactive chat\nexperience. The interpreter also supports sandboxing, which causes it to launch in a container,\nrecommended when executing LLM-generated actions or code.\nThe PDL IDE support enhances VSCode, making it easier to write PDL code via syntax highlight-\ning, auto-complete, tooltips for PDL keywords, and error checking. These capabilities are, in part,\ndriven by the PDL meta-schema i.e., the JSON schema that defines what constitutes valid PDL.\nThe %%pd1 cell magic enhances Jupyter Notebooks so developers can write code cells directly\nin PDL. That way, hosted notebook platforms can serve as a simple playground for interactively\nexploring prompts. Given multiple PDL code cells in the same notebook, later cells can use variables\ndefined in earlier cells. Furthermore, the background context for later cells is continued from earlier\ncells; when not desired, developers can override this behavior via %%pdl --reset-context.\nThe PDL live document visualizer shows the concrete execution trace of a PDL program with\ncolored nested boxes, similar to typical figures in papers or blog posts about LLM prompting.\nThen, the user can select one of the boxes to display the corresponding PDL code, similar to how\nspreadsheet cells show data, but the user can select them to inspect the formula that created that\ndata. This live view is a way to let users quickly understand concrete data, and then move from\nthat to understanding the code that produced it.\nFinally, PDL has an SDK (software development kit), which is a small Python library for calling\nfrom Python into PDL. This is useful for extending larger Python applications to use prompt-based\nprograms, such as agents. As discussed in Section 3, a PDL file can contain Python in code: blocks.\nWhen developing larger applications with PDL, we have found it useful to keep these to a few\nlines of code, by defining a function in a separate Python file and then calling it from PDL. A good\npractice is to pass data from PDL to Python and vice versa as JSON objects. Optionally, this can be\ntype-checked using the spec: keyword in PDL, and TypedDict or Pydantic on the Python side, as\nillustrated in the next section in Fig. 3."}, {"title": "5 Case Studies", "content": "We already saw a simple PDL chatbot example in Section 2. This section illustrates slightly more\nadvanced use-cases for PDL: RAG, agents, and generating PDL from PDL."}, {"title": "5.1 Retrieval-Augmented Generation", "content": "Retrieval-augmented generation, or RAG, works by first retrieving relevant context, then adding\nthat to the prompt for a model to generate an answer [18]. Fig. 3(a) shows a PDL program that uses\nRAG to retrieve few-shot samples for a code-generation task. The code: block in Lines 2\u20136 uses\nPython to initialize a vector database for the training split of the MBPP dataset of \u201cmostly basic\nPython programs\u201d [2]. It uses a Python function defined in Fig. 3(b), together with a PDL_SESSION\nspecial variable that enables it to carry state to a later code block. Lines 8-11 of Fig. 3(a) initialize\nvariable test_query with a natural-language request for Python code to be generated. Lines 12-19\ninitialize variable retrieved with the five most similar samples from the training data."}, {"title": "5.2 ReAct Agent", "content": "An LLM-based agent lets an LLM select and configure actions, executes those actions in an environ-\nment, and feeds the outputs from the actions back to the LLM as observations. There are different\npatterns for such agents, such as ReAct [34] and ReWOO [33]. An action is an LLM-based tool\ncall [28], and an agent chains together multiple tool calls in a dynamic LLM-directed sequence. The\nambition is to make AI-based applications less prescriptive and more goal-driven. Moreover, when\nsomething goes wrong with an action, agents can use the observation as feedback to recover.\nFig. 4 shows a PDL example of a simple ReAct agent. The core of ReAct is a think-act-observe\nloop, which manifests in the code as variable definitions for thought (Line 8), action (Line 13), and\nobservation (Line 19). The thought is model-generated natural language e.g., 'I need to search the\ndiscoverer of the Hudson River, find when he was born' in the interpreter trace in Fig. 4(b). The"}, {"title": "5.3 Generating PDL from PDL with LLMs", "content": "The previous sections showed examples of how a human developer can use PDL to encode\ndifferent prompting patterns. This section turns to LLMs and shows how they can also be used to\ngenerate PDL. This meta PDL generation is helpful when LLMs need to create a plan for solving a\nproblem, for example as part of an agentic workflow. Traditionally, such plans are just text or JSON\nor Python code. With PDL, these plans can be a composition of model and code calls that are fully\nexecutable. This section explores using PDL meta generation for the GSMHard dataset5.\nGSMHard is a harder version of GSM8k, which consists of grade-school math problems that\nrequire simple arithmetic or symbolic reasoning. GSMHard contains an input which is a math\nproblem statement, together with an output which is Python code that solves the problem. We\nimplemented the PAL [10] approach but instead of generating Python code, we ask an LLM to\ngenerate PDL. The textual chain-of-thought is represented as PDL text blocks, and arithmetic is\ndone using PDL code blocks.\nFig. 5 shows a PDL program that generates PDL code and executes it all in the same program.\nThe demos variable holds few-shot samples designed to teach a model how to generate PDL code.\nOn Line 32, a model call block uses these samples, together with a question, which is a free variable,\nas input. The result is a PDL program to solve the question. Line 38 extracts the PDL program and\nexecutes it in Python. This program is applied to the GSMHard dataset, where question is filled\nwith input questions.\nThis experiment resulted in the discovery that 10% of the GSMHard dataset is actually incorrect\nin the sense that the ground truth is inconsistent with the question that was asked. Fig. 6 shows an\nexample of such an inconsistency. Using PDL helped in this discovery because the generated PDL\ncode is human-readable, so we were able to easily check data points that did not match the ground\ntruth and found that the ground truth is incorrect in some cases. We used an LLM to cover the entire\ndataset and systematically pick examples that seemed inconsistent. We then manually filtered the\nresult to remove false positives and identified 10% of data points that present this problem."}, {"title": "6 Related Work", "content": "A recent survey defines a prompting framework as a layer that manages, simplifies and facilitates\nthe interaction between LLMs and users, tools, or other models [20]. The survey highlights that a\nmajor pitfall of prompting frameworks is increasingly steep learning curves."}, {"title": "7 Conclusion", "content": "PDL is a declarative data-oriented language: a program consists of YAML blocks, where each block\neither is a literal piece of data or produces data. The mental model is that executing a block appends\nits data to the background context, and subsequent LLM calls use that context as their prompt.\nThis paper introduces the language via example programs and a tour of the grammar and tooling.\nThe declarative nature of the language also makes it amenable to automatic optimizations for\nspeed, accuracy, and security, which will be forthcoming in future work. PDL is ready to use and\nopen-source at https://github.com/IBM/prompt-declaration-language."}]}