{"title": "KNOWFORMER: Revisiting Transformers for Knowledge Graph Reasoning", "authors": ["Junnan Liu", "Qianren Mao", "Weifeng Jiang", "Jianxin Li"], "abstract": "Knowledge graph reasoning plays a vital role in various applications and has garnered considerable attention. Recently, path-based methods have achieved impressive performance. However, they may face limitations stemming from constraints in message-passing neural networks, such as missing paths and information over-squashing. In this paper, we revisit the application of transformers for knowledge graph reasoning to address the constraints faced by path-based methods and propose a novel method KNOWFORMER. KNOWFORMER utilizes a transformer architecture to perform reasoning on knowledge graphs from the message-passing perspective, rather than reasoning by textual information like previous pretrained language model based methods. Specifically, we define the attention computation based on the query prototype of knowledge graph reasoning, facilitating convenient construction and efficient optimization. To incorporate structural information into the self-attention mechanism, we introduce structure-aware modules to calculate query, key, and value respectively. Additionally, we present an efficient attention computation method for better scalability. Experimental results demonstrate the superior performance of KNOWFORMER compared to prominent baseline methods on both transductive and inductive benchmarks.", "sections": [{"title": "1. Introduction", "content": "Knowledge graphs (KGs) are structured knowledge bases that store known facts in the form of triplets (Hogan et al., 2022; Ji et al., 2022). Each triplet consists of a head entity, a relation, and a tail entity. However, real-world KGs often suffer from high incompleteness, making it challenging to retrieve the desired facts (Wang et al., 2017; Ji et al., 2022). Knowledge graph reasoning, which involves inferring new facts based on existing ones, is a fundamental and indispensable task in KGs with a wide range of applications (Qiu et al., 2019; Cao et al., 2019; Huang et al., 2019; Abujabal et al., 2018; Cheng et al., 2020; Zeng et al., 2022).\nA substantial line of previous research has been dedicated to developing effective and robust methods for knowledge graph reasoning. Knowledge graph embedding (KGE) methods focus on embedding entities and relations into a low-dimensional space (Bordes et al., 2013; Sun et al., 2019; Yang et al., 2015; Trouillon et al., 2017; Li et al., 2022). Despite their impressive performance, these embedding-based methods struggle to generalize to inductive scenarios since they cannot leverage the local structures of knowledge graphs (Teru et al., 2020). As a result, researchers have turned to path-based methods to enhance reasoning performance and improve generalization. These methods learn pairwise representations from subgraphs (Teru et al., 2020; Mai et al., 2021; Wang et al., 2022a; Chamberlain et al., 2023) or relational paths (Zhu et al., 2021; Zhang & Yao, 2022; Zhang et al., 2023; Zhu et al., 2023) between entities which can be used to predict unseen facts. However, path-based methods can be limited by the missing paths (Franceschi et al., 2019) and over-squashing information (Alon & Yahav, 2021) as shown in Figure 1.\nRecent works have leveraged the transformers for knowledge graph reasoning, drawing inspiration from the success of transformer-based models in various domains (Vaswani et al., 2017; Devlin et al., 2019; Dosovitskiy et al., 2021; Rives et al., 2021). Typically, these studies encode components of knowledge graphs using their text descriptions through pretrained language models (PLMs) and utilize either a discriminative paradigm (Lv et al., 2022; Wang et al., 2022b) or a generative paradigm (Xie et al., 2022; Saxena et al., 2022) to predict new facts. However, there are two main issues: (1) textual descriptions often contain ambiguity and require domain-specific knowledge for accurate encoding, e.g., pretrained language models like BERT (Devlin et al., 2019), which are trained with commonsense knowledge, may not transfer well to domain-specific KGs; and (2) capturing structural information, which can be crucial for knowledge graph reasoning owing to the low-rank assumption (Koren et al., 2009; Lacroix et al., 2018) is challenging, and some approaches address this by utilizing path and context encoding or pretraining on large-scale KGs as alternatives (Chen et al., 2021; Li et al., 2023).\nIn this paper, we aim to further investigate the potential benefits of applying the transformer architecture (Vaswani et al., 2017) to the task of knowledge graph reasoning. Contrary to previous approaches that utilize pretrained language models for encoding text descriptions, we leverage the attention mechanism within the transformer architecture to capture interactions between any pair of entities. Our principal contribution is the introduction of an expressive and scalable attention mechanism tailored specifically to knowledge graph reasoning. The resulting model, named KNOWFORMER, provides informative representations for knowledge graph reasoning in both transductive and inductive scenarios. It can address the limitations of vanilla path-based methods, such as the path missing and information over-squashing. Specifically:\n\u2022 We redefine the self-attention mechanism, originally introduced by Vaswani et al. (2017), as a weighted aggregation of pairwise information for specific queries based on the plausibility of entity pairs as query prototypes. This redefinition enables us to perform attention computation on multi-relational KGs and reduce the modeling complexity.\n\u2022 For constructing the attention mechanism, we introduce two modules that generate informative representations for query, key, and value, respectively. These modules are designed based on relational message passing neural networks, allowing us to consider structural information during attention calculation.\n\u2022 To improve scalability, we adopt an instance-based similarity measure (Cui & Chen, 2022) to reduce the significant number of degrees of freedom and introduce an approximation method that maintains complexity linearly proportional to entities. Moreover, we provide theoretical guarantees for its stability and expressivity.\n\u2022 We also provide empirical evidence of the effectiveness of our proposed KNOWFORMER on knowledge graph"}, {"title": "2. Related Work", "content": "We classify the related work on knowledge graph reasoning into three main paradigms: embedding-based methods, path-based methods, and transformer models.\nEmbedding-Based Methods. Embedding-based methods aim to learn distributed representations for entities and relations by preserving the triplets in the knowledge graph. Notable early methods include TransE (Bordes et al., 2013), DistMult (Yang et al., 2015) and ComplEX (Trouillon et al., 2017). Subsequent work has focused on improving the score function or embedding space of these methods to enhance the modeling of semantic patterns (Sun et al., 2019; Tang et al., 2020; Zhang et al., 2020; Li et al., 2022). In addition, some researchers have explored the application of neural networks in an encoder-decoder paradigm to obtain adaptive and robust embeddings. Representative methods include convolutional neural networks (Dettmers et al., 2018; Nguyen et al., 2018) and graph neural networks (Schlichtkrull et al., 2018; Vashishth et al., 2020; You et al., 2021). Although embedding methods have demonstrated promising performance in knowledge graph reasoning and scalability on large KGs, these embeddings are hard to generalize to unseen entities and relations, thus limiting their applicability in the inductive setting.\nPath-Based Methods. Path-based methods in knowledge graph reasoning have their origins in traditional heuristic similarity approaches, which include measuring the weighted count of paths (Katz, 1953), random walk probability (Page, 1998), and shortest path length (Liben-Nowell & Kleinberg, 2007). In recent years, there have been proposals to employ neural networks to encode paths between entities, such as recurrent neural networks (Neelakantan et al., 2015), and aggregate these representations for reasoning. Another research direction focuses on learning probabilistic logical rules over KGs (Yang et al., 2017; Sadeghian et al., 2019)"}, {"title": "Transformers for Knowledge Graph Reasoning", "content": "Currently, most research utilizes pretrained models based on transformer architecture to encode textual descriptions and leverage its knowledge and the semantic understanding ability for reasoning. For instance, KG-BERT (Yao et al., 2019) and PKGC (Lv et al., 2022) employ a pretrained language model encoder to represent KG triples with text descriptions, enabling the inference of new facts through a classification layer using a special token representation. In contrast, some studies explore a sequence-to-sequence approach for generating reasoning results (Xie et al., 2022; Saxena et al., 2022). This paradigm has received increasing attention with the emergence of large language models. To bridge the gap between unstructured textual descriptions and structured KGs, researchers have explored incorporating contextual information by sampling and encoding neighborhood triplets or paths (Xie et al., 2022; Chen et al., 2021; Li et al., 2023). However, these methods face limitations due to their heavy reliance on text description, including the lack of textual data, domain knowledge requirements, and the neglect of structural information in KGs. In this paper, we revisit how to perform knowledge graph reasoning leveraging the transformer architecture from the perspective of knowledge graph structure."}, {"title": "3. Preliminary", "content": "In this section, we introduce the background knowledge of knowledge graphs and knowledge graph reasoning. Due to the page limitations, more preliminaries about transformers can be found in the appendix.\nKnowledge Graph. Typically, a knowledge graph G = {V,E,R} is a collection of triplets \\u03b5 = {(hi, ri,ti) |hi,ti \u2208 V,ri \u2208 R} consist a set of entities V and a set of relations R. Each triplet is a relational edge from head entity h\u2081 to tail entity ti with the relation ri. For ease of notation, we can also represent a fact as r(u, v) \u2208 E where u, v \u2208 V and r \u2208 R. Additionally, we define the neighborhood set of an entity u \u2208 V relative to a relation r \u2208 R as Nr(u) = {v | r(u, v) \u2208 E}.\nKnowledge Graph Reasoning. Given a knowledge graph G = {V, E, R}, the goal of knowledge graph reasoning is to leverage existing facts to infer the missing elements of the query fact (h, rq, t), where rq is a query relation. Based on the type of missing elements, there are three sub-tasks: head reasoning to infer (?, rq, t), tail reasoning to infer (h, rq, ?), and relation reasoning to infer (h, ?, t). This paper mainly focuses on the tail reasoning task, as the other tasks can be transformed into the same form."}, {"title": "4. Proposed Method", "content": "In this section, we introduce the construction of the KNOW-FORMER. Specifically, we focus on how to create the proposed attention mechanism and integrate it into a transformer model.\n4.1. Attention Computation of KNOWFORMER\nWe adopt a modified definition of self-attention, similar to the formulation used in Tsai et al. (2019) and Chen et al. (2022). Assuming that X \u2208 Rn\u00d7d is input features, we have:\n$$Attn(x) = \\sum_{v \\in V} \\sum_{\\omega \\in V} \\frac{\\kappa(f_q(x_u), f_q(x_v))}{\\sum_{\\nu \\in V} \\kappa(f_q(x_u), f_q(x_\\omega))} \\cdot f_v(x_v),$$\nhere, fq(\u00b7) represents the query function, fr(\u00b7) represents the value function, and \u03ba(\u00b7, \u00b7) : Rd \u00d7 Rd \u2192 R+ is a positive-definite kernel that measures the pairwise similarity.\nIn a knowledge graph, different relation types r \u2208 R determine various modes of interaction between entity pairs. Consequently, a straightforward approach is to incorporate relation-specific attention to model these diverse interactions:\n$$Attn(x) = \\sum_{r \\in R} \\sum_{u \\in V} \\sum_{v \\in V} \\frac{\\kappa(f_r(x_u), f_r(x_v))}{\\sum_{\\nu \\in V} \\kappa(f_r(x_u), f_r(x_{\\nu}))} \\cdot f_v(x_v),$$\nwhere fr (.) means a relation-specific function. However, this approach will result in significant computational overhead (O(|R|\u00b7 |V|2)) and suboptimal optimization due to a high degree of freedom. Inspired by instance-based learning (Stanfill & Waltz, 1986; Cui & Chen, 2022), we consider modeling the pairwise interactions based on the plausibility of entity pairs as query prototypes.\nDefinition 4.1 (Query Prototype). Given a relation r \u2208 R, entity u \u2208 V and v \u2208 V are the prototypes for relation r if (u, r, ?) \u2227 (v, r, ?) \u2260 0.\nFor instance, Barack Obama and Joseph Biden are the prototypes for relation Nationality, since"}, {"title": "Query Function", "content": "Query function fq(\u00b7) is designed to provide informative representations, denoted as Zu, for each entity u to distinguish query prototypes effectively. A key insight is that considering the context of each entity on the knowledge graph (i.e., its k-hop neighbor facts) is essential in determining query prototypes. For example, entities such as Barack Obama and Joseph Biden can serve as prototypes for the relation Nationality due to their common neighbor relation President of. We introduce Q-RMPNN, a relational message-passing neural network (Gilmer et al., 2017; Xu et al., 2019; Wang et al., 2021) designed to incorporate neighbor facts into the entity representations. Q-RMPNN consists of two stages: (1) each entity sends relational messages to its neighbors; (2) each entity aggregates the received relational messages and updates its representation. Drawing inspiration from knowledge graph embeddings, we generate a relational message mvlu,r for each fact r(u, v) by maximizing its continuous truth value (Wang et al., 2023):\n$$m_{v|u,r} = arg \\max_{w \\in D} \\phi(z_u, r, w) = g(z_u, r),$$\nwhere D indicates the range domain for the entity embeddings and zu is the representation of entity u within fq(\u00b7), \u2191 is the representation of relation r conditioned on query relation rq, $(\u00b7,\u00b7, \u00b7) is a score function and g(\u00b7,\u00b7) is an estimation function. In this paper, we adopt the DistMult method (Yang et al., 2015) as the foundation for our implementation similar to Zhu et al. (2021), to say:\n$$r = R[r_q] W_r+b_r, g(z_u, r) = z_u^T r,$$\n$$\\phi(z_u, r, w) = (g(z_u, r), w),$$\nwhere R is the input features of relation set R, and Wr, br is the relation-specific parameters. At the l-th layer of fq(\u00b7), we fuse zu for each u \u2208 V by the summation of aggregated information from the (l \u2013 1)-th layer:\n$$z_u^{(0)} \\leftarrow [x_u, \\epsilon],$$\n$$z_u^{(l)} \\leftarrow \\Phi^{(l)} (\\alpha^{(l)} \\cdot z_u^{(l-1)} + \\sum_{r(v,u) \\in E} m_{ulv,r}^{(l-1)} ),$$\nwhere xu \u2208 Rd represents the input features, \u0454 ~ N(0, I) denotes Gaussian noise to distinguish different source entities, [\u00b7, \u00b7] indicates concatenate function, &(l) captures layer-specific parameters to retain the original information, and u(l)(\u00b7) represents a layer-specific update function parameterized by an MLP network. After L layers of updates, we obtain the final representation Zu given by Zu = z(L). Through Q-RMPNN, Zu is optimized to capture L-hop neighbor facts, resulting in a structure-aware and knowledge-oriented representation."}, {"title": "Value Function", "content": "We leverage the value function to generate pairwise representations \u00c2u conditioned on the query relation rq for the query (h, rq, ?). The structural information between node pairs is crucial in knowledge graph reasoning, which can be viewed as a link prediction task (Zhang et al., 2021). However, it is challenging for the attention mechanism to capture the structural information of the input graph explicitly. To address this, we design V-RMPNN to encode pairwise structural information into the value. Specifically, V-RMPNN is implemented as follows:\n$$z_{u|h,r_q}^{(0)} \\leftarrow [x_u, \\mathbb{I}_{u=h} \\cdot \\mathbb{1}],$$\n$$z_{u|h,r_q}^{(l)} \\leftarrow \\Psi^{(l)} (\\beta^{(l)} \\cdot z_{u|h,r_q}^{(l-1)} + \\sum_{r(v,u) \\in E} m_{ulv,r}^{(l-1)} ),$$\nwhere xu, muv,r, (-1), (l), [\u00b7, \u00b7] and \u58d1(l) () are defined similarly to Q-RMPNN. The term Iu=h 1 represents head entity labeling features to enhance node representation, which is essential for the link prediction task (Zhang et al., 2021; You et al., 2021). After L layers of updates, the final representation \u0109u is obtained as Zu = zuh,rq\nKernel Function. The kernel function \u03ba(Zu, Zv) is employed to quantify pairwise similarity. One common choice is the exponential kernel specified as follows:\n$$\\mathcal{K}_{exp}(z_u, z_v) = exp (\\frac{(z_u W_1, z_v W_2)}{\\sqrt{d}}),$$\nwhere W\u2081 and W2 are the linear projection matrixes and the bias is omitted for convenience. However, the dot-then-exponentiate operation will lead to quadratic complexity"}, {"title": "4.2. Overall Architecture of KNOWFORMER", "content": "Based on the attention definition provided in Section 4.1, we complete the construction of our proposed model KNOW-FORMER. The remaining components of KNOWFORMER adhere to the standard transformer architecture described in Vaswani et al. (2017). Each layer of KNOWFORMER comprises an attention function and a feedforward network. In addition, a skip-connection is applied after the attention function, and normalization layers are inserted before and after the feedforward network. Finally, we stack L layers to construct KNOWFORMER.\nAt the start of training, we initialize the entity feature matrix X and relation feature matrix R. Specifically, X is set to an all-zero vector, while R is randomly initialized and is learnable. We then iteratively calculate the entity representations as follows:\n$$A^{(l)} = LayerNorm(X^{(l-1)} + Attn^{(l)} (X^{(l-1)}, R)),$$\n$$X^{(l)} = LayerNorm(A^{(l)} + FFN^{(l)}(A^{(l)})).$$ \nLastly, we obtain the final representation of entities X (L). We update model parameters by optimizing a negative sampling loss (Mikolov et al., 2013; Sun et al., 2019) using Adam optimizer (Kingma & Ba, 2015). The loss function is defined for each training fact (h, r, t) as:\n$$L = -log(\\sigma(t|h, r)) - \\sum_{t'} log (1 - \\sigma(t'|h, r)),$$"}, {"title": "4.3. Discussion", "content": "In this section, we provide a thorough discussion of the KNOWFORMER, focusing on the analysis of its time complexity and expressivity.\nTime Complexity. Time complexity of our proposed model primarily depends on the attention function. We can break down the time complexity into the query function, the value function, and the kernel function. The query function is called L times, with each call taking O(L(|E|\\u00b7d+ |V|\\u00b7d\u00b2)). Similarly, the value function is called L times, with a single call taking O(L(|E|\\u00b7d+ |V|\\u00b7d\u00b2)). Finally, the kernel function is called L times, with each call taking O(|V|d\u00b2). Therefore, the total complexity amounts to O(L(L+\u00ce)(|E|\\u00b7d+|V|\\u00b7d\u00b2)). The above conclusion shows that the time complexity of our proposed method is linearly related to the number of facts and entities in the knowledge graph, exhibiting better scalability.\nExpressivity Analysis. Huang et al. (2023) introduced a variant of the Weisfeiler-Leman Test (Weisfeiler & Leman, 1968; Xu et al., 2019; Barcel\u00f3 et al., 2022) called the Relational Asymmetric Local 2-WL Test (RA-WL2) to evaluate the expressive power of message-passing networks in the knowledge graph reasoning task. We formally demonstrate the expressivity of KNOWFORMER based on RA-WL2. This is stated in the following theorem:\nTheorem 4.3 (Expressivity of KNOWFORMER). Assuming the estimated graph by the attention layer of KNOWFORMER is E, the attention layer of KNOWFORMER can achieve at least the same level of expressive ability as RA-WL2 on the extended graph E \u222a E.\nWe provide proof in Appendix D. The estimated graph E can be viewed as a collection of facts of special relations determining the equivalence between entity pairs for a specific query. KNOWFORMER exhibits stronger expressive power due to its ability to propagate information on an extended graph, distinguishing it from vanilla path-based methods."}, {"title": "5. Experimental Results", "content": "In this section, we conduct empirical studies motivated by the following aspects:\n\u2022 Transductive Performance. As a general knowledge graph reasoning task, we aim to demonstrate the performance of KNOWFORMER in transductive knowledge graph reasoning tasks.\n\u2022 Inductive Performance. Our proposed model supports reasoning in inductive settings. How does KNOWFORMER perform in inductive knowledge graph reasoning tasks?\n\u2022 Ablation Study. How important are the various components within our proposed framework? For instance, how does model performance change if we omit the query function?\n\u2022 Further Experiments. We conduct additional experiments to further showcase the effectiveness of KNOW-FORMER. For example, does KNOWFORMER perform better for longer paths?\nOur code is available at https://github.com/jnanliu/KnowFormer.\n5.1. Transductive Performance\nDatasets. We conduct experiments on four widely utilized transductive knowledge graph reasoning datasets: FB15k-237 (Toutanova & Chen, 2015), WN18RR (Dettmers et al., 2018), NELL-995 (Xiong et al., 2017) and YAGO3-10 (Mahdisoltani et al., 2015). For consistency, we utilize the same data splits as in prior studies (Zhu et al., 2021; Zhang et al., 2023).\nBaselines. We compare KNOWFORMER to several prominent baselines, categorized into three classes:\n\u2022 Embedding-based methods, including TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), RotatE (Sun et al., 2019), and HousE (Li et al., 2022). These methods learn embeddings for entities and relations and perform reasoning based on distance or similarity.\n\u2022 Path-based methods, including DRUM (Sadeghian et al., 2019), CompGCN (Vashishth et al., 2020), RNNLogic (Qu et al., 2021), NBFNet (Zhu et al., 2021), RED-GNN (Zhang & Yao, 2022), A*Net (Zhu et al., 2023), AdaProp (Zhang et al., 2023), and ULTRA (Galkin et al., 2024). These methods conduct reasoning by utilizing the path information connecting entities.\n\u2022 Text-based methods, including HittER (Chen et al., 2021), SimKGC (Wang et al., 2022b), and KGT5 (Saxena et al., 2022). These methods utilize textual information for reasoning in knowledge graphs.\nResults and Analysis. We evaluate the performance of reasoning using standard metrics (Wang et al., 2017; Ji et al., 2022), namely MRR (\u2191), Hit@1 (\u2191), and Hit@10 (\u2191). The results on various datasets are presented in Table 1. Across all metrics, KNOWFORMER demonstrates a substantial performance advantage over the best baseline method in FB15k-237, NELL-995, and YAGO3-10. Particularly in the large-scale YAGO3-10 dataset, KNOWFORMER exhibits a significant performance advantage over the best baseline methods. This highlights the high effectiveness of KNOWFORMER in transductive knowledge graph reasoning. In the WN18RR dataset, KNOWFORMER achieves the second-best performance among all baselines. Notably, SimKGC (Wang et al., 2022b) performs exceptionally well in this dataset, which can be attributed to its ability to acquire knowledge from extensive pretrained data, allowing it to capture the semantic relationships in WN18RR as a subset of a comprehensive English lexical database. Conversely, methods like SimKGC that rely on pretrained language models tend to underperform in datasets that require domain-specific knowledge. In contrast, our proposed text-free method relies solely on the structure of the knowledge graphs, resulting in enhanced robustness and scalability."}, {"title": "5.2. Inductive Performance", "content": "Inductive reasoning has become a prominent research topic (Hamilton et al., 2017; Teru et al., 2020) due to the ubiquitous occurrence of emergent entities in real-world applications (Zhang & Chen, 2020). In this part, we will show the performance of KNOWFORMER on the inductive knowledge edge graph reasoning task. Note that in this paper, we focus on the semi-inductive task, which has an invariant relation set. However, we believe that our method can be adapted to handle full-inductive tasks with slight modifications. We leave this aspect as future work to address.\nDatasets. In line with Teru et al. (2020), we employ the same data divisions of FB15k-237 (Toutanova & Chen, 2015), WN18RR (Dettmers et al., 2018), and NELL-995 (Xiong et al., 2017). Each of these divisions comprises 4 versions, resulting in a total of 12 subsets. It is worth noting that each subset has a unique separation between the training and test sets. Specifically, the training and test sets within each subset have unique sets of entities while sharing the same set of relations.\nBaselines. We compare KNOWFORMER with 7 baseline methods for inductive knowledge graph reasoning, including DRUM (Sadeghian et al., 2019), NBFNet (Zhu et al., 2021), RED-GNN (Zhang & Yao, 2022), A*Net (Zhu et al., 2023), AdaProp (Zhang et al., 2023), SimKGC (Wang et al., 2022b), and INGRAM (Lee et al., 2023). Note that ULTRA (Galkin et al., 2024) is not included as a baseline model, since we consider ULTRA to be a distinctive method based on the pre-training and fine-tuning paradigm, which can benefit from a large-scale amount of training data. However, our method still achieves comparable results to ULTRA on some datasets. Please refer to the experimental results in Appendix F.1 for more details.\nResults and Analysis. We also evaluate the performance by standard metrics (Wang et al., 2017; Ji et al., 2022), including MRR (\u2191), Hit@1 (\u2191), and Hit@10 (\u2191). Table 2 showcases the inductive performance of KNOWFORMER and baselines. We have observed that KNOWFORMER consistently achieves the highest performance on the majority of metrics across all versions of inductive datasets. Furthermore, our method also produces competitive results for the remaining metrics. Compared to transductive settings, KNOWFORMER demonstrates a relatively small performance gap. This can be attributed to the limited size of inductive datasets. Path-based methods can attain superior performance by utilizing shorter paths and avoiding excessive compression of information. However, KNOWFORMER maintains a performance advantage, indicating its effectiveness. Another noteworthy observation is the inadequate performance of SimKGC (Wang et al., 2022b) in inductive reasoning. This suggests a potential necessity for text-based and pretrained language model methods to have access to larger amounts of training data to converge"}, {"title": "5.3. Ablation Study", "content": "In this part, we aim to evaluate the effectiveness of the proposed attention mechanism in KNOWFORMER. We conduct comparisons against several variations, including: (1) removing attention, which results in KNOWFORMER degenerating into vanilla path-based methods such as NBFNet (Zhu et al., 2021); (2) excluding the query function; (3) excluding the value function; (4) substituting the kernel function with an approximation method based on random features (Sinha & Duchi, 2016; Liu et al., 2022; Wu et al., 2022); (5) substituting the kernel function with full-exponential kernel function.\nThe results of the ablation study on FB15k-237 are presented in Table 3. We can initially observe a decline in model performance when attention is removed, underscoring the importance of the proposed attention mechanism. The performance decrease caused by omitting the query function is relatively limited, suggesting that the input of the attention layer retains some structural information after the update of previous layers. Furthermore, omitting the value function leads to a significant decrease in model performance, as the explicit integration of structural information is crucial for reasoning tasks in knowledge graphs. In contrast, the pure transformer model faces difficulties in achieving this, highlighting the necessity of structural-aware modules. Lastly, a comparison with RF-based methods demonstrates the effectiveness of our kernel function.\nTo compare the performance between our proposed approximation kernel and the original full-exponential kernel, we conduct an additional ablation study on a small dataset UMLS (Kok & Domingos, 2007) in Table 6 due to the computation overhead. The minimal variation in the performance of both variants indicates the effectiveness of the approximate kernel. The full exponential kernel has a slight performance advantage, yet the quadratic computational complexity it brings is unacceptable in knowledge graph reasoning."}, {"title": "5.4. Further Analysis", "content": "In this section, we present additional experiments to demonstrate the superiority of KNOWFORMER over vanilla path-based methods (Zhu et al., 2021; Zhang & Yao, 2022). Specifically, we concentrate on two folds: (1) comparing methods in scenarios with gradually growing reasoning paths, and (2) comparing methods in scenarios with missing reasoning paths, where we randomly drop paths between pairs of entities in the test data. The experimental results on the WN18RR dataset are illustrated in Figure 3.\nPerformance w.r.t. Longer Paths. The individual performance of KNOWFORMER remained unaffected even as the reasoning path extended, validating its capacity to mitigate information over-squashing resulting from prolonged entity interactions. This finding is consistent with the results in Alon & Yahav (2021).\nPerformance w.r.t. Missing Paths. As the proportion of missing paths increases, KNOWFORMER exhibits stronger robustness compared to the baseline methods, which show a noticeable performance decline. This demonstrates that the presence of all-pair interactions in KNOWFORMER enables it to better handle missing paths."}, {"title": "6. Conclusion", "content": "This paper proposes a novel transformer-based method KNOWFORMER for knowledge graph reasoning. KNOW-FORMER consists of an expressive and scalable attention mechanism. Specifically, we introduce message-passing neural network based query function and value function to obtain informative key and value representations. Additionally, we present an efficient attention computation method to enhance the scalability of KNOWFORMER. Experimental results show that KNOWFORMER outperforms salient baselines on both transductive and inductive benchmarks."}, {"title": "A. More Preliminaries & Backgrounds", "content": "Transformer Architecture. The Transformer architecture is originally introduced in Vaswani et al. (2017). A vanilla transformer block consists of two main modules: a self-attention module followed by a feed-forward neural network. In the self-attention module, the input feature matrix denoted as X \u2208 Rnxd are projected to query Q, key K and value V using linear projection matrices Wq \u2208 Rd'\u00d7n, Wk \u2208 Rd'\u00d7n and W\u2082 \u2208 Rd'on. This is done by Q = XWquery, K = XW key and V = XW value, where the bias is omited. The self-attention computation is then given by:\n$$Self-Attention(X) = Softmax(\\frac{QKT}{\\sqrt{d'}} ) V.$$\nIn practice, it is common to utilize multi-head attention, where multiple instances of Equation (13) are concatenated. This approach has demonstrated effectiveness in capturing multi-view interactions. Subsequently, the output of the self-attention is combined with layer normalization (Ba et al., 2016) and a feedforward network (FFN) to form a transformer block.\nGraph Transformers. Recently, transformer models (Vaswani et al., 2017) have gained popularity in graph learning due to their ability to capture complex relationships beyond those captured by regular graph neural networks (GNNs) differently. Injecting structural bias into the original attention mechanism is a key problem in graph transformers. Early work by Dwivedi & Bresson (2020) used Laplacian eigenvectors as positional encodings, and since then, several extensions and other models have been proposed (Min et al., 2022; Kim et al., 2022; Ma et al., 2023). Wu et al. (2021) propose a hybrid architecture that uses a stack of message-passing GNN layers followed by regular transformer layers. The most relevant work to this paper is SAT (Chen et al., 2022), which reformulates the self-attention mechanism as a smooth kernel and incorporates structural information by extracting a subgraph representation rooted at each node before attention computation. Computation during the process follows the equations displayed below:\n$$Self-Attention(x) = \\sum_{u \\in V} \\frac{\\kappa(x_u, x_v)}{\\sum_{\\nu \\in V} \\kappa(x_u, x_{\\omega})} \\cdot f (x_v), \\forall u \\in V,$$\nwhere f(x) = xWvalue + bvalue, and the non-symmetric kernel k is defined as:\n$$\\kappa(x_u, x_v) = exp (\\frac{(GNN(x_u)W_{query} + b_{query}, GNN(x_v)W_{key} + b_{key})}{\\sqrt{d}}).$$"}]}