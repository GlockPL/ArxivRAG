{"title": "Attention-Guided Integration of CLIP and SAM for Precise Object Masking in Robotic Manipulation", "authors": ["Muhammad A. Muttaqien", "Tomohiro Motoda", "Ryo Hanai", "Domae Yukiyasu"], "abstract": "This paper introduces a novel pipeline to enhance the precision of object masking for robotic manipulation within the specific domain of masking products in convenience stores. The approach integrates two advanced AI models, CLIP and SAM, focusing on their synergistic combination and the effective use of multimodal data (image and text). Emphasis is placed on utilizing gradient-based attention mechanisms and customized datasets to fine-tune performance. While CLIP, SAM, and Grad-CAM are established components, their integration within this structured pipeline represents a significant contribution to the field. The resulting segmented masks, generated through this combined approach, can be effectively utilized as inputs for robotic systems, enabling more precise and adaptive object manipulation in the context of convenience store products.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the ability to recognize and manipulate spe- cific objects within well-defined domains, such as products in convenience stores, has become increasingly important in the field of robotic manipulation [1] [2] [3]. As robots are expected to perform more complex tasks in diverse environments, the need for precise object identification and interaction grows, particularly in domains where a high level of accuracy is crucial. For instance, in convenience stores (Figure 1), robots must reliably identify and handle a wide variety of products, each with unique visual characteristics, to automate tasks such as stocking, sorting, and customer assistance.\nThe advent of large foundation models like CLIP (Con- trastive Language-Image Pretraining) [4] and SAM (Segment Anything Model) [5] has opened new possibilities for im- proving object recognition and segmentation tasks. These foundation models, which are pre-trained on vast datasets, provide a robust framework for tackling complex AI chal- lenges. CLIP's ability to align images and text through a shared embedding space, combined with SAM's flexibility in segmenting any part of an image, offers a strong foundation for developing advanced robotic systems. Fine-tuning CLIP for specific applications within a system can enhance the precision of the prompts that guide SAM in producing accurate object masks, which are critical for robots to perform complex manipulation tasks more accurately and reliably.\nHowever, integrating language and vision in multi-modal tasks presents significant challenges. While models like CLIP and SAM are powerful individually, their real-world use is limited by a key bottleneck: the lack of domain-specific data, such as object detection, grounding, and caption data, which are essential for fine-tuning. Existing pipelines like GroundedSAM [6], FastSAM [7], or even YOLO World [8] integrated with SAM perform well in generalized settings but struggle in highly specific environments, such as convenience stores, where the variety of objects and their contextual relevance differ significantly from the data these models were trained on. The difficulty lies in the fact that preparing such domain time-consuming and labor-intensive.\nIn this paper, we propose a novel pipeline that integrates fine-tuned CLIP and SAM, enhanced by gradient-based atten- tion mechanisms, to address the challenges of object masking in robotic manipulation. Focusing on convenience stores, our method optimizes performance with customized datasets and task-specific fine-tuning, overcoming the data limitations faced by existing pipelines. This allows for more precise and reliable robotic systems in real-world applications. The"}, {"title": "II. RELATED WORK", "content": "The development of object recognition, segmentation, and multimodal AI has been significantly shaped by pioneering models like AlexNet [9], VGGNet [10] and ResNet [11], which introduced deep architectures capable of capturing intricate visual patterns. VGGNet, known for its deep and simple structure, uses small convolution filters to achieve high accuracy, while ResNet introduced residual learning, en- abling deeper networks without the vanishing gradient prob- lem. These advancements in convolutional neural networks (CNNs) have laid the groundwork for more sophisticated models that are crucial for tasks such as object manipulation in complex environments like convenience stores.\nObject detection has also evolved with the introduction of models such as Faster R-CNN [12] and YOLO [13], both of which have significantly influenced the field. Faster R-CNN integrates region proposal networks with CNNs to enhance detection accuracy, making it a preferred choice for tasks requiring precision. In contrast, YOLO redefined object detection by framing it as a single regression prob- lem, allowing for real-time performance. The progression of object detection models has been further supported by attention mechanisms, particularly through the Transformer architecture and its adaptation to visual tasks with Vision Transformers (ViT) [14]. ViT demonstrated that self-attention could effectively capture global dependencies in images, offering a new approach to image recognition that reduces reliance on convolutional operations.\nThe integration of language and vision has opened new possibilities in multimodal AI, with models like CLIP, GLIP [15], and DINO [16] playing a central role. CLIP, developed by OpenAI, showed the power of aligning text and image representations, enabling robust zero-shot learning across various vision tasks. Building on this, GLIP enhanced the grounding of language in visual contexts, making it more effective for object detection in multimodal scenarios. DINO and its extension, Grounding DINO [17], explored self-supervised learning for visual representations, emphasizing the importance of grounding visual concepts in multimodal inputs. These advancements have significantly enhanced the capabilities of systems that require a deep understanding of both visual and textual information.\nSegmentation, a crucial aspect of object manipulation, has advanced significantly with models like the SAM by Meta AI. SAM represents a major leap in segmentation technology, delivering high-quality results across a wide range of objects and scenes. Trained using a vast and diverse dataset, SAM en- hances its ability to generalize across various environments. These advanced segmentation models are particularly rele- vant in environments like convenience stores, where precise object detection and manipulation are critical.\nTo ensure the reliability and interpretability of these com- plex systems, techniques like Grad-CAM [18] have been developed to provide visual explanations of model decisions. Grad-CAM highlights the regions of an image that influence a model's output, offering insights into the model's decision-making process. This explainability is essential in multimodal AI systems, where understanding the model's focus areas can lead to more trustworthy and effective applications."}, {"title": "III. FOUNDATION MODEL", "content": "Foundation models have revolutionized artificial intelli- gence by providing versatile, pre-trained models that can be adapted across various tasks and domains. These models, trained on extensive datasets, capture a wide range of patterns and representations, making them invaluable for complex applications like robotic manipulation. Their scalability and ability to be fine-tuned for specific tasks allow for significant advancements in performance, particularly in environments requiring precise and reliable AI systems.\nCLIP (Contrastive Language-Image Pretraining) is a prime example of a foundation model that bridges the gap between language and vision. Developed by OpenAI, CLIP uses a dual-encoder architecture to align images and text within a shared embedding space. This capability enables CLIP to perform zero-shot learning and effectively recognize and distinguish objects in varied contexts. In our research, CLIP is crucial for generating accurate prompts that guide the segmentation process, particularly in the complex and diverse environment of convenience stores.\nSAM (Segment Anything Model) complements CLIP by offering high-precision image segmentation. Built on a vision transformer architecture, SAM processes images to create detailed segmentation masks based on user-defined prompts. Its versatility and accuracy are crucial for robotic manipula- tion, particularly in complex environments like convenience stores, where detecting objects on cluttered shelves can be challenging. By integrating SAM with CLIP, we achieve a more precise and adaptive object masking process, enhancing the ability of robots to interact accurately with products in such demanding settings.\nThe relevance of these foundation models to our research lies in their ability to tackle the challenges of multi-modal AI tasks. As shown in Figure 2, the integration of CLIP and SAM, enhanced by gradient-based attention mechanisms like a modified Grad-CAM, forms the core of our proposed pipeline. By fine-tuning on domain-specific datasets, this ap- proach enhances the system's ability to accurately recognize and mask objects of interest in specific domains, such as convenience stores, based on defined prompts."}, {"title": "IV. GRADIENT-BASED ATTENTION", "content": "In this section, we discuss the role of gradient-based attention mechanisms in our system integration, focusing on how Grad-CAM is utilized to enhance the interpretability and effectiveness of the CLIP and SAM models within our"}, {"title": "V. TECHNICAL SETUP", "content": "To fine-tune the CLIP model for our application, we prepared a dataset of approximately 30,000 high-resolution RGB images paired with text (without the need for detection or grounding data) from a realistic mock-up of a convenience store in our laboratory. The dataset includes a wide variety of products across multiple categories, capturing diverse angles, lighting conditions, and arrangements. While the dataset contains many specific items, we highlight some specific products for detailed visualization.\nCLIP employs two primary architecture variants: ViT (Vision Transformer) and ResNet-based models. For our implementation, we focused on the ResNet variant because its architecture is more suitable for gradient-based attention techniques, such as Grad-CAM, which we use not only to enhance the interpretability of the model but also to effec- tively connect CLIP and SAM in our pipeline. The ResNet- based CLIP model processes images through a series of convolutional layers, which are followed by fully connected layers that map the visual data into a shared embedding space with text.\nFor fine-tuning the ResNet-based CLIP model, we selected hyperparameters optimized for our domain-specific dataset, as depicted in Table I. These settings were chosen based on preliminary experiments to balance training efficiency and model performance. The ResNet-based CLIP model, with approximately 150 million parameters, captures complex visual patterns and effectively aligns them with corresponding textual descriptions, seamlessly integrating with SAM for precise object masking in our system.\nSAM is available in several variants, including base, large, and huge, each differing in model size, number of parameters, and computational complexity. The base variant is lightweight and suitable for limited resources, while the"}, {"title": "D. Grad-CAM", "content": "Grad-CAM was chosen for its ability to work without requiring changes to the CLIP architecture and its proven effectiveness with multimodal inputs like image captioning and visual question answering. We explore how Grad-CAM is applied in our system to enhance the effectiveness of the SAM model using different types of prompts: Single Point, Multiple Points, and Bounding Box.\n\u2022 Single Point: Grad-CAM generates attention maps by focusing on a single pixel, providing localized insight into the model's focus and refining segmentation masks around specific points of interest. This is especially useful for precise segmentation of small or detailed objects.\n\u2022 Multiple Points: Applying Grad-CAM to multiple points on the object of interest creates a more comprehensive attention map, covering different areas of the object. This method improves the model's ability to accurately segment complex shapes or objects with varying features by integrating information from key locations.\n\u2022 Bounding Box: Grad-CAM is also used with bounding box prompts, where the model focuses on a rectangular region surrounding the object. This approach is effective for segmenting larger objects or when the general area of the object is known. The bounding box prompt guides the segmentation process, ensuring that attention remains within the specified boundaries."}, {"title": "E. System Integration", "content": "We outline a pipeline integrating CLIP, SAM, and Grad- CAM to achieve precise object masking for robotic manipu- lation in convenience stores. The process begins with CLIP, which calculates a similarity score between the image and the prompt. A high similarity score suggests the presence of the object, which is then visualized using Grad-CAM through the ResNet-based model in CLIP. Grad-CAM generates an attention map that highlights relevant areas of the image. This attention map is then converted into prompts-single points, multiple points, or bounding boxes-that guide SAM to accurately mask the object. As illustrated in Figure 2, this pipeline demonstrates how CLIP's output informs SAM, with SAM optimizing the process for real-time applications.\nTo determine the optimal threshold for our application, we calculate similarity scores between each product image and all product names using the fine-tuned CLIP model. Each image is classified as correct if the highest similarity score matches the correct product name; otherwise, it is classified as incorrect. By analyzing these scores, we compute the F1- score at various thresholds through precision-recall calcula- tions. The threshold that maximizes the F1-score provides the best balance between precision and recall, indicating when a similarity score is sufficiently high to classify an object as present. In this case, the optimal score threshold is 0.489.\nThe process starts with CLIP's visual encoding of the input image $I$ using a visual encoder $f_v$, producing the visual em- bedding $V$. Simultaneously, the text prompt $t$ is processed by the text encoder $f_t$, resulting in the text embedding $T$. These embeddings are normalized to $E_V$ and $E_T$, respectively. The dot product of these normalized embeddings yields the similarity score $S$, indicating the relevance of the object in the image.\n$V = f_v(I)$\n$T = f_t(t)$\n$E_v = \\frac{V}{||V||}$\n$E_T = \\frac{T}{||T||}$\n$S = E_v \\cdot E_T$\n$A(x, y) = ReLU(\\sum_k \\alpha_k \\cdot f_k (x,y))$\n$P = f(A) \\in \\{single point, multiple points, bounding box\\}$\n$M = g(I, P)$\nNext, Grad-CAM generates an attention map $A(x, y)$ based on the similarity score, highlighting relevant areas in the im- age. Here, $x$ and $y$ represent the spatial coordinates of a pixel within the attention map. This attention map is then converted into prompts $P$ (such as single points, multiple points, or bounding boxes that guide the SAM model). Finally, SAM uses the image and prompts to generate the segmentation mask $M$, accurately isolating the object of interest. This pipeline seamlessly integrates CLIP's multimodal capabilities with SAM's precise segmentation, enhanced by Grad-CAM's attention mapping."}, {"title": "VI. EXPERIMENTS", "content": "In this section, we evaluate the performance of our pro- posed pipeline, focusing on the effectiveness of our approach in improving object recognition, segmentation accuracy, and overall system reliability. We also discuss the insights gained from our analysis and the technical limitations that impact the system's applicability in real-world scenarios.\nWe implemented our pipeline using Python programming and the PyTorch Deep Learning Framework [19], and trained the models on an NVIDIA GeForce RTX 4090. Before fine-tuning the CLIP model, we assessed its original performance on our domain-specific dataset, which focuses on conve- nience store products. We used zero-shot and top-5 accuracy metrics to evaluate how well CLIP could recognize and distinguish similar products without additional training. We initially used CLIP as a classifier to measure these metrics and then compared its performance with that of fine-tuned versions using ResNet and ViT backbones.\nAs shown in Table II, the original CLIP model strug- gles to recognize products in our specific domain, which is understandable given that many of the products have Japanese names, such as Edamariko and Tonkotsu. In this context, \"Product\" refers to specific items like Edamariko or Tonkotsu, while \"Category\" refers to broader classifi- cations such as snacks. Both ResNet and ViT-based CLIP models show significant improvement after being trained on our prepared datasets. Interestingly, smaller models like ResNet50 and ViT16 outperform their larger counterparts, such as ResNet101 and ViT32. This suggests that the larger models may be prone to overfitting, whereas the smaller models generalize better for the task at hand.\nAfter establishing baseline performance, we shifted to treating CLIP as a model capable of calculating the similarity between image captions and image inputs. This approach leveraged CLIP's multimodal capabilities, where it processes text and image inputs to produce a similarity score. Figure 3 illustrates that a high score indicates the presence of the object of interest in the image. The fine-tuned CLIP model can adapt to our specific dataset, improving its ability to accu- rately recognize and match products with their descriptions."}, {"title": "B. Attention Map Analysis", "content": "We generated attention maps using Grad-CAM applied to the fine-tuned CLIP model. These maps visually represent the areas of an image that the model focuses on when making decisions. To evaluate the performance of these attention maps, we visually analyzed whether the highlighted regions were located within the object of interest. If the attention was focused inside the object of interest, we considered the map effective, regardless of the specific part of the object being highlighted as shown in Figure 4. This evaluation was key in assessing how effectively the fine-tuned model focuses on relevant regions, validating the fine-tuning success. The attention maps also laid the groundwork for integrating with the SAM model, ensuring accurate segmentation masks."}, {"title": "C. Attention-Guided Integration", "content": "The integration of CLIP and SAM in our pipeline is enhanced by the use of attention maps generated by Grad- CAM. These maps serve as a guide for the SAM model,"}, {"title": "D. Masking Result", "content": "The integration of CLIP and SAM in our pipeline is signif- icantly enhanced by the use of attention maps generated by Grad-CAM. These maps guide the SAM model in producing more precise and accurate segmentation masks by directing the model's focus to the most relevant areas of the image. To evaluate the effectiveness of this attention-guided integration, we tested how well the attention maps functioned as prompts for SAM, exploring different prompt types, including single point, multiple points, and bounding box. Each prompt type contributed uniquely to the object masking quality, adapting to different scenarios and object complexities.\nTo assess masking accuracy, we compared the generated masks with ground truth masks using Intersection over Union (IoU) as the evaluation metric, calculating IoU scores by evaluating the predicted masks against the label masks. As shown in Table III, ResNet50 CLIP model consistently outperformed the ResNet101 CLIP model across all three prompts demonstrating higher IoU scores. This suggests that the smaller ResNet50 model is better suited to our task, likely due to reduced overfitting. Additionally, the table indicates that using points as prompts for SAM yields better perfor- mance than bounding boxes, supporting the effectiveness of point-based prompts in improving mask accuracy.\nThis comparison revealed that the focus areas identified by CLIP were effectively utilized by SAM, resulting in accurate masking across various test cases. The high IoU scores confirmed the robustness and reliability of our masking process, which is essential for ensuring accurate and effective robotic manipulation tasks in real-world applications. Figure 6 demonstrates how our system highlights key regions in the attention map, selects a single point for segmentation, and accurately masks the identified object."}, {"title": "E. Pipeline Evaluation", "content": "The accuracy results of fine-tuned ResNet-based and ViT- based CLIP models for recognizing products in our specific domain (convenience store) demonstrate the potential of CLIP to be effectively integrated into our pipeline for solving the object recognition problem. Furthermore, by leveraging the gradient-based attention mechanism of Grad-CAM to integrate CLIP with SAM, we have shown the effectiveness of this approach, as evidenced by the remarkable IOU scores achieved with both single and multiple points as prompts.\nHowever, while our system demonstrates strong perfor- mance in specific conditions, there are limitations that need to be addressed. The current system is optimized for a controlled camera position, which limits its flexibility in more variable environments. Additionally, the focus on single-object mask- ing restricts the system's ability to handle multiple objects simultaneously."}, {"title": "VII. CONCLUSION", "content": "In this paper, we have presented a novel pipeline that inte- grates CLIP, SAM, and Grad-CAM to enhance the precision of object masking for robotic manipulation in convenience stores. By leveraging the strengths of these advanced AI models and fine-tuning them with customized datasets, we have demonstrated that the integration of these components can significantly improve the accuracy and relevance of segmentation tasks. The resulting masks are highly effective as inputs for robotic systems, enabling more accurate manip- ulation of products in dynamic environments.\nFuture work will focus on extending the pipeline to handle multiple object masking in complex scenes with overlapping objects. We also aim to improve scalability for a broader range of products and explore other foundation models like YOLO World and GPT [20] to enhance robustness and versa- tility. These advancements will make our solution applicable to a wider array of real-world scenarios."}]}