{"title": "A Multi-Stage Framework for 3D Individual\nTooth Segmentation in Dental CBCT", "authors": ["Chunshi Wang", "Bin Zhao", "Shuxue Ding"], "abstract": "Cone beam computed tomography (CBCT) is a common\nway of diagnosing dental related diseases. Accurate segmentation of 3D\ntooth is of importance for the treatment. Although deep learning based\nmethods have achieved convincing results in medical image processing,\nthey need a large of annotated data for network training, making it very\ntime-consuming in data collection and annotation. Besides, domain shift\nwidely existing in the distribution of data acquired by different devices\nimpacts severely the model generalization. To resolve the problem, we\npropose a multi-stage framework for 3D tooth segmentation in dental\nCBCT, which achieves the third place in the \"Semi-supervised Teeth\nSegmentation\" 3D (STS-3D) challenge. The experiments on validation\nset compared with other semi-supervised segmentation methods further\nindicate the validity of our approach.", "sections": [{"title": "1 Introduction", "content": "Advances in modern digital dentistry rely heavily on the acquisition and segmen-\ntation of three-dimensional (3D) imaging. In particular, cone beam computed\ntomography (CBCT) plays a crucial role in obtaining accurate 3D digital models\nof the jaws and teeth while keeping costs and radiation doses relatively low.\nThe acquisition and segmentation of 3D digital images has a variety of appli-\ncations in the field of oral and maxillofacial disciplines, with the most prominent\napplications being orthodontic diagnosis and treatment planning. Specifically,\nthese techniques are commonly used in 3D-guided implant surgery, guided en-\ndodontic and apical surgery, CBCT-based planning and fabrication of donor\ntooth replicas, etc. 3D tooth segmentation is an important part of the aforemen-\ntioned treatment processes and can be used in orthodontics to develop treatment\nplans and to follow the evolution of root resorption after treatment. However,\nprecise tooth segmentation is challenging. The presence of a large number of\nteeth on each arch in the mouth complicates the segmentation process. Teeth\nin the emergence stage have some specialized structures that make them diffi-\ncult to distinguish. Metal fillings and dental restorations can cause artifacts in\nCBCT, which can lead to aberrations in the segmentation results. In addition,\nthe composition of the tooth itself is more complex, with elements consisting\nof cementum, dentin, pulp and enamel in close contact with other anatomical\nstructures in the mouth, making it difficult to determine tooth edges. There is\nalso a similar density between the alveolar bone and the tooth structure, which\nleads to separate the upper and lower teeth from each other difficultly [1].\nOver the past decade, many attempts have been made to develop 3D tooth\nsegmentation methods, most of which have been level-set based methods [2,3].\nUnfortunately, these methods do not achieve fully automatic segmentation for\nthe reason that they need to manually intervene in the initialization of the level-\nset and the complex image structure between adjacent teeth and bones further\nprevents automatic initialization. Besides, level-set based methods require a lot\nof mathematical operations and perform poorly for metal artifacts and upper-\nlower tooth separation. There are also some graph cut based methods for tooth\nsegmentation [4,5], but they need priors to guide the segmentation process and\nlack robustness.\nRecently, convolutional neural networks (CNNs) have been applied to 3D\ntooth segmentation aiming to overcome the limitations of traditional segmen-\ntation methods. The focus of researchers has shifted to the development of an\nalgorithm for fully automated tooth segmentation without human intervention,\nstriving for accuracy and speed [6]. For instance, Chen et al. propose a 3D full\nconvolutional neural network (FCN) combined with the watershed transform for\ntooth segmentation [7]. Hsuet al. propose a 3.5D U-Net model to improve the\nperformance of tooth segmentation [8]. Cui et al. propose Toothnet, a network\nthat utilizes edge maps, similarity matrix and spatial relationships between teeth\n[9]. Cui et al. further develop an AI system for fully automated segmentation of\nteeth and alveolar bone using the 2-Stage scheme, which is validated on a large\nnumber of CBCT scans, demonstrating to some extent the effectiveness of AI\ntechnology in enhancing clinical workflow [10].\nThe above-mentioned methods are performed in fully supervised scenarios,\nwhere the data need to be labeled in detail by experts. However, the labeling\nprocess for 3D medical data is both complex and expensive. In addition, there are\nlimited datasets available for 3D tooth segmentation studies [11], which severely\nlimits its wide application in the field of tooth segmentation. To address this\nproblem, semi-supervised learning (SSL) methods for medical image segmen-\ntation have emerged [12,13]. These methods require less expert annotation for\nmodel training, thus reducing the time and effort required for data annotation.\nBut Since domain offsets exist widely in medical data, semi-supervised learning\ncannot directly obtain better segmentation results. Therefore, in this paper, we\npropose a multi-stage framework based on SSL and domain adaptation to seg-\nment tooth from CBCT, which achieves the third place in the \"Semi-supervised\nTeeth Segmentation\" 3D (STS-3D) challenge. The extensive experiments com-\npared with other semi-supervised segmentation methods further indicate the\nvalidity of our approach."}, {"title": "2 Materials and Method", "content": "2.1 Research subjects\nThe experimental data used in this paper is provided by the STS-3D challenge\nwhere training set includes 12 CT scans with annotations and 300 CT scans\nwithout annotations [11,14]. There are 50 CT scans for test and they do not\nprovided the annotations for participants.\n2.2 Multi-stage framework\nOur proposed multi-stage framework is illustrated in the Fig. 1. As shown in\nthe Fig. 1, in stage 1, we utilize supervised learning based on the 2D nnU-\nNet[15] model to generate pixel-level pseudo-annotations for a small number of\nrandomly sampled unlabeled samples from the training set, and then combine\nthese pseudo-annotated samples with the labeled samples in the training set to\nform new labeled samples. In stage 2, these new labeled samples are fed into\nthe Improved-UniMatch model for parameter learning after domain adaptation\ntogether with the unlabeled samples in the training set.\nIn particular, in stage 1, the labeled samples is first used to train the 2D nnU-\nNet 20 epochs, and then the trained 2D nnU-Net is used to generate low-quality\npixel-level pseudo-annotations for the 10 randomly selected unlabeled samples\nin the unlabeled set. These 10 pseudo-annotated samples are then merged with\nthe labeled samples to form new labeled samples to train the stage 2. In stage\n2, the new labeled samples and the remaining unlabeled samples are fed into\nFourier Transform Augment (FTA) module to make the model learn the differ-\nence between two domains. The FTA module is presented in Fig. 2. During the\ntraining process, a labeled image $x^w$ and a unlabeled image $x^u$ are randomly se-\nlected, and then a Fourier transform $F$ is performed to transfer the images to the\nfrequency domain and obtain the magnitude spectrum $\\{A^w, A^u\\}$ and the phase\nimage $\\{P^w, P^u\\}$, where the magnitude spectrum contains the low-level statistics\nand the phase image includes the high-level semantics of the original signal [16]."}, {"title": null, "content": "The image $x^w$ is then enhanced by merging the magnitude information of the\nimage $x^u$,\n$A_{new} = (1 - \\lambda)A^w * (1 \u2013 M) + \\lambda A^u * M$,                                                                  (1)\nwhere $A_{new}$ is the newly generated phase map. $\\lambda$ is a parameter to adjust the\nratio between the phase information of $x^w$ and $x^u$. $M$is used to control the spa-\ntial extent of the magnitude spectrum to be exchanged, and here M is set to the\ncenter region of the magnitude spectrum containing low frequency information.\nAfter that, the merged samples are transformed from the frequency domain to\nthe image domain by $F^{-1}$ to obtain the image sample $Z^w$ enhanced by Fourier\ntransform and fused with the low-level information of the other sample,\n$Z^w = F^{-1}(A_{new}, P^w)$.                                                                   (2)\nSimilarly exchanging $x^w$ and $x^u$ gives $Z^u$:\n$Z^u = F^{-1}(A_{new}, P^u)$.                                                                   (3)\nIt is worth mentioning that $Z^w$ and $Z^u$ can be merged together for compu-\ntation, which will greatly reduce the computational cost. That is, it is necessary\nto augment both $x^u$ and $x^w$ at the same time, and send the result of mutual\naugmentation into the subsequent training process.\nThe data performed through FTA are fed into the Improved-Unimatch for\nsegmentation training process. In particular, we introduce the idea of self-adaptive\nthresholding in FreeMatch [17] into the current segmentation task, allowing\nUniMatch[18] to adaptive adjust the threshold during training and improve"}, {"title": "2.3 Evaluation metrics", "content": "In this research, dice coefficient and intersection over union (IoU) are used to\nevaluate the pixel-level segmentation performance, which are formulated as\n$Dice = \\frac{2* A\u2229B}{|A|+|B|},$                                                                                               (4)\nand\n$IoU = \\frac{A\u2229B}{A\u222aB},$                                                                                              (5)\nrespectively, where A and B indicate the predicted segmentation and the ground\ntruth, respectively. In addition, the 3D Hausdorff distance is used to evaluate\nthe voxel-level segmentation performance and is formulated as\n$H(d) = min(|x1 \u2212 x2| + |y1 \u2013 y2| + |z1 \u2013 z2|),$                                                                      (6)\nwhere $(x1, y1, z1)$ and $(x2, y2, z2)$ denote the coordinates of the two voxels. $|x1-$\n$x2|$, $|y1 y2|$ and $|z1 z2|$ denote the distances on the corresponding axes.\nFor scoring purposes, the challenge uniformly normalizes the hausdorff dis-\ntance to the range of [0, 1]. The final weighted average of the three metrics is\ntaken and the specific scoring formula is\n$score = 0.4 * Dice + 0.3 * IoU + 0.3 * (1 \u2013 H(d)).$                                                      (7)"}, {"title": "3 Experiment", "content": "3.1 Data preprocessing\nTo take advantage of the pixel-level pseudo-annotations generated from unla-\nbeled data and obtain rich tooth morphology, the training data are sliced in 3-\naxis. That is, the 3D tooth data are sliced in axial, coronal, and sagittal planes,\ne.g., a 640 x 640 \u00d7 400 CT image would result in 640 + 640 + 400 = 1680 slices.\nUsing file suffixes as the division of slice axes, for simplicity, x, y and_z are di-\nrectly used as the suffixes of slice files [20]. In addition, the slices are normalized\nto a range of [0,1] and 10% of slices are used as the validation set.\nIn order to explore the effect of different thresholds of the input image on the\nperformance of the proposed method, we conduct thresholding experiments to\nselect the best threshold for training. In particular, to save time, we randomly\nselect 25% of slices from the training slices to train our model. We train 3 epochs\nand the quantitative results on validation set of each epoch are shown in Tab. 1.\nFrom Tab. 1, it can be seen that when the combination of 500 and 2000 is chosen\nfor the threshold value, the proposed method has the best segmentation results\non the validation set, so this combination is chosen as the threshold value in the\nexperiment."}, {"title": "3.2 Implementation Details", "content": "The optimizer is the AdamW method [21] with weightdecay = 0.0001. The initial\nlearning rate is $10^{-4}$. During training, the learning rate is updated using the\nfollowing formula,\n$lr = lrb \u00d7 (1 \u2013 \\frac{i}{N})^{-0.9},$                                                                                      (8)\nwhere $lrs$ indicates the initial learning rate. $i$ and $N$ indicate the current number\nand the total number of iterations, respectively.\nThe experiments are performed on a computer with an Intel Core i7-6800K\nCPU, 64 GB RAM and NVIDIA Tesla V100 GPU with 32 GB memory. All\nnetworks are implemented in PyTorch."}, {"title": "3.3 Results", "content": "We train our model only 3 epochs for limited time and submit the results to\nthe challenge. Tab. 2 summaries the top five submission results of the STS-3D\nchallenge and our result has been highlighted in red. As shown in the Tab. 2, our\nproposed method achieves the third place in the challenge. In addition, we visu-\nalize some segmentation results in the Fig. 3. No matter from the axial, coronal\nand sagittal, our proposed method obtains the better segmentation results."}, {"title": "4 Conclusion", "content": "In this paper, we propose a multi-stage framework for 3D tooth segmentation,\nwhich achieves the third place in the STS-3D challenge. The experiments on val-\nidation set compared with other semi-supervised segmentation methods further\nindicate the validity of our approach."}, {"title": "5 Authors' Contributions", "content": "Chunshi Wang and Bin Zhao contribute equally to this paper."}]}