{"title": "Anytime Probabilistically Constrained Provably Convergent Online Belief Space Planning", "authors": ["Andrey Zhitnikov", "Vadim Indelman"], "abstract": "Taking into account future risk is essential for an autonomously operating robot to find online not only the best but also a safe action to execute. In this paper, we build upon the recently introduced formulation of probabilistic belief-dependent constraints. We present an anytime approach employing the Monte Carlo Tree Search (MCTS) method in continuous domains. Unlike previous approaches, our method assures safety anytime with respect to the currently expanded search tree without relying on the convergence of the search. We prove convergence in probability with an exponential rate of a version of our algorithms and study proposed techniques via extensive simulations. Even with a tiny number of tree queries, the best action found by our approach is much safer than the baseline. Moreover, our approach constantly finds better than the baseline action in terms of objective. This is because we revise the values and statistics maintained in the search tree and remove from them the contribution of the pruned actions.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "CASTING decision-making under uncertainty as a Partially Observable Markov Decision Process (POMDP) is considered State-Of-The-Art (SOTA). Under partial observability the decision-making agent does not have complete information about the state of the problem, so it can only make its decisions based on its \"belief\" about the state. In a continuous domains in terms of POMDP state, the belief, in a particular time index, is the Probability Density Function (PDF) of the state given all concurrent information in terms of performed actions and received observations in an alternating manner, plus the prior belief. A POMDP is known to be undecidable [1] in finite time.\nIntroducing various constraint formulations into POMDP is essential for, e.g., ensuring safety [2], [3] and efficient Autonomous Exploration [4]. Yet, the existing online approaches in anytime setting have problems and therefore fall short of providing reliable and safe optimal autonomy. This crucial gap we aim to fill in this paper.\nSimilar to almost any online POMDP solver today such as MCTS, our method constructs a belief tree and uses the tree to represent the POMDP policy. We prune dangerous actions from the belief tree and revise the values and statistics that an MCTS tree maintains. Anytime, our search tree contains only the safe actions in accord to our definition of safe action, which will appear shortly. Our work lies in continuous domain in terms of actions and the observations. In such a setting, there are approaches to tackle averaged cumulative constraint using anytime MCTS methods [5], [6]. We now linger on the explanation of what the averaged constraint is.\nUnder partial observability, namely in the POMDP setting, there are naturally two stages to consider in order to introduce a constraint. The first stage arises from the belief itself. Usually, at this stage, the state-dependent payoff operator is averaged with respect to the corresponding belief to obtain a belief-dependent one. It is then summed up to achieve a cumulative payoff. We use the term payoff to differentiate between reward operator and emphasize that a belief-dependent payoff constraint operator shall be as large as possible as opposed to the cost operator. The second stage arises from the distribution of possible future observations episodes. At this stage, commonly, the cumulative payoff is again averaged but with respect to future observations episodes and then thresholded, thereby forming an averaged cumulative constraint. Such a formulation is sufficient for ensuring safety in limited cases as we will further see in Section VI-A. This is because it permits deviations of the individual values within the summation.\nLet us now describe the MCTS methods mentioned above to tackle averaged cumulative constraint. The seminal paper in this direction is [7]. It leans on the rearrangement of the constrained objective using the occupancy measure described in [8]. Such a reformulation is appealing since it transforms the problem into linear programming bringing convexity to the table and enjoying from strong duality. The authors of [5] extend the approach from [7] to continuous spaces. Still, both papers [7] and [5] assure constraint satisfiability only at the limit of the convergence of the iterative procedure, namely in infinite time. Since these are iterative methods, to assure anytime constraint satisfiability we need to project the obtained occupancy measure at each iteration to the space defined by the constraint. If dual methods are involved [9] such a projection does not make much sense, e.g., the projection might lead to a step direction vector on the boundary of all the constraints, making it zero vector. Employing the primal methods in continuous spaces also appears to be problematic since the summations in [7] are transformed into integrals.\nThe paper [6] provides some sort of anytime satisfiability by introducing high-level action primitives (options). Still, [6] suffers from limitations, e.g. it requires crafting low-level policies, meaning knowing how the robot shall behave a priori. In addition, the options shall be locally feasible. Additionally, for efficiency reasons, the duality based approaches perform a single tree query of the MCTS, instead of running MCTS until convergence in the maximization of the Lagrangian dual objective function phase (See section 8.5.2 in [9]) of dual ascend.\nIn all three papers [7], [5], [6] the averaged cumulative constraint is enforced solely from the root of the belief tree. This is suboptimal since within a planning session it is not taken into account that the constraint will be enforced at the future planning sessions. In other words, the contemplation of a robot about the future differs from its actual future behavior. This aspect has been fixed by [10]. As we will further see in Section IV, our approach naturally handles this problem. Moreover, [10] assures fulfillment (admission) of the recursive averaged cumulative constraint anytime with respect to search tree constructed partially with the reward bounds and partially with rewards themselves. Yet, the algorithm presented in [10] requires that the value function is bounded on the way down the tree to assure the exploration. This is commonly achieved by assuming that the state-dependent reward is trivially bounded from above and below. This does not hold for general belief-dependent reward functions. Moreover, the exploration outlined in that paper is valid for discrete spaces only. All in all, the extension of that work to continuous spaces and belief-dependent rewards requires clarification.\na) Support for general belief dependent rewards and payoff/cost operators and MCTS convergence: We now clarify whether or not the mentioned above solvers support belief-dependent cost/payoff operators and rewards. It was suggested in [3],[4] that general belief-dependent payoff/cost operators are extremely important. As mentioned in [3] Value-at-Risk (VaR) and Conditional VaR (CVaR) over the distance to the safe space allow for control of the depth the robot can plunge into the obstacle. To rephrase that, these operators measure how bad the disaster (collision) will be. See Appendix D, for details. The Information Gain discussed in [4] is relevant for exploration. The paper [4] discussed the general belief-dependent averaged constraint of the form (38) in a high dimensional setting and in the context of Information Gain. The iterative schemes in [7], [5] lean on the convergence of MCTS. It has been shown in [11] that even in discrete spaces and with bounded rewards it can take a very long time for MCTS to converge. In the case of unbounded reward or the cost-augmented objective of [7], [5], the MCTS may converge slowly. If such an augmented reward has a large variance, it will be needed a huge amount of tree queries for action-value estimate (to be defined shortly) at each belief node of the belief tree to converge. The large variance can be the result of an unrestrained variability of the rewards or a large Lagrange multiplier.\nThere are several constraint formulations for POMDP. Below we discuss the most prominent techniques one by one.\nb) Shielding POMDPs: There is a growing body of literature on shielding POMDPs. The shield is a technique to disable the actions that can be executed by the agent and violate the shield definition. There are several shield definitions. Online methods [12], [13] in this category utilize Partially Observable Monte-Carlo Planning (POMCP) algorithm [14]. These works have the same problems we are solving in this paper: one way or another, the actions violating the shield definition participate in the planning procedure, yielding a suboptimal result. The work [13] enforces the shied outside the POMCP planning. As we further show, not considering safety in the future times, namely within the planning session, can lead to a suboptimal planning result.\nc) Chance Constrained (CC) Online Planning: A recent work [15] tackles online planning with chance constraints in an anytime setting. This paper suggests using a Neural Network (NN) to approximate CC enforced, with an adaptive threshold, from each belief considered in the planning session. This work trains NN offline. Therefore the error stemming from the discrepancy of simulated and real data is unknown. Moreover, it is not clear how complex the NN shall be to achieve zero loss in training to ensure no error in CC approximation, so even if no discrepancy discussed before exists, the NN inference may be slow. In this method, dangerous actions do not participate in the planning session.\nd) Safe control Under Partial Observability: There are a variety of robust control approaches natively tailored for continuous state/action/observation spaces [16],[17]. However, these methods are usually limited to very specific rewards/objectives and tasks, such as reaching a goal state or to be as close as possible to a nominal trajectory. Moreover, in both papers the system dynamics are control-affine. Without this assumption, it is not clear how to enforce the constraint through a derivative of the barrier function."}, {"title": "A. Contributions", "content": "Below we list down our contributions in the same order as they appear in the manuscript.\n\u2022 By constraining directly the problem space and not the dual space we present an anytime MCTS based algorithm for safe online decision making with safety governed by a Probabilistic Constraint (PC). Our approach enjoys anytime safety guarantees with respect to the belief-tree expanded so far and works in continuous state, action and observation spaces. When stopped anytime, the action returned can be considered as the best safe action under the safe future policy (tree policy) expanded so far. Our search tree solely consists of safe actions. We prove convergence in probability with an exponential rate of our approach.\n\u2022 Another contribution on our end is constraining the beliefs with incorporated outcome uncertainty stemming from an action performed by the robot and without incorporating the received observation. This is alongside the constraint over the posterior belief with included last observation. To the best of our knowledge, no previous works do that.\n\u2022 We also spot a problem happening in duality based approaches arising from averaging unsafe actions in MCTS phase. Therefore, an additional contribution of ours is an analysis of this phenomenon.\n\u2022 We simulate our finding on several continuous POMDP problems."}, {"title": "B. Notation", "content": "We use the \u25a1 as a placeholder for various quantities. The values in can be replaced by one of the respective options. We also extensively use the indicator function notation, which is 1A(). This function equals to one if and only if \u2208A. By lowercase letters we denote the random variables of their realizations depending on context. By the bold font we denote vectors of operators in time of different lengths. We denote estimated values by."}, {"title": "C. Paper Roadmap", "content": "This paper proceeds with the following structure. Section II presents relevant background. Section III then formulates the problem. Section IV presents our approach. Section VI discusses our baseline. Section VII gives experimental validation of the proposed methodology. Finally, Section VIII concludes the paper."}, {"title": "II. BACKGROUND", "content": "This section gives the background required for presenting our approach. Specifically, we discuss belief-dependent POMDP, its reformulation to Belief-MDP (BMDP), and the MCTS."}, {"title": "A. Belief-dependent POMDP", "content": "The POMDP is a tuple (X, A, Z, T, O, \u03c1, \u03b3, bo) where X, A, Z represent continuous state, action, and observation spaces with x\u2208X, a\u2208A, z\u2208Z the individual state, action, and observation, respectively. T(x', a, x)\u2252P\u03c4(x'|x, a) is a stochastic transition model from the past state x to the subsequent x' through action a, O(z, x)\u2252Po(z|x) is the stochastic observation model. \u03c1:B\u00d7A\u00d7Z\u00d7B\u2192R is a belief-dependent reward incurred as a result of taking an action a from the belief b, receiving and observation z' and updating the belief to b'. By B we denote the space of all possible beliefs. \u03b3\u2208(0, 1] is the discount factor, bo is the prior belief. Purely for clarity of the exposition we further assume that the reward depends solely on a pair of consecutive-in-time beliefs and an action in between. In addition we suppose \u03b3=1. To remove unnecessary clutter we assume that planning starts from bo. Extension to the arbitrary planning time is straightforward.\nLet he be a history. The history is the set that comprises the prior belief bo, the actions a0:l-1 and the observations z1:l that would be obtained by the agent up to time instance l such that hl \u2286 {bo, a0:l-1, z1:l}. We emphasize by the green color that bo is given, but the actions a0:l-1 and observations z1:l can vary. In addition due to the assumption that the planning session starts from the prior belief bo we can have only the future history simulated in planning in this work. For completeness we define ho \u2286 {bo}. The posterior belief bl is given by\n\\(b_l(x_l) \triangleq P(x_l | b_0, a_{0:l-1}, z_{1:l}) = P(x_l | h_l) = P(x_l | b_l).\\)                                                        (1)\nThe belief is a function of history such that we sometimes write b(h) instead of b(x) and use the corresponding h notation to point to the belief b(h). The actions within the history are coming from the execution policy. A deterministic policy \u03c0 is a sequence of functions \u03c0 = \u03c00:l-1 for l \u2208 [1...L-1], where the momentary function \u03c0l: B -> A Vi. In each time index, the policy maps belief to action. For better readability sometimes we will omit the time index for policy or denote \u03c00:l-1 as \u03c00+ and \u03c01:l-1 as \u03c01+. The policy can also be stochastic. In this case, it is a distribution of taking an action al from a bl belief  \u03c0l(al, bl) = \u03c0l(al, hl) = P(al | bl(hl)) = P(al | hl)\u00b9. Here the action space A is the space of outcomes and the mapping is \u03c0l: B \u00d7 A -> R. We have that \u03c00:L-1 = {P_l}^{L-1}_{l=0}. Yet, in hl we have a specific realization of actions of such a policy in previous time instances. When the agent performs an action a and receives an observation z', it shall update its belief from b to b'. Let us denote the update operator by \u03c8 such that b' = \u03c8(b, a, z'). In our context, it will be a Particle Filter (PF) since we focus on the setting of nonparametric beliefs. However, this is not an inherent limitation of our approach. Any belief update method would be suitable. We define a propagated belief b' as the belief b after the robot performed an action a and before it received and observation, namely by\n\\(b'_l(x_l) \triangleq P(x_l | h_{l-1}, a_{l-1}) = P(x_l | h_{\\overline{l}}) = P(x_l | b_{\\overline{l}}).\\)                         (2)\nWe define $\\overline{h_l} \triangleq h_{\\overline{l}} \\{z_l\\} = \\{b_0, a_{0:l-1}, z_{1:l-1}\\}$. The unconstrained, online decision making objective is the action-value function specified as\n\\(Q^{\\pi} (b_0, a_0; \\rho) \triangleq E[\\rho_1 (b_0, a_0, b_1) + V^{\\pi} (b_1; \\rho_2) |b_0, a_0].\\)                  (3)\nHere the we added the subscript to the reward \u03c1\u25a1+1(b, bl+1) to emphasize that it is a random variable and it is allowed not to specify dependency on consecutive-in-time beliefs and the"}, {"title": "B. Belief State MDP", "content": "To employ solvers crafted for fully observable Markov Decision Processes (MDP) we can cast POMDP as a Belief-MDP (BMDP). The BMDP is a following tuple (B, A, T\u266d, \u03c1, \u03b3, bo), where B is the space of all possible beliefs defined by (1). The belief state transition model follows\n\\(T_\\mathbb{B}(b, a, b') \\triangleq P_{T_\\mathbb{B}}(b'|b, a) = \\int_{z' \\in Z} P(\\hat{b}'|b, a, z') P(z'|b, a) dz' \\triangleq \\int_{z' \\in Z} \\delta(b' - \\psi(b, a, z')) P(z'|b, a) dz'.\\)         (6)\nThe next section describes SOTA approach to solve unconstrained continuous POMDP online, namely MCTS. There we deal with estimators of the (3) and (4). We denote estimated values by \u02c6.\nFurther, we shorten the notation and mark V**(bl; \u03c1') by V**(hl) and Q**(bal; \u03c1') by Q(hal). We will use the dependence on history h and the corresponding belief b(h) interchangeably since the history h defines the location in the belief tree as opposed to the belief which possibly can be identical for more than single history. It will be clarified in the next section. In the next section we will see why in time zero we have deterministic policy and in future time the policy is stochastic."}, {"title": "C. Monte Carlo Tree Search", "content": "MCTS constructs the search tree comprised by belief nodes (transparent circles) and belief-action nodes (black squares), by iteratively descending down the tree and ascending back to the root (See Fig. 1 and 2). On the way down the tree, the exploration mechanics selects an action. The Double Progressive Widening (DPW) manages the sampling of new actions and observations. On the way back to the root MCTS updates action value estimates at each belief action node (Fig. 2a) and relevant visitation counts. In the case of belief-dependent rewards, beliefs represented by particles and continuous setting of states, actions, and observations, MCTS is applied on the level of Belief-MDP (BMDP) and called Particle Filter Tree with DPW (PFT-DPW) [18]. DPW solves the problem of shallow trees in a continuous setting. This problem arises because in this setting it is impossible to sample the same action and observation twice. The DPW technique enables gradually expanding new actions and observations as the tree search progresses. With a slight abuse of notation, we sometimes switch the dependence of various quantities on belief and dependence on the corresponding history. This is because same belief can correspond to different histories. Therefore to properly mark the position at the search tree we shall use history h instead of belief b(h). The exploration score is defined as\n\\(sc(h, a) = \\overline{Q(h, a)} + k\\sqrt{f(n(h))/n(ha)}\\)\n(7)\ngoverns the selection of the actions down the tree, where n(h) is the visitation count of the belief nodes, n(ha) is the visitation count of belief-action nodes and k is the exploration constant (Fig. 2b). The notation ha is the history h with"}, {"title": "A. Problem Formulation", "content": "Our aim is to tackle the problem presented in [3] and [4] narrowed to the multiplicative form of the inner constraint considering a stochastic future policy. In [3] and [4] we presented our Probabilistic Constraint (PC) defined as such P(c=1|bo, \u03b1\u03bf, \u03c0)=1 where c is a Bernoulli random variable. In this work c maps to one the event $\\cap_{l=0}^L \\mathbb{A}_l$ such that the problem we want to solve is\n\\(a_0 \\in arg max_{a_0 \\in \\mathbb{A}} Q^{\\pi} (b_0, a_0; \\rho_1) \\qquad subject to\\)\n(9)\n\\(P(\\cap_{l=0}^L \\mathbb{A}_l | b_0, a_0, \\pi_{1:L-1}) = 1 \\qquad \\text{ outer constraint}.\\)\n(10)\nIn this paper, we define the following sets as said $\\mathbb{A} \\triangleq \\{b_0: \\phi(b_0) \\geq \\delta\\}$ and for l \u2208 [1:L] the relevant set appears as\n\\$\\mathbb{A}_l \\triangleq \\{b_{\\overline{l}}, b_l: b_{\\overline{l}} \\in \\mathbb{B}, b_l \\in \\mathbb{B}_l, \\phi(b_{\\overline{l}}) \\geq \\delta, \\phi(b_l) \\geq \\delta\\}.\\$\n(11)\nOne example of an operator \u03c6 is the probability to be safe given belief, specified as:\n$\\phi(b_l) = P(\\{x_l \\in \\mathbb{X}_{safe}\\} | b_l) = E_{x_l \\sim b_l} [1_{\\{x_l \\in \\mathbb{X}_{safe}\\}}]$\n(12)\n$\\phi(b_{\\overline{l}}) = P(\\{x_{\\overline{l}} \\in \\mathbb{X}_{safe}\\} | b_{\\overline{l}}) = P(\\{x_{\\overline{l}} \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}).$\n(13)\nHere, Xsafe is the safe space, e.g. the space where a robot can move without inflicting damage on itself. Therefore, we can think about the event $\\cap_{l=0}^L \\mathbb{A}_l$ as the Safe Belief Space.\nThe Bl and Bl in (11) are the reachable spaces in time l of propagated beliefs bl and posteriors bl respectively. The reachable space in time l is the space of all the beliefs in time l that can be reached from a belief given in planning session, using the stochastic execution policy \u03c0 and changing the actions and the observations in (1) and (2) accordingly. In our case, the belief given in planning session is bo. By the green color in (11) we highlight that we constrain the propagated beliefs in addition to the posteriors.\nThe probability of the event $\\cap_{l=0}^L \\mathbb{A}_l$ equals to the probability of the event $\\(1_{\\mathbb{A}_0} (b_0) \\Pi_{l=1}^L 1_{\\mathbb{A}_l} (b_{\\overline{l}}, b_l) = 1\\)$. In this work, although we use Particle Filter (PF) as the belief update we do not take into account the stochasticity of the belief update operator as opposed to [21],[22] and treat \u03c6 operator as deterministic. Since it would significantly complicate the paper, we leave this aspect to the future work.\nOne can extract the propagated belief from the belief update \u03c8, namely \u03c8(b, a, z') = \u03c8post(\u03c8prop (b, a), z'). Therefore, to make the exposition clearer, from now on the indicator $1_{\\mathbb{A}_l} (b_{\\overline{l}})$ depends solely on the posterior bl and not both the posterior bl and the propagated belief b\u012b. Note that in algorithms, for the sake of clarity, we make the indicators dependent on both beliefs, propagated and posterior.\nThe \u03c01:L-1 is the best future exploratory stochastic policy approximated by our probabilistically-constrained MCTS as we will further see. The approximation of the best future tree policy improves over time as proved by [20] for an unconstrained problem. In our problem, instead of the best future stochastic tree policy, we have the best future stochastic probabilistically-constrained policy. This is because our PC"}, {"title": "B. Implications of Constraining Propagated Belief", "content": "In this section we shed light on the question what does it mean to constrain the propagated beliefs alongside with posterior beliefs. To cancel the constraining of the propagated beliefs one must redefine the set Al for every l as follows\n$\\mathbb{A}_l \\triangleq \\{b_{\\overline{l}}, b_l: b_{\\overline{l}} \\in \\mathbb{B}, b_l \\in \\mathbb{B}_l, \\phi(b_{\\overline{l}}) \\geq \\delta, \\phi(b_l) \\geq \\delta\\} .$\nFurther in the paper all the developments are valid for both versions of the set A. The probability to be safe given a propagated belief equals to\n$P(\\{x_l \\in \\mathbb{X}_{safe}\\} | b_{\\overline{l}}) = P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}) = \\int_{z \\in Z} P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}, z) P(z|h_{\\overline{l}}) dz = E_z [P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}, z) | h_{\\overline{l}}] = E_z [P(\\{x_l \\in \\mathbb{X}_{safe}\\} | b_l) | b_{\\overline{l}}].$\n(15)\nThe theoretical expectation in (15) is out of the reach. Yet we evaluate it using the propagated belief b\u00af(h\u00af). Defining the set Al as (11), with the propagated beliefs, allows to account for all the possible posterior beliefs in (15). Additionally, we know that \u2200e>0\n$\\lim_{|\\mathbb{C}(h_{\\overline{l}})| \\to \\infty} P(|P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}) - \\frac{1}{|\\mathbb{C}(h_{\\overline{l}})|} \\Sigma_{z \\in \\mathbb{C}(h_{\\overline{l}})} P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}, z)| > e | h_{\\overline{l}}) = 0.$\n(16)\nWith a slight abuse of notation, $\\mathbb{C}(h_{\\overline{l}})$ is now a list of the enumerated observations that are children of $\\mathbb{H}_{\\overline{l}}$. Equation (16) means that for any arbitrary small error \u03b5, the difference between (15) and its approximation by the children of $\\mathbb{H}_{\\overline{l}}$ tends to zero as the number of children of $\\mathbb{H}_{\\overline{l}}$ grows.\nTheorem 1 (Necessary condition for entire observation space Z of children of $h_{\\overline{l}}$ to be safe): Fix \u03b4 \u2208 [0, 1] and assume that\n$P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}) \\geq \\delta.$\n(17)\nEq. (17) is a necessary condition for the entire observation space Z of children of hl to be safe. To rephrase that\n$P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}) < \\delta$\n(18)\nimplies that bl(hl) a child of $h_{\\overline{l}}$ which is not safe, namely,\n$P(\\{x_l \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}, z_l) < \\delta.$\nNote that if for every sampled observation P({x \u2208Xsafe}|h\u0113, zl)\u2265d, it implies that $\\frac{1}{|\\mathbb{C}(h_{\\overline{l}})|}(\\Sigma_{z_l \\in \\mathbb{C}(h_{\\overline{l}})} P(\\{x \\in \\mathbb{X}_{safe}\\} | h_{\\overline{l}}, z_l)) \\geq \\delta.$\n(19)\nTo conclude, by constraining the propagated belief, we constrain the theoretical expectation of the posteriors given ho, and by constraining each posterior we also constrain its sample approximation portrayed by Eq. (19). Without constraining the propagated belief, if the number of children of by (h) is small, namely, |C(h\u012b)| is small, we anticipate poor robot's safety in execution of the best action found by our planner (e.g. number of collisions). This is because constraining the propagated belief allows to account in expectation for all the observations in the observation space, and not only the sampled observations. This will happen if the number of MCTS tree queries is small.\nIt is possible that other definitions of safety of the beliefs can be utilized. While this is outside the scope of this paper, we specified relevant operators \u03c6 in the Appendix, Section D. Remark: To assure feasibility of our PC (10) at the limit of MCTS convergence, the robot has to have a bounded support of the belief bo and bounded motion models. If we deal with a particle based representation of bo we perceive the particles as true robot positions, so it is left only to assure that the motion model is bounded. This is, however, natural since the robot cannot have limitless actuators."}, {"title": "IV. PC-MCTS (ANYTIME APPROACH)", "content": "Our constraint depends on a stochastic policy. Similar to the objective (5) in our PC we land at the following result.\nTheorem 2 (Representation of PC, recursive form): The PC defined by (14) conforms to the following recursive form.\n$1_{\\mathbb{A}_0} (b_0) E_{z_1} E_{a_1 \\sim \\pi_1(b_1)} E_{b_2} E_{a_2 \\sim \\pi_2(b_2)} ...\nE_{b_{L-1}} E_{a_{L-1} \\sim \\pi_{L-1}(b_{L-1})} [1_{\\mathbb{A}_{L-1}} (b_{\\overline{L-1}}, b_{L-1}) | b_1, a_1] ... | b_1, a_1, b_0, a_0] =\n1_{\\mathbb{A}_0} (b_0) E[\n1_{\\mathbb{A}_1}\n\\mathbb{E}[1_{\\mathbb{A}_l} (b_{\\overline{l}}, b_l) P(\\cap_{l=1}^{L} 1_{\\mathbb{A}_l} (b_{\\overline{l}})=1 / b_l, a_l, \\pi)] | b_1, \\pi]\n(20)\nIn this section, we present our anytime safety approach. To invalidate the sample approximation of (10) it is sufficient that a single belief (propagated or posterior) in the belief tree fails to be safe and the corresponding indicator is zero. In our methodology, we leverage the classical iterative MCTS scheme of descending down the search tree of histories and"}, {"title": "V. THE ALGORITHMS AND GUARANTEES", "content": "This section describes our algorithms followed by convergence guarantees. Alg. 2 summarizes our main result. Similar to [18], we present a provable modified variant summarized by Alg. 5."}, {"title": "VI. SOTA CONTINUOUS CONSTRAINED MCTS", "content": "We now firm up the loose ends and turn to the description of the existing constrained POMDP considered in an anytime setting which will serve as our baseline."}, {"title": "A. Expectation Constrained Belief-dependent POMDPS", "content": "The averaged constraint formulated with payoff operator and including the propagated beliefs would be\n$E_{\\tau, O} [\\Sigma_{l=0}^{L-1} \\theta (b_{\\overline{l}}, b_l)|b_0, \\pi] \\geq \\delta.$\n(38)\nOne possibility is to define \u03b8(b\u012b, bl)=\u03c6(b\u012b)+\u03c6(bl). Clearly the cumulative averaged formulation (38) is not suitable for safety since it permits deviations of the individual safety operators \u03c6. It can happen that with the low probability of future observation the resulting posterior belief will be extremely unsafe. However, sometimes the operator \u03c6 is naturally bounded from above. It holds that P({x\u2208Xsafe}|\u25a1) 0$. This will assure that (39) is satisfied if and only if all (b\u0113, bl) inside are zero. This is because P({x\u2208Xsafe})\u22650. In the light of the discussion about deviation of the cost values, further in this paper we assume that 80=0. Now, if we set \u03b4=1-\u03b40=1 in our PC (10) and payoff as in (12) two formulations are equivalent. Yet, this will happen solely with payoff operator being as in (12), cost as in (41) and \u03b4=1. Another possibility is to define cost in (39) as\ndebe, be)1-1 be, be)\nAbe, bebe B, be\u2208Be, $(b\u012b)\u2265\u0431, \u0444(be)\u2265\u0431}\n\n(42)\nWe obtain that the (39) is satisfied if and only if our PC (10) is satisfied and in both formulations we have the freedom to select operator \u03c6 and \u03b4 (we still need to assure that the \u03b4 is the same in both formulations). Importantly, unlike the cost from (41), the cost from (42) can not be represented as expectation over the state dependent cost. This cost is general belief dependent operator even if the payoff inside is as (12). Remark: In general the transition between cost constraint and payoff constraint is not trivial. To do that one must use the linearity of the expectation and the relation between the cost and payoff operators."}, {"title": "B. Duality Based Approach", "content": "We now turn to the discussion about duality based approach in continuous spaces suggested in [5]. Suppose that 80=0 in (39); The iterative scheme of duality based approach subsumes two steps iteratively solving the following objective\n$\\max_{\\pi} \\min_{\\lambda \\geq 0} (V^{\\pi} (b_0; \\rho_1) - \\lambda V^{\\pi} (b_0; \\theta_0)),$\n(43)\nwhere one step minimizes for \u03bb and another maximizes for execution stochastic policy \u03c0. Here, \u03b80 is a vector of cost operators (starting from time 0). The Dual ascend goes towards"}, {"title": "A. Problems Composition", "content": "We present the Safe Lidar Roomba problem.\na) Safe Lidar Roomba: Roomba is a robotic vacuum cleaner that attempts to localize itself in a familiar room and reach the target region. The POMDP state is the position of the agent x, its orientation angle \u03b8, and the status. The status is a binary variable and it tells of whether the robot has reached goal state or stairs. The Roomba action space is defined as A={a1, a2, a3, a4,a5, a6}. The action space A comprises the pairs (\u03c5,\u03c9", "\u03c9": "."}]}