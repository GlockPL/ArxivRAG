{"title": "Hound: Hunting Supervision Signals for Few and Zero Shot Node Classification on Text-attributed Graph", "authors": ["Yuxiang Wang", "Xiao Yan", "Shiyu Jin", "Quanqing Xu", "Chuanhui Yang", "Chuang Hu", "Yuanyuan Zhu", "Bo Du", "Jiawei Jiang"], "abstract": "Text-attributed graph (TAG) is an important type of graph structured data with text descriptions for each node. Few- and zero-shot node classification on TAGs have many applications in fields such as academia and social networks. However, the two tasks are challenging due to the lack of supervision signals, and existing methods only use the contrastive loss to align graph-based node embedding and language-based text embedding. In this paper, we propose HOUND to improve accuracy by introducing more supervision signals, and the core idea is to go beyond the node-text pairs that come with data. Specifically, we design three augmentation techniques, i.e., node perturbation, text matching, and semantics negation to provide more reference nodes for each text and vice versa. Node perturbation adds/drops edges to produce diversified node embeddings that can be matched with a text. Text matching retrieves texts with similar embeddings to match with a node. Semantics negation uses a negative prompt to construct a negative text with the opposite semantics, which is contrasted with the original node and text. We evaluate HOUND on 5 datasets and compare with 13 state-of-the-art baselines. The results show that HOUND consistently outperforms all baselines, and its accuracy improvements over the best-performing baseline are usually over 5%.", "sections": [{"title": "1 Introduction", "content": "Text-attributed graph (TAG) [37, 42, 45] is a prevalent type of graph-structured data, where each node is associated with a text description. For instance, in a citation network, the papers (i.e., nodes) are linked by the citation relations (i.e., edges), and the abstract of each paper serves as the text description. Few-shot and zero-shot node classification on TAGs (FZNC-TAG) predict the categories of the nodes using a few or even no labeled data since labeled data are expensive to obtain [16, 18, 31, 32, 47]. The two tasks have many applications in areas such as recommender system [7, 30], social network analysis [41, 46], and anomaly detection [3, 22].\nExisting methods for FZNC-TAG typically follow a two-step process: first learn node and text embeddings on the TAGs and then use prompting to produce classification results [12, 33]. They mainly differ in embedding learning and can be classified into three categories. Some methods use pre-trained language models (PLMs) such as Bert [4], ROBERTa [17], and GPT [23] to generate text embeddings. They exploit the text but ignores the information in the graph topology. Some methods encode the text using PLMs and add the text embedding as additional node features. Then, semi-supervised graph neural network (GNN) methods, e.g., TextGCN [39] and GraphSAGE [8], are used to train the node embeddings. The limitation of these approaches is that the PLMs"}, {"title": "2 Preliminaries", "content": "Text-attributed graph. We denote a text-attributed graph (TAG) as G = (V, &, X), in which V, &, and X are the node set, edge set, and text set, respectively. Take citation network as an example for TAG. Each node $v_i \\in V$ is a paper, interconnected by the edges $e \\in \\&$ that signify citation relations. Let $x_i \\in X$ denote the text description (i.e., paper abstract) of the i-th node. Each node has a ground-truth label to indicate the topic of the paper. Since the graph nodes and papers have a strict one-to-one correspondence, node $v_i$ and text $x_i$ share an identical label.\nFew- and zero-shot learning. For few-shot classification, the test dataset encompasses a support set S and a query set Q. S comprises C classes of nodes, with K labeled nodes drawn from each class. These nodes can be used to train or fine-tune the classifier, which is then utilized to classify the nodes in Q. This is also called the C-way K-shot classification problem [6]. Zero-shot node classification is essentially a special case of few-shot classification with K = 0. There"}, {"title": "3 The HOUND Framework", "content": "In this section, we present a novel pre-training and prompting framework, named HOUND, designed for the TAGs under the few- and zero-shot setting. We start with a framework overview and follow up with the detailed descriptions of its components."}, {"title": "3.1 Overview", "content": "The overall architecture of our framework is illustrated in Figure 2. The pre-training model for few-shot consists of a graph encoder and a text encoder, and an extra negative text encoder is included for zero-shot pre-training. We introduce them as follows.\n\u2022 Graph encoder $\\phi$. We adopt a graph neural network as the encoder to generate the node embedding n.\n\u2022 Text encoder $\\psi$. We choose Transformer [27] as the text encoder, and it produces a text embedding t for each text description.\n\u2022 Negative text encoder $\\psi_{neg}$. This maintains the same architecture and inputs as the text encoder, with the difference that we train it independently with a negative prompt to generate the negative text presentation $t_{neg}$.\nTo effectively train the above encoders, we design three novel loss functions: node perturbation loss, text matching loss, and semantics negation loss, which can assist the pre-training model in acquiring more supervision signals. Then, we detail the basic paradigm for few- and zero-shot node classification. Finally, we propose a strategy in Section 3.3, probability-average, to enhance zero-shot node classification by merging the probabilities produced from both the text encoder and the negative text encoder outputs."}, {"title": "3.2 Supervision Signals", "content": "The current methods [12, 33, 44] neglect supervision signals within the graph and text modalities during the pre-training phase. Therefore, in this section, we introduce three novel augmentation techniques: node perturbation, text matching, and semantics negation, to provide more nodes for each text and vice versa.\nNode perturbation loss. The prior research [33] solely contrasts the original node (without perturbation) with text t (i.e., Equation (2)). However, it fails to fully exploit the supervision signals in the graph modality. To solve this issue, as shown in Figure 2(2), we propose node perturbation to introduce more nodes for text embeddings. Specifically, we generate multiple perturbed nodes by randomly removing or adding a portion of edges, and then maximize the similarity between the text embedding $t_i$ and the perturbed node embedding $\\hat{n_i}$. The rationale behind the augmentation technique is that when the node perturbation is applied, the corresponding prior of the node data distribution is injected, forcing the text encoder to learn an embedding that is invariant to the node perturbation. The benefits of this are intuitive: first, the model does not change the classification results due to minor changes in topology; second, the node perturbations provide the text with more pairs of samples, facilitating the text to learn a more generalized embedding. The node perturbation loss can be represented as follows:\n$L_{NP} = \\frac{1}{|B|} \\sum_{(n_i,t_i) \\in B} \\log \\frac{exp(sim(n_i, t_i)/\\tau)}{\\sum_{j\\neq i}exp(sim(\\hat{n}_i, t_j)/\\tau)},$ (3)\nwhere $\\hat{n_i} = \\phi(\\zeta(v_i))$ is generated by the graph encoder with a perturbed node as input, and $\\zeta(\\cdot)$ is the augmentation function.\nNote that a similar data augmentation operation is performed in graph contrastive learning [40, 49]. It is used to generate diverse nodes and mitigate over-smoothing resulting from the shallower GNN structure. Differently, our objective is to provide more perturbed nodes for the texts to address the lack of supervision signals in the few- and zero-shot classification. Thus, the application scenarios and purposes of these two approaches differ significantly.\nText matching loss. In addition to providing multiple node embeddings for text embeddings, in turn, we also provide more text embeddings for each node embedding. G2P2 [33] defaults to only one text embedding similar to each node embedding, however there may be multiple similar texts to the target node in TAGs [2, 9, 43]. Therefore, we search for multiple text embeddings that are similar to the text embedding of the target node and subsequently encourage the target node embedding to align with these similar text embeddings $\\hat{t}$. The text matching loss is denoted as follow:\n$L_{TM} = \\frac{1}{|B|} \\sum_{(n_i,t_i) \\in B} \\log \\frac{exp(sim(n_i, t)/\\tau)}{\\sum_{j\\neq i}exp(sim(n_i, t_j)/\\tau)},$ (4)\nwhere K is the number of similar text embeddings.\nHowever, the above method has two serious drawbacks: first, the complexity of violently searching for similar text embeddings among all text embeddings is unacceptable; second, storing all text embeddings in GPU memory may lead to out-of-memory. To address these issues, we create a text bank with a capacity of 32K to model the whole text embedding space. As illustrated in the Figure 2(3), whenever a new batch of data arrives, the earliest text"}, {"title": "Semantics negation", "content": "After co-training the graph and text encoder using Equations (2), (3) and (4), the model now possesses the base capability to distinguish node-text pairs. However, understanding the negative semantics within the input text description poses a challenge for the model. For example, we represent a text description such as \"a paper is published at KDD\u201d and its negation \u201ca paper is not published at KDD\". In the embedding space, these two descriptions are likely to be very similar, as their raw texts differ by only one word. To address this issue, we employ negative prompts to generate multiple negative texts that are semantically opposed to the original text descriptions. These negative texts are then used to train a negative text encoder independently. This process helps the negative text encoder learn parameters that are contrary to those of the text encoder. This augmentation technique generates an additional negative text for the nodes and texts, providing more semantics supervisions to make the classification robust. Next we detail the design of negative prompts and negative text encoders.\nOur initial idea is to manually construct a series of negative texts. Specifically, we alter the text descriptions by incorporating negation terms such as \u201cno\u201d, \u201cnot\u201d, \u201cwithout\u201d, etc., thus creating a negative text corpus that are semantically opposite to the original ones, denoted as $X_{neg}$. Then, we input the negative text $x_{neg}$ into negative text encoder $\\psi_{neg}$ to generate negative text embedding $t_{neg}$, as denoted below:\n$t_{neg} = \\psi_{neg}(x_{neg}), x_{neg} \\in X_{neg}.$ (5)\nHowever, manual modification of the raw text is time-consuming and labor-intensive. To solve this problem, inspired by CoOp [48], we propose a learnable negative prompt h and add it to the front of raw text. The underlying logic is to represent negative semantics by constantly optimizing the learnable h, thereby mirroring the hand-crafted negative texts. Specifically, we splice the text description"}, {"title": "Negative prompt", "content": "with M learnable vectors and then input it into the negative text encoder $\\psi_{neg}$, denoted as follows:\n$h = [ V_1, V_2, ...V_M, x], t_{neg} = \\psi_{neg}(h_{neg}),$ (6)\nwhere the negative text encoder is a transformer [27] with the same architecture as the text encoder.\nThere is still an unsolved problem: how do we train a negative text encoder? In other words, how do we ensure that the semantics of the negative text embeddings contradict the original text embeddings. To address this, we design two novel loss functions: margin loss and semantics-opposite loss.\nThe margin loss anticipates the greatest possible similarity between positive pairs, and conversely, it expects dissimilarity in the case of negative pairs. As shown in Figure 2(5), given a target node $v_i$ (i.e., \"a conference paper published at KDD\u201d), the corresponding negative text description $t_i$ (i.e., \"it's not a conference paper published at KDD", "it's not a journal paper published at TPAMI": "are considered positive texts. Subsequently, we employ margin loss to assess the degree of matching between the target nodes, positive texts, and negative texts. Specifically, margin loss ensures that the similarity between the target node embedding and the positive text embedding is at least a margin higher than the similarity with the negative text embedding. We maintain a margin of up to m with no additional benefits for further widening this gap. The margin loss $L_{ML}$ is denoted as follows:\n$L_{ML} = max(0, m + sim(n_i, t_{neg}) \u2013 sim(n_i, t_{tr})).$ (7)\nAs shown in Figure 2(4), semantics-opposite loss seeks to maximize the mean square error between positive and negative text embeddings. As text $x_i$ and negative text $t_{neg}$ are semantically opposite, their corresponding embeddings should be as far apart as possible in the text embedding space. We compute the semantics-opposite loss as follow:\n$L_{SO} = \\frac{1}{|B|} \\sum_{t_i \\in B}||n_i - t_{neg}||^2,$ (8)"}, {"title": "Total loss", "content": "where $|| \\cdot ||_2$ is the $L_2$ norm. Thus, the semantics negation loss is equal to the sum of margin loss and semantics-opposite loss, denoted as $L_{SN} = L_{ML} + L_{SO}$. It enforces both the node and text embeddings are dissimilar to the corresponding negative text embedding.\nIn summary, we denote the total loss of our HOUND as:\n$L = L_{CL} + \\alpha L_{NP} + \\beta L_{TM} + \\gamma L_{SN},$ (9)\nwhere $\\alpha, \\beta$ and $\\gamma$ are the hyperparameters used to balance the loss. In few-shot pre-training, we do not activate the semantic negation loss (i.e., $\\gamma = 0$) because the prompts in few-shot are inherently learnable, incorporating negative prompts would introduce more noise and lead to sub-optimal performance. In contrast, zero-shot classification lacks labeled data during the pre-training. Thus, we activate the semantics negation loss to provide more supervision signals (i.e., $\\gamma = 1$). Overall, in the total loss L, we only modify $\\alpha$ and $\\beta$, which does not require extensive hyperparameter tuning. We analyze the ablation experiments on the loss function in detail in Section 4.3.\nComplexity Analysis. Our pre-trained model incorporates both a GNN and a Transformer. The GNN takes O(LNd\u00b2) time for aggregating the neighboring nodes, where L is the network depth, N is the number of nodes and d is the number of dimensions. The Transformer's time complexity is $O(sd^2 + s^2d)$, where s the maximum length of the input sequence. $O(sd^2)$ time is used for mapping vectors at each position to query, key and value vectors, and $O(s^2d)$ time is utilized for the computation of the attention score. Consequently, the overall time complexity of our method is $O(LNd^2+sd^2+s^2d)$. The state-of-the-art G2P2 also contains a GNN and Transformer, so the time complexity of our method is comparable to its. In this paper, computations during pre-training are performed in batches, where the number of batch $|B| << N$. Thus the actual complexity is significantly lower than $O(LNd^2 + sd^2 + s^2d)$."}, {"title": "3.3 Prompt Tuning and Inference", "content": "Based on the pre-trained model, we tune the model parameters to adapt to the classification tasks. However, there are two limitations inherent in the conventional pre-training and fine-tuning paradigm [19, 25, 26]. Firstly, it requires labeled data for training a prediction head, such as an MLP layer. Secondly, it requires fine-tuning the enormous parameters of pre-trained model. These issues can be solved through few- and zero-shot learning, which enables classification with a few even no labeled samples while concurrently freezing the pre-trained model's parameters. Next, we introduce the foundational paradigm of few- and zero-shot node classification.\nZero-shot classification. In the zero-shot setting, we operate without any labeled samples and rely solely on class name description. To perform C-way node classification, we construct a series of class descriptions $\\{D_c\\}_{C=1}$, via discrete prompts, such as \u201ca paper of [class]\u201d. Then, we input the description text into the pre-trained text encoder to generate the class embedding $g_c = \\psi(D_c)$. We predict the category of a node $v_i$ by computing the similarity between the node embedding $n_i$ with the class embedding $g_c$. The insight behind this is that we align the pre-training and prompting objectives (i.e., to determine whether nodes and texts are similar). Thus, we do not have to tune the parameters of the pre-trained model. The similarity probability between the target node and the candidate"}, {"title": "Calculated probabilities", "content": "class description is calculated as follows:\n$p_i = \\frac{exp(sim(n_i, g_c)/\\tau)}{\\sum_{c=1}^C exp(sim(n_i, g_c)/\\tau)}$ (10)\nFew-shot classification. In the few-shot setting, we conduct a C-way K-shot classification task. Unlike discrete prompts (i.e., \u201ca paper of...\") in the zero-shot setting, we have C \u00d7 K labeled samples to train learnable prompts. Specifically, we construct a continuous prompt ge by adding M learnable vectors to the front of the class description Dc. Formally, we denote gc = ([e1, e2, ..., eM, Dc]). Then, we use Equation (10) to predict the node category, and update the continuous prompts by minimizing the discrepancy between the predicted and ground-truth labels via cross-entropy loss. It is worth noting that because CX K is a small value, the parameters required to fine-tune the prompts are considerably less than those needed for the pre-trained model.\nProbability-average. As shown in Figure 3, we propose probability-average to predict node category. Specifically, we first compute $p_i$ by Equation (10). We use the negative text encoder to generate the negative class embedding. Then, we compute negative probability $p_i^{neg}$ by contrasting these negative class embeddings with the target node embedding using Equation (10). $p_i$ denotes the probability that a node belongs to each category and vice versa, $p_i^{neg}$ represents the probability that a node does not belong to each category. Finally, we utilize $(p_i + 1 \u2212 p_i^{neg})/2$ to predict the node label. Unlike using a single text encoder, integrating a negative text encoder provides additional evaluation metric. This strategy balances the output probabilities of the positive and negative text encoders, thereby enhancing classification accuracy. Formally, the probability-average strategy can be denoted as follows:\n$\\Upsilon_i = arg \\underset{i}{max} (p_i + 1 \u2212 p_i^{neg})/2.$ (11)\nNote that the probability-average stragety is only applicable to zero-shot classification, as it requires the negative prompts and negative text encoder to calculate $p_i^{neg}$. In contrast, few-shot classification directly uses $p_i$ to predict node categories. This is because few-shot classification can learn a prompt from labeled samples, and the additional introduction of negative prompts introduces noise and may reduce accuracy.\""}, {"title": "4 Experimental Evaluation", "content": "In this section, we conduct extensive experiments to evaluate HOUND and answer the following research questions.\n\u2022 RQ1: How does HOUND Compare with state-of-the-art methods in the accuracy for few- and zero-shot classification on TAGs?\n\u2022 RQ2: Do our augmentation techniques improve accuracy?\n\u2022 RQ3: How efficient is HOUND in terms of training and inference?"}, {"title": "4.1 Experiment Settings", "content": "Datasets. Following related researches [21, 36], we use the 5 datasets in Table 1 for experiments. Cora [20] is a citation network, where papers are linked by citation relations and abstract serves as the text. Art, Industrial, M.I., and Sports are derived from Amazon product categories [36], namely, arts, crafts and sewing for Art; industrial and scientific for Industrial; musical instruments for M.I.; and sports-fitness for Fitness, respectively. For the four datasets, an edge is added to construct the graph if a user visits two products successively, and the text is the product description. The five datasets cover different scales (from thousands to millions of nodes) and number of classes (from tens to thousands).\nBaselines. We compare HOUND with 13 baselines from 5 categories.\n\u2022 End-to-end GNNs: GCN [13], SAGEsup [8], TextGCN [39]. They are trained in a supervised manner for the classification tasks.\n\u2022 Pre-trained GNNs: GPT-GNN [11], DGI [29], SAGEself [8]. They are first pre-trained via self-supervise learning and then fine-tuned for the classification tasks.\n\u2022 Graph prompt methods: GPPT [25], GFP [5], GraphPromt [19]. They reduce the divergence between the pre-training and downstream tasks by designing the training objectives and prompts.\n\u2022 Language models: BERT [4], RoBERTa [17], P-Tuning v2 [15]. They are first pre-trained and then fine-tuned on the text.\n\u2022 Co-trained model: G2P2 [33]. It employs the contrastive loss to train the GNN and language model jointly such that they produce similar embeddings for node-text pairs.\nFollowing G2P2 [33], we use classification accuracy and F1 score to measure performance. We report the average value and standard deviation across 5 runs. Note that we only select language models and G2P2 as the baselines for zero-shot classification, since the other baselines require at least one labeled sample per class for either training or fine-tuning.\nTask configurations. For few-shot classification, we use a 5-way 5-shot setup, i.e., 5 classes are taken from all classes, and then 5 nodes are sampled from these classes to construct the training set. The validation set is generated in the same way as the training"}, {"title": "4.2 Main Results (RQ1)", "content": "Few-shot node classification. Table 2 reports the accuracy of HOUND and the baselines for few-shot node classification. The results show that HOUND consistently outperforms all baselines across the datasets, with an average improvement of 4.6% and 6.9% for classification accuracy and F1 score, respectively. Moreover, the improvements of HOUND over the baselines are over than 5% in 6 out of the 10 cases. On Cora, the improvements of HOUND are smaller than the other datasets because Cora is the smallest among the datasets, and thus existing methods learn relatively well.\nRegarding the baselines, end-to-end GNNs have the lowest accuracy since they are trained with only a few labeled nodes. Pre-trained GNNs outperform end-to-end GNNs because they employ self-supervised pre-training [40, 49], suggesting that supervision signals are important. Graph prompt methods only utilize the graph structures and neglect the text descriptions. Conversely, language models only use the text descriptions and ignore the graph structures. G2P2 [33] jointly trains the GNN and language model using both the graph structures and text descriptions, and thus it achieves the best performance among the baselines. Nonetheless, HOUND outperforms G2P2 because it introduces more supervision signals with our augmentation tecnqiues, which help to generate more robust and informative embeddings.\nZero-shot node classification. Table 3 reports the accuracy of HOUND and the baselines for zero-shot node classification. We only include the language models and G2P2 because the other methods require at least one labeled sample for each class. We also enhance BERT and ROBERTa as BERT* and RoBERTa* by re-tuning them on the text descriptions of the evaluated datasets (i.e., the datasets in Table 1) to tackle domain mismatch.\nThe results show that HOUND consistently outperform all baselines by a large margin. Compared with the best-performing baseline G2P2, the average improvements of HOUND in classification accuracy and F1 score are 8.8% and 9.3%, respectively. All methods have lower accuracy for zero-shot classification than few-shot classification because zero-shot classification does not provided labeled samples, and thus the task is more challenging. However, the improvements of HOUND are larger for zero-shot classification because it introduces more supervision signals for learning."}, {"title": "Robustness", "content": "Robustness to task configuration. We conduct the fewer-way and fewer-shots classification on M.I. dataset. In Figure 4, we experiment with 3-way and 5-way classification and change the number of"}, {"title": "4.3 Micro Experiments", "content": "Effect of the augmentations (RQ2). In Table 4, we conduct an ablation study by trying different combinations of our three augmentations techniques. We make the following observations.\n\u2022 The augmentations are all effective in improving accuracy, as adding each of them individually outperforms the baseline LCL.\n\u2022 The best-performing combination for few-shot classification deactivates semantic negation while zero-shot classification actives semantics negation. This is because few-shot classification uses labeled samples to learn the prompt, and the negative prompt learned by semantics negation may interfere with prompt tuning. In contrast, zero-shot classification lacks labeled data for prompt tuning, and the negative prompt helps to provide more supervision signals and improve robustness.\n\u2022 Text matching and node perturbation should not be utilized jointly. This may be because using both of them introduces too many node-text pairs (i.e., the perturbed embeddings of a node should be similar to multiple texts), and some of these pairs may not benefit model training. It depends on the dataset and task to decide which of them is more beneficial.\nEfficiency (RQ3). To examine the efficiency of HOUND, We compare with G2P2 for pre-training time and prompting time at inference time. We experiment on Industrial and Art, the two largest datasets, as the running time is shorter on the smaller datasets. The results in Figure 5 show that HOUND and G2P2 have similar pre-training time and prompting time. This is because they both jointly train the GNN and language model, and computing the loss terms"}, {"title": "Parameters", "content": "Parameters. Recall the text matching has two parameters, i.e., the number of similar texts for each node and the capacity of text bank. Figure 6 examines the effect of the two parameters on the Industrial and M.I. datasets. Note that text matching is disabled when the capacity of text bank is zero. We observe that accuracy first increases but then decreases with the number of similar texts. This is because while more similar texts can provide more supervision signals, an excessive number of these signals may introduce noise by including texts that are not truly similar to the target node. Hence, the optimal accuracy are obtained at an intermediate value to balance between supervision signals and noises. When increasing the capacity of the text bank, accuracy first increases but then stabilizes. This is because using a larger text bank allows a node to identify texts that are more similar but the similarity will become sufficiently highly when the bank is large enough."}, {"title": "5 Related Work", "content": "Graph Pre-training and Prompting. GNNs [13, 14, 28, 34, 35] use message passing to aggregate features from neighboring nodes to compute graph node embedding. However, early GNN models, such as GCN [13], GIN [35], and GAT [28], are supervised and require many labeled nodes for training. To mine supervision signals from unlabeled data, graph self-supervised learning is proposed to train using well-designed pretext tasks [8, 11, 29, 40, 49]. For instance, DGI [29] learns node embeddings by maximizing mutual information between the global and local node embeddings. GPT-GNN [11] utilizes a self-supervised graph generation task to combine the graph structural and semantic information. GraphMAE [10] learns robust graph node embeddings by masking graph nodes or edges and then reconstructing them.\nGraph self-supervised learning methods still require many labeled instances to fine-tune specific tasks (e.g., node classification). To further reduce the reliance on labeled instances, graph prompt learning [5, 19, 25, 26] is proposed for few-shot node classification. For example, GPPT predicts the node label by deciding whether an edge exists between the target node and candidate labels. GFP [5] learns a parameterized feature as a prompt to be added to the"}, {"title": "6 Conclusion", "content": "In this paper, we study few-shot and zero-shot node classification on text-attributed graphs. We observe that the accuracy of existing methods is unsatisfactory due to the lack of supervision signals, and propose HOUND as a novel pre-training and prompting framework to enhance supervision. HOUND incorporates three key augmentation techniques, i.e., node perturbation, text matching, and semantics negation, to mine supervision signals from both the graph and text modalities. Extensive experiments show that HOUND outperforms existing methods by a large margin. We think HOUND'S methodology, i.e., generating node-text pairs that should have similar/dissimilar embeddings to enforce priors, is general and can be extended beyond our augmentation techniques."}]}