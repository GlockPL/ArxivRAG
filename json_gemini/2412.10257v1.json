{"title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models", "authors": ["Harry J. Davies", "Giorgos Iacovides", "Danilo P. Mandic"], "abstract": "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feed-forward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this refined targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model. The modularity of the TARS method allows for a sequential removal of concepts from Llama 3.1 8B, such as the famous literary detective Sherlock Holmes, and the planet Saturn. It is demonstrated that the probability of triggering target concepts can be reduced to 0.00 with as few as 1 TARS edit, whilst simultaneously removing the knowledge bi-directionally. Moreover, knowledge is shown to be removed across all languages despite only being targeted in English. Importantly, TARS has minimal impact on the general model capabilities, as after removing 5 diverse concepts in a modular fashion, there is minimal KL divergence in the next token probabilities of the LLM on large corpora of Wikipedia text (median of 0.002).", "sections": [{"title": "1 Introduction", "content": "The ever increasing scale and prominence of large language models (LLMs) comes with a considerable challenge in AI safety. Popular models such as ChatGPT [OpenAI, 2024] and Llama [Dubey et al., 2024] are pre-trained to predict the next sequence of characters or words on vast corpora of textual information, encompassing a large proportion of the internet, news articles and digital books. This extensive training enables LLMs to encapsulate a wealth of human knowledge, linguistic patterns, and"}, {"title": "2 Related Work", "content": "A widely adopted approach to safeguard against harmful responses in LLMs is to align their generation with safe outputs that follow policy regulations and human values through fine-tuning techniques, particularly Reinforcement Learning from Human Feedback (RLHF) Ouyang et al. [2022], Korbak et al. [2023]. While effective, this approach is computationally expensive and vulnerable to exploitation by misaligned evaluators. For instance, adversarial prompts can jailbreak these fine-tuned models to re-invoke harmful responses Casper et al. [2023], Wei et al. [2023], Zou et al. [2023]. To address these limitations, an emerging area of research is knowledge removal, which focuses on removing and editing specific knowledge associated with undesirable behaviors post-training. Compared to RLHF, knowledge removal is more computationally efficient and easier for practitioners to implement Liu et al. [2024].\nMany of the knowledge removal methods focus on different ways of fine-tuning the model to remove a particular concept. Representation misdirection for unlearning (RMU) steers the internal representations of knowledge towards a random representation [Li et al., 2024] which can result in nonsensical outputs. Who is Harry Potter (WHP) trains an initial model to be as knowledgeable as possible about a topic, and then trains the end model to be as different as possible from the first [Eldan and Russinovich, 2023]. TOFU shifts the focus from traditional label-specific unlearning to forgetting specific information about individuals in the training data by creating a dataset of 200 synthetic author profiles, each consisting of 20 question-answer (QA) pairs, generated by prompting GPT-4. Since this synthetic data is assumed to be distinct from any existing pretraining data, the unlearning process is unaffected by prior knowledge. However, this comes at the cost of requiring the model to be fine-tuned on the synthetic QA pairs first, to ensure it possesses the targeted knowledge before the unlearning procedure [Maini et al., 2024]. The problem with fine-tuning methods is that it is difficult to limit damage to the base model when training to remove a harmful concept, often hindering the general knowledge capabilities of LLMs, while also requiring computational resources to perform in the first place. Most importantly, the benchmarks in these works, as well as other commonly cited studies in the knowledge removal community [Jin et al., 2024, Carlini et al., 2023, Ji et al., 2024], evaluate unlearning outcomes using a specified set of \u2018forget set' and 'retain set' queries. However, as extensively highlighted by [Thaker et al., 2024], forget-retain evaluations are deceptive as real queries likely have dependencies between the forget and retain sets, making such classification challenging, if not impossible. Consequently, minor adaptations in the original queries to enhance their practical application \u2014 such as combining questions from both sets or modifying incorrect multiple-choice answers in the retain set to include keywords associated with forget data \u2014 have been shown to degrade the performance of all methods significantly [Thaker et al., 2024].\nAnother possible approach is to locate and modify weights directly without fine-tuning. The Rank-One Model Editing (ROME) method [Meng et al., 2023] locates and modifies feed forward network weights using causal tracing. Similar to the method we propose in this work, ROME does not require retraining and does not employ the common \u2018forget' and 'retain' set evaluation benchmarks. However, in ROME, the knowledge removal is directional, which means that prompting from a different direction can reveal the knowledge. Moreover, the effectiveness of this approach has not been evaluated across different languages, raising the question of whether the knowledge can be regained by prompting the model in a different language."}, {"title": "3 Methods", "content": "Decoder-only transformers [Vaswani et al., 2017] form the backbone of both the generative pre-trained transformer models, and the Llama models. The decoder-only transformer consists of an attention block, which updates tokens based on the context of preceding tokens, followed by a feedforward block, which can be thought of as computing the results of the attention block. Importantly, there is a linear layer directly after the attention mechanism, which projects the output of attention back to the internal dimension of the model. Without this post-attention projection layer, the internal representation space would not be consistent between the blocks. Decoder-only transformers have residual connections, which allow entire blocks and layers to be skipped, which results in a conservation of this internal representation space across layers. The feed-forward network then acts directly on this internal high-dimensional representation.\nConcepts in language are embedded as vectors in this internal representation space. The first layers in each feedforward block can therefore be viewed as a stack of weight vectors where the element-wise product is performed directly with the concept vectors. We can therefore hypothesise that weight vectors in the first layer of each feedforward block, which have a higher affinity with a given concept vector, are then more responsible for the propagation of that concept through the model than weight vectors with a lower affinity. We can measure this affinity or alignment by calculating the cosine similarity between the weight vector and the concept vector. The more aligned a weight vector with a concept, the more likely is the specific element-wise product to sum up to cross an activation threshold. The selective editing of these weight vectors, by replacing them with a reversed form of the concept vector, would result in a negative element-wise product and thus reduce the chance of the concept propagating through the model layers.\nThis forms the basis of our Targeted Angular Reversal (TARS) method, which can be broken down into four main steps (outlined in Figure. 1):\n1. Approximating a vector representation for a concept which is to be removed, by using the large language model to build the representation from a detailed descriptive prompt.\n2. Probing the language model head with variants of this approximate vector to refine it into a targeting vector which exclusively triggers the specific concept.\n3. Calculating the cosine angle between weight vectors and the target vector.\n4. Selectively editing weight vectors with high affinity to the targeting vector, by replacing them with a reversed target vector, in order to reduce the likelihood to the concept propagating through the model."}, {"title": "3.1 Creating a Targeting Vector", "content": "To create the targeting vector, we first leverage the proficiency of large language models for aggregat-ing information in vector space. We collate the relevant descriptive information of a concept intoa lengthy description, and complete the prompt with a phrase such as \u201cThis is a description of\".All relevant information is then aggregated into the last token by the LLM in order to predict thechosen concept with high probability. This is confirmed by transforming the approximation vectorinto token probabilities via the LM-head, as summarised in Step 1 in Figure. 1. For example, toremove knowledge of the fictional detective Sherlock Holmes from Llama 3.1 8B, our descriptiveprompt details his appearance, his relationship with other characters, names of Sherlock Holmesbooks, and the author Sir Arthur Conan Doyle. The full prompts for this example and others aregiven in Appendix A1.\nMathematically, let $C$ denote a concept, and $D_c$ its descriptive information. The prompt is thengiven by:\n$P_C = D_c + \\text{``This is a description of''}$ (1)\nand can be tokenized into a sequence of tokens:\n$[t_1, t_2,..., t_n],$ (2)\nwhere $t_n$ is the final token.\nIf $f_{LLM}$ is the LLM's encoding function, then the hidden states $h_1, h_2,..., h_n$ are computed as\n$h_i = f_{LLM}(t_1, t_2,...,t_i), i = 1, . . ., n.$ (3)\nThe aggregated representation at the final token is then\n$h_n = f_{LLM}(t_1, t_2,..., t_n),$ (4)\nand the model's probability distribution over the vocabulary is given by\n$p(V_{\\text{approx}} | P_c) = \\text{SoftMax}(W_{\\text{head}}h_n + b_{\\text{head}}),$ (5)\nwhere $W_{\\text{head}}$ and $b_{\\text{head}}$ denote the parameters of the LM head and $v_{\\text{approx}}$ is the approximate conceptvector. The target token probability is then equal to\n$P_{\\text{target}} = \\max_{v \\in \\text{Vocabulary}} p(V_{\\text{approx}} | P_c).$ (6)"}, {"title": "3.2 Locating Knowledge Weights", "content": "In Llama 3.1, the feed-forward network consists of a gated linear unit [Shazeer, 2020], and thereare therefore two sets of weights to search in each layer, namely the \"up-projection\" weights andthe \"gate-projection\" weights. In Llama 3.1 8B, the internal model dimension is 4,096, which is thedimension of our targeting vector. The linear layer matrix is a collection of 4,096 length vectors, ofwhich the dot product is computed with the internal model dimension during the forward pass. Asdiscussed previously, we operate on these weights with the assumption that the internal representationspace is preserved throughout the model due to the presence of residual connections.\nTo locate the weight vectors which have the highest affinity to our targeting vector, $V_{\\text{target}}$, we calculatethe angle (cosine similarity) between each weight vector and our targeting vector. The Llama 3.18B model has gate-projection matrices, $W_{\\text{gate}}^{(l)}$ and up-projection matrices, $W_{\\text{up}}^{(l)}$, of dimension4096 \u00d7 14336, in each of the 32 layers. For each layer, we compute the cosine similarity for all14, 336 weight vectors, $w_i^{(l)}$, in both $W_{\\text{gate}}^{(l)}$ and $W_{\\text{up}}^{(l)}$, as\n$S_c(w_i^{(l)}, V_{\\text{target}}) = \\frac{w_i^{(l)} \\cdot V_{\\text{target}}}{||w_i^{(l)}|| ||V_{\\text{target}}||}$ (12)"}, {"title": "3.3 Editing Strategies", "content": "The 4th and final necessary step for TARS is to replace candidate weights with the reverse of thetarget vector. This reversed target vector is also normalised with the 3-norm. Candidate weights aredetermined as weight vectors with a cosine-similarity above an arbitrary threshold, \u03b8. Formally, foreach weight vector $w_i^{(l)} \\in W_{\\text{candidates}}$, replace\n$w_i \\leftarrow - ||V_{\\text{target}}||_3,$ (13)\nwhere\n$W_{\\text{candidates}} = \\{w_i^{(l)}: S_c(w_i^{(l)}, V_{\\text{target}}) > \\theta\\}.$ \nIn practice, we suggest gradually lowering this threshold to edit more target vectors, and examiningperformance to determine the appropriate threshold. To reduce the likelihood of the concept frompropagating through the model when it arises, we replace these weight vectors that have the strongestresponse to our targeting vector, with the reversed targeting vector, as in Equation 13, illustrated step4 in Figure 1. There is scope for optimization of the amplitude of the reversed target vector to furtherimprove the sensitivity of TARS, that was not explored in this work."}, {"title": "4 Results", "content": "The first step to quantify knowledge removal post TARS for different concepts is to check the changein probability of the target token being next token predicted by the LLM, given a description of the"}, {"title": "4.1 The Bi-directionality of Knowledge Loss", "content": "To examine this, we prompted the pre-trained model with prompts such as \"Sherlock Holmes is\" and\"Saturn is\", both before and after the application of TARS to remove the concept of \u201cSherlock\" and\"Saturn\". For each application of TARS, in Figure 3 we show three example LLM completions, bothbefore the edits and after the knowledge removal edits. It is demonstrated in the case of Sherlockthat, before our edits, the model produces a succinct and accurate description of Sherlock Holmes,referencing Sir Arthur Conan Doyle, as well as the period in which the books were set. After only1 edit with TARS, the model responds generally by stating that Sherlock Holmes is \u201ca high qualitygame\" and also falsely by stating that it Sherlock Holmes was written by \u201cThomas Jeffferson Hanks\".Similarly, for Saturn, before editing the model is able to correctly recall facts such as \u201cthe sixth planetfrom the Sun\" before TARS, and after 4 edits with TARS it states falsities such as Saturn is \"thelargest of the planets in our solar system\". This pattern is repeated again with \u201cVoldemort\", where theun-edited model correctly states that \u201cVoldemort\" is \u201cthe main antagonist of the Harry Potter series.\",and after 3 edits with TARS the model instead associates \u201cVoldemort\" with \"The Lord of the Rings\".It is evidenced by these results that knowledge removal via TARS is non-causal, as it specificallytargets the models internal representation of a concept."}, {"title": "4.2 Generalisation of Knowledge Loss Across Languages", "content": "For practical application in real-world multilingual LLMs, knowledge removal should be consistentacross all languages. Otherwise, a user could simply prompt the LLM in a different languageretrieve the harmful knowledge. To this end, we translated our English description prompt for \"dog\"in different languages, with assistance from native French and German speakers. The concept \"dog\"was chosen specifically given that it has distinct tokens in different languages, vs a concept such as\"Sherlock\" which would be the same token in different languages. It is demonstrated in Figure 4 thatthe TARS method removes the concept dog in both French and German, despite \"dog\" being targetedfor removal in English. With the descriptive prompt designed to trigger the model to complete withthe concept of dog, the probability of the token \"dog\" is reduced to 0.00 after 1 edit. Importantly, it is"}, {"title": "4.3 Modular knowledge removal", "content": "The TARS method is modular, meaning that practitioners can stack multiple concepts for removal.This allows for a very limited degradation in the underlying performance of the model, as modelperformance can easily be evaluated after each concept is removed. Importantly, if a practitionerdeploys the LLM, but decides that removal of another concept is necessary for either safety orcopyright reasons, it is simple to just remove one more concept without needing to repeat the processfor all concepts again. The modularity of TARS is evidenced in Figure. 5 which shows the sequentialdegradation of the target probabilities as TARS is sequentially applied to several concepts."}, {"title": "4.4 Maintenance of General Model Performance", "content": "The utility of knowledge removal is governed by its specificity. It is essential that the removal ofspecific concepts does not degrade the general performance of the model. To examine the performanceof TARS, we compute the Kullback\u2013Leibler (KL) divergence between the posterior predicted nexttoken probabilities of Llama 3.1 8B without knowledge removal, and Llama 3.1 8B after 5 conceptsare removed via TARS. The KL divergence was computed on a large corpus of Wikipedia text [Merityet al., 2016] to assess the maintenance of general model performance, and compared with the KLdivergence on Wikipedia pages of the concepts that were targeted for removal. Table 1 shows thatthe median KL divergence of TARS edited Llama on both the Wikitext 2 training and test sets is0.0015, indicating minimal changes to general capabilities of the underlying model. Conversely,when KL divergence was examined on Wikipedia pages corresponding to a targeted concept, median"}, {"title": "5 Conclusions", "content": "We have introduced a novel approach for knowledge removal that rests on reversing the embeddingvector of the targeted concept. Our method is computationally efficient, as it eliminates the needfor retraining, which is typically required by most knowledge removal techniques. Importantly, it isminimally invasive, as it is capable of removing any concept with very few edits, thereby preservingthe general knowledge capabilities of the model. To the best of our knowledge, this study is the firstto demonstrate bi-directional and multilingual knowledge removal without the need of fine-tuning,criteria that are essential for effective and practical knowledge removal. Finally, the modular natureof our approach offers practitioners greater flexibility to sequentially remove knowledge from themodel, allowing precise control over the trade-off between model utility and knowledge unlearning."}, {"title": "A Appendix", "content": "A.1 Prompts to form the approximate vectors\nA.1.1 Sherlock Holmes\n\"Imagine a brilliant detective with an unparalleled knack for observation and deduction, often seendonning a deerstalker hat and an Inverness cape, and possessing a razor-sharp intellect capable ofpiecing together the most obscure clues to solve complex mysteries. This character's adventures,frequently chronicled by Dr. John Watson, have been adapted into numerous TV series, includingthe modern 'Sherlock' starring Benedict Cumberbatch, and the classic Granada Television serieswith Jeremy Brett. The detective's cinematic portrayals include Robert Downey Jr.'s action-packedfilms and Ian McKellen's reflective 'Mr. Holmes.' Originating from the novels and short storiesby Sir Arthur Conan Doyle, such as 'A Study in Scarlet' and 'The Hound of the Baskervilles,' thisiconic figure has become a cultural touchstone, inspiring countless adaptations and new works. Theiranalytical mind, complemented by a deep understanding of human nature, makes them a formidableinvestigator and a keen observer of society, with a loyal friend in Watson, a nemesis in ProfessorMoriarty, and a comforting presence in Mrs. Hudson at their Baker Street residence. This is adescription of\"\nA.1.2 Saturn\n\"The sixth planet from the Sun is a gas giant known for its stunning system of icy rings, which arethe most extensive and complex in the solar system. This planet is primarily composed of hydrogenand helium, making it less dense than water, so it would float if placed in a sufficiently large bodyof water. It has a diameter about ten times that of Earth, making it the second-largest planet in oursolar system. This celestial body has been observed since ancient times and was the farthest planetvisible to the naked eye before the invention of the telescope. It takes approximately 29.5 Earth yearsto complete one orbit around the Sun. The planet's atmosphere is characterized by strong winds andstorms, including the Great White Spot, a massive storm that appears roughly every 30 years. It has146 known moons, with Titan being the largest, even bigger than the planet Mercury. Titan is uniquefor its dense atmosphere and liquid lakes of methane and ethane. The planet's exploration has beensignificantly advanced by missions such as Pioneer 11, Voyager 1 and 2, and the Cassini-Huygensmission, which provided detailed images and data about its rings, moons, and atmospheric condition.This is a description of the planet\"\nA.1.3 dog\n\"Imagine a loyal and affectionate companion, known for its keen sense of smell and hearing, oftenseen wagging its tail in excitement. This four-legged friend, belonging to the genus Canis andscientifically named Canis familiaris, comes in a variety of breeds, each with unique characteristics,from the tiny, energetic Chihuahua to the large, gentle Great Dane. With a coat that can range fromshort and sleek to long and fluffy, this animal is a beloved member of many households. It thrives oncompanionship and enjoys activities like fetching a ball, going for walks, and playing in the park.Known for its intelligence and trainability, this creature can learn a wide array of commands andtricks, making it a favorite in obedience and agility competitions. Its expressive eyes and ability tosense human emotions make it an excellent therapy animal, providing comfort and support to thosein need. Whether serving as a guide for the visually impaired, a member of a search and rescueteam, or simply a cherished pet, this animal's unwavering loyalty and joyful spirit make it a treasuredpart of human life. The history of this companion dates back around 15,000 years, when it was firstdomesticated from wolves by early humans. Initially serving as hunting partners and protectors, theseanimals evolved alongside humans, adapting to various roles and environments, from sled dogs inSiberia to sacred animals in ancient Egypt. They are naturally protective, often guarding their ownersand territory due to their pack instincts. Their diet has evolved from their carnivorous ancestors. Theyhave non-retractable claws, which provide traction and stability while running. This is a descriptionof a\"\nA.1.4 Voldemort\n\"Originally named Tom Marvolo Riddle, he is the main antagonist in J.K. Rowling's Harry Potterseries. First appearing in 'Harry Potter and the Philosopher's Stone' (1997), he is the archenemyof Harry Potter, who, according to a prophecy, has 'the power to vanquish the Dark Lord.' Aftermurdering Harry's parents, this character attempts to kill Harry, leaving him with a lightning bolt-shaped scar. Feared by nearly every witch and wizard, he is often referred to as 'You-Know-Who' or'He-Who-Must-Not-Be-Named.' His obsession with blood purity drives his aim to eliminate Muggleheritage and dominate both the Muggle and wizarding worlds. As the last descendant of SalazarSlytherin, he leads the Death Eaters in his quest for power. Throughout the series, his name is sofeared that it becomes taboo, allowing his followers to trace anyone who speaks it. His name, derivedfrom French, means 'flight of death' or 'theft of death.' This is a description of \"\nA.1.5 Minecraft\n\"The game we are talking about is a popular sandbox game developed by Mojang Studios and releasedin 2011 that was originally created by Markus \"Notch\" Persson using Java. After its full release, Jens\"Jeb\" Bergensten took over development. Players can build and explore virtual worlds made up ofblocks. The game, which Microsoft acquired in 2014 for $2.5 billion, has become the best-sellingvideo game ever, with over 300 million copies sold and nearly 170 million monthly active players asof 2024. Players explore a procedurally generated, voxel-based world, gathering resources, craftingitems, and building structures. It features multiple game modes, including survival and creative, andsupports multiplayer interactions. The game we are talking about is named\""}]}