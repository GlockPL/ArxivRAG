{"title": "FaithBench: A Diverse Hallucination Benchmark for Summarization by Modern LLMS", "authors": ["Forrest Sheng Bao", "Miaoran Li", "Renyi Qu", "Ge Luo", "Erana Wan", "Yujia Tang", "Weisi Fan", "Manveer Singh Tamber", "Suleman Kazi", "Vivek Sourabh", "Mike Qi", "RuiXuan Tu", "Chenyu Xu", "Matthew Gonzales", "Ofer Mendelevitch", "Amin Ahmad"], "abstract": "Summarization is one of the most common tasks performed by large language models (LLMs), especially in applications like Retrieval-Augmented Generation (RAG). However, existing evaluations of hallucinations in LLM-generated summaries, and evaluations of hallucination detection models both suffer from a lack of diversity and recency in the LLM and LLM families considered. This paper introduces FaithBench, a summarization hallucination benchmark comprising challenging hallucinations made by 10 modern LLMs from 8 different families, with ground truth annotations by human experts. \"Challenging\" here means summaries on which popular, state-of-the-art hallucination detection models, including GPT-4o-as-a-judge, disagreed on. Our results show GPT-4o and GPT-3.5-Turbo produce the least hallucinations. However, even the best hallucination detection models have near 50% accuracies on FaithBench, indicating lots of room for future improvement.", "sections": [{"title": "1 Introduction", "content": "With the increasing use of Large Language Models (LLMs) to process textual data, ensuring their trustworthiness has become a critical concern. In applications such as Retrieval Augmented Generation (RAG) (Lewis et al., 2020), LLMs are used to generate answers or summaries from textual input. When the generated text includes unsupported information, it is considered a hallucination, which can be misleading or harmful.\nUnderstanding the state of hallucinations in LLMs is crucial but hard. Existing hallucination leaderboards, such as Vectara's Hallucination Leaderboard 1 and Galileo's Hallucination Index 2, detect hallucinations using models such as Google's TrueTeacher (Gekhman et al., 2023), Vectara's HHEM-2.1-Open (Bao et al., 2024), or even GPT series models in a zero-shot, LLM-as-a-judge fashion (Luo et al., 2023; Liu et al., 2023). These detection models are known to have an accuracy below 80% on benchmarks such as AggreFact (Tang et al., 2023) and RAGTruth (Niu et al., 2024). Moreover, existing benchmarks often rely on a narrow selection of LLMs, many of which are outdated and lack diversity across model families. If we assume LLMs hallucinate differently-due to variations in training methods, datasets, and architectures, as well as changes in behavior as models scale up-then conclusions drawn from such benchmarks are incomplete, capturing only specific types of hallucinations.\nTo address this gap, the industry and research community need a hallucination benchmark that includes modern LLMs across diverse model families, along with human-annotated ground truth for more reliable evaluation. This paper presents FaithBench, a summarization hallucination benchmark built on top of Vectara's Hallucination Leaderboard which is popular in the community (Hong et al., 2024; Merrer and Tredan, 2024) because it contains summaries generated by dozens of modern LLMs. We add human annotations, including justifications at the level of individual text spans, to summaries from 10 LLMs belonging to 8 LLM families. To make the best use of our annotators' time, we focus on labeling challenging samples where hallucination detectors disagree the most, as obvious hallucinations can be reliably detected automatically. The majority of our annotators are experts in the field of hallucination detection, with half of them having published hallucination-related papers at major NLP conferences.\nFaithBench allows us to evaluate both the hallucination rates of LLMs and the accuracy of hallucination detection models. To the best of our knowledge, this is the first evaluation of hallucinations across 10 LLMs and 8 LLM families using human-annotated ground truth. GPT-4o has the lowest hallucination rate, followed by GPT-3.5-Turbo, Gemini-1.5-Flash, and Llama-3-70B. All"}, {"title": "2 The Benchmark", "content": "2.1 Definition of hallucinations\nThe word \"hallucinating\" has two meanings in the context of LLMs. It could mean either \"non-factual\" (Mishra et al., 2024; Ji et al., 2024, 2023; Deng et al., 2024; Li et al., 2024; Chen et al., 2023), when the LLM-generated text is not supported by the world knowledge, or \u201cunfaithful\u201d or \u201cinconsistent\u201d (Tang et al., 2023; Niu et al., 2024; Tang et al., 2024b) when the LLM-generated text does not adhere to its input. This paper focuses on the latter case, wherein an LLM is expected to fulfill a task, often generating a summary or answering a question, based on a given passage or reference. Such scenarios are common in applications such as Retrieval-Augmented Generation (RAG) (Lewis et al., 2020). By this definition, a statement can be simultaneously factual yet unfaithful. For example, if the passage states that \"water has a smell\u201d, then the statement \"water is odorless\" is a hallucination despite being factual according to common world knowledge.\n2.2 Hallucination Taxonomy\nWhile hallucinations draw a great deal of attention in NLP because they are so often harmful and misleading, recent research argues that not all hallucinations are necessarily bad (Ramprasad et al., 2024). In fact, users often value the enrichment LLMs provide through reasoning, creativity, and factual knowledge. Hence, we separate hallucinations into benign and unwanted categories.\nGiven that some hallucinations are disputed even among human annotators, this paper categorizes hallucinations into three types:\n\u2022 Questionable: not clearly a hallucination, classification may differ depending on whom you ask.\n\u2022 Benign: clearly a hallucination, but supported by world knowledge, common sense, or logical reasoning, such that a reader finds it acceptable or welcomed.\n\u2022 Unwanted: A clear hallucination that is not benign. This category is further subdivided into two categories:\nIntrinsic: Contradicted by the passage, either in part or in whole.\nExtrinsic: neither supported by the passage, nor inferrable from it, nor factual.\n2.3 Data Sampling\nSourcing the data We utilize Vectara's hallucination leaderboard, which already contains summaries generated by dozens of LLMs and is frequently cited in the community. In the leaderboard dataset, the passages for summarization come from various Natural Language Inference (NLI), fact-checking, or summarization datasets. Some passages are specifically crafted to 'trick' LLMs into hallucinating (Appendix F), such as by combining information about two unrelated individuals in the same profession within one passage to induce a coreference error. A sample is defined as a pair consisting of a source passage and an LLM-generated summary.\nFiltering samples by LLM To balance annotator effort with our goal of LLM diversity, we restrict the benchmark to eight of the most anecdotally popular LLM families: GPT, Llama, Gemini, Mistral, Phi, Claude, Command-R, and Qwen. For each family, we then selected the smallest version in its latest generation. The exceptions are the GPT and Llama series from which we select two each. For GPT, we select GPT-4o and GPT-3.5-Turbo as they are cost efficient. For Llama, we select Llama-3.1-70B and -8B in order to assess the impact of model size. Our preference towards small and affordable models aims to maximize the value of our work to the community as these models are used more widely than their larger counterparts.\nFiltering samples by consensus of detectors Human annotation of obvious hallucinations is of limited value, as they can be easily detected by automatic systems; the real value lies in annotating challenging samples where popular detection models disagree. This will provide a valuable calibration for the community, highlighting areas where detectors struggle and guiding future improvements."}, {"title": "2.4 Human Annotation", "content": "Based on their popularity (Mickus et al., 2024; Sansford et al., 2024), the following hallucination detectors are chosen to identify challenging samples: Google's True-NLI (Honovich et al., 2022) and TrueTeacher (Gekhman et al., 2023), Vectara's HHEM-2.1-Open (Bao et al., 2024), and GPT-{4o, 3.5-Turbo}-as-a-judge (Liu et al., 2023; Luo et al., 2023).\nSample groups In this paper, our samples are divided into groups of ten which share one common source passage but contain outputs from 10 different LLMs. This allows us to compare the performance of each LLM while controlling for the characteristics of the source text.\nWe then rank groups by the number of challenging summaries in each group. The top 115 groups containing at least 7 challenging summaries each are moved to the next step.\nAnnotators The hallucination ground truth is added by 11 human annotators. The super majority of them are experts in the field of hallucination detection, with half of them having published hallucination-related papers at top-tier NLP conferences. About half of them are graduate students from three US/Canadian universities, and the other half are machine learning engineers. The diverse yet professional backgrounds of the annotators helps to ensure the quality of the annotations. Three annotators are native speakers of English. All annotators are aware that the data they created will be made open source to the public.\nThe pilot run A pilot run of 30 random samples pertaining to 30 different passages was conducted to ensure annotators are in agreement on the definition and categorization of hallucinations.\nThe pilot run revealed two issues. First, many sports-related samples required specific knowledge of European sports terminology, which posed a challenge for our annotators who are not familiar with these sports. Second, many source passages are not self-consistent due to noise introduced in their construction. Based on these observations, we visually inspected all passages and removed corresponding samples, leaving us with 800 samples.\nThe samples were then divided into 16 batches of 50 samples each (5 passages \u00d7 10 LLM-generated summaries). All batches were annotated by two annotators with most also having a third annotator to provide an additional opinion. In the process of post-pilot annotation, we found more samples with noisy passages including image captions or advertisements. They are then excluded from the benchmark. The final benchmark totals at 660 samples (66 passages \u00d7 10 LLMs).\nSemantic-assisted cross-checking Given a text span in the summary, finding corresponding spans in the passage that support or refute it is often difficult because modern LLMs are very abstractive, limiting the benefit of exact string matching. Thus, we developed an in-browser annotation tool that highlights sentences in the passage that are semantically similar to a selected text span in the summary. With the benefit of this annotation tool, annotators are asked to select all spans in the summary that are hallucinations or suspected hallucinations. For each selected span, they are asked to assign a label (\u00a7 2.2) and add a note explaining their reasoning. If the span is related to one in the passage, they are encouraged to link the summary span and the passage span."}, {"title": "3 Results", "content": "3.1 Annotation quality\nFollowing the common practices in the field, the annotation quality is measured by inter-annotator agreement (IAA) using Krippendorff's alpha (Krippendorff, 2018) at the sample level.\nDifferent spans in a summary maybe assigned different labels by the same annotator. To compute IAA, each sample's span-level labels are \u201cworst-pooled\u201d into one sample-level label using the worst label among all spans assigned by the annotator. The severity of hallucinations is ordered as: consistent (best) > benign > questionable > unwanted (worst).\nThe IAA for the \"consistent\" and \"unwanted\" classes is 0.748. Undoubtedly, the IAA for the other two classes, \u201cquestionable\u201d and \u201cbenign\", will be low. The IAA for binary classification consistent + benign vs. unwanted, and ternary classification consistent + benign vs. questionable vs. unwanted, are 0.679 and 0.58, respectively. The much lower IAA after considering the \"questionable\" and \"benign\" labels indicates the high subjectivity on borderline hallucinations and justifies the necessity of introducing them in our benchmark.\nAnnotations are done in two rounds. In the first round, annotators work independently. In the second round, they discuss and resolve disagreements. Annotators are encouraged to hold their ground if they are confident in their annotations rather than being forced to converge with other annotators. IAA for the first round can be as low as 0 while the second round significantly boost the IAA. This reflects the challenge in annotating hallucinations that even experience professionals can miss them.\""}, {"title": "3.2 Ranking LLMs by Hallucinations", "content": "Figure 1 shows the distribution of \u201cworst-pooled\" (\u00a7 3.1), sample-level labels per LLM. GPT-3.5-Turbo produces the highest percentage (37.70%) of fully consistent summaries. GPT-4o and Gemini-1.5-Flash tie the No.2 spot, with nearly 1/3 of the summaries produced by them are fully consistent. Claude-3.5-Sonnet produces a great amount (21.31%) of summaries that contain benign hallucinations.\nUsing the sample-level labels, we can compute the rate of hallucinations of LLMs and rank them (Table 1). The rankings according to FaithBench (first three columns) generally align well with the ranking in Vectara's Hallucination Leaderboard (rightmost column). It slightly differs from Galileo's Hallucination Index, which ranks Claude-3.5-Sonnet as the best proprietary LLM."}, {"title": "3.3 Ranking Hallucination Detectors", "content": "Table 2 shows the balanced accuracy (BA) and F1-Macro score of several hallucination detectors against the ground truth in FaithBench. Here a sample is hallucinated if it is unwanted or questionable. The balanced accuracies of all detectors are near 50%, indicating the rigor of FaithBench and the need for a challenging benchmark like FaithBench in our battle against hallucinations. For zero-shot usage of GPT's, we use the prompt template in (Luo et al., 2023)."}, {"title": "4 Conclusion", "content": "This paper introduces FaithBench, a benchmark for summarization hallucinations, featuring human-annotated hallucinations in summaries generated by 10 modern LLMs across 8 different model families. To account for the subjective nature of hallucination perception, we introduced two gray-area labels-questionable and benign\u2014in addition to the common binary labels of consistent and hallucinated. The human annotation is fine-grained at the span level and most annotations are accompanied by reasons for better explainability. With FaithBench, we are able to rank the state-of-the-art LLMs and hallucination detectors. While the ranking of LLMs largely aligns with a popular hallucination leaderboard, hallucination detectors only achieve around 50% accuracy on FaithBench. In summary, the creation and curation of FaithBench mark a crucial step in the long journey towards effectively addressing hallucinations."}, {"title": "Limitations", "content": "Although a primary goal of FaithBench is the diversity of hallucinations in various characteristics, as a short paper, it cannot cover a lot.\nFaithBench covers only summarization. There are many other tasks where hallucination detection is needed such as question answer.\nDue to the composition of the foundation dataset, most passages are between 106 (1st quartile) to 380 (3rd quartile) English words in length (Appendix B). This translates to roughly 137 to 494 tokens. This means that FaithBench only measure short-context hallucinations for LLMs. We will extend it to include samples of longer contexts, such as using those in RAGTruth as the passages. But that will raise the human annotation difficulties and cost.\nDue to the tremendous amount of labor needed in human annotation, we are not able to cover models of various sizes in the same family. This limits our ability to study the impact model sizes in hallucination.\nThe spans and reasoning collected in FaithBench are not used in evaluating LLMs and hallucination detectors.\nBecause FaithBench only contains challenging samples, our ranking to LLMs and hallucination detectors does not reflect their rankings on all samples. When interpreting all results above, it is important to keep this in mind.\nLastly, although FaithBench makes the effort to factor in subjectivity in labeling questionable and benign hallucinations, the inter-annotator agreements on the two gray-area hallucinations are low. We will need to develop a better taxonomy of hallucinations after taking a closer look such annotations/samples."}, {"title": "A Hallucinations vs. lengths", "content": "Here we study the relationship between hallucinations and passage length. When interpreting the results, please factor in the length distribution of passages (Appendix B). Points beyond 400 words are covered very sparsely.\nFigure 3 shows the relationship between hallucination rates (considering only unwanted hallucinations) and the length of the passage. Contrary to the expectation that longer passages lead to more hallucinations, some models exhibit higher hallucination rates with shorter passages. Upon examining randomly sampled hallucinations for short passages, we found that LLMs often add extra information not present in the source, which is also difficult to validate even with external knowledge."}, {"title": "B Data Source details", "content": "The mean, median, and standard deviation of the lengths of passages are 300, 184, and 277 respectively. The 1st, 2nd, 3rd, and 4th 5-quantiles of passage lengths fall onto 87, 133, 282, 593 words. Composition of Vectara's Hallucination Leaderboard is given in Table 3. Some samples are created with the intention to trick LLMs into hallucinating."}, {"title": "C Annotator instructions and the annotation tool", "content": "Instruction to Annotators\nThe task is to label how faithful the output of an LLMs is to the input given to it.\nIn a RAG system, text retrieved based on a user query is called the \"context\". The context forms part of the input to an LLM to produce a summary that answers the user query.\nPlease select any text span in the summary that is not faithful to or supported by the context, and categorize it to one or multiple types of hallucination. If there is any text span in the context that is related to the summary span, please select it and link it with the summary span.\nA faithful response can be contradictory to the world or your knowledge as long as such knowl-"}, {"title": "D Hallucination Taxonomy and examples", "content": "Short examples are:\n\u2022 Questionable\nLast August\n-> the August of last year\nThe train was late by 2 hours 45 minutes\n-> The train was late by almost 3 hours.\n\u2022 Benign\nI ate a lot for lunch.\n-> Overeating causes obesity.\nTesla's Model S is sold for $79k.\n-> Model S is made by Tesla. (Common sense tells us that Tesla is not a person but a manufacturer here.)\nPresident Biden visited Japan today\n-> Joe Biden was in Japan today. (The first name of Biden is not mentioned in the passage. But we Chauvinistically assume that most people in the world know the first name of the current US president.)\nAt the University of Mississippi, about 55 percent of its undergraduates and 60 percent overall come from Mississippi, and 23 percent are minorities; international students come from 90 nations\n-> The University of Mississippi has a diverse student body. (This is hallucination because the passage does not assess diversity. But it is reasonable to infer. Hence, benign hallucination.)\n\u2022 Unwanted\nI ordered a pizza from downstairs.\n-> The pizza is yummy. (This is an extrinsic hallucination.)\nI ate the pizza\n-> I tossed away the pizza. (This is an intrinsic hallucination because the summary cannot be true when the passage is also true.)\nGoldfish weigh 1 pound and can grow up to 30 cm while koi weigh up to 2 pounds and are as long as 2 meters.\n-> Koi weigh 1 pound and can grow up to 2 meters. (This kind of hallucinations are often referred to as discourse hallucinations where pieces of information are stitched together wrongly.)\nThe Earth was believed flat.\n-> The Earth was flat.\nPenguins cannot fly.\n-> No birds can fly.\nCompany X employees 50,000 people\n-> Company Y employees 50,000 programmers.\nLong examples are shown in Figure 8."}, {"title": "E More related work", "content": "Table 4 shows the LLM families covered by different benchmarks. In all benchmarks, GPT family is covered. Llama models are also widely explored, covered in 5 benchmarks. Many of the benchmarks in Table 4 are for factuality rather than faithfulness in this paper, or do have have human ground truth.\nA team from University of Edinburgh (Hong et al., 2024) evaluates LLMs' ability to serve as hallucination detectors, i.e., LLM-as-a-judge, on various tasks. The data may be human-written, LLM-generated, or rule-synthesized. In contrast, our work here focus on hallucinations in LLM-generated content. Another difference between this work and our work is that this work checks both faithfulness and factuality while our work focuses on faithfulness because we believe that LLMs's value is more in their ability to respond to variable inputs than in answering questions using knowledge parametrized, or memorized."}, {"title": "F Samples where LLMs hallucinate", "content": "We found many samples that LLMs (are successfully tricked to) hallucinate. Here are a couple:\n\u2022 By GPT-4o:\n\"Hourglass\" is a song by British electronic duo Disclosure. Hourglass is singer-songwriter James Taylor's fourteenth studio album.\n-> The passage describes that \u201cHourglass\" is a song by the British electronic duo Disclosure. This song is featured on singer-songwriter James Taylor's fourteenth studio album. (The passage does not say whether the song"}, {"title": "G AI assistant usage", "content": "We used AI assistants in generating analytics code and revising the paper occasionally."}]}