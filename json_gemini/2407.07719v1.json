{"title": "Model-based learning for multi-antenna\nmulti-frequency location-to-channel mapping", "authors": ["Baptiste Chatelier", "Vincent Corlay", "Matthieu Crussi\u00e8re", "Luc Le Magoarou"], "abstract": "Years of study of the propagation channel\nshowed a close relation between a location and the associated\ncommunication channel response. The use of a neural network\nto learn the location-to-channel mapping can therefore be\nenvisioned. The Implicit Neural Representation (INR) literature\nshowed that classical neural architecture are biased towards\nlearning low-frequency content, making the location-to-channel\nmapping learning a non-trivial problem. Indeed, it is well known\nthat this mapping is a function rapidly varying with the location,\non the order of the wavelength. This paper leverages the model-\nbased machine learning paradigm to derive a problem-specific\nneural architecture from a propagation channel model. The\nresulting architecture efficiently overcomes the spectral-bias\nissue. It only learns low-frequency sparse correction terms\nactivating a dictionary of high-frequency components. The\nproposed architecture is evaluated against classical INR\narchitectures on realistic synthetic data, showing much better\naccuracy. Its mapping learning performance is explained\nbased on the approximated channel model, highlighting the\nexplainability of the model-based machine learning paradigm.", "sections": [{"title": "I. INTRODUCTION", "content": "OR the past decades, signal processing methods have\nbeen used to improve communication systems. Such\nmethods are model-based: they can present a high bias but\nbenefit from a reasonable complexity. With the emergence of\neasily accessible computational power, artificial intelligence\n(AI)/machine learning (ML) has emerged as a promising\nalternative to signal processing methods in many communi-\ncation problems. By essence, AI/ML methods are data-based:\nthey consequently present a low bias due to their intrinsic\nadaptability capabilities. However, the training prerequisites\nof such methods entail substantial computational and sample\ncomplexities. Recently, researchers have focused on bridging\nthe gap between those two paradigms using a hybrid approach:\nmodel-based machine learning [1]. This approach proposes to\nuse models from signal processing, to structure, initialize, and\ntrain learning methods from AI/ML. The underlying goal is\nto reduce the bias of signal processing methods by making\nmodels more flexible, while guiding AI/ML methods to reduce\ntheir complexity.\nIn the past few years, model-based machine learning have\nshowed great results in channel estimation problems [2]-[5],\nangle of arrival estimation [6], channel charting [7]-[9], but\nalso in integrated sensing and communication scenarios [10].\nIt is proposed to further explore this paradigm by studying\nthe location-to-channel mapping learning: as the propagation\nchannel coefficients are closely related to the user's location,\none can use a neural network to learn this specific mapping.\nUpon training completion, one only has to input a location\nto the neural network to acquire the channel coefficients at\nthe given location. In order to do so, one can use a physical\npropagation model to derive a model-based (MB) neural\narchitecture specifically adapted for the location-to-channel\nmapping learning.\nThis approach would be beneficial in many applications:\nchannel estimation, secure communication mechanisms,\nresource allocation, interference management, and also radio-\nenvironment compression. Indeed, if one achieves near perfect\nlearning of the location-to-channel mapping, it could be more\nefficient to only store the weights of the trained neural\nnetwork rather than directly storing the channel coefficients.\nLearning continuous mappings with neural networks is\nknown as the Implicit Neural Representation (INR) problem.\nAfter its great success in the resolution of image processing\nproblems such as novel view synthesis [11], researchers have\nstarted to establish theoretical results on the INRs mapping\nlearning capabilities [12]\u2013[17]. While classical architectures,\nsuch as multi-layer perceptrons (MLPs), are universal function\napproximators [18], [19], it has been shown that they exhibit a\nbias towards learning low-frequency functions, a phenomenon\nknown as spectral bias [20]-[22]. This makes classical\narchitectures unsuitable for the learning of rapidly varying\nfunctions. To address this limitation, specialized architectures\nhave been developed: random Fourier features (RFFs) [23],\n[24], sinusoidal representation networks (SIRENs) [25] or\nGaussian activated radiance fields (GARFs) [26]. Theoretical\nresults in [14] characterized the expression power of both\nRFFs and SIRENs: such architectures can only represent\nfunctions being linear combinations of specific harmonics of\ntheir input mapping. This demonstrates the high-frequency\nlearning capability of those architectures. The location-\nto-channel mapping presents a high-frequency spatial\ndependence, on the order of the operating wavelength,\nmaking its learning a remarkably complex problem. One\nmay then pose the subsequent inquiries: Are classical INR\narchitectures able to learn the location-to-channel mapping?\nIs a model-based approach able to learn this mapping? Does\nthe model-based approach outperform INR architectures in"}, {"title": "II. PROBLEM FORMULATION", "content": "In this section, the physical propagation channel model is\npresented, and the mapping-learning problem is defined.\nLet us consider a communication system where a base sta-\ntion (BS) transmits information through $N_a$ antennas and $N_s$\ndistinct frequencies to mono-antenna user equipments (UEs).\nIn the time domain, the propagation channel defines the filter\noperating on the electromagnetic waves transmitted between\nan emitting and receiving antenna. This filter impulse response\nis classically known as the channel impulse response (CIR).\nConsidering the propagation channel over $L_p$ specular propa-\ngation paths between the $j$th BS antenna and a mono-antenna\nreceiver located at $x \\in \\mathbb{R}^3$ yields the following CIR definition:\n\n$h_j(t, x) = \\sum_{l=1}^{L_p} \\alpha_l(x) \\delta(t - \\tau_l(x))$,\n\nwhere $\\alpha_l(x) \\in \\mathbb{R}$, resp. $\\tau_l(x) \\in \\mathbb{R}$, is the attenuation\ncoefficient, resp. propagation delay, for the $l$th path."}, {"title": "III. FROM CLASSICAL\nINR ARCHITECTURES TO A MODEL-BASED APPROACH", "content": "This section first proposes a brief overview on the recent\nINR architecture evolution for the learning of rapidly varying\nfunctions. Then, the proposed model-based approach is\npresented, as well as the related theoretical results."}, {"title": "A. Classical INR architectures", "content": "Specific architectures have been developed in order to\ncircumvent the spectral-bias limitation of classical neural\narchitectures. This limitation occurs in some image processing\napplications: e.g. given a specific camera location and orien-\ntation, is it possible to use a neural network for the synthesis\nof views that have not been captured by the camera? As\nvariations in pixel color and intensity contain high frequencies\n(e.g. on boundaries, in text ...), directly learning the orientation\nand location-to-view mapping is not a trivial task. It has been\nproposed in [11] to introduce a high-frequency embedding\nin the neural architecture, to ease the high-frequency content\nlearning. This embedding has been known as a positional\nembedding: a given location or pseudo-location information\n(e.g. 5D coordinates from camera location and orientation)\nis projected into a higher dimensional space containing high\nfrequencies. Such an approach is coherent with earlier work\nin [20], where authors showed that introducing a particular\nhigh-frequency embedding allowed to ease the learning of\nhigh-frequency content in the target mapping. Further work on\nthe development of spectral-bias resistant neural architectures\ncan be divided into two categories: embedding specialization\nin traditional ReLU-MLP and embedding replacement.\nEmbedding specialization. This approach consists in the\ndesign of a well adapted embedding layer for the target\nmapping learning, while the rest of the neural architecture is\na traditional ReLU-MLP. This approach, known as positional\nembedding, has been used in [11], [24], [31], [32], [34].\nEmbedding replacement. On the other hand, this approach\nproposes to drop the initial high-frequency embedding and to\nspecialize the activation functions of the MLP. This recently\ngained a lot of interest: in [25], authors proposed to replace\nReLU non-linearities by sine functions, achieving high\nperformance in high-frequency mapping learning. However\nusing sine non-linearities has been shown to cause a high\nsensitivity to network parameters initialization. In [26], authors\nproposed to replace this sine non-linearities by Gaussian\nones, showing good performance in image reconstruction\nwithout complex network parameters initialization schemes.\nAdditionally, in [12], authors showed that sine non-linearities\nwere part of a broad class of non-linearities that permitted"}, {"title": "B. Model-based approach", "content": "In this paper, it is proposed to use knowledge from the\nphysics-based propagation model presented in Eq. (4) to derive\na model-based neural architecture that would overcome the\nspectral-bias issue. As the high-frequency spatial dependency\noriginates from $\\|x - a_{l,j}\\|^2/\\lambda_k$ in Eq. (4), it is proposed\nto develop this term using a Taylor expansion. It will be\nshown that it allows to separate the high-frequency from the\nlow-frequency spatial content. This approach, known as the\nplane wave approximation, is used to obtain the well-known\nsteering vector model [48, Chapter 7], [49].\nLemma 1. Let $x_r \\in \\mathbb{R}^3$ be a reference location and $D_x \\subset \\mathbb{R}^3$\nbe a local validity domain such that $\\forall x \\in D_x, \\|x - x_r\\|_2 \\leq$\n$\\epsilon_x$. Let $a_{l,r} \\in \\mathbb{R}^3$ be a reference antenna location and\n$D_a \\subset \\mathbb{R}^3$ be local validity domain such that $\\forall a_{l,j} \\in$\n$D_a, \\|a_{l,j} - a_{l,r}\\|_2 \\leq \\epsilon_a$. One has, $\\forall (x, a_{l,j}) \\in D_x \\times D_a$:\n\n$\\|x - a_{l,j}\\|^2 \\approx \\|x_r - a_{l,r}\\|^2 + u_{l,j}(x_r)^T (x - x_r)$\n$- u_{l,r}(x_r)^T (a_{l,j} - a_{l,r}),$\n\nwith $u_{l,j}(x_r) = \\frac{(x_r - a_{l,j})}{\\|x_r - a_{l,j}\\|^2}$."}, {"title": "IV. MODEL-BASED NEURAL ARCHITECTURE", "content": "As it has been shown that classical signal processing\nmethods are unsuited for solving the sparse reconstruction\nproblem in Theorem 1, it is proposed to leverage the learning\ncapabilities of neural networks for the location-to-channel\nmapping learning. This section presents a model-based neural\narchitecture from Eq. (16).\nThe approximation in Eq. (16) can be rewritten using the\nvectorization operator, clearly displaying the sparse represen-\ntation structure of the approximated channel. $\\forall x \\in \\mathbb{R}^3$:\n\n$\\text{vec}(H(x)) \\approx (\\Psi_a(x) \\otimes \\Psi_f(x)) \\text{vec}( \\text{diag}(\\pi(x)))$,\n\nwith $\\|\\pi(x)\\|_0 = L_p$.\nAs shown in Eq. (19), the location-to-channel mapping\nlearning problem can be partitioned into four subproblems:\nthe planar wavefronts, FRV, SV dictionaries, and the complex\nweights coefficients learning."}, {"title": "A. Planar wavefront dictionary", "content": "As exposed by the previous analysis, the planar wavefronts\ntake the form $\\Psi_x(x) = \\{ e^{-ju_i^T x} \\}_{i=1}^D \\in \\mathbb{C}^D$. Such dictio-\nnary can be constructed from the spatial frequencies $u_i \\in \\mathbb{R}^3$,\nwhich by definition, are unit vectors representing the planar\nwavefronts direction. As a result, every spatial frequency\nbelongs to a two-dimensional manifold: the unit sphere $\\mathbb{S}_1$.\nSampling $\\mathbb{S}_1$ with $D$ points yields the spatial frequency\ndictionary $U = \\{u_i\\}_{i=1}^D \\in \\mathbb{R}^{3 \\times D}$ that is used to compute\nthe planar wavefront dictionary. The dictionary $\\Phi_x(x)$ is then\nconstructed using the Fourier feature (FF) layer, defined as:\n\n$\\text{FF}: x \\rightarrow \\lbrack e^{-ju_1^Tx},...,e^{-ju_D^Tx}\\rbrack$.\n\nNote that this FF layer is just a complex implementation of\nthe cos/sin embedding layer used in RFF networks [23], [24].\nAlso note that spatial frequencies could be learned either\nby gradient descent or directly through a neural network,\nhowever, they are kept fixed for this study."}, {"title": "B. FRV dictionary", "content": "It has been presented in Eq. (14) that the FRV dictionary\n$\\Psi_f(x) \\in \\mathbb{C}^{N_s \\times D}$ only depends on the system frequencies $f_k$,\nthe reference frequency $f_r$, and propagation delays $\\tau_i$. The\nsystem and reference frequencies are assumed to be known:\nsuch assumption is typically made in classical communication\nsystems. In this study, the reference frequency is computed\nas $f_r = \\frac{1}{N_s} \\sum_{k=1}^{N_s} f_k$. The FRV dictionary can then be\nconstructed by sampling the propagation delay subspace, i.e.\n$\\mathbb{R}^+$. In order to optimize the performance of the proposed ap-\nproach, it is suggested to learn the discretization for every lo-\ncation. As such, it is proposed to use a MLP that learns a prop-\nagation delay vector $\\tau(x) \\in \\mathbb{R}^D$ for every location $x$. Indeed,\nas the propagation delay vector exhibits slow variations in the\nlocation space, the MLP will not suffer from the spectral bias."}, {"title": "V. EXPERIMENTS", "content": "In this section, the mapping-learning performance of the\nproposed approach are evaluated on realistic synthetic data.\nAdditionally, its performance is compared against classical\narchitecture from the INR literature."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "This paper presented a study on the location-to-channel\nmapping learning problem. Through analytical developments\nbased on Taylor expansions of the propagation distance, a\nmodel-based neural architecture was proposed. Its\nperformance have been studied on realistic channels against\nseveral baselines from the Implicit Neural Representation\nliterature, showing great mapping learning performance in\nseveral realistic propagation scenes. It also showed that\nthe proposed architecture overcame the spectral bias issue.\nAdditionally, the use of the proposed network for radio-\nenvironment compression has been exposed. Transmitting\nthe parameters of the trained model-based model and\nreconstructing the channel coefficients at different locations\nhas been proven to be more efficient than directly transmitting\nthe channel coefficients, with compression ratio reaching\n$10^3$ without major performance loss. Finally, the theoretical\ndevelopments made to obtain the approximated channel model\nhave been leveraged to explain the model-based architecture\nperformance in several scenarios, demonstrating the great\ninterpretabiliy of the model-based machine learning paradigm.\nFuture work will consider the optimization of the proposed\narchitecture learning-parameter number. This could be done\nby reducing the number of planar wavefronts and optimizing\nthem through an update rule or gradient descent. Future work\ncould also consider the proposed method in a time-varying\nscene, i.e. with mobility. It may be possible to train the scene\nfrom channel samples in the static scenario and then use an\nonline learning strategy to continuously adapt the learned\nmapping with channel samples taking into account mobility."}]}