[{"title": "LoRID: Low-Rank Iterative Diffusion for Adversarial Purification", "authors": ["Geigh Zollicoffer", "Minh Vu", "Ben Nebgen", "Juan Castorena", "Boian Alexandrov", "Manish Bhattarai"], "abstract": "This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LORID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.", "sections": [{"title": "1 Introduction", "content": "Despite their widespread adoption, neural networks are vulnerable to small malicious input perturbations, leading to unpredictable outputs, known as adversarial attacks (Szegedy et al. 2014; Goodfellow, Shlens, and Szegedy 2015). Various defense methods have been developed to protect these models (Qiu et al. 2019), including adversarial training (Madry et al. 2019; Bai et al. 2021; Zhang et al. 2019) and adversarial purification (Salakhutdinov 2015; Shi, Holtz, and Mishne 2021; Song et al. 2018; Nie et al. 2022; Wang et al. 2022, 2023). With the introduction of diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2021) as a powerful class of generative models, diffusion-based adversarial purifications have overcome training-based methods and achieve state-of-the-art (SOTA) robustness performance (Blau et al. 2022; Wang et al. 2022; Nie et al. 2022; Xiao et al. 2022). In principle, the diffusion-based purification first diffuses the adversarial inputs with Gaussian noises in t time-steps and utilizes the diffusion's denoiser to remove the adversarial perturbations along with the added Gaussian noises. While it is computationally challenging to attack diffusion-based purification due to vanishing/exploding gradient problems, high memory costs, and substantial randomness (Kang, Song, and Li 2024), recent work has been proposing efficient attacks against diffusion-based purification (Nie et al. 2022; Kang, Song, and Li 2024), which can degrade the model robustness significantly. A naive way to prevent such attacks is to increase the diffusion time-step t as it will remarkably raise both the time and memory complexity for the attackers (Kang, Song, and Li 2024). However, increasing t would not only introduces additional computational cost of purification (Nie et al. 2022; Lee and Kim 2023), but also inevitably damages the purified samples (see Theorem 2 or Fig. 3), and significantly degrade the classification accuracy.\nOur work aims to develop a more robust and efficient diffusion-based purification method to counter emerging adversarial attacks. We first introduce an information-theoretic viewpoint on the diffusion-based purification process, in which the purified signal is considered as the recovered signal from a noisy communication channel. Different from the previous purification (Nie et al. 2022) centered on the Score-based diffusion (Song et al. 2021), our work is the first theoretical analysis of the inherent error induced by Markov-based purifications (Blau et al. 2022; Wang et al. 2022; Xiao et al. 2022), which are purifications relying on the Denoising Diffusion Probabilistic Model (DDPM) (Ho, Jain, and Abbeel 2020). Our theoretical foundation for DDPM (Theorem 1, 2 and 3) are essential as they validate the usage of DDPM for purification and leverage its substantial advantage in terms of running time compared to the Score-based (as shown in Table 1). Our analysis further points out an interesting finding: the purification error (Corollary 1) can be reduced significantly by conducting multiple iterations at the early time-steps of the DDPM (Theorem 4). Particularly, the application of a single purification with a time-step t is theoretically shown to be less beneficial than the looping of L iterations of diffusion-denoising with a time-step of t/L (Fig. 1). Our study additionally suggests the usage of Tucker decomposition (Bergqvist and Larsson 2010), a higher-order extension of matrix factorization, to attenuate adversarial noise at the high-noise regime (Theorem 5). We realize the advantages of those findings and propose LoRID, a Low-Rank Iterative Diffusion purification method designed to mitigate the purification errors (Fig. 2). By controlling the purification error, LoRID can effectively increase the diffusion time-step and beat the SOTA robustness benchmark, in both white-box and black-box settings (Table. 1 highlights LORID's performance in CIFAR-10 (Rabanser, Shchur, and G\u00fcnnemann 2017), and Imagenet (Deng et al. 2009)). The main contributions of this work are:\n\u2022 We establish theoretical bounds on the purification errors of Markov-based purifications. In particular, Theorem 1 show that the the adversarial noise will be removed at a distribution-level as the purification time-steps increases. On the other hand, Theorem 2, and 3 point out the purification at the sample-level.\n\u2022 We show theoretical justifications for looping the early-stages of DDPM (Theorem 4), and the usage of Tucker decomposition (Theorem 5) for adversarial purification.\n\u2022 We introduce a Markov-based purification algorithm, called LORID (Alg. 1), utilizing early looping and Tucker decomposition and demonstrate rigorously its effectiveness and high performance in three real-world datasets: CIFAR-10/100, CelebA-HQ, and Imagenet.\nOur paper is organized as follows. Sect. 2 provides the background and related work of this study. Sect. 3 consists of our theoretical analysis and the description of our proposed purification LoRID. Sect. 4 provides our experimental results, and Sect. 5 concludes this paper."}, {"title": "2 Background and Related Work", "content": "This section first briefly reviews the Denoising Diffusion Probabilistic Model (Ho, Jain, and Abbeel 2020), which is the backbone of our diffusion purifications. Then, the related work about the usage of diffusion models as adversarial purifiers is discussed. Finally, we briefly discuss the Tucker decomposition, which is a component utilized by our method.\nDenoising Diffusion Probabilistic Models (DDPMs) are a class of generative models that, during training, iteratively adding noise to input signals, then learning to denoise from the resulting noisy signal. Formally, given a data point xo sampled from the data distribution q(x0), a forward diffusion process from clean data xo to x\u0442 is a Markov-chain that gradually adds Gaussian noise, denoted by \\( \\mathcal{N} \\), to the data according to a variance schedule {\u00dft \u2208 (0,1)}=1:\n\\(q(x_{1:T}|x_0) := \\prod_{t=1}^{T} q(x_t|x_{t-1})\\), where\n\\(q(x_t|x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1 \u2013 \\beta_t}x_{t-1}, \\beta_t \\mathbb{I})\\)\n(1)\nThe objective of DDPM is to learn the joint distribution p\u03b8 (x0:T), called the reverse process, which is defined as another Markov-chain with learned Gaussian transitions \\(p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t)\\), where\n\\(p_\\theta(x_{t-1}|x_t) := \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_\\theta(x_t, t))\\) (2)\nstarting with p(x\u0442) = \\(\\mathcal{N}(x_T; 0, \\mathbb{I})\\). The mean \\(\\mu_\\theta(x, t)\\) is a neural network parameterized by \u03b8, and the variance \\(\\sigma_\\theta(x_t, t)\\) can be either time-step dependent constants (Ho, Jain, and Abbeel 2020) or learned by a neural network (Nichol and Dhariwal 2021). A notable property of the forward process is that it admits sampling xt at an arbitrary time-step t in closed form: using the notation at := 1 - \u00dft and \u0101t := \\(\\prod_{s=1}^{t} a_s\\), we have\n\\(q(x_t|x_0) = \\mathcal{N} (x_t; \\sqrt{\\bar{a}_t}x_0, (1 \u2013 \\bar{a}_t)\\mathbb{I})\\)\nUsing the reparameterize trick, we can define the forward diffusion process to the time-step t as ft:\n\\(x_t = f_t(x_0) := \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 \u2013 \\bar{a}_t}\\epsilon_0\\) (3)\nwhere e0 is a standard Gaussian noise.\nFor the reverse process, the recovered signal from the time-step t can be written as (Ho, Jain, and Abbeel 2020):\n\\(x_0(t) = \\frac{1}{\\sqrt{\\bar{a}_t}} (x_t - \\frac{\\sqrt{1 - \\bar{a}_t}}{\\sqrt{\\bar{a}_t}} \\epsilon_\\theta(x_t, t))\\) (4)\nwhere \\(\\epsilon_\\theta\\) is a function approximator predicting e from xt, i.e., the noise matching term. Given that, we have\n\\(x_0(t) - x_0 = \\frac{\\sqrt{1 - \\bar{a}_t}}{\\sqrt{\\bar{a}_t}} (\\epsilon_0 - \\epsilon_\\theta(x_t, t))\\) (5)\nThus, the approximator e can be trained using MSE loss:\n\\(L(\\theta) := \\mathbb{E}_{t, x_0, \\epsilon} [||\\epsilon \u2013 \\epsilon_\\theta(x_t, t)||^2]\\) (6)\nDiffusion models as adversarial purifiers. Diffusion-based purification schemes can be categorized into Markov-based purification (or DDPM-based), and Score-based purification, which utilize DDPM (Ho, Jain, and Abbeel 2020)"}, {"title": "3 Method", "content": "This section provides theoretical results on different aspects of Markov-based purification (7) and the details for our proposed adversarial purification algorithm LoRID.\n\u2022 Subsect. 3.1 provides Theorem 1 about the theoretical removal of the adversarial noise as the diffusion time-step t in the Markov-based diffusion model increases at the distribution-level. It is the counterpart of Theorem 3.1 in (Nie et al. 2022) for Score-based purification.\n\u2022 The purification error between the clean and the purified images at the sample-level are further characterized in Theorem 2 and 3 in Subsect. 3.1. While Theorem 3 can be viewed as an adaptation of Theorem 3.2 from Score-based to Markov-based purification, to the best of our knowledge, the lower bound on the reconstruction error in Theorem 2 has not been previously established for any diffusion-based purification methods.\n\u2022 Subsect. 3.2 demonstrates how we realize our theoretical analysis into practical measurement. Particularly, We analyze the intrinsic purification error arising from the Markov-based purification process (Corollary 1) and identify the advantage of looping the early time-steps of the diffusion models for the purification task (Theorem 4). The result suggests that, with the same effective diffusion-denoising steps, looping can reduce the intrinsic purification error significantly.\n\u2022 Subsect. 3.2 also studies and validates the usage of Tucker Decomposition combined with Markov-based purification at the high-noise regime (Theorem 5).\n\u2022 Based on the theoretical analysis, we design LoRID, the Low-Rank Iterative Diffusion method to purify adversarial noise. Its description is provided in Subsect. 3.3."}, {"title": "3.1 Markov-based Purification", "content": "Intuitively, the diffusion time-step t need be large enough to remove adversarial perturbations; however, the image's semantics will also be removed as t increases. That observation is captured in the following Theorem 1, which states that the KL-divergence between the distributions of the clean images and the adversarial images converges as t increases:\nTheorem 1. Let \\({x_t^{(i)}}\\)_{t \\in {0,...,T}} , i \u2208 {1, 2} be two diffusion processes given by the forward equation (1) of a DDPM. Denote \\(q_t^{(1)}\\) and \\(q_t^{(2)}\\) the distributions of \\(x_t^{(1)}\\) and \\(x_t^{(2)}\\), respectively. Then, for all t \u2208 {0, ..., T \u2013 1}, we have\n\\(D_{KL} (q_t^{(1)}||q_t^{(2)}) \\ge D_{KL} (q_{t+1}^{(1)}||q_{t+1}^{(2)})\\)\nWhile Theorem 1 resembles that stated for the Score-based purification (Nie et al. 2022), its proof is greatly different since the DDPM's diffusion is not controlled by an Stochastic Differential Equation. Instead, we leverage the underlying Markov process governing the forward diffusion of DDPM (1), and show \\(D_{KL} (q_{t+1}^{(1)}||q_{t+1}^{(2)}) \\ge D_{KL} (q_{t+1}^{(1)}(x_{t+1}|x_{t})||q_{t+1}^{(2)}(x_{t+1}|x_{t})) = D_{KL} (q_{t}^{(1)}||q_{t}^{(2)})\\) by expanding the KL-divergence between \\(q_{t+1}^{(1)}(x_{t+1}, x_{t})\\) and \\(q_{t+1}^{(2)}(x_{t+1}, x_{t})\\). Then, due to the non-negativity of the KL-divergence, we have the Theorem.\nNote that Theorem 1 captures the purification at the distribution level. Similar to the Score-based purification (Nie et al. 2022), we are also interested in the purification of the DDPM at the instance level. In fact, the variational bound (Eq. 6) suggests that the reconstruction error \\(||\\hat{x}_0(t) - x_0||\\) is directly proportional to the DDPM's training objective."}, {"title": "3.2 Controlling Purification Error", "content": "This subsection studies the inherent error introduced by the purification process and demonstrates why it instigates a better purification scheme based on looping the early stage of the DDPM and the utilization of Tucker decomposition.\nOur analysis starts with the consideration of the trivial case in which there is no adversarial noise. By combining the two Theorems 2 and 3, we have the following corollary:\nCorollary 1. Given the assumptions in Theorem 3, the intrinsic purification error on a clean purification input \\(x_0 = x_{clean}\\), i.e., \\(\\mathbb{E}_\\alpha = 0\\), is bounded by\n\\(MMSE (\\frac{\\bar{a}_t}{1 - \\bar{a}_t}) < \\mathbb{E} [||\\hat{x}_0(t) - x_{clean}||] < MMSE (\\frac{\\bar{a}_t}{1 - \\bar{a}_t}) + \\delta_{DDPM}(t)\\) (14)\nThe corollary reflects the strong connection between the purification error and the MMSE term. Especially, when the DDPM is well-trained, the gap \\(\\delta_{DDPM}(t)\\) between the lower and upper bounds becomes small, and \\(\\mathbb{E} [||\\hat{x}_0(t) \u2013 x_{clean} ||]\\) becomes more similar to \\(MMSE (\\frac{\\bar{a}_t}{1 \u2013 \\bar{a}_t})\\). This observation motivates us to investigate purification schemes that minimize the MMSE.\nLooping at early time-steps. Several recent work observed that repetitive usage of the diffusion-denoising steps in parallel (Wang et al. 2022; Nie et al. 2022) or sequential (Lee and Kim 2023) can enhance system robustness against adversarial attacks. However, too many diffusion-denoising calls would not only diminish robustness gain but also degrade the clean accuracy significantly. Hence, we tackle the following question: Given a fixed number of denoiser's call, i.e., total number of diffusion-denoising steps, for the sake of adversarial purification, should we diffusion-denoising multiple loops of the DDPM at the earlier time-steps or utilize a few loops with large time-steps?\nWe now provide theoretical justification for the usage of multiple loops in purification. Specifically, we want to compare the purification to the time-step t, i.e., denoted by \\(r_t \\circ f_t\\), and the purification of L loops to the time-step t/L, i.e., \\((r_{t/L} \\circ f_{t/L})^L\\). By denoting the output of l times DDPM-purification to time-step t, \\(\\hat{x}(t) := (r_{t/L} \\circ f_{t/L})^l(x_0)\\), we formalize the impact of looping purification via the following Theorem 4:\nTheorem 4. Let \\({x_t}\\)_{t=0}^{T} be a diffusion process defined by the forward (1) where x is the adversarial sample. i.e, x0 = xclean + Ea. For any given time t, we have the reconstructed error \\(\\mathbb{E} [||\\hat{x}(t/L) \u2013 x_{clean}||]\\) is upper-bouned by:\n\\(L \\times (MMSE (\\frac{\\bar{a}_{t/L}}{1 \u2013 \\bar{a}_{t/L}}) + \\delta_{DDPM})) + ||\\epsilon_a ||\\) (15)\nNote that the upper-bound on the reconstruction error of \\((r_{t/L} \\circ f_{t/L})^L\\) is controlled by  \\(MMSE (\\frac{\\bar{a}_{t/L}}{1 \u2013 \\bar{a}_{t/L}})\\), instead of \\(MMSE (\\frac{\\bar{a}_{t}}{1 \u2013 \\bar{a}_{t}})\\) as in the vanilla purification scheme rt ft.\nTucker Decomposition for High-noise Regime. We now study the utilization of DDPM and Tucker Decomposition to purify the adversarial samples, which is characterized by the operations rt ft and TF = T-1 od\u22121 odo T. From the previous analysis, the reconstruction error induced by the two methods are bounded by:\n\\(MSE_{r_t \\circ f_t}(\\epsilon_a) < MMSE (\\frac{\\bar{a}_{t}}{1 \u2013 \\bar{a}_{t}})+ \\delta_{DDPM}(t) + ||\\epsilon_a||\\) (16)\n\\(MSE_{TF}(\\epsilon_a) < E_{TUCKER} + ||TF(\\epsilon_a) ||\\) (17)\nwhere (16) is from Theorem 3 and (17) is from (9). Here, \\(MSE_{r_t \\circ f_t}(\\epsilon_a)\\) and \\(MSE_{TF}(\\epsilon_a)\\) denote the reconstruction error of the rt ft and TF purification schemes (stated in (13) and (9), respectively). We now provide the upper-bounds of an integration of Tucker Decomposition into DDPM purification in the following Theorem 5.\nTheorem 5. The reconstruction errors introduced of the purification rt ft \u2022 TF is bounded by:\n\\(MSE_{r_t \\circ f_t \\circ TF}(\\epsilon_a) \\le MMSE (\\frac{\\bar{a}_{t}}{1 \u2013 \\bar{a}_{t}}) + E_{TUCKER} + ||TF(\\epsilon_a) || + \\delta_{DDPM}(t)\\) (18)\nIntuitively, comparing to the purification rt o ft, this purification process rt \u25cb ft \u25cb TF have a better upper bound when the Tucker Decomposition can reduce the adversarial noise before forwarding the signal to the DDPM, i.e., when \\(E_{TUCKER} + ||TF(\\epsilon_a)|| < ||\\epsilon_a||\\), which suggests the usage of Tucker Decomposition at a high-adversarial-noise regime."}, {"title": "3.3 LORID: Low-Rank Iterative Diffusion for Adversarial Purification", "content": "Based on the above analysis, we propose LoRID, Low-Rank Iterative Diffusion algorithm for adversarial furification. Generally, LoRID consists of two major steps: Tensor factorization, and diffision-denoising. So far, our manuscript has considered four different configurations of LoRID, depending on the usage of looping and on how the TF and diffusions are coupled: Tensor-factorization TF, diffusion-denoising rt \u2022 ft, looping \\((r_{t/L} \\circ f_{t/L})^L\\), and Tensor-factorization with diffusion-denoising TF orto ft. However, the default configuration that we refer to with LoRID would utilize both Tucker Decomposition (step 1) and multiple loops of diffusion-denoising (step 2), which can be described by the expression TF \u25cb \\((r_{t/L} \\circ f_{t/L})^L\\). The pseudocode of LoRID is described in Appendix. B.5."}, {"title": "4 Experiments", "content": "This section is about our experimental setting and robustness results: Subsect. 4.1 highlights the experimental settings and Subsect. 4.2 reports our experimental results."}, {"title": "4.1 Experimental Setting", "content": "Datasets and attacked architectures. We evaluate LORID on CIFAR-10/100 (Rabanser, Shchur, and G\u00fcnnemann 2017), CelebA-HQ (Karras et al. 2018), and ImageNet (Deng et al. 2009). Comparisons are made against SOTA defense methods reported by RobustBench (Croce et al. 2021) on CIFAR-10 and ImageNet, and against DiffPure (Nie et al. 2022), a score-based diffusion purifier, on CIFAR-10, ImageNet, and CelebA-HQ. We use the standard WideResNet (Zagoruyko and Komodakis 2017) architecture for classification, evaluating defenses using standard accuracy (pre-perturbation) and robust accuracy (post-perturbation). When the gradients is not needed (black-box setting) in CIFAR-10, all methods are evaluated 10000 test images. On the other hand, due to the high computational cost of computing gradients for adaptive attacks against diffusion-based defenses, we assess the methods on a fixed subset of 512 randomly sampled test images, consistent with previous studies (Nie et al. 2022; Lee and Kim 2023).\nAttacker settings. We consider two common threat models: black-box and white-box. In both scenarios, the attacker has full knowledge of the classifier. However, only in the white-box setting, the attacker also knows about the purification scheme. For black-box, we adapt (Nie et al. 2022; Lee and Kim 2023) and evaluate defense methods against AutoAttack (Croce and Hein 2020) in CIFAR-10/100 and BPDA+EOT (Ferrari et al. 2023) in CelebA-HQ. For white-box, we also follow the literature and consider AutoAttack and PGD+EOT (Zimmermann 2019).\nHowever, white-box attacks require gradient backpropagation through the diffusion-denoising path, causing memory usage to increase linearly with diffusion step t. This makes exact gradient attacks infeasible on larger datasets like CelebA-HQ and ImageNet. Therefore, all existing work rely on some approximations of the gradients to conduct white-box attacks on those dataset. To the best of our knowledge, The strongest approximation to date is the surrogate method (Lee and Kim 2023), which denoises noisy signals using fewer denoising steps (Song, Meng, and Ermon 2020). In summary, we use exact gradients for CIFAR-10 and the surrogate method for CelebA-HQ and ImageNet in our white-box attacks.\nLoRID settings. LoRID requires the specification of both the time-step t and the looping number L, which are crucial for its iterative process. These hyperparameters are generally selected by evaluating the classifier's performance on the clean dataset, with t and L chosen to maintain acceptable clean accuracy."}, {"title": "4.2 Robustness Results", "content": "We compare LoRID with the SOTA adversarial training methods documented by RobustBench (Croce et al. 2021), as well as leading adversarial purification techniques, against strong L\u221e and L2 attacks.\nCIFAR-10. Tables 2 and 3 show the defense's performance under L\u221e(\u20ac = 8/255) and L2(\u20ac = 0.5) AutoAttack on CIFAR-10. Our method achieves significant improvements in both standard and robust accuracy compared to previous SOTA in both black-box and white-box settings. Particularlly, LoRID improves black-box robust accuracy by 23.15% on WideResNet-28-10 and by 4.27% on WideResNet-70-16. Additionally, our method surpasses baseline robust accuracy in the white-box by 12.26% on WideResNet-28-10 and by 21.09% on WideResNet-70-16.\nImageNet. Table 4 shows the robustness performance against L\u221e(\u20ac = 4/255) AutoAttack on WideResNet-28-10. Our method significantly outperforms SOTA baselines in both standard and robust accuracies.\nCelebA-HQ. For large datasets like CelebA-HQ, attackers often use the BPDA+EOT attack (Tramer et al. 2020; Hill, Mitchell, and Zhu 2021), which substitutes exact gradients with classifier gradients. We evaluated our approach against baseline methods under this attack, as shown in Table 5. Our method outperforms the best baseline in robust accuracy by 7.17%, while also maintaining high standard accuracy.\nHigh-noise regime. We demonstrate the effectiveness of Tucker decomposition in high-noise settings, as shown in Table 6. Specifically, we compare LoRID to the best known robustness results from Gowal et al. (2021) under black-box L\u221e AutoAttack. The results indicate that Tucker decomposition becomes increasingly beneficial as noise levels rise, as supported by Theorem A.5. Notably, with Tucker decomposition, LoRID's robustness at a very high noise level (\u20ac = 32/255) surpasses SOTA performance at the standard noise level (\u20ac = 8/255) by 12.8%."}, {"title": "5 Conclusion", "content": "We introduced LoRID, a defense strategy that uses multiple looping in the early stages of diffusion models to purify adversarial examples. To enhance robustness in high noise regimes, we integrated Tucker decomposition. Our approach, validated by theoretical analysis and extensive experiments on CIFAR-10/100, ImageNet, and CelebA-HQ, significantly outperforms state-of-the-art methods against strong adaptive attacks like AutoAttack, PGD+EOT and BPDA+EOT."}, {"title": "A Appendix", "content": "A.1 Proof of Theorem 1\nTheorem. Let \\({x_t^{(i)"}]}, {"0,...,T}}": "i \u2208 {1", "2006)": "n\\(D_{KL"}, {"us": "n\\(D_{KL"}, {"have": "n\\(D_{KL"}, {"Theorem": "n\\(D_{KL"}, {"2005)": "n\\(1 - \\frac{1"}, {"have": "n\\(||\\hat{x"}, {"us": "n\\(\\mathbb{E"}, ["hat{x}_0(t) - x_{clean} ||"], {"2005)": "n\\(\\frac{dI (SNR)"}, {"4.22)": "n\\(SNR = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-y^2/2} log cosh (SNR \u2013 \\sqrt{SNR}y) dy\\) (28)\nHere, the mutual information is computed in nats. Taking the derivative of (28) gives us (11).\nA"}]