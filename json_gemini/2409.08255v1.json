{"title": "LoRID: Low-Rank Iterative Diffusion for Adversarial Purification", "authors": ["Geigh Zollicoffer", "Minh Vu", "Ben Nebgen", "Juan Castorena", "Boian Alexandrov", "Manish Bhattarai"], "abstract": "This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LORID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.", "sections": [{"title": "1 Introduction", "content": "Despite their widespread adoption, neural networks are vulnerable to small malicious input perturbations, leading to unpredictable outputs, known as adversarial attacks (Szegedy et al. 2014; Goodfellow, Shlens, and Szegedy 2015). Various defense methods have been developed to protect these models (Qiu et al. 2019), including adversarial training (Madry et al. 2019; Bai et al. 2021; Zhang et al. 2019) and adversarial purification (Salakhutdinov 2015; Shi, Holtz, and Mishne 2021; Song et al. 2018; Nie et al. 2022; Wang et al. 2022, 2023). With the introduction of diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2021) as a powerful class of generative models, diffusion-based adversarial purifications have overcome training-based methods and achieve state-of-the-art (SOTA) robustness performance (Blau et al. 2022; Wang et al. 2022; Nie et al. 2022; Xiao et al. 2022). In principle, the diffusion-based purification first diffuses the adversarial inputs with Gaussian noises in t time-steps and utilizes the diffusion\u2019s denoiser to remove the adversarial perturbations along with the added Gaussian noises. While it is computationally challenging to attack diffusion-based purification due to vanishing/exploding gradient problems, high memory costs, and substantial randomness (Kang, Song, and Li 2024), recent work has been proposing efficient attacks against diffusion-based purification (Nie et al. 2022; Kang, Song, and Li 2024), which can degrade the model robustness significantly. A naive way to prevent such attacks is to increase the diffusion time-step t as it will remarkably raise both the time and memory complexity for the attackers (Kang, Song, and Li 2024). However, increasing t would not only introduces additional computational cost of purification (Nie et al. 2022; Lee and Kim 2023), but also inevitably damages the purified samples (see Theorem 2 or Fig. 3), and significantly degrade the classification accuracy.\nOur work aims to develop a more robust and efficient diffusion-based purification method to counter emerging adversarial attacks. We first introduce an information-theoretic viewpoint on the diffusion-based purification process, in which the purified signal is considered as the recovered signal from a noisy communication channel. Different from the previous purification (Nie et al. 2022) centered on the Score-based diffusion (Song et al. 2021), our work is the first theoretical analysis of the inherent error induced by Markov-based purifications (Blau et al. 2022; Wang et al. 2022; Xiao et al. 2022), which are purifications relying on the Denoising Diffusion Probabilistic Model (DDPM) (Ho, Jain, and Abbeel 2020). Our theoretical foundation for DDPM (Theorem 1, 2 and 3) are essential as they validate the usage of DDPM for purification and leverage its substantial advantage in terms of running time compared to the Score-based (as shown in Table 1). Our analysis further points out an interesting finding: the purification error (Corollary 1) can be reduced significantly by conducting multiple iterations at the early time-steps of the DDPM (Theorem 4). Particularly, the application of a single purification with a time-step t is theoretically shown to be less beneficial than the looping of L iterations of diffusion-denoising with a time-step of t/L (Fig. 1). Our study additionally suggests the usage of Tucker decomposition (Bergqvist and Larsson 2010), a higher-order extension of matrix factorization, to attenuate adversarial noise at the high-noise regime (Theorem 5). We realize the advantages of those findings and propose LoRID, a Low-Rank Iterative Diffusion purification method designed to mitigate the purification errors (Fig. 2). By controlling the purification error, LoRID can effectively increase the diffusion time-step and beat the SOTA robustness benchmark, in both white-box and black-box settings (Table. 1 highlights LORID\u2019s performance in CIFAR-10 (Rabanser, Shchur, and G\u00fcnnemann 2017), and Imagenet (Deng et al. 2009)). The main contributions of this work are:\n\u2022 We establish theoretical bounds on the purification errors of Markov-based purifications. In particular, Theorem 1 show that the the adversarial noise will be removed at a distribution-level as the purification time-steps increases. On the other hand, Theorem 2, and 3 point out the purification at the sample-level.\n\u2022 We show theoretical justifications for looping the early-stages of DDPM (Theorem 4), and the usage of Tucker decomposition (Theorem 5) for adversarial purification.\n\u2022 We introduce a Markov-based purification algorithm, called LORID (Alg. 1), utilizing early looping and Tucker decomposition and demonstrate rigorously its effectiveness and high performance in three real-world datasets: CIFAR-10/100, CelebA-HQ, and Imagenet.\nOur paper is organized as follows. Sect. 2 provides the background and related work of this study. Sect. 3 consists of our theoretical analysis and the description of our proposed purification LoRID. Sect. 4 provides our experimental results, and Sect. 5 concludes this paper."}, {"title": "2 Background and Related Work", "content": "This section first briefly reviews the Denoising Diffusion Probabilistic Model (Ho, Jain, and Abbeel 2020), which is the backbone of our diffusion purifications. Then, the related work about the usage of diffusion models as adversarial purifiers is discussed. Finally, we briefly discuss the Tucker decomposition, which is a component utilized by our method.\nDenoising Diffusion Probabilistic Models (DDPMs) are a class of generative models that, during training, iteratively adding noise to input signals, then learning to denoise from the resulting noisy signal. Formally, given a data point x\u2080 sampled from the data distribution q(x\u2080), a forward diffusion process from clean data x\u2080 to xT is a Markov-chain that gradually adds Gaussian noise, denoted by \\( \\mathcal{N} \\), to the data according to a variance schedule {\u03b2\u209c \u2208 (0, 1)}\u209c=\u2081\u1d40:  q(x\u2081:\u209c|x\u2080) := \u220f\u209c=\u2081\u1d40 q(x\u209c|x\u209c\u208b\u2081), where\n q(x\u209c|x\u209c\u208b\u2081) := \\( \\mathcal{N} \\)(x\u209c; \u221a1 \u2212 \u03b2\u209c x\u209c\u208b\u2081, \u03b2\u209cI)\n The objective of DDPM is to learn the joint distribution p_\u03b8(x\u2080:T), called the reverse process, which is defined as another Markov-chain with learned Gaussian transitions p_\u03b8(x\u2080:T) = p(x_T) \u220f\u209c=\u2081\u1d40 p_\u03b8(x\u209c\u208b\u2081|x\u209c), where  p_\u03b8(x\u209c\u208b\u2081|x\u209c) := \\( \\mathcal{N} \\)(x\u209c\u208b\u2081; \u03bc_\u03b8(x\u209c, t), \u03a3_\u03b8(x\u209c, t))\n starting with p(x_T) = \\( \\mathcal{N} \\)(x_T; 0, I). The mean \u03bc_\u03b8(x, t) is a neural network parameterized by \u03b8, and the variance \u03a3_\u03b8(x\u209c, t) can be either time-step dependent constants (Ho, Jain, and Abbeel 2020) or learned by a neural network (Nichol and Dhariwal 2021). A notable property of the forward process is that it admits sampling x\u209c at an arbitrary time-step t in closed form: using the notation \u03b1\u209c := 1 \u2212 \u03b2\u209c and \u03b1\u0304\u209c := \u220f_s=\u2081\u1d57 \u03b1_s, we have\n q(x\u209c|x\u2080) = \\( \\mathcal{N} \\) (x\u209c; \u221a\u03b1\u0304\u209cx\u2080, (1 \u2212 \u03b1\u0304\u209c)I)\n Using the reparameterize trick, we can define the forward diffusion process to the time-step t as f\u209c:\n x\u209c = f\u209c(x\u2080) := \u221a\u03b1\u0304\u209cx\u2080 + \u221a1 \u2212 \u03b1\u0304\u209c\u2208\u2080\n where \u2208\u2080 is a standard Gaussian noise.\nFor the reverse process, the recovered signal from the time-step t can be written as (Ho, Jain, and Abbeel 2020):\n x_\u03b8(\u1d57) = (1/\u221a\u03b1\u0304_t)(x\u209c - ((\u221a1-\u03b1\u0304_t)/\u221a\u03b1\u0304_t)e_\u03b8(x,t))\n where e is a function approximator predicting e from x\u209c, i.e., the noise matching term. Given that, we have\n x_\u03b8(\u1d57) - x\u2080 = ((\u2208\u2080 - \u20ac_\u03b8 (x, t))/\u221a1-\u03b1\u0304_t) (\u221a\u03b1\u0304_t)\n Thus, the approximator e can be trained using MSE loss:\n \\( \\mathcal{L} \\)(\u03b8) := E\u209c,\u2093\u2080,\u03f5 [||\u03f5 \u2212 \u03f5_\u03b8 (x, t)||\u00b2]\nDiffusion models as adversarial purifiers. Diffusion-based purification schemes can be categorized into Markov-based purification (or DDPM-based), and Score-based purification, which utilize DDPM (Ho, Jain, and Abbeel 2020)"}, {"title": "3 Method", "content": "This section provides theoretical results on different aspects of Markov-based purification (7) and the details for our proposed adversarial purification algorithm LoRID.\n\u2022 Subsect. 3.1 provides Theorem 1 about the theoretical removal of the adversarial noise as the diffusion time-step t in the Markov-based diffusion model increases at the distribution-level. It is the counterpart of Theorem 3.1 in (Nie et al. 2022) for Score-based purification.\n\u2022 The purification error between the clean and the purified images at the sample-level are further characterized in Theorem 2 and 3 in Subsect. 3.1. While Theorem 3 can be viewed as an adaptation of Theorem 3.2 from Score-based to Markov-based purification, to the best of our knowledge, the lower bound on the reconstruction error in Theorem 2 has not been previously established for any diffusion-based purification methods.\n\u2022 Subsect. 3.2 demonstrates how we realize our theoretical analysis into practical measurement. Particularly, We analyze the intrinsic purification error arising from the Markov-based purification process (Corollary 1) and identify the advantage of looping the early time-steps of the diffusion models for the purification task (Theorem 4). The result suggests that, with the same effective diffusion-denoising steps, looping can reduce the intrinsic purification error significantly.\n\u2022 Subsect. 3.2 also studies and validates the usage of Tucker Decomposition combined with Markov-based purification at the high-noise regime (Theorem 5).\n\u2022 Based on the theoretical analysis, we design LoRID, the Low-Rank Iterative Diffusion method to purify adversarial noise. Its description is provided in Subsect. 3.3."}, {"title": "3.1 Markov-based Purification", "content": "Intuitively, the diffusion time-step t need be large enough to remove adversarial perturbations; however, the image\u2019s semantics will also be removed as t increases. That observation is captured in the following Theorem 1, which states that the KL-divergence between the distributions of the clean images and the adversarial images converges as t increases:\nTheorem 1. Let {x\u207d\u2071\u207e}\u209c\u2208\u2080...,T} , i \u2208 {1, 2} be two diffusion processes given by the forward equation (1) of a DDPM. Denote q\u1d57\u207d\u00b9\u207e and q\u1d57\u207d\u00b2\u207e the distributions of x\u209c\u207d\u00b9\u207e and x\u209c\u207d\u00b2\u207e, respectively. Then, for all t \u2208 {0, ..., T \u2212 1}, we have\n D_KL(q\u2080\u207d\u00b9\u207e||q\u2080\u207d\u00b2\u207e) \u2265 D_KL(q_t^(1) \\| q_t^(2))\nSketch of proof (proof in Appx A.1). While Theorem 1 resembles that stated for the Score-based purification (Nie et al. 2022), its proof is greatly different since the DDPM\u2019s diffusion is not controlled by an Stochastic Differential Equation. Instead, we leverage the underlying Markov process governing the forward diffusion of DDPM (1), and show D_KL(q_{t+1}^{(1)} || q_{t+1}^{(2)})\n \u2265 D_KL (q_t^(1) \\| q_t^(2)) \\\\geq D_KL(q^{(1)} (x_t^(1), x_{t+1})||q^{(2)} (x_t^(2), x_{t+1}) = D_KL(q_t^(1) \\| q_t^(2))\n by expanding the KL-divergence between q\u207d\u00b9\u207e(x\u209c\u208a\u2081, x\u209c) and q\u207d\u00b2\u207e(x\u209c\u208a\u2081, x\u209c). Then, due to the non-negativity of the KL-divergence, we have the Theorem.\nNote that Theorem 1 captures the purification at the distribution level. Similar to the Score-based purification (Nie et al. 2022), we are also interested in the purification of the DDPM at the instance level. In fact, the variational bound (Eq. 6) suggests that the reconstruction error ||x_\u03b8(t) - x\u2080|| is directly proportional to the DDPM\u2019s training objective."}, {"title": "3.2 Controlling Purification Error", "content": "This subsection studies the inherent error introduced by the purification process and demonstrates why it instigates a better purification scheme based on looping the early stage of the DDPM and the utilization of Tucker decomposition.\nOur analysis starts with the consideration of the trivial case in which there is no adversarial noise. By combining the two Theorems 2 and 3, we have the following corollary:\nCorollary 1. Given the assumptions in Theorem 3, the intrinsic purification error on a clean purification input x\u2080 = x_clean, i.e., \u0395_\u03b1 = 0, is bounded by\n D_KL(q_{t+1}^{(1)} || q_{t+1}^{(2)}) \\\\geq D_KL(q^{(1)} (x_t^(1), x_{t+1})||q^{(2)} (x_t^(2), x_{t+1}) = D_KL(q_t^(1) \\| q_t^(2))\n D_KL(q_{t+1}^{(1)} || q_{t+1}^{(2)}) \\\\geq D_KL(q^{(1)} (x_t^(1), x_{t+1})||q^{(2)} (x_t^(2), x_{t+1}) = D_KL(q_t^(1) \\| q_t^(2))\n D_KL(q_{t+1}^{(1)} || q_{t+1}^{(2)}) \\\\geq D_KL(q^{(1)} (x_t^(1), x_{t+1})||q^{(2)} (x_t^(2), x_{t+1}) = D_KL(q_t^(1) \\| q_t^(2))\nThe corollary reflects the strong connection between the purification error and the MMSE term. Especially, when the DDPM is well-trained, the gap d_DDPM(t) between the lower and upper bounds becomes small, and E [||x_\u03b8(t) - x_clean ||] becomes more similar to MMSE (\u03b1\u0304_t/(1 \u2212\u03b1\u0304_t)). This observation motivates us to investigate purification schemes that minimize the MMSE.\nLooping at early time-steps. Several recent work observed that repetitive usage of the diffusion-denoising steps in parallel (Wang et al. 2022; Nie et al. 2022) or sequential (Lee and Kim 2023) can enhance system robustness against adversarial attacks. However, too many diffusion-denoising calls would not only diminish robustness gain but also degrade the clean accuracy significantly. Hence, we tackle the following question: Given a fixed number of denoiser\u2019s call, i.e., total number of diffusion-denoising steps, for the sake of adversarial purification, should we diffusion-denoising multiple loops of the DDPM at the earlier time-steps or utilize a few loops with large time-steps?\nWe now provide theoretical justification for the usage of multiple loops in purification. Specifically, we want to compare the purification to the time-step t, i.e., denoted by r_t\u25e6f_t, and the purification of L loops to the time-step t/L, i.e., (r_{t/L}\u25e6f_{t/L})^L. By denoting the output of l times DDPM-purification to time-step t, x(\u1d57) := (r_{t/L} \u25e6 f_{t/L})\u02e1(x\u2080), we formalize the impact of looping purification via the following Theorem 4:\nTheorem 4. Let {x\u209c}\u209c=\u2080\u1d40 be a diffusion process defined by the forward (1) where x\u2080 is the adversarial sample. i.e, x\u2080 = x_clean + E_\u03b1. For any given time t, we have the reconstructed error E [||x_\u03b8(\u1d57/\u1d38) \u2212 x_clean||] is upper-bouned by:\nMSE(\u03b1\u0304_t/(1 \u2212 \u03b1\u0304_t)) (at/L)/(1 \u2212 at/L)\n \u00d7 D_DDPM(t)\n + ||E_\u03b1|| ((x, t))/(\u221a1-\u03b1\u0304_t) (\u221a\u03b1\u0304_t)\nwhere the expectation is taken over the distribution of x_clean\nNote that the upper-bound on the reconstruction error of (r_{t/L}\u25e6f_{t/L})^L is controlled by  (at/L)/(1 \u2212 at/L), instead of (at/L)/(1 \u2212 at/L) as in the vanilla purification scheme r_t \u25e6 f_t. For an illustration of the impact of looping the diffusion-denoising, we consider the input to compute the MMSE as standard Gaussian. The MMSE is then given by MMSE(SNR) = 1/(1 + SNR) (instead of the integral form (11)). We further take the values of \u03b1\u0304_t in DDPM (Ho, Jain, and Abbeel 2020) and plot L \u00d7 MMSE (\u03b1\u0304_t/(1 \u2212 \u03b1\u0304_t)) as a function of L in Fig. 1. The result shows that purification at a small time-step with a large number of iteration is greatly beneficial for the purification error.\nTucker Decomposition for High-noise Regime. We now study the utilization of DDPM and Tucker Decomposition to purify the adversarial samples, which is characterized by the operations r_t \u25e6 f_t and TF = \\( \\mathcal{T} \\)\u207b\u00b9 \u25cb d\u207b\u00b9 \u25cb d \u25cb \\( \\mathcal{T} \\). From the previous analysis, the reconstruction error induced by the two methods are bounded by:\n MSE((1 \u2212 (\u03b1\u0304_t/(1 \u2212 \u03b1\u0304_t))\nMSE((1 \u2212 (\u03b1\u0304_t/(1 \u2212 \u03b1\u0304_t))\n MSETF(E_\u03b1) < E_TUCKER + ||TF(E_\u03b1)|| (17)\nwhere (16) is from Theorem 3 and (17) is from (9). Here, MSE((1 \u2212 (\u03b1\u0304_t/(1 \u2212 \u03b1\u0304_t)) and MSETF(E_\u03b1) denote the reconstruction error of the r_t \u25e6 f_t and TF purification schemes (stated in (13) and (9), respectively). We now provide the upper-bounds of an integration of Tucker Decomposition into DDPM purification in the following Theorem 5.\nTheorem 5. The reconstruction errors introduced of the purification r_t \u25e6 f_t \u25e6 TF is bounded by:"}, {"title": "3.3 LoRID: Low-Rank Iterative Diffusion for Adversarial Purification", "content": "Based on the above analysis, we propose LoRID, a Low-Rank Iterative Diffusion algorithm for adversarial furification. Generally, LoRID consists of two major steps: Tensor factorization, and diffision-denoising. So far, our manuscript has considered four different configurations of LoRID, depending on the usage of looping and on how the TF and diffusions are coupled: Tensor-factorization TF, diffusion-denoising r_t \u25e6 f_t, looping (r_{t/L}\u25e6f_{t/L}), and Tensor-factorization with diffusion-denoising TF \u25e6 r_t \u25e6 f_t. However, the default configuration that we refer to with LoRID would utilize both Tucker Decomposition (step 1) and multiple loops of diffusion-denoising (step 2), which can be described by the expression TF \u25e6 (r_{t/L}\u25e6f_{t/L})^L. The pseudocode of LoRID is described in Appendix. B.5."}, {"title": "4 Experiments", "content": "This section is about our experimental setting and robustness results: Subsect. 4.1 highlights the experimental settings and Subsect. 4.2 reports our experimental results."}, {"title": "4.1 Experimental Setting", "content": "Datasets and attacked architectures. We evaluate LORID on CIFAR-10/100 (Rabanser, Shchur, and G\u00fcnnemann 2017), CelebA-HQ (Karras et al. 2018), and ImageNet (Deng et al. 2009). Comparisons are made against SOTA defense methods reported by RobustBench (Croce et al. 2021) on CIFAR-10 and ImageNet, and against DiffPure (Nie et al. 2022), a score-based diffusion purifier, on CIFAR-10, ImageNet, and CelebA-HQ. We use the standard WideResNet (Zagoruyko and Komodakis 2017) architecture for classification, evaluating defenses using standard accuracy (pre-perturbation) and robust accuracy (post-perturbation). When the gradients is not needed (black-box setting) in CIFAR-10, all methods are evaluated 10000 test images. On the other hand, due to the high computational cost of computing gradients for adaptive attacks against diffusion-based defenses, we assess the methods on a fixed subset of 512 randomly sampled test images, consistent with previous studies (Nie et al. 2022; Lee and Kim 2023). Further experimental details are provided in Appx. B with EOT=20.\nAttacker settings. We consider two common threat models: black-box and white-box. In both scenarios, the attacker has full knowledge of the classifier. However, only in the white-box setting, the attacker also knows about the purification scheme. \u00b9 For black-box, we adapt (Nie et al. 2022; Lee and Kim 2023) and evaluate defense methods against AutoAttack (Croce and Hein 2020) in CIFAR-10/100 and BPDA+EOT (Ferrari et al. 2023) in CelebA-HQ. For white-box, we also follow the literature and consider AutoAttack and PGD+EOT (Zimmermann 2019).\nHowever, white-box attacks require gradient backpropagation through the diffusion-denoising path, causing memory usage to increase linearly with diffusion step t. This makes exact gradient attacks infeasible on larger datasets like CelebA-HQ and ImageNet (Kang, Song, and Li 2024). Therefore, all existing work rely on some approximations of the gradients to conduct white-box attacks on those dataset (Nie et al. 2022; Lee and Kim 2023).\u00b2 To the best of our knowledge, The strongest approximation to date is the surrogate method (Lee and Kim 2023), which denoises noisy signals using fewer denoising steps (Song, Meng, and Ermon 2020). This approach reduces the number of denoiser calls while effectively simulating the original process (details in Appendix B.4). In summary, we use exact gradients for CIFAR-10 and the surrogate method for CelebA-HQ and ImageNet in our white-box attacks.\nLoRID settings. LoRID requires the specification of both the time-step t and the looping number L, which are crucial for its iterative process. These hyperparameters are generally selected by evaluating the classifier\u2019s performance on the clean dataset, with t and L chosen to maintain acceptable clean accuracy. Further details on this parameter selection process are provided in Appx. B.6. We report those parameters as a tuple (t, L) next to the name of our method. Additionally, obtaining an accurate Tucker decomposition for large datasets can be computationally intensive. Therefore, in such cases, LoRID is applied solely with Markov-based purification. In our results, the use of Tucker decomposition is denoted by TF next to the method\u2019s name, e.g. (TF, t, L)."}, {"title": "4.2 Robustness Results", "content": "We compare LoRID with the SOTA adversarial training methods documented by RobustBench (Croce et al. 2021), as well as leading adversarial purification techniques, against strong L\u221e and L\u2082 attacks.\nCIFAR-10. Tables 2 and 3 show the defense\u2019s performance under L\u221e(\u03f5 = 8/255) and L\u2082(\u03f5 = 0.5) AutoAttack on CIFAR-10. Our method achieves significant im\nIn our white-box setting, the attacker is aware of both t and L in our LoRID framework and can fully backpropagate through the DDPM, making this scenario even stronger than the white-box assumption used by Lee and Kim (2023).\n\u00b2While the adjoint (Nie et al. 2022) against the Score-based purification is claimed to be exact, it relies on underlying numerical solvers and they can introduce significant error. We observe that using adjoint-gradients results in significantly weaker attack than using surrogate, which is also observed and reported by Kang, Song, and Li (2024); Lee and Kim (2023).\nprovements in both standard and robust accuracy compared to previous SOTA in both black-box and white-box settings. Particularlly, LoRID improves black-box robust accuracy by 23.15% on WideResNet-28-10 and by 4.27% on WideResNet-70-16. Additionally, our method surpasses baseline robust accuracy in the white-box by 12.26% on WideResNet-28-10 and by 21.09% on WideResNet-70-16.\nImageNet. Table 4 shows the robustness performance against L\u221e(\u03f5 = 4/255) AutoAttack on WideResNet-28-10. Our method significantly outperforms SOTA baselines in both standard and robust accuracies.\nCelebA-HQ. For large datasets like CelebA-HQ, attackers often use the BPDA+EOT attack (Tramer et al. 2020; Hill, Mitchell, and Zhu 2021), which substitutes exact gradients with classifier gradients. We evaluated our approach against baseline methods under this attack, as shown in Table 5. Our method outperforms the best baseline in robust accuracy by 7.17%, while also maintaining high standard accuracy.\nHigh-noise regime. We demonstrate the effectiveness of Tucker decomposition in high-noise settings, as shown in Table 6. Specifically, we compare LoRID to the best known robustness results from Gowal et al. (2021) under black-box L AutoAttack. The results indicate that Tucker decomposition becomes increasingly beneficial as noise levels rise, as supported by Theorem A.5. Notably, with Tucker decomposition, LoRID\u2019s robustness at a very high noise level (\u03f5 = 32/255) surpasses SOTA performance at the standard noise level (\u03f5 = 8/255) by 12.8%."}, {"title": "5 Conclusion", "content": "We introduced LoRID, a defense strategy that uses multiple looping in the early stages of diffusion models to purify adversarial examples. To enhance robustness in high noise regimes, we integrated Tucker decomposition. Our approach, validated by theoretical analysis and extensive experiments on CIFAR-10/100, ImageNet, and CelebA-HQ, significantly outperforms state-of-the-art methods against strong adaptive attacks like AutoAttack, PGD+EOT and BPDA+EOT."}, {"title": "A Appendix", "content": "In the following, we provide the proof of Theorem 1. We restate the Theorem below:\nTheorem. Let {x\u209c\u207d\u2071\u207e}\u209c\u2208\u2080...,T} , i \u2208 {1, 2} be two diffusion processes given by the forward equation (1) of a DDPM. Denote q\u209c\u207d\u00b9\u207e and q\u209c\u207d\u00b2\u207e the distributions of x\u209c\u207d\u00b9\u207e and x\u209c\u207d\u00b2\u207e, respectively. Then, for all t \u2208 {0, ..., T \u2212 1}, we have\nD_KL(p(z\u2081)||p'(z\u2081)) + D_KL (p(z\u2082(31.6+70+97.75+66) z\u2081)||p'(z\u208235.5+71+67) z\u2081))\nThen, by denoting q\u207d\u2071\u207e(x\u209c\u208a\u2081, x\u209c) the joint distribution of x_t^{(i)} and x_t), the chain rule gives us:\nDKL (q(1) (Xt+1, xt)||q(2) (Xt+1, xt))\\= DKL (9{1)\\9{2))\\\n+ DKL (q(1) (XtXt+1)||q(2) (XtXt+1))\nNote that, due to (1), we have  q^(1) (x{t+1}|X{t})\\= q^(2) (x{t+1}|X{t}) implies DKL (q^(1) (x{t+1}|X{t})||q^(2) (x{t+1}|X{t}))\\\\\\\\\\\\0, Thus DKL (q(1) {t+1}||q^(2) {t+1}),\n DKL (q^(1) {t+1}||q^(2) {t+1}).\n DKL (q^(1) {t+1}||q^(2) {t+1}),+ DKL q^(1) (X_t||X_t||  1,x_t )\\= = DKL q^(1) (X_t||X_t||  1,x_t ), thus have,\nThus, due to the non-negativity of the KL divergence DKL (q(1)(x{t}\\\\  ||x{t}\\\\  1)||q(2)(x{t}\\\\  ||x{t}\\\\  1)), DKL\n= DKL q^(1) {t+1}||q^(2) {t+1}).\nProof. We consider (3) as a Gaussian channel yt = \u221a(\u03b1\u0304_t)x\u2080 + \u03f5\u2080, and x_\u03b8(t) as an estimation of x\u2080(t) given yt. By denoting y_{t} as the best estimator of x\u2080 given yt, we have\n 234+234+57 57 \nE [||xo(t) - xo||] < [|| (yt) - xo||].\\\n\n53 53 \nwhere the equality is from the definition of the MMSE(SNR) function. We are now ready to show (10). In fact, from the triangle inequality, we have:\nE [||xo(t) - Xclean ||] = E [||xo(t) - Xo\\Ea\\\\ > ||xo(t) - Xo||\\\\ - ||Ea\\\\Combine all the above in our equations\nFor comprehensiveness, we now highlight how to derive (11). Particularly, we use the following relation between mutual information of a channel, i.e., I(SNR), and the minimum mean-square error of Gaussian channel:\ndI (SNR)\\\\ = +MMSE(SNR)\\\\  24 24 \n = +MMSE(SNR)\\\\  56 56 234+234+56.\nA.2 Proof of Theorem 2\nTheorem. Let {xt}t\u2208{0,...,T} be a diffusion process defined by the forward equation (1) where x0 is the adversarial sample. i.e, x0 = Xclean + Ea. For any given time t, we have\n\nProof. From the triangle inequality, we have:\nE [||xo(t) - xo||] =E||xo(t) - Xo\\|- Ea\\ \n  4302913953986/4/23\\|-E||26888\n10729\n8191\n 93357936035264\n8116133467046742782\n(2006):"}, {"title": "5.5 Proof of Theorem", "content": "58 58 . . ,, ."}]}