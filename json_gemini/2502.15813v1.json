{"title": "STOCK PRICE PREDICTION USING A HYBRID LSTM-GNN MODEL: INTEGRATING TIME-SERIES AND GRAPH-BASED ANALYSIS", "authors": ["Meet Satishbhai Sonani", "Atta Badii", "Armin Moin"], "abstract": "This paper presents a novel hybrid model that integrates long-short-term memory (LSTM) networks and Graph Neural Networks (GNNs) to significantly enhance the accuracy of stock market predic- tions. The LSTM component adeptly captures temporal patterns in stock price data, effectively modeling the time series dynamics of financial markets. Concurrently, the GNN component leverages Pearson correlation and association analysis to model inter-stock relational data, capturing complex nonlinear polyadic dependencies influencing stock prices. The model is trained and evaluated using an expanding window validation approach, enabling continuous learning from increasing amounts of data and adaptation to evolving market conditions. Extensive experiments conducted on historical stock data demonstrate that our hybrid LSTM-GNN model achieves a mean square error (MSE) of 0.00144, representing a substantial reduction of 10.6% compared to the MSE of the standalone LSTM model of 0.00161. Furthermore, the hybrid model outperforms traditional and advanced benchmarks, including linear regression, convolutional neural networks (CNN), and dense networks. These compelling results underscore the significant potential of combining temporal and relational data through a hybrid approach, offering a powerful tool for real-time trading and financial analysis.", "sections": [{"title": "1 Introduction", "content": "The stock market is a complex and dynamic system that plays a crucial role in the global economy. Stock prices are influenced by complex non-linear polyadic dependencies including economic indicators, market sentiment, geopolitical events, company-specific news, environmental events such as natural disasters, epidemics and pandemics, as well as political tensions, wars, fiscal policies, and legislation (Martinez 2003, Kullmann et al. 2002). These factors create intricate patterns and interdependencies that make stock price movements highly unpredictable.\nAccurate prediction of stock prices is of immense importance to investors, financial analysts, and policymakers. Successful predictions can lead to significant financial gains, informed decision-making, and improved economic stability. However, forecasting stock prices is inherently challenging due to the market high volatility, non-linear dynamics, and the influence of numerous interconnected factors such as economic indicators, political events, and investor sentiment (Fama 1970, Malkiel 2003).\nTraditional statistical methods for stock market prediction often fall short in capturing the intricate patterns and relationships present in financial data. Linear models, for instance, may not adequately account for non-linear dependencies and complex temporal behaviours inherent in stock price movements (Hiremath & Kamaiah 2010).\nIn recent years, there has been a growing interest in leveraging advanced machine learning techniques to enhance predictive accuracy. Deep learning models have shown promise in modelling complex patterns in large datasets. Long Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network (RNN), have been particularly effective in handling time-series data due to their ability to capture long-term dependencies and mitigate the vanishing gradient"}, {"title": "2 Literature Review", "content": null}, {"title": "2.1 Introduction", "content": "Accurate stock market prediction remains challenging due to market volatility, non-linearity, and sensitivity to various factors. Traditional statistical methods such as linear regression and autoregressive models often fail to capture complex dependencies and temporal dynamics in financial data (Gandhmal & Kannan 2019). This has led to increased interest in machine learning and deep learning approaches, which handle complex data sets more effectively and adapt to market fluctuations, improving the modelling of inter-stock relationships and temporal trends (Senapati et al. 2018).\nEarly machine learning models such as Support Vector Machines (SVM) and Artificial Neural Networks (ANN) improved on traditional methods by identifying historical patterns, but struggled with the sequential nature of financial time series. Convolutional neural networks (CNNs) excel in capturing spatial patterns but are less effective with the temporal dependencies essential for stock forecasting (Senapati et al. 2018). To address these limitations, advanced models such as Long-Short-Term Memory (LSTM) networks and Graph Neural Networks (GNNs) have emerged. LSTMs model long-term dependencies in time-series data, while GNNs capture relational data among stocks. Combining these models into hybrid architectures has shown promise in analysing both temporal and relational aspects of stock data (Thakkar & Chaudhari 2021, Ran et al. 2024).\nExplainable AI (XAI) has gained importance in financial forecasting by enhancing transparency and making AI model decisions more interpretable, which is crucial for trust and compliance in financial markets. Studies by Kuiper et al. (2022) highlight that integrating XAI improves decision-making by elucidating how predictions are made."}, {"title": "2.2 Data Acquisition and Challenges", "content": "Developing reliable stock market prediction models requires accurate data acquisition, but financial data is inherently noisy, incomplete, and unpredictable, posing challenges for machine learning algorithms (Hadavandi et al. 2010). Robust preprocessing techniques, including handling missing data and noise management, are essential for enhancing data quality. Optimising hyperparameters during data processing is vital, especially with large and complex datasets. Scaling and normalisation improve the accuracy of models such as LSTM by maintaining variable relationships and preventing data distortion. Detecting and managing noise in financial time series further enhances model resilience in unpredictable market conditions (Yeung et al. 2020). Thus, advanced preprocessing and meticulous hyperparameter tuning are necessary for accurate stock market predictions."}, {"title": "2.3 Machine Learning Approaches in Financial Forecasting", "content": "LSTM networks have become prominent in financial time-series prediction due to their ability to capture long-term dependencies and manage the vanishing gradient problem, making them suitable for modelling the non-linear and volatile nature of stock prices (Fjellstr\u00f6m 2022). They have been applied successfully in various financial contexts,"}, {"title": "2.4 Research Challenges", "content": "Despite the advancements in stock market prediction, several outstanding research issues persist:\n1.  Integration of Temporal and Relational Models: While hybrid models combining LSTM and GNN have shown potential, empirical evaluations in real-world conditions remain limited. There is a need for more extensive studies assessing their performance in volatile, real-time trading environments(Chen et al. 2018, Shi et al. 2024).\n2.  Robust Data Handling: Many models overlook real-time challenges such as noisy data, missing values, and market shifts. Although preprocessing methods have been proposed (Bhanja & Das 2018, Yeung et al. 2020), these need further refinement to better handle the complexities of financial data and enhance model resilience.\n3.  Explainability: The lack of transparency in LSTM and GNN models poses challenges in interpreting predictions. While Explainable AI (XAI) offers solutions (Kuiper et al. 2022), its application in financial models is still limited. Integrating XAI techniques could improve trust and compliance in AI-driven financial forecasting.\n4.  Scalability and Efficiency: Hybrid models are often computationally intensive, making real-time application difficult. Future research should focus on optimising these models for better scalability without sacrificing accuracy, possibly through algorithmic innovations or hardware acceleration.\n5.  Real-Time Adaptation: Although expanding window analysis improves real-time predictions (Feng et al. 2024), models require better strategies for continuous adaptation to new data in fast-changing markets. This includes developing methods to quickly retrain models or update predictions without extensive computational overheads."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Data Collection and Preprocessing", "content": "The dataset was obtained from Kaggle via the YFinance API, providing comprehensive historical stock data essential for time-series analysis. Ten prominent stocks representing diverse sectors were selected: Apple Inc. (AAPL), Microsoft Corporation (MSFT), Comcast Corporation (CMCSA), Costco Wholesale Corporation (COST), Qualcomm"}, {"title": "3.1.1 Feature Engineering", "content": "To enhance predictive capabilities, feature engineering was conducted separately for the LSTM and GNN components. For the LSTM network, the primary input consisted of sequences of closing prices. Structuring the data into sequences enabled the model to capture temporal dependencies effectively. The batch size, a crucial hyperparameter, was optimised during training, with values tested at 11 and 21 to balance computational efficiency and the ability to learn long-term dependencies.\nFor the GNN component, a graph representing relationships between stocks was constructed. Each node represented a stock, and edges represented relationships based on Pearson correlation coefficients and association analysis. Pearson correlation captured linear relationships by calculating the correlation coefficients between the daily returns of stock pairs, while association analysis identified non-linear and complex relationships. Combining these methods provided a comprehensive representation of inter-stock relationships, crucial for modelling the interconnected nature of the stock market."}, {"title": "3.1.2 Data Preprocessing", "content": "Data preprocessing focused on scaling and structuring the data. The raw data was clean, with no missing values or significant outliers. Min-Max Scaling was applied to normalise the data. Specifically, each stock price x was transformed to a normalised value x' in the range [0, 1] using\n$x' = \\frac{X - Xmin}{Xmax - Xmin}$ (1)\nas shown in Equation 1. This normalisation was essential for the convergence of the neural network models and prevented stocks with higher absolute prices from disproportionately influencing the learning process.\nTime-series batches were then created for the LSTM network, each of comprising consecutive daily closing prices. The batch size determined the number of days of historical data instances to be ingested into the model to enable it to make a prediction, impacting its ability to capture dependencies. Outlier detection and removal were omitted to preserve time-series integrity and reflect market realities, as extreme price movements may represent significant events rather than anomalies."}, {"title": "3.2 Graphical Representation of the Stock Network", "content": "The interdependencies between stocks were modelled using a graph G = (V, E) where each vertex v \u2208 V represents a stock, and each edge e \u2208 E represents a significant relationship between two stocks. Pearson correlation coefficients were calculated between the daily returns of each pair of stocks to quantify linear relationships. The daily return rt for a stock at time t was computed as\n$Tt = \\frac{Pt - Pt-1}{Pt-1}$ (2)\nwhere Pt is the closing price at time t (see Equation 2). The Pearson correlation coefficient pij between stocks i and j was computed as\n$Pij = \\frac{\\sum_{t=1}^{T} (ri,t - \\bar{ri}) (rj,t \u2013 \\bar{rj})}{\\sqrt{\\sum_{t=1}^{T} (ri,t - \\bar{ri})^2 \\sqrt{\\sum_{t=1}^{T} (rj,t - \\bar{rj})^2}}}$ (3)\nas given in Equation 3. Edges were established between stocks with |pij| > 0.7, indicating a strong linear relationship.\nTo capture non-linear relationships, association analysis was employed via the Apriori algorithm. In addition to support (the fraction of days over which two stocks moved together) and confidence (the probability of the movement of one stock responsive to the price fluctuations of other stocks), lift was used to gauge whether the co-movement was stronger than random chance. A lift threshold of 1.7 was set, meaning that two stocks appeared 70% more frequently together than if they were independent. Only rules exceeding this threshold (and meeting minimum support and confidence criteria) contributed additional edges in the final undirected graph."}, {"title": "3.3 LSTM Component", "content": "The LSTM network captured temporal dependencies in stock prices by processing sequences of historical data. The architecture included an input layer that accepted normalised closing price sequences over optimised time windows. Multiple LSTM layers were stacked to learn complex temporal patterns, utilising gating mechanisms (forget, input, and output gates) along with cell state updates and hidden states. A dense layer subsequently transformed the LSTM outputs into feature vectors for integration with the GNN component.\nHyperparameters such as learning rate, batch size, and the number of epochs were optimised through experimentation. Learning rates of 0.001, 0.005, and 0.01 were tested, with batch sizes of 11 and 21 evaluated for optimal sequence length. The number of epochs varied between 10 and 50, employing early stopping to prevent overfitting. The Adam optimiser was used for efficient training and Mean Squared Error (MSE) served as the loss function."}, {"title": "3.4 GNN Component", "content": "The GNN component modelled relational dependencies among stocks based on the constructed graph. The architecture comprised an input layer that received the stock graph and initial node features (including stock-specific attributes). Two"}, {"title": "3.5 Hybrid Model Integration", "content": "The hybrid model integrated outputs from the LSTM and GNN components to leverage both temporal and relational information. Temporal embeddings from the LSTM captured historical price patterns, while relational embeddings from the GNN encapsulated inter-stock relationships. These embeddings were concatenated to form a unified feature vector, which was then passed through additional dense layers to learn complex interactions. A final dense layer with a linear activation function produced the predicted closing price. Hidden layers utilised ReLU activations to capture non-linear relationships, and the output layer employed a linear activation suitable for regression tasks. The model was trained using the MSE loss function with the Adam optimiser."}, {"title": "3.6 Training Strategy", "content": "To evaluate the performance of the model in a manner that reflects real-world trading scenarios, an expanding window validation strategy was implemented. Baseline models (including linear models, CNNs, dense neural networks, and standalone LSTM models) were first trained to provide a comparative benchmark. The overall training period was set to two years, with 50 days reserved for testing. In this setup, the model was tested on one day at a time; after each test, the data from the corresponding day was added to the training set, effectively expanding the training window. This iterative process, illustrated in Figure 3, ensured that the model consistently incorporated new information from the most recent market conditions, reflecting a dynamic and adaptive learning process. The model was retrained at each step with the updated dataset, enabling it to learn from evolving market trends and continuously improve its predictions. This strategy prevented data leakage by ensuring that only historical data was used for training and enhanced the robustness of the model."}, {"title": "3.7 Training Parameters", "content": "\u2022 Early Stopping: Implemented based on validation loss to prevent overfitting, with a patience parameter set to halt training if no improvement was observed over several epochs.\n\u2022 Number of Epochs: Varied between 10 and 50, with the optimal number determined through experimentation.\n\u2022 Batch Size: Maintained consistency with the batch sizes used during model input preparation."}, {"title": "4 Experiments and Results", "content": null}, {"title": "4.1 Experiment Setup", "content": "The experiments were conducted on a high-performance computing platform equipped with an NVIDIA GTX 1080 GPU (8 GB VRAM), 16 GB of RAM, and a multi-core Intel i7 processor. This hardware configuration was essential for"}, {"title": "4.2 Performance Analysis", "content": "The hybrid LSTM-GNN model demonstrated strong performance in stock price prediction by effectively integrating time-series data (captured by the LSTM component) and relational data (modelled by the GNN). By accounting for both the temporal patterns of individual stocks and the interdependencies among different stocks, the model achieved enhanced predictive accuracy. Consistently low MSE values were observed across most test days.\nFigure 4 presents the MSE values across all test days using the best parameter configuration. Notably, two significant spikes in MSE were observed on November 10, 2022, and November 30, 2022. These spikes, as depicted in Figure 4, can be attributed to abrupt market volatility triggered by external factors such as major financial news, earnings reports, or geopolitical events."}, {"title": "4.3 Impact of Expanding Window Validation", "content": "The expanding window validation strategy played a crucial role in adapting the model to changing market conditions. By continually updating the training set with new data, the model maintained dynamic, real-time applicability. This method enabled the model to balance between retaining historical trends and incorporating recent market dynamics, ensuring that predictions remained reflective of current market behaviour. The expanding window approach not only enhanced backtesting accuracy but also provides a clear path toward real-world deployment, where constant data updates are necessary. Over time, exposing the model to a larger and more diverse dataset also helped mitigate overfitting and improved generalisation."}, {"title": "4.4 Hyperparameter Tuning", "content": "Hyperparameter tuning was conducted using a grid search to identify the optimal combination of learning rates, batch sizes, and epochs. The final configuration that yielded the best performance consisted of a learning rate of 0.005, 40 epochs, and a batch size of 11. This setup produced the lowest MSE, indicating effective convergence while minimising prediction errors. Early stopping was applied during training to halt the process if no improvement in validation loss was observed for five consecutive epochs, thereby optimising both training time and model performance."}, {"title": "4.5 Comparison with Baseline Models", "content": "The hybrid LSTM-GNN model was evaluated against several baseline models, including Linear Regression, Convolutional Neural Networks (CNN), Dense Neural Networks (DNN), and a standalone LSTM. The hybrid model outperformed all baselines in terms of MSE.\nFigure 5 compares the MSE values across the different models. The hybrid LSTM-GNN achieved the low- est MSE of 0.00144. In comparison, the Linear Regression model recorded an MSE of 0.00224, while the standalone LSTM achieved an MSE of 0.00161. The CNN and DNN models underperformed, with MSE values of 0.00302 and 0.00335, respectively, due to their limited ability to capture temporal dependencies and complex inter-stock relationships."}, {"title": "4.6 Comparative Study", "content": "The comparative analysis demonstrates the clear advantages of the hybrid LSTM-GNN model over baseline models in predictive accuracy and robustness. While the standalone LSTM captured temporal dependencies effectively, it lacked the ability to model inter-stock relationships significantly influencing market behaviour. Incorporating the GNN component enabled the hybrid model to utilise relational data, capturing complex interactions between stocks and enhancing predictions.\nCompared to the standalone LSTM, the hybrid model achieved a 10.6% reduction in average MSE (0.00144 vs 0.00161). This improvement highlights the value of incorporating relational data through the GNN component, which captures both linear and non-linear relationships between stocks. The added contextual information from the GNN enabled the model to leverage insights from stock correlations and broader market trends, improving overall predictive performance.\nThe DNN and CNN models, although capable of modelling non-linear relationships, underperformed due to their inability to capture the sequential nature of stock price movements. Stock prices are inherently temporal, and models that do not account for this structure often fail to identify critical patterns. Consequently, the LSTM-based models outperformed the DNN and CNN, reinforcing the importance of temporal modelling in financial forecasting.\nLinear Regression, as a simple baseline model, showed limitations in capturing non-linear relationships and temporal dependencies, resulting in an MSE of 0.00224. Although its MSE was lower than that of the CNN and DNN models, it was less effective than the LSTM-based models due to its inability to model the intricate dynamics of stock markets. These limitations were especially evident in more volatile stocks, where Linear Regression struggled with complex market movements.\nA key factor driving the hybrid model superior performance was the expanding window training approach. This method progressively increased the training dataset by incorporating new data as it became available, enabling the model to remain up-to-date with recent market trends. Retraining with the most current data enabled the hybrid LSTM-GNN to continuously adapt to changes in market behaviour.\nThe expanding window training approach offers several key advantages. Firstly, it enhances adaptability by enabling the model to learn from recent patterns and market anomalies, which improves predictive accuracy in a dynamic"}, {"title": "5 Discussion", "content": null}, {"title": "5.1 Interpretation of Results", "content": "The hybrid LSTM-GNN model significantly outperformed baseline models in stock price prediction due to its ability to capture both temporal dynamics and relational dependencies. The LSTM component modelled sequential patterns and long-term trends in stock prices, essential in financial time-series data where past events influence future movements. The GNN component captured complex inter-stock relationships by constructing a graph based on Pearson correlation and association analysis, accounting for both linear and non-linear dependencies.\nBy integrating temporal and relational embeddings, the hybrid model leveraged the strengths of both approaches, resulting in lower Mean Squared Error (MSE) compared to the standalone LSTM. The expanding window training approach enhanced adaptability by continuously incorporating new data, ensuring the model remained attuned to recent market conditions\u2014a critical factor in the dynamic financial environment. The robust performance of the model across various stocks, including those which are highly volatile such as AMD, indicates its effectiveness in capturing both stable patterns and sudden market shifts."}, {"title": "5.2 Limitations", "content": "Despite its advantages, the hybrid model has limitations. The increased computational complexity from integrating LSTM and GNN components demands significant processing power and memory, which may not be readily available to all practitioners. The expanding window approach, while improving adaptability, complicates validation since it lacks a separate validation set, increasing the risk of overfitting without proper feedback during training.\nThe model performance is sensitive to hyperparameter tuning, requiring extensive experimentation that can be resource-intensive. Data limitations, such as missing values or anomalies, can degrade the model effectiveness. Additionally, assuming that past relationships persist into the future may not hold during unprecedented market events or structural economic changes, reducing the model predictive accuracy.\nFrequent retraining due to the expanding window method increases computational load, which can be impractical in real-time applications where swift predictions are essential. This could limit the model applicability in high-frequency trading environments that demand rapid decision-making."}, {"title": "5.3 Implications for Practice", "content": "The enhanced predictive accuracy of the hybrid model holds significant implications for real-time trading and financial analysis. It can aid traders and investors in making informed decisions, improving risk management and potentially increasing returns. By accurately forecasting stock prices, the model supports strategies such as algorithmic trading, portfolio optimisation, and risk assessment."}, {"title": "6 Conclusion", "content": "This study introduced a hybrid Long Short-Term Memory (LSTM) and Graph Neural Network (GNN) model for stock price prediction, integrating temporal and relational data to enhance predictive accuracy. The key findings demonstrate that the hybrid model significantly outperforms traditional models such as linear regression, convolutional neural networks, dense neural networks, and standalone LSTM models. The hybrid model achieved a notable reduction in Mean Squared Error (MSE), approximately 10.6% lower than the standalone LSTM, highlighting the effectiveness of incorporating inter-stock relationships through the GNN component.\nThe primary contribution of this research is the demonstration of how integrating temporal dependencies with relational information can lead to more accurate stock price predictions. By utilising the expanding window training approach, the model remained adaptive to evolving market conditions, further enhancing its performance. This approach addresses the non-stationary nature of financial markets, enabling the model to capture both long-term trends and recent market shifts.\nFor future research, several avenues can be explored to build upon the findings of this study. Incorporating additional data sources such as macroeconomic indicators, news sentiment analysis, or social media trends could provide a more comprehensive understanding of the factors influencing stock prices. Refining the model architecture to improve computational efficiency would make it more suitable for real-time trading applications. Additionally, extending the model to predict other financial instruments, such as commodities, cryptocurrencies, or foreign exchange rates, could validate its applicability across different markets.\nIn conclusion, the hybrid LSTM-GNN model presents a significant advancement in stock price prediction by effectively capturing the complexities of financial data. This research contributes to the field by demonstrating the value of combining temporal and relational modelling, offering a promising direction for future developments in financial forecasting."}]}