{"title": "Regulating Multifunctionality", "authors": ["Cary Coglianese", "Colton R. Crum"], "abstract": "Foundation models and generative artificial intelligence (AI) exacerbate a core regulatory challenge associated with AI: its heterogeneity. By their very nature, foundation models and generative AI can perform multiple functions for their users, thus presenting a vast array of different risks. This multifunctionality means that prescriptive, one-size-fits-all regulation will not be a viable option. Even performance standards and ex post liability-regulatory approaches that usually afford flexibility are unlikely to be strong candidates for responding to multifunctional AI's risks, given challenges in monitoring and enforcement. Regulators will do well instead to promote proactive risk management on the part of developers and users by using management-based regulation, an approach that has proven effective in other contexts of heterogeneity. Regulators will also need to maintain ongoing vigilance and agility. More than in other contexts, regulators of multifunctional AI will need sufficient resources, top human talent and leadership, and organizational cultures committed to regulatory excellence.", "sections": [{"title": "Regulating Multifunctionality", "content": "Many types of tools and technologies perform multiple functions. Consider one of humanity's most primal of tools: the knife [30]. The knife is not a singular tool; rather, it comes in many different varieties that serve many functions, each of which can generate value for society. Knives are used in the kitchen to prepare delicious meals, and then they are used by diners to consume those same meals. Knives carve objects, cut rope, and open packages. They clear paths through forests and jungles, and they help in harvesting seasonal crops. Knives can be used, of course, to injure or kill people. But in the hands of surgeons, knives are routinely used to save lives. And even though knives take many different forms and are often designed for many different purposes\u2014think of, for example, the many types and sizes of surgical scalpels, woodcarver's chisels, and kitchen implements, among others\u2014knives designed for one purpose also can be adapted for different uses, as anyone who has used a dinner knife to open a postal letter can attest. Many knives, though, are deliberately intended to serve multiple functions, as is the case with a simple pocketknife or, even more emblematically, the classic Swiss army knife, some models of which boast a combination of more than 30 different tools in one.\nThe proliferation of functions performed by different knives has led over the years to different forms and sources of rules governing their manufacture, sale, and deployment. Many governments place restrictions on the carrying of knives in public when they exceed specified sizes or take a particular form, such as switchblades or butterfly knives. Around the world, airport regulations prohibit the carrying of knives of virtually any kind or size onto the passenger cabins of commercial aircraft. Other rules related to the use of knives take the form of professional licensure standards\u2014such as for surgeons\u2014or private rules of management practice\u2014such as hospital procedures that demand strict accounting of all knives and other surgical tools so that none are accidentally left inside patients. In other spheres, private management standards emphasize training and protocols. Cooking schools provide extensive training in the handling of knives before they award certifications to their students. Youth recreational programs and camps place age restrictions and require adherence to strict safety protocols for when and how camping or craft knives are used. And, of course, overarching all of these other forms of governance, when knives are used improperly or intentionally in ways that harm other people, governmental rules that impose criminal or civil liability come into play.\nDigital tools are not entirely unlike their physical counterparts. They, too, call for a variety of different regulatory interventions based on their uses as well as overarching risks. As with knives, AI tools come in many different varieties and serve many different functions that provide much positive value to individuals and society. Some AI tools\u2014such as foundation models\u2014are deliberately multifunctional. They are the digital equivalents of the Swiss army knife. Just as it is hard to imagine every possible use that different individuals might make of actual Swiss army knives, it is hard\u2014even impossible\u2014to specify in advance every single use that can be made of foundation models. Of course, foundation models take multifunctionality to a new level altogether compared with physical tools like the Swiss army knife. Given their extensive range of possible uses, foundation models greatly exacerbate the core regulatory challenge that AI more generally presents: its heterogeneity [13]. As AI tools become ubiquitous throughout modern society, their varied nature and highly diverse applications\u2014a heterogeneity that is rapidly expanding and changing\u2014make it increasingly difficult to categorize, define, and subject all the functions that need to be governed.\nThe result is one of the most daunting governance challenges of our time. How can digital tools that put multifunctionality on steroids ever be subjected to meaningful public oversight and regulatory control so society can be sure that they are used for good, while only posing acceptable, well-managed risks to members of the public? This is the core governance question confronting society in an era of foundation models and generative AI. Regulating multifunctional AI may seem like an impossible"}, {"title": "AI's Multifunctionality", "content": "The multifunctional nature of AI derives from the demand to use it for highly varied purposes. This \u201cuse heterogeneity\" also drives AI's \u201cdesign heterogeneity\u201d\u2014something which is true for physical tools, too. Knives take different shapes and forms (design heterogeneity) when they are intended for different uses. A surgeon's scalpel, for example, has a different shape and size than a hunter's sheath knife. The design of an AI tool also can vary depending on the type of use. But, also as with physical tools, the same basic AI tool can perform multiple functions. Just as it is possible to use a standard kitchen knife to open the wrapping of a birthday present, the same type of AI tool or model especially a foundational model-can be deployed for different uses. In grappling with AI governance, it is important to recognize the extreme heterogeneity that accompanies AI\u2014in terms of differing designs and, more importantly, across differing uses. It is also important to keep in mind the distinction between tools designed for single or multiple purposes because the intended range of foreseeable uses of a tool can provide regulators the awareness needed to address the tool's associated risks.\nThe uses for which AI can be deployed are virtually limitless. It is this use heterogeneity-especially with the multifunctional nature of foundation models and generative AI tools\u2014that presents the most substantial challenge when it comes to regulating AI [13, 21]. Use heterogeneity arises from (a) the existence of a vast array of different AI tools being designed for specific but varied purposes in mind, and (b) AI tools that are themselves designed for multiple purposes, perhaps only some of which can be fully anticipated.\nUse heterogeneity arising from (a)\u2014the existence of varied tools designed with different specific purposes in mind\u2014describes many physical and digital tools. Like many knives, for example, traditional AI tools are often designed for a specific purpose, each intended to satisfy a single-function use case. That is not to say that these traditional AI tools oriented toward single uses are feeble or of limited importance. On the contrary, they can prove quite powerful and important. AI tools expressly designed and trained to classify cancerous tumors would fall into the category of a single-function use case, and yet they could hardly be more important. This is not unlike a surgical scalpel that a physician uses to excise a patient's brain tumor: a vital, albeit single-purpose tool. For these single-function tools (both in the physical and digital realms), it is often possible for regulators or other standard-setters to develop detailed, targeted descriptions of the well-designed tool, such as an AI model's architectural components or the scalpel's type of handle, or specifications about how or by whom the tool is used to ensure that it is deployed optimally to deliver benefits and minimize risks.\nBy contrast, use heterogeneity arising from (b)\u2014that is, from the existence of versatile technologies"}, {"title": "The Multifaceted Governance of Multifunctionality", "content": "The heterogeneity of problems posed by AI creates a core challenge for AI governance. This is true for both varied single-function AI tools as well as for multifunctional AI tools. The problems presented by a specialized AI tool that misreads MRIs used to identify cancer, for example, will obviously be quite different from those presented by a specialized AI tool assisting in the automatic braking of an autonomous vehicle. Likewise, the problems created by an LLM tool \u201challucinating\u201d about legal cases will be different from these same foundational LLM tools put to use in social media that contribute to teenagers engaging in self-harm. The question thus becomes how society should approach regulation when both uses and problems can be so varied. In other words, how does society regulate the AI equivalent of the Swiss army knife?\nIt should be recognized initially that, among the array of problems arising from AI, many can be grouped into categories that reflect classic market failure problems that have historically necessitated regulatory intervention: market concentration, information asymmetries, and externalities. When A\u0399 is used to power automated pricing mechanisms by online retailers, for example, it may lead to anti-competitive practices, distorting the market and harming consumer welfare [1]. When embedded in health care products and consumer goods, AI tools can create information asymmetries, as consumers will not be as aware as manufacturers of any safety risks. And the risks posed by AI-enabled self-driving cars illustrate how AI tools can create the kind of externalities that typically justify some regulatory oversight.\nIn addition, AI tools can present concerns beyond those considered to be classic market failures. When models are trained on biased data that reflect societal prejudices, they can perpetuate and even amplify existing inequalities. This risk is particularly acute in employment contexts, where algorithms used in hiring processes may unintentionally reinforce discriminatory practices. AI tools also pose a range of other problems or potential problems, such as those arising from the amount of energy they demand, their"}, {"title": "The Ill Fit of Prescriptive Regulation", "content": "Just as multifunctional physical tools such as knives can be used in both beneficial and harmful ways, so can AI tools. When any tool or technology delivers benefits but comes with side effects or risks, it becomes a prima facie subject for regulation. Regulation seeks to manage risks, not necessarily to prevent them entirely. After all, consider the loss that humanity would incur if all knives and other sharp objects were banned instead of regulated simply because they also posed some residual dangers. In a similar manner, it would surely be undesirable, even if it were possible (which is doubtful), to ban foundation models and generative AI outright, purely because they can pose risks. But just as different risk management rules\u2014public and private\u2014have arisen around the use of physical tools, such as knives, we should expect to see an array of regulatory efforts to manage the risks posed by AI tools. The same basic array of approaches taken in regulating multifunctional physical tools and other risky technologies and behaviors can be applied to regulating foundation models and generative AI tools.\nOne common regulatory strategy, though, will prove especially difficult when it comes to regulating the multifunctionality of foundation models: the so-called prescriptive strategy. Prescriptive regulatory measures\u2014sometimes called micro-means rules [43, 15]\u2014impose fixed mandates on the design of products or the actions or behavior of regulated entities, including in their development or use of products. With AI, for example, this would mean specifying exactly how algorithms need to be designed and trained-such as what particular algorithmic forms and mathematical functions would be permissible to be used or how different algorithms could be used. This prescriptive approach will prove to be both unrealistic and inapplicable to governing multifunctional AI for several reasons.\nFor one, as discussed previously, Al's use heterogeneity makes it unrealistic that regulators would ever have enough information to tell the firms developing AI exactly how their explicit digital tools can and cannot be designed or used. Moreover, regulators would need to foresee all the uses of an AI tool's client or user base. With foundation models, the sprawling and often surprising uses (including both beneficial and problematic) that the public has found since the release of OpenAI's ChatGPT reveals the"}, {"title": "Flexible Regulatory Approaches", "content": "Instead of rigid, prescriptive rules, the future of AI regulation will likely depend on more flexible regulatory strategies. At least four more feasible regulatory strategies exist that could be considered for the governance of multifunctional AI: performance standards, disclosure regulation, ex post liability, and management-based regulation. These are by no means panaceas, as the conditions for their effective use will not always apply. Still, they are more promising in the face of AI's hyper-heterogeneity than so-called prescriptive rules. Moreover, regulators can rely on a combination of these strategies depending on their overarching goals and context. Importantly, none of these strategies necessarily demand a \u201cone size fits all\" approach."}, {"title": "Performance Standards", "content": "Performance standards could be conceivably used for known and well-defined problems with specific applications of AI. These standards will thus be more likely to apply to AI tools that are intended to serve a single or limited set of functions, where outcomes can be adequately measured and defined with precision. AI tools that might suitably come under performance standards will typically be those of the more \"traditional AI\u201d variety. However, even multifunctional AI models, such as LLMs, may be re-purposed to serve a single function\u2014akin to a Swiss Army knife only being used to open mail. Although it may make more sense from the manufacturer's point of view to change knives, the ease of grabbing the digital equivalent of the Swiss army knife-foundational models\u2014\u201coff the shelf\u201d has led many users to rely on them for singular functions. For example, a foundational model may be tasked to take patient information and respond with medical advice. Or it may be incorporated into a legal research tool to support specific tasks such as citation checking or contract compliance detection. Performance standards might well be applicable within these narrow and well-defined contexts, even though multifunctional AI serves as the foundation for these specific function tools.\nOf course, once the purpose or function of an AI tool changes or increases in complexity, performance standards will be less viable. Consider the success of a foundation model built into a digital system used by a restaurant to take orders and relay them to the kitchen staff. That use may seem benign. But if that same foundation model is then repurposed to answer customer questions related to life-threatening allergens, the consequences could be significant. If the same model is further used to communicate in other ways with customers, it could potentially respond with offensive answers. Performance standards are by necessity designed for identifiable problems. Unless all problems can be specified by the regulator in advance, this approach will be incomplete. Performance standards also necessitate the existence of some way for regulators to test or monitor for compliance, which might be exceedingly difficult to ensure, even when suitable problems can be anticipated and standards defined [12]. These limitations may, in many if not most cases, make reliance on performance standards as infeasible as reliance on prescriptive or micro-means regulations [13].\nFirms, rather than regulators, will generally be in a much better position (assuming they have the right incentives) to understand the ways in which users can use their technologies, the broad range of problems associated with the tools they develop, and how to monitor for the presence or level of those problems. Furthermore, an algorithm's design might be thought of as a performance \u201cstandard\u201d itself, as it is through this design that firms can effectively \u201cembed\" value choices in ways that keep AI tools in check. But just as with prescriptive regulation, regulators will be clearly at a disadvantage in instructing anyone about how algorithms should be designed, which will also necessarily imply limits on their ability to rely on their own externally crafted performance standards. Regulators' main challenge will likely instead be to ensure that firms use their comparative advantage to establish their own internal \"performance standards\" to protect consumers and the public from undue harm from their tools-an approach we discuss further below in the section of this chapter on management-based regulation.\""}, {"title": "Information Disclosure Regulation", "content": "In the face of the information asymmetries associated with advanced technologies, regulators can require that firms disclose information. Information disclosure regulation may enable regulators to do better by way of monitoring the risks associated with AI technology and potentially provide an incentive for firms to improve their management of those risks [34]. With respect to AI, two main types of information disclosure regulations are plausible.\nThe first type might be called existence disclosure regulation. Under this type, consumers and other"}, {"title": "Ex Post Liability", "content": "Another flexible regulatory tool for governing AI involves the development of standards for ex post liability once problems with an AI tool arise. Given multifunctional AI's hyper-heterogeneity, ex post liability must surely be considered as part of an overall multi-faceted approach to AI governance. It is not surprising that there have been many calls to implement ex post liability, whether through AI-specific insurance compensation schemes [37, 38] or by reliance on tort law [10, 22, 23, 33, 40]. Ex post liability may be advantageous because it does not demand that a regulator know in advance whether a general LLM will cause damages from errors in data collection or cleansing (e.g., biased data), specific architectural components (e.g., the algorithmic design), or training configurations\u2014or from fine-tuning or even hardware problems. Nor does the regulator have to know with precision in advance how to define"}, {"title": "Management-Based Regulation", "content": "A more realistic preventative strategy for regulating multifunctional AI would embrace its hyper-heterogeneity and apply rules that leverage the private sector's ability to provide a frontline defense against its risks. Management-based regulation is arguably the only regulatory strategy equipped to handle the heterogeneity challenges posed by foundational and generative AI. This type of regulation is already used widely in other diverse and dynamic risk settings where neither prescriptive nor performance standards are feasible [13, 20, 11].\nUnder a management-based approach, a regulated entity is required to develop an internal plan to identify and monitor risks, establish and implement protective procedures to manage those risks, and document changes made to keep addressing those risks over time. A management-based approach applied to multifunctional AI would obligate AI developers to think deeply about and gather information on the broad range of potential uses of their multifunctional tools and subsequently take those uses into"}, {"title": "Regulatory Vigilance and Agility", "content": "Just like a surgeon choosing the appropriate knife, or a firm selecting the appropriate AI chatbot for customer service, regulators must select the \u201cright\u201d regulatory tools to fulfill their vital mission. But selecting a particular regulatory strategy or a combination of strategies\u2014will be only the beginning. Perhaps the most difficult work in regulating multifunctionality will lie in providing ongoing vigilance and agility. Effective AI governance will require constantly adapting, issuing alerts, and prodding action. Regulators need to see themselves as overseers of dynamic AI ecosystems, staying flexible, remaining vigilant, and always seeking ways to improve.\nMaintaining regulatory vigilance and ability will demand resources: financial resources, technological tools, and, perhaps somewhat ironically, human capital [41, 14]. Human oversight of complex, flexible regulatory systems is essential. Without fervent attention to changes in AI's uses and problems, new risks can emerge and threaten the public. Regulatory slippage can also arise, especially if private firms'"}, {"title": "Conclusion", "content": "As powerful, multifunctional digital tools, foundation models and generative AI tools operate like Swiss army knives. They offer a seemingly limitless number of potential uses and create an array of exciting opportunities when used safely and fairly. But they are not without their risks. Furthermore, they exacerbate the core challenge in regulating AI: its heterogeneity. AI's heterogeneity begins with its numerous uses and extends to the number of potential problems it can create. It is AI's heterogeneity of uses and problems that makes it akin to a very expansive, multi-tooled Swiss army knife. Although regulating multifunctionality is a daunting challenge, regulators can meet that challenge by taking a multifaceted approach. Much as physical tools\u2014such as actual knives\u2014fall under different sets of rules depending on their context, AI tools can also come under varying regulatory regimes based on their uses.\nRegulators should not expect to be able to rely solely on rigid, prescriptive rules, as if they could provide fixed \u201cguardrails\u201d that protect the public from the risks of generative AI and foundation models [16]. \u201cOne size\" will decidedly not \u201cfit all\u201d when it comes to such multifunctional AI. Instead, regulators will need to consider more flexible strategies: performance standards, disclosure regulation, ex post liability, and management-based requirements. These more flexible approaches, when applied wisely and, as needed, in combination with one another, can give regulators a better grip on the use and problem heterogeneity of multifunctional AI.\nExperience with regulating heterogeneous processes and products in other realms suggests that successful AI governance will rest ultimately upon efforts to build human capital and organizational capacity-both within firms and across regulatory bodies. These efforts should aim to facilitate regulatory flexibility and an ability for rules and private risk management behavior to adapt to ever-changing environments. Of course, regulatory flexibility will itself become risky if not combined with regulatory vigilance and agility, so that appropriate action can be taken swiftly when needed to respond to new problems as they emerge. The road ahead demands the utmost in regulatory excellence\u2014but just as with strategies for addressing the risks associated with other multifunctional technologies, meaningful, multifaceted regulatory strategies do exist for responding to the risks of foundation models and generative \u0391\u0399."}]}