{"title": "ResidualDroppath: Enhancing Feature Reuse over Residual Connections", "authors": ["Sejik Park"], "abstract": "Residual connections are one of the most important components in neural network architectures for mitigating the vanishing gradient problem and facilitating the training of much deeper networks. One possible explanation for how residual connections aid deeper network training is by promoting feature reuse. However, we identify and analyze the limitations of feature reuse with vanilla residual connections. To address these limitations, we propose modifications in training methods. Specifically, we provide an additional opportunity for the model to learn feature reuse with residual connections through two types of iterations during training. The first type of iteration involves using droppath, which enforces feature reuse by randomly dropping a subset of layers. The second type of iteration focuses on training the dropped parts of the model while freezing the undropped parts. As a result, the dropped parts learn in a way that encourages feature reuse, as the model relies on the undropped parts with feature reuse in mind. Overall, we demonstrated performance improvements in models with residual connections for image classification in certain cases.", "sections": [{"title": "1. Introduction", "content": "Residual connection [38] is such a general and popular technique that almost no model exists without it. For example, it is utilized in foundation models like GPT-4 [80], Llama 2 [102], and Stable Diffusion XL [84]. This is because residual connections allow gradients to flow more directly through layers, reducing the vanishing gradient problem. Additionally, the advantages of residual connections are sometimes interpreted as benefits of leveraging identity mapping across layers [39].\nOur work shows that, despite the presence of residual connections, there are limitations in effectively utilizing identity mappings. To address this limitation, we propose an algorithm that provides additional opportunities for the model to learn identity mappings. This approach further enhances the model's performance.\nThe distinguishing feature of residual connections is their skip connection. This means that for a block within the model, the input x goes through a series of transformations F and then x is added back to produce the output. In other words, the operation $F(x) + x$ is performed for the block. This can be seen in (a) of Figure 5.\nThe residual connection utilizes the previous block's output directly with the skip connection, which can be interpreted as beneficial for feature reuse [47]. However, there is limited analysis within models on how features from previous blocks are reused. Considering the difficulty that deep learning models face when learning sparse matrices [111], it can be anticipated that reusing features across multiple blocks through residual connections may be challenging. Therefore, we analyzed the potential limitations of feature reuse in model training that includes residual connections.\nTo analyze feature reuse within the model, we visualized the node outputs directly. Specifically, we employed TFMeter [44], an extension of TensorBoard Playground, which trains the model on a two-dimensional dataset and visualizes the output of nodes for grid inputs by generating contour plots. In this process, we scaled the analysis from the maximum of 6 layers supported by TFMeter to 32 layers. Through this, we demonstrated a deficiency in feature reuse across multiple layers, not only in small models but also in large models.\nTo address the deficiency, we propose an algorithm called ResidualDroppath. ResidualDroppath alternates between two types of iterations during training. In the first iteration, the model is trained with enforced feature reuse without any transformation over parts of the block by applying droppath [46]. In the second iteration, the model learns to consider whether to utilize these paths. This approach enables the model to reuse features in their identity form, facilitating feature reuse across multiple layers.\nThen, we verified whether the proposed ResidualDroppath is beneficial for deep learning training in image classification tasks. To this end, we applied our algorithm to ResNet50 and ResNet50d models and tested it on the CI-"}, {"title": "2. Related Work", "content": "Our paper is broadly divided into an analysis of how the intermediate outputs of a model change depending on learning, and methods to enhance them. Therefore, we summarize related work on feature analysis in Section 2.1. Additionally, we summarize the methods to enhance feature learning in Section 2.2."}, {"title": "2.1. Feature Analysis", "content": "Recent works have conducted feature analysis by leveraging deep and high-dimensional neural networks to handle complex data. For instance, the feature output from intermediate layers is encoded using sparse autoencoders, and the correlation between these encoded values and the final output is analyzed [17, 100]. Another approach involves utilizing Singular Value Decomposition (SVD) to identify the presence of fine-grained concepts within the features [33]. Alternatively, clustering features can create a dictionary of meaningful feature units [21]. In contrast, we use a method where we train on a two-dimensional toy dataset and visualize the output on a grid input, plotting each dimension individually in two-dimensional space [44]. In other words, we extend traditional visualization methods, originally intended for educational purposes, to more complex network architectures and learning algorithms. This approach provides an intuitive visualization of the features, which helps gain new insights into the critical aspects of the learning process. For additional feature analysis methods, please refer to Section 6.2 of the Supplementary Materials."}, {"title": "2.2. Enhancing Learning Feature", "content": "The methods for influencing connections within the model to enhance performance based on the characteristics of learned features, included in PyTorch Image Models [112], which is designed to reproduce a wide variety of state-of-the-art (SOTA) models, are as follows. First, there is a DenseNet method of connecting residual connections to all layers [47, 53]. Next, there is the droppath method, where only the residual connection is retained while the computed values of the block are probabilistically masked [46, 81]. Similarly, there are methods like dropout [96] and dropblock [31], which apply masks to only a part of the block rather than the entire block. These methods can be regarded as approaches that allow the model to utilize short networks or thinned networks. On the other hand, our algorithm pro-"}, {"title": "3. Method", "content": "In this paper, we propose an algorithm based on the analysis of features. Specifically, we analyze each dimension of the intermediate outputs of the model as features in Section 3.1. Moreover, our feature analysis reveals that, even with residual connections, feature reuse across multiple layers is challenging. To address this limitation of feature reuse, we propose the ResidualDroppath algorithm in Section 3.2."}, {"title": "3.1. Feature Reuse Across Multiple Layers", "content": "In this session, we demonstrate that naive residual connections may impede feature reuse across multiple layers. Specifically, when similar features appear in distant layers, we show that intermediate layers do not effectively leverage these features by transmitting them close to identity. This suggests that even when feature reuse is necessary, the model may tend to learn by applying additional transformations. Furthermore, we show that this learning approach may be disadvantageous in terms of information retention.\nWe utilized TFMeter [44], an extension of the TensorBoard Playground, to analyze feature reuse. The primary distinction between the analysis conducted with TFMeter and our own lies in the network depth: while previous studies examined networks with a maximum of 6 layers, we extended this to a significantly deeper multi-layer perceptron (MLP) architecture, with up to 32 layers. For the training and test datasets, we adopted the following approach: a toy dataset containing 16,384 samples was generated for a binary classification task, with two classes based on a spiral function with 2D inputs. For visualization, the test data was designed with grid inputs, enabling us to observe not only the regions covered by the training data but also the potential range of values, including both in-distribution and out-of-distribution areas. The detailed overall process is as"}, {"title": "3.2. Algorithm", "content": "To reduce unnecessary transformations within the model that may cause a loss of information when learning to utilize features with similar distributions across different layers, we propose ResidualDroppath. This approach enhances feature reuse by iteratively alternating between two types of training iterations. The first type of iteration uses Droppath to enforce feature reuse by dropping certain residual parts, compelling the model to learn specifically in cases where feature reuse occurs. The second type of iteration references the first, centering the learning process on the dropped"}, {"title": "4. Experiments", "content": "In this section, we evaluate the effectiveness of our ResidualDroppath on the image classification task. For the image classification task, we used the MNIST [59], CIFAR10 [58], and ImageNet1k [88] datasets. The model architectures used for applying the algorithm were ResNet50 [38] and ResNet50d [41]. Detailed explanations and results for each component are as follows."}, {"title": "4.1. Settings", "content": "Our experiment aimed to demonstrate that our algorithm performs well across various dataset scales by using three datasets with differing image sizes and quantities. The datasets used were as follows: First, we utilized MNIST, a grayscale dataset with an image size of (28, 28). This dataset consists of 70,000 images across 10 classes representing different digits, with 60,000 images used for training and 10,000 images for testing. Second, we used CIFAR-10, a color dataset with an image size of (32, 32). This dataset includes 60,000 images across 10 classes that distinguish objects and animals, with 50,000 images for training and 10,000 images for testing. Third, we employed ImageNet-1k, a color dataset with an image size of (224, 224), comprising approximately 1.3 million images across 1,000 classes of specific objects and animals. Of these, 1,281,167 images were used for training, and 50,000 images were allocated as a validation dataset, which we also used as the test dataset.\nTo ensure training within a reasonable time frame, given the differing scales of the datasets, we adjusted the batch size accordingly. We used a batch size of 128 to train MNIST and CIFAR-10 on a single A6000 GPU. For ImageNet-1k, we used a batch size of 1024 and trained on four A100 GPUs.\nThen, we aimed to demonstrate that our algorithm provides significant performance improvements when applied to model architectures that leverage residual connections. For this purpose, we used ResNet50 and its enhanced version, ResNet50d, with hyperparameters set to the default values provided in PyTorch Image Models [112].\nThe baseline comparison was conducted using a model trained with a standard training procedure (without any additional procedure like Droppath or ResidaulDroppath) and a model trained with the Droppath training procedure [46]. For Droppath, all hyperparameters were set identically to the default settings in PyTorch Image Models [112], except for one. Specifically, the drop rate was set to 0.1. The one difference in settings relates to maintaining the distribution scale of the residual output. In Droppath, a scaling factor is applied based on the drop rate to preserve the distribution of the residuals as they would be without Droppath. However, in our ResidualDroppath, the dropped part of the residual distribution should be treated as a zero matrix, so scaling was not applied."}, {"title": "4.2. Results", "content": "As shown in Table 1, our method demonstrates superior top-1 and top-5 accuracy compared to other methods when information is richer for each class. This indicates that our algorithm effectively helps retain and leverage the increased information for learning. In other words, this highlights the effectiveness of our Droppath algorithm in enhancing the performance of models with residual connections by facilitating feature reuse. A comprehensive analysis is provided below.\nOur method provides a slight performance edge on the MNIST dataset. Specifically, our algorithm achieves a top-1 accuracy of 99.32% with the ResNet50 model, showing an improvement of 0.06% and 0.03% over the Standard and Droppath methods, respectively. Similarly, with the ResNet50d model, our approach demonstrates a performance improvement of 0.07% and 0.05% compared to the Standard and Droppath baselines.\nOur algorithm shows a greater performance difference compared to other methods on the CIFAR-10 dataset than on the MNIST dataset. This may be due to its effective utilization of newly added color information, which is not present in MNIST. Detailed results are as follows: our algorithm achieves a Top-1 accuracy of 90.84% on the ResNet50d model. This demonstrates a performance improvement of over 1% in Top-1 accuracy compared to other methods, such as Standard and Droppath. Similarly, enhanced results are observed on the ResNet50 model in both Top-1 and Top-5 accuracies.\nOur algorithm demonstrated the best performance on the ImageNet-1K dataset with ResNet50d, although it achieved slightly lower performance than Droppath when applied to ResNet50. This may be attributed to a reduction in information per class as the number of images per class decreases. The detailed results are as follows: For ResNet50d, our algorithm achieved a Top-1 accuracy of 76.57%, marking the highest performance. In terms of Top-5 accuracy, our model showed a gain of 0.38% over the standard model and 0.01%"}, {"title": "5. Conclusion", "content": "With similarity analysis of layer-wise features on a toy dataset, we demonstrate that residual connections in existing methods are inefficient in preserving and utilizing information across multiple layers. To address this, we alternate between two-stage iterations: enforcing full information preservation through a masked residual connection and learning the enforced residual components. As a result, our proposed algorithm improves performance on various image classification datasets using ResNet50 and ResNet50d.\nOur limitation is that ResNet-50 with our algorithm does not achieve the best performance on ImageNet-1K. We interpreted this as potentially due to differences in the amount of information across classes, suggesting the need for a more detailed analysis of this factor. Furthermore, our method does not account for the possibility that, as illustrated in Feature 4, multiple nodes within the same layer may exhibit similar feature distributions. Although our approach assumes a distribution shift across intermediate layers when using the same features across multiple layers, the actual model training process may involve complex inter-node relationships that allow for more efficient feature reuse.\nFor future work, we plan to investigate how the model can retain features within each layer to enable effective feature reuse. As shown in Figure 1 or Figure 4, transformations occur between features of different layers through weights. This implies that, to achieve feature reuse, certain nodes may need opposing signs to allow for cancellation, or conditions where all weights are effectively zero. In summary, exploring the characteristics of features that are useful for the model could be a valuable research direction. Additionally, for models with residual connections, architectures beyond ResNet, such as transformers, are worth considering."}, {"title": "6. Supplementary", "content": "We appreciate the high-performance GPU computing support of HPC-AI Open Infrastructure via GIST SCENT."}, {"title": "6.2. Related Work: Feature Analysis", "content": "There are several methods to analyze features: directly analyzing features, analyzing the relationships between features, or analyzing the differences that arise from modifying features. Direct methods involve analyzing features, including the output, which has been addressed in various studies [3, 21, 33, 40, 48, 54, 66, 74, 78, 94, 97, 100, 105, 106, 128]. When considering relationships, methods can involve examining the relationship between input and features [7, 8], between output and features [5, 48, 94, 118, 127], or the relationship between input and output [3]. Methods that analyze differences resulting from changes include modifying features and observing the change in output [11, 15, 21, 30, 43, 49, 50, 61, 65, 79, 82, 83, 101, 110, 122, 123, 126, 131], using only a subset of features to observe the differences [9], or modifying the input to see the changes in output [49]. Feature analysis can be performed within a single model [19, 74, 117], or across different models [1, 30, 66, 128]. The visualization-based analysis we conducted falls under the category of direct feature analysis within a single model."}, {"title": "6.3. Related Work: Enhancing Learning Feature", "content": "To improve feature learning, appropriate feature exploration should be conducted to allow for the learning of compositional features [22] [82, 83]. In other words, learning should proceed with exploration that allows for exploitation in meaningful regions. The following approaches can be taken to achieve this goal: either directly increasing the exploration space by providing diverse features to the model through augmentation and other techniques [10, 11, 15, 32, 43, 50, 61, 63, 65, 69, 82, 83, 89, 101, 106, 108, 110, 122, 126, 131], or indirectly designing a model that can learn the characteristics of the data [2, 5, 12, 16, 23, 24, 28, 29, 35-37, 45, 51, 52, 54, 62, 67, 70, 71, 73, 85-87, 90-93, 95, 98, 99, 114-116, 120, 124, 129]. Alternatively, the loss function can be adjusted to better align with the learning objectives [4, 6, 13, 14, 18, 55, 60, 64, 75, 77]. Another approach involves reducing the feature space that can be learned in a controlled manner using regularization [19, 25-27, 34, 57, 68, 76, 78, 103, 104, 107, 109, 113, 117, 119, 121, 125, 130]. Our approach adds a regularization that utilizes residual connections to help feature exploration during learning."}]}